@article{KARIM2020177,
title = {Modeling and Simulation of a Robotic Bridge Inspection System},
journal = {Procedia Computer Science},
volume = {168},
pages = {177-185},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.276},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304154},
author = {Muhammad Monjurul Karim and Cihan H. Dagli and Ruwen Qin},
keywords = {Simulation modeling, bridge inspection, UAV, cellular automata, deep learning, discrete-event simulation, AnyLogic},
abstract = {Inspection and preservation of the aging bridges to extend their service life has been recognized as one of the important tasks of the State Departments of Transportation. Yet manual inspection procedure is not efficient to determine the safety status of the bridges in order to facilitate the implementation of appropriate maintenance. In this paper, a complex model involving a remotely controlled robotic platform is proposed to inspect the safety status of the bridges which will eliminate labor-intensive inspection. Mobile cameras from unmanned airborne vehicles (UAV) are used to collect bridge inspection data in order to record the periodic changes of bridge components. All the UAVs are controlled via a control station and continuously feed image data to a deep learning-based detection algorithm to analyze the data to detect critical structural components. A cellular automata-based pattern recognition algorithm is used to find the pattern of structural damage. A simulation model is developed to validate the proposed method by knowing the frequency and time required for each task involved in bridge inspection and maintenance. The effectiveness of the model is demonstrated by simulating the bridge inspection and maintenance with the proposed model for five years in AnyLogic. The simulated result shows around 80% of man-hour can be saved with the proposed approach.}
}
@article{JAIN2019101016,
title = {MVO-based path planning scheme with coordination of UAVs in 3-D environment},
journal = {Journal of Computational Science},
volume = {37},
pages = {101016},
year = {2019},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877750318310263},
author = {Gatij Jain and Gaurav Yadav and Dhruv Prakash and Anupam Shukla and Ritu Tiwari},
keywords = {Path planning, Meta-heuristics, Multiverse optimizer, Swarm intelligence, UAV, Robotics},
abstract = {The path planning of UAV deals with the process of figuring the most optimal path from the source to the destination and avoiding obstacles on course to avoid collisions in a given environment. This particular problem is classified as an optimisation problem in high dimension and comes under the category of NP-Hard. Due to the usage of UAVs in many domains like the army, communications, security, disaster management, survey mapping etc., the provisioning of time efficiency as well as accuracy is a critical and challenging task in a 3-D environment. Moreover, the ability of the UAVs can be enhanced if there is coordination between them to find the best cost-effective target in a dynamic manner. However, a majority of the solutions reported theoretically are not efficient concerning time efficiency, accuracy and coordination simultaneously for many applications. Drawing motivation from this, the paper gives a novel way of solving the problem of planning of path for UAVs in 3-D while maintaining the coordination for choosing the target. A new algorithm based on Multiverse optimizer (MVO) algorithm is applied to the problem. Munkres algorithm is applied for coordination of UAVs in case of multiple targets. By carrying out the simulations, on three different maps over 50 iterations the results shows that MVO algorithm perform better in the most of the cases for finding optimized path cost with minimum average execution time when compared to meta-heuristics GSO and BBO.}
}
@article{LAI2021101283,
title = {Intelligent secure mobile edge computing for beyond 5G wireless networks},
journal = {Physical Communication},
volume = {45},
pages = {101283},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2021.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1874490721000203},
author = {Shiwei Lai and Rui Zhao and Shunpu Tang and Junjuan Xia and Fasheng Zhou and Liseng Fan},
keywords = {Deep reinforcement learning, Mobile edge computing, Physical layer security},
abstract = {Computational task offloading at the mobile edge servers is a promising strategy to reduce latency and energy consumption in 5G wireless networks. In this paper, we study the problem of offloading decision and system design in the intelligent secure mobile edge computing (MEC) system with a UAV eavesdropper, where the eavesdropper can overhear the secure computational task from the user to the computational access point (CAP). The proposed framework is aimed to ensure the physical-layer security and decrease the latency and energy consumption in communication and computation. We firstly formulate the optimization of the MEC networks as a multi-objective optimization problem and then use a linear combination of the latency and energy consumption to measure the system cost. We devise an adaptive offloading strategy by incorporating the wireless bandwidth allocation and the transmit power allocation among users through the deep reinforcement learning (DRL) algorithm. In particular, we propose a deep Q-network (DQN) based strategy to automatically solve the optimization problem by designing the system state, action, and the reward function at detail. At last, we demonstrate the usefulness of the considered intelligent offloading strategy for the design of the MEC networks by comparing with the other schemes. The proposed strategy can perform significant system cost saving of the considered MEC networks.}
}
@article{WU2022259,
title = {Downscaling land surface temperature: A framework based on geographically and temporally neural network weighted autoregressive model with spatio-temporal fused scaling factors},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {187},
pages = {259-272},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622000740},
author = {Jinhua Wu and Linyuan Xia and Ting {On Chan} and Joseph Awange and Bo Zhong},
keywords = {Land surface temperature (LST), Downscaling, Spatio-temporal fusion, Neural network, Non-stationarity, Spatial autocorrelation},
abstract = {Downscaling land surface temperatures (LST) from satellite imagery is essential for many fine-scale applications. However, the accuracy of the downscaling is often limited by different environmental and geographical conditions. In this work, a novel LST downscaling framework is proposed to improve the accuracy, especially for heterogeneous areas with varying land covers and complex terrains. The framework focuses on downscaling the MODIS LST from 1 km to 100 m, using the proposed geographically and temporally neural network weighted autoregression (GTNNWAR) model with spatio-temporal fused scaling factors derived from Landsat 8 imagery and digital surface models (DSM). To tackle the issues of the non-stationarity of the scaling factors in heterogenous areas, a region-adaptive parameterization approach is first applied. Then, the GTNNWAR invokes a two-stage deep neural network to estimate the regression coefficients, resulting in the adaption of varying weights for the scaling factors to raise the prediction performance. Moreover, the GTNNWAR is incorporated with a spatial autoregressive model which intakes the neighbor effects so that the overall accuracy can be further improved. Prior to the actual downscaling with the GTNNWAR, a filter-based fusion method is applied to ensure the spatio-temporal consistency of scaling factors is high enough for the neural networks to converge. The results suggest that the proposed framework exhibits high accuracy at the boundaries of different land covers and complex terrains. Compared with several other downscaling algorithms in three case study areas (Beijing and Zhangye in China, Netherlands–Germany in Europe), the proposed framework outperforms with a 28% improved R-squared (R2) and a root mean square error (RMSE) of 1.02 K. In addition, the downscaled LST has R2 over 0.63 for the UAV observations (Guangdong). It is concluded that the proposed framework has high reliability and robustness to provide LST datasets with high spatio-temporal resolutions in a wide range of land types.}
}
@article{DORNAIKA2016130,
title = {Building detection from orthophotos using a machine learning approach: An empirical study on image segmentation and descriptors},
journal = {Expert Systems with Applications},
volume = {58},
pages = {130-142},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416301154},
author = {Fadi Dornaika and Abdelmalik Moujahid and Youssef {El Merabet} and Yassine Ruichek},
keywords = {Automatic building detection and delineation, Orthophotos, Image segmentation, Image descriptors, Supervised learning, Classifier},
abstract = {Building detection from aerial images has many applications in fields like urban planning, real-estate management, and disaster relief. In the last two decades, a large variety of methods on automatic building detection have been proposed in the remote sensing literature. Many of these approaches make use of local features to classify each pixel or segment to an object label, therefore involving an extra step to fuse pixelwise decisions. This paper presents a generic framework that exploits recent advances in image segmentation and region descriptors extraction for the automatic and accurate detection of buildings on aerial orthophotos. The proposed solution is supervised in the sense that appearances of buildings are learnt from examples. For the first time in the context of building detection, we use the matrix covariance descriptor, which proves to be very informative and compact. Moreover, we introduce a principled evaluation that allows selecting the best pair segmentation algorithm-region descriptor for the task of building detection. Finally, we provide a performance evaluation at pixel level using different classifiers. This evaluation is conducted over 200 buildings using different segmentation algorithms and descriptors. The performance analysis quantifies the quality of both the image segmentation and the descriptor used. The proposed approach presents several advantages in terms of scalability, suitability and simplicity with respect to the existing methods. Furthermore, the proposed scheme (detection chain and evaluation) can be deployed for detecting multiple object categories that are present in images and can be used by intelligent systems requiring scene perception and parsing such as intelligent unmanned aerial vehicle navigation and automatic 3D city modeling.}
}
@article{ZHANG2022470,
title = {LISU: Low-light indoor scene understanding with joint learning of reflectance restoration},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {183},
pages = {470-481},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621003087},
author = {Ning Zhang and Francesco Nex and Norman Kerle and George Vosselman},
keywords = {Semantic segmentation, Deep learning, Intrinsic image decomposition, Low-light},
abstract = {Semantic segmentation using convolutional neural networks (CNNs) achieves higher accuracy than traditional methods, but it fails to yield satisfactory results under illumination variants when the training set is limited. In this paper we present a new data set containing both real and rendered images and a novel cascade network to study semantic segmentation in low-light indoor environments. Specifically, the network decomposes a low-light image into illumination and reflectance components, and then a multi-tasking learning scheme is built. One branch learns to reduce noise and restore information on the reflectance (reflectance restoration branch). Another branch learns to segment the reflectance map (semantic segmentation branch). The CNN features from two tasks are concatenated together so as to improve the segmentation accuracy by embedding the illumination-invariant features. We compare our approach with other CNN-based segmentation frameworks, including the state-of-the-art DeepLab v3+, on the proposed real data set, and our approach achieves the highest mIoU (47.6%). The experimental results also show that the semantic information supports the restoration of a sharper reflectance map, thus further improving the segmentation. Besides, we pre-train a model with the proposed large-scale rendered images and then fine-tune it on the real images. The pre-training results in an improvement of mIoU by 7.2%. Our models and data set are publicly available for research. This research is part of the EU project INGENIOUS11https://ingenious-first-responders.eu/. Our data sets and models are available on our website22https://github.com/noahzn/LISU.}
}
@article{MADEC2019225,
title = {Ear density estimation from high resolution RGB imagery using deep learning technique},
journal = {Agricultural and Forest Meteorology},
volume = {264},
pages = {225-234},
year = {2019},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2018.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S016819231830337X},
author = {Simon Madec and Xiuliang Jin and Hao Lu and Benoit {De Solan} and Shouyang Liu and Florent Duyme and Emmanuelle Heritier and Frédéric Baret},
keywords = {Wheat ear density, Object detection, Object counting, Convolutional neural networks, Phenotyping, Broad-sense heritability},
abstract = {Wheat ear density estimation is an appealing trait for plant breeders. Current manual counting is tedious and inefficient. In this study we investigated the potential of convolutional neural networks (CNNs) to provide accurate ear density using nadir high spatial resolution RGB images. Two different approaches were investigated, either using the Faster-RCNN state-of-the-art object detector or with the TasselNet local count regression network. Both approaches performed very well (rRMSE≈6%) when applied over the same conditions as those prevailing for the calibration of the models. However, Faster-RCNN was more robust when applied to a dataset acquired at a later stage with ears and background showing a different aspect because of the higher maturity of the plants. Optimal spatial resolution for Faster-RCNN was around 0.3 mm allowing to acquire RGB images from a UAV platform for high-throughput phenotyping of large experiments. Comparison of the estimated ear density with in-situ manual counting shows reasonable agreement considering the relatively small sampling area used for both methods. Faster-RCNN and in-situ counting had high and similar heritability (H²≈85%), demonstrating that ear density derived from high resolution RGB imagery could replace the traditional counting method.}
}
@article{OU2021300,
title = {Autonomous quadrotor obstacle avoidance based on dueling double deep recurrent Q-learning with monocular vision},
journal = {Neurocomputing},
volume = {441},
pages = {300-310},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221002629},
author = {Jiajun Ou and Xiao Guo and Ming Zhu and Wenjie Lou},
keywords = {Unmanned aerial vehicle, Obstacle avoidance, Deep reinforcement learning, Depth estimation},
abstract = {This paper proposes a novel learning-based framework to realize quadrotor autonomous obstacle avoidance with monocular vision. The framework adopts a two-stage architecture, consisting of a sensing module and a decision module. The sensing module trained in an unsupervised manner can extract depth information from the on-board camera image. Moreover, the decision module uses dueling double deep recurrent Q-learning to eliminate the adverse effects of the on-board monocular camera’s limited observation capacity while choosing practical obstacle avoidance action. The framework has two advantages: (1) it enables the quadrotor to realize autonomous obstacle avoidance without any prior environment information or labeled datasets for training, and (2) its model can be easily updated while facing new application scenarios. The experiments in several different simulation scenes show that the trained framework outperforms a high passing rate in crowded environments and a good generalization ability for transformed scenarios.}
}
@article{HOSSEINALIZADEH20191,
title = {Gully headcut susceptibility modeling using functional trees, naïve Bayes tree, and random forest models},
journal = {Geoderma},
volume = {342},
pages = {1-11},
year = {2019},
issn = {0016-7061},
doi = {https://doi.org/10.1016/j.geoderma.2019.01.050},
url = {https://www.sciencedirect.com/science/article/pii/S0016706118319177},
author = {Mohsen Hosseinalizadeh and Narges Kariminejad and Wei Chen and Hamid Reza Pourghasemi and Mohammad Alinejad and Ali {Mohammadian Behbahani} and John P. Tiefenbacher},
keywords = {Loess sediments, Gully headcut, Spatial prediction, Machine learning, UAV},
abstract = {Gully headcuts are due to erosion generated by concentrated overland flow, non-uniform infiltration, the presence of impermeable sub-surface soil layers, and a hydraulic gradient. It has been seen in an extensive range of continuous and categorical conditioning factors in several countries. Researchers fail to determine the factor that is the most important cause of GHs occurrence. This study determines the morphological and physio-chemical soil features, the locations with significant GHs, and the soil and geomorphic parameters in the loess area of Golestan Province, Iran. The spatial distribution of GHs was determined by field surveys and analysis of 1 m-resolution aerial photography collected with an UAV. Physical and chemical features of soil samples were determined through lab analysis. The relationships between features and GHs were ranked by statistical significance using chi-square tests. Functional trees - (FT), naïve Bayes tree (NBTree), and random forest (RF) models were employed to generate GHs sensitivity maps. One-hundred and twenty-seven GHs were identified and randomly divided into a training dataset comprised of 70% (89) of the GHs, and a validation dataset encompassing 30% (38) of the GHs. Twenty-two GHs conditioning factors were input into the models. The results of the chi-square analysis confirmed land use (90.8%) had the greatest influence on GHs occurrence. The results of training indicated that the AUC value for the FT, NBTree, and RF models were 83.4%, 94.8%, and 96.5%, respectively. The confidence interval (CI) for these methods was 0.771–0.885, 0.905–0.975, and 0.926–0.987, respectively. The highest standard error (SE) was related to the FT model (0.0314), followed by NBTree (0.0151), and RF (0.0119). The validation results indicated an AUC of 0.888, 0.947, and 0.981 for these methods, respectively. Therefore, RF was found to be the most effective model for predicting and mapping GHs in the future.}
}
@article{KHAN2017126,
title = {Neuro-adaptive dynamic integral sliding mode control design with output differentiation observer for uncertain higher order MIMO nonlinear systems},
journal = {Neurocomputing},
volume = {226},
pages = {126-134},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216314382},
author = {Qudrat Khan and Rini Akmeliawati},
keywords = {Uncertain systems, Integral manifold, Neural networks, Equivalent output injection observer, Robotic manipulator},
abstract = {This paper proposes a practical design method for the robust control of a class of MIMO nonlinear plants operating under model uncertainties and matched disturbances where the only available information for feedback are the outputs of the plant. A neural networks based dynamic integral sliding mode control (NNDISMC) with output differentiator observer is developed for the considered class. This NNDISMC approach utilizes the robust output differentiation observer for the higher derivative estimation and neural networks to estimate the nonlinear functions which are assumed unknown. Having estimated the unknown derivatives and uncertain functions, an integral manifold based on the estimated states is designed and a control law is proposed which confirms the sliding mode enforcement across the designed integral manifold from the very start of the process. The overall robustness of the controller is guaranteed by using the neural networks, differentiator observer and dynamic integral control law in a closed loop. The closed loop stability analysis is presented in detail, and the asymptotic convergence of the system states to the equilibrium is confirmed. The proposed method is very practical and plays a very significant role in the robust control of electromechanical systems, such as robotic manipulators, unmanned air vehicles and underwater vehicles. The simulation results on a robotic manipulator are presented to demonstrate the effectiveness of the proposed method.}
}
@article{SUN2022106705,
title = {Wheat head counting in the wild by an augmented feature pyramid networks-based convolutional neural network},
journal = {Computers and Electronics in Agriculture},
volume = {193},
pages = {106705},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106705},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922000229},
author = {Jun Sun and Kaifeng Yang and Chen Chen and Jifeng Shen and Yu Yang and Xiaohong Wu and Tomas Norton},
keywords = {Wheat head counting, CNN, Multi-scale feature, Dense targets detection, Wheat yield estimation},
abstract = {Wheat head counting plays an important role in crop yield estimation, which also meets great challenges of high density, scale variation, and illumination difference. In this paper, an improved wheat head counting network (WHCnet) was proposed to count wheat heads from the top view in the wild, with a novel object counting by detection pipeline. Firstly, data enhancement such as contrast adjustment and Gaussian blur were conducted to improve the generalization ability. Secondly, the Augmented Feature Pyramid Networks (AugFPN) was used to pool the original information adaptively, which made full use of the underlying information and solved the problem of poor detection of small wheat heads. Finally, cascaded Intersection over Union (IoU) threshold was used to make the IoU of the training model as close as possible to the input target, which could effectively remove negative samples in the complex background, reduce the noise bounding box and improve the target positioning accuracy of occluded wheat heads. Experiments on the test set image showed that the proposed method had an average error rate of 3.7% and an AP of 95.17%, which was significantly better than other state-of-the-art methods. Besides, the average counting time for a single wheat field image (3968 × 2976) obtained by an unmanned aerial vehicle was 87 ms, and the counting accuracy was 95.8%, which effectively verified the generalization ability of the model. These results indicate that the proposed method can meet the requirements of wheat head counting in the field environment and provide reliable reference data for wheat yield estimation.}
}
@article{KARIMINEJAD2020104344,
title = {Optimizing collapsed pipes mapping: Effects of DEM spatial resolution},
journal = {CATENA},
volume = {187},
pages = {104344},
year = {2020},
issn = {0341-8162},
doi = {https://doi.org/10.1016/j.catena.2019.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0341816219304862},
author = {Narges Kariminejad and Mohsen Hosseinalizadeh and Hamid Reza Pourghasemi and Majid Ownegh and Mauro Rossi and John P. Tiefenbacher},
keywords = {Collapsed pipes, DEM resolution, Piping mapping, SVM, ME, BRT},
abstract = {Finding a digital elevation model (DEM) of suitable spatial resolution is vital to investigate piping erosion using aerial remote-sensing platforms like unmanned aerial vehicles (UAV). Previous studies have implied that the best spatial resolution is a DEM with the most detail. This study evaluates piping-affected areas with five DEMs (1, 5, 10, 20, and 30 m resolutions) with three trained machine-learning methods: support vector machine (SVM), maximum entropy (ME), and boosted regression tree (BRT). This method enables the identification of the specific impacts caused by changing pixel resolution to guide the selection of the most effective DEM. This study employs piping morphometry data to predict the locations of completely collapsed pipes. The performance of the methods for mapping of pipes was assessed against a piping inventory map. The results demonstrate that the finest resolution DEM is not always the most useful. Though 1 m-resolution DEMs show the most detail, the best performance was the 5 m-resolution DEM when tested for all three mapping models. The 5 m-resolution DEM-SVM combination was the best predictor of known piping sites (AUC = 81.0%). The 5-m DEM-ME was second most effective model (AUC = 75.8%). And 5-m DEM-BRT was third (AUC = 72.9%). Applying more DEM derivatives may increase confidence in the selection of the most appropriate resolution.}
}
@article{NAEEM2021374,
title = {Formal approach to thwart against drone discovery attacks: A taxonomy of novel 3D obfuscation mechanisms},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {374-386},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2030337X},
author = {Faizana Naeem and Mujahid Mohsin and Usman Rauf and Liaqat Ali Khan},
keywords = {Location privacy, 3D obfuscation, Aerial vehicles, UAVs, Obfuscation taxonomy, Obfuscation operators, Drone discovery attacks},
abstract = {The pervasive capabilities and myriad of mission performance abilities of Unmanned (Combat) Aerial Vehicles (UAVs/UCAVs) have exponentially grown their deployment possibilities in the recent past. Advancements in artificial intelligence, sensing technologies and autonomous guidance, navigation and control capabilities have further fueled wide-scale deployments of UAVs, both for military and commercial applications, ranging from autonomous air taxis and cargo deliveries to intelligence surveillance, reconnaissance, and combat missions. Most of these applications consume Global Navigation Satellite System (GNSS) based location information for their services, which is also shared in real-time with ground control stations and centralized service operators, often using insecure communication channels. This limitation has significantly raised the location privacy concerns of aerial vehicles, deployed to conduct user-centric, safety-critical and localization-sensitive operations. A compromise of location privacy of a UAV can pose serious threats, including stalking, theft or damage of UAV/payload or even use of GNSS-guided munitions. These emerging threats call for robust and trust-worthy solutions for preserving the location privacy of aerial vehicles. This paper proposes a novel obfuscation-based mechanism to safeguard location information against privacy attacks. Our proposed solution conceals actual information by transmitting modified location parameters, either after diluting their accuracy or by fabricating deceptive trajectories for a known eavesdropper. Based on these two broad categories, defined as Attenuation and Deception-based obfuscation techniques respectively, we also present a novel taxonomy of 3D obfuscation mechanisms, supported by formal descriptions of underlying operators. The operators can be used independently or in conjunction to satisfy diverse mission-specific obfuscation profiles. The proposed operators have been practically implemented and evaluated using a customizable obfuscator deployed over a Global Positioning System (GPS) guided UAV. The field experiments validate the efficacy, security and deployability of the proposed solution against location-privacy threats.}
}
@article{WINKLER2020250,
title = {High-quality meets low-cost: Approaches for hybrid-mobility sensor networks},
journal = {Materials Today: Proceedings},
volume = {32},
pages = {250-253},
year = {2020},
note = {DAS2019},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.05.799},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320344126},
author = {Nicolas P. Winkler and Patrick P. Neumann and Arto Säämänen and Erik Schaffernicht and Achim J. Lilienthal},
keywords = {Mobile robot olfaction, Air quality monitoring, Wireless sensor network, Gas distribution mapping, Occupational health},
abstract = {Air pollution within industrial scenarios is a major risk for workers, which is why detailed knowledge about the dispersion of dusts and gases is necessary. This paper introduces a system combining stationary low-cost and high-quality sensors, carried by ground robots and unmanned aerial vehicles. Based on these dense sampling capabilities, detailed distribution maps of dusts and gases will be created. This system enables various research opportunities, especially on the fields of distribution mapping and sensor planning. Standard approaches for distribution mapping can be enhanced with knowledge about the environment’s characteristics, while the effectiveness of new approaches, utilizing neural networks, can be further investigated. The influence of different sensor network setups on the predictive quality of distribution algorithms will be researched and metrics for the quantification of a sensor network’s quality will be investigated.}
}
@article{ALVAREZMONTOYA2020106526,
title = {In-flight and wireless damage detection in a UAV composite wing using fiber optic sensors and strain field pattern recognition},
journal = {Mechanical Systems and Signal Processing},
volume = {136},
pages = {106526},
year = {2020},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2019.106526},
url = {https://www.sciencedirect.com/science/article/pii/S0888327019307472},
author = {Joham Alvarez-Montoya and Alejandro Carvajal-Castrillón and Julián Sierra-Pérez},
keywords = {Structural health monitoring, Damage detection, Composite materials, Aerospace structures, Machine learning, Remote sensing},
abstract = {Aiming to provide more efficient, lightweight structures, composite materials are being extensively used in aerospace vehicles. As the failure mechanisms of these materials are complex, damage detection becomes challenging, requiring advanced techniques for assessing structural integrity and maintaining aircraft safety. In this context, Structural Health Monitoring (SHM) seeks for integrating sensors into the structures in a way that Nondestructive Testing (NDT) is implemented continuously. One promising approach is to use Fiber Optic Sensors (FOS) to acquire strain signals, taking advantages of their capabilities over conventional sensors. Despite several works have developed Health and Usage Monitoring Systems (HUMS) using FOS for performing in-flight SHM in aircraft structures, automatic damage detection using the acquired signals has not been achieved in a robust way against environmental and operational variability, in all flight stages or considering different types of damages. In this work, a HUMS was developed and implemented in an Unmanned Aerial Vehicle (UAV) based on 20 Fiber Bragg Gratings (FBGs) embedded into the composite front spar of the aircraft’s wing, a miniaturized data acquisition subsystem for gathering strain signals and a wireless transmission subsystem for remote sensing. The HUMS was tested in 16 flights, six of them were carried out with the pristine structure and the remaining after inducing different artificial damages. The in-flight data were used to validate a previously developed damage detection methodology based on strain field pattern recognition, or strain mapping, which utilizes machine learning algorithms, specifically a Self-Organizing Map (SOM)-based procedure for clustering operational conditions and Principal Component Analysis (PCA) in conjunction with damage indices for final classification. The performance of the damage detection demonstrated a highest accuracy of 0.981 and a highest F1 score of 0.978. As a main contribution, this work implements in-flight strain monitoring, remote sensing and automatic damage detection in an operating composite aircraft structure.}
}
@article{RISTIC20171183,
title = {Integration of modern remote sensing technologies for faster utility mapping and data extraction},
journal = {Construction and Building Materials},
volume = {154},
pages = {1183-1198},
year = {2017},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2017.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0950061817313648},
author = {Aleksandar Ristić and Željko Bugarinović and Milan Vrtunski and Miro Govedarica and Dušan Petrovački},
keywords = {District heating network, GPR, Aerial thermography, Neural networks, Edge detection, Automated data extraction},
abstract = {The aim of the research presented in this paper is to analyze the benefits of integrating a mobile system capable of very fast, reliable and relatively inexpensive detection, identification and status examination of district heating network. Thermal imaging using unmanned aerial vehicle is used for pipeline route detection, inspection of validity of cadastral data and for locating possible leakages. Ground Penetrating Radar – GPR technology is used for control sampling of radargrams on specific locations of routes in order to achieve following: identification of the geometric characteristics of district heating pipelines and structure, prevention and registration of damage, as well as automated data extraction. The main part of the paper is dedicated to the algorithm for automated data extraction, based on artificial neural networks and pattern recognition. Radargrams of district heating pipeline were used as input data for the extraction algorithm, while the results are geometric characteristics such as pipe depth, distance between pipes and diameter.}
}
@article{KIM2019102,
title = {Retrieving shallow stream bathymetry from UAV-assisted RGB imagery using a geospatial regression method},
journal = {Geomorphology},
volume = {341},
pages = {102-114},
year = {2019},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X19302314},
author = {Jun Song Kim and Donghae Baek and Il Won Seo and Jaehyun Shin},
keywords = {Shallow water bathymetry, RGB imagery, Spatial heterogeneity, Geographically weighted regression},
abstract = {Bathymetric mapping is a prerequisite procedure to conduct assessments of water quality, habitat and environmental flow for riverine ecosystems using hydraulic modelling. This study evaluates the capability of a geographically weighted regression (GWR) model, which can capture a spatially heterogeneous relationship between inputs and an output, to retrieve bathymetry of a shallow stream, of which water depth is less than about 1 m from simple RGB imagery. A field experiment was performed for measuring water depth and simultaneously for acquiring remotely-sensed data with RGB digital numbers (DN) using a digital camera mounted on an unmanned aerial vehicle (UAV). A 2D shallow water model, which was validated by comparison with the field-surveyed data, was used to simulate the water depth of unmeasured regions. Band ratios of ln(DNG/DNR) was selected as an optimal spectral input of bathymetric inversion models through the principal component analysis (PCA). Results showed that global inversion models based on multiple linear regression (MLR) and artificial neural network (ANN) resulted in large discrepancy between estimation and observation due to the spatially varying response of the PCA-selected band ratio to water depth over the experimental channel. In contrast, the GWR model successfully alleviated the biases of the conventional models as R2 increased to 0.85 from 0.60 by accurately modelling the effect of spatial heterogeneity, which arose from variable bottom types attributed to submerged vegetation, on the remote-sensing radiance-water depth relationship.}
}
@article{CAPRIGLIONE2021110237,
title = {Performance analysis of MEMS-based inertial measurement units in terrestrial vehicles},
journal = {Measurement},
volume = {186},
pages = {110237},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110237},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121011465},
author = {Domenico Capriglione and Marco Carratù and Marcantonio Catelani and Lorenzo Ciani and Gabriele Patrizi and Antonio Pietrosanto and Roberto Singuaroli and Paolo Sommella},
keywords = {Testing, Reliability, Accelerometer, Gyroscope, Metrological performance, Automotive},
abstract = {In the recent period, Inertial Measurement Units (IMUs) are widely employed in many applications, such as smartphones, robotics, Unmanned Aerial vehicles, automotive and self‑driving vehicles, artificial intelligence, and numerous others. However, the dynamical metrological performances and the reliability analysis when these microelectronic devices operate under real environmental conditions are not sufficiently covered by scientific literature. Starting from standard tests for automotive applications, to emulate the real operating conditions of IMUs, a new test plan based on sine sweep vibration profiles has also been developed, including different service conditions characterized by the presence of a sinusoidal component with the addition of a random vibration noise typical of automotive scenarios. In-depth analysis has been carried out in the time and frequency domains leading to the employment of suitable figures of merit, highlighting the effects of mechanical stress on the metrological performances of microelectromechanical sensors. The developed test plan could be used to investigate if sinusoidal vibrations at specific frequencies influence the correct operation of low-cost platforms in typical automotive applications. The experimental results have confirmed the suitability of the proposed figures of merit in analyzing the effects of vibrations typical of the automotive context on the IMUs operating. In particular, they have allowed investigating the axes cross-sensitivity of triaxial systems, spurious responses, and unexpected behaviors due to the devices' non-ideality.}
}
@article{DEMORAES2019392,
title = {Experimental analysis of heuristic solutions for the moving target traveling salesman problem applied to a moving targets monitoring system},
journal = {Expert Systems with Applications},
volume = {136},
pages = {392-409},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302532},
author = {Rodrigo S. {de Moraes} and Edison P. {de Freitas}},
keywords = {Ant Colony Optimization, Genetic Algorithms, Simulated Annealing, Artificial intelligence, Moving target traveling salesman problem, Moving targets},
abstract = {The Traveling Salesman Problem (TSP) is an important problem in computer science which consists in finding a path linking a set of cities so that each of then can be visited once, before the traveler comes back to the starting point. This is highly relevant because several real world problems can be mapped to it. A special case of TSP is the one in which the cities (the points to be visited) are not static as the cities, but mobile, changing their positions as the time passes. This variation is known as Moving Target TSP (MT-TSP). Emerging systems for crowd monitoring and control based on unmanned aerial vehicles (UAVs) can be mapped to this variation of the TSP problem, as a number of persons (targets) in the crowd can be assigned to be monitored by a given number of UAVs, which by their turn divide the targets among them. These target persons have to be visited from time to time, in a similar way to the cities in the traditional TSP. Aiming at finding a suitable solution for this type of crowd monitoring application, and considering the fact that exact solutions are too complex to perform in a reasonable time, this work explores and compares different heuristic methods for the intended solution. The performed experiments showed that the Genetic Algorithms present the best performance in finding acceptable solutions for the problem in restricted time and processing power situations, performing better compared to Ant Colony Optimization and Simulated Annealing Algorithms.}
}
@article{MYLONAS2022100028,
title = {Eden Library: A long-term database for storing agricultural multi-sensor datasets from UAV and proximal platforms},
journal = {Smart Agricultural Technology},
volume = {2},
pages = {100028},
year = {2022},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2021.100028},
url = {https://www.sciencedirect.com/science/article/pii/S2772375521000289},
author = {Nikos Mylonas and Ioannis Malounas and Sofia Mouseti and Eleanna Vali and Borja Espejo-Garcia and Spyros Fountas},
keywords = {Open-data, Dataset, Precision agriculture, Deep learning, Computer vision, Robotics, Artificial intelligence},
abstract = {In modern agriculture, visual recognition systems based on deep learning are arising to allow autonomous machines to execute field operations in crops. However, for obtaining high performances, these methods need high amounts of data, which are usually scarce in agriculture. The main reason is that building an agricultural dataset covering exhaustively a specific problem is challenging, as visual characteristics of the symptoms may change, and there is a high dependency on environmental factors, such as temperature, humidity and light conditions. Therefore, an efficient methodology is necessary to consistently cover the entire workflow for creating an agricultural dataset, from the image acquisition to its online publication. This paper presents the Eden Library, a platform for contributing to this existing gap in open access crop/plant databases covering proximal and aerial images. The complete workflow on the design and deployment of the platform is also explained and discussed. This workflow is relevant because the provided datasets are thought to be maintained and enriched along the time, and they do not just remain as a static research output covering only specific species, growth stages, and conditions. The image annotations of plants and symptoms are provided, saving users from manually annotating images. Currently, the Eden Library covers 15 different crops, 9 weeds and 30 disorders (pests, diseases and nutrient deficiencies). Eden Library aspires to close this gap by providing a large and diversified image collection of plants, organized in a consistent manner, in order to boost further vision-based and AI-enabled field applications.}
}
@article{ZEYLIGER2022151121,
title = {Field test of the surface soil moisture mapping using Sentinel-1 radar data},
journal = {Science of The Total Environment},
volume = {807},
pages = {151121},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.151121},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721061994},
author = {A.M. Zeyliger and K.V. Muzalevskiy and E.V. Zinchenko and O.S. Ermolaeva},
keywords = {Sentinel-1, UAV, Digital elevation model, Radar backscattering, Artificial neural network, Soil moisture},
abstract = {Soil surface moisture is one of the key parameters for describing the hydrological state and assessing the potential availability of water for irrigated plants. Because the radar backscattering coefficient is sensitive to soil moisture, the application of Sentinel-1 data may support soil surface moisture mapping at high spatial resolution by detecting spatial and temporal changes at the field scale for precision irrigation management. This mapping is required to control soil water erosion and preferential water flow to improve irrigation water efficiency and minimise negative impacts on surface and ground water bodies. Direct observations of soil surface moisture (5-cm thickness) were performed at an experimental plot in the study site of the All-Russian Scientific Research Institute of Irrigated Agriculture, near the village Vodnyy, Volgograd region. Soil surface moisture retrieval from Sentinel-1 was performed at the same location. A second set of soil surface moisture was calculated for the soil sampling sites using the permittivity model, based on the estimates of soil surface characteristics: a) reflectivity, obtained by the neural network method from Sentinel-1 observations; b) roughness, obtained from the geodata of the stereoscopic survey with unmanned aerial vehicle Phantom 4 Pro. The raster set of soil surface moisture geodata was obtained based on the reflectivity geodata raster set to solve the inverse problem using a permittivity model that considers the soil texture of the experimental plot. The determination coefficient (0.948) and standard deviation (2.04%) were obtained by comparing both sets of soil moisture point geodata taken from the same soil sampling sites. The values confirmed a satisfactory linear correlation between the directly measured and indirectly modelled sets. A comparison of the two sets of geodata indicated a satisfactory reproduction of the first set by the second set. As a result, the developed method can be considered as the scientific and methodological basis of the new technology of soil surface moisture monitoring by radar, which is one of the basic characteristics used in precision irrigation management.}
}
@article{LI20221,
title = {Efficient depthwise separable convolution accelerator for classification and UAV object detection},
journal = {Neurocomputing},
volume = {490},
pages = {1-16},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.02.071},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222002387},
author = {Guoqing Li and Jingwei Zhang and Meng Zhang and Ruixia Wu and Xinye Cao and Wenzhao Liu},
keywords = {Depthwise separable convolutions, Convolutional neural networks, Hardware accelerator, FPGA, Object detection},
abstract = {Depthwise separable convolutions (DSC) have been widely deployed in lightweight convolutional neural networks due to high efficiency. But the acceleration performance of the Graphics Processing Unit for DSC was not as well as in theory. In this paper, some approaches were proposed for accelerating DSC based on Field-Programmable Gate Array (FPGA). For the preceding layers, S2C (spatial to channel) was proposed to accelerate computing and improve the utilization rate of computational resources and bandwidth. An efficient SharePE was proposed to accelerate the DSC, which can improve the efficiency of the computing resource. The regulable parallelism approach was proposed to compute efficiently the different pointwise convolutional layers. P2D&D2P approach is proposed to reduce the external memory access. For the entire accelerating system, the pre-load workflow was proposed to reduce the waiting time of the accelerator between two images. We demonstrated our approaches on the SkyNet using the Ultra96V2 development board. Results indicated that our proposed accelerator obtained 80.030 frames per second and 0.072 Joule per image for UAV object detection, which achieved the state-of-the-art results for SkyNet. Besides, the MobileNetV2 model was implemented on a larger XC7Z100 FPGA, and the results showed our accelerator classified each picture from ImageNet in 2.69 ms. Code is available at https://github.com/AILearnerLi/DAC-SDC-2020-SEUer.}
}
@article{WHITELEY2021106189,
title = {Rapid characterisation of landslide heterogeneity using unsupervised classification of electrical resistivity and seismic refraction surveys},
journal = {Engineering Geology},
volume = {290},
pages = {106189},
year = {2021},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2021.106189},
url = {https://www.sciencedirect.com/science/article/pii/S0013795221002003},
author = {J.S. Whiteley and A. Watlet and S. Uhlemann and P. Wilkinson and J.P. Boyd and C. Jordan and J.M. Kendall and J.E. Chambers},
keywords = {Landslide, Geophysics, Resistivity, Seismic, Machine learning, UAV},
abstract = {The characterisation of the subsurface of a landslide is a critical step in developing ground models that inform planned mitigation measures, remediation works or future early-warning of instability. When a landslide failure may be imminent, the time pressures on producing such models may be great. Geoelectrical and seismic geophysical surveys are able to rapidly acquire volumetric data across large areas of the subsurface at the slope-scale. However, analysis of the individual model derived from each survey is typically undertaken in isolation, and a robust, accurate interpretation is highly dependent on the experience and skills of the operator. We demonstrate a machine learning process for constructing a rapid reconnaissance ground model, by integrating several sources of geophysical data in to a single ground model in a rapid and objective manner. Firstly, we use topographic data acquired by a UAV survey to co-locate three geophysical surveys of the Hollin Hill Landslide Observatory in the UK. The data are inverted using a joint 2D mesh, resulting in a set of co-located models of resistivity, P-wave velocity and S-wave velocity. Secondly, we analyse the relationships and trends present between the variables for each point in the mesh (resistivity, P-wave velocity, S-wave velocity, depth) to identify correlations. Thirdly, we use a Gaussian Mixture Model (GMM), a form of unsupervised machine learning, to classify the geophysical data into cluster groups with similar ranges and trends in measurements. The resulting model created from probabilistically assigning each subsurface point to a cluster group characterises the heterogeneity of landslide materials based on their geophysical properties, identifying the major subsurface discontinuities at the site. Finally, we compare the results of the cluster groups to intrusive borehole data, which show good agreement with the spatial variations in lithology. We demonstrate the applicability of integrated geophysical surveys coupled with simple unsupervised machine learning for producing rapid reconnaissance ground models in time-critical situations with minimal prior knowledge about the subsurface.}
}
@article{THIELE2021104252,
title = {Multi-scale, multi-sensor data integration for automated 3-D geological mapping},
journal = {Ore Geology Reviews},
volume = {136},
pages = {104252},
year = {2021},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2021.104252},
url = {https://www.sciencedirect.com/science/article/pii/S016913682100278X},
author = {Samuel T. Thiele and Sandra Lorenz and Moritz Kirsch and I. {Cecilia Contreras Acosta} and Laura Tusa and Erik Herrmann and Robert Möckel and Richard Gloaguen},
keywords = {Hypercloud, Hyperspectral, Digital outcrop geology, Machine learning, Corta Atalaya, },
abstract = {Enhanced digital outcrop models attributed with hyperspectral reflectance data, or hyperclouds, provide a flexible, three-dimensional medium for data-driven mapping of geological exposures, mine faces or cliffs. This approach facilitates the collection of spatially contiguous information on exposed mineralogy, and so helps to quantify mineralising processes, interpret 1-D drillhole data, and optimise mineral extraction. In this contribution we present an open-source python workflow, hylite, for creating hyperclouds by seamlessly fusing geometric information with data from a variety of hyperspectral imaging sensors and applying necessary atmospheric and illumination corrections. These rich datasets can be analysed using a variety of techniques, including minimum wavelength mapping and spectral indices, to accurately map geological objects from a distance. Reference spectra from spectral libraries, ground or laboratory measurements can also be included to derive supervised classifications using machine learning techniques. We demonstrate the potential of the hypercloud approach by integrating hyperspectral data from laboratory, tripod and unmanned aerial vehicle acquisitions to automatically map relevant lithologies and alterations associated with volcanic hosted massive sulphide (VHMS) mineralisation in the Corta Atalaya open-pit, Spain. These analyses allow quantitative and objective mineral mapping at the outcrop and open-pit scale, facilitating quantitative research and smart-mining approaches. Our results highlight the seamless sensor integration made possible with hylite and the power of data-driven mapping methods applied to hyperclouds. Significantly, we also show that random forests (RF) trained only on laboratory data from labelled hand-samples can be used to map appropriately corrected outcrop scale data.}
}
@article{WU20164083,
title = {Error compensation based on BP neural network for airborne laser ranging},
journal = {Optik},
volume = {127},
number = {8},
pages = {4083-4088},
year = {2016},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2016.01.066},
url = {https://www.sciencedirect.com/science/article/pii/S0030402616001157},
author = {Bing Wu and Shaojun Han and Jin Xiao and Xiaoguang Hu and Jianxin Fan},
keywords = {Error compensation, Quad-rotor, BP neural network, Laser ranging},
abstract = {This paper proposes a general realization method of laser ranging for micro UAVs, which is based on secondary development for the laser ranging module of FLUKE 411D. And the airborne dynamic laser ranging experiments are finished with the micro UAV DJI Phantom1. Through data processing with the least-square-based method and BP neural network-based method, the measurement error with BP neural network-based method is obtained for 4.35%, which is significantly less than that 6.69% with least-square-based method. It means that the measurement accuracy is improved significantly with BP neural network-based method. Therefore, the error compensation method based on BP neural network for airborne laser ranging is verified effectively in the paper.}
}
@article{LI2016126,
title = {Adaptive RBFNNs/integral sliding mode control for a quadrotor aircraft},
journal = {Neurocomputing},
volume = {216},
pages = {126-134},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216307780},
author = {Shushuai Li and Yaonan Wang and Jianhao Tan and Yan Zheng},
keywords = {Quadrotor aircrafts, Adaptive RBFNNs control, Double-loop integral sliding mode control, Hierarchical control},
abstract = {This paper presents a novel hierarchical control strategy based on adaptive radical basis function neural networks (RBFNNs) and double-loop integral sliding mode control (IntSMC) for the position and attitude tracing of quadrotor unmanned aerial vehicles (UAVs) subjected to sustained disturbances and parameter uncertainties. The dynamical motion equations are obtained by the Lagrange–Euler formalism. The proposed controller combines the advantage of the IntSMC with the approximation ability of arbitrary functions ensured by RBFNNs to generate a control law to guarantee the faster convergence of the state variables to their desired values in short time and compensation for the disturbances and uncertainties. Capabilities of online adaptive estimating of the unknown uncertainties and null tracking error are proved by using the Lyapunov stability theory. Simulation results, also compared with traditional PD/IntSMC algorithms and with the backstepping/nonlinear H∞ controller, verify the effectiveness and robustness of the proposed control laws.}
}
@article{MITTAL2022117106,
title = {Dilated Convolution based RCNN using Feature Fusion for Low-Altitude Aerial Objects},
journal = {Expert Systems with Applications},
pages = {117106},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117106},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422005103},
author = {Payal Mittal and Akashdeep Sharma and Raman Singh and Vishal Dhull},
keywords = {Region proposal, Feature fusion, Dilated convolutions, Receptive field, Object detection},
abstract = {The low-altitude aerial objects are hard to detect by existing deep learning-based object detectors because of the scale variance, small size, and occlusion-related problems. Deep learning-based detectors do not consider contextual information about the scale information of small-sized objects in low-altitude aerial images. This paper proposes a new system using the concept of receptive fields and fusion of feature maps to improve the efficiency of deep object detectors in low-altitude aerial images. A Dilated ResNet Module (DRM) is proposed, motivated from the trident networks, which works on dilated convolutions to study the contextual data for specifically small-sized objects. Applicability of this component builds the model strong towards scale variations in low-altitude aerial objects. Then, Feature Fusion Module (FFM) is created to offer semantic intelligence for better detection of low-altitude aerial objects. We have chosen vastly deployed faster RCNN as the base detector for the proposal of our technique. The dilated convolution-based RCNN using feature fusion (DCRFF) system is implemented on a benchmark low-altitude UAV based-object detection dataset, VisDrone, which contains multiple object categories of pedestrians, vehicles in crowded scenes. The experiments exhibit the enactment of the given detector on chosen low-altitude aerial object dataset. The proposed system of DCRFF achieves 35.04% mAP on the challenging VisDrone dataset, indicating an average improvement of 2% when compared.}
}
@article{SAMY2011658,
title = {Survey and application of sensor fault detection and isolation schemes},
journal = {Control Engineering Practice},
volume = {19},
number = {7},
pages = {658-674},
year = {2011},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2011.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0967066111000414},
author = {Ihab Samy and Ian Postlethwaite and Da-Wei Gu},
keywords = {Multiple sensor fault detection, Fault isolation, Fault accommodation, Neural networks, UAV},
abstract = {Model-based sensor fault detection, isolation and accommodation (SFDIA) is a direction of development in particular with UAVs where sensor redundancy may not be an option due to weight, cost and space implications. SFDIA via neural networks (NNs) have been proposed over the years due to their nonlinear structures and online learning capabilities. The majority of papers tend to consider single sensor faults. While useful, this assumption can limit application to real systems where sensor faults can occur simultaneously or consecutively. In this paper we consider the latter scenario, where it is assumed that a 1s time gap is present between consecutive faults. Furthermore few applications have considered fixed-wing UAVs where full autonomy is most needed. In this paper an EMRAN RBF NN is chosen for modelling purposes due to its ability to adapt well to nonlinear environments while maintaining high computational speeds. A nonlinear UAV model is used for demonstration, where decoupled longitudinal motion is considered. System and measurement noise is also included in the UAV model as wind gust disturbances on the angle of attack and sensor noise, respectively. The UAV is assumed to operate at an initial trimmed condition of speed, 32m/s and altitude, 1000m. After 30 separate SFDIA tests implemented on a 1.6GHz Pentium processor, the NN-SFDIA scheme detected all but 2 faults and the NN processing time was 97% lower than the flight data sampling time.}
}
@article{HABIBNIA2019105495,
title = {ANN assisted flow modeling and analysis for a cyclorotor in ground effect},
journal = {Aerospace Science and Technology},
volume = {95},
pages = {105495},
year = {2019},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.105495},
url = {https://www.sciencedirect.com/science/article/pii/S1270963819305978},
author = {M. Habibnia and J. Pascoa},
keywords = {Cycloidal rotors, VTOL aircrafts, UAV, CFD simulations, OpenFOAM, Neural network analysis},
abstract = {This work presents a study on the operational characteristics of a cyclorotor-thruster able to be applied in an aircraft, particularly in take-off and landing phases. Firstly, the behavior of cyclorotor principles in ascending and descending phases while in close distance to the ground was numerically studied. In addition, a second study phase was also conducted by Artificial Neural Network (ANN) to train the obtained database of results obtained numerically to further interpret the operating conditions in ground effect. CFD results predict that the optimum operational status of cyclorotor in close-ground altitudes is 30° pitching oscillation and 200 rpm for rotating speed. The results are indicating that the vertical distance of the aircraft highly influences their efficiency and functionality in producing thrust. A wide range of parameters is assessed using ANN analysis in several flying altitudes from the ground take-off level up to a reasonable height in which still the ground effects are noticeable. These results strongly indicate that an active control of both pitching oscillation and rotation speed is essential in operating at the optimum desired state.}
}
@article{XU2020105762,
title = {Establishing a model to predict the single boll weight of cotton in northern Xinjiang by using high resolution UAV remote sensing data},
journal = {Computers and Electronics in Agriculture},
volume = {179},
pages = {105762},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105762},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920304671},
author = {Weicheng Xu and Weiguang Yang and Shengde Chen and Changsheng Wu and Pengchao Chen and Yubin Lan},
keywords = {UAV remote sensing, Neural networks, Multiple linear regression, Single boll weight prediction of cotton, Cotton boll recognition},
abstract = {Single boll weight is the main factor of cotton yield and a key index used to evaluate the quality of cotton. Predicting the single boll weight in a large area is important for variety selection and yield improvement. A model was established to predict the single boll weight by using the multitemporal high-resolution visible light remote sensing data obtained from UAV. Specifically, remote sensing data were collected for 29 fields in the Changji, Shihezi and Shawan areas in northern Xinjiang during the blooming period and boll opening stage. Five circular areas with a radius of 1 m were selected from each field as the ground investigation area for collection of the cotton boll samples. Fully convolutional networks (FCN) was used to recognize and extract bolls at the boll opening stage in the remote sensing images as a dependent variable of the model. Correlation analysis was carried out by combining VDVI (Visible-band difference vegetation index) at the flowering and boll setting stages, VDVI at boll opening stages, VDVI at boll opening areas (Extracted by FCN) and RGB mean values, then use the least squares linear regression and BP neural networks to model the upper, middle, lower cotton layers and average single boll weight in investigation area. Subsequently, K-fold cross-validation was performed to evaluate the results. The results showed that the results of the least squares linear regression (R2 = 0.8162) and BP neural networks (R2 = 0.8170) were nearly equivalent. The percentage of boll opening in the area and VDVI at the flowering and boll setting stages were highly correlated with upper single boll weight. This study proposes a method to realize the large scale prediction of single boll weight, which provided a new idea for cotton yield prediction and breeding screening.}
}
@article{JEONG2021107060,
title = {Hazardous flight region prediction for a small UAV operated in an urban area using a deep neural network},
journal = {Aerospace Science and Technology},
volume = {118},
pages = {107060},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.107060},
url = {https://www.sciencedirect.com/science/article/pii/S1270963821005708},
author = {Shinkyu Jeong and Kangkuk You and Donghoon Seok},
keywords = {Hazardous flight region prediction system, WRF+LES, Deep neural network (DNN)},
abstract = {With an increase of UAVs in logistics and transportation, the safety of UAVs operated in the urban wind environment becomes an important issue. Small UAVs are more sensitive to the wind environment because of their small size, slow cruising speed, and limited endurance. In the unmanned aircraft system traffic management (UTM), a safety risk assessment under bad weather conditions is an important component. In this study, a hazardous flight region prediction system for small UAVs operated in urban areas is developed using a deep neural network (DNN) to support a risk assessment and safe trajectory planning. A large eddy simulation (LES) is applied to reflect the terrain-driven wind environment in the urban area. The result of a weather research and forecasting (WRF) model is used as an initial and boundary condition of the LES to generate a realistic complicated wind environment in an urban area. Furthermore, an iterative nesting algorithm is applied to the LES to obtain a sufficient resolution of the wind environment, which is suitable for the small UAV scale. The deviation distance from the original flight path due to the wind environment is considered as a flight hazard criterion in this study. The proposed system is able to predict deviation distance due to the wind environment over the entire flight space over time by using the DNN model. The training data for the DNN is obtained using the multicopter flight dynamics simulator, which can take into account the influence of a specific wind environment. With the indexes considering this deviation distance and the local topography (distribution of buildings) in the urban area, the hazardous flight region is predicted. The information supplied by the proposed hazardous flight region prediction model can be used for the flight risk assessment and safe flight trajectory planning to increase the flight safety of small UAVs.}
}
@article{TEIZER2015225,
title = {Status quo and open challenges in vision-based sensing and tracking of temporary resources on infrastructure construction sites},
journal = {Advanced Engineering Informatics},
volume = {29},
number = {2},
pages = {225-238},
year = {2015},
note = {Infrastructure Computer Vision},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2015.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034615000336},
author = {Jochen Teizer},
keywords = {Building information modeling, Computer vision and machine learning, Resource location tracking and progress monitoring, Safety and health, Sensors: photo and video cameras, unmanned aerial vehicles, Surveying: laser scanning, photo- and videogrammetry},
abstract = {Modern construction projects require sufficient planning and management of resources to become successful. Core issues are tasks that deal with maintaining the schedule, such as procuring materials, guaranteeing the supply chain, controlling the work status, and monitoring safety and quality. Timely feedback of project status aids project management by providing accurate percentages of task completions and appropriately allocating resources (workforce, equipment, material) to coordinate the next work packages. However, current methods for measuring project status or progress, especially on large infrastructure projects, are mostly based on manual assessments. Recent academic research and commercial development has focused on semi- or fully-automated approaches to collect and process images of evolving worksites. Preliminary results are promising and show capturing, analyzing, and documenting construction progress and linking to information models is possible. This article presents first an overview to vision-based sensing technology available for temporary resource tracking at infrastructure construction sites. Second, it provides the status quo of research applications by highlighting exemplary case. Third, a discussion follows on existing advantages and current limitations of vision based sensing and tracking. Open challenges that need to be addressed in future research efforts conclude this paper.}
}
@article{CHOI2021113895,
title = {Weakly supervised power line detection algorithm using a recursive noisy label update with refined broken line segments},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113895},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113895},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306953},
author = {Hyeyeon Choi and Gyogwon Koo and Bum Jun Kim and Sang Woo Kim},
keywords = {Weakly supervised learning, Power lines, Semantic segmentation, Line segments, Industrial application},
abstract = {Detection of power lines in aerial images is an important problem to prevent accidents of unmanned aerial vehicles operating at low altitudes in the electrical industry. Recently, pixel-level power line detection using deep learning has been studied but production of the pixel-level annotations for massive dataset is difficult. In this study, we propose a power line detection algorithm using weakly supervised learning method to reduce the labeling cost for dataset generation. The algorithm is divided into two stages. First, an approximately localized mask was generated based on a convolutional neural network which was trained with only patch-level labels. Second, recursive training of segmentation network with refined broken line segments was executed. A refinement algorithm, line segment connecting (LSC) is a power-line-specialized refinement module that connects broken lines by approximating the segments as partially straight. In proposed algorithm, predicted image at each recursive step was updated as a label of the next training and the label was developed by itself with LSC. The comprehensive experimental results of our algorithm showed state-of-art F1-score of 94.3% in weakly supervised learning approaches on public dataset. This result suggests that the proposed algorithm is useful for low labeling cost with high performance in line detection application.}
}
@article{LANG2019330,
title = {Detection of Chlorophyll Content in Maize Canopy from UAV Imagery},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {30},
pages = {330-335},
year = {2019},
note = {6th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.561},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319324802},
author = {Qiao Lang and Zhang Zhiyong and Chen Longsheng and Sun Hong and Li Minzan and Li Li and Ma Junyong},
keywords = {chlorophyll content, UAV sensing technology, image processing, BP neural network, visual distribution},
abstract = {Chlorophyll is an important indicator for the evaluation of plant photosynthesis ability and growth status. In order to obtain the spatial distribution of chlorophyll content in field crops quickly and non-destructively, the chlorophyll content detection of maize canopy was carried out based on UAV image processing. In this paper, the RGB (red, green, blue) images of the maize canopy were measured in the Hengshui, Hebei province. The processing method was proposed to estimate the chlorophyll content in the field. Firstly, the image was segmented based on the HSV (hue, saturation, value) color model to remove soil background. The parameters were extracted related to the color feature and the texture feature in the image. On the one hand, there were 10 color parameters were involved including the red, green, blue, green and red differences, normalized red and green differences, and so on. On the other hand, the texture parameters were calculated with mean, standard deviation, smoothness, third moment, etc. The detection model of maize chlorophyll content was established and discussed based on BP neural network. The experiment results showed that: (1) The detecting accuracy of chlorophyll content was increased by the image parameter combination of color and texture features. Compared with the color feature, the determination coefficient of the model was increased from 0.6987 to 0.7246 by involving the texture feature. (2) The segmentation of canopy could help to improve the estimation accuracy due to the influence elimination of soil background, and the determination coefficient of model increased from 0.7246 to 0.7564, meanwhile, the root mean square error (RMSE) decreased from 4.4659 mg·L-1 to 4.4425 mg·L-1. The chlorophyll content of maize canopy was calculated at pixel level to indicate the field statues. The distribution map of chlorophyll content in field maize canopy was drawn based on pseudo-color technique. It provided a tool to visually distinguish the field road and canopy area, showing the difference in chlorophyll distribution of the plot. The UAV imagery could help to measure the content and distribution of maize chlorophyll non-destructively, and provide a support for crop evaluation and precision management in the field.}
}
@article{INNOCENTE201980,
title = {Self-organising swarms of firefighting drones: Harnessing the power of collective intelligence in decentralised multi-robot systems},
journal = {Journal of Computational Science},
volume = {34},
pages = {80-101},
year = {2019},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877750318310238},
author = {Mauro S. Innocente and Paolo Grasso},
keywords = {Self-organisation, Particle swarm, Fire spread modelling, Swarm robotics, Autonomous unmanned aerial vehicles},
abstract = {Swarm intelligence (SI) is concerned with the collective behaviour that emerges from decentralised self-organising systems, whilst swarm robotics (SR) is an approach to the self-coordination of large numbers of simple robots which emerged as the application of SI to multi-robot systems. Given the increasing severity and frequency of occurrence of wildfires and the hazardous nature of fighting their propagation, the use of disposable inexpensive robots in place of humans is of special interest. This paper demonstrates the feasibility and potential of employing SR to fight fires autonomously, with a focus on the self-coordination mechanisms for the desired firefighting behaviour to emerge. Thus, an efficient physics-based model of fire propagation and a self-organisation algorithm for swarms of firefighting drones are developed and coupled, with the collaborative behaviour based on a particle swarm algorithm adapted to individuals operating within physical dynamic environments of high severity and frequency of change. Numerical experiments demonstrate that the proposed self-organising system is effective, scalable and fault-tolerant, comprising a promising approach to dealing with the suppression of wildfires – one of the world's most pressing challenges of our time.}
}
@article{CHENG2020102341,
title = {Automatic delamination segmentation for bridge deck based on encoder-decoder deep learning through UAV-based thermography},
journal = {NDT & E International},
volume = {116},
pages = {102341},
year = {2020},
issn = {0963-8695},
doi = {https://doi.org/10.1016/j.ndteint.2020.102341},
url = {https://www.sciencedirect.com/science/article/pii/S0963869520303224},
author = {Chongsheng Cheng and Zhexiong Shang and Zhigang Shen},
keywords = {Concrete delamination, Thermography, Nondestructive evaluation, Deep learning, Encoder-decoder architecture, Semantic segmentation, UAV},
abstract = {Concrete deck delamination often demonstrates strong variations in size, shape, and temperature distribution under the influences of outdoor weather conditions. The strong variations create challenges for pure analytical solutions in infrared image segmentation of delaminated areas. The recently developed supervised deep learning approach demonstrated the potentials in achieving automatic segmentation of RGB images. However, its effectiveness in segmenting thermal images remains under-explored. The main challenge lies in the development of specific models and the generation of a large range of labeled infrared images for training. To address this challenge, a customized deep learning model based on encoder-decoder architecture is proposed to segment the delaminated areas in thermal images at the pixel level. Data augmentation strategies were implemented in creating the training data set to improve the performance of the proposed model. The deep learning generated model was deployed in a real-world project to further evaluate the model's applicability and robustness. The results of these experimental studies supported the effectiveness of the deep learning model in segmenting concrete delamination areas from infrared images. It also suggested that data augmentation is a helpful technique to address the small size issue of training samples. The field test with validation further demonstrated the generalizability of the proposed framework. Limitations of the proposed approach were also briefed at the end of the paper.}
}
@article{HABIBNIA2021107141,
title = {Active control assessments towards optimizing the performance of a cycloidal rotor at hover},
journal = {Aerospace Science and Technology},
volume = {119},
pages = {107141},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.107141},
url = {https://www.sciencedirect.com/science/article/pii/S1270963821006519},
author = {M. Habibnia and J. Pascoa},
keywords = {Cycloidal rotor, Hover state, CFD simulations, Neural network optimization, Active control, Performance analysis},
abstract = {The present study demonstrates an improved performance of cycloidal rotors by actively controlling the pitching oscillations and rotational speeds. The computational fluid dynamics (CFD) coupled with artificial neural network (ANN) were the methodologies used in the optimization analysis for the hover-state operation rather than the take-off mode under ground effects [1]. The former is carried out to obtain numerical predictions at various operating conditions for an UAV-scale cyclorotor. The oscillating-rotating blades and the corresponding flowfield is computed unsteadily along the complete circular trace for performance considerations. From CFD simulations, the optimum operational state is predicted for a 30∘ and 500 (rpm) pitch angle and rotating speed, respectively. On a second step, by training the ANN with the CFD database at various operating conditions and parameters, the ANN was then capable of analyzing the optimum states for operating at different conditions. The pitching oscillation schedule is then optimized for each rotational speed by using ANN and for each azimuthal location over the traversing trace. This will imply to perform on-board control in active mode for the blades, rather than assigning constant pitching oscillations for all operating states. This active control concept showed to be a potential approach to enhance the cyclorotor efficiency by 12 percent in average.}
}
@article{HAMLEDARI201778,
title = {Automated computer vision-based detection of components of under-construction indoor partitions},
journal = {Automation in Construction},
volume = {74},
pages = {78-94},
year = {2017},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516304046},
author = {Hesam Hamledari and Brenda McCabe and Shakiba Davari},
keywords = {Computer vision, Interior construction, Machine learning, Image processing, Digital images, Indoors},
abstract = {This paper presents a computer vision-based algorithm that automatically detects the components of an interior partition and infers its current state using 2D digital images. The algorithm relies on four integrated shape and color-based modules, which detect studs, insulation, electrical outlets, and three states for drywall sheets (installed, plastered, and painted). Based on the results of the four modules, images are classified into five states. The proposed method was validated using three image databases of indoor construction sites captured by a quadcopter (a type of unmanned aerial vehicle), a smartphone, and collected from publically available sources on the internet. The method's high accuracy rates, its fast performance, and applicability to different contexts such as automated robotic inspection are indicative of its promising performance. The visual detection results can potentially provide situational awareness for construction trades, provide future progress tracking systems with information on actual state, and help leverage the use of image processing at indoor sites.}
}
@article{MABBAS2019243,
title = {Autonomous Canal Following by a Micro-Aerial Vehicle Using Deep CNN⁎⁎We acknowledgement the finanical support of National Center for Robotics and Automation (NCRA), Pakistan; the German Academic Exchange Service (DAAD); Center for Water Informatics & Technology at LUMS, in carrying out this research.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {30},
pages = {243-250},
year = {2019},
note = {6th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.529},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319324486},
author = {Syed {M. Abbas} and Hashim Ali and Abubakr Muhammad},
keywords = {Agricultural robotics, water management, precision agriculture, trajectory tracking, CNN, autonomous navigation, Unmanned Aerial Vehicles},
abstract = {Globally, large-scale irrigation canal networks serve as the backbone of agriculture in many important river basins. However, these water channels are in a constant threat of erosion, silt accumulation and structural damages over time which significantly reduces the water carrying capacity. Therefore, periodic inspections of the canals are required for critical operations and maintenance tasks. Due to the vast lengths of the channels and time-critical operations, automation has become a necessity. In this paper, we have proposed an aerial autonomous canal traversal system using ResNet50 inspired deep convolutional neural network. Given the uniqueness of our problem, we have generated our dataset for supervised learning and validation and later evaluated the proposed approach on a real canal. We have implemented our approach on a COTS micro-aerial vehicle. We have designed our system in such a way that it takes 200ms from perception to action thereby making the system real-time. We compare the superior performance of our Res Net 50 inspired network with other state-of-the-art CNNs trained on canal datasets.}
}
@article{LEITE2022112764,
title = {Large scale multi-layer fuel load characterization in tropical savanna using GEDI spaceborne lidar data},
journal = {Remote Sensing of Environment},
volume = {268},
pages = {112764},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112764},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721004843},
author = {Rodrigo Vieira Leite and Carlos Alberto Silva and Eben North Broadbent and Cibele Hummel do Amaral and Veraldo Liesenberg and Danilo Roberti Alves de Almeida and Midhun Mohan and Sérgio Godinho and Adrian Cardil and Caio Hamamura and Bruno Lopes de Faria and Pedro H.S. Brancalion and André Hirsch and Gustavo Eduardo Marcatti and Ana Paula {Dalla Corte} and Angelica Maria Almeyda Zambrano and Máira Beatriz Teixeira da Costa and Eraldo Aparecido Trondoli Matricardi and Anne Laura da Silva and Lucas Ruggeri Ré Y. Goya and Ruben Valbuena and Bruno Araujo Furtado de Mendonça and Celso H.L. {Silva Junior} and Luiz E.O.C. Aragão and Mariano García and Jingjing Liang and Trina Merrick and Andrew T. Hudak and Jingfeng Xiao and Steven Hancock and Laura Duncason and Matheus Pinheiro Ferreira and Denis Valle and Sassan Saatchi and Carine Klauberg},
keywords = {Active remote sensing, Fire, Modeling, Machine learning, UAV-lidar, Cerrado, Vegetation structure},
abstract = {Quantifying fuel load over large areas is essential to support integrated fire management initiatives in fire-prone regions to preserve carbon stock, biodiversity and ecosystem functioning. It also allows a better understanding of global climate regulation as a potential carbon sink or source. Large area assessments usually require data from spaceborne remote sensors, but most of them cannot measure the vertical variability of vegetation structure, which is required for accurately measuring fuel loads and defining management interventions. The recently launched NASA's Global Ecosystem Dynamics Investigation (GEDI) full-waveform lidar sensor holds potential to meet this demand. However, its capability for estimating fuel load has yet not been evaluated. In this study, we developed a novel framework and tested machine learning models for predicting multi-layer fuel load in the Brazilian tropical savanna (i.e., Cerrado biome) using GEDI data. First, lidar data were collected using an unnamed aerial vehicle (UAV). The flights were conducted over selected sample plots in distinct Cerrado vegetation formations (i.e., grassland, savanna, forest) where field measurements were conducted to determine the load of surface, herbaceous, shrubs and small trees, woody fuels and the total fuel load. Subsequently, GEDI-like full-waveforms were simulated from the high-density UAV-lidar 3-D point clouds from which vegetation structure metrics were calculated and correlated to field-derived fuel load components using Random Forest models. From these models, we generate fuel load maps for the entire Cerrado using all on-orbit available GEDI data. Overall, the models had better performance for woody fuels and total fuel loads (R2 = 0.88 and 0.71, respectively). For components at the lower stratum, models had moderate to low performance (R2 between 0.15 and 0.46) but still showed reliable results. The presented framework can be extended to other fire-prone regions where accurate measurements of fuel components are needed. We hope this study will contribute to the expansion of spaceborne lidar applications for integrated fire management activities and supporting carbon monitoring initiatives in tropical savannas worldwide.}
}
@article{SHETH2020279,
title = {A taxonomy of AI techniques for 6G communication networks},
journal = {Computer Communications},
volume = {161},
pages = {279-303},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420318478},
author = {Karan Sheth and Keyur Patel and Het Shah and Sudeep Tanwar and Rajesh Gupta and Neeraj Kumar},
keywords = {Artificial Intelligence, 6G, Communication networks, Mobile edge computing, Intelligent transportation system},
abstract = {With 6G flagship program launched by the University of Oulu, Finland, for full future adaptation of 6G by 2030, many institutes worldwide have started to explore various issues and challenges in 6G communication networks. 6G offers ultra high-reliable and massive ultra-low latency while opening the doors for many applications currently not viable by today’s 4G and 5G communication standards. The current 5G technology has security and privacy issues which makes its usage in limited applications. In such an environment, we believe that AI can offer efficient solutions for the aforementioned issues having low communication overhead cost. Keeping focus on all these issues, in this paper, we presented a comprehensive survey on AI-enabled 6G communication technology, which can be used in wide range of future applications. In this article, we explore how AI can be integrated into different applications such as object localization, UAV communication, surveillance, security and privacy preservation etc. Finally, we discussed a use case that shows the adoption of AI techniques in intelligent transport system.}
}
@article{HENDRIA2021,
title = {Combining transformer and CNN for object detection in UAV imagery},
journal = {ICT Express},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405959521001715},
author = {Willy Fitra Hendria and Quang Thinh Phan and Fikriansyah Adzaka and Cheol Jeong},
keywords = {Convolutional neural network, Object detection, Transformer, UAV imagery},
abstract = {Combining multiple models is a well-known technique to improve predictive performance in challenging tasks such as object detection in UAV imagery. In this paper, we propose fusion of transformer-based and convolutional neural network-based (CNN) models with two approaches. First, we ensemble Swin Transformer and DetectoRS with ResNet backbone, and conduct performance comparison on four typical methods for combining predictions of multiple object detection models. Second, we design a hybrid architecture by combining Swin Transformer backbone with a neck of DetectoRS. We show that the fusion of the transformer and the CNN-based models performs better compared to the respective baseline model.}
}
@article{AMIRKHANI2016128,
title = {Visual-based quadrotor control by means of fuzzy cognitive maps},
journal = {ISA Transactions},
volume = {60},
pages = {128-142},
year = {2016},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2015.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019057815002839},
author = {Abdollah Amirkhani and Masoud Shirzadeh and Elpiniki I. Papageorgiou and Mohammad R. Mosavi},
keywords = {Image-based visual servoing, Moving target, Fuzzy cognitive map, Perspective image moments, Nonlinear Hebbian learning},
abstract = {By applying an image-based visual servoing (IBVS) method, the intelligent image-based controlling of a quadrotor type unmanned aerial vehicle (UAV) tracking a moving target is studied in this paper. A fuzzy cognitive map (FCM) is a soft computing method which is classified as a fuzzy neural system and exploits the main aspects of fuzzy logic and neural network systems; so it seems to be a suitable choice for implementing a vision-based intelligent technique. An FCM has been employed in implementing an IBVS scheme on a quadrotor UAV, so that the UAV can track a moving target on the ground. For this purpose, by properly combining the perspective image moments, some features with the desired characteristics for controlling the translational and yaw motions of a UAV have been presented. In designing a vision-based control method for a UAV quadrotor, there are some challenges, including the target mobility and not knowing the height of UAV above the target. Also, no sensor has been installed on the moving object and the changes of its yaw angle are not available. Despite all the stated challenges, the proposed method, which uses an FCM in controlling the translational motion and the yaw rotation of a UAV, adequately enables the quadrotor to follow the moving target. The simulation results for different paths show the satisfactory performance of the designed controller.}
}
@article{DAS2021221,
title = {UAV-Thermal imaging and agglomerative hierarchical clustering techniques to evaluate and rank physiological performance of wheat genotypes on sodic soil},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {173},
pages = {221-237},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000149},
author = {Sumanta Das and Jack Christopher and Armando Apan and Malini {Roy Choudhury} and Scott Chapman and Neal W. Menzies and Yash P. Dang},
keywords = {Canopy temperature, Plant water stress, Vegetation indices, Agglomerative hierarchical clustering, Wheat genotypes, Sodic soil},
abstract = {Sodicity is a major soil constraint in many arid and semi-arid regions worldwide, including Australia, which adversely affects the ability of crops to take up water and nutrients from the soil, reducing yield. Reliable methods and tools are required for appropriate selection of traits, may provide a better understanding of crop responses to multiple stresses, especially in sodic soil. A novel strategy was developed using unmanned aerial vehicle (UAV)-thermal imaging and agglomerative hierarchical clustering-based techniques to evaluate and rank the physiological performance of 18 contrasting wheat genotypes grown on a moderately sodic and a highly sodic soil in north-eastern Australia. We obtained UAV-thermal imaging data at different times of the day (9:30, 12:00, and 15:00 hrs) close to flowering stage. Crop biophysical parameters (Leaf potassium concentration, normalized difference vegetation index, crop water uptake, stomatal conductance, plant moisture content, and aboveground biomass) were measured at close to flowering by destructive plant sampling and ground-based proximal sensing and yield was machine harvested at maturity. Canopy temperatures derived from thermal imagery between 28.9 and 35.4 °C were observed at the moderately sodic site, and between 36.2 and 41.0 °C at the highly sodic site from 9:30 to 15:00 hrs. Canopy temperature was consistently higher than corresponding ambient air temperatures indicating plant water stress at both sites. While the air temperature was not significantly different (p > 0.05) between the two sites, canopy temperature was significantly higher (p < 0.01) on highly sodic soil compared to moderately sodic soil, indicating greater water stress at the highly sodic site. This difference was most likely due to the adverse impacts of sodic soil constraints and not primarily due to environmental variations. Hence, our study revealed that sodic soil constraints can intensify plant water stress. Statistical analysis between canopy temperature (9:30, 12:00, and 15:00 hrs) and crop biophysical parameters showed close negative correlations at both moderately sodic (R2 = 0.54 to 0.83) and highly sodic (R2 = 0.30 to 0.89) sites. A closer correlation was observed at 15:00 hrs for both sites. Thus, high-resolution UAV-thermal imaging has potential to detect water-stressed plants on sodic soil. Agglomerative hierarchical clustering was used as an unsupervised machine learning tool for ranking of physiological performance of wheat genotypes. Results suggest that UAV-thermal imaging and AHC techniques can discriminate cultivars tolerant to sodicity. The study improves our understanding of crop physiological behaviour and can assist farmers in selection of water stress tolerant genotypes to sustain food security in sodic soil under water-limited environments.}
}
@article{FOROUTAN2017156,
title = {Semi-automatic mapping of linear-trending bedforms using ‘Self-Organizing Maps’ algorithm},
journal = {Geomorphology},
volume = {293},
pages = {156-166},
year = {2017},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2017.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X16306535},
author = {M. Foroutan and J.R. Zimbelman},
keywords = {Semi-automatic mapping, Linear-trending bedforms, Self-organizing maps, Image processing, Transverse Aeolian Ridges},
abstract = {Increased application of high resolution spatial data such as high resolution satellite or Unmanned Aerial Vehicle (UAV) images from Earth, as well as High Resolution Imaging Science Experiment (HiRISE) images from Mars, makes it necessary to increase automation techniques capable of extracting detailed geomorphologic elements from such large data sets. Model validation by repeated images in environmental management studies such as climate-related changes as well as increasing access to high-resolution satellite images underline the demand for detailed automatic image-processing techniques in remote sensing. This study presents a methodology based on an unsupervised Artificial Neural Network (ANN) algorithm, known as Self Organizing Maps (SOM), to achieve the semi-automatic extraction of linear features with small footprints on satellite images. SOM is based on competitive learning and is efficient for handling huge data sets. We applied the SOM algorithm to high resolution satellite images of Earth and Mars (Quickbird, Worldview and HiRISE) in order to facilitate and speed up image analysis along with the improvement of the accuracy of results. About 98% overall accuracy and 0.001 quantization error in the recognition of small linear-trending bedforms demonstrate a promising framework.}
}
@article{XI2021106506,
title = {Evaluation of dimensionality reduction methods for individual tree crown delineation using instance segmentation network and UAV multispectral imagery in urban forest},
journal = {Computers and Electronics in Agriculture},
volume = {191},
pages = {106506},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106506},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005238},
author = {Xiangshu Xi and Kai Xia and Yinhui Yang and Xiaochen Du and Hailin Feng},
keywords = {Dimensionality reduction, Individual tree crown delineation, Instance segmentation, Multispectral imagery},
abstract = {The diversity of features in urban forest poses challenges to the task of delineation individual tree crowns. Multispectral image helps to improve the accuracy of individual tree crown delineation. It is necessary to reduce the dimensionality of multispectral images, but which dimensionality reduction method is suitable for the individual tree crown task based on deep learning still needs further research. In this study, four dimensionality reduction methods (principal component analysis, independent component analysis, optimum index factor, standard false color composite) were used to reduce the dimensionality of multispectral images. The images after dimensionality reduction were made as dataset for network training. Two instance segmentation networks (BlendMask, Mask R-CNN) were used to delineate the ginkgo tree crowns of UAV multispectral images after dimensionality reduction in urban. The effect of dimensionality reduction methods on two networks was evaluated in detail. The result of experiments presented that the standard false color composite method obtained the best value with 60.0% in average precision, 95.3% in average precision (Intersection over Union = 0.5) and 70.8% in average recall. The feature extraction methods (principal component analysis, independent component analysis) showed a good performance in the simple plot, but failed in the dense plot. The band selection methods (optimum index factor, standard false color composite) were more stable than the feature extraction methods in both plots. This article provides an important reference for related researchers on the choice of dimensionality reduction methods.}
}
@article{KAPOUTSIS2021107029,
title = {Building synergetic consensus for dynamic gas-plume tracking applications using UAV platforms},
journal = {Computers & Electrical Engineering},
volume = {91},
pages = {107029},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107029},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621000525},
author = {Athanasios Ch. Kapoutsis and Iakovos T. Michailidis and Yiannis Boutalis and Elias B. Kosmatopoulos},
keywords = {Remote gas sensing, Swarm Intelligence, Multi-robot, Gas-plume tracking, Autonomous UAVs},
abstract = {This article investigates the problem of deploying a swarm of UAVs equipped with gas sensors for industrial remote gas-plume sensing. This setup’s objective is to continuously adjust the swarm formation to maximize the combined perception for the dynamically evolved plume’s cloud, focusing around areas with the highest concentration/intensity. Initially, such a setup is formulated into an optimization problem, the solution of which could be acquired by the maximization of an appropriately defined objective function. Due to the model-free approach, this objective function’s analytical form is not available, prohibiting standard gradient descent methodologies. To this end, a tracking algorithm is developed and studied, which operates in a distributed manner and enables the UAV swarm to build a common consensus dynamically, during the evolution of the leakage phenomenon. The overall performance is tested in a simulative yet realistic environment using ANSYS Fluent suite, considering a simultaneous gas-leak incident at two different points. Aside from the standalone evaluation study, the proposed gas-plume tracking scheme is able to outperform a state-of-the-art alternative algorithm, namely Efficient Global Optimization (EGO), in various simulation setups, deploying a different number of UAVs on the field.}
}
@article{REN2020107509,
title = {Design and construction of the knowledge base system for geological outfield cavities classifications: An example of the fracture-cavity reservoir outfield in Tarim basin, NW China},
journal = {Journal of Petroleum Science and Engineering},
volume = {194},
pages = {107509},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107509},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520305805},
author = {Qiqiang Ren and Qiang Jin and Jianwei Feng and He Du},
keywords = {UAV scanning, 3D digital model, Multiple linear regression, Discriminant classification, Fracture-cavity reservoir},
abstract = {Tahe oilfield, located in NW Tarim Basin, is one of the largest and most difficult fracture cavity reservoirs in the world. Different fracture cavities, different generated mechanisms, and different oil production capacities. In order to study the significant parameters that can characterize the categories of facture-cavity. This research adopted outfield manual measurement, 3D digital modeling technique to obtain characterization parameters. According to experienced geological survey, typical outcrops were selected, then scanned by UAV (Unmanned aerial vehicle). Consequently, 3D digital models, including real coordinates and parameter information, were established by Agisoft Photoscan. Through geological testing results, various combination characteristic patterns of relative categories were analyzed. By using digital measure tool, combined with manually measured data, the parameters were extracted from the 3D digital model (DM). Then an initial geological database was established. For furtherly analyzing the database, the mathematic statistics methods of multiple linear regression (MLR), neural network technique (NNT) and discriminative classification technique (DCT) were applied. Using software of SPSS statistics 17.0, more than 200 groups of geological data (various categories of fracture-cavity) were optimally processed. Consequently, the significant characteristic parameters were interpreted to determine diverse categories. The results showed that: (1) cavity width, height, fracture length and cavity aspect ratio were significant parameters to classify runoff cavity categories. (2) Fault-controlled cavities could be accurately classified by fracture length and fracture density. (3) The main cavity categories could be distinguished by cavity width, cavity height and fracture density. Performances of the approach have been examined with 10 percentages of the samples, and a good agreement performed in the simulated results, and anastomosis rate was more than 80%. The researched results have critical guiding significance to evaluate types of fracture-cavity, develop and explore of fracture-cavity reservoirs. The construction technique of knowledge base can be applied for diverse fracture-cavity reservoirs in the various formations in different areas in the world.}
}
@article{EUGENIO2020100397,
title = {Estimation of soybean yield from machine learning techniques and multispectral RPAS imagery},
journal = {Remote Sensing Applications: Society and Environment},
volume = {20},
pages = {100397},
year = {2020},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2020.100397},
url = {https://www.sciencedirect.com/science/article/pii/S2352938520301646},
author = {Fernando Coelho Eugenio and Mara Grohs and Luan Peroni Venancio and Mateus Schuh and Eduardo Leonel Bottega and Régis Ruoso and Cristine Schons and Caroline Lorenci Mallmann and Tiago Luis Badin and Pablo Fernandes},
keywords = {Precision agriculture, UAV, Artificial intelligence, Predictive modeling, Crop yield},
abstract = {Throughout the plant development, the physiological processes of growth, as well as the handling of water and fertilizers represent sources of variation in the spectral response of the crops. Thus, in the context of precision agriculture, information on the phenological stage of cultivation can be decisive in accurately estimating agronomic variables through the use of aerial images obtained by RPAS (Remotely Piloted Aircraft System) platforms. Therefore, the aim of this study was: i) to investigate the potential of using RPAS embedded with multispectral sensors in order to obtain data on a soybean irrigated field with a central pivot system ii) to identify the best stage of development of irrigated soybean in order to obtain multispectral images aiming to the yield prediction; iii) to test different vegetation indices (VIs) for estimating the yield of soybean crop using artificial neural networks. The treatments were established using different metric water potentials in the soil and the analysis was comprised by 30 sample units. Multispectral images were acquired through a Sequoia® camera aboard the Phantom 4® Pro platform, during seven phenological crop stages. In addition to the spectral bands, nine VIs were taken as predictors based on the response variables derived from the site survey. The selection of the best growth stage was based on the highest level of association, represented by the Spearman correlation coefficient between predictors and each response variable. The Multi-Layer Perceptron (MLP) algorithm was used to adjust the predictive model, whose performance was assessed in terms of training and testing. To have independent measurements of the performance of the proposed algorithm was used by k-fold cross-validation. The results obtained from the correlation between the image-derived data and the soybean yield indicate that the ideal monitoring window for irrigated soybean can be found at the end of the vegetative stage (V6). The selection of the phenological crop stage had a positive impact on the predictions. The MLP models showed a good adjustment and good generalization capability. In turn, grain yield was the most complex agronomic parameter for modeling, with correlations of 0.70 and 0.92 for training/testing, and excellent results in other statistical tests, reinforce the great combining capacity between remote sensing via RPAS and machine learning (ML) in applications aimed at precision agriculture. This approach offers a useful tool for evaluating grain yield in center-pivot-irrigated soybean crops and emphasizes the need of observation regarding the phenological crop stage as a factor of analysis in the prediction of grain yield via remote sensing.}
}
@article{BROOK2020111679,
title = {A smart multiple spatial and temporal resolution system to support precision agriculture from satellite images: Proof of concept on Aglianico vineyard},
journal = {Remote Sensing of Environment},
volume = {240},
pages = {111679},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.111679},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720300481},
author = {A. Brook and V. {De Micco} and G. Battipaglia and A. Erbaggio and G. Ludeno and I. Catapano and A. Bonfante},
keywords = {CNN image reconstruction, Pan-sharpening, Vineyard plant status, Dendro-ecological analysis, Plant hydraulics, Precision agriculture, Sentinel-2A, UAV, Wood anatomy, And isotopes},
abstract = {In this century, one of the main objectives of agriculture is sustainability addressed to achieve food security, based on the improvement of use efficiency of farm resources, the increasing of crop yield and quality, under climate change conditions. The optimization of farm resources, as well as the control of soil degradation processes (e.g., soil erosion), can be realized through crop monitoring in the field, aiming to manage the local spatial variability (time and space) with a high resolution. In the case of high profitability crops, as the case of vineyards for high-quality wines, the capability to manage and follow spatial behavior of plants during the season represents an opportunity to improve farmer incomes and preserve the environmental health. However, any field monitoring represents an additional cost for the farmer, which slows down the objective of a diffuse sustainable agriculture. Satellite multispectral images have been widely used for production management in large areas. However, their observation is limited by the pre-defined and fixed scale with relatively coarse spatial resolution, resulting in limitations in their application. In this paper, encouraged by recent achievements in convolutional neural network (CNN), a multiscale full-connected CNN is constructed for the pan-sharpening of Sentinel-2A images by UAV images. The reconstructed data are validated by independent multispectral UAV images and in-situ spectral measurements. The reconstructed Sentinel-2A images provide a temporal evaluation of plant responses using selected vegetation indices. The proposed methodology has been tested on plant measurements taken either in-vivo and through the retrospective reconstruction of the eco-physiological vine behavior, by the evaluation of water conductivity and water use efficiency indexes from anatomical and isotopic traits recorded in vine trunk wood. In this study, the use of such a methodology able to combine the pro and cons of space-borne and UAVs data to evaluate plant responses, with high spatial and temporal resolution, has been applied in a vineyard of southern Italy by analyzing the period from 2015 to 2018. The obtained results have shown a good correspondence between the vegetation indexes obtained from reconstructed Sentinel-2A data and plant hydraulic traits obtained from tree-ring based retrospective reconstruction of vine eco-physiological behavior.}
}
@article{RODRIGUEZ2021106061,
title = {Assessment of potato late blight from UAV-based multispectral imagery},
journal = {Computers and Electronics in Agriculture},
volume = {184},
pages = {106061},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106061},
url = {https://www.sciencedirect.com/science/article/pii/S016816992100079X},
author = {Jorge Rodríguez and Iván Lizarazo and Flavio Prieto and Victor Angulo-Morales},
keywords = {Late blight, UAV, Machine learning, Potato, Agriculture},
abstract = {This paper evaluates a method for assessment and detection of potato late blight using UAV-based multispectral imagery. Traditional methods of detection and mapping of late blight are time consuming, require large human effort and, in many cases, are subjective. The approach evaluated integrates morphological operations and evaluates the performance of five Machine Learning (ML) algorithms: Random forest, Gradient Boosting Classifier, Support Vector Classifier, Linear Support Vector Classifier and K-Nearest Neighbours Classifier to detect zones of late blight occurrence. The main components of the proposed approach are: (i) radiometric and geometric correction of raw images; (ii) soil and weed removal by application of a thresholding technique; (iii) a supervised classification procedure using ML algorithms; and (iv) use of trained models to classify a new data set. The performance of the method is evaluated on two dates in an experimental potato field. Results showed that the Linear Support Vector Classifier and Random Forest algorithms had the best performance in terms of both accuracy metrics and run time. The study showed that the proposed method allows the detection of late blight with little human intervention.}
}
@article{JEON2021101430,
title = {Semantic segmentation of seagrass habitat from drone imagery based on deep learning: A comparative study},
journal = {Ecological Informatics},
volume = {66},
pages = {101430},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101430},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002211},
author = {Eui-ik Jeon and Sunghak Kim and Soyoung Park and Juwon Kwak and Imho Choi},
keywords = {Seagrass, Drone, UAV, Semantic segmentation, Deep learning, Image normalization},
abstract = {In this study, the utilization of drone images and deep learning to monitor the seagrass habitat, which is important in the marine ecosystem, is evaluated. Two experiments were conducted to compare the effect of image normalization and the performance of deep learning models in semantic segmentation with drone optical images acquired for the alpine habitats in coastal waters. Z-score and Min-Max normalization techniques were used to examine the effect of image normalization, and U-Net, SegNet, PSPNet, and DeepLab v3+ were used to compare the performance of the deep learning models. As a result, Min-Max normalization demonstrated outstanding performance for optical images, and Z-score normalization for black and white images. Regardless of the normalization of the image, the performance of the models was ranked in the order of U-Net, PSPNet, SegNet, and DeepLab v3+. Although the latest model, DeepLab v3+, was expected to have excellent performance, in fact, U-Net, having a relatively simple structure and a small number of parameters, showed the best performance. As the accuracy of semantic results seems to depend on the deep learning models and normalization methods, an experiment to determine an appropriate normalization method and deep learning model should be preceded for the semantic segmentation of high-resolution optical images in coastal waters.}
}
@article{WANG2021107505,
title = {Neural sliding mode control of low-altitude flying UAV considering wave effect},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107505},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107505},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004523},
author = {Xia Wang and Shaoshan Sun and Chenggang Tao and Bin Xu},
keywords = {Flight dynamics, Wave effect, Sliding mode control, Neural approximation},
abstract = {This paper investigates the adaptive sliding mode control (SMC) for fixed-wing UAV using neural networks (NNs). Considering the wave effect during low-altitude flying, the wave height is identified based on autoregressive model while the model parameters are handled by recursive least square method. Considering the aerodynamic uncertainties caused by unknown sea environment, NNs are employed to deal with the system nonlinearities. For the update of neural weights, the prediction error that indicates the learning performance is constructed. Based on the information of neural approximation and wave height identification, the neural learning control is finally developed for the altitude subsystem. The neural SMC is accordingly constructed for the velocity subsystem. Under the proposed method, the uniformly ultimately bounded stability is achieved. Simulation is presented to show that the proposed method can achieve good tracking performance.}
}
@article{TAN2022107264,
title = {Anti-saturation adaptive fault-tolerant control with fixed-time prescribed performance for UAV under AOA asymmetric constraint},
journal = {Aerospace Science and Technology},
volume = {120},
pages = {107264},
year = {2022},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.107264},
url = {https://www.sciencedirect.com/science/article/pii/S1270963821007744},
author = {Jian Tan and Yanfei Dong and Pengyuan Shao and Gaomin Qu},
keywords = {Prescribed performance, Fixed-time control, Actuator faults, Control input saturation, AOA asymmetric constraint},
abstract = {This paper presents an anti-saturation adaptive fault-tolerant control scheme with fixed-time prescribed performance for the longitudinal model of fixed wing UAV under actuator faults, control input saturation, angle of attack (AOA) asymmetric constraint and uncertainties. First, by introducing equivalent error transformation technique, the error transformed models of the altitude and airspeed tracking subsystems are strictly constructed. Second, asymmetric saturation function, auxiliary system and barrier Lyapunov function (BLF) are incorporated to guarantee AOA maintain in the asymmetric constraint. Then, neural network approximation structures are combined with adaptive robust terms to handle the lumped disturbances. Furthermore, Nussbaum-type gain technique is adopted to deal with the unknown time-varying parameter arising from input saturation and actuator faults. According to the stability analysis, the altitude and airspeed tracking errors evolve strictly inside the fixed-time performance envelops, while the AOA remains in asymmetric constraint and all signals in closed-loop system are uniformly ultimately bounded. Finally, numerical simulation is presented to verify the effectiveness and superiority of proposed control scheme.}
}
@article{FLOREZ202199,
title = {Inducting Chaos on a Drone Network},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {17},
pages = {99-104},
year = {2021},
note = {6th IFAC Conference on Analysis and Control of Chaotic Systems CHAOS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321020516},
author = {Julio A. Flórez and Luz A. Vega and Edgar N. Sanchez and Alexander G. Loukianov and Carlos Borrás},
keywords = {Formation flight, complex networks synchronization, Quadrotor neural model, UAVs, discrete sliding mode control},
abstract = {This paper presents a trajectory tracking method for the formation flight of drones, which based on the synchronization of desired references in trajectory by means of complex networks in discrete-time. The pinning sliding modes control is used to track the desired trajectory controlling only the trajectory of the leader drone, and the coupling strengths are updated online during the process to achieve a smooth transition towards synchronization. The whole formation is realized by means of distancing and orientation of the drones with respect to the leader drone. The results are illustrated via simulations where each node corresponds with a quadrotor modelled by means of a high-order recurrent neural network locally controlled, whose goal is achieving that the whole drone network track an attractor chaotic type Rossler.}
}
@article{DONMEZ20118138,
title = {Design and Control of a Shape Memory Alloy Actuator for Flap Type Aerodynamic Surfaces},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {8138-8143},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.01113},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016449177},
author = {Burcu Dönmez and Bülent Özkan},
keywords = {Shape memory alloys and control, antagonistic actuation, power feedback, self-sensing capability},
abstract = {Abstract
This paper presents flap control of unmanned aerial vehicles (UAVs) using shape memory alloy (SMA) wires as actuator. In this content, the mathematical model of the SMA wire is obtained through characterization tests then model of the proposed flap mechanism is derived. Later, based on these models, both flap angle and power dissipation of the SMA wire are controlled in two different loops employing compensated proportional-integral type and neural network-based control schemes. The angle commands are converted to power commands through the outer loop controller later which are updated based on the error in the flap angle induced because of the indirect control and external effects. In this study, power consumption of the wire is introduced as a new internal feedback variable instead of generally employed resistance feedback. Constructed simulation models are run and performance specifications of the proposed control systems are investigated. Eventually, it is shown that proposed controllers perform well in terms of achieving small tracking errors.}
}
@article{LI2013230,
title = {A Software Scheme for UAV's Safe Landing Area Discovery},
journal = {AASRI Procedia},
volume = {4},
pages = {230-235},
year = {2013},
note = {2013 AASRI Conference on Intelligent Systems and Control},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.035},
url = {https://www.sciencedirect.com/science/article/pii/S221267161300036X},
author = {Xiaoming Li},
keywords = {UAV, landing area discovery, machine learning, computer vision},
abstract = {This paper proposes a software processing scheme for small-sized UAV in its landing area discovery using its single onboard camera and machine learning algorithms. The two-stage processing procedure was proposed. In first stage a similarity based textured area identification method was adopted to find the possible landing areas. Afterwards, in second stage, these results were refined and evaluated by using some machine learning algorithms. The UAV can then take use of these results as its emergency landing target options. The software scheme we designed implemented the whole process but still allow the developers to embed their own algorithms to make better results. Our preliminary research has disclosed that this software and application are useful and can provide great convenience and efficiency.}
}
@article{KUMAR20211,
title = {A drone-based networked system and methods for combating coronavirus disease (COVID-19) pandemic},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {1-19},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20317064},
author = {Adarsh Kumar and Kriti Sharma and Harvinder Singh and Sagar Gupta Naugriya and Sukhpal Singh Gill and Rajkumar Buyya},
keywords = {Artificial intelligence, Collision avoidance, COVID-19, Drones, Internet of Things, Pandemic},
abstract = {Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. It is similar to influenza viruses and raises concerns through alarming levels of spread and severity resulting in an ongoing pandemic worldwide. Within eight months (by August 2020), it infected 24.0 million persons worldwide and over 824 thousand have died. Drones or Unmanned Aerial Vehicles (UAVs) are very helpful in handling the COVID-19 pandemic. This work investigates the drone-based systems, COVID-19 pandemic situations, and proposes an architecture for handling pandemic situations in different scenarios using real-time and simulation-based scenarios. The proposed architecture uses wearable sensors to record the observations in Body Area Networks (BANs) in a push–pull data fetching mechanism. The proposed architecture is found to be useful in remote and highly congested pandemic areas where either the wireless or Internet connectivity is a major issue or chances of COVID-19 spreading are high. It collects and stores the substantial amount of data in a stipulated period and helps to take appropriate action as and when required. In real-time drone-based healthcare system implementation for COVID-19 operations, it is observed that a large area can be covered for sanitization, thermal image collection, and patient identification within a short period (2 KMs within 10 min approx.) through aerial route. In the simulation, the same statistics are observed with an addition of collision-resistant strategies working successfully for indoor and outdoor healthcare operations. Further, open challenges are identified and promising research directions are highlighted.}
}
@article{PADHY2018643,
title = {Deep Neural Network for Autonomous UAV Navigation in Indoor Corridor Environments},
journal = {Procedia Computer Science},
volume = {133},
pages = {643-650},
year = {2018},
note = {International Conference on Robotics and Smart Manufacturing (RoSMa2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918310524},
author = {Ram Prasad Padhy and Sachin Verma and Shahzad Ahmad and Suman Kumar Choudhury and Pankaj Kumar Sa},
keywords = {UAV, Monocular, GPS, Convolution, Deep Neural Network, Classification Task},
abstract = {In recent years, the UAV technology is unceasingly emerging as a revolutionary reform among the research community. In this paper, we propose a method that facilitates UAVs with a monocular camera to navigate autonomously in previously unknown and GPS-denied indoor corridor arenas. The proposed system uses a state-of-the-art Convolutional Neural Network (CNN) model to achieve the task. We propose a novel approach, which uses the video feed extracted from the front camera of the UAV and passes it through a deep neural network model to decide on the next course of maneuver. The entire process is treated as a classification task where the deep neural network model is responsible for classifying the image as left, right or center of the corridor. The training is performed over a dataset of images, collected from various indoor corridor environments. Apart from utilizing the front facing camera, the model is not dependent on any other sensor. We demonstrate the efficacy of the proposed system in real-time indoor corridor scenarios.}
}
@article{ZHANG2020280,
title = {Identifying and mapping individual plants in a highly diverse high-elevation ecosystem using UAV imagery and deep learning},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {169},
pages = {280-291},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302720},
author = {Ce Zhang and Peter M. Atkinson and Charles George and Zhaofei Wen and Mauricio Diazgranados and France Gerard},
keywords = {Multi-scale deep learning, Residual U-Net, Scale sequence, Semantic segmentation, Páramos},
abstract = {The identification and counting of plant individuals is essential for environmental monitoring. UAV based imagery offer ultra-fine spatial resolution and flexibility in data acquisition, and so provide a great opportunity to enhance current plant and in-situ field surveying. However, accurate mapping of individual plants from UAV imagery remains challenging, given the great variation in the sizes and geometries of individual plants and in their distribution. This is true even for deep learning based semantic segmentation and classification methods. In this research, a novel Scale Sequence Residual U-Net (SS Res U-Net) deep learning method was proposed, which integrates a set of Residual U-Nets with a sequence of input scales that can be derived automatically. The SS Res U-Net classifies individual plants by continuously increasing the patch scale, with features learned at small scales passing gradually to larger scales, thus, achieving multi-scale information fusion while retaining fine spatial details of interest. The SS Res U-Net was tested to identify and map frailejones (all plant species of the subtribe Espeletiinae), the dominant plants in one of the world’s most biodiverse high-elevation ecosystems (i.e. the páramos) from UAV imagery. Results demonstrate that the SS Res U-Net has the ability to self-adapt to variation in objects, and consistently achieved the highest classification accuracy (91.67% on average) compared with four state-of-the-art benchmark approaches. In addition, SS Res U-Net produced the best performances in terms of both robustness to training sample size reduction and computational efficiency compared with the benchmarks. Thus, SS Res U-Net shows great promise for solving remotely sensed semantic segmentation and classification tasks, and more general machine intelligence. The prospective implementation of this method to identify and map frailejones in the páramos will benefit immensely the monitoring of their populations for conservation assessments and management, among many other applications.}
}
@article{ALLEN2019174,
title = {A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance},
journal = {Robotics and Autonomous Systems},
volume = {115},
pages = {174-193},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017308692},
author = {Ross E. Allen and Marco Pavone},
keywords = {Motion planning, Kinodynamic, Real-time, Obstacle avoidance, Quadrotor, Unmanned aerial vehicle, Machine learning, Human–robot interaction},
abstract = {The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.}
}
@article{PARK2012498,
title = {Bird strike event monitoring in a composite UAV wing using high speed optical fiber sensing system},
journal = {Composites Science and Technology},
volume = {72},
number = {4},
pages = {498-505},
year = {2012},
issn = {0266-3538},
doi = {https://doi.org/10.1016/j.compscitech.2011.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0266353811004349},
author = {Chan Yik Park and Byeong-Wook Jang and Jong Heon Kim and Chun-Gon Kim and Seung-Moon Jun},
keywords = {A. Composite structures, A. Fiber Bragg Grating (FBG) sensor, B. Bird impact, C. Neural network},
abstract = {In this study, high speed bird strikes on a composite structure were successfully monitored using optical fiber sensors. Four multiplexed optical fiber sensors in a single cable were surface-bonded on the leading edge of a composite UAV wing box. In order to acquire those high frequency signals, a newly developed interrogation system was used to process strain signals from four sensors simultaneously at a sampling frequency of 100kHz. Before the bird strike tests, pre-impact tests using a rubber hammer were performed to verify the suitability of the FBG signal acquisitions. The pre-test data were used in the neural network training procedures to estimate the bird strike locations. Then, the bird strike tests were accomplished using dummy projectiles and a pneumatic gun. The one-pound dummy birds, made of gelatin, hit the leading edge with a maximum speed of 201km/h. The impact signals were successfully recorded during the tests and their frequency characteristics were then analyzed. Finally, the strike locations were estimated with the neural network which was trained through the pre-tests. The average error was 33.6mm.}
}
@article{KALKE2018225,
title = {Support vector machine learning applied to digital images of river ice conditions},
journal = {Cold Regions Science and Technology},
volume = {155},
pages = {225-236},
year = {2018},
issn = {0165-232X},
doi = {https://doi.org/10.1016/j.coldregions.2018.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0165232X17304251},
author = {H. Kalke and M. Loewen},
abstract = {In this study the use of support vector machines (SVM), a popular type of machine learning algorithm, for monitoring river ice properties during freeze-up was investigated. The goal was to develop an automated image processing method that could be used to accurately compute the total surface ice concentration and also discriminate between frazil and released floating anchor ice pans. Total surface ice concentration has previously been computed from digital images of surface ice conditions using the most common image segmentation method, thresholding. However, thresholding techniques often produce inaccurate results or if performed manually can be highly subjective and labour intensive. Three site specific SVM models were trained in this study to accurately distinguish between surface ice and water, and produce binary images from which the total surface ice concentration could be computed. The digital images of river ice conditions were acquired using bridge-mounted game cameras and an unmanned aerial vehicle on two Alberta rivers. The total surface ice concentrations computed from the SVM generated binary images were significantly more accurate than concentrations computed using four thresholding methods. The trained SVM models were then used to compute spatial distributions of surface ice concentration across the two rivers from the UAV images. In addition, time-series of surface ice concentrations during freeze-up were also computed from the bridge-mounted game camera images. Site specific SVM models for estimating surface ice concentration were shown to be a feasible and accurate tool for river ice monitoring that could potentially aid in the validation of numerical models and improve understanding of river freeze-up processes. An SVM to separate total surface ice into frazil and anchor ice components was also trained and validated. This model produced mixed results when tested on images from two anchor ice release events and additional research is needed to more fully assess its potential.}
}
@article{AMBATI201412202,
title = {A Neuro-Adaptive Augmented Dynamic Inversion Design for Robust Auto-Landing},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {12202-12207},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.01315},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016435561},
author = {Pradeep R. Ambati and Radhakant Padhi},
abstract = {A neuro-adaptive augmented nonlinear dynamic inversion approach is proposed in this paper for robust automatic landing of unmanned aerial vehicles. Following the philosophy of indirect adaptive control, a set of linear in weight neural networks are used to rapidly learn the unknown part of the system model online. This continuously updated model is simultaneously used in a dynamic inversion framework that results in an adaptive controller which is quite robust to the modeling inaccuracies (i.e. parametric uncertainties of the model) and external wind shear disturbance. The training rule of neural networks is obtained from Lyapunov stability theory, where a Sobolev norm based Lyapunov function is chosen. This leads to 'directional learning', resulting in fast learning without exciting too much of transient oscillations. Robust performance of this neuro-adaptive augmented nonlinear controller is successfully validated through six degree-of-freedom simulation studies.}
}
@article{LIU2018392,
title = {Deep learning based trajectory optimization for UAV aerial refueling docking under bow wave},
journal = {Aerospace Science and Technology},
volume = {80},
pages = {392-402},
year = {2018},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2018.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S127096381830823X},
author = {Yiheng Liu and Honglun Wang and Zikang Su and Jiaxuan Fan},
keywords = {Autonomous aerial refueling (AAR), Receiver trajectory optimization, Deep learning, Bow wave, Drogue motion prediction},
abstract = {In the autonomous aerial refueling (AAR) docking process, the bow wave generated by the receiver has a strong effect on the drogue, which affects the docking success rate greatly. Thus, a deep learning based trajectory optimization method which aims to decrease the bow wave effect on the drogue is proposed in this paper. There are mainly three parts in the proposed trajectory optimization method. Firstly, a precise bow wave model based on deep learning is presented to estimate the bow wave effect on the drogue. Furthermore, due to the dynamic characteristic of the drogue, a simple and practical drogue motion prediction model under multiple disturbances is carried out to provide a precise prediction of the drogue position at the next time. Moreover, considering the strict attitude constraints requirements in the AAR docking process, a novel reference observer is designed to estimate the receiver attitude from the optimized trajectory under wind perturbations. Then, the proposed trajectory optimization method could not only diminish the bow wave effect on the drogue largely but also satisfy the attitude constraints of the receiver. Finally, the effectiveness of the proposed method is demonstrated by the simulations.}
}
@article{KWAK2013251,
title = {Optimization of Decentralized Task Assignment for Heterogeneous UAVs},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {11},
pages = {251-256},
year = {2013},
note = {11th IFAC Workshop on Adaptation and Learning in Control and Signal Processing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130703-3-FR-4038.00072},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016329548},
author = {Dong Jun Kwak and Sungwon Moon and Suseong Kim and H. Jin Kim},
keywords = {Multi-agent system, Heterogeneous UAVs, Decentralized task assignment, CBBA},
abstract = {In this paper, the optimization of a decentralized task assignment strategy for heterogeneous UAVs in a probabilistic engagement scenario is investigated. In the engagement scenario, each UAV selects its targets by employing the consensus-based bundle algorithm (CBBA). This paper uses a scoring matrix to reflect heterogeneity among the UAVs and targets with different capabilities. Therefore, a performance improvement of CBBA is closely connected with the scoring matrix and it should be optimally selected. The values of scoring matrix can be obtained by employing an episodic parameter optimization (EPO). The EPO algorithm is performed during the numerous repeated simulation runs of the engagement and the reward of each episode is updated using reinforcement learning. The candidate scoring matrices are selected by using particle swarm optimization. The optimization results show that the team survivability of the UAVs is increased after performing the EPO algorithm and the values of the optimized score matrix are also optimally selected.}
}
@article{ZHOU2019305,
title = {Scale adaptive image cropping for UAV object detection},
journal = {Neurocomputing},
volume = {366},
pages = {305-313},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.07.073},
url = {https://www.sciencedirect.com/science/article/pii/S092523121931080X},
author = {Jingkai Zhou and Chi-Man Vong and Qiong Liu and Zhenyu Wang},
keywords = {Data enhancement, UAV aerial imagery, Object detection, Deep neural network},
abstract = {Although deep learning methods have made significant breakthroughs in generic object detection, their performance on aerial images is not satisfactory. Unlike generic images, aerial images have smaller object relative scales (ORS), more low-resolution objects, and serious object scale diversity. Most researches focus on modifying network structures to address these challenges, while few studies pay attention to data enhancement which can be used in combination with model modification to further improve detection accuracy. In this work, a novel data enhancement method called scale adaptive image cropping (SAIC) is proposed to address these three challenges. Specifically, SAIC consists three steps: ORS estimation in which a specific neural network is designed to estimate ORS levels of images; image resizing in which a GAN-based super-resolution method is adopted to up-sample images with the smallest ORS level, easing low-resolution object detection; image cropping in which three cropping strategies are proposed to crop resized images, adjusting ORS. Extensive experiments are conducted to demonstrate the effectiveness of our method. SAIC improves the accuracy of feature pyramid network (FPN) by 9.65% (or relatively 37.06%). Without any major modification, FPN trained with SAIC won the 3rd rank on 2018 VisDrone challenge detection task.}
}
@article{JACKSON2020658,
title = {Season, Classifier, and Spatial Resolution Impact Honey Mesquite and Yellow Bluestem Detection using an Unmanned Aerial System},
journal = {Rangeland Ecology & Management},
volume = {73},
number = {5},
pages = {658-672},
year = {2020},
issn = {1550-7424},
doi = {https://doi.org/10.1016/j.rama.2020.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1550742420300762},
author = {Matthew Jackson and Carlos Portillo-Quintero and Robert Cox and Glen Ritchie and Mark Johnson and Kamal Humagain and Mukti Ram Subedi},
keywords = {Honey mesquite, Rangelands, Remote sensing, Texas, UAV, Yellow bluestem},
abstract = {In Texas, mesquite and yellow-bluestem invasions are widespread. Identifying and monitoring juvenile and adult plants using high-resolution imagery from airborne sensors while they colonize new areas across the landscape can help land managers prioritize locations for treatment and eradication. In this study, we evaluated how data collection design using an unmanned aerial system (UAS) can affect plant detection and mapping. We used a Phantom 3 Professional unmanned aerial vehicle with a Parrot Sequoia multispectral camera for detecting and mapping native honey mesquite (Prosopis glandulosa) and non-native yellow bluestem (Bothriochloa ischaemum) at a rangeland site in northwest Texas. Flights were conducted seasonally during the period from summer 2017 to fall 2018 to test the seasonal impact of detecting plant species. Flights were conducted at altitudes of 30, 60, and 100 m, and four image classification techniques were tested to determine their viability of detecting distinct plant species. Results suggest that flights at 100-m aircraft altitude during the spring season are more effective (>80% user accuracies) for mapping mesquite canopies based on reflectance values and image segmentation information. Yellow bluestem mapping accuracies were low (< 20% user accuracies). Lower spatial resolution (100-m altitude flights, 12-cm pixel resolution) provided less noise and more generalization capabilities for the image classification methods. Overall, random forests and Support Vector Machine classification algorithms outperformed probability-based image classifiers. Land owners and rangeland ecologists using their own UAS in rangeland management can use this information to plan their data collection campaigns before the application of chemical treatments or manual eradication.}
}
@article{RAZMI201912,
title = {Neural network-based adaptive sliding mode control design for position and attitude control of a quadrotor UAV},
journal = {Aerospace Science and Technology},
volume = {91},
pages = {12-27},
year = {2019},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S1270963818310447},
author = {Hadi Razmi and Sima Afshinfar},
keywords = {Sliding mode control, Neural network, Quadrotor},
abstract = {In this paper, a novel method is suggested for the position and attitude tracking control of a quadrotor UAV in the presence of parametric uncertainties and external disturbance. The proposed method combines neural network adaptive scheme with sliding mode control, which preserves the advantages of the two methods. Firstly, dynamic model of quadrotor is divided into two fully actuated and under actuated subsystems. Secondly, sliding mode controllers are corresponding designed for each subsystem, and their coefficients in sliding manifolds are adaptively tuned by the neural network method. In each section, using Lyapunov theory, stability of closed loop system is proven. Finally, the method is examined for a square path tracking and a maximum overshoot of 7.5133% and a settling time 5.6648 s are obtained. By comparing the results obtained through different methods, it is concluded that the proposed controller provides the following main advantages: (1) good transient and steady state behaviors, (2) insensitivity to parameter variations, (3) disturbance rejection capability, and (4) remarkable stability and performance robustness. Hence, for operational purposes in which the fast and accurate response are of crucial importance, using the neural network-based adaptive sliding mode control approach is recommended.}
}
@article{LIU2020185,
title = {Pattern identification and analysis for the traditional village using low altitude UAV-borne remote sensing: multifeatured geospatial data to support rural landscape investigation, documentation and management},
journal = {Journal of Cultural Heritage},
volume = {44},
pages = {185-195},
year = {2020},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2019.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1296207418305958},
author = {Chun Liu and Yujie Cao and Chen Yang and Yuan Zhou and Mengchi Ai},
keywords = {Ancient village, UAV remote sensing, Multispectral imagery, Object-Based analysis, Spatial pattern analysis},
abstract = {Spatial pattern of landscapes is viewed as the fabric and structure of traditional village. Accurate detection and analysis for the landscapes pattern plays key role in understanding the sociocultural milieu and human-natural relations. Current methods perceive their problems. Pedestrian survey is labor and time consuming. Meanwhile, the derived data tend to be subjective, qualitative and monotonous, which can hardly be used for further analysis. Remote sensing technique has been successfully applied in the field of heritage protection for its ability in object detection. But these methods are limited by visiting circle, spatial resolution and data richness. Therefore, the scientific methods of landscape pattern detection, documentation and analysis for the traditional village has long been under discussion. By taking Baojiatun castle village as a case study, the present paper aims to detect and analyze spatial pattern of traditional village by the geospatial data from a low altitude UAV-borne remote sensing. A four-leveled hierarchical landscape recognition scheme and the corresponding landscape category regulation were established. Based on the derived data and the established scheme, a three-level classification model was construct by using Object-Oriented Image Analysis method (OBIA) method and machine learning classifiers (Random Forest classifier and SVM classifier). The model was proven to be accurate and stable by ten-fold cross validation, and five major heritage landscape elements of the village were finally extracted. Furthermore, spatial pattern characteristics and distribution differences of targeted landscapes were unveiled based on distance statistics and clustering analysis. Lastly, further discussion is fostered, which focuses on the usefulness of remote sensing technique in the field of heritage landscape investigation, documentation and management.}
}
@article{TEAGUE2021100027,
title = {Time series classification of radio signal strength for qualitative estimate of UAV motion},
journal = {Machine Learning with Applications},
volume = {4},
pages = {100027},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000086},
author = {Samuel Teague and Javaan Chahl},
keywords = {LSTM, Received signal strength, UAV, Navigation},
abstract = {Many common navigation solutions fall short when an aircraft’s GPS signal is either jammed or spoofed. This is typically due to the iterative nature of the estimation process, which requires an acceptably accurate initial estimate, or due to the accumulated error of inertial sensors, which are unable to directly observe the position of an aircraft. A mechanism is presented in this paper which operates on qualitative information, allowing an aircraft to remain within a vicinity despite an absence of precision localization. A long-short-term-memory neural network was used for time series classification of radio signal strength data on a light weight fixed wing UAV. Simulation results show that the two class classifier is able to determine the motion of an aircraft with respect to a radio beacon with 97.73% accuracy. The classes used for classification represent motion as either towards, or away from a beacon. A simple high level controller was designed to use the classification output and converge on a beacon. Results from this paper indicate that this unique application of qualitative navigation by the application of time series classification offers a viable alternative to aircraft navigation in GPS denied environments.}
}
@article{MA2022116226,
title = {Towards improved accuracy of UAV-based wheat ears counting: A transfer learning method of the ground-based fully convolutional network},
journal = {Expert Systems with Applications},
volume = {191},
pages = {116226},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116226},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015396},
author = {Juncheng Ma and Yunxia Li and Hongjie Liu and Yongfeng Wu and Lingxian Zhang},
keywords = {Number of wheat ears, UAV, Digital images, Convolutional neural network, Transfer learning},
abstract = {In order to achieve accurate UAV-based wheat ear counting, a transfer learning method of the ground-based fully convolutional network, i.e., EarDensityNet, was proposed in this study. The EarDensityNet, which integrated the filter pyramid block and dilated convolution, was designed to map the wheat canopy images to ear density maps generated by dot annotations. The wheat ear counting can be obtained by summing all the pixel values of the corresponding ear density map. Results showed strong correlations could be observed between the actual number of wheat ears to those estimated by the EarDensityNet, with high coefficient of determination (R2 = 0.9179) and low Root-Mean-Square-Error (RMSE = 17.61 ears, NRMSE = 4.47%), outperforming the compared methods. Ground resolution of canopy images had a significant impact on the performance of the EarDensityNet. Transfer learning of the ground-based EarDensityNet could take full advantage of the rich details presented by the ground-based images with high pixel resolution, thus effectively alleviating the degradation of counting performance caused by the decreased ground resolution. Therefore, obtained results showed the fine-tuned EarDensityNet more accurate UAV-based wheat ear counting (R2 = 0.9570, RMSE = 801.34, and NRMSE = 22.06%) than one learned from scratch, demonstrating the superiority and applicability. Border effect from splitting digital images with high pixel resolution into sub-images did not make a major problem to the EarDensityNet, demonstrating great potentials to be generalized from plot-wise to field-wise. Wheat ear counting was recommended after the flowering stage since the textures of wheat ears were more obvious for EarDensityNet to learn complex feature representations.}
}
@article{ZHAO201822,
title = {A robust extreme learning machine for modeling a small-scale turbojet engine},
journal = {Applied Energy},
volume = {218},
pages = {22-35},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.02.175},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918303088},
author = {Yong-Ping Zhao and Qian-Kun Hu and Jian-Guo Xu and Bing Li and Gong Huang and Ying-Ting Pan},
keywords = {Extreme learning machine, Small-scale turbojet engine, System modeling, Machine learning},
abstract = {In this paper, a robust extreme learning machine is proposed. In comparison with the original extreme learning machine and the regularized extreme learning machine, this robust algorithm minimizes both the mean and variance of modeling errors in the objective function to overcome the bias-variance dilemma. As a result, its generalization performance and robustness are enhanced, and these merits are further proved theoretically. In addition, this proposed algorithm can keep the same computational efficiency as the original extreme learning machine and the regularized extreme learning machine. Then, several benchmark data sets are used to test the effectiveness and soundness of the proposed algorithm. Finally, it is employed to model a real small-scale turbojet engine. This engine is fit well. Especially, on the idle phase, where the signal-to-noise ratio is low and it is very hard to model, the proposed algorithm performs well and its robustness is sufficiently showcased. All in all, the proposed algorithm provides a candidate technique for modeling real systems.}
}
@article{DIAZGOMEZ2022108106,
title = {Mapping subaerial sand-gravel-cobble fluvial sediment facies using airborne lidar and machine learning},
journal = {Geomorphology},
volume = {401},
pages = {108106},
year = {2022},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.108106},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X21005146},
author = {Romina {Díaz Gómez} and Gregory B. Pasternack and Hervé Guillon and Colin F. Byrne and Sebastian Schwindt and Kenneth G. Larrieu and Samuel Sandoval Solis},
keywords = {Grain size, Lidar, Substrate, Gravelometry, Machine learning, Remote sensing, Fluvial geomorphology},
abstract = {Substrate facies monitoring is critical for the understanding of fluvial geomorphologic and ecohydraulic patterns and processes. However, direct substrate measurement is time-consuming and subjected to data sparsity because of small sample, size, and limited data collections within an area of interest, which make it difficult to capture facies patterns. Most new experimental studies focus on mapping substrate based on median grain size of a specific grain size class using automatic or semiautomatic photosieving techniques. This study aimed to develop and apply a method to accurately predict size-mixture facies patterns on exposed riverbeds with minimal ground truth plots (100) using airborne lidar and machine learning. The selected testbed river was a 37.5-km stretch of the regulated lower Yuba River in California, USA, mapped at sub-meter resolution in 2017. First, we designed a grid-by-point grain size sampling method and binned grain sizes into representative mixtures, such as fine or large gravel, to assign subaerial facies labels. Second, we classified facies based on a multivariate cluster analysis. Third, we generated 15 lidar-derived topographic and spectral predictors. Six distinct size-mixture facies were identified from field data and a seventh, pure sand facies, from UAV data. A random forest predictive model with an 86% 10-fold cross-validation accuracy was applied to produce a facies map at the 1.54 m pixel scale. The detrended elevation was identified as the most important variable for predicting facies spatial patterning, followed by baseflow, wetted area proximity, and green lidar intensity. We conclude that machine learning combined with intensity lidar data is highly effective for distinguishing mixed classes of substrates. Ultimately, the new substrate mixture-binning approach also provides novel insights into the arrangement of river sediment facies patterns.}
}
@article{BHUSAL201918,
title = {Improving Pest Bird Detection in a Vineyard Environment using Super-Resolution and Deep Learning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {30},
pages = {18-23},
year = {2019},
note = {6th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.483},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319323924},
author = {Santosh Bhusal and Uddhav Bhattarai and Manoj Karkee},
keywords = {Bird detection, Vineyard, Unmanned Aerial Vehicles (UASs), Deep Learning, Super-resolution},
abstract = {Pest bird detection, classification, and recognition in vineyard environment are challenging because of their varying shapes, small size, movement, and outdoor environment. Motion is often used to detect flying birds in outdoor environment from video sequences. However, motion detection is sensitive to noise as well as background movement of leaves and give rise to false detection. The high-quality image resolution is desired for performance improvement in pattern recognition and analysis. This work presents the integration of super-resolution technology to enhance quality of small moving objects which were later on classified as birds or false positives using deep learning. Implementation of the super-resolution enhanced the image resolution which offers high pixel density and more details about the scene. With the implementation of super-resolution, the CNN-based classifier received enhanced feature information to perform more informed decision in classifying birds. The classification accuracy shows a significant rise from 70% to more than 90% after resolution enhancement. Results also show that the model trained with combined varying spatial resolution for the same set of images performs almost equally over any spatial resolution.}
}
@article{WANG202054,
title = {Joint-learning segmentation in Internet of drones (IoD)-based monitor systems},
journal = {Computer Communications},
volume = {152},
pages = {54-62},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419313118},
author = {Eric Ke Wang and Chien-Ming Chen and Fan Wang and Muhammad Khurram Khan and Saru Kumari},
keywords = {Unmanned aerial vehicle, Object segmentation, Internet of drones, Conditional random field},
abstract = {Object segmentation of monitor systems based on the Internet of drones plays an important role in the practical applications of wide-area smart-city intelligent monitoring systems. It is an important step for extracting objects from remote-sensing images, and provides a reliable theoretical basis for key property monitoring, environmental monitoring, disaster monitoring, and agricultural monitoring. To improve the accuracy of object segmentation and to solve the problem of inadequate edge recognition, a joint-learning segmentation scheme was designed that combines the conditional random field (CRF) model with an improved U-net model. It employs the improved U-net model as the front-end model of the joint-learning framework for feature fusion and the CRF model as the back-end of the joint-learning framework for transforming to gradient optimization-based recurrent neural networks. The joint-learning framework enables the front and back parts to interact with each other to obtain the location of the target and its classification information accurately. The joint-learning framework was realized on open datasets and compared with state-of-the-art remote-sensing image segmentation algorithms. The experiment results show that the accuracy of the ground object segmentation improved to 86.1%, which is an encouraging improvement.}
}
@article{CAMCI20181,
title = {An aerial robot for rice farm quality inspection with type-2 fuzzy neural networks tuned by particle swarm optimization-sliding mode control hybrid algorithm},
journal = {Swarm and Evolutionary Computation},
volume = {41},
pages = {1-8},
year = {2018},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2210650217302407},
author = {Efe Camci and Devesh Raju Kripalani and Linlu Ma and Erdal Kayacan and Mojtaba Ahmadieh Khanesar},
keywords = {Type-2 fuzzy neural networks, Particle swarm optimization, Sliding mode control, Unmanned aerial vehicle, System identification, Control},
abstract = {Agricultural robots, or agrobots, have been increasingly adopted in every aspect of farming from surveillance to fruit harvesting in order to improve the overall productivity over the last few decades. Motivated by the compelling growth of the agricultural robots in modern farms, in this work, an autonomous quality inspection over rice farms is proposed by employing quadcopters. Real-time control of these vehicles, however, is still challenging as they exhibit a highly nonlinear behavior especially for agile maneuvers. What is more, these vehicles have to operate under uncertain working conditions such as wind and gust disturbances as well as positioning errors caused by inertial measurement units and global positioning system. To handle these difficulties, as a model-free and learning control algorithm, type-2 fuzzy neural networks (T2-FNNs) are designed for the control of a quadcopter. The novel particle swarm optimization-sliding mode control (PSO-SMC) theory-based hybrid algorithm is proposed for the training of the T2-FNNs. In particular, the continuous version of PSO is adopted for the identification of the antecedent part of the T2-FNNs while the SMC-based update rules are utilized for the online learning of the consequent part during control. In the virtual environment, the quadcopter is expected to perform an autonomous flight including agile maneuvers such as steep turning and sudden altitude changes over a rice terrace farm in Longsheng, China. The simulation results for the T2-FNNs are compared with the outcome of conventional proportional-derivative (PD) controllers for different case studies. The results show that our method decreases the trajectory tracking integral squared error by %26 over PD controllers in the ideal case, while this ratio goes up to %95 under uncertain working conditions.}
}
@article{THARAYIL2020103904,
title = {Sensor Defense In-Software (SDI): Practical software based detection of spoofing attacks on position sensors},
journal = {Engineering Applications of Artificial Intelligence},
volume = {95},
pages = {103904},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103904},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620302402},
author = {Kevin Sam Tharayil and Benyamin Farshteindiker and Shaked Eyal and Nir Hasidim and Roy Hershkovitz and Shani Houri and Ilia Yoffe and Michal Oren and Yossi Oren},
keywords = {Sensor spoofing, Sensor fusion, Machine learning},
abstract = {Position sensors, such as the gyroscope, the magnetometer and the accelerometer, are found in a staggering variety of devices, from smartphones and UAVs to autonomous robots. Several works have shown how adversaries can mount spoofing attacks to remotely corrupt or even completely control the outputs of these sensors. With more and more critical applications relying on sensor readings to make important decisions, defending sensors from these attacks is of prime importance. In this work we present practical software based defenses against attacks on two common types of position sensors, specifically the gyroscope and the magnetometer. We first characterize the sensitivity of these sensors to acoustic and magnetic adversaries. Next, we present two software-only defenses: a machine learning-based single sensor defense, and a sensor fusion defense which makes use of the mathematical relationship between the two sensors. We performed a detailed theoretical analysis of our defenses, and implemented them on a variety of smartphones, as well as on a resource-constrained IoT sensor node. Our defenses do not require any hardware or OS-level modifications, making it possible to use them with existing hardware. Moreover, they provide a high detection accuracy, a short detection time and a reasonable power consumption.}
}
@article{ZHANG2020105845,
title = {Automatic extraction of wheat lodging area based on transfer learning method and deeplabv3+ network},
journal = {Computers and Electronics in Agriculture},
volume = {179},
pages = {105845},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105845},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920307341},
author = {Dongyan Zhang and Yang Ding and Pengfei Chen and Xiangqian Zhang and Zhenggao Pan and Dong Liang},
keywords = {Wheat lodging, Multiple growth stages, Deep learning, Transfer learning, DeepLabv3+ network},
abstract = {To provide technical support for lodging-resistant wheat breeding, post-disaster assessment, and analysis of factors affecting lodging, the dynamic and accurate extraction of lodging area is particularly important. Most existing methods of monitoring wheat lodging aim to extract the lodging area at a single growth stage, rendering the dynamic monitoring of wheat lodging highly difficult. Thus, this study was aimed at developing a method of estimating wheat lodging at multiple growth stages. For this purpose, nitrogen fertilizers were utilized at different levels to induce different lodging conditions in wheat fields. Unmanned aerial vehicles (UAVs) were used to obtain Red, Green and Blue (RGB) and multispectral images of the field at different wheat-growth stages. Based on these two types of images, a new method combining transfer learning and the DeepLabv3+ network is proposed herein to extract lodging areas at various wheat-growth stages. The proposed method was compared with the commonly used UNet for the extraction of the lodging area. The results show that the proposed method and UNet achieved dice coefficients of 0.82 and 0.75 (early flowering), 0.88 and 0.80 (late flowering), 0.89 and 0.86 (filling stage), 0.90 and 0.87 (early maturity), and 0.90 and 0.88 (late maturity), respectively, using RGB images; further, the proposed method and UNet achieved dice coefficients of 0.91 and 0.51 (early flowering), 0.89 and 0.28 (late flowering), 0.91 and 0.82 (filling stage), 0.93 and 0.76 (early maturity), and 0.92 and 0.56 (late maturity), respectively, at different wheat-growth stages using multispectral image data. Thus, the proposed method can be used to predict lodging at multiple wheat-growth stages, and it outperforms UNet. An effective tool for dynamic monitoring of wheat lodging has been proposed herein.}
}
@article{TETILA2020105836,
title = {Detection and classification of soybean pests using deep learning with UAV images},
journal = {Computers and Electronics in Agriculture},
volume = {179},
pages = {105836},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105836},
url = {https://www.sciencedirect.com/science/article/pii/S016816991831055X},
author = {Everton Castelão Tetila and Bruno Brandoli Machado and Gilberto Astolfi and Nícolas Alessandro de Souza Belete and Willian Paraguassu Amorim and Antonia Railda Roel and Hemerson Pistori},
keywords = {UAV, Remote sensing, Soybean pests, Precision agriculture, Deep learning},
abstract = {This paper presents the results of the evaluation of five deep learning architectures for the classification of soybean pest images. The performance of Inception-v3, Resnet-50, VGG-16, VGG-19 and Xception was evaluated for different fine-tuning and transfer learning strategies over a dataset of 5,000 images captured in real field conditions. The experimental results showed that the deep learning architectures trained with a fine-tuning can lead to higher classification rates in comparison to other approaches, reaching accuracies of up to 93.82%. In addition, deep learning architectures outperformed traditional feature extraction methods, such as SIFT and SURF with Bag-of-Visual Words approach, the semi-supervised learning method OPFSEMImst, and supervised learning methods used to classify images, for example, SVM, k-NN and Random Forest. The results indicate that architectures evaluated can support specialists and farmers in the pest control management in soybean fields.}
}
@article{LI2022106828,
title = {Study on flow distribution of irrigation canal system based on image velocimetry},
journal = {Computers and Electronics in Agriculture},
volume = {195},
pages = {106828},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106828},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001454},
author = {Song Li and Huhu Liu and Shiyu Wang and Yi Zhou and Bingxu Zhou and Yu Han},
keywords = {Deep learning, Monocular ranging, Open channel, UAV velocimetry, Surface flow field},
abstract = {With the development of computer technology, it has become possible to measure surface flow velocity using video technology. Ripples are produced when flowing, and flow velocity can be determined based on the ripples and floating objects in water. A UAV velocity measurement system based on optical flow method and YOLOV5 algorithm in deep learning has been established, to realize the monitoring of surface flow field of large rivers. The optical flow method and the deep learning algorithm are used to track the target, convert the pixel distance to actual distance by monocular ranging, and calculate the actual flow velocity. The method was successfully applied to the surface flow field measurement of the Yongji canal in the river-loop irrigation area of Inner Mongolia, and the results showed that the orthophoto graphic images obtained by the method were of high quality, the flow field calculation results were reasonable, and the calculation results agreed well with the measurements.}
}
@article{BAYRAKTAR20201,
title = {A low-cost UAV framework towards ornamental plant detection and counting in the wild},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {1-11},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620301696},
author = {Ertugrul Bayraktar and Muhammed Enes Basarkan and Numan Celebi},
keywords = {Object counting, Plant detection, Remote sensing, Aerial imagery, Geometrical relations},
abstract = {Object detection still keeps its role as one of the fundamental challenges within the computer vision territory. In particular, achieving satisfying results concerning object detection from outdoor images occupies a considerable space. In this study, in addition to comparing handcrafted feature detector/descriptor performance with deep learning methods over ornamental plant images at the outdoor, we propose a framework to improve the detection of these plants. Firstly, we take query images in the RGB format from the onboard UAV camera. Secondly, our model classifies the scene as a planting or an urban area. Thirdly, if the images are from planting area, thirdly, we filter the field according to the color and acquire only the green parts. Lastly, we feed the object detector model with the filtered area and obtain the category and localization of the plants as a result. In parallel, we also estimate the number of interested plants using the geometrical relations and predefined average plant size, then we verify the outputs of the object detector with this results. The conducted experiments show that deep learning based object detection methods overtake conventional feature detector/descriptor techniques in terms of accuracy, recall, precision, and sensitivity rates. The field classifier model, VGGNet, achieves a 98.17% accuracy for this task, whilst YoloV3 achieves 91.6% accuracy with 0.12 IOU for object detection as the best method. The proposed framework also improves the overall performance of these algorithms by 1.27% for accuracy and 0.023 for IOU. By specifying the limits thoroughly and developing task-dependent approaches, we reveal the great potential of our framework plant detection and counting in the wild consisting of basic image preprocessing techniques, geometrical operations, and deep neural network.}
}
@article{COHEN202080,
title = {Technological advances relevant to transport – understanding what drives them},
journal = {Transportation Research Part A: Policy and Practice},
volume = {135},
pages = {80-95},
year = {2020},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0965856419301442},
author = {Tom Cohen and Peter Jones},
keywords = {Technology, Futures, Governance, MaaS, Drones, AVs},
abstract = {Transport policy makers are increasingly perplexed by the pace of change in their sector and by the increasing influence of external actors. This leads to a variety of responses, including “business as usual”, technological optimism, technological fatalism and technological ignorance. To explore this perplexity and its justification, we examine four areas of technological advance relevant to transport: mobility as a service; unmanned aerial vehicles (drones); automated vehicles; and telehealth. In each case, we identify the principal underlying shifts which are driving these technological advances, concluding that there is considerable overlap: three of the advances rely on ubiquitous sensing and on artificial intelligence and all four rely, to some degree, on connectedness. We then explore these three “drivers”, finding that progress is steadier than may be generally thought. We discuss the implications for our set of transport-related technological developments, concluding that policy makers could approach the future with greater confidence than is currently typical. They could also draw on the concepts of anticipatory governance to support their management of emerging technology and, at the same time, of the influence of external actors.}
}
@article{GUO2021107523,
title = {Two-level K-nearest neighbors approach for invasive plants detection and classification},
journal = {Applied Soft Computing},
volume = {108},
pages = {107523},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107523},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621004464},
author = {Yanhui Guo and Chunlai Du and Yun Zhao and Tih-Fen Ting and Thomas A. Rothfus},
keywords = {Deep learning, K-nearest neighbors, Image classification, Unmanned aerial system, Invasive plant detection},
abstract = {Invasive plants (IPs) are considered major threat to biodiversity and ecosystems due to their potential to compete for resources with other plants or crops. Timely and effective detection is important to prevent and control their growth. Recent advancements in unmanned aerial systems (UAS) have brought an effective, inexpensive, and non-invasive approach to monitoring plants. The use of UAS is promising in the accurate and rapid identification of IPs in the field. However, challenges exist when using UAS for IP detection as they generate a huge number of images that must be interpreted by researchers manually. This time-consuming and error-prone process significantly limits the applicability of UAS in a variety of environmental and agriculture monitoring applications. To reduce processing time and increase detection accuracy, we develop an automatic and intelligent detection approach to identify IPs with high levels of efficiency and accuracy from images of the monitoring areas captured by UAS. First, the original field images are segmented into patch images using the simple linear iterative clustering (SLIC) superpixels algorithm. Then a novel two-level K-nearest neighbors (TLKNN) algorithm is proposed to classify the patch images into different IP categories. In it, a deep convolutional neural network (CNN) is used to extract the features from the patch image and a kernel map function is utilized to obtain the kernel features. Two feature sets are fed to train two K-nearest neighbor (KNN) models. Finally, the final classification is determined by the selection results by KNN models. In the experiments, two public UAV image sets are employed to train and verify the proposed method. Several metrics such as accuracy, recall, precision, and F1-score, are used to evaluate the performance. The proposed TLKNN is compared with two newly proposed convolutional neural network (CNN) methods on the same datasets. Experimental results indicate the proposed TLKNN algorithm achieves better classification performance and reaches the highest accuracy, recall rate, precision rate, and F1-score. The improved performance not only means that IPs can be detected more accurately and automatically, but also that there are environmental benefits in developing an intelligent strategy for IP prevention and control.}
}
@article{SHENG201563,
title = {MEMS-based low-cost strap-down AHRS research},
journal = {Measurement},
volume = {59},
pages = {63-72},
year = {2015},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2014.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S0263224114004217},
author = {Hanlin Sheng and Tianhong Zhang},
keywords = {AHRS, Direction cosine matrix, Inertial navigation, Data fusion, MEMS sensors, BP neural network},
abstract = {With the rapid development of modern Micro-Electro-Mechanical Systems (MEMS) technology, the requirement of low cost strapdown AHRS based on MEMS sensor is becoming feasible. This paper presents the results of the attitude determination using low-cost MEMS-based sensor. Aiming at the accumulative deviation incurring by low-cost MEMS gyroscope drifting and integration deviation, this study proposes a multi-sensor data fusion algorithm that uses benchmark reference vectors to detect and correct the gyro drift. The proposed algorithm adopts the method of attitude evaluation based on Direction Cosine Matrix (DCM). A BP neural network PID controller is used to compute gyro drift for real-time correction. Numerical simulation shows that the algorithm meets the requirements of the system. In order to test the actual performance of the system, a hardware platform of strap-down AHRS is designed, based on a three-axis gyroscope, a three-axis accelerometer, three-axis magnetic sensors and a Digital Signal Processor (DSP). Finally, the three-axis flight turntable table test results obtained, show that the static accuracy of the attitude angle is better than 0.5° and the dynamic accuracy is better than 3°. Moreover, the AHRS is tested on a micro-Unmanned Aerial Vehicle (UAV), and the practical application feasibility is validated.}
}
@article{MCCUNE20132537,
title = {Swarm Control of UAVs for Cooperative Hunting with DDDAS},
journal = {Procedia Computer Science},
volume = {18},
pages = {2537-2544},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.436},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913005796},
author = {R. Ryan McCune and Gregory R. Madey},
keywords = {DDDAS, Swarm Intelligence, Agent-Based Simulation},
abstract = {Swarm control is a problem of increasing importance with technological advancements. Recently, governments have begun employing UAVs for reconnaissance, including swarms of drones searching for evasive targets. An agent-based simulation for dynamic cooperative cleaning is augmented with additional behaviors and implemented into a Dynamic Data-Driven Applica- tion System (DDDAS) framework for dynamic swarm control.}
}
@article{TOPALOV201094,
title = {Neuro-adaptive Approach for Controlling a Quad-rotor Helicopter Using Sliding Mode Learning Algorithm},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {10},
pages = {94-99},
year = {2010},
note = {10th IFAC Workshop on the Adaptation and Learning in Control and Signal Processing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100826-3-TR-4015.00020},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015323466},
author = {Andon V. Topalov and Nikola G. Shakev and Okyay Kaynak and Erdal Kayacan},
keywords = {unmanned aerial vehicles, helicopter control, neural networks, variable structure systems, adaptive systems},
abstract = {The interest into the autonomous aerial vehicles has largely increased recently. With the advancement of the technology in the area it has become possible to test efficiently and cost-effectively different autonomous flight control concepts and design variations using small-scale aircrafts. The paper presents a new neuro-adaptive approach for controlling the altitude and yaw angle of a miniature rotorcraft having four rotors. The adaptive structures in the proposed control schemes are comprised of radial basis functions neural networks using a new stable on-line learning algorithm. The latter is based on the variable structure systems theory and establishes a sliding motion in term of the network parameters, leading the learning error toward zero. The pitch and roll movements of the rotorcraft are controlled by an algorithm that employs the principles of the well known nested saturation control strategy. The results obtained from flight simulations with an accurate dynamic model of the DraganFlyer V Ti miniature quad-rotor helicopter demonstrate that the proposed neuro-adaptive control approach can deal successfully with the existing uncertainties and variations in the model parameters and/or changes in the environmental conditions.}
}
@article{HUA2021264,
title = {Adaptive neural network finite-time tracking quantized control for uncertain nonlinear systems with full-state constraints and applications to QUAVs},
journal = {Neurocomputing},
volume = {440},
pages = {264-274},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.12.078},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220319901},
author = {Changchun Hua and Anqi Jiang and Kuo Li},
keywords = {Adaptive neural network finite-time control, Full state constraints, Input quantization, Uncertain nonlinear systems, Quadrotor UAVs},
abstract = {A novel adaptive neural network finite-time tracking control strategy is developed for a class of uncertain nonlinear systems with asymmetric time-varying full-state constraints and input quantization. With the help of an introduced TVABLF, all state variables are confined to predefined regions. Based on the backstepping method and NNs approximation technique, a smooth tracking controller and its adaptive laws are co-designed for the uncertain systems. The system effects caused by the input quantization is compensated by the proposed algorithm with nonlinear decomposition. By adopting the SGPFS theory, the tracking performances are ensured to be achieved in finite time. It is rigorously proved that the output of the system follows the specified trajectory in finite time, whilst the system state variables are constrained within asymmetric boundaries. QUAVs are typical nonlinear systems of 6-Dof, and the control input signal requires quantization. We apply the developed method to the controller design of uncertain QUAVs, and the simulation results verify the effectiveness of the main results.}
}
@article{DOSSANTOSFERREIRA2017314,
title = {Weed detection in soybean crops using ConvNets},
journal = {Computers and Electronics in Agriculture},
volume = {143},
pages = {314-324},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917301977},
author = {Alessandro {dos Santos Ferreira} and Daniel {Matte Freitas} and Gercina {Gonçalves da Silva} and Hemerson Pistori and Marcelo {Theophilo Folhes}},
keywords = {Deep Learning, Weed detection, Computer vision},
abstract = {Weeds are undesirable plants that grow in agricultural crops, such as soybean crops, competing for elements such as sunlight and water, causing losses to crop yields. The objective of this work was to use Convolutional Neural Networks (ConvNets or CNNs) to perform weed detection in soybean crop images and classify these weeds among grass and broadleaf, aiming to apply the specific herbicide to weed detected. For this purpose, a soybean plantation was carried out in Campo Grande, Mato Grosso do Sul, Brazil, and the Phantom DJI 3 Professional drone was used to capture a large number of crop images. With these photographs, an image database was created containing over fifteen thousand images of the soil, soybean, broadleaf and grass weeds. The Convolutional Neural Networks used in this work represent a Deep Learning architecture that has achieved remarkable success in image recognition. For the training of Neural Network the CaffeNet architecture was used. Available in Caffe software, it consists of a replication of the well known AlexNet, network which won the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012). A software was also developed, Pynovisão, which through the use of the superpixel segmentation algorithm SLIC, was used to build a robust image dataset and classify images using the model trained by Caffe software. In order to compare the results of ConvNets, Support Vector Machines, AdaBoost and Random Forests were used in conjunction with a collection of shape, color and texture feature extraction techniques. As a result, this work achieved above 98% accuracy using ConvNets in the detection of broadleaf and grass weeds in relation to soil and soybean, with an accuracy average between all images above 99%.}
}
@article{WANG2018298,
title = {Low-Altitude Remote Sensing Based on Convolutional Neural Network for Weed Classification in Ecological Irrigation Area},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {17},
pages = {298-303},
year = {2018},
note = {6th IFAC Conference on Bio-Robotics BIOROBOTICS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.180},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318312989},
author = {Shubo Wang and Hongtao Liu and Yu Han and Jian Chen and Yue Pan and Yi Cao and Hao Meng and Yongjun Zheng},
keywords = {ecological irrigation area, UAV, weed classification, convolutional neural network},
abstract = {With the development of ecological irrigation area at present, it requires higher detection and control of weeds in irrigation area. In this paper, aiming at the ecological irrigation area, a classification method of weeds based on convolutional neural network(CNN) is proposed. By collecting 3 kinds of weeds and 3 kinds of crops as data sets UAV-based, through cutting, gray scale and so on, data is transported to the CNN. Finally, 6 categories of classifications are implemented. The classification results show that the recognition rate of weeds can reach 95.6%. In order to prevent and control specific weeds, a method of detecting single weeds density is also presented in this paper. The accurate monitoring of weeds in irrigation area can be realized through the method proposed in this paper, and there is basis for precise weed control in later stage.}
}
@article{BAI2021106456,
title = {Optimal window size selection for spectral information extraction of sampling points from UAV multispectral images for soil moisture content inversion},
journal = {Computers and Electronics in Agriculture},
volume = {190},
pages = {106456},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106456},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921004737},
author = {Xuqian Bai and Yinwen Chen and Junying Chen and Wenxuan Cui and Xiang Tai and Zhitao Zhang and Jiguang Cui and Jifeng Ning},
keywords = {Soil moisture content, Window size, UAV remote sensing, ANOVA, Local variance},
abstract = {Soil moisture content monitoring with UAV remote sensing always involves the selection of an appropriate window size for spectral information extraction, but research on the effect of window size on the accuracy of soil moisture content monitoring models and the selection of the optimal window size has been rarely reported. To solve these problems, an experiment was conducted on three typical bare plots in Shahaoqu Experimental Station at Hetao Irrigation District, Inner Mongolia, China. First, remote sensing images were obtained from the three bare plots from April 15 through 17, 2019, with a six-rotor UAV equipped with a six-channel multispectral camera. Synchronously, the moisture content at 0–10 cm of the surface soil was measured using the drying method. Then, the spectral information was extracted through windows of 16 different sizes (ranging from 1 * 1 to 31 * 31). Followed was the construction of thirty spectral indices using the ratio and normalized ratio methods, and the processing of the constructed indices using principal component analysis. The principal components accounting for 95% of the cumulative contribution rate were selected as the input variables for the construction of the monitoring models based on BP neural network. Finally, the model accuracy was tested using ANOVA, and the local variogram of the spectrum was used to explore the optimal window size selection. The results demonstrated: (1) There are differences in the spectral information extracted from different sizes of windows, which affects the accuracy of soil moisture monitoring model; (2) The spatial autocorrelation threshold of the plots at the local variogram was 13 * 13, resembling the window size with the highest accuracy, so it is feasible to select the optimal window size with the local variogram; (3) As the window size of spectral information increased, R2 first increased and then decreased, reaching the maximum value of 0.261 at the size of 13 * 13, and RMSE first decreased and then increased, reaching the minimum value of 0.017 when at the size of 7 * 7. These results can provide some reference for window size selection in spectral information extraction to monitor soil moisture content.}
}
@article{XIONG2021106550,
title = {Bio-inspired, intelligent flexible sensing skin for multifunctional flying perception},
journal = {Nano Energy},
volume = {90},
pages = {106550},
year = {2021},
issn = {2211-2855},
doi = {https://doi.org/10.1016/j.nanoen.2021.106550},
url = {https://www.sciencedirect.com/science/article/pii/S2211285521008028},
author = {Wennan Xiong and Chen Zhu and Dongliang Guo and Chao Hou and Zhaoxi Yang and Zhangyu Xu and Lei Qiu and Hua Yang and Kan Li and YongAn Huang},
keywords = {Electronic skin, Flying perception, Multifunctional sensor array, Airflow sensing, Structural health monitoring},
abstract = {Developing multifunctional flying perceptibility, by mimicking the biological system, is meaningful for aircrafts to adapt to the complicated and ambiguous flow environment. In this work, we show that the comprehensive capabilities (skin, synapse, immune system, and brain) of flying creatures can serve as bio-inspirations to develop an intelligent flexible sensing (iFlexSense) skin with similar components, i.e., skin-like mechanosensing, neuron-like data transmission, immune system-like impact monitoring and brain-like artificial intelligence. The iFlexSence skin brings substantial improvements to the aircrafts by the multifunctionalities including airflow perception, state awareness and self-diagnosis of complex surfaces, validated by the wind tunnel tests mounted on a NACA 0012 airfoil, in which it accurately measures the surface pressure, temperature, wall shear stress and flutter, locates the sudden impacts and correctly predicts the occurrence of separation and stall. It shows great potential for future wind tunnel tests and to extend the capabilities of unmanned air vehicles and underwater vehicles.}
}
@article{XING2020105343,
title = {Comparison of different models for evaluating vehicle collision risks at upstream diverging area of toll plaza},
journal = {Accident Analysis & Prevention},
volume = {135},
pages = {105343},
year = {2020},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2019.105343},
url = {https://www.sciencedirect.com/science/article/pii/S0001457519307584},
author = {Lu Xing and Jie He and Ye Li and Yina Wu and Jinghui Yuan and Xin Gu},
keywords = {Collision risk, Trajectory data, Toll plaza, Diverging area, Logistic regression, Non-Parametric model},
abstract = {Toll plazas with both Electronic Toll Collection (ETC) lane(s) and Manual Toll Collection (MTC) lane(s) could increase crash risks especially at upstream diverging areas because of frequency lane-change behaviors. This study develops the logistic regression (LR) model and five typical non-parametric models including, K-Nearest Neighbor (KNN), Artificial Neural Networks (ANN), Support Vector Machines (SVM), Decision Trees (DT), and Random Forest (RF) to examine the relationship between influencing factors and vehicle collision risk. Based on the vehicle trajectory data extracted from unmanned aerial vehicle (UAV) videos using an automated video analysis system, the unconstrained vehicle motion’s collision risk can be evaluated by the extended time to collision (ETTC). Results of model performance comparison indicate that not all non-parametric models have a better prediction performance than the LR model. Specifically, the KNN, SVM, DT and RF models have better model performance than LR model in model training, while the ANN model has the worst model performance. In model prediction, the accuracy of LR model is higher than that of other five non-parametric models under various ETTC thresholds conditions. The LR model implies a pretty good performance and its results also indicate that vehicle yields the higher collision risk when it drives on the left side of toll plaza diverging area and more dangerous situations could be found for an ETC vehicle. Moreover, the vehicle collision risks are positively associated with the speed of the following vehicle and the angle between the leading vehicle speed vector and X axis. Furthermore, the results of DT model show that three factors play important roles in classifying vehicle collision risk and the effects of them on collision risk are consistent with the results of LR model. These findings provide valuable information for accurate assessment of collision risk, which is a key step toward improving safety performance of the toll plaza diverging area.}
}
@article{NOUSI2020115969,
title = {Re-identification framework for long term visual object tracking based on object detection and classification},
journal = {Signal Processing: Image Communication},
volume = {88},
pages = {115969},
year = {2020},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2020.115969},
url = {https://www.sciencedirect.com/science/article/pii/S0923596520301429},
author = {Paraskevi Nousi and Danai Triantafyllidou and Anastasios Tefas and Ioannis Pitas},
keywords = {Visual object tracking, Long-term tracking, Re-detection, Deep learning},
abstract = {In this paper, we address the problem of long-term visual object tracking and we present an efficient real-time single object tracking system suitable for integration in autonomous platforms that need to encompass intelligent capabilities. We propose a novel long-term tracking framework for classification based re-detection and tracking, that incorporates state estimation, object re-identification and automated management of tracking and detection results. Our method integrates a novel object re-identification technique which efficiently filters a number of detection candidates and systematically corrects the tracking results. Through extensive experimental validation on the UAV123, UAV20L and TLP datasets, we demonstrate the effectiveness of the proposed system and its advantage over several state-of-the art trackers. The results furthermore highlight the proposed tracker’s ability to handle challenges arising from real-world and long-term scenarios, such as variations in pose, scale, occlusions and out-of-view situations. Furthermore, we propose a variant that is suitable for deployment on autonomous robots, such as Unmanned Aerial Vehicles.}
}
@article{KWON2022104076,
title = {Measurement of suspended sediment concentration in open channel flows based on hyperspectral imagery from UAVs},
journal = {Advances in Water Resources},
volume = {159},
pages = {104076},
year = {2022},
issn = {0309-1708},
doi = {https://doi.org/10.1016/j.advwatres.2021.104076},
url = {https://www.sciencedirect.com/science/article/pii/S0309170821002281},
author = {Siyoon Kwon and Jaehyun Shin and Il Won Seo and Hyoseob Noh and Sung Hyun Jung and Hojun You},
keywords = {Suspended sediment concentration, Hyperspectral imagery, UAVs,  measurement, Optical variability, Machine learning-based regression model},
abstract = {Since conventional in situ measurements of suspended sediments in the river system are labor-intensive and time-consuming, remote sensing approaches using multi- or hyper-spectral cameras have widely been applied to obtain high-resolution suspended sediment concentration (SSC) distributions in rivers and streams. However, in nature, the properties of heterogeneous sediment, such as the mineral content and particle size distribution, induce a strong variability in the optical images of the suspended sediments. For this reason, the robust estimation of the suspended sediment using the remote sensing technique is challenging due to the optical variability of the suspended sediment. Thus, it is necessary to deal with this variability of the optical images to improve the accuracy of remote sensing-based SSC measurements and extend them to the global estimator. In this study, a robust Machine Learning (ML) model for SSC estimation based on hyperspectral images was developed by considering the optical variability of the suspended sediment in water bodies. A series of field-scale tracer experiments were conducted in open channels with three different sediment types in order to obtain both the SSC using laser diffraction sensors and hyperspectral images using a UAV camera. The experimental results showed that the optical characteristics of SSC were critically heterogeneous due to the properties of the sediment. Using these experimental dataset, four explicit regression models and two implicit ML regression models were developed and compared to select an optimal estimator. Consequently, a Support Vector Regression (SVR) model using relevant spectral bands in a wide wavelength range yielded the most accurate results, with an R2 of 0.90 for the whole dataset. However, linear regression models, which could not consider various spectral bands and the nonlinear effect of the optical variability of SSC, were limited in their ability to retrieve SSC from hyperspectral images. Furthermore, the SVR model accurately reproduced the spatio-temporal SSC distributions in all study cases, including low-visibility suspended sediments, thus successfully resolved the optical variability of SSC with widely selected spectral bands from recursive feature elimination (RFE). The SVR model also successfully retrieved the SSC distribution in uncalibrated rivers. The results of this study demonstrated that the proposed ML regression models based on the hyperspectral imagery achieved a significant improvement in SSC estimation in terms of accuracy and global applicability.}
}
@article{HIBA20197,
title = {The applicability of on-line contextual calibration to a neural network based monocular collision avoidance system on a UAV⁎⁎Research is supported by 2018-1.2.1-NKP-00008: Exploring the Mathematical Foundations of Artificial Intelligence and by MTA Premium Postdoctoral Grant 2018. This paper was also supported by the Janos Bolyai Research Scholarship of the Hungarian Academy of Sciences and the Higher Education Institutional Excellence Program.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {11},
pages = {7-12},
year = {2019},
note = {5th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319307396},
author = {Antal Hiba and Rita Aleksziev and Koppány Pázmán and Péter Bauer and András Benczúr and Ákos Zarándy and Bálint Daróczy},
keywords = {Learning, adaptation, evaluation, Aerospace, Robotics, autonomous systems},
abstract = {Contextual calibration for object detection is a technique where a pretrained network collects attractive false positives during a calibration phase and use this calibration data for further training. This paper investigates the applicability of this method to a vision based onboard sense and avoid system, which requires intruder aircraft detection in camera images. Various landscape and sky backgrounds were generated by Unreal4 3D engine for calibration tests. Contextual calibration is a promising candidate for handling extreme situations which are not covered well in the training data.}
}
@article{HAN201610202,
title = {Performance prediction and analysis of a PEM fuel cell operating on pure oxygen using data-driven models: A comparison of artificial neural network and support vector machine},
journal = {International Journal of Hydrogen Energy},
volume = {41},
number = {24},
pages = {10202-10211},
year = {2016},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2016.04.247},
url = {https://www.sciencedirect.com/science/article/pii/S036031991530389X},
author = {In-Su Han and Chang-Bock Chung},
keywords = {Polymer electrolyte membrane (PEM) fuel cell, Pure oxygen, Data-driven model, Artificial neural network (ANN), Support vector machine (SVM)},
abstract = {Two data-driven models are presented to predict and analyze the performance of a PEM fuel cell operating on pure oxygen that can be used as an effective power source for air-independent operation of underwater vehicles, spacecrafts, unmanned aerial vehicles, etc. Both artificial neural network (ANN) and support vector machine (SVM) were employed as modeling methods to construct the nonlinear empirical models for a 1.2-kW PEM fuel-cell stack operating on high-pressure pure hydrogen and oxygen in dead-end mode. A sufficient amount of data was collected from a full factorial design of test operations of the fuel-cell stack. The hyper-parameters of the ANN and SVM models were determined using a 10-fold cross-validation scheme. The ANN model was found to show excellent performance and outperform the SVM model in predicting the polarization curves of the stack under various operating conditions, with the root mean square errors of 1.8–2.9 mV and the mean absolute percentage errors of 0.16–0.27%. Consequently, the ANN model was used to analyze and discuss in detail the effects of the major operating variables, including the stack temperature, the reactant inlet pressures, and the relative humidity of the supplied oxygen, on the performance of the stack.}
}
@article{BOUHLEL2021318,
title = {Suspicious Person Retrieval from UAV-sensors based on part level deep features},
journal = {Procedia Computer Science},
volume = {192},
pages = {318-327},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015209},
author = {Fatma Bouhlel and Hazar Mliki and Mohamed Hammami},
keywords = {Video Surveillance, UAV, Scene stabilization, Person detection, Person retrieval, Deep learning, Part-Level Deep Features.},
abstract = {Intelligent video surveillance systems represent a potent tool for preserving human security in public places. Indeed, these surveillance systems are requested in several real-life scenarios, in order to assist security guards by alerting them in abnormal situations and helping them to retrieve a suspicious person. Especially, intelligent video surveillance systems based on UAV-sensors have the asset of monitoring large as well as difficult access spaces. In this scope, we introduce a new approach for suspicious person retrieval from UAV-sensors. The proposed approach implies two complementary phases which are an offline phase and an inference phase. Within these phases, a scene stabilization step is carried out. The offline phase allows building the non-person/person model as well as the person retrieval model. Nonetheless, the inference phase enables to detect persons and retrieve suspicious ones using the already generated models. The main contribution of the proposed approach is the use of part-level deep features in order to retrieve persons. The experimental results validate the contributions of our approach compared to the state-of-the-art approaches.}
}
@article{SUBRAMANIAN2021103132,
title = {A deep genetic algorithm for human activity recognition leveraging fog computing frameworks},
journal = {Journal of Visual Communication and Image Representation},
volume = {77},
pages = {103132},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103132},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321000857},
author = {R. Raja Subramanian and V. Vasudevan},
keywords = {Deep genetic algorithm, Human activity recognition, Fog computing, Ambulatory healthcare},
abstract = {With modern e-healthcare developments, ambulatory healthcare has become a prominent requirement for physical or mental ailed, elderly, childhood people. One of the major challenges in such applications is timing and precision. A potential solution to this problem is the fog-assisted cloud computing architecture. The activity recognition task is performed with the hybrid advantages of deep learning and genetic algorithms. The video frames captured from vision cameras are subjected to the genetic change detection algorithm, which detects changes in activities of subsequent frames. Consequently, the deep learning algorithm recognizes the activity of the changed frame. This hybrid algorithm is run on top of fog-assisted cloud framework, fogbus and the performance measures including latency, execution time, arbitration time and jitter are observed. Empirical evaluations of the proposed model against three activity data sets shows that the proposed deep genetic algorithm exhibits higher accuracy in inferring human activities as compared to the state-of-the-art algorithms.}
}
@article{ARUN202220,
title = {Multimodal Earth observation data fusion: Graph-based approach in shared latent space},
journal = {Information Fusion},
volume = {78},
pages = {20-39},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001779},
author = {P.V. Arun and R. Sadeh and A. Avneri and Y. Tubul and C. Camino and K.M. Buddhiraju and A. Porwal and R.N. Lati and P.J. Zarco-Tejada and Z. Peleg and I. Herrmann},
keywords = {Convolutional neural networks, Fusion, Ground measured spectra, Multispectral UAV, Hyperspectral},
abstract = {Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.}
}