@article{MOGHIMI2020105299,
title = {Aerial hyperspectral imagery and deep neural networks for high-throughput yield phenotyping in wheat},
journal = {Computers and Electronics in Agriculture},
volume = {172},
pages = {105299},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105299},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919320721},
author = {Ali Moghimi and Ce Yang and James A. Anderson},
keywords = {Deep learning, Endmember, Hyperspectral imaging, Neural network, Phenotyping, UAV, Unmixing, Yield},
abstract = {Crop production needs to increase in a sustainable manner to meet the growing global demand for food. To identify crop varieties with high yield potential, plant scientists and breeders evaluate the performance of hundreds of lines in multiple locations over several years. To facilitate the process of selecting advanced varieties, an automated framework was developed in this study. A hyperspectral camera was mounted on an unmanned aerial vehicle to collect aerial imagery with high spatial and spectral resolution in a fast, cost-effective manner. Aerial images were captured in two consecutive growing seasons from three experimental yield fields composed of hundreds experimental wheat lines. The grain of more than thousand wheat plots was harvested by a combine, weighed, and recorded as the ground truth data. To investigate the yield variation at sub-plot scale and leverage the high spatial resolution, plots were divided into sub-plots using image processing techniques integrated by domain knowledge. Subsequent to extracting features from each sub-plot, deep neural networks were trained for yield estimation. The coefficient of determination for predicting the yield was 0.79 and 0.41 with normalized root mean square error of 0.24 and 0.14 g at sub-plot and plot scale, respectively. The results revealed that the proposed framework, as a valuable decision support tool, can facilitate the process of high-throughput yield phenotyping by offering the possibility of remote visual inspection of the plots as well as optimizing plot size to investigate more lines in a dedicated field each year.}
}
@article{KONG2020105442,
title = {Automatic identification and characterization of discontinuities in rock masses from 3D point clouds},
journal = {Engineering Geology},
volume = {265},
pages = {105442},
year = {2020},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2019.105442},
url = {https://www.sciencedirect.com/science/article/pii/S0013795219305848},
author = {Deheng Kong and Faquan Wu and Charalampos Saroglou},
keywords = {3D point cloud, Rock mass, Automatic, Discontinuity, LiDAR, UAV},
abstract = {The routine application of remote surveying techniques which can quickly acquire 3D digital data with high resolution, in particular digital photogrammetry, light detection and ranging (LiDAR) and unmanned aerial vehicle (UAV) for rock mass characterization has rapidly grown over the past decade. In this paper, a new method for automatic identification and interpretation of rock mass discontinuities, clustering of discontinuity sets and characterization of discontinuity orientation, persistence and spacing using 3D point clouds, is presented. The proposed method is based on a four-stage procedure consisting of: (1) normal vector calculation using the iterative reweighted plane fitting (IRPF) method, (2) discontinuity sets clustering by fast search and find of density peaks (CFSFDP) algorithm, and Fisher’s K value iterative calculation to eliminate noise points, (3) discontinuity segmentation using density-ratio based method, and discontinuity plane fitting using the random sample consensus (RANSAC) algorithm, (4) persistence and spacing calculation using the theory of analytic geometry. The method is applied to two case studies (i.e. rock slopes) and compared with the results from previous studies and from manual survey. It is concluded that the proposed method is reliable and yields a great accuracy for automatic identification of discontinuities in rock masses.}
}
@article{AHMED2019109253,
title = {Solving visual pollution with deep learning: A new nexus in environmental management},
journal = {Journal of Environmental Management},
volume = {248},
pages = {109253},
year = {2019},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2019.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0301479719309557},
author = {Nahian Ahmed and M. Nazmul Islam and Ahmad Saraf Tuba and M.R.C. Mahdy and Mohammad Sujauddin},
keywords = {Visual pollution, Deep learning, Convolutional neural network, Image recognition, Pollutant classification, Environmental management},
abstract = {Visual pollution is a relatively new concern amidst the existing plethora of mainstream environmental pollution, recommending the necessity for research to conceptualize, formalize, quantify and assess it from different dimensions. The purpose of this study is to create a new field of automated visual pollutant classification, harnessing the technological prowess of the 21st century for applications in environmental management. From the wide range of visual pollutants, four categories have been considered viz. (i) billboards and signage, (ii) telephone and communication wires, (iii) network and communication towers and (iv) street litter. The deep learning model used in this study simulates the human learning experience in the context of image recognition for visual pollutant classification by training and testing a convolutional neural network with several layers of artificial neurons. Data augmentation using image processing techniques and a train-test split ratio of 80:20 have been used. Training accuracy of 95% and validation accuracy of 85% have been achieved by the deep learning model. The results indicate that the upper limit of accuracy i.e. the asymptote, depends on the dataset size for this type of task. This study has several applications in environmental management. For example, the deployment of the trained model for processing of video/live footage from smartphone applications, closed-circuit television and drones/unmanned aerial vehicles can be applied for both the removal and management of visual pollutants in the natural and built environment. Furthermore, generating the ‘visual pollution score/index’ of urban regions such as towns and cities will create a new ‘metric/indicator’ in the field of urban environmental management.}
}
@article{CHEN2021,
title = {Event-triggered prescribed settling time consensus control of uncertain nonlinear multiagent systems with given transient performance},
journal = {ISA Transactions},
year = {2021},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821006364},
author = {Zicong Chen and Jianhui Wang and Li Zhang and Kemao Ma and Yanhui Liu},
keywords = {Consensus control, Given transient performance, Prescribed settling time, Event-triggered control},
abstract = {Multiagent systems (MASs) are usually used in unmanned aerial vehicle formations, multi-manipulator coordinated, traffic vehicle control and other fields, which have attracted a lot of attention from scholars. In this research, with the help of the designed performance function, the nonlinear transformation of synchronization error is realized. And the synchronization error of MASs with given transient performance could converge to the predefined interval. According to the designed transformation function, a prescribed setting time consensus control is investigated with the advantages of Radial Basis Function Neural Networks (RBFNNs) in dealing with unknown functions. It guarantees that the MASs under consideration are uniformly bounded convergent. Furthermore, event-triggered mechanism is applied to relieve pressure of MASs’ communication resources. Simulation results demonstrate its effectiveness.}
}
@article{FU2022126405,
title = {Combining UAV multispectral imagery and ecological factors to estimate leaf nitrogen and grain protein content of wheat},
journal = {European Journal of Agronomy},
volume = {132},
pages = {126405},
year = {2022},
issn = {1161-0301},
doi = {https://doi.org/10.1016/j.eja.2021.126405},
url = {https://www.sciencedirect.com/science/article/pii/S1161030121001763},
author = {Zhaopeng Fu and Shanshan Yu and Jiayi Zhang and Hui Xi and Yang Gao and Ruhua Lu and Hengbiao Zheng and Yan Zhu and Weixing Cao and Xiaojun Liu},
keywords = {Wheat, UAV multispectral imagery, Leaf nitrogen content, Grain protein content, Texture, Ecological factors, Artificial Neural Network},
abstract = {Nitrogen is an essential element of wheat growth and grain quality. Leaf nitrogen content (LNC), a critical monitoring indicator of crop nitrogen status, plays a reference role for later estimations of grain protein content (GPC). Developments in unmanned aerial vehicle (UAV) platforms and multispectral sensors have provided new approaches for LNC monitoring and GPC estimation, with great convenience for assessing the nutritional status of plants and grains without traditional destructive sampling. The objective of this study was to evaluate the feasibility of wheat LNC monitoring and GPC estimation based on UAV multispectral imagery. Wheat experiments were carried out in Xinghua, Kunshan and Suining of Jiangsu Province during 2018−2019 and in Rugao of Jiangsu Province during 2020−2021 with different varieties and nitrogen application rates. Remote sensing images were obtained by a multi-rotor UAV carrying a multispectral camera. The destructive sampling method was used to collect LNC, GPC and other field data. Wheat LNC monitoring and GPC estimation models were established after selection of the optimal indicators. Different modelling methods were used for the comparative analysis, including unitary linear regression, multiple linear regression and artificial neural network (ANN) methods. Three techniques were adopted to improve the GPC prediction accuracy: (1) multiple factors were substituted for single factor for the prediction; (2) texture information was added through further imagery mining; and (3) ecological factors were considered to improve the prediction mechanism. The results showed that the use of UAV-based Airphen multispectral imagery had a good effect on wheat LNC monitoring and GPC estimation. The vegetation indices constructed by red-edge and near-infrared bands had good performances in LNC monitoring and GPC estimation. The addition of texture information and ecological factors further improved the modelling accuracy. In this study, the optimal wheat GPC estimation model was established by NDVI (675, 730) at the jointing stage, NDVIT (730mea., 850) at the booting stage, NDVIT (730mea., 850) at the flowering stage and NDVI (730, 850) at the early filling stage. The modelling R2, validation R2 and relative root mean square error (RRMSE) reached 0.662, 0.7445 and 0.0635, respectively. The results provide a reference for crop LNC monitoring and GPC estimation based on UAV multispectral imagery.}
}
@article{TWEEDALE20141033,
title = {Enhancing the Dgree of Autonomy on a ‘Tier 1’ Unmanned Aerial Vehicle Using a Visual Landing Framework},
journal = {Procedia Computer Science},
volume = {35},
pages = {1033-1042},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011557},
author = {Jeffrey W. Tweedale and Dion Gonano},
keywords = {Agents, Artificial Intelligence, Automation, Autonomy, Autonomous Systems, Computational Intelligence, M2 M, Machine Intelligence, Machine Vision, Multi-Agent Systems, SIFT.},
abstract = {Humans continue to use tools to manually transform raw resources into valued outputs. The type of tool, amount of effort and form of energy required vary depending on the output; however they now enable industry to manufacture goods (with excellent quality and extremely high volume). Industry continues to invest heavily in machines so that people can operate productively. Similarly, researchers continue to pursue automation to increase the Degree of Autonomy (DOA) using Advanced Information Processing (AIP) techniques. Artificial Intelligence (AI), Computational Intelligence (CI) and Machine Intelligence (MI) now facilitate automation for numerous achievements. The proposed Visual Landing Framework (VLF) design uses a Multi-Agent System (MAS) to facilitate the development of components, that interoperate, via embedded business logic, to deliver the coordination and cooperation techniques required to automate a higher-level cognitive processing problem. As technology incorporates this ever increasing Level of Automation (LOA), humans remain in charge and are retained to make higher-order decisions. Unlike humans, heuristic and declarative logic systems suffers under these conditions and to need to adapt or make human-like decision to succeed. This paper discusses one possible avenue of enhancing the DOA on a ‘Tier 1’ Unmanned Aerial Vehicle (UAV) by reducing the need for the human to concentrate on a difficult cognitive task. The Machine to Machine (M2M) autonomy component uses an on-board camera, the Open Source Computer Vision Library (OpenCV) and Scale-Invariant Feature Transform (SIFT) algorithms to translate a fixed ground reference into positional commands. With this increased LOA the platform should be able to generate greater independence and enable more autonomous behaviour within unmanned control systems.}
}
@article{MANNO2021114315,
title = {Deep learning strategies for automatic fault diagnosis in photovoltaic systems by thermographic images},
journal = {Energy Conversion and Management},
volume = {241},
pages = {114315},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.114315},
url = {https://www.sciencedirect.com/science/article/pii/S019689042100491X},
author = {D. Manno and G. Cipriani and G. Ciulla and V. {Di Dio} and S. Guarino and V. {Lo Brano}},
keywords = {Automatic Fault recognition, Convolutional Neural Network, Photovoltaics, TensorFlow, Infrared Thermography},
abstract = {Losses of electricity production in photovoltaic systems are mainly caused by the presence of faults that affect the efficiency of the systems. The identification of any overheating in a photovoltaic module, through the thermographic non-destructive test, may be essential to maintain the correct functioning of the photovoltaic system quickly and cost-effectively, without interrupting its normal operation. This work proposes a system for the automatic classification of thermographic images using a convolutional neural network, developed via open-source libraries. To reduce image noise, various pre-processing strategies were evaluated, including normalization and homogenization of pixels, greyscaling, thresholding, discrete wavelet transform, and Sobel Feldman and box blur filtering. These techniques allow the classification of thermographic images of differen quality and acquired using different equipments, without specific protocols. Several tests with different parameters and overfitting reduction techniques were carried out to assess the performance of the neural networks: images acquired by unmanned aerial vehicles and ground-based operators were compared for the network performance and for the time required to execute the thermographic inspection. Our tool is based on a convolutional neural network that allows to immediately recognize a failure in a PV panel reaching a very high accuracy. Considering a dataset of 1000 images that refer to different acquisition protocols, it was reached an accuracy of 99% for a convolutional neural network with 30 min of computational time on Low Mid-Range CPU. While a dataset of 200 sectioned images, the same tool achieved 90% accuracy with a multi-layer perceptron architecture and 100% accuracy for a convolutional neural network. The proposed methodology offers an open alternative and a valid tool that improves the resolution of image classification for remote failure detection problems and that can be used in any scientific sector.}
}
@article{LI2022102686,
title = {Ultrahigh-resolution boreal forest canopy mapping: Combining UAV imagery and photogrammetric point clouds in a deep-learning-based approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {107},
pages = {102686},
year = {2022},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2022.102686},
url = {https://www.sciencedirect.com/science/article/pii/S0303243422000125},
author = {Linyuan Li and Xihan Mu and Francesco Chianucci and Jianbo Qi and Jingyi Jiang and Jiaxin Zhou and Ling Chen and Huaguo Huang and Guangjian Yan and Shouyang Liu},
keywords = {Canopy mapping, Crown cover, UAV imagery, SfM point cloud, Self-supervised deep learning, Image overlap},
abstract = {Accurate wall-to-wall estimation of forest crown cover is critical for a wide range of ecological studies. Notwithstanding the increasing use of UAVs in forest canopy mapping, the ultrahigh-resolution UAV imagery requires an appropriate procedure to separate the contribution of understorey from overstorey vegetation, which is complicated by the spectral similarity between the two forest components and the illumination environment. In this study, we investigated the integration of deep learning and the combined data of imagery and photogrammetric point clouds for boreal forest canopy mapping. The procedure enables the automatic creation of training sets of tree crown (overstorey) and background (understorey) data via the combination of UAV images and their associated photogrammetric point clouds and expands the applicability of deep learning models with self-supervision. Based on the UAV images with different overlap levels of 12 conifer forest plots that are categorized into “I”, “II” and “III” complexity levels according to illumination environment, we compared the self-supervised deep learning-predicted canopy maps from original images with manual delineation data and found an average intersection of union (IoU) larger than 0.9 for “complexity I” and “complexity II” plots and larger than 0.75 for “complexity III” plots. The proposed method was then compared with three classical image segmentation methods (i.e., maximum likelihood, Kmeans, and Otsu) in the plot-level crown cover estimation, showing outperformance in overstorey canopy extraction against other methods. The proposed method was also validated against wall-to-wall and pointwise crown cover estimates using UAV LiDAR and in situ digital cover photography (DCP) benchmarking methods. The results showed that the model-predicted crown cover was in line with the UAV LiDAR method (RMSE of 0.06) and deviate from the DCP method (RMSE of 0.18). We subsequently compared the new method and the commonly used UAV structure-from-motion (SfM) method at varying forward and lateral overlaps over all plots and a rugged terrain region, yielding results showing that the method-predicted crown cover was relatively insensitive to varying overlap (largest bias of less than 0.15), whereas the UAV SfM-estimated crown cover was seriously affected by overlap and decreased with decreasing overlap. In addition, canopy mapping over rugged terrain verified the merits of the new method, with no need for a detailed digital terrain model (DTM). The new method is recommended to be used in various image overlaps, illuminations, and terrains due to its robustness and high accuracy. This study offers opportunities to promote forest ecological applications (e.g., leaf area index estimation) and sustainable management (e.g., deforestation).}
}
@article{WHITE2022114386,
title = {Cost benefit analysis of survey methods for assessing intertidal sediment disturbance: A bait collection case study},
journal = {Journal of Environmental Management},
volume = {306},
pages = {114386},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114386},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721024488},
author = {Shannon M. White and Martin Schaefer and Peter Barfield and Ruth Cantrell and Gordon J. Watson},
keywords = {Drone, Benthic, Mud flats, Fishery, Aerial imagery, Remote sensing},
abstract = {Coastal management requires cost-effective, yet accurate, assessments of habitat condition, especially in areas protected by statutory conservation measures. Unmanned Aerial Vehicles (UAVs) provide alternatives to manned aircraft and walk-over (WO) surveys. To support coastal managers with method selection, we compare the costs and benefits of the three techniques using the extent of bait collection (sediment scarring from manual digging) on intertidal mudflats from three UK sites. UAV and WO surveys were conducted in parallel and aerial photography was downloaded from the Channel Coastal Observatory (CCO). Digging was digitised from estimations on foot (WO) or by manually labelling imagery with confidence assigned (UAV/CCO). Method efficacy is compared with respect to spatial coverage, control over survey time/location, spatial resolution, positioning accuracy, and area of digging detected. Personnel hours and up-front costs (e.g. training/equipment), costs for personnel time standardised by shore area, personnel risk, and environmental impact are also compared. Regarding efficacy, CCO imagery had extensive shore coverage compared to UAV and WO, however, assessments are restricted to times/locations with available imagery. Each method's resolution was sufficient to detect digging. WO achieved the highest resolution (on foot), but the lowest positioning accuracy, in contrast to accurate feature delineation on aerial imagery. An additive two-way ANOVA revealed a significantly higher percent area of ‘dug’ sediment (all confidence levels) recorded by UAV than WO. CCO was the most cost-effective with no fieldwork/equipment costs. UAV had the highest up-front costs, but WO was more costly for personnel hours/km2 for survey time and digitisation. For all methods, digitisation was the most time-consuming aspect. Compared to WO, UAV achieved rapid shore surveys and the CCO and UAV methods minimise personnel risks. UAV and WO both cause wildlife disturbance, with trampling an additional WO impact. With each method suited to sediment disturbance assessment, selection will depend on resources and objectives and will be aided by this holistic cost-benefit analysis. Cost-effectiveness will improve with evolving regulations that facilitate UAV use and technological developments (e.g. machine learning for disturbance detection) that could significantly expedite imagery analysis and enable broadscale assessments from CCO or satellite imagery.}
}
@article{YU202011830,
title = {Decentralized finite-time adaptive fault-tolerant synchronization tracking control for multiple UAVs with prescribed performance},
journal = {Journal of the Franklin Institute},
volume = {357},
number = {16},
pages = {11830-11862},
year = {2020},
note = {Finite-Time Stability Analysis and Synthesis of Complex Dynamic Systems},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2019.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S0016003219308609},
author = {Ziquan Yu and Youmin Zhang and Zhixiang Liu and Yaohong Qu and Chun-Yi Su and Bin Jiang},
abstract = {This paper is concerned with the decentralized finite-time fault-tolerant attitude synchronization tracking control problem for multiple unmanned aerial vehicles (multi-UAVs) with prescribed performance. Failure to counteract actuator faults in the formation flight of multi-UAVs in a limited time may lead to catastrophic consequences. By integrating the prescribed performance functions into the synchronization tracking errors, a new set of errors is defined. Based on the transformed errors, a finite-time attitude synchronization tracking control scheme is developed by using neural networks and finite-time differentiator techniques. The neural networks are utilized to identify the unknown nonlinear terms induced by uncertainties and actuator faults. To reduce the computational burden caused by estimating the weight vectors, the norms of weight vectors are used for the estimation, such that the number of adaptive parameters is significantly reduced and independent from the number of neurons. The finite-time differentiators are utilized to estimate the intermediate control signals and their derivatives. Moreover, auxiliary dynamic signals with explicit consideration of differentiator estimation errors are introduced into the control scheme to guarantee the finite-time convergences of the synchronized tracking errors. Furthermore, it is shown that by using the Lyapunov method, all UAVs can track their individual attitude references, while the synchronized tracking errors among UAVs are all bounded in finite time and confined within the prescribed performance bounds. Finally, comparative simulation studies on multi-UAVs are conducted to verify the effectiveness of the proposed scheme.}
}
@article{HU20191,
title = {Pixel size of aerial imagery constrains the applications of unmanned aerial vehicle in crop breeding},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {154},
pages = {1-9},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619301303},
author = {Pengcheng Hu and Wei Guo and Scott C. Chapman and Yan Guo and Bangyou Zheng},
keywords = {Plant phenotyping, Ground coverage, Remote sensing, Pixel size, UAV},
abstract = {Image analysis using proximal sensors can help accelerate the selection process in plant breeding and improve the breeding efficiency. However, the accuracies of extracted phenotypic traits, especially those that require image classification, are affected by the pixel size in images. Ground coverage (GC), the ratio of projected to ground vegetation area to total land area, is a simple and important trait to monitor crop growth and development and is often captured by visual-spectrum cameras on multiple platforms from ground-based vehicles to satellites. In this study, we used GC as an example trait and explored its dependency on pixel size. In developing new spring wheat varieties, breeders often aim for rapid GC estimation, which is challenging especially when coverage is low (<25%) in a species with thin leaves (ranging from 2 to 15 mm across). In a wheat trial comprising 28 treatments, high-resolution images were manually taken at ca. 1 m above canopies on seven occasions from emergence to flowering. Using a cubic interpolation algorithm, the original images with small pixel size were degraded into coarse images with large pixel size (from 0.1 to 5.0 cm per pixel, 26 extra levels in total) to mimic the image acquisition at different flight heights of an unmanned aerial vehicle (UAV) based platform. A machine learning based classification model was used to classify pixels of the original images and the corresponding degraded images into either vegetation and background classes, and then computed their GCs. GCs of original images were referred as reference values to their corresponding degraded images. As pixel size increased, GC of the degraded images tended to be underestimated when reference GC was less than about 50% and overestimated for GC > 50%. The greatest errors (about 30%) were observed when reference GCs were around 30% and 70%. Meanwhile, the largest pixel sizes to distinguish between two treatments depended on the difference between GCs of the two treatments and were rapidly increased when differences were greater than the specific values at given significance levels (i.e. about 10%, 8% and 6% for P < 0.01, 0.05 and 0.1, respectively). For wheat, small pixel size (e.g. <0.1 cm) is always required to accurately estimate ground coverage when the most practical flight height is about 20 to 30 m at present. This study provides a guideline to choose appropriate pixel sizes and flight plans to estimate GC and other traits in crop breeding using UAV based HTP platforms.}
}
@article{ZHANG2019105052,
title = {Bayesian calibration of AquaCrop model for winter wheat by assimilating UAV multi-spectral images},
journal = {Computers and Electronics in Agriculture},
volume = {167},
pages = {105052},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105052},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919313468},
author = {Tianxiang Zhang and Jinya Su and Cunjia Liu and Wen-Hua Chen},
keywords = {Unmanned Aerial Vehicle (UAV), Multispectral image, Machine learning, Model calibration, Bayesian inference},
abstract = {Crop growth model plays a paramount role in smart farming management, which not only provides quantitative information on crop development but also evaluates various management strategies. A reliable model is desirable but challenging due to the presence of unknown and uncertain parameters; therefore, crop model calibration is significant to achieve its potentials. This work is focused on the calibration of AquaCrop model by leveraging advanced Bayesian inference algorithms and UAV multi-spectral images at field scales. In particular, aerial images with high spatial-temporal resolutions are first applied to obtain Canopy Cover (CC) value by using machine learning based classification. The CC is then assimilated into AquaCrop model and uncertain parameters could be inferred by Markov Chain Monte Carlo (MCMC). Both simulation and experimental validation are performed. The experimental aerial images of winter wheat at Yangling district from Oct/2017 to June/2018 are applied to validate the proposed method against the conventional optimisation based approach by Simulated Annealing (SA). 100 Monte Carlo simulations show that the root mean squared error (RMSE) of Bayesian approach yields a smaller parameter estimation error than optimisation approach. While the experimental results show that: (i) a good wheat/background classification result is obtained for the accurate calculation of CC; (ii) the predicted CC values by Bayesian approach are consistent with measurements by 4-fold cross validation, where the RMSE is 0.0271 smaller than optimisation approach (0.0514); (iii) in addition to parameter estimation, their distribution information is also obtained in the developed Bayesian approach, reflecting the prediction confidence. It is believed that the Bayesian model calibration, although is developed for AquaCrop model, can find a wide range of applications to various simulation models in agriculture and forestry.}
}
@article{FILIPI2022150041,
title = {Honeybee-based biohybrid system for landmine detection},
journal = {Science of The Total Environment},
volume = {803},
pages = {150041},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150041},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721051160},
author = {Janja Filipi and Vladan Stojnić and Mario Muštra and Ross N. Gillanders and Vedran Jovanović and Slavica Gajić and Graham A. Turnbull and Zdenka Babić and Nikola Kezić and Vladimir Risojević},
keywords = {REST sampling, Organic semiconductors, Unmanned aerial vehicles, Convolutional neural networks, Humanitarian demining},
abstract = {Legacy landmines in post-conflict areas are a non-discriminatory lethal hazard and can still be triggered decades after the conflict has ended. Efforts to detect these explosive devices are expensive, time-consuming, and dangerous to humans and animals involved. While methods such as metal detectors and sniffer dogs have successfully been used in humanitarian demining, more tools are required for both site surveying and accurate mine detection. Honeybees have emerged in recent years as efficient bioaccumulation and biomonitoring animals. The system reported here uses two complementary landmine detection methods: passive sampling and active search. Passive sampling aims to confirm the presence of explosive materials in a mine-suspected area by the analysis of explosive material brought back to the colony on honeybee bodies returning from foraging trips. Analysis is performed by light-emitting chemical sensors detecting explosives thermally desorbed from a preconcentrator strip. The active search is intended to be able to pinpoint the place where individual landmines are most likely to be present. Used together, both methods are anticipated to be useful in an end-to-end process for area surveying, suspected hazardous area reduction, and post-clearing internal and external quality control in humanitarian demining.}
}
@article{SHAO2021106906,
title = {Mapping maize crop coefficient Kc using random forest algorithm based on leaf area index and UAV-based multispectral vegetation indices},
journal = {Agricultural Water Management},
volume = {252},
pages = {106906},
year = {2021},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2021.106906},
url = {https://www.sciencedirect.com/science/article/pii/S0378377421001712},
author = {Guomin Shao and Wenting Han and Huihui Zhang and Shouyang Liu and Yi Wang and Liyuan Zhang and Xin Cui},
keywords = {Crop water requirements, FAO56 approach, LAI, Evapotranspiration, Random forest regression, Remote sensing},
abstract = {Rapid and accurate acquisition of crop coefficient (Kc) values is essential for estimating field crop evapotranspiration (ET). The lack of rapid access to the high-resolution spatial and temporal distribution of Kc values hinders obtaining a crop Kc value for application in precision irrigation agriculture. This study aimed to explore the potential of leaf area index (LAI) and multispectral vegetation indices (VIs) obtained by an unmanned aerial vehicle (UAV) for estimating the Kc value for a maize crop on a field scale and to obtain a high-resolution spatial-temporal map of Kc values. Hence, the performance of the estimation model for daily maize Kc derived by two machine learning algorithms (random forest regression-RFR and multiple linear regression-MLR) based on the ground-based LAI and six types of UAV-based multispectral VIs (normalized difference vegetation index, NDVI; soil adjusted vegetation index, SAVI; enhanced vegetation index, EVI; transformed chlorophyll absorption in reflectance index, TCARI; green normalized vegetation index, GNDVI; and visual atmospheric resistance index, VARI), was evaluated under multiple irrigation conditions during the entire cropping cycle. Maize RFR with VIs-LAI-based ET was compared to soil water balance (SWB) and FAO-56-based ET. The results showed that the RFR algorithm effectively (R2 = 0.65) estimated maize Kc values based on ground-based LAI and UAV-based VIs. The UAV-based VIs based on Red-edge-Red and Green-Red spectral bands and ground-based LAI were suitable predictors in the Kc prediction model under different irrigation conditions. Further, we successfully obtained a high resolution (pixel size of centimeter) spatial distribution of maize Kc values based on EVI-based LAI and UAV-based VIs. Furthermore, the results indicated that the combination of UAV multispectral remote sensing technology and the RFR algorithm provides a potential solution for the distribution of water use and precision irrigation on a field scale.}
}
@article{YU201691,
title = {Development of methods to improve soybean yield estimation and predict plant maturity with an unmanned aerial vehicle based platform},
journal = {Remote Sensing of Environment},
volume = {187},
pages = {91-101},
year = {2016},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0034425716303777},
author = {Neil Yu and Liujun Li and Nathan Schmitz and Lei F. Tian and Jonathan A. Greenberg and Brian W. Diers},
keywords = {Soybean, Breeding efficiency, UAV, Multispectral image, Object classification},
abstract = {Advances in phenotyping technology are critical to ensure the genetic improvement of crops meet future global demands for food and fuel. Field-based phenotyping platforms are being evaluated for their ability to deliver the necessary throughput for large scale experiments and to provide an accurate depiction of trait performance in real-world environments. We developed a dual-camera high throughput phenotyping (HTP) platform on an unmanned aerial vehicle (UAV) and collected time course multispectral images for large scale soybean [Glycine max (L.) Merr.] breeding trials. We used a supervised machine learning model (Random Forest) to measure crop geometric features and obtained high correlations with final yield in breeding populations (r=0.82). The traditional yield estimation model was significantly improved by incorporating plot row length as covariate (p<0.01). We developed a binary prediction model from time-course multispectral HTP image data and achieved over 93% accuracy in classifying soybean maturity. This prediction model was validated in an independent breeding trial with a different plot type. These results show that multispectral data collected from the UAV-based HTP platform could improve yield estimation accuracy and maturity recording efficiency in a modern soybean breeding program.}
}
@article{ALVESRIBEIRO201711607,
title = {Multi-Objective Model Selection for Unmanned Aerial Vehicles Automatic Target Recognition Systems},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {11607-11612},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1652},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317322577},
author = {Victor Henrique {Alves Ribeiro} and Gilberto Reynoso-Meza and Leandro {dos Santos Coelho}},
keywords = {Classifiers, Artificial neural networks, Optimization, Multi-objective optimization, Genetic algorithms, Signal processing, Electromagnetic signals},
abstract = {Within unmanned aerial vehicles on-going research topics, automatic target recognition is acquiring relevance. This is due to the easiness with which it is possible to acquire such devices. In order to do this, signal processing and classification techniques could be adopted. Also, in order to improve the classification ratio, optimization techniques could be used. This subject is object of study for many researchers, but the authors worry about the lack of information while using single-objective optimization techniques. The proposed work comprises the application of multi-objective optimization design in order to create an automatic target recognition system to discriminate different types of unmanned aerial vehicles. In order to accomplish this, a K-band radar system is used to send and receive electromagnetic signals, which are then processed with feature extraction techniques, and finally, applied on an artificial neural network system. In order to improve the system’s classification ratio, the classifier is defined as a multi-objective problem, and evolutionary multi-objective optimization techniques are applied. Finally, in order to select the best possible trade-off, the level diagrams multi-criteria decision making methodology is used to compare different solutions.}
}
@article{FINN2022363,
title = {Unsupervised spectral-spatial processing of drone imagery for identification of pine seedlings},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {183},
pages = {363-388},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621003117},
author = {Anthony Finn and Pankaj Kumar and Stefan Peters and Jim O'Hehir},
keywords = {UAV, Seedling identification, Forest establishment, Object detection, Unsupervised learning},
abstract = {Reliable and accurate monitoring of replanted forest areas is a vital component of the plantation cycle if sustainable management practices are to be maintained following resource extraction. Unmanned aerial vehicles (UAVs) can rapidly survey newly planted forest areas using high resolution imagery. Automated image processing techniques can cost-effectively examine large quantities of data. Correctly combined, such technologies can efficiently create accurate maps of seedling locations, which offers the prospect of management practices for forestry and ecology operations that involve less fieldwork. This paper presents a technique based on unsupervised machine learning for detecting healthy young pinus caribaea (Caribbean pine) and pinus radiata (Monterey pine) seedlings. The locations of individual trees requiring replacement are also determined based on the spatial distribution of the identified healthy trees. The approach was tested on data from 30 sites covering over 700 ha, with seedlings ranging from 9 months to 3 years’ old (mean tree heights 30–2 m). Test sites contained a range of geomorphologies and levels of weed infestation. The results indicate detection precision and specificity in excess of 90% and recall approaching 99% for dense point cloud resolution of 4–6 cm. As network pre-training is unnecessary, there is no need for high resolution imagery, which allows greater operational coverage per unit time. The algorithm may also be used on multi- or hyperspectral data; and may be useful if employed in conjunction with other supervised learning techniques to reduce the need for manual labelling of training data sets.}
}
@article{CHEEMA2022108706,
title = {Blockchain-based secure delivery of medical supplies using drones},
journal = {Computer Networks},
volume = {204},
pages = {108706},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108706},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621005661},
author = {Muhammad Asaad Cheema and Rafay Iqbal Ansari and Nouman Ashraf and Syed Ali Hassan and Hassaan Khaliq Qureshi and Ali Kashif Bashir and Christos Politis},
keywords = {Unmanned aerial vehicles, Blockchain, Machine learning, Fifth generation (5G) and beyond},
abstract = {The advantages provided by the drones with regards to three dimensional mobility and ease of deployment makes them a viable candidate for 5G and beyond (B5G) networks. Significant amount of research has been conducted on the aspect of networking for using drones as base stations to provide different services. In this work, we deviate from the traditional use of drones to provide connectivity and explore the delivery of products through drones in the context of maintaining social distancing. However, drone delivery process for critical applications such as delivering medical supplies is vulnerable to attacks such as impersonation attacks. The security of drone operation is important to save the users from any breaches that can lead to financial and physical losses. To cope with these security issues and to make the delivery process transparent, we propose a blockchain-based drone delivery system that registers and authenticates the participating entities including products (medical supplies), warehouse (medical centers) and drones. To this end, we utilize Ethereum platform for implementation of blockchain and smart contract and we present an analysis of different factors that influence the authentication process in terms of time and the number of transactions. Furthermore, to make the communication of a drone with command and control center more secure and robust, we use machine learning (ML)-based intrusion detection system.}
}
@article{BHATNAGAR2021151,
title = {A nested drone-satellite approach to monitoring the ecological conditions of wetlands},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {174},
pages = {151-165},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000125},
author = {Saheba Bhatnagar and Laurence Gill and Shane Regan and Stephen Waldren and Bidisha Ghosh},
keywords = {Drone imagery augmentation, Colour correction, Machine learning, Sentinel-2, Wetland mapping},
abstract = {Monitoring wetlands is necessary in order to understand and protect their ecohydrological balance. In Ireland, traditionally wetland-monitoring is carried out by manual field visits which can be very time-consuming. To automate the process, this study extends the ability of remote sensing-based monitoring of wetlands by combining RGB image processing, machine learning algorithms, and satellite data analysis to create seasonal maps of vegetation communities within the wetlands. The methodology matches multispectral and broad coverage of open-source Sentinel-2 (S2) imagery with the high spatial granularity of Unmanned Aerial Vehicles (UAV) or drone images. Single sensor drone imagery was captured, colour corrected and classified using random forest (RF) classifier for a subset of the wetland. The classified imagery was upsampled to satellite imagery scale to create training data for vegetation-segmentation in the entire wetland. The process was repeated for multiple seasons, and an annual map was created utilising the majority voting. The proposed framework has been evaluated on various wetlands across Ireland, with results presented herein for an ombrotrophic peatland complex, Clara Bog. The accuracy of the maps was checked utilising a set of area-based performance metric. The application of this method thereby reduces the number of field surveys typically required to assess the long-term ecological change of such wetland habitats. The performance of the proposed method demonstrates that the technique is a robust, quick, and cost-effective way to map wetland habitats seasonally and to explore their ecohydrological synergies.}
}
@article{WANG2021102998,
title = {A systematic method to develop three dimensional geometry models of buildings for urban building energy modeling},
journal = {Sustainable Cities and Society},
volume = {71},
pages = {102998},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102998},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721002845},
author = {Chao Wang and Shen Wei and Sihong Du and Dian Zhuang and Yanxia Li and Xing Shi and Xing Jin and Xin Zhou},
keywords = {Urban building energy models, 3D models, Geometric data, Open-access data, GIS},
abstract = {Three dimensional (3D) geometric models of buildings are foundational for urban building energy modeling. A complete 3D geometric model contains building-relevant information like building footprint, height and Window-to-Wall Ratio (WWR). Existing methods creating these models have certain limitations, such as unavailability of Geographic Information System (GIS) databases and Light Detection and Ranging (LiDAR) data for many cities, and restricted flying space for Unmanned Aerial Vehicles (UAV). To tackle these issues, this study has developed a systematic method developing 3D geometric models, with 1) building footprint acquired from combination of two internet maps, namely Baidu Map and OpenStreetMap; 2) building height estimated from the number of storeys for residential buildings and determined using the building vertical edge method for non-residential buildings, and 3) building WWR calculated from buildings’ elevation images using an Artificial Intelligence (AI). The validation work revealed that more than 85 % of acquired building footprints had absolute relative errors less than 10 %, and this percentage was 87 %, 74 % and 75 %, for height of residential buildings, height of non-residential buildings and WWR, respectively. To demonstrate the application of the method, a newly developed urban district in Nanjing, China was used as a case study.}
}
@article{YU2021102363,
title = {A machine learning algorithm to detect pine wilt disease using UAV-based hyperspectral imagery and LiDAR data at the tree level},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {101},
pages = {102363},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102363},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421000702},
author = {Run Yu and Youqing Luo and Quan Zhou and Xudong Zhang and Dewei Wu and Lili Ren},
keywords = {Pine wilt disease, Airborne hyperspectral imagery, Airborne LiDAR, Random forest},
abstract = {Pine wilt disease (PWD) is a global destructive threat to forests, having caused extreme damage in China. Therefore, the establishment of an effective method to accurately monitor and map the infection stage by PWD is imperative. Unmanned aerial vehicle (UAV)-based hyperspectral imaging (HI) and light detection and ranging (LiDAR) technique is an effective approach for forest health monitoring. However, few previous studies have used airborne HI and LiDAR to detect PWD and compared the capability for predicting PWD infection stage at the tree level. In this paper, PWD infection was divided into five stages (green, early, middle, heavy, and grey), and HI and LiDAR data were integrated to detect PWD. We estimated the power of the hyperspectral method (HI data only), LiDAR (LiDAR data only), and their combination (HI plus LiDAR data) to predict the infection stages of PWD using the random forest (RF) algorithm. We obtained the following results: (1) The classification accuracies of HI (OA: 66.86%, Kappa: 0.57) were higher than those of LiDAR (OA: 45.56%, Kappa: 0.27) for predicting PWD infection stages, and their combination had the best accuracies (OA: 73.96%, Kappa: 0.66); (2) LiDAR data had higher ability for dead tree identification than HI data; and (3) The combined use of HI and LiDAR data for estimation of PWD infection stages showed that LiDAR metrics (e.g., crown volume) were essential in the classification model, although the variables derived from HI data contributed more than those extracted from LiDAR. Therefore, we proposed a new approach combining the merits of HI and LiDAR data to precisely predict PWD infection stages at the tree level, allowing better PWD monitoring and control. The approach could also be employed for mapping and monitoring other forest disturbance issues.}
}
@article{SATHEESHKUMAR20211813,
title = {Analysis of Connected Word Recognition systems using Levenberg Marquardt Algorithm for cockpit control in unmanned aircrafts},
journal = {Materials Today: Proceedings},
volume = {37},
pages = {1813-1819},
year = {2021},
note = {International Conference on Newer Trends and Innovation in Mechanical Engineering: Materials Science},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.399},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320355097},
author = {S. {Satheesh Kumar} and R. Sowmya and B. {Maruthi Shankar} and N. Lingaraj and S.A. Sivakumar},
keywords = {Continuous Word Recognition system, Multimodal interactions, Cockpit control, Levenberg Marquardt Algorithm, Neural networks},
abstract = {Due to advances in computation, the computer system needs sufficient input data, and it allows it a better computer tool for efficient operation of the human-computer, such as the fast-moving Automatic Speech Recognition System. This paper aims in particular to provide an insight into the contact distance between humans and computers in unmanned aircraft vehicles. While there are several algorithms, a critical analysis of algorithms suitable for large-scale applications is still important. The aircraft without a human pilot on board is an unmanned aerial vehicle. Continuous Word Recognition systems for voice enhancement (commanding) based cockpit control are commonly used in unmanned aircraft. The goal is to evaluate the efficiency of the Levenberg Marquardt algorithm by using these recognition systems. To do this, optimal preparation can be selected using neural networks to increase the machine recognition effectiveness. MATLAB verify simulated findings and tests show that a high accuracy of recognition of over 87 percent is obtained.}
}
@article{TEPANOSYAN2021107390,
title = {Studying spatial-temporal changes and relationship of land cover and surface Urban Heat Island derived through remote sensing in Yerevan, Armenia},
journal = {Building and Environment},
volume = {187},
pages = {107390},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.107390},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320307599},
author = {Garegin Tepanosyan and Vahagn Muradyan and Azatuhi Hovsepyan and Gleb Pinigin and Andrey Medvedev and Shushanik Asmaryan},
keywords = {Land cover, Surface urban heat island, Remote sensing, Machine learning, Thermal UAV survey, Yerevan},
abstract = {The city of Yerevan, Armenia has undergone major environmental and economic changes after the collapse of the Soviet Union. The objectives of this study were to: (i) investigate the changes of the Land Cover (LC) and Surface Urban Heat Island (SUHI) in Yerevan and analyze relations between them, (ii) study the relationships between land surface temperature (LST) and environmental factors/parameters, (iii) explore the accuracy of satellite derived LST. LC and SUHI were derived from Landsat TM/ETM+/OLI-TIRS images (years 1989, 2000, 2010 and 2018) by means of three Machine Learning algorithms and the Urban Thermal Field Variance Index (UTFVI) ecological evaluation index, respectively. The comparison between Unmanned Aerial Vehicle (UAV) and satellite LSTs showed that the overall spatial pattern of Landsat and UAV LSTs matched. It was found that the green and built-up areas were the main factors affecting LST variation in Yerevan. The results of the LC change analysis revealed an expansion of built-up areas and the reduction of green spaces. Yerevan shares almost an equal percentage of land for the excellent and the worst categories of the UTFVI. The transformations from excellent to the worst category of UTFVI were mainly related to the loss of green spaces, while the opposite transformations were associated with the gain of vegetation cover, the construction of new districts and the reduction/cessation of anthropogenic heat emission. It appeared that the urban construction had possibly led to the improvement of UTFVI index in the case of no/low anthropogenic heat emission.}
}
@article{AHMED2021102632,
title = {Deep learning-driven opportunistic spectrum access (OSA) framework for cognitive 5G and beyond 5G (B5G) networks},
journal = {Ad Hoc Networks},
volume = {123},
pages = {102632},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102632},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521001529},
author = {Ramsha Ahmed and Yueyun Chen and Bilal Hassan},
keywords = {Deep learning, Cognitive radio (CR), Spectrum sensing, Opportunistic spectrum access (OSA), 5G/B5G wireless networks},
abstract = {The evolving 5G and beyond 5G (B5G) wireless technologies are envisioned to provide ubiquitous connectivity and great heterogeneity in communication infrastructure by connecting diverse devices and providing multifarious services. Recently, the Internet of Things (IoT) and unmanned aerial vehicles (UAVs) are realized as an essential component of the upcoming 5G/B5G networks, enabling enhanced communication capacity, high reliability, low latency, and massive connectivity. However, one limiting factor in the expansion of 5G/B5G technology is the finite radio spectrum, which necessitates managing the anticipated spectrum crunch for future wireless networks. One potential solution is to develop intelligent cognitive methods to dynamically optimize the use of spectrum in 5G/B5G networks to solve the imminent problem of spectrum congestion and improve radio efficiency. This paper addresses the opportunistic spectrum access (OSA) problem in the 5G/B5G cognitive radio (CR) network of IoTs and UAVs through the novel deep learning-based detector, dubbed as Deep-CRNet. The proposed detector employs residual connections with cascaded multi-kernel convolutions to identify the primary user (PU) spectrum usage by extracting the inherent multi-scale signal and noise features in the sensed transmission patterns. Thereby, Deep-CRNet intelligently learns and locates the spectrum holes so that secondary users (SUs) and PUs can dynamically share network spectrum resources. The efficacy of Deep-CRNet is validated through simulation results, where it achieved 99.74% accuracy with 99.65% precision and 99.83% recall in accurately classifying the PU status. In addition, the average correct detection probability of Deep-CRNet in the low signal-to-noise ratio (−20 dB to −15 dB) range is 38.21% higher than the second best-performing detector.}
}
@article{DAMATO2015162,
title = {Nonlinear Dynamic Inversion and Neural Networks for a Tilt Tri-Rotor UAV},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {9},
pages = {162-167},
year = {2015},
note = {1st IFAC Workshop on Advanced Control and Navigation for Autonomous Aerospace Vehicles ACNAAV’15},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.08.077},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315009441},
author = {E. D'Amato and G. {Di Francesco} and I. Notaro and G. Tartaglione and M. Mattei},
keywords = {Unmanned Aerial Vehicle, Flight Control, Nonlinear Dynamic Inversion, Neural Networks, Adaptive Control},
abstract = {A small scale tri-rotor test bed with tilting propellers has been built to test flight control laws in view of the construction of a larger tilt rotor UAV. As a first step to achieve autonomous flight capabilities, a nonlinear dynamic inversion based flight controller is developed. This controller is designed on the basis of a time-scale principle with two levels. A lower level, fast control action, designed to achieve attitude control and stability goals, is driven by a higher level trajectory tracking control law. To achieve robust stability and performance in the presence of parametric variations and modelling uncertainties, an adaptive flight control law correction based on neural networks is investigated. A RBF neural network is implemented to mitigate the effects of imprecise inverse dynamics. The overall proposed flight controller performance are tested via numerical simulations on the mathematical model of the small scale tri-rotor. Preliminary results on the full tilt rotor are also shown.}
}
@article{KANITHAN2020254,
title = {An intelligent energy efficient cooperative MIMO-AF multi-hop and relay based communications for Unmanned Aerial Vehicular networks},
journal = {Computer Communications},
volume = {154},
pages = {254-261},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419308643},
author = {S. Kanithan and N. Arun Vignesh and E. Karthikeyan and N. Kumareshan},
keywords = {Precoding/beamforming, Energy Efficiency (EE), Partial Channel State Information (PCSI), Cooperative communication, MIMO, Channel estimation, Hybrid Fuzzy Firefly Algorithm (HFFA), Design and analysis, Amplify-and-Forward (AF) system, Branch Convolutional Neural Network (B-CNN) classifier},
abstract = {Unmanned Aerial Vehicles (UAVs) are recently used for both civilian and military applications in worldwide. Energy Efficiency (EE) is an exceptional design approach for modern communication based systems. New advance technology is needed in order to support UAV applications with reduced energy usage. In this paper, an Energy Efficiency with Hybrid Fuzzy Firefly Algorithm (EE-HFFA) method is introduced for Multiple-Input–Multiple-Output (MIMO) Amplify-And-Forward (AF) systems in which Partial Channel State Information (PCSI) estimation is existing at the relays because of the high speed mobility. A new EE-HFFA algorithm is presented in this research, by means of merging the benefits of the Firefly Algorithm (FA) as well as Differential Evolution (DE). For increasing information sharing both the techniques are implemented in parallel and as a result improve searching efficiency. The outcomes of these two techniques are based upon the fuzzy membership function. For approximation of PCSI for the source node as well as relay nodes, Branch Convolutional Neural Network (B-CNN) classifier is presented to raise the capability of cooperative MIMO-AF systems. EE-optimal source and relay precoding matrices are cooperatively enhanced by means of EE-HFFA. Simulation out comes illustrate that the presented EE-HFFA as well as B-CNN classifier could enhance the EE of MIMO-AF systems with PSCI while matched up with direct/relay link merely precoding optimization.}
}
@article{MOROCHOCAYAMCELA2021,
title = {An optimal location strategy for multiple drone base stations in massive MIMO},
journal = {ICT Express},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405959521000990},
author = {Manuel Eugenio Morocho-Cayamcela and Wansu Lim and Martin Maier},
keywords = {Disaster scenario, Massive MIMO, Path planning, Unmanned aerial vehicle, User association},
abstract = {The concept of drone base stations (DBSs) has been applied to reduce the distance of the wireless link between a macro base station and its active users under diverse scenarios in military communications, smart industries, and high-density networks, and to provide service in topologies with damaged infrastructure. In this paper, we address the optimal positioning of multiple DBSs in a multiple-input multiple-output wireless network setting. We present a low-complexity machine learning-based algorithm to optimize the location of the DBSs by minimizing the collective wireless received signal strength experienced by the active terminals. The proposed algorithm reduces the propagation loss in the system and provides a lower bit error rate when compared with the Euclidean cost benchmark.}
}
@article{DANG2020100250,
title = {UAV based wilt detection system via convolutional neural networks},
journal = {Sustainable Computing: Informatics and Systems},
volume = {28},
pages = {100250},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917304018},
author = {L. Minh Dang and Syed {Ibrahim Hassan} and Im Suhyeon and Arun kumar Sangaiah and Irfan Mehmood and Seungmin Rho and Sanghyun Seo and Hyeonjoon Moon},
keywords = {Unmanned aerial vehicles, Feature extraction, Radish field clustering, Fusarium wilt of radish classification},
abstract = {The significant role of plants can be observed through the dependency of animals and humans on them. Oxygen, materials, food and the beauty of the world are contributed by plants. Climate change, the decrease in pollinators, and plant diseases are causing a significant decline in both quality and coverage ratio of the plants and crops on a global scale. In developed countries, above 80 percent of rural production is produced by sharecropping. However, due to widespread diseases in plants, yields are reported to have declined by more than a half. These diseases are identified and diagnosed by the agricultural and forestry department. Manual inspection on a large area of fields requires a huge amount of time and effort, thereby reduces the effectiveness significantly. To counter this problem, we propose an automatic disease detection and classification method in radish fields by using a camera attached to an unmanned aerial vehicle (UAV) to capture high quality images from the fields and analyze them by extracting both color and texture features, then we used K-means clustering to filter radish regions and feeds them into a fine-tuned GoogleNet to detect Fusarium wilt of radish efficiently at early stage and allow the authorities to take timely action which ensures the food safety for current and future generations.}
}
@article{QIU2015690,
title = {A decoupling receding horizon search approach to agent routing and optical sensor tasking based on brain storm optimization},
journal = {Optik},
volume = {126},
number = {7},
pages = {690-696},
year = {2015},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2015.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0030402615000418},
author = {Huaxin Qiu and Haibin Duan and Yuhui Shi},
keywords = {Decoupling, Receding horizon control (RHC), Optical sensor, Brain storm optimization (BSO)},
abstract = {Search problem of unmanned air vehicles (UAVs) is a rather complicated multi-objective optimization problem with different constrains under complex combat field environment, the crux of which is the joint optimization for agent and optical sensor. In this paper, a decoupling receding horizon search approach is proposed to simplify the optimization problem. A brand-new swarm intelligence optimization algorithm named brain storm optimization (BSO) inspired by a human being's behavior of brainstorming is utilized to optimize the parameters of the proposed approach. Comparative experimental results are presented to show the better performance in feasibility, validity and superiority of our proposed approach than traditional method.}
}
@article{RODRIGUEZFERNANDEZ2017103,
title = {Analysing temporal performance profiles of UAV operators using time series clustering},
journal = {Expert Systems with Applications},
volume = {70},
pages = {103-118},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.10.044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416305851},
author = {Víctor Rodríguez-Fernández and Héctor D. Menéndez and David Camacho},
keywords = {UAVs, UAV operators, Time Series Clustering, Performance measures, Simulation-Based Training},
abstract = {The continuing growth in the use of Unmanned Aerial Vehicles (UAVs) is causing an important social step forward in the performance of many sensitive tasks, reducing both human and economical risks. The work of UAV operators is a key aspect to guarantee the success of this kind of tasks, and thus UAV operations are studied in many research fields, ranging from human factors to data analysis and machine learning. The present work aims to describe the behaviour of operators over time using a profile-based model where the evolution of the operator performance during a mission is the main unit of measure. In order to compare how different operators act throughout a mission, we describe a methodology based of multivariate-time series clustering to define and analyse a set of representative temporal performance profiles. The proposed methodology is applied in a multi-UAV simulation environment with inexperienced operators, obtaining a fair description of the temporal behavioural patterns followed during the course of the simulation.}
}
@article{ROMERO2018109,
title = {Vineyard water status estimation using multispectral imagery from an UAV platform and machine learning algorithms for irrigation scheduling management},
journal = {Computers and Electronics in Agriculture},
volume = {147},
pages = {109-117},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917315533},
author = {Maria Romero and Yuchen Luo and Baofeng Su and Sigfredo Fuentes},
keywords = {Remote sensing, Stem water potential, Artificial neural networks, Unmanned aerial vehicle, Water stress},
abstract = {Remote sensing can provide a fast and reliable alternative for traditional in situ water status measurement in vineyards. Several vegetation indices (VIs) derived from aerial multispectral imagery were tested to estimate midday stem water potential (Ψstem) of grapevines. The experimental trial was carried out in a vineyard in the Shangri-La region, located in Yunnan province in China. Statistical methods and machine learning algorithms were used to evaluate the correlations between Ψstem and VIs. Results by simple regression between VIs individually and Ψstem showed no significant relationships, with coefficient of determination (R2) for linear fitting smaller than 0.3 for almost all the indices studied, except for the Optimal Soil Adjusted Vegetation Index (OSAVI); R2 = 0.42 with statistical significance (p ≤ 0.001). However, results from a model obtained by fitting using Artificial Neural Network (ANN), using all VIs calculated as inputs and real Ψstem from plants within the study site (n = 90) as targets (Model 1), showed high correlation between the estimated water potential through ANN (Ψstem ANN) and the actual measured Ψstem. Training, validation and testing data sets presented individual correlations of R = 0.8, 0.72 and 0.62 respectively. The models obtained from the study site were then applied to a wider area from the vineyard studied and compared to further Ψstem measured obtained from different sites (n = 23) showing high correlation values between Ψstem ANN and real Ψstem (R2 = 0.83; slope = 1; p ≤ 0.001). Finally, a pattern recognition ANN model (Model 2) was developed for irrigation scheduling purposes using the same Ψstem measured in the study site as inputs and with the following thresholds as outputs: Ψstem below −1.2 MPa considered as severe water stress (SS), Ψstem between −0.8 to −1.2 MPa as moderate stress (MS) and Ψstem over −0.8 MPa with no water stress (NS). This model can be applied to analyze on a plant by plant basis to identify sectors of stress within the vineyard for optimal irrigation management and to identify spatial variability within the vineyards.}
}
@article{YANO2016415,
title = {Identification of weeds in sugarcane fields through images taken by UAV and Random Forest classifier},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {16},
pages = {415-420},
year = {2016},
note = {5th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.076},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316316391},
author = {Inacio H. Yano and Jose R. Alves and Wesley E. Santiago and Barbara J.T. Mederos},
keywords = {weed, pattern recognition, machine learning, images, UAV, sugarcane},
abstract = {Abstract:
Sugarcane is one of the most important cultures in the world. The productivity of sugarcane is affected by many factors, among them weeds can cause several problems. Weed control is made usually by herbicides application because sugarcane occupies extensive areas, and due to the same reason, the decision about herbicide type and dosage has been done by sampling. This work mode does not allow variation and causes problems of herbicide application, since the presence and weed type may not be uniform in whole field. There are some solutions based on satellite image analysis that allow the coverage of the entire field, solving the problem caused by sampling sense, but this solution depends on high weed infestation and a clear sky for good results. This work proposes a system for weed surveying, based on image pattern recognition with pictures taken by a UAV (Unmanned Aerial Vehicle); this alternative can take pictures very close to the plants, which allows species recognition in lower infestation levels and without clouds interference. This solution achieved an overall accuracy of 82 % and kappa coefficient of 0.73 in preliminary tests.}
}
@article{INTHIZAMI2022,
title = {Flood video segmentation on remotely sensed UAV using improved Efficient Neural Network},
journal = {ICT Express},
year = {2022},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2022.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S2405959522000169},
author = {Naili Suri Inthizami and M. Anwar Ma’sum and Machmud R. Alhamidi and Ahmad Gamal and Ronni Ardhianto and  Kurnianingsih and Wisnu Jatmiko},
keywords = {Atrous separable convolution, Depth-wise separable convolution, Efficient Neural Network, Semantic segmentation},
abstract = {Semantic segmentation can be used to analyze the video data taken by UAV in the flood monitoring system. An accurate analysis can help rescue teams to assess and mitigate flood disasters. This paper proposed an improved Efficient Neural Network architecture to segment the UAV video of flood disaster. The proposed method consists of atrous separable convolution as the encoder and depth-wise separable convolution as the decoder. The experimental results reveal that the proposed method outperforms Efficient Neural Networks’ other architecture and gives the highest frame per second.}
}
@article{GASPAROVIC2020105385,
title = {An automatic method for weed mapping in oat fields based on UAV imagery},
journal = {Computers and Electronics in Agriculture},
volume = {173},
pages = {105385},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105385},
url = {https://www.sciencedirect.com/science/article/pii/S016816991930359X},
author = {Mateo Gašparović and Mladen Zrinjski and Đuro Barković and Dorijan Radočaj},
keywords = {UAV, Imagery classification, Weed mapping, Oats, Precision agriculture},
abstract = {The accurate detection and treatment of weeds in agricultural fields is a necessary procedure for managing crop yield and avoiding herbicide pollution. With the emergence of unmanned aerial vehicles (UAV), the ability to acquire spatial data at the desired spatial and temporal resolution became available, and the resulting input data met high standards for weed management. In this paper, we tested four independent classification algorithms for the creation of weed maps, combining automatic and manual methods, as well as object-based and pixel-based classification approaches, which were used separately on two subsets. Input UAV data were collected using a low-cost RGB camera due to its affordability compared to multispectral cameras. Classification algorithms were based on the random forest machine learning algorithm for weed and bare soil extraction, following an unsupervised classification with the K-means algorithm for further estimation of weeds and bare soil presence in non-weed and non-soil areas. Of the four classification algorithms tested, the automatic object-based classification method achieved the highest classification accuracy, resulting in an overall accuracy of 89.0% for subset A and 87.1% for subset B. Automatic classification methods were robustly developed, using at least 0.25% of the scene size as the training data set in all circumstances anticipated for the random forest classification algorithm to operate. The use of the algorithm resulted in weed maps consisting of zoned classes and covering areas with similar biological properties, making them ready for use as inputs in weed treatments that use agricultural machinery.}
}
@article{WEN2019105004,
title = {Single-rotor UAV flow field simulation using generative adversarial networks},
journal = {Computers and Electronics in Agriculture},
volume = {167},
pages = {105004},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105004},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919306878},
author = {Sheng Wen and Ningwen Shen and Jiantao Zhang and Yubin Lan and Jie Han and Xuanchun Yin and Quanyong Zhang and Yufeng Ge},
keywords = {Flow-field, CFD, GAN, Flow field prediction, Feature extraction},
abstract = {In recent years, with the large-scale application of unmanned aerial vehicles (UAV) in agricultural plant protection, various shortcomings of have been identified; for example, the rotor flow field of the UAV will cause drift of the droplets, resulting in waste and secondary disaster. Therefore, digital simulation has become a necessity. However, due to the complexity of the rotor flow field of the rotor UAV and the operating environment of the UAV, digital simulation is associated with a large workload and great computational costs. It is urgent to explore new models using deep learning to identify the law of the rotor flow field. To address this problem, deep learning, in combination with flow field methods, is used to explore new models in this paper. A generative adversarial network (GAN) prediction model is proposed in this paper. The GAN includes a generation network and a discrimination network. In this paper, the features of the flow field are learned by the generative network to identify deep features of the flow field. The discrimination network distinguishes between true and false pictures by extracting features during training to realize adversarial training. This model can predict the flow field by identifying features of the flow-field distribution in training samples to build a predictive model. The compression effects of the computational fluid dynamics (CFD) model and the GAN model are compared in this paper. The GAN model outperforms the CFD model in predicting the flow field and compressing the data.}
}
@article{PI2021101278,
title = {3D-CNN based UAV hyperspectral imagery for grassland degradation indicator ground object classification research},
journal = {Ecological Informatics},
volume = {62},
pages = {101278},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101278},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000698},
author = {Weiqiang Pi and Jianmin Du and Yuge Bi and Xinchao Gao and Xiangbing Zhu},
keywords = {3D convolutional neural networks, Degraded grasslands, Plant population classification, UAV hyperspectral remote sensing},
abstract = {The identification and counting of grassland degradation indicator ground objects is an important component of grassland ecological monitoring. These steps are also an important basis for developing ecological restoration and management programs for degraded grasslands. Compared with a traditional human survey, the use of remote sensing images can not only achieve dynamic monitoring of a large area, but also improve the efficiency. Recently, most studies regarding ground object classification based on remote sensing images address the development and optimization of classification models for features in several widely used datasets. For the remote sensing of desertified grasslands, remote sensing images with high spatial resolutions are used for studies on small and sparse features in degraded grasslands. The spatial resolution of the above mentioned datasets yields difficulties when attempting to classify small and sparse indicator features for desertified grasslands because generalization becomes limited. Therefore, establishing a lightweight classification model suitable for degraded grassland features with high spatial resolution is important. In this study, a low altitude unmanned aerial vehicle (UAV) hyperspectral remote sensing platform was constructed to collect high spatial resolution remote sensing images of degraded grasslands. The GDIF-3D-CNN classification model was used to classify the pure pixels and all pixels datasets, whose accuracy and efficiency were further improved by optimizing the eight parameters of the model. This study explores the remote sensing ground object classification of thin small plants and a large number of mixed pixels, realizing high precision classification among desertification degradation indicating plant populations of a species, and provides key quantitative data for grassland degradation research.}
}
@article{ALBABA2021103417,
title = {A 3D game theoretical framework for the evaluation of unmanned aircraft systems airspace integration concepts},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {133},
pages = {103417},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103417},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21004113},
author = {Berat Mert Albaba and Negin Musavi and Yildiray Yildiz},
keywords = {UAS integration into NAS, Reinforcement Learning, Behavioral Game Theory, Human Modeling},
abstract = {Predicting the outcomes of integrating Unmanned Aerial System (UAS) into the National Airspace System (NAS) is a complex problem, which is required to be addressed by simulation studies before allowing the routine access of UAS into the NAS. This paper focuses on providing a 3-dimensional (3D) simulation framework using a game-theoretical methodology to evaluate integration concepts using scenarios where manned and unmanned air vehicles co-exist. In the proposed method, the human pilot interactive decision-making process is incorporated into airspace models which can fill the gap in the literature where the pilot behavior is generally assumed to be known a priori. The proposed human pilot behavior is modeled using a dynamic level-k reasoning concept and approximate reinforcement learning. The level-k reasoning concept is a notion in game theory and is based on the assumption that humans have various levels of decision making. In the conventional “static” approach, each agent makes assumptions about his or her opponents and chooses his or her actions accordingly. On the other hand, in the dynamic level-k reasoning, agents can update their beliefs about their opponents and revise their level-k rule. In this study, Neural Fitted Q Iteration, which is an approximate reinforcement learning method, is used to model time-extended decisions of pilots with 3D maneuvers. An analysis of UAS integration is conducted using an Example 3D scenario in the presence of manned aircraft and fully autonomous UAS equipped with sense and avoid algorithms.}
}
@article{NIU2021106414,
title = {Estimating fractional vegetation cover of maize under water stress from UAV multispectral imagery using machine learning algorithms},
journal = {Computers and Electronics in Agriculture},
volume = {189},
pages = {106414},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106414},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921004312},
author = {Yaxiao Niu and Wenting Han and Huihui Zhang and Liyuan Zhang and Haipeng Chen},
keywords = {Threshold method, Regression algorithms, UAV RGB images, Model suitability, FVC maps},
abstract = {Crop water stress is an inevitable and increasing challenge for agriculture. To improve crop water use efficiency, management of water stress and accurate estimation of crop traits were required. Among crop traits used to detect crop growth status and predict yield, fractional vegetation cover (FVC) is of great significance. We conducted studies in a maize field located in Inner Mongolia, China with different irrigation levels during 2018 and 2019 growing seasons. UAV RGB imagery was captured to investigate the effect of image sensors on thresholds obtained by the fixed-threshold method (proposed in our recent study), and to provide reference FVC (FVCUAV_R) for FVC models based on UAV multispectral imagery. Five vegetation indices (VIs), calculated from UAV multispectral imagery, and three regression algorithms (RF: random forest, ANN: artificial neural network, and MLR: multivariate linear regression) were used to build the FVC model suitable for different growing seasons, growth stages, and crop water stress. The results showed that there was a change in thresholds obtained using the fixed-threshold method based on different image sensors, but this change did not make a big difference on the accuracy of FVCUAV_R, with the R2 difference of 0.01 and the RMSE difference of 0.01. As for the three FVC regression models, RF model was the most suitable model when these models established in 2018 were used to estimate maize FVC in 2019 for different growth stages and water stress. The low estimation accuracy for high FVC levels was the reason why MLR model could not be used in the other maize growing season. This study provides a low cost and easy way to estimate maize FVC and its inter-field variability under various water status in different maize growing seasons or growth stages.}
}
@article{AHMED2022103290,
title = {Deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5G smart city networks},
journal = {Digital Signal Processing},
volume = {120},
pages = {103290},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2021.103290},
url = {https://www.sciencedirect.com/science/article/pii/S1051200421003298},
author = {Ramsha Ahmed and Yueyun Chen and Bilal Hassan},
keywords = {Cognitive radio, Deep learning, Spectrum sensing, Signal detection, Transmission pattern classification},
abstract = {Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.}
}
@article{HUYNH2019102844,
title = {Quasi-autonomous bolt-loosening detection method using vision-based deep learning and image processing},
journal = {Automation in Construction},
volume = {105},
pages = {102844},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102844},
url = {https://www.sciencedirect.com/science/article/pii/S092658051930250X},
author = {Thanh-Canh Huynh and Jae-Hyung Park and Hyung-Jo Jung and Jeong-Tae Kim},
keywords = {Bolted connection, Bolt-loosening, Deep learning, CNN, Hough transform, Canny line detector, Bolt detection, Bolt rotation estimation},
abstract = {In this study, a quasi-autonomous vision-based method is newly proposed for detecting loosened bolts in critical connections. The main idea of the approach is to estimate the rotational angles of bolts from the connection images by integrating deep learning technology with image processing techniques. Firstly, a regional convolutional neural network (RCNN)-based deep learning algorithm is developed to automatically detect and crop plausible bolts in the connection image. Also, the Hough line transform (HLT)-based image processing algorithm is designed to automatically estimate the bolt angles from the cropped bolt images. Secondly, the proposed vision-based approach is validated for bolt-loosening detection in a lab-scale girder connection using images captured by a smartphone camera. The accuracy of the RCNN-based bolt detector and the HLT-based bolt angle estimator are examined under different levels of perspective distortion and shooting distance. Finally, the practicality of the proposed vision-based method is verified on a real-scale girder bridge connection containing numerous bolts. The images of the connection are captured by an unmanned aerial vehicle and transferred to a computer where a quasi-autonomous bolt-loosening detection process is performed via the proposed algorithm. The experimental results demonstrate potentials of the proposed approach for quasi real-time bolt-loosening monitoring of large bolted connections. The results show that the perspective angle should not go beyond 40 degrees to ensure the accuracy of the detection results.}
}
@article{SILVA2022125,
title = {Automatic detection of Flavescense Dorée grapevine disease in hyperspectral images using machine learning},
journal = {Procedia Computer Science},
volume = {196},
pages = {125-132},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.11.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022201},
author = {Diogo M. Silva and Théo Bernardin and Kévin Fanton and Roshan Nepaul and Luís Pádua and Joaquim J. Sousa and António Cunha},
keywords = {Hyperspectral images, grapevine diseases, automatic detection, autoencoder, machine learning},
abstract = {The technological revolution that we have been witnessing recently has allowed components miniaturization and made electronic components accessible. Hyperspectral sensors benefited from these advances and could be mounted on unmanned aerial vehicles, which was unthinkable until recently. This fact significantly increased the applications of hyperspectral data, namely in agriculture, especially in the detection of diseases at an early stage. The vineyard is one of the agricultural sectors that has the most to gain from the use of this type of data, both by the economic value and by the number of diseases the plants are exposed to. The Flavescense dorée is a disease that attacks vineyards and may conduct to a significant loss. Nowadays, the detection of this disease is based on the visual identification of symptoms performed by experts who cover the entire area. However, this work remains tedious and relies only on the human eye, which is a problem since sometimes healthy plants are torn out, while diseased ones are left. If the experts think they have found symptoms, they take samples to send to the laboratory for further analysis. If the test is positive, then the whole vine is uprooted, to limit the spread of the disease. In this context, the use of hyperspectral data will allow the development of new disease detection methods. However, it will be necessary to reduce the volume of data used to make them usable by conventional resources. Fortunately, the advent of machine learning techniques empowered the development of systems that allow better decisions to be made, and consequently save time and money. In this article, a machine learning approach, which is based on an Autoencoder to automatically detect wine disease, is proposed.}
}
@article{PENG2021108158,
title = {Random forest regression results in accurate assessment of potato nitrogen status based on multispectral data from different platforms and the critical concentration approach},
journal = {Field Crops Research},
volume = {268},
pages = {108158},
year = {2021},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2021.108158},
url = {https://www.sciencedirect.com/science/article/pii/S0378429021001040},
author = {Junxiang Peng and Kiril Manevski and Kirsten Kørup and René Larsen and Mathias Neumann Andersen},
keywords = {Nitrogen requirement, Nutrient index, Hand-held Rapidscan, Sentinel-2, Unmanned aerial vehicle},
abstract = {Remote sensing can be used for precision nutrient management to assess plant nitrogen (N) status in a spatially detailed and real-time manner. Despite recent advances in satellite- and drone technology and machine learning, neither differences between platforms nor methodological aspects for estimating plant N status have been sufficiently investigated. In this study, multispectral data obtained by ground (handheld Rapidscan), air- (unmanned aerial vehicle, UAV) and spaceborne (Sentinel-2) platforms were exploited to estimate plant N uptake (PNU), concentration (PNC) and N nutrition index (NNI). The test plant was potato grown for three years on a sandy soil in Denmark and the analysis was based on the critical N dilution curve. Parametric (PR) and non-parametric (random forest, RFR) regressions were conducted and compared in predicting mid-season PNU, PNC and NNI from band reflectances or vegetation indices (VIs) derived from each platform data. The results obtained by the UAV data had the highest accuracy, largely due to the fine spatial resolution. For both regression types, PNU and NNI correlated better than PNC to reflectance data. For the UAV data, validation Nash-Sutcliffe model efficiency (NSE) of PNU and NNI ranged between 0.64–0.95 and 0.41–0.92 respectively, with corresponding values for relative root mean square error (RRMSE) of 7.1–22% and 5.86–22%. The lower end of NSE and higher end RRMSE intervals systematically being from the PR, which demonstrates the robustness and the high accuracy of RFR in predicting plant N status. The other platforms resulted in acceptable results, with validation NSE and RRMSE for PNU and NNI of, respectively, 0.60–0.79 and 14–20%, 0.25–0.79 and 10–17% for Rapidscan, and 0.48–0.83 and 17–28%, 0.42–0.82 and 12–19% for Sentinel-2. The band reflectance and the VIs were equally suited as input predictors for the RFR algorithm. The N requirement calculated from all three datasets reflected the field observations well. The study reveals the potential of different regression methods for detailed spatial estimation of plant N status to guide in-season fertilization by matching the plant growth demands, emphasizing the strengths of the RFR. The procedure is helpful for the digital agriculture and the smart farming industry aiming to avoid excess application of N.}
}
@article{REZAEI2019807,
title = {Numerical evaluation of gamma radiation monitoring},
journal = {Nuclear Engineering and Technology},
volume = {51},
number = {3},
pages = {807-817},
year = {2019},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2018.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S1738573318305370},
author = {Mohsen Rezaei and Mansour Ashoor and Leila Sarkhosh},
keywords = {Artificial neural networks, BFGS training algorithm, Airborne gamma ray spectrometry, Nuclear site surveillance},
abstract = {Airborne Gamma Ray Spectrometry (AGRS) with its important applications such as gathering radiation information of ground surface, geochemistry measuring of the abundance of Potassium, Thorium and Uranium in outer earth layer, environmental and nuclear site surveillance has a key role in the field of nuclear science and human life. The Broyden–Fletcher–Goldfarb–Shanno (BFGS), with its advanced numerical unconstrained nonlinear optimization in collaboration with Artificial Neural Networks (ANNs) provides a noteworthy opportunity for modern AGRS. In this study a new AGRS system empowered by ANN-BFGS has been proposed and evaluated on available empirical AGRS data. To that effect different architectures of adaptive ANN-BFGS were implemented for a sort of published experimental AGRS outputs. The selected approach among of various training methods, with its low iteration cost and non-diagonal scaling allocation is a new powerful algorithm for AGRS data due to its inherent stochastic properties. Experiments were performed by different architectures and trainings, the selected scheme achieved the smallest number of epochs, the minimum Mean Square Error (MSE) and the maximum performance in compare with different types of optimization strategies and algorithms. The proposed method is capable to be implemented on a cost effective and minimum electronic equipment to present its real-time process, which will let it to be used on board a light Unmanned Aerial Vehicle (UAV). The advanced adaptation properties and models of neural network, the training of stochastic process and its implementation on DSP outstands an affordable, reliable and low cost AGRS design. The main outcome of the study shows this method increases the quality of curvature information of AGRS data while cost of the algorithm is reduced in each iteration so the proposed ANN-BFGS is a trustworthy appropriate model for Gamma-ray data reconstruction and analysis based on advanced novel artificial intelligence systems.}
}
@article{LIU20141273,
title = {Fuzzy adaptive tracking control within the full envelope for an unmanned aerial vehicle},
journal = {Chinese Journal of Aeronautics},
volume = {27},
number = {5},
pages = {1273-1287},
year = {2014},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2014.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1000936114001381},
author = {Zhi Liu and Yong Wang},
keywords = {Flight control systems, Full flight envelope, Fuzzy adaptive tracking control, Fuzzy multiple Lyapunov function, Fuzzy T–S model, Single hidden layer neural network},
abstract = {Motivated by the autopilot of an unmanned aerial vehicle (UAV) with a wide flight envelope span experiencing large parametric variations in the presence of uncertainties, a fuzzy adaptive tracking controller (FATC) is proposed. The controller consists of a fuzzy baseline controller and an adaptive increment, and the main highlight is that the fuzzy baseline controller and adaptation laws are both based on the fuzzy multiple Lyapunov function approach, which helps to reduce the conservatism for the large envelope and guarantees satisfactory tracking performances with strong robustness simultaneously within the whole envelope. The constraint condition of the fuzzy baseline controller is provided in the form of linear matrix inequality (LMI), and it specifies the satisfactory tracking performances in the absence of uncertainties. The adaptive increment ensures the uniformly ultimately bounded (UUB) predication errors to recover satisfactory responses in the presence of uncertainties. Simulation results show that the proposed controller helps to achieve high-accuracy tracking of airspeed and altitude desirable commands with strong robustness to uncertainties throughout the entire flight envelope.}
}
@article{WILKE2021106380,
title = {Assessment of plant density for barley and wheat using UAV multispectral imagery for high-throughput field phenotyping},
journal = {Computers and Electronics in Agriculture},
volume = {189},
pages = {106380},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106380},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003975},
author = {Norman Wilke and Bastian Siegmann and Johannes A. Postma and Onno Muller and Vera Krieger and Ralf Pude and Uwe Rascher},
keywords = {Plant density, Germination rate, Barley, Wheat, High-throughput phenotyping, UAV},
abstract = {Cereal plant density is a relevant agronomic trait in agriculture and high-throughput phenotyping of plant density is important for the decision-making process in precision farming and breeding. It influences the water as well as the fertilization requirements, the intraspecific competition, and the occurrence of weeds or pathogens. Recent studies have determined plant density using machine-learning approaches and feature extraction. This requires spatially very highly resolved images (0.02 cm) because the accuracy distinctly decreased when images had lower resolution. In this study, we present an approach that uses the linear relationship between plant density manually counted in the field and fractional cover derived from a RGB and a multispectral camera equipped on an unmanned aerial vehicle (UAV). We assumed that at an early seedling stage fractional cover is closely related to the number of plants. Spring barley and spring wheat experiments, each with three genotypes and four different sowing densities, were examined. The practicability and repeatability of the methodology were evaluated with an independent experiment consisting of 42 winter wheat genotypes. This experiment mainly differed for genotypes, sowing density and season. The empirical regression models that make us of multispectral images having a GSD of 0.69 cm were able to determine plant density with a high prediction accuracy for barley and wheat (R2 > 0.91, mean absolute error (MAE) < 28 plants). In addition, prediction accuracy only slightly declines for multispectral image data having 1.4 cm GSD or RGB image data having 0.6 cm GSD (MAE < 35 plants m−2). BBCH stage 13 was identified as the ideal growth stage in which the plants were large enough to accurately determine fractional cover even from the lower resolution image data. Moreover, a developed empirical regression model was transferred to an independent experimental field verifying its robustness across different conditions. The prediction accuracy of UAV estimated plant density showed an R2 value of 0.83 and an MAE of less than 21 plants m−2. Furthermore, manual measurements of 11 randomly selected plots proved sufficient for a user-based training of the regression model (R2 = 0.83, MAE < 23 plants m−2) adapted to the independent experimental field. The method and the use of UAV image data enable high-throughput phenotyping of cereal plant density with uncertainties of less than 10 %.The practicability, repeatability and robustness of the developed approach were demonstrated in this study.}
}
@article{LIU2021,
title = {Ephemeral gully recognition and accuracy evaluation using deep learning in the hilly and gully region of the Loess Plateau in China},
journal = {International Soil and Water Conservation Research},
year = {2021},
issn = {2095-6339},
doi = {https://doi.org/10.1016/j.iswcr.2021.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095633921000976},
author = {Boyang Liu and Biao Zhang and Hao Feng and Shufang Wu and Jiangtao Yang and Yufeng Zou and Kadambot H.M. Siddique},
keywords = {, , , , , },
abstract = {Ephemeral gullies are widely distributed in the hilly and gully region of the Loess Plateau and play a unique role in the slope gully erosion system. Rapid and accurate identification of ephemeral gullies impacts the distribution law and development trend of soil erosion on the Loess Plateau. Deep learning algorithms can quickly and accurately process large data samples that recognize ephemeral gullies from remote sensing images. Here, we investigated ephemeral gullies in the Zhoutungou watershed in the hilly and gully region of the Loess Plateau in China using satellite and unmanned aerial vehicle images and combined a deep learning image semantic segmentation model to realize automatic recognition and feature extraction. Using Accuracy, Precision, Recall, F1value, and AUC, we compared the ephemeral gully recognition results and accuracy evaluation of U-Net, R2U-Net, and SegNet image semantic segmentation models. The SegNet model was ranked first, followed by the R2U-Net and U-Net models, for ephemeral gully recognition in the hilly and gully region of the Loess Plateau. The ephemeral gully length and width between predicted and measured values had RMSE values of 6.78 m and 0.50 m, respectively, indicating that the model has an excellent recognition effect. This study identified a fast and accurate method for ephemeral gully recognition in the hilly and gully region of the Loess Plateau based on remote sensing images to provide an academic reference and practical guidance for soil erosion monitoring and slope and gully management in the Loess Plateau region.}
}
@article{FLAH2020103781,
title = {Classification and quantification of cracks in concrete structures using deep learning image-based techniques},
journal = {Cement and Concrete Composites},
volume = {114},
pages = {103781},
year = {2020},
issn = {0958-9465},
doi = {https://doi.org/10.1016/j.cemconcomp.2020.103781},
url = {https://www.sciencedirect.com/science/article/pii/S0958946520302870},
author = {Majdi Flah and Ahmed R. Suleiman and Moncef L. Nehdi},
keywords = {, , , , , },
abstract = {Visual inspection has been the most widely used technique for monitoring concrete structures in service. Inspectors visually evaluate defects based on experience, skill, and engineering judgment. However, this process is subjective, laborious, time-consuming, and hampered by demanding access to numerous parts of complex structures. Accordingly, the present study proposes a nearly automated inspection model based on image processing and deep learning for detecting defects in typically inaccessible areas of concrete structures. Results indicate that using the Keras classifier combined with Otsu image processing can achieve superior classification accuracy of 97.63%, 96.5%, and 96.17% for training, validation, and testing data, respectively, along with low quantification error of 1.5%, 5% and 2% for the crack length, width, and angle of orientation, respectively. The type of structural damage and its severity are identified based on the allowed range of concrete crack width for different structures, including buildings and bridges based on different international standards and codes. The proposed method can deploy unmanned aerial vehicle image acquisition to offer a nearly automated inspection platform for the colossal backlog of aging concrete structures.}
}
@article{NI2021,
title = {Energy-optimal trajectory planning for solar-powered aircraft using soft actor-critic},
journal = {Chinese Journal of Aeronautics},
year = {2021},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2021.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1000936121003848},
author = {Wenjun NI and Ying BI and Di WU and Xiaoping MA},
keywords = {Flight strategy, Guidance and control, Reinforcement learning, Solar powered aeroplane, Trajectory generation},
abstract = {High-Altitude Long-Endurance (HALE) solar-powered Unmanned Aircraft Vehicles (UAVs) can utilize solar energy as power source and maintain extremely long cruise endurance, which has attracted extensive attentions from researchers. Trajectory optimization is a promising way to achieve superior flight time because of the finite solar energy absorbed in a day. In this work, a method of trajectory optimization and guidance for HALE solar-powered aircraft based on a Reinforcement Learning (RL) framework is introduced. According to flight and environment information, a neural network controller outputs commands of thrust, attack angle, and bank angle to realize an autonomous flight based on energy maximization. The validity of the proposed method was evaluated in a 5-km radius area in simulation, and results have shown that after one day-night cycle, the battery energy of the RL-controller was improved by 31% and 17% compared with those of a Steady-State (SS) strategy with a constant speed and a constant altitude and a kind of state-machine strategy, respectively. In addition, results of an uninterrupted flight test have shown that the endurance of the RL controller was longer than those of the control cases.}
}
@article{FERREIRA2020118397,
title = {Individual tree detection and species classification of Amazonian palms using UAV images and deep learning},
journal = {Forest Ecology and Management},
volume = {475},
pages = {118397},
year = {2020},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2020.118397},
url = {https://www.sciencedirect.com/science/article/pii/S037811272031166X},
author = {Matheus Pinheiro Ferreira and Danilo Roberti Alves de Almeida and Daniel de Almeida Papa and Juliano Baldez Silva Minervino and Hudson Franklin Pessoa Veras and Arthur Formighieri and Caio Alexandre Nascimento Santos and Marcio Aurélio Dantas Ferreira and Evandro Orfanó Figueiredo and Evandro José Linhares Ferreira},
keywords = {Drone, DeepLabv3+, Amazon forest, Açaí, },
abstract = {Information regarding the spatial distribution of palm trees in tropical forests is crucial for commercial exploitation and management. However, spatially continuous knowledge of palms occurrence is scarce and difficult to obtain with conventional approaches such as field inventories. Here, we developed a new method to map Amazonian palm species at the individual tree crown (ITC) level using RGB images acquired by a low-cost unmanned aerial vehicle (UAV). Our approach is based on morphological operations performed in the score maps of palm species derived from a fully convolutional neural network model. We first constructed a labeled dataset by dividing the study area (135 ha within an old-growth Amazon forest) into 28 plots of 250 m × 150 m. Then, we manually outlined all palm trees seen in RGB images with 4 cm pixels. We identified three palm species: Attalea butyracea, Euterpe precatoria and Iriartea deltoidea. We randomly selected 22 plots (80%) for training and six plots (20%) for testing. We changed the plots for training and testing to evaluate the variability in the classification accuracy and assess model generalization. Our method outperformed the average producer’s accuracy of conventional patch-wise semantic segmentation (CSS) in 4.7%. Moreover, our method correctly identified, on average, 34.7 percentage points more ITCs than CSS, which tended to merge trees that are close to each other. The producer’s accuracy of A. butyracea, E. precatoria and I. deltoidea was 78.6 ± 5.5%, 98.6 ± 1.4% and 96.6 ± 3.4%, respectively. Fortunately, one of the most exploited and commercialized palm species in the Amazon (E. precatoria, a.k.a, Açaí) was mapped with the highest classification accuracy. Maps of E. precatoria derived from low-cost UAV systems can support management projects and community-based forest monitoring programs in the Amazon.}
}
@article{FRAVOLINI2010288,
title = {LMI-based design of a Neuro-Adaptive augmentation controller for an Unmanned Aerial Vehicle},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {16},
pages = {288-293},
year = {2010},
note = {7th IFAC Symposium on Intelligent Autonomous Vehicles},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100906-3-IT-2019.00051},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016350716},
author = {M.L. Fravolini and S. Fiani and G. Campa},
keywords = {, , , , , },
abstract = {This paper presents a practical approach for verifying worst-case tracking performance of neuro-adaptive systems in presence of bounded uncertainties. Boundedness of the tracking error vector within an a-priori specified compact domain is obtained by applying robust invariant set analysis to the uncertain linear plant where the uncertainty and Neural Network (NN) reconstruction error are considered as norm bounded persistent uncertainties. In this framework it was possible to specify worst-case tracking error requirements via a set of LMIs and to systematically verify the specifications using a numerical LMI solver. The presented method was applied to the performance verification of an adaptive augmentation controller for the short term dynamics of an UAV model.}
}
@article{DEOLIVEIRA2021119496,
title = {Eucalyptus growth recognition using machine learning methods and spectral variables},
journal = {Forest Ecology and Management},
volume = {497},
pages = {119496},
year = {2021},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2021.119496},
url = {https://www.sciencedirect.com/science/article/pii/S0378112721005867},
author = {Bruno Rodrigues {de Oliveira} and Arlindo Ananias Pereira {da Silva} and Larissa Pereira Ribeiro Teodoro and Gileno Brito {de Azevedo} and Glauce Taís de Oliveira Sousa Azevedo and Fábio Henrique Rojo Baio and Renato Lustosa Sobrinho and Carlos Antonio {da Silva Junior} and Paulo Eduardo Teodoro},
keywords = {Random forest, Classification, Vegetation index},
abstract = {Growth and production models can help to simulate the growth of tree dimensions to predict forest productivity at different levels. In this context, the following questions arise: (i) is it possible to recognize the growth pattern of eucalyptus species based on spectral features using machine learning (ML) for data modeling? (ii) what spectral features provides better accuracy? and (iii) what ML algorithms are most accurate for performing this modeling? To answer these questions, the present study evaluated the use of ML techniques using breast height and total plant height to classify the growth of five species of eucalyptus and Corymbria citriodora in an unsupervised learning, and the obtained classes for induce ML algorithms to recognize the species with relation to their growth using vegetation indices (VIs) and spectral bands (SBs). It were evaluated five eucalyptus species (E. camaldulensis, E. uroplylla, E. saligna, E. grandis e E. urograndis) and C. citriodora in experimental design of randomized blocks with four replicates, with 20 plants inside each experimental plot. The diameter at breast height and total plant height at stand level were obtained by measuring five trees in each experimental unit in seven measurements. During this same period, a flight was carried out using a remotely piloted aircraft for the acquisition of spectral variables (SBs and VIs). For recognition of eucalyptus species in relation to their growth two machine learning approaches were employed: supervised and unsupervised. The average accuracy obtained from 10-fold cross-validation, employing Random Forest algorithm and 24 features, was 0.76. This result shows that the proposed approach is appropriate to recognize different eucalyptus species based on their growth.}
}
@article{CHEN2014436,
title = {Robust tracking control for uncertain MIMO nonlinear systems with input saturation using RWNNDO},
journal = {Neurocomputing},
volume = {144},
pages = {436-447},
year = {2014},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2014.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214006195},
author = {Mou Chen and Yanlong Zhou and William W. Guo},
keywords = {Nonlinear system, Unmanned aerial vehicle, Input saturation, Recurrent wavelet neural network, Disturbance observer, Dynamic surface control},
abstract = {In this paper, the robust tracking control scheme is proposed for a class of uncertain multi-input and multi-output (MIMO) nonlinear systems with input saturation and unknown external disturbance based on the recurrent wavelet neural network disturbance observer (RWNNDO) and the backstepping technique. And then, the developed robust tracking control scheme is applied to an unmanned aerial vehicle (UAV) system. To handle the input saturation, a hyperbolic tangent function and a Nussbaum function are employed, and the dynamic surface method is applied to solve the problem of “explosion of complexity” in backstepping control. It is proved that the proposed control scheme can guarantee that all signals of the closed-loop system are bounded through the Lyapunov analysis. Simulation results are presented to demonstrate the effectiveness of the proposed control scheme for uncertain MIMO nonlinear systems.}
}
@article{MLIKI2020107140,
title = {Human activity recognition from UAV-captured video sequences},
journal = {Pattern Recognition},
volume = {100},
pages = {107140},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.107140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319304418},
author = {Hazar Mliki and Fatma Bouhlel and Mohamed Hammami},
keywords = {Scene stabilization, Human detection, Human activity recognition, Deep learning, Convolutional neural networks, UAV},
abstract = {This research paper introduces a new approach for human activity recognition from UAV-captured video sequences. The proposed approach involves two phases: an offline phase and an inference phase. A scene stabilization step is performed together with these two phases. The offline phase aims to generate the human/non-human model as well as a human activity model using a convolutional neural network. The inference phase makes use of the already generated models in order to detect humans and recognize their activities. Our main contribution lies in adapting the convolutional neural networks, normally dedicated to the classification task, to detect humans. In addition, the classification of human activities is carried out according to two scenarios: An instant classification of video frames and an entire classification of the video sequences. Relying on an experimental evaluation of the proposed methods for human detection and human activity classification on the UCF-ARG dataset, we validated not only these contributions but also the performance of our methods compared to the existing ones.}
}
@article{FU2018593,
title = {Adaptive robust backstepping attitude control for a multi-rotor unmanned aerial vehicle with time-varying output constraints},
journal = {Aerospace Science and Technology},
volume = {78},
pages = {593-603},
year = {2018},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2018.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S1270963818304292},
author = {Chunyang Fu and Wei Hong and Huiqiu Lu and Lei Zhang and Xiaojun Guo and Yantao Tian},
keywords = {Multi-rotor unmanned aerial vehicle, Attitude control, Asymmetric time-varying output constraints, Asymmetric time-varying barrier Lyapunov function, Adaptive neural network, Backstepping dynamic surface control},
abstract = {Output constraints and uncertainties are the main factors that degrade the control performance of the multi-rotor unmanned aerial vehicle (MUAV). In this paper, an adaptive neural network backstepping dynamic surface control algorithm based on asymmetric time-varying Barrier Lyapunov Function is proposed for the attitude system of a novel MUAV under asymmetric time-varying output constraints, model uncertainties and external disturbances. The asymmetric time-varying Barrier Lyapunov Function, which will grow infinite when its arguments approach some limit, is introduced to keep the output under time-varying asymmetric constraints. Considering the derivation problem of the virtual control function in backstepping, the dynamic surface control is applied to simplify the algorithm. The adaptive neural network is used to approximate the dynamic model of the attitude system, and the minimal learning parameters are employed at the same time to reduce online computation burden. In order to balance out the external disturbance and further reduce the approximate error of the adaptive neural network, a robust term is designed to compensate the above negative impacts. The proposed algorithm guarantees that all the signals of the closed-loop system bounded by Lyapunov theory. Finally, some contrast simulation experiments are given to illustrate the effectiveness and superiority of the control scheme.}
}
@article{MARIN2021106476,
title = {Detecting coffee leaf rust with UAV-based vegetation indices and decision tree machine learning models},
journal = {Computers and Electronics in Agriculture},
volume = {190},
pages = {106476},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106476},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921004932},
author = {Diego Bedin Marin and Gabriel Araújo e Silva Ferraz and Lucas Santos Santana and Brenon Diennevan Souza Barbosa and Rafael Alexandre Pena Barata and Lucas Prado Osco and Ana Paula Marques Ramos and Paulo Henrique Sales Guimarães},
keywords = {Multispectral imagery, Precision agriculture, Plant disease, Logistic model tree},
abstract = {Coffee leaf rust (CLR) is one of the most devastating leaf diseases in coffee plantations. By knowing the symptoms, severity, and spatial distribution of CLR, farmers can improve disease management procedures and reduce losses associated with it. Recently, Unmanned Aerial Vehicles (UAVs)-based images, in conjunction with machine learning (ML) techniques, helped solve multiple agriculture-related problems. In this sense, vegetation indices processed with ML algorithms are a promising strategy. It is still a challenge to map severity levels of CLR using remote sensing data and an ML approach. Here we propose a framework to detect CLR severity with only vegetation indices extracted from UAV imagery. For that, we based our approach on decision tree models, as they demonstrated important results in related works. We evaluated a coffee field with different infestation classes of CLR: class 1 (from 2% to 5% rust); class 2 (from 5% to 10% rust); class 3 (from 10% to 20% rust), and; class 4 (from 20% to 40% rust). We acquired data with a Sequoia camera, producing images with a spatial resolution of 10.6 cm, in four spectral bands: green (530–570 nm), red (640–680 nm), red-edge (730–740 nm), and near-infrared (770–810 nm). A total of 63 vegetation indices was extracted from the images, and the following learners were evaluated in a cross-validation method with 10 folders: Logistic Model Tree (LMT); J48; ExtraTree; REPTree; Functional Trees (FT); Random Tree (RT), and; Random Forest (RF). The results indicated that the LMT method contributed the most to the accurate prediction of early and several infestation classes. For these classes, LMT returned F-measure values of 0.915 and 0.875, thus being a good indicator of early CLR (2 to 5% of rust) and later stages of CLR (20 to 40% of rust). We demonstrated a valid approach to model rust in coffee plants using only vegetation indices and ML algorithms, specifically for the disease's early and later stages. We concluded that the proposed framework allows inferring the predicted classes in remaining plants within the sampled area, thus helping the identification of potential CLR in non-sampled plants. We corroborate that the decision tree-based model may assist in precision agriculture practices, including mapping rust in coffee plantations, providing both an efficient non-invasive and spatially continuous monitoring of the disease.}
}
@article{CHEMALI2018242,
title = {State-of-charge estimation of Li-ion batteries using deep neural networks: A machine learning approach},
journal = {Journal of Power Sources},
volume = {400},
pages = {242-255},
year = {2018},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2018.06.104},
url = {https://www.sciencedirect.com/science/article/pii/S0378775318307080},
author = {Ephrem Chemali and Phillip J. Kollmeyer and Matthias Preindl and Ali Emadi},
keywords = {Battery management systems, Deep neural networks, Energy storage system, Li-ion batteries, Machine learning, State of charge estimation},
abstract = {Accurate State of Charge (SOC) estimation is crucial to ensure the safe and reliable operation of Li-ion batteries, which are increasingly being used in Electric Vehicles (EV), grid-tied load-leveling applications as well as manned and unmanned aerial vehicles to name a few applications. In this paper, a novel approach using Deep Feedforward Neural Networks (DNN) is used for battery SOC estimation where battery measurements are directly mapped to SOC. Training data is generated in the lab by applying drive cycle loads at various ambient temperatures to a Li-ion battery so that the battery is exposed to variable dynamics. The DNN's ability to encode the dependencies in time into the network weights and in the process provide accurate estimates of SOC is presented. Moreover, data recorded at ambient temperatures lying between −20 °C and 25 °C are fed into the DNN during training. Once trained, this single DNN is able to estimate SOC at various ambient temperature conditions. The DNN is validated over many different datasets and achieves a Mean Absolute Error (MAE) of 1.10% over a 25 °C dataset as well as an MAE of 2.17% over a −20 °C dataset.}
}
@article{HOSSEINALIZADEH2019184,
title = {Spatial modelling of gully headcuts using UAV data and four best-first decision classifier ensembles (BFTree, Bag-BFTree, RS-BFTree, and RF-BFTree)},
journal = {Geomorphology},
volume = {329},
pages = {184-193},
year = {2019},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X19300066},
author = {Mohsen Hosseinalizadeh and Narges Kariminejad and Wei Chen and Hamid Reza Pourghasemi and Mohammad Alinejad and Ali {Mohammadian Behbahani} and John P. Tiefenbacher},
keywords = {Gully headcut, Unmanned aerial vehicle, Ensemble modelling, Sensitivity analysis, Iran, Iky Aghzly sub-watershed},
abstract = {Despite the importance of delineating spatial modelling of gully headcuts (GHs) in erosion-prone environments, assessments of the factors that control the occurrence of headcuts is lacking. To fill this gap in the research, we identified 129 GHS field surveys. These 129 cases were randomly divided into two groups: 90 GHs (70%) for model training and 39 GHs (30%) for model validation. Subsequently, new unmanned aerial vehicle (UAV) imagery is used to develop spatial modelling to predict the location of GHs at sites prone to soil erosion in Golestan Province, Iran. Mapping GHs enables evaluation of 4 machine-learning techniques (or ensembles) – best-first decision tree (BFTree), bagging best-first decision tree (Bag-BFTree), random-subspace best-first decision tree (RS-BFTree), and rotation-forest best-first decision tree (RF-BFTree) – for modelling GHs. We use the information-gain ratio method to analyze the relationships between GHS and 22 GH conditioning factors. The 4 ensemble outputs are validated using a receiver operating characteristic (ROC) curve. The areas under the curves (AUCs) for prediction rates of the ensemble methods applied to the training group are BFTree – 88.3%, Bag-BFTree – 92.7%, RS-BFTree – 95.7%, and RF-BFTree – 93.2%. The AUCs for the model-validation group cases, however, are 84.9%, 94.1%, 97.4%, and 9.18%, respectively. Therefore, RS-BFTree is, statistically, the most effective ensemble method for accurate modelling of GHs. Variable-importance analyses using information-gain ratio indicate that out of 22 GH-influential factors, land use, slope degree, and slope-length are of more importance in developing of GH occurrence. Finally, to address the need for detailed observations and highly accurate erosion data in the field, UAV image-acquisition technologies are demonstrated.}
}
@article{SARABAKHA2017361,
title = {Novel Levenberg–Marquardt based learning algorithm for unmanned aerial vehicles},
journal = {Information Sciences},
volume = {417},
pages = {361-380},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517308393},
author = {Andriy Sarabakha and Nursultan Imanberdiyev and Erdal Kayacan and Mojtaba Ahmadieh Khanesar and Hani Hagras},
keywords = {Fuzzy neural networks, Sliding mode control, Levenberg–Marquardt algorithm, Type-1 fuzzy logic control, Unmanned aerial vehicle},
abstract = {In this paper, Levenberg–Marquardt inspired sliding mode control theory based adaptation laws are proposed to train an intelligent fuzzy neural network controller for a quadrotor aircraft. The proposed controller is used to control and stabilize a quadrotor unmanned aerial vehicle in the presence of periodic wind gust. A proportional-derivative controller is firstly introduced based on which fuzzy neural network is able to learn the quadrotor’s control model on-line. The proposed design allows handling uncertainties and lack of modelling at a computationally inexpensive cost. The parameter update rules of the learning algorithms are derived based on a Levenberg–Marquardt inspired approach, and the proof of the stability of two proposed control laws are verified by using the Lyapunov stability theory. In order to evaluate the performance of the proposed controllers extensive simulations and real-time experiments are conducted. The 3D trajectory tracking problem for a quadrotor is considered in the presence of time-varying wind conditions.}
}
@article{ZHOU2021107076,
title = {Diagnosis of winter-wheat water stress based on UAV-borne multispectral image texture and vegetation indices},
journal = {Agricultural Water Management},
volume = {256},
pages = {107076},
year = {2021},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2021.107076},
url = {https://www.sciencedirect.com/science/article/pii/S0378377421003413},
author = {Yongcai Zhou and Congcong Lao and Yalong Yang and Zhitao Zhang and Haiying Chen and Yinwen Chen and Junying Chen and Jifeng Ning and Ning Yang},
keywords = {Unmanned aerial vehicle, Vegetation indices, Image texture, Stomatal conductance, Water stress},
abstract = {Timely and accurate detection of crop water stress is vital for precision irrigation. Whether the accuracy of the prevailing diagnosis of crop water stress using vegetation indices (VIs) and spectral reflectance can be improved still remains to be investigated. The crop surface characteristics such as grayscale or color vary under different water stress, so in this study one more variable, image texture, was utilized together to diagnose water stress. For this end, the canopy image of winter wheat in bloom was obtained by unmanned aerial vehicle (UAV) equipped with multispectral sensor, and the effect of soil background was eliminated using vegetation index threshold method. On this basis, Grey level co-occurrence matrix (GLCM) was used to calculate the mean (MEA), variance (VAR), homogeneity (HOM), contrast (CON), dissimilarity (DIS), entropy (ENT), second moment (SEC) and correlation (COR) of the image texture under different spatial resolutions (0.008 m, 0.01 m, 0.02 m, 0.05 m, 0.1 m and 0.2 m). Next, the canopy vegetation indices were obtained by mathematical transformation of canopy reflectance, and then sensitive image texture and vegetation indices by full subset regression method. Finally, Cubist, BPNN (Back Propagation Neural Network) and ELM (Extreme Learning Machine) methods were adopted to build the estimation models of the stomatal conductance (Gs) of winter wheat (between the sensitive image texture and Gs, and between vegetation index and Gs), and the water stress map was plotted based on the optimal Gs estimation model. The result showed: (i) the image texture obtained from the high-resolution multispectral image had a high correlation with Gs, and the image texture (VAR, HOM, CON, DIS, ENT and SEC) at 550 nm had the most significant correlation; (ii) the higher the ground resolution, the higher the correlation between the Gs and the image texture, the vegetation indices, respectively. The image texture with a ground resolution of 0.008 m combined with VIs and Gs had the highest correlation, and combining image texture and vegetation index can significantly improve the estimation accuracy of winter wheat Gs; (iii) Among the three estimation models, the BPNN model constructed by combining the image texture and VIs (MEA, VAR, ENT, DWSI and EXG) had the best estimation performance (Calibration:Rc2 = 0.899, RMSEc = 0.01, MAEc = 0.006; Validation:Rc2 = 0.834, RMSEv =;0.018, MAEv = 0.014), and an accurate estimation could even be achieved at a lower Gs value. Compared with the BPNN model solely based on VIs or image texture, the Rc2 of the BPNN model based on the combined variables increased by 24% and 22.48%, respectively. Therefore, combining UAV multispectral image texture and VIs to estimate Gs provides a feasible and accurate method for water stress diagnosis of winter wheat.}
}
@article{TRAN2021100156,
title = {Hybrid adaptive negative imaginary- neural-fuzzy control with model identification for a quadrotor},
journal = {IFAC Journal of Systems and Control},
volume = {16},
pages = {100156},
year = {2021},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2021.100156},
url = {https://www.sciencedirect.com/science/article/pii/S2468601821000110},
author = {Vu Phi Tran and Mohamed A. Mabrok and Matthew A. Garratt and Ian R. Petersen},
keywords = {Strictly Negative Imaginary controller, Neural-Fuzzy controller, Hybrid control, Online identification, Quadcopter unmanned aerial vehicle, Uncertainties},
abstract = {Quadrotor system is subject to multiple disturbances, including both internal and external effects (e.g. wind gusts, coupling effects, and unmodeled dynamics). For example, severe wind disturbances may significantly degrade trajectory tracking during the flight of autonomous aerial vehicles, or even cause loss of control or failure of a tracking mission. This paper introduces a robust hybrid control system, including a linear Strictly Negative Imaginary (SNI) controller and an adaptive nonlinear Neural-Fuzzy control law, to enable high-precision trajectory tracking tasks for a quadcopter drone. Based on a parallel form, both proposed controllers are able to enhance the transient performance, the system response, and the robustness of the quadcopter controllers. Also, a linear time-invariant SNI UAV dynamic model, in combination with an online adaptive residual nonlinear model using the neural network identification, is proposed to model the natural behavior of a quadcopter system. Through a series of numerical simulations, this paper highlights the effectiveness of our hybrid controller in the face of some parameter variations, such as nonlinear aerodynamic models and exogenous disturbances (e.g., wind gusts). Moreover, it compares its tracking performance with that of a fixed-gain SNI controller and the adaptive Neural-Fuzzy controller separately. Finally, the stability of the closed-loop control system is also discussed using the SNI theorem.}
}
@article{MOEINIZADE2022100233,
title = {An applied deep learning approach for estimating soybean relative maturity from UAV imagery to aid plant breeding decisions},
journal = {Machine Learning with Applications},
volume = {7},
pages = {100233},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100233},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021001171},
author = {Saba Moeinizade and Hieu Pham and Ye Han and Austin Dobbels and Guiping Hu},
keywords = {Soybean relative maturity, Prediction, Deep learning, Time series, Convolutional neural networks},
abstract = {For a global breeding organization, identifying the next generation of superior crops is vital for its success. Recognizing new genetic varieties requires years of in-field testing to gather data about the crop’s yield, pest resistance, heat resistance, etc. At the conclusion of the growing season, organizations need to determine which varieties will be advanced to the next growing season (or sold to farmers) and which ones will be discarded from the candidate pool. Specifically for soybeans, identifying their relative maturity is a vital piece of information used for advancement decisions. However, this trait needs to be physically observed, and there are resource limitations (time, money, etc.) that bottleneck the data collection process. To combat this, breeding organizations are moving towards advanced image capturing devices. In this paper, we develop a robust and automatic approach for estimating the relative maturity of soybeans using a time series of UAV images. An end-to-end hybrid model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) is proposed to extract features and capture the sequential behavior of time series data. The proposed deep learning model was tested on six different environments across the United States Results suggest the effectiveness of our proposed CNN-LSTM model compared to the local regression method. Furthermore, we demonstrate how this newfound information can be used to aid in plant breeding advancement decisions.}
}
@article{ZHANG2021117618,
title = {Retrieval of water quality parameters from hyperspectral images using a hybrid feedback deep factorization machine model},
journal = {Water Research},
volume = {204},
pages = {117618},
year = {2021},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2021.117618},
url = {https://www.sciencedirect.com/science/article/pii/S0043135421008137},
author = {Yishan Zhang and Lun Wu and Licui Deng and Bin Ouyang},
keywords = {Hyperspectral images, Water quality monitoring, Deep learning, Spectral unmixing, Spatial distribution analysis},
abstract = {Environmental protection of water resources is of critical importance to daily life of human beings. In recent years, monitoring the variation of water quality using remote sensing techniques has become prevalent. Unmanned aerial vehicle (UAV) based remote sensing techniques have been applied to quantitative retrieval of concentrations of water quality parameters including phosphorus, nitrogen, chemical oxygen demand (COD), biochemical oxygen demand (BOD), and chlorophyll a (Chl-a), successfully and efficiently. In this study, a novel method with deep factorization machine, spatial distribution pattern analysis, and probabilistic analysis engaged, named hybrid feedback deep factorization machine (HF-DFM), has been developed to quantitatively estimate concentrations of water quality parameters based on hyperspectral reflectance data on large scale effectively. Our proposed method is a unified model for quantifying concentrations of water quality parameters with an end to end structure, which integrates UAV based optical remote sensing techniques and deep learning to estimate concentrations of water quality parameters. Furthermore, our proposed model was applied to real-time quantitative monitoring the variation of water quality of Mazhou River, Shenzhen, Guangdong, China. Finally, we evaluate the performance of proposed model on a real-world dataset in terms of root of mean squared error (RMSE), mean absolute percent error (MAPE), and coefficient of determination (R2). The experimental results show that our proposed model outperforms other state-of-the-art models with respect to RMSE, MAPE, and R2, where resulting MAPEs for quantifying all water quality parameters range from 8.78% to 12.36%, and resulting R2s range from 0.81 to 0.93. It can serve as a useful tool for decision makers in effectively monitoring water quality of urban rivers.}
}
@article{ARUN2019431,
title = {Convolutional network architectures for super-resolution/sub-pixel mapping of drone-derived images},
journal = {Pattern Recognition},
volume = {88},
pages = {431-446},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2018.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0031320318304217},
author = {Pattathal V. Arun and Ittai Herrmann and Krishna M. Budhiraju and Arnon Karnieli},
keywords = {Sub-pixel mapping, Super-resolution, Convolutional neural network, Class distribution, Drone, UAV},
abstract = {Spatial resolution enhancement is a pre-requisite for integrating unmanned aerial vehicle (UAV) datasets with the data from other sources. However, the mobility of UAV platforms, along with radiometric and atmospheric distortions, makes the task difficult. In this paper, various convolutional neural network (CNN) architectures are explored for resolving the issues related to sub-pixel classification and super-resolution of drone-derived datasets. The main contributions of this work are: 1) network-inversion based architectures for super-resolution and sub-pixel mapping of drone-derived images taking into account their spectral-spatial characteristics and the distortions prevalent in them 2) a feature-guided transformation for regularizing the inversion problem 3) loss functions for improving the spectral fidelity and inter-label compatibility of coarser to finer-scale mapping 4) use of multi-size kernel units for avoiding over-fitting. The proposed approach is the first of its kind in using neural network inversion for super-resolution and sub-pixel mapping. Experiments indicate that the proposed super-resolution approach gives better results in comparison with the sparse-code based approaches which generally result in corrupted dictionaries and sparse codes for multispectral aerial images. Also, the proposed use of neural network inversion, for projecting spatial affinities to sub-pixel maps, facilitates the consideration of coarser-scale texture and color information in modeling the finer-scale spatial-correlation. The simultaneous consideration of spectral bands, as proposed in this study, gives better super-resolution results when compared to the individual band enhancements. The proposed use of different data-augmentation strategies, for emulating the distortions, improves the generalization capability of the framework. Sensitivity of the proposed super-resolution and sub-pixel mapping frameworks with regard to the network parameters is thoroughly analyzed. The experiments over various standard datasets as well as those collected from known locations indicate that the proposed frameworks perform better when compared to the prominent published approaches.}
}
@article{DAS2021108477,
title = {Evaluation of water status of wheat genotypes to aid prediction of yield on sodic soils using UAV-thermal imaging and machine learning},
journal = {Agricultural and Forest Meteorology},
volume = {307},
pages = {108477},
year = {2021},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2021.108477},
url = {https://www.sciencedirect.com/science/article/pii/S016819232100160X},
author = {Sumanta Das and Jack Christopher and Armando Apan and Malini Roy Choudhury and Scott Chapman and Neal W. Menzies and Yash P. Dang},
keywords = {Thermal remote sensing, Crop water stress, Classification and regression tree, Biomass yield, Grain yield, Sodic soils},
abstract = {Water stress limits wheat growth and the yield on rain-fed sodic soils. Appropriate selection of traits and novel methods are required to forecast yield and to identify water stress tolerant wheat genotypes on sodic soils. In this study, we proposed a thermal remote sensing and machine learning-based approach to help predict the biomass and grain yields of wheat genotypes grown with variable water stress in sodic soil environments. We employed unmanned aerial vehicle-based thermal imaging to quantify water stress of 18 contrasting wheat genotypes grown on moderately sodic (MS) and highly sodic (HS) soils in north-eastern grains growing regions of Australia and related these to ground-measured plant biomass and grain yields. We evaluated crop water stress indices; standardized canopy temperature index, crop water stress index, stomatal conductance index, vapour pressure deficit, and crop stress index, which were computed from thermal imagery and on-site agro-meteorological parameters close to flowering. We then employed a classification and regression tree (CRT) as a supervised machine learning algorithm to classify crop water stress and predict biomass and grain yields as a function of crop water stress indices. The CRT accurately predicted biomass yield (coefficient of determination (R2) = 0.86; root mean square error (RMSE) = 41.3 g/m2 and R2 = 0.75; RMSE = 47.7 g/m2 for the MS and HS site) and grain yield (R2 = 0.78; RMSE = 16.7 g/m2 and R2 = 0.69; RMSE = 23.2 g/m2 for the MS and HS site, respectively). High sodic soil constraints increased crop water stress more than moderately sodic constraints soil that limits wheat yield ~40%. Wheat genotypes; Bremer, Gregory, Lancer, Mace, and Mitch were more productive than Gladius, Flanker, Scout, Emu Rock, and Janz in sodic soil environments. The study improves our ability to develop decision-making tools to assist farmers and breeders in securing agricultural productivity on sodic soils.}
}
@article{DAS2022101540,
title = {Building of an edge enabled drone network ecosystem for bird species identification},
journal = {Ecological Informatics},
volume = {68},
pages = {101540},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101540},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003319},
author = {Nabanita Das and Neelamadhab Padhy and Nilanjan Dey and Amartya Mukherjee and Ananjan Maiti},
keywords = {Edge computing, Random forest, Songbird, Drone network, Bioacoustics monitoring},
abstract = {The behavioral study of animals and especially avians, and the way of their immunization are highly needed to understand the environment in a better way. Automatically classifying bird species by their vocalization is of crucial relevance for the research of ornithologists and ecologists. It was observed that impartial survey information for songbird species is inherently challenging due to observer biases, habitat insurance biases, and logistical constraints. To get to the bottom of all the challenges, ecologists are trying a machine that let them decide the distribution and density of species, which are essential baseline facts for conservation. For this reason, the utilization of a network of unmanned aerial vehicles is introduced for monitoring and capturing the data of a wide variety of terrestrial and aquatic species. In this study, an edge-enabled drone network has been engineered that amalgamated with the mobile edge computing framework within the drone network and the machine learning models to predict the bird species. The experiment has been performed in two geographic regions. The research reported 98.2% and 96.9% accuracy of random forest classifier with the, 0.07 and 0.4 log loss by utilizing 1.4% of CPU and 329.14 Mb of buffer memory of the edge device with an execution time of 45 milliseconds.}
}
@article{XU2021125691,
title = {Recognition of lane-changing behaviour with machine learning methods at freeway off-ramps},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {567},
pages = {125691},
year = {2021},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.125691},
url = {https://www.sciencedirect.com/science/article/pii/S0378437120309894},
author = {Ting Xu and Zhishun Zhang and Xingqi Wu and Long Qi and Yi Han},
keywords = {Lane-changing recognition, Convolutional neural network, Lane-changing behaviour, Freeway off-ramps},
abstract = {Crashes are occurred frequently at freeway off-ramps due to improper lane-changing (LC) behaviours. The LC behaviour is the main cause of freeway off-ramp crashes. It is important to warn the LC to reduce potential crashes. The uncertainty of LC behaviour increases the difficulties of predicting in advance. The off-ramps at Xi’an Raocheng freeway were chosen for investigation. The datasets were collected by the UAV. There was a total of 637 LC images extracted from the 200 minutes’ video stream. All LC behaviours were divided into twelve categories according to the changing direction and the influence of other vehicles in the target-lane or ego-lane. The machine learning technology is efficient in the image recognition. Thus, the vision technology was applied to devised a lane-changing recognition (LCR) model with the two-level convolutional neural network. A novel convolutional neural network based on the AlexNet was also proposed to compare with the LCR model. All samples were divided into a training dataset and a testing dataset for two models. The performance of two machines networks was compared. The training average accuracy was above 94.6% with the LCR model. The LCR model outperformed the model based on the AlexNet which was only 73.97% on average.}
}
@article{SHAFIEE2021106036,
title = {Sequential forward selection and support vector regression in comparison to LASSO regression for spring wheat yield prediction based on UAV imagery},
journal = {Computers and Electronics in Agriculture},
volume = {183},
pages = {106036},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106036},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921000545},
author = {Sahameh Shafiee and Lars Martin Lied and Ingunn Burud and Jon Arne Dieseth and Muath Alsheikh and Morten Lillemo},
keywords = {Machine learning, Support Vector Regression (SVR), SFS (Sequential Forward Selection), LASSO, Yield, Wheat phenotyping},
abstract = {Traditional plant breeding based on selection for grain yield is time-consuming and costly; therefore, new innovative methods are in high demand to reduce costs and accelerate genetic gains. Remote sensing-based platforms such as unmanned aerial vehicles (UAV) show promise to predict different traits including grain yield. Attention is currently being devoted to machine learning methods in order to extract the most meaningful information from the massive amounts of data generated by UAV images. These methods have shown a promising capability to come up with nonlinearity and explore patterns beyond the human ability. This study investigates the application of two different machine learning based regressor methods to predict wheat grain yield using extracted vegetation indices from UAV images. The goal of the study was to investigate the strength of Support Vector Regression (SVR) in combination with Sequential Forward Selection (SFS) for grain yield prediction and compare the results with LASSO regressor with an internal feature selector. Models were tested on grain yield data from 600 plots of spring wheat planted in South-Eastern Norway in 2018. Five spectral bands along with three different vegetation indices; the Normalized Difference Vegetation Index (NDVI), Enhanced Vegetation Index (EVI), and MERIS Terrestrial Chlorophyll Index (MTCI) were extracted from multispectral images at three dates between heading and maturity of the plants. These features for each field trial plot at each date were used as input data for the SVR model. The best model hyperparameters were estimated using grid search. Based on feature selection results from both methods, NDVI showed the highest prediction ability for grain yield at all dates and its explanatory power increased toward maturity, while adding MTCI and EVI at earlier stages of grain filling improved model performance. Combined models based on all indices and dates explained up to 90% of the variation in grain yield on the test set. Inclusion of individual bands added collinearity to the models and did not improve the predictions. Although both regression methods showed a good capability for grain yield prediction, LASSO regressor proved to be more affordable and economical in terms of time.}
}
@article{HU2021112757,
title = {Utilizing unsupervised learning, multi-view imaging, and CNN-based attention facilitates cost-effective wetland mapping},
journal = {Remote Sensing of Environment},
volume = {267},
pages = {112757},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112757},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721004776},
author = {Qiao Hu and Wayne Woldt and Christopher Neale and Yuzhen Zhou and Jeff Drahota and Dana Varner and Andy Bishop and Ted LaGrange and Ligang Zhang and Zhenghong Tang},
keywords = {Wetland mapping, Semantic segmentation, Multiscale CNN, Attention mechanism, Multi-view, UAV, Feature selection, Automation, Network pruning, Deep learning},
abstract = {The combination of Unmanned/Unoccupied Aerial Vehicle (UAV) data and deep learning, especially convolutional neural networks (CNNs), offers robust new tools for precision land cover mapping. However, its successful application is highly dependent on local experiences that are rarely documented, resulting in practical limitations during implementation. Cost-effective deep learning frameworks for fast deployment are required. This study presents a deep learning adaptation framework, named Auto-UNet++, trying to streamline wetland mapping tasks (including training data labeling and organizing). The framework treats mapping tasks as an intact semantic segmentation pipeline and then integrates automatic strategies into each step to reduce human intervention. These automatic strategies are achieved by standard computer vision techniques, including multi-view (MV) imaging—highly overlapped UAV images over an area (for labeling/voting), unsupervised clustering (for labeling), multi-scale CNN (for feature extraction), and attention mechanism—a CNN design used to select informative features from input (for feature exploration/selection). The framework was tested on playa wetland mapping in the Rainwater Basin, Nebraska, USA, with multispectral UAV datasets. Generally, the multi-scale CNN mapping task achieved a high of 87% overall accuracy and over 90% accuracy in water delineation. The results indicate that the multi-view and attention strategies have the potential to improve segmentation performance, and together with unsupervised learning, save considerable labor/expertise. Interestingly, evidence shows that the band/scale attention (weight) is adaptively associated with the land cover percentages per input image, indicating spatial contexts captured. This finding highlights the potential usages of the attention rule in automatic feature exploration, selection, and model interpretation. The framework illustrating a highly automated deep learning deployment on small MV datasets facilitates cost-effective wetland cover mapping. Although limitations exist, the study demonstrated the possibility of where/how conventional segmentation pipelines can be improved in typical UAV wetland mapping tasks. The framework and findings are useful for similar applications (including non-UAV studies) that only have limited time, labor, and expertise to implement sophisticated semantic segmentation models.}
}
@article{ZHANG2018368,
title = {Intelligent GNSS/INS integrated navigation system for a commercial UAV flight control system},
journal = {Aerospace Science and Technology},
volume = {80},
pages = {368-380},
year = {2018},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2018.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S127096381830600X},
author = {Guohao Zhang and Li-Ta Hsu},
keywords = {UAV, GPS, Navigation, Kalman filter, Adaptive tuning, Machine learning},
abstract = {Owing to the increase in civil applications using quadcopters, commercial flight control systems such as Pixhawk are a popular solution to provide the sensing and control functions of an unmanned aerial vehicle (UAV). A low-cost global navigation satellite system (GNSS) receiver is crucial for the low-cost flight control system. However, the accuracy of GNSS positioning is severely degraded by the notorious multipath effect in mega-urbanized cities. The multipath effect cannot be eliminated but can be mitigated; hence, the GNSS/inertial navigation system (INS) integrated navigation is a popular approach to reduce this error. This study proposes an adaptive Kalman filter for adjusting the noise covariance of GNSS measurements under different positioning accuracies. The adaptive tuning is based on a proposed accuracy classification model trained by a supervised machine-learning method. First, principal component analysis is employed to identify the significant GNSS accuracy related features. Subsequently, the positioning accuracy model is trained based on a random forest learning algorithm with the labeled real GNSS dataset encompassing most scenarios concerning modern urban areas. To reduce the cases of misclassifying the GNSS accuracy, a fuzzy logic algorithm is employed to consider the GNSS accuracy propagation. Additionally, the process noise covariance of the INS is determined using the Allan variance analysis. The positioning performance of the proposed adaptive Kalman filter is compared with both a conventional Kalman filter and the positioning solution provided by the commercial flight control system, Pixhawk 2. The results show that the proposed adaptive Kalman filter using random forest with fuzzy logic can achieve a better classification of GNSS accuracy compared to the others. The overall positioning result improved by approximately 50% compared with the onboard solution.}
}
@article{ZHANG2021106174,
title = {Evaluating the sensitivity of water stressed maize chlorophyll and structure based on UAV derived vegetation indices},
journal = {Computers and Electronics in Agriculture},
volume = {185},
pages = {106174},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106174},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921001915},
author = {Liyuan Zhang and Wenting Han and Yaxiao Niu and José L. Chávez and Guomin Shao and Huihui Zhang},
keywords = {Stomatal conductance, Chlorophyll content, Leaf area index, Random forest, Artificial neural networks},
abstract = {To further assess the sensitivity of crop chlorophyll and structure based on UAV vegetation indices (VIs) to maize water stress, a study was carried out in a maize field located in Inner Mongolia, China, with various levels of deficit irrigation over the entire 2018 and 2019 growing seasons. Ground measurements of stomatal conductance (Gs), leaf area index and leaf chlorophyll were used as references for maize water status, canopy structure and chlorophyll content, respectively. Four structure VIs and two chlorophyll VIs, and three regression algorithms (multiple linear, random forest and artificial neural networks regression) were adopted. The results showed that canopy structure derived from VIs had a significant correlation (p < 0.001) with Gs with the highest r value of 0.64 (n = 270) in 2018 and 2019. The transformed chlorophyll absorption in reflectance index, chlorophyll VI, could only estimate severe maize water stress with an r value of −0.47 (p < 0.001, n = 270) for the drier 2019. The water stress sensitivity of chlorophyll and structure VIs maybe significantly influenced by different responses of canopy structure and chlorophyll concentration to water stress, and the different spectral resolution of UAV multispectral cameras. Compared to non-linear machine learning regression algorithms, the multiple linear regression was robust enough to relate UAV-based multispectral VIs to Gs with coefficients of determination of 0.48 and 0.45 (n = 270) for 2018 and 2019, respectively. Although stable significant correlations were found between UAV multispectral VIs and Gs, annual changes in these specific expressions were also observed. Overall, our results demonstrated the potential of using structure VIs derived from UAV multispectral images and multiple linear regression approach to estimate maize water status in field scale.}
}
@article{FAUST2017381,
title = {Automated aerial suspended cargo delivery through reinforcement learning},
journal = {Artificial Intelligence},
volume = {247},
pages = {381-398},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001416},
author = {Aleksandra Faust and Ivana Palunko and Patricio Cruz and Rafael Fierro and Lydia Tapia},
keywords = {Reinforcement learning, UAVs, Aerial cargo delivery, Probabilistic roadmaps, Motion planning, Trajectory planning, Robotics, Rotorcraft},
abstract = {Cargo-bearing unmanned aerial vehicles (UAVs) have tremendous potential to assist humans by delivering food, medicine, and other supplies. For time-critical cargo delivery tasks, UAVs need to be able to quickly navigate their environments and deliver suspended payloads with bounded load displacement. As a constraint balancing task for joint UAV-suspended load system dynamics, this task poses a challenge. This article presents a reinforcement learning approach for aerial cargo delivery tasks in environments with static obstacles. We first learn a minimal residual oscillations task policy in obstacle-free environments using a specifically designed feature vector for value function approximation that allows generalization beyond the training domain. The method works in continuous state and discrete action spaces. Since planning for aerial cargo requires very large action space (over 106 actions) that is impractical for learning, we define formal conditions for a class of robotics problems where learning can occur in a simplified problem space and successfully transfer to a broader problem space. Exploiting these guarantees and relying on the discrete action space, we learn the swing-free policy in a subspace several orders of magnitude smaller, and later develop a method for swing-free trajectory planning along a path. As an extension to tasks in environments with static obstacles where the load displacement needs to be bounded throughout the trajectory, sampling-based motion planning generates collision-free paths. Next, a reinforcement learning agent transforms these paths into trajectories that maintain the bound on the load displacement while following the collision-free path in a timely manner. We verify the approach both in simulation and in experiments on a quadrotor with suspended load and verify the method's safety and feasibility through a demonstration where a quadrotor delivers an open container of liquid to a human subject. The contributions of this work are two-fold. First, this article presents a solution to a challenging, and vital problem of planning a constraint-balancing task for an inherently unstable non-linear system in the presence of obstacles. Second, AI and robotics researchers can both benefit from the provided theoretical guarantees of system stability on a class of constraint-balancing tasks that occur in very large action spaces.}
}
@article{MAIMAITIJIANG2020111599,
title = {Soybean yield prediction from UAV using multimodal data fusion and deep learning},
journal = {Remote Sensing of Environment},
volume = {237},
pages = {111599},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.111599},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719306194},
author = {Maitiniyazi Maimaitijiang and Vasit Sagan and Paheding Sidike and Sean Hartling and Flavio Esposito and Felix B. Fritschi},
keywords = {Remote sensing, Yield prediction, Multimodality, Data fusion, Deep learning, Phenotyping, Spatial autocorrelation},
abstract = {Preharvest crop yield prediction is critical for grain policy making and food security. Early estimation of yield at field or plot scale also contributes to high-throughput plant phenotyping and precision agriculture. New developments in Unmanned Aerial Vehicle (UAV) platforms and sensor technology facilitate cost-effective data collection through simultaneous multi-sensor/multimodal data collection at very high spatial and spectral resolutions. The objective of this study is to evaluate the power of UAV-based multimodal data fusion using RGB, multispectral and thermal sensors to estimate soybean (Glycine max) grain yield within the framework of Deep Neural Network (DNN). RGB, multispectral, and thermal images were collected using a low-cost multi-sensory UAV from a test site in Columbia, Missouri, USA. Multimodal information, such as canopy spectral, structure, thermal and texture features, was extracted and combined to predict crop grain yield using Partial Least Squares Regression (PLSR), Random Forest Regression (RFR), Support Vector Regression (SVR), input-level feature fusion based DNN (DNN-F1) and intermediate-level feature fusion based DNN (DNN-F2). The results can be summarized in three messages: (1) multimodal data fusion improves the yield prediction accuracy and is more adaptable to spatial variations; (2) DNN-based models improve yield prediction model accuracy: the highest accuracy was obtained by DNN-F2 with an R2 of 0.720 and a relative root mean square error (RMSE%) of 15.9%; (3) DNN-based models were less prone to saturation effects, and exhibited more adaptive performance in predicting grain yields across the Dwight, Pana and AG3432 soybean genotypes in our study. Furthermore, DNN-based models demonstrated consistent performance over space with less spatial dependency and variations. This study indicates that multimodal data fusion using low-cost UAV within a DNN framework can provide a relatively accurate and robust estimation of crop yield, and deliver valuable insight for high-throughput phenotyping and crop field management with high spatial precision.}
}
@article{HE2022106815,
title = {Angular effect of algorithms for monitoring leaf nitrogen concentration of wheat using multi-angle remote sensing data},
journal = {Computers and Electronics in Agriculture},
volume = {195},
pages = {106815},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106815},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001326},
author = {Li He and Meng-Ran Liu and Yu-Long Guo and Yong-Kang Wei and Hai-Yan Zhang and Xiao Song and Wei Feng and Tian-Cai Guo},
keywords = {Multi-angular hyperspectral, Angle affect, Leaf nitrogen concentration, Monitoring accuracy, Algorithms},
abstract = {Leaf nitrogen concentration (LNC) is a key indicator of crops’ growth status, and the timely and accurate large-scale area monitoring of LNC is crucial for guiding field management. Here, we collected LNC data and hyperspectral data at the canopy scale, from 13 view zenith angles (VZAs) and two locations (Zhengzhou and Shangshui, in China) across 2 years (2012–2014). Four data processing methods—vegetation indices (VIs), back-propagation neural network (BPNN), eXtreme gradient boost (XGBoost), and partial least squares regression (PLSR)—were applied and compared for their ability to estimate LNC under 13 VZAs. These results revealed consistent trends in the performance of the four algorithms at 13 VZAs: the nadir direction is better than the extreme angle, and the best performance is obtained for the range of –30° to 0° (R2 ≥ 0.83; RMSE ≤ 0.41%). The number of bands is a critical factor affecting the accuracy of LNC monitoring: including red edge bands can alleviate angular effect to some extent. The accuracy of PLSR for monitoring LNC is not only superior near the nadir direction (R2 = 0.91), but also better than that provided VIs (16%–17%), BPNN (15%–16%), or XGBoost (29%–58%) at ± 60°. Therefore, it is strongly recommended the PLSR algorithm be used to process multi-angle remote sensing data to estimate the LNC of field crops. This work provides a timely reference basis for selecting the appropriate flight angle and the number and positioning of bands for unmanned aerial vehicle (UAV) and satellite applications in the future.}
}
@article{RAMONSORIA202077,
title = {Grasp Planning and Visual Servoing for an Outdoors Aerial Dual Manipulator},
journal = {Engineering},
volume = {6},
number = {1},
pages = {77-88},
year = {2020},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2019.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2095809919308653},
author = {Pablo Ramon-Soria and Begoña C. Arrue and Anibal Ollero},
keywords = {Aerial manipulation, Grasp planning, Visual servoing},
abstract = {This paper describes a system for grasping known objects with unmanned aerial vehicles (UAVs) provided with dual manipulators using an RGB-D camera. Aerial manipulation remains a very challenging task. This paper covers three principal aspects for this task: object detection and pose estimation, grasp planning, and in-flight grasp execution. First, an artificial neural network (ANN) is used to obtain clues regarding the object’s position. Next, an alignment algorithm is used to obtain the object’s six-dimensional (6D) pose, which is filtered with an extended Kalman filter. A three-dimensional (3D) model of the object is then used to estimate an arranged list of good grasps for the aerial manipulator. The results from the detection algorithm—that is, the object’s pose—are used to update the trajectories of the arms toward the object. If the target poses are not reachable due to the UAV’s oscillations, the algorithm switches to the next feasible grasp. This paper introduces the overall methodology, and provides the experimental results of both simulation and real experiments for each module, in addition to a video showing the results.}
}
@article{PETTI2022106734,
title = {Weakly-supervised learning to automatically count cotton flowers from aerial imagery},
journal = {Computers and Electronics in Agriculture},
volume = {194},
pages = {106734},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106734},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922000515},
author = {Daniel Petti and Changying Li},
keywords = {Multiple-instance learning, Active learning, Object counting, High-throughput phenotyping},
abstract = {Counting plant flowers is a common task with applications for estimating crop yields and selecting favorable genotypes. Typically, this requires a laborious manual process, rendering it impractical to obtain accurate flower counts throughout the growing season. The model proposed in this study uses weak supervision, based on Convolutional Neural Networks (CNNs), which automates such a counting task for cotton flowers using imagery collected from an unmanned aerial vehicle (UAV). Furthermore, the model is trained using Multiple Instance Learning (MIL) in order to reduce the required amount of annotated data. MIL is a binary classification task in which any image with at least one flower falls into the positive class, and all others are negative. In the process, a novel loss function was developed that is designed to improve the performance of image-processing models that use MIL. The model is trained on a large dataset of cotton plant imagery which was collected over several years and will be made publicly available. Additionally, an active-learning-based approach is employed in order to generate the annotations for the dataset while minimizing the required amount of human intervention. Despite having minimal supervision, the model still demonstrates good performance on the testing dataset. Multiple models were tested with different numbers of parameters and input sizes, achieving a minimum average absolute count error of 2.43. Overall, this study demonstrates that a weakly-supervised model is a promising method for solving the flower counting problem while minimizing the human labeling effort.}
}
@article{WANG2019107665,
title = {Landscape-level vegetation classification and fractional woody and herbaceous vegetation cover estimation over the dryland ecosystems by unmanned aerial vehicle platform},
journal = {Agricultural and Forest Meteorology},
volume = {278},
pages = {107665},
year = {2019},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2019.107665},
url = {https://www.sciencedirect.com/science/article/pii/S0168192319302734},
author = {Haozhou Wang and Dong Han and Yue Mu and Lina Jiang and Xueling Yao and Yongfei Bai and Qi Lu and Feng Wang},
keywords = {Dryland vegetation, Machine learning, Decision tree model, Digital orthophoto map, Otindag sandy land, Semi-arid ecosystem, Classification and regression tree ()},
abstract = {The change of fraction vegetation cover (FVC) is the key ecological index for vegetation dynamics of dryland ecosystem. However, it is difficult to directly map woody vegetation and herbaceous vegetation in the dryland from the satellite images due to the mixture of their distribution at small scale. Emerging UAV remote sensing provides a good opportunity to capture and quantify the distribution of the sparse vegetation in the drylands ecosystem. In this study, we proposed a new method to classify woody vegetation and herbaceous vegetation and calculate their FVC based on the high-resolution orthomosaic generated from UAV images by the machine learning algorithm of classification and regression tree (CART). This proposed method was validated and evaluated by visual interpretation, the detailed ground measurement dataset of 4832 trees and 18,798 shrubs and three popular machine learning algorithms of Support Vector Machine(SVM), Random Forest(RF), Gradient Boosting Decision Tree(GBDT). The overall assessments showed good overall accuracy (0.78), average accuracy (0.76), and the Kappa coefficient (0.64). The FVC of woody vegetation calculated from orthomosaic agreed well with that estimated from ground measurements. Both group of FVC have a stable linear relationship over different spatial scales. The proposed method showed higher efficiency of 166%, 111% and 290% than SVM, RF, GBDT respectively. A new optimized model was developed to reduce the workload of vegetation investigation and to design more efficient sampling strategies. The proposed method was incorporated into an interactive web-based software “UAV- High Resolution imagery Analysis Platform” (UAV-HiRAP, http://www.uav-hirap.org). Our study demonstrates that UAV-HiRAP combined with UAV platform can be a powerful tool to classify woody vegetation and herbaceous vegetation and calculate their FVC for sparse vegetation in the drylands. The new optimization model will inspire researchers to design more effective sampling strategies for future field investigation.}
}
@article{SEROV2021257,
title = {A neuro-evolutionary synthesis of coordinated stable-effective compromises in hierarchical systems under conflict and uncertainty},
journal = {Procedia Computer Science},
volume = {186},
pages = {257-268},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.145},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921009613},
author = {V.A. Serov and E.M. Voronov and D.A. Kozlov},
keywords = {FANET (Flying Ad Hoc Network) remote monitoring system, unmanned aerial vehicle, neural network control, neural network of radial basis functions, hierarchical game under uncertainty with the right of the first move, coordinated stable-effective compromise, robast quality assurance},
abstract = {The main principles of neuro-evolutionary technology for hierarchical systems control optimization under conflict and uncertainty are considered. The problem statement is formulated as a hierarchical game under uncertainty with the right first move, which uses the principle of a coordinated stable-effective compromises generalizing the Stackelberg hierarchical equilibrium principle. To construct the optimal control law, the evolutionary synthesis of coordinated stable-effective compromises method is applied. An artificial neural network based on radial-basis functions multi-criteria synthesis method that implements the optimal control law in real time is developed. The problem of a hierarchical FANET-based (Flying Ad Hoc Network) remote monitoring system control optimizing in real time is solved.}
}
@article{JAISWAL2016296,
title = {Adaptive Longitudinal Control of UAVs with Direct Lift Control},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {1},
pages = {296-301},
year = {2016},
note = {4th IFAC Conference on Advances in Control and Optimization of Dynamical Systems ACODS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.03.069},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316300696},
author = {Ravi Jaiswal and Abhishek Shastry and Swati Swarnkar and Mangal Kothari},
keywords = {DLC, Flap, Dynamic inversion, Adaptive control, Neural network},
abstract = {In this paper, a nonlinear control technique based on Direct Lift Control (DLC) is proposed to control the longitudinal dynamics of unmanned aerial vehicles. The baseline controller is designed using a nonlinear dynamic inversion technique. As the effectiveness of the baseline controller depends on the knowledge of aircraft dynamic model and aerodynamic coefficients, which is difficult to be found accurately for the whole flight regime, the baseline controller is augmented with a neuro-adaptive controller. The approach uses a single layer neural network to learn the unknown dynamics and an adaptive law is employed to ensure that the UAV behaves in the desired manner. Lyapunov theory is used to show that the approximated dynamics remains bounded. Simulations results are presented to demonstrate the effectiveness of the proposed design on a six degrees of freedom model.}
}
@article{NAVARRO201961,
title = {Sense and Avoid using Hybrid Convolutional and Recurrent Neural Networks},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {12},
pages = {61-66},
year = {2019},
note = {21st IFAC Symposium on Automatic Control in Aerospace ACA 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.070},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319310055},
author = {Daniel Vidal Navarro and Chang-Hun Lee and Antonios Tsourdos},
keywords = {Sense, Avoid, neural networks, deep learning, computer vision, Kalman filter, range estimation, UAV},
abstract = {This work develops a Sense and Avoid strategy based on a deep learning approach to be used by UAVs using only one electro-optical camera to sense the environment. Hybrid Convolutional and Recurrent Neural Networks (CRNN) are used for object detection, classification and tracking whereas an Extended Kalman Filter (EKF) is considered for relative range estimation. Probabilistic conflict detection and geometric avoidance trajectory are considered for the last stage of this technique. The results show that the considered deep learning approach can work faster than other state-of-the-art computer vision methods. They also show that the collision can be successfully avoided considering design parameters that can be adjusted to adapt to different scenarios.}
}
@article{FERDAUS2019313,
title = {Online identification of a rotary wing Unmanned Aerial Vehicle from data streams},
journal = {Applied Soft Computing},
volume = {76},
pages = {313-325},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618307026},
author = {Md Meftahul Ferdaus and Mahardhika Pratama and Sreenatha G. Anavatti and Matthew A. Garratt},
keywords = {Evolving, Fuzzy, Metacognitive, Online identification, Quadcopter, Scaffolding},
abstract = {Until now the majority of the neuro and fuzzy modeling and control approaches for rotary wing Unmanned Aerial Vehicles (UAVs), such as the quadrotor, have been based on batch learning techniques, therefore static in structure, and cannot adapt to rapidly changing environments. Implication of Evolving Intelligent System (EIS) based model-free data-driven techniques in fuzzy system are good alternatives, since they are able to evolve both their structure and parameters to cope with sudden changes in behavior, and performs perfectly in a single pass learning mode which is suitable for online real-time deployment. The Metacognitive Scaffolding Learning Machine (McSLM) is seen as a generalized version of EIS since the metacognitive concept enables the what-to-learn, how-to-learn, and when-to-learn scheme, and the scaffolding theory realizes a plug-and-play property which strengthens the online working principle of EISs. This paper proposes a novel online identification scheme, applied to a quadrotor using real-time experimental flight data streams based on McSLM, namely Metacognitive Scaffolding Interval Type 2 Recurrent Fuzzy Neural Network (McSIT2RFNN). Our proposed approach demonstrated significant improvements in both accuracy and complexity against some renowned existing variants of the McSLMs and EISs.}
}
@article{GUERBER2021102940,
title = {Machine Learning and Software Defined Network to secure communications in a swarm of drones},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102940},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102940},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001551},
author = {Christophe Guerber and Mickaël Royer and Nicolas Larrieu},
keywords = {FANET, SDN, AODV, Security architecture, Machine Learning, Random Forest Classifier},
abstract = {As drones become more and more frequent in industry and perhaps tomorrow in everyday life, the variety and sensitivity of their missions will increase. Securing the communication taking place with the drones and especially in the network of a swarm, is of primary importance to allow a safe integration of Unmanned Aerial Vehicles into air traffic. Drones are subject to a range of attacks, from GPS jamming to application bug exploits. Among these attacks, and irrespective to whether they have already been implemented or not, communication is one of the main contributors, both as a vector and as a target. In this article, we use previous work on security threats concerning drones to identify two main types of attack in a network of drones: intrusion from the outside and network usage from inside. We demonstrate the robustness of the Software Defined Network (SDN) architecture facing most common attacks from the outside. In addition, we propose a traffic injection detection technique and corresponding countermeasures based on SDN flow counters. Finally, we present an innovative machine learning solution based on Random Forest Classifier to address insider attacks, relying solely on flow creation events. We propose two specific features that characterizes the activity in the network. They allow detecting common network attacks such as denial of service, port scanning and brute force and are easily available to the controller. Detection performance of these abnormal behaviors are promising, both in terms of true positive and false negative, and in terms of detection delay. Detection of these common attacks will allow tightening of security in such wireless network by denying further access to the network by rogue nodes.}
}
@article{HAN2022106804,
title = {An explainable XGBoost model improved by SMOTE-ENN technique for maize lodging detection based on multi-source unmanned aerial vehicle images},
journal = {Computers and Electronics in Agriculture},
volume = {194},
pages = {106804},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106804},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001211},
author = {Liang Han and Guijun Yang and Xiaodong Yang and Xiaoyu Song and Bo Xu and Zhenhai Li and Jintao Wu and Hao Yang and Jianwei Wu},
keywords = {Lodging, XGBoost, SHAP, Remote sensing, SMOTE},
abstract = {Remote sensing image is becoming an increasingly popular tool for crop lodging detection because it conveniently provides features for building machine learning models and predicting lodging. However, difficulties in interpreting machine learning models and their predictions limit the confidence of using remote sensing images to detect lodging. In addition, the lodging datasets used for modeling are difficult to balance under natural conditions. Designing a robust and interpretable classification model for the detection of lodging in an imbalanced distribution dataset poses a particularly difficult challenge. In this study, visible and multi-spectral images were collected with a UAV to extract relevant features from remote sensing images. In a preliminary step, Synthetic Minority Oversampling Technique (SMOTE) and Edited Nearest Neighbors (ENN) method were used to treat imbalanced datasets. The SMOTE-ENN-XGBoost model is proposed for the efficient identification of maize lodging at the plot scale. The SMOTE-ENN-XGBoost model achieved an F1-score of 0.930 and a recall of 0.899 on a testing set, suggesting that it can be used for modeling lodging detection. Additionally, the SHapley Additive exPlanations (SHAP) approach was employed to interpret the identification and prioritization of features that determine lodging classification and activity prediction. The results showed that canopy structure and textural features are relatively stable compared with spectral features, which are susceptible to the external environment when modeling is employed to detect lodging. This work also showed that canopy structural, spectral, and textural information should be considered simultaneously rather than separately when detecting crop lodging in a crop breeding program in order to prevent differences in expression controlled by the interaction between genotype and environment obscuring the change in a single feature before and after lodging. For practical applications of machine learning models in crop lodging detection, such insights are of critical relevance. Taken together, the results of this study encourage further applications of remote sensing techniques to build interpretable machine learning models.}
}
@article{ZHOU2020105369,
title = {Detection of ground straw coverage under conservation tillage based on deep learning},
journal = {Computers and Electronics in Agriculture},
volume = {172},
pages = {105369},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105369},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919325517},
author = {Deyi Zhou and Mao Li and Yang Li and Jiangtao Qi and Kai Liu and Xu Cong and Xinliang Tian},
keywords = {Straw coverage, Conservation tillage, Semantic segmentation, Remote sensing, U-Net},
abstract = {Given the technical requirement for the fast acquisition of ground straw coverage from conservation tillage fields, we investigated the accurate detection of straw coverage by using unmanned aerial vehicle (UAV) low-altitude remote sensing images. Ground images of farmlands under conservation tillage were acquired using low-altitude UAV. A semantic segmentation algorithm of straw coverage was established with the improved U-Net on ResNet18. The algorithm was evaluated and compared with VGG11–U-Net (the algorithm with VGG11 as the U-Net extraction module), triangle algorithm (TRIANGLER) and ‘RGB, HSV and grey + support vector machine (SVM)’ (the algorithm based on multicolour space information fusion combined with SVM). The mean absolute deviation of field straw coverage determined by the new algorithm was 3.56%, which was lower than that by other algorithms. Thus, the new algorithm offers data for the rapid acquisition of ground straw coverage from farmlands under conservation tillage.}
}
@article{LIU20202430,
title = {High precision detection algorithm based on improved RetinaNet for defect recognition of transmission lines},
journal = {Energy Reports},
volume = {6},
pages = {2430-2440},
year = {2020},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352484720312907},
author = {Jun Liu and Rong Jia and Wei Li and Fuqi Ma and Heba M. Abdullah and Hengrui Ma and Mohamed A. Mohamed},
keywords = {Transmission line defects, Convolutional neural network, RetinaNet, DenseNet, Intelligent identification},
abstract = {The Unmanned Aerial Vehicle (UAV) inspection mode has been gradually implemented in the power system. The UAV inspection image is checked by the target detection technology, but there is no high-precision target detection algorithm as the technical support. In this regard, this paper proposes a target detection algorithm based on the improved RetinaNet which is suitable for transmission lines defect detection. In this algorithm, the shortcomings of the RetinaNet anchor frame extraction mechanism based on Apriori are corrected. At the same time, the number and size of anchor frames are redesigned by using the improved K-means + + algorithm, so that the anchor frame of the improved algorithm gets the highest average IoU (Intersection over Union) value, which matches the actual size of the transmission line defects. Then, in RetinaNet, the feature pyramid network based on DenseNet is built as the backbone network to improve the model accuracy and make the model lighter. The improved model is trained and tested by using the data set of transmission line defects for validation. The results show that the proposed method has advantages and effectiveness in the detection of transmission line defects, and meets the requirements of intelligent inspection in terms of accuracy.}
}
@article{ANDARU2021107255,
title = {The use of UAV remote sensing for observing lava dome emplacement and areas of potential lahar hazards: An example from the 2017–2019 eruption crisis at Mount Agung in Bali},
journal = {Journal of Volcanology and Geothermal Research},
volume = {415},
pages = {107255},
year = {2021},
issn = {0377-0273},
doi = {https://doi.org/10.1016/j.jvolgeores.2021.107255},
url = {https://www.sciencedirect.com/science/article/pii/S0377027321000846},
author = {Ruli Andaru and Jiann-Yeou Rau and Devy Kamil Syahbana and Ardy Setya Prayoga and Heruningtyas Desi Purnamasari},
keywords = {UAV remote sensing, Lava dome emplacement, Mount Agung, Potential lahar hazard, Eruptive crisis},
abstract = {Mount Agung (the highest volcano in Bali, Indonesia) began to erupt on November 21, 2017, after having been dormant for 53 years. More than 100,000 people were evacuated within the hazard zone between September 2017 (when the highest volcanic alert was issued) and early 2018. The eruptions continued until June 2019, accompanied by at least 110 explosions. During the eruptive crisis, the observation of the lava dome's emplacement was essential for mitigating the potential hazard. Details of the lava dome growth, including the volumetric changes and effusion rates, provide valuable information about potential eruption scenarios and lahar depositions. In this paper, the essential role of multi-temporal unmanned aerial vehicle (UAV) images in the monitoring of Mt. Agung's lava dome, and in determining the areas of potential lahar hazards during the crisis between 2017 and 2019 is described. A fixed-wing UAV was launched outside the hazard zone to photograph the lava dome on five occasions. Image enhancement, machine learning, and photogrammetry were combined to improve image quality, remove point clouds outliers, and generate digital terrain models (DTMs) and orthoimages. The analysis of the obtained DTMs and orthoimages resulted in qualitative and quantitative data highlighting the changes inside the crater and on the surrounding slopes. These results reveal that, from November 25 to December 16, 2017, the lava dome grew vertically by 126 m and reached a volume of 26.86 ± 0.64 × 106 m3. In addition, its surface experienced a maximal uplift of approximately 52 m until July 2019 with the emergence of a new dome with a volume estimated at 9.52 ± 0.086 × 106 m3. The difference between the DTMs of 2017 and 2019 reveals the total volume of erupted material (886,100 ± 8000 m3) that was deposited on the surrounding slopes. According to the lahar inundation simulation, the deposited material may cause dangerous lahars in 21 drainages, which extend in the north (N), north-east (N-E), south (S), south-east (S-E), and south-west (S-W) sectors of the volcano. This paper presents the use of UAV remote sensing for the production of high-spatial resolution DTMs, which can be used to both observe the emplacement of a lava dome, and to identify areas with potential lahar risk during a volcano crisis.}
}
@article{REY2017341,
title = {Detecting animals in African Savanna with UAVs and the crowds},
journal = {Remote Sensing of Environment},
volume = {200},
pages = {341-351},
year = {2017},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717303942},
author = {Nicolas Rey and Michele Volpi and Stéphane Joost and Devis Tuia},
keywords = {Animal conservation, Wildlife monitoring, Object detection, Active learning, Crowd-sourcing data, Unmanned aerial vehicles, Very high resolution},
abstract = {Unmanned aerial vehicles (UAVs) offer new opportunities for wildlife monitoring, with several advantages over traditional field-based methods. They have readily been used to count birds, marine mammals and large herbivores in different environments, tasks which are routinely performed through manual counting in large collections of images. In this paper, we propose a semi-automatic system able to detect large mammals in semi-arid Savanna. It relies on an animal-detection system based on machine learning, trained with crowd-sourced annotations provided by volunteers who manually interpreted sub-decimeter resolution color images. The system achieves a high recall rate and a human operator can then eliminate false detections with limited effort. Our system provides good perspectives for the development of data-driven management practices in wildlife conservation. It shows that the detection of large mammals in semi-arid Savanna can be approached by processing data provided by standard RGB cameras mounted on affordable fixed wings UAVs.}
}
@article{SU2018157,
title = {Wheat yellow rust monitoring by learning from multispectral UAV aerial imagery},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {157-166},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918312584},
author = {Jinya Su and Cunjia Liu and Matthew Coombes and Xiaoping Hu and Conghao Wang and Xiangming Xu and Qingdong Li and Lei Guo and Wen-Hua Chen},
keywords = {Wheat yellow rust, Multispectral image, Spectral vegetation index (SVI), Unmanned Aerial Vehicle (UAV), Random forest},
abstract = {The use of a low-cost five-band multispectral camera (RedEdge, MicaSense, USA) and a low-altitude airborne platform is investigated for the detection of plant stress caused by yellow rust disease in winter wheat for sustainable agriculture. The research is mainly focused on: (i) determining whether or not healthy and yellow rust infected wheat plants can be discriminated; (ii) selecting spectral band and Spectral Vegetation Index (SVI) with a strong discriminating capability; (iii) developing a low-cost yellow rust monitoring system for use at farmland scales. An experiment was carefully designed by infecting winter wheat with different levels of yellow rust inoculum, where aerial multispectral images under different developmental stages of yellow rust were captured by an Unmanned Aerial Vehicle at an altitude of 16–24 m with a ground resolution of 1–1.5 cm/pixel. An automated yellow rust detection system is developed by learning (via random forest classifier) from labelled UAV aerial multispectral imagery. Experimental results indicate that: (i) good classification performance (with an average Precision, Recall and Accuracy of 89.2%, 89.4% and 89.3%) was achieved by the developed yellow rust monitoring at a diseased stage (45 days after inoculation); (ii) the top three SVIs for separating healthy and yellow rust infected wheat plants are RVI, NDVI and OSAVI; while the top two spectral bands are NIR and Red. The learnt system was also applied to the whole farmland of interest with a promising monitoring result. It is anticipated that this study by seamlessly integrating low-cost multispectral camera, low-altitude UAV platform and machine learning techniques paves the way for yellow rust monitoring at farmland scales.}
}
@article{PASSOS2022101754,
title = {Automatic detection of Aedes aegypti breeding grounds based on deep networks with spatio-temporal consistency},
journal = {Computers, Environment and Urban Systems},
volume = {93},
pages = {101754},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101754},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001617},
author = {Wesley L. Passos and Gabriel M. Araujo and Amaro A. {de Lima} and Sergio L. Netto and Eduardo A.B. {da Silva}},
keywords = {Vector control, , Aerial images, Convolutional neural networks, Image and video processing, Computer vision, Object detection},
abstract = {Every year, the Aedes aegypti mosquito infects millions of people with diseases such as dengue, zika, chikungunya, and urban yellow fever. The main form to combat these diseases is to avoid mosquito reproduction by searching for and eliminating the potential mosquito breeding grounds. In this work, we introduce a comprehensive dataset of aerial videos, acquired with an unmanned aerial vehicle, containing possible mosquito breeding sites. All frames of the video dataset were manually annotated with bounding boxes identifying all objects of interest. This dataset was employed to develop an automatic detection system of such objects based on deep convolutional networks. We propose the exploitation of the temporal information contained in the videos by the incorporation, in the object detection pipeline, of a spatio-temporal consistency module that can register the detected objects, minimizing most false-positive and false-negative occurrences. Also, we experimentally show that using videos is more beneficial than only composing a mosaic using the frames. Using the ResNet-50-FPN as a backbone, we achieve F1-scores of 0.65 and 0.77 on the object-level detection of ‘tires’ and ‘water tanks’, respectively, illustrating the system capabilities to properly locate potential mosquito breeding objects.}
}
@article{HATAMLEH2015457,
title = {Unmanned Aerial Vehicles parameter estimation using Artificial Neural Networks and Iterative Bi-Section Shooting method},
journal = {Applied Soft Computing},
volume = {36},
pages = {457-467},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615003804},
author = {Khaled S. Hatamleh and Mohammad Al-Shabi and Adnan Al-Ghasem and Asad A. Asad},
keywords = {UAV, Parameter Estimation, ANN, IBSS},
abstract = {Quadrotor Unmanned Aerial Vehicles (UAVs) can perform numerous tasks fearless of unnecessary loss of human life. Lately, to enhance UAV control performance, system identification and states estimation has been an active field of research. This work presents a simulation study that investigates unknown dynamics model parameters estimation of a Quadrotor UAV under presence of noisy feedback signals. The latter constitute a challenge for UAV control performance especially with the presence of uncertainties. Therefore, estimation techniques are usually used to reduce the effect of such uncertainties. In this paper, three estimation methods are presented to estimate unknown parameters of the “OS4” Quadrotor. Those methods are Iterative Bi-Section Shooting method “IBSS”, Artificial Neural Network method “ANN”, and “Hybrid ANN_IBSS”, which is a novel method that integrates ANN with IBSS. The “Hybrid ANN_IBSS” is the main contribution of this work. Percentage error of the estimated parameters is used to evaluate accuracy of the aforementioned methods. Results show that IBSS and ANN are capable of estimating most of the parameters even with the presence of noisy feedback signals. However, their performance lacks accuracy when estimating small-value parameters. On the other hand, Hybrid ANN_IBSS achieved higher estimation accuracy compared to the other two methods. Accurate parameter estimation is expected to enhance reliability of the “OS4” dynamics model and hence improve control quality.}
}
@article{UDDIN2020236,
title = {Amateur Drones Detection: A machine learning approach utilizing the acoustic signals in the presence of strong interference},
journal = {Computer Communications},
volume = {154},
pages = {236-245},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.065},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419318341},
author = {Zahoor Uddin and Muhammad Altaf and Muhammad Bilal and Lewis Nkenyereye and Ali Kashif Bashir},
keywords = {Amateur drone detection, Acoustic signals processing, Independent component analysis, Features extraction, Signals classification, Security, Safety},
abstract = {Owing to small size, sensing capabilities and autonomous nature, the Unmanned Air Vehicles (UAVs) have enormous applications in various areas e.g., remote sensing, navigation, archaeology, journalism, environmental science, and agriculture. However, the un-monitored deployment of UAVs called the amateur drones (AmDr) can lead to serious security threats and risk to human life and infrastructure. Therefore, timely detection of the AmDr is essential for the protection and security of sensitive organizations, human life and other vital infrastructure. AmDrs can be detected using different techniques based on sound, video, thermal, and radio frequencies. However, the performance of these techniques is limited in sever atmospheric conditions. In this paper, we propose an efficient un-supervise machine learning approach of independent component analysis (ICA) to detect various acoustic signals i.e., sounds of bird, airplanes, thunderstorm, rain, wind and the UAVs in practical scenario. After unmixing the signals, the features like Mel Frequency Cepstral Coefficients (MFCC), the power spectral density (PSD) and the Root Mean Square Value (RMS) of the PSD are extracted by using ICA. The PSD and the RMS of PSD signals are extracted by first passing the signals from octave band filter banks. Based on the above features the signals are classified using Support Vector Machines (SVM)and K Nearest Neighbour (KNN)to detect the presence or absence of AmDr. Unique feature of the proposed technique is the detection of a single or multiple AmDrs at a time in the presence of multiple acoustic interfering signals. The proposed technique is verified through extensive simulations and it is observed that the RMS values of PSD with KNN performs better than the MFCC with KNN and SVM.}
}
@article{GEVAERT2018106,
title = {A deep learning approach to DTM extraction from imagery using rule-based training labels},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {142},
pages = {106-123},
year = {2018},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618301643},
author = {C.M. Gevaert and C. Persello and F. Nex and G. Vosselman},
keywords = {Digital Terrain Models (DTM), Unmanned Aerial Vehicles (UAV), Aerial photogrammetry, Deep learning, Fully Convolutional Networks (FCN)},
abstract = {Existing algorithms for Digital Terrain Model (DTM) extraction still face difficulties due to data outliers and geometric ambiguities in the scene such as contiguous off-ground areas or sloped environments. We postulate that in such challenging cases, the radiometric information contained in aerial imagery may be leveraged to distinguish between ground and off-ground objects. We propose a method for DTM extraction from imagery which first applies morphological filters to the Digital Surface Model to obtain candidate ground and off-ground training samples. These samples are used to train a Fully Convolutional Network (FCN) in the second step, which can then be used to identify ground samples for the entire dataset. The proposed method harnesses the power of state-of-the-art deep learning methods, while showing how they can be adapted to the application of DTM extraction by (i) automatically selecting and labelling dataset-specific samples which can be used to train the network, and (ii) adapting the network architecture to consider a larger surface area without unnecessarily increasing the computational burden. The method is successfully tested on four datasets, indicating that the automatic labelling strategy can achieve an accuracy which is comparable to the use of manually labelled training samples. Furthermore, we demonstrate that the proposed method outperforms two reference DTM extraction algorithms in challenging areas.}
}
@article{FOUCHE2018142,
title = {Drone as an autonomous aerial sensor system for motion planning},
journal = {Measurement},
volume = {119},
pages = {142-155},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2018.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0263224118300332},
author = {Gavin J. Fouché and Reza Malekian},
keywords = {Unmanned Aerial Vehicle, Aerial sensor system, Autonomous navigation, Motion planning},
abstract = {A system capable of both autonomous navigation and remote fire detection was developed from first principles. Using a complementary filter and attitude fundamentals, an Euler coordinate system representation of the aircraft’s orientation was measured using a widely available low-cost inertial measurement unit incorporating MEMS accelerometers, gyroscopes and magnetometers. Using line-of-sight guidance principles, navigation trajectories could be calculated in real-time providing autonomous navigation between user designated waypoints. A stabilisation control system using proportional-integral-derivative (PID) controllers was developed in order to achieve stabilised flight on the calculated navigation trajectories. Fire detection was facilitated with the use of three low-cost air composition sensors fed into an artificial neural network. A motion planner was developed to assist in flight planning by using topology information for the flight area to construct an optimal flight path based on distance and climb penalties.}
}
@article{LIU2020282,
title = {Predictor-based model reference adaptive roll and yaw control of a quad-tiltrotor UAV},
journal = {Chinese Journal of Aeronautics},
volume = {33},
number = {1},
pages = {282-295},
year = {2020},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1000936119302973},
author = {Ningjun LIU and Zhihao CAI and Jiang ZHAO and Yingxun WANG},
keywords = {Adaptive control, Flight control, Tiltrotor, UAV, VTOL},
abstract = {An attempt is made to apply modern control technology to the roll and yaw control of a rudderless quad-tiltrotor Unmanned Aerial Vehicle (UAV) in the latter part of the flight mode transition, where aerodynamic forces on the tiltrotor’s wings start to take effect. A predictor-based adaptive roll and yaw controller is designed to compensate for system uncertainties and parameter changes. A dynamics model of the tiltrotor is built. A Radial-Basis Function (RBF) neural network and offline adaptation method are used to reduce flight controller workload and cope with the nonlinearities in the controls. Simulations are conducted to verify the reference model response tracking and yaw-roll control decoupling ability of the adaptive controller, as well as the validity of the offline adaptation method. Flight tests are conducted to confirm the ability of the adaptive controller to track different roll and yaw reference model responses. The decoupling of roll and yaw controls is also tested in flight via coordinated turn maneuvers with different rotor tilt angles.}
}
@article{ZHOU2020105576,
title = {Classification of soybean leaf wilting due to drought stress using UAV-based imagery},
journal = {Computers and Electronics in Agriculture},
volume = {175},
pages = {105576},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105576},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920307195},
author = {Jing Zhou and Jianfeng Zhou and Heng Ye and Md Liakat Ali and Henry T. Nguyen and Pengyin Chen},
keywords = {Soybean breeding, Slow-wilting, Drought tolerance, UAV-based imagery, Machine learning},
abstract = {Drought stress is one of the major limiting factors in soybean growth and productivity. Canopy leaf wilting (i.e. fast- and slow-wilting) is considered as an important visible symptom of soybeans under drought conditions. In soybean breeding programs, genotypes with the slow-wilting trait have been identified as drought-tolerant cultivars. Traditional method measures canopy leaf wilting traits using visual observations, which is subjective and time-consuming. Recent developments of field high-throughput phenotyping technology using Unmanned Aerial Vehicle (UAV)-based imagery have shown great potential in quantifying crop traits and detecting crop responses to abiotic and biotic stresses. The goal of this study was to investigate the potential use of UAV-based imagery in classifying soybean genotypes with fast- and slow-wilting traits. A UAV imaging system consisting of an RGB (Red-Green-Blue) camera, an infrared thermal camera, and a multispectral camera was used to collect imagery data of 116 soybean genotypes planted in a rain-fed breeding field at the reproductive stage. Visual-based canopy wilting scores were collected by breeders in the same day of imagery data collection. Seven image features were extracted, namely normalized difference vegetation index (NDVI), green-based NDVI (gNDVI), temperature, color hue, color saturation, canopy size and plant height for quantifying canopy wilting trait. Results show that all image features significantly (p-value < 0.01) correlated with soybean yield under drought. A Support Vector Machine model was developed to classify the two wilting traits using the images features and achieved an average classification accuracy of 0.8 with the highest one of 0.9. Slow-wilting genotypes had significantly (p-value < 0.01) higher NDVI, hue, saturation, canopy size, height, and lower temperature than fast-wilting genotypes. The significant broad-sense heritability (H2) indicates the dominating genetic factors in the variations of the image features. The study demonstrates the good potential use of UAV-based imagery technologies in the selection of soybeans genotypes with drought tolerance.}
}
@article{BAEK201976,
title = {UAV-based measurements of spatio-temporal concentration distributions of fluorescent tracers in open channel flows},
journal = {Advances in Water Resources},
volume = {127},
pages = {76-88},
year = {2019},
issn = {0309-1708},
doi = {https://doi.org/10.1016/j.advwatres.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0309170818308637},
author = {Donghae Baek and Il Won Seo and Jun Song Kim and Jonathan M. Nelson},
keywords = {Pollutant mixing, Tracer test, Spatio-temporal distribution,  measurement, Remote measurement, Artificial neural network},
abstract = {A new method of unmanned aerial vehicle (UAV)-based tracer tests using RGB (red, green, blue) images was developed in order to acquire the spatio-temporal concentration distribution of tracer clouds in open channel flows. Tracer tests using Rhodamine WT were conducted to collect the RGB images using a commercial digital camera mounted on a UAV, and the concentration of Rhodamine WT using in-situ fluorometric probes. The correlation analysis showed that the in-situ measured concentrations of Rhodamine WT were strongly correlated with the digital number (DN) of the RGB images, even though the response of DN to the concentration was spatially heterogeneous. The empirical relationship between the DN values and the Rhodamine WT concentration data was estimated using artificial neural network (ANN) models. The trained ANN models, which consider the effect of water depth and river bed, accurately retrieved the detailed spatio-temporal concentration distributions of all study areas that had an R2 higher than 0.9. The acquired spatio-temporal concentration distributions by the proposed method based on the UAV images gave general as well as detailed views of the tracer cloud moving dynamically in open channel flows that cannot be easily observed using conventional in-situ measurements.}
}
@article{NEVAVUORI2019104859,
title = {Crop yield prediction with deep convolutional neural networks},
journal = {Computers and Electronics in Agriculture},
volume = {163},
pages = {104859},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.104859},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919306842},
author = {Petteri Nevavuori and Nathaniel Narra and Tarmo Lipping},
keywords = {Crop yield prediction, Convolutional neural network, Wheat, Barley, UAV, Multispectral, NDVI, Growth phase},
abstract = {Using remote sensing and UAVs in smart farming is gaining momentum worldwide. The main objectives are crop and weed detection, biomass evaluation and yield prediction. Evaluating machine learning methods for remote sensing based yield prediction requires availability of yield mapping devices, which are still not very common among farmers. In this study Convolutional Neural Networks (CNNs) – a deep learning methodology showing outstanding performance in image classification tasks – are applied to build a model for crop yield prediction based on NDVI and RGB data acquired from UAVs. The effect of various aspects of the CNN such as selection of the training algorithm, depth of the network, regularization strategy, and tuning of the hyperparameters on the prediction efficiency are tested. Using the Adadelta training algorithm, L2 regularization with early stopping and a CNN with 6 convolutional layers, mean absolute error (MAE) in yield prediction of 484.3 kg/ha and mean absolute percentage error (MAPE) of 8.8% was achieved for data acquired during the early period of the growth season (i.e., in June of 2017, growth phase <25%) with RGB data. When using data acquired later in July and August of 2017 (growth phase >25%), MAE of 624.3 kg/ha (MAPE: 12.6%) was obtained. Significantly, the CNN architecture performed better with RGB data than the NDVI data.}
}
@article{SUN2021102373,
title = {Retrieval of rapeseed leaf area index using the PROSAIL model with canopy coverage derived from UAV images as a correction parameter},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {102},
pages = {102373},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102373},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421000805},
author = {Bo Sun and Chufeng Wang and Chenghai Yang and Baodong Xu and Guangsheng Zhou and Xiaoyong Li and Jing Xie and Shijie Xu and Bin Liu and Tianjin Xie and Jie Kuai and Jian Zhang},
keywords = {Leaf area index, Rapeseed, PROSAIL model, Empirical statistical model, Canopy coverage},
abstract = {Leaf area index (LAI), which is an important structural parameter, plays a vital role in evaluating crop growth and yield. In this study, we used the canopy coverage (CC) derived from unmanned aerial vehicle (UAV) images as a correction parameter in the PROSAIL model coupled with a neural network (NN) to improve the accuracy of LAI inversion of rapeseed plots. CC had a significantly positive impact on the accuracy of LAI inversion especially in sparse canopy structure with the 22.24% decrease in the entire dataset and 35.76% decrease in the sparse canopy dataset. We then compared the inversion performances of an empirical statistical model (ESM) based on a vegetation index and the PROSAIL model incorporating CC correction for 2016 and 2018 datasets. The ESM performed better in modeling the 2016 dataset, but its accuracy was much lower for the 2018 dataset (2016: NRMSE = 0.131; 2018: NRSME = 0.348). Overall, the PROSAIL model was more robust over these two datasets (2016: NRMSE = 0.152; 2018: NRMSE = 0.168). In addition, the original-resolution images were resampled to six coarse resolutions to evaluate the influence of image resolution on the LAI inversion performance of the PROSAIL model. When pixel size increased to more than 10 cm, the inversion accuracy began to decrease dramatically. In conclusion, introducing a canopy coverage correction parameter in the PROSAIL model improved its performance in retrieving rapeseed LAI.}
}
@article{BANG2020103198,
title = {Image augmentation to improve construction resource detection using generative adversarial networks, cut-and-paste, and image transformation techniques},
journal = {Automation in Construction},
volume = {115},
pages = {103198},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103198},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519311653},
author = {Seongdeok Bang and Francis Baek and Somin Park and Wontae Kim and Hyoungkwan Kim},
keywords = {Data augmentation, Construction resource detection, On-site management, GAN, UAV},
abstract = {The paper proposes an image augmentation method to construct a large-size dataset for improving construction resource detection. The method consists of three techniques: removing-and-inpainting, cut-and-paste, and image-variation. The removing-and-inpainting technique arbitrarily removes objects from images and reconstructs the removed regions via generative adversarial networks (GAN). The cut-and-paste technique extracts objects from the original dataset and places them into the reconstructed images via the previous technique. The image-variation technique applies three image transformation techniques, intensity-, blur- and scale-variation, to the images. To evaluate the method, 656 unmanned aerial vehicle (UAV)-acquired construction site images were used as the original dataset. A faster region-based convolutional neural network (Faster R-CNN) trained with the augmented training dataset achieves better performance, which is higher than that of a network trained with the original dataset. These results prove that the method is optimal for improving construction resource detection in UAV-acquired images.}
}
@article{REN2020124,
title = {Distributed cooperative learning over time-varying random networks using a gossip-based communication protocol},
journal = {Fuzzy Sets and Systems},
volume = {394},
pages = {124-145},
year = {2020},
note = {Neurofuzzy Systems and Learning},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0165011419302647},
author = {Pengfei Ren and Hao Dai and Weisheng Chen},
keywords = {Machine learning, Distributed cooperative learning (DCL), Consensus, Gossip-based communication protocol, Fuzzy logic systems},
abstract = {Motivated by applications of distributed estimation and distributed decision making in wireless sensor networks (WSNs) and unmanned aerial vehicle (UAV) networks, we study a distributed learning problem over time-varying undirected random networks. Using a gossip-based communication protocol, a novel distributed cooperative learning (DCL) algorithm, termed the gossip-based DCL (GBDCL) algorithm, is presented to solve the problem by training the raw data distributed and blocked throughout different nodes. Exploiting the robustness of the gossip-based protocol, each node is guaranteed to build the same learning model in theory against random disconnections and communication route variations in the network topology. It is proved that the GBDCL algorithm converges to the optimal consensus asymptotically. The correctness and effectiveness of the presented GBDCL algorithm are verified in the theoretical analysis and simulations.}
}
@article{VILLACRES202276,
title = {Construction of 3D maps of vegetation indices retrieved from UAV multispectral imagery in forested areas},
journal = {Biosystems Engineering},
volume = {213},
pages = {76-88},
year = {2022},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2021.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S153751102100297X},
author = {Juan Villacrés and Fernando A. {Auat Cheein}},
keywords = {Fuel moisture content, Multispectral images, Vegetation indices estimation, Canopy segmentation},
abstract = {The construction of fuel moisture content (FMC) maps, as well as temperature, terrain topography, and wind speed maps, are essential for the development of fire susceptibility models in forested areas. Moisture distribution in tree canopies requires exploration and a three-dimensional representation. This paper presents the construction of FMC maps expressed as vegetation indices (VIs) in a point cloud. Multispectral images were captured by a camera mounted on an unmanned aerial vehicle to create the point cloud. VIs were estimated in the points that belonged to the forest canopy. To classify the canopy points, we a combination of filtering of ground points and thresholding of VIs was evaluated. On such canopy points, random forest (RF), kernel ridge regression (KRR), and Gaussian process retrieval (GPR) regressors were investigated to estimate twelve VIs related to FMC. The input set of the models consisted of the points representing five wavelengths provided by the multispectral camera. The ground truth of VIs was obtained using a spectrometer. The study area was a 1 ha forest of Pinus radiata in the Maule Region, Chile. The results demonstrated that combining ground filtering and VIs thresholding for canopy points segmentation achieved a precision of 93.27%, recall of 95.65%, F1 score of 90.12%, and accuracy of 87.82%. Furthermore, the recovery of the VIs using GPR achieved a root mean square error of 0.175 and a coefficient of determination of 0.18. According to the correlation coefficient, GPR was able to recover eleven of the twelve VIs, KRR recovered three, and RF failed to recover any.}
}