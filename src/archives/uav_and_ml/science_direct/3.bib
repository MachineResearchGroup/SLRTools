@article{MARQUESRAMOS2020105791,
title = {A random forest ranking approach to predict yield in maize with uav-based vegetation spectral indices},
journal = {Computers and Electronics in Agriculture},
volume = {178},
pages = {105791},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105791},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920319591},
author = {Ana Paula {Marques Ramos} and Lucas {Prado Osco} and Danielle {Elis Garcia Furuya} and Wesley {Nunes Gonçalves} and Dthenifer {Cordeiro Santana} and Larissa {Pereira Ribeiro Teodoro} and Carlos {Antonio da Silva Junior} and Guilherme {Fernando Capristo-Silva} and Jonathan Li and Fábio {Henrique Rojo Baio} and José {Marcato Junior} and Paulo {Eduardo Teodoro} and Hemerson Pistori},
keywords = {Precision agriculture, Multispectral images, Shallow learner, Random Forest},
abstract = {Random Forest (RF) is a machine learning technique that has been proved to be highly accurate in several agricultural applications. However, to yield prediction, how much this technique may be improved with the adoption of a ranking-based strategy is still an unknown issue. Here we propose a ranking-based approach to potentialize the RF method for maize yield prediction. This approach is based on the correlation parameter of individual vegetation indices (VIs). The VIs were individually ranked based on a merit metric that measures the improvement on the Pearson’s correlation coefficient by using RF against a baseline method. As a result, only the most relevant VIs were considered as input features to the RF model. We used 33 VIs extracted from multispectral UAV-based (unmanned aerial vehicle) imagery. The multispectral data were generated with two different sensors: Sequoia and MicaSense; during the 2017/2018 and 2018/2019 crop seasons, respectively. Amongst all the evaluated indices, NDVI, NDRE, and GNDVI were the top three in the ranking-based analysis, and their combination with RF increased the maize yield prediction. Our approach also outperformed other known machine learning methods, like support vector machine and artificial neural network. Additive regression, using the RF as the base weak learner, provided a higher accuracy with a correlation coefficient and MAE (Mean Absolute Error) of 0.78 and 853.11 kg ha−1, respectively. We conclude that the ranking-based strategy of VIs is appropriate to predict maize yield using machine learning methods and data derived from multispectral images. We demonstrated that our approach reduces the number of VIs needed to determine a high accuracy and relative low MAE, and the approach may contribute to decision-making actions, resulting in accurate management of maize fields.}
}
@article{SHAO201715289,
title = {Disturbance observer-based discrete-time neural control for unmanned aerial vehicles with uncertainties and disturbances **This work is supported by National Natural Science Foundation of China (No. 61573184), 333 Talents Project in Jiangsu Province (No. BRA2015359) and Jiangsu Innovation Program for Graduate Education (No. KYLX16 0375).},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {15289-15294},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2439},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317333086},
author = {Shuyi Shao and Mou Chen and Rong Mei},
keywords = {UAV, disturbance observer, neural network, backstepping, tracking control},
abstract = {In this paper, a disturbance observer-based discrete-time neural control problem is studied for unmanned aerial vehicle (UAV) in the presence of external disturbances and system uncertainties. To estimate the external disturbance, a nonlinear discrete-time disturbance observer (DTDO) is designed. Furthermore, the system uncertainties are approximated by employing neural network (NN). Then, a discrete-time neural tracking control scheme is proposed based on the designed DTDO, the discrete-time tracking differentiator and the backstepping technique. Under the discrete-time Lyapunov analysis, the boundness of all the closed-loop system signals are proven. Finally, numerical simulation results are shown to demonstrate the effectiveness of the proposed control scheme.}
}
@article{LI2020103807,
title = {Path planning of multiple UAVs with online changing tasks by an ORPFOA algorithm},
journal = {Engineering Applications of Artificial Intelligence},
volume = {94},
pages = {103807},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103807},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620301895},
author = {Kun Li and Fawei Ge and Ying Han and Yi’an Wang and Wensu Xu},
keywords = {Oilfield inspection, UAVs, Path planning, Task assignment, Swarm intelligence algorithm, Fruit fly optimization algorithm},
abstract = {The unmanned aerial vehicle (UAV) is a new type oilfield inspection tool which is characterized by high flexibility, low cost and high efficiency. In the UAV based oilfield inspection technology, the path planning is an indispensable element which finds an optimal flight path for UAV to finish the inspection jobs successfully. In comparison with the other researches, our study focuses on two challenging issues: path planning of multiple UAVs by traversing a certain amount of task points in the three-dimensional environment within the required completion time, and optimizing solving for the best flight path with online changing tasks. In the research, a novel task assignment method including the initial task assignment and the task assignment with changing tasks is proposed to determine the initial task sequences of each UAV and rapidly replan task sequences after tasks change. An improved fruit fly optimization algorithm (named ORPFOA) is proposed to solve the path planning problem in both initial task sequences and new task sequences after tasks change, in which the optimal reference point and a distance cost matrix are used to reach both faster solving and higher optimizing precision for the optimal flight path. In ORPFOA, two cost functions are defined to evaluate the optimizing results in the initial phase and the new phase after task changes, respectively. A simulation model of the three-dimensional oilfield environment is established to verify the effectiveness of the proposed method in comparison with other six algorithms.}
}
@article{REN2022102660,
title = {An improved mask-RCNN algorithm for UAV TIR video stream target detection},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {106},
pages = {102660},
year = {2022},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102660},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421003676},
author = {Xiang Ren and Min Sun and Xianfeng Zhang and Lei Liu and Hang Zhou and Xiaoping Ren},
keywords = {UAV, Thermal infrared, Mask-RCNN, Target detection},
abstract = {In emergency rescue operations, unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) sensors are essential to obtain ground information during nighttime operations. However, existing target detection algorithms mainly consider the detection accuracy but not the processing speed and storage space requirements. Furthermore, current neural network target detection algorithms primarily focus on conventional RGB images and are not optimized for UAV-based TIR video stream data. This paper proposes an improved Mask-RCNN algorithm for target detection in UAV TIR video streams to address current research deficiencies. First, MobileNetV3, which is used to process RGB images, is applied to process TIR data for outdoor emergency rescue operations, significantly increasing the time efficiency of the algorithm. Second, prior knowledge such as the projection model of the airborne camera and the target temperature characteristics in the UAV TIR video stream is utilized to filter the detecting results instead of pre-detection temperature masks. Compared with the original Mask-RCNN algorithm, the improved algorithm increases the processing speed, reduces the storage space requirements, and provides detection performances equal or slightly superior to that before the improvement.}
}
@article{LI2019272,
title = {Wavelet transform based modulation classification for 5G and UAV communication in multipath fading channel},
journal = {Physical Communication},
volume = {34},
pages = {272-282},
year = {2019},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2018.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1874490718304221},
author = {Wenwen Li and Zheng Dou and Lin Qi and Chengzhuo Shi},
keywords = {Multipath, Modulation classification, Wavelet transform, 5G, UAV},
abstract = {Nowadays, fifth generation (5G) network and unmanned aerial vehicle (UAV) are more and more important in the civil and military field. Only communicating correctly in 5G network and between UAVs, they can play a role in real world. Modulation classification is the premise to ensure communication in 5G network and between UAVs correctly. However, the effects of multipath fading always exists in the 5G communication environment and UAV communication channel, which leads to severe modulation classification performance and communication performance degradation. In order to resolve this problem, we proposed a novel modulation classification algorithm that can classify signals without priori information in multipath channel. The proposed algorithm makes the mean, variance, skewness and kurtosis of wavelet transform as the feature set, then uses principal component analysis (PCA) for feature subset selection, in the end neural network is used as classifier to classify signals. The simulation results show that the proposed algorithm can achieve the much better classification accuracy than the existing methods in multipath fading channels.}
}
@article{XIONG2020102994,
title = {Automated regional seismic damage assessment of buildings using an unmanned aerial vehicle and a convolutional neural network},
journal = {Automation in Construction},
volume = {109},
pages = {102994},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519304224},
author = {Chen Xiong and Qiangsheng Li and Xinzheng Lu},
keywords = {Regional seismic damage assessment, GIS, UAV, Convolutional neural network},
abstract = {A rapid assessment of the seismic damage to buildings can facilitate improved emergency response and timely relief in earthquake-prone areas. In this study, an automated building seismic damage assessment method using an unmanned aerial vehicle (UAV) and a convolutional neural network (CNN) is introduced. The method consists of three parts: (1) data preparation, (2) building image segmentation, and (3) CNN-based building seismic damage assessment. First, a three-dimensional (3D) building model, aerial images, and camera data are used for the following simulation. Next, a building image segmentation method is proposed using the 3D building model as georeference, through which multi-view segmented building images can be obtained. Subsequently, a CNN model based on VGGNet is adopted to assess the seismic damage of each building. The CNN model is fine-tuned based on manually tagged building images obtained from the Internet. Finally, a case study of the old Beichuan town is used to demonstrate the effectiveness of the proposed method. The damage distribution of the area is obtained with an accuracy of 89.39%.}
}
@article{CHOUDHARY201959,
title = {Sustainable and secure trajectories for the military Internet of Drones (IoD) through an efficient Medium Access Control (MAC) protocol},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {59-73},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618321712},
author = {Gaurav Choudhary and Vishal Sharma and Ilsun You},
keywords = {Internet of Drones, UAVs, Trajectory, Security, MAC Protocol, Deep neural networks, Link durations},
abstract = {Internet of Drones (IoD) involves cooperation and policing amongst Unmanned Aerial Vehicles (UAVs) based on waypoints generated through a series of algorithms. The dynamic reconfigurability of conveyances in IoD demands a secure trajectory and channel accessibility which is procurable through an efficient Medium Access Control (MAC) protocol. The majority of existing works considers the security of UAV-trajectories as a part of cryptographic and channel accessibility only, whereas the proposed approach idealizes link stability as one of the essential components for attaining secure trajectories. Furthermore, this paper proposes a deep neural network-based security framework, which embeds an efficient MAC protocol controlled by Macaulay’s duration. The proposed approach fortifies that an extensive control over links provides an adequate duration for enhancing the security aspects of IoD and launching countermeasures against any known cyber attacks.}
}
@article{BEHJAT2020106665,
title = {A physics-aware learning architecture with input transfer networks for predictive modeling},
journal = {Applied Soft Computing},
volume = {96},
pages = {106665},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106665},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306037},
author = {Amir Behjat and Chen Zeng and Rahul Rai and Ion Matei and David Doermann and Souma Chowdhury},
keywords = {Hybrid modeling, Neural networks, Particle Swarm Optimization, Physics-aware machine learning, Unmanned aerial vehicle},
abstract = {Hybrid modeling architectures seek to combine a machine learning model with a computationally efficient (simplified or partial) physics model to predict the behavior of physical systems. Existing sequential or parallel approaches to hybrid modeling do not typically exploit the potential relationship between the input or latent features of the partial and full physics. In addition, very few existing architectures take advantage of the provision to generously or on-demand sample the partial physics model. To address these gaps, we have developed a novel neural network-based hybrid architecture called “Opportunistic Physics-mining Transfer Mapping Architecture” or OPTMA. The goal of the OPTMA architecture is to facilitate greater exploitation of input space correlations between partial and full physics where they exist. To this end, a transfer neural network is used to transform the original inputs into modified inputs or latent features, where the partial physics operates on these artificially transformed features to produce the final prediction. An extended back-propagation approach and a Particle Swarm Optimization (to deal with multimodal loss functions) are used to train the network weights. The new architecture is first tested on a simple regression problem for analysis. It is then used to predict the behavior of more complex dynamic systems — an Inverted pendulum and the motion of an unmanned aerial vehicle, both under wind effects. Subsequent tests on unseen samples demonstrate OPTMA’s competitive performance compared to pure ANN and sequential hybrid models and provide empirical validation of the transfer concepts underlying OPTMA.}
}
@article{LI201851,
title = {Automatic road detection system for an air–land amphibious car drone},
journal = {Future Generation Computer Systems},
volume = {85},
pages = {51-59},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.02.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1732770X},
author = {Yujie Li and Huimin Lu and Yoshiki Nakayama and Hyoungseop Kim and Seiichi Serikawa},
keywords = {Unmanned aerial vehicle, Air–land amphibious car drone, Road detection},
abstract = {In recent years, unmanned aerial vehicle (UAV) technologies have rapidly developed. Drones, which are one type of UAV, are used in many industrial fields, such as photography, delivery and agriculture. However, a commercial drone can fly for only approximately 20 min on one charge. Furthermore, drones are prohibited from flying in some areas, and cannot be operated in bad weather. Due to the development of drone technologies, we must reduce energy consumption and achieve long-range movement. To overcome these limitations, we develop a new air–land amphibious car drone that can fly and requires less power consumption in land mode; this extends the range of mobility of the drone. Moreover, land mode can be used to pass through restricted areas or bad weather conditions by sliding. Furthermore, we develop a Convolutional Neural Network (CNN)-based algorithm for detecting the road in a captured scene. To more accurately segment the road region based on images from the equipped camera of the drone, we propose atrous spatial pyramid pooling (ASPP) ResNet blocks, instead of Resblocks, which were proposed by DeepLab. The experimental results demonstrate that the proposed method improves the pixel accuracy (PA) to 85.6% and achieves a mean Intersection over Union (mIoU) of 55.8%.}
}
@article{KARAKOSTAS2020273,
title = {Shot type constraints in UAV cinematography for autonomous target tracking},
journal = {Information Sciences},
volume = {506},
pages = {273-294},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519307455},
author = {Iason Karakostas and Ioannis Mademlis and Nikos Nikolaidis and Ioannis Pitas},
keywords = {UAV Cinematography, Shot type, Target tracking, Autonomous drones},
abstract = {During the past years, camera-equipped Unmanned Aerial Vehicles (UAVs) have revolutionized aerial cinematography, allowing easy acquisition of impressive footage. In this context, autonomous functionalities based on machine learning and computer vision modules are gaining ground. During live coverage of outdoor events, an autonomous UAV may visually track and follow a specific target of interest, under a specific desired shot type, mainly adjusted by choosing appropriate focal length and UAV/camera trajectory relative to the target. However, the selected UAV/camera trajectory and the object tracker requirements (which impose limits on the maximum allowable focal length) affect the range of feasible shot types, thus constraining cinematography planning. Therefore, this paper explores the interplay between cinematography and computer vision in the area of autonomous UAV filming. UAV target-tracking trajectories are formalized and geometrically modeled, so as to analytically compute maximum allowable focal length per scenario, to avoid 2D visual tracker failure. Based on this constraint, formulas for estimating the appropriate focal length to achieve the desired shot type in each situation are extracted, so as to determine shot feasibility. Such rules can be embedded into practical UAV intelligent shooting systems, in order to enhance their robustness by facilitating on-the-fly adjustment of the cinematography plan.}
}
@article{VONG2021106214,
title = {Early corn stand count of different cropping systems using UAV-imagery and deep learning},
journal = {Computers and Electronics in Agriculture},
volume = {186},
pages = {106214},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106214},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921002313},
author = {Chin Nee Vong and Lance S. Conway and Jianfeng Zhou and Newell R. Kitchen and Kenneth A. Sudduth},
keywords = {Corn, Cropping systems, Deep learning, Stand count, UAV imagery},
abstract = {Optimum plant stand density and uniformity is vital in order to maximize corn (Zea mays L.) yield potential. Assessment of stand density can occur shortly after seedlings begin to emerge, allowing for timely replant decisions. The conventional methods for evaluating an early plant stand rely on manual measurement and visual observation, which are time consuming, subjective because of the small sampling areas used, and unable to capture field-scale spatial variability. This study aimed to evaluate the feasibility of an unmanned aerial vehicle (UAV)-based imaging system for estimating early corn stand count in three cropping systems (CS) with different tillage and crop rotation practices. A UAV equipped with an on-board RGB camera was used to collect imagery of corn seedlings (~14 days after planting) of CS, i.e., minimum-till corn-soybean rotation (MTCS), no-till corn-soybean rotation (NTCS), and no-till corn-corn rotation with cover crop implementation (NTCC). An image processing workflow based on a deep learning (DL) model, U-Net, was developed for plant segmentation and stand count estimation. Results showed that the DL model performed best in segmenting seedlings in MTCS, followed by NTCS and NTCC. Similarly, accuracy for stand count estimation was highest in MTCS (R2 = 0.95), followed by NTCS (0.94) and NTCC (0.92). Differences by CS were related to amount and distribution of soil surface residue cover, with increasing residue generally reducing the performance of the proposed method in stand count estimation. Thus, the feasibility of using UAV imagery and DL modeling for estimating early corn stand count is qualified influenced by soil and crop management practices.}
}
@article{QIAN2020105519,
title = {UAV and a deep convolutional neural network for monitoring invasive alien plants in the wild},
journal = {Computers and Electronics in Agriculture},
volume = {174},
pages = {105519},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105519},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920302921},
author = {Wanqiang Qian and Yiqi Huang and Qi Liu and Wei Fan and Zhongyu Sun and Hui Dong and Fanghao Wan and Xi Qiao},
keywords = {Invasive alien plant, Multi-object identification, Deep learning, Computer vision},
abstract = {Invasive alien plants (IAPs) are considered to be among the greatest global threats to biodiversity and ecosystems. Timely and effective monitoring is important for their prevention and control. However, monitoring remains mainly dependent on satellite remote sensing and manual inspection, which has a high cost and rather low accuracy and efficiency. We considered that this problem could be solved using unmanned aerial vehicle (UAV) intelligent monitoring. Accurate and rapid identification of IAPs in the wild is the core of intelligent monitoring. We intended to acquire colour images of the monitoring area in a field environment using an UAV and proposing a novel IAPsNet based on a deep convolutional neural network (CNN) to identify the IAPs appearing in the images. 6400 samples were one by one manually divided into seven IAP categories and one background category as training set. IAPsNet incorporated AlexNet local response normalization (LRN), GoogLeNet inception models, and continuous VGG convolution. Through training and testing, the IAPsNet performance for 893 testing samples was rather satisfactory, reaching an accuracy of 93.39% within a time of 1.8846 s and the average recall, average precision and average F1-score can reach 93.3%, 93.74% and 93.52% respectively. Moreover, in quantitative and qualitative comparative analysis, IAPsNet not only has high accuracy, high recall, high precision, high F1-score and efficiency but also has a high anti-interference capacity against blur, environment and multi-scales. Additionally, IAPsNet was applied to 4 different real wild conditions, proving that it is able to adapt to different scenes and simultaneously identify multiple species; it has potential to be used in the wild. High-quality distributional data of invasive plants are provided for subsequent ecological analysis. The data will help management authorities to implement the necessary steps in an identified area to develop a comprehensive strategy for IAP control.}
}
@article{SANKEY2021112223,
title = {Quantifying plant-soil-nutrient dynamics in rangelands: Fusion of UAV hyperspectral-LiDAR, UAV multispectral-photogrammetry, and ground-based LiDAR-digital photography in a shrub-encroached desert grassland},
journal = {Remote Sensing of Environment},
volume = {253},
pages = {112223},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112223},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720305964},
author = {Joel B. Sankey and Temuulen T. Sankey and Junran Li and Sujith Ravi and Guan Wang and Joshua Caster and Alan Kasprak},
keywords = {Airborne data, Drone, Unmanned aerial system (UAS), Unmanned aerial vehicle (UAV), Terrestrial laser scanning, Photogrammetry, Structure from motion (SFM), Lidar, Hyperspectral, Machine learning, Digital elevation model (DEM), Digital elevation model of difference (DOD), Change detection, Rangeland, Shrub, Grass, Soil, Nutrient, Fire, Islands of fertility},
abstract = {Rangelands cover 70% of the world's land surface, and provide critical ecosystem services of primary production, soil carbon storage, and nutrient cycling. These ecosystem services are governed by very fine-scale spatial patterning of soil carbon, nutrients, and plant species at the centimeter-to-meter scales, a phenomenon known as “islands of fertility”. Such fine-scale dynamics are challenging to detect with most satellite and manned airborne platforms. Remote sensing from unmanned aerial vehicles (UAVs) provides an alternative option for detecting fine-scale soil nutrient and plant species changes in rangelands tn0020 smaller extents. We demonstrate that a model incorporating the fusion of UAV multispectral and structure-from-motion photogrammetry classifies plant functional types and bare soil cover with an overall accuracy of 95% in rangelands degraded by shrub encroachment and disturbed by fire. We further demonstrate that employing UAV hyperspectral and LiDAR fusion greatly improves upon these results by classifying 9 different plant species and soil fertility microsite types (SFMT) with an overall accuracy of 87%. Among them, creosote bush and black grama, the most important native species in the rangeland, have the highest producer's accuracies at 98% and 94%, respectively. The integration of UAV LiDAR-derived plant height differences was critical in these improvements. Finally, we use synthesis of the UAV datasets with ground-based LiDAR surveys and lab characterization of soils to estimate that the burned rangeland potentially lost 1474 kg/ha of C and 113 kg/ha of N owing to soil erosion processes during the first year after a prescribed fire. However, during the second-year post-fire, grass and plant-interspace SFMT functioned as net sinks for sediment and nutrients and gained approximately 175 kg/ha C and 14 kg/ha N, combined. These results provide important site-specific insight that is relevant to the 423 Mha of grasslands and shrublands that are burned globally each year. While fire, and specifically post-fire erosion, can degrade some rangelands, post-fire plant-soil-nutrient dynamics might provide a competitive advantage to grasses in rangelands degraded by shrub encroachment. These novel UAV and ground-based LiDAR remote sensing approaches thus provide important details towards more accurate accounting of the carbon and nutrients in the soil surface of rangelands.}
}
@article{QU2021247,
title = {DroneCOCoNet: Learning-based edge computation offloading and control networking for drone video analytics},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {247-262},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002351},
author = {Chengyi Qu and Prasad Calyam and Jeromy Yu and Aditya Vandanapu and Osunkoya Opeoluwa and Ke Gao and Songjie Wang and Raymond Chastain and Kannappan Palaniappan},
keywords = {Edge/fog computation offloading, Drone video analytics, Mobile edge computing, Learning-based scheme, Data processing in fog computing},
abstract = {Multi-Unmanned Aerial Vehicle (UAV) systems with high-resolution cameras have been found useful for operations such as smart city and disaster management. These systems feature Flying Ad-Hoc Networks (FANETs) that connect the computation edge with UAVs and a Ground Control Station (GCS) through air-to-ground wireless network links. Leveraging the edge/fog computation resources effectively with energy-latency-awareness, and handling intermittent failures of FANETs are the major challenges in supporting video processing applications. In this paper, we propose a novel “DroneCOCoNet” framework for drone video analytics that coordinates intelligent processing of large video datasets using edge computation offloading and performs network protocol selection based on resource-awareness. We present two edge computation offloading approaches, i.e., heuristic-based and reinforcement learning-based approaches. These approaches provide intelligent task sharing and co-ordination for dynamic offloading decision-making among UAVs. Our scheme handles the problem of computation offloading tasks in two separate ways: (i) heuristic decision-making process, and (ii) Markov decision process; wherein we aim to minimize the total computation costs as well as latency in the edge/fog resources while minimizing video processing times to meet application requirements. Our experimental results show that our heuristic-based offloading decision-making scheme enables lower scheduling time and energy consumption for low drone-to-ground server ratios. In comparison, our dynamic reinforcement learning-based decision-making approach increases the accuracy and saves overall time periodically. Notably, these results also hold in various other multi-UAV scenarios involving largely different numbers of detected objects in e.g., smart farming, transportation traffic flow monitoring and disaster response.}
}
@article{YANG2020107938,
title = {A near real-time deep learning approach for detecting rice phenology based on UAV images},
journal = {Agricultural and Forest Meteorology},
volume = {287},
pages = {107938},
year = {2020},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2020.107938},
url = {https://www.sciencedirect.com/science/article/pii/S016819232030040X},
author = {Qi Yang and Liangsheng Shi and Jingye Han and Jin Yu and Kai Huang},
keywords = {Rice phenology detection, UAV, Convolutional neural network, Shape model fitting, Deep learning},
abstract = {Near real-time crop phenology detection is essential for crop management, estimation of harvest time and yield estimation. Previous approaches to crop phenology detection have relied on time-series (multi-temporal) vegetation index (VI) data, and have included threshold-based, phenometrics-based and shape-model-fitting-based (SMF) methods. However, the performance of these methods depends on the duration and temporal resolution of the time-series data. In this study, we propose a new approach which identifies the principal growth stages of rice (Oryza sativa L.) directly from RGB images. Only a mono-temporal unmanned aerial vehicle (UAV) imagery was required for a large-area phenology detection via the well-trained network. An efficient convolutional neural network (CNN) architecture was designed to estimate rice phenology. The CNN incorporated spatial pyramid pooling (SPP), transfer learning and an auxiliary branch with external data. A total of 82 plots across a 160-hectare rice cultivation area of Southern China were selected to evaluate the proposed network. CNN predictions were ground truthed using rice phenology measurements taken from each plot throughout the growing season. Aerial data were collected using a fixed-wing UAV equipped with multispectral and RGB cameras. The performance of traditional SMF methods deteriorated when time-series VI data were of short duration. In contrast, the phenological stage estimated by the proposed network showed good agreement with ground observations, with a top-1 accuracy rate of 83.9% and mean absolute error (MAE) of 0.18. The spatial distribution of harvest dates for 627 plots in the study area were computed from the phenological stage estimates. The estimates matched well with the observed harvest dates. The results demonstrated the excellent performance of the proposed deep learning approach in near real-time phenology detection and harvest time estimation.}
}
@article{TIAN2021146816,
title = {Aboveground mangrove biomass estimation in Beibu Gulf using machine learning and UAV remote sensing},
journal = {Science of The Total Environment},
volume = {781},
pages = {146816},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.146816},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721018842},
author = {Yichao Tian and Hu Huang and Guoqing Zhou and Qiang Zhang and Jin Tao and Yali Zhang and Junliang Lin},
keywords = {Machine learning method, UAV remote sensing, Aboveground biomass, LiDAR point cloud, Mangrove forests, Beibu Gulf},
abstract = {On the basis of canopy height variables, vegetation index, texture index, and laser point cloud index measured with unmanned aerial vehicle (UAV) low altitude remote sensing, we used eight machine learning (ML) models to estimate the aboveground biomass of different species of mangroves in Beibu Gulf and compared the accuracy of different ML models for these estimations. The main species of typical mangrove communities in Kangxiling were Aegiceras corniculata and Sonneratia apetala. The trunks of Sonneratia apetala were thicker, with an average height of 11.82 m, whereas Aegiceras corniculata trees were shorter, with an average height of 2.58 m. The XGBoost regressor (XGBR) model had the highest accuracy in estimating mangrove aboveground biomass (R2 = 0.8319, RMSE = 22.7638 Mg/ha), followed by the random forest regressor model (R2 = 0.7887, RMSE = 25.5193 Mg/ha). Support vector regression, decision tree regressor, and extra trees regressor had poor fitting effects. Mangrove texture index ranked first in importance for the model, followed by the mangrove laser point cloud height index, and the laser point cloud intensity index performed the worst in the model. Mangrove aboveground biomass in the study area is high in the north and low in the south, ranging from 38.23 to 171.80 Mg/ha, with an average value of 94.37 Mg/ha. Generally, the XGBR method can better estimate the aboveground biomass of mangroves based on the measured mangrove plot data and UAV low-altitude remote sensing data.}
}
@article{FENG2020105711,
title = {Evaluation of cotton emergence using UAV-based imagery and deep learning},
journal = {Computers and Electronics in Agriculture},
volume = {177},
pages = {105711},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105711},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920314216},
author = {Aijing Feng and Jianfeng Zhou and Earl Vories and Kenneth A. Sudduth},
keywords = {Emergence evaluation, Stand count, Row geo-reference, Real-time processing},
abstract = {Crop emergence is an important agronomic factor for making field management decisions, such as replanting, that are time-sensitive and need to be made at very early stages. Crop emergence, evaluated using plant population, stand count and uniformity, is conventionally quantified manually, not accurate, and labor and time intensive. Unmanned aerial vehicle (UAV)-based imaging systems are able to scout crop fields rapidly. However, data processing can be too slow to make timely decision making. The goal of this study was to develop a novel image processing method for processing UAV images in nearly real-time. In this study, a UAV imaging system was used to capture RGB image frames of cotton seedlings to evaluate stand count and canopy size. Images were pre-processed to correct distortions, calculate ground sample distance and geo-reference cotton rows in the images. A pre-trained deep learning model, resnet 18, was used to estimate stand count and canopy size of cotton seedlings in each image frame. Results showed that the developed method could estimate stand count accurately with R2 = 0.95 in the test dataset. Similar results were achieved for canopy size with an estimation accuracy of R2 = 0.93 in the test dataset. The processing time for each image frame of 20 M pixels with each crop row geo-referenced was 2.22 s (including 1.80 s for pre-processing), which was more efficient than traditional mosaic-based image processing methods. An open-source automated image-processing framework was developed for cotton emergence evaluation and is available to the community for efficient data processing and analytics.}
}
@article{ATITALLAH2021573,
title = {An Enhanced Randomly Initialized Convolutional Neural Network for Columnar Cactus Recognition in Unmanned Aerial Vehicle imagery},
journal = {Procedia Computer Science},
volume = {192},
pages = {573-581},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015465},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Anis Koubaa and Nesrine Atitallah and Henda Ben Ghézala},
keywords = {Convolutional neural networks, Weight initialization, Randomization, Remote sensing images, Recognition, Columnar cactus},
abstract = {Recently, Convolutional Neural Networks (CNNs) have made a great performance for remote sensing image classification. Plant recognition using CNNs is one of the active deep learning research topics due to its added-value in different related fields, especially environmental conservation and natural areas preservation. Automatic recognition of plants in protected areas helps in the surveillance process of these zones and ensures the sustainability of their ecosystems. In this work, we propose an Enhanced Randomly Initialized Convolutional Neural Network (ERI-CNN) for the recognition of columnar cactus, which is an endemic plant that exists in the Tehuacán-Cuicatlán Valley in southeastern Mexico. We used a public dataset created by a group of researchers that consists of more than 20000 remote sensing images. The experimental results confirm the effectiveness of the proposed model compared to other models reported in the literature like InceptionV3 and the modified LeNet-5 CNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall, 97.5% as f1-score, and 0.056 loss.}
}
@article{MAITY2021,
title = {Flying robot path planning techniques and its trends},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.174},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321045405},
author = {Ritu Maity and Ruby Mishra and Prasant {Kumar Pattnaik}},
keywords = {Flying robot, Path planning, Optimization techniques, Collision free, Routing protocols, 2D, 3D},
abstract = {A Flying robot is a system capable of vertical take-off and landing to perform some specific task with no direct human intervention. Flying robots are distinct from other robots by their ability to fly with no direct human control and are capable of making a decision based on the situation. Drones, Aerial robots, Unmanned Aerial Vehicles (UAV) are few examples of flying robots. Path determination of flying robots is one of the most critical aspects of robot Routing design. Path planning is a technique which tells flying robot how to fly, where to fly and to find a collision-free optimal path. Numerous techniques have been put forward over the past few years to find an optimal path for flying robots. This work discusses on new trends led by artificial intelligence and safe human-robot interaction for optimal path planning of aerial robots. Furthermore, a comprehensive study is carried out on various 2D and 3D path planning algorithms done to date and different algorithms are classified based on important criteria for optimal path planning for flying robots.}
}
@article{NIU202015784,
title = {A Low-cost Soil Moisture Monitoring Method by Using Walabot and Machine Learning Algorithms},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {15784-15789},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320304717},
author = {Haoyu Niu and Yanan Wang and Tiebiao Zhao and YangQuan Chen},
keywords = {Soil moisture, proximate sensing, Walabot, machine learning},
abstract = {Soil moisture plays an important role in agricultural processes, which has a significant effect on crop evapotranspiration, the exchange of water, and energy fluxes. Recently, soil moisture can be measured by remote sensing or proximate sensing techniques, such as thermal, optical, and microwave measurements. However, there are limitations to the applications of these methods, such as low spatial resolution, limited surface penetration, and vegetation. In this study, it proposed a new low-cost soil moisture monitoring method by using a Walabot sensor and machine learning algorithms. Walabot is a pocket-sized device cutting-edge technology for Radio Frequency tridimensional sensing. Unlike the remote sensing tools such as unmanned aerial vehicles (UAVs) limited by cloud cover or payload capability, the Walabot can be used flexibly in the field and provide data information more promptly and accurately than UAVs or satellite. By putting different moisture levels of soil on the Walabot, the Walabot can collect radio frequency reflectance data from different levels of soil moisture. Then, machine learning algorithms, such as principal component analysis (PCA), linear discriminant analysis (LDA), have been applied for data processing. Results showed that Walabot has a state-of-art performance in estimating soil moisture.}
}
@article{HU2022147,
title = {S3ANet: Spectral-spatial-scale attention network for end-to-end precise crop classification based on UAV-borne H2 imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {183},
pages = {147-163},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621002823},
author = {Xin Hu and Xinyu Wang and Yanfei Zhong and Liangpei Zhang},
keywords = {Precise crop classification, Spectral attention, Spatial attention, Scale attention, UAV-borne hyperspectral imagery, WHU-Hi dataset},
abstract = {High spatial and spectral resolution (H2) imagery collected by unmanned aerial vehicle (UAV) systems is an important data source for precise crop classification. Although this data source can provide us with abundant information about the crops of interest, it also introduces new challenges for the image processing. Specifically, the spectral similarities of green crops lead to small inter-class distances, and the severe intra-class spectral variability and high spatial heterogeneity in H2 imagery increases the difficulty of precise classification. In addition, the scales of the different crop plots can show great differences, which makes it difficult to determine the optimal patch size for deep learning based classification models. In this paper, a spectral-spatial-scale attention network (S3ANet) is proposed for H2 imagery based precise crop classification. In the proposed method, each channel, each pixel, and each scale perception of the feature map is adaptively weighted to relieve the intra-class spectral variability, the spatial heterogeneity, and the scale difference of the crop plots, respectively. Furthermore, the proposed S3ANet method introduces the additive angular margin loss function to further increase the inter-class distances between the different crops, and reduce the misclassification effect. S3ANet was verified using the public WHU-Hi UAV-borne hyperspectral dataset and the new WHU-Hi-JiaYu dataset, which is a dataset for precise rice classification that was built by the authors. In these experiments, the overall accuracy of the proposed S3ANet method all exceeds 96% under 50 training pixels per class, and it achieved significant improvement compared with some state-of-the-art hyperspectral images classifiers (such as SSRN, CNNCRF and FPGA, etc.). The code of S3ANet is available at http://rsidea.whu.edu.cn/resource_sharing.htm.}
}
@article{ZHONG2020112012,
title = {WHU-Hi: UAV-borne hyperspectral with high spatial resolution (H2) benchmark datasets and classifier for precise crop identification based on deep convolutional neural network with CRF},
journal = {Remote Sensing of Environment},
volume = {250},
pages = {112012},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112012},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720303825},
author = {Yanfei Zhong and Xin Hu and Chang Luo and Xinyu Wang and Ji Zhao and Liangpei Zhang},
keywords = {Precise crop classification, UAV-borne hyperspectral imagery, Convolutional neural network, Conditional random fields, WHU-Hi dataset},
abstract = {Unmanned aerial vehicle (UAV)-borne hyperspectral systems can acquire hyperspectral imagery with a high spatial resolution (which we refer to here as H2 imagery). As a result of the low operating cost, high flexibility, and the ability to achieve real-time data acquisition, UAV-borne hyperspectral systems have become an important data source for remote sensing based agricultural monitoring. However, precise crop classification based on UAV-borne H2 imagery is a challenging task when faced with a number of different crop classes. The traditional hyperspectral classification methods, such as the spectral-based and object-oriented classification methods, have difficulty in classifying H2 imagery, faced with the problems of salt-and-pepper (SP) noise and scale selection. In this article, the deep convolutional neural network with a conditional random field classifier (CNNCRF) framework is proposed for precise crop classification with UAV-borne H2 imagery. In the proposed method, a deep convolutional neural network (CNN) is designed to extract and fuse in-depth spectral and local spatial features, and the conditional random field (CRF) model further incorporates the spatial-contextual information to improve the problem of holes and isolated regions in the classification map. Meanwhile, virtual sample augmentation based on the hyperspectral imaging mechanism is used to lessen the issue of the limited labeled samples. To validate the results, a new dataset—the Wuhan UAV-borne hyperspectral image (WHU-Hi) dataset—has been built for precise crop classification. The experimental results obtained using the WHU-Hi dataset confirm the accuracy and visualization performance of the proposed CNNCRF classification method, which outperforms the previous methods. In addition, the WHU-Hi dataset could serve as a benchmark dataset for hyperspectral image classification studies.}
}
@article{ZHENG202195,
title = {Growing status observation for oil palm trees using Unmanned Aerial Vehicle (UAV) images},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {173},
pages = {95-121},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000083},
author = {Juepeng Zheng and Haohuan Fu and Weijia Li and Wenzhao Wu and Le Yu and Shuai Yuan and Wai Yuk William Tao and Tan Kian Pang and Kasturi Devi Kanniah},
keywords = {Individual tree detection, Growing status, Oil palm, UAV images, Deep learning},
abstract = {For both the positive economic benefit and the negative ecological impact of the rapid expansion of oil palm plantations in tropical developing countries, it is significant to achieve accurate detection for oil palm trees in large-scale areas. Especially, growing status observation and smart oil palm plantation management enabled by such accurate detections would improve plantation planning, oil palm yield, and reduce manpower and consumption of fertilizer. Although existing studies have already reached a high accuracy in oil palm tree detection, rare attention has been paid to automated observation of each single oil palm tree’s growing status. Nowadays, with its high spatial resolution and low cost, Unmanned Aerial Vehicle (UAV) has become a promising tool for monitoring the growing status of individual oil palms. However, the accuracy is still a challenging issue because of the extreme imbalance and high similarity between different classes. In this paper, we propose a Multi-class Oil PAlm Detection approach (MOPAD) to reap both accurate detection of oil palm trees and accurate monitoring of their growing status. Based on Faster RCNN, MOPAD combines a Refined Pyramid Feature (RPF) module and a hybrid class-balanced loss module to achieve satisfying observation of the growing status for individual oil palms. The former takes advantage of multi-level features to distinguish similar classes and detect small oil palms, and the latter effectively resolves the problem of extremely imbalanced samples. Moreover, we elaborately analyze the distribution of different kinds of oil palms, and propose a practical workflow for detecting oil palm vacancy. We evaluate MOPAD using three large-scale UAV images photographed in two sites in Indonesia (denoted by Site 1 and Site 2), containing 363,877 oil palms of five categories: healthy palms, dead palms, mismanaged palms, smallish palms and yellowish palms. Our proposed MOPAD achieves an F1-score of 87.91% (Site 1) and 99.04% (Site 2) for overall oil palm tree detection, and outperforms other state-of-the-art object detection methods by a remarkable margin of 10.37–17.09% and 8.14%-21.32% with respect to the average F1-score for multi-class oil palm detection in Site 1 and Site 2, respectively. Our method demonstrates excellent potential for individual oil palm tree detection and observation of growing status from UAV images, leading to more precise and efficient management of oil palm plantations.}
}
@article{PEREIRA2022106645,
title = {Nitrogen variability assessment of pasture fields under an integrated crop-livestock system using UAV, PlanetScope, and Sentinel-2 data},
journal = {Computers and Electronics in Agriculture},
volume = {193},
pages = {106645},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106645},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921006621},
author = {F.R.da S. Pereira and J.P. {de Lima} and R.G. Freitas and A.A. {Dos Reis} and L.R.do Amaral and G.K.D.A. Figueiredo and R.A.C. Lamparelli and P.S.G. Magalhães},
keywords = {Remote sensing, Machine learning, Ruzi grass, Precision agriculture},
abstract = {In agricultural production, nitrogen (N) deficiency reduces yield, while overapplication may have an unwanted impact on the natural environment and farm finances. Frequent field N monitoring is impractical due to the time and cost required for traditional laboratory analysis. However, remotely sensed data are an alternative to evaluate and monitor crop nutrition status throughout the growing season. This study evaluates the spatial distribution of N in pasture fields cultivated under an integrated crop-livestock system (ICLS) using unmanned aerial vehicle (UAV) and satellites data. We assessed the performance of UAV, PlanetScope, and Sentinel-2A platforms to predict the N parameters: plant N content (PNC), aboveground biomass (AGB), and nutritional nitrogen index (NNI). Moreover, we also simulated a UAV device with a visible light sensor (i.e., red–green-blue (RGB)) as a costly alternative to near-infrared (NIR) sensors to monitor N status. Finally, we assessed whether combining the information from these platforms would improve the N predictions in our study area. The UAV, PlanetScope, and Sentinel-2A data were able to estimate N parameters in the studied pasture fields using the random forest regression algorithm. The UAV multispectral data resulted in the best prediction accuracies (R2: 0.84-PNC, 0.70-AGB, and 0.84-NNI). The combination of UAV_RGB with either PlanetScope (R2: 0.79-PNC, 0.67-AGB, 0.77-NNI) or Sentinel-2A (R2: 0.76-PNC, 0.57-AGB, 0.69-NNI) improved the performance of the three platforms individually (UAV_RGB, PlanetScope or Sentinel-2A). The association between high spatial and spectral resolutions contributes to the highest prediction accuracy in estimating N variability in pasture fields using remote sensing data. Our results suggest that remote sensing techniques are a reliable approach for N monitoring in commercial pasture fields under ICLS.}
}
@article{YU2021107406,
title = {Nussbaum-based finite-time fractional-order backstepping fault-tolerant flight control of fixed-wing UAV against input saturation with hardware-in-the-loop validation},
journal = {Mechanical Systems and Signal Processing},
volume = {153},
pages = {107406},
year = {2021},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2020.107406},
url = {https://www.sciencedirect.com/science/article/pii/S0888327020307925},
author = {Ziquan Yu and Youmin Zhang and Bin Jiang and Chun-Yi Su and Jun Fu and Ying Jin and Tianyou Chai},
keywords = {Fixed-wing UAV, Fault-tolerant control, Fractional-order calculus, Finite-time, Input saturation, Hardware-in-the-loop},
abstract = {This paper investigates a new finite-time fault-tolerant control (FTC) using a fractional-order backstepping iterative design strategy for a fixed-wing unmanned aerial vehicle (UAV) in the presence of actuator faults and input saturation. To compensate for the lumped disturbance induced by the actuator faults, a neural network disturbance observer (NNDO) with finite-time observation capability is first developed as the fault diagnostic unit. Then, based on the diagnosed fault information, fractional-order (FO) calculus is artfully utilized to enhance the FTC performance within the backstepping design architecture. The salient feature of the developed control scheme is that the finite-time NNDO and FO calculus are simultaneously used to significantly increase the FTC performance against unexpected actuator faults. Moreover, to address the input saturation problem, the faulty UAV dynamics is augmented by a new auxiliary system. Furthermore, a Nussbaum function is incorporated into the FTC scheme to further avoid the calculation of the inverse gain matrix involved within the auxiliary system. It is shown by Lyapunov analysis that the tracking errors are convergent in finite time. Finally, comparative simulations are conducted to show the effectiveness of the developed FTC scheme. Some hardware-in-the-loop (HIL) experimental results are illustrated to further demonstrate the feasibility of the proposed finite-time fractional-order fault-tolerant control (FTFOFTC) method.}
}
@article{MARAVALL2015101,
title = {Vision-based anticipatory controller for the autonomous navigation of an UAV using artificial neural networks},
journal = {Neurocomputing},
volume = {151},
pages = {101-107},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2014.09.077},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214013228},
author = {Darío Maravall and Javier {de Lope} and Juan {Pablo Fuentes}},
keywords = {Unmanned aerial vehicles, Vision-based dual anticipatory reactive controllers, Nearest neighbors methods, Topological maps, Neural networks},
abstract = {A vision-based anticipatory controller for the autonomous indoor navigation of an unmanned aerial vehicle (UAV) is the topic of this paper. A dual Feedforward/Feedback architecture has been used as the UAV׳s controller and the K-NN classifier using the gray level image histogram as discriminant variables has been applied for landmarks recognition. After a brief description of the UAV, we first identify the two main components of its autonomous navigation, namely, the landmark recognition and the dual controller based on cerebellar system of living beings, then we focus on the anticipatory module that has been implemented by an artificial neural network. Afterwards, the paper describes the experimental setup and discusses the experimental results centered mainly on the basic UAV׳s behavior of landmark approximation maneuver, which in topological navigation is known as the beaconing or homing problem.}
}
@article{OSCO202097,
title = {A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {160},
pages = {97-106},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619302989},
author = {Lucas Prado Osco and Mauro dos Santos de Arruda and José {Marcato Junior} and Neemias Buceli {da Silva} and Ana Paula Marques Ramos and Érika Akemi Saito Moryia and Nilton Nobuhiro Imai and Danillo Roberto Pereira and José Eduardo Creste and Edson Takashi Matsubara and Jonathan Li and Wesley Nunes Gonçalves},
keywords = {Deep learning, Multispectral image, UAV-borne sensor, Object detection, Citrus tree counting, Orchard},
abstract = {Visual inspection has been a common practice to determine the number of plants in orchards, which is a labor-intensive and time-consuming task. Deep learning algorithms have demonstrated great potential for counting plants on unmanned aerial vehicle (UAV)-borne sensor imagery. This paper presents a convolutional neural network (CNN) approach to address the challenge of estimating the number of citrus trees in highly dense orchards from UAV multispectral images. The method estimates a dense map with the confidence that a plant occurs in each pixel. A flight was conducted over an orchard of Valencia-orange trees planted in linear fashion, using a multispectral camera with four bands in green, red, red-edge and near-infrared. The approach was assessed considering the individual bands and their combinations. A total of 37,353 trees were adopted in point feature to evaluate the method. A variation of σ (0.5; 1.0 and 1.5) was used to generate different ground truth confidence maps. Different stages (T) were also used to refine the confidence map predicted. To evaluate the robustness of our method, we compared it with two state-of-the-art object detection CNN methods (Faster R-CNN and RetinaNet). The results show better performance with the combination of green, red and near-infrared bands, achieving a Mean Absolute Error (MAE), Mean Square Error (MSE), R2 and Normalized Root-Mean-Squared Error (NRMSE) of 2.28, 9.82, 0.96 and 0.05, respectively. This band combination, when adopting σ = 1 and a stage (T = 8), resulted in an R2, MAE, Precision, Recall and F1 of 0.97, 2.05, 0.95, 0.96 and 0.95, respectively. Our method outperforms significantly object detection methods for counting and geolocation. It was concluded that our CNN approach developed to estimate the number and geolocation of citrus trees in high-density orchards is satisfactory and is an effective strategy to replace the traditional visual inspection method to determine the number of plants in orchards trees.}
}
@article{TOGEIRODEALCKMIN2022106574,
title = {Perennial ryegrass biomass retrieval through multispectral UAV data},
journal = {Computers and Electronics in Agriculture},
volume = {193},
pages = {106574},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106574},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005913},
author = {Gustavo {Togeirode Alckmin} and Arko Lucieer and Richard Rawnsley and Lammert Kooistra},
keywords = {Perennial ryegrass, Machine learning, Vegetation indices, Biomass, UAV, Radiometric calibration},
abstract = {Frequent biomass measurement is a key activity for optimal perennial ryegrass (Lolium perenne) management in intensive forage-based dairy operations. Due to the necessary high-frequency (i.e., weekly or monthly) pasture monitoring and continuous trend of larger dairy farms, such activity is perceived as an operational bottleneck. Consequently, substantial effort is directed to the development of accurate and automated technological solutions for biomass assessment. The popularization of unmanned aerial vehicles (UAVs) combined with multispectral cameras should allow for an optimal observational system able to deploy machine learning algorithms for near real-time biomass dry-matter (DM) mapping. For successful operation, these systems should deliver radiometrically accurate orthomosaics and robust models able to generalize across different periods. Nevertheless, the accuracy of radiometric calibration and generalization ability of these models is seldom evaluated. Also, such pipelines should require minimum processing power and allow for fast deployment. This study has established a two-year experiment comparing reflectance measurements between a handheld spectrometer and a commercial multispectral UAV camera. Different algorithms based on regression-tree architecture were contrasted regarding accuracy, speed, and model size. Model performances were validated, providing error-metrics for baseline accuracy and temporal validation. The results have shown that the standard procedure for multispectral imagery radiometric calibration is sub-optimal, requiring further post-processing and presenting low correlation with handheld measurements across spectral bands and dates. Nevertheless, after post-calibration, the use of spectral imagery has presented better baseline error than the point-based sensors, respectively displaying an average of 397.3 and 464.2 kg DM/ha when employed alongside the best performing algorithm (i.e., Cubist). When trained and validated across different years, model performance was largely reduced and deemed unfit for operational purposes. The Cubist/M5 family of algorithms have exhibited advantageous characteristics such as compact model structure, allowing for a higher level of model interpretability, while displaying a smaller size and faster deployment than the Random Forest, Boosted, and Bagged Regression Trees algorithms.}
}
@article{SEO202015852,
title = {Soil Moisture Retrieval from Airborne Multispectral and Infrared Images using Convolutional Neural Network⁎⁎This work was supported by Science and Technology Facilities Council (STFC) under Newton fund with grant number ST/N006852/1.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {15852-15857},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.240},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320305152},
author = {Min-Guk Seo and Hyo-Sang Shin and Antonios Tsourdos},
keywords = {Remote Sensing, Sensor Fusion, Convolutional Neural Network, Soil Moisture Retrieval},
abstract = {This paper deals with the modeling of soil moisture retrieval from multispectral and infrared (IR) images using convolutional neural network (CNN). Since it is difficult to measure the soil moisture level of large fields, it is essential to retrieve soil moisture level from remotely sensed data. Quadrotor unmanned aerial vehicle (UAV) is considered as sensing platform in order to acquire data with high spatial resolution at anytime by non-experts. With considerations both on the availability of sensors for the platform and the information needed to overcome the effects of the canopies covering soil, IR and multispectral images are selected to be used for soil moisture retrieval. In order to prevent information loss by the calculation of parameters from measurements and enhance the applicabiliy for online operations, CNN is applied for the construction of soil moisture retrieval model to use the sensor measurement images directly as input data. Training and testing are conducted for the proposed CNN-based soil moisture retrieval model using the data from actual quadrotor flight over an agricultural field.}
}
@article{GUAN2022102677,
title = {Road marking extraction in UAV imagery using attentive capsule feature pyramid network},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {107},
pages = {102677},
year = {2022},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2022.102677},
url = {https://www.sciencedirect.com/science/article/pii/S0303243422000034},
author = {Haiyan Guan and Xiangda Lei and Yongtao Yu and Haohao Zhao and Daifeng Peng and José {Marcato Junior} and Jonathan Li},
keywords = {Road markings, Capsule, Feature pyramid network, Dense atrous convolution, UAV images},
abstract = {Accurately and precisely delineating road-markings from very high spatial resolution unmanned aerial vehicle (UAV) images face many challenges, such as complex scenarios, diverse road marking sizes and shapes, and absent and occluded road markings. To address these issues, we formulate an attentive capsule feature pyramid network (ACapsFPN) by integrating capsule representations with attention mechanisms into the feature pyramid network (FPN), aiming at improving road marking extraction accuracy. Different from the current convolutional neural network (CNN) models based on scalar neuron representations, capsule networks characterize entity features by leveraging vectorial capsule neurons, whose lengths and instantiation parameters contribute to the identification of features and their variants. By constructing a capsule FPN, the ACapsFPN is capable of extracting and integrating multi-level and multi-scale capsule features to provide high-quality and semantically-strong feature abstractions. By formulating a multi-scale context feature descriptor and the ternary feature attention modules, the ACapsFPN can emphasize informative features to generate a class-specific feature representation. Quantitative and qualitative evaluations show the ACapsFPN provides a valuable means for extracting road markings in UAV images under different kinds of complex conditions. In addition, comparative analyses with existing alternatives also demonstrate the superiority and robustness of the ACapsFPN in UAV road marking extraction.}
}
@article{CHENG2021106172,
title = {UAV photogrammetry-based remote sensing and preliminary assessment of the behavior of a landslide in Guizhou, China},
journal = {Engineering Geology},
volume = {289},
pages = {106172},
year = {2021},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2021.106172},
url = {https://www.sciencedirect.com/science/article/pii/S0013795221001836},
author = {Zhan Cheng and Wenping Gong and Huiming Tang and C. Hsein Juang and Qinglu Deng and Jun Chen and Xiongfei Ye},
keywords = {UAV, Landslide, Failure mechanism, Crack recognition, Geohazards assessment},
abstract = {Unmanned Aerial Vehicle (UAV) technique has been widely utilized in geohazards assessment. In this work, the effectiveness of the UAV photogrammetry in the remote sensing and assessment of the landslide behavior is demonstrated through a case study of a landslide that occurred in Guizhou, China on 10 June 2018. The post-landslide assessments were conducted through a field investigation by a team of experts; and, two UAV photogrammetry-based surveys were carried out. On the basis of a detailed inspection of the high-resolution aerial photographs collected from the UAV photogrammetry and historical satellite imagery, subsurface stratigraphic configuration revealed from borehole explorations and the local rainfall data collected, the failure mechanism of this landslide is investigated. The occurrence of this landslide is probably attributed to the combined influence of the long-term rainfall and the engineering activities at this site. An automatic landslide cracks recognition model, which is based on the deep learning-based image recognition technique of RetinaNet, is further developed to map the cracks at the landslide site. The effectiveness of this automatic crack recognition model is validated through quantitative comparisons between the landslide cracks recognized and the field survey results. Based upon the landslide cracks identified on two different dates after this landslide event (by the field survey and automatic crack recognition model) and the subsurface displacement revealed from a drilled hole, the evolution behavior of this landslide is analyzed. The results show that the stability of this landslide was not achieved during the first slide in June 2018 and the limit of the landslide was increased much from June to November 2018. In such a situation, the elements at risk in the zones that are potentially impacted by this landslide are identified (with the aid of the UAV images collected), and a preliminary consequence assessment is conducted.}
}
@article{SIKERIDIS201869,
title = {Wireless powered Public Safety IoT: A UAV-assisted adaptive-learning approach towards energy efficiency},
journal = {Journal of Network and Computer Applications},
volume = {123},
pages = {69-79},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S108480451830290X},
author = {Dimitrios Sikeridis and Eirini Eleni Tsiropoulou and Michael Devetsikiotis and Symeon Papavassiliou},
keywords = {Public Safety Networks, Internet of Things, UAV, Wireless powered communication, Minority games, Reinforcement learning, Game theory, Energy efficiency},
abstract = {Public Safety Networks (PSN) provide resilient communication paradigms under disaster recovery scenarios. In this context, the increased integration of Internet of Things (IoT) architectures can further support critical and massive information flows. In this paper, we propose a framework that combines Unmanned Aerial Vehicle (UAV)-support with wireless powered communication (WPC) techniques to further improve energy efficiency in a distributed non-orthogonal multiple access (NOMA) PSN. The IoT devices form coalitions by initially choosing their role (coalition head or coalition member) in the network independently and in a distributed fashion, following the theory of Minority Games (MG). Subsequently, the member nodes act as stochastic learning automata to associate with a coalition head using a reinforcement learning technique. Towards extending the PSN's lifetime, we utilize a harvest-transmit-store WPC mechanism, where the IoT nodes harvest energy from the mobile UAV before transmitting their information. The UAV optimal positioning in the Euclidean 3D space is determined through an optimization problem of maximizing the coalition head's total energy availability, as these nodes play a critical role within the PSN acting as emergency gateways. Finally, a non-cooperative game-theoretic approach is adopted to determine the optimal uplink transmission power of each IoT node in a distributed manner and the existence of a unique Nash equilibrium is shown. The performance evaluation of the proposed framework is achieved via modeling and simulation, and the numerical results demonstrate its energy efficiency, robustness, and scalability.}
}
@article{SANKARALINGAM2021108379,
title = {Angle of attack measurement using low-cost 3D printed five hole probe for UAV applications},
journal = {Measurement},
volume = {168},
pages = {108379},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.108379},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120309155},
author = {L. Sankaralingam and C. Ramprasadh},
keywords = {Angle of attack measurement, Multihole probe, Unmanned aerial vehicles, Air data probes},
abstract = {This paper presents a low-cost 3D printed Five Hole Probe (FHP) for Angle of Attack (AOA) measurement and a solution to predict the missing pressure data due to hole blockage using machine learning. The five Hole Probe is designed and fabricated taking reference to the open-source probe called “the oxford probe”. It is intended to use this probe in Mini Unmanned Aerial Vehicles for AOA measurement because the UAVs are primarily designed to carry out unconventional missions like flying at low speed and low altitude, which makes the AOA measurement as one of the absolute requirement. The main challenges involved in mini flyers are their size and weight constraints. Due to these constraints, the probe should have lightweight but highly accurate. Keeping this in mind, suitable design modifications and material selection are made on the Oxford probe and it is manufactured by 3D printing. Then the 3D printed probe is tested in the wind tunnel. In parallel, a CFD analysis of the FHP is carried out in the ANSYS WORKBENCH environment and the results are presented. The CFD results are compared with windtunnel measurements of AOA, and the results are analyzed. Calibration of FHP is carried out, and a lookup table is generated using the pressure coefficients CPα and CPβ corresponding to respective AOA and AOS. For validation, the probe is kept at known AOA and AOS, and the pressure coefficients were calculated. It has been found that the accuracy of measurement is increased by using the mean of ten pressure samples for calculating the pressure coefficients, instead of every pressure sample. And due to the small diameter, the ports may get blocked from dust particles. The machine learning algorithm (SVR – Support Vector Regression) has been trained and tested to tackle the problem of hole blockage. Various regression models were tested to predict the missing Pressure. The Quadratic SVR regression model is selected based on RMSE value. The selected regression model is validated by blocking one of the ports of FHP.}
}
@article{RENAULT2015160,
title = {A Model for Assessing UAV System Architectures},
journal = {Procedia Computer Science},
volume = {61},
pages = {160-167},
year = {2015},
note = {Complex Adaptive Systems San Jose, CA November 2-4, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.180},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030100},
author = {Andrew Renault},
keywords = {Architecture Generation, Architecture Assessment, Unmanned Aerial Vehicle (UAV), Artificial Intelligence (AI)},
abstract = {There is a current need for Unmanned Aerial Vehicle (UAV) systems architecture generation and assessment models. Architecture assessment models that presently exist tend to be fractional and do not account for all dynamic attributes that should be considered in the architecture assessment. There are infinite possibilities of architecture assessment modeling methods that can be used to assess conceptual systems architecture. Developing a good architecture assessment model for evaluating the functional and systems architecture reduces system ambiguity while increasing the tangibility of the system. This paper will present an assessment model specifically for UAV systems and this model can also be adapted to virtually any type of complex adaptive system. On new complex UAV systems it is essential evaluate the appropriate Level of Autonomy (LOA) required to satisfy customer and mission requirements. The assessment model detailed in this paper combines known design heuristics with quantitative, qualitative, and visual representations that assess the probability that the generated architecture will meet performance and capability requirements. The architecture assessment model will assess and generate a combined score that will indicate if the architecture is acceptable or unacceptable. Future UAV systems may reach a LOA that would be considered as Artificial Intelligence (AI); and this pursuit will increase the need for advanced LOA architecture modeling and assessment.}
}
@article{ALSAD201986,
title = {RF-based drone detection and identification using deep learning approaches: An initiative towards a large open source drone database},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {86-97},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18330760},
author = {Mohammad F. Al-Sa’d and Abdulla Al-Ali and Amr Mohamed and Tamer Khattab and Aiman Erbad},
keywords = {UAV detection, Drone identification, Deep learning, Neural networks, Machine learning},
abstract = {The omnipresence of unmanned aerial vehicles, or drones, among civilians can lead to technical, security, and public safety issues that need to be addressed, regulated and prevented. Security agencies are in continuous search for technologies and intelligent systems that are capable of detecting drones. Unfortunately, breakthroughs in relevant technologies are hindered by the lack of open source databases for drone’s Radio Frequency (RF) signals, which are remotely sensed and stored to enable developing the most effective way for detecting and identifying these drones. This paper presents a stepping stone initiative towards the goal of building a database for the RF signals of various drones under different flight modes. We systematically collect, analyze, and record raw RF signals of different drones under different flight modes such as: off, on and connected, hovering, flying, and video recording. In addition, we design intelligent algorithms to detect and identify intruding drones using the developed RF database. Three deep neural networks (DNN) are used to detect the presence of a drone, the presence of a drone and its type, and lastly, the presence of a drone, its type, and flight mode. Performance of each DNN is validated through a 10-fold cross-validation process and evaluated using various metrics. Classification results show a general decline in performance when increasing the number of classes. Averaged accuracy has decreased from 99.7% for the first DNN (2-classes), to 84.5% for the second DNN (4-classes), and lastly, to 46.8% for the third DNN (10-classes). Nevertheless, results of the designed methods confirm the feasibility of the developed drone RF database to be used for detection and identification. The developed drone RF database along with our implementations are made publicly available for students and researchers alike.}
}
@article{LI20201747,
title = {Multi-block SSD based on small object detection for UAV railway scene surveillance},
journal = {Chinese Journal of Aeronautics},
volume = {33},
number = {6},
pages = {1747-1755},
year = {2020},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2020.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S1000936120301126},
author = {Yundong LI and Han DONG and Hongguang LI and Xueyan ZHANG and Baochang ZHANG and Zhifeng XIAO},
keywords = {Deep learning, Multi-block Single Shot MultiBox Detector (SSD), Objection detection, Railway scene, Unmanned aerial vehicle remote sensing},
abstract = {A method of multi-block Single Shot MultiBox Detector (SSD) based on small object detection is proposed to the railway scene of unmanned aerial vehicle surveillance. To address the limitation of small object detection, a multi-block SSD mechanism, which consists of three steps, is designed. First, the original input images are segmented into several overlapped patches. Second, each patch is separately fed into an SSD to detect the objects. Third, the patches are merged together through two stages. In the first stage, the truncated object of the sub-layer detection result is spliced. In the second stage, a sub-layer suppression and filtering algorithm applying the concept of non-maximum suppression is utilized to remove the overlapped boxes of sub-layers. The boxes that are not detected in the main-layer are retained. In addition, no sufficient labeled training samples of railway circumstance are available, thereby hindering the deployment of SSD. A two-stage training strategy leveraging to transfer learning is adopted to solve this issue. The deep learning model is preliminarily trained using labeled data of numerous auxiliaries, and then it is refined using only a few samples of railway scene. A railway spot in China, which is easily damaged by landslides, is investigated as a case study. Experimental results show that the proposed multi-block SSD method produces an overall accuracy of 96.6% and obtains an improvement of up to 9.2% compared with the traditional SSD.}
}
@article{VARELA201454,
title = {Autonomous UAV based search operations using Constrained Sampling Evolutionary Algorithms},
journal = {Neurocomputing},
volume = {132},
pages = {54-67},
year = {2014},
note = {Innovations in Nature Inspired Optimization and Learning Methods Machines learning for Non-Linear Processing},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2013.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S0925231213010904},
author = {Gervasio Varela and Pilar Caamaño and Félix Orjales and Álvaro Deibe and Fernando López-Peña and Richard J. Duro},
keywords = {Evolutionary algorithms, Swarm intelligence, UAV, Team coordination, Robot coordination, Unmanned Aerial Vehicles},
abstract = {This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.}
}
@article{SALDANAOCHOA201953,
title = {A framework for the management of agricultural resources with automated aerial imagery detection},
journal = {Computers and Electronics in Agriculture},
volume = {162},
pages = {53-69},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918315096},
author = {Karla {Saldana Ochoa} and Zifeng Guo},
keywords = {Trees detection, Street segmentation, Agriculture, Machine learning, CNN, UAV},
abstract = {The acquisition of data through remote sensing represents a significant advantage in agriculture, as it allows researchers to perform faster and cheaper inspections over large areas. Currently, extensive researches have been done on technical solutions that can benefit simultaneously from both: vast amounts of raw data (big data) extracted from satellite images and Unmanned Aerial Vehicle (UAV) and novel algorithms in Machine Learning for image processing. In this experiment, we provide an approach that fulfills the necessities of rapid food security, assessment, planning, exploitation, and management of agricultural resources by introducing a pipeline for the automatic localization and classification of four types of fruit trees (coconut, banana, mango, and papaya) and the segmentation of roads in the Kingdom of Tonga, using high-resolution aerial imagery (0.04 m). We used two supervised deep convolutional neural network (CNN): the first, to localize and classify trees (localization) and the second, to mask the streets from the aerial imagery for transportation purposes (semantic segmentation). Additionally, we propose auxiliary methods to determine the density of groupings of each of these trees species, based on the detection results from the localization task and render it in Density Maps that allow comprehending the condition of the agriculture site quickly. Ultimately, we introduce a method to optimize the harvesting of fruits, based on specific sceneries, such as maximum time, path length, and location of warehouses and security points.}
}
@article{GERASTA2018661,
title = {Design of Unmanned Aerial Vehicle Based Remote Time Domain Reflectory},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {28},
pages = {661-665},
year = {2018},
note = {10th IFAC Symposium on Control of Power and Energy Systems CPES 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.779},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318335006},
author = {Olga Joy L. Gerasta and Jonathan Maglasang and Chi-Fang Huang},
keywords = {UAV, Time Domain Reflectory},
abstract = {This manuscript presents remote Time Domain Reflectory (TDR) system to measure soil moisture through electromagnetic wave signal aboard in Unmanned Aerial Vehicle (UAV) platform. The UAV-based remote TDR design is significant in obtaining spatial resolution in different soil types. The proposed UAV software is equipped with several digital signal processing techniques. It has database structure feature to facilitate creation of machine learning dataset from different soil type. It also has capacity to automatically create interpolated model for estimation which is very significant in further training. To verify the results quantitatively, the set-up was investigated using one-way ANOVA test at 0.05 significance level applied in both onsite testing and derived model measurements. The results have shown that there is no significant difference on the mean of data measured in any point locations and the derived model, which simply confirms that the derived model is effective and the designed and developed systems worked well as intended.}
}
@article{DENG2021254,
title = {Air–Ground Surveillance Sensor Network based on edge computing for target tracking},
journal = {Computer Communications},
volume = {166},
pages = {254-261},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420319605},
author = {Xiaoheng Deng and Yajun Liu and Congxu Zhu and Honggang Zhang},
keywords = {UAV, Edge computing, Reliability, Timeliness, Opportunistic},
abstract = {With the development of Internet of Things (IoT), Unmanned Aerial Vehicle (UAV) target tracking recently has received lots of attention in research community. The two major research topics in UAV target tracking include loss of a tracked target and high tracking latency, especially in the case where tracking is conducted in an area of many sight-blocking obstacles that put restrictions on a UAV’s flight altitude and block its line of sight. Most recent work on these topics focuses on improving tracking accuracy through adjusting tracking algorithms based on deep learning. But the complexity of these algorithms requires a computation capacity that a regular UAV cannot afford, and therefore tracking failure probability is still non-negligible. To address this challenge, we propose a new target tracking system, referred to as an Air–Ground Surveillance Sensor Network (AGSSN). We build AGSSN by jointly optimizing the network establishment and data transmission, and we design an algorithm ARIT to achieve an optimal tracking performance. We further carry out a series of simulations by deploying our AGSSN on a university’s campus map, and our simulation results show that our proposed AGSSN system can achieve higher reliability and significantly better performance than regular tracking systems in an area with many visually blocking obstacles.}
}
@article{MA2020105159,
title = {Segmenting ears of winter wheat at flowering stage using digital images and deep learning},
journal = {Computers and Electronics in Agriculture},
volume = {168},
pages = {105159},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105159},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919313845},
author = {Juncheng Ma and Yunxia Li and Keming Du and Feixiang Zheng and Lingxian Zhang and Zhihong Gong and Weihua Jiao},
keywords = {Ears of winter wheat, RGB images, Deep convolutional neural network, Fully convolutional network, Segmentation},
abstract = {Segmenting ears of winter wheat from canopy images was considered to be an important procedure prior to the extraction of related traits. Current segmentation method based on computer vision was susceptible to noise, which is limited in practical applications. In this study, a two-stage segmentation method for ears of winter wheat based on digital images of unit ground area and the state-of-the-art deep learning techniques was proposed. In the coarse segmentation stage, a deep convolutional neural network (DCNN) was constructed to classify the superpixels generated by entropy rate superpixel algorithm, achieving the coarse results. In the fine segmentation stage, a fully convolutional network (FCN) allowing pixel-wise semantic segmentation was constructed to eliminate the non-ear pixels in the coarse results. To compare the results of the proposed two-stage segmentation method, conventionally adopted methods for image segmentation were used. Results showed that the proposed two-stage segmentation method was able to accurately segmenting ears of winter wheat from canopy images captured at flowering stage (Qseg = 0.7197, F1 score = 83.70%, SSIM = 0.8605), outperforming the other compared methods. Generalization tests were conducted to evaluate the utility of the proposed two-stage segmentation method. Results showed that the two-stage segmentation method was still capable of accurately segmenting ears of winter wheat, even though the performance slightly decreased. Change of winter wheat cultivar and lack of descriptive information were two factors that could degrade the performance of the two-stage segmentation method. Tests of the methods on Unmanned Aerial Vehicle (UAV) based RGB images showed the Fully Convolutional Network stride 8 predictions (FCN-8s) had a good chance to achieve satisfactory performances on UAV based canopy images.}
}
@article{OSCO20211,
title = {A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {174},
pages = {1-17},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000307},
author = {Lucas Prado Osco and Mauro {dos Santos de Arruda} and Diogo Nunes Gonçalves and Alexandre Dias and Juliana Batistoti and Mauricio {de Souza} and Felipe David Georges Gomes and Ana Paula Marques Ramos and Lúcio André {de Castro Jorge} and Veraldo Liesenberg and Jonathan Li and Lingfei Ma and José Marcato and Wesley Nunes Gonçalves},
keywords = {Deep learning, UAV imagery, Object detection, Remote sensing, Precision agriculture},
abstract = {Accurately mapping croplands is an important prerequisite for precision farming since it assists in field management, yield-prediction, and environmental management. Crops are sensitive to planting patterns and some have a limited capacity to compensate for gaps within a row. Optical imaging with sensors mounted on Unmanned Aerial Vehicles (UAV) is a cost-effective option for capturing images covering croplands nowadays. However, visual inspection of such images can be a challenging and biased task, specifically for detecting plants and rows on a one-step basis. Thus, developing an architecture capable of simultaneously extracting plant individually and plantation-rows from UAV-images is yet an important demand to support the management of agricultural systems. In this paper, we propose a novel deep learning method based on a Convolutional Neural Network (CNN) that simultaneously detects and geolocates plantation-rows while counting its plants considering highly-dense plantation configurations. The experimental setup was evaluated in (a) a cornfield (Zea mays L.) with different growth stages (i.e. recently planted and mature plants) and in a (b) Citrus orchard (Citrus Sinensis Pera). Both datasets characterize different plant density scenarios, in different locations, with different types of crops, and from different sensors and dates. This scheme was used to prove the robustness of the proposed approach, allowing a broader discussion of the method. A two-branch architecture was implemented in our CNN method, where the information obtained within the plantation-row is updated into the plant detection branch and retro-feed to the row branch; which are then refined by a Multi-Stage Refinement method. In the corn plantation datasets (with both growth phases – young and mature), our approach returned a mean absolute error (MAE) of 6.224 plants per image patch, a mean relative error (MRE) of 0.1038, precision and recall values of 0.856, and 0.905, respectively, and an F-measure equal to 0.876. These results were superior to the results from other deep networks (HRNet, Faster R-CNN, and RetinaNet) evaluated with the same task and dataset. For the plantation-row detection, our approach returned precision, recall, and F-measure scores of 0.913, 0.941, and 0.925, respectively. To test the robustness of our model with a different type of agriculture, we performed the same task in the citrus orchard dataset. It returned an MAE equal to 1.409 citrus-trees per patch, MRE of 0.0615, precision of 0.922, recall of 0.911, and F-measure of 0.965. For the citrus plantation-row detection, our approach resulted in precision, recall, and F-measure scores equal to 0.965, 0.970, and 0.964, respectively. The proposed method achieved state-of-the-art performance for counting and geolocating plants and plant-rows in UAV images from different types of crops. The method proposed here may be applied to future decision-making models and could contribute to the sustainable management of agricultural systems.}
}
@article{LUNGU2020105912,
title = {Auto-landing of UAVs with variable centre of mass using the backstepping and dynamic inversion control},
journal = {Aerospace Science and Technology},
volume = {103},
pages = {105912},
year = {2020},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2020.105912},
url = {https://www.sciencedirect.com/science/article/pii/S1270963820305940},
author = {Mihai Lungu},
keywords = {UAV, Backstepping, Dynamic inversion, Auto-landing},
abstract = {The paper proposes the design methodology of a new auto-landing system for unmanned aerial vehicles (UAVs) with variable centre of mass subject to wind shears, wind gusts, and atmospheric turbulences. Starting from the UAV nonlinear dynamics, a new control architecture is developed. It combines the backstepping and dynamic inversion approaches for the control of UAV attitude angles, lateral deviation from runway, flight altitude, and forward speed during the three landing stages (final approach, glide slope, flare). The estimation of the wind shears, wind gusts, and atmospheric turbulences is achieved via a neural network based disturbance observer, included in the new auto-landing architecture. By its software implementation and validation, the robustness to wind type disturbances and the stability of the new auto-landing system are proved. There are cancelled the altitude error, the lateral deviation from runway, while the UAV trajectory is the desired one with continuous and smooth transition from one stage of landing to another.}
}
@article{YU20202471,
title = {Optimal UAV Circumnavigation Control with Input Saturation Based on Information Geometry},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {2471-2476},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.196},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320304614},
author = {Yangguang Yu and Xiangke Wang and Lincheng Shen},
keywords = {UAV, optimal control, Fisher information, circumnavigation, input saturation},
abstract = {In this paper, we investigate the problem of the optimal circumnavigation around a ground moving target for a fixed-wing unmanned aerial vehicle equipped with a radar. We propose an optimal circumnavigation control law which not only achieves the circumnavigation of a UAV around a moving target, but also maximizes the utilization of the sensor information. Firstly, an optimization criterion reflecting the extent of the sensor information utilization is established based on the Fisher information. Then, based on a neural network, an optimal circumnavigation control law with input saturation is designed. The result is a nearly optimal state feedback controller that has been tuned a priori off-line. Finally, a simulation is presented to demonstrate the validity and correctness of the proposed method.}
}
@article{SONG2020105812,
title = {Identifying sunflower lodging based on image fusion and deep semantic segmentation with UAV remote sensing imaging},
journal = {Computers and Electronics in Agriculture},
volume = {179},
pages = {105812},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105812},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920316938},
author = {Zhishuang Song and Zhitao Zhang and Shuqin Yang and Dianyuan Ding and Jifeng Ning},
keywords = {Sunflower lodging identification, Image fusion, Deep learning, Unmanned aerial vehicle remote sensing image},
abstract = {Sunflower lodging is a common agricultural disorder taking place in the middle and late sunflower growth periods. This disorder reduces the sunflower seed yield, damages the seed quality, and hence usually causes great losses in both crop quantity and quality. Sunflower lodging is mainly caused by extreme and destructive weather events, which have been recently occurring more frequently. This is why it is highly crucial to develop methods for fast and accurate identification of sunflower lodging. In this work, an efficient method for sunflower lodging identification is proposed based on image fusion and deep semantic segmentation of remote sensing images obtained from an unmanned aerial vehicle (UAV). First, the resolution of low-resolution multispectral images was enhanced through matching their features with those of high-resolution visible-range images. Then, for effective lodging assessment, high-quality multispectral images with rich spectral information and high spatial resolution were obtained through fusing the visible-range images and the enhanced multispectral ones. Subsequently, in order to refine the identification outcomes, a variant of the segmentation network (SegNet) deep architecture was developed for semantic segmentation. This variant has skip connections, separable convolution, and a conditional random field. Experimental evaluation shows that the fusion-based approaches clearly outperform the no-fusion ones in terms of the lodging identification accuracy for all compared architectures including support vector machine (SVM), fully convolutional network (FCN), SegNet, and the proposed SegNet variant. Meanwhile, the deep semantic segmentation methods consistently outperform the classical SVM one with hand-crafted features. As well, the improved SegNet method outperformed all of the compared methods and achieved the best accuracies of 84.4% and 89.8% without and with image fusion, respectively, on one test. The corresponding accuracies on another test set were 76.6% and 83.3%, respectively. Moreover, the proposed method can also identify the sunflower lodging and non-lodging patterns and separate them from the background. These capabilities are highly beneficial for lodging hazard assessment and sunflower harvest survey. Overall, the proposed method effectively exploited UAV remote sensing image data with fusion and deep semantic segmentation modules in order to provide a useful reference for sunflower lodging assessment and mapping.}
}
@article{YANG2019142,
title = {Deep convolutional neural networks for rice grain yield estimation at the ripening stage using UAV-based remotely sensed images},
journal = {Field Crops Research},
volume = {235},
pages = {142-153},
year = {2019},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2019.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S037842901831390X},
author = {Qi Yang and Liangsheng Shi and Jinye Han and Yuanyuan Zha and Penghui Zhu},
keywords = {Yield estimation, UAV, Rice crop, Deep learning, CNN},
abstract = {Forecasting rice grain yield prior to harvest is essential for crop management, food security evaluation, food trade, and policy-making. Many successful applications have been made in crop yield estimation using remotely sensed products, such as vegetation index (VI) from multispectral imagery. However, VI-based approaches are only suitable for estimating rice grain yield at the middle stage of growth but have limited capability at the ripening stage. In this study, an efficient convolutional neural network (CNN) architecture was proposed to learn the important features related to rice grain yield from low-altitude remotely sensed imagery. In one major region for rice cultivation of Southern China, a 160-hectare site with over 800 management units was chosen to investigate the ability of CNN in rice grain yield estimation. The datasets of RGB and multispectral images were obtained by a fixed-wing, unmanned aerial vehicle (UAV), which was mounted with a digital camera and multispectral sensors. The network was trained with different datasets and compared against the traditional vegetation index-based method. In addition, the temporal and spatial generality of the trained network was investigated. The results showed that the CNNs trained by RGB and multispectral datasets perform much better than VIs-based regression model for rice grain yield estimation at the ripening stage. The RGB imagery of very high spatial resolution contains important spatial features with respect to grain yield distribution, which can be learned by deep CNN. The results highlight the promising potential of deep convolutional neural networks for rice grain yield estimation with excellent spatial and temporal generality, and a wider time window of yield forecasting.}
}
@article{DING2021102949,
title = {Detection and tracking of infrared small target by jointly using SSD and pipeline filter},
journal = {Digital Signal Processing},
volume = {110},
pages = {102949},
year = {2021},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2020.102949},
url = {https://www.sciencedirect.com/science/article/pii/S1051200420302943},
author = {Lianghui Ding and Xin Xu and Yuan Cao and Guangtao Zhai and Feng Yang and Liang Qian},
keywords = {Infrared small target, Object detection, Target tracking, Deep learning, Anti-UAV},
abstract = {Infrared imaging has been an efficient anti-drone approach due to its low-cost, anti-interference and all-weather working characteristics. However, the detection of Unmanned Aerial Vehicle (UAV) through infrared camera is still a challenging issue because infrared targets in the field-of-view are usually small and lack of shape and texture features. In this paper, we propose an infrared small target detection and tracking method based on deep learning. We improve the network architecture of Single Shot MultiBox Detector (SSD) for infrared small target detection, called Single Shot MultiBox Detector for Small Target (SSD-ST), by dropping low-resolution layers and enhance high-resolution layer. In addition, in order to further reduce the false alarm rate and improve the precision, we also design an Adaptive Pipeline Filter (APF) based on the temporal correlation and motion information to correct the detection results. We have evaluated our method over a dataset with 16177 infrared images and 30 trajectories. The results show our method is more robust than traditional methods in complex scenes, and achieve a recall rate higher than 90% and a precision higher than 95%, which prove that our method can well complete the detection and tracking task of infrared small targets.}
}
@article{MANSOURI20174727,
title = {Remaining Useful Battery Life Prediction for UAVs based on Machine Learning**This work has received partial funding from the European Union’s Horizon 2020 Research and Innovation Programme under the Grant Agreement No.644128, AEROWORKS},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {4727-4732},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.863},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317313253},
author = {Sina Sharif Mansouri and Petros Karvelis and George Georgoulas and George Nikolakopoulos},
keywords = {Battery, Remaining Useful Life, Machine Learning, UAVs, Prediction},
abstract = {Unmanned Aerial Vehicles are becoming part of many industrial applications. The advancements in battery technologies played a crucial part for this trend. However, no matter what the advancements are, all batteries have a fixed capacity and after some time drain out. In order to extend the flying time window, the prediction of the time that the battery will no longer be able to support a flying condition is crucial. This in fact can be cast as a standard Remaining Useful Life prognostic problem, similarly encountered in many fields. In this article, the problem of Remaining Useful Life estimation of a battery, under different flight conditions, is tackled using four machine learning techniques: a linear sparse model, a variant of support vector regression, a multilayer perceptron and an advanced tree based algorithm. The efficiency of the overall proposed machine learning techniques, in the field of batteries prognostics, is evaluated based on multiple experimental data from different flight conditions.}
}
@article{SOARES2021106354,
title = {Cattle counting in the wild with geolocated aerial images in large pasture areas},
journal = {Computers and Electronics in Agriculture},
volume = {189},
pages = {106354},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106354},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003719},
author = {V.H.A. Soares and M.A. Ponti and R.A. Gonçalves and R.J.G.B. Campello},
keywords = {Cattle counting, Precision farming, UAV, Object detection, CNN},
abstract = {Among the production areas with largest impact on global economy, agriculture and livestock play a prominent role. Technologies have been developed in order to automate and increase the efficiency of these fields. The use of Unmanned Aerial Vehicles (UAVs) has been extensively investigated to improve the efficiency of agricultural production and in the monitoring of animals. One of the most important and challenging tasks in animal monitoring is cattle counting. In this paper, we propose a method for detecting and counting cattle in aerial images obtained by UAVs, based on Convolutional Neural Networks (CNNs) and a graph-based optimization to remove duplicated animals detected in overlapped images. We show that maximizing the degree of matching between animals is a suitable strategy to reduce duplicate counting. We also offer a dataset of real images, obtained from large pasture areas, both for training as well as for testing/benchmarking of cattle counting techniques. Our results show that the proposed method is very competitive, outperforming the state of the art in detecting duplicated animals, while significantly reducing the computational cost of the overall counting task.}
}
@article{ALFEO201934,
title = {Enhancing biologically inspired swarm behavior: Metaheuristics to foster the optimization of UAVs coordination in target search},
journal = {Computers & Operations Research},
volume = {110},
pages = {34-47},
year = {2019},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2019.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0305054819301340},
author = {Antonio L. Alfeo and Mario G.C.A. Cimino and Gigliola Vaglini},
keywords = {UAVs, Swarm intelligence, Stigmergy, Flocking, Differential evolution, Distributed targets search},
abstract = {Recent miniaturization in Unmanned Aerial Vehicles (UAVs) technology encourages the use of many small UAVs for search missions in unknown environments, provided that the autonomous and adaptive coordination logic can be effective. In this research field, biologically inspired metaheuristics have been proposed to mimics swarms, flocks, and other coordination schemas. The design and management of such systems is a research challenge when considering (i) combination and optimization of multiple metaheuristics and (ii) enhancements of biologically inspired metaheuristic through technological advances. In this paper the swarm coordination of UAVs employed in target search is based on flocking and stigmergy, to provide robust formation control and dynamic environmental information sharing, respectively. The design of both metaheuristics takes into account UAVs equipment, and the coordination logic is adapted to the mission by means of a differential evolutionary algorithm. This algorithm optimizes the aggregated structural parameters of all metaheuristics to allow the most efficient coordination with respect to the mission environment. Some possible enhancements of stigmergy are studied by simulating target search tasks on synthetic and real-world scenarios.}
}
@article{MATSUI2021101276,
title = {Improving the resolution of UAV-based remote sensing data of water quality of Lake Hachiroko, Japan by neural networks},
journal = {Ecological Informatics},
volume = {62},
pages = {101276},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101276},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000674},
author = {Kai Matsui and Hikaru Shirai and Yoichi Kageyama and Hiroshi Yokoyama},
keywords = {Remote sensing, Unmanned aerial vehicle, Neural network algorithm, Water area, Resolution aerial image},
abstract = {Remote sensing techniques for periodically collecting global data have been widely used for water quality monitoring. Satellites with a ground resolution of 10 to 30 m obtain information over broad areas, making it difficult to use them to evaluate local water quality. Methods for improving satellite data resolution allow for water quality monitoring over both wide and local areas. However, previous studies have failed to target water bodies that undergo drastic changes; moreover, they have not sufficiently examined features contributing to resolution improvement. This study proposes a resolution improvement method using a neural network, which performs learning so that the output matches the high-resolution data when the target pixel and its surrounding pixels in the low-resolution data are input. Moreover, the band ratio of data obtained from an unmanned aerial vehicle was used in the learning process as an input feature. We investigated the (i) band ratio providing highly accurate resolution improvement and (ii) application of a new resolution improvement method to the estimation of suspended solid conditions for water quality parameters. Finally, the proposed method was compared with the bicubic method for validation. The results indicate that the estimated map at the band ratio B/R in the resolution improvement data created via the proposed method can be used to greatly improve the resolution in areas with high levels of suspended solids, compared to the water quality estimation maps created using the bicubic method.}
}
@article{ZEFRI2022102652,
title = {Developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big UAV imagery data},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {106},
pages = {102652},
year = {2022},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102652},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421003597},
author = {Yahya Zefri and Imane Sebari and Hicham Hajji and Ghassane Aniba},
keywords = {Digital photogrammetry, Unmanned Aerial Vehicle, Thermography, Photovoltaics, Big imagery data, Deep learning},
abstract = {The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.}
}
@article{PASSALIS201937,
title = {Deep reinforcement learning for controlling frontal person close-up shooting},
journal = {Neurocomputing},
volume = {335},
pages = {37-47},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.01.046},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219300724},
author = {Nikolaos Passalis and Anastasios Tefas},
keywords = {Reinforcement learning, Deep learning, Drone-based cinematography},
abstract = {Drones, also known as Unmanned Aerial Vehicles, are capable of capturing spectacular aerial shots and can be used to aid several cinematography-oriented tasks. However, flying drones in a professional setting requires the cooperation of several people, increasing the production cost and possibly reducing the quality of the obtained shots. In this paper, a generic way for formulating cinematography-oriented control objectives, that can be used for training any RL agent to automate the drone and camera control processes, is proposed. To increase the convergence speed and learn more accurate deep RL agents, a hint-based reward function is also employed. Two simulation environments, one for drone control and one for camera control, were developed and used for training and evaluating the proposed methods. The proposed method can be combined both with methods capable of performing discrete control, as well as with continuous control methods. It was experimentally demonstrated that the proposed method improves the control accuracy over both handcrafted control techniques and deep RL models trained with other reward functions.}
}
@article{XU2019232,
title = {Morphing control of a new bionic morphing UAV with deep reinforcement learning},
journal = {Aerospace Science and Technology},
volume = {92},
pages = {232-243},
year = {2019},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.05.058},
url = {https://www.sciencedirect.com/science/article/pii/S127096381930344X},
author = {Dan Xu and Zhe Hui and Yongqi Liu and Gang Chen},
keywords = {Morphing aircraft, Deep neural networks, Reinforcement learning, Actor-critic, Model-free, Deep deterministic policy gradient},
abstract = {With rapid development of aviation technology, materials science and artificial intelligence, aircraft design is pursuing higher requirements both in civil and military fields. The new generation of aircraft should have the autonomous capable of performing a variety of tasks (such as take-off and landing, cruising, maneuvering, hover, attack, etc.) under a highly variable flight environment (height, Mach number, etc.) and meanwhile maintaining good performance. Morphing aircraft can use smart materials and actuators to autonomously deform the shape according to the changes in flight environment and mission, and always maintain an optimal aerodynamic shape, therefore get flourished developments. Based on the ability of birds to stretch wings when flying at low speed and to constrict wings at high speed, a new bionic morphing UAV has been designed and developed as the study model by our team. In order to make this new aircraft be able to complete rapid autonomous morphing and aerodynamic performance optimization under different missions and flight conditions, we developed deep neural networks and reinforcement learning techniques as a control strategy. Considering the continuity of the state and action spaces for model, the Deep Deterministic Policy Gradient (DDPG) algorithm based on the actor-critic, model-free algorithm was adopted and verified on the classic nonlinear Pendulum model and Cart Pole game. After the feasibility was verified, morphing aircraft model was controlled to complete prescribed deformation using DDPG algorithm. Furthermore, on the condition that the DDPG algorithm can control morphing well, through training and testing on model using simulation data from wind tunnel tests and actual flight, the autonomous morphing control for the shape optimization of the bionic morphing UAV model could be realized.}
}
@article{ISHIDA201880,
title = {A novel approach for vegetation classification using UAV-based hyperspectral imaging},
journal = {Computers and Electronics in Agriculture},
volume = {144},
pages = {80-85},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917310499},
author = {Tetsuro Ishida and Junichi Kurihara and Fra Angelico Viray and Shielo Baes Namuco and Enrico C. Paringit and Gay Jane Perez and Yukihiro Takahashi and Joel Joseph Marciano},
keywords = {Liquid crystal tunable filter, Unmanned aerial vehicle, Vegetation classification, Machine learning},
abstract = {The use of unmanned aerial vehicle (UAV)-based spectral imaging offers considerable advantages in high-resolution remote-sensing applications. However, the number of sensors mountable on a UAV is limited, and selecting the optimal combination of spectral bands is complex but crucial for conventional UAV-based multispectral imaging systems. To overcome these limitations, we adopted a liquid crystal tunable filter (LCTF), which can transmit selected wavelengths without the need to exchange optical filters. For calibration and validation of the LCTF-based hyperspectral imaging system, a field campaign was conducted in the Philippines during March 28–April 3, 2016. In this campaign, UAV-based hyperspectral imaging was performed in several vegetated areas, and the spectral reflectances of 14 different ground objects were measured. Additionally, the machine learning (ML) approach using a support vector machine (SVM) model was applied to the obtained dataset, and a high-resolution classification map was then produced from the aerial hyperspectral images. The results clearly showed that a large amount of misclassification occurred in shaded areas due to the difference in spectral reflectance between sunlit and shaded areas. It was also found that the classification accuracy was drastically improved by training the SVM model with both sunlit and shaded spectral data. As a result, we achieved a classification accuracy of 94.5% in vegetated areas.}
}
@article{WANG2022106721,
title = {UAV remote sensing based estimation of green cover during turfgrass establishment},
journal = {Computers and Electronics in Agriculture},
volume = {194},
pages = {106721},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106721},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922000382},
author = {Tianyi Wang and Ambika Chandra and Jinha Jung and Anjin Chang},
keywords = {Turfgrass, UAV, Remote sensing, Percent green cover, Establishment, Machine learning, Classification},
abstract = {Turfgrass is an important urban crop in the United States. Determining the percent green cover (PGC) to assess turfgrass quality/health and the rate of establishment is a crucial parameter for evaluating different species and experimental lines within species. However, evaluating the PGC of individual plots within large breeding nurseries in a conventional way, either visually or through digital image analysis is a time-consuming and laborious process. In the present study, we used the unmanned aerial vehicle (UAV) with multispectral and RGB sensors to estimate PGC during turfgrass establishment. We evaluated thirty approaches with different levels of complexity based on vegetation indices, supervised and unsupervised machine learning classification methods, and image processing methods for high-throughput turfgrass PGC estimation. An HSV (Hue-Saturation-Value) color space-based green pixel identification (GPI) method was introduced for the first time for estimating UAV derived PGC (UAVPGC). The results indicate that the GPI achieved the highest coefficient of determination, 0.86–0.96, with lowest mean absolute error when compared to ground percent green cover (GroundPGC). Overall, UAV-derived RGB image-based support vector machine methods were in agreement with GroundPGC (R2 = 0.88–0.95). This suggests that UAV-derived RGB images are adequate in accurately determining percent green cover (green vegetation within an experimental plot); however, multispectral images might offer a solution to determine turfgrass coverage (green and non-green vegetation within an experimental plot) during turfgrass establishment to account for non-green vegetation which is not captured by RGB (visible light spectrum) based estimation of PGC.}
}
@article{HUAMIN2011352,
title = {Neutral-Network -Based Output-Redefinition Control of an Unmanned Aerial Vehicle},
journal = {Procedia Engineering},
volume = {15},
pages = {352-357},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.068},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811015694},
author = {Zhang Hua-min and Ding Xiao-liang and Tao Jin-niu},
keywords = {output redefinition, neutral network, nonminimun phase system},
abstract = {in this thesis, a flight control design of an unmanned aerial vehicle (UAV) using output redefinition based neutral network control technique is presented. The UAV model chosen is a nonlinear non-minimum phase system. The output redefinition technique is used in a way such that the resulting system is minimum phase and can be inverted.NARMA-L2 neural network is trained off-line to identify the forward dynamics of the UAV model with the redefined output, and then inverted to force the real output to approximately track the desired trajectory. The results show that a good tracking performance can be achieved using this control scheme.}
}
@article{CHENG20218811,
title = {Robust finite-time cooperative formation control of UGV-UAV with model uncertainties and actuator faults},
journal = {Journal of the Franklin Institute},
volume = {358},
number = {17},
pages = {8811-8837},
year = {2021},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2021.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0016003221005238},
author = {Wanglei Cheng and Bin Jiang and Ke Zhang and Steven X. Ding},
abstract = {This paper investigates the finite-time cooperative formation control problem for a heterogeneous system consisting of an unmanned ground vehicle (UGV) - the leader and an unmanned aerial vehicle (UAV) - the follower. The UAV system under consideration is subject to modeling uncertainties, external disturbance as well as actuator faults simultaneously, which is associated with aerodynamic and gyroscopic effects, payload mass, and other external forces. First, a backstepping controller is developed to stabilize the leader system to track the desired trajectory. Second, a robust nonsingular fast terminal sliding mode surface is designed for UAV and finite-time position control is achieved using terminal sliding mode technique, which ensures the formation error converges to zero in finite time in the presence of actuator faults and other uncertainties. Furthermore, by combining the radial basis function neural networks (NNs) with adaptive virtual parameter technology, a novel NN-based adaptive nonsingular fast terminal sliding formation controller (NN-ANFTSMFC) is developed. By means of the proposed adaptive control strategy, both uncertainties and actuator faults can be compensated without the prior knowledges of the uncertainty bounds and fault information. By using the proposed control schemes, larger actuator faults can be tolerated while eliminating control chattering. In order to realize fast coordinated formation, the expected position trajectory of UAV is composed of the leader position information and the desired relative distance with UGV, based on local distributed theory, in the three-dimensional space. The tracking and formation controllers are proved to be stable by the Lyapunov theory and the simulation results demonstrate the effectiveness of proposed algorithms.}
}
@article{TRIANTAFYLLIDOU201865,
title = {Fast Deep Convolutional Face Detection in the Wild Exploiting Hard Sample Mining},
journal = {Big Data Research},
volume = {11},
pages = {65-76},
year = {2018},
note = {Selected papers from the 2nd INNS Conference on Big Data: Big Data & Neural Networks},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2017.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617300096},
author = {Danai Triantafyllidou and Paraskevi Nousi and Anastasios Tefas},
keywords = {Deep learning, Convolutional Neural Networks, Face detection},
abstract = {Face detection constitutes a key visual information analysis task in Machine Learning. The rise of Big Data has resulted in the accumulation of a massive volume of visual data which requires proper and fast analysis. Deep Learning methods are powerful approaches towards this task as training with large amounts of data exhibiting high variability has been shown to significantly enhance their effectiveness, but often requires expensive computations and leads to models of high complexity. When the objective is to analyze visual content in massive datasets, the complexity of the model becomes crucial to the success of the model. In this paper, a lightweight deep Convolutional Neural Network (CNN) is introduced for the purpose of face detection, designed with a view to minimize training and testing time, and outperforms previously published deep convolutional networks in this task, in terms of both effectiveness and efficiency. To train this lightweight deep network without compromising its efficiency, a new training method of progressive positive and hard negative sample mining is introduced and shown to drastically improve training speed and accuracy. Additionally, a separate deep network was trained to detect individual facial features and a model that combines the outputs of the two networks was created and evaluated. Both methods are capable of detecting faces under severe occlusion and unconstrained pose variation and meet the difficulties of large scale real-world, real-time face detection, and are suitable for deployment even in mobile environments such as Unmanned Aerial Vehicles (UAVs).}
}
@article{SHIN2011109,
title = {Adaptive support vector regression for UAV flight control},
journal = {Neural Networks},
volume = {24},
number = {1},
pages = {109-120},
year = {2011},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2010.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608010001851},
author = {Jongho Shin and H. {Jin Kim} and Youdan Kim},
keywords = {Support vector regression, Feedback linearization, Unmanned aerial vehicle},
abstract = {This paper explores an application of support vector regression for adaptive control of an unmanned aerial vehicle (UAV). Unlike neural networks, support vector regression (SVR) generates global solutions, because SVR basically solves quadratic programming (QP) problems. With this advantage, the input–output feedback-linearized inverse dynamic model and the compensation term for the inversion error are identified off-line, which we call I-SVR (inversion SVR) and C-SVR (compensation SVR), respectively. In order to compensate for the inversion error and the unexpected uncertainty, an online adaptation algorithm for the C-SVR is proposed. Then, the stability of the overall error dynamics is analyzed by the uniformly ultimately bounded property in the nonlinear system theory. In order to validate the effectiveness of the proposed adaptive controller, numerical simulations are performed on the UAV model.}
}
@article{KHAN2019105650,
title = {Unsupervised anomaly detection in unmanned aerial vehicles},
journal = {Applied Soft Computing},
volume = {83},
pages = {105650},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105650},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619304302},
author = {Samir Khan and Chun Fui Liew and Takehisa Yairi and Richard McWilliam},
keywords = {System health monitioring, Machine learning, Isolation forest, Fault diagnostics and isolation},
abstract = {A real-time anomaly detection solution indicates a continuous stream of operational and labelled data that must satisfy several resources and latency requirements. Traditional solutions to the problem rely heavily on well-defined features and prior supervised knowledge, where most techniques refer to hand-crafted rules derived from known conditions. While successful in controlled situations, these rules assume that good data is available for them to detect anomalies; indicating that these rules will fail to generalise beyond known scenarios. To investigate these issues, current literature is examined for solutions that can be used to detect known and unknown anomalous instances whilst functioning as an out-of-the-box approach for efficient decision-making. The applicability of the isolation forest is discussed for engineering applications using the Aero-Propulsion System Simulation dataset as a benchmark where it is shown to outperform other unsupervised distance-based approaches. Also, the authors have carried out real-time experiments on an unmanned aerial vehicle to highlight further applications of the method. Finally, some conclusions are drawn concerning its simplicity and robustness in handling diagnostic problems.}
}
@article{SUN2022107263,
title = {Stability control of a fixed full-wing layout UAV under manipulation constraints},
journal = {Aerospace Science and Technology},
volume = {120},
pages = {107263},
year = {2022},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.107263},
url = {https://www.sciencedirect.com/science/article/pii/S1270963821007732},
author = {Ruijie Sun and Zhou Zhou and Xiaoping Zhu},
keywords = {Fixed full-wing layout UAV, Stability control, Radial basis function neural network, Dynamic surface control, Sliding mode control, Vector field-based lateral-directional path following control},
abstract = {The fixed full-wing layout unmanned aerial vehicle (UAV) has simple structure and high aerodynamic efficiency, but the special configuration and manipulation characteristics bring challenges to the controller design, which has little related research work. Aiming at the aileronless and rudderless manipulation mode, considering the disturbance and uncertainty, this paper proposes a comprehensive stability controller for the fixed full-wing layout UAV under manipulation constraints. First, adaptive neural network dynamic surface control schemes based on the asymmetric barrier Lyapunov function and auxiliary system are designed for pitch and yaw attitude control. Then, this paper combines the adaptive super-twisting observer, terminal sliding mode control, and double power reaching law to design the airspeed controller and the altitude controller, and further introduces the pitch angle command constraint in the altitude controller. In addition, an improved vector field-based lateral-directional path following control method is designed to realize straight-line and circular orbit path following control. In this paper, the approximation and compensation effects of the radial basis function neural network and robust compensator are fully verified. The design of the pitch angle command constraint specifically considers the characteristics of weak pitch manipulation ability, small available trim airspeed range, and low longitudinal moment of inertia. The developed path following control approach effectively prevents the nonlinear tracking differentiator from generating unreasonable yaw angle commands. Simulation results also show that the proposed controller can enable the fixed full-wing layout UAV to achieve the typical three-dimensional mission flight with high robustness and high precision, and make the fixed full-wing layout UAV remain stable even after suffering stall and manipulation saturation under the action of the gust wind field.}
}
@article{WANG2020105523,
title = {UAV environmental perception and autonomous obstacle avoidance: A deep learning and depth camera combined solution},
journal = {Computers and Electronics in Agriculture},
volume = {175},
pages = {105523},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105523},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920303379},
author = {Dashuai Wang and Wei Li and Xiaoguang Liu and Nan Li and Chunlong Zhang},
keywords = {UAVs, Deep learning, Depth camera, Object detection, Environmental perception, Obstacle avoidance},
abstract = {In agriculture, Unmanned Aerial Vehicles (UAVs) have shown great potential for plant protection. Uncertain obstacles randomly distributed in the unstructured farmland usually pose significant collision risks to flight safety. In order to improve the UAV’s intelligence and minimize the obstacle’s adverse impacts on operating safety and efficiency, we put forward a comprehensive solution which consists of deep-learning based object detection, image processing, RGB-D information fusion and Task Control System (TCS). Taking full advantages of both deep learning and depth camera, this solution allows the UAV to perceive not only the presence of obstacles, but also their attributes like category, profile and 3D spatial position. Based on the object detection results, collision avoidance strategy generation method and the corresponding calculation approach of optimal collision avoidance flight path are elaborated detailly. A series of experiments are conducted to verify the UAV’s environmental perception ability and autonomous obstacle avoidance performance. Results show that the average detection accuracy of CNN model is 75.4% and the mean time cost for processing single image is 53.33 ms. Additionally, we find that the prediction accuracy of obstacle’s profile and position depends heavily on the relative distance between the object and the depth camera. When the distance is between 4.5 m and 8.0 m, errors of object’s depth data, width and height are −0.53 m, −0.26 m and −0.24 m respectively. Outcomes of simulation flight experiments indicated that the UAV can autonomously determine optimal obstacle avoidance strategy and generate distance-minimized flight path based on the results of RGB-D information fusion. The proposed solution has extensive potential to enhance the UAV’s environmental perception and autonomous obstacle avoidance abilities.}
}
@article{QIAO20201292,
title = {MmNet: Identifying Mikania micrantha Kunth in the wild via a deep Convolutional Neural Network},
journal = {Journal of Integrative Agriculture},
volume = {19},
number = {5},
pages = {1292-1300},
year = {2020},
issn = {2095-3119},
doi = {https://doi.org/10.1016/S2095-3119(19)62829-7},
url = {https://www.sciencedirect.com/science/article/pii/S2095311919628297},
author = {Xi QIAO and Yan-zhou LI and Guang-yuan SU and Hong-kun TIAN and Shuo ZHANG and Zhong-yu SUN and Long YANG and Fang-hao WAN and Wan-qiang QIAN},
keywords = { Kunth, invasive alien plant, image processing, deep learning},
abstract = {Mikania micrantha Kunth is an invasive alien weed and known as a plant killer around the world. Accurately and rapidly identifying M. micrantha in the wild is important for monitoring its growth status, as this helps management officials to take the necessary steps to devise a comprehensive strategy to control the invasive weed in the identified area. However, this approach still mainly depends on satellite remote sensing and manual inspection. The cost is high and the accuracy rate and efficiency are low. We acquired color images of the monitoring area in the wild environment using an Unmanned Aerial Vehicle (UAV) and proposed a novel network —MmNet— based on a deep Convolutional Neural Network (CNN) to identify M. micrantha in the images. The network consists of AlexNet Local Response Normalization (LRN), along with the GoogLeNet and continuous convolution of VGG inception models. After training and testing, the identification of 400 testing samples by MmNet is very good, with accuracy of 94.50% and time cost of 10.369 s. Moreover, in quantitative comparative analysis, the proposed MmNet not only has high accuracy and efficiency but also simple construction and outstanding repeatability. Compared with recently popular CNNs, MmNet is more suitable for the identification of M. micrantha in the wild. However, to meet the challenge of wild environments, more M. micrantha images need to be acquired for MmNet training. In addition, the classification labels need to be sorted in more detail. Altogether, this research provides some theoretical and scientific basis for the development of intelligent monitoring and early warning systems for M. micrantha and other invasive species.}
}
@article{PI2020101009,
title = {Convolutional neural networks for object detection in aerial imagery for disaster response and recovery},
journal = {Advanced Engineering Informatics},
volume = {43},
pages = {101009},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.101009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619305828},
author = {Yalong Pi and Nipun D. Nath and Amir H. Behzadan},
keywords = {Disaster management, Convolutional neural network (CNN), Aerial reconnaissance, Deep learning, Unmanned aerial vehicle (UAV)},
abstract = {Accurate and timely access to data describing disaster impact and extent of damage is key to successful disaster management (a process that includes prevention, mitigation, preparedness, response, and recovery). Airborne data acquisition using helicopter and unmanned aerial vehicle (UAV) helps obtain a bird’s-eye view of disaster-affected areas. However, a major challenge to this approach is robustly processing a large amount of data to identify and map objects of interest on the ground in real-time. The current process is resource-intensive (must be carried out manually) and requires offline computing (through post-processing of aerial videos). This research introduces and evaluates a series of convolutional neural network (CNN) models for ground object detection from aerial views of disaster’s aftermath. These models are capable of recognizing critical ground assets including building roofs (both damaged and undamaged), vehicles, vegetation, debris, and flooded areas. The CNN models are trained on an in-house aerial video dataset (named Volan2018) that is created using web mining techniques. Volan2018 contains eight annotated aerial videos (65,580 frames) collected by drone or helicopter from eight different locations in various hurricanes that struck the United States in 2017–2018. Eight CNN models based on You-Only-Look-Once (YOLO) algorithm are trained by transfer learning, i.e., pre-trained on the COCO/VOC dataset and re-trained on Volan2018 dataset, and achieve 80.69% mAP for high altitude (helicopter footage) and 74.48% for low altitude (drone footage), respectively. This paper also presents a thorough investigation of the effect of camera altitude, data balance, and pre-trained weights on model performance, and finds that models trained and tested on videos taken from similar altitude outperform those trained and tested on videos taken from different altitudes. Moreover, the CNN model pre-trained on the VOC dataset and re-trained on balanced drone video yields the best result in significantly shorter training time.}
}
@article{ABBASPOUR2016193,
title = {Detection of Fault Data Injection Attack on UAV Using Adaptive Neural Network},
journal = {Procedia Computer Science},
volume = {95},
pages = {193-200},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.312},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324851},
author = {Alireza Abbaspour and Kang K. Yen and Shirin Noei and Arman Sargolzaei},
keywords = {Cyber-attack, UAV, Sensors, Attacks and Faults Detection, Fault Data Injection, Adaptive Neural Network},
abstract = {A resilient and secure control system should be designed to be as safe and robust as possible in face of different types of attacks such as fault data injection (FDI) attacks; thus, nowadays, the control designers should also consider the probable attacks in their control design from the beginning. For this reason, detection of intentional faults and cyber-attacks attracts a great concern among researchers. This issue plays a great role in the safety of unmanned aerial vehicles (UAVs) due to the need of continuous supervision and control of these systems. In order to have a cyber-attack tolerant (CAT) controller, the attack and the type of attack should be detected in the first step. This paper introduces a new algorithm to detect fault data injection attack in UAV. An adaptive neural network is used to detect the injected faults in sensors of an UAV. An embedded Kalman filter (EKF) is used for online tuning of neural networks weights; these online tuning makes the attack detection faster and more accurate. The simulation results show that the proposed method can successfully detect FDI attacks applied to an UAV.}
}
@article{GARCIAGARIN2021116490,
title = {Automatic detection and quantification of floating marine macro-litter in aerial images: Introducing a novel deep learning approach connected to a web application in R},
journal = {Environmental Pollution},
volume = {273},
pages = {116490},
year = {2021},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2021.116490},
url = {https://www.sciencedirect.com/science/article/pii/S0269749121000683},
author = {Odei Garcia-Garin and Toni Monleón-Getino and Pere López-Brosa and Asunción Borrell and Alex Aguilar and Ricardo Borja-Robalino and Luis Cardona and Morgana Vighi},
keywords = {Remote sensing, Machine learning, Unmanned aerial vehicles, Convolutional neural network, Marine litter},
abstract = {The threats posed by floating marine macro-litter (FMML) of anthropogenic origin to the marine fauna, and marine ecosystems in general, are universally recognized. Dedicated monitoring programmes and mitigation measures are in place to address this issue worldwide, with the increasing support of new technologies and the automation of analytical processes. In the current study, we developed algorithms capable of detecting and quantifying FMML in aerial images, and a web-oriented application that allows users to identify FMML within images of the sea surface. The proposed algorithm is based on a deep learning approach that uses convolutional neural networks (CNNs) capable of learning from unstructured or unlabelled data. The CNN-based deep learning model was trained and tested using 3723 aerial images (50% containing FMML, 50% without FMML) taken by drones and aircraft over the waters of the NW Mediterranean Sea. The accuracies of image classification (performed using all the images for training and testing the model) and cross-validation (performed using 90% of images for training and 10% for testing) were 0.85 and 0.81, respectively. The Shiny package of R was then used to develop a user-friendly application to identify and quantify FMML within the aerial images. The implementation of this, and similar algorithms, allows streamlining substantially the detection and quantification of FMML, providing support to the monitoring and assessment of this environmental threat. However, the automated monitoring of FMML in the open sea still represents a technological challenge, and further research is needed to improve the accuracy of current algorithms.}
}
@article{LIU2021106790,
title = {Anti-saturation adaptive finite-time neural network based fault-tolerant tracking control for a quadrotor UAV with external disturbances},
journal = {Aerospace Science and Technology},
volume = {115},
pages = {106790},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.106790},
url = {https://www.sciencedirect.com/science/article/pii/S127096382100300X},
author = {Kang Liu and Rujing Wang and Xiaodong Wang and Xingxian Wang},
keywords = {Adaptive finite-time control, NN based fault-tolerant control, Input saturation, Quadrotor UAV},
abstract = {This brief is focused on the finite-time tracking control of the quadrotor unmanned aerial vehicle (UAV) subject to external disturbances, parametric uncertainties, actuator faults, and input saturation. According to the principle of the Euler-Lagrangian methodology, a comprehensive dynamics is first decomposed into the position and attitude subsystems to accommodate the controller design. Different from the most of previous studies with an ideal assumption that velocity information is available, a robust exact differentiator is employed to gain the precise information of unavailable velocity in finite time. Then, by utilizing the strong approximation of the radial basis function neural network (RBFNN), the NN-based fault-tolerant control scheme is proposed to compensate for parametric uncertainties, external disturbances and actuator faults. More importantly, a novel adaptive mechanism is responsible for automatically adjusting the NN's parameters, which not only can effectively avoid the selection of large adaptive gains, but can also greatly decrease the number of online-updated leaning parameters. To solve the input saturation problem, an auxiliary dynamics system is constructed. Based on the Lyapunov theoretical framework, it is proved that all the closed-loop signals are uniformly ultimately bounded and the tracking errors can converge into small neighborhoods around the origin in finite time. Finally, simulation results are verified to intuitively reveal the good tracking performance of the introduced composite controller in terms of the finite-time error convergence, strong robustness, fault tolerance, and saturation elimination.}
}
@article{AMORIM2019104932,
title = {Semi-supervised learning with convolutional neural networks for UAV images automatic recognition},
journal = {Computers and Electronics in Agriculture},
volume = {164},
pages = {104932},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.104932},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919305137},
author = {Willian Paraguassu Amorim and Everton Castelão Tetila and Hemerson Pistori and João Paulo Papa},
keywords = {Semi-supervised learning, Convolutional Neural Networks, Fine tuning, Transfer learning},
abstract = {The annotation of large datasets is an issue whose challenge increases as the number of labeled samples available to train the classifier reduces in comparison to the amount of unlabeled data. In this context, semi-supervised learning methods aim at discovering and propagating labels to unsupervised samples, such that their correct labeling can improve the classification performance. Our proposal makes use of semi-supervised methodologies to classify an unlabeled training set that is used to train a Convolution Neural Network using different training strategies. The proposed approach is experimentally validated for soybean leaf and herbivorous pest identification using images captured by Unmanned Aerial Vehicles and can support specialists and farmers in the pest control management in soybean fields, especially when they have a limited amount of labeled samples.}
}
@article{ZICHEN2021102573,
title = {Comparison of the backpropagation network and the random forest algorithm based on sampling distribution effects consideration for estimating nonphotosynthetic vegetation cover},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {104},
pages = {102573},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102573},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002804},
author = {Guo Zi–chen and Wang Tao and Liu Shu–lin and Kang Wen–ping and Chen Xiang and Feng Kun and Zhi Ying},
keywords = {UAV, Nonphotosynthetic vegetation cover, Mu Us sandy land, Random forests, Backpropagation neural network, The sampling distribution},
abstract = {Non–photosynthetic vegetation (NPV) plays a crucial role in arid and semi-arid ecosystems. Remote sensing methods can extract NPV information accurately and quantitatively, which helps in studying the water use, community health, and climate response of vegetation communities. This study used the backpropagation network (BP) and random forest (RF) methods to test NPV cover extraction from Landsat 8-OLI images in Mu Us Sandy Land. Pixel-level NPV cover, photosynthetic vegetation (PV) cover, and bare soil (BS) cover from unmanned aerial vehicle (UAV) field sampling data were used to model the BP and RF. After the generalisation ability of the NPV detection model of BP and RF was evaluated using ten-fold cross-validation, the influence of the distribution of sampling data on BP and RF fitting results was also evaluated. The results were as follows: 1. Considering the selection of appropriate parameters and input layers, both BP and RF exhibited high accuracy in detecting NPV, and the detection accuracy of the RF algorithm for PV and BS was slightly higher than that of the BP algorithm (R2RF-NPV = 0.8426, R2BP-NPV = 0.8277, R2RF-PV = 0.8606, R2BP-PV = 0.8514, R2RF-BS = 0.8123, R2BP-BS = 0.7396). 2. When the BP and RF algorithms were used for geospatial continuous value prediction, the distribution of samples affected the final prediction results. The RF algorithm is less sensitive to the sample data distribution. 3. The random sampling method is the best method for collecting training samples. Even with uniform sampling, when there was a large difference between the distribution of the sampling value and the distribution of the real value, the fitting result would have a large deviation. This paper provides suggestions for the fitting of nonphotosynthetic vegetation in arid and semi-arid regions and provides a new method for evaluating the results of remote sensing regression fitting.}
}
@article{PEREZORTIZ201685,
title = {Selecting patterns and features for between- and within- crop-row weed mapping using UAV-imagery},
journal = {Expert Systems with Applications},
volume = {47},
pages = {85-94},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.10.043},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415007472},
author = {María Pérez-Ortiz and José Manuel Peña and Pedro Antonio Gutiérrez and Jorge Torres-Sánchez and César Hervás-Martínez and Francisca López-Granados},
keywords = {Remote sensing, Unmanned aerial vehicles (UAV), Weed detection, Object based image analysis},
abstract = {This paper approaches the problem of weed mapping for precision agriculture, using imagery provided by Unmanned Aerial Vehicles (UAVs) from sunflower and maize crops. Precision agriculture referred to weed control is mainly based on the design of early post-emergence site-specific control treatments according to weed coverage, where one of the most important challenges is the spectral similarity of crop and weed pixels in early growth stages. Our work tackles this problem in the context of object-based image analysis (OBIA) by means of supervised machine learning methods combined with pattern and feature selection techniques, devising a strategy for alleviating the user intervention in the system while not compromising the accuracy. This work firstly proposes a method for choosing a set of training patterns via clustering techniques so as to consider a representative set of the whole field data spectrum for the classification method. Furthermore, a feature selection method is used to obtain the best discriminating features from a set of several statistics and measures of different nature. Results from this research show that the proposed method for pattern selection is suitable and leads to the construction of robust sets of data. The exploitation of different statistical, spatial and texture metrics represents a new avenue with huge potential for between and within crop-row weed mapping via UAV-imagery and shows good synergy when complemented with OBIA. Finally, there are some measures (specially those linked to vegetation indexes) that are of great influence for weed mapping in both sunflower and maize crops.}
}
@article{TIAN2022108694,
title = {Aboveground biomass of typical invasive mangroves and its distribution patterns using UAV-LiDAR data in a subtropical estuary: Maoling River estuary, Guangxi, China},
journal = {Ecological Indicators},
volume = {136},
pages = {108694},
year = {2022},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2022.108694},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X22001650},
author = {Yichao Tian and Qiang Zhang and Hu Huang and Youju Huang and Jin Tao and Guoqing Zhou and Yali Zhang and Yongwei Yang and Junliang Lin},
keywords = {Machine learning algorithms, UAV–LiDAR, Aboveground biomass, Invasive mangroves, Hydrology and geomorphology, Beibu Gulf, Maoling river estuary},
abstract = {Quantitative assessment of aboveground biomass (AGB) and spatial distribution pattern of exotic mangrove plants (Sonneratia apetala) is of great significance for blue carbon management and ecological restoration in typical subtropical estuaries in China. Although unmanned aerial vehicle (UAV) light detection and ranging (LiDAR) has certain advantages in the investigation of the vertical three-dimensional structure of mangroves, the existing mangrove investigation results are mainly based on plot investigation method. Few scholars use machine learning (ML) method to estimate AGB of invasive Sonneratia apetala by combining plot investigation and LiDAR data. Therefore, on the basis of the height and intensity variables of UAV-LiDAR data, this study used four different ML algorithms, namely, xgboost regressor (XGBR), catboost regressor (CBR), light gradient boosting regressor (LGBR) and AdaBoost regressor (ABR), to estimate AGB of invasive mangrove. Then, the quantitative relationship between invasive mangrove biomass and hydrological unit was analysed. We found that CBR model had the highest accuracy in estimation of mangrove AGB (R2 = 0.7644, RMSE = 11.1725 Mg/ha), followed by XGBR model (R2 = 0.6759, 13.1053 Mg/ha). However, LGBR model (R2 = 0.3506, RMSE = 18.5510 Mg/ha) had poor fitting effect. The AGB of invasive mangroves showed a spatial distribution pattern of high in northwest and low in southeast, and its value ranged from 7.31 Mg/ha to 114.04 Mg/ha, with an average of 25.57 Mg/ha. The AGB of invasive mangroves was independent of the area size of the hydrological response unit but depended on the elevation of the beach surface and the distance from the main tidal ditch. This study demonstrates the feasibility of UAV-LiDAR remote sensing and CBR model in estimating AGB of invasive mangrove species, which can provide scientific basis and technical support for the assessment of invasive mangrove ecosystem and the protection of local mangrove tree species.}
}
@article{WAN202290,
title = {UAV swarm based radar signal sorting via multi-source data fusion: A deep transfer learning framework},
journal = {Information Fusion},
volume = {78},
pages = {90-101},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001834},
author = {Liangtian Wan and Rong Liu and Lu Sun and Hansong Nie and Xianpeng Wang},
keywords = {Signal sorting, Deep transfer learning, Data fusion, UAV swarm},
abstract = {Traditional clustering algorithms can be applied for the pre-sorting step of radar signal sorting. It can effectively dilute the pulse stream and prevent the dense pulse stream from interfering pulse repetition interval (PRI) extraction. However, the pre-sorting deviation will cause interference and missing pulses during the main sorting process. To solve this problem, we deploy the unmanned aerial vehicle (UAV) swarm to monitor reconnaissance areas and put forward a novel deep transfer learning based signal sorting method. The UAV swarm can collect the pulses from different time and spatial domains, and interference and missing pulses in main sorting processing can be relieved dramatically. In our model, we pre-train our model with the data collected from multiple source areas, which corresponds to different areas detected by different parts of UAV swarms. Then we fine-tune our model with the data of the target area. The experimental results prove that the signal sorting accuracy of methods based on deep transfer learning, i.e., YOLO-MobileNet, F-RCNN and cascade RCNN, are higher than that of the baseline methods. In addition, the signal sorting accuracy of traditional methods based on deep learning can be greatly improved with the help of transfer learning.}
}
@article{ABBASPOUR2017317,
title = {Neural adaptive observer-based sensor and actuator fault detection in nonlinear systems: Application in UAV},
journal = {ISA Transactions},
volume = {67},
pages = {317-329},
year = {2017},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0019057816306656},
author = {Alireza Abbaspour and Payam Aboutalebi and Kang K. Yen and Arman Sargolzaei},
keywords = {Adaptive fault detection, Sensor and actuator faults, Unmanned aerial vehicle, Nonlinear dynamic model},
abstract = {A new online detection strategy is developed to detect faults in sensors and actuators of unmanned aerial vehicle (UAV) systems. In this design, the weighting parameters of the Neural Network (NN) are updated by using the Extended Kalman Filter (EKF). Online adaptation of these weighting parameters helps to detect abrupt, intermittent, and incipient faults accurately. We apply the proposed fault detection system to a nonlinear dynamic model of the WVU YF-22 unmanned aircraft for its evaluation. The simulation results show that the new method has better performance in comparison with conventional recurrent neural network-based fault detection strategies.}
}
@article{ZHANG2018229,
title = {Social-class pigeon-inspired optimization and time stamp segmentation for multi-UAV cooperative path planning},
journal = {Neurocomputing},
volume = {313},
pages = {229-246},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218307689},
author = {Daifeng Zhang and Haibin Duan},
keywords = {Multi-UAV cooperative path planning, Time stamp segmentation, Swarm intelligence optimization, Social-class strategy, Pigeon-inspired optimization},
abstract = {Path planning is a significant issue for the safe flight of unmanned aerial vehicles (UAVs). In the situation of multiple UAVs, the problem is even more challenging due to the tough manipulation of coordination and constrains. In this paper, a novel multi-UAV path planning model is developed which is based on the time stamp segmentation (TSS) technique. Other than the traditional methods, the TSS model utilizes the common time bases to simplify the handling of multi-UAV coordination cost. Then, a novel social-class pigeon-inspired optimization (SCPIO) algorithm is proposed as the solver of optimal search on the TSS model. The social-class strategy is utilized to enhance the convergence capabilities of the standard PIO. The efficiency of the proposed method is verified through the comparative experiments and the performance profiles (PP). Integrated experiment in a 3D environment demonstrates the reliability of the proposed system.}
}
@article{MELLAL2020101347,
title = {Obsolescence – A review of the literature},
journal = {Technology in Society},
volume = {63},
pages = {101347},
year = {2020},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2020.101347},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2030350X},
author = {Mohamed Arezki Mellal},
keywords = {Technological progress, Obsolescence, Replacement strategies, Maintenance},
abstract = {Technology progress and fierce competitiveness between manufacturers creates intense pressues to innovatively develop and sell new products. These products could be household or industrial items, such as telephones, computers, machines, robots, unmanned aerial vehicle (UAV), motors, industrial processes, electronic devices, tools, and spare parts in general. The technological progress implies the use of the word “obsolescence.” The new products have higher performance metrics compared to the older units, such as reliability, resilience, memory capacity, improved material, precision, artificial intelligence, lower energy consumption, ergonomics, and safety. Therefore, obsolescence became a paradox in our daily life and industry. This paper presents a literature review of the main published works on obsolescence (1976–2020). Its typologies, consequences and replacement strategies are illustrated.}
}
@article{NASCIMENTO2021230526,
title = {Hybrid physics-informed neural networks for lithium-ion battery modeling and prognosis},
journal = {Journal of Power Sources},
volume = {513},
pages = {230526},
year = {2021},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2021.230526},
url = {https://www.sciencedirect.com/science/article/pii/S0378775321010259},
author = {Renato G. Nascimento and Matteo Corbetta and Chetan S. Kulkarni and Felipe A.C. Viana},
keywords = {Physics-informed neural networks, Li-ion battery prognostics, Battery aging, Scientific machine learning, Uncertainty quantification, Hybrid models},
abstract = {Lithium-ion batteries are commonly used to power unmanned aircraft vehicles (UAVs). The ability to model and forecast remaining useful life of these batteries enables UAV reliability assurance. Building principled accurate models is challenging due to the complex electrochemistry that governs battery operation. Alternatively, reduced order models have the advantage of capturing the overall behavior of battery discharge, although they suffer from simplifications and residual discrepancy. This paper presents a hybrid modeling approach that directly implements physics within deep neural networks. While most of the input–output relationship is captured by reduced-order models, data-driven kernels reduce the gap between predictions and observations. A reduced-order model based on Nernst and Butler–Volmer equations represents the overall battery discharge, and a multilayer perceptron models the battery non-ideal voltage. Battery aging is characterized by time-dependent internal resistance and the amount of available Li-ions, which are modeled through an ensemble of variational Bayesian multilayer perceptrons. The approach is validated using data publicly available through the NASA Prognostics Center of Excellence website. Results showed that our hybrid battery prognosis model can be successfully calibrated, even with a limited number of observations. Moreover, the model can help optimizing battery operation by offering long-term forecast of battery capacity.}
}
@article{JAFARI2020100096,
title = {A biologically-inspired reinforcement learning based intelligent distributed flocking control for Multi-Agent Systems in presence of uncertain system and dynamic environment},
journal = {IFAC Journal of Systems and Control},
volume = {13},
pages = {100096},
year = {2020},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2020.100096},
url = {https://www.sciencedirect.com/science/article/pii/S2468601820300146},
author = {Mohammad Jafari and Hao Xu and Luis Rodolfo Garcia Carrillo},
keywords = {Biologically-inspired reinforcement learning based intelligent control, BELBIC, Flocking control, Multi-Agent Systems},
abstract = {In this paper, we investigate the real-time flocking control of Multi-Agent Systems (MAS) in the presence of system uncertainties and dynamic environment. To handle the impacts from system uncertainties and dynamic environment, a novel reinforcement learning technique, which is appropriate for real-time implementation, has been integrated with multi-agent flocking control in this paper. The Brain Emotional Learning Based Intelligent Controller (BELBIC) is a biologically-inspired reinforcement learning-based controller relying on a computational model of emotional learning in the mammalian limbic system. The learning capabilities, multi-objective properties, and low computational complexity of BELBIC make it a very promising learning technique for implementation in real-time applications. Firstly, a novel brain emotional learning-based flocking control structure is proposed. Then, the real-time update laws are developed to tune the emotional signals based on real-time operational data. It is important to note that this data-driven reinforcement learning approach relaxes the requirement for system dynamics and effectively handle the uncertain impacts of the environment. Using the tuned emotional signals, the optimal flocking control can be obtained. The Lyapunov analysis has been used to prove the convergence of the proposed design. The effectiveness of the proposed design is also demonstrated through numerical and experimental results based on the coordination of multiple Unmanned Aerial Vehicles (UAVs).}
}
@article{LYU2020108,
title = {UAVid: A semantic segmentation dataset for UAV imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {165},
pages = {108-119},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620301295},
author = {Ye Lyu and George Vosselman and Gui-Song Xia and Alper Yilmaz and Michael Ying Yang},
keywords = {UAV, Semantic segmentation, Deep learning, Dataset},
abstract = {Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the nadir views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing high-resolution images in oblique views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50%. We have also explored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF). Our UAVid website and the labeling tool have been published online (https://uavid.nl/).}
}
@article{OUATTARA202015777,
title = {Drone based Mapping and Identification of Young Spruce Stand for Semiautonomous Cleaning⁎⁎Strategic Research Council at the Academy of Finland is acknowledged for financial support of project “Competence-Based Growth Through Integrated Disruptive Technologies of 3D Digitalization, Robotics, Geospatial Information and Image Processing/Computing - Point Cloud Ecosystem” (project decision numbers 293389 and 314312).Issouf Ouattara is a PhD Student with the Autonomous Systems research group, Department of Electrical Engineering and Automation, School of Electrical Engineering, Aalto University, PO Box 15500, FIN-00076 Aalto, FinlandHeikki Hyyti is a researcher with the department of Remote Sensing and Photogrammetry at the Finnish Geospatial Research Institute (FGI), National Land Survey of FinlandArto Visala is a Professor with the Autonomous Systems research group, Department of Electrical Engineering and Automation, School of Electrical Engineering, Aalto University, PO Box 15500, FIN-00076 Aalto, Finland},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {15777-15783},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320304705},
author = {Issouf Ouattara and Heikki Hyyti and Arto Visala},
keywords = {unmanned aerial vehicle, mapping, individual tree identification, convolutional neural network, autonomous vehicle, forestry},
abstract = {We propose a novel method to locate spruces in a young stand with a low cost unmanned aerial vehicle. The method has three stages: 1) the forest area is mapped and a digital surface model and terrain models are generated, 2) the locations of trees are found from a canopy height model using local maximum and watershed algorithms, and 3) these locations are used in a convolution neural network architecture to detect young spruces. Our result for detecting young spruce trees among other vegetation using only color images from a single RGB camera were promising. The proposed method is able to achieve a detection accuracy of more than 91%. As low cost unmanned aerial vehicles with color cameras are versatile today, the proposed work is enabling low cost forest inventory for automating forest management.}
}
@article{ZHANG2021105909,
title = {State and parameter estimation of the AquaCrop model for winter wheat using sensitivity informed particle filter},
journal = {Computers and Electronics in Agriculture},
volume = {180},
pages = {105909},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105909},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920331148},
author = {Tianxiang Zhang and Jinya Su and Cunjia Liu and Wen-Hua Chen},
keywords = {Particle filter, Sensitivity analysis, Machine learning, Multispectral image, Unmanned Aerial Vehicle},
abstract = {Crop models play a paramount role in providing quantitative information on crop growth and field management. However, its prediction performance degrades significantly in the presence of unknown, uncertain parameters and noisy measurements. Consequently, simultaneous state and parameter estimation (SSPE) for crop model is required to maximize its potentials. This work aims to develop an integrated dynamic SSPE framework for the AquaCrop model by leveraging constrained particle filter, crop sensitivity analysis and UAV remote sensing. Both Monte Carlo simulation and one winter wheat experimental case study are performed to validate the proposed framework. It is shown that: (i) the proposed framework with state/parameter bound and parameter sensitivity information outperforms conventional particle filter and constrained particle filter in both state and parameter estimation in Monte Carlo simulations; (ii) in real-world experiment, the proposed approach achieves the smallest root mean squared error for canopy cover estimation among the three algorithms by using day forward-chaining validation method.}
}
@article{PEREZORTIZ2015533,
title = {A semi-supervised system for weed mapping in sunflower crops using unmanned aerial vehicles and a crop row detection method},
journal = {Applied Soft Computing},
volume = {37},
pages = {533-544},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.08.027},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615005281},
author = {M. Pérez-Ortiz and J.M. Peña and P.A. Gutiérrez and J. Torres-Sánchez and C. Hervás-Martínez and F. López-Granados},
keywords = {Remote sensing, Unmanned aerial vehicles (UAV), Weed detection, Machine learning, Hough transform, Support vector machine},
abstract = {This paper presents a system for weed mapping, using imagery provided by unmanned aerial vehicles (UAVs). Weed control in precision agriculture is based on the design of site-specific control treatments according to weed coverage. A key component is precise and timely weed maps, and one of the crucial steps is weed monitoring, by ground sampling or remote detection. Traditional remote platforms, such as piloted planes and satellites, are not suitable for early weed mapping, given their low spatial and temporal resolutions. Nonetheless, the ultra-high spatial resolution provided by UAVs can be an efficient alternative. The proposed method for weed mapping partitions the image and complements the spectral information with other sources of information. Apart from the well-known vegetation indexes, which are commonly used in precision agriculture, a method for crop row detection is proposed. Given that crops are always organised in rows, this kind of information simplifies the separation between weeds and crops. Finally, the system incorporates classification techniques for the characterisation of pixels as crop, soil and weed. Different machine learning paradigms are compared to identify the best performing strategies, including unsupervised, semi-supervised and supervised techniques. The experiments study the effect of the flight altitude and the sensor used. Our results show that an excellent performance is obtained using very few labelled data complemented with unlabelled data (semi-supervised approach), which motivates the use of weed maps to design site-specific weed control strategies just when farmers implement the early post-emergence weed control.}
}
@article{KHOSHBORESHMASOULEH2019172,
title = {Development and evaluation of a deep learning model for real-time ground vehicle semantic segmentation from UAV-based thermal infrared imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {155},
pages = {172-186},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619301765},
author = {Mehdi {Khoshboresh Masouleh} and Reza Shah-Hosseini},
keywords = {UAV-based thermal infrared imagery, Ground vehicle, Semantic segmentation, Deep learning, Gaussian-Bernoulli Restricted Boltzmann Machine},
abstract = {Real-time unmanned aerial vehicles (UAVs)-based thermal infrared images processing, due to high spatial resolution and knowledge of the various infrared radiant energy level distribution of solid bodies, has important applications such as monitoring and control of the various phenomena in different natural situations. One of these applications is monitoring the ground vehicles in cities by using detection or semantic segmentation of them in the thermal images. In this research, our purpose is to improve the performance of deep learning combined model by using Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM) specifications for the segmentation of the ground vehicles from UAV-based thermal infrared imagery. The proposed model is studied in three steps. First, designing the proposed model by using an encoder-decoder structure and addition of extracted features from convolutional layers and restricted Boltzmann machine in the network. Second, the implementation of the research goals on four sets of UAV-based thermal infrared imagery named NPU_CS_UAV_IR_DATA that was collected from some streets of China by using FLIR TAU2 thermal infrared sensor in 2017. Finally, analyzing the performance of the proposed model by using five state-of-the-art models in semantic segmentation. The results evaluated the performance of the proposed model as a robust model with the average precision and average processing time of approximately 0.97, and 19.73 s for all datasets, respectively.}
}
@article{IUGA2018199,
title = {Fall monitoring and detection for at-risk persons using a UAV},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {10},
pages = {199-204},
year = {2018},
note = {3rd IFAC Conference on Embedded Systems, Computational Intelligence and Telematics in Control CESCIT 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.262},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318305834},
author = {Cristi Iuga and Paul Drăgan and Lucian Bușoniu},
keywords = {unmanned aerial vehicles, deep learning, fall detection},
abstract = {We describe a demonstrator application that uses a UAV to monitor and detect falls of an at-risk person. The position and state (upright or fallen) of the person are determined with deep-learning-based computer vision, where existing network weights are used for position detection, while for fall detection the last layer is fine-tuned in additional training. A simple visual servoing control strategy keeps the person in view of the drone, and maintains the drone at a set distance from the person. In experiments, falls were reliably detected, and the algorithm was able to successfully track the person indoors.}
}
@article{HUERTAHERRAIZ2020334,
title = {Photovoltaic plant condition monitoring using thermal images analysis by convolutional neural network-based structure},
journal = {Renewable Energy},
volume = {153},
pages = {334-348},
year = {2020},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2020.01.148},
url = {https://www.sciencedirect.com/science/article/pii/S0960148120301701},
author = {Álvaro {Huerta Herraiz} and Alberto {Pliego Marugán} and Fausto Pedro {García Márquez}},
keywords = {Photovoltaic solar panels, Artificial neural networks, Unmanned aerial vehicle, Thermography, Convolutional neural network, Reliability},
abstract = {The size and the complexity of photovoltaic solar power plants are increasing, and it requires an advanced and robust condition monitoring systems for ensuring their reliability. This paper proposes a novel method for faults detection in photovoltaic panels employing a thermographic camera embedded in an unmanned aerial vehicle. The large amount of data generated by these systems must be processed and analyzed. This paper presents a novel approach to identify panels to detect hot spots, and to set their locations. Two novels region-based convolutional neural networks are unified to generate a robust detection structure. The main contribution is the combination of thermography and telemetry data to provide a response of the panel condition monitoring. The data are acquired and then automatically processed, allowing fault detection during the inspection. A detailed description of the methodology is presented, including the different stages to build the neural networks, i.e. the training process, the acquisition and processing of data and the outcomes generation. A thermographic inspection of a real photovoltaic solar plant is done to validate the proposed methodology. The accuracy, the efficiency and the performance of the approach under different real scenarios are evaluated statistically obtaining satisfactory results.}
}
@article{YU2021103514,
title = {A real-time detection approach for bridge cracks based on YOLOv4-FPM},
journal = {Automation in Construction},
volume = {122},
pages = {103514},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103514},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520310943},
author = {Zhenwei Yu and Yonggang Shen and Chenkai Shen},
keywords = {YOLOv4-FPM, Crack detection, UAV, Real-time detection},
abstract = {In order to realize real-time detection for bridge cracks by unmanned aerial vehicle (UAV), a deep learning model named YOLOv4-FPM is proposed on the basis of the YOLOv4 model. In YOLOv4-FPM, focal loss is used to optimize the loss function, which improves the accuracy and overcomes the challenges of complex background. Pruning algorithm is used to simplify the network and accelerate the detection speed. The multi-scale dataset is used to expand the predictable range of YOLOv4-FPM and enhance its scale robustness. The experimental results show that the mean average precision (mAP) of YOLOv4-FPM is 0.976, which is 0.064 higher than YOLOv4. The size and parameters of the model are reduced to 18.2%, and the model processes in real-time (119FPS) images at 1000 × 1000 pixels, which is 20 times faster than in a recent work. Moreover, it can effectively detect cracks in images of different sizes.}
}
@article{GRANZIG2021102281,
title = {Mapping the fractional coverage of the invasive shrub Ulex europaeus with multi-temporal Sentinel-2 imagery utilizing UAV orthoimages and a new spatial optimization approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {96},
pages = {102281},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2020.102281},
url = {https://www.sciencedirect.com/science/article/pii/S0303243420309247},
author = {Tobias Gränzig and Fabian Ewald Fassnacht and Birgit Kleinschmit and Michael Förster},
keywords = {Invasive species, Remote sensing, Unmanned aerial vehicles (UAV), Sentinel-2, Co-registration, UAV vegetation indices, UAV texture parameters, Time-series, Plant phenology, Chile},
abstract = {Mapping the occurrence patterns of invasive plant species and understanding their invasion dynamics is a crucial requirement for preventing further spread to so far unaffected regions. An established approach to map invasive species across large areas is based on the combination of satellite or aerial remote sensing data with ground truth data from fieldwork. Unmanned aerial vehicles (UAV, also referred to as unmanned aerial systems (UAS)) may represent an interesting and low-cost alternative to labor-intensive fieldwork. Despite the increasing use of UAVs in the field of remote sensing in the last years, operational methods to combine UAV and satellite data are still sparse. Here, we present a new methodological framework to estimate the fractional coverage (FC%) of the invasive shrub species Ulex europaeus (common gorse) on Chiloé Island (south-central Chile), based on ultra-high-resolution UAV images and a medium resolution intra-annual time-series of Sentinel-2. Our framework is based on three steps: 1) Land cover classification of the UAV orthoimages, 2) reduce the spatial shift between UAV-based land cover classification maps and Sentinel-2 imagery and 3) identify optimal satellite acquisition dates for estimating the actual distribution of Ulex europaeus. In Step 2 we translate the challenging co-registration task between two datasets with very different spatial resolutions into an (machine learning) optimization problem where the UAV-based land cover classification maps obtained in Step 1 are systematically shifted against the satellite images. Based on several Random Forest (RF) models, an optimal fit between varying land cover fractions and the spectral information of Sentinel-2 is identified to correct the spatial offset between both datasets. Considering the spatial shifts of the UAV orthoimages and using optimally timed Sentinel-2 acquisitions led to a significant improvement for the estimation of the current distribution of Ulex europaeus. Furthermore, we found that the Sentinel-2 acquisition from November (flowering time of Ulex europaeus) was particularly important in distinguishing Ulex europaeus from other plant species. Our mapping results could support local efforts in controlling Ulex europaeus. Furthermore, the proposed workflow should be transferable to other use cases where individual target species that are visually detectable in UAV imagery are considered. These findings confirm and underline the great potential of UAV-based groundtruth data for detecting invasive species.}
}
@article{KUMAR2021100549,
title = {Efficient Maize Tassel-Detection Method using UAV based remote sensing},
journal = {Remote Sensing Applications: Society and Environment},
volume = {23},
pages = {100549},
year = {2021},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2021.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2352938521000859},
author = {Ajay Kumar and Sai Vikas Desai and Vineeth N. Balasubramanian and P. Rajalakshmi and Wei Guo and B. {Balaji Naik} and M. Balram and Uday B. Desai},
keywords = {Automatic annotation, Labeled data, UAV based Remote sensing, Tassel detection, Maize crop},
abstract = {Regular monitoring is worthwhile to maintain a healthy crop. Historically, the manual observation was used to monitor crops, which is time-consuming and often costly. The recent boom in the development of Unmanned Aerial Vehicles (UAVs) has established a quick and easy way to monitor crops. UAVs can cover a wide area in a few minutes and obtain useful crop information with different sensors such as RGB, multispectral, hyperspectral cameras. Simultaneously, Convolutional Neural Networks (CNNs) have been effectively used for various vision-based agricultural monitoring activities, such as flower detection, fruit counting, and yield estimation. However, Convolutional Neural Network (CNN) requires a massive amount of labeled data for training, which is not always easy to obtain. Especially in agriculture, generating labeled datasets is time-consuming and exhaustive since interest objects are typically small in size and large in number. This paper proposes a novel method using k-means clustering with adaptive thresholding for detecting maize crop tassels to address these issues. The qualitative and quantitative analysis of the proposed method reveals that our method performs close to reference approaches and has an advantage over computational complexity. The proposed method detected and counted tassels with precision: 0.97438, recall: 0.88132, and F1 Score: 0.92412. In addition, using maize tassel detection from UAV images as the task in this paper, we propose a semi-automatic image annotation method to create labeled datasets of the maize crop easily. Based on the proposed method, the developed tool can be used in conjunction with a machine learning model to provide initial annotations for a given image, modified further by the user. Our tool's performance analysis reveals promising savings in annotation time, enabling the rapid production of maize crop labeled datasets.}
}
@article{MAZHAR2013210,
title = {On using neural networks in UAV structural design for CFD data fitting and classification},
journal = {Aerospace Science and Technology},
volume = {30},
number = {1},
pages = {210-225},
year = {2013},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1270963813001454},
author = {Farrukh Mazhar and Abdul Munem Khan and Imran Ali Chaudhry and Mansoor Ahsan},
keywords = {Artificial neural networks (ANN), Aircraft structural design, Unmanned aerial vehicle (UAV), Finite element analysis (FEM/FEA), Computational fluid dynamics (CFD), One way fluid–solid interaction (FSI)},
abstract = {In this paper, we present a novel technique based upon artificial neural network (ANN), for applying aerodynamic pressure loads on the unmanned aerial vehicle (UAV) for the purpose of carrying out finite element (FE) analysis during its structural design process. The objective of the work aims at carrying out one way fluid–solid interaction (FSI) for UAV structural design, in which aerodynamics loads obtained from Computational Fluid Dynamics (CFD) analysis are applied on the vehicle structure for steady-state static FE analysis. CFD analysis of the UAV was performed using FLUENT® software. While, the FE analysis of the UAV was performed in ANSYS® software. As CFD and FE software employ different meshing schemes, thus pressure points coordinates obtained from CFD are not concurrent with the FE mesh. A methodology was, therefore, devised using artificial neural networks to generate pressure functions. In this method, aerodynamic pressure data was first sorted in terms of coordinates for different regions; a feed forward back propagation neural network model was then trained for each data set to generate approximate pressure functions in terms of coordinates. These pressure equations are subsequently used for applying pressure loads on the aircraft for strength and stiffness computation and internal layout design of the UAV structure. The work exhibits successful employment of ANN to match actual pressure profile on the aircraft. In comparison with conventional 3D regression techniques, this technique yielded very satisfactory and reliable results. It has been shown that this technique provided superior performance in comparison with 2D curve fitting employing higher order polynomials.}
}
@article{BANERJEE2021103297,
title = {Report on UG2+ challenge Track 1: Assessing algorithms to improve video object detection and classification from unconstrained mobility platforms},
journal = {Computer Vision and Image Understanding},
volume = {213},
pages = {103297},
year = {2021},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2021.103297},
url = {https://www.sciencedirect.com/science/article/pii/S1077314221001417},
author = {Sreya Banerjee and Rosaura G. VidalMata and Zhangyang Wang and Walter J. Scheirer},
keywords = {Computational photography, Object recognition, Object detection, Evaluation protocols, Deep learning},
abstract = {How can we effectively engineer a computer vision system that is able to interpret videos from unconstrained mobility platforms like UAVs? One promising option is to make use of image restoration and enhancement algorithms from the area of computational photography to improve the quality of the underlying frames in a way that also improves automatic visual recognition. Along these lines, exploratory work is needed to find out which image pre-processing algorithms, in combination with the strongest features and supervised machine learning approaches, are good candidates for difficult scenarios like motion blur, weather, and mis-focus — all common artifacts in UAV acquired images. This paper summarizes the protocols and results of Track 1 of the UG2+ Challenge held in conjunction with IEEE/CVF CVPR 2019. The challenge looked at two separate problems: (1) object detection improvement in video, and (2) object classification improvement in video. The challenge made use of new protocols for the UG2 (UAV, Glider, Ground) dataset, which is an established benchmark for assessing the interplay between image restoration and enhancement and visual recognition. In total, 16 algorithms were submitted by academic and corporate teams, and a detailed analysis of them is reported here.}
}
@article{DEROSA2021105880,
title = {Predicting pasture biomass using a statistical model and machine learning algorithm implemented with remotely sensed imagery},
journal = {Computers and Electronics in Agriculture},
volume = {180},
pages = {105880},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105880},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920330854},
author = {Daniele {De Rosa} and Bruno Basso and Matteo Fasiolo and Johannes Friedl and Bill Fulkerson and Peter R. Grace and David W. Rowlings},
keywords = {Pasture, Generalised additive model, Random forest, Remote sensing, UAV, Multispectral imagery},
abstract = {Accurate daily estimates of pasture biomass can improve the profitability of pasture-based dairy system by optimising input of feed supplements and pasture utilisation. However, obtaining accurate pasture mass estimates is a laborious and time-consuming task. The aim of this study was to test the performance of an integrated method combining remote sensing imagery acquired with a multispectral camera mounted on an unmanned aerial vehicle (UAV), statistical models (generalised additive model, GAM) and machine learning algorithms (random forest, RF) implemented with publicly available data to predict future pasture biomass loads. This study showed that using observations of pasture growth along with environmental and pasture management variables enabled both models, GAM and RF to predict the pre-grazing pasture biomass production at field scale with an average error below 20%. If predictive variables (i.e. post-grazing pasture biomass) were excluded, model performance was reduced, generating errors up to 40%. The post-grazing biomass information at high spatial resolution (<1 m) acquired with the UAV-multispectral camera system was used as predictive variable for future pasture biomass. With the inclusion of the spatially explicit post-grazing biomass variable both models accurately predicted the pre-grazing pasture biomass with an error of 27.7% and 22.9% for RF and GAM, respectively. However, the GAM model performed better than RF in reproducing the spatial variability of pre-grazing pasture biomass. This study demonstrates the capability of statistical and machine learning models implemented with UAV or manually obtained pasture information along with publicly available data to accurately predict future pasture biomass at field and farm scale.}
}
@article{MATASSINI2016308,
title = {Adaptive Control with Neural Networks-based Disturbance Observer for a Spherical UAV},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {17},
pages = {308-313},
year = {2016},
note = {20th IFAC Symposium on Automatic Control in AerospaceACA 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316315257},
author = {Tommaso Matassini and Hyo-Sang Shin and Antonios Tsourdos and Mario Innocenti},
keywords = {Spherical UAV, model uncertainties and external disturbances, disturbance observer, adaptive control, neural networks},
abstract = {This paper develops a control scheme for a Spherical Unmanned Aerial Vehicle (UAV) which can be used in complex scenarios where traditional navigation and communications systems would not succeed. The proposed scheme is based on the nonlinear control theory combined with Adaptive Neural-Networks Disturbance Observer (NN-DOB) and controls the attitude and altitude of the UAV in presence of model uncertainties and external disturbances. The NN-DOB can effectively estimate the uncertainties without the knowledge of their bounds and the control system stability is proven using Lyapunov’s stability theorems. Numerical simulation results demonstrate the validity of the proposed method on the UAV under model uncertainties and external disturbances.}
}
@article{YADAV2021101247,
title = {Identification of disease using deep learning and evaluation of bacteriosis in peach leaf},
journal = {Ecological Informatics},
volume = {61},
pages = {101247},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101247},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000388},
author = {Saumya Yadav and Neha Sengar and Akriti Singh and Anushikha Singh and Malay Kishore Dutta},
keywords = {Bacterial spot, Deep learning, Image processing, Morphological processing, Peach crops},
abstract = {Bacteriosis is one of the most common and devastating diseases for peach crops all over the world. Timely identification of bacteriosis disease is necessary for reducing the usage of pesticides and minimize loss of crops. In this proposed work, convolutional neural network (CNN) models using deep learning and an imaging method is developed for bacteriosis detection from the peach leaf images. In the imaging method, disease affected area is quantified and an adaptive operation is applied to a selected suitable channel of the color image. Gray level slicing is done on pre-processed leaf images for segmentation and automatic identification of bacterial spot disease in peach crops. The datasets are augmented to make the algorithm more robust to different illumination conditions. The proposed work compares the result of imaging method and CNN method. Model architectures generated with different deep learning algorithms, had the best performance reaching an accuracy of 98.75%% identifying the corresponding peach leaf [bacterial and healthy] in 0.185 s per image. The test dataset is consist of images from real cultivation field and also from the laboratory conditions. The significantly high identification rate makes the model diagnostic or early warning tool, and an approach that could be further integrated with the unmanned aerial vehicle to operate in real farming conditions}
}
@article{GONCALVES2021,
title = {Automatic detection of Acacia longifolia invasive species based on UAV-acquired aerial imagery},
journal = {Information Processing in Agriculture},
year = {2021},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2021.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214317321000317},
author = {Carolina Gonçalves and Pedro Santana and Tomás Brandão and Magno Guedes},
keywords = {Pattern recognition, Convolutional neural networks, Invasive plants, },
abstract = {The Acacia longifolia species is known for its rapid growth and dissemination, causing loss of biodiversity in the affected areas. In order to avoid the uncontrolled spread of this species, it is important to effectively monitor its distribution on the agroforestry regions. For this purpose, this paper proposes the use of Convolutional Neural Networks (CNN) for the detection of Acacia longifolia, from images acquired by an unmanned aerial vehicle. Two models based on the same CNN architecture were elaborated. One classifies image patches into one of nine possible classes, which are later converted into a binary model; this model presented an accuracy of 98.6% and 98.5% in the validation and training sets, respectively. The second model was trained directly for binary classification and showed an accuracy of 98.8% and 98.7% for the validation and test sets, respectively. The results show that the use of multiple classes, useful to provide the aerial vehicle with richer semantic information regarding the environment, does not hamper the accuracy of Acacia longifolia detection in the classifier’s primary task. The presented system also includes a method for increasing classification’s accuracy by consulting an expert to review the model’s predictions on an automatically selected sub-set of the samples.}
}
@article{QADIR2021114,
title = {Addressing disasters in smart cities through UAVs path planning and 5G communications: A systematic review},
journal = {Computer Communications},
volume = {168},
pages = {114-135},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421000116},
author = {Zakria Qadir and Fahim Ullah and Hafiz Suliman Munawar and Fadi Al-Turjman},
keywords = {Unmanned aerial vehicles (UAVs), UAV path planning, UAV communication networks (UAVCN), Disaster management, Smart cities},
abstract = {UAVs are increasingly incorporated in a wide range of domains such as disaster management and rescue missions. UAV path planning deals with finding the most optimal or shortest path for UAVs such that minimum energy and resources are utilized. This paper examines the path planning algorithms for UAVs through a literature survey conducted on 139 systematically retrieved articles published in the last decade that are narrowed down to 36 highly relevant articles. As retrieved from the shortlisted articles, the path planning algorithms include RRT, Artificial Potential, Voronoi, D-Star, A-Star, Dijkstra, MILP, Neural Network, Ant Colony Optimization, and Particle Swarm Optimization that are classified into four main types: Model-based, Conventional, Learning-based, and Cell-based. Most of the disaster-related articles are focused on the post-disaster phase only and use conventional and learning-based algorithms with applications to localize victims and optimize paths. Regarding the UAV communication network (UAVCN), the key challenges are communication issues, resource allocation, UAV deployment, defining UAV trajectory, and content security. UAV path planning’s key barriers are path optimization, path completeness, optimality, efficiency, and achieving robustness. Accordingly, a holistic IoT-powered UAV-based smart city management system has been recommended in the current study where all the smart city key components are integrated to address disasters like floods, earthquakes, and bush fire. The proposed holistic system can help prepare for disasters and mitigate them as soon as these arise and help enhance the smart city governance.}
}
@article{CHEN2021101205,
title = {Bottom-up image detection of water channel slope damages based on superpixel segmentation and support vector machine},
journal = {Advanced Engineering Informatics},
volume = {47},
pages = {101205},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101205},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301749},
author = {Junjie Chen and Donghai Liu},
keywords = {Mega infrastructure, Image detection, Unmanned aerial vehicles (UAV), Slope damages, Machine learning, Superpixel segmentation},
abstract = {The operation of water supply channels is threatened by the occasionally occurred slope damages. Timely detection of their occurrence is critical for the rapid enforcement of mitigation measures. However, current practices based on routine inspection and structural heath monitoring are inefficient, laborious and tend to be biased. As an attempt to address the limitations, this paper proposes a bottom-up image detection approach for slope damages, which includes four steps, i.e. superpixel segmentation, feature handcrafting, superpixel classification based on support vector machine (SVM), and slope damage recognition. The approach employs a bottom-up strategy to infer the upper-level slope condition from the classification results of individual superpixels in the bottom level. Experiments were conducted to demonstrate the effectiveness of the approach. The handcrafted feature “LBP + HSV” was demonstrated to be effective in characterizing the image features of slope damages. An SVM model with “LBP + HSV” as input can reliably identify the slope condition in superpixels. Based on the SVM model, the bottom-up strategy achieved high recognition performance, of which the overall accuracy can be up to 91.7%. The proposed approach has potential to facilitate the early and comprehensive awareness of slope damages along the entire route of water channel by the integration with unmanned aerial vehicles.}
}
@article{HE2021112731,
title = {Integration of multi-scale remote sensing data for reindeer lichen fractional cover mapping in Eastern Canada},
journal = {Remote Sensing of Environment},
volume = {267},
pages = {112731},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112731},
url = {https://www.sciencedirect.com/science/article/pii/S003442572100451X},
author = {Liming He and Wenjun Chen and Sylvain G. Leblanc and Julie Lovitt and André Arsenault and Isabelle Schmelzer and Robert H. Fraser and Rasim Latifovic and Lixin Sun and Christian Prévost and H. Peter White and Darren Pouliot},
keywords = {Lichen fractional cover, Landsat, Mapping, Deep learning, Multi-scale, Machine learning},
abstract = {Reindeer lichens (Cladonia spp.) are an essential food source for caribou especially during winter. They can also be a valuable indicator for ecosystem health and climate change. Inventory of lichen abundance at regional scales is required to assess availability within caribou ranges, and assess potential declines from natural and anthropogenic disturbances. Previous studies have mapped lichen cover and volume using remote sensing, but these efforts were often constrained by the limited availability of ground truth information needed for model calibration and validation. In this study, we leveraged unoccupied aerial vehicle (UAV) surveys and WorldView (WV) satellite scenes in a nested upscaling approach in order to expand the number of training samples at the 30 m Landsat resolution. These were used to develop machine learning models to map fractional reindeer lichen cover in Eastern Canada. We found that the best correlation between UAV and WV derived lichen coverages exists at an optimal scale that is slightly larger than 30 m and varies with landscape type and observation geometry. Based on training data from UAV-calibrated lichen coverage from WV data, a neural network model with simple structure achieved a root mean square error (RMSE) = 0.09, a mean absolute error (MAE) = 0.07 and R2 = 0.79 for mapping fractional lichen cover from Landsat without the use of ancillary data. We then applied our model and Landsat data to produce a lichen fractional cover map for the Red Wine Mountain caribou herd range in Labrador, NL and the Manicouagan caribou herd range in Québec. Validation against domain-averaged lichen cover in eight UAV survey sites suggests an accuracy with RMSE = 0.04, MAE = 0.03 and R2 = 0.62 for low lichen cover. Compared to aggregated lichen cover at 30 m from UAV surveys, map accuracy decreases to RMSE = 0.09, MAE = 0.06, and R2 = 0.49, partially due to registration error between UAV and Landsat images. Our study demonstrates that upscaling of lichen cover from UAV data to Landsat via an intermediate image scale is an effective regional-scale mapping approach.}
}
@article{HE2021107485,
title = {Neuro-adaptive singularity-free finite-time attitude tracking control of quadrotor UAVs},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107485},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107485},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004407},
author = {Xiongxiong He and Meiling Tao and Shuzong Xie and Qiang Chen},
keywords = {Adaptive control, Finite-time control, Neural network, Quadrotors, Singularity-free sliding mode surface},
abstract = {In this paper, a neuro-adaptive finite-time control law is proposed for the quadrotor unmanned aerial vehicles. Instead of employing any piecewise continuous functions, a singularity-free terminal sliding mode surface and an auxiliary function are presented to overcome the singularity issues resulted from the differentiation of the sliding variable and the error-related inverse matrix in the controller design, respectively. A simple neural network is employed to estimate the unknown dynamics, such that no prior knowledge on system model uncertainties is required for designing attitude controllers. With the presented control law, the finite-time convergence of the attitude and angular velocity errors can be guaranteed by rigorously theoretical analysis, and numerical simulations are performed to demonstrate the satisfactory performance of the proposed method.}
}
@article{WANG2021106320,
title = {An image segmentation method based on deep learning for damage assessment of the invasive weed Solanum rostratum Dunal},
journal = {Computers and Electronics in Agriculture},
volume = {188},
pages = {106320},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106320},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003379},
author = {Qifan Wang and Man Cheng and Xuepeng Xiao and Hongbo Yuan and Jiajun Zhu and Caihu Fan and Jinlin Zhang},
keywords = {Invasive weed, UAV, Convolutional neural network, Image segmentation, Damage assessment},
abstract = {Solanum rostratum Dunal is a common invasive weed that can cause significant harm to local natural environments and ecosystems. The prerequisite for preventing and managing the invasion of Solanum rostratum Dunal is timely detection and reasonable assessment of its damage level. Therefore, this paper proposes a deep learning-based image segmentation method for the detection of the invasion degree of Solanum rostratum Dunal. The Solanum rostratum Dunal images are acquired by UAV and then cropped into sub-images of the same size following a specific processing method. The sub-images are fed into a U-Net based convolutional neural network DeepSolanum-Net for processing. The pixels belonging to the Solanum rostratum Dunal plants are extracted and labeled after the sub-images pass through the DeepSolanum-Net. All the processed sub-images are stitched and reduced to the size of the original image, and all the target pixels belonging to the Solanum rostratum Dunal in the original image are segmented out from the background image. The coverage rate and the area covered by the Solanum rostratum Dunal on the ground are calculated based on the image segmentation results and the flight altitude of the UAV. A field test is executed and the test results demonstrate that the recognition precision of effective pixels of Solanum rostratum Dunal plants reach 89.95% and the recall rate reach 90.3% when the proposed method is used.}
}
@article{KIM2019168,
title = {Remote proximity monitoring between mobile construction resources using camera-mounted UAVs},
journal = {Automation in Construction},
volume = {99},
pages = {168-182},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518304102},
author = {Daeho Kim and Meiyin Liu and SangHyun Lee and Vineet R. Kamat},
keywords = {Struck-by accident, Proximity monitoring, Unmanned aerial vehicle, Deep neural network, Computer vision},
abstract = {Struck-by accidents have resulted in a significant number of fatal and nonfatal injuries in the construction industry. As a proactive safety measure against struck-by hazards, the authors present an Unmanned Aerial Vehicle (UAV)-assisted visual monitoring method that can automatically measure proximities among construction entities. To attain this end, this research conducts two research thrusts: (i) object localization using a deep neural network, YOLO-V3; and (ii) development of an image rectification method that allows for the measurement of actual distance from a 2D image collected from a UAV. Tests on real-site aerial videos show the promising accuracy of the proposed method; the mean absolute distance errors for estimated proximity were less than 0.9 m and the mean absolute percentage errors were around 4%. The proposed method enables the advanced detection of struck-by hazards around workers, which in turn can make timely intervention possible. This proactive intervention can ultimately promote a safer working environment for construction workers.}
}