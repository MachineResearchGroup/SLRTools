@article{VANDEVIJVER2020105106,
title = {In-field detection of Alternaria solani in potato crops using hyperspectral imaging},
journal = {Computers and Electronics in Agriculture},
volume = {168},
pages = {105106},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105106},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919304582},
author = {Ruben {Van De Vijver} and Koen Mertens and Kurt Heungens and Ben Somers and David Nuyttens and Irene Borra-Serrano and Peter Lootens and Isabel Roldán-Ruiz and Jürgen Vangeyte and Wouter Saeys},
keywords = {Precision crop farming, Crop protection, Proximal sensing, PLS-DA, SVM},
abstract = {Automatic detection of early blight caused by Alternaria solani could promote a drastic reduction in the consumption of plant protection agents and the related production losses. A proximal sensing platform was constructed and calibrated for acquiring high resolution hyperspectral images in the field, and used to accurately map Alternaria lesions. High resolution canopy reflectance images were obtained for 32 potato plants that had been infected with A. solani and 32 healthy reference plants. Spectral classifiers like partial least squares discriminant analysis (PLS-DA) and support vector machines (SVM) based on PCA scores were tested to discriminate affected and non-affected pixels. Both spectral classifiers performed well at pixel level with accuracies above 0.92. The NIR region (750 nm) was identified as the most discriminative part of the spectrum for detecting the lesions. As the disease pressure is typically expressed as the number of lesions per area, the accuracy was also evaluated at this level. This indicated a considerable number of false detections at the edges of the leaves and the leaf axils. Therefore, a decision tree was designed based on expert knowledge about the shape of Alternaria lesions, and used to post-process the classified images. This reduced the number of false detections, increasing the precision from 0.17 to 0.22 at the expense of a reduction in recall from 0.88 to 0.84. This leaves considerable room for improvement in the classification accuracy at the object level. We learned that (1) few, broad wavelengths are sufficient and (2) spatial context is essential for the detection of lesions caused by Alternaria infection. The application of more powerful object classification techniques such as convolutional neural networks to enhance the model performance by efficiently encapsulating the spatial context in the classifier might further improve the detection performance. This could pave the way to UAV or tractor based Alternaria mapping.}
}
@article{XU2020102173,
title = {Tree species classification using UAS-based digital aerial photogrammetry point clouds and multispectral imageries in subtropical natural forests},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {92},
pages = {102173},
year = {2020},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2020.102173},
url = {https://www.sciencedirect.com/science/article/pii/S0303243420300647},
author = {Zhong Xu and Xin Shen and Lin Cao and Nicholas C. Coops and Tristan R.H. Goodbody and Tai Zhong and Weidong Zhao and Qinglei Sun and Sang Ba and Zhengnan Zhang and Xiangqian Wu},
keywords = {UAS, Multispectral, High-resolution, Individual tree crown delineation, Tree species classification},
abstract = {Tree species composition of forest stand is an important indicator of forest inventory attributes for assessing ecosystem health, understanding successional processes, and digitally displaying forest biodiversity. In this study, we acquired high spatial resolution multispectral and RGB imagery over a subtropical natural forest in southwest China using a fixed-wing UAV system. Digital aerial photogrammetric (DAP) technique was used to generate multi-spectral and RGB derived point clouds, upon which individual tree crown (ITC) delineation algorithms and a machine learning classifier were used to identify dominant tree species. To do so, the structure-from-motion method was used to generate RGB imagery-based DAP point clouds. Then, three ITC delineation algorithms (i.e., point cloud segmentation (PCS), image-based multiresolution segmentation (IMRS), and advanced multiresolution segmentation (AMRS)) were used and assessed for ITC detection. Finally, tree-level metrics (i.e., multispectral, texture and point cloud metrics) were used as metrics in the random forest classifier used to classify eight dominant tree species. Results indicated that the accuracy of the AMRS ITC segmentation was highest (F1-score = 82.5 %), followed by the segmentation using PCS (F1-score = 79.6 %), the IMRS exhibited the lowest accuracy (F1-score = 78.6 %); forest types classification (coniferous and deciduous) had a higher accuracy than the classification of all eight tree species, and the combination of spectral, texture and structural metrics had the highest classification accuracy (overall accuracy = 80.20 %). In the classification of both eight tree species and two forest types, the classification accuracies were lowest when only using spectral metrics, indicated that the texture metrics and point cloud structural metrics had a positive impact on the classification (the overall accuracy and kappa accuracy increased by 1.49–4.46 % and 2.86–6.84 %, respectively).}
}
@article{KUMAR201618,
title = {Wireless telemetry system for real-time estimation of ship air wakes with UAVs},
journal = {Mechatronics},
volume = {36},
pages = {18-26},
year = {2016},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2016.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0957415816300113},
author = {Anil Kumar and Pinhas Ben-Tzvi and Wael Saab and Murray R. Snyder},
keywords = {Air turbulence estimation, Artificial neural networks, RC helicopter, Ship air wake, Wireless instrumentation, telemetry},
abstract = {This paper presents a wireless instrumentation system developed for real-time estimation of air turbulence patterns arising from the interaction of wind with any structure under consideration, which is an important study in the aerospace industry. In particular, this paper focuses on the application of the proposed system in a naval research problem for off-board measurement of ship air wake patterns using an instrumented radio controlled (RC) helicopter. We propose the use of an Inertial Measurement Unit (IMU) as a sensor to measure air wake in the form of induced vibrations on the helicopter while it maneuvers through regions of active air wake. The proposed system makes use of Back Propagation Neural Networks to compensate for the vibrational noise contributed by pilot inputs. The instrumentation system was integrated and tested on a modified training vessel in the Chesapeake Bay, which provided a wide range of wind conditions.}
}
@article{CHOI2017158,
title = {Two-layer obstacle collision avoidance with machine learning for more energy-efficient unmanned aircraft trajectories},
journal = {Robotics and Autonomous Systems},
volume = {98},
pages = {158-173},
year = {2017},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015301421},
author = {Youngjun Choi and Hernando Jimenez and Dimitri N. Mavris},
keywords = {Obstacle avoidance, Optimal trajectory, Clustering algorithm, Model predictive control, UAV, Path-planning},
abstract = {This paper proposes a new two-layer obstacle avoidance algorithm that allows an unmanned aircraft system to avoid multiple obstacles with minimal effort. The algorithm includes a global-path optimization that identifies the number of obstacles resulting from a clustering technique based on obstacle information from an airborne sensor, and specifies a potential threat. A local-path trajectory optimization employs a model predictive control structure based on a multi-phase optimal trajectory resulting from approximated dynamics, vehicle constraints, and the result of the global-path optimization. Numerical flight simulations are conducted with a conventional one-layer obstacle avoidance algorithm and the two-layer obstacle avoidance algorithm. The results of the numerical simulation show that the proposed two-layer optimal obstacle avoidance algorithm generates more energy-efficient avoidance trajectories when an unmanned aircraft meets multiple obstacles.}
}
@article{ULLAH2022293,
title = {Neuro-adaptive fast integral terminal sliding mode control design with variable gain robust exact differentiator for under-actuated quadcopter UAV},
journal = {ISA Transactions},
volume = {120},
pages = {293-304},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.02.045},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821001294},
author = {Safeer Ullah and Qudrat Khan and Adeel Mehmood and Syed Abdul Mannan Kirmani and Omar Mechali},
keywords = {Quadcopter, Under-actuated system, Regular form, Neural network, Robust differentiator, Integral terminal sliding mode control},
abstract = {In this paper, a robust global fast terminal attractor based full flight trajectory tracking control law has been developed for the available regular form which is operated under matched uncertainties. Based on the hierarchical control principle, the aforesaid model is first subdivided into two subsystems, i.e., a fully-actuated subsystem and an under-actuated subsystem. In other words, the under-actuated subsystem is further transformed into a regular form whereby the under-actuated characteristics are decoupled in terms of control inputs. In the proposed design, the nonlinear drift terms, which certainly varies in full flight, are estimated via functional link neural networks to improve the performance of the controller in full flight. Besides, a variable gain robust exact differentiator (VG-RED) is designed to provide us with estimated flight velocities. It has consequently reduced the noise in system’s velocities and has mapped this controller as a practical one. The finite-time sliding mode enforcement and the states’ convergence are shown, for all flight loops, i.e., forward flight and backward flight, via the Lyapunov approach. All these claims are verified via numerical simulations and experimental implementation of the quadcopter system in a Matlab environment. For a more impressive presentation, the developed simulation results are compared with standard literature.}
}
@article{BRUNIER2020111717,
title = {Assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from UAV-based Structure-from-Motion photogrammetry},
journal = {Remote Sensing of Environment},
volume = {241},
pages = {111717},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.111717},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720300869},
author = {Guillaume Brunier and Emma Michaud and Jules Fleury and Edward J. Anthony and Sylvain Morvan and Antoine Gardel},
keywords = {Mudflat biogeomorphology, SfM photogrammetry, Crab burrows, Biofilm, Bioturbation, Amazon-influenced coast},
abstract = {Characterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the water-logged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the time-consuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms.}
}
@article{ZEGHLACHE2018267,
title = {Actuator fault tolerant control using adaptive RBFNN fuzzy sliding mode controller for coaxial octorotor UAV},
journal = {ISA Transactions},
volume = {80},
pages = {267-278},
year = {2018},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019057818302234},
author = {Samir Zeghlache and Hemza Mekki and Abderrahmen Bouguerra and Ali Djerioui},
keywords = {Coaxial octorotor, Radial basis function neural network, Fuzzy logic, Sliding mode control, Actuator fault tolerant control, Lyapunov stability},
abstract = {In this paper, a robust controller for a Six Degrees of Freedom (6 DOF) coaxial octorotor helicopter control is proposed in presence of actuator faults. Radial Base Function Neural Network (RBFNN), Fuzzy Logic Control approach (FLC) and Sliding Mode Control (SMC) technique are used to design a controller, named Fault Tolerant Control (FTC), for each subsystem of the octorotor helicopter. The proposed FTC scheme allows avoiding difficult modeling, attenuating the chattering effect of the SMC, reducing the rules number of the fuzzy controller, and guaranteeing the stability and the robustness of the system. The simulation results show that the proposed FTC can greatly alleviate the chattering effect, good tracking in presence of actuator faults.}
}
@article{ZAKI201741,
title = {Learning a deeply supervised multi-modal RGB-D embedding for semantic scene and object category recognition},
journal = {Robotics and Autonomous Systems},
volume = {92},
pages = {41-52},
year = {2017},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0921889016304225},
author = {Hasan F.M. Zaki and Faisal Shafait and Ajmal Mian},
keywords = {RGB-D image, Visual place recognition, Object categorization, Multi-modal deep learning},
abstract = {Recognizing semantic category of objects and scenes captured using vision-based sensors is a challenging yet essential capability for mobile robots and UAVs to perform high-level tasks such as long-term autonomous navigation. However, extracting discriminative features from multi-modal inputs, such as RGB-D images, in a unified manner is non-trivial given the heterogeneous nature of the modalities. We propose a deep network which seeks to construct a joint and shared multi-modal representation through bilinearly combining the convolutional neural network (CNN) streams of the RGB and depth channels. This technique motivates bilateral transfer learning between the modalities by taking the outer product of each feature extractor output. Furthermore, we devise a technique for multi-scale feature abstraction using deeply supervised branches which are connected to all convolutional layers of the multi-stream CNN. We show that end-to-end learning of the network is feasible even with a limited amount of training data and the trained network generalizes across different datasets and applications. Experimental evaluations on benchmark RGB-D object and scene categorization datasets show that the proposed technique consistently outperforms state-of-the-art algorithms.}
}
@article{PANTAZI2017224,
title = {Evaluation of hierarchical self-organising maps for weed mapping using UAS multispectral imagery},
journal = {Computers and Electronics in Agriculture},
volume = {139},
pages = {224-230},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916308973},
author = {X.E. Pantazi and A.A. Tamouridou and T.K. Alexandridis and A.L. Lagopodi and J. Kashefi and D. Moshou},
keywords = {Precision farming, Site-specific weed management, Unmanned aircraft system, Neural networks, eBee},
abstract = {Remote sensing has been used for species discrimination and for operational weed mapping. In the study presented here, the detection and mapping of Silybum marianum using a hierarchical self-organising map is reported. A multispectral camera (green-red-NIR) mounted on a fixed wing Unmanned Aircraft System (UAS) was used for the acquisition of high-resolution images of a pixel size of 0.1m, resampled to 0.5m. The Supervised Kohonen Network (SKN), Counter-propagation Artificial Neural Network (CP-ANN) and XY-Fusion network (XY-F) were used to identify the S. marianum among other vegetation in a field, with Avena sterilis L. being predominant. As input features to the classifiers, the three spectral bands of Red, Green, Near Infrared (NIR) and the texture layer were used. The S. marianum identification rates using SKN achieved an accuracy level of 98.64%, the CP-ANN achieved 98.87%, while XY-F was 98.64%. The results prove the feasibility of operational S. marianum mapping using hierarchical self-organising maps on multispectral UAS imagery.}
}
@article{GUO2022106670,
title = {Soil moisture content estimation in winter wheat planting area for multi-source sensing data using CNNR},
journal = {Computers and Electronics in Agriculture},
volume = {193},
pages = {106670},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106670},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921006876},
author = {Jiao Guo and Qingyuan Bai and Wenchuan Guo and Zhendong Bu and Weitao Zhang},
keywords = {Soil moisture content, Ultra wideband radar, Multispectral, Convolutional neural network regression},
abstract = {Rapid and accurate estimation of soil moisture content (SMC) is an important part of precision agriculture, and it is also one of the key problems to be solved in field real-time monitoring and precision irrigation. Most of the existing studies are limited to SMC monitoring of bare soil, which can be obtained by optical remote sensing (e.g., multispectral, hyperspectral) or Synthetic Aperture Radar (SAR). However, for the soil covered by vegetation, such as farmland, there are some theoretical defects with only one of the measuring methods. Meanwhile, in order to break through the limitations of low spatial and temporal resolutions of satellite remote sensing, it is of great significance to study SMC retrieval based on multi-source remote sensing data for the near earth UAV remote sensing systems. Based on this, this paper, taking the winter wheat planting area in Guanzhong plain of China as the research area, combines the advantages of ultra-wideband (UWB) radar, and multispectral remote sensing data, to reduce the influences of vegetation coverage on the estimation accuracy. A one-dimensional regression convolution neural network model is constructed to realize the quantitative prediction and estimation of SMC in farmland. The carried out experiments show that the proposed CNNR model has a better performance than traditional SVR and GRNN models and the R2, RMSE and RPD are 0.7453, 0.0140 cm3/cm3 and 1.9246, respectively. After introducing NDVI, MSAVI and DVI vegetation indices generated from multispectral images, the accuracy of the three models increased significantly. Among the three models, the constructed CNNR model has the best performance, and its R2, RMSE and RPD reach 0.9168, 0.0089 cm3/cm3, and 3.0201. Furthermore, after adding different levels of Gaussian noise to the original radar echoes, the CNNR model constructed in this paper still has the highest prediction accuracy and the strongest noise robustness.}
}
@article{STOGIANNOS2020106135,
title = {An enhanced decentralized artificial immune-based strategy formulation algorithm for swarms of autonomous vehicles},
journal = {Applied Soft Computing},
volume = {89},
pages = {106135},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106135},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620300752},
author = {Marios Stogiannos and Alex Alexandridis and Haralambos Sarimveis},
keywords = {Artificial immune system, Autonomous vehicle swarm, Decentralized path planning, Optimal task allocation, Swarm intelligence},
abstract = {This work presents an algorithmic approach to the problem of strategy assignment to the members of a swarm of autonomous vehicles. The proposed methodology draws inspiration from the artificial immune system (AIS), where a large number of antibodies cooperate in order to protect an organism from foreign threats by local exchange of information. The decentralized nature of the methodology does not suffer from problems like the need of a central control unit, the high maintenance costs and the risks associated with having a single point of system failure, which are common to centralized control techniques. Decentralized and distributed optimization schemes employ simple algorithms, which are fast, robust and can run locally on an autonomous unit due to their low processing power requirements. In contrast to standard AIS-based decentralized schemes, the proposed methodology makes use of a dynamic formulation of the available strategies and avoids the possibility of choosing an invalid strategy, which may lead to inferior swarm performance. The methodology is further enhanced by a dual strategy activation decay technique and a blind threat-follow rule. Statistical testing on different case studies based on “enemy search and engage” type scenarios in a simulated environment demonstrates the superior performance of the proposed algorithm against the standard AIS, an enhanced AIS version and a centralized particle swarm optimization (PSO) based methodology.}
}
@article{FENG2020101694,
title = {Efficient drone hijacking detection using two-step GA-XGBoost},
journal = {Journal of Systems Architecture},
volume = {103},
pages = {101694},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.101694},
url = {https://www.sciencedirect.com/science/article/pii/S1383762119305016},
author = {Zhiwei Feng and Nan Guan and Mingsong Lv and Wenchen Liu and Qingxu Deng and Xue Liu and Wang Yi},
keywords = {Cyber-physical system, UAV, Security, GPS spoofing, Machine learning},
abstract = {With the fast growth of civilian drones, their security problems meet significant challenges. A commercial drone may be hijacked by Global Positioning System (GPS)-spoofing attacks for illegal activities, such as terrorist attacks. Ideally, comparing positions respectively estimated by GPS and Inertial Navigation System (INS) can detect such attacks, while the results may always get fault because of the accumulated errors over time in INS. Therefore, in this paper, we propose a two-step GA-XGBoost method to detect GPS-spoofing attacks that just uses GPS and Inertial Measurement Unit (IMU) data. However, tunning the proper values of XGBoost parameters directly on the drone to achieve high prediction results consumes lots of resources which would influence the real-time performance of the drone. The proposed method separates the training phase into offboard step and onboard step. In offboard step, model is first trained by flight logs, and the training parameter values are automatically tuned by Genetic Algorithm (GA). Once the offboard model is trained, it could be uploaded to drones. To adapt our method to drones with different types of sensors and improve the correctness of prediction results, in onboard step, the model is further trained when a drone starts a mission. After onboard training finishes, the proposed method switches to the prediction mode. Besides, our method does not require any extra onboard hardware. The experiments with a real quadrotor drone also show the detection correctness is 96.3% and 100% in hijacked and non-hijacked cases at each sampling time respectively. Moreover, our method can achieve 100% detection correctness just within 1 s just after the attacks start.}
}
@article{DENG2021323,
title = {M2H-Net: A Reconstruction Method For Hyperspectral Remotely Sensed Imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {173},
pages = {323-348},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000253},
author = {Lei Deng and Jie Sun and Yong Chen and Han Lu and Fuzhou Duan and Lin Zhu and Tianxing Fan},
keywords = {Hyperspectral, Reconstruction, Deep learning, M2H-Net, GF-5, Remote sensing},
abstract = {Hyperspectral remote sensing can get spatially and spectrally continuous data simultaneously. However, the imaging equipment is usually expensive and complex, along with the low spatial resolution. In recent years, reconstruction of hyperspectral image by deep learning from the widely used low-cost, high spatial resolution RGB camera, has attracted extensive attention in many fields. However, most research is limited to three bands in the range of 400–700 nm, which greatly restrains its application in remote sensing. In this study, a more suitable for remote sensing multispectral to hyperspectral network (M2H-Net) is proposed, which can take many bands as input and output hyperspectral images with any number of bands within a wider spectral range (380–2500 nm). Its characteristics include adding residual connection on U-Net to reduce vanishing gradients; adding convolution combinations with different kernel sizes (1 × 1 and 3 × 3) to balance the spectral and spatial relationships. It is applied on images from different platforms (UAVs and Satellites), different imaging modes (frame and pushbroom) and different spectral response functions (narrow and wide bandwidth), and the results show that: 1) it has a very high accuracy of hyperspectral image reconstruction. The mean relative absolute error (MRAE) and root mean squared error (RMSE) are between 0.039 and 0.074 and 0.010–0.016, respectively, which are 69.2% and 41.2% lower than those of U-Net; 2) it has high efficiency with fast convergence (about 40 epochs) and stable performance. Compared with many algorithms won in the new trends in image restoration and enhancement (NTIRE) competition, M2H-Net ranked 7th in accuracy, but took less time (0.44 s); 3) it has strong generalization ability. Using the pre-trained M2H-Nets to reconstruct Cubert S185 and GF-5 hyperspectral images in different locations, different times and complex scenes, high accuracy (MRAE = 0.072, RMSE = 0.011) can still be obtained. This method is more suitable for remote sensing to meet the needs of multiple bands, spectrum width and complex scenes, thus provides the possibility to generate the global coverage hyperspectral imagery by using the massive in-orbit or historical archived multispectral images, which will not only greatly save the R&D and investment on hyperspectral imaging equipment, but also conduct data collection with higher efficiency and lower complexity. Due to the ability to reconstruct hyperspectral images in specified bands on demand, M2H-Net is also of great value in hyperspectral image processing, such as data compression, storage and transmission, etc.}
}
@article{BRIECHLE2020345,
title = {Detection of radioactive waste sites in the Chornobyl exclusion zone using UAV-based lidar data and multispectral imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {345-362},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620301738},
author = {S. Briechle and N. Molitor and P. Krzystek and G. Vosselman},
keywords = {UAV, Lidar, Multispectral imagery, Radioactive waste sites, 3D vegetation mapping, Machine learning},
abstract = {The severe accident at the Chornobyl Nuclear Power Plant (ChNPP) in 1986 resulted in extraordinary contamination of the surrounding territory, which necessitated the creation of the Chornobyl Exclusion Zone (ChEZ). During the accident, liquidation materials contaminated by radioactive fallout (e.g., contaminated soil and trees) were buried in so-called Radioactive Waste Temporary Storage Places (RWTSPs). The exact locations of these burials were not always sufficiently documented. However, for safety management, including eventual remediation works, it is crucial to know their locations and rely on precise hazard maps. Over the past 34 years, most of these so-called trenches and clamps have been exposed to natural processes. In addition to settlement and erosion, they have been overgrown with dense vegetation. To date, more than 700 burials have been thoroughly investigated, but a large number of burial sites (approximately 300) are still unknown. In the past, numerous burials were identified based on settlement or elevation in the decimeter range, and vegetation anomalies that tend to appear in the immediate vicinity. Nevertheless, conventional detection methods are time-, effort- and radiation dose-intensive. Airborne gamma spectrometry and visual ground inspection of morphology and vegetation can provide useful complementary information, but it is insufficient for precisely localizing unknown burial sites in many cases. Therefore, sensor technologies, such as UAV-based lidar and multispectral imagery, have been identified as potential alternative solutions. This paper presents a novel method to detect radioactive waste sites based on a set of prominent features generated from high-resolution remote sensing data in combination with a random forest (RF) classifier. Initially, we generate a digital terrain model (DTM) and 3D vegetation map from the data and derive tree-based features, including tree density, tree height, and tree species. Feature subsets compiled from normalized DTM height, fast point feature histograms (FPFH), and lidar metrics are then incorporated. Next, an RF classifier is trained on reference areas defined by visual interpretation of the DTM grid. A backward feature selection strategy reduces the feature space significantly and avoids overfitting. Feature relevance assessment clearly demonstrates that the members of all feature subsets represent a final list of the most prominent features. For three representative study areas, the mean overall accuracy (OA) is 98.2% when using area-wide test data. Cohens’ kappa coefficient κ ranges from 0.609 to 0.758. Additionally, we demonstrate the transferability of a trained classifier to an adjacent study area (OA = 93.6%, κ = 0.452). As expected, when utilizing the classifier on geometrically incorrect and incomplete reference data, which were generated from old maps and orthophotos based on visual inspection, the OA decreases significantly to 65.1% (κ = 0.481). Finally, detection is verified through 38 borings that successfully confirm the existence of previously unknown buried nuclear materials in classified areas. These results demonstrate that the proposed methodology is applicable to detecting area-wide unknown radioactive biomass burials in the ChEZ.}
}
@article{SHIRZADEH2015290,
title = {An indirect adaptive neural control of a visual-based quadrotor robot for pursuing a moving target},
journal = {ISA Transactions},
volume = {59},
pages = {290-302},
year = {2015},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0019057815002451},
author = {Masoud Shirzadeh and Abdollah Amirkhani and Aliakbar Jalali and Mohammad R. Mosavi},
keywords = {Visual-based control, Quadrotor robot, RBF neural network, Indirect adaptive neural control},
abstract = {This paper aims to use a visual-based control mechanism to control a quadrotor type aerial robot which is in pursuit of a moving target. The nonlinear nature of a quadrotor, on the one hand, and the difficulty of obtaining an exact model for it, on the other hand, constitute two serious challenges in designing a controller for this UAV. A potential solution for such problems is the use of intelligent control methods such as those that rely on artificial neural networks and other similar approaches. In addition to the two mentioned problems, another problem that emerges due to the moving nature of a target is the uncertainty that exists in the target image. By employing an artificial neural network with a Radial Basis Function (RBF) an indirect adaptive neural controller has been designed for a quadrotor robot in search of a moving target. The results of the simulation for different paths show that the quadrotor has efficiently tracked the moving target.}
}
@article{REYNAUD201641,
title = {Design of a force-based controlled mobility on aerial vehicles for pest management},
journal = {Ad Hoc Networks},
volume = {53},
pages = {41-52},
year = {2016},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2016.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570870516302128},
author = {Laurent Reynaud and Isabelle Guérin-Lassous},
keywords = {Controlled mobility, UAV networking, Physics-based swarm intelligence},
abstract = {Vespa velutina, also known as the Asian hornet, is considered as an invasive species out of its native zone. In particular, since it preys on honey bees, its recent progression in Europe could soon pose a significant risk to the local apiculture activity. European beekeepers are therefore investigating adapted control strategies, including V. velutina nest destruction. Unfortunately, nest location pinpointing generally follows a manual process which can prove tedious, time-consuming and inaccurate. In this article, we propose the use of a network of micro aerial vehicles featuring autonomous and cooperative flight capabilities. We describe an adapted controlled mobility strategy and detail the design of our Virtual Force Protocol (VFP) which allows a swarm of vehicles to track and follow hornets to their nests, while maintaining connectivity through a wireless multi-hop communication route with a remote ground station used to store applicative data such as hornet trajectory and vehicle telemetry. In order to achieve the mission objectives with a minimum of vehicles, we identify through simulations appropriate value for the key parameters of VFP and discuss the obtained network performance.}
}
@article{ZHANG2021112724,
title = {Transfer-learning-based approach for leaf chlorophyll content estimation of winter wheat from hyperspectral data},
journal = {Remote Sensing of Environment},
volume = {267},
pages = {112724},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112724},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721004442},
author = {Yao Zhang and Jian Hui and Qiming Qin and Yuanheng Sun and Tianyuan Zhang and Hong Sun and Minzan Li},
keywords = {Hyperspectral, Chlorophyll, PROSAIL, Transfer learning, UAV experiment, Ground-based measurement},
abstract = {Leaf chlorophyll, as a key factor for carbon circulation in the ecosystem, is significant for the photosynthetic productivity estimation and crop growth monitoring in agricultural management. Hyperspectral remote sensing (RS) provides feasible solutions for obtaining crop leaf chlorophyll content (LCC) by the advantages of its repeated and high throughput observations. However, the data redundancy and the poor robustness of the inversion models are still major obstacles that prevent the widespread application of hyperspectral RS for crop LCC evaluation. For winter wheat LCC inversion from hyperspectral observations, this study described a novel hybrid method, which is based on the combination of amplitude- and shape- enhanced 2D correlation spectrum (2DCOS) and transfer learning. The innovative feature selection method, amplitude- and shape- enhanced 2DCOS, which originated from 2DCOS, additionally considered the relationships between external perturbations and hyperspectral amplitude and shape characteristics to enhance the dynamic spectrum response. To extract the representative LCC featured wavelengths, the amplitude- and shape- enhanced 2DCOS was conducted on the leaf optical PROperties SPECTra (PROSPECT) + Scattering from Arbitrarily Inclined Leaves (SAIL) (PROSAIL) simulated dataset, which covered most possible winter wheat canopy spectra. Nine wavelengths (i.e., 455, 545, 571, 615, 641, 662, 706, 728, and 756 nm) were then extracted as the sensitive wavelengths of LCC with the amplitude- and shape- enhanced 2DCOS. These wavelengths had specificity to LCC and showed good correlation with LCC from the aspect of photosynthesis mechanism, molecular structure, and optical properties. The transfer learning techniques based on the deep neural network was then introduced to transfer the knowledge learned from the PROSAIL simulated dataset to the inversion tasks of field measured LCC. Parts of the labeled samples in field observations were used to finetune the model pre-trained by the simulated dataset to improve the inversion accuracy of the winter wheat LCC in different field scenes, aiming to reduce the need for the field measured and labeled sample size. To further ascertain the universality, transferability and predictive ability of the proposed hybrid method, field samples collected from different locations at different phenological phases, including the jointing and heading stages in 2013, 2014, and 2018, were utilized as target tasks to validate the proposed hybrid method. Moreover, the LCC of winter wheat estimated with the proposed method was evaluated with the ground-based platform and the UAV-based platform to verify the model versatility for different monitoring platforms. Various validations demonstrated that the hybrid inversion method combining the amplitude- and shape- enhanced 2DCOS and the fine-tuned transfer learning model could effectively estimate winter wheat LCC with good accuracy and robustness, and can be extended to the detection and inversion of other key variables of crops.}
}
@article{VETRIVEL201845,
title = {Disaster damage detection through synergistic use of deep learning and 3D point cloud features derived from very high resolution oblique aerial images, and multiple-kernel-learning},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {140},
pages = {45-59},
year = {2018},
note = {Geospatial Computer Vision},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2017.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0924271616305913},
author = {Anand Vetrivel and Markus Gerke and Norman Kerle and Francesco Nex and George Vosselman},
keywords = {Oblique images, UAV, 3D point cloud features, CNN features, Multiple-kernel-learning, Transfer learning, Model transferability, Structural damage detections},
abstract = {Oblique aerial images offer views of both building roofs and façades, and thus have been recognized as a potential source to detect severe building damages caused by destructive disaster events such as earthquakes. Therefore, they represent an important source of information for first responders or other stakeholders involved in the post-disaster response process. Several automated methods based on supervised learning have already been demonstrated for damage detection using oblique airborne images. However, they often do not generalize well when data from new unseen sites need to be processed, hampering their practical use. Reasons for this limitation include image and scene characteristics, though the most prominent one relates to the image features being used for training the classifier. Recently features based on deep learning approaches, such as convolutional neural networks (CNNs), have been shown to be more effective than conventional hand-crafted features, and have become the state-of-the-art in many domains, including remote sensing. Moreover, often oblique images are captured with high block overlap, facilitating the generation of dense 3D point clouds – an ideal source to derive geometric characteristics. We hypothesized that the use of CNN features, either independently or in combination with 3D point cloud features, would yield improved performance in damage detection. To this end we used CNN and 3D features, both independently and in combination, using images from manned and unmanned aerial platforms over several geographic locations that vary significantly in terms of image and scene characteristics. A multiple-kernel-learning framework, an effective way for integrating features from different modalities, was used for combining the two sets of features for classification. The results are encouraging: while CNN features produced an average classification accuracy of about 91%, the integration of 3D point cloud features led to an additional improvement of about 3% (i.e. an average classification accuracy of 94%). The significance of 3D point cloud features becomes more evident in the model transferability scenario (i.e., training and testing samples from different sites that vary slightly in the aforementioned characteristics), where the integration of CNN and 3D point cloud features significantly improved the model transferability accuracy up to a maximum of 7% compared with the accuracy achieved by CNN features alone. Overall, an average accuracy of 85% was achieved for the model transferability scenario across all experiments. Our main conclusion is that such an approach qualifies for practical use.}
}
@article{DIXON2021112197,
title = {Satellite prediction of forest flowering phenology},
journal = {Remote Sensing of Environment},
volume = {255},
pages = {112197},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112197},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720305708},
author = {Dan J. Dixon and J. Nikolaus Callow and John M.A. Duncan and Samantha A. Setterfield and Natasha Pauli},
keywords = {Flower mapping, Phenology, Time series analysis, CubeSat, PlanetScope, Drone/UAV/UAS},
abstract = {Knowledge of flowering phenology is essential for understanding the condition of forest ecosystems and responses to various anthropogenic and environmental drivers. However, monitoring the spatial and temporal variability in forest flowering at landscape scales is challenging (e.g. current monitoring is often highly localized and in-situ or for single dates). This study presents a method that combines drone and satellite images (PlanetScope) that can produce landscape-scale maps of flowering dynamics. This method is demonstrated in forest landscapes dominated by the eucalypt Corymbia calophylla (red gum or marri) in Western Australia. Drone-derived images of flowering eucalypt canopies, available for restricted temporal and spatial extents, are used to label satellite image pixels with the proportion of a pixel footprint that is flowering. The pixels labelled with flowering proportion, the response variable, are combined with various metrics that characterize time series of spectral indices sensitive to the presence of green vegetation and cream-colored flowers, the predictor variables. A machine learning model then predicts daily pixel-level flowering proportions. The model is trained with data from two sites and is tested with data from three sites and various dates throughout the Corymbia calophylla season. The model is able to accurately predict pixel-level flowering proportion throughout the flowering season (RMSE <4% across all sites and dates), across sites with dense to sparse canopy, different background soil covers, and is robust to not detecting false positive flowering when no flowering events are occurring. Due to the spatiotemporal coverage of satellite images, this model can be deployed to generate regional maps of flowering dynamics in forest ecosystems that can be used for monitoring forest ecosystem condition and supporting research into drivers of eucalypt forest phenology.}
}
@article{LV2021102407,
title = {Modeling of winter wheat fAPAR by integrating Unmanned Aircraft Vehicle-based optical, structural and thermal measurement},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {102},
pages = {102407},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102407},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421001148},
author = {Zhengang Lv and Ran Meng and Jianguo Man and Linglin Zeng and Meiyv Wang and Binyuan Xu and Renjie Gao and Rui Sun and Feng Zhao},
keywords = {Winter wheat, fAPAR, Saturation problem, Multi-source remote sensing fusion, Machine learning, Smart agriculture, UAV},
abstract = {The fraction of absorbed photosynthetically active radiation (fAPAR) is a critical biophysical parameter for crop growth monitoring and yield estimation. Remote sensing provides an efficient way for measuring fAPAR over large areas, compared with the time-consuming and labor-intensive field measurements. However, the optical remote sensing signals usually saturate over dense vegetation (e.g., Leaf Area Index (LAI) > 5 or fAPAR > 0.7), limiting the performance of optical remote sensing in modeling fAPAR. Multi-source remote sensing data fusion has proven to be a feasible method to overcome the saturation problem of optical remote sensing in vegetation monitoring, but little is known about the performance of optical, structural and thermal features fusion for modeling winter wheat fAPAR. Also, the modeling powers of optical, structural, and thermal features for fAPAR estimation have seldom been compared. To fill in these knowledge gaps, the very high spatial resolution RGB-optical and thermal imagery collected by Unmanned Aircraft Vehicle (UAV) were used to quantify the powers of RGB-derived vegetation indices (VIs), Structural Indices (SIs, crop height/canopy cover), and Canopy Temperature (CT) and their combinations in modeling winter wheat fAPAR in this study. The modeling powers of different remote sensing features were compared with the commonly used hyperspectral vegetation indices (HVIs) from field spectrometer measurements. Results showed that (1) multi-source data fusion that integrates optical, structural, and thermal features provided the best model in winter wheat fAPAR mapping (R2 = 0.907 and RMSE = 0.041) ; (2) the RGB imagery-derived optical (i.e., RGB VIs) and structural features (i.e., RGB SIs) were important preditors for winter wheat fAPAR modeling, and their combination can steadily improve the modeling accuracy (~2% improvement in R2 compared to optical-only model); (3) the thermal feature alone performed the worst among all experiments, but it still can complement other types of remote sensing features (i.e., RGB VIs&SIs) and further improve the modeling accuracy within the framework of data fusion (~3% improvement in R2 compared to optical-only model). In general, this study indicates that the framework of multi-source remote sensing data fusion can provide more accurate, efficient measurements of winter wheat fAPAR for crop management in precision agriculture, which can help improve resource utilization efficiency (e.g., determine where and when to apply nitrogen fertilizer) and ensure food security in the face of climate change.}
}
@article{LIU2019175,
title = {MSSTResNet-TLD: A robust tracking method based on tracking-learning-detection framework by using multi-scale spatio-temporal residual network feature model},
journal = {Neurocomputing},
volume = {362},
pages = {175-194},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219309646},
author = {Bing Liu and Qiao Liu and Taiping Zhang and Yong Yang},
keywords = {Tracking-learning-detection, Spatio-temporal feature, Multi-scale feature, Residual network},
abstract = {The performance of tracking task is directly dependent on the appearance features of target object, a robust approach for constructing appearance features is crucial for adaptation the appearance change. To construct an accurate and robust appearance model for visual object tracking, we modify original deep residual learning network architecture and name it Multi-Scale Residual Network (MSResNet). The first video frame image and its related information of the current input video sequence are used to learn a multi-scale appearance model of target object and a loss function is minimized over the appearance features. Meanwhile, spatial information of each video frame and temporal information between successive video frames effectively combine with MSResNet. And thus the features are generated by Multi-Scale Spatio-Temporal Residual Network, which is named MSSTResNet feature model, can adapt to scale variation, illumination variation, background clutters, severe deformation of the target object, and so on. We implement a robust tracking method based on tracking-learning-detection framework by using our proposed MSSTResNet feature model and name it MSSTResNet-TLD tracker. Unlike the previous tracking methods, the MSResNet architecture is not offline pre-trained on a large auxiliary datasets but is directly learned end-to-end with a multi-task loss by using the current input video sequence. Furthermore, the multi-task loss function utilizes the classification loss and regression loss that is more accurate for target localization. Our experimental results demonstrate that the proposed tracking method outperforms the current state-of-the-art tracking methods on Visual Object Tracking Benchmark (VOT-2016), Object Tracking Benchmark (OTB-2015), and Unmanned Aerial Vehicles (UAV20L) test datasets. Furthermore, our MSSTResNet-TLD tracker is faster than previous most trackers based on deep Convolutional Neural Network (ConvNet or CNN) and our tracker is extremely robust to the tiny target object. Our source code is available for download at https://github.com/binger1225/MSSTResNet-TLD-Tracker.}
}
@article{WANG201944,
title = {Research on automatic target detection and recognition based on deep learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {44-50},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300240},
author = {Jia Wang and Chen Liu and Tian Fu and Lili Zheng},
keywords = {Image processing, Target detection, Target recognition, In-depth learning},
abstract = {With the development of computer technology, the related achievements of image processing have been applied. Among them, the results of automatic target detection and recognition are widely used in the fields of reconnaissance, early warning and traffic control with the application of UAV. But now, the research of automatic target detection and tracking is becoming smaller and smaller. The original automatic target detection and recognition algorithm seems to be inadequate. The bottleneck of low-level feature design and optimization makes the accuracy and efficiency of automatic target detection inefficient. Therefore, based on in-depth learning, this paper establishes a method to automatically learn effective image features from images to achieve automatic target detection. Through the simulation of target detection in VEDAI database. The results show that the recognition rate of the proposed model is more than 95%. The results show that the proposed method can realize the automatic detection and recognition of targets very well.}
}
@article{HAMBRECHT2019109,
title = {Detecting ‘poachers’ with drones: Factors influencing the probability of detection with TIR and RGB imaging in miombo woodlands, Tanzania},
journal = {Biological Conservation},
volume = {233},
pages = {109-117},
year = {2019},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0006320718315726},
author = {Leonard Hambrecht and Richard P. Brown and Alex K. Piel and Serge A. Wich},
keywords = {UAV, Drone, Thermal, TIR, RGB, Comparison, Contrast, Distance, Centerline, Poachers, People, Time of day, Poaching, Conservation, Canopy, Density},
abstract = {Conservation biologists increasingly employ drones to reduce poaching of animals. However, there are no published studies on the probability of detecting poachers and the factors influencing detection. In an experimental setting with voluntary subjects, we evaluated the influence of various factors on poacher detection probability: camera (visual spectrum: RGB and thermal infrared: TIR), density of canopy cover, subject distance from the image centreline, subject contrast against the background, altitude of the drone and image analyst. We manually analysed the footage and marked all recorded subject detections. A multilevel model was used to analyse the TIR image data and a general linear model approach was used for the RGB image data. We found that the TIR camera had a higher detection probability than the RGB camera. Detection probability in TIR images was significantly influenced by canopy density, subject distance from the centreline and the analyst. Detection probability in RGB images was significantly influenced by canopy density, subject contrast against the background, altitude and the analyst. Overall, our findings indicate that TIR cameras improve human detection, particularly at cooler times of the day, but this is significantly hampered by thick vegetation cover. The effects of diminished detection with increased distance from the image centreline can be improved by increasing the overlap between images although this requires more flights over a specific area. Analyst experience also contributed to increased detection probability, but this might cease being a problem following the development of automated detection using machine learning.}
}
@article{LIU2020499,
title = {UAV monitoring and forecasting model in intelligent traffic oriented applications},
journal = {Computer Communications},
volume = {153},
pages = {499-506},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419320365},
author = {Jingyu Liu and Jing Wu and Mingyu Liu},
keywords = {IOT network, Intelligent transportation, Vehicle monitoring, Monitoring and prediction model},
abstract = {Intelligent transportation system is a traffic management system developed with the progress of society and traffic. Its idea is to integrate the real-time operation of people, vehicles, roads and traffic involved in the traffic. The purpose of this paper is to build a safe, reliable and efficient vehicle monitoring and forecasting model for IOT. Based on the Beidou satellite positioning technology and Lora communication technology, aiming at the problem that the deep learning detection method cannot meet the real-time requirements in processing the monitoring video, this paper proposes a method of using multiple single target trackers instead of some yolov3 detection tasks, and puts forward the design idea and specific implementation scheme of the vehicle monitoring and prediction model. The vehicle monitoring and prediction model is used to detect four kinds of targets, namely, small cars, buses, trucks and pedestrians. The multi-target trajectory tracking is used to carry out the traffic statistics of multi vehicle types, the detection of two kinds of abnormal behaviors of traffic targets is low speed and parking, and the capture of pedestrians. The experimental results show that the vehicle monitoring and prediction model has the highest accuracy of location and type recognition for four types of traffic objects, namely, small cars, trucks, buses and pedestrians, reaching 80%.}
}
@article{BETHGE2018517,
title = {Multi-Mode Learning Supported Model Predictive Control with Guarantees},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {20},
pages = {517-522},
year = {2018},
note = {6th IFAC Conference on Nonlinear Model Predictive Control NMPC 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318326946},
author = {Johanna Bethge and Bruno Morabito and Janine Matschek and Rolf Findeisen},
keywords = {Nonlinear model predictive control, multi-mode systems, machine learning, robustness},
abstract = {Many systems exhibit multiple modes of operation, where the real mode is not known a priori. Examples are the grasping of objects by a robot, where a variety of object forms and stiffnesses are possible, or a quadcopter lifting and dropping several objects with different weights. In such cases it is challenging to design a controller that ensures robustness and safety for all possible modes without jeopardising performance. To this end, we present a learning supported predictive control approach for multi-mode uncertain environments with robustness guarantees. The approach allows to fuse a priori knowledge via including multiple models of the system with on-line and off-line learning to improve the system models and thus the overall performance. Guaranteed robustness is ensured by decoupling it from performance. While learned and improved models are used for performance optimization, robustness is guaranteed by ensuring that all possible modes respect the constraints and are repeatedly feasible. The ideas are confirmed considering an UAV package delivery example.}
}
@article{GONCALVES2019218,
title = {SegOptim—A new R package for optimizing object-based image analyses of high-spatial resolution remotely-sensed data},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {76},
pages = {218-230},
year = {2019},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2018.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0303243418303556},
author = {João Gonçalves and Isabel Pôças and Bruno Marcos and C.A. Mücher and João P. Honrado},
keywords = {Geographic object-based image analysis, GEOBIA, Image segmentation, Supervised classification, Genetic algorithms, Optimization, High-spatial resolution, Open-source software, R package},
abstract = {Geographic Object-based Image Analysis (GEOBIA) is increasingly used to process high-spatial resolution imagery, with applications ranging from single species detection to habitat and land cover mapping. Image segmentation plays a key role in GEOBIA workflows, allowing to partition images into homogenous and mutually exclusive regions. Nonetheless, segmentation techniques require a robust parameterization to achieve the best results. Frequently, inappropriate parameterization leads to sub-optimal results and difficulties in comparing distinct methods. Here, we present an approach based on Genetic Algorithms (GA) to optimize image segmentation parameters by using the performance scores from object-based classification, thus allowing to assess the adequacy of a segmented image in relation to the classification problem. This approach was implemented in a new R package called SegOptim, in which several segmentation algorithms are interfaced, mostly from open-source software (GRASS GIS, Orfeo Toolbox, RSGISLib, SAGA GIS, TerraLib), but also from proprietary software (ESRI ArcGIS). SegOptim also provides access to several machine-learning classification algorithms currently available in R, including Gradient Boosted Modelling, Support Vector Machines, and Random Forest. We tested our approach using very-high to high spatial resolution images collected from an Unmanned Aerial Vehicle (0.03 – 0.10 m), WorldView-2 (2 m), RapidEye (5 m) and Sentinel-2 (10 – 20 m) in six different test sites located in northern Portugal with varying environmental conditions and for different purposes, including invasive species detection and land cover mapping. The results highlight the added value of our novel comparison of image segmentation and classification algorithms. Overall classification performances (assessed through cross-validation with the Kappa index) ranged from 0.85 to 1.00. Pilot-tests show that our GA-based approach is capable of providing sound results for optimizing the parameters of different segmentation algorithms, with benefits for classification accuracy and for comparison across techniques. We also verified that no particular combination of an image segmentation and a classification algorithm is suited for all the tasks/objectives. Consequently, it is crucial to compare and optimize available methods to understand which one is more suited for a certain objective. Our approach allows a closer integration between the segmentation and classification stages, which is of high importance for GEOBIA workflows. The results from our tests confirm that this integration has benefits for comparing and optimizing both processes. We discuss some limitations of the SegOptim approach (and potential solutions) as well as a future roadmap to expand its current functionalities.}
}
@article{QAYYUM2021120762,
title = {Fusion of CNN and sparse representation for threat estimation near power lines and poles infrastructure using aerial stereo imagery},
journal = {Technological Forecasting and Social Change},
volume = {168},
pages = {120762},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120762},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521001943},
author = {Abdul Qayyum and Imran Razzak and Aamir Saeed Malik and Sajid Anwar},
keywords = {Critical infrastructure, CNN, Sparse representation, Threat estimation, Power lines, Aerial stereo imagery},
abstract = {Fires or electrical hazards and accidents can occur if vegetation is not controlled or cleared around overhead power lines, resulting in serious risks to people and property and significant costs to the community. There are numerous blackouts due to interfering the trees with the power transmission lines in hilly and urban areas. Power distribution companies are facing a challenge to monitor the vegetation to avoid blackouts and flash-over threats. Recently, several methods have been developed for vegetation monitoring; however, existing methods are either not accurate or could not provide better disparity map in the textureless region. Moreover, are not able to handle depth discontinuity in stereo thus are not able to find a feasible solution in the smooth areas to compute the disparity map. This study presents a cost-effective framework based on UAV and satellite Stereo images to monitor the trees and vegetation, which provide better disparity. We present a novel approach based on the fusion of the convolutional neural network (CNN) and sparse representation that handled textureless region, depth discontinuity and smooth region to produce better disparity map that further used for threat estimation using height and distance of vegetation/trees near power lines and poles. Extensive experimental evaluation on real time powerline monitoring showed considerable imporvemnt in vegetation threat estimation with accuracy of 90.3% in comparison to graph-cut, dynamic programming, belief propagation, and area-based methods.}
}
@article{ZHANG2022106617,
title = {Two-step ResUp&Down generative adversarial network to reconstruct multispectral image from aerial RGB image},
journal = {Computers and Electronics in Agriculture},
volume = {192},
pages = {106617},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106617},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921006347},
author = {Yanchao Zhang and Wen Yang and Wenbo Zhang and Jiya Yu and Jianxin Zhang and Yongjie Yang and Yongliang Lu and Wei Tang},
keywords = {Multispectral image reconstruction, Images registration, Generative adversarial network, UAV, ResNet},
abstract = {Convolutional neural network has brought breakthroughs on multispectral image reconstruction research. Previous work has largely focused on reconstructing MSI using the R-G-B channels from the MSI as inputs of the model. However, it’s image manipulation rather than practical use. In real application, to reconstruct multispectral image using images from RGB camera is a research that has hardly been studied. In this research, high resolution aerial RGB images are collected by drone with RGB camera and multispectral images are collected by drone with RedEdge-M multispectral Camera. Then a new two-step Generative Adversarial Network (GAN)-based reconstruction method was proposed as follows: At first, MSI and RGB images are carefully registered to make sure that pixels are one–one correspondent. Then two data sources are cropped to form dataset. After that, a novel R-MSI GAN using is proposed. It uses a ResUp&Down block to replace the ResNet block of the Generator network and it outperforms ResNet-based GAN. The experimental results show that: (1) the combination of Mean Square Error and Discriminator (MSE-D) can alleviate the problem of the high-frequency loss of generated images. (2) The root means square error (RMSE), mean relative absolute error (MRAE) and Structural Similarity (SSIM) can only reflect overall error but can’t reflect details in reconstructed images, while different bands' statistical histogram can present the total high-frequency loss of generated bands. (3) 3 indexes, which are intersection over union (IoU) based normalized difference vegetation index (NDVI)-IoU, normalized difference red edge (NDRE)-IoU and enhance vegetation index (EVI)-IoU, were defined to verify the effect of the generated MSI and they show good consistence with vegetation index map. 4 In comparisons among ResNet-based GAN, single step ResUp&Down GAN and two-step ResUp&Down GAN(T-GAN) with 3 loss functions (L1, MSE, Discriminator), the two-step ResUp&Down GAN(T-GAN) with MSE-D loss function performs best in reconstructing RGB bands. The T-GAN with L1loss-D (mean absolute error loss) performs best in reconstructing NIR and rededge bands. In summary, the proposed methods can effectively reconstruct MSI using images from RGB camera at drone based remote sensing.}
}