
@Article{rs14010050,
AUTHOR = {He, Haiqing and Yu, Jing and Cheng, Penggen and Wang, Yuqian and Zhu, Yufeng and Lin, Taiqing and Dai, Guoqiang},
TITLE = {Automatic, Multiview, Coplanar Extraction for CityGML Building Model Texture Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2072-4292/14/1/50},
ISSN = {2072-4292},
ABSTRACT = {Most 3D CityGML building models in street-view maps (e.g., Google, Baidu) lack texture information, which is generally used to reconstruct real-scene 3D models by photogrammetric techniques, such as unmanned aerial vehicle (UAV) mapping. However, due to its simplified building model and inaccurate location information, the commonly used photogrammetric method using a single data source cannot satisfy the requirement of texture mapping for the CityGML building model. Furthermore, a single data source usually suffers from several problems, such as object occlusion. We proposed a novel approach to achieve CityGML building model texture mapping by multiview coplanar extraction from UAV remotely sensed or terrestrial images to alleviate these problems. We utilized a deep convolutional neural network to filter out object occlusion (e.g., pedestrians, vehicles, and trees) and obtain building-texture distribution. Point-line-based features are extracted to characterize multiview coplanar textures in 2D space under the constraint of a homography matrix, and geometric topology is subsequently conducted to optimize the boundary of textures by using a strategy combining Hough-transform and iterative least-squares methods. Experimental results show that the proposed approach enables texture mapping for building fa&ccedil;ades to use 2D terrestrial images without the requirement of exterior orientation information; that is, different from the photogrammetric method, a collinear equation is not an essential part to capture texture information. In addition, the proposed approach can significantly eliminate blurred and distorted textures of building models, so it is suitable for automatic and rapid texture updates.},
DOI = {10.3390/rs14010050}
}



@Article{drones6010005,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Heravi, Amirhossein and Thaheem, Muhammad Jamaluddin and Maqsoom, Ahsen},
TITLE = {Inspecting Buildings Using Drones and Computer Vision: A Machine Learning Approach to Detect Cracks and Damages},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {5},
URL = {https://www.mdpi.com/2504-446X/6/1/5},
ISSN = {2504-446X},
ABSTRACT = {Manual inspection of infrastructure damages such as building cracks is difficult due to the objectivity and reliability of assessment and high demands of time and costs. This can be automated using unmanned aerial vehicles (UAVs) for aerial imagery of damages. Numerous computer vision-based approaches have been applied to address the limitations of crack detection but they have their limitations that can be overcome by using various hybrid approaches based on artificial intelligence (AI) and machine learning (ML) techniques. The convolutional neural networks (CNNs), an application of the deep learning (DL) method, display remarkable potential for automatically detecting image features such as damages and are less sensitive to image noise. A modified deep hierarchical CNN architecture has been used in this study for crack detection and damage assessment in civil infrastructures. The proposed architecture is based on 16 convolution layers and a cycle generative adversarial network (CycleGAN). For this study, the crack images were collected using UAVs and open-source images of mid to high rise buildings (five stories and above) constructed during 2000 in Sydney, Australia. Conventionally, a CNN network only utilizes the last layer of convolution. However, our proposed network is based on the utility of multiple layers. Another important component of the proposed CNN architecture is the application of guided filtering (GF) and conditional random fields (CRFs) to refine the predicted outputs to get reliable results. Benchmarking data (600 images) of Sydney-based buildings damages was used to test the proposed architecture. The proposed deep hierarchical CNN architecture produced superior performance when evaluated using five methods: GF method, Baseline (BN) method, Deep-Crack BN, Deep-Crack GF, and SegNet. Overall, the GF method outperformed all other methods as indicated by the global accuracy (0.990), class average accuracy (0.939), mean intersection of the union overall classes (IoU) (0.879), precision (0.838), recall (0.879), and F-score (0.8581) values. Overall, the proposed CNN architecture provides the advantages of reduced noise, highly integrated supervision of features, adequate learning, and aggregation of both multi-scale and multilevel features during the training procedure along with the refinement of the overall output predictions.},
DOI = {10.3390/drones6010005}
}



@Article{rs14010093,
AUTHOR = {Santos, Adão F. and Lacerda, Lorena N. and Rossi, Chiara and Moreno, Leticia de A. and Oliveira, Mailson F. and Pilon, Cristiane and Silva, Rouverson P. and Vellidis, George},
TITLE = {Using UAV and Multispectral Images to Estimate Peanut Maturity Variability on Irrigated and Rainfed Fields Applying Linear Models and Artificial Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {93},
URL = {https://www.mdpi.com/2072-4292/14/1/93},
ISSN = {2072-4292},
ABSTRACT = {Using UAV and multispectral images has contributed to identifying field variability and improving crop management through different data modeling methods. However, knowledge on application of these tools to manage peanut maturity variability is still lacking. Therefore, the objective of this study was to compare and validate linear and multiple linear regression with models using artificial neural networks (ANN) for estimating peanut maturity under irrigated and rainfed conditions. The models were trained (80% dataset) and tested (20% dataset) using results from the 2018 and 2019 growing seasons from irrigated and rainfed fields. In each field, plant reflectance was collected weekly from 90 days after planting using a UAV-mounted multispectral camera. Images were used to develop vegetation indices (VIs). Peanut pods were collected on the same dates as the UAV flights for maturity assessment using the peanut maturity index (PMI). The precision and accuracy of the linear models to estimate PMI using VIs were, in general, greater in irrigated fields with R2 &gt; 0.40 than in rainfed areas, which had a maximum R2 value of 0.21. Multiple linear regressions combining adjusted growing degree days (aGDD) and VIs resulted in decreased RMSE for both irrigated and rainfed conditions and increased R2 in irrigated areas. However, these models did not perform successfully in the test process. On the other hand, ANN models that included VIs and aGDD showed accuracy of R2 = 0.91 in irrigated areas, regardless of using Multilayer Perceptron (MLP; RMSE = 0.062) or Radial Basis Function (RBF; RMSE = 0.065), as well as low tendency (1:1 line). These results indicated that, regardless of the ANN architecture used to predict complex and non-linear variables, peanut maturity can be estimated accurately through models with multiple inputs using VIs and aGDD. Although the accuracy of the MLP or RBF models for irrigated and rainfed areas separately was high, the overall ANN models using both irrigated and rainfed areas can be used to predict peanut maturity with the same precision.},
DOI = {10.3390/rs14010093}
}



@Article{rs14010157,
AUTHOR = {Jiang, Zongchen and Zhang, Jie and Ma, Yi and Mao, Xingpeng},
TITLE = {Hyperspectral Remote Sensing Detection of Marine Oil Spills Using an Adaptive Long-Term Moment Estimation Optimizer},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {157},
URL = {https://www.mdpi.com/2072-4292/14/1/157},
ISSN = {2072-4292},
ABSTRACT = {Marine oil spills can damage marine ecosystems, economic development, and human health. It is important to accurately identify the type of oil spills and detect the thickness of oil films on the sea surface to obtain the amount of oil spill for on-site emergency responses and scientific decision-making. Optical remote sensing is an important method for marine oil-spill detection and identification. In this study, hyperspectral images of five types of oil spills were obtained using unmanned aerial vehicles (UAV). To address the poor spectral separability between different types of light oils and weak spectral differences in heavy oils with different thicknesses, we propose the adaptive long-term moment estimation (ALTME) optimizer, which cumulatively learns the spectral characteristics and then builds a marine oil-spill detection model based on a one-dimensional convolutional neural network. The results of the detection experiment show that the ALTME optimizer can store in memory multiple batches of long-term oil-spill spectral information, accurately identify the type of oil spills, and detect different thicknesses of oil films. The overall detection accuracy is larger than 98.09%, and the Kappa coefficient is larger than 0.970. The F1-score for the recognition of light-oil types is larger than 0.971, and the F1-score for detecting films of heavy oils with different film thicknesses is larger than 0.980. The proposed optimizer also performs well on a public hyperspectral dataset. We further carried out a feasibility study on oil-spill detection using UAV thermal infrared remote sensing technology, and the results show its potential for oil-spill detection in strong sunlight.},
DOI = {10.3390/rs14010157}
}



@Article{f13010048,
AUTHOR = {Kamarulzaman, Aisyah Marliza Muhmad and Wan Mohd Jaafar, Wan Shafrina and Abdul Maulud, Khairul Nizam and Saad, Siti Nor Maizah and Omar, Hamdan and Mohan, Midhun},
TITLE = {Integrated Segmentation Approach with Machine Learning Classifier in Detecting and Mapping Post Selective Logging Impacts Using UAV Imagery},
JOURNAL = {Forests},
VOLUME = {13},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {48},
URL = {https://www.mdpi.com/1999-4907/13/1/48},
ISSN = {1999-4907},
ABSTRACT = {Selective logging can cause significant impacts on the residual stands, affecting biodiversity and leading to environmental changes. Proper monitoring and mapping of the impacts from logging activities, such as the stumps, felled logs, roads, skid trails, and forest canopy gaps, are crucial for sustainable forest management operations. The purpose of this study is to assess the indicators of selective logging impacts by detecting the individual stumps as the main indicators, evaluating the performance of classification methods to assess the impacts and identifying forest gaps from selective logging activities. The combination of forest inventory field plots and unmanned aerial vehicle (UAV) RGB and overlapped imaged were used in this study to assess these impacts. The study area is located in Ulu Jelai Forest Reserve in the central part of Peninsular Malaysia, covering an experimental study area of 48 ha. The study involved the integration of template matching (TM), object-based image analysis (OBIA), and machine learning classification&mdash;support vector machine (SVM) and artificial neural network (ANN). Forest features and tree stumps were classified, and the canopy height model was used for detecting forest canopy gaps in the post selective logging region. Stump detection using the integration of TM and OBIA produced an accuracy of 75.8% when compared with the ground data. Forest classification using SVM and ANN methods were adopted to extract other impacts from logging activities such as skid trails, felled logs, roads and forest canopy gaps. These methods provided an overall accuracy of 85% and kappa coefficient value of 0.74 when compared with conventional classifier. The logging operation also caused an 18.6% loss of canopy cover. The result derived from this study highlights the potential use of UAVs for efficient post logging impact analysis and can be used to complement conventional forest inventory practices.},
DOI = {10.3390/f13010048}
}



@Article{jmse10010051,
AUTHOR = {Li, Jiqiang and Zhang, Guoqing and Li, Bo},
TITLE = {Robust Adaptive Neural Cooperative Control for the USV-UAV Based on the LVS-LVA Guidance Principle},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {10},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {51},
URL = {https://www.mdpi.com/2077-1312/10/1/51},
ISSN = {2077-1312},
ABSTRACT = {Around the cooperative path-following control for the underactuated surface vessel (USV) and the unmanned aerial vehicle (UAV), a logic virtual ship-logic virtual aircraft (LVS-LVA) guidance principle is developed to generate the reference heading signals for the USV-UAV system by using the &ldquo;virtual ship&rdquo; and the &ldquo;virtual aircraft&rdquo;, which is critical to establish an effective correlation between the USV and the UAV. Taking the steerable variables (the main engine speed and the rudder angle of the USV, and the rotor angular velocities of the UAV) as the control input, a robust adaptive neural cooperative control algorithm was designed by employing the dynamic surface control (DSC), radial basic function neural networks (RBF-NNs) and the event-triggered technique. In the proposed algorithm, the reference roll angle and pitch angle for the UAV can be calculated from the position control loop by virtue of the nonlinear decouple technique. In addition, the system uncertainties were approximated through the RBF-NNs and the transmission burden from the controller to the actuators was reduced for merits of the event-triggered technique. Thus, the derived control law is superior in terms of the concise form, low transmission burden and robustness. Furthermore, the tracking errors of the USV-UAV cooperative control system can converge to a small compact set through adjusting the designed control parameters appropriately, and it can be also guaranteed that all the signals are the semi-global uniformly ultimately bounded (SGUUB). Finally, the effectiveness of the proposed algorithm has been verified via numerical simulations in the presence of the time-varying disturbances.},
DOI = {10.3390/jmse10010051}
}



@Article{rs14010223,
AUTHOR = {Hernández, Daniel and Cecilia, José M. and Cano, Juan-Carlos and Calafate, Carlos T.},
TITLE = {Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {223},
URL = {https://www.mdpi.com/2072-4292/14/1/223},
ISSN = {2072-4292},
ABSTRACT = {With the proliferation of unmanned aerial vehicles (UAVs) in different contexts and application areas, efforts are being made to endow these devices with enough intelligence so as to allow them to perform complex tasks with full autonomy. In particular, covering scenarios such as disaster areas may become particularly difficult due to infrastructure shortage in some areas, often impeding a cloud-based analysis of the data in near-real time. Enabling AI techniques at the edge is therefore fundamental so that UAVs themselves can both capture and process information to gain an understanding of their context, and determine the appropriate course of action in an independent manner. Towards this goal, in this paper, we take determined steps towards UAV autonomy in a disaster scenario such as a flood. In particular, we use a dataset of UAV images relative to different floods taking place in Spain, and then use an AI-based approach that relies on three widely used deep neural networks (DNNs) for semantic segmentation of images, to automatically determine the regions more affected by rains (flooded areas). The targeted algorithms are optimized for GPU-based edge computing platforms, so that the classification can be carried out on the UAVs themselves, and only the algorithm output is uploaded to the cloud for real-time tracking of the flooded areas. This way, we are able to reduce dependency on infrastructure, and to reduce network resource consumption, making the overall process greener and more robust to connection disruptions. Experimental results using different types of hardware and different architectures show that it is feasible to perform advanced real-time processing of UAV images using sophisticated DNN-based solutions.},
DOI = {10.3390/rs14010223}
}



@Article{app12020610,
AUTHOR = {Isufaj, Ralvi and Omeri, Marsel and Piera, Miquel Angel},
TITLE = {Multi-UAV Conflict Resolution with Graph Convolutional Reinforcement Learning},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {610},
URL = {https://www.mdpi.com/2076-3417/12/2/610},
ISSN = {2076-3417},
ABSTRACT = {Safety is the primary concern when it comes to air traffic. In-flight safety between Unmanned Aircraft Vehicles (UAVs) is ensured through pairwise separation minima, utilizing conflict detection and resolution methods. Existing methods mainly deal with pairwise conflicts, however, due to an expected increase in traffic density, encounters with more than two UAVs are likely to happen. In this paper, we model multi-UAV conflict resolution as a multiagent reinforcement learning problem. We implement an algorithm based on graph neural networks where cooperative agents can communicate to jointly generate resolution maneuvers. The model is evaluated in scenarios with 3 and 4 present agents. Results show that agents are able to successfully solve the multi-UAV conflicts through a cooperative strategy.},
DOI = {10.3390/app12020610}
}



@Article{rs14020295,
AUTHOR = {Yu, Kunyong and Hao, Zhenbang and Post, Christopher J. and Mikhailova, Elena A. and Lin, Lili and Zhao, Gejin and Tian, Shangfeng and Liu, Jian},
TITLE = {Comparison of Classical Methods and Mask R-CNN for Automatic Tree Detection and Mapping Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {295},
URL = {https://www.mdpi.com/2072-4292/14/2/295},
ISSN = {2072-4292},
ABSTRACT = {Detecting and mapping individual trees accurately and automatically from remote sensing images is of great significance for precision forest management. Many algorithms, including classical methods and deep learning techniques, have been developed and applied for tree crown detection from remote sensing images. However, few studies have evaluated the accuracy of different individual tree detection (ITD) algorithms and their data and processing requirements. This study explored the accuracy of ITD using local maxima (LM) algorithm, marker-controlled watershed segmentation (MCWS), and Mask Region-based Convolutional Neural Networks (Mask R-CNN) in a young plantation forest with different test images. Manually delineated tree crowns from UAV imagery were used for accuracy assessment of the three methods, followed by an evaluation of the data processing and application requirements for three methods to detect individual trees. Overall, Mask R-CNN can best use the information in multi-band input images for detecting individual trees. The results showed that the Mask R-CNN model with the multi-band combination produced higher accuracy than the model with a single-band image, and the RGB band combination achieved the highest accuracy for ITD (F1 score = 94.68%). Moreover, the Mask R-CNN models with multi-band images are capable of providing higher accuracies for ITD than the LM and MCWS algorithms. The LM algorithm and MCWS algorithm also achieved promising accuracies for ITD when the canopy height model (CHM) was used as the test image (F1 score = 87.86% for LM algorithm, F1 score = 85.92% for MCWS algorithm). The LM and MCWS algorithms are easy to use and lower computer computational requirements, but they are unable to identify tree species and are limited by algorithm parameters, which need to be adjusted for each classification. It is highlighted that the application of deep learning with its end-to-end-learning approach is very efficient and capable of deriving the information from multi-layer images, but an additional training set is needed for model training, robust computer resources are required, and a large number of accurate training samples are necessary. This study provides valuable information for forestry practitioners to select an optimal approach for detecting individual trees.},
DOI = {10.3390/rs14020295}
}



@Article{s22020546,
AUTHOR = {Yu, Xinyang and Chang, Chunyan and Song, Jiaxuan and Zhuge, Yuping and Wang, Ailing},
TITLE = {Precise Monitoring of Soil Salinity in China&rsquo;s Yellow River Delta Using UAV-Borne Multispectral Imagery and a Soil Salinity Retrieval Index},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {546},
URL = {https://www.mdpi.com/1424-8220/22/2/546},
PubMedID = {35062514},
ISSN = {1424-8220},
ABSTRACT = {Monitoring salinity information of salinized soil efficiently and precisely using the unmanned aerial vehicle (UAV) is critical for the rational use and sustainable development of arable land resources. The sensitive parameter and a precise retrieval method of soil salinity, however, remain unknown. This study strived to explore the sensitive parameter and construct an optimal method for retrieving soil salinity. The UAV-borne multispectral image in China&rsquo;s Yellow River Delta was acquired to extract band reflectance, compute vegetation indexes and soil salinity indexes. Soil samples collected from 120 different study sites were used for laboratory salt content measurements. Grey correlation analysis and Pearson correlation coefficient methods were employed to screen sensitive band reflectance and indexes. A new soil salinity retrieval index (SSRI) was then proposed based on the screened sensitive reflectance. The Partial Least Squares Regression (PLSR), Multivariable Linear Regression (MLR), Back Propagation Neural Network (BPNN), Support Vector Machine (SVM), and Random Forest (RF) methods were employed to construct retrieval models based on the sensitive indexes. The results found that green, red, and near-infrared (NIR) bands were sensitive to soil salinity, which can be used to build SSRI. The SSRI-based RF method was the optimal method for accurately retrieving the soil salinity. Its modeling determination coefficient (R2) and Root Mean Square Error (RMSE) were 0.724 and 1.764, respectively; and the validation R2, RMSE, and Residual Predictive Deviation (RPD) were 0.745, 1.879, and 2.211.},
DOI = {10.3390/s22020546}
}



@Article{drones6010019,
AUTHOR = {Kundid Vasić, Mirela and Papić, Vladan},
TITLE = {Improving the Model for Person Detection in Aerial Image Sequences Using the Displacement Vector: A Search and Rescue Scenario},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {19},
URL = {https://www.mdpi.com/2504-446X/6/1/19},
ISSN = {2504-446X},
ABSTRACT = {Recent results in person detection using deep learning methods applied to aerial images gathered by Unmanned Aerial Vehicles (UAVs) have demonstrated the applicability of this approach in scenarios such as Search and Rescue (SAR) operations. In this paper, the continuation of our previous research is presented. The main goal is to further improve detection results, especially in terms of reducing the number of false positive detections and consequently increasing the precision value. We present a new approach that, as input to the multimodel neural network architecture, uses sequences of consecutive images instead of only one static image. Since successive images overlap, the same object of interest needs to be detected in more than one image. The correlation between successive images was calculated, and detected regions in one image were translated to other images based on the displacement vector. The assumption is that an object detected in more than one image has a higher probability of being a true positive detection because it is unlikely that the detection model will find the same false positive detections in multiple images. Based on this information, three different algorithms for rejecting detections and adding detections from one image to other images in the sequence are proposed. All of them achieved precision value about 80% which is increased by almost 20% compared to the current state-of-the-art methods.},
DOI = {10.3390/drones6010019}
}



@Article{s22020601,
AUTHOR = {Sharma, Prakriti and Leigh, Larry and Chang, Jiyul and Maimaitijiang, Maitiniyazi and Caffé, Melanie},
TITLE = {Above-Ground Biomass Estimation in Oats Using UAV Remote Sensing and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {601},
URL = {https://www.mdpi.com/1424-8220/22/2/601},
PubMedID = {35062559},
ISSN = {1424-8220},
ABSTRACT = {Current strategies for phenotyping above-ground biomass in field breeding nurseries demand significant investment in both time and labor. Unmanned aerial vehicles (UAV) can be used to derive vegetation indices (VIs) with high throughput and could provide an efficient way to predict forage yield with high accuracy. The main objective of the study is to investigate the potential of UAV-based multispectral data and machine learning approaches in the estimation of oat biomass. UAV equipped with a multispectral sensor was flown over three experimental oat fields in Volga, South Shore, and Beresford, South Dakota, USA, throughout the pre- and post-heading growth phases of oats in 2019. A variety of vegetation indices (VIs) derived from UAV-based multispectral imagery were employed to build oat biomass estimation models using four machine-learning algorithms: partial least squares (PLS), support vector machine (SVM), Artificial neural network (ANN), and random forest (RF). The results showed that several VIs derived from the UAV collected images were significantly positively correlated with dry biomass for Volga and Beresford (r = 0.2&ndash;0.65), however, in South Shore, VIs were either not significantly or weakly correlated with biomass. For Beresford, approximately 70% of the variance was explained by PLS, RF, and SVM validation models using data collected during the post-heading phase. Likewise for Volga, validation models had lower coefficient of determination (R2 = 0.20&ndash;0.25) and higher error (RMSE = 700&ndash;800 kg/ha) than training models (R2 = 0.50&ndash;0.60; RMSE = 500&ndash;690 kg/ha). In South Shore, validation models were only able to explain approx. 15&ndash;20% of the variation in biomass, which is possibly due to the insignificant correlation values between VIs and biomass. Overall, this study indicates that airborne remote sensing with machine learning has potential for above-ground biomass estimation in oat breeding nurseries. The main limitation was inconsistent accuracy in model prediction across locations. Multiple-year spectral data, along with the inclusion of textural features like crop surface model (CSM) derived height and volumetric indicators, should be considered in future studies while estimating biophysical parameters like biomass.},
DOI = {10.3390/s22020601}
}



@Article{rs14020382,
AUTHOR = {Jing, Yafei and Ren, Yuhuan and Liu, Yalan and Wang, Dacheng and Yu, Linjun},
TITLE = {Automatic Extraction of Damaged Houses by Earthquake Based on Improved YOLOv5: A Case Study in Yangbi},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {382},
URL = {https://www.mdpi.com/2072-4292/14/2/382},
ISSN = {2072-4292},
ABSTRACT = {Efficiently and automatically acquiring information on earthquake damage through remote sensing has posed great challenges because the classical methods of detecting houses damaged by destructive earthquakes are often both time consuming and low in accuracy. A series of deep-learning-based techniques have been developed and recent studies have demonstrated their high intelligence for automatic target extraction for natural and remote sensing images. For the detection of small artificial targets, current studies show that You Only Look Once (YOLO) has a good performance in aerial and Unmanned Aerial Vehicle (UAV) images. However, less work has been conducted on the extraction of damaged houses. In this study, we propose a YOLOv5s-ViT-BiFPN-based neural network for the detection of rural houses. Specifically, to enhance the feature information of damaged houses from the global information of the feature map, we introduce the Vision Transformer into the feature extraction network. Furthermore, regarding the scale differences for damaged houses in UAV images due to the changes in flying height, we apply the Bi-Directional Feature Pyramid Network (BiFPN) for multi-scale feature fusion to aggregate features with different resolutions and test the model. We took the 2021 Yangbi earthquake with a surface wave magnitude (Ms) of 6.4 in Yunan, China, as an example; the results show that the proposed model presents a better performance, with the average precision (AP) being increased by 9.31% and 1.23% compared to YOLOv3 and YOLOv5s, respectively, and a detection speed of 80 FPS, which is 2.96 times faster than YOLOv3. In addition, the transferability test for five other areas showed that the average accuracy was 91.23% and the total processing time was 4 min, while 100 min were needed for professional visual interpreters. The experimental results demonstrate that the YOLOv5s-ViT-BiFPN model can automatically detect damaged rural houses due to destructive earthquakes in UAV images with a good performance in terms of accuracy and timeliness, as well as being robust and transferable.},
DOI = {10.3390/rs14020382}
}



@Article{s22030721,
AUTHOR = {Cui, Xue-Zhi and Feng, Quan and Wang, Shu-Zhi and Zhang, Jian-Hua},
TITLE = {Monocular Depth Estimation with Self-Supervised Learning for Vineyard Unmanned Agricultural Vehicle},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {721},
URL = {https://www.mdpi.com/1424-8220/22/3/721},
PubMedID = {35161463},
ISSN = {1424-8220},
ABSTRACT = {To find an economical solution to infer the depth of the surrounding environment of unmanned agricultural vehicles (UAV), a lightweight depth estimation model called MonoDA based on a convolutional neural network is proposed. A series of sequential frames from monocular videos are used to train the model. The model is composed of two subnetworks&mdash;the depth estimation subnetwork and the pose estimation subnetwork. The former is a modified version of U-Net that reduces the number of bridges, while the latter takes EfficientNet-B0 as its backbone network to extract the features of sequential frames and predict the pose transformation relations between the frames. The self-supervised strategy is adopted during the training, which means the depth information labels of frames are not needed. Instead, the adjacent frames in the image sequence and the reprojection relation of the pose are used to train the model. Subnetworks&rsquo; outputs (depth map and pose relation) are used to reconstruct the input frame, then a self-supervised loss between the reconstructed input and the original input is calculated. Finally, the loss is employed to update the parameters of the two subnetworks through the backward pass. Several experiments are conducted to evaluate the model&rsquo;s performance, and the results show that MonoDA has competitive accuracy over the KITTI raw dataset as well as our vineyard dataset. Besides, our method also possessed the advantage of non-sensitivity to color. On the computing platform of our UAV&rsquo;s environment perceptual system NVIDIA JETSON TX2, the model could run at 18.92 FPS. To sum up, our approach provides an economical solution for depth estimation by using monocular cameras, which achieves a good trade-off between accuracy and speed and can be used as a novel auxiliary depth detection paradigm for UAVs.},
DOI = {10.3390/s22030721}
}



@Article{s22030891,
AUTHOR = {Yuan, Songhe and Ota, Kaoru and Dong, Mianxiong and Zhao, Jianghai},
TITLE = {A Path Planning Method with Perception Optimization Based on Sky Scanning for UAVs},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {891},
URL = {https://www.mdpi.com/1424-8220/22/3/891},
PubMedID = {35161639},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are frequently adopted in disaster management. The vision they provide is extremely valuable for rescuers. However, they face severe problems in their stability in actual disaster scenarios, as the images captured by the on-board sensors cannot consistently give enough information for deep learning models to make accurate decisions. In many cases, UAVs have to capture multiple images from different views to output final recognition results. In this paper, we desire to formulate the fly path task for UAVs, considering the actual perception needs. A convolutional neural networks (CNNs) model is proposed to detect and localize the objects, such as the buildings, as well as an optimization method to find the optimal flying path to accurately recognize as many objects as possible with a minimum time cost. The simulation results demonstrate that the proposed method is effective and efficient, and can address the actual scene understanding and path planning problems for UAVs in the real world well.},
DOI = {10.3390/s22030891}
}



@Article{rs14030663,
AUTHOR = {Ding, Jiujie and Zhang, Jiahuan and Zhan, Zongqian and Tang, Xiaofang and Wang, Xin},
TITLE = {A Precision Efficient Method for Collapsed Building Detection in Post-Earthquake UAV Images Based on the Improved NMS Algorithm and Faster R-CNN},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {663},
URL = {https://www.mdpi.com/2072-4292/14/3/663},
ISSN = {2072-4292},
ABSTRACT = {The results of collapsed building detection act as an important reference for damage assessment after an earthquake, which is crucial for governments in order to efficiently determine the affected area and execute emergency rescue. For this task, unmanned aerial vehicle (UAV) images are often used as the data sources due to the advantages of high flexibility regarding data acquisition time and flying requirements and high resolution. However, collapsed buildings are typically distributed in both connected and independent pieces and with arbitrary shapes, and these are generally more obvious in the UAV images with high resolution; therefore, the corresponding detection is restricted by using conventional convolutional neural networks (CNN) and the detection results are difficult to evaluate. In this work, based on faster region-based convolutional neural network (Faster R-CNN), deformable convolution was used to improve the adaptability to the arbitrarily shaped collapsed buildings. In addition, inspired by the idea of pixelwise semantic segmentation, in contrast to the intersection over union (IoU), a new method which estimates the intersected proportion of objects (IPO) is proposed to describe the degree of the intersection of bounding boxes, leading to two improvements: first, the traditional non-maximum suppression (NMS) algorithm is improved by integration with the IPO to effectively suppress the redundant bounding boxes; second, the IPO is utilized as a new indicator to determine positive and negative bounding boxes, and is introduced as a new strategy for precision and recall estimation, which can be considered a more reasonable measurement of the degree of similarity between the detected bounding boxes and ground truth bounding boxes. Experiments show that compared with other models, our work can obtain better precision and recall for detecting collapsed buildings for which an F1 score of 0.787 was achieved, and the evaluation results from the suggested IPO are qualitatively closer to the ground truth. In conclusion, the improved NMS with the IPO and Faster R-CNN in this paper is feasible and efficient for the detection of collapsed buildings in UAV images, and the suggested IPO strategy is more suitable for the corresponding detection result&rsquo;s evaluation.},
DOI = {10.3390/rs14030663}
}



@Article{electronics11030441,
AUTHOR = {Wang, Min and Chen, Peng and Cao, Zhenxin and Chen, Yun},
TITLE = {Reinforcement Learning-Based UAVs Resource Allocation for Integrated Sensing and Communication (ISAC) System},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {441},
URL = {https://www.mdpi.com/2079-9292/11/3/441},
ISSN = {2079-9292},
ABSTRACT = {Due to the limited ability of a single unmanned aerial vehicle (UAV), group unmanned aerial vehicles (UAVs) have attracted more attention in communication and radar fields. The use of an integrated sensing and communication (ISAC) system can make communication and radar modules share a radar module&rsquo;s resources, coupled with efficient resource allocation methods. It can effectively solve the problem of inadequate UAV resources and the low utilization rate of resources. In this paper, the resource allocation problem is addressed for group UAVs to achieve a trade-off between the detection and communication performance, where the ISAC system is equipped in group UAVs. The resource allocation problem is described by an optimization problem, but with group UAVs, the problem is complex and cannot be solved efficiently. Compared with the traditional resource allocation scheme, which needs a lot of calculation or sample set problems, a novel reinforcement-learning-based method is proposed. We formulate a new reward function by combining mutual information (MI) and the communication rate (CR). The MI describes the radar detection performance, and the CR is for wireless communication. Simulation results show that compared with the traditional Kuhn Munkres (KM) or the deep neural network (DNN) methods, this method has better performance with the increase in problem complexity. Additionally, the execution time of this scheme is close to that of the DNN scheme, and it is better than the KM algorithm.},
DOI = {10.3390/electronics11030441}
}



@Article{buildings12020156,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Shahzad, Danish and Heravi, Amirhossein and Qayyum, Siddra and Akram, Junaid},
TITLE = {Civil Infrastructure Damage and Corrosion Detection: An Application of Machine Learning},
JOURNAL = {Buildings},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {156},
URL = {https://www.mdpi.com/2075-5309/12/2/156},
ISSN = {2075-5309},
ABSTRACT = {Automatic detection of corrosion and associated damages to civil infrastructures such as bridges, buildings, and roads, from aerial images captured by an Unmanned Aerial Vehicle (UAV), helps one to overcome the challenges and shortcomings (objectivity and reliability) associated with the manual inspection methods. Deep learning methods have been widely reported in the literature for civil infrastructure corrosion detection. Among them, convolutional neural networks (CNNs) display promising applicability for the automatic detection of image features less affected by image noises. Therefore, in the current study, we propose a modified version of deep hierarchical CNN architecture, based on 16 convolution layers and cycle generative adversarial network (CycleGAN), to predict pixel-wise segmentation in an end-to-end manner using the images of Bolte Bridge and sky rail areas in Victoria (Melbourne). The convolutedly designed model network proposed in the study is based on learning and aggregation of multi-scale and multilevel features while moving from the low convolutional layers to the high-level layers, thus reducing the consistency loss in images due to the inclusion of CycleGAN. The standard approaches only use the last convolutional layer, but our proposed architecture differs from these approaches and uses multiple layers. Moreover, we have used guided filtering and Conditional Random Fields (CRFs) methods to refine the prediction results. Additionally, the effectiveness of the proposed architecture was assessed using benchmarking data of 600 images of civil infrastructure. Overall, the results show that the deep hierarchical CNN architecture based on 16 convolution layers produced advanced performances when evaluated for different methods, including the baseline, PSPNet, DeepLab, and SegNet. Overall, the extended method displayed the Global Accuracy (GA); Class Average Accuracy (CAC); mean Intersection Of the Union (IOU); Precision (P); Recall (R); and F-score values of 0.989, 0.931, 0.878, 0.849, 0.818 and 0.833, respectively.},
DOI = {10.3390/buildings12020156}
}



@Article{rs14030733,
AUTHOR = {Varela, Sebastian and Pederson, Taylor L. and Leakey, Andrew D. B.},
TITLE = {Implementing Spatio-Temporal 3D-Convolution Neural Networks and UAV Time Series Imagery to Better Predict Lodging Damage in Sorghum},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/14/3/733},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV)-based remote sensing is gaining momentum in a variety of agricultural and environmental applications. Very-high-resolution remote sensing image sets collected repeatedly throughout a crop growing season are becoming increasingly common. Analytical methods able to learn from both spatial and time dimensions of the data may allow for an improved estimation of crop traits, as well as the effects of genetics and the environment on these traits. Multispectral and geometric time series imagery was collected by UAV on 11 dates, along with ground-truth data, in a field trial of 866 genetically diverse biomass sorghum accessions. We compared the performance of Convolution Neural Network (CNN) architectures that used image data from single dates (two spatial dimensions, 2D) versus multiple dates (two spatial dimensions + temporal dimension, 3D) to estimate lodging detection and severity. Lodging was detected with 3D-CNN analysis of time series imagery with 0.88 accuracy, 0.92 Precision, and 0.83 Recall. This outperformed the best 2D-CNN on a single date with 0.85 accuracy, 0.84 Precision, and 0.76 Recall. The variation in lodging severity was estimated by the best 3D-CNN analysis with 9.4% mean absolute error (MAE), 11.9% root mean square error (RMSE), and goodness-of-fit (R2) of 0.76. This was a significant improvement over the best 2D-CNN analysis with 11.84% MAE, 14.91% RMSE, and 0.63 R2. The success of the improved 3D-CNN analysis approach depended on the inclusion of &ldquo;before and after&rdquo; data, i.e., images collected on dates before and after the lodging event. The integration of geometric and spectral features with 3D-CNN architecture was also key to the improved assessment of lodging severity, which is an important and difficult-to-assess phenomenon in bioenergy feedstocks such as biomass sorghum. This demonstrates that spatio-temporal CNN architectures based on UAV time series imagery have significant potential to enhance plant phenotyping capabilities in crop breeding and Precision agriculture applications.},
DOI = {10.3390/rs14030733}
}



@Article{rs14040874,
AUTHOR = {Zhang, Chong and Zhou, Jiawei and Wang, Huiwen and Tan, Tianyi and Cui, Mengchen and Huang, Zilu and Wang, Pei and Zhang, Li},
TITLE = {Multi-Species Individual Tree Segmentation and Identification Based on Improved Mask R-CNN and UAV Imagery in Mixed Forests},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {874},
URL = {https://www.mdpi.com/2072-4292/14/4/874},
ISSN = {2072-4292},
ABSTRACT = {High-resolution UAV imagery paired with a convolutional neural network approach offers significant advantages in accurately measuring forestry ecosystems. Despite numerous studies existing for individual tree crown delineation, species classification, and quantity detection, the comprehensive situation in performing the above tasks simultaneously has rarely been explored, especially in mixed forests. In this study, we propose a new method for individual tree segmentation and identification based on the improved Mask R-CNN. For the optimized network, the fusion type in the feature pyramid network is modified from down-top to top-down to shorten the feature acquisition path among the different levels. Meanwhile, a boundary-weighted loss module is introduced to the cross-entropy loss function Lmask to refine the target loss. All geometric parameters (contour, the center of gravity and area) associated with canopies ultimately are extracted from the mask by a boundary segmentation algorithm. The results showed that F1-score and mAP for coniferous species were higher than 90%, and that of broadleaf species were located between 75&ndash;85.44%. The producer&rsquo;s accuracy of coniferous forests was distributed between 0.8&ndash;0.95 and that of broadleaf ranged in 0.87&ndash;0.93; user&rsquo;s accuracy of coniferous was distributed between 0.81&ndash;0.84 and that of broadleaf ranged in 0.71&ndash;0.76. The total number of trees predicted was 50,041 for the entire study area, with an overall error of 5.11%. The method under study is compared with other networks including U-net and YOLOv3. Results in this study show that the improved Mask R-CNN has more advantages in broadleaf canopy segmentation and number detection.},
DOI = {10.3390/rs14040874}
}



@Article{drones6020047,
AUTHOR = {Pádua, Luís and Antão-Geraldes, Ana M. and Sousa, Joaquim J. and Rodrigues, Manuel Ângelo and Oliveira, Verónica and Santos, Daniela and Miguens, Maria Filomena P. and Castro, João Paulo},
TITLE = {Water Hyacinth (Eichhornia crassipes) Detection Using Coarse and High Resolution Multispectral Data},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {47},
URL = {https://www.mdpi.com/2504-446X/6/2/47},
ISSN = {2504-446X},
ABSTRACT = {Efficient detection and monitoring procedures of invasive plant species are required. It is of crucial importance to deal with such plants in aquatic ecosystems, since they can affect biodiversity and, ultimately, ecosystem function and services. In this study, it is intended to detect water hyacinth (Eichhornia crassipes) using multispectral data with different spatial resolutions. For this purpose, high-resolution data (&lt;0.1 m) acquired from an unmanned aerial vehicle (UAV) and coarse-resolution data (10 m) from Sentinel-2 MSI were used. Three areas with a high incidence of water hyacinth located in the Lower Mondego region (Portugal) were surveyed. Different classifiers were used to perform a pixel-based detection of this invasive species in both datasets. From the different classifiers used, the results were achieved by the random forest classifiers stand-out (overall accuracy (OA): 0.94). On the other hand, support vector machine performed worst (OA: 0.87), followed by Gaussian naive Bayes (OA: 0.88), k-nearest neighbours (OA: 0.90), and artificial neural networks (OA: 0.91). The higher spatial resolution from UAV-based data enabled us to detect small amounts of water hyacinth, which could not be detected in Sentinel-2 data. However, and despite the coarser resolution, satellite data analysis enabled us to identify water hyacinth coverage, compared well with a UAV-based survey. Combining both datasets and even considering the different resolutions, it was possible to observe the temporal and spatial evolution of water hyacinth. This approach proved to be an effective way to assess the effects of the mitigation/control measures taken in the study areas. Thus, this approach can be applied to detect invasive species in aquatic environments and to monitor their changes over time.},
DOI = {10.3390/drones6020047}
}



@Article{en15051763,
AUTHOR = {Rao, Jinjun and Li, Bo and Zhang, Zhen and Chen, Dongdong and Giernacki, Wojciech},
TITLE = {Position Control of Quadrotor UAV Based on Cascade Fuzzy Neural Network},
JOURNAL = {Energies},
VOLUME = {15},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1763},
URL = {https://www.mdpi.com/1996-1073/15/5/1763},
ISSN = {1996-1073},
ABSTRACT = {In this article, a cascade fuzzy neural network (FNN) control approach is proposed for position control of quadrotor unmanned aerial vehicle (UAV) system with high coupling and underactuated. For the attitude loop with limited range, the FNN controller parameters were trained offline using flight data, whereas for the position loop, the method based on FNN compensation proportional-integral-derivative (PID) was adopted to tune the system online adaptively. This method not only combined the advantages of fuzzy systems and neural networks but also reduced the amount of calculation for cascade neural network control. Simulations of fixed set point flight and spiral and square trajectory tracking flight were then conducted. The comparison of the results showed that our method had advantages in terms of minimizing overshoot and settling time. Finally, flight experiments were carried out on a DJI Tello quadrotor UAV. The experimental results showed that the proposed controller had good performance in position control.},
DOI = {10.3390/en15051763}
}



@Article{sym14030498,
AUTHOR = {Zheng, Bochao and Wu, Yuewen and Li, Hui and Chen, Zhipeng},
TITLE = {Adaptive Sliding Mode Attitude Control of Quadrotor UAVs Based on the Delta Operator Framework},
JOURNAL = {Symmetry},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {498},
URL = {https://www.mdpi.com/2073-8994/14/3/498},
ISSN = {2073-8994},
ABSTRACT = {In this paper, a novel adaptive sliding-mode control algorithm is proposed for the attitude control of quadrotor unmanned aerial vehicles (UAVs) under the delta operator framework. First, the delta operator technique is used to discretize the attitude control systems of a quadrotor UAV. Then, based on the linear matrix inequality technique, a linear sliding surface is designed to ensure the asymptotical stability of the quadrotor UAV attitude control system during the sliding motion process. Second, by the estimated external disturbance using a radical basis function (RBF) neural network, an adaptive sliding-mode attitude controller is designed such that the states of the quadrotor UAV attitude systems can be driven towards the desired sliding surface, and thus the attitude control objective of the qudarotor UAV is achieved. Compared with the traditional adaptive sliding-mode control algorithm, the proposed adaptive sliding-mode control algorithm can effectively realize the attitude control of a quadrotor UAV subject to strong disturbances and couplings. Finally, comparisons of the simulation results verify the effectiveness and superiority of the control algorithm proposed in this paper.},
DOI = {10.3390/sym14030498}
}



@Article{rs14051231,
AUTHOR = {Zhang, Shimin and Li, Xiuhua and Ba, Yuxuan and Lyu, Xuegang and Zhang, Muqing and Li, Minzan},
TITLE = {Banana Fusarium Wilt Disease Detection by Supervised and Unsupervised Methods from UAV-Based Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1231},
URL = {https://www.mdpi.com/2072-4292/14/5/1231},
ISSN = {2072-4292},
ABSTRACT = {Banana Fusarium wilt (BFW) is a devastating disease with no effective cure methods. Timely and effective detection of the disease and evaluation of its spreading trend will help farmers in making right decisions on plantation management. The main purpose of this study was to find the spectral features of the BFW-infected canopy and build the optimal BFW classification models for different stages of infection. A RedEdge-MX camera mounted on an unmanned aerial vehicle (UAV) was used to collect multispectral images of a banana plantation infected with BFW in July and August 2020. Three types of spectral features were used as the inputs of classification models, including three-visible-band images, five-multispectral-band images, and vegetation indices (VIs). Four supervised methods including Support Vector Machine (SVM), Random Forest (RF), Back Propagation Neural Networks (BPNN) and Logistic Regression (LR), and two unsupervised methods including Hotspot Analysis (HA) and Iterative Self-Organizing Data Analysis Technique Algorithm (ISODATA) were adopted to detect the BFW-infected canopies. Comparing to the healthy canopies, the BFW-infected canopies had higher reflectance in the visible region, but lower reflectance in the NIR region. The classification results showed that most of the supervised and unsupervised methods reached excellent accuracies. Among all the supervised methods, RF based on the five-multispectral-band was considered as the optimal model, with higher overall accuracy (OA) of 97.28% and faster running time of 22 min. For the unsupervised methods, HA reached high and balanced OAs of more than 95% based on the selected VIs derived from the red and NIR band, especially for WDRVI, NDVI, and TDVI. By comprehensively evaluating the classification results of different metrics, the unsupervised method HA was recommended for BFW recognition, especially in the late stage of infection; the supervised method RF was recommended in the early stage of infection to reach a slightly higher accuracy. The results found in this study could give advice for banana plantation management and provide approaches for plant disease detection.},
DOI = {10.3390/rs14051231}
}



@Article{rs14051262,
AUTHOR = {Trenčanová, Bianka and Proença, Vânia and Bernardino, Alexandre},
TITLE = {Development of Semantic Maps of Vegetation Cover from UAV Images to Support Planning and Management in Fine-Grained Fire-Prone Landscapes},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1262},
URL = {https://www.mdpi.com/2072-4292/14/5/1262},
ISSN = {2072-4292},
ABSTRACT = {In Mediterranean landscapes, the encroachment of pyrophytic shrubs is a driver of more frequent and larger wildfires. The high-resolution mapping of vegetation cover is essential for sustainable land planning and the management for wildfire prevention. Here, we propose methods to simplify and automate the segmentation of shrub cover in high-resolution RGB images acquired by UAVs. The main contribution is a systematic exploration of the best practices to train a convolutional neural network (CNN) with a segmentation network architecture (U-Net) to detect shrubs in heterogeneous landscapes. Several semantic segmentation models were trained and tested in partitions of the provided data with alternative methods of data augmentation, patch cropping, rescaling and hyperparameter tuning (the number of filters, dropout rate and batch size). The most effective practices were data augmentation, patch cropping and rescaling. The developed classification model achieved an average F1 score of 0.72 on three separate test datasets even though it was trained on a relatively small training dataset. This study demonstrates the ability of state-of-the-art CNNs to map fine-grained land cover patterns from RGB remote sensing data. Because model performance is affected by the quality of data and labeling, an optimal selection of pre-processing practices is a requisite to improve the results.},
DOI = {10.3390/rs14051262}
}



@Article{s22052068,
AUTHOR = {Gromada, Krzysztof and Siemiątkowska, Barbara and Stecz, Wojciech and Płochocki, Krystian and Woźniak, Karol},
TITLE = {Real-Time Object Detection and Classification by UAV Equipped With SAR},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {2068},
URL = {https://www.mdpi.com/1424-8220/22/5/2068},
PubMedID = {35271213},
ISSN = {1424-8220},
ABSTRACT = {The article presents real-time object detection and classification methods by unmanned aerial vehicles (UAVs) equipped with a synthetic aperture radar (SAR). Two algorithms have been extensively tested: classic image analysis and convolutional neural networks (YOLOv5). The research resulted in a new method that combines YOLOv5 with post-processing using classic image analysis. It is shown that the new system improves both the classification accuracy and the location of the identified object. The algorithms were implemented and tested on a mobile platform installed on a military-class UAV as the primary unit for online image analysis. The usage of objective low-computational complexity detection algorithms on SAR scans can reduce the size of the scans sent to the ground control station.},
DOI = {10.3390/s22052068}
}



@Article{rs14061434,
AUTHOR = {Liu, Sunxiangyu and Li, Guitao and Zhan, Yafeng and Gao, Peng},
TITLE = {MUSAK: A Multi-Scale Space Kinematic Method for Drone Detection},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1434},
URL = {https://www.mdpi.com/2072-4292/14/6/1434},
ISSN = {2072-4292},
ABSTRACT = {Accurate and robust drone detection is an important and challenging task. However, on this issue, previous research, whether based on appearance or motion features, has not yet provided a satisfactory solution, especially under a complex background. To this end, the present work proposes a motion-based method termed the Multi-Scale Space Kinematic detection method (MUSAK). It fully leverages the motion patterns by extracting 3D, pseudo 3D and 2D kinematic parameters at three scale spaces according to the keypoints quality and builds three Gated Recurrent Unit (GRU)-based detection branches for drone recognition. The MUSAK method is evaluated on a hybrid dataset named multiscale UAV dataset (MUD), consisting of public datasets and self-collected data with motion labels. The experimental results show that MUSAK improves the performance by a large margin, a 95% increase in average precision (AP), compared with the previous state-of-the-art (SOTA) motion-based methods, and the hybrid MUSAK method, which integrates with the appearance-based method Faster Region-based Convolutional Neural Network (Faster R-CNN), achieves a new SOTA performance on AP metrics (AP, APM, and APS).},
DOI = {10.3390/rs14061434}
}



@Article{s22062354,
AUTHOR = {Yu, Ruonan and Li, Hongguang and Jiang, Yalong and Zhang, Baochang and Wang, Yufeng},
TITLE = {Tiny Vehicle Detection for Mid-to-High Altitude UAV Images Based on Visual Attention and Spatial-Temporal Information},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {2354},
URL = {https://www.mdpi.com/1424-8220/22/6/2354},
PubMedID = {35336525},
ISSN = {1424-8220},
ABSTRACT = {Mid-to-high altitude Unmanned Aerial Vehicle (UAV) imagery can provide important remote sensing information between satellite and low altitude platforms, and vehicle detection in mid-to-high altitude UAV images plays a crucial role in land monitoring and disaster relief. However, the high background complexity of images and limited pixels of objects challenge the performance of tiny vehicle detection. Traditional methods suffer from poor adaptation ability to complex backgrounds, while deep neural networks (DNNs) have inherent defects in feature extraction of tiny objects with finite pixels. To address the issue above, this paper puts forward a vehicle detection method combining the DNNs-based and traditional methods for mid-to-high altitude UAV images. We first employ the deep segmentation network to exploit the co-occurrence of the road and vehicles, then detect tiny vehicles based on visual attention mechanism with spatial-temporal constraint information. Experimental results show that the proposed method achieves effective detection of tiny vehicles in complex backgrounds. In addition, ablation experiments are performed to inspect the effectiveness of each component, and comparative experiments on tinier objects are carried out to prove the superior generalization performance of our method in detecting vehicles with a limited size of 5 &times; 5 pixels or less.},
DOI = {10.3390/s22062354}
}



@Article{rs14071561,
AUTHOR = {Hao, Zhenbang and Post, Christopher J. and Mikhailova, Elena A. and Lin, Lili and Liu, Jian and Yu, Kunyong},
TITLE = {How Does Sample Labeling and Distribution Affect the Accuracy and Efficiency of a Deep Learning Model for Individual Tree-Crown Detection and Delineation},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {1561},
URL = {https://www.mdpi.com/2072-4292/14/7/1561},
ISSN = {2072-4292},
ABSTRACT = {Monitoring and assessing vegetation using deep learning approaches has shown promise in forestry applications. Sample labeling to represent forest complexity is the main limitation for deep learning approaches for remote sensing vegetation classification applications, and few studies have focused on the impact of sample labeling methods on model performance and model training efficiency. This study is the first-of-its-kind that uses Mask region-based convolutional neural networks (Mask R-CNN) to evaluate the influence of sample labeling methods (including sample size and sample distribution) on individual tree-crown detection and delineation. A flight was conducted over a plantation with Fokienia hodginsii as the main tree species using a Phantom4-Multispectral (P4M) to obtain UAV imagery, and a total of 2061 manually and accurately delineated tree crowns were used for training and validating (1689) and testing (372). First, the model performance of three pre-trained backbones (ResNet-34, ResNet-50, and ResNet-101) was evaluated. Second, random deleting and clumped deleting methods were used to repeatedly delete 10% from the original sample set to reduce the training and validation set, to simulate two different sample distributions (the random sample set and the clumped sample set). Both RGB image and Multi-band images derived from UAV flights were used to evaluate model performance. Each model&rsquo;s average per-epoch training time was calculated to evaluate the model training efficiency. The results showed that ResNet-50 yielded a more robust network than ResNet-34 and ResNet-101 when the same parameters were used for Mask R-CNN. The sample size determined the influence of sample labeling methods on the model performance. Random sample labeling had lower requirements for sample size compared to clumped sample labeling, and unlabeled trees in random sample labeling had no impact on model training. Additionally, the model with clumped samples provides a shorter average per-epoch training time than the model with random samples. This study demonstrates that random sample labeling can greatly reduce the requirement of sample size, and it is not necessary to accurately label each sample in the image during the sample labeling process.},
DOI = {10.3390/rs14071561}
}



@Article{s22072711,
AUTHOR = {Li, Xiuhua and Ba, Yuxuan and Zhang, Muqing and Nong, Mengling and Yang, Ce and Zhang, Shimin},
TITLE = {Sugarcane Nitrogen Concentration and Irrigation Level Prediction Based on UAV Multispectral Imagery},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {2711},
URL = {https://www.mdpi.com/1424-8220/22/7/2711},
ISSN = {1424-8220},
ABSTRACT = {Sugarcane is the main industrial crop for sugar production, and its growth status is closely related to fertilizer, water, and light input. Unmanned aerial vehicle (UAV)-based multispectral imagery is widely used for high-throughput phenotyping, since it can rapidly predict crop vigor at field scale. This study focused on the potential of drone multispectral images in predicting canopy nitrogen concentration (CNC) and irrigation levels for sugarcane. An experiment was carried out in a sugarcane field with three irrigation levels and five fertilizer levels. Multispectral images at an altitude of 40 m were acquired during the elongating stage. Partial least square (PLS), backpropagation neural network (BPNN), and extreme learning machine (ELM) were adopted to establish CNC prediction models based on various combinations of band reflectance and vegetation indices. The simple ratio pigment index (SRPI), normalized pigment chlorophyll index (NPCI), and normalized green-blue difference index (NGBDI) were selected as model inputs due to their higher grey relational degree with the CNC and lower correlation between one another. The PLS model based on the five-band reflectance and the three vegetation indices achieved the best accuracy (Rv = 0.79, RMSEv = 0.11). Support vector machine (SVM) and BPNN were then used to classify the irrigation levels based on five spectral features which had high correlations with irrigation levels. SVM reached a higher accuracy of 80.6%. The results of this study demonstrated that high resolution multispectral images could provide effective information for CNC prediction and water irrigation level recognition for sugarcane crop.},
DOI = {10.3390/s22072711}
}



