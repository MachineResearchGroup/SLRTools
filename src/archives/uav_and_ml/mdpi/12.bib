
@Article{s21062180,
AUTHOR = {Liu, Chang and Szirányi, Tamás},
TITLE = {Real-Time Human Detection and Gesture Recognition for On-Board UAV Rescue},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2180},
URL = {https://www.mdpi.com/1424-8220/21/6/2180},
PubMedID = {33804718},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play an important role in numerous technical and scientific fields, especially in wilderness rescue. This paper carries out work on real-time UAV human detection and recognition of body and hand rescue gestures. We use body-featuring solutions to establish biometric communications, like yolo3-tiny for human detection. When the presence of a person is detected, the system will enter the gesture recognition phase, where the user and the drone can communicate briefly and effectively, avoiding the drawbacks of speech communication. A data-set of ten body rescue gestures (i.e., Kick, Punch, Squat, Stand, Attention, Cancel, Walk, Sit, Direction, and PhoneCall) has been created by a UAV on-board camera. The two most important gestures are the novel dynamic Attention and Cancel which represent the set and reset functions respectively. When the rescue gesture of the human body is recognized as Attention, the drone will gradually approach the user with a larger resolution for hand gesture recognition. The system achieves 99.80% accuracy on testing data in body gesture data-set and 94.71% accuracy on testing data in hand gesture data-set by using the deep learning method. Experiments conducted on real-time UAV cameras confirm our solution can achieve our expected UAV rescue purpose.},
DOI = {10.3390/s21062180}
}



@Article{s21062233,
AUTHOR = {Li, Ke and Zhang, Kun and Zhang, Zhenchong and Liu, Zekun and Hua, Shuai and He, Jianliang},
TITLE = {A UAV Maneuver Decision-Making Algorithm for Autonomous Airdrop Based on Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2233},
URL = {https://www.mdpi.com/1424-8220/21/6/2233},
PubMedID = {33806886},
ISSN = {1424-8220},
ABSTRACT = {How to operate an unmanned aerial vehicle (UAV) safely and efficiently in an interactive environment is challenging. A large amount of research has been devoted to improve the intelligence of a UAV while performing a mission, where finding an optimal maneuver decision-making policy of the UAV has become one of the key issues when we attempt to enable the UAV autonomy. In this paper, we propose a maneuver decision-making algorithm based on deep reinforcement learning, which generates efficient maneuvers for a UAV agent to execute the airdrop mission autonomously in an interactive environment. Particularly, the training set of the learning algorithm by the Prioritized Experience Replay is constructed, that can accelerate the convergence speed of decision network training in the algorithm. It is shown that a desirable and effective maneuver decision-making policy can be found by extensive experimental results.},
DOI = {10.3390/s21062233}
}



@Article{app11093863,
AUTHOR = {Öztürk, Ali Emre and Erçelebi, Ergun},
TITLE = {Real UAV-Bird Image Classification Using CNN with a Synthetic Dataset},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3863},
URL = {https://www.mdpi.com/2076-3417/11/9/3863},
ISSN = {2076-3417},
ABSTRACT = {A large amount of training image data is required for solving image classification problems using deep learning (DL) networks. In this study, we aimed to train DL networks with synthetic images generated by using a game engine and determine the effects of the networks on performance when solving real-image classification problems. The study presents the results of using corner detection and nearest three-point selection (CDNTS) layers to classify bird and rotary-wing unmanned aerial vehicle (RW-UAV) images, provides a comprehensive comparison of two different experimental setups, and emphasizes the significant improvements in the performance in deep learning-based networks due to the inclusion of a CDNTS layer. Experiment 1 corresponds to training the commonly used deep learning-based networks with synthetic data and an image classification test on real data. Experiment 2 corresponds to training the CDNTS layer and commonly used deep learning-based networks with synthetic data and an image classification test on real data. In experiment 1, the best area under the curve (AUC) value for the image classification test accuracy was measured as 72%. In experiment 2, using the CDNTS layer, the AUC value for the image classification test accuracy was measured as 88.9%. A total of 432 different combinations of trainings were investigated in the experimental setups. The experiments were trained with various DL networks using four different optimizers by considering all combinations of batch size, learning rate, and dropout hyperparameters. The test accuracy AUC values for networks in experiment 1 ranged from 55% to 74%, whereas the test accuracy AUC values in experiment 2 networks with a CDNTS layer ranged from 76% to 89.9%. It was observed that the CDNTS layer has considerable effects on the image classification accuracy performance of deep learning-based networks. AUC, F-score, and test accuracy measures were used to validate the success of the networks.},
DOI = {10.3390/app11093863}
}



@Article{rs13091670,
AUTHOR = {Avola, Danilo and Cinque, Luigi and Diko, Anxhelo and Fagioli, Alessio and Foresti, Gian Luca and Mecca, Alessio and Pannone, Daniele and Piciarelli, Claudio},
TITLE = {MS-Faster R-CNN: Multi-Stream Backbone for Improved Faster R-CNN Object Detection and Aerial Tracking from UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1670},
URL = {https://www.mdpi.com/2072-4292/13/9/1670},
ISSN = {2072-4292},
ABSTRACT = {Tracking objects across multiple video frames is a challenging task due to several difficult issues such as occlusions, background clutter, lighting as well as object and camera view-point variations, which directly affect the object detection. These aspects are even more emphasized when analyzing unmanned aerial vehicles (UAV) based images, where the vehicle movement can also impact the image quality. A common strategy employed to address these issues is to analyze the input images at different scales to obtain as much information as possible to correctly detect and track the objects across video sequences. Following this rationale, in this paper, we introduce a simple yet effective novel multi-stream (MS) architecture, where different kernel sizes are applied to each stream to simulate a multi-scale image analysis. The proposed architecture is then used as backbone for the well-known Faster-R-CNN pipeline, defining a MS-Faster R-CNN object detector that consistently detects objects in video sequences. Subsequently, this detector is jointly used with the Simple Online and Real-time Tracking with a Deep Association Metric (Deep SORT) algorithm to achieve real-time tracking capabilities on UAV images. To assess the presented architecture, extensive experiments were performed on the UMCD, UAVDT, UAV20L, and UAV123 datasets. The presented pipeline achieved state-of-the-art performance, confirming that the proposed multi-stream method can correctly emulate the robust multi-scale image analysis paradigm.},
DOI = {10.3390/rs13091670}
}



@Article{rs13101975,
AUTHOR = {Wang, Lin and Zhou, Yuzhen and Hu, Qiao and Tang, Zhenghong and Ge, Yufeng and Smith, Adam and Awada, Tala and Shi, Yeyin},
TITLE = {Early Detection of Encroaching Woody Juniperus virginiana and Its Classification in Multi-Species Forest Using UAS Imagery and Semantic Segmentation Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1975},
URL = {https://www.mdpi.com/2072-4292/13/10/1975},
ISSN = {2072-4292},
ABSTRACT = {Woody plant encroachment into grasslands ecosystems causes significantly ecological destruction and economic losses. Effective and efficient management largely benefits from accurate and timely detection of encroaching species at an early development stage. Recent advances in unmanned aircraft systems (UAS) enabled easier access to ultra-high spatial resolution images at a centimeter level, together with the latest machine learning based image segmentation algorithms, making it possible to detect small-sized individuals of target species at early development stage and identify them when mixed with other species. However, few studies have investigated the optimal practical spatial resolution of early encroaching species detection. Hence, we investigated the performance of four popular semantic segmentation algorithms (decision tree, DT; random forest, RF; AlexNet; and ResNet) on a multi-species forest classification case with UAS-collected RGB images in original and down-sampled coarser spatial resolutions. The objective of this study was to explore the optimal segmentation algorithm and spatial resolution for eastern redcedar (Juniperus virginiana, ERC) early detection and its classification within a multi-species forest context. To be specific, firstly, we implemented and compared the performance of the four semantic segmentation algorithms with images in the original spatial resolution (0.694 cm). The highest overall accuracy was 0.918 achieved by ResNet with a mean interaction over union at 85.0%. Secondly, we evaluated the performance of ResNet algorithm with images in down-sampled spatial resolutions (1 cm to 5 cm with 0.5 cm interval). When applied on the down-sampled images, ERC segmentation performance decreased with decreasing spatial resolution, especially for those images coarser than 3 cm spatial resolution. The UAS together with the state-of-the-art semantic segmentation algorithms provides a promising tool for early-stage detection and localization of ERC and the development of effective management strategies for mixed-species forest management.},
DOI = {10.3390/rs13101975}
}



@Article{rs13112077,
AUTHOR = {Fetai, Bujar and Račič, Matej and Lisec, Anka},
TITLE = {Deep Learning for Detection of Visible Land Boundaries from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2077},
URL = {https://www.mdpi.com/2072-4292/13/11/2077},
ISSN = {2072-4292},
ABSTRACT = {Current efforts aim to accelerate cadastral mapping through innovative and automated approaches and can be used to both create and update cadastral maps. This research aims to automate the detection of visible land boundaries from unmanned aerial vehicle (UAV) imagery using deep learning. In addition, we wanted to evaluate the advantages and disadvantages of programming-based deep learning compared to commercial software-based deep learning. For the first case, we used the convolutional neural network U-Net, implemented in Keras, written in Python using the TensorFlow library. For commercial software-based deep learning, we used ENVINet5. UAV imageries from different areas were used to train the U-Net model, which was performed in Google Collaboratory and tested in the study area in Odranci, Slovenia. The results were compared with the results of ENVINet5 using the same datasets. The results showed that both models achieved an overall accuracy of over 95%. The high accuracy is due to the problem of unbalanced classes, which is usually present in boundary detection tasks. U-Net provided a recall of 0.35 and a precision of 0.68 when the threshold was set to 0.5. A threshold can be viewed as a tool for filtering predicted boundary maps and balancing recall and precision. For equitable comparison with ENVINet5, the threshold was increased. U-Net provided more balanced results, a recall of 0.65 and a precision of 0.41, compared to ENVINet5 recall of 0.84 and a precision of 0.35. Programming-based deep learning provides a more flexible yet complex approach to boundary mapping than software-based, which is rigid and does not require programming. The predicted visible land boundaries can be used both to speed up the creation of cadastral maps and to automate the revision of existing cadastral maps and define areas where updates are needed. The predicted boundaries cannot be considered final at this stage but can be used as preliminary cadastral boundaries.},
DOI = {10.3390/rs13112077}
}



@Article{s21134417,
AUTHOR = {Ukaegbu, Uchechi F. and Tartibu, Lagouge K. and Okwu, Modestus O. and Olayode, Isaac O.},
TITLE = {Development of a Light-Weight Unmanned Aerial Vehicle for Precision Agriculture},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4417},
URL = {https://www.mdpi.com/1424-8220/21/13/4417},
PubMedID = {34203187},
ISSN = {1424-8220},
ABSTRACT = {This paper describes the development of a modular unmanned aerial vehicle for the detection and eradication of weeds on farmland. Precision agriculture entails solving the problem of poor agricultural yield due to competition for nutrients by weeds and provides a faster approach to eliminating the problematic weeds using emerging technologies. This research has addressed the aforementioned problem. A quadcopter was built, and components were assembled with light-weight materials. The system consists of the electric motor, electronic speed controller, propellers, frame, lithium polymer (li-po) battery, flight controller, a global positioning system (GPS), and receiver. A sprayer module which consists of a relay, Raspberry Pi 3, spray pump, 12 V DC source, water hose, and the tank was built. It operated in such a way that when a weed is detected based on the deep learning algorithms deployed on the Raspberry Pi, general purpose input/output (GPIO) 17 or GPIO 18 (of the Raspberry Pi) were activated to supply 3.3 V, which turned on a DC relay to spray herbicides accordingly. The sprayer module was mounted on the quadcopter and from the test-running operation conducted, broadleaf and grass weeds were accurately detected and the spraying of herbicides according to the weed type occurred in less than a second.},
DOI = {10.3390/s21134417}
}



@Article{rs13132643,
AUTHOR = {Pedro, Dário and Matos-Carvalho, João P. and Fonseca, José M. and Mora, André},
TITLE = {Collision Avoidance on Unmanned Aerial Vehicles Using Neural Network Pipelines and Flow Clustering Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2643},
URL = {https://www.mdpi.com/2072-4292/13/13/2643},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Autonomous Vehicles (UAV), while not a recent invention, have recently acquired a prominent position in many industries, and they are increasingly used not only by avid customers, but also in high-demand technical use-cases, and will have a significant societal effect in the coming years. However, the use of UAVs is fraught with significant safety threats, such as collisions with dynamic obstacles (other UAVs, birds, or randomly thrown objects). This research focuses on a safety problem that is often overlooked due to a lack of technology and solutions to address it: collisions with non-stationary objects. A novel approach is described that employs deep learning techniques to solve the computationally intensive problem of real-time collision avoidance with dynamic objects using off-the-shelf commercial vision sensors. The suggested approach’s viability was corroborated by multiple experiments, firstly in simulation, and afterward in a concrete real-world case, that consists of dodging a thrown ball. A novel video dataset was created and made available for this purpose, and transfer learning was also tested, with positive results.},
DOI = {10.3390/rs13132643}
}



@Article{rs13142705,
AUTHOR = {Mhango, Joseph K. and Harris, Edwin W. and Green, Richard and Monaghan, James M.},
TITLE = {Mapping Potato Plant Density Variation Using Aerial Imagery and Deep Learning Techniques for Precision Agriculture},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2705},
URL = {https://www.mdpi.com/2072-4292/13/14/2705},
ISSN = {2072-4292},
ABSTRACT = {In potato (Solanum tuberosum) production, the number of tubers harvested and their sizes are related to the plant population. Field maps of the spatial variation in plant density can therefore provide a decision support tool for spatially variable harvest timing to optimize tuber sizes by allowing densely populated management zones more tuber-bulking time. Computer vision has been proposed to enumerate plant numbers using images from unmanned aerial vehicles (UAV) but inaccurate predictions in images of merged canopies remains a challenge. Some research has been done on individual potato plant bounding box prediction but there is currently no information on the spatial structure of plant density that these models may reveal and its relationship with potato yield quality attributes. In this study, the Faster Region-based Convolutional Neural Network (FRCNN) framework was used to produce a plant detection model and estimate plant densities across a UAV orthomosaic. Using aerial images of 2 mm ground sampling distance (GSD) collected from potatoes at 40 days after planting, the FRCNN model was trained to an average precision (aP) of 0.78 on unseen testing data. The model was then used to generate predictions on quadrants imposed on orthorectified rasters captured at 14 and 18 days after emergence. After spatially interpolating the plant densities, the resultant surfaces were highly correlated to manually-determined plant density (R2 = 0.80). Further correlations were observed with tuber number (r = 0.54 at Butter Hill; r = 0.53 at Horse Foxhole), marketable tuber weight per plant (r = −0.57 at Buttery Hill; r = −0.56 at Horse Foxhole) and the normalized difference vegetation index (r = 0.61). These results show that accurate two-dimensional maps of plant density can be constructed from UAV imagery with high correlation to important yield components, despite the loss of accuracy of FRCNN models in partially merged canopies.},
DOI = {10.3390/rs13142705}
}



@Article{rs13142721,
AUTHOR = {Li, Guang and Han, Wenting and Huang, Shenjin and Ma, Weitong and Ma, Qian and Cui, Xin},
TITLE = {Extraction of Sunflower Lodging Information Based on UAV Multi-Spectral Remote Sensing and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2721},
URL = {https://www.mdpi.com/2072-4292/13/14/2721},
ISSN = {2072-4292},
ABSTRACT = {The rapid and accurate identification of sunflower lodging is important for the assessment of damage to sunflower crops. To develop a fast and accurate method of extraction of information on sunflower lodging, this study improves the inputs to SegNet and U-Net to render them suitable for multi-band image processing. Random forest and two improved deep learning methods are combined with RGB, RGB + NIR, RGB + red-edge, and RGB + NIR + red-edge bands of multi-spectral images captured by a UAV (unmanned aerial vehicle) to construct 12 models to extract information on sunflower lodging. These models are then combined with the method used to ignore edge-related information to predict sunflower lodging. The results of experiments show that the deep learning methods were superior to the random forest method in terms of the obtained lodging information and accuracy. The predictive accuracy of the model constructed by using a combination of SegNet and RGB + NIR had the highest overall accuracy of 88.23%. Adding NIR to RGB improved the accuracy of extraction of the lodging information whereas adding red-edge reduced it. An overlay analysis of the results for the lodging area shows that the extraction error was mainly caused by the failure of the model to recognize lodging in mixed areas and low-coverage areas. The predictive accuracy of information on sunflower lodging when edge-related information was ignored was about 2% higher than that obtained by using the direct splicing method.},
DOI = {10.3390/rs13142721}
}



@Article{app11146524,
AUTHOR = {Pérez-González, Andrés and Jaramillo-Duque, Álvaro and Cano-Quintero, Juan Bernardo},
TITLE = {Automatic Boundary Extraction for Photovoltaic Plants Using the Deep Learning U-Net Model},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {6524},
URL = {https://www.mdpi.com/2076-3417/11/14/6524},
ISSN = {2076-3417},
ABSTRACT = {Nowadays, the world is in a transition towards renewable energy solar being one of the most promising sources used today. However, Solar Photovoltaic (PV) systems present great challenges for their proper performance such as dirt and environmental conditions that may reduce the output energy of the PV plants. For this reason, inspection and periodic maintenance are essential to extend useful life. The use of unmanned aerial vehicles (UAV) for inspection and maintenance of PV plants favor a timely diagnosis. UAV path planning algorithm over a PV facility is required to better perform this task. Therefore, it is necessary to explore how to extract the boundary of PV facilities with some techniques. This research work focuses on an automatic boundary extraction method of PV plants from imagery using a deep neural network model with a U-net structure. The results obtained were evaluated by comparing them with other reported works. Additionally, to achieve the boundary extraction processes, the standard metrics Intersection over Union (IoU) and the Dice Coefficient (DC) were considered to make a better conclusion among all methods. The experimental results evaluated on the Amir dataset show that the proposed approach can significantly improve the boundary and segmentation performance in the test stage up to 90.42% and 91.42% as calculated by IoU and DC metrics, respectively. Furthermore, the training period was faster. Consequently, it is envisaged that the proposed U-Net model will be an advantage in remote sensing image segmentation.},
DOI = {10.3390/app11146524}
}



@Article{s21154953,
AUTHOR = {Al-Emadi, Sara and Al-Ali, Abdulla and Al-Ali, Abdulaziz},
TITLE = {Audio-Based Drone Detection and Identification Using Deep Learning Techniques with Dataset Enhancement through Generative Adversarial Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {4953},
URL = {https://www.mdpi.com/1424-8220/21/15/4953},
PubMedID = {34372189},
ISSN = {1424-8220},
ABSTRACT = {Drones are becoming increasingly popular not only for recreational purposes but in day-to-day applications in engineering, medicine, logistics, security and others. In addition to their useful applications, an alarming concern in regard to the physical infrastructure security, safety and privacy has arisen due to the potential of their use in malicious activities. To address this problem, we propose a novel solution that automates the drone detection and identification processes using a drone’s acoustic features with different deep learning algorithms. However, the lack of acoustic drone datasets hinders the ability to implement an effective solution. In this paper, we aim to fill this gap by introducing a hybrid drone acoustic dataset composed of recorded drone audio clips and artificially generated drone audio samples using a state-of-the-art deep learning technique known as the Generative Adversarial Network. Furthermore, we examine the effectiveness of using drone audio with different deep learning algorithms, namely, the Convolutional Neural Network, the Recurrent Neural Network and the Convolutional Recurrent Neural Network in drone detection and identification. Moreover, we investigate the impact of our proposed hybrid dataset in drone detection. Our findings prove the advantage of using deep learning techniques for drone detection and identification while confirming our hypothesis on the benefits of using the Generative Adversarial Networks to generate real-like drone audio clips with an aim of enhancing the detection of new and unfamiliar drones.},
DOI = {10.3390/s21154953}
}



@Article{rs13163165,
AUTHOR = {Li, Wenning and Li, Yi and Gong, Jianhua and Feng, Quanlong and Zhou, Jieping and Sun, Jun and Shi, Chenhui and Hu, Weidong},
TITLE = {Urban Water Extraction with UAV High-Resolution Remote Sensing Data Based on an Improved U-Net Model},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3165},
URL = {https://www.mdpi.com/2072-4292/13/16/3165},
ISSN = {2072-4292},
ABSTRACT = {Obtaining water body images quickly and reliably is important to guide human production activities and study urban change. This paper presents a fast and accurate method to identify water bodies in complex environments based on UAV high-resolution images. First, an improved U-Net (SU-Net) model is proposed in this paper. By increasing the number of connections in the middle layer of the neural network, more image features can be retained through S-shaped circular connections. Second, aiming at the interference of mixed ground objects and dark ground objects on water detection, the fusion of a deep learning network and visual features is investigated. We analyse the influence of a wavelet transform and grey level cooccurrence matrix (GLCM) on water extraction. Using a confusion matrix to evaluate accuracy, the following conclusions are drawn: (1) Compared with existing methods, the SU-Net method achieves a significant improvement in accuracy, and the overall accuracy (OA) is 96.25%. The kappa coefficient (KC) is 0.952. (2) SU-Net combined with the GLCM has a higher accuracy (OA is 97.4%) and robustness in distinguishing mixed and dark objects. Based on this method, a distinct water boundary in urban areas, which provides data for urban water vector mapping, can be obtained.},
DOI = {10.3390/rs13163165}
}



@Article{rs13173437,
AUTHOR = {Qi, Yuan and Dong, Xuhua and Chen, Pengchao and Lee, Kyeong-Hwan and Lan, Yubin and Lu, Xiaoyang and Jia, Ruichang and Deng, Jizhong and Zhang, Yali},
TITLE = {Canopy Volume Extraction of Citrus reticulate Blanco cv. Shatangju Trees Using UAV Image-Based Point Cloud Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3437},
URL = {https://www.mdpi.com/2072-4292/13/17/3437},
ISSN = {2072-4292},
ABSTRACT = {Automatic acquisition of the canopy volume parameters of the Citrus reticulate Blanco cv. Shatangju tree is of great significance to precision management of the orchard. This research combined the point cloud deep learning algorithm with the volume calculation algorithm to segment the canopy of the Citrus reticulate Blanco cv. Shatangju trees. The 3D (Three-Dimensional) point cloud model of a Citrus reticulate Blanco cv. Shatangju orchard was generated using UAV tilt photogrammetry images. The segmentation effects of three deep learning models, PointNet++, MinkowskiNet and FPConv, on Shatangju trees and the ground were compared. The following three volume algorithms: convex hull by slices, voxel-based method and 3D convex hull were applied to calculate the volume of Shatangju trees. Model accuracy was evaluated using the coefficient of determination (R2) and Root Mean Square Error (RMSE). The results show that the overall accuracy of the MinkowskiNet model (94.57%) is higher than the other two models, which indicates the best segmentation effect. The 3D convex hull algorithm received the highest R2 (0.8215) and the lowest RMSE (0.3186 m3) for the canopy volume calculation, which best reflects the real volume of Citrus reticulate Blanco cv. Shatangju trees. The proposed method is capable of rapid and automatic acquisition for the canopy volume of Citrus reticulate Blanco cv. Shatangju trees.},
DOI = {10.3390/rs13173437}
}



@Article{drones5030089,
AUTHOR = {Hoseini, Sayed Amir and Hassan, Jahan and Bokani, Ayub and Kanhere, Salil S.},
TITLE = {In Situ MIMO-WPT Recharging of UAVs Using Intelligent Flying Energy Sources},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {89},
URL = {https://www.mdpi.com/2504-446X/5/3/89},
ISSN = {2504-446X},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), used in civilian applications such as emergency medical deliveries, precision agriculture, wireless communication provisioning, etc., face the challenge of limited flight time due to their reliance on the on-board battery. Therefore, developing efficient mechanisms for in situ power transfer to recharge UAV batteries holds potential to extend their mission time. In this paper, we study the use of the far-field wireless power transfer (WPT) technique from specialized, transmitter UAVs (tUAVs) carrying Multiple Input Multiple Output (MIMO) antennas for transferring wireless power to receiver UAVs (rUAVs) in a mission. The tUAVs can fly and adjust their distance to the rUAVs to maximize energy transfer gain. The use of MIMO antennas further boosts the energy reception by narrowing the energy beam toward the rUAVs. The complexity of their dynamic operating environment increases with the growing number of tUAVs and rUAVs with varying levels of energy consumption and residual power. We propose an intelligent trajectory selection algorithm for the tUAVs based on a deep reinforcement learning model called Proximal Policy Optimization (PPO) to optimize the energy transfer gain. The simulation results demonstrate that the PPO-based system achieves about a tenfold increase in flight time for a set of realistic transmit power, distance, sub-band number and antenna numbers. Further, PPO outperforms the benchmark movement strategies of “Traveling Salesman Problem” and “Low Battery First” when used by the tUAVs.},
DOI = {10.3390/drones5030089}
}



@Article{app11188419,
AUTHOR = {Zhao, Jiang and Sun, Jiaming and Cai, Zhihao and Wang, Longhong and Wang, Yingxun},
TITLE = {End-to-End Deep Reinforcement Learning for Image-Based UAV Autonomous Control},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8419},
URL = {https://www.mdpi.com/2076-3417/11/18/8419},
ISSN = {2076-3417},
ABSTRACT = {To achieve the perception-based autonomous control of UAVs, schemes with onboard sensing and computing are popular in state-of-the-art work, which often consist of several separated modules with respective complicated algorithms. Most methods depend on handcrafted designs and prior models with little capacity for adaptation and generalization. Inspired by the research on deep reinforcement learning, this paper proposes a new end-to-end autonomous control method to simplify the separate modules in the traditional control pipeline into a single neural network. An image-based reinforcement learning framework is established, depending on the design of the network architecture and the reward function. Training is performed with model-free algorithms developed according to the specific mission, and the control policy network can map the input image directly to the continuous actuator control command. A simulation environment for the scenario of UAV landing was built. In addition, the results under different typical cases, including both the small and large initial lateral or heading angle offsets, show that the proposed end-to-end method is feasible for perception-based autonomous control.},
DOI = {10.3390/app11188419}
}



@Article{app11188434,
AUTHOR = {Wang, Kaipeng and Meng, Zhijun and Wu, Zhe},
TITLE = {Deep Learning-Based Ground Target Detection and Tracking for Aerial Photography from UAVs},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8434},
URL = {https://www.mdpi.com/2076-3417/11/18/8434},
ISSN = {2076-3417},
ABSTRACT = {Target detection and tracking can be widely used in military and civilian scenarios. Unmanned aerial vehicles (UAVs) have high maneuverability and strong concealment, thus they are very suitable for using as a platform for ground target detection and tracking. Most of the existing target detection and tracking algorithms are aimed at conventional targets. Because of the small scale and the incomplete details of the targets in the aerial image, it is difficult to apply the conventional algorithms to aerial photography from UAVs. This paper proposes a ground target image detection and tracking algorithm applied to UAVs using a revised deep learning technology. Aiming at the characteristics of ground targets in aerial images, target detection algorithms and target tracking algorithms are improved. The target detection algorithm is improved to detect small targets on the ground. The target tracking algorithm is designed to recover the target after the target is lost. The target detection and tracking algorithm is verified on the aerial dataset.},
DOI = {10.3390/app11188434}
}



@Article{ijgi10090606,
AUTHOR = {Daranagama, Samitha and Witayangkurn, Apichon},
TITLE = {Automatic Building Detection with Polygonizing and Attribute Extraction from High-Resolution Images},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {606},
URL = {https://www.mdpi.com/2220-9964/10/9/606},
ISSN = {2220-9964},
ABSTRACT = {Buildings can be introduced as a fundamental element for forming a city. Therefore, up-to-date building maps have become vital for many applications, including urban mapping and urban expansion analysis. With the development of deep learning, segmenting building footprints from high-resolution remote sensing imagery has become a subject of intense study. Here, a modified version of the U-Net architecture with a combination of pre- and post-processing techniques was developed to extract building footprints from high-resolution aerial imagery and unmanned aerial vehicle (UAV) imagery. Data pre-processing with the logarithmic correction image enhancing algorithm showed the most significant improvement in the building detection accuracy for aerial images; meanwhile, the CLAHE algorithm improved the most concerning UAV images. This study developed a post-processing technique using polygonizing and polygon smoothing called the Douglas–Peucker algorithm, which made the building output directly ready to use for different applications. The attribute information, land use data, and population count data were applied using two open datasets. In addition, the building area and perimeter of each building were calculated as geometric attributes.},
DOI = {10.3390/ijgi10090606}
}



@Article{smartcities4030065,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Qayyum, Siddra and Heravi, Amirhossein},
TITLE = {Application of Deep Learning on UAV-Based Aerial Images for Flood Detection},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {3},
PAGES = {1220--1242},
URL = {https://www.mdpi.com/2624-6511/4/3/65},
ISSN = {2624-6511},
ABSTRACT = {Floods are one of the most fatal and devastating disasters, instigating an immense loss of human lives and damage to property, infrastructure, and agricultural lands. To cater to this, there is a need to develop and implement real-time flood management systems that could instantly detect flooded regions to initiate relief activities as early as possible. Current imaging systems, relying on satellites, have demonstrated low accuracy and delayed response, making them unreliable and impractical to be used in emergency responses to natural disasters such as flooding. This research employs Unmanned Aerial Vehicles (UAVs) to develop an automated imaging system that can identify inundated areas from aerial images. The Haar cascade classifier was explored in the case study to detect landmarks such as roads and buildings from the aerial images captured by UAVs and identify flooded areas. The extracted landmarks are added to the training dataset that is used to train a deep learning algorithm. Experimental results show that buildings and roads can be detected from the images with 91% and 94% accuracy, respectively. The overall accuracy of 91% is recorded in classifying flooded and non-flooded regions from the input case study images. The system has shown promising results on test images belonging to both pre- and post-flood classes. The flood relief and rescue workers can quickly locate flooded regions and rescue stranded people using this system. Such real-time flood inundation systems will help transform the disaster management systems in line with modern smart cities initiatives.},
DOI = {10.3390/smartcities4030065}
}



@Article{rs13193853,
AUTHOR = {Wu, Yiguang and Wang, Meizhen and Liu, Xuejun and Wang, Ziran and Ma, Tianwu and Lu, Zhimin and Liu, Dan and Xie, Yujia and Li, Xiuquan and Wang, Xing},
TITLE = {Monitoring the Work Cycles of Earthmoving Excavators in Earthmoving Projects Using UAV Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3853},
URL = {https://www.mdpi.com/2072-4292/13/19/3853},
ISSN = {2072-4292},
ABSTRACT = {Monitoring the work cycles of earthmoving excavators is an important aspect of construction productivity assessment. Currently, the most advanced method for the recognition of work cycles is the “Stretching-Bending” Sequential Pattern (SBSP), which is based on fixed-carrier video monitoring (FC-SBSP). However, the application of this method presupposes the availability of preconstructed installation carriers to act as a surveillance camera as well as installed and commissioned surveillance systems that work in tandem with them. Obviously, this method is difficult to apply to projects with no conditions for a monitoring camera installation or which have a short construction time. This highlights the potential application of Unmanned Aerial Vehicle (UAV) remote sensing, which is flexible and mobile. Unfortunately, few studies have been conducted on the application of UAV remote sensing for the work cycle monitoring of earthmoving excavators. This research is necessary because the use of UAV remote sensing for monitoring the work cycles of earthmoving excavators can improve construction productivity and save time and costs, especially in post-disaster reconstruction projects involving harsh construction environments, and emergency projects with short construction periods. In addition, the challenges posed by UAV shaking may have to be taken into account when using the SBSP for UAV remote sensing. To this end, this study used application experiments in which stabilization processing of UAV video data was performed for UAV shaking. The application experimental results show that the work cycle performance of UAV remote-sensing-based SBSP (UAV-SBSP) for UAV video data was 2.45% and 5.36% lower in terms of precision and recall, respectively, without stabilization processing than after stabilization processing. Comparative experiments were also designed to investigate the applicability of the SBSP oriented toward UAV remote sensing. Comparative experimental results show that the same level of performance was obtained for the recognition of work cycles with the UAV-SBSP as compared with the FC-SBSP, demonstrating the good applicability of this method. Therefore, the results of this study show that UAV remote sensing enables effective monitoring of earthmoving excavator work cycles in construction sites where monitoring cameras are not available for installation, and it can be used as an alternative technology to fixed-carrier video monitoring for onsite proximity monitoring.},
DOI = {10.3390/rs13193853}
}



@Article{rs13193892,
AUTHOR = {Zhang, Tianxiang and Xu, Zhiyong and Su, Jinya and Yang, Zhifang and Liu, Cunjia and Chen, Wen-Hua and Li, Jiangyun},
TITLE = {Ir-UNet: Irregular Segmentation U-Shape Network for Wheat Yellow Rust Detection by UAV Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3892},
URL = {https://www.mdpi.com/2072-4292/13/19/3892},
ISSN = {2072-4292},
ABSTRACT = {Crop disease is widely considered as one of the most pressing challenges for food crops, and therefore an accurate crop disease detection algorithm is highly desirable for its sustainable management. The recent use of remote sensing and deep learning is drawing increasing research interests in wheat yellow rust disease detection. However, current solutions on yellow rust detection are generally addressed by RGB images and the basic semantic segmentation algorithms (e.g., UNet), which do not consider the irregular and blurred boundary problems of yellow rust area therein, restricting the disease segmentation performance. Therefore, this work aims to develop an automatic yellow rust disease detection algorithm to cope with these boundary problems. An improved algorithm entitled Ir-UNet by embedding irregular encoder module (IEM), irregular decoder module (IDM) and content-aware channel re-weight module (CCRM) is proposed and compared against the basic UNet while with various input features. The recently collected dataset by DJI M100 UAV equipped with RedEdge multispectral camera is used to evaluate the algorithm performance. Comparative results show that the Ir-UNet with five raw bands outperforms the basic UNet, achieving the highest overall accuracy (OA) score (97.13%) among various inputs. Moreover, the use of three selected bands, Red-NIR-RE, in the proposed Ir-UNet can obtain a comparable result (OA: 96.83%) while with fewer spectral bands and less computation load. It is anticipated that this study by seamlessly integrating the Ir-UNet network and UAV multispectral images can pave the way for automated yellow rust detection at farmland scales.},
DOI = {10.3390/rs13193892}
}



@Article{rs13193919,
AUTHOR = {Mo, Jiawei and Lan, Yubin and Yang, Dongzi and Wen, Fei and Qiu, Hongbin and Chen, Xin and Deng, Xiaoling},
TITLE = {Deep Learning-Based Instance Segmentation Method of Litchi Canopy from UAV-Acquired Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3919},
URL = {https://www.mdpi.com/2072-4292/13/19/3919},
ISSN = {2072-4292},
ABSTRACT = {Instance segmentation of fruit tree canopies from images acquired by unmanned aerial vehicles (UAVs) is of significance for the precise management of orchards. Although deep learning methods have been widely used in the fields of feature extraction and classification, there are still phenomena of complex data and strong dependence on software performances. This paper proposes a deep learning-based instance segmentation method of litchi trees, which has a simple structure and lower requirements for data form. Considering that deep learning models require a large amount of training data, a labor-friendly semi-auto method for image annotation is introduced. The introduction of this method allows for a significant improvement in the efficiency of data pre-processing. Facing the high requirement of a deep learning method for computing resources, a partition-based method is presented for the segmentation of high-resolution digital orthophoto maps (DOMs). Citrus data is added to the training set to alleviate the lack of diversity of the original litchi dataset. The average precision (AP) is selected to evaluate the metric of the proposed model. The results show that with the help of training with the litchi-citrus datasets, the best AP on the test set reaches 96.25%.},
DOI = {10.3390/rs13193919}
}



@Article{rs13194017,
AUTHOR = {Harvey, Winthrop and Rainwater, Chase and Cothren, Jackson},
TITLE = {Direct Aerial Visual Geolocalization Using Deep Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {4017},
URL = {https://www.mdpi.com/2072-4292/13/19/4017},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) must keep track of their location in order to maintain flight plans. Currently, this task is almost entirely performed by a combination of Inertial Measurement Units (IMUs) and reference to GNSS (Global Navigation Satellite System). Navigation by GNSS, however, is not always reliable, due to various causes both natural (reflection and blockage from objects, technical fault, inclement weather) and artificial (GPS spoofing and denial). In such GPS-denied situations, it is desirable to have additional methods for aerial geolocalization. One such method is visual geolocalization, where aircraft use their ground facing cameras to localize and navigate. The state of the art in many ground-level image processing tasks involve the use of Convolutional Neural Networks (CNNs). We present here a study of how effectively a modern CNN designed for visual classification can be applied to the problem of Absolute Visual Geolocalization (AVL, localization without a prior location estimate). An Xception based architecture is trained from scratch over a &gt;1000 km2 section of Washington County, Arkansas to directly regress latitude and longitude from images from different orthorectified high-altitude survey flights. It achieves average localization accuracy on unseen image sets over the same region from different years and seasons with as low as 115 m average error, which localizes to 0.004% of the training area, or about 8% of the width of the 1.5 × 1.5 km input image. This demonstrates that CNNs are expressive enough to encode robust landscape information for geolocalization over large geographic areas. Furthermore, discussed are methods of providing uncertainty for CNN regression outputs, and future areas of potential improvement for use of deep neural networks in visual geolocalization.},
DOI = {10.3390/rs13194017}
}



@Article{rs13204027,
AUTHOR = {Byun, Sungwoo and Shin, In-Kyoung and Moon, Jucheol and Kang, Jiyoung and Choi, Sang-Il},
TITLE = {Road Traffic Monitoring from UAV Images Using Deep Learning Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4027},
URL = {https://www.mdpi.com/2072-4292/13/20/4027},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we propose a deep neural network-based method for estimating speed of vehicles on roads automatically from videos recorded using unmanned aerial vehicle (UAV). The proposed method includes the following; (1) detecting and tracking vehicles by analyzing the videos, (2) calculating the image scales using the distances between lanes on the roads, and (3) estimating the speeds of vehicles on the roads. Our method can automatically measure the speed of the vehicles from the only videos recorded using UAV without additional information in both directions on the roads simultaneously. In our experiments, we evaluate the performance of the proposed method with the visual data at four different locations. The proposed method shows 97.6% recall rate and 94.7% precision rate in detecting vehicles, and it shows error (root mean squared error) of 5.27 km/h in estimating the speeds of vehicles.},
DOI = {10.3390/rs13204027}
}



@Article{s21206826,
AUTHOR = {Yang, Baohua and Zhu, Yue and Zhou, Shuaijun},
TITLE = {Accurate Wheat Lodging Extraction from Multi-Channel UAV Images Using a Lightweight Network Model},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {6826},
URL = {https://www.mdpi.com/1424-8220/21/20/6826},
PubMedID = {34696038},
ISSN = {1424-8220},
ABSTRACT = {The extraction of wheat lodging is of great significance to post-disaster agricultural production management, disaster assessment and insurance subsidies. At present, the recognition of lodging wheat in the actual complex field environment still has low accuracy and poor real-time performance. To overcome this gap, first, four-channel fusion images, including RGB and DSM (digital surface model), as well as RGB and ExG (excess green), were constructed based on the RGB image acquired from unmanned aerial vehicle (UAV). Second, a Mobile U-Net model that combined a lightweight neural network with a depthwise separable convolution and U-Net model was proposed. Finally, three data sets (RGB, RGB + DSM and RGB + ExG) were used to train, verify, test and evaluate the proposed model. The results of the experiment showed that the overall accuracy of lodging recognition based on RGB + DSM reached 88.99%, which is 11.8% higher than that of original RGB and 6.2% higher than that of RGB + ExG. In addition, our proposed model was superior to typical deep learning frameworks in terms of model parameters, processing speed and segmentation accuracy. The optimized Mobile U-Net model reached 9.49 million parameters, which was 27.3% and 33.3% faster than the FCN and U-Net models, respectively. Furthermore, for RGB + DSM wheat lodging extraction, the overall accuracy of Mobile U-Net was improved by 24.3% and 15.3% compared with FCN and U-Net, respectively. Therefore, the Mobile U-Net model using RGB + DSM could extract wheat lodging with higher accuracy, fewer parameters and stronger robustness.},
DOI = {10.3390/s21206826}
}



@Article{su132011359,
AUTHOR = {Aliyari, Mostafa and Droguett, Enrique Lopez and Ayele, Yonas Zewdu},
TITLE = {UAV-Based Bridge Inspection via Transfer Learning},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {11359},
URL = {https://www.mdpi.com/2071-1050/13/20/11359},
ISSN = {2071-1050},
ABSTRACT = {As bridge inspection becomes more advanced and more ubiquitous, artificial intelligence (AI) techniques, such as machine and deep learning, could offer suitable solutions to the nation’s problems of overdue bridge inspections. AI coupling with various data that can be captured by unmanned aerial vehicles (UAVs) enables fully automated bridge inspections. The key to the success of automated bridge inspection is a model capable of detecting failures from UAV data like images and films. In this context, this paper investigates the performances of state-of-the-art convolutional neural networks (CNNs) through transfer learning for crack detection in UAV-based bridge inspection. The performance of different CNN models is evaluated via UAV-based inspection of Skodsberg Bridge, located in eastern Norway. The low-level features are extracted in the last layers of the CNN models and these layers are trained using 19,023 crack and non-crack images. There is always a trade-off between the number of trainable parameters that CNN models need to learn for each specific task and the number of non-trainable parameters that come from transfer learning. Therefore, selecting the optimized amount of transfer learning is a challenging task and, as there is not enough research in this area, it will be studied in this paper. Moreover, UAV-based bridge inception images require specific attention to establish a suitable dataset as the input of CNN models that are trained on homogenous images. However, in the real implementation of CNN models in UAV-based bridge inspection images, there are always heterogeneities and noises, such as natural and artificial effects like different luminosities, spatial positions, and colors of the elements in an image. In this study, the effects of such heterogeneities on the performance of CNN models via transfer learning are examined. The results demonstrate that with a simplified image cropping technique and with minimum effort to preprocess images, CNN models can identify crack elements from non-crack elements with 81% accuracy. Moreover, the results show that heterogeneities inherent in UAV-based bridge inspection data significantly affect the performance of CNN models with an average 32.6% decrease of accuracy of the CNN models. It is also found that deeper CNN models do not provide higher accuracy compared to the shallower CNN models when the number of images for adoption to a specific task, in this case crack detection, is not large enough; in this study, 19,023 images and shallower models outperform the deeper models.},
DOI = {10.3390/su132011359}
}



@Article{drones5040127,
AUTHOR = {Raza, Wamiq and Osman, Anas and Ferrini, Francesco and Natale, Francesco De},
TITLE = {Energy-Efficient Inference on the Edge Exploiting TinyML Capabilities for UAVs},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {127},
URL = {https://www.mdpi.com/2504-446X/5/4/127},
ISSN = {2504-446X},
ABSTRACT = {In recent years, the proliferation of unmanned aerial vehicles (UAVs) has increased dramatically. UAVs can accomplish complex or dangerous tasks in a reliable and cost-effective way but are still limited by power consumption problems, which pose serious constraints on the flight duration and completion of energy-demanding tasks. The possibility of providing UAVs with advanced decision-making capabilities in an energy-effective way would be extremely beneficial. In this paper, we propose a practical solution to this problem that exploits deep learning on the edge. The developed system integrates an OpenMV microcontroller into a DJI Tello Micro Aerial Vehicle (MAV). The microcontroller hosts a set of machine learning-enabled inference tools that cooperate to control the navigation of the drone and complete a given mission objective. The goal of this approach is to leverage the new opportunistic features of TinyML through OpenMV including offline inference, low latency, energy efficiency, and data security. The approach is successfully validated on a practical application consisting of the onboard detection of people wearing protection masks in a crowded environment.},
DOI = {10.3390/drones5040127}
}



@Article{app112110310,
AUTHOR = {Jang, Keunyoung and Kim, Jong-Woo and Ju, Ki-Beom and An, Yun-Kyu},
TITLE = {Infrastructure BIM Platform for Lifecycle Management},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {10310},
URL = {https://www.mdpi.com/2076-3417/11/21/10310},
ISSN = {2076-3417},
ABSTRACT = {Recently, the application of the BIM technique to infrastructure lifecycle management has increased rapidly to improve the efficiency of infrastructure management systems. Research on the lifecycle management of infrastructure, from planning and design to construction and management, has been carried out. Therefore, a systematic review of the literature on recent research is performed to analyze the current state of the BIM technique. State-of-the-art techniques for infrastructure lifecycle management, such as unmanned robots, sensors and processing techniques, artificial intelligence, etc., are also reviewed. An infrastructure BIM platform framework composed of BIM and state-of-the-art techniques is then proposed. The proposed platform is a web-based platform that contains quantity, schedule (4D), and cost (5D) construction management, and the monitoring systems enable collaboration with stakeholders in a Common Data Environment (CDE). The lifecycle management methodology, after infrastructure construction, is then completed and is developed using state-of-the-art techniques using unmanned robots, scan-to-BIM, and deep learning networks, etc. It is confirmed that collaboration with stakeholders in the CDE in construction management is possible using an infrastructure BIM platform. Moreover, lifecycle management of infrastructure is possible by systematic management, such as time history analysis, damage growth prediction, decision of repair and demolition, etc., using a regular inspection database based on an infrastructure BIM platform.},
DOI = {10.3390/app112110310}
}



@Article{rs13214476,
AUTHOR = {Traore, Adama and Ata-Ul-Karim, Syed Tahir and Duan, Aiwang and Soothar, Mukesh Kumar and Traore, Seydou and Zhao, Ben},
TITLE = {Predicting Equivalent Water Thickness in Wheat Using UAV Mounted Multispectral Sensor through Deep Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {4476},
URL = {https://www.mdpi.com/2072-4292/13/21/4476},
ISSN = {2072-4292},
ABSTRACT = {The equivalent water thickness (EWT) is an important biophysical indicator of water status in crops. The effective monitoring of EWT in wheat under different nitrogen and water treatments is important for irrigation management in precision agriculture. This study aimed to investigate the performances of machine learning (ML) algorithms in retrieving wheat EWT. For this purpose, a rain shelter experiment (Exp. 1) with four irrigation quantities (0, 120, 240, 360 mm) and two nitrogen levels (75 and 255 kg N/ha), and field experiments (Exps. 2–3) with the same irrigation and rainfall water levels (360 mm) but different nitrogen levels (varying from 75 to 255 kg N/ha) were conducted in the North China Plain. The canopy reflectance was measured for all plots at 30 m using an unmanned aerial vehicle (UAV)-mounted multispectral camera. Destructive sampling was conducted immediately after the UAV flights to measure total fresh and dry weight. Deep Neural Network (DNN) is a special type of neural network, which has shown performance in regression analysis is compared with other machine learning (ML) models. A feature selection (FS) algorithm named the decision tree (DT) was used as the automatic relevance determination method to obtain the relative relevance of 5 out of 67 vegetation indices (Vis), which were used for estimating EWT. The selected VIs were used to estimate EWT using multiple linear regression (MLR), deep neural network multilayer perceptron (DNN-MLP), artificial neural networks multilayer perceptron (ANN-MLP), boosted tree regression (BRT), and support vector machines (SVMs). The results show that the DNN-MLP with R2 = 0.934, NSE = 0.933, RMSE = 0.028 g/cm2, and MAE of 0.017 g/cm2 outperformed other ML algorithms (ANN-MPL, BRT, and SVM- Polynomial) owing to its high capacity for estimating EWT as compared to other ML methods. Our findings support the conclusion that ML can potentially be applied in combination with VIs for retrieving EWT. Despite the complexity of the ML models, the EWT map should help farmers by improving the real-time irrigation efficiency of wheat by quantifying field water content and addressing variability.},
DOI = {10.3390/rs13214476}
}



@Article{telecom2040027,
AUTHOR = {Singh, Simran and Kumbhar, Abhaykumar and Güvenç, İsmail and Sichitiu, Mihail L.},
TITLE = {Intelligent Interference Management in UAV-Based HetNets},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {472--488},
URL = {https://www.mdpi.com/2673-4001/2/4/27},
ISSN = {2673-4001},
ABSTRACT = {Unmanned aerial vehicles (UAVs) can play a key role in meeting certain demands of cellular networks. UAVs can be used not only as user equipment (UE) in cellular networks but also as mobile base stations (BSs) wherein they can either augment conventional BSs by adapting their position to serve the changing traffic and connectivity demands or temporarily replace BSs that are damaged due to natural disasters. The flexibility of UAVs allows them to provide coverage to UEs in hot-spots, at cell-edges, in coverage holes, or regions with scarce cellular infrastructure. In this work, we study how UAV locations and other cellular parameters may be optimized in such scenarios to maximize the spectral efficiency (SE) of the network. We compare the performance of machine learning (ML) techniques with conventional optimization approaches. We found that, on an average, a double deep Q learning approach can achieve 93.46% of the optimal median SE and 95.83% of the optimal mean SE. A simple greedy approach, which tunes the parameters of each BS and UAV independently, performed very well in all the cases that we tested. These computationally efficient approaches can be utilized to enhance the network performance in existing cellular networks.},
DOI = {10.3390/telecom2040027}
}



@Article{s21237888,
AUTHOR = {Lo, Li-Yu and Yiu, Chi Hao and Tang, Yu and Yang, An-Shik and Li, Boyang and Wen, Chih-Yung},
TITLE = {Dynamic Object Tracking on Autonomous UAV System for Surveillance Applications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {7888},
URL = {https://www.mdpi.com/1424-8220/21/23/7888},
PubMedID = {34883913},
ISSN = {1424-8220},
ABSTRACT = {The ever-burgeoning growth of autonomous unmanned aerial vehicles (UAVs) has demonstrated a promising platform for utilization in real-world applications. In particular, a UAV equipped with a vision system could be leveraged for surveillance applications. This paper proposes a learning-based UAV system for achieving autonomous surveillance, in which the UAV can be of assistance in autonomously detecting, tracking, and following a target object without human intervention. Specifically, we adopted the YOLOv4-Tiny algorithm for semantic object detection and then consolidated it with a 3D object pose estimation method and Kalman filter to enhance the perception performance. In addition, UAV path planning for a surveillance maneuver is integrated to complete the fully autonomous system. The perception module is assessed on a quadrotor UAV, while the whole system is validated through flight experiments. The experiment results verified the robustness, effectiveness, and reliability of the autonomous object tracking UAV system in performing surveillance tasks. The source code is released to the research community for future reference.},
DOI = {10.3390/s21237888}
}



@Article{app112412093,
AUTHOR = {Pérez-González, Andrés and Benítez-Montoya, Nelson and Jaramillo-Duque, Álvaro and Cano-Quintero, Juan Bernardo},
TITLE = {Coverage Path Planning with Semantic Segmentation for UAV in PV Plants},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {12093},
URL = {https://www.mdpi.com/2076-3417/11/24/12093},
ISSN = {2076-3417},
ABSTRACT = {Solar energy is one of the most strategic energy sources for the world&rsquo;s economic development. This has caused the number of solar photovoltaic plants to increase around the world; consequently, they are installed in places where their access and manual inspection are arduous and risky tasks. Recently, the inspection of photovoltaic plants has been conducted with the use of unmanned aerial vehicles (UAV). Although the inspection with UAVs can be completed with a drone operator, where the UAV flight path is purely manual or utilizes a previously generated flight path through a ground control station (GCS). However, the path generated in the GCS has many restrictions that the operator must supply. Due to these restrictions, we present a novel way to develop a flight path automatically with coverage path planning (CPP) methods. Using a DL server to segment the region of interest (RoI) within each of the predefined PV plant images, three CPP methods were also considered and their performances were assessed with metrics. The UAV energy consumption performance in each of the CPP methods was assessed using two different UAVs and standard metrics. Six experiments were performed by varying the CPP width, and the consumption metrics were recorded in each experiment. According to the results, the most effective and efficient methods are the exact cellular decomposition boustrophedon and grid-based wavefront coverage, depending on the CPP width and the area of the PV plant. Finally, a relationship was established between the size of the photovoltaic plant area and the best UAV to perform the inspection with the appropriate CPP width. This could be an important result for low-cost inspection with UAVs, without high-resolution cameras on the UAV board, and in small plants.},
DOI = {10.3390/app112412093}
}



@Article{rs14010075,
AUTHOR = {Reder, Stefan and Mund, Jan-Peter and Albert, Nicole and Waßermann, Lilli and Miranda, Luis},
TITLE = {Detection of Windthrown Tree Stems on UAV-Orthomosaics Using U-Net Convolutional Networks},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {75},
URL = {https://www.mdpi.com/2072-4292/14/1/75},
ISSN = {2072-4292},
ABSTRACT = {The increasing number of severe storm events is threatening European forests. Besides the primary damages directly caused by storms, there are secondary damages such as bark beetle outbreaks and tertiary damages due to negative effects on the market. These subsequent damages can be minimized if a detailed overview of the affected area and the amount of damaged wood can be obtained quickly and included in the planning of clearance measures. The present work utilizes UAV-orthophotos and an adaptation of the U-Net architecture for the semantic segmentation and localization of windthrown stems. The network was pre-trained with generic datasets, randomly combining stems and background samples in a copy&ndash;paste augmentation, and afterwards trained with a specific dataset of a particular windthrow. The models pre-trained with generic datasets containing 10, 50 and 100 augmentations per annotated windthrown stems achieved F1-scores of 73.9% (S1Mod10), 74.3% (S1Mod50) and 75.6% (S1Mod100), outperforming the baseline model (F1-score 72.6%), which was not pre-trained. These results emphasize the applicability of the method to correctly identify windthrown trees and suggest the collection of training samples from other tree species and windthrow areas to improve the ability to generalize. Further enhancements of the network architecture are considered to improve the classification performance and to minimize the calculative costs.},
DOI = {10.3390/rs14010075}
}



@Article{s22010270,
AUTHOR = {Domingo, Mari Carmen},
TITLE = {Power Allocation and Energy Cooperation for UAV-Enabled MmWave Networks: A Multi-Agent Deep Reinforcement Learning Approach},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {270},
URL = {https://www.mdpi.com/1424-8220/22/1/270},
PubMedID = {35009812},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicle (UAV)-assisted cellular networks over the millimeter-wave (mmWave) frequency band can meet the requirements of a high data rate and flexible coverage in next-generation communication networks. However, higher propagation loss and the use of a large number of antennas in mmWave networks give rise to high energy consumption and UAVs are constrained by their low-capacity onboard battery. Energy harvesting (EH) is a viable solution to reduce the energy cost of UAV-enabled mmWave networks. However, the random nature of renewable energy makes it challenging to maintain robust connectivity in UAV-assisted terrestrial cellular networks. Energy cooperation allows UAVs to send their excessive energy to other UAVs with reduced energy. In this paper, we propose a power allocation algorithm based on energy harvesting and energy cooperation to maximize the throughput of a UAV-assisted mmWave cellular network. Since there is channel-state uncertainty and the amount of harvested energy can be treated as a stochastic process, we propose an optimal multi-agent deep reinforcement learning algorithm (DRL) named Multi-Agent Deep Deterministic Policy Gradient (MADDPG) to solve the renewable energy resource allocation problem for throughput maximization. The simulation results show that the proposed algorithm outperforms the Random Power (RP), Maximal Power (MP) and value-based Deep Q-Learning (DQL) algorithms in terms of network throughput.},
DOI = {10.3390/s22010270}
}



@Article{rs14010199,
AUTHOR = {Carbonell-Rivera, Juan Pedro and Torralba, Jesús and Estornell, Javier and Ruiz, Luis Ángel and Crespo-Peremarch, Pablo},
TITLE = {Classification of Mediterranean Shrub Species from UAV Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {199},
URL = {https://www.mdpi.com/2072-4292/14/1/199},
ISSN = {2072-4292},
ABSTRACT = {Modelling fire behaviour in forest fires is based on meteorological, topographical, and vegetation data, including species&rsquo; type. To accurately parameterise these models, an inventory of the area of analysis with the maximum spatial and temporal resolution is required. This study investigated the use of UAV-based digital aerial photogrammetry (UAV-DAP) point clouds to classify tree and shrub species in Mediterranean forests, and this information is key for the correct generation of wildfire models. In July 2020, two test sites located in the Natural Park of Sierra Calderona (eastern Spain) were analysed, registering 1036 vegetation individuals as reference data, corresponding to 11 shrub and one tree species. Meanwhile, photogrammetric flights were carried out over the test sites, using a UAV DJI Inspire 2 equipped with a Micasense RedEdge multispectral camera. Geometrical, spectral, and neighbour-based features were obtained from the resulting point cloud generated. Using these features, points belonging to tree and shrub species were classified using several machine learning methods, i.e., Decision Trees, Extra Trees, Gradient Boosting, Random Forest, and MultiLayer Perceptron. The best results were obtained using Gradient Boosting, with a mean cross-validation accuracy of 81.7% and 91.5% for test sites 1 and 2, respectively. Once the best classifier was selected, classified points were clustered based on their geometry and tested with evaluation data, and overall accuracies of 81.9% and 96.4% were obtained for test sites 1 and 2, respectively. Results showed that the use of UAV-DAP allows the classification of Mediterranean tree and shrub species. This technique opens a wide range of possibilities, including the identification of species as a first step for further extraction of structure and fuel variables as input for wildfire behaviour models.},
DOI = {10.3390/rs14010199}
}



@Article{s22010404,
AUTHOR = {Chang, Ching-Wei and Lo, Li-Yu and Cheung, Hiu Ching and Feng, Yurong and Yang, An-Shik and Wen, Chih-Yung and Zhou, Weifeng},
TITLE = {Proactive Guidance for Accurate UAV Landing on a Dynamic Platform: A Visual&ndash;Inertial Approach},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {404},
URL = {https://www.mdpi.com/1424-8220/22/1/404},
PubMedID = {35009946},
ISSN = {1424-8220},
ABSTRACT = {This work aimed to develop an autonomous system for unmanned aerial vehicles (UAVs) to land on moving platforms such as an automobile or a marine vessel, providing a promising solution for a long-endurance flight operation, a large mission coverage range, and a convenient recharging ground station. Unlike most state-of-the-art UAV landing frameworks that rely on UAV onboard computers and sensors, the proposed system fully depends on the computation unit situated on the ground vehicle/marine vessel to serve as a landing guidance system. Such a novel configuration can therefore lighten the burden of the UAV, and the computation power of the ground vehicle/marine vessel can be enhanced. In particular, we exploit a sensor fusion-based algorithm for the guidance system to perform UAV localization, whilst a control method based upon trajectory optimization is integrated. Indoor and outdoor experiments are conducted, and the results show that precise autonomous landing on a 43 cm &times; 43 cm platform can be performed.},
DOI = {10.3390/s22010404}
}



@Article{rs14020265,
AUTHOR = {Wang, Yanjun and Li, Shaochun and Teng, Fei and Lin, Yunhao and Wang, Mengjie and Cai, Hengfan},
TITLE = {Improved Mask R-CNN for Rural Building Roof Type Recognition from UAV High-Resolution Images: A Case Study in Hunan Province, China},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {265},
URL = {https://www.mdpi.com/2072-4292/14/2/265},
ISSN = {2072-4292},
ABSTRACT = {Accurate roof information of buildings can be obtained from UAV high-resolution images. The large-scale accurate recognition of roof types (such as gabled, flat, hipped, complex and mono-pitched roofs) of rural buildings is crucial for rural planning and construction. At present, most UAV high-resolution optical images only have red, green and blue (RGB) band information, which aggravates the problems of inter-class similarity and intra-class variability of image features. Furthermore, the different roof types of rural buildings are complex, spatially scattered, and easily covered by vegetation, which in turn leads to the low accuracy of roof type identification by existing methods. In response to the above problems, this paper proposes a method for identifying roof types of complex rural buildings based on visible high-resolution remote sensing images from UAVs. First, the fusion of deep learning networks with different visual features is investigated to analyze the effect of the different feature combinations of the visible difference vegetation index (VDVI) and Sobel edge detection features and UAV visible images on model recognition of rural building roof types. Secondly, an improved Mask R-CNN model is proposed to learn more complex features of different types of images of building roofs by using the ResNet152 feature extraction network with migration learning. After we obtained roof type recognition results in two test areas, we evaluated the accuracy of the results using the confusion matrix and obtained the following conclusions: (1) the model with RGB images incorporating Sobel edge detection features has the highest accuracy and enables the model to recognize more and more accurately the roof types of different morphological rural buildings, and the model recognition accuracy (Kappa coefficient (KC)) compared to that of RGB images is on average improved by 0.115; (2) compared with the original Mask R-CNN, U-Net, DeeplabV3 and PSPNet deep learning models, the improved Mask R-CNN model has the highest accuracy in recognizing the roof types of rural buildings, with F1-score, KC and OA averaging 0.777, 0.821 and 0.905, respectively. The method can obtain clear and accurate profiles and types of rural building roofs, and can be extended for green roof suitability evaluation, rooftop solar potential assessment, and other building roof surveys, management and planning.},
DOI = {10.3390/rs14020265}
}



@Article{s22020464,
AUTHOR = {Nepal, Upesh and Eslamiat, Hossein},
TITLE = {Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {464},
URL = {https://www.mdpi.com/1424-8220/22/2/464},
PubMedID = {35062425},
ISSN = {1424-8220},
ABSTRACT = {In-flight system failure is one of the major safety concerns in the operation of unmanned aerial vehicles (UAVs) in urban environments. To address this concern, a safety framework consisting of following three main tasks can be utilized: (1) Monitoring health of the UAV and detecting failures, (2) Finding potential safe landing spots in case a critical failure is detected in step 1, and (3) Steering the UAV to a safe landing spot found in step 2. In this paper, we specifically look at the second task, where we investigate the feasibility of utilizing object detection methods to spot safe landing spots in case the UAV suffers an in-flight failure. Particularly, we investigate different versions of the YOLO objection detection method and compare their performances for the specific application of detecting a safe landing location for a UAV that has suffered an in-flight failure. We compare the performance of YOLOv3, YOLOv4, and YOLOv5l while training them by a large aerial image dataset called DOTA in a Personal Computer (PC) and also a Companion Computer (CC). We plan to use the chosen algorithm on a CC that can be attached to a UAV, and the PC is used to verify the trends that we see between the algorithms on the CC. We confirm the feasibility of utilizing these algorithms for effective emergency landing spot detection and report their accuracy and speed for that specific application. Our investigation also shows that the YOLOv5l algorithm outperforms YOLOv4 and YOLOv3 in terms of accuracy of detection while maintaining a slightly slower inference speed.},
DOI = {10.3390/s22020464}
}



@Article{aerospace9010031,
AUTHOR = {Samadzadegan, Farhad and Dadrass Javan, Farzaneh and Ashtari Mahini, Farnaz and Gholamshahi, Mehrnaz},
TITLE = {Detection and Recognition of Drones Based on a Deep Convolutional Neural Network Using Visible Imagery},
JOURNAL = {Aerospace},
VOLUME = {9},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {31},
URL = {https://www.mdpi.com/2226-4310/9/1/31},
ISSN = {2226-4310},
ABSTRACT = {Drones are becoming increasingly popular not only for recreational purposes but also in a variety of applications in engineering, disaster management, logistics, securing airports, and others. In addition to their useful applications, an alarming concern regarding physical infrastructure security, safety, and surveillance at airports has arisen due to the potential of their use in malicious activities. In recent years, there have been many reports of the unauthorized use of various types of drones at airports and the disruption of airline operations. To address this problem, this study proposes a novel deep learning-based method for the efficient detection and recognition of two types of drones and birds. Evaluation of the proposed approach with the prepared image dataset demonstrates better efficiency compared to existing detection systems in the literature. Furthermore, drones are often confused with birds because of their physical and behavioral similarity. The proposed method is not only able to detect the presence or absence of drones in an area but also to recognize and distinguish between two types of drones, as well as distinguish them from birds. The dataset used in this work to train the network consists of 10,000 visible images containing two types of drones as multirotors, helicopters, and also birds. The proposed deep learning method can directly detect and recognize two types of drones and distinguish them from birds with an accuracy of 83%, mAP of 84%, and IoU of 81%. The values of average recall, average accuracy, and average F1-score were also reported as 84%, 83%, and 83%, respectively, in three classes.},
DOI = {10.3390/aerospace9010031}
}



@Article{rs14020396,
AUTHOR = {Shi, Yue and Han, Liangxiu and Kleerekoper, Anthony and Chang, Sheng and Hu, Tongle},
TITLE = {Novel CropdocNet Model for Automated Potato Late Blight Disease Detection from Unmanned Aerial Vehicle-Based Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/2072-4292/14/2/396},
ISSN = {2072-4292},
ABSTRACT = {The accurate and automated diagnosis of potato late blight disease, one of the most destructive potato diseases, is critical for precision agricultural control and management. Recent advances in remote sensing and deep learning offer the opportunity to address this challenge. This study proposes a novel end-to-end deep learning model (CropdocNet) for accurate and automated late blight disease diagnosis from UAV-based hyperspectral imagery. The proposed method considers the potential disease-specific reflectance radiation variance caused by the canopy&rsquo;s structural diversity and introduces multiple capsule layers to model the part-to-whole relationship between spectral&ndash;spatial features and the target classes to represent the rotation invariance of the target classes in the feature space. We evaluate the proposed method with real UAV-based HSI data under controlled and natural field conditions. The effectiveness of the hierarchical features is quantitatively assessed and compared with the existing representative machine learning/deep learning methods on both testing and independent datasets. The experimental results show that the proposed model significantly improves accuracy when considering the hierarchical structure of spectral&ndash;spatial features, with average accuracies of 98.09% for the testing dataset and 95.75% for the independent dataset, respectively.},
DOI = {10.3390/rs14020396}
}



@Article{rs14030731,
AUTHOR = {Psiroukis, Vasilis and Espejo-Garcia, Borja and Chitos, Andreas and Dedousis, Athanasios and Karantzalos, Konstantinos and Fountas, Spyros},
TITLE = {Assessment of Different Object Detectors for the Maturity Level Classification of Broccoli Crops Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {731},
URL = {https://www.mdpi.com/2072-4292/14/3/731},
ISSN = {2072-4292},
ABSTRACT = {Broccoli is an example of a high-value crop that requires delicate handling throughout the growing season and during its post-harvesting treatment. As broccoli heads can be easily damaged, they are still harvested by hand. Moreover, human scouting is required to initially identify the field segments where several broccoli plants have reached the desired maturity level, such that they can be harvested while they are in the optimal condition. The aim of this study was to automate this process using state-of-the-art Object Detection architectures trained on georeferenced orthomosaic-derived RGB images captured from low-altitude UAV flights, and to assess their capacity to effectively detect and classify broccoli heads based on their maturity level. The results revealed that the object detection approach for automated maturity classification achieved comparable results to physical scouting overall, especially for the two best-performing architectures, namely Faster R-CNN and CenterNet. Their respective performances were consistently over 80% mAP@50 and 70% mAP@75 when using three levels of maturity, and even higher when simplifying the use case into a two-class problem, exceeding 91% and 83%, respectively. At the same time, geometrical transformations for data augmentations reported improvements, while colour distortions were counterproductive. The best-performing architecture and the trained model could be tested as a prototype in real-time UAV detections in order to assist in on-field broccoli maturity detection.},
DOI = {10.3390/rs14030731}
}



@Article{s22031200,
AUTHOR = {Jang, Younghoon and Raza, Syed M. and Kim, Moonseong and Choo, Hyunseung},
TITLE = {Proactive Handover Decision for UAVs with Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {1200},
URL = {https://www.mdpi.com/1424-8220/22/3/1200},
PubMedID = {35161945},
ISSN = {1424-8220},
ABSTRACT = {The applications of Unmanned Aerial Vehicles (UAVs) are rapidly growing in domains such as surveillance, logistics, and entertainment and require continuous connectivity with cellular networks to ensure their seamless operations. However, handover policies in current cellular networks are primarily designed for ground users, and thus are not appropriate for UAVs due to frequent fluctuations of signal strength in the air. This paper presents a novel handover decision scheme deploying Deep Reinforcement Learning (DRL) to prevent unnecessary handovers while maintaining stable connectivity. The proposed DRL framework takes the UAV state as an input for a proximal policy optimization algorithm and develops a Received Signal Strength Indicator (RSSI) based on a reward function for the online learning of UAV handover decisions. The proposed scheme is evaluated in a 3D-emulated UAV mobility environment where it reduces up to 76 and 73% of unnecessary handovers compared to greedy and Q-learning-based UAV handover decision schemes, respectively. Furthermore, this scheme ensures reliable communication with the UAV by maintaining the RSSI above &minus;75 dBm more than 80% of the time.},
DOI = {10.3390/s22031200}
}



@Article{s22031244,
AUTHOR = {Vlaminck, Michiel and Heidbuchel, Rugen and Philips, Wilfried and Luong, Hiep},
TITLE = {Region-Based CNN for Anomaly Detection in PV Power Plants Using Aerial Imagery},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {1244},
URL = {https://www.mdpi.com/1424-8220/22/3/1244},
PubMedID = {35161990},
ISSN = {1424-8220},
ABSTRACT = {Today, solar energy is taking an increasing share of the total energy mix. Unfortunately, many operational photovoltaic plants suffer from a plenitude of defects resulting in non-negligible power loss. The latter highly impacts the overall performance of the PV site; therefore, operators need to regularly inspect their solar parks for anomalies in order to prevent severe performance drops. As this operation is naturally labor-intensive and costly, we present in this paper a novel system for improved PV diagnostics using drone-based imagery. Our solution consists of three main steps. The first step locates the solar panels within the image. The second step detects the anomalies within the solar panels. The final step identifies the root cause of the anomaly. In this paper, we mainly focus on the second step comprising the detection of anomalies within solar panels, which is done using a region-based convolutional neural network (CNN). Experiments on six different PV sites with different specifications and a variety of defects demonstrate that our anomaly detector achieves a true positive rate or recall of more than 90% for a false positive rate of around 2% to 3% tested on a dataset containing nearly 9000 solar panels. Compared to the best state-of-the-art methods, the experiments revealed that we achieve a slightly higher true positive rate for a substantially lower false positive rate, while tested on a more realistic dataset.},
DOI = {10.3390/s22031244}
}



@Article{min12020268,
AUTHOR = {Sinaice, Brian Bino and Owada, Narihiro and Ikeda, Hajime and Toriya, Hisatoshi and Bagai, Zibisani and Shemang, Elisha and Adachi, Tsuyoshi and Kawamura, Youhei},
TITLE = {Spectral Angle Mapping and AI Methods Applied in Automatic Identification of Placer Deposit Magnetite Using Multispectral Camera Mounted on UAV},
JOURNAL = {Minerals},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {268},
URL = {https://www.mdpi.com/2075-163X/12/2/268},
ISSN = {2075-163X},
ABSTRACT = {The use of drones in mining environments is one way in which data pertaining to the state of a site in various industries can be remotely collected. This paper proposes a combined system that employs a 6-bands multispectral image capturing camera mounted on an Unmanned Aerial Vehicle (UAV) drone, Spectral Angle Mapping (SAM), as well as Artificial Intelligence (AI). Depth possessing multispectral data were captured at different flight elevations. This was in an attempt to find the best elevation where remote identification of magnetite iron sands via the UAV drone specialized in collecting spectral information at a minimum accuracy of +/&minus; 16 nm was possible. Data were analyzed via SAM to deduce the cosine similarity thresholds at each elevation. Using these thresholds, AI algorithms specialized in classifying imagery data were trained and tested to find the best performing model at classifying magnetite iron sand. Considering the post flight logs, the spatial area coverage of 338 m2, a global classification accuracy of 99.7%, as well the per-class precision of 99.4%, the 20 m flight elevation outputs presented the best performance ratios overall. Thus, the positive outputs of this study suggest viability in a variety of mining and mineral engineering practices.},
DOI = {10.3390/min12020268}
}



@Article{s22052068,
AUTHOR = {Gromada, Krzysztof and Siemiątkowska, Barbara and Stecz, Wojciech and Płochocki, Krystian and Woźniak, Karol},
TITLE = {Real-Time Object Detection and Classification by UAV Equipped With SAR},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {2068},
URL = {https://www.mdpi.com/1424-8220/22/5/2068},
PubMedID = {35271213},
ISSN = {1424-8220},
ABSTRACT = {The article presents real-time object detection and classification methods by unmanned aerial vehicles (UAVs) equipped with a synthetic aperture radar (SAR). Two algorithms have been extensively tested: classic image analysis and convolutional neural networks (YOLOv5). The research resulted in a new method that combines YOLOv5 with post-processing using classic image analysis. It is shown that the new system improves both the classification accuracy and the location of the identified object. The algorithms were implemented and tested on a mobile platform installed on a military-class UAV as the primary unit for online image analysis. The usage of objective low-computational complexity detection algorithms on SAR scans can reduce the size of the scans sent to the ground control station.},
DOI = {10.3390/s22052068}
}



@Article{rs14061400,
AUTHOR = {Zhang, Xin and Han, Liangxiu and Sobeih, Tam and Lappin, Lewis and Lee, Mark A. and Howard, Andew and Kisdi, Aron},
TITLE = {The Self-Supervised Spectral&ndash;Spatial Vision Transformer Network for Accurate Prediction of Wheat Nitrogen Status from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1400},
URL = {https://www.mdpi.com/2072-4292/14/6/1400},
ISSN = {2072-4292},
ABSTRACT = {Nitrogen (N) fertilizer is routinely applied by farmers to increase crop yields. At present, farmers often over-apply N fertilizer in some locations or at certain times because they do not have high-resolution crop N status data. N-use efficiency can be low, with the remaining N lost to the environment, resulting in higher production costs and environmental pollution. Accurate and timely estimation of N status in crops is crucial to improving cropping systems&rsquo; economic and environmental sustainability. Destructive approaches based on plant tissue analysis are time consuming and impractical over large fields. Recent advances in remote sensing and deep learning have shown promise in addressing the aforementioned challenges in a non-destructive way. In this work, we propose a novel deep learning framework: a self-supervised spectral&ndash;spatial attention-based vision transformer (SSVT). The proposed SSVT introduces a Spectral Attention Block (SAB) and a Spatial Interaction Block (SIB), which allows for simultaneous learning of both spatial and spectral features from UAV digital aerial imagery, for accurate N status prediction in wheat fields. Moreover, the proposed framework introduces local-to-global self-supervised learning to help train the model from unlabelled data. The proposed SSVT has been compared with five state-of-the-art models including: ResNet, RegNet, EfficientNet, EfficientNetV2, and the original vision transformer on both testing and independent datasets. The proposed approach achieved high accuracy (0.96) with good generalizability and reproducibility for wheat N status estimation.},
DOI = {10.3390/rs14061400}
}



@Article{rs14061523,
AUTHOR = {Ye, Zhangxi and Wei, Jiahao and Lin, Yuwei and Guo, Qian and Zhang, Jian and Zhang, Houxi and Deng, Hui and Yang, Kaijie},
TITLE = {Extraction of Olive Crown Based on UAV Visible Images and the U2-Net Deep Learning Model},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1523},
URL = {https://www.mdpi.com/2072-4292/14/6/1523},
ISSN = {2072-4292},
ABSTRACT = {Olive trees, which are planted widely in China, are economically significant. Timely and accurate acquisition of olive tree crown information is vital in monitoring olive tree growth and accurately predicting its fruit yield. The advent of unmanned aerial vehicles (UAVs) and deep learning (DL) provides an opportunity for rapid monitoring parameters of the olive tree crown. In this study, we propose a method of automatically extracting olive crown information (crown number and area of olive tree), combining visible-light images captured by consumer UAV and a new deep learning model, U2-Net, with a deeply nested structure. Firstly, a data set of an olive tree crown (OTC) images was constructed, which was further processed by the ESRGAN model to enhance the image resolution and was augmented (geometric transformation and spectral transformation) to enlarge the data set to increase the generalization ability of the model. Secondly, four typical subareas (A&ndash;D) in the study area were selected to evaluate the performance of the U2-Net model in olive crown extraction in different scenarios, and the U2-Net model was compared with three current mainstream deep learning models (i.e., HRNet, U-Net, and DeepLabv3+) in remote sensing image segmentation effect. The results showed that the U2-Net model achieved high accuracy in the extraction of tree crown numbers in the four subareas with a mean of intersection over union (IoU), overall accuracy (OA), and F1-Score of 92.27%, 95.19%, and 95.95%, respectively. Compared with the other three models, the IoU, OA, and F1-Score of the U2-Net model increased by 14.03&ndash;23.97 percentage points, 7.57&ndash;12.85 percentage points, and 8.15&ndash;14.78 percentage points, respectively. In addition, the U2-Net model had a high consistency between the predicted and measured area of the olive crown, and compared with the other three deep learning models, it had a lower error rate with a root mean squared error (RMSE) of 4.78, magnitude of relative error (MRE) of 14.27%, and a coefficient of determination (R2) higher than 0.93 in all four subareas, suggesting that the U2-Net model extracted the best crown profile integrity and was most consistent with the actual situation. This study indicates that the method combining UVA RGB images with the U2-Net model can provide a highly accurate and robust extraction result for olive tree crowns and is helpful in the dynamic monitoring and management of orchard trees.},
DOI = {10.3390/rs14061523}
}



@Article{ijgi11040222,
AUTHOR = {Ren, Simiao and Malof, Jordan and Fetter, Rob and Beach, Robert and Rineer, Jay and Bradbury, Kyle},
TITLE = {Utilizing Geospatial Data for Assessing Energy Security: Mapping Small Solar Home Systems Using Unmanned Aerial Vehicles and Deep Learning},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {11},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {222},
URL = {https://www.mdpi.com/2220-9964/11/4/222},
ISSN = {2220-9964},
ABSTRACT = {Solar home systems (SHS), a cost-effective solution for rural communities far from the grid in developing countries, are small solar panels and associated equipment that provides power to a single household. A crucial resource for targeting further investment of public and private resources, as well as tracking the progress of universal electrification goals, is shared access to high-quality data on individual SHS installations including information such as location and power capacity. Though recent studies utilizing satellite imagery and machine learning to detect solar panels have emerged, they struggle to accurately locate many SHS due to limited image resolution (some small solar panels only occupy several pixels in satellite imagery). In this work, we explore the viability and cost-performance tradeoff of using automatic SHS detection on unmanned aerial vehicle (UAV) imagery as an alternative to satellite imagery. More specifically, we explore three questions: (i) what is the detection performance of SHS using drone imagery; (ii) how expensive is the drone data collection, compared to satellite imagery; and (iii) how well does drone-based SHS detection perform in real-world scenarios? To examine these questions, we collect and publicly-release a dataset of high-resolution drone imagery encompassing SHS imaged under a variety of real-world conditions and use this dataset and a dataset of imagery from Rwanda to evaluate the capabilities of deep learning models to recognize SHS, including those that are too small to be reliably recognized in satellite imagery. The results suggest that UAV imagery may be a viable alternative to identify very small SHS from perspectives of both detection accuracy and financial costs of data collection. UAV-based data collection may be a practical option for supporting electricity access planning strategies for achieving sustainable development goals and for monitoring the progress towards those goals.},
DOI = {10.3390/ijgi11040222}
}



@Article{s22072513,
AUTHOR = {Yoo, Minjeong and Na, Yuseung and Song, Hamin and Kim, Gamin and Yun, Junseong and Kim, Sangho and Moon, Changjoo and Jo, Kichun},
TITLE = {Motion Estimation and Hand Gesture Recognition-Based Human&ndash;UAV Interaction Approach in Real Time},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {2513},
URL = {https://www.mdpi.com/1424-8220/22/7/2513},
ISSN = {1424-8220},
ABSTRACT = {As an alternative to traditional remote controller, research on vision-based hand gesture recognition is being actively conducted in the field of interaction between human and unmanned aerial vehicle (UAV). However, vision-based gesture system has a challenging problem in recognizing the motion of dynamic gesture because it is difficult to estimate the pose of multi-dimensional hand gestures in 2D images. This leads to complex algorithms, including tracking in addition to detection, to recognize dynamic gestures, but they are not suitable for human&ndash;UAV interaction (HUI) systems that require safe design with high real-time performance. Therefore, in this paper, we propose a hybrid hand gesture system that combines an inertial measurement unit (IMU)-based motion capture system and a vision-based gesture system to increase real-time performance. First, IMU-based commands and vision-based commands are divided according to whether drone operation commands are continuously input. Second, IMU-based control commands are intuitively mapped to allow the UAV to move in the same direction by utilizing estimated orientation sensed by a thumb-mounted micro-IMU, and vision-based control commands are mapped with hand&rsquo;s appearance through real-time object detection. The proposed system is verified in a simulation environment through efficiency evaluation with dynamic gestures of the existing vision-based system in addition to usability comparison with traditional joystick controller conducted for applicants with no experience in manipulation. As a result, it proves that it is a safer and more intuitive HUI design with a 0.089 ms processing speed and average lap time that takes about 19 s less than the joystick controller. In other words, it shows that it is viable as an alternative to existing HUI.},
DOI = {10.3390/s22072513}
}



@Article{rs14071710,
AUTHOR = {Onishi, Masanori and Watanabe, Shuntaro and Nakashima, Tadashi and Ise, Takeshi},
TITLE = {Practicality and Robustness of Tree Species Identification Using UAV RGB Image and Deep Learning in Temperate Forest in Japan},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {1710},
URL = {https://www.mdpi.com/2072-4292/14/7/1710},
ISSN = {2072-4292},
ABSTRACT = {Identifying tree species from the air has long been desired for forest management. Recently, combination of UAV RGB image and deep learning has shown high performance for tree identification in limited conditions. In this study, we evaluated the practicality and robustness of the tree identification system using UAVs and deep learning. We sampled training and test data from three sites in temperate forests in Japan. The objective tree species ranged across 56 species, including dead trees and gaps. When we evaluated the model performance on the dataset obtained from the same time and same tree crowns as the training dataset, it yielded a Kappa score of 0.97, and 0.72, respectively, for the performance on the dataset obtained from the same time but with different tree crowns. When we evaluated the dataset obtained from different times and sites from the training dataset, which is the same condition as the practical one, the Kappa scores decreased to 0.47. Though coniferous trees and representative species of stands showed a certain stable performance regarding identification, some misclassifications occurred between: (1) trees that belong to phylogenetically close species, (2) tree species with similar leaf shapes, and (3) tree species that prefer the same environment. Furthermore, tree types such as coniferous and broadleaved or evergreen and deciduous do not always guarantee common features between the different trees belonging to the tree type. Our findings promote the practicalization of identification systems using UAV RGB images and deep learning.},
DOI = {10.3390/rs14071710}
}



