
@Article{s17102196,
AUTHOR = {Sandino, Juan and Wooler, Adam and Gonzalez, Felipe},
TITLE = {Towards the Automatic Detection of Pre-Existing Termite Mounds through UAS and Hyperspectral Imagery},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {10},
ARTICLE-NUMBER = {2196},
URL = {https://www.mdpi.com/1424-8220/17/10/2196},
ISSN = {1424-8220},
ABSTRACT = {The increased technological developments in Unmanned Aerial Vehicles (UAVs) combined with artificial intelligence and Machine Learning (ML) approaches have opened the possibility of remote sensing of extensive areas of arid lands. In this paper, a novel approach towards the detection of termite mounds with the use of a UAV, hyperspectral imagery, ML and digital image processing is intended. A new pipeline process is proposed to detect termite mounds automatically and to reduce, consequently, detection times. For the classification stage, several ML classification algorithms’ outcomes were studied, selecting support vector machines as the best approach for their role in image classification of pre-existing termite mounds. Various test conditions were applied to the proposed algorithm, obtaining an overall accuracy of 68%. Images with satisfactory mound detection proved that the method is “resolution-dependent”. These mounds were detected regardless of their rotation and position in the aerial image. However, image distortion reduced the number of detected mounds due to the inclusion of a shape analysis method in the object detection phase, and image resolution is still determinant to obtain accurate results. Hyperspectral imagery demonstrated better capabilities to classify a huge set of materials than implementing traditional segmentation methods on RGB images only.},
DOI = {10.3390/s17102196}
}



@Article{rs9111187,
AUTHOR = {Meng, Xuelian and Shang, Nan and Zhang, Xukai and Li, Chunyan and Zhao, Kaiguang and Qiu, Xiaomin and Weeks, Eddie},
TITLE = {Photogrammetric UAV Mapping of Terrain under Dense Coastal Vegetation: An Object-Oriented Classification Ensemble Algorithm for Classification and Terrain Correction},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {1187},
URL = {https://www.mdpi.com/2072-4292/9/11/1187},
ISSN = {2072-4292},
ABSTRACT = {Photogrammetric UAV sees a surge in use for high-resolution mapping, but its use to map terrain under dense vegetation cover remains challenging due to a lack of exposed ground surfaces. This paper presents a novel object-oriented classification ensemble algorithm to leverage height, texture and contextual information of UAV data to improve landscape classification and terrain estimation. Its implementation incorporates multiple heuristics, such as multi-input machine learning-based classification, object-oriented ensemble, and integration of UAV and GPS surveys for terrain correction. Experiments based on a densely vegetated wetland restoration site showed classification improvement from 83.98% to 96.12% in overall accuracy and from 0.7806 to 0.947 in kappa value. Use of standard and existing UAV terrain mapping algorithms and software produced reliable digital terrain model only over exposed bare grounds (mean error = −0.019 m and RMSE = 0.035 m) but severely overestimated the terrain by ~80% of mean vegetation height in vegetated areas. The terrain correction method successfully reduced the mean error from 0.302 m to −0.002 m (RMSE from 0.342 m to 0.177 m) in low vegetation and from 1.305 m to 0.057 m (RMSE from 1.399 m to 0.550 m) in tall vegetation. Overall, this research validated a feasible solution to integrate UAV and RTK GPS for terrain mapping in densely vegetated environments. },
DOI = {10.3390/rs9111187}
}



@Article{rs10020285,
AUTHOR = {De Castro, Ana I. and Torres-Sánchez, Jorge and Peña, Jose M. and Jiménez-Brenes, Francisco M. and Csillik, Ovidiu and López-Granados, Francisca},
TITLE = {An Automatic Random Forest-OBIA Algorithm for Early Weed Mapping between and within Crop Rows Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {285},
URL = {https://www.mdpi.com/2072-4292/10/2/285},
ISSN = {2072-4292},
ABSTRACT = {Accurate and timely detection of weeds between and within crop rows in the early growth stage is considered one of the main challenges in site-specific weed management (SSWM). In this context, a robust and innovative automatic object-based image analysis (OBIA) algorithm was developed on Unmanned Aerial Vehicle (UAV) images to design early post-emergence prescription maps. This novel algorithm makes the major contribution. The OBIA algorithm combined Digital Surface Models (DSMs), orthomosaics and machine learning techniques (Random Forest, RF). OBIA-based plant heights were accurately estimated and used as a feature in the automatic sample selection by the RF classifier; this was the second research contribution. RF randomly selected a class balanced training set, obtained the optimum features values and classified the image, requiring no manual training, making this procedure time-efficient and more accurate, since it removes errors due to a subjective manual task. The ability to discriminate weeds was significantly affected by the imagery spatial resolution and weed density, making the use of higher spatial resolution images more suitable. Finally, prescription maps for in-season post-emergence SSWM were created based on the weed maps—the third research contribution—which could help farmers in decision-making to optimize crop management by rationalization of the herbicide application. The short time involved in the process (image capture and analysis) would allow timely weed control during critical periods, crucial for preventing yield loss.},
DOI = {10.3390/rs10020285}
}



@Article{s18020605,
AUTHOR = {Sandino, Juan and Gonzalez, Felipe and Mengersen, Kerrie and Gaston, Kevin J.},
TITLE = {UAVs and Machine Learning Revolutionising Invasive Grass and Vegetation Surveys in Remote Arid Lands},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {605},
URL = {https://www.mdpi.com/1424-8220/18/2/605},
ISSN = {1424-8220},
ABSTRACT = {The monitoring of invasive grasses and vegetation in remote areas is challenging, costly, and on the ground sometimes dangerous. Satellite and manned aircraft surveys can assist but their use may be limited due to the ground sampling resolution or cloud cover. Straightforward and accurate surveillance methods are needed to quantify rates of grass invasion, offer appropriate vegetation tracking reports, and apply optimal control methods. This paper presents a pipeline process to detect and generate a pixel-wise segmentation of invasive grasses, using buffel grass (Cenchrus ciliaris) and spinifex (Triodia sp.) as examples. The process integrates unmanned aerial vehicles (UAVs) also commonly known as drones, high-resolution red, green, blue colour model (RGB) cameras, and a data processing approach based on machine learning algorithms. The methods are illustrated with data acquired in Cape Range National Park, Western Australia (WA), Australia, orthorectified in Agisoft Photoscan Pro, and processed in Python programming language, scikit-learn, and eXtreme Gradient Boosting (XGBoost) libraries. In total, 342,626 samples were extracted from the obtained data set and labelled into six classes. Segmentation results provided an individual detection rate of 97% for buffel grass and 96% for spinifex, with a global multiclass pixel-wise detection rate of 97%. Obtained results were robust against illumination changes, object rotation, occlusion, background cluttering, and floral density variation.},
DOI = {10.3390/s18020605}
}



@Article{f9030102,
AUTHOR = {Puliti, Stefano and Talbot, Bruce and Astrup, Rasmus},
TITLE = {Tree-Stump Detection, Segmentation, Classification, and Measurement Using Unmanned Aerial Vehicle (UAV) Imagery},
JOURNAL = {Forests},
VOLUME = {9},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {102},
URL = {https://www.mdpi.com/1999-4907/9/3/102},
ISSN = {1999-4907},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are increasingly used as tools to perform a detailed assessment of post-harvest sites. One of the potential use of UAV photogrammetric data is to obtain tree-stump information that can then be used to support more precise decisions. This study developed and tested a methodology to automatically detect, segment, classify, and measure tree-stumps. Among the potential applications for single stump data, this study assessed the possibility (1) to detect and map root- and butt-rot on the stumps using a machine learning approach, and (2) directly measure or model tree stump diameter from the UAV data. The results revealed that the tree-stumps were detected with an overall accuracy of 68–80%, and once the stump was detected, the presence of root- and butt-rot was detected with an accuracy of 82.1%. Furthermore, the root mean square error of the UAV-derived measurements or model predictions for the stump diameter was 7.5 cm and 6.4 cm, respectively, and with the former systematically under predicting the diameter by 3.3 cm. The results of this study are promising and can lead to the development of more cost-effective and comprehensive UAV post-harvest surveys.},
DOI = {10.3390/f9030102}
}



@Article{s18040944,
AUTHOR = {Sandino, Juan and Pegg, Geoff and Gonzalez, Felipe and Smith, Grant},
TITLE = {Aerial Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral Sensors, and Artificial Intelligence},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {944},
URL = {https://www.mdpi.com/1424-8220/18/4/944},
ISSN = {1424-8220},
ABSTRACT = {The environmental and economic impacts of exotic fungal species on natural and plantation forests have been historically catastrophic. Recorded surveillance and control actions are challenging because they are costly, time-consuming, and hazardous in remote areas. Prolonged periods of testing and observation of site-based tests have limitations in verifying the rapid proliferation of exotic pathogens and deterioration rates in hosts. Recent remote sensing approaches have offered fast, broad-scale, and affordable surveys as well as additional indicators that can complement on-ground tests. This paper proposes a framework that consolidates site-based insights and remote sensing capabilities to detect and segment deteriorations by fungal pathogens in natural and plantation forests. This approach is illustrated with an experimentation case of myrtle rust (Austropuccinia psidii) on paperbark tea trees (Melaleuca quinquenervia) in New South Wales (NSW), Australia. The method integrates unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Imagery is acquired using a Headwall Nano-Hyperspec     ®     camera, orthorectified in Headwall SpectralView     ®    , and processed in Python programming language using eXtreme Gradient Boosting (XGBoost), Geospatial Data Abstraction Library (GDAL), and Scikit-learn third-party libraries. In total, 11,385 samples were extracted and labelled into five classes: two classes for deterioration status and three classes for background objects. Insights reveal individual detection rates of 95% for healthy trees, 97% for deteriorated trees, and a global multiclass detection rate of 97%. The methodology is versatile to be applied to additional datasets taken with different image sensors, and the processing of large datasets with freeware tools.},
DOI = {10.3390/s18040944}
}



@Article{rs10050805,
AUTHOR = {Moeckel, Thomas and Dayananda, Supriya and Nidamanuri, Rama Rao and Nautiyal, Sunil and Hanumaiah, Nagaraju and Buerkert, Andreas and Wachendorf, Michael},
TITLE = {Estimation of Vegetable Crop Parameter by Multi-temporal UAV-Borne Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {805},
URL = {https://www.mdpi.com/2072-4292/10/5/805},
ISSN = {2072-4292},
ABSTRACT = {3D point cloud analysis of imagery collected by unmanned aerial vehicles (UAV) has been shown to be a valuable tool for estimation of crop phenotypic traits, such as plant height, in several species. Spatial information about these phenotypic traits can be used to derive information about other important crop characteristics, like fresh biomass yield, which could not be derived directly from the point clouds. Previous approaches have often only considered single date measurements using a single point cloud derived metric for the respective trait. Furthermore, most of the studies focused on plant species with a homogenous canopy surface. The aim of this study was to assess the applicability of UAV imagery for capturing crop height information of three vegetables (crops eggplant, tomato, and cabbage) with a complex vegetation canopy surface during a complete crop growth cycle to infer biomass. Additionally, the effect of crop development stage on the relationship between estimated crop height and field measured crop height was examined. Our study was conducted in an experimental layout at the University of Agricultural Science in Bengaluru, India. For all the crops, the crop height and the biomass was measured at five dates during one crop growth cycle between February and May 2017 (average crop height was 42.5, 35.5, and 16.0 cm for eggplant, tomato, and cabbage). Using a structure from motion approach, a 3D point cloud was created for each crop and sampling date. In total, 14 crop height metrics were extracted from the point clouds. Machine learning methods were used to create prediction models for vegetable crop height. The study demonstrates that the monitoring of crop height using an UAV during an entire growing period results in detailed and precise estimates of crop height and biomass for all three crops (R2 ranging from 0.87 to 0.97, bias ranging from &minus;0.66 to 0.45 cm). The effect of crop development stage on the predicted crop height was found to be substantial (e.g., median deviation increased from 1% to 20% for eggplant) influencing the strength and consistency of the relationship between point cloud metrics and crop height estimates and, thus, should be further investigated. Altogether the results of the study demonstrate that point cloud generated from UAV-based RGB imagery can be used to effectively measure vegetable crop biomass in larger areas (relative error = 17.6%, 19.7%, and 15.2% for eggplant, tomato, and cabbage, respectively) with a similar accuracy as biomass prediction models based on measured crop height (relative error = 21.6, 18.8, and 15.2 for eggplant, tomato, and cabbage).},
DOI = {10.3390/rs10050805}
}



@Article{s18072026,
AUTHOR = {Parsons, Mark and Bratanov, Dmitry and Gaston, Kevin J. and Gonzalez, Felipe},
TITLE = {UAVs, Hyperspectral Remote Sensing, and Machine Learning Revolutionizing Reef Monitoring},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2026},
URL = {https://www.mdpi.com/1424-8220/18/7/2026},
ISSN = {1424-8220},
ABSTRACT = {Recent advances in unmanned aerial system (UAS) sensed imagery, sensor quality/size, and geospatial image processing can enable UASs to rapidly and continually monitor coral reefs, to determine the type of coral and signs of coral bleaching. This paper describes an unmanned aerial vehicle (UAV) remote sensing methodology to increase the efficiency and accuracy of existing surveillance practices. The methodology uses a UAV integrated with advanced digital hyperspectral, ultra HD colour (RGB) sensors, and machine learning algorithms. This paper describes the combination of airborne RGB and hyperspectral imagery with in-water survey data of several types in-water survey of coral under diverse levels of bleaching. The paper also describes the technology used, the sensors, the UAS, the flight operations, the processing workflow of the datasets, the methods for combining multiple airborne and in-water datasets, and finally presents relevant results of material classification. The development of the methodology for the collection and analysis of airborne hyperspectral and RGB imagery would provide coral reef researchers, other scientists, and UAV practitioners with reliable data collection protocols and faster processing techniques to achieve remote sensing objectives.},
DOI = {10.3390/s18072026}
}



@Article{ICEM18-05387,
AUTHOR = {Silva, Wilson Ricardo Leal da and Lucena, Diogo Schwerz de},
TITLE = {Concrete Cracks Detection Based on Deep Learning Image Classification},
JOURNAL = {Proceedings},
VOLUME = {2},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {489},
URL = {https://www.mdpi.com/2504-3900/2/8/489},
ISSN = {2504-3900},
ABSTRACT = {This work aims at developing a machine learning-based model to detect cracks on concrete surfaces. Such model is intended to increase the level of automation on concrete infrastructure inspection when combined to unmanned aerial vehicles (UAV). The developed crack detection model relies on a deep learning convolutional neural network (CNN) image classification algorithm. Provided a relatively heterogeneous dataset, the use of deep learning enables the development of a concrete cracks detection system that can account for several conditions, e.g., different light, surface finish and humidity that a concrete surface might exhibit. These conditions are a limiting factor when working with computer vision systems based on conventional digital image processing methods. For this work, a dataset with 3500 images of concrete surfaces balanced between images with and without cracks was used. This dataset was divided into training and testing data at an 80/20 ratio. Since our dataset is rather small to enable a robust training of a complete deep learning model, a transfer-learning methodology was applied; in particular, the open-source model VGG16 was used as basis for the development of the model. The influence of the model’s parameters such as learning rate, number of nodes in the last fully connected layer and training dataset size were investigated. In each experiment, the model’s accuracy was recorded to identify the best result. For the dataset used in this work, the best experiment yielded a model with accuracy of 92.27%, showcasing the potential of using deep learning for concrete crack detection.},
DOI = {10.3390/ICEM18-05387}
}



@Article{s18072244,
AUTHOR = {De Oliveira, Diulhio Candido and Wehrmeister, Marco Aurelio},
TITLE = {Using Deep Learning and Low-Cost RGB and Thermal Cameras to Detect Pedestrians in Aerial Images Captured by Multirotor UAV},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2244},
URL = {https://www.mdpi.com/1424-8220/18/7/2244},
ISSN = {1424-8220},
ABSTRACT = {The use of Unmanned Aerial Vehicles (UAV) has been increasing over the last few years in many sorts of applications due mainly to the decreasing cost of this technology. One can see the use of the UAV in several civilian applications such as surveillance and search and rescue. Automatic detection of pedestrians in aerial images is a challenging task. The computing vision system must deal with many sources of variability in the aerial images captured with the UAV, e.g., low-resolution images of pedestrians, images captured at distinct angles due to the degrees of freedom that a UAV can move, the camera platform possibly experiencing some instability while the UAV flies, among others. In this work, we created and evaluated different implementations of Pattern Recognition Systems (PRS) aiming at the automatic detection of pedestrians in aerial images captured with multirotor UAV. The main goal is to assess the feasibility and suitability of distinct PRS implementations running on top of low-cost computing platforms, e.g., single-board computers such as the Raspberry Pi or regular laptops without a GPU. For that, we used four machine learning techniques in the feature extraction and classification steps, namely Haar cascade, LBP cascade, HOG + SVM and Convolutional Neural Networks (CNN). In order to improve the system performance (especially the processing time) and also to decrease the rate of false alarms, we applied the Saliency Map (SM) and Thermal Image Processing (TIP) within the segmentation and detection steps of the PRS. The classification results show the CNN to be the best technique with 99.7% accuracy, followed by HOG + SVM with 92.3%. In situations of partial occlusion, the CNN showed 71.1% sensitivity, which can be considered a good result in comparison with the current state-of-the-art, since part of the original image data is missing. As demonstrated in the experiments, by combining TIP with CNN, the PRS can process more than two frames per second (fps), whereas the PRS that combines TIP with HOG + SVM was able to process 100 fps. It is important to mention that our experiments show that a trade-off analysis must be performed during the design of a pedestrian detection PRS. The faster implementations lead to a decrease in the PRS accuracy. For instance, by using HOG + SVM with TIP, the PRS presented the best performance results, but the obtained accuracy was 35 percentage points lower than the CNN. The obtained results indicate that the best detection technique (i.e., the CNN) requires more computational resources to decrease the PRS computation time. Therefore, this work shows and discusses the pros/cons of each technique and trade-off situations, and hence, one can use such an analysis to improve and tailor the design of a PRS to detect pedestrians in aerial images.},
DOI = {10.3390/s18072244}
}



@Article{s18124464,
AUTHOR = {Cao, Feng and Liu, Fei and Guo, Han and Kong, Wenwen and Zhang, Chu and He, Yong},
TITLE = {Fast Detection of Sclerotinia Sclerotiorum on Oilseed Rape Leaves Using Low-Altitude Remote Sensing Technology},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {12},
ARTICLE-NUMBER = {4464},
URL = {https://www.mdpi.com/1424-8220/18/12/4464},
ISSN = {1424-8220},
ABSTRACT = {Sclerotinia sclerotiorum, one of the major diseases infecting oilseed rape leaves, has seriously affected crop yield and quality. In this study, an indoor unmanned aerial vehicle (UAV) low-altitude remote sensing simulation platform was built for disease detection. Thermal, multispectral and RGB images were acquired before and after being artificially inoculated with Sclerotinia sclerotiorum on oilseed rape leaves. New image registration and fusion methods based on scale-invariant feature transform (SIFT) were presented to construct a fused database using multi-model images. The changes of temperature distribution in different sections of infected areas were analyzed by processing thermal images, the maximum temperature difference (MTD) on a single leaf reached 1.7 degrees Celsius 24 h after infection. Four machine learning models were established using thermal images and fused images respectively, including support vector machine (SVM), random forest (RF), K-nearest neighbor (KNN) and na&iuml;ve Bayes (NB). The results demonstrated that the classification accuracy was improved by 11.3% after image fusion, and the SVM model obtained a classification accuracy of 90.0% on the task of classifying disease severity. The overall results indicated the UAV low-altitude remote sensing simulation platform equipped with multi-sensors could be used to early detect Sclerotinia sclerotiorum on oilseed rape leaves.},
DOI = {10.3390/s18124464}
}



@Article{s19020313,
AUTHOR = {Gao, Pengbo and Zhang, Yan and Zhang, Linhuan and Noguchi, Ryozo and Ahamed, Tofael},
TITLE = {Development of a Recognition System for Spraying Areas from Unmanned Aerial Vehicles Using a Machine Learning Approach},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {313},
URL = {https://www.mdpi.com/1424-8220/19/2/313},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicle (UAV)-based spraying systems have recently become important for the precision application of pesticides, using machine learning approaches. Therefore, the objective of this research was to develop a machine learning system that has the advantages of high computational speed and good accuracy for recognizing spray and non-spray areas for UAV-based sprayers. A machine learning system was developed by using the mutual subspace method (MSM) for images collected from a UAV. Two target lands: agricultural croplands and orchard areas, were considered in building two classifiers for distinguishing spray and non-spray areas. The field experiments were conducted in target areas to train and test the system by using a commercial UAV (DJI Phantom 3 Pro) with an onboard 4K camera. The images were collected from low (5 m) and high (15 m) altitudes for croplands and orchards, respectively. The recognition system was divided into offline and online systems. In the offline recognition system, 74.4% accuracy was obtained for the classifiers in recognizing spray and non-spray areas for croplands. In the case of orchards, the average classifier recognition accuracy of spray and non-spray areas was 77%. On the other hand, the online recognition system performance had an average accuracy of 65.1% for croplands, and 75.1% for orchards. The computational time for the online recognition system was minimal, with an average of 0.0031 s for classifier recognition. The developed machine learning system had an average recognition accuracy of 70%, which can be implemented in an autonomous UAV spray system for recognizing spray and non-spray areas for real-time applications.},
DOI = {10.3390/s19020313}
}



@Article{app9040643,
AUTHOR = {Kwak, Geun-Ho and Park, No-Wook},
TITLE = {Impact of Texture Information on Crop Classification with Machine Learning and UAV Images},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {643},
URL = {https://www.mdpi.com/2076-3417/9/4/643},
ISSN = {2076-3417},
ABSTRACT = {Unmanned aerial vehicle (UAV) images that can provide thematic information at much higher spatial and temporal resolutions than satellite images have great potential in crop classification. Due to the ultra-high spatial resolution of UAV images, spatial contextual information such as texture is often used for crop classification. From a data availability viewpoint, it is not always possible to acquire time-series UAV images due to limited accessibility to the study area. Thus, it is necessary to improve classification performance for situations when a single or minimum number of UAV images are available for crop classification. In this study, we investigate the potential of gray-level co-occurrence matrix (GLCM)-based texture information for crop classification with time-series UAV images and machine learning classifiers including random forest and support vector machine. In particular, the impact of combining texture and spectral information on the classification performance is evaluated for cases that use only one UAV image or multi-temporal images as input. A case study of crop classification in Anbandegi of Korea was conducted for the above comparisons. The best classification accuracy was achieved when multi-temporal UAV images which can fully account for the growth cycles of crops were combined with GLCM-based texture features. However, the impact of the utilization of texture information was not significant. In contrast, when one August UAV image was used for crop classification, the utilization of texture information significantly affected the classification performance. Classification using texture features extracted from GLCM with larger kernel size significantly improved classification accuracy, an improvement of 7.72%p in overall accuracy for the support vector machine classifier, compared with classification based solely on spectral information. These results indicate the usefulness of texture information for classification of ultra-high-spatial-resolution UAV images, particularly when acquisition of time-series UAV images is difficult and only one UAV image is used for crop classification.},
DOI = {10.3390/app9040643}
}



@Article{app9040648,
AUTHOR = {Giernacki, Wojciech},
TITLE = {Iterative Learning Method for In-Flight Auto-Tuning of UAV Controllers Based on Basic Sensory Information},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {648},
URL = {https://www.mdpi.com/2076-3417/9/4/648},
ISSN = {2076-3417},
ABSTRACT = {With an increasing number of multirotor unmanned aerial vehicles (UAVs), solutions supporting the improvement in their precision of operation and safety of autonomous flights are gaining importance. They are particularly crucial in transportation tasks, where control systems are required to provide a stable and controllable flight in various environmental conditions, especially after changing the total mass of the UAV (by adding extra load). In the paper, the problem of using only available basic sensory information for fast, locally best, iterative real-time auto-tuning of parameters of fixed-gain altitude controllers is considered. The machine learning method proposed for this purpose is based on a modified zero-order optimization algorithm (golden-search algorithm) and bootstrapping technique. It has been validated in numerous simulations and real-world experiments in terms of its effectiveness in such aspects as: the impact of environmental disturbances (wind gusts); flight with change in mass; and change of sensory information sources in the auto-tuning procedure. The main advantage of the proposed method is that for the trajectory primitives repeatedly followed by an UAV (for programmed controller gains), the method effectively minimizes the selected performance index (cost function). Such a performance index might, e.g., express indirect requirements about tracking quality and energy expenditure. In the paper, a comprehensive description of the method, as well as a wide discussion of the results obtained from experiments conducted in the AeroLab for a low-cost UAV (Bebop 2), are included. The results have confirmed high efficiency of the method at the expected, low computational complexity.},
DOI = {10.3390/app9040648}
}



@Article{rs11040410,
AUTHOR = {Ampatzidis, Yiannis and Partel, Victor},
TITLE = {UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {410},
URL = {https://www.mdpi.com/2072-4292/11/4/410},
ISSN = {2072-4292},
ABSTRACT = {Traditional plant breeding evaluation methods are time-consuming, labor-intensive, and costly. Accurate and rapid phenotypic trait data acquisition and analysis can improve genomic selection and accelerate cultivar development. In this work, a technique for data acquisition and image processing was developed utilizing small unmanned aerial vehicles (UAVs), multispectral imaging, and deep learning convolutional neural networks to evaluate phenotypic characteristics on citrus crops. This low-cost and automated high-throughput phenotyping technique utilizes artificial intelligence (AI) and machine learning (ML) to: (i) detect, count, and geolocate trees and tree gaps; (ii) categorize trees based on their canopy size; (iii) develop individual tree health indices; and (iv) evaluate citrus varieties and rootstocks. The proposed remote sensing technique was able to detect and count citrus trees in a grove of 4,931 trees, with precision and recall of 99.9% and 99.7%, respectively, estimate their canopy size with overall accuracy of 85.5%, and detect, count, and geolocate tree gaps with a precision and recall of 100% and 94.6%, respectively. This UAV-based technique provides a consistent, more direct, cost-effective, and rapid method to evaluate phenotypic characteristics of citrus varieties and rootstocks.},
DOI = {10.3390/rs11040410}
}



@Article{rs11060643,
AUTHOR = {Safonova, Anastasiia and Tabik, Siham and Alcaraz-Segura, Domingo and Rubtsov, Alexey and Maglinets, Yuriy and Herrera, Francisco},
TITLE = {Detection of Fir Trees (Abies sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {643},
URL = {https://www.mdpi.com/2072-4292/11/6/643},
ISSN = {2072-4292},
ABSTRACT = {Invasion of the Polygraphus proximus Blandford bark beetle causes catastrophic damage to forests with firs (Abies sibirica Ledeb) in Russia, especially in Central Siberia. Determining tree damage stage based on the shape, texture and colour of tree crown in unmanned aerial vehicle (UAV) images could help to assess forest health in a faster and cheaper way. However, this task is challenging since (i) fir trees at different damage stages coexist and overlap in the canopy, (ii) the distribution of fir trees in nature is irregular and hence distinguishing between different crowns is hard, even for the human eye. Motivated by the latest advances in computer vision and machine learning, this work proposes a two-stage solution: In a first stage, we built a detection strategy that finds the regions of the input UAV image that are more likely to contain a crown, in the second stage, we developed a new convolutional neural network (CNN) architecture that predicts the fir tree damage stage in each candidate region. Our experiments show that the proposed approach shows satisfactory results on UAV Red, Green, Blue (RGB) images of forest areas in the state nature reserve “Stolby” (Krasnoyarsk, Russia).},
DOI = {10.3390/rs11060643}
}



@Article{app9061128,
AUTHOR = {Li, Yundong and Hu, Wei and Dong, Han and Zhang, Xueyan},
TITLE = {Building Damage Detection from Post-Event Aerial Imagery Using Single Shot Multibox Detector},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {1128},
URL = {https://www.mdpi.com/2076-3417/9/6/1128},
ISSN = {2076-3417},
ABSTRACT = {Using aerial cameras, satellite remote sensing or unmanned aerial vehicles (UAV) equipped with cameras can facilitate search and rescue tasks after disasters. The traditional manual interpretation of huge aerial images is inefficient and could be replaced by machine learning-based methods combined with image processing techniques. Given the development of machine learning, researchers find that convolutional neural networks can effectively extract features from images. Some target detection methods based on deep learning, such as the single-shot multibox detector (SSD) algorithm, can achieve better results than traditional methods. However, the impressive performance of machine learning-based methods results from the numerous labeled samples. Given the complexity of post-disaster scenarios, obtaining many samples in the aftermath of disasters is difficult. To address this issue, a damaged building assessment method using SSD with pretraining and data augmentation is proposed in the current study and highlights the following aspects. (1) Objects can be detected and classified into undamaged buildings, damaged buildings, and ruins. (2) A convolution auto-encoder (CAE) that consists of VGG16 is constructed and trained using unlabeled post-disaster images. As a transfer learning strategy, the weights of the SSD model are initialized using the weights of the CAE counterpart. (3) Data augmentation strategies, such as image mirroring, rotation, Gaussian blur, and Gaussian noise processing, are utilized to augment the training data set. As a case study, aerial images of Hurricane Sandy in 2012 were maximized to validate the proposed method&rsquo;s effectiveness. Experiments show that the pretraining strategy can improve of 10% in terms of overall accuracy compared with the SSD trained from scratch. These experiments also demonstrate that using data augmentation strategies can improve mAP and mF1 by 72% and 20%, respectively. Finally, the experiment is further verified by another dataset of Hurricane Irma, and it is concluded that the paper method is feasible.},
DOI = {10.3390/app9061128}
}



@Article{rs11060733,
AUTHOR = {Windrim, Lloyd and Bryson, Mitch and McLean, Michael and Randle, Jeremy and Stone, Christine},
TITLE = {Automated Mapping of Woody Debris over Harvested Forest Plantations Using UAVs, High-Resolution Imagery, and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/11/6/733},
ISSN = {2072-4292},
ABSTRACT = {Surveying of woody debris left over from harvesting operations on managed forests is an important step in monitoring site quality, managing the extraction of residues and reconciling differences in pre-harvest inventories and actual timber yields. Traditional methods for post-harvest survey involving manual assessment of debris on the ground over small sample plots are labor-intensive, time-consuming, and do not scale well to heterogeneous landscapes. In this paper, we propose and evaluate new automated methods for the collection and interpretation of high-resolution, Unmanned Aerial Vehicle (UAV)-borne imagery over post-harvested forests for estimating quantities of fine and coarse woody debris. Using high-resolution, geo-registered color mosaics generated from UAV-borne images, we develop manual and automated processing methods for detecting, segmenting and counting both fine and coarse woody debris, including tree stumps, exploiting state-of-the-art machine learning and image processing techniques. Results are presented using imagery over a post-harvested compartment in a Pinus radiata plantation and demonstrate the capacity for both manual image annotations and automated image processing to accurately detect and quantify coarse woody debris and stumps left over after harvest, providing a cost-effective and scalable survey method for forest managers.},
DOI = {10.3390/rs11060733}
}



@Article{rs11091018,
AUTHOR = {Li, Zhen and Zan, Qijie and Yang, Qiong and Zhu, Dehuang and Chen, Youjun and Yu, Shixiao},
TITLE = {Remote Estimation of Mangrove Aboveground Carbon Stock at the Species Level Using a Low-Cost Unmanned Aerial Vehicle System},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1018},
URL = {https://www.mdpi.com/2072-4292/11/9/1018},
ISSN = {2072-4292},
ABSTRACT = {There is ongoing interest in developing remote sensing technology to map and monitor the spatial distribution and carbon stock of mangrove forests. Previous research has demonstrated that the relationship between remote sensing derived parameters and aboveground carbon (AGC) stock varies for different species types. However, the coarse spatial resolution of satellite images has restricted the estimated AGC accuracy, especially at the individual species level. Recently, the availability of unmanned aerial vehicles (UAVs) has provided an operationally efficient approach to map the distribution of species and accurately estimate AGC stock at a fine scale in mangrove areas. In this study, we estimated mangrove AGC in the core area of northern Shenzhen Bay, South China, using four kinds of variables, including species type, canopy height metrics, vegetation indices, and texture features, derived from a low-cost UAV system. Three machine-learning algorithm models, including Random Forest (RF), Support Vector Regression (SVR), and Artificial Neural Network (ANN), were compared in this study, where a 10-fold cross-validation was used to evaluate each model&rsquo;s effectiveness. The results showed that a model that used all four type of variables, which were based on the RF algorithm, provided better AGC estimates (R2 = 0.81, relative RMSE (rRMSE) = 0.20, relative MAE (rMAE) = 0.14). The average predicted AGC from this model was 93.0 &plusmn; 24.3 Mg C ha&minus;1, and the total estimated AGC was 7903.2 Mg for the mangrove forests. The species-based model had better performance than the considered canopy-height-based model for AGC estimation, and mangrove species was the most important variable among all the considered input variables; the mean height (Hmean) the second most important variable. Additionally, the RF algorithms showed better performance in terms of mangrove AGC estimation than the SVR and ANN algorithms. Overall, a low-cost UAV system with a digital camera has the potential to enable satisfactory predictions of AGC in areas of homogenous mangrove forests.},
DOI = {10.3390/rs11091018}
}



@Article{rs11101238,
AUTHOR = {De Luca, Giandomenico and N. Silva, João M. and Cerasoli, Sofia and Araújo, João and Campos, José and Di Fazio, Salvatore and Modica, Giuseppe},
TITLE = {Object-Based Land Cover Classification of Cork Oak Woodlands using UAV Imagery and Orfeo ToolBox},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1238},
URL = {https://www.mdpi.com/2072-4292/11/10/1238},
ISSN = {2072-4292},
ABSTRACT = {This paper investigates the reliability of free and open-source algorithms used in the geographical object-based image classification (GEOBIA) of very high resolution (VHR) imagery surveyed by unmanned aerial vehicles (UAVs). UAV surveys were carried out in a cork oak woodland located in central Portugal at two different periods of the year (spring and summer). Segmentation and classification algorithms were implemented in the Orfeo ToolBox (OTB) configured in the QGIS environment for the GEOBIA process. Image segmentation was carried out using the Large-Scale Mean-Shift (LSMS) algorithm, while classification was performed by the means of two supervised classifiers, random forest (RF) and support vector machines (SVM), both of which are based on a machine learning approach. The original, informative content of the surveyed imagery, consisting of three radiometric bands (red, green, and NIR), was combined to obtain the normalized difference vegetation index (NDVI) and the digital surface model (DSM). The adopted methodology resulted in a classification with higher accuracy that is suitable for a structurally complex Mediterranean forest ecosystem such as cork oak woodlands, which are characterized by the presence of shrubs and herbs in the understory as well as tree shadows. To improve segmentation, which significantly affects the subsequent classification phase, several tests were performed using different values of the range radius and minimum region size parameters. Moreover, the consistent selection of training polygons proved to be critical to improving the results of both the RF and SVM classifiers. For both spring and summer imagery, the validation of the obtained results shows a very high accuracy level for both the SVM and RF classifiers, with kappa coefficient values ranging from 0.928 to 0.973 for RF and from 0.847 to 0.935 for SVM. Furthermore, the land cover class with the highest accuracy for both classifiers and for both flights was cork oak, which occupies the largest part of the study area. This study shows the reliability of fixed-wing UAV imagery for forest monitoring. The study also evidences the importance of planning UAV flights at solar noon to significantly reduce the shadows of trees in the obtained imagery, which is critical for classifying open forest ecosystems such as cork oak woodlands.},
DOI = {10.3390/rs11101238}
}



@Article{rs11111261,
AUTHOR = {Niu, Yaxiao and Zhang, Liyuan and Zhang, Huihui and Han, Wenting and Peng, Xingshuo},
TITLE = {Estimating Above-Ground Biomass of Maize Using Features Derived from UAV-Based RGB Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1261},
URL = {https://www.mdpi.com/2072-4292/11/11/1261},
ISSN = {2072-4292},
ABSTRACT = {The rapid, accurate, and economical estimation of crop above-ground biomass at the farm scale is crucial for precision agricultural management. The unmanned aerial vehicle (UAV) remote-sensing system has a great application potential with the ability to obtain remote-sensing imagery with high temporal-spatial resolution. To verify the application potential of consumer-grade UAV RGB imagery in estimating maize above-ground biomass, vegetation indices and plant height derived from UAV RGB imagery were adopted. To obtain a more accurate observation, plant height was directly derived from UAV RGB point clouds. To search the optimal estimation method, the estimation performances of the models based on vegetation indices alone, based on plant height alone, and based on both vegetation indices and plant height were compared. The results showed that plant height directly derived from UAV RGB point clouds had a high correlation with ground-truth data with an R2 value of 0.90 and an RMSE value of 0.12 m. The above-ground biomass exponential regression models based on plant height alone had higher correlations for both fresh and dry above-ground biomass with R2 values of 0.77 and 0.76, respectively, compared to the linear regression model (both R2 values were 0.59). The vegetation indices derived from UAV RGB imagery had great potential to estimate maize above-ground biomass with R2 values ranging from 0.63 to 0.73. When estimating the above-ground biomass of maize by using multivariable linear regression based on vegetation indices, a higher correlation was obtained with an R2 value of 0.82. There was no significant improvement of the estimation performance when plant height derived from UAV RGB imagery was added into the multivariable linear regression model based on vegetation indices. When estimating crop above-ground biomass based on UAV RGB remote-sensing system alone, looking for optimized vegetation indices and establishing estimation models with high performance based on advanced algorithms (e.g., machine learning technology) may be a better way.},
DOI = {10.3390/rs11111261}
}



@Article{rs11111380,
AUTHOR = {Abeysinghe, Tharindu and Simic Milas, Anita and Arend, Kristin and Hohman, Breann and Reil, Patrick and Gregory, Andrew and Vázquez-Ortega, Angélica},
TITLE = {Mapping Invasive Phragmites australis in the Old Woman Creek Estuary Using UAV Remote Sensing and Machine Learning Classifiers},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1380},
URL = {https://www.mdpi.com/2072-4292/11/11/1380},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAV) are increasingly used for spatiotemporal monitoring of invasive plants in coastal wetlands. Early identification of invasive species is necessary in planning, restoring, and managing wetlands. This study assessed the effectiveness of UAV technology to identify invasive Phragmites australis in the Old Woman Creek (OWC) estuary using machine learning (ML) algorithms: Neural network (NN), support vector machine (SVM), and k-nearest neighbor (kNN). The ML algorithms were compared with the parametric maximum likelihood classifier (MLC) using pixel- and object-based methods. Pixel-based NN was identified as the best classifier with an overall accuracy of 94.80% and the lowest error of omission of 1.59%, the outcome desirable for effective eradication of Phragmites. The results were reached combining Sequoia multispectral imagery (green, red, red edge, and near-infrared bands) combined with the canopy height model (CHM) acquired in the mid-growing season and normalized difference vegetation index (NDVI) acquired later in the season. The sensitivity analysis, using various vegetation indices, image texture, CHM, and principal components (PC), demonstrated the impact of various feature layers on the classifiers. The study emphasizes the necessity of a suitable sampling and cross-validation methods, as well as the importance of optimum classification parameters.},
DOI = {10.3390/rs11111380}
}



@Article{app9112389,
AUTHOR = {Zhou, Chengquan and Ye, Hongbao and Xu, Zhifu and Hu, Jun and Shi, Xiaoyan and Hua, Shan and Yue, Jibo and Yang, Guijun},
TITLE = {Estimating Maize-Leaf Coverage in Field Conditions by Applying a Machine Learning Algorithm to UAV Remote Sensing Images},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2389},
URL = {https://www.mdpi.com/2076-3417/9/11/2389},
ISSN = {2076-3417},
ABSTRACT = {Leaf coverage is an indicator of plant growth rate and predicted yield, and thus it is crucial to plant-breeding research. Robust image segmentation of leaf coverage from remote-sensing images acquired by unmanned aerial vehicles (UAVs) in varying environments can be directly used for large-scale coverage estimation, and is a key component of high-throughput field phenotyping. We thus propose an image-segmentation method based on machine learning to extract relatively accurate coverage information from the orthophoto generated after preprocessing. The image analysis pipeline, including dataset augmenting, removing background, classifier training and noise reduction, generates a set of binary masks to obtain leaf coverage from the image. We compare the proposed method with three conventional methods (Hue-Saturation-Value, edge-detection-based algorithm, random forest) and a frontier deep-learning method called DeepLabv3+. The proposed method improves indicators such as Qseg, Sr, Es and mIOU by 15% to 30%. The experimental results show that this approach is less limited by radiation conditions, and that the protocol can easily be implemented for extensive sampling at low cost. As a result, with the proposed method, we recommend using red-green-blue (RGB)-based technology in addition to conventional equipment for acquiring the leaf coverage of agricultural crops.},
DOI = {10.3390/app9112389}
}



@Article{rs11131534,
AUTHOR = {Park, John Y. and Muller-Landau, Helene C. and Lichstein, Jeremy W. and Rifai, Sami W. and Dandois, Jonathan P. and Bohlman, Stephanie A.},
TITLE = {Quantifying Leaf Phenology of Individual Trees and Species in a Tropical Forest Using Unmanned Aerial Vehicle (UAV) Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1534},
URL = {https://www.mdpi.com/2072-4292/11/13/1534},
ISSN = {2072-4292},
ABSTRACT = {Tropical forests exhibit complex but poorly understood patterns of leaf phenology. Understanding species- and individual-level phenological patterns in tropical forests requires datasets covering large numbers of trees, which can be provided by Unmanned Aerial Vehicles (UAVs). In this paper, we test a workflow combining high-resolution RGB images (7 cm/pixel) acquired from UAVs with a machine learning algorithm to monitor tree and species leaf phenology in a tropical forest in Panama. We acquired images for 34 flight dates over a 12-month period. Crown boundaries were digitized in images and linked with forest inventory data to identify species. We evaluated predictions of leaf cover from different models that included up to 14 image features extracted for each crown on each date. The models were trained and tested with visual estimates of leaf cover from 2422 images from 85 crowns belonging to eight species spanning a range of phenological patterns. The best-performing model included both standard color metrics, as well as texture metrics that quantify within-crown variation, with r2 of 0.84 and mean absolute error (MAE) of 7.8% in 10-fold cross-validation. In contrast, the model based only on the widely-used Green Chromatic Coordinate (GCC) index performed relatively poorly (r2 = 0.52, MAE = 13.6%). These results highlight the utility of texture features for image analysis of tropical forest canopies, where illumination changes may diminish the utility of color indices, such as GCC. The algorithm successfully predicted both individual-tree and species patterns, with mean r2 of 0.82 and 0.89 and mean MAE of 8.1% and 6.0% for individual- and species-level analyses, respectively. Our study is the first to develop and test methods for landscape-scale UAV monitoring of individual trees and species in diverse tropical forests. Our analyses revealed undescribed patterns of high intraspecific variation and complex leaf cover changes for some species.},
DOI = {10.3390/rs11131534}
}



@Article{rs11151812,
AUTHOR = {Dash, Jonathan P. and Watt, Michael S. and Paul, Thomas S. H. and Morgenroth, Justin and Pearse, Grant D.},
TITLE = {Early Detection of Invasive Exotic Trees Using UAV and Manned Aircraft Multispectral and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1812},
URL = {https://www.mdpi.com/2072-4292/11/15/1812},
ISSN = {2072-4292},
ABSTRACT = {Exotic conifers can provide significant ecosystem services, but in some environments, they have become invasive and threaten indigenous ecosystems. In New Zealand, this phenomenon is of considerable concern as the area occupied by invasive exotic trees is large and increasing rapidly. Remote sensing methods offer a potential means of identifying and monitoring land infested by these trees, enabling managers to efficiently allocate resources for their control. In this study, we sought to develop methods for remote detection of exotic invasive trees, namely Pinus sylvestris and P. ponderosa. Critically, the study aimed to detect these species prior to the onset of maturity and coning as this is important for preventing further spread. In the study environment in New Zealand&rsquo;s South Island, these species reach maturity and begin bearing cones at a young age. As such, detection of these smaller individuals requires specialist methods and very high-resolution remote sensing data. We examined the efficacy of classifiers developed using two machine learning algorithms with multispectral and laser scanning data collected from two platforms&mdash;manned aircraft and unmanned aerial vehicles (UAV). The study focused on a localized conifer invasion originating from a multi-species pine shelter belt in a grassland environment. This environment provided a useful means of defining the detection thresholds of the methods and technologies employed. An extensive field dataset including over 17,000 trees (height range = 1 cm to 476 cm) was used as an independent validation dataset for the detection methods developed. We found that data from both platforms and using both logistic regression and random forests for classification provided highly accurate (kappa     &lt; 0.996    ) detection of invasive conifers. Our analysis showed that the data from both UAV and manned aircraft was useful for detecting trees down to 1 m in height and therefore shorter than 99.3% of the coning individuals in the study dataset. We also explored the relative contribution of both multispectral and airborne laser scanning (ALS) data in the detection of invasive trees through fitting classification models with different combinations of predictors and found that the most useful models included data from both sensors. However, the combination of ALS and multispectral data did not significantly improve classification accuracy. We believe that this was due to the simplistic vegetation and terrain structure in the study site that resulted in uncomplicated separability of invasive conifers from other vegetation. This study provides valuable new knowledge of the efficacy of detecting invasive conifers prior to the onset of coning using high-resolution data from UAV and manned aircraft. This will be an important tool in managing the spread of these important invasive plants.},
DOI = {10.3390/rs11151812}
}



@Article{ECRS-3-06184,
AUTHOR = {KAPLAN, Gordana and AVDAN, Ugur},
TITLE = {Evaluating Sentinel-2 Red-Edge Bands for Wetland Classification},
JOURNAL = {Proceedings},
VOLUME = {18},
YEAR = {2019},
NUMBER = {1},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2504-3900/18/1/12},
ISSN = {2504-3900},
ABSTRACT = {Due to the high spatial heterogeneity and temporal variability, wetlands are one of the most difficult ecosystems to observe using remote sensing data. With the additional Sentinel-2 vegetation red-edge bands, an improvement of the vegetated classes classification is expected. In order to investigate the influence of the Sentinel-2 red-edge bands, in this paper we evaluate two classification scenarios over wetland classes. The first scenario excludes the red-edge bands, while in the second scenario all red-edge bands are included in the classification dataset where two different wetland classes&mdash;intensive vegetated wetland classes such as swamps and partially decayed vegetated wetland areas such as bogs&mdash;are classified using a support vector machine (SVM) learning classifier. The classes are defined using high-resolution images from an Unmanned Aerial Vehicle (UAV) obtained on the same date with the passing of the Sentinel-2 satellite over the study area. As expected, the results show a significant improvement of the intensive vegetated wetlands, with more than 30% in both user and producer accuracy, while no significant changes are noted in the partially decayed vegetated wetlands. For future studies, we recommend evaluating the influence of the Sentinel radar data over wetland areas.},
DOI = {10.3390/ECRS-3-06184}
}



@Article{rs11172050,
AUTHOR = {Revill, Andrew and Florence, Anna and MacArthur, Alasdair and Hoad, Stephen P. and Rees, Robert M. and Williams, Mathew},
TITLE = {The Value of Sentinel-2 Spectral Bands for the Assessment of Winter Wheat Growth and Development},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2050},
URL = {https://www.mdpi.com/2072-4292/11/17/2050},
ISSN = {2072-4292},
ABSTRACT = {Leaf Area Index (LAI) and chlorophyll content are strongly related to plant development and productivity. Spatial and temporal estimates of these variables are essential for efficient and precise crop management. The availability of open-access data from the European Space Agency’s (ESA) Sentinel-2 satellite—delivering global coverage with an average 5-day revisit frequency at a spatial resolution of up to 10 metres—could provide estimates of these variables at unprecedented (i.e., sub-field) resolution. Using synthetic data, past research has demonstrated the potential of Sentinel-2 for estimating crop variables. Nonetheless, research involving a robust analysis of the Sentinel-2 bands for supporting agricultural applications is limited. We evaluated the potential of Sentinel-2 data for retrieving winter wheat LAI, leaf chlorophyll content (LCC) and canopy chlorophyll content (CCC). In coordination with destructive and non-destructive ground measurements, we acquired multispectral data from an Unmanned Aerial Vehicle (UAV)-mounted sensor measuring key Sentinel-2 spectral bands (443 to 865 nm). We applied Gaussian processes regression (GPR) machine learning to determine the most informative Sentinel-2 bands for retrieving each of the variables. We further evaluated the GPR model performance when propagating observation uncertainty. When applying the best-performing GPR models without propagating uncertainty, the retrievals had a high agreement with ground measurements—the mean R2 and normalised root-mean-square error (NRMSE) were 0.89 and 8.8%, respectively. When propagating uncertainty, the mean R2 and NRMSE were 0.82 and 11.9%, respectively. When accounting for measurement uncertainty in the estimation of LAI and CCC, the number of most informative Sentinel-2 bands was reduced from four to only two—the red-edge (705 nm) and near-infrared (865 nm) bands. This research demonstrates the value of the Sentinel-2 spectral characteristics for retrieving critical variables that can support more sustainable crop management practices.},
DOI = {10.3390/rs11172050}
}



@Article{rs11182075,
AUTHOR = {Zhou, Jing and Yungbluth, Dennis and Vong, Chin Nee and Scaboo, Andrew and Zhou, Jianfeng},
TITLE = {Estimation of the Maturity Date of Soybean Breeding Lines Using UAV-Based Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2075},
URL = {https://www.mdpi.com/2072-4292/11/18/2075},
ISSN = {2072-4292},
ABSTRACT = {Physiological maturity date is a critical parameter for the selection of breeding lines in soybean breeding programs. The conventional method to estimate the maturity dates of breeding lines uses visual ratings based on pod senescence by experts, which is subjective by human estimation, labor-intensive and time-consuming. Unmanned aerial vehicle (UAV)-based phenotyping systems provide a high-throughput and powerful tool of capturing crop traits using remote sensing, image processing and machine learning technologies. The goal of this study was to investigate the potential of predicting maturity dates of soybean breeding lines using UAV-based multispectral imagery. Maturity dates of 326 soybean breeding lines were taken using visual ratings from the beginning maturity stage (R7) to full maturity stage (R8), and the aerial multispectral images were taken during this period on 27 August, 14 September and 27 September, 2018. One hundred and thirty features were extracted from the five-band multispectral images. The maturity dates of the soybean lines were predicted and evaluated using partial least square regression (PLSR) models with 10-fold cross-validation. Twenty image features with importance to the estimation were selected and their changing rates between each two of the data collection days were calculated. The best prediction (R2 = 0.81, RMSE = 1.4 days) was made by the PLSR model using image features taken on 14 September and their changing rates between 14 September and 27 September with five components, leading to the conclusion that the UAV-based multispectral imagery is promising and practical in estimating maturity dates of soybean breeding lines.},
DOI = {10.3390/rs11182075}
}



@Article{f10090815,
AUTHOR = {Zou, Xiaodan and Liang, Anjie and Wu, Bizhi and Su, Jun and Zheng, Renhua and Li, Jian},
TITLE = {UAV-Based High-Throughput Approach for Fast Growing Cunninghamia lanceolata (Lamb.) Cultivar Screening by Machine Learning},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {815},
URL = {https://www.mdpi.com/1999-4907/10/9/815},
ISSN = {1999-4907},
ABSTRACT = {Obtaining accurate measurements of tree height and diameter at breast height (DBH) in forests to evaluate the growth rate of cultivars is still a significant challenge, even when using light detection and ranging (LiDAR) and three-dimensional (3-D) modeling. As an alternative, we provide a novel high-throughput strategy for predicting the biomass of forests in the field by vegetation indices. This study proposes an integrated pipeline methodology to measure the biomass of different tree cultivars in plantation forests with high crown density, which combines unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Using a planation of Cunninghamia lanceolate, which is commonly known as Chinese fir, in Fujian, China, images were collected while using a hyperspectral camera. Vegetation indices and modeling were processed in Python using decision trees, random forests, support vector machine, and eXtreme Gradient Boosting (XGBoost) third-party libraries. The tree height and DBH of 2880 samples were manually measured and clustered into three groups&mdash;&ldquo;Fast&rdquo;, &ldquo;median&rdquo;, and &ldquo;normal&rdquo; growth groups&mdash;and 19 vegetation indices from 12,000 pixels were abstracted as the input of features for the modeling. After modeling and cross-validation, the classifier that was generated by random forests had the best prediction accuracy when compared to other algorithms (75%). This framework can be applied to other tree species to make management and business decisions.},
DOI = {10.3390/f10090815}
}



@Article{electronics8101188,
AUTHOR = {Tang, Tao and Hong, Tao and Hong, Haohui and Ji, Senyuan and Mumtaz, Shahid and Cheriet, Mohamed},
TITLE = {An Improved UAV-PHD Filter-Based Trajectory Tracking Algorithm for Multi-UAVs in Future 5G IoT Scenarios},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1188},
URL = {https://www.mdpi.com/2079-9292/8/10/1188},
ISSN = {2079-9292},
ABSTRACT = {The 5G cellular network is expected to provide core service platform for the expanded Internet of Things (IoT) by supporting enhanced mobile broadband (eMBB), massive machine-type communication (mMTC), and ultra-reliable low latency communications (URLLC). Unmanned aerial vehicles (UAVs), also known as drones, provide civil, commercial, and government services in various fields. Particularly in a 5G IoT scenario, UAV-aided network communications will fulfill an increasingly important role and will require the tracking of multiple UAV targets. As UAVs move quickly, maintaining the stability of the communication connection in 5G will be a challenge. Therefore, it is necessary to track the trajectory of UAVs. At present, the GM-PHD filter has a problem that the new target intensity must be known, and it cannot obtain the moving target trajectory and the influence of the clutter is likely to cause false alarm. A UAV-PHD filter is proposed in this work to improve the traditional GM-PHD filter by applying machine learning to the emergency detection and trajectory tracking of UAV targets. An out-of-sight detection algorithm for multiple UAVs is then presented to improve tracking performance. The method is assessed by simulation using MATLAB, and OSPA distance is utilized as an evaluation indicator. The simulation results illustrate that the proposed method can be applied to the tracking of multiple UAV targets in future 5G-IoT scenarios, and the performance is superior to the traditional GM-PHD filter.},
DOI = {10.3390/electronics8101188}
}



@Article{rs11212575,
AUTHOR = {Tavakkoli Piralilou, Sepideh and Shahabi, Hejar and Jarihani, Ben and Ghorbanzadeh, Omid and Blaschke, Thomas and Gholamnia, Khalil and Meena, Sansar Raj and Aryal, Jagannath},
TITLE = {Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2575},
URL = {https://www.mdpi.com/2072-4292/11/21/2575},
ISSN = {2072-4292},
ABSTRACT = {Landslides represent a severe hazard in many areas of the world. Accurate landslide maps are needed to document the occurrence and extent of landslides and to investigate their distribution, types, and the pattern of slope failures. Landslide maps are also crucial for determining landslide susceptibility and risk. Satellite data have been widely used for such investigations—next to data from airborne or unmanned aerial vehicle (UAV)-borne campaigns and Digital Elevation Models (DEMs). We have developed a methodology that incorporates object-based image analysis (OBIA) with three machine learning (ML) methods, namely, the multilayer perceptron neural network (MLP-NN) and random forest (RF), for landslide detection. We identified the optimal scale parameters (SP) and used them for multi-scale segmentation and further analysis. We evaluated the resulting objects using the object pureness index (OPI), object matching index (OMI), and object fitness index (OFI) measures. We then applied two different methods to optimize the landslide detection task: (a) an ensemble method of stacking that combines the different ML methods for improving the performance, and (b) Dempster–Shafer theory (DST), to combine the multi-scale segmentation and classification results. Through the combination of three ML methods and the multi-scale approach, the framework enhanced landslide detection when it was tested for detecting earthquake-triggered landslides in Rasuwa district, Nepal. PlanetScope optical satellite images and a DEM were used, along with the derived landslide conditioning factors. Different accuracy assessment measures were used to compare the results against a field-based landslide inventory. All ML methods yielded the highest overall accuracies ranging from 83.3% to 87.2% when using objects with the optimal SP compared to other SPs. However, applying DST to combine the multi-scale results of each ML method significantly increased the overall accuracies to almost 90%. Overall, the integration of OBIA with ML methods resulted in appropriate landslide detections, but using the optimal SP and ML method is crucial for success.},
DOI = {10.3390/rs11212575}
}



@Article{s19214817,
AUTHOR = {Sosa-Herrera, Jesús A. and Vallejo-Pérez, Moisés R. and Álvarez-Jarquín, Nohemí and Cid-García, Néstor M. and López-Araujo, Daniela J.},
TITLE = {Geographic Object-Based Analysis of Airborne Multispectral Images for Health Assessment of Capsicum annuum L. Crops},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4817},
URL = {https://www.mdpi.com/1424-8220/19/21/4817},
ISSN = {1424-8220},
ABSTRACT = {Vegetation health assessment by using airborne multispectral images throughout crop production cycles, among other precision agriculture technologies, is an important tool for modern agriculture practices. However, to really take advantage of crop fields imagery, specialized analysis techniques are needed. In this paper we present a geographic object-based image analysis (GEOBIA) approach to examine a set of very high resolution (VHR) multispectral images obtained by the use of small unmanned aerial vehicles (UAVs), to evaluate plant health states and to generate cropland maps for Capsicum annuum L. The scheme described here integrates machine learning methods with semi-automated training and validation, which allowed us to develop an algorithmic sequence for the evaluation of plant health conditions at individual sowing point clusters over an entire parcel. The features selected at the classification stages are based on phenotypic traits of plants with different health levels. Determination of areas without data dependencies for the algorithms employed allowed us to execute some of the calculations as parallel processes. Comparison with the standard normalized difference vegetation index (NDVI) and biological analyses were also performed. The classification obtained showed a precision level of about     95 %     in discerning between vegetation and non-vegetation objects, and clustering efficiency ranging from     79 %     to     89 %     for the evaluation of different vegetation health categories, which makes our approach suitable for being incorporated at C. annuum crop&rsquo;s production systems, as well as to other similar crops. This methodology can be reproduced and adjusted as an on-the-go solution to get a georeferenced plant health estimation.},
DOI = {10.3390/s19214817}
}



@Article{rs11242925,
AUTHOR = {Prado Osco, Lucas and Marques Ramos, Ana Paula and Roberto Pereira, Danilo and Akemi Saito Moriya, Érika and Nobuhiro Imai, Nilton and Takashi Matsubara, Edson and Estrabis, Nayara and de Souza, Maurício and Marcato Junior, José and Gonçalves, Wesley Nunes and Li, Jonathan and Liesenberg, Veraldo and Eduardo Creste, José},
TITLE = {Predicting Canopy Nitrogen Content in Citrus-Trees Using Random Forest Algorithm Associated to Spectral Vegetation Indices from UAV-Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2925},
URL = {https://www.mdpi.com/2072-4292/11/24/2925},
ISSN = {2072-4292},
ABSTRACT = {The traditional method of measuring nitrogen content in plants is a time-consuming and labor-intensive task. Spectral vegetation indices extracted from unmanned aerial vehicle (UAV) images and machine learning algorithms have been proved effective in assisting nutritional analysis in plants. Still, this analysis has not considered the combination of spectral indices and machine learning algorithms to predict nitrogen in tree-canopy structures. This paper proposes a new framework to infer the nitrogen content in citrus-tree at a canopy-level using spectral vegetation indices processed with the random forest algorithm. A total of 33 spectral indices were estimated from multispectral images acquired with a UAV-based sensor. Leaf samples were gathered from different planting-fields and the leaf nitrogen content (LNC) was measured in the laboratory, and later converted into the canopy nitrogen content (CNC). To evaluate the robustness of the proposed framework, we compared it with other machine learning algorithms. We used 33,600 citrus trees to evaluate the performance of the machine learning models. The random forest algorithm had higher performance in predicting CNC than all models tested, reaching an R2 of 0.90, MAE of 0.341 g&middot;kg&minus;1 and MSE of 0.307 g&middot;kg&minus;1. We demonstrated that our approach is able to reduce the need for chemical analysis of the leaf tissue and optimizes citrus orchard CNC monitoring.},
DOI = {10.3390/rs11242925}
}



@Article{s19245436,
AUTHOR = {Barbedo, Jayme Garcia Arnal and Koenigkan, Luciano Vieira and Santos, Thiago Teixeira and Santos, Patrícia Menezes},
TITLE = {A Study on the Detection of Cattle in UAV Images Using Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/1424-8220/19/24/5436},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are being increasingly viewed as valuable tools to aid the management of farms. This kind of technology can be particularly useful in the context of extensive cattle farming, as production areas tend to be expansive and animals tend to be more loosely monitored. With the advent of deep learning, and convolutional neural networks (CNNs) in particular, extracting relevant information from aerial images has become more effective. Despite the technological advancements in drone, imaging and machine learning technologies, the application of UAVs for cattle monitoring is far from being thoroughly studied, with many research gaps still remaining. In this context, the objectives of this study were threefold: (1) to determine the highest possible accuracy that could be achieved in the detection of animals of the Canchim breed, which is visually similar to the Nelore breed (Bos taurus indicus); (2) to determine the ideal ground sample distance (GSD) for animal detection; (3) to determine the most accurate CNN architecture for this specific problem. The experiments involved 1853 images containing 8629 samples of animals, and 15 different CNN architectures were tested. A total of 900 models were trained (15 CNN architectures &times; 3 spacial resolutions &times; 2 datasets &times; 10-fold cross validation), allowing for a deep analysis of the several aspects that impact the detection of cattle using aerial images captured using UAVs. Results revealed that many CNN architectures are robust enough to reliably detect animals in aerial images even under far from ideal conditions, indicating the viability of using UAVs for cattle monitoring.},
DOI = {10.3390/s19245436}
}



@Article{rs12010056,
AUTHOR = {de Castro, Ana I. and Peña, José M. and Torres-Sánchez, Jorge and Jiménez-Brenes, Francisco M. and Valencia-Gredilla, Francisco and Recasens, Jordi and López-Granados, Francisca},
TITLE = {Mapping Cynodon Dactylon Infesting Cover Crops with an Automatic Decision Tree-OBIA Procedure and UAV Imagery for Precision Viticulture},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {56},
URL = {https://www.mdpi.com/2072-4292/12/1/56},
ISSN = {2072-4292},
ABSTRACT = {The establishment and management of cover crops are common practices widely used in irrigated viticulture around the world, as they bring great benefits not only to protect and improve the soil, but also to control vine vigor and improve the yield quality, among others. However, these benefits are often reduced when cover crops are infested by Cynodon dactylon (bermudagrass), which impacts crop production due to its competition for water and nutrients and causes important economic losses for the winegrowers. Therefore, the discrimination of Cynodon dactylon in cover crops would enable site-specific control to be applied and thus drastically mitigate damage to the vineyard. In this context, this research proposes a novel, automatic and robust image analysis algorithm for the quick and accurate mapping of Cynodon dactylon growing in vineyard cover crops. The algorithm was developed using aerial images taken with an Unmanned Aerial Vehicle (UAV) and combined decision tree (DT) and object-based image analysis (OBIA) approaches. The relevance of this work consisted in dealing with the constraint caused by the spectral similarity of these complex scenarios formed by vines, cover crops, Cynodon dactylon, and bare soil. The incorporation of height information from the Digital Surface Model and several features selected by machine learning tools in the DT-OBIA algorithm solved this spectral similarity limitation and allowed the precise design of Cynodon dactylon maps. Another contribution of this work is the short time needed to apply the full process from UAV flights to image analysis, which can enable useful maps to be created on demand (within two days of the farmer&acute;s request) and is thus timely for controlling Cynodon dactylon in the herbicide application window. Therefore, this combination of UAV imagery and a DT-OBIA algorithm would allow winegrowers to apply site-specific control of Cynodon dactylon and maintain cover crop-based management systems and their consequent benefits in the vineyards, and also comply with the European legal framework for the sustainable use of agricultural inputs and implementation of integrated crop management.},
DOI = {10.3390/rs12010056}
}



@Article{rs12020215,
AUTHOR = {Zha, Hainie and Miao, Yuxin and Wang, Tiantian and Li, Yue and Zhang, Jing and Sun, Weichao and Feng, Zhengqi and Kusnierek, Krzysztof},
TITLE = {Improving Unmanned Aerial Vehicle Remote Sensing-Based Rice Nitrogen Nutrition Index Prediction with Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {215},
URL = {https://www.mdpi.com/2072-4292/12/2/215},
ISSN = {2072-4292},
ABSTRACT = {Optimizing nitrogen (N) management in rice is crucial for China&rsquo;s food security and sustainable agricultural development. Nondestructive crop growth monitoring based on remote sensing technologies can accurately assess crop N status, which may be used to guide the in-season site-specific N recommendations. The fixed-wing unmanned aerial vehicle (UAV)-based remote sensing is a low-cost, easy-to-operate technology for collecting spectral reflectance imagery, an important data source for precision N management. The relationships between many vegetation indices (VIs) derived from spectral reflectance data and crop parameters are known to be nonlinear. As a result, nonlinear machine learning methods have the potential to improve the estimation accuracy. The objective of this study was to evaluate five different approaches for estimating rice (Oryza sativa L.) aboveground biomass (AGB), plant N uptake (PNU), and N nutrition index (NNI) at stem elongation (SE) and heading (HD) stages in Northeast China: (1) single VI (SVI); (2) stepwise multiple linear regression (SMLR); (3) random forest (RF); (4) support vector machine (SVM); and (5) artificial neural networks (ANN) regression. The results indicated that machine learning methods improved the NNI estimation compared to VI-SLR and SMLR methods. The RF algorithm performed the best for estimating NNI (R2 = 0.94 (SE) and 0.96 (HD) for calibration and 0.61 (SE) and 0.79 (HD) for validation). The root mean square errors (RMSEs) were 0.09, and the relative errors were &lt;10% in all the models. It is concluded that the RF machine learning regression can significantly improve the estimation of rice N status using UAV remote sensing. The application machine learning methods offers a new opportunity to better use remote sensing data for monitoring crop growth conditions and guiding precision crop management. More studies are needed to further improve these machine learning-based models by combining both remote sensing data and other related soil, weather, and management information for applications in precision N and crop management.},
DOI = {10.3390/rs12020215}
}



@Article{s20030743,
AUTHOR = {Haque, Akkas and Elsaharti, Ahmed and Elderini, Tarek and Elsaharty, Mohamed Atef and Neubert, Jeremiah},
TITLE = {UAV Autonomous Localization Using Macro-Features Matching with a CAD Model},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {743},
URL = {https://www.mdpi.com/1424-8220/20/3/743},
ISSN = {1424-8220},
ABSTRACT = {Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has significantly advanced in recent years, mainly due to their relevance in a large variety of commercial, industrial, and military applications. However, UAV navigation in GPS-denied environments continues to be a challenging problem that has been tackled in recent research through sensor-based approaches. This paper presents a novel offline, portable, real-time in-door UAV localization technique that relies on macro-feature detection and matching. The proposed system leverages the support of machine learning, traditional computer vision techniques, and pre-existing knowledge of the environment. The main contribution of this work is the real-time creation of a macro-feature description vector from the UAV captured images which are simultaneously matched with an offline pre-existing vector from a Computer-Aided Design (CAD) model. This results in a quick UAV localization within the CAD model. The effectiveness and accuracy of the proposed system were evaluated through simulations and experimental prototype implementation. Final results reveal the algorithm&rsquo;s low computational burden as well as its ease of deployment in GPS-denied environments.},
DOI = {10.3390/s20030743}
}



@Article{rs12030508,
AUTHOR = {Fu, Zhaopeng and Jiang, Jie and Gao, Yang and Krienke, Brian and Wang, Meng and Zhong, Kaitai and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
TITLE = {Wheat Growth Monitoring and Yield Estimation based on Multi-Rotor Unmanned Aerial Vehicle},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {508},
URL = {https://www.mdpi.com/2072-4292/12/3/508},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) and leaf dry matter (LDM) are important indices of crop growth. Real-time, nondestructive monitoring of crop growth is instructive for the diagnosis of crop growth and prediction of grain yield. Unmanned aerial vehicle (UAV)-based remote sensing is widely used in precision agriculture due to its unique advantages in flexibility and resolution. This study was carried out on wheat trials treated with different nitrogen levels and seeding densities in three regions of Jiangsu Province in 2018&ndash;2019. Canopy spectral images were collected by the UAV equipped with a multi-spectral camera during key wheat growth stages. To verify the results of the UAV images, the LAI, LDM, and yield data were obtained by destructive sampling. We extracted the wheat canopy reflectance and selected the best vegetation index for monitoring growth and predicting yield. Simple linear regression (LR), multiple linear regression (MLR), stepwise multiple linear regression (SMLR), partial least squares regression (PLSR), artificial neural network (ANN), and random forest (RF) modeling methods were used to construct a model for wheat yield estimation. The results show that the multi-spectral camera mounted on the multi-rotor UAV has a broad application prospect in crop growth index monitoring and yield estimation. The vegetation index combined with the red edge band and the near-infrared band was significantly correlated with LAI and LDM. Machine learning methods (i.e., PLSR, ANN, and RF) performed better for predicting wheat yield. The RF model constructed by normalized difference vegetation index (NDVI) at the jointing stage, heading stage, flowering stage, and filling stage was the optimal wheat yield estimation model in this study, with an R2 of 0.78 and relative root mean square error (RRMSE) of 0.1030. The results provide a theoretical basis for monitoring crop growth with a multi-rotor UAV platform and explore a technical method for improving the precision of yield estimation.},
DOI = {10.3390/rs12030508}
}



@Article{rs12050814,
AUTHOR = {Vilar, Pedro and Morais, Tiago G. and Rodrigues, Nuno R. and Gama, Ivo and Monteiro, Marta L. and Domingos, Tiago and Teixeira, Ricardo F. M.},
TITLE = {Object-Based Classification Approaches for Multitemporal Identification and Monitoring of Pastures in Agroforestry Regions using Multispectral Unmanned Aerial Vehicle Products},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {814},
URL = {https://www.mdpi.com/2072-4292/12/5/814},
ISSN = {2072-4292},
ABSTRACT = {Sown Biodiverse Pastures (SBP) are the basis of a high-yield grazing system tailored for Mediterranean ecosystems and widely implemented in Southern Portugal. The application of precision farming methods in SBP requires cost-effective monitoring using remote sensing (RS). The main hurdle for the remote monitoring of SBP is the fact that the bulk of the pastures are installed in open Montado agroforestry systems. Sparsely distributed trees cast shadows that hinder the identification of the underlaying pasture using Unmanned Aerial Vehicles (UAV) imagery. Image acquisition in the Spring is made difficult by the presence of flowers that mislead the classification algorithms. Here, we tested multiple procedures for the geographical, object-based image classification (GEOBIA) of SBP, aiming to reduce the effects of tree shadows and flowers in open Montado systems. We used remotely sensed data acquired between November 2017 and May 2018 in three Portuguese farms. We used three machine learning supervised classification algorithms: Random Forests (RF), Support Vector Machine (SVM) and Artificial Neural Networks (ANN). We classified SBP based on: (1) a single-period image for the maximum Normalized Difference Vegetation Index (NDVI) epoch in each of the three farms, and (2) multi-temporal image stacking. RF, SVM and ANN were trained using some visible (red, green and blue bands) and near-infrared (NIR) reflectance bands, plus NDVI and a Digital Surface Model (DSM). We obtained high overall accuracy and kappa index (higher than 79% and 0.60, respectively). The RF algorithm had the highest overall accuracy (more than 92%) for all farms. Multitemporal image classification increased the accuracy of the algorithms. as it helped to correctly identify as SBP the areas covered by tree shadows and flower patches, which would be misclassified using single image classification. This study thus established the first workflow for SBP monitoring based on remotely sensed data, suggesting an operational approach for SBP identification. The workflow can be applied to other types of pastures in agroforestry regions to reduce the effects of shadows and flowering in classification problems.},
DOI = {10.3390/rs12050814}
}



@Article{app10051759,
AUTHOR = {Guo, Han and Zhou, Jun and Liu, Fei and He, Yong and Huang, He and Wang, Hongyan},
TITLE = {Application of Machine Learning Method to Quantitatively Evaluate the Droplet Size and Deposition Distribution of the UAV Spray Nozzle},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1759},
URL = {https://www.mdpi.com/2076-3417/10/5/1759},
ISSN = {2076-3417},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) spray has been used for efficient and adaptive pesticide applications with its low costs. However, droplet drift is the main problem for UAV spray and will induce pesticide waste and safety concerns. Droplet size and deposition distribution are both highly related to droplet drift and spray effect, which are determined by the nozzle. Therefore, it is necessary to propose an evaluating method for a specific UAV spray nozzles. In this paper, four machine learning methods (REGRESS, least squares support vector machines (LS-SVM), extreme learning machine, and radial basis function neural network (RBFNN)) were applied for quantitatively evaluating one type of UAV spray nozzle (TEEJET XR110015VS), and the case of twin nozzles was investigated. The results showed REGRESS and LS-SVM are good candidates for droplet size evaluation with the coefficient of determination in the calibration set above 0.9 and root means square errors of the prediction set around 2 &micro;m. RBFNN achieved the best performance for the evaluation of deposition distribution and showed its potential for determining the droplet size of overlapping area. Overall, this study proved the accuracy and efficiency of using the machine learning method for UAV spray nozzle evaluation. Additionally, the study demonstrated the feasibility of using machine learning model to predict the droplet size in the overlapping area of twin nozzles.},
DOI = {10.3390/app10051759}
}



@Article{rs12071081,
AUTHOR = {Gibril, Mohamed Barakat A. and Kalantar, Bahareh and Al-Ruzouq, Rami and Ueda, Naonori and Saeidi, Vahideh and Shanableh, Abdallah and Mansor, Shattri and Shafri, Helmi Z. M.},
TITLE = {Mapping Heterogeneous Urban Landscapes from the Fusion of Digital Surface Model and Unmanned Aerial Vehicle-Based Images Using Adaptive Multiscale Image Segmentation and Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1081},
URL = {https://www.mdpi.com/2072-4292/12/7/1081},
ISSN = {2072-4292},
ABSTRACT = {Considering the high-level details in an ultrahigh-spatial-resolution (UHSR) unmanned aerial vehicle (UAV) dataset, detailed mapping of heterogeneous urban landscapes is extremely challenging because of the spectral similarity between classes. In this study, adaptive hierarchical image segmentation optimization, multilevel feature selection, and multiscale (MS) supervised machine learning (ML) models were integrated to accurately generate detailed maps for heterogeneous urban areas from the fusion of the UHSR orthomosaic and digital surface model (DSM). The integrated approach commenced through a preliminary MS image segmentation parameter selection, followed by the application of three supervised ML models, namely, random forest (RF), support vector machine (SVM), and decision tree (DT). These models were implemented at the optimal MS levels to identify preliminary information, such as the optimal segmentation level(s) and relevant features, for extracting 12 land use/land cover (LULC) urban classes from the fused datasets. Using the information obtained from the first phase of the analysis, detailed MS classification was iteratively conducted to improve the classification accuracy and derive the final urban LULC maps. Two UAV-based datasets were used to develop and assess the effectiveness of the proposed framework. The hierarchical classification of the pilot study area showed that the RF was superior with an overall accuracy (OA) of 94.40% and a kappa coefficient (K) of 0.938, followed by SVM (OA = 92.50% and K = 0.917) and DT (OA = 91.60% and K = 0.908). The classification results of the second dataset revealed that SVM was superior with an OA of 94.45% and K of 0.938, followed by RF (OA = 92.46% and K = 0.916) and DT (OA = 90.46% and K = 0.893). The proposed framework exhibited an excellent potential for the detailed mapping of heterogeneous urban landscapes from the fusion of UHSR orthophoto and DSM images using various ML models.},
DOI = {10.3390/rs12071081}
}



@Article{rs12091357,
AUTHOR = {Maimaitijiang, Maitiniyazi and Sagan, Vasit and Sidike, Paheding and Daloye, Ahmad M. and Erkbol, Hasanjan and Fritschi, Felix B.},
TITLE = {Crop Monitoring Using Satellite/UAV Data Fusion and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1357},
URL = {https://www.mdpi.com/2072-4292/12/9/1357},
ISSN = {2072-4292},
ABSTRACT = {Non-destructive crop monitoring over large areas with high efficiency is of great significance in precision agriculture and plant phenotyping, as well as decision making with regards to grain policy and food security. The goal of this research was to assess the potential of combining canopy spectral information with canopy structure features for crop monitoring using satellite/unmanned aerial vehicle (UAV) data fusion and machine learning. Worldview-2/3 satellite data were tasked synchronized with high-resolution RGB image collection using an inexpensive unmanned aerial vehicle (UAV) at a heterogeneous soybean (Glycine max (L.) Merr.) field. Canopy spectral information (i.e., vegetation indices) was extracted from Worldview-2/3 data, and canopy structure information (i.e., canopy height and canopy cover) was derived from UAV RGB imagery. Canopy spectral and structure information and their combination were used to predict soybean leaf area index (LAI), aboveground biomass (AGB), and leaf nitrogen concentration (N) using partial least squares regression (PLSR), random forest regression (RFR), support vector regression (SVR), and extreme learning regression (ELR) with a newly proposed activation function. The results revealed that: (1) UAV imagery-derived high-resolution and detailed canopy structure features, canopy height, and canopy coverage were significant indicators for crop growth monitoring, (2) integration of satellite imagery-based rich canopy spectral information with UAV-derived canopy structural features using machine learning improved soybean AGB, LAI, and leaf N estimation on using satellite or UAV data alone, (3) adding canopy structure information to spectral features reduced background soil effect and asymptotic saturation issue to some extent and led to better model performance, (4) the ELR model with the newly proposed activated function slightly outperformed PLSR, RFR, and SVR in the prediction of AGB and LAI, while RFR provided the best result for N estimation. This study introduced opportunities and limitations of satellite/UAV data fusion using machine learning in the context of crop monitoring.},
DOI = {10.3390/rs12091357}
}



@Article{w12051281,
AUTHOR = {Chen, Je-Chian and Wang, Yu-Min},
TITLE = {Comparing Activation Functions in Modeling Shoreline Variation Using Multilayer Perceptron Neural Network},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1281},
URL = {https://www.mdpi.com/2073-4441/12/5/1281},
ISSN = {2073-4441},
ABSTRACT = {The study has modeled shoreline changes by using a multilayer perceptron (MLP) neural network with the data collected from five beaches in southern Taiwan. The data included aerial survey maps of the Forestry Bureau for years 1982, 2002, and 2006, which served as predictors, while the unmanned aerial vehicle (UAV) surveyed data of 2019 served as the respondent. The MLP was configured using five different activation functions with the aim of evaluating their significance. These functions were Identity, Tahn, Logistic, Exponential, and Sine Functions. The results have shown that the performance of an MLP model may be affected by the choice of an activation function. Logistic and the Tahn activation functions outperformed the other models, with Logistic performing best in three beaches and Tahn having the rest. These findings suggest that the application of machine learning to shoreline changes should be accompanied by an extensive evaluation of the different activation functions.},
DOI = {10.3390/w12051281}
}



@Article{drones4020018,
AUTHOR = {Gorkin, Robert and Adams, Kye and Berryman, Matthew J and Aubin, Sam and Li, Wanqing and Davis, Andrew R and Barthelemy, Johan},
TITLE = {Sharkeye: Real-Time Autonomous Personal Shark Alerting via Aerial Surveillance},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2504-446X/4/2/18},
ISSN = {2504-446X},
ABSTRACT = {While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens.},
DOI = {10.3390/drones4020018}
}



@Article{rs12101571,
AUTHOR = {Zhang, Fan and Hu, Zhenqi and Fu, Yaokun and Yang, Kun and Wu, Qunying and Feng, Zewei},
TITLE = {A New Identification Method for Surface Cracks from UAV Images Based on Machine Learning in Coal Mining Areas},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1571},
URL = {https://www.mdpi.com/2072-4292/12/10/1571},
ISSN = {2072-4292},
ABSTRACT = {Obtaining real-time, objective, and high-precision distribution information of surface cracks in mining areas is the first task for studying the development regularity of surface cracks and evaluating the risk. The complex geological environment in the mining area leads to low accuracy and efficiency of the existing extracting cracks methods from unmanned air vehicle (UAV) images. Therefore, this manuscript proposes a new identification method of surface cracks from UAV images based on machine learning in coal mining areas. First, the acquired UAV image is cut into small sub-images, and divided into four datasets according to the characteristics of background information: Bright Ground, Dark Dround, Withered Vegetation, and Green Vegetation. Then, for each dataset, a training sample is established with cracks and no cracks as labels and the RGB (red, green, and blue) three-band value of the sub-image as feature. Finally, the best machine learning algorithms, dimensionality reduction methods and image processing techniques are obtained through comparative analysis. The results show that using the V-SVM (Support vector machine with V as penalty function) machine learning algorithm, principal component analysis (PCA) to reduce the full features to 95% of the original variance, and image color enhancement by Laplace sharpening, the overall accuracy could reach 88.99%. This proves that the method proposed in this manuscript can achieve high-precision crack extraction from UAV image.},
DOI = {10.3390/rs12101571}
}



@Article{rs12111843,
AUTHOR = {Revill, Andrew and Florence, Anna and MacArthur, Alasdair and Hoad, Stephen and Rees, Robert and Williams, Mathew},
TITLE = {Quantifying Uncertainty and Bridging the Scaling Gap in the Retrieval of Leaf Area Index by Coupling Sentinel-2 and UAV Observations},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1843},
URL = {https://www.mdpi.com/2072-4292/12/11/1843},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) estimates can inform decision-making in crop management. The European Space Agency&rsquo;s Sentinel-2 satellite, with observations in the red-edge spectral region, can monitor crops globally at sub-field spatial resolutions (10&ndash;20 m). However, satellite LAI estimates require calibration with ground measurements. Calibration is challenged by spatial heterogeneity and scale mismatches between field and satellite measurements. Unmanned Aerial Vehicles (UAVs), generating high-resolution (cm-scale) LAI estimates, provide intermediary observations that we use here to characterise uncertainty and reduce spatial scaling discrepancies between Sentinel-2 observations and field surveys. We use a novel UAV multispectral sensor that matches Sentinel-2 spectral bands, flown in conjunction with LAI ground measurements. UAV and field surveys were conducted on multiple dates&mdash;coinciding with different wheat growth stages&mdash;that corresponded to Sentinel-2 overpasses. We compared chlorophyll red-edge index (CIred-edge) maps, derived from the Sentinel-2 and UAV platforms. We used Gaussian processes regression machine learning to calibrate a UAV model for LAI, based on ground data. Using the UAV LAI, we evaluated a two-stage calibration approach for generating robust LAI estimates from Sentinel-2. The agreement between Sentinel-2 and UAV CIred-edge values increased with growth stage&mdash;R2 ranged from 0.32 (stem elongation) to 0.75 (milk development). The CIred-edge variance between the two platforms was more comparable later in the growing season due to a more homogeneous and closed wheat canopy. The single-stage Sentinel-2 LAI calibration (i.e., direct calibration from ground measurements) performed poorly (mean R2 = 0.29, mean NRMSE = 17%) when compared to the two-stage calibration using the UAV data (mean R2 = 0.88, mean NRMSE = 8%). The two-stage approach reduced both errors and biases by &gt;50%. By upscaling ground measurements and providing more representative model training samples, UAV observations provide an effective and viable means of enhancing Sentinel-2 wheat LAI retrievals. We anticipate that our UAV calibration approach to resolving spatial heterogeneity would enhance the retrieval accuracy of LAI and additional biophysical variables for other arable crop types and a broader range of vegetation cover types.},
DOI = {10.3390/rs12111843}
}



@Article{s20113245,
AUTHOR = {Zhang, Tianyao and Hu, Xiaoguang and Xiao, Jin and Zhang, Guofeng},
TITLE = {A Machine Learning Method for Vision-Based Unmanned Aerial Vehicle Systems to Understand Unknown Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3245},
URL = {https://www.mdpi.com/1424-8220/20/11/3245},
ISSN = {1424-8220},
ABSTRACT = {What makes unmanned aerial vehicles (UAVs) intelligent is their capability of sensing and understanding new unknown environments. Some studies utilize computer vision algorithms like Visual Simultaneous Localization and Mapping (VSLAM) and Visual Odometry (VO) to sense the environment for pose estimation, obstacles avoidance and visual servoing. However, understanding the new environment (i.e., make the UAV recognize generic objects) is still an essential scientific problem that lacks a solution. Therefore, this paper takes a step to understand the items in an unknown environment. The aim of this research is to enable the UAV with basic understanding capability for a high-level UAV flock application in the future. Specially, firstly, the proposed understanding method combines machine learning and traditional algorithm to understand the unknown environment through RGB images; secondly, the You Only Look Once (YOLO) object detection system is integrated (based on TensorFlow) in a smartphone to perceive the position and category of 80 classes of objects in the images; thirdly, the method makes the UAV more intelligent and liberates the operator from labor; fourthly, detection accuracy and latency in working condition are quantitatively evaluated, and properties of generality (can be used in various platforms), transportability (easily deployed from one platform to another) and scalability (easily updated and maintained) for UAV flocks are qualitatively discussed. The experiments suggest that the method has enough accuracy to recognize various objects with high computational speed, and excellent properties of generality, transportability and scalability.},
DOI = {10.3390/s20113245}
}



@Article{drones4020021,
AUTHOR = {Rodríguez-Puerta, Francisco and Alonso Ponce, Rafael and Pérez-Rodríguez, Fernando and Águeda, Beatriz and Martín-García, Saray and Martínez-Rodrigo, Raquel and Lizarralde, Iñigo},
TITLE = {Comparison of Machine Learning Algorithms for Wildland-Urban Interface Fuelbreak Planning Integrating ALS and UAV-Borne LiDAR Data and Multispectral Images},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {21},
URL = {https://www.mdpi.com/2504-446X/4/2/21},
ISSN = {2504-446X},
ABSTRACT = {Controlling vegetation fuels around human settlements is a crucial strategy for reducing fire severity in forests, buildings and infrastructure, as well as protecting human lives. Each country has its own regulations in this respect, but they all have in common that by reducing fuel load, we in turn reduce the intensity and severity of the fire. The use of Unmanned Aerial Vehicles (UAV)-acquired data combined with other passive and active remote sensing data has the greatest performance to planning Wildland-Urban Interface (WUI) fuelbreak through machine learning algorithms. Nine remote sensing data sources (active and passive) and four supervised classification algorithms (Random Forest, Linear and Radial Support Vector Machine and Artificial Neural Networks) were tested to classify five fuel-area types. We used very high-density Light Detection and Ranging (LiDAR) data acquired by UAV (154 returns&middot;m&minus;2 and ortho-mosaic of 5-cm pixel), multispectral data from the satellites Pleiades-1B and Sentinel-2, and low-density LiDAR data acquired by Airborne Laser Scanning (ALS) (0.5 returns&middot;m&minus;2, ortho-mosaic of 25 cm pixels). Through the Variable Selection Using Random Forest (VSURF) procedure, a pre-selection of final variables was carried out to train the model. The four algorithms were compared, and it was concluded that the differences among them in overall accuracy (OA) on training datasets were negligible. Although the highest accuracy in the training step was obtained in SVML (OA=94.46%) and in testing in ANN (OA=91.91%), Random Forest was considered to be the most reliable algorithm, since it produced more consistent predictions due to the smaller differences between training and testing performance. Using a combination of Sentinel-2 and the two LiDAR data (UAV and ALS), Random Forest obtained an OA of 90.66% in training and of 91.80% in testing datasets. The differences in accuracy between the data sources used are much greater than between algorithms. LiDAR growth metrics calculated using point clouds in different dates and multispectral information from different seasons of the year are the most important variables in the classification. Our results support the essential role of UAVs in fuelbreak planning and management and thus, in the prevention of forest fires.},
DOI = {10.3390/drones4020021}
}



@Article{rs12122017,
AUTHOR = {Karunaratne, Senani and Thomson, Anna and Morse-McNabb, Elizabeth and Wijesingha, Jayan and Stayches, Dani and Copland, Amy and Jacobs, Joe},
TITLE = {The Fusion of Spectral and Structural Datasets Derived from an Airborne Multispectral Sensor for Estimation of Pasture Dry Matter Yield at Paddock Scale with Time},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2017},
URL = {https://www.mdpi.com/2072-4292/12/12/2017},
ISSN = {2072-4292},
ABSTRACT = {This study aimed to develop empirical pasture dry matter (DM) yield prediction models using an unmanned aerial vehicle (UAV)-borne sensor at four flying altitudes. Three empirical models were developed using features generated from the multispectral sensor: Structure from Motion only (SfM), vegetation indices only (VI), and in combination (SfM+VI) within a machine learning modelling framework. Four flying altitudes were tested (25 m, 50 m, 75 m and 100 m) and based on independent model validation, combining features from SfM+VI outperformed the other models at all heights. However, the importance of SfM-based features changed with altitude, with limited importance at 25 m but at all higher altitudes SfM-based features were included in the top 10 features in a variable importance plot. Based on the independent validation results, data generated at 25 m flying altitude reported the best model performances with model accuracy of 328 kg DM/ha. In contrast, at 100 m flying altitude, the model reported an accuracy of 402 kg DM/ha which demonstrates the potential of scaling up this technology at farm scale. The spatial-temporal maps provide valuable information on pasture DM yield and DM accumulation of herbage mass over the time, supporting on-farm management decisions.},
DOI = {10.3390/rs12122017}
}



@Article{rs12122028,
AUTHOR = {Feng, Luwei and Zhang, Zhou and Ma, Yuchi and Du, Qingyun and Williams, Parker and Drewry, Jessica and Luck, Brian},
TITLE = {Alfalfa Yield Prediction Using UAV-Based Hyperspectral Imagery and Ensemble Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2028},
URL = {https://www.mdpi.com/2072-4292/12/12/2028},
ISSN = {2072-4292},
ABSTRACT = {Alfalfa is a valuable and intensively produced forage crop in the United States, and the timely estimation of its yield can inform precision management decisions. However, traditional yield assessment approaches are laborious and time-consuming, and thus hinder the acquisition of timely information at the field scale. Recently, unmanned aerial vehicles (UAVs) have gained significant attention in precision agriculture due to their efficiency in data acquisition. In addition, compared with other imaging modalities, hyperspectral data can offer higher spectral fidelity for constructing narrow-band vegetation indices which are of great importance in yield modeling. In this study, we performed an in-season alfalfa yield prediction using UAV-based hyperspectral images. Specifically, we firstly extracted a large number of hyperspectral indices from the original data and performed a feature selection to reduce the data dimensionality. Then, an ensemble machine learning model was developed by combining three widely used base learners including random forest (RF), support vector regression (SVR) and K-nearest neighbors (KNN). The model performance was evaluated on experimental fields in Wisconsin. Our results showed that the ensemble model outperformed all the base learners and a coefficient of determination (R2) of 0.874 was achieved when using the selected features. In addition, we also evaluated the model adaptability on different machinery compaction treatments, and the results further demonstrate the efficacy of the proposed ensemble model.},
DOI = {10.3390/rs12122028}
}



@Article{rs12132071,
AUTHOR = {Lee, Hwang and Wang, Jinfei and Leblon, Brigitte},
TITLE = {Using Linear Regression, Random Forests, and Support Vector Machine with Unmanned Aerial Vehicle Multispectral Images to Predict Canopy Nitrogen Weight in Corn},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {2071},
URL = {https://www.mdpi.com/2072-4292/12/13/2071},
ISSN = {2072-4292},
ABSTRACT = {The optimization of crop nitrogen fertilization to accurately predict and match the nitrogen (N) supply to the crop N demand is the subject of intense research due to the environmental and economic impact of N fertilization. Excess N could seep into the water supplies around the field and cause unnecessary spending by the farmer. The drawbacks of N deficiency on crops include poor plant growth, ultimately reducing the final yield potential. The objective of this study is to use Unmanned Aerial Vehicle (UAV) multispectral imagery to predict canopy nitrogen weight (g/m2) of corn fields in south-west Ontario, Canada. Simple/multiple linear regression, Random Forests, and support vector regression (SVR) were established to predict the canopy nitrogen weight from individual multispectral bands and associated vegetation indices (VI). Random Forests using the current techniques/methodologies performed the best out of all the models tested on the validation set with an R2 of 0.85 and Root Mean Square Error (RMSE) of 4.52 g/m2. Adding more spectral variables into the model provided a marginal improvement in the accuracy, while extending the overall processing time. Random Forests provided marginally better results than SVR, but the concepts and analysis are much easier to interpret on Random Forests. Both machine learning models provided a much better accuracy than linear regression. The best model was then applied to the UAV images acquired at different dates for producing maps that show the spatial variation of canopy nitrogen weight within each field at that date.},
DOI = {10.3390/rs12132071}
}



@Article{rs12132169,
AUTHOR = {Arce, Samuel and Vernon, Cory A. and Hammond, Joshua and Newell, Valerie and Janson, Joseph and Franke, Kevin W. and Hedengren, John D.},
TITLE = {Automated 3D Reconstruction Using Optimized View-Planning Algorithms for Iterative Development of Structure-from-Motion Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {2169},
URL = {https://www.mdpi.com/2072-4292/12/13/2169},
ISSN = {2072-4292},
ABSTRACT = {Unsupervised machine learning algorithms (clustering, genetic, and principal component analysis) automate Unmanned Aerial Vehicle (UAV) missions as well as the creation and refinement of iterative 3D photogrammetric models with a next best view (NBV) approach. The novel approach uses Structure-from-Motion (SfM) to achieve convergence to a specified orthomosaic resolution by identifying edges in the point cloud and planning cameras that &ldquo;view&rdquo; the holes identified by edges without requiring an initial model. This iterative UAV photogrammetric method successfully runs in various Microsoft AirSim environments. Simulated ground sampling distance (GSD) of models reaches as low as     3.4     cm per pixel, and generally, successive iterations improve resolution. Besides analogous application in simulated environments, a field study of a retired municipal water tank illustrates the practical application and advantages of automated UAV iterative inspection of infrastructure using     63 %     fewer photographs than a comparable manual flight with analogous density point clouds obtaining a GSD of less than 3 cm per pixel. Each iteration qualitatively increases resolution according to a logarithmic regression, reduces holes in models, and adds details to model edges.},
DOI = {10.3390/rs12132169}
}



@Article{asi3030029,
AUTHOR = {Grác, Šimon and Beňo, Peter and Duchoň, František and Dekan, Martin and Tölgyessy, Michal},
TITLE = {Automated Detection of Multi-Rotor UAVs Using a Machine-Learning Approach},
JOURNAL = {Applied System Innovation},
VOLUME = {3},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {29},
URL = {https://www.mdpi.com/2571-5577/3/3/29},
ISSN = {2571-5577},
ABSTRACT = {The objective of this article is to propose and verify a reliable detection mechanism of multi-rotor unmanned aerial vehicles (UAVs). Such a task needs to be solved in many areas such as in the protection of vulnerable buildings or in the protection of privacy. Our system was firstly realized by standard computer vision methods using the Oriented FAST and Rotated BRIEF (ORB) feature detector. Due to the low success rate achieved in real-world conditions, the machine-learning approach was used as an alternative detection mechanism. The &ldquo;Common Objects in Context dataset&rdquo; was used as a predefined dataset and it was extended by 1000 samples of UAVs from the SafeShore dataset. The effectiveness and the reliability of our system are proven by four basic experiments&mdash;drone in a static image and videos which are displaying a drone in the sky, multiple drones in one image, and a drone with another flying object in the sky. The successful detection rate achieved was 97.3% in optimal conditions.},
DOI = {10.3390/asi3030029}
}



@Article{rs12142280,
AUTHOR = {Iordache, Marian-Daniel and Mantas, Vasco and Baltazar, Elsa and Pauly, Klaas and Lewyckyj, Nicolas},
TITLE = {A Machine Learning Approach to Detecting Pine Wilt Disease Using Airborne Spectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2280},
URL = {https://www.mdpi.com/2072-4292/12/14/2280},
ISSN = {2072-4292},
ABSTRACT = {Pine Wilt Disease is one of the most destructive pests affecting coniferous forests. After being infected by the harmful Bursaphelenchus xylophilus nematode, most trees die within one year. The complex spreading pattern of the disease and the tedious hard labor process of diagnosis involving field wood sampling followed by laboratory analysis call for alternative methods to detect and manage the infected areas. Remote sensing comes naturally into play owing to the possibility of covering relatively large areas and the ability to discriminate healthy from sick trees based on spectral characteristics. This paper presents the development of machine learning classification algorithms for the detection of Pine Wilt Disease in Pinus pinaster, performed in the framework of the European Commission&rsquo;s Horizon 2020 project &ldquo;Operational Forest Monitoring using Copernicus and UAV Hyperspectral Data&rdquo; (FOCUS) in two provinces of central Portugal. Five flight campaigns have been carried out in two consecutive years in order to capture a multitemporal variation of disease distribution. Classification algorithms based on a Random Forest approach were separately designed for the acquired very-high-resolution multispectral and hyperspectral data, respectively. Both algorithms achieved overall accuracies higher than 0.91 in test data. Furthermore, our study shows that the early detection of decaying trees is feasible, even before symptoms are visible in the field.},
DOI = {10.3390/rs12142280}
}



@Article{s20143947,
AUTHOR = {Mohamed, Ehab Mahmoud and Hashima, Sherief and Aldosary, Abdallah and Hatano, Kohei and Abdelghany, Mahmoud Ahmed},
TITLE = {Gateway Selection in Millimeter Wave UAV Wireless Networks Using Multi-Player Multi-Armed Bandit},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3947},
URL = {https://www.mdpi.com/1424-8220/20/14/3947},
ISSN = {1424-8220},
ABSTRACT = {Recently, unmanned aerial vehicle (UAV)-based communications gained a lot of attention due to their numerous applications, especially in rescue services in post-disaster areas where the terrestrial network is wholly malfunctioned. Multiple access/gateway UAVs are distributed to fully cover the post-disaster area as flying base stations to provide communication coverage, collect valuable information, disseminate essential instructions, etc. The access UAVs after gathering/broadcasting the necessary information should select and fly towards one of the surrounding gateways for relaying their information. In this paper, the gateway UAV selection problem is addressed. The main aim is to maximize the long-term average data rates of the UAVs relays while minimizing the flights&rsquo; battery cost, where millimeter wave links, i.e., using 30~300 GHz band, employing antenna beamforming, are used for backhauling. A tool of machine learning (ML) is exploited to address the problem as a budget-constrained multi-player multi-armed bandit (MAB) problem. In this setup, access UAVs act as the players, and the arms are the gateway UAVs, while the rewards are the average data rates of the constructed relays constrained by the battery cost of the access UAV flights. In this decentralized setting, where information is neither prior available nor exchanged among UAVs, a selfish and concurrent multi-player MAB strategy is suggested. Towards this end, three battery-aware MAB (BA-MAB) algorithms, namely upper confidence bound (UCB), Thompson sampling (TS), and the exponential weight algorithm for exploration and exploitation (EXP3), are proposed to realize gateways selection efficiently. The proposed BA-MAB-based gateway UAV selection algorithms show superior performance over approaches based on near and random selections in terms of total system rate and energy efficiency.},
DOI = {10.3390/s20143947}
}



@Article{f11080827,
AUTHOR = {Campos-Vargas, Carlos and Sanchez-Azofeifa, Arturo and Laakso, Kati and Marzahn, Philip},
TITLE = {Unmanned Aerial System and Machine Learning Techniques Help to Detect Dead Woody Components in a Tropical Dry Forest},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {827},
URL = {https://www.mdpi.com/1999-4907/11/8/827},
ISSN = {1999-4907},
ABSTRACT = {Background and Objectives: Increased frequency and intensity of drought events are predicted to occur throughout the world because of climate change. These extreme climate events result in higher tree mortality and fraction of dead woody components, phenomena that are currently being reported worldwide as critical indicators of the impacts of climate change on forest diversity and function. In this paper, we assess the accuracy and processing times of ten machine learning (ML) techniques, applied to multispectral unmanned aerial vehicle (UAV) data to detect dead canopy woody components. Materials and Methods: This work was conducted on five secondary dry forest plots located at the Santa Rosa National Park Environmental Monitoring Super Site, Costa Rica. Results: The coverage of dead woody components at the selected secondary dry forest plots was estimated to range from 4.8% to 16.1%, with no differences between the successional stages. Of the ten ML techniques, the support vector machine with radial kernel (SVMR) and random forests (RF) provided the highest accuracies (0.982 vs. 0.98, respectively). Of these two ML algorithms, the processing time of SVMR was longer than the processing time of RF (8735.64 s vs. 989 s). Conclusions: Our results demonstrate that it is feasible to detect and quantify dead woody components, such as dead stands and fallen trees, using a combination of high-resolution UAV data and ML algorithms. Using this technology, accuracy values higher than 95% were achieved. However, it is important to account for a series of factors, such as the optimization of the tuning parameters of the ML algorithms, the environmental conditions and the time of the UAV data acquisition.},
DOI = {10.3390/f11080827}
}



@Article{agronomy10081108,
AUTHOR = {Ranđelović, Predrag and Đorđević, Vuk and Milić, Stanko and Balešević-Tubić, Svetlana and Petrović, Kristina and Miladinović, Jegor and Đukić, Vojin},
TITLE = {Prediction of Soybean Plant Density Using a Machine Learning Model and Vegetation Indices Extracted from RGB Images Taken with a UAV},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1108},
URL = {https://www.mdpi.com/2073-4395/10/8/1108},
ISSN = {2073-4395},
ABSTRACT = {Soybean plant density is an important factor of successful agricultural production. Due to the high number of plants per unit area, early plant overlapping and eventual plant loss, the estimation of soybean plant density in the later stages of development should enable the determination of the final plant number and reflect the state of the harvest. In order to assess soybean plant density in a digital, nondestructive, and less intense way, analysis was performed on RGB images (containing three channels: RED, GREEN, and BLUE) taken with a UAV (Unmanned Aerial Vehicle) on 66 experimental plots in 2018, and 200 experimental plots in 2019. Mean values of the R, G, and B channels were extracted for each plot, then vegetation indices (VIs) were calculated and used as predictors for the machine learning model (MLM). The model was calibrated in 2018 and validated in 2019. For validation purposes, the predicted values for the 200 experimental plots were compared with the real number of plants per unit area (m2). Model validation resulted in the correlation coefficient&mdash;R = 0.87, mean absolute error (MAE) = 6.24, and root mean square error (RMSE) = 7.47. The results of the research indicate the possibility of using the MLM, based on simple values of VIs, for the prediction of plant density in agriculture without using human labor.},
DOI = {10.3390/agronomy10081108}
}



@Article{drones4030045,
AUTHOR = {Musci, Maria Angela and Mazzara, Luigi and Lingua, Andrea Maria},
TITLE = {Ice Detection on Aircraft Surface Using Machine Learning Approaches Based on Hyperspectral and Multispectral Images},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {45},
URL = {https://www.mdpi.com/2504-446X/4/3/45},
ISSN = {2504-446X},
ABSTRACT = {Aircraft ground de-icing operations play a critical role in flight safety. However, to handle the aircraft de-icing, a considerable quantity of de-icing fluids is commonly employed. Moreover, some pre-flight inspections are carried out with engines running; thus, a large amount of fuel is wasted, and CO2 is emitted. This implies substantial economic and environmental impacts. In this context, the European project (reference call: MANUNET III 2018, project code: MNET18/ICT-3438) called SEI (Spectral Evidence of Ice) aims to provide innovative tools to identify the ice on aircraft and improve the efficiency of the de-icing process. The project includes the design of a low-cost UAV (uncrewed aerial vehicle) platform and the development of a quasi-real-time ice detection methodology to ensure a faster and semi-automatic activity with a reduction of applied operating time and de-icing fluids. The purpose of this work, developed within the activities of the project, is defining and testing the most suitable sensor using a radiometric approach and machine learning algorithms. The adopted methodology consists of classifying ice through spectral imagery collected by two different sensors: multispectral and hyperspectral camera. Since the UAV prototype is under construction, the experimental analysis was performed with a simulation dataset acquired on the ground. The comparison among the two approaches, and their related algorithms (random forest and support vector machine) for image processing, was presented: practical results show that it is possible to identify the ice in both cases. Nonetheless, the hyperspectral camera guarantees a more reliable solution reaching a higher level of accuracy of classified iced surfaces.},
DOI = {10.3390/drones4030045}
}



@Article{s20185055,
AUTHOR = {Guo, Yahui and Wang, Hanxi and Wu, Zhaofei and Wang, Shuxin and Sun, Hongyong and Senthilnath, J. and Wang, Jingzhe and Robin Bryant, Christopher and Fu, Yongshuo},
TITLE = {Modified Red Blue Vegetation Index for Chlorophyll Estimation and Yield Prediction of Maize from Visible Images Captured by UAV},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5055},
URL = {https://www.mdpi.com/1424-8220/20/18/5055},
ISSN = {1424-8220},
ABSTRACT = {The vegetation index (VI) has been successfully used to monitor the growth and to predict the yield of agricultural crops. In this paper, a long-term observation was conducted for the yield prediction of maize using an unmanned aerial vehicle (UAV) and estimations of chlorophyll contents using SPAD-502. A new vegetation index termed as modified red blue VI (MRBVI) was developed to monitor the growth and to predict the yields of maize by establishing relationships between MRBVI- and SPAD-502-based chlorophyll contents. The coefficients of determination (R2s) were 0.462 and 0.570 in chlorophyll contents&rsquo; estimations and yield predictions using MRBVI, and the results were relatively better than the results from the seven other commonly used VI approaches. All VIs during the different growth stages of maize were calculated and compared with the measured values of chlorophyll contents directly, and the relative error (RE) of MRBVI is the lowest at 0.355. Further, machine learning (ML) methods such as the backpropagation neural network model (BP), support vector machine (SVM), random forest (RF), and extreme learning machine (ELM) were adopted for predicting the yields of maize. All VIs calculated for each image captured during important phenological stages of maize were set as independent variables and the corresponding yields of each plot were defined as dependent variables. The ML models used the leave one out method (LOO), where the root mean square errors (RMSEs) were 2.157, 1.099, 1.146, and 1.698 (g/hundred grain weight) for BP, SVM, RF, and ELM. The mean absolute errors (MAEs) were 1.739, 0.886, 0.925, and 1.356 (g/hundred grain weight) for BP, SVM, RF, and ELM, respectively. Thus, the SVM method performed better in predicting the yields of maize than the other ML methods. Therefore, it is strongly suggested that the MRBVI calculated from images acquired at different growth stages integrated with advanced ML methods should be used for agricultural- and ecological-related chlorophyll estimation and yield predictions.},
DOI = {10.3390/s20185055}
}



@Article{s20185130,
AUTHOR = {Guo, Yahui and Yin, Guodong and Sun, Hongyong and Wang, Hanxi and Chen, Shouzhi and Senthilnath, J. and Wang, Jingzhe and Fu, Yongshuo},
TITLE = {Scaling Effects on Chlorophyll Content Estimations with RGB Camera Mounted on a UAV Platform Using Machine-Learning Methods},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5130},
URL = {https://www.mdpi.com/1424-8220/20/18/5130},
ISSN = {1424-8220},
ABSTRACT = {Timely monitoring and precise estimation of the leaf chlorophyll contents of maize are crucial for agricultural practices. The scale effects are very important as the calculated vegetation index (VI) were crucial for the quantitative remote sensing. In this study, the scale effects were investigated by analyzing the linear relationships between VI calculated from red&ndash;green&ndash;blue (RGB) images from unmanned aerial vehicles (UAV) and ground leaf chlorophyll contents of maize measured using SPAD-502. The scale impacts were assessed by applying different flight altitudes and the highest coefficient of determination (R2) can reach 0.85. We found that the VI from images acquired from flight altitude of 50 m was better to estimate the leaf chlorophyll contents using the DJI UAV platform with this specific camera (5472 &times; 3648 pixels). Moreover, three machine-learning (ML) methods including backpropagation neural network (BP), support vector machine (SVM), and random forest (RF) were applied for the grid-based chlorophyll content estimation based on the common VI. The average values of the root mean square error (RMSE) of chlorophyll content estimations using ML methods were 3.85, 3.11, and 2.90 for BP, SVM, and RF, respectively. Similarly, the mean absolute error (MAE) were 2.947, 2.460, and 2.389, for BP, SVM, and RF, respectively. Thus, the ML methods had relative high precision in chlorophyll content estimations using VI; in particular, the RF performed better than BP and SVM. Our findings suggest that the integrated ML methods with RGB images of this camera acquired at a flight altitude of 50 m (spatial resolution 0.018 m) can be perfectly applied for estimations of leaf chlorophyll content in agriculture.},
DOI = {10.3390/s20185130}
}



@Article{agriculture10100451,
AUTHOR = {López-Calderón, Magali J. and Estrada-Ávalos, Juan and Rodríguez-Moreno, Víctor M. and Mauricio-Ruvalcaba, Jorge E. and Martínez-Sifuentes, Aldo R. and Delgado-Ramírez, Gerardo and Miguel-Valle, Enrique},
TITLE = {Estimation of Total Nitrogen Content in Forage Maize (Zea mays L.) Using Spectral Indices: Analysis by Random Forest},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {451},
URL = {https://www.mdpi.com/2077-0472/10/10/451},
ISSN = {2077-0472},
ABSTRACT = {Knowing the total Nitrogen content (Nt) of forage maize (Zea mays) is important so that decisions can be made quickly and efficiently to adjust the timing and amount of both irrigation and fertilizer. In 2017 and 2018 during three growing cycles in two study plots, leaf samples were collected and the Dumas method was used to estimate Nt. During the same growing seasons and on the same sampling plots, a Parrot Sequoia camera mounted on an unmanned aerial vehicle (UAV) was used to collect high resolution images of forage maize study plots. Thirteen multispectral indices were generated and, from these, a Random Forest (RF) algorithm was used to estimate Nt. RF is a machine-learning technique and is designed to work with extremely large datasets. Overall analysis showed five of the 13 indices as the most important. One of these five, the Transformed Chlorophyll Absorption in Reflectance Index/Optimized Soil-Adjusted Vegetation Index, was found to be the most important for estimation of Nt in forage maize (R2 = 0.76). RF handled the complex dataset in a time-efficient manner and Nt did not differ significantly when compared between traditional methods of evaluating Nt at the canopy level and using UAVs and RF to estimate Nt in forage maize. This result is an opportunity to explore many new research options in precision farming and digital agriculture.},
DOI = {10.3390/agriculture10100451}
}



@Article{s20195630,
AUTHOR = {Xie, Jingyi and Peng, Xiaodong and Wang, Haijiao and Niu, Wenlong and Zheng, Xiao},
TITLE = {UAV Autonomous Tracking and Landing Based on Deep Reinforcement Learning Strategy},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5630},
URL = {https://www.mdpi.com/1424-8220/20/19/5630},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicle (UAV) autonomous tracking and landing is playing an increasingly important role in military and civil applications. In particular, machine learning has been successfully introduced to robotics-related tasks. A novel UAV autonomous tracking and landing approach based on a deep reinforcement learning strategy is presented in this paper, with the aim of dealing with the UAV motion control problem in an unpredictable and harsh environment. Instead of building a prior model and inferring the landing actions based on heuristic rules, a model-free method based on a partially observable Markov decision process (POMDP) is proposed. In the POMDP model, the UAV automatically learns the landing maneuver by an end-to-end neural network, which combines the Deep Deterministic Policy Gradients (DDPG) algorithm and heuristic rules. A Modular Open Robots Simulation Engine (MORSE)-based reinforcement learning framework is designed and validated with a continuous UAV tracking and landing task on a randomly moving platform in high sensor noise and intermittent measurements. The simulation results show that when the moving platform is moving in different trajectories, the average landing success rate of the proposed algorithm is about 10% higher than that of the Proportional-Integral-Derivative (PID) method. As an indirect result, a state-of-the-art deep reinforcement learning-based UAV control method is validated, where the UAV can learn the optimal strategy of a continuously autonomous landing and perform properly in a simulation environment.},
DOI = {10.3390/s20195630}
}



@Article{rs12193237,
AUTHOR = {Osco, Lucas Prado and Junior, José Marcato and Ramos, Ana Paula Marques and Furuya, Danielle Elis Garcia and Santana, Dthenifer Cordeiro and Teodoro, Larissa Pereira Ribeiro and Gonçalves, Wesley Nunes and Baio, Fábio Henrique Rojo and Pistori, Hemerson and Junior, Carlos Antonio da Silva and Teodoro, Paulo Eduardo},
TITLE = {Leaf Nitrogen Concentration and Plant Height Prediction for Maize Using UAV-Based Multispectral Imagery and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3237},
URL = {https://www.mdpi.com/2072-4292/12/19/3237},
ISSN = {2072-4292},
ABSTRACT = {Under ideal conditions of nitrogen (N), maize (Zea mays L.) can grow to its full potential, reaching maximum plant height (PH). As a rapid and nondestructive approach, the analysis of unmanned aerial vehicles (UAV)-based imagery may be of assistance to estimate N and height. The main objective of this study is to present an approach to predict leaf nitrogen concentration (LNC, g kg&minus;1) and PH (m) with machine learning techniques and UAV-based multispectral imagery in maize plants. An experiment with 11 maize cultivars under two rates of N fertilization was carried during the 2017/2018 and 2018/2019 crop seasons. The spectral vegetation indices (VI) normalized difference vegetation index (NDVI), normalized difference red-edge index (NDRE), green normalized difference vegetation (GNDVI), and the soil adjusted vegetation index (SAVI) were extracted from the images and, in a computational system, used alongside the spectral bands as input parameters for different machine learning models. A randomized 10-fold cross-validation strategy, with a total of 100 replicates, was used to evaluate the performance of 9 supervised machine learning (ML) models using the Pearson&rsquo;s correlation coefficient (r), mean absolute error (MAE), coefficient of regression (R&sup2;), and root mean square error (RMSE) metrics. The results indicated that the random forest (RF) algorithm performed better, with r and RMSE, respectively, of 0.91 and 1.9 g.kg&minus;&sup1; for LNC, and 0.86 and 0.17 m for PH. It was also demonstrated that VIs contributed more to the algorithm&rsquo;s performances than individual spectral bands. This study concludes that the RF model is appropriate to predict both agronomic variables in maize and may help farmers to monitor their plants based upon their LNC and PH diagnosis and use this knowledge to improve their production rates in the subsequent seasons.},
DOI = {10.3390/rs12193237}
}



@Article{rs12203396,
AUTHOR = {Colorado, Julian D. and Cera-Bornacelli, Natalia and Caldas, Juan S. and Petro, Eliel and Rebolledo, Maria C. and Cuellar, David and Calderon, Francisco and Mondragon, Ivan F. and Jaramillo-Botero, Andres},
TITLE = {Estimation of Nitrogen in Rice Crops from UAV-Captured Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3396},
URL = {https://www.mdpi.com/2072-4292/12/20/3396},
ISSN = {2072-4292},
ABSTRACT = {Leaf nitrogen (N) directly correlates to chlorophyll production, affecting crop growth and yield. Farmers use soil plant analysis development (SPAD) devices to calculate the amount of chlorophyll present in plants. However, monitoring large-scale crops using SPAD is prohibitively time-consuming and demanding. This paper presents an unmanned aerial vehicle (UAV) solution for estimating leaf N content in rice crops, from multispectral imagery. Our contribution is twofold: (i) a novel trajectory control strategy to reduce the angular wind-induced perturbations that affect image sampling accuracy during UAV flight, and (ii) machine learning models to estimate the canopy N via vegetation indices (VIs) obtained from the aerial imagery. This approach integrates an image processing algorithm using the GrabCut segmentation method with a guided filtering refinement process, to calculate the VIs according to the plots of interest. Three machine learning methods based on multivariable linear regressions (MLR), support vector machines (SVM), and neural networks (NN), were applied and compared through the entire phonological cycle of the crop: vegetative (V), reproductive (R), and ripening (Ri). Correlations were obtained by comparing our methods against an assembled ground-truth of SPAD measurements. The higher N correlations were achieved with NN: 0.98 (V), 0.94 (R), and 0.89 (Ri). We claim that the proposed UAV stabilization control algorithm significantly improves on the N-to-SPAD correlations by minimizing wind perturbations in real-time and reducing the need for offline image corrections.},
DOI = {10.3390/rs12203396}
}



@Article{rs12213515,
AUTHOR = {Moghimi, Ali and Pourreza, Alireza and Zuniga-Ramirez, German and Williams, Larry E. and Fidelibus, Matthew W.},
TITLE = {A Novel Machine Learning Approach to Estimate Grapevine Leaf Nitrogen Concentration Using Aerial Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3515},
URL = {https://www.mdpi.com/2072-4292/12/21/3515},
ISSN = {2072-4292},
ABSTRACT = {Assessment of the nitrogen status of grapevines with high spatial, temporal resolution offers benefits in fertilizer use efficiency, crop yield and quality, and vineyard uniformity. The primary objective of this study was to develop a robust predictive model for grapevine nitrogen estimation at bloom stage using high-resolution multispectral images captured by an unmanned aerial vehicle (UAV). Aerial imagery and leaf tissue sampling were conducted from 150 grapevines subjected to five rates of nitrogen applications. Subsequent to appropriate pre-processing steps, pixels representing the canopy were segmented from the background per each vine. First, we defined a binary classification problem using pixels of three vines with the minimum (low-N class) and two vines with the maximum (high-N class) nitrogen concentration. Following optimized hyperparameters configuration, we trained five machine learning classifiers, including support vector machine (SVM), random forest, XGBoost, quadratic discriminant analysis (QDA), and deep neural network (DNN) with fully-connected layers. Among the classifiers, SVM offered the highest F1-score (82.24%) on the test dataset at the cost of a very long training time compared to the other classifiers. Alternatively, QDA and XGBoost required the minimum training time with promising F1-score of 80.85% and 80.27%, respectively. Second, we transformed the classification into a regression problem by averaging the posterior probability of high-N class for all pixels within each of 150 vines. XGBoost exhibited a slightly larger coefficient of determination (R2 = 0.56) and lower root mean square error (RMSE) (0.23%) compared to other learning methods in the prediction of nitrogen concentration of all vines. The proposed approach provides values in (i) leveraging high-resolution imagery, (ii) investigating spatial distribution of nitrogen across a vine&rsquo;s canopy, and (iii) defining spatial zones for nitrogen application and smart sampling.},
DOI = {10.3390/rs12213515}
}



@Article{rs12213587,
AUTHOR = {Masjedi, Ali and Crawford, Melba M. and Carpenter, Neal R. and Tuinstra, Mitchell R.},
TITLE = {Multi-Temporal Predictive Modelling of Sorghum Biomass Using UAV-Based Hyperspectral and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3587},
URL = {https://www.mdpi.com/2072-4292/12/21/3587},
ISSN = {2072-4292},
ABSTRACT = {High-throughput phenotyping using high spatial, spectral, and temporal resolution remote sensing (RS) data has become a critical part of the plant breeding chain focused on reducing the time and cost of the selection process for the &ldquo;best&rdquo; genotypes with respect to the trait(s) of interest. In this paper, the potential of accurate and reliable sorghum biomass prediction using visible and near infrared (VNIR) and short-wave infrared (SWIR) hyperspectral data as well as light detection and ranging (LiDAR) data acquired by sensors mounted on UAV platforms is investigated. Predictive models are developed using classical regression-based machine learning methods for nine experiments conducted during the 2017 and 2018 growing seasons at the Agronomy Center for Research and Education (ACRE) at Purdue University, Indiana, USA. The impact of the regression method, data source, timing of RS and field-based biomass reference data acquisition, and the number of samples on the prediction results are investigated. R2 values for end-of-season biomass ranged from 0.64 to 0.89 for different experiments when features from all the data sources were included. Geometry-based features derived from the LiDAR point cloud to characterize plant structure and chemistry-based features extracted from hyperspectral data provided the most accurate predictions. Evaluation of the impact of the time of data acquisition during the growing season on the prediction results indicated that although the most accurate and reliable predictions of final biomass were achieved using remotely sensed data from mid-season to end-of-season, predictions in mid-season provided adequate results to differentiate between promising varieties for selection. The analysis of variance (ANOVA) of the accuracies of the predictive models showed that both the data source and regression method are important factors for a reliable prediction; however, the data source was more important with 69% significance, versus 28% significance for the regression method.},
DOI = {10.3390/rs12213587}
}



@Article{agriengineering2040035,
AUTHOR = {Barnetson, Jason and Phinn, Stuart and Scarth, Peter},
TITLE = {Estimating Plant Pasture Biomass and Quality from UAV Imaging across Queensland’s Rangelands},
JOURNAL = {AgriEngineering},
VOLUME = {2},
YEAR = {2020},
NUMBER = {4},
PAGES = {523--543},
URL = {https://www.mdpi.com/2624-7402/2/4/35},
ISSN = {2624-7402},
ABSTRACT = {The aim of this research was to test recent developments in the use of Remotely Piloted Aircraft Systems or Unmanned Aerial Vehicles (UAV)/drones to map both pasture quantity as biomass yield and pasture quality as the proportions of key pasture nutrients, across a selected range of field sites throughout the rangelands of Queensland. Improved pasture management begins with an understanding of the state of the resource base, UAV based methods can potentially achieve this at improved spatial and temporal scales. This study developed machine learning based predictive models of both pasture measures. UAV-based structure from motion photogrammetry provided a measure of yield from overlapping high resolution visible colour imagery. Pasture nutrient composition was estimated from the spectral signatures of visible near infrared hyperspectral UAV sensing. An automated pasture height surface modelling technique was developed, tested and used along with field site measurements to predict further estimates across each field site. Both prior knowledge and automated predictive modelling techniques were employed to predict yield and nutrition. Pasture height surface modelling was assessed against field measurements using a rising plate meter, results reported correlation coefficients (R2) ranging from 0.2 to 0.4 for both woodland and grassland field sites. Accuracy of the predictive modelling was determined from further field measurements of yield and on average indicated an error of 0.8 t ha&minus;1 in grasslands and 1.3 t ha&minus;1 in mixed woodlands across both modelling approaches. Correlation analyses between measures of pasture quality, acid detergent fibre and crude protein (ADF, CP), and spectral reflectance data indicated the visible red (651 nm) and red-edge (759 nm) regions were highly correlated (ADF R2 = 0.9 and CP R2 = 0.5 mean values). These findings agreed with previous studies linking specific absorption features with grass chemical composition. These results conclude that the practical application of such techniques, to efficiently and accurately map pasture yield and quality, is possible at the field site scale; however, further research is needed, in particular further field sampling of both yield and nutrient elements across such a diverse landscape, with the potential to scale up to a satellite platform for broader scale monitoring.},
DOI = {10.3390/agriengineering2040035}
}



@Article{s20226521,
AUTHOR = {Qi, Guanghui and Zhao, Gengxing and Xi, Xue},
TITLE = {Soil Salinity Inversion of Winter Wheat Areas Based on Satellite-Unmanned Aerial Vehicle-Ground Collaborative System in Coastal of the Yellow River Delta},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6521},
URL = {https://www.mdpi.com/1424-8220/20/22/6521},
ISSN = {1424-8220},
ABSTRACT = {Soil salinization is an important factor affecting winter wheat growth in coastal areas. The rapid, accurate and efficient estimation of soil salt content is of great significance for agricultural production. The Kenli area in the Yellow River Delta was taken as the research area. Three machine learning inversion models, namely, BP neural network (BPNN), support vector machine (SVM) and random forest (RF) were constructed using ground-measured data and UAV images, and the optimal model is applied to UAV images to obtain the salinity inversion result, which is used as the true salt value of the Sentinel-2A image to establish BPNN, SVM and RF collaborative inversion models, and apply the optimal model to the study area. The results showed that the RF collaborative inversion model is optimal, R2 = 0.885. The inversion results are verified by using the measured soil salt data in the study area, which is significantly better than the directly satellite remote sensing inversion method. This study integrates the advantages of multi-scale data and proposes an effective &ldquo;Satellite-UAV-Ground&rdquo; collaborative inversion method for soil salinity, so as to obtain more accurate soil information, and provide more effective technical support for agricultural production.},
DOI = {10.3390/s20226521}
}



@Article{rs12233925,
AUTHOR = {Pilaš, Ivan and Gašparović, Mateo and Novkinić, Alan and Klobučar, Damir},
TITLE = {Mapping of the Canopy Openings in Mixed Beech–Fir Forest at Sentinel-2 Subpixel Level Using UAV and Machine Learning Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3925},
URL = {https://www.mdpi.com/2072-4292/12/23/3925},
ISSN = {2072-4292},
ABSTRACT = {The presented study demonstrates a bi-sensor approach suitable for rapid and precise up-to-date mapping of forest canopy gaps for the larger spatial extent. The approach makes use of Unmanned Aerial Vehicle (UAV) red, green and blue (RGB) images on smaller areas for highly precise forest canopy mask creation. Sentinel-2 was used as a scaling platform for transferring information from the UAV to a wider spatial extent. Various approaches to an improvement in the predictive performance were examined: (I) the highest R2 of the single satellite index was 0.57, (II) the highest R2 using multiple features obtained from the single-date, S-2 image was 0.624, and (III) the highest R2 on the multitemporal set of S-2 images was 0.697. Satellite indices such as Atmospherically Resistant Vegetation Index (ARVI), Infrared Percentage Vegetation Index (IPVI), Normalized Difference Index (NDI45), Pigment-Specific Simple Ratio Index (PSSRa), Modified Chlorophyll Absorption Ratio Index (MCARI), Color Index (CI), Redness Index (RI), and Normalized Difference Turbidity Index (NDTI) were the dominant predictors in most of the Machine Learning (ML) algorithms. The more complex ML algorithms such as the Support Vector Machines (SVM), Random Forest (RF), Stochastic Gradient Boosting (GBM), Extreme Gradient Boosting (XGBoost), and Catboost that provided the best performance on the training set exhibited weaker generalization capabilities. Therefore, a simpler and more robust Elastic Net (ENET) algorithm was chosen for the final map creation.},
DOI = {10.3390/rs12233925}
}



@Article{ijgi9120728,
AUTHOR = {Zhou, Dongbo and Liu, Shuangjian and Yu, Jie and Li, Hao},
TITLE = {A High-Resolution Spatial and Time-Series Labeled Unmanned Aerial Vehicle Image Dataset for Middle-Season Rice},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {728},
URL = {https://www.mdpi.com/2220-9964/9/12/728},
ISSN = {2220-9964},
ABSTRACT = {The existing remote sensing image datasets target the identification of objects, features, or man-made targets but lack the ability to provide the date and spatial information for the same feature in the time-series images. The spatial and temporal information is important for machine learning methods so that networks can be trained to support precision classification, particularly for agricultural applications of specific crops with distinct phenological growth stages. In this paper, we built a high-resolution unmanned aerial vehicle (UAV) image dataset for middle-season rice. We scheduled the UAV data acquisition in five villages of Hubei Province for three years, including 11 or 13 growing stages in each year that were accompanied by the annual agricultural surveying business. We investigated the accuracy of the vector maps for each field block and the precise information regarding the crops in the field by surveying each village and periodically arranging the UAV flight tasks on a weekly basis during the phenological stages. Subsequently, we developed a method to generate the samples automatically. Finally, we built a high-resolution UAV image dataset, including over 500,000 samples with the location and phenological growth stage information, and employed the imagery dataset in several machine learning algorithms for classification. We performed two exams to test our dataset. First, we used four classical deep learning networks for the fine classification of spatial and temporal information. Second, we used typical models to test the land cover on our dataset and compared this with the UCMerced Land Use Dataset and RSSCN7 Dataset. The results showed that the proposed image dataset supported typical deep learning networks in the classification task to identify the location and time of middle-season rice and achieved high accuracy with the public image dataset.},
DOI = {10.3390/ijgi9120728}
}



@Article{rs12244148,
AUTHOR = {Emanuele, Pontoglio and Nives, Grasso and Andrea, Cagninei and Carlo, Camporeale and Paolo, Dabove and Andrea Maria, Lingua},
TITLE = {Bathymetric Detection of Fluvial Environments through UASs and Machine Learning Systems},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4148},
URL = {https://www.mdpi.com/2072-4292/12/24/4148},
ISSN = {2072-4292},
ABSTRACT = {In recent decades, photogrammetric and machine learning technologies have become essential for a better understanding of environmental and anthropic issues. The present work aims to respond one of the most topical problems in environmental photogrammetry, i.e., the automatic classification of dense point clouds using the machine learning (ML) technology for the refraction correction on the fluvial water table. The applied methodology for the acquisition of multiple photogrammetric flights was made through UAV drones, also in RTK configuration, for various locations along the Orco River, sited in Piedmont (Italy) and georeferenced with GNSS&mdash;RTK topographic method. The authors considered five topographic fluvial cross-sections to set the correction methodology. The automatic classification in ML has found a valid identification of different patterns (Water, Gravel bars, Vegetation, and Ground classes), in specific hydraulic and geomatic conditions. The obtained results about the automatic classification and refraction reduction led us the definition of a new procedure, with precise conditions of validity.},
DOI = {10.3390/rs12244148}
}



@Article{s20247321,
AUTHOR = {Oh, Donggeun and Han, Junghee},
TITLE = {Fisheye-Based Smart Control System for Autonomous UAV Operation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7321},
URL = {https://www.mdpi.com/1424-8220/20/24/7321},
ISSN = {1424-8220},
ABSTRACT = {Recently, as UAVs (unmanned aerial vehicles) have become smaller and higher-performance, they play a very important role in the Internet of Things (IoT). Especially, UAVs are currently used not only in military fields but also in various private sectors such as IT, agriculture, logistics, construction, etc. The range is further expected to increase. Drone-related techniques need to evolve along with this change. In particular, there is a need for the development of an autonomous system in which a drone can determine and accomplish its mission even in the absence of remote control from a GCS (Ground Control Station). Responding to such requirements, there have been various studies and algorithms developed for autonomous flight systems. Especially, many ML-based (Machine-Learning-based) methods have been proposed for autonomous path finding. Unlike other studies, the proposed mechanism could enable autonomous drone path finding over a large target area without size limitations, one of the challenges of ML-based autonomous flight or driving in the real world. Specifically, we devised Multi-Layer HVIN (Hierarchical VIN) methods that increase the area applicable to autonomous flight by overlaying multiple layers. To further improve this, we developed Fisheye HVIN, which applied an adaptive map compression ratio according to the drone’s location. We also built an autonomous flight training and verification platform. Through the proposed simulation platform, it is possible to train ML-based path planning algorithms in a realistic environment that takes into account the physical characteristics of UAV movements.},
DOI = {10.3390/s20247321}
}



@Article{robotics10010012,
AUTHOR = {Lim, Yixiang and Pongsakornsathien, Nichakorn and Gardi, Alessandro and Sabatini, Roberto and Kistan, Trevor and Ezer, Neta and Bursch, Daniel J.},
TITLE = {Adaptive Human-Robot Interactions for Multiple Unmanned Aerial Vehicles},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2218-6581/10/1/12},
ISSN = {2218-6581},
ABSTRACT = {Advances in unmanned aircraft systems (UAS) have paved the way for progressively higher levels of intelligence and autonomy, supporting new modes of operation, such as the one-to-many (OTM) concept, where a single human operator is responsible for monitoring and coordinating the tasks of multiple unmanned aerial vehicles (UAVs). This paper presents the development and evaluation of cognitive human-machine interfaces and interactions (CHMI2) supporting adaptive automation in OTM applications. A CHMI2 system comprises a network of neurophysiological sensors and machine-learning based models for inferring user cognitive states, as well as the adaptation engine containing a set of transition logics for control/display functions and discrete autonomy levels. Models of the user&rsquo;s cognitive states are trained on past performance and neurophysiological data during an offline calibration phase, and subsequently used in the online adaptation phase for real-time inference of these cognitive states. To investigate adaptive automation in OTM applications, a scenario involving bushfire detection was developed where a single human operator is responsible for tasking multiple UAV platforms to search for and localize bushfires over a wide area. We present the architecture and design of the UAS simulation environment that was developed, together with various human-machine interface (HMI) formats and functions, to evaluate the CHMI2 system&rsquo;s feasibility through human-in-the-loop (HITL) experiments. The CHMI2 module was subsequently integrated into the simulation environment, providing the sensing, inference, and adaptation capabilities needed to realise adaptive automation. HITL experiments were performed to verify the CHMI2 module&rsquo;s functionalities in the offline calibration and online adaptation phases. In particular, results from the online adaptation phase showed that the system was able to support real-time inference and human-machine interface and interaction (HMI2) adaptation. However, the accuracy of the inferred workload was variable across the different participants (with a root mean squared error (RMSE) ranging from 0.2 to 0.6), partly due to the reduced number of neurophysiological features available as real-time inputs and also due to limited training stages in the offline calibration phase. To improve the performance of the system, future work will investigate the use of alternative machine learning techniques, additional neurophysiological input features, and a more extensive training stage.},
DOI = {10.3390/robotics10010012}
}



@Article{s21020399,
AUTHOR = {Mongay Batalla, Jordi and Mavromoustakis, Constandinos X. and Mastorakis, George and Markakis, Evangelos K. and Pallis, Evangelos and Wichary, Tomasz and Krawiec, Piotr and Lekston, Przemysław},
TITLE = {On Analyzing Routing Selection for Aerial Autonomous Vehicles Connected to Mobile Network},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {399},
URL = {https://www.mdpi.com/1424-8220/21/2/399},
PubMedID = {33430000},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a two-phase algorithm for multi-criteria selection of packet forwarding in unmanned aerial vehicles (UAV), which communicate with the control station through commercial mobile network. The selection of proper data forwarding in the two radio link: From UAV to the antenna and from the antenna to the control station, are independent but subject to constrains. The proposed approach is independent of the intra-domain forwarding, so it may be useful for a number of different scenarios of Unmanned Aerial Vehicles connectivity (e.g., a swarm of drones). In the implementation developed in this paper, the connection is served by three different mobile network operators in order to ensure reliable connectivity. The proposed algorithm makes use of Machine Learning tools that are properly trained for predicting the behavior of the link connectivity during the flight duration. The results presented in the last section validate the algorithm and the training process of the machines.},
DOI = {10.3390/s21020399}
}



@Article{rs13020216,
AUTHOR = {Wang, Yutang and Wang, Jia and Chang, Shuping and Sun, Lu and An, Likun and Chen, Yuhan and Xu, Jiangqi},
TITLE = {Classification of Street Tree Species Using UAV Tilt Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {216},
URL = {https://www.mdpi.com/2072-4292/13/2/216},
ISSN = {2072-4292},
ABSTRACT = {As an important component of the urban ecosystem, street trees have made an outstanding contribution to alleviating urban environmental pollution. Accurately extracting tree characteristics and species information can facilitate the monitoring and management of street trees, as well as aiding landscaping and studies of urban ecology. In this study, we selected the suburban areas of Beijing and Zhangjiakou and investigated six representative street tree species using unmanned aerial vehicle (UAV) tilt photogrammetry. We extracted five tree attributes and four combined attribute parameters and used four types of commonly-used machine learning classification algorithms as classifiers for tree species classification. The results show that random forest (RF), support vector machine (SVM), and back propagation (BP) neural network provide better classification results when using combined parameters for tree species classification, compared with those using individual tree attributes alone; however, the K-nearest neighbor (KNN) algorithm produced the opposite results. The best combination for classification is the BP neural network using combined attributes, with a classification precision of 89.1% and F-measure of 0.872, and we conclude that this approach best meets the requirements of street tree surveys. The results also demonstrate that optical UAV tilt photogrammetry combined with a machine learning classification algorithm is a low-cost, high-efficiency, and high-precision method for tree species classification.},
DOI = {10.3390/rs13020216}
}



@Article{rs13030352,
AUTHOR = {Neuville, Romain and Bates, Jordan Steven and Jonard, François},
TITLE = {Estimating Forest Structure from UAV-Mounted LiDAR Point Cloud Using Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {352},
URL = {https://www.mdpi.com/2072-4292/13/3/352},
ISSN = {2072-4292},
ABSTRACT = {Monitoring the structure of forest stands is of high importance for forest managers to help them in maintaining ecosystem services. For that purpose, Unmanned Aerial Vehicles (UAVs) open new prospects, especially in combination with Light Detection and Ranging (LiDAR) technology. Indeed, the shorter distance from the Earth&rsquo;s surface significantly increases the point density beneath the canopy, thus offering new possibilities for the extraction of the underlying semantics. For example, tree stems can now be captured with sufficient detail, which is a gateway to accurately locating trees and directly retrieving metrics&mdash;e.g., the Diameter at Breast Height (DBH). Current practices usually require numerous site-specific parameters, which may preclude their use when applied beyond their initial application context. To overcome this shortcoming, the machine learning Hierarchical Density-Based Spatial Clustering of Application of Noise (HDBSCAN) clustering algorithm was further improved and implemented to segment tree stems. Afterwards, Principal Component Analysis (PCA) was applied to extract tree stem orientation for subsequent DBH estimation. This workflow was then validated using LiDAR point clouds collected in a temperate deciduous closed-canopy forest stand during the leaf-on and leaf-off seasons, along with multiple scanning angle ranges. The results show that the proposed methodology can correctly detect up to 82% of tree stems (with a precision of 98%) during the leaf-off season and have a Maximum Scanning Angle Range (MSAR) of 75 degrees, without having to set up any site-specific parameters for the segmentation procedure. In the future, our method could then minimize the omission and commission errors when initially detecting trees, along with assisting further tree metrics retrieval. Finally, this research shows that, under the study conditions, the point density within an approximately 1.3-meter height above the ground remains low within closed-canopy forest stands even during the leaf-off season, thus restricting the accurate estimation of the DBH. As a result, autonomous UAVs that can both fly above and under the canopy provide a clear opportunity to achieve this purpose.},
DOI = {10.3390/rs13030352}
}



@Article{rs13030457,
AUTHOR = {Zhou, Xixuan and Yang, Liao and Wang, Weisheng and Chen, Baili},
TITLE = {UAV Data as an Alternative to Field Sampling to Monitor Vineyards Using Machine Learning Based on UAV/Sentinel-2 Data Fusion},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {457},
URL = {https://www.mdpi.com/2072-4292/13/3/457},
ISSN = {2072-4292},
ABSTRACT = {Pests and diseases affect the yield and quality of grapes directly and engender noteworthy economic losses. Diagnosing &ldquo;lesions&rdquo; on vines as soon as possible and dynamically monitoring symptoms caused by pests and diseases at a larger scale are essential to pest control. This study has appraised the capabilities of high-resolution unmanned aerial vehicle (UAV) data as an alternative to manual field sampling to obtain sampling canopy sets and to supplement satellite-based monitoring using machine learning models including partial least squared regression (PLSR), support vector regression (SVR), random forest regression (RFR), and extreme learning regression (ELR) with a new activation function. UAV data were acquired from two flights in Turpan to determine disease severity (DS) and disease incidence (DI) and compared with field visual assessments. The UAV-derived canopy structure including canopy height (CH) and vegetation fraction cover (VFC), as well as satellite-based spectral features calculated from Sentinel-2A/B data were analyzed to evaluate the potential of UAV data to replace manual sampling data and predict DI. It was found that SVR slightly outperformed the other methods with a root mean square error (RMSE) of 1.89%. Moreover, the combination of canopy structure (CS) and vegetation index (VIs) improved prediction accuracy compared with single-type features (RMSEcs of 2.86% and RMSEVIs of 1.93%). This study tested the ability of UAV sampling to replace manual sampling on a large scale and introduced opportunities and challenges of fusing different features to monitor vineyards using machine learning. Within this framework, disease incidence can be estimated efficiently and accurately for larger area monitoring operation.},
DOI = {10.3390/rs13030457}
}



@Article{computation9020012,
AUTHOR = {Maltezos, Evangelos and Douklias, Athanasios and Dadoukis, Aris and Misichroni, Fay and Karagiannidis, Lazaros and Antonopoulos, Markos and Voulgary, Katerina and Ouzounoglou, Eleftherios and Amditis, Angelos},
TITLE = {The INUS Platform: A Modular Solution for Object Detection and Tracking from UAVs and Terrestrial Surveillance Assets},
JOURNAL = {Computation},
VOLUME = {9},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2079-3197/9/2/12},
ISSN = {2079-3197},
ABSTRACT = {Situational awareness is a critical aspect of the decision-making process in emergency response and civil protection and requires the availability of up-to-date information on the current situation. In this context, the related research should not only encompass developing innovative single solutions for (real-time) data collection, but also on the aspect of transforming data into information so that the latter can be considered as a basis for action and decision making. Unmanned systems (UxV) as data acquisition platforms and autonomous or semi-autonomous measurement instruments have become attractive for many applications in emergency operations. This paper proposes a multipurpose situational awareness platform by exploiting advanced on-board processing capabilities and efficient computer vision, image processing, and machine learning techniques. The main pillars of the proposed platform are: (1) a modular architecture that exploits unmanned aerial vehicle (UAV) and terrestrial assets; (2) deployment of on-board data capturing and processing; (3) provision of geolocalized object detection and tracking events; and (4) a user-friendly operational interface for standalone deployment and seamless integration with external systems. Experimental results are provided using RGB and thermal video datasets and applying novel object detection and tracking algorithms. The results show the utility and the potential of the proposed platform, and future directions for extension and optimization are presented.},
DOI = {10.3390/computation9020012}
}



@Article{rs13050858,
AUTHOR = {Koh, Joshua C.O. and Spangenberg, German and Kant, Surya},
TITLE = {Automated Machine Learning for High-Throughput Image-Based Plant Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {858},
URL = {https://www.mdpi.com/2072-4292/13/5/858},
ISSN = {2072-4292},
ABSTRACT = {Automated machine learning (AutoML) has been heralded as the next wave in artificial intelligence with its promise to deliver high-performance end-to-end machine learning pipelines with minimal effort from the user. However, despite AutoML showing great promise for computer vision tasks, to the best of our knowledge, no study has used AutoML for image-based plant phenotyping. To address this gap in knowledge, we examined the application of AutoML for image-based plant phenotyping using wheat lodging assessment with unmanned aerial vehicle (UAV) imagery as an example. The performance of an open-source AutoML framework, AutoKeras, in image classification and regression tasks was compared to transfer learning using modern convolutional neural network (CNN) architectures. For image classification, which classified plot images as lodged or non-lodged, transfer learning with Xception and DenseNet-201 achieved the best classification accuracy of 93.2%, whereas AutoKeras had a 92.4% accuracy. For image regression, which predicted lodging scores from plot images, transfer learning with DenseNet-201 had the best performance (R2 = 0.8303, root mean-squared error (RMSE) = 9.55, mean absolute error (MAE) = 7.03, mean absolute percentage error (MAPE) = 12.54%), followed closely by AutoKeras (R2 = 0.8273, RMSE = 10.65, MAE = 8.24, MAPE = 13.87%). In both tasks, AutoKeras models had up to 40-fold faster inference times compared to the pretrained CNNs. AutoML has significant potential to enhance plant phenotyping capabilities applicable in crop breeding and precision agriculture.},
DOI = {10.3390/rs13050858}
}



@Article{app11052185,
AUTHOR = {Nakama, Justin and Parada, Ricky and Matos-Carvalho, João P. and Azevedo, Fábio and Pedro, Dário and Campos, Luís},
TITLE = {Autonomous Environment Generator for UAV-Based Simulation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2185},
URL = {https://www.mdpi.com/2076-3417/11/5/2185},
ISSN = {2076-3417},
ABSTRACT = {The increased demand for Unmanned Aerial Vehicles (UAV) has also led to higher demand for realistic and efficient UAV testing environments. The current use of simulated environments has been shown to be a relatively inexpensive, safe, and repeatable way to evaluate UAVs before real-world use. However, the use of generic environments and manually-created custom scenarios leaves more to be desired. In this paper, we propose a new testbed that utilizes machine learning algorithms to procedurally generate, scale, and place 3D models to create a realistic environment. These environments are additionally based on satellite images, thus providing users with a more robust example of real-world UAV deployment. Although certain graphical improvements could be made, this paper serves as a proof of concept for an novel autonomous and relatively-large scale environment generator. Such a testbed could allow for preliminary operational planning and testing worldwide, without the need for on-site evaluation or data collection in the future.},
DOI = {10.3390/app11052185}
}



@Article{rs13050937,
AUTHOR = {Najafi, Payam and Feizizadeh, Bakhtiar and Navid, Hossein},
TITLE = {A Comparative Approach of Fuzzy Object Based Image Analysis and Machine Learning Techniques Which Are Applied to Crop Residue Cover Mapping by Using Sentinel-2 Satellite and UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {937},
URL = {https://www.mdpi.com/2072-4292/13/5/937},
ISSN = {2072-4292},
ABSTRACT = {Conservation tillage methods through leaving the crop residue cover (CRC) on the soil surface protect it from water and wind erosions. Hence, the percentage of the CRC on the soil surface is very critical for the evaluation of tillage intensity. The objective of this study was to develop a new methodology based on the semiautomated fuzzy object based image analysis (fuzzy OBIA) and compare its efficiency with two machine learning algorithms which include: support vector machine (SVM) and artificial neural network (ANN) for the evaluation of the previous CRC and tillage intensity. We also considered the spectral images from two remotely sensed platforms of the unmanned aerial vehicle (UAV) and Sentinel-2 satellite, respectively. The results indicated that fuzzy OBIA for multispectral Sentinel-2 image based on Gaussian membership function with overall accuracy and Cohen’s kappa of 0.920 and 0.874, respectively, surpassed machine learning algorithms and represented the useful results for the classification of tillage intensity. The results also indicated that overall accuracy and Cohen’s kappa for the classification of RGB images from the UAV using fuzzy OBIA method were 0.860 and 0.779, respectively. The semiautomated fuzzy OBIA clearly outperformed machine learning approaches in estimating the CRC and the classification of the tillage methods and also it has the potential to substitute or complement field techniques.},
DOI = {10.3390/rs13050937}
}



@Article{s21051766,
AUTHOR = {Müezzinoğlu, Taha and Karaköse, Mehmet},
TITLE = {An Intelligent Human–Unmanned Aerial Vehicle Interaction Approach in Real Time Based on Machine Learning Using Wearable Gloves},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1766},
URL = {https://www.mdpi.com/1424-8220/21/5/1766},
PubMedID = {33806388},
ISSN = {1424-8220},
ABSTRACT = {The interactions between humans and unmanned aerial vehicles (UAVs), whose applications are increasing in the civilian field rather than for military purposes, are a popular future research area. Human–UAV interactions are a challenging problem because UAVs move in a three-dimensional space. In this paper, we present an intelligent human–UAV interaction approach in real time based on machine learning using wearable gloves. The proposed approach offers scientific contributions such as a multi-mode command structure, machine-learning-based recognition, task scheduling algorithms, real-time usage, robust and effective use, and high accuracy rates. For this purpose, two wearable smart gloves working in real time were designed. The signal data obtained from the gloves were processed with machine-learning-based methods and classified multi-mode commands were included in the human–UAV interaction process via the interface according to the task scheduling algorithm to facilitate sequential and fast operation. The performance of the proposed approach was verified on a data set created using 25 different hand gestures from 20 different people. In a test using the proposed approach on 49,000 datapoints, process time performance of a few milliseconds was achieved with approximately 98 percent accuracy.},
DOI = {10.3390/s21051766}
}



@Article{s21061947,
AUTHOR = {Nemer, Ibrahim and Sheltami, Tarek and Ahmad, Irfan and Yasar, Ansar Ul-Haque and Abdeen, Mohammad A. R.},
TITLE = {RF-Based UAV Detection and Identification Using Hierarchical Learning Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1947},
URL = {https://www.mdpi.com/1424-8220/21/6/1947},
PubMedID = {33802189},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are widely available in the current market to be used either for recreation as a hobby or to serve specific industrial requirements, such as agriculture and construction. However, illegitimate and criminal usage of UAVs is also on the rise which introduces their effective identification and detection as a research challenge. This paper proposes a novel machine learning-based for efficient identification and detection of UAVs. Specifically, an improved UAV identification and detection approach is presented using an ensemble learning based on the hierarchical concept, along with pre-processing and feature extraction stages for the Radio Frequency (RF) data. Filtering is applied on the RF signals in the detection approach to improve the output. This approach consists of four classifiers and they are working in a hierarchical way. The sample will pass the first classifier to check the availability of the UAV, and then it will specify the type of the detected UAV using the second classifier. The last two classifiers will handle the sample that is related to Bebop and AR to specify their mode. Evaluation of the proposed approach with publicly available dataset demonstrates better efficiency compared to existing detection systems in the literature. It has the ability to investigate whether a UAV is flying within the area or not, and it can directly identify the type of UAV and then the flight mode of the detected UAV with accuracy around 99%.},
DOI = {10.3390/s21061947}
}



@Article{aerospace8030079,
AUTHOR = {Swinney, Carolyn J. and Woods, John C.},
TITLE = {Unmanned Aerial Vehicle Operating Mode Classification Using Deep Residual Learning Feature Extraction},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {79},
URL = {https://www.mdpi.com/2226-4310/8/3/79},
ISSN = {2226-4310},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) undoubtedly pose many security challenges. We need only look to the December 2018 Gatwick Airport incident for an example of the disruption UAVs can cause. In total, 1000 flights were grounded for 36 h over the Christmas period which was estimated to cost over 50 million pounds. In this paper, we introduce a novel approach which considers UAV detection as an imagery classification problem. We consider signal representations Power Spectral Density (PSD); Spectrogram, Histogram and raw IQ constellation as graphical images presented to a deep Convolution Neural Network (CNN) ResNet50 for feature extraction. Pre-trained on ImageNet, transfer learning is utilised to mitigate the requirement for a large signal dataset. We evaluate performance through machine learning classifier Logistic Regression. Three popular UAVs are classified in different modes; switched on; hovering; flying; flying with video; and no UAV present, creating a total of 10 classes. Our results, validated with 5-fold cross validation and an independent dataset, show PSD representation to produce over 91% accuracy for 10 classifications. Our paper treats UAV detection as an imagery classification problem by presenting signal representations as images to a ResNet50, utilising the benefits of transfer learning and outperforming previous work in the field.},
DOI = {10.3390/aerospace8030079}
}



@Article{rs13071239,
AUTHOR = {Oddi, Ludovica and Cremonese, Edoardo and Ascari, Lorenzo and Filippa, Gianluca and Galvagno, Marta and Serafino, Davide and Cella, Umberto Morra di},
TITLE = {Using UAV Imagery to Detect and Map Woody Species Encroachment in a Subalpine Grassland: Advantages and Limits},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1239},
URL = {https://www.mdpi.com/2072-4292/13/7/1239},
ISSN = {2072-4292},
ABSTRACT = {Woody species encroachment on grassland ecosystems is occurring worldwide with both negative and positive consequences for biodiversity conservation and ecosystem services. Remote sensing and image analysis represent useful tools for the monitoring of this process. In this paper, we aimed at evaluating quantitatively the potential of using high-resolution UAV imagery to monitor the encroachment process during its early development and at comparing the performance of manual and semi-automatic classification methods. The RGB images of an abandoned subalpine grassland on the Western Italian Alps were acquired by drone and then classified through manual photo-interpretation, with both pixel- and object-based semi-automatic models, using machine-learning algorithms. The classification techniques were applied at different resolution levels and tested for their accuracy against reference data including measurements of tree dimensions collected in the field. Results showed that the most accurate method was the photo-interpretation (≈99%), followed by the pixel-based approach (≈86%) that was faster than the manual technique and more accurate than the object-based one (≈78%). The dimensional threshold for juvenile tree detection was lower for the photo-interpretation but comparable to the pixel-based one. Therefore, for the encroachment mapping at its early stages, the pixel-based approach proved to be a promising and pragmatic choice.},
DOI = {10.3390/rs13071239}
}



@Article{electronics10070771,
AUTHOR = {Liu, Chuanyang and Wu, Yiquan and Liu, Jingjing and Sun, Zuo},
TITLE = {Improved YOLOv3 Network for Insulator Detection in Aerial Images with Diverse Background Interference},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {771},
URL = {https://www.mdpi.com/2079-9292/10/7/771},
ISSN = {2079-9292},
ABSTRACT = {Automatic inspection of insulators from high-voltage transmission lines is of paramount importance to the safety and reliable operation of the power grid. Due to different size insulators and the complex background of aerial images, it is a difficult task to recognize insulators in aerial views. Most of the traditional image processing methods and machine learning methods cannot achieve sufficient performance for insulator detection when diverse background interference is present. In this study, a deep learning method—based on You Only Look Once (YOLO)—will be proposed, capable of detecting insulators from aerial images with complex backgrounds. Firstly, aerial images with common aerial scenes were collected by Unmanned Aerial Vehicle (UAV), and a novel insulator dataset was constructed. Secondly, to enhance feature reuse and propagation, on the basis of YOLOv3 and Dense-Blocks, the YOLOv3-dense network was utilized for insulator detection. To improve detection accuracy for different sized insulators, a structure of multiscale feature fusion was adapted to the YOLOv3-dense network. To obtain abundant semantic information of upper and lower layers, multilevel feature mapping modules were employed across the YOLOv3-dense network. Finally, the YOLOv3-dense network and compared networks were trained and tested on the testing set. The average precision of YOLOv3-dense, YOLOv3, and YOLOv2 were 94.47%, 90.31%, and 83.43%, respectively. Experimental results and analysis validate the claim that the proposed YOLOv3-dense network achieves good performance in the detection of different size insulators amid diverse background interference.},
DOI = {10.3390/electronics10070771}
}



@Article{rs13081411,
AUTHOR = {Zhang, Yanchao and Yang, Wen and Sun, Ying and Chang, Christine and Yu, Jiya and Zhang, Wenbo},
TITLE = {Fusion of Multispectral Aerial Imagery and Vegetation Indices for Machine Learning-Based Ground Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1411},
URL = {https://www.mdpi.com/2072-4292/13/8/1411},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are emerging and promising platforms for carrying different types of cameras for remote sensing. The application of multispectral vegetation indices for ground cover classification has been widely adopted and has proved its reliability. However, the fusion of spectral bands and vegetation indices for machine learning-based land surface investigation has hardly been studied. In this paper, we studied the fusion of spectral bands information from UAV multispectral images and derived vegetation indices for almond plantation classification using several machine learning methods. We acquired multispectral images over an almond plantation using a UAV. First, a multispectral orthoimage was generated from the acquired multispectral images using SfM (Structure from Motion) photogrammetry methods. Eleven types of vegetation indexes were proposed based on the multispectral orthoimage. Then, 593 data points that contained multispectral bands and vegetation indexes were randomly collected and prepared for this study. After comparing six machine learning algorithms (Support Vector Machine, K-Nearest Neighbor, Linear Discrimination Analysis, Decision Tree, Random Forest, and Gradient Boosting), we selected three (SVM, KNN, and LDA) to study the fusion of multi-spectral bands information and derived vegetation index for classification. With the vegetation indexes increased, the model classification accuracy of all three selected machine learning methods gradually increased, then dropped. Our results revealed that that: (1) spectral information from multispectral images can be used for machine learning-based ground classification, and among all methods, SVM had the best performance; (2) combination of multispectral bands and vegetation indexes can improve the classification accuracy comparing to only spectral bands among all three selected methods; (3) among all VIs, NDEGE, NDVIG, and NDVGE had consistent performance in improving classification accuracies, and others may reduce the accuracy. Machine learning methods (SVM, KNN, and LDA) can be used for classifying almond plantation using multispectral orthoimages, and fusion of multispectral bands with vegetation indexes can improve machine learning-based classification accuracy if the vegetation indexes are properly selected.},
DOI = {10.3390/rs13081411}
}



@Article{rs13081425,
AUTHOR = {Chan, Catherine and Nelson, Peter R. and Hayes, Daniel J. and Zhang, Yong-Jiang and Hall, Bruce},
TITLE = {Predicting Water Stress in Wild Blueberry Fields Using Airborne Visible and Near Infrared Imaging Spectroscopy},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1425},
URL = {https://www.mdpi.com/2072-4292/13/8/1425},
ISSN = {2072-4292},
ABSTRACT = {Water management and irrigation practices are persistent challenges for many agricultural systems, exacerbated by changing seasonal and weather patterns. The wild blueberry industry is at heightened susceptibility due to its unique growing conditions and uncultivated nature. Stress detection in agricultural fields can prompt management responses to mitigate detrimental conditions, including drought and disease. We assessed airborne spectral data accompanied by ground sampled water potential over three developmental stages of wild blueberries collected throughout the 2019 summer on two adjacent fields, one irrigated and one non-irrigated. Ground sampled leaves were collected in tandem to the hyperspectral image collection with an unoccupied aerial vehicle (UAV) and then measured for leaf water potential. Using methods in machine learning and statistical analysis, we developed models to determine irrigation status and water potential. Seven models were assessed in this study, with four used to process six hyperspectral cube images for analysis. These images were classified as irrigated or non-irrigated and estimated for water potential levels, resulting in an R2 of 0.62 and verified with a validation dataset. Further investigation relating imaging spectroscopy and water potential will be beneficial in understanding the dynamics between the two for future studies.},
DOI = {10.3390/rs13081425}
}



@Article{rs13081428,
AUTHOR = {Marang, Ian J. and Filippi, Patrick and Weaver, Tim B. and Evans, Bradley J. and Whelan, Brett M. and Bishop, Thomas F. A. and Murad, Mohammed O. F. and Al-Shammari, Dhahi and Roth, Guy},
TITLE = {Machine Learning Optimised Hyperspectral Remote Sensing Retrieves Cotton Nitrogen Status},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1428},
URL = {https://www.mdpi.com/2072-4292/13/8/1428},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imaging spectrometers mounted on unmanned aerial vehicle (UAV) can capture high spatial and spectral resolution to provide cotton crop nitrogen status for precision agriculture. The aim of this research was to explore machine learning use with hyperspectral datacubes over agricultural fields. Hyperspectral imagery was collected over a mature cotton crop, which had high spatial (~5.2 cm) and spectral (5 nm) resolution over the spectral range 475–925 nm that allowed discrimination of individual crop rows and field features as well as a continuous spectral range for calculating derivative spectra. The nominal reflectance and its derivatives clearly highlighted the different treatment blocks and were strongly related to N concentration in leaf and petiole samples, both in traditional vegetation indices (e.g., Vogelman 1, R2 = 0.8) and novel combinations of spectra (R2 = 0.85). The key hyperspectral bands identified were at the red-edge inflection point (695–715 nm). Satellite multispectral was compared against the UAV hyperspectral remote sensing’s performance by testing the ability of Sentinel MSI to predict N concentration using the bands in VIS-NIR spectral region. The Sentinel 2A Green band (B3; mid-point 559.8 nm) explained the same amount of variation in N as the hyperspectral data and more than the Sentinel Red Edge Point 1 (B5; mid-point 704.9 nm) with the lower 10 m resolution Green band reporting an R2 = 0.85, compared with the R2 = 0.78 of downscaled Sentinel Red Edge Point 1 at 5 m. The remaining Sentinel bands explained much lower variation (maximum was NIR at R2 = 0.48). Investigation of the red edge peak region in the first derivative showed strong promise with RIDAmid (R2 = 0.81) being the best index. The machine learning approach narrowed the range of bands required to investigate plant condition over this trial site, greatly improved processing time and reduced processing complexity. While Sentinel performed well in this comparison and would be useful in a broadacre crop production context, the impact of pixel boundaries relative to a region of interest and coarse spatial and temporal resolution impacts its utility in a research capacity.},
DOI = {10.3390/rs13081428}
}



@Article{rs13081534,
AUTHOR = {Zhang, Fan and Hu, Zhenqi and Yang, Kun and Fu, Yaokun and Feng, Zewei and Bai, Mingbo},
TITLE = {The Surface Crack Extraction Method Based on Machine Learning of Image and Quantitative Feature Information Acquisition Method},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1534},
URL = {https://www.mdpi.com/2072-4292/13/8/1534},
ISSN = {2072-4292},
ABSTRACT = {In order to effectively control the damage caused by surface cracks to a geological environment, we need to find a convenient, efficient, and accurate method to obtain crack information. The existing crack extraction methods based on unmanned air vehicle (UAV) images inevitably have some erroneous pixels because of the complexity of background information. At the same time, there are few researches on crack feature information. In view of this, this article proposes a surface crack extraction method based on machine learning of UAV images, the data preprocessing steps, and the content and calculation methods for crack feature information: length, width, direction, location, fractal dimension, number, crack rate, and dispersion rate. The results show that the method in this article can effectively avoid the interference by vegetation and soil crust. By introducing the concept of dispersion rate, the method combining crack rate and dispersion rate can describe the distribution characteristics of regional cracks more clearly. Compared to field survey data, the calculation result of the crack feature information in this article is close to the true value, which proves that this is a reliable method for obtaining quantitative crack feature information.},
DOI = {10.3390/rs13081534}
}



@Article{agriculture11050387,
AUTHOR = {Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu, Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A. and Moore, Steven and Rahman, Sk Mostafizur},
TITLE = {Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {387},
URL = {https://www.mdpi.com/2077-0472/11/5/387},
ISSN = {2077-0472},
ABSTRACT = {This paper explores the potential of machine learning algorithms for weed and crop classification from UAV images. The identification of weeds in crops is a challenging task that has been addressed through orthomosaicing of images, feature extraction and labelling of images to train machine learning algorithms. In this paper, the performances of several machine learning algorithms, random forest (RF), support vector machine (SVM) and k-nearest neighbours (KNN), are analysed to detect weeds using UAV images collected from a chilli crop field located in Australia. The evaluation metrics used in the comparison of performance were accuracy, precision, recall, false positive rate and kappa coefficient. MATLAB is used for simulating the machine learning algorithms; and the achieved weed detection accuracies are 96% using RF, 94% using SVM and 63% using KNN. Based on this study, RF and SVM algorithms are efficient and practical to use, and can be implemented easily for detecting weed from UAV images.},
DOI = {10.3390/agriculture11050387}
}



@Article{drones5020031,
AUTHOR = {Song, Bonggeun and Park, Kyunghun},
TITLE = {Comparison of Outdoor Compost Pile Detection Using Unmanned Aerial Vehicle Images and Various Machine Learning Techniques},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {31},
URL = {https://www.mdpi.com/2504-446X/5/2/31},
ISSN = {2504-446X},
ABSTRACT = {Since outdoor compost piles (OCPs) contain large amounts of nitrogen and phosphorus, they act as a major pollutant that deteriorates water quality, such as eutrophication and green algae, when the OCPs enter the river during rainfall. In South Korea, OCPs are frequently used, but there is a limitation that a lot of manpower and budget are consumed to investigate the current situation, so it is necessary to efficiently investigate the OCPs. This study compared the accuracy of various machine learning techniques for the efficient detection and management of outdoor compost piles (OCPs), a non-point pollution source in agricultural areas in South Korea, using unmanned aerial vehicle (UAV) images. RGB, multispectral, and thermal infrared UAV images were taken in August and October 2019. Additionally, vegetation indices (NDVI, NDRE, ENDVI, and GNDVI) and surface temperature were also considered. Four machine learning techniques, including support vector machine (SVM), decision tree (DT), random forest (RF), and k-NN, were implemented, and the machine learning technique with the highest accuracy was identified by adjusting several variables. The accuracy of all machine learning techniques was very high, reaching values of up to 0.96. Particularly, the accuracy of the RF method with the number of estimators set to 10 was highest, reaching 0.989 in August and 0.987 in October. The proposed method allows for the prediction of OCP location and area over large regions, thereby foregoing the need for OCP field measurements. Therefore, our findings provide highly useful data for the improvement of OCP management strategies and water quality.},
DOI = {10.3390/drones5020031}
}



@Article{rs13091723,
AUTHOR = {Kuzmin, Anton and Korhonen, Lauri and Kivinen, Sonja and Hurskainen, Pekka and Korpelainen, Pasi and Tanhuanpää, Topi and Maltamo, Matti and Vihervaara, Petteri and Kumpula, Timo},
TITLE = {Detection of European Aspen (Populus tremula L.) Based on an Unmanned Aerial Vehicle Approach in Boreal Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1723},
URL = {https://www.mdpi.com/2072-4292/13/9/1723},
ISSN = {2072-4292},
ABSTRACT = {European aspen (Populus tremula L.) is a keystone species for biodiversity of boreal forests. Large-diameter aspens maintain the diversity of hundreds of species, many of which are threatened in Fennoscandia. Due to a low economic value and relatively sparse and scattered occurrence of aspen in boreal forests, there is a lack of information of the spatial and temporal distribution of aspen, which hampers efficient planning and implementation of sustainable forest management practices and conservation efforts. Our objective was to assess identification of European aspen at the individual tree level in a southern boreal forest using high-resolution photogrammetric point cloud (PPC) and multispectral (MSP) orthomosaics acquired with an unmanned aerial vehicle (UAV). The structure-from-motion approach was applied to generate RGB imagery-based PPC to be used for individual tree-crown delineation. Multispectral data were collected using two UAV cameras: Parrot Sequoia and MicaSense RedEdge-M. Tree-crown outlines were obtained from watershed segmentation of PPC data and intersected with multispectral mosaics to extract and calculate spectral metrics for individual trees. We assessed the role of spectral data features extracted from PPC and multispectral mosaics and a combination of it, using a machine learning classifier—Support Vector Machine (SVM) to perform two different classifications: discrimination of aspen from the other species combined into one class and classification of all four species (aspen, birch, pine, spruce) simultaneously. In the first scenario, the highest classification accuracy of 84% (F1-score) for aspen and overall accuracy of 90.1% was achieved using only RGB features from PPC, whereas in the second scenario, the highest classification accuracy of 86 % (F1-score) for aspen and overall accuracy of 83.3% was achieved using the combination of RGB and MSP features. The proposed method provides a new possibility for the rapid assessment of aspen occurrence to enable more efficient forest management as well as contribute to biodiversity monitoring and conservation efforts in boreal forests.},
DOI = {10.3390/rs13091723}
}



@Article{rs13091748,
AUTHOR = {Abdelbaki, Asmaa and Schlerf, Martin and Retzlaff, Rebecca and Machwitz, Miriam and Verrelst, Jochem and Udelhoven, Thomas},
TITLE = {Comparison of Crop Trait Retrieval Strategies Using UAV-Based VNIR Hyperspectral Imaging},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1748},
URL = {https://www.mdpi.com/2072-4292/13/9/1748},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral cameras onboard unmanned aerial vehicles (UAVs) have recently emerged for monitoring crop traits at the sub-field scale. Different physical, statistical, and hybrid methods for crop trait retrieval have been developed. However, spectra collected from UAVs can be confounded by various issues, including illumination variation throughout the crop growing season, the effect of which on the retrieval performance is not well understood at present. In this study, four retrieval methods are compared, in terms of retrieving the leaf area index (LAI), fractional vegetation cover (fCover), and canopy chlorophyll content (CCC) of potato plants over an agricultural field for six dates during the growing season. We analyzed: (1) The standard look-up table method (LUTstd), (2) an improved (regularized) LUT method that involves variable correlation (LUTreg), (3) hybrid methods, and (4) random forest regression without (RF) and with (RFexp) the exposure time as an additional explanatory variable. The Soil–Leaf–Canopy (SLC) model was used in association with the LUT-based inversion and hybrid methods, while the statistical modelling methods (RF and RFexp) relied entirely on in situ data. The results revealed that RFexp was the best-performing method, yielding the highest accuracies, in terms of the normalized root mean square error (NRMSE), for LAI (5.36%), fCover (5.87%), and CCC (15.01%). RFexp was able to reduce the effects of illumination variability and cloud shadows. LUTreg outperformed the other two retrieval methods (hybrid methods and LUTstd), with an NRMSE of 9.18% for LAI, 10.46% for fCover, and 12.16% for CCC. Conversely, LUTreg led to lower accuracies than those derived from RF for LAI (5.51%) and for fCover (6.23%), but not for CCC (16.21%). Therefore, the machine learning approaches—in particular, RF—appear to be the most promising retrieval methods for application to UAV-based hyperspectral data.},
DOI = {10.3390/rs13091748}
}



@Article{rs13091756,
AUTHOR = {Belcore, Elena and Pittarello, Marco and Lingua, Andrea Maria and Lonati, Michele},
TITLE = {Mapping Riparian Habitats of Natura 2000 Network (91E0*, 3240) at Individual Tree Level Using UAV Multi-Temporal and Multi-Spectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1756},
URL = {https://www.mdpi.com/2072-4292/13/9/1756},
ISSN = {2072-4292},
ABSTRACT = {Riparian habitats provide a series of ecological services vital for the balance of the environment, and are niches and resources for a wide variety of species. Monitoring riparian environments at the intra-habitat level is crucial for assessing and preserving their conservation status, although it is challenging due to their landscape complexity. Unmanned aerial vehicles (UAV) and multi-spectral optical sensors can be used for very high resolution (VHR) monitoring in terms of spectral, spatial, and temporal resolutions. In this contribution, the vegetation species of the riparian habitat (91E0*, 3240 of Natura 2000 network) of North-West Italy were mapped at individual tree (ITD) level using machine learning and a multi-temporal phenology-based approach. Three UAV flights were conducted at the phenological-relevant time of the year (epochs). The data were analyzed using a structure from motion (SfM) approach. The resulting orthomosaics were segmented and classified using a random forest (RF) algorithm. The training dataset was composed of field-collected data, and was oversampled to reduce the effects of unbalancing and size. Three-hundred features were computed considering spectral, textural, and geometric information. Finally, the RF model was cross-validated (leave-one-out). This model was applied to eight scenarios that differed in temporal resolution to assess the role of multi-temporality over the UAV’s VHR optical data. Results showed better performances in multi-epoch phenology-based classification than single-epochs ones, with 0.71 overall accuracy compared to 0.61. Some classes, such as Pinus sylvestris and Betula pendula, are remarkably influenced by the phenology-based multi-temporality: the F1-score increased by 0.3 points by considering three epochs instead of two.},
DOI = {10.3390/rs13091756}
}



@Article{agronomy11050915,
AUTHOR = {Muharam, Farrah Melissa and Nurulhuda, Khairudin and Zulkafli, Zed and Tarmizi, Mohamad Arif and Abdullah, Asniyani Nur Haidar and Che Hashim, Muhamad Faiz and Mohd Zad, Siti Najja and Radhwane, Derraz and Ismail, Mohd Razi},
TITLE = {UAV- and Random-Forest-AdaBoost (RFA)-Based Estimation of Rice Plant Traits},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {915},
URL = {https://www.mdpi.com/2073-4395/11/5/915},
ISSN = {2073-4395},
ABSTRACT = {Rapid, accurate and inexpensive methods are required to analyze plant traits throughout all crop growth stages for plant phenotyping. Few studies have comprehensively evaluated plant traits from multispectral cameras onboard UAV platforms. Additionally, machine learning algorithms tend to over- or underfit data and limited attention has been paid to optimizing their performance through an ensemble learning approach. This study aims to (1) comprehensively evaluate twelve rice plant traits estimated from aerial unmanned vehicle (UAV)-based multispectral images and (2) introduce Random Forest AdaBoost (RFA) algorithms as an optimization approach for estimating plant traits. The approach was tested based on a farmer’s field in Terengganu, Malaysia, for the off-season from February to June 2018, involving five rice cultivars and three nitrogen (N) rates. Four bands, thirteen indices and Random Forest-AdaBoost (RFA) regression models were evaluated against the twelve plant traits according to the growth stages. Among the plant traits, plant height, green leaf and storage organ biomass, and foliar nitrogen (N) content were estimated well, with a coefficient of determination (R2) above 0.80. In comparing the bands and indices, red, Normalized Difference Vegetation Index (NDVI), Ratio Vegetation Index (RVI), Red-Edge Wide Dynamic Range Vegetation Index (REWDRVI) and Red-Edge Soil Adjusted Vegetation Index (RESAVI) were remarkable in estimating all plant traits at tillering, booting and milking stages with R2 values ranging from 0.80–0.99 and root mean square error (RMSE) values ranging from 0.04–0.22. Milking was found to be the best growth stage to conduct estimations of plant traits. In summary, our findings demonstrate that an ensemble learning approach can improve the accuracy as well as reduce under/overfitting in plant phenotyping algorithms.},
DOI = {10.3390/agronomy11050915}
}



@Article{f12050582,
AUTHOR = {da Silva, Ana Karina Vieira and Borges, Marcus Vinicius Vieira and Batista, Tays Silva and da Silva Junior, Carlos Antonio and Furuya, Danielle Elis Garcia and Prado Osco, Lucas and Teodoro, Larissa Pereira Ribeiro and Baio, Fábio Henrique Rojo and Ramos, Ana Paula Marques and Gonçalves, Wesley Nunes and Marcato Junior, José and Teodoro, Paulo Eduardo and Pistori, Hemerson},
TITLE = {Predicting Eucalyptus Diameter at Breast Height and Total Height with UAV-Based Spectral Indices and Machine Learning},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {582},
URL = {https://www.mdpi.com/1999-4907/12/5/582},
ISSN = {1999-4907},
ABSTRACT = {Machine learning techniques (ML) have gained attention in precision agriculture practices since they efficiently address multiple applications, like estimating the growth and yield of trees in forest plantations. The combination between ML algorithms and spectral vegetation indices (VIs) from high-spatial-resolution line measurement, segment: 0.079024 m multispectral imagery, could optimize the prediction of these biometric variables. In this paper, we investigate the performance of ML techniques and VIs acquired with an unnamed aerial vehicle (UAV) to predict the diameter at breast height (DBH) and total height (Ht) of eucalyptus trees. An experimental site with six eucalyptus species was selected, and the Parrot Sequoia sensor was used. Several ML techniques were evaluated, like random forest (RF), REPTree (DT), alternating model tree (AT,) k-nearest neighbor (KNN), support vector machine (SVM), artificial neural network (ANN), linear regression (LR), and radial basis function (RBF). Each algorithm performance was verified using the correlation coefficient (r) and the mean absolute error (MAE). We used, as input, 34 VIs as numeric variables to predict DHB and Ht. We also added to the model a categorical variable as input identifying the different eucalyptus trees species. The RF technique obtained an overall superior estimation for all the tested configurations. Still, the RBF also showed a higher performance for predicting DHB, numerically surpassing the RF both in r and MAE, in some cases. For Ht variable, the technique that obtained the smallest MAE was SVM, though in a particular test. In this regard, we conclude that a combination of ML and VIs extracted from UAV-based imagery is suitable to estimate DBH and Ht in eucalyptus species. The approach presented constitutes an interesting contribution to the inventory and management of planted forests.},
DOI = {10.3390/f12050582}
}



@Article{drones5020037,
AUTHOR = {Wei, Bingsheng and Barczyk, Martin},
TITLE = {Experimental Evaluation of Computer Vision and Machine Learning-Based UAV Detection and Ranging},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {37},
URL = {https://www.mdpi.com/2504-446X/5/2/37},
ISSN = {2504-446X},
ABSTRACT = {We consider the problem of vision-based detection and ranging of a target UAV using the video feed from a monocular camera onboard a pursuer UAV. Our previously published work in this area employed a cascade classifier algorithm to locate the target UAV, which was found to perform poorly in complex background scenes. We thus study the replacement of the cascade classifier algorithm with newer machine learning-based object detection algorithms. Five candidate algorithms are implemented and quantitatively tested in terms of their efficiency (measured as frames per second processing rate), accuracy (measured as the root mean squared error between ground truth and detected location), and consistency (measured as mean average precision) in a variety of flight patterns, backgrounds, and test conditions. Assigning relative weights of 20%, 40% and 40% to these three criteria, we find that when flying over a white background, the top three performers are YOLO v2 (76.73 out of 100), Faster RCNN v2 (63.65 out of 100), and Tiny YOLO (59.50 out of 100), while over a realistic background, the top three performers are Faster RCNN v2 (54.35 out of 100, SSD MobileNet v1 (51.68 out of 100) and SSD Inception v2 (50.72 out of 100), leading us to recommend Faster RCNN v2 as the recommended solution. We then provide a roadmap for further work in integrating the object detector into our vision-based UAV tracking system.},
DOI = {10.3390/drones5020037}
}



@Article{rs13112151,
AUTHOR = {Miranda, Alejandro and Catalán, Germán and Altamirano, Adison and Zamorano-Elgueta, Carlos and Cavieres, Manuel and Guerra, Javier and Mola-Yudego, Blas},
TITLE = {How Much Can We See from a UAV-Mounted Regular Camera? Remote Sensing-Based Estimation of Forest Attributes in South American Native Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2151},
URL = {https://www.mdpi.com/2072-4292/13/11/2151},
ISSN = {2072-4292},
ABSTRACT = {Data collection from large areas of native forests poses a challenge. The present study aims at assessing the use of UAV for forest inventory on native forests in Southern Chile, and seeks to retrieve both stand and tree level attributes from forest canopy data. Data were collected from 14 plots (45 × 45 m) established at four locations representing unmanaged Chilean temperate forests: seven plots on secondary forests and seven plots on old-growth forests, including a total of 17 different native species. The imagery was captured using a fixed-wing airframe equipped with a regular RGB camera. We used the structure from motion and digital aerial photogrammetry techniques for data processing and combined machine learning methods based on boosted regression trees and mixed models. In total, 2136 trees were measured on the ground, from which 858 trees were visualized from the UAV imagery of the canopy, ranging from 26% to 88% of the measured trees in the field (mean = 45.7%, SD = 17.3), which represented between 70.6% and 96% of the total basal area of the plots (mean = 80.28%, SD = 7.7). Individual-tree diameter models based on remote sensing data were constructed with R2 = 0.85 and R2 = 0.66 based on BRT and mixed models, respectively. We found a strong relationship between canopy and ground data; however, we suggest that the best alternative was combining the use of both field-based and remotely sensed methods to achieve high accuracy estimations, particularly in complex structure forests (e.g., old-growth forests). Field inventories and UAV surveys provide accurate information at local scales and allow validation of large-scale applications of satellite imagery. Finally, in the future, increasing the accuracy of aerial surveys and monitoring is necessary to advance the development of local and regional allometric crown and DBH equations at the species level.},
DOI = {10.3390/rs13112151}
}



@Article{rs13112169,
AUTHOR = {Lee, Seunghyeon and Song, Youngkeun and Kil, Sung-Ho},
TITLE = {Feasibility Analyses of Real-Time Detection of Wildlife Using UAV-Derived Thermal and RGB Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2169},
URL = {https://www.mdpi.com/2072-4292/13/11/2169},
ISSN = {2072-4292},
ABSTRACT = {Wildlife monitoring is carried out for diverse reasons, and monitoring methods have gradually advanced through technological development. Direct field investigations have been replaced by remote monitoring methods, and unmanned aerial vehicles (UAVs) have recently become the most important tool for wildlife monitoring. Many previous studies on detecting wild animals have used RGB images acquired from UAVs, with most of the analyses depending on machine learning–deep learning (ML–DL) methods. These methods provide relatively accurate results, and when thermal sensors are used as a supplement, even more accurate detection results can be obtained through complementation with RGB images. However, because most previous analyses were based on ML–DL methods, a lot of time was required to generate training data and train detection models. This drawback makes ML–DL methods unsuitable for real-time detection in the field. To compensate for the disadvantages of the previous methods, this paper proposes a real-time animal detection method that generates a total of six applicable input images depending on the context and uses them for detection. The proposed method is based on the Sobel edge algorithm, which is simple but can detect edges quickly based on change values. The method can detect animals in a single image without training data. The fastest detection time per image was 0.033 s, and all frames of a thermal video could be analyzed. Furthermore, because of the synchronization of the properties of the thermal and RGB images, the performance of the method was above average in comparison with previous studies. With target images acquired at heights below 100 m, the maximum detection precision and detection recall of the most accurate input image were 0.804 and 0.699, respectively. However, the low resolution of the thermal sensor and its shooting height limitation were hindrances to wildlife detection. The aim of future research will be to develop a detection method that can improve these shortcomings.},
DOI = {10.3390/rs13112169}
}



@Article{app11125410,
AUTHOR = {Zheng, Ke and Jia, Guozhu and Yang, Linchao and Wang, Jiaqing},
TITLE = {A Compound Fault Labeling and Diagnosis Method Based on Flight Data and BIT Record of UAV},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5410},
URL = {https://www.mdpi.com/2076-3417/11/12/5410},
ISSN = {2076-3417},
ABSTRACT = {In the process of Unmanned Aerial Vehicle (UAV) flight testing, plenty of compound faults exist, which could be composed of concurrent single faults or over-limit states alarmed by Built-In-Test (BIT) equipment. At present, there still lacks a suitable automatic labeling approach for UAV flight data, effectively utilizing the information of the BIT record. The performance of the originally employed flight data-driven fault diagnosis models based on machine learning needs to be improved as well. A compound fault labeling and diagnosis method based on actual flight data and the BIT record of the UAV during flight test phase is proposed, through labeling the flight data with compound fault modes corresponding to concurrent single faults recorded by the BIT system, and upgrading the original diagnosis model based on Gradient Boosting Decision Tree (GBDT) and Fully Convolutional Network (FCNN), to eXtreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and modified Convolutional Neural Network (CNN). The experimental results based on actual test flight data show that the proposed method could effectively label the flight data and obtain a significant improvement in diagnostic performance, appearing to be practical in the UAV test flight process.},
DOI = {10.3390/app11125410}
}



@Article{rs13132436,
AUTHOR = {Calamita, Federico and Imran, Hafiz Ali and Vescovo, Loris and Mekhalfi, Mohamed Lamine and La Porta, Nicola},
TITLE = {Early Identification of Root Rot Disease by Using Hyperspectral Reflectance: The Case of Pathosystem Grapevine/Armillaria},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2436},
URL = {https://www.mdpi.com/2072-4292/13/13/2436},
ISSN = {2072-4292},
ABSTRACT = {Armillaria genus represents one of the most common causes of chronic root rot disease in woody plants. Prompt recognition of diseased plants is crucial to control the pathogen. However, the current disease detection methods are limited at a field scale. Therefore, an alternative approach is needed. In this study, we investigated the potential of hyperspectral techniques to identify fungi-infected vs. healthy plants of Vitis vinifera. We used the hyperspectral imaging sensor Specim-IQ to acquire leaves’ reflectance data of the Teroldego Rotaliano grapevine cultivar. We analyzed three different groups of plants: healthy, asymptomatic, and diseased. Highly significant differences were found in the near-infrared (NIR) spectral region with a decreasing pattern from healthy to diseased plants attributable to the leaf mesophyll changes. Asymptomatic plants emerged from the other groups due to a lower reflectance in the red edge spectrum (around 705 nm), ascribable to an accumulation of secondary metabolites involved in plant defense strategies. Further significant differences were observed in the wavelengths close to 550 nm in diseased vs. asymptomatic plants. We evaluated several machine learning paradigms to differentiate the plant groups. The Naïve Bayes (NB) algorithm, combined with the most discriminant variables among vegetation indices and spectral narrow bands, provided the best results with an overall accuracy of 90% and 75% in healthy vs. diseased and healthy vs. asymptomatic plants, respectively. To our knowledge, this study represents the first report on the possibility of using hyperspectral data for root rot disease diagnosis in woody plants. Although further validation studies are required, it appears that the spectral reflectance technique, possibly implemented on unmanned aerial vehicles (UAVs), could be a promising tool for a cost-effective, non-invasive method of Armillaria disease diagnosis and mapping in-field, contributing to a significant step forward in precision viticulture.},
DOI = {10.3390/rs13132436}
}



@Article{rs13132483,
AUTHOR = {Meng, Baoping and Yang, Zhigui and Yu, Hongyan and Qin, Yu and Sun, Yi and Zhang, Jianguo and Chen, Jianjun and Wang, Zhiwei and Zhang, Wei and Li, Meng and Lv, Yanyan and Yi, Shuhua},
TITLE = {Mapping of Kobresia pygmaea Community Based on Umanned Aerial Vehicle Technology and Gaofen Remote Sensing Data in Alpine Meadow Grassland: A Case Study in Eastern of Qinghai–Tibetan Plateau},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2483},
URL = {https://www.mdpi.com/2072-4292/13/13/2483},
ISSN = {2072-4292},
ABSTRACT = {The Kobresia pygmaea (KP) community is a key succession stage of alpine meadow degradation on the Qinghai–Tibet Plateau (QTP). However, most of the grassland classification and mapping studies have been performed at the grassland type level. The spatial distribution and impact factors of KP on the QTP are still unclear. In this study, field measurements of the grassland vegetation community in the eastern part of the QTP (Counties of Zeku, Henan and Maqu) from 2015 to 2019 were acquired using unmanned aerial vehicle (UAV) technology. The machine learning algorithms for grassland vegetation community classification were constructed by combining Gaofen satellite images and topographic indices. Then, the spatial distribution of KP community was mapped. The results showed that: (1) For all field observed sites, the alpine meadow vegetation communities demonstrated a considerable spatial heterogeneity. The traditional classification methods can hardly distinguish those communities due to the high similarity of their spectral characteristics. (2) The random forest method based on the combination of satellite vegetation indices, texture feature and topographic indices exhibited the best performance in three counties, with overall accuracy and Kappa coefficient ranged from 74.06% to 83.92% and 0.65 to 0.80, respectively. (3) As a whole, the area of KP community reached 1434.07 km2, and accounted for 7.20% of the study area. We concluded that the combination of satellite remote sensing, UAV surveying and machine learning can be used for KP classification and mapping at community level.},
DOI = {10.3390/rs13132483}
}



@Article{electronics10131549,
AUTHOR = {Shrestha, Rakesh and Omidkar, Atefeh and Roudi, Sajjad Ahmadi and Abbas, Robert and Kim, Shiho},
TITLE = {Machine-Learning-Enabled Intrusion Detection System for Cellular Connected UAV Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1549},
URL = {https://www.mdpi.com/2079-9292/10/13/1549},
ISSN = {2079-9292},
ABSTRACT = {The recent development and adoption of unmanned aerial vehicles (UAVs) is due to its wide variety of applications in public and private sector from parcel delivery to wildlife conservation. The integration of UAVs, 5G, and satellite technologies has prompted telecommunication networks to evolve to provide higher-quality and more stable service to remote areas. However, security concerns with UAVs are growing as UAV nodes are becoming attractive targets for cyberattacks due to enormously growing volumes and poor and weak inbuilt security. In this paper, we propose a UAV- and satellite-based 5G-network security model that can harness machine learning to effectively detect of vulnerabilities and cyberattacks. The solution is divided into two main parts: the model creation for intrusion detection using various machine learning (ML) algorithms and the implementation of ML-based model into terrestrial or satellite gateways. The system identifies various attack types using realistic CSE-CIC IDS-2018 network datasets published by Canadian Establishment for Cybersecurity (CIC). It consists of seven different types of new and contemporary attack types. This paper demonstrates that ML algorithms can be used to classify benign or malicious packets in UAV networks to enhance security. Finally, the tested ML algorithms are compared for effectiveness in terms of accuracy rate, precision, recall, F1-score, and false-negative rate. The decision tree algorithm performed well by obtaining a maximum accuracy rate of 99.99% and a minimum false negative rate of 0% in detecting various attacks as compared to all other types of ML classifiers.},
DOI = {10.3390/electronics10131549}
}



@Article{s21134442,
AUTHOR = {Niu, Zijie and Deng, Juntao and Zhang, Xu and Zhang, Jun and Pan, Shijia and Mu, Haotian},
TITLE = {Identifying the Branch of Kiwifruit Based on Unmanned Aerial Vehicle (UAV) Images Using Deep Learning Method},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4442},
URL = {https://www.mdpi.com/1424-8220/21/13/4442},
PubMedID = {34209571},
ISSN = {1424-8220},
ABSTRACT = {It is important to obtain accurate information about kiwifruit vines to monitoring their physiological states and undertake precise orchard operations. However, because vines are small and cling to trellises, and have branches laying on the ground, numerous challenges exist in the acquisition of accurate data for kiwifruit vines. In this paper, a kiwifruit canopy distribution prediction model is proposed on the basis of low-altitude unmanned aerial vehicle (UAV) images and deep learning techniques. First, the location of the kiwifruit plants and vine distribution are extracted from high-precision images collected by UAV. The canopy gradient distribution maps with different noise reduction and distribution effects are generated by modifying the threshold and sampling size using the resampling normalization method. The results showed that the accuracies of the vine segmentation using PSPnet, support vector machine, and random forest classification were 71.2%, 85.8%, and 75.26%, respectively. However, the segmentation image obtained using depth semantic segmentation had a higher signal-to-noise ratio and was closer to the real situation. The average intersection over union of the deep semantic segmentation was more than or equal to 80% in distribution maps, whereas, in traditional machine learning, the average intersection was between 20% and 60%. This indicates the proposed model can quickly extract the vine distribution and plant position, and is thus able to perform dynamic monitoring of orchards to provide real-time operation guidance.},
DOI = {10.3390/s21134442}
}



@Article{telecom2030017,
AUTHOR = {Pourroostaei Ardakani, Saeid and Cheshmehzangi, Ali},
TITLE = {Reinforcement Learning-Enabled UAV Itinerary Planning for Remote Sensing Applications in Smart Farming},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {255--270},
URL = {https://www.mdpi.com/2673-4001/2/3/17},
ISSN = {2673-4001},
ABSTRACT = {UAV path planning for remote sensing aims to find the best-fitted routes to complete a data collection mission. UAVs plan the routes and move through them to remotely collect environmental data from particular target zones by using sensory devices such as cameras. Route planning may utilize machine learning techniques to autonomously find/select cost-effective and/or best-fitted routes and achieve optimized results including: minimized data collection delay, reduced UAV power consumption, decreased flight traversed distance and maximized number of collected data samples. This paper utilizes a reinforcement learning technique (location and energy-aware Q-learning) to plan UAV routes for remote sensing in smart farms. Through this, the UAV avoids heuristically or blindly moving throughout a farm, but this takes the benefits of environment exploration–exploitation to explore the farm and find the shortest and most cost-effective paths into target locations with interesting data samples to collect. According to the simulation results, utilizing the Q-learning technique increases data collection robustness and reduces UAV resource consumption (e.g., power), traversed paths, and remote sensing latency as compared to two well-known benchmarks, IEMF and TBID, especially if the target locations are dense and crowded in a farm.},
DOI = {10.3390/telecom2030017}
}



@Article{aerospace8070179,
AUTHOR = {Swinney, Carolyn J. and Woods, John C.},
TITLE = {The Effect of Real-World Interference on CNN Feature Extraction and Machine Learning Classification of Unmanned Aerial Systems},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {179},
URL = {https://www.mdpi.com/2226-4310/8/7/179},
ISSN = {2226-4310},
ABSTRACT = {Small unmanned aerial systems (UASs) present many potential solutions and enhancements to industry today but equally pose a significant security challenge. We only need to look at the levels of disruption caused by UASs at airports in recent years. The accuracy of UAS detection and classification systems based on radio frequency (RF) signals can be hindered by other interfering signals present in the same frequency band, such as Bluetooth and Wi-Fi devices. In this paper, we evaluate the effect of real-world interference from Bluetooth and Wi-Fi signals concurrently on convolutional neural network (CNN) feature extraction and machine learning classification of UASs. We assess multiple UASs that operate using different transmission systems: Wi-Fi, Lightbridge 2.0, OcuSync 1.0, OcuSync 2.0 and the recently released OcuSync 3.0. We consider 7 popular UASs, evaluating 2 class UAS detection, 8 class UAS type classification and 21 class UAS flight mode classification. Our results show that the process of CNN feature extraction using transfer learning and machine learning classification is fairly robust in the presence of real-world interference. We also show that UASs that are operating using the same transmission system can be distinguished. In the presence of interference from both Bluetooth and Wi-Fi signals, our results show 100% accuracy for UAV detection (2 classes), 98.1% (+/−0.4%) for UAV type classification (8 classes) and 95.4% (+/−0.3%) for UAV flight mode classification (21 classes).},
DOI = {10.3390/aerospace8070179}
}



@Article{rs13132622,
AUTHOR = {Wang, Haozhou and Duan, Yulin and Shi, Yun and Kato, Yoichiro and Ninomiya, Seishi and Guo, Wei},
TITLE = {EasyIDP: A Python Package for Intermediate Data Processing in UAV-Based Plant Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2622},
URL = {https://www.mdpi.com/2072-4292/13/13/2622},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) and structure from motion (SfM) photogrammetry techniques are widely used for field-based, high-throughput plant phenotyping nowadays, but some of the intermediate processes throughout the workflow remain manual. For example, geographic information system (GIS) software is used to manually assess the 2D/3D field reconstruction quality and cropping region of interests (ROIs) from the whole field. In addition, extracting phenotypic traits from raw UAV images is more competitive than directly from the digital orthomosaic (DOM). Currently, no easy-to-use tools are available to implement previous tasks for commonly used commercial SfM software, such as Pix4D and Agisoft Metashape. Hence, an open source software package called easy intermediate data processor (EasyIDP; MIT license) was developed to decrease the workload in intermediate data processing mentioned above. The functions of the proposed package include (1) an ROI cropping module, assisting in reconstruction quality assessment and cropping ROIs from the whole field, and (2) an ROI reversing module, projecting ROIs to relative raw images. The result showed that both cropping and reversing modules work as expected. Moreover, the effects of ROI height selection and reversed ROI position on raw images to reverse calculation were discussed. This tool shows great potential for decreasing workload in data annotation for machine learning applications.},
DOI = {10.3390/rs13132622}
}



@Article{rs13132639,
AUTHOR = {Gautam, Deepak and Ostendorf, Bertram and Pagay, Vinay},
TITLE = {Estimation of Grapevine Crop Coefficient Using a Multispectral Camera on an Unmanned Aerial Vehicle},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2639},
URL = {https://www.mdpi.com/2072-4292/13/13/2639},
ISSN = {2072-4292},
ABSTRACT = {Crop water status and irrigation requirements are of great importance to the horticultural industry due to changing climatic conditions leading to high evaporative demands, drought and water scarcity in semi-arid and arid regions worldwide. Irrigation scheduling strategies based on evapotranspiration (ET), such as regulated deficit irrigation, requires the estimation of seasonal crop coefficients (kc). The ET-driven irrigation decisions for grapevines rely on the sampling of several kc values from each irrigation zone. Here, we present an unmanned aerial vehicle (UAV)-based technique to estimate kc at the single vine level in order to capture the spatial variability of water requirements in a commercial vineyard located in South Australia. A UAV carrying a multispectral sensor is used to extract the spectral, as well as the structural, information of Cabernet Sauvignon grapevines. The spectral and structural information, acquired at the various phenological stages of the vine through two seasons, is used to model kc using univariate (simple linear), multivariate (generalised linear and additive) and machine learning (convolution neural network and random forest) model frameworks. The structural information (e.g., canopy top view area) had the strongest correlation with kc throughout the season (p ≤ 0.001; Pearson R = 0.56), while the spectral indices (e.g., normalised indices) turned less-sensitive post véraison—the onset of ripening in grapes. Combining structural and spectral information improved the model’s performance. Among the investigated predictive models, the random forest predicted kc with the highest accuracy (R2: 0.675, root mean square error: 0.062, and mean absolute error: 0.047). This UAV-based approach improves the precision of irrigation by capturing the spatial variability of kc within a vineyard. Combined with an energy balance model, the water needs of a vineyard can be computed on a weekly or sub-weekly basis for precision irrigation. The UAV-based characterisation of kc can further enhance the water management and irrigation zoning by matching the infrastructure with the spatial variability of the irrigation demand.},
DOI = {10.3390/rs13132639}
}



@Article{s21134618,
AUTHOR = {Oliveira, Francisco and Luís, Miguel and Sargento, Susana},
TITLE = {Machine Learning for the Dynamic Positioning of UAVs for Extended Connectivity},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4618},
URL = {https://www.mdpi.com/1424-8220/21/13/4618},
PubMedID = {34283165},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) networks are an emerging technology, useful not only for the military, but also for public and civil purposes. Their versatility provides advantages in situations where an existing network cannot support all requirements of its users, either because of an exceptionally big number of users, or because of the failure of one or more ground base stations. Networks of UAVs can reinforce these cellular networks where needed, redirecting the traffic to available ground stations. Using machine learning algorithms to predict overloaded traffic areas, we propose a UAV positioning algorithm responsible for determining suitable positions for the UAVs, with the objective of a more balanced redistribution of traffic, to avoid saturated base stations and decrease the number of users without a connection. The tests performed with real data of user connections through base stations show that, in less restrictive network conditions, the algorithm to dynamically place the UAVs performs significantly better than in more restrictive conditions, reducing significantly the number of users without a connection. We also conclude that the accuracy of the prediction is a very important factor, not only in the reduction of users without a connection, but also on the number of UAVs deployed.},
DOI = {10.3390/s21134618}
}



@Article{su13147547,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Qayyum, Siddra and Khan, Sara Imran and Mojtahedi, Mohammad},
TITLE = {UAVs in Disaster Management: Application of Integrated Aerial Imagery and Convolutional Neural Network for Flood Detection},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {7547},
URL = {https://www.mdpi.com/2071-1050/13/14/7547},
ISSN = {2071-1050},
ABSTRACT = {Floods have been a major cause of destruction, instigating fatalities and massive damage to the infrastructure and overall economy of the affected country. Flood-related devastation results in the loss of homes, buildings, and critical infrastructure, leaving no means of communication or travel for the people stuck in such disasters. Thus, it is essential to develop systems that can detect floods in a region to provide timely aid and relief to stranded people, save their livelihoods, homes, and buildings, and protect key city infrastructure. Flood prediction and warning systems have been implemented in developed countries, but the manufacturing cost of such systems is too high for developing countries. Remote sensing, satellite imagery, global positioning system, and geographical information systems are currently used for flood detection to assess the flood-related damages. These techniques use neural networks, machine learning, or deep learning methods. However, unmanned aerial vehicles (UAVs) coupled with convolution neural networks have not been explored in these contexts to instigate a swift disaster management response to minimize damage to infrastructure. Accordingly, this paper uses UAV-based aerial imagery as a flood detection method based on Convolutional Neural Network (CNN) to extract flood-related features from the images of the disaster zone. This method is effective in assessing the damage to local infrastructures in the disaster zones. The study area is based on a flood-prone region of the Indus River in Pakistan, where both pre-and post-disaster images are collected through UAVs. For the training phase, 2150 image patches are created by resizing and cropping the source images. These patches in the training dataset train the CNN model to detect and extract the regions where a flood-related change has occurred. The model is tested against both pre-and post-disaster images to validate it, which has positive flood detection results with an accuracy of 91%. Disaster management organizations can use this model to assess the damages to critical city infrastructure and other assets worldwide to instigate proper disaster responses and minimize the damages. This can help with the smart governance of the cities where all emergent disasters are addressed promptly.},
DOI = {10.3390/su13147547}
}



@Article{rs13142678,
AUTHOR = {Ge, Haixiao and Ma, Fei and Li, Zhenwang and Tan, Zhengzheng and Du, Changwen},
TITLE = {Improved Accuracy of Phenological Detection in Rice Breeding by Using Ensemble Models of Machine Learning Based on UAV-RGB Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2678},
URL = {https://www.mdpi.com/2072-4292/13/14/2678},
ISSN = {2072-4292},
ABSTRACT = {Accurate and timely detection of phenology at plot scale in rice breeding trails is crucial for understanding the heterogeneity of varieties and guiding field management. Traditionally, remote sensing studies of phenology detection have heavily relied on the time-series vegetation index (VI) data. However, the methodology based on time-series VI data was often limited by the temporal resolution. In this study, three types of ensemble models including hard voting (majority voting), soft voting (weighted majority voting) and model stacking, were proposed to identify the principal phenological stages of rice based on unmanned aerial vehicle (UAV) RGB imagery. These ensemble models combined RGB-VIs, color space (e.g., RGB and HSV) and textures derived from UAV-RGB imagery, and five machine learning algorithms (random forest; k-nearest neighbors; Gaussian naïve Bayes; support vector machine and logistic regression) as base models to estimate phenological stages in rice breeding. The phenological estimation models were trained on the dataset of late-maturity cultivars and tested independently on the dataset of early-medium-maturity cultivars. The results indicated that all ensemble models outperform individual machine learning models in all datasets. The soft voting strategy provided the best performance for identifying phenology with the overall accuracy of 90% and 93%, and the mean F1-scores of 0.79 and 0.81, respectively, in calibration and validation datasets, which meant that the overall accuracy and mean F1-scores improved by 5% and 7%, respectively, in comparison with those of the best individual model (GNB), tested in this study. Therefore, the ensemble models demonstrated great potential in improving the accuracy of phenology detection in rice breeding.},
DOI = {10.3390/rs13142678}
}



@Article{geosciences11080305,
AUTHOR = {Karantanellis, Efstratios and Marinos, Vassilis and Vassilakis, Emmanuel and Hölbling, Daniel},
TITLE = {Evaluation of Machine Learning Algorithms for Object-Based Mapping of Landslide Zones Using UAV Data},
JOURNAL = {Geosciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {305},
URL = {https://www.mdpi.com/2076-3263/11/8/305},
ISSN = {2076-3263},
ABSTRACT = {Landslides are a critical geological phenomenon with devastating and catastrophic consequences. With the recent advancements in the geoinformation domain, landslide documentation and inventorization can be achieved with automated workflows using aerial platforms such as unmanned aerial vehicles (UAVs). As a result, ultra-high-resolution datasets are available for analysis at low operational costs. In this study, different segmentation and classification approaches were utilized for object-based landslide mapping. An integrated object-based image analysis (OBIA) workflow is presented incorporating orthophotomosaics and digital surface models (DSMs) with expert-based and machine learning (ML) algorithms. For segmentation, trial and error tests and the Estimation of Scale Parameter 2 (ESP 2) tool were implemented for the evaluation of different scale parameters. For classification, machine learning algorithms (K- Nearest Neighbor, Decision Tree, and Random Forest) were assessed with the inclusion of spectral, spatial, and contextual characteristics. For the ML classification of landslide zones, 60% of the reference segments have been used for training and 40% for validation of the models. The quality metrics of Precision, Recall, and F1 were implemented to evaluate the models’ performance under the different segmentation configurations. Results highlight higher performances for landslide mapping when DSM information was integrated. Hence, the configuration of spectral and DSM layers with the RF classifier resulted in the highest classification agreement with an F1 value of 0.85.},
DOI = {10.3390/geosciences11080305}
}



@Article{rs13152881,
AUTHOR = {Karami, Azam and Quijano, Karoll and Crawford, Melba},
TITLE = {Advancing Tassel Detection and Counting: Annotation and Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2881},
URL = {https://www.mdpi.com/2072-4292/13/15/2881},
ISSN = {2072-4292},
ABSTRACT = {Tassel counts provide valuable information related to flowering and yield prediction in maize, but are expensive and time-consuming to acquire via traditional manual approaches. High-resolution RGB imagery acquired by unmanned aerial vehicles (UAVs), coupled with advanced machine learning approaches, including deep learning (DL), provides a new capability for monitoring flowering. In this article, three state-of-the-art DL techniques, CenterNet based on point annotation, task-aware spatial disentanglement (TSD), and detecting objects with recursive feature pyramids and switchable atrous convolution (DetectoRS) based on bounding box annotation, are modified to improve their performance for this application and evaluated for tassel detection relative to Tasselnetv2+. The dataset for the experiments is comprised of RGB images of maize tassels from plant breeding experiments, which vary in size, complexity, and overlap. Results show that the point annotations are more accurate and simpler to acquire than the bounding boxes, and bounding box-based approaches are more sensitive to the size of the bounding boxes and background than point-based approaches. Overall, CenterNet has high accuracy in comparison to the other techniques, but DetectoRS can better detect early-stage tassels. The results for these experiments were more robust than Tasselnetv2+, which is sensitive to the number of tassels in the image.},
DOI = {10.3390/rs13152881}
}



@Article{rs13152956,
AUTHOR = {Wang, Li and Chen, Shuisen and Li, Dan and Wang, Chongyang and Jiang, Hao and Zheng, Qiong and Peng, Zhiping},
TITLE = {Estimation of Paddy Rice Nitrogen Content and Accumulation Both at Leaf and Plant Levels from UAV Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2956},
URL = {https://www.mdpi.com/2072-4292/13/15/2956},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing-based mapping of crop nitrogen (N) status is beneficial for precision N management over large geographic regions. Both leaf/canopy level nitrogen content and accumulation are valuable for crop nutrient diagnosis. However, previous studies mainly focused on leaf nitrogen content (LNC) estimation. The effects of growth stages on the modeling accuracy have not been widely discussed. This study aimed to estimate different paddy rice N traits—LNC, plant nitrogen content (PNC), leaf nitrogen accumulation (LNA) and plant nitrogen accumulation (PNA)—from unmanned aerial vehicle (UAV)-based hyperspectral images. Additionally, the effects of the growth stage were evaluated. Univariate regression models on vegetation indices (VIs), the traditional multivariate calibration method, partial least squares regression (PLSR) and modern machine learning (ML) methods, including artificial neural network (ANN), random forest (RF), and support vector machine (SVM), were evaluated both over the whole growing season and in each single growth stage (including the tillering, jointing, booting and heading growth stages). The results indicate that the correlation between the four nitrogen traits and the other three biochemical traits—leaf chlorophyll content, canopy chlorophyll content and aboveground biomass—are affected by the growth stage. Within a single growth stage, the performance of selected VIs is relatively constant. For the full-growth-stage models, the performance of the VI-based models is more diverse. For the full-growth-stage models, the transformed chlorophyll absorption in the reflectance index/optimized soil-adjusted vegetation index (TCARI/OSAVI) performs best for LNC, PNC and PNA estimation, while the three band vegetation index (TBVITian) performs best for LNA estimation. There are no obvious patterns regarding which method performs the best of the PLSR, ANN, RF and SVM in either the growth-stage-specific or full-growth-stage models. For the growth-stage-specific models, a lower mean relative error (MRE) and higher R2 can be acquired at the tillering and jointing growth stages. The PLSR and ML methods yield obviously better estimation accuracy for the full-growth-stage models than the VI-based models. For the growth-stage-specific models, the performance of VI-based models seems optimal and cannot be obviously surpassed. These results suggest that building linear regression models on VIs for paddy rice nitrogen traits estimation is still a reasonable choice when only a single growth stage is involved. However, when multiple growth stages are involved or missing the phenology information, using PLSR or ML methods is a better option.},
DOI = {10.3390/rs13152956}
}



@Article{agronomy11081554,
AUTHOR = {Lee, Dong-Ho and Kim, Hyeon-Jin and Park, Jong-Hwa},
TITLE = {UAV, a Farm Map, and Machine Learning Technology Convergence Classification Method of a Corn Cultivation Area},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1554},
URL = {https://www.mdpi.com/2073-4395/11/8/1554},
ISSN = {2073-4395},
ABSTRACT = {South Korea’s agriculture is characterized by a mixture of various cultivated crops. In such an agricultural environment, convergence technology for ICT (information, communications, and technology) and AI (artificial intelligence) as well as agriculture is required to classify objects and predict yields. In general, the classification of paddy fields and field boundaries takes a lot of time and effort. The Farm Map was developed to clearly demarcate and classify the boundaries of paddy fields and fields in Korea. Therefore, this study tried to minimize the time and effort required to divide paddy fields and fields through the application of the Farm Map. To improve the fact that UAV image processing for a wide area requires a lot of time and effort to classify objects, we suggest a method for optimizing cultivated crop recognition. This study aimed to evaluate the applicability and effectiveness of machine learning classification techniques using a Farm Map in object-based mapping of agricultural land using unmanned aerial vehicles (UAVs). In this study, the advanced function selection method for object classification is to improve classification accuracy by using two types of classifiers, support vector machine (SVM) and random forest (RF). As a result of classification by applying a Farm Map-based SVM algorithm to wide-area UAV images, producer’s accuracy (PA) was 81.68%, user’s accuracy (UA) was 75.09%, the Kappa coefficient was 0.77, and the F-measure was 0.78. The results of classification by the Farm Map-based RF algorithm were as follows: PA of 96.58%, UA of 92.27%, a Kappa coefficient of 0.94, and the F-measure of 0.94. In the cultivation environment in which various crops were mixed, the corn cultivation area was estimated to be 96.54 ha by SVM, showing an accuracy of 90.27%. RF provided an estimate of 98.77 ha and showed an accuracy of 92.36%, which was higher than that of SVM. As a result of using the Farm Map for the object-based classification method, the agricultural land classification showed a higher efficiency in terms of time than the existing object classification method. Most importantly, it was confirmed that the efficiency of data processing can be increased by minimizing the possibility of misclassification in the obtained results. The obtained results confirmed that rapid and reliable analysis is possible when the cultivated area of crops is identified using UAV images, a Farm Map, and machine learning.},
DOI = {10.3390/agronomy11081554}
}



@Article{min11080846,
AUTHOR = {Sinaice, Brian Bino and Owada, Narihiro and Saadat, Mahdi and Toriya, Hisatoshi and Inagaki, Fumiaki and Bagai, Zibisani and Kawamura, Youhei},
TITLE = {Coupling NCA Dimensionality Reduction with Machine Learning in Multispectral Rock Classification Problems},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {846},
URL = {https://www.mdpi.com/2075-163X/11/8/846},
ISSN = {2075-163X},
ABSTRACT = {Though multitudes of industries depend on the mining industry for resources, this industry has taken hits in terms of declining mineral ore grades and its current use of traditional, time-consuming and computationally costly rock and mineral identification methods. Therefore, this paper proposes integrating Hyperspectral Imaging, Neighbourhood Component Analysis (NCA) and Machine Learning (ML) as a combined system that can identify rocks and minerals. Modestly put, hyperspectral imaging gathers electromagnetic signatures of the rocks in hundreds of spectral bands. However, this data suffers from what is termed the ‘dimensionality curse’, which led to our employment of NCA as a dimensionality reduction technique. NCA, in turn, highlights the most discriminant feature bands, number of which being dependent on the intended application(s) of this system. Our envisioned application is rock and mineral classification via unmanned aerial vehicle (UAV) drone technology. In this study, we performed a 204-hyperspectral to 5-band multispectral reduction, because current production drones are limited to five multispectral bands sensors. Based on these bands, we applied ML to identify and classify rocks, thereby proving our hypothesis, reducing computational costs, attaining an ML classification accuracy of 71%, and demonstrating the potential mining industry optimisations attainable through this integrated system.},
DOI = {10.3390/min11080846}
}



@Article{app11167240,
AUTHOR = {Jembre, Yalew Zelalem and Nugroho, Yuniarto Wimbo and Khan, Muhammad Toaha Raza and Attique, Muhammad and Paul, Rajib and Shah, Syed Hassan Ahmed and Kim, Beomjoon},
TITLE = {Evaluation of Reinforcement and Deep Learning Algorithms in Controlling Unmanned Aerial Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7240},
URL = {https://www.mdpi.com/2076-3417/11/16/7240},
ISSN = {2076-3417},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are abundantly becoming a part of society, which is a trend that is expected to grow even further. The quadrotor is one of the drone technologies that is applicable in many sectors and in both military and civilian activities, with some applications requiring autonomous flight. However, stability, path planning, and control remain significant challenges in autonomous quadrotor flights. Traditional control algorithms, such as proportional-integral-derivative (PID), have deficiencies, especially in tuning. Recently, machine learning has received great attention in flying UAVs to desired positions autonomously. In this work, we configure the quadrotor to fly autonomously by using agents (the machine learning schemes being used to fly the quadrotor autonomously) to learn about the virtual physical environment. The quadrotor will fly from an initial to a desired position. When the agent brings the quadrotor closer to the desired position, it is rewarded; otherwise, it is punished. Two reinforcement learning models, Q-learning and SARSA, and a deep learning deep Q-network network are used as agents. The simulation is conducted by integrating the robot operating system (ROS) and Gazebo, which allowed for the implementation of the learning algorithms and the physical environment, respectively. The result has shown that the Deep Q-network network with Adadelta optimizer is the best setting to fly the quadrotor from the initial to desired position.},
DOI = {10.3390/app11167240}
}



@Article{electronics10161965,
AUTHOR = {Bhatia, Jyoti and Dayal, Aveen and Jha, Ajit and Vishvakarma, Santosh Kumar and Joshi, Soumya and Srinivas, M. B. and Yalavarthy, Phaneendra K. and Kumar, Abhinav and Lalitha, V. and Koorapati, Sagar and Cenkeramaddi, Linga Reddy},
TITLE = {Classification of Targets Using Statistical Features from Range FFT of mmWave FMCW Radars},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {1965},
URL = {https://www.mdpi.com/2079-9292/10/16/1965},
ISSN = {2079-9292},
ABSTRACT = {Radars with mmWave frequency modulated continuous wave (FMCW) technology accurately estimate the range and velocity of targets in their field of view (FoV). The targeted angle of arrival (AoA) estimation can be improved by increasing receiving antennas or by using multiple-input multiple-output (MIMO). However, obtaining target features such as target type remains challenging. In this paper, we present a novel target classification method based on machine learning and features extracted from a range fast Fourier transform (FFT) profile by using mmWave FMCW radars operating in the frequency range of 77–81 GHz. The measurements are carried out in a variety of realistic situations, including pedestrian, automotive, and unmanned aerial vehicle (UAV) (also known as drone). Peak, width, area, variance, and range are collected from range FFT profile peaks and fed into a machine learning model. In order to evaluate the performance, various light weight classification machine learning models such as logistic regression, Naive Bayes, support vector machine (SVM), and lightweight gradient boosting machine (GBM) are used. We demonstrate our findings by using outdoor measurements and achieve a classification accuracy of 95.6% by using LightGBM. The proposed method will be extremely useful in a wide range of applications, including cost-effective and dependable ground station traffic management and control systems for autonomous operations, and advanced driver-assistance systems (ADAS). The presented classification technique extends the potential of mmWave FMCW radar beyond the detection of range, velocity, and AoA to classification. mmWave FMCW radars will be more robust in computer vision, visual perception, and fully autonomous ground control and traffic management cyber-physical systems as a result of the added new feature.},
DOI = {10.3390/electronics10161965}
}



@Article{rs13163272,
AUTHOR = {Ivošević, Bojana and Lugonja, Predrag and Brdar, Sanja and Radulović, Mirjana and Vujić, Ante and Valente, João},
TITLE = {UAV-Based Land Cover Classification for Hoverfly (Diptera: Syrphidae) Habitat Condition Assessment: A Case Study on Mt. Stara Planina (Serbia)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3272},
URL = {https://www.mdpi.com/2072-4292/13/16/3272},
ISSN = {2072-4292},
ABSTRACT = {Habitat degradation, mostly caused by human impact, is one of the key drivers of biodiversity loss. This is a global problem, causing a decline in the number of pollinators, such as hoverflies. In the process of digitalizing ecological studies in Serbia, remote-sensing-based land cover classification has become a key component for both current and future research. Object-based land cover classification, using machine learning algorithms of very high resolution (VHR) imagery acquired by an unmanned aerial vehicle (UAV) was carried out in three different study sites on Mt. Stara Planina, Eastern Serbia. UAV land cover classified maps with seven land cover classes (trees, shrubs, meadows, road, water, agricultural land, and forest patches) were studied. Moreover, three different classification algorithms—support vector machine (SVM), random forest (RF), and k-NN (k-nearest neighbors)—were compared. This study shows that the random forest classifier performs better with respect to the other classifiers in all three study sites, with overall accuracy values ranging from 0.87 to 0.96. The overall results are robust to changes in labeling ground truth subsets. The obtained UAV land cover classified maps were compared with the Map of the Natural Vegetation of Europe (EPNV) and used to quantify habitat degradation and assess hoverfly species richness. It was concluded that the percentage of habitat degradation is primarily caused by anthropogenic pressure, thus affecting the richness of hoverfly species in the study sites. In order to enable research reproducibility, the datasets used in this study are made available in a public repository.},
DOI = {10.3390/rs13163272}
}



@Article{ai2030023,
AUTHOR = {Xue, Zhihan and Gonsalves, Tad},
TITLE = {Vision Based Drone Obstacle Avoidance by Deep Reinforcement Learning},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {366--380},
URL = {https://www.mdpi.com/2673-2688/2/3/23},
ISSN = {2673-2688},
ABSTRACT = {Research on autonomous obstacle avoidance of drones has recently received widespread attention from researchers. Among them, an increasing number of researchers are using machine learning to train drones. These studies typically adopt supervised learning or reinforcement learning to train the networks. Supervised learning has a disadvantage in that it takes a significant amount of time to build the datasets, because it is difficult to cover the complex and changeable drone flight environment in a single dataset. Reinforcement learning can overcome this problem by using drones to learn data in the environment. However, the current research results based on reinforcement learning are mainly focused on discrete action spaces. In this way, the movement of drones lacks precision and has somewhat unnatural flying behavior. This study aims to use the soft-actor-critic algorithm to train a drone to perform autonomous obstacle avoidance in continuous action space using only the image data. The algorithm is trained and tested in a simulation environment built by Airsim. The results show that our algorithm enables the UAV to avoid obstacles in the training environment only by inputting the depth map. Moreover, it also has a higher obstacle avoidance rate in the reconfigured environment without retraining.},
DOI = {10.3390/ai2030023}
}



@Article{rs13163322,
AUTHOR = {Li, Dan and Miao, Yuxin and Gupta, Sanjay K. and Rosen, Carl J. and Yuan, Fei and Wang, Chongyang and Wang, Li and Huang, Yanbo},
TITLE = {Improving Potato Yield Prediction by Combining Cultivar Information and UAV Remote Sensing Data Using Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3322},
URL = {https://www.mdpi.com/2072-4292/13/16/3322},
ISSN = {2072-4292},
ABSTRACT = {Accurate high-resolution yield maps are essential for identifying spatial yield variability patterns, determining key factors influencing yield variability, and providing site-specific management insights in precision agriculture. Cultivar differences can significantly influence potato (Solanum tuberosum L.) tuber yield prediction using remote sensing technologies. The objective of this study was to improve potato yield prediction using unmanned aerial vehicle (UAV) remote sensing by incorporating cultivar information with machine learning methods. Small plot experiments involving different cultivars and nitrogen (N) rates were conducted in 2018 and 2019. UAV-based multi-spectral images were collected throughout the growing season. Machine learning models, i.e., random forest regression (RFR) and support vector regression (SVR), were used to combine different vegetation indices with cultivar information. It was found that UAV-based spectral data from the early growing season at the tuber initiation stage (late June) were more correlated with potato marketable yield than the spectral data from the later growing season at the tuber maturation stage. However, the best performing vegetation indices and the best timing for potato yield prediction varied with cultivars. The performance of the RFR and SVR models using only remote sensing data was unsatisfactory (R2 = 0.48–0.51 for validation) but was significantly improved when cultivar information was incorporated (R2 = 0.75–0.79 for validation). It is concluded that combining high spatial-resolution UAV images and cultivar information using machine learning algorithms can significantly improve potato yield prediction than methods without using cultivar information. More studies are needed to improve potato yield prediction using more detailed cultivar information, soil and landscape variables, and management information, as well as more advanced machine learning models.},
DOI = {10.3390/rs13163322}
}



@Article{drones5030086,
AUTHOR = {Grigusova, Paulina and Larsen, Annegret and Achilles, Sebastian and Klug, Alexander and Fischer, Robin and Kraus, Diana and Übernickel, Kirstin and Paulino, Leandro and Pliscoff, Patricio and Brandl, Roland and Farwig, Nina and Bendix, Jörg},
TITLE = {Area-Wide Prediction of Vertebrate and Invertebrate Hole Density and Depth across a Climate Gradient in Chile Based on UAV and Machine Learning},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {86},
URL = {https://www.mdpi.com/2504-446X/5/3/86},
ISSN = {2504-446X},
ABSTRACT = {Burrowing animals are important ecosystem engineers affecting soil properties, as their burrowing activity leads to the redistribution of nutrients and soil carbon sequestration. The magnitude of these effects depends on the spatial density and depth of such burrows, but a method to derive this type of spatially explicit data is still lacking. In this study, we test the potential of using consumer-oriented UAV RGB imagery to determine the density and depth of holes created by burrowing animals at four study sites along a climate gradient in Chile, by combining UAV data with empirical field plot observations and machine learning techniques. To enhance the limited spectral information in RGB imagery, we derived spatial layers representing vegetation type and height and used landscape textures and diversity to predict hole parameters. Across-site models for hole density generally performed better than those for depth, where the best-performing model was for the invertebrate hole density (R2 = 0.62). The best models at individual study sites were obtained for hole density in the arid climate zone (R2 = 0.75 and 0.68 for invertebrates and vertebrates, respectively). Hole depth models only showed good to fair performance. Regarding predictor importance, the models heavily relied on vegetation height, texture metrics, and diversity indices.},
DOI = {10.3390/drones5030086}
}



@Article{rs13173459,
AUTHOR = {Pranga, Joanna and Borra-Serrano, Irene and Aper, Jonas and De Swaef, Tom and Ghesquiere, An and Quataert, Paul and Roldán-Ruiz, Isabel and Janssens, Ivan A. and Ruysschaert, Greet and Lootens, Peter},
TITLE = {Improving Accuracy of Herbage Yield Predictions in Perennial Ryegrass with UAV-Based Structural and Spectral Data Fusion and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3459},
URL = {https://www.mdpi.com/2072-4292/13/17/3459},
ISSN = {2072-4292},
ABSTRACT = {High-throughput field phenotyping using close remote sensing platforms and sensors for non-destructive assessment of plant traits can support the objective evaluation of yield predictions of large breeding trials. The main objective of this study was to examine the potential of unmanned aerial vehicle (UAV)-based structural and spectral features and their combination in herbage yield predictions across diploid and tetraploid varieties and breeding populations of perennial ryegrass (Lolium perenne L.). Canopy structural (i.e., canopy height) and spectral (i.e., vegetation indices) information were derived from data gathered with two sensors: a consumer-grade RGB and a 10-band multispectral (MS) camera system, which were compared in the analysis. A total of 468 field plots comprising 115 diploid and 112 tetraploid varieties and populations were considered in this study. A modelling framework established to predict dry matter yield (DMY), was used to test three machine learning algorithms, including Partial Least Squares Regression (PLSR), Random Forest (RF), and Support Vector Machines (SVM). The results of the nested cross-validation revealed: (a) the fusion of structural and spectral features achieved better DMY estimates as compared to models fitted with structural or spectral data only, irrespective of the sensor, ploidy level or machine learning algorithm applied; (b) models built with MS-based predictor variables, despite their lower spatial resolution, slightly outperformed the RGB-based models, as lower mean relative root mean square error (rRMSE) values were delivered; and (c) on average, the RF technique reported the best model performances among tested algorithms, regardless of the dataset used. The approach introduced in this study can provide accurate yield estimates (up to an RMSE = 308 kg ha−1) and useful information for breeders and practical farm-scale applications.},
DOI = {10.3390/rs13173459}
}



@Article{rs13173482,
AUTHOR = {Roy Choudhury, Malini and Das, Sumanta and Christopher, Jack and Apan, Armando and Chapman, Scott and Menzies, Neal W. and Dang, Yash P.},
TITLE = {Improving Biomass and Grain Yield Prediction of Wheat Genotypes on Sodic Soil Using Integrated High-Resolution Multispectral, Hyperspectral, 3D Point Cloud, and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3482},
URL = {https://www.mdpi.com/2072-4292/13/17/3482},
ISSN = {2072-4292},
ABSTRACT = {Sodic soils adversely affect crop production over extensive areas of rain-fed cropping worldwide, with particularly large areas in Australia. Crop phenotyping may assist in identifying cultivars tolerant to soil sodicity. However, studies to identify the most appropriate traits and reliable tools to assist crop phenotyping on sodic soil are limited. Hence, this study evaluated the ability of multispectral, hyperspectral, 3D point cloud, and machine learning techniques to improve estimation of biomass and grain yield of wheat genotypes grown on a moderately sodic (MS) and highly sodic (HS) soil sites in northeastern Australia. While a number of studies have reported using different remote sensing approaches and crop traits to quantify crop growth, stress, and yield variation, studies are limited using the combination of these techniques including machine learning to improve estimation of genotypic biomass and yield, especially in constrained sodic soil environments. At close to flowering, unmanned aerial vehicle (UAV) and ground-based proximal sensing was used to obtain remote and/or proximal sensing data, while biomass yield and crop heights were also manually measured in the field. Grain yield was machine-harvested at maturity. UAV remote and/or proximal sensing-derived spectral vegetation indices (VIs), such as normalized difference vegetation index, optimized soil adjusted vegetation index, and enhanced vegetation index and crop height were closely corresponded to wheat genotypic biomass and grain yields. UAV multispectral VIs more closely associated with biomass and grain yields compared to proximal sensing data. The red-green-blue (RGB) 3D point cloud technique was effective in determining crop height, which was slightly better correlated with genotypic biomass and grain yield than ground-measured crop height data. These remote sensing-derived crop traits (VIs and crop height) and wheat biomass and grain yields were further simulated using machine learning algorithms (multitarget linear regression, support vector machine regression, Gaussian process regression, and artificial neural network) with different kernels to improve estimation of biomass and grain yield. The artificial neural network predicted biomass yield (R2 = 0.89; RMSE = 34.8 g/m2 for the MS and R2 = 0.82; RMSE = 26.4 g/m2 for the HS site) and grain yield (R2 = 0.88; RMSE = 11.8 g/m2 for the MS and R2 = 0.74; RMSE = 16.1 g/m2 for the HS site) with slightly less error than the others. Wheat genotypes Mitch, Corack, Mace, Trojan, Lancer, and Bremer were identified as more tolerant to sodic soil constraints than Emu Rock, Janz, Flanker, and Gladius. The study improves our ability to select appropriate traits and techniques in accurate estimation of wheat genotypic biomass and grain yields on sodic soils. This will also assist farmers in identifying cultivars tolerant to sodic soil constraints.},
DOI = {10.3390/rs13173482}
}



@Article{rs13183669,
AUTHOR = {Martínez Prentice, Ricardo and Villoslada Peciña, Miguel and Ward, Raymond D. and Bergamo, Thaisa F. and Joyce, Chris B. and Sepp, Kalev},
TITLE = {Machine Learning Classification and Accuracy Assessment from High-Resolution Images of Coastal Wetlands},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3669},
URL = {https://www.mdpi.com/2072-4292/13/18/3669},
ISSN = {2072-4292},
ABSTRACT = {High-resolution images obtained by multispectral cameras mounted on Unmanned Aerial Vehicles (UAVs) are helping to capture the heterogeneity of the environment in images that can be discretized in categories during a classification process. Currently, there is an increasing use of supervised machine learning (ML) classifiers to retrieve accurate results using scarce datasets with samples with non-linear relationships. We compared the accuracies of two ML classifiers using a pixel and object analysis approach in six coastal wetland sites. The results show that the Random Forest (RF) performs better than K-Nearest Neighbors (KNN) algorithm in the classification of pixels and objects and the classification based on pixel analysis is slightly better than the object-based analysis. The agreement between the classifications of objects and pixels is higher in Random Forest. This is likely due to the heterogeneity of the study areas, where pixel-based classifications are most appropriate. In addition, from an ecological perspective, as these wetlands are heterogeneous, the pixel-based classification reflects a more realistic interpretation of plant community distribution.},
DOI = {10.3390/rs13183669}
}



@Article{rs13183681,
AUTHOR = {Krause, Johannes R. and Hinojosa-Corona, Alejandro and Gray, Andrew B. and Burke Watson, Elizabeth},
TITLE = {Emerging Sensor Platforms Allow for Seagrass Extent Mapping in a Turbid Estuary and from the Meadow to Ecosystem Scale},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3681},
URL = {https://www.mdpi.com/2072-4292/13/18/3681},
ISSN = {2072-4292},
ABSTRACT = {Seagrass meadows are globally important habitats, protecting shorelines, providing nursery areas for fish, and sequestering carbon. However, both anthropogenic and natural environmental stressors have led to a worldwide reduction seagrass habitats. For purposes of management and restoration, it is essential to produce accurate maps of seagrass meadows over a variety of spatial scales, resolutions, and at temporal frequencies ranging from months to years. Satellite remote sensing has been successfully employed to produce maps of seagrass in the past, but turbid waters and difficulty in obtaining low-tide scenes pose persistent challenges. This study builds on an increased availability of affordable high temporal frequency imaging platforms, using seasonal unmanned aerial vehicle (UAV) surveys of seagrass extent at the meadow scale, to inform machine learning classifications of satellite imagery of a 40 km2 bay. We find that object-based image analysis is suitable to detect seasonal trends in seagrass extent from UAV imagery and find that trends vary between individual meadows at our study site Bahía de San Quintín, Baja California, México, during our study period in 2019. We further suggest that compositing multiple satellite imagery classifications into a seagrass probability map allows for an estimation of seagrass extent in turbid waters and report that in 2019, seagrass covered 2324 ha of Bahía de San Quintín, indicating a recovery from losses reported for previous decades.},
DOI = {10.3390/rs13183681}
}



@Article{separations8090160,
AUTHOR = {Ahmed, Shara and Nicholson, Catherine E. and Muto, Paul and Perry, Justin J. and Dean, John R.},
TITLE = {The Use of an Unmanned Aerial Vehicle for Tree Phenotyping Studies},
JOURNAL = {Separations},
VOLUME = {8},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {160},
URL = {https://www.mdpi.com/2297-8739/8/9/160},
ISSN = {2297-8739},
ABSTRACT = {A strip of 20th-century landscape woodland planted alongside a 17th to mid-18th century ancient and semi-natural woodland (ASNW) was investigated by applied aerial spectroscopy using an unmanned aerial vehicle (UAV) with a multispectral image camera (MSI). A simple classification approach of normalized difference spectral index (NDSI), derived using principal component analysis (PCA), enabled the identification of the non-native trees within the 20th-century boundary. The tree species within this boundary, classified by NDSI, were further segmented by the machine learning segmentation method of k-means clustering. This combined innovative approach has enabled the identification of multiple tree species in the 20th-century boundary. Phenotyping of trees at canopy level using the UAV with MSI, across 8052 m2, identified black pine (23%), Norway maple (19%), Scots pine (12%), and sycamore (19%) as well as native trees (oak and silver birch, 27%). This derived data was corroborated by field identification at ground-level, over an area of 6785 m2, that confirmed the presence of black pine (26%), Norway maple (30%), Scots pine (10%), and sycamore (14%) as well as other trees (oak and silver birch, 20%). The benefits of using a UAV, with an MSI camera, for monitoring tree boundaries next to a new housing development are demonstrated.},
DOI = {10.3390/separations8090160}
}



@Article{rs13183777,
AUTHOR = {Zhang, He and Bauters, Marijn and Boeckx, Pascal and Van Oost, Kristof},
TITLE = {Mapping Canopy Heights in Dense Tropical Forests Using Low-Cost UAV-Derived Photogrammetric Point Clouds and Machine Learning Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3777},
URL = {https://www.mdpi.com/2072-4292/13/18/3777},
ISSN = {2072-4292},
ABSTRACT = {Tropical forests are a key component of the global carbon cycle and climate change mitigation. Field- or LiDAR-based approaches enable reliable measurements of the structure and above-ground biomass (AGB) of tropical forests. Data derived from digital aerial photogrammetry (DAP) on the unmanned aerial vehicle (UAV) platform offer several advantages over field- and LiDAR-based approaches in terms of scale and efficiency, and DAP has been presented as a viable and economical alternative in boreal or deciduous forests. However, detecting with DAP the ground in dense tropical forests, which is required for the estimation of canopy height, is currently considered highly challenging. To address this issue, we present a generally applicable method that is based on machine learning methods to identify the forest floor in DAP-derived point clouds of dense tropical forests. We capitalize on the DAP-derived high-resolution vertical forest structure to inform ground detection. We conducted UAV-DAP surveys combined with field inventories in the tropical forest of the Congo Basin. Using airborne LiDAR (ALS) for ground truthing, we present a canopy height model (CHM) generation workflow that constitutes the detection, classification and interpolation of ground points using a combination of local minima filters, supervised machine learning algorithms and TIN densification for classifying ground points using spectral and geometrical features from the UAV-based 3D data. We demonstrate that our DAP-based method provides estimates of tree heights that are identical to LiDAR-based approaches (conservatively estimated NSE = 0.88, RMSE = 1.6 m). An external validation shows that our method is capable of providing accurate and precise estimates of tree heights and AGB in dense tropical forests (DAP vs. field inventories of old forest: r2 = 0.913, RMSE = 31.93 Mg ha−1). Overall, this study demonstrates that the application of cheap and easily deployable UAV-DAP platforms can be deployed without expert knowledge to generate biophysical information and advance the study and monitoring of dense tropical forests.},
DOI = {10.3390/rs13183777}
}



@Article{rs13193928,
AUTHOR = {Lu, Qikai and Si, Wei and Wei, Lifei and Li, Zhongqiang and Xia, Zhihong and Ye, Song and Xia, Yu},
TITLE = {Retrieval of Water Quality from UAV-Borne Hyperspectral Imagery: A Comparative Study of Machine Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3928},
URL = {https://www.mdpi.com/2072-4292/13/19/3928},
ISSN = {2072-4292},
ABSTRACT = {The rapidly increasing world population and human activities accelerate the crisis of the limited freshwater resources. Water quality must be monitored for the sustainability of freshwater resources. Unmanned aerial vehicle (UAV)-borne hyperspectral data can capture fine features of water bodies, which have been widely used for monitoring water quality. In this study, nine machine learning algorithms are systematically evaluated for the inversion of water quality parameters including chlorophyll-a (Chl-a) and suspended solids (SS) with UAV-borne hyperspectral data. In comparing the experimental results of the machine learning model on the water quality parameters, we can observe that the prediction performance of the Catboost regression (CBR) model is the best. However, the prediction performances of the Multi-layer Perceptron regression (MLPR) and Elastic net (EN) models are very unsatisfactory, indicating that the MLPR and EN models are not suitable for the inversion of water quality parameters. In addition, the water quality distribution map is generated, which can be used to identify polluted areas of water bodies.},
DOI = {10.3390/rs13193928}
}



@Article{w13192734,
AUTHOR = {Šulyová, Dominika and Vodák, Josef and Kubina, Milan},
TITLE = {Effective Management of Scarce Water Resources: From Antiquity to Today and into the Future},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {2734},
URL = {https://www.mdpi.com/2073-4441/13/19/2734},
ISSN = {2073-4441},
ABSTRACT = {Water is a critically important element of human life. The best practice of effective water management comes from ancient civilizations that, despite their technologies and practices, were unable to prevent collapse from water scarcity. In the 21st century, in an era of climate change, pollution or population explosion, cities are looking for innovative ways to effectively manage scarce resources for future generations. Which elements should cities of the future follow to avoid water collapse? The following article aims to identify the key elements of effective management and to represent them graphically in the form of a recommended model, which will be verified in the future in Slovakia. The article uses case analysis of best past and current practices, comparison and summarization to identify the elements, creativity, and logic in the development of the model, including induction and deduction. The article serves as a basis for fellow researchers (analyses carried out) and strategic urban management (effective urban water management). The main finding of the article is that ecological change puts pressure on social elements and therefore it is necessary to focus on the area of strategic management. Cities should not only know how to manage resource abundance or short-term scarcity, but also long-term scarcity. They should use elements of trust, awareness and continuous improvement through modern monitoring technologies (UAVs, sensors) and prediction (machine learning). This is the only way to generate water sustainability in the urban concept of the future.},
DOI = {10.3390/w13192734}
}



@Article{rs13193983,
AUTHOR = {Pontoglio, Emanuele and Dabove, Paolo and Grasso, Nives and Lingua, Andrea Maria},
TITLE = {Automatic Features Detection in a Fluvial Environment through Machine Learning Techniques Based on UAVs Multispectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3983},
URL = {https://www.mdpi.com/2072-4292/13/19/3983},
ISSN = {2072-4292},
ABSTRACT = {The present work aims to demonstrate how machine learning (ML) techniques can be used for automatic feature detection and extraction in fluvial environments. The use of photogrammetry and machine learning algorithms has improved the understanding of both environmental and anthropic issues. The developed methodology was applied considering the acquisition of multiple photogrammetric images thanks to unmanned aerial vehicles (UAV) carrying multispectral cameras. These surveys were carried out in the Salbertrand area, along the Dora Riparia River, situated in Piedmont (Italy). The authors developed an algorithm able to identify and detect the water table contour concerning the landed areas: the automatic classification in ML found a valid identification of different patterns (water, gravel bars, vegetation, and ground classes) in specific hydraulic and geomatics conditions. Indeed, the RE+NIR data gave us a sharp rise in terms of accuracy by about 11% and 13.5% of F1-score average values in the testing point clouds compared to RGB data. The obtained results about the automatic classification led us to define a new procedure with precise validity conditions.},
DOI = {10.3390/rs13193983}
}



@Article{rs13204091,
AUTHOR = {Ndlovu, Helen S. and Odindi, John and Sibanda, Mbulisi and Mutanga, Onisimo and Clulow, Alistair and Chimonyo, Vimbayi G. P. and Mabhaudhi, Tafadzwanashe},
TITLE = {A Comparative Estimation of Maize Leaf Water Content Using Machine Learning Techniques and Unmanned Aerial Vehicle (UAV)-Based Proximal and Remotely Sensed Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4091},
URL = {https://www.mdpi.com/2072-4292/13/20/4091},
ISSN = {2072-4292},
ABSTRACT = {Determining maize water content variability is necessary for crop monitoring and in developing early warning systems to optimise agricultural production in smallholder farms. However, spatially explicit information on maize water content, particularly in Southern Africa, remains elementary due to the shortage of efficient and affordable primary sources of suitable spatial data at a local scale. Unmanned Aerial Vehicles (UAVs), equipped with light-weight multispectral sensors, provide spatially explicit, near-real-time information for determining the maize crop water status at farm scale. Therefore, this study evaluated the utility of UAV-derived multispectral imagery and machine learning techniques in estimating maize leaf water indicators: equivalent water thickness (EWT), fuel moisture content (FMC), and specific leaf area (SLA). The results illustrated that both NIR and red-edge derived spectral variables were critical in characterising the maize water indicators on smallholder farms. Furthermore, the best models for estimating EWT, FMC, and SLA were derived from the random forest regression (RFR) algorithm with an rRMSE of 3.13%, 1%, and 3.48%, respectively. Additionally, EWT and FMC yielded the highest predictive performance and were the most optimal indicators of maize leaf water content. The findings are critical towards developing a robust and spatially explicit monitoring framework of maize water status and serve as a proxy of crop health and the overall productivity of smallholder maize farms.},
DOI = {10.3390/rs13204091}
}



@Article{rs13214282,
AUTHOR = {Yu, Jin-Woo and Yoon, Young-Woong and Baek, Won-Kyung and Jung, Hyung-Sup},
TITLE = {Forest Vertical Structure Mapping Using Two-Seasonal Optic Images and LiDAR DSM Acquired from UAV Platform through Random Forest, XGBoost, and Support Vector Machine Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {4282},
URL = {https://www.mdpi.com/2072-4292/13/21/4282},
ISSN = {2072-4292},
ABSTRACT = {Research on the forest structure classification is essential, as it plays an important role in assessing the vitality and diversity of vegetation. However, classifying forest structure involves in situ surveying, which requires considerable time and money, and cannot be conducted directly in some instances; also, the update cycle of the classification data is very late. To overcome these drawbacks, feasibility studies on mapping the forest vertical structure from aerial images using machine learning techniques were conducted. In this study, we investigated (1) the performance improvement of the forest structure classification, using a high-resolution LiDAR-derived digital surface model (DSM) acquired from an unmanned aerial vehicle (UAV) platform and (2) the performance comparison of results obtained from the single-seasonal and two-seasonal data, using random forest (RF), extreme gradient boosting (XGBoost), and support vector machine (SVM). For the performance comparison, the UAV optic and LiDAR data were divided into three cases: (1) only used autumn data, (2) only used winter data, and (3) used both autumn and winter data. From the results, the best model was XGBoost, and the F1 scores achieved using this method were approximately 0.92 in the autumn and winter cases. A remarkable improvement was achieved when both two-seasonal images were used. The F1 score improved by 35.3% from 0.68 to 0.92. This implies that (1) the seasonal variation in the forest vertical structure can be more important than the spatial resolution, and (2) the classification performance achieved from the two-seasonal UAV optic images and LiDAR-derived DSMs can reach 0.9 with the application of an optimal machine learning approach.},
DOI = {10.3390/rs13214282}
}



@Article{drones5040127,
AUTHOR = {Raza, Wamiq and Osman, Anas and Ferrini, Francesco and Natale, Francesco De},
TITLE = {Energy-Efficient Inference on the Edge Exploiting TinyML Capabilities for UAVs},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {127},
URL = {https://www.mdpi.com/2504-446X/5/4/127},
ISSN = {2504-446X},
ABSTRACT = {In recent years, the proliferation of unmanned aerial vehicles (UAVs) has increased dramatically. UAVs can accomplish complex or dangerous tasks in a reliable and cost-effective way but are still limited by power consumption problems, which pose serious constraints on the flight duration and completion of energy-demanding tasks. The possibility of providing UAVs with advanced decision-making capabilities in an energy-effective way would be extremely beneficial. In this paper, we propose a practical solution to this problem that exploits deep learning on the edge. The developed system integrates an OpenMV microcontroller into a DJI Tello Micro Aerial Vehicle (MAV). The microcontroller hosts a set of machine learning-enabled inference tools that cooperate to control the navigation of the drone and complete a given mission objective. The goal of this approach is to leverage the new opportunistic features of TinyML through OpenMV including offline inference, low latency, energy efficiency, and data security. The approach is successfully validated on a practical application consisting of the onboard detection of people wearing protection masks in a crowded environment.},
DOI = {10.3390/drones5040127}
}



@Article{s21217312,
AUTHOR = {Fuentes, Sigfredo and Gonzalez Viejo, Claudia and Hall, Chelsea and Tang, Yidan and Tongson, Eden},
TITLE = {Berry Cell Vitality Assessment and the Effect on Wine Sensory Traits Based on Chemical Fingerprinting, Canopy Architecture and Machine Learning Modelling},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7312},
URL = {https://www.mdpi.com/1424-8220/21/21/7312},
PubMedID = {34770618},
ISSN = {1424-8220},
ABSTRACT = {Berry cell death assessment can become one of the most objective parameters to assess important berry quality traits, such as aroma profiles that can be passed to the wine in the winemaking process. At the moment, the only practical tool to assess berry cell death in the field is using portable near-infrared spectroscopy (NIR) and machine learning (ML) models. This research tested the NIR and ML approach and developed supervised regression ML models using Shiraz and Chardonnay berries and wines from a vineyard located in Yarra Valley, Victoria, Australia. An ML model was developed using NIR measurements from intact berries as inputs to estimate berry cell death (BCD), living tissue (LT) (Model 1). Furthermore, canopy architecture parameters obtained from cover photography of grapevine canopies and computer vision analysis were also tested as inputs to develop ML models to assess BCD and LT (Model 2) and the intensity of sensory descriptors based on visual and aroma profiles of wines for Chardonnay (Model 3) and Shiraz (Model 4). The results showed high accuracy and performance of models developed based on correlation coefficient (R) and slope (b) (M1: R = 0.87; b = 0.82; M2: R = 0.98; b = 0.93; M3: R = 0.99; b = 0.99; M4: R = 0.99; b = 1.00). Models developed based on canopy architecture, and computer vision can be used to automatically estimate the vigor and berry and wine quality traits using proximal remote sensing and with visible cameras as the payload of unmanned aerial vehicles (UAV).},
DOI = {10.3390/s21217312}
}



@Article{s21217397,
AUTHOR = {Wang, Yanjun and Li, Shaochun and Lin, Yunhao and Wang, Mengjie},
TITLE = {Lightweight Deep Neural Network Method for Water Body Extraction from High-Resolution Remote Sensing Images with Multisensors},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7397},
URL = {https://www.mdpi.com/1424-8220/21/21/7397},
PubMedID = {34770701},
ISSN = {1424-8220},
ABSTRACT = {Rapid and accurate extraction of water bodies from high-spatial-resolution remote sensing images is of great value for water resource management, water quality monitoring and natural disaster emergency response. For traditional water body extraction methods, it is difficult to select image texture and features, the shadows of buildings and other ground objects are in the same spectrum as water bodies, the existing deep convolutional neural network is difficult to train, the consumption of computing resources is large, and the methods cannot meet real-time requirements. In this paper, a water body extraction method based on lightweight MobileNetV2 is proposed and applied to multisensor high-resolution remote sensing images, such as GF-2, WorldView-2 and UAV orthoimages. This method was validated in two typical complex geographical scenes: water bodies for farmland irrigation, which have a broken shape and long and narrow area and are surrounded by many buildings in towns and villages; and water bodies in mountainous areas, which have undulating topography, vegetation coverage and mountain shadows all over. The results were compared with those of the support vector machine, random forest and U-Net models and also verified by generalization tests and the influence of spatial resolution changes. First, the results show that the F1-score and Kappa coefficients of the MobileNetV2 model extracting water bodies from three different high-resolution images were 0.75 and 0.72 for GF-2, 0.86 and 0.85 for Worldview-2 and 0.98 and 0.98 for UAV, respectively, which are higher than those of traditional machine learning models and U-Net. Second, the training time, number of parameters and calculation amount of the MobileNetV2 model were much lower than those of the U-Net model, which greatly improves the water body extraction efficiency. Third, in other more complex surface areas, the MobileNetV2 model still maintained relatively high accuracy of water body extraction. Finally, we tested the effects of multisensor models and found that training with lower and higher spatial resolution images combined can be beneficial, but that using just lower resolution imagery is ineffective. This study provides a reference for the efficient automation of water body classification and extraction under complex geographical environment conditions and can be extended to water resource investigation, management and planning.},
DOI = {10.3390/s21217397}
}



@Article{rs13214476,
AUTHOR = {Traore, Adama and Ata-Ul-Karim, Syed Tahir and Duan, Aiwang and Soothar, Mukesh Kumar and Traore, Seydou and Zhao, Ben},
TITLE = {Predicting Equivalent Water Thickness in Wheat Using UAV Mounted Multispectral Sensor through Deep Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {4476},
URL = {https://www.mdpi.com/2072-4292/13/21/4476},
ISSN = {2072-4292},
ABSTRACT = {The equivalent water thickness (EWT) is an important biophysical indicator of water status in crops. The effective monitoring of EWT in wheat under different nitrogen and water treatments is important for irrigation management in precision agriculture. This study aimed to investigate the performances of machine learning (ML) algorithms in retrieving wheat EWT. For this purpose, a rain shelter experiment (Exp. 1) with four irrigation quantities (0, 120, 240, 360 mm) and two nitrogen levels (75 and 255 kg N/ha), and field experiments (Exps. 2–3) with the same irrigation and rainfall water levels (360 mm) but different nitrogen levels (varying from 75 to 255 kg N/ha) were conducted in the North China Plain. The canopy reflectance was measured for all plots at 30 m using an unmanned aerial vehicle (UAV)-mounted multispectral camera. Destructive sampling was conducted immediately after the UAV flights to measure total fresh and dry weight. Deep Neural Network (DNN) is a special type of neural network, which has shown performance in regression analysis is compared with other machine learning (ML) models. A feature selection (FS) algorithm named the decision tree (DT) was used as the automatic relevance determination method to obtain the relative relevance of 5 out of 67 vegetation indices (Vis), which were used for estimating EWT. The selected VIs were used to estimate EWT using multiple linear regression (MLR), deep neural network multilayer perceptron (DNN-MLP), artificial neural networks multilayer perceptron (ANN-MLP), boosted tree regression (BRT), and support vector machines (SVMs). The results show that the DNN-MLP with R2 = 0.934, NSE = 0.933, RMSE = 0.028 g/cm2, and MAE of 0.017 g/cm2 outperformed other ML algorithms (ANN-MPL, BRT, and SVM- Polynomial) owing to its high capacity for estimating EWT as compared to other ML methods. Our findings support the conclusion that ML can potentially be applied in combination with VIs for retrieving EWT. Despite the complexity of the ML models, the EWT map should help farmers by improving the real-time irrigation efficiency of wheat by quantifying field water content and addressing variability.},
DOI = {10.3390/rs13214476}
}



@Article{electronics10222764,
AUTHOR = {Hassan, Syed-Ali and Rahim, Tariq and Shin, Soo-Young},
TITLE = {An Improved Deep Convolutional Neural Network-Based Autonomous Road Inspection Scheme Using Unmanned Aerial Vehicles},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {2764},
URL = {https://www.mdpi.com/2079-9292/10/22/2764},
ISSN = {2079-9292},
ABSTRACT = {Recent advancements in the field of machine learning (ML) provide opportunity to conduct research on autonomous devices for a variety of applications. Intelligent decision-making is a critical task for self-driving systems. An attempt is made in this study to use a deep learning (DL) approach for the early detection of road cracks, potholes, and the yellow lane. The accuracy is not sufficient after training with the default model. To enhance accuracy, a convolutional neural network (CNN) model with 13 convolutional layers, a softmax layer as an output layer, and two fully connected layers (FCN) are constructed. In order to achieve the deeper propagation and to prevent saturation in the training phase, mish activation is employed in the first 12 layers with a rectified linear unit (ReLU) activation function. The upgraded CNN model performs better than the default CNN model in terms of accuracy. For the varied situation, a revised and enriched dataset for road cracks, potholes, and the yellow lane is created. The yellow lane is detected and tracked in order to move the unmanned aerial vehicle (UAV) autonomously by following yellow lane. After identifying a yellow lane, the UAV performs autonomous navigation while concurrently detecting road cracks and potholes using the robot operating system within the UAV. The performance model is benchmarked using performance measures, such as accuracy, sensitivity, F1-score, F2-score, and dice-coefficient, which demonstrate that the suggested technique produces better outcomes.},
DOI = {10.3390/electronics10222764}
}



@Article{rs13224591,
AUTHOR = {Zhou, Xiaoteng and Liu, Chun and Akbar, Akram and Xue, Yun and Zhou, Yuan},
TITLE = {Spectral and Spatial Feature Integrated Ensemble Learning Method for Grading Urban River Network Water Quality},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {4591},
URL = {https://www.mdpi.com/2072-4292/13/22/4591},
ISSN = {2072-4292},
ABSTRACT = {Urban river networks have the characteristics of medium and micro scales, complex water quality, rapid change, and time–space incoherence. Aiming to monitor the water quality accurately, it is necessary to extract suitable features and establish a universal inversion model for key water quality parameters. In this paper, we describe a spectral- and spatial-feature-integrated ensemble learning method for urban river network water quality grading. We proposed an in situ sampling method for urban river networks. Factor and correlation analyses were applied to extract the spectral features. Moreover, we analyzed the maximum allowed bandwidth for feature bands. We demonstrated that spatial features can improve the accuracy of water quality grading using kernel canonical correlation analysis (KCCA). Based on the spectral and spatial features, an ensemble learning model was established for total phosphorus (TP) and ammonia nitrogen (NH3-N). Both models were evaluated by means of fivefold validation. Furthermore, we proposed an unmanned aerial vehicle (UAV)-borne water quality multispectral remote sensing application process for urban river networks. Based on the process, we tested the model in practice. The experiment confirmed that our model can improve the grading accuracy by 30% compared to other machine learning models that use only spectral features. Our research can extend the application field of water quality remote sensing to complex urban river networks.},
DOI = {10.3390/rs13224591}
}



@Article{sym13112190,
AUTHOR = {Hashim, Wahidah and Eng, Lim Soon and Alkawsi, Gamal and Ismail, Rozita and Alkahtani, Ammar Ahmed and Dzulkifly, Sumayyah and Baashar, Yahia and Hussain, Azham},
TITLE = {A Hybrid Vegetation Detection Framework: Integrating Vegetation Indices and Convolutional Neural Network},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2190},
URL = {https://www.mdpi.com/2073-8994/13/11/2190},
ISSN = {2073-8994},
ABSTRACT = {Vegetation inspection and monitoring is a time-consuming task. In the era of industrial revolution 4.0 (IR 4.0), unmanned aerial vehicles (UAV), commercially known as drones, are in demand, being adopted for vegetation inspection and monitoring activities. However, most off-the-shelf drones are least favoured by vegetation maintenance departments for on-site inspection due to limited spectral bands camera restricting advanced vegetation analysis. Most of these drones are normally equipped with a normal red, green, and blue (RGB) camera. Additional spectral bands are found to produce more accurate analysis during vegetation inspection, but at the cost of advanced camera functionalities, such as multispectral camera. Vegetation indices (VI) is a technique to maximize detection sensitivity related to vegetation characteristics while minimizing other factors which are not categorised otherwise. The emergence of machine learning has slowly influenced the existing vegetation analysis technique in order to improve detection accuracy. This study focuses on exploring VI techniques in identifying vegetation objects. The selected VIs investigated are Visible Atmospheric Resistant Index (VARI), Green Leaf Index (GLI), and Vegetation Index Green (VIgreen). The chosen machine learning technique is You Only Look Once (YOLO), which is a clever convolutional neural network (CNN) offering object detection in real time. The CNN model has a symmetrical structure along the direction of the tensor flow. Several series of data collection have been conducted at identified locations to obtain aerial images. The proposed hybrid methods were tested on captured aerial images to observe vegetation detection performance. Segmentation in image analysis is a process to divide the targeted pixels for further detection testing. Based on our findings, more than 70% of the vegetation objects in the images were accurately detected, which reduces the misdetection issue faced by previous VI techniques. On the other hand, hybrid segmentation methods perform best with the combination of VARI and YOLO at 84% detection accuracy.},
DOI = {10.3390/sym13112190}
}



@Article{rs13224632,
AUTHOR = {Teodoro, Paulo Eduardo and Teodoro, Larissa Pereira Ribeiro and Baio, Fábio Henrique Rojo and da Silva Junior, Carlos Antonio and dos Santos, Regimar Garcia and Ramos, Ana Paula Marques and Pinheiro, Mayara Maezano Faita and Osco, Lucas Prado and Gonçalves, Wesley Nunes and Carneiro, Alexsandro Monteiro and Junior, José Marcato and Pistori, Hemerson and Shiratsuchi, Luciano Shozo},
TITLE = {Predicting Days to Maturity, Plant Height, and Grain Yield in Soybean: A Machine and Deep Learning Approach Using Multispectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {4632},
URL = {https://www.mdpi.com/2072-4292/13/22/4632},
ISSN = {2072-4292},
ABSTRACT = {In soybean, there is a lack of research aiming to compare the performance of machine learning (ML) and deep learning (DL) methods to predict more than one agronomic variable, such as days to maturity (DM), plant height (PH), and grain yield (GY). As these variables are important to developing an overall precision farming model, we propose a machine learning approach to predict DM, PH, and GY for soybean cultivars based on multispectral bands. The field experiment considered 524 genotypes of soybeans in the 2017/2018 and 2018/2019 growing seasons and a multitemporal–multispectral dataset collected by embedded sensor in an unmanned aerial vehicle (UAV). We proposed a multilayer deep learning regression network, trained during 2000 epochs using an adaptive subgradient method, a random Gaussian initialization, and a 50% dropout in the first hidden layer for regularization. Three different scenarios, including only spectral bands, only vegetation indices, and spectral bands plus vegetation indices, were adopted to infer each variable (PH, DM, and GY). The DL model performance was compared against shallow learning methods such as random forest (RF), support vector machine (SVM), and linear regression (LR). The results indicate that our approach has the potential to predict soybean-related variables using multispectral bands only. Both DL and RF models presented a strong (r surpassing 0.77) prediction capacity for the PH variable, regardless of the adopted input variables group. Our results demonstrated that the DL model (r = 0.66) was superior to predict DM when the input variable was the spectral bands. For GY, all machine learning models evaluated presented similar performance (r ranging from 0.42 to 0.44) for each tested scenario. In conclusion, this study demonstrated an efficient approach to a computational solution capable of predicting multiple important soybean crop variables based on remote sensing data. Future research could benefit from the information presented here and be implemented in subsequent processes related to soybean cultivars or other types of agronomic crops.},
DOI = {10.3390/rs13224632}
}



@Article{rs13224643,
AUTHOR = {Liu, Jinhua and Ding, Jianli and Ge, Xiangyu and Wang, Jingzhe},
TITLE = {Evaluation of Total Nitrogen in Water via Airborne Hyperspectral Data: Potential of Fractional Order Discretization Algorithm and Discrete Wavelet Transform Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {4643},
URL = {https://www.mdpi.com/2072-4292/13/22/4643},
ISSN = {2072-4292},
ABSTRACT = {Controlling and managing surface source pollution depends on the rapid monitoring of total nitrogen in water. However, the complex factors affecting water quality (plant shading and suspended matter in water) make direct estimation extremely challenging. Considering the spectral response mechanisms of emergent plants, we coupled discrete wavelet transform (DWT) and fractional order discretization (FOD) techniques with three machine learning models (random forest (RF), bagging algorithm (bagging), and eXtreme Gradient Boosting (XGBoost)) to mine this potential spectral information. A total of 567 models were developed, and airborne hyperspectral data processed with various DWT scales and FOD techniques were compared. The effective information in the hyperspectral reflectance data were better emphasized after DWT processing. After DWT processing the original spectrum (OR), its sensitivity to TN in water was maximally improved by 0.22, and the correlation between FOD and TN in water was optimally increased by 0.57. The transformed spectral information enhanced the TN model accuracy, especially for FOD after DWT. For RF, 82% of the model R2 values improved by 0.02~0.72 compared to the model using FOD spectra; 78.8% of the bagging values improved by 0.01~0.53 and 65.0% of the XGBoost values improved by 0.01~0.64. The XGBoost model with DWT coupled with grey relation analysis (GRA) yielded the best estimation accuracy, with the highest precision of R2 = 0.91 for L6. In conclusion, appropriately scaled DWT analysis can substantially improve the accuracy of extracting TN from UAV hyperspectral images. These outcomes may facilitate the further development of accurate water quality monitoring in sophisticated global waters from drone or satellite hyperspectral data.},
DOI = {10.3390/rs13224643}
}



@Article{rs13224716,
AUTHOR = {Zhu, Wanxue and Rezaei, Ehsan Eyshi and Nouri, Hamideh and Yang, Ting and Li, Binbin and Gong, Huarui and Lyu, Yun and Peng, Jinbang and Sun, Zhigang},
TITLE = {Quick Detection of Field-Scale Soil Comprehensive Attributes via the Integration of UAV and Sentinel-2B Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {4716},
URL = {https://www.mdpi.com/2072-4292/13/22/4716},
ISSN = {2072-4292},
ABSTRACT = {Satellite and unmanned aerial vehicle (UAV) remote sensing can be used to estimate soil properties; however, little is known regarding the effects of UAV and satellite remote sensing data integration on the estimation of soil comprehensive attributes, or how to estimate quickly and robustly. In this study, we tackled those gaps by employing UAV multispectral and Sentinel-2B data to estimate soil salinity and chemical properties over a large agricultural farm (400 ha) covered by different crops and harvest areas at the coastal saline-alkali land of the Yellow River Delta of China in 2019. Spatial information of soil salinity, organic matter, available/total nitrogen content, and pH at 0&ndash;10 cm and 10&ndash;20 cm layers were obtained via ground sampling (n = 195) and two-dimensional spatial interpolation, aiming to overlap the soil information with remote sensing information. The exploratory factor analysis was conducted to generate latent variables, which represented the salinity and chemical characteristics of the soil. A machine learning algorithm (random forest) was applied to estimate soil attributes. Our results indicated that the integration of UAV texture and Sentinel-2B spectral data as random forest model inputs improved the accuracy of latent soil variable estimation. The remote sensing-based information from cropland (crop-based) had a higher accuracy compared to estimations performed on bare soil (soil-based). Therefore, the crop-based approach, along with the integration of UAV texture and Sentinel-2B data, is recommended for the quick assessment of soil comprehensive attributes.},
DOI = {10.3390/rs13224716}
}



@Article{rs13234757,
AUTHOR = {Sekrecka, Aleksandra},
TITLE = {Application of the XBoost Regressor for an A Priori Prediction of UAV Image Quality},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {4757},
URL = {https://www.mdpi.com/2072-4292/13/23/4757},
ISSN = {2072-4292},
ABSTRACT = {In general, the quality of imagery from Unmanned Aerial Vehicles (UAVs) is evaluated after the flight, and then a decision is made on the further value and use of the acquired data. In this paper, an a priori (preflight) image quality prediction methodology is proposed to estimate the preflight image quality and to avoid unfavourable flights, which is extremely important from a time and cost management point of view. The XBoost Regressor model and cross-validation were used for machine learning of the model and image quality prediction. The model was learned on a rich database of real-world images acquired from UAVs under conditions varying in both sensor type, UAV type, exposure parameters, weather, topography, and land cover. Radiometric quality indices (SNR, Entropy, PIQE, NIQE, BRISQUE, and NRPBM) were calculated for each image to train and test the model and to assess the accuracy of image quality prediction. Different variants of preflight parameter knowledge were considered in the study. The proposed methodology offers the possibility of predicting image quality with high accuracy. The correlation coefficient between the actual and predicted image quality, depending on the number of parameters known a priori, ranged from 0.90 to 0.96. The methodology was designed for data acquired from a UAV. Similar prediction accuracy is expected for other low-altitude or close-range photogrammetric data.},
DOI = {10.3390/rs13234757}
}



@Article{telecom2040027,
AUTHOR = {Singh, Simran and Kumbhar, Abhaykumar and Güvenç, İsmail and Sichitiu, Mihail L.},
TITLE = {Intelligent Interference Management in UAV-Based HetNets},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {472--488},
URL = {https://www.mdpi.com/2673-4001/2/4/27},
ISSN = {2673-4001},
ABSTRACT = {Unmanned aerial vehicles (UAVs) can play a key role in meeting certain demands of cellular networks. UAVs can be used not only as user equipment (UE) in cellular networks but also as mobile base stations (BSs) wherein they can either augment conventional BSs by adapting their position to serve the changing traffic and connectivity demands or temporarily replace BSs that are damaged due to natural disasters. The flexibility of UAVs allows them to provide coverage to UEs in hot-spots, at cell-edges, in coverage holes, or regions with scarce cellular infrastructure. In this work, we study how UAV locations and other cellular parameters may be optimized in such scenarios to maximize the spectral efficiency (SE) of the network. We compare the performance of machine learning (ML) techniques with conventional optimization approaches. We found that, on an average, a double deep Q learning approach can achieve 93.46% of the optimal median SE and 95.83% of the optimal mean SE. A simple greedy approach, which tunes the parameters of each BS and UAV independently, performed very well in all the cases that we tested. These computationally efficient approaches can be utilized to enhance the network performance in existing cellular networks.},
DOI = {10.3390/telecom2040027}
}



@Article{rs13234873,
AUTHOR = {Fraser, Benjamin T. and Congalton, Russell G.},
TITLE = {Monitoring Fine-Scale Forest Health Using Unmanned Aerial Systems (UAS) Multispectral Models},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {4873},
URL = {https://www.mdpi.com/2072-4292/13/23/4873},
ISSN = {2072-4292},
ABSTRACT = {Forest disturbances&mdash;driven by pests, pathogens, and discrete events&mdash;have led to billions of dollars in lost ecosystem services and management costs. To understand the patterns and severity of these stressors across complex landscapes, there must be an increase in reliable data at scales compatible with management actions. Unmanned aerial systems (UAS or UAV) offer a capable platform for collecting local scale (e.g., individual tree) forestry data. In this study, we evaluate the capability of UAS multispectral imagery and freely available National Agricultural Imagery Program (NAIP) imagery for differentiating coniferous healthy, coniferous stressed, deciduous healthy, deciduous stressed, and degraded individual trees throughout a complex, mixed-species forests. These methods are first compared to assessments of crown vigor in the field, to evaluate the potential in supplementing this resource intensive practice. This investigation uses the random forest and support vector machine (SVM) machine learning algorithms to classify the imagery into the five forest health classes. Using the random forest classifier, the UAS imagery correctly classified five forest Health classes with an overall accuracy of 65.43%. Using similar methods, the high-resolution airborne NAIP imagery achieved an overall accuracy of 50.50% for the five health classes, a reduction of 14.93%. When these classes were generalized to healthy, stressed, and degraded trees, the accuracy improved to 71.19%, using UAS imagery, and 70.62%, using airborne imagery. Further analysis into the precise calibration of UAS multispectral imagery, a refinement of image segmentation methods, and the fusion of these data with more widely distributed remotely sensed imagery would further enhance the potential of these methods to more effectively and efficiently collect forest health information from the UAS instead of using field methods.},
DOI = {10.3390/rs13234873}
}



@Article{rs13234910,
AUTHOR = {Zhou, Rui and Yang, Chao and Li, Enhua and Cai, Xiaobin and Yang, Jiao and Xia, Ying},
TITLE = {Object-Based Wetland Vegetation Classification Using Multi-Feature Selection of Unoccupied Aerial Vehicle RGB Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {4910},
URL = {https://www.mdpi.com/2072-4292/13/23/4910},
ISSN = {2072-4292},
ABSTRACT = {Wetland vegetation is an important component of wetland ecosystems and plays a crucial role in the ecological functions of wetland environments. Accurate distribution mapping and dynamic change monitoring of vegetation are essential for wetland conservation and restoration. The development of unoccupied aerial vehicles (UAVs) provides an efficient and economic platform for wetland vegetation classification. In this study, we evaluated the feasibility of RGB imagery obtained from the DJI Mavic Pro for wetland vegetation classification at the species level, with a specific application to Honghu, which is listed as a wetland of international importance. A total of ten object-based image analysis (OBIA) scenarios were designed to assess the contribution of five machine learning algorithms to the classification accuracy, including Bayes, K-nearest neighbor (KNN), support vector machine (SVM), decision tree (DT), and random forest (RF), multi-feature combinations and feature selection implemented by the recursive feature elimination algorithm (RFE). The overall accuracy and kappa coefficient were compared to determine the optimal classification method. The main results are as follows: (1) RF showed the best performance among the five machine learning algorithms, with an overall accuracy of 89.76% and kappa coefficient of 0.88 when using 53 features (including spectral features (RGB bands), height information, vegetation indices, texture features, and geometric features) for wetland vegetation classification. (2) The RF model constructed by only spectral features showed poor classification results, with an overall accuracy of 73.66% and kappa coefficient of 0.70. By adding height information, VIs, texture features, and geometric features to construct the RF model layer by layer, the overall accuracy was improved by 8.78%, 3.41%, 2.93%, and 0.98%, respectively, demonstrating the importance of multi-feature combinations. (3) The contribution of different types of features to the RF model was not equal, and the height information was the most important for wetland vegetation classification, followed by the vegetation indices. (4) The RFE algorithm effectively reduced the number of original features from 53 to 36, generating an optimal feature subset for wetland vegetation classification. The RF based on the feature selection result of RFE (RF-RFE) had the best performance in ten scenarios, and provided an overall accuracy of 90.73%, which was 0.97% higher than the RF without feature selection. The results illustrate that the combination of UAV-based RGB imagery and the OBIA approach provides a straightforward, yet powerful, approach for high-precision wetland vegetation classification at the species level, in spite of limited spectral information. Compared with satellite data or UAVs equipped with other types of sensors, UAVs with RGB cameras are more cost efficient and convenient for wetland vegetation monitoring and mapping.},
DOI = {10.3390/rs13234910}
}



@Article{app112411611,
AUTHOR = {Igonin, Dmitry M. and Kolganov, Pavel A. and Tiumentsev, Yury V.},
TITLE = {Situational Awareness and Problems of Its Formation in the Tasks of UAV Behavior Control},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {11611},
URL = {https://www.mdpi.com/2076-3417/11/24/11611},
ISSN = {2076-3417},
ABSTRACT = {Situational awareness formation is one of the most critical elements in solving the problem of UAV behavior control. It aims to provide information support for UAV behavior control according to its objectives and tasks to be completed. We consider the UAV to be a type of controlled dynamic system. The article shows the place of UAVs in the hierarchy of dynamic systems. We introduce the concepts of UAV behavior and activity and formulate requirements for algorithms for controlling UAV behavior. We propose the concept of situational awareness as applied to the problem of behavior control of highly autonomous UAVs (HA-UAVs) and analyze the levels and types of this situational awareness. We show the specifics of situational awareness formation for UAVs and analyze its differences from situational awareness for manned aviation and remotely piloted UAVs. We propose the concept of situational awareness as applied to the problem of UAV behavior control and analyze the levels and types of this situational awareness. We highlight and discuss in more detail two crucial elements of situational awareness for HA-UAVs. The first of them is related to the analysis and prediction of the behavior of objects in the vicinity of the HA-UAV. The general considerations involved in solving this problem, including the problem of analyzing the group behavior of such objects, are discussed. As an illustrative example, the solution to the problem of tracking an aircraft maneuvering in the vicinity of a HA-UAV is given. The second element of situational awareness is related to the processing of visual information, which is one of the primary sources of situational awareness formation required for the operation of the HA-UAV control system. As an example here, we consider solving the problem of semantic segmentation of images processed when selecting a landing site for the HA-UAV in unfamiliar terrain. Both of these problems are solved using machine learning methods and tools. In the field of situational awareness for HA-UAVs, there are several problems that need to be solved. We formulate some of these problems and briefly describe them.},
DOI = {10.3390/app112411611}
}



@Article{rs13244985,
AUTHOR = {Kilwenge, Regina and Adewopo, Julius and Sun, Zhanli and Schut, Marc},
TITLE = {UAV-Based Mapping of Banana Land Area for Village-Level Decision-Support in Rwanda},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {4985},
URL = {https://www.mdpi.com/2072-4292/13/24/4985},
ISSN = {2072-4292},
ABSTRACT = {Crop monitoring is crucial to understand crop production changes, agronomic practice decision-support, pests/diseases mitigation, and developing climate change adaptation strategies. Banana, an important staple food and cash crop in East Africa, is threatened by Banana Xanthomonas Wilt (BXW) disease. Yet, there is no up-to-date information about the spatial distribution and extent of banana lands, especially in Rwanda, where banana plays a key role in food security and livelihood. Therefore, delineation of banana-cultivated lands is important to prioritize resource allocation for optimal productivity. We mapped the spatial extent of smallholder banana farmlands by acquiring and processing high-resolution (25 cm/px) multispectral unmanned aerial vehicles (UAV) imageries, across four villages in Rwanda. Georeferenced ground-truth data on different land cover classes were combined with reflectance data and vegetation indices (NDVI, GNDVI, and EVI2) and compared using pixel-based supervised multi-classifiers (support vector models-SVM, classification and regression trees-CART, and random forest&ndash;RF), based on varying ground-truth data richness. Results show that RF consistently outperformed other classifiers regardless of data richness, with overall accuracy above 95%, producer&rsquo;s/user&rsquo;s accuracies above 92%, and kappa coefficient above 0.94. Estimated banana farmland areal coverage provides concrete baseline for extension-delivery efforts in terms of targeting banana farmers relative to their scale of production, and highlights opportunity to combine UAV-derived data with machine-learning methods for rapid landcover classification.},
DOI = {10.3390/rs13244985}
}



@Article{rs13245166,
AUTHOR = {Wang, Jianjun and Zhou, Qi and Shang, Jiali and Liu, Chang and Zhuang, Tingxuan and Ding, Junjie and Xian, Yunyu and Zhao, Lingtian and Wang, Weiling and Zhou, Guisheng and Tan, Changwei and Huo, Zhongyang},
TITLE = {UAV- and Machine Learning-Based Retrieval of Wheat SPAD Values at the Overwintering Stage for Variety Screening},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {5166},
URL = {https://www.mdpi.com/2072-4292/13/24/5166},
ISSN = {2072-4292},
ABSTRACT = {In recent years, the delay in sowing has become a major obstacle to high wheat yield in Jiangsu Province, one of the major wheat producing areas in China; hence, it is necessary to screen wheat varieties are resilient for late sowing. This study aimed to provide an effective, fast, and non-destructive monitoring method of soil plant analysis development (SPAD) values, which can represent leaf chlorophyll contents, for late-sown winter wheat variety screening. This study acquired multispectral images using an unmanned aerial vehicle (UAV) at the overwintering stage of winter wheat growth, and further processed these images to extract reflectance of five single spectral bands and calculated 26 spectral vegetation indices. Based on these 31 variables, this study combined three variable selection methods (i.e., recursive feature elimination (RFE), random forest (RF), and Pearson correlation coefficient (r)) with four machine learning algorithms (i.e., random forest regression (RFR), linear kernel-based support vector regression (SVR), radial basis function (RBF) kernel-based SVR, and sigmoid kernel-based SVR), resulted in seven SVR models (i.e., RFE-SVR_linear, RF-SVR_linear, RF-SVR_RBF, RF-SVR_sigmoid, r-SVR_linear, r-SVR_RBF, and r-SVR_sigmoid) and three RFR models (i.e., RFE-RFR, RF-RFR, and r-RFR). The performances of the 10 machine learning models were evaluated and compared with each other according to the achieved coefficient of determination (R2), residual prediction deviation (RPD), root mean square error (RMSE), and relative RMSE (RRMSE) in SPAD estimation. Of the 10 models, the best one was the RF-SVR_sigmoid model, which was the combination of the RF variable selection method and the sigmoid kernel-based SVR algorithm. It achieved high accuracy in estimating SPAD values of the wheat canopy (R2 = 0.754, RPD = 2.017, RMSE = 1.716 and RRMSE = 4.504%). The newly developed UAV- and machine learning-based model provided a promising and real time method to monitor chlorophyll contents at the overwintering stage, which can benefit late-sown winter wheat variety screening.},
DOI = {10.3390/rs13245166}
}



@Article{rs13245173,
AUTHOR = {Cao, Xiaofeng and Liu, Yulin and Yu, Rui and Han, Dejun and Su, Baofeng},
TITLE = {A Comparison of UAV RGB and Multispectral Imaging in Phenotyping for Stay Green of Wheat Population},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {5173},
URL = {https://www.mdpi.com/2072-4292/13/24/5173},
ISSN = {2072-4292},
ABSTRACT = {High throughput phenotyping (HTP) for wheat (Triticum aestivum L.) stay green (SG) is expected in field breeding as SG is a beneficial phenotype for wheat high yield and environment adaptability. The RGB and multispectral imaging based on the unmanned aerial vehicle (UAV) are widely popular multi-purpose HTP platforms for crops in the field. The purpose of this study was to compare the potential of UAV RGB and multispectral images (MSI) in SG phenotyping of diversified wheat germplasm. The multi-temporal images of 450 samples (406 wheat genotypes) were obtained and the color indices (CIs) from RGB and MSI and spectral indices (SIs) from MSI were extracted, respectively. The four indices (CIs in RGB, CIs in MSI, SIs in MSI, and CIs + SIs in MSI) were used to detect four SG stages, respectively, by machine learning classifiers. Then, all indices&rsquo; dynamics were analyzed and the indices that varied monotonously and significantly were chosen to calculate wheat temporal stay green rates (SGR) to quantify the SG in diverse genotypes. The correlations between indices&rsquo; SGR and wheat yield were assessed and the dynamics of some indices&rsquo; SGR with different yield correlations were tracked in three visual observed SG grades samples. In SG stage detection, classifiers best average accuracy reached 93.20&ndash;98.60% and 93.80&ndash;98.80% in train and test set, respectively, and the SIs containing red edge or near-infrared band were more effective than the CIs calculated only by visible bands. Indices&rsquo; temporal SGR could quantify SG changes on a population level, but showed some differences in the correlation with yield and in tracking visual SG grades samples. In SIs, the SGR of Normalized Difference Red-edge Index (NDRE), Red-edge Chlorophyll Index (CIRE), and Normalized Difference Vegetation Index (NDVI) in MSI showed high correlations with yield and could track visual SG grades at an earlier stage of grain filling. In CIs, the SGR of Normalized Green Red Difference Index (NGRDI), the Green Leaf Index (GLI) in RGB and MSI showed low correlations with yield and could only track visual SG grades at late grain filling stage and that of Norm Red (NormR) in RGB images failed to track visual SG grades. This study preliminarily confirms the MSI is more available and reliable than RGB in phenotyping for wheat SG. The index-based SGR in this study could act as HTP reference solutions for SG in diversified wheat genotypes.},
DOI = {10.3390/rs13245173}
}



@Article{rs14010046,
AUTHOR = {Wei, Lele and Luo, Yusen and Xu, Lizhang and Zhang, Qian and Cai, Qibing and Shen, Mingjun},
TITLE = {Deep Convolutional Neural Network for Rice Density Prescription Map at Ripening Stage Using Unmanned Aerial Vehicle-Based Remotely Sensed Images},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {46},
URL = {https://www.mdpi.com/2072-4292/14/1/46},
ISSN = {2072-4292},
ABSTRACT = {In this paper, UAV (unmanned aerial vehicle, DJI Phantom4RTK) and YOLOv4 (You Only Look Once) target detection deep neural network methods were employed to collected mature rice images and detect rice ears to produce a rice density prescription map. The YOLOv4 model was used for rice ear quick detection of rice images captured by a UAV. The Kriging interpolation algorithm was used in ArcGIS to make rice density prescription maps. Mature rice images collected by a UAV were marked manually and used to build the training and testing datasets. The resolution of the images was 300 &times; 300 pixels. The batch size was 2, and the initial learning rate was 0.01, and the mean average precision (mAP) of the best trained model was 98.84%. Exceptionally, the network ability to detect rice in different health states was also studied with a mAP of 95.42% in the no infection rice images set, 98.84% in the mild infection rice images set, 94.35% in the moderate infection rice images set, and 93.36% in the severe infection rice images set. According to the severity of rice sheath blight, which can cause rice leaves to wither and turn yellow, the blighted grain percentage increased and the thousand-grain weight decreased, the rice images were divided into these four infection levels. The ability of the network model (R2 = 0.844) was compared with traditional image processing segmentation methods (R2 = 0.396) based on color and morphology features and machine learning image segmentation method (Support Vector Machine, SVM R2 = 0.0817, and K-means R2 = 0.1949) for rice ear counting. The results highlight that the CNN has excellent robustness, and can generate a wide range of rice density prescription maps.},
DOI = {10.3390/rs14010046}
}



@Article{drones6010005,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Heravi, Amirhossein and Thaheem, Muhammad Jamaluddin and Maqsoom, Ahsen},
TITLE = {Inspecting Buildings Using Drones and Computer Vision: A Machine Learning Approach to Detect Cracks and Damages},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {5},
URL = {https://www.mdpi.com/2504-446X/6/1/5},
ISSN = {2504-446X},
ABSTRACT = {Manual inspection of infrastructure damages such as building cracks is difficult due to the objectivity and reliability of assessment and high demands of time and costs. This can be automated using unmanned aerial vehicles (UAVs) for aerial imagery of damages. Numerous computer vision-based approaches have been applied to address the limitations of crack detection but they have their limitations that can be overcome by using various hybrid approaches based on artificial intelligence (AI) and machine learning (ML) techniques. The convolutional neural networks (CNNs), an application of the deep learning (DL) method, display remarkable potential for automatically detecting image features such as damages and are less sensitive to image noise. A modified deep hierarchical CNN architecture has been used in this study for crack detection and damage assessment in civil infrastructures. The proposed architecture is based on 16 convolution layers and a cycle generative adversarial network (CycleGAN). For this study, the crack images were collected using UAVs and open-source images of mid to high rise buildings (five stories and above) constructed during 2000 in Sydney, Australia. Conventionally, a CNN network only utilizes the last layer of convolution. However, our proposed network is based on the utility of multiple layers. Another important component of the proposed CNN architecture is the application of guided filtering (GF) and conditional random fields (CRFs) to refine the predicted outputs to get reliable results. Benchmarking data (600 images) of Sydney-based buildings damages was used to test the proposed architecture. The proposed deep hierarchical CNN architecture produced superior performance when evaluated using five methods: GF method, Baseline (BN) method, Deep-Crack BN, Deep-Crack GF, and SegNet. Overall, the GF method outperformed all other methods as indicated by the global accuracy (0.990), class average accuracy (0.939), mean intersection of the union overall classes (IoU) (0.879), precision (0.838), recall (0.879), and F-score (0.8581) values. Overall, the proposed CNN architecture provides the advantages of reduced noise, highly integrated supervision of features, adequate learning, and aggregation of both multi-scale and multilevel features during the training procedure along with the refinement of the overall output predictions.},
DOI = {10.3390/drones6010005}
}



@Article{drones6010008,
AUTHOR = {Basan, Elena and Basan, Alexandr and Nekrasov, Alexey and Fidge, Colin and Sushkin, Nikita and Peskova, Olga},
TITLE = {GPS-Spoofing Attack Detection Technology for UAVs Based on Kullback&ndash;Leibler Divergence},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {8},
URL = {https://www.mdpi.com/2504-446X/6/1/8},
ISSN = {2504-446X},
ABSTRACT = {Here, we developed a method for detecting cyber security attacks aimed at spoofing the Global Positioning System (GPS) signal of an Unmanned Aerial Vehicle (UAV). Most methods for detecting UAV anomalies indicative of an attack use machine learning or other such methods that compare normal behavior with abnormal behavior. Such approaches require large amounts of data and significant &ldquo;training&rdquo; time to prepare and implement the system. Instead, we consider a new approach based on other mathematical methods for detecting UAV anomalies without the need to first collect a large amount of data and describe normal behavior patterns. Doing so can simplify the process of creating an anomaly detection system, which can further facilitate easier implementation of intrusion detection systems in UAVs. This article presents issues related to ensuring the information security of UAVs. Development of the GPS spoofing detection method for UAVs is then described, based on a preliminary study that made it possible to form a mathematical apparatus for solving the problem. We then explain the necessary analysis of parameters and methods of data normalization, and the analysis of the Kullback&mdash;Leibler divergence measure needed to detect anomalies in UAV systems.},
DOI = {10.3390/drones6010008}
}



@Article{rs14010199,
AUTHOR = {Carbonell-Rivera, Juan Pedro and Torralba, Jesús and Estornell, Javier and Ruiz, Luis Ángel and Crespo-Peremarch, Pablo},
TITLE = {Classification of Mediterranean Shrub Species from UAV Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {199},
URL = {https://www.mdpi.com/2072-4292/14/1/199},
ISSN = {2072-4292},
ABSTRACT = {Modelling fire behaviour in forest fires is based on meteorological, topographical, and vegetation data, including species&rsquo; type. To accurately parameterise these models, an inventory of the area of analysis with the maximum spatial and temporal resolution is required. This study investigated the use of UAV-based digital aerial photogrammetry (UAV-DAP) point clouds to classify tree and shrub species in Mediterranean forests, and this information is key for the correct generation of wildfire models. In July 2020, two test sites located in the Natural Park of Sierra Calderona (eastern Spain) were analysed, registering 1036 vegetation individuals as reference data, corresponding to 11 shrub and one tree species. Meanwhile, photogrammetric flights were carried out over the test sites, using a UAV DJI Inspire 2 equipped with a Micasense RedEdge multispectral camera. Geometrical, spectral, and neighbour-based features were obtained from the resulting point cloud generated. Using these features, points belonging to tree and shrub species were classified using several machine learning methods, i.e., Decision Trees, Extra Trees, Gradient Boosting, Random Forest, and MultiLayer Perceptron. The best results were obtained using Gradient Boosting, with a mean cross-validation accuracy of 81.7% and 91.5% for test sites 1 and 2, respectively. Once the best classifier was selected, classified points were clustered based on their geometry and tested with evaluation data, and overall accuracies of 81.9% and 96.4% were obtained for test sites 1 and 2, respectively. Results showed that the use of UAV-DAP allows the classification of Mediterranean tree and shrub species. This technique opens a wide range of possibilities, including the identification of species as a first step for further extraction of structure and fuel variables as input for wildfire behaviour models.},
DOI = {10.3390/rs14010199}
}



@Article{f13010048,
AUTHOR = {Kamarulzaman, Aisyah Marliza Muhmad and Wan Mohd Jaafar, Wan Shafrina and Abdul Maulud, Khairul Nizam and Saad, Siti Nor Maizah and Omar, Hamdan and Mohan, Midhun},
TITLE = {Integrated Segmentation Approach with Machine Learning Classifier in Detecting and Mapping Post Selective Logging Impacts Using UAV Imagery},
JOURNAL = {Forests},
VOLUME = {13},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {48},
URL = {https://www.mdpi.com/1999-4907/13/1/48},
ISSN = {1999-4907},
ABSTRACT = {Selective logging can cause significant impacts on the residual stands, affecting biodiversity and leading to environmental changes. Proper monitoring and mapping of the impacts from logging activities, such as the stumps, felled logs, roads, skid trails, and forest canopy gaps, are crucial for sustainable forest management operations. The purpose of this study is to assess the indicators of selective logging impacts by detecting the individual stumps as the main indicators, evaluating the performance of classification methods to assess the impacts and identifying forest gaps from selective logging activities. The combination of forest inventory field plots and unmanned aerial vehicle (UAV) RGB and overlapped imaged were used in this study to assess these impacts. The study area is located in Ulu Jelai Forest Reserve in the central part of Peninsular Malaysia, covering an experimental study area of 48 ha. The study involved the integration of template matching (TM), object-based image analysis (OBIA), and machine learning classification&mdash;support vector machine (SVM) and artificial neural network (ANN). Forest features and tree stumps were classified, and the canopy height model was used for detecting forest canopy gaps in the post selective logging region. Stump detection using the integration of TM and OBIA produced an accuracy of 75.8% when compared with the ground data. Forest classification using SVM and ANN methods were adopted to extract other impacts from logging activities such as skid trails, felled logs, roads and forest canopy gaps. These methods provided an overall accuracy of 85% and kappa coefficient value of 0.74 when compared with conventional classifier. The logging operation also caused an 18.6% loss of canopy cover. The result derived from this study highlights the potential use of UAVs for efficient post logging impact analysis and can be used to complement conventional forest inventory practices.},
DOI = {10.3390/f13010048}
}



@Article{app12020670,
AUTHOR = {Tursunboev, Jamshid and Kang, Yong-Sung and Huh, Sung-Bum and Lim, Dong-Woo and Kang, Jae-Mo and Jung, Heechul},
TITLE = {Hierarchical Federated Learning for Edge-Aided Unmanned Aerial Vehicle Networks},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {670},
URL = {https://www.mdpi.com/2076-3417/12/2/670},
ISSN = {2076-3417},
ABSTRACT = {Federated learning (FL) allows UAVs to collaboratively train a globally shared machine learning model while locally preserving their private data. Recently, the FL in edge-aided unmanned aerial vehicle (UAV) networks has drawn an upsurge of research interest due to a bursting increase in heterogeneous data acquired by UAVs and the need to build the global model with privacy; however, a critical issue is how to deal with the non-independent and identically distributed (non-i.i.d.) nature of heterogeneous data while ensuring the convergence of learning. To effectively address this challenging issue, this paper proposes a novel and high-performing FL scheme, namely, the hierarchical FL algorithm, for the edge-aided UAV network, which exploits the edge servers located in base stations as intermediate aggregators with employing commonly shared data. Experiment results demonstrate that the proposed hierarchical FL algorithm outperforms several baseline FL algorithms and exhibits better convergence behavior.},
DOI = {10.3390/app12020670}
}



@Article{drones6010021,
AUTHOR = {Zhang, Ruohao and Condomines, Jean-Philippe and Lochin, Emmanuel},
TITLE = {A Multifractal Analysis and Machine Learning Based Intrusion Detection System with an Application in a UAS/RADAR System},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {21},
URL = {https://www.mdpi.com/2504-446X/6/1/21},
ISSN = {2504-446X},
ABSTRACT = {The rapid development of Internet of Things (IoT) technology, together with mobile network technology, has created a never-before-seen world of interconnection, evoking research on how to make it vaster, faster, and safer. To support the ongoing fight against the malicious misuse of networks, in this paper we propose a novel algorithm called AMDES (unmanned aerial system multifractal analysis intrusion detection system) for spoofing attack detection. This novel algorithm is based on both wavelet leader multifractal analysis (WLM) and machine learning (ML) principles. In earlier research on unmanned aerial systems (UAS), intrusion detection systems (IDS) based on multifractal (MF) spectral analysis have been used to provide accurate MF spectrum estimations of network traffic. Such an estimation is then used to detect and characterize flooding anomalies that can be observed in an unmanned aerial vehicle (UAV) network. However, the previous contributions have lacked the consideration of other types of network intrusions commonly observed in UAS networks, such as the man in the middle attack (MITM). In this work, this promising methodology has been accommodated to detect a spoofing attack within a UAS. This methodology highlights a robust approach in terms of false positive performance in detecting intrusions in a UAS location reporting system.},
DOI = {10.3390/drones6010021}
}



@Article{s22020601,
AUTHOR = {Sharma, Prakriti and Leigh, Larry and Chang, Jiyul and Maimaitijiang, Maitiniyazi and Caffé, Melanie},
TITLE = {Above-Ground Biomass Estimation in Oats Using UAV Remote Sensing and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {601},
URL = {https://www.mdpi.com/1424-8220/22/2/601},
PubMedID = {35062559},
ISSN = {1424-8220},
ABSTRACT = {Current strategies for phenotyping above-ground biomass in field breeding nurseries demand significant investment in both time and labor. Unmanned aerial vehicles (UAV) can be used to derive vegetation indices (VIs) with high throughput and could provide an efficient way to predict forage yield with high accuracy. The main objective of the study is to investigate the potential of UAV-based multispectral data and machine learning approaches in the estimation of oat biomass. UAV equipped with a multispectral sensor was flown over three experimental oat fields in Volga, South Shore, and Beresford, South Dakota, USA, throughout the pre- and post-heading growth phases of oats in 2019. A variety of vegetation indices (VIs) derived from UAV-based multispectral imagery were employed to build oat biomass estimation models using four machine-learning algorithms: partial least squares (PLS), support vector machine (SVM), Artificial neural network (ANN), and random forest (RF). The results showed that several VIs derived from the UAV collected images were significantly positively correlated with dry biomass for Volga and Beresford (r = 0.2&ndash;0.65), however, in South Shore, VIs were either not significantly or weakly correlated with biomass. For Beresford, approximately 70% of the variance was explained by PLS, RF, and SVM validation models using data collected during the post-heading phase. Likewise for Volga, validation models had lower coefficient of determination (R2 = 0.20&ndash;0.25) and higher error (RMSE = 700&ndash;800 kg/ha) than training models (R2 = 0.50&ndash;0.60; RMSE = 500&ndash;690 kg/ha). In South Shore, validation models were only able to explain approx. 15&ndash;20% of the variation in biomass, which is possibly due to the insignificant correlation values between VIs and biomass. Overall, this study indicates that airborne remote sensing with machine learning has potential for above-ground biomass estimation in oat breeding nurseries. The main limitation was inconsistent accuracy in model prediction across locations. Multiple-year spectral data, along with the inclusion of textural features like crop surface model (CSM) derived height and volumetric indicators, should be considered in future studies while estimating biophysical parameters like biomass.},
DOI = {10.3390/s22020601}
}



@Article{agronomy12010202,
AUTHOR = {Li, Zongpeng and Chen, Zhen and Cheng, Qian and Duan, Fuyi and Sui, Ruixiu and Huang, Xiuqiao and Xu, Honggang},
TITLE = {UAV-Based Hyperspectral and Ensemble Machine Learning for Predicting Yield in Winter Wheat},
JOURNAL = {Agronomy},
VOLUME = {12},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {202},
URL = {https://www.mdpi.com/2073-4395/12/1/202},
ISSN = {2073-4395},
ABSTRACT = {Winter wheat is a widely-grown cereal crop worldwide. Using growth-stage information to estimate winter wheat yields in a timely manner is essential for accurate crop management and rapid decision-making in sustainable agriculture, and to increase productivity while reducing environmental impact. UAV remote sensing is widely used in precision agriculture due to its flexibility and increased spatial and spectral resolution. Hyperspectral data are used to model crop traits because of their ability to provide continuous rich spectral information and higher spectral fidelity. In this study, hyperspectral image data of the winter wheat crop canopy at the flowering and grain-filling stages was acquired by a low-altitude unmanned aerial vehicle (UAV), and machine learning was used to predict winter wheat yields. Specifically, a large number of spectral indices were extracted from the spectral data, and three feature selection methods, recursive feature elimination (RFE), Boruta feature selection, and the Pearson correlation coefficient (PCC), were used to filter high spectral indices in order to reduce the dimensionality of the data. Four major basic learner models, (1) support vector machine (SVM), (2) Gaussian process (GP), (3) linear ridge regression (LRR), and (4) random forest (RF), were also constructed, and an ensemble machine learning model was developed by combining the four base learner models. The results showed that the SVM yield prediction model, constructed on the basis of the preferred features, performed the best among the base learner models, with an R2 between 0.62 and 0.73. The accuracy of the proposed ensemble learner model was higher than that of each base learner model; moreover, the R2 (0.78) for the yield prediction model based on Boruta&rsquo;s preferred characteristics was the highest at the grain-filling stage.},
DOI = {10.3390/agronomy12010202}
}



@Article{rs14020396,
AUTHOR = {Shi, Yue and Han, Liangxiu and Kleerekoper, Anthony and Chang, Sheng and Hu, Tongle},
TITLE = {Novel CropdocNet Model for Automated Potato Late Blight Disease Detection from Unmanned Aerial Vehicle-Based Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/2072-4292/14/2/396},
ISSN = {2072-4292},
ABSTRACT = {The accurate and automated diagnosis of potato late blight disease, one of the most destructive potato diseases, is critical for precision agricultural control and management. Recent advances in remote sensing and deep learning offer the opportunity to address this challenge. This study proposes a novel end-to-end deep learning model (CropdocNet) for accurate and automated late blight disease diagnosis from UAV-based hyperspectral imagery. The proposed method considers the potential disease-specific reflectance radiation variance caused by the canopy&rsquo;s structural diversity and introduces multiple capsule layers to model the part-to-whole relationship between spectral&ndash;spatial features and the target classes to represent the rotation invariance of the target classes in the feature space. We evaluate the proposed method with real UAV-based HSI data under controlled and natural field conditions. The effectiveness of the hierarchical features is quantitatively assessed and compared with the existing representative machine learning/deep learning methods on both testing and independent datasets. The experimental results show that the proposed model significantly improves accuracy when considering the hierarchical structure of spectral&ndash;spatial features, with average accuracies of 98.09% for the testing dataset and 95.75% for the independent dataset, respectively.},
DOI = {10.3390/rs14020396}
}



@Article{rs14020415,
AUTHOR = {Ilniyaz, Osman and Kurban, Alishir and Du, Qingyun},
TITLE = {Leaf Area Index Estimation of Pergola-Trained Vineyards in Arid Regions Based on UAV RGB and Multispectral Data Using Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {415},
URL = {https://www.mdpi.com/2072-4292/14/2/415},
ISSN = {2072-4292},
ABSTRACT = {The leaf area index (LAI), a valuable variable for assessing vine vigor, reflects nutrient concentrations in vineyards and assists in precise management, including fertilization, improving yield, quality, and vineyard uniformity. Although some vegetation indices (VIs) have been successfully used to assess LAI variations, they are unsuitable for vineyards of different types and structures. By calibrating the light extinction coefficient of a digital photography algorithm for proximal LAI measurements, this study aimed to develop VI-LAI models for pergola-trained vineyards based on high-resolution RGB and multispectral images captured by an unmanned aerial vehicle (UAV). The models were developed by comparing five machine learning (ML) methods, and a robust ensemble model was proposed using the five models as base learners. The results showed that the ensemble model outperformed the base models. The highest R2 and lowest RMSE values that were obtained using the best combination of VIs with multispectral data were 0.899 and 0.434, respectively; those obtained using the RGB data were 0.825 and 0.547, respectively. By improving the results by feature selection, ML methods performed better with multispectral data than with RGB images, and better with higher spatial resolution data than with lower resolution data. LAI variations can be monitored efficiently and accurately for large areas of pergola-trained vineyards using this framework.},
DOI = {10.3390/rs14020415}
}



@Article{agriculture12020124,
AUTHOR = {Song, Xiaoxin and Wu, Fei and Lu, Xiaotong and Yang, Tianle and Ju, Chengxin and Sun, Chengming and Liu, Tao},
TITLE = {The Classification of Farming Progress in Rice&ndash;Wheat Rotation Fields Based on UAV RGB Images and the Regional Mean Model},
JOURNAL = {Agriculture},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {124},
URL = {https://www.mdpi.com/2077-0472/12/2/124},
ISSN = {2077-0472},
ABSTRACT = {Extraction of farming progress information in rice&ndash;wheat rotation regions is an important topic in smart field research. In this study, a new method for the classification of farming progress types using unmanned aerial vehicle (UAV) RGB images and the proposed regional mean (RM) model is presented. First, RGB information was extracted from the images to create and select the optimal color indices. After index classification, we compared the brightness reflection of the corresponding grayscale map, the classification interval, and the standard deviation of each farming progress type. These comparisons showed that the optimal classification color indices were the normalized red&ndash;blue difference index (NRBDI), the normalized green&ndash;blue difference index (NGBDI), and the modified red&ndash;blue difference index (MRBDI). Second, the RM model was built according to the whole-field farming progress classification requirements to achieve the final classification. We verified the model accuracy, and the Kappa coefficients obtained by combining the NRBDI, NGBDI, and MRBDI with the RM model were 0.86, 0.82, and 0.88, respectively. The proposed method was then applied to predict UAV RGB images of unharvested wheat, harvested wheat, and tilled and irrigated fields. The results were compared with those obtained with traditional machine learning methods, that is, the support vector machine, maximum likelihood classification, and random forest methods. The NRBDI, NGBDI, and MRBDI were combined with the RM model to monitor farming progress of ground truth ROIs, and the Kappa coefficients obtained were 0.9134, 0.8738, and 0.9179, respectively, while traditional machine learning methods all produced a Kappa coefficient less than 0.7. The results indicate a significantly higher accuracy of the proposed method than those of the traditional machine learning classification methods for the identification of farming progress type. The proposed work provides an important reference for the application of UAV to the field classification of progress types.},
DOI = {10.3390/agriculture12020124}
}



@Article{telecom3010005,
AUTHOR = {Tsipi, Lefteris and Karavolos, Michail and Vouyioukas, Demosthenes},
TITLE = {An Unsupervised Machine Learning Approach for UAV-Aided Offloading of 5G Cellular Networks},
JOURNAL = {Telecom},
VOLUME = {3},
YEAR = {2022},
NUMBER = {1},
PAGES = {86--102},
URL = {https://www.mdpi.com/2673-4001/3/1/5},
ISSN = {2673-4001},
ABSTRACT = {Today&rsquo;s terrestrial cellular communications networks face difficulties in serving coexisting users and devices due to the enormous demands of mass connectivity. Further, natural disasters and unexpected events lead to an unpredictable amount of data traffic, thus causing congestion to the network. In such cases, the addition of on-demand network entities, such as fixed or aerial base stations, has been proposed as a viable solution for managing high data traffic and offloading the existing terrestrial infrastructure. This paper presents an unmanned aerial vehicles (UAVs) aided offloading strategy of the terrestrial network, utilizing an unsupervised machine learning method for the best placement of UAVs in sites with high data traffic. The proposed scheme forms clusters of users located in the affected area using the k-medoid algorithm. Followingly, based on the number of available UAVs, a cluster selection scheme is employed to select the available UAVs that will be deployed to achieve maximum offloading in the system. Comparisons with traditional offloading strategies integrating terrestrial picocells and other UAV-aided schemes show that significant offloading, throughput, spectral efficiency, and sum rate gains can be harvested through the proposed method under a varying number of UAVs.},
DOI = {10.3390/telecom3010005}
}



@Article{rs14030518,
AUTHOR = {Brewer, Kiara and Clulow, Alistair and Sibanda, Mbulisi and Gokool, Shaeden and Naiken, Vivek and Mabhaudhi, Tafadzwanashe},
TITLE = {Predicting the Chlorophyll Content of Maize over Phenotyping as a Proxy for Crop Health in Smallholder Farming Systems},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {518},
URL = {https://www.mdpi.com/2072-4292/14/3/518},
ISSN = {2072-4292},
ABSTRACT = {Smallholder farmers depend on healthy and productive crop yields to sustain their socio-economic status and ensure livelihood security. Advances in South African precision agriculture in the form of unmanned aerial vehicles (UAVs) provide spatially explicit near-real-time information that can be used to assess crop dynamics and inform smallholder farmers. The use of UAVs with remote-sensing techniques allows for the acquisition of high spatial resolution data at various spatio-temporal planes, which is particularly useful at the scale of fields and farms. Specifically, crop chlorophyll content is assessed as it is one of the best known and reliable indicators of crop health, due to its biophysical pigment and biochemical processes that indicate plant productivity. In this regard, the study evaluated the utility of multispectral UAV imagery using the random forest machine learning algorithm to estimate the chlorophyll content of maize through the various growth stages. The results showed that the near-infrared and red-edge wavelength bands and vegetation indices derived from these wavelengths were essential for estimating chlorophyll content during the phenotyping of maize. Furthermore, the random forest model optimally estimated the chlorophyll content of maize over the various phenological stages. Particularly, maize chlorophyll was best predicted during the early reproductive, late vegetative, and early vegetative growth stages to RMSE accuracies of 40.4 &micro;mol/m&minus;2, 39 &micro;mol/m&minus;2, and 61.6 &micro;mol/m&minus;2, respectively. The least accurate chlorophyll content results were predicted during the mid-reproductive and late reproductive growth stages to RMSE accuracies of 66.6 &micro;mol/m&minus;2 and 69.6 &micro;mol/m&minus;2, respectively, as a consequence of a hailstorm. A resultant chlorophyll variation map of the maize growth stages captured the spatial heterogeneity of chlorophyll within the maize field. Therefore, the study&rsquo;s findings demonstrate that the use of remotely sensed UAV imagery with a robust machine algorithm is a critical tool to support the decision-making and management in smallholder farms.},
DOI = {10.3390/rs14030518}
}



@Article{rs14030592,
AUTHOR = {Reedha, Reenul and Dericquebourg, Eric and Canals, Raphael and Hafiane, Adel},
TITLE = {Transformer Neural Network for Weed and Crop Classification of High Resolution UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {592},
URL = {https://www.mdpi.com/2072-4292/14/3/592},
ISSN = {2072-4292},
ABSTRACT = {Monitoring crops and weeds is a major challenge in agriculture and food production today. Weeds compete directly with crops for moisture, nutrients, and sunlight. They therefore have a significant negative impact on crop yield if not sufficiently controlled. Weed detection and mapping is an essential step in weed control. Many existing research studies recognize the importance of remote sensing systems and machine learning algorithms in weed management. Deep learning approaches have shown good performance in many agriculture-related remote sensing tasks, such as plant classification, disease detection, etc. However, despite the success of these approaches, they still face many challenges such as high computation cost, the need of large labelled datasets, intra-class discrimination (in growing phase weeds and crops share many attributes similarity as color, texture, and shape), etc. This paper aims to show that the attention-based deep network is a promising approach to address the forementioned problems, in the context of weeds and crops recognition with drone system. The specific objective of this study was to investigate visual transformers (ViT) and apply them to plant classification in Unmanned Aerial Vehicles (UAV) images. Data were collected using a high-resolution camera mounted on a UAV, which was deployed in beet, parsley and spinach fields. The acquired data were augmented to build larger dataset, since ViT requires large sample sets for better performance, we also adopted the transfer learning strategy. Experiments were set out to assess the effect of training and validation dataset size, as well as the effect of increasing the test set while reducing the training set. The results show that with a small labeled training dataset, the ViT models outperform state-of-the-art models such as EfficientNet and ResNet. The results of this study are promising and show the potential of ViT to be applied to a wide range of remote sensing image analysis tasks.},
DOI = {10.3390/rs14030592}
}



@Article{rs14030799,
AUTHOR = {Kurihara, Junichi and Koo, Voon-Chet and Guey, Cheaw Wen and Lee, Yang Ping and Abidin, Haryati},
TITLE = {Early Detection of Basal Stem Rot Disease in Oil Palm Tree Using Unmanned Aerial Vehicle-Based Hyperspectral Imaging},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {799},
URL = {https://www.mdpi.com/2072-4292/14/3/799},
ISSN = {2072-4292},
ABSTRACT = {Early detection of basal stem rot (BSR) disease in oil palm trees is important for the sustainable production of palm oil in the limited land for plantation in Southeast Asia. However, previous studies based on satellite and aircraft hyperspectral remote sensing could not discriminate oil palm trees in the early-stage of the BSR disease from healthy or late-stage trees. In this study, hyperspectral imaging of oil palm trees from an unmanned aerial vehicle (UAV) and machine learning using a random forest algorithm were employed for the classification of four infection categories of the BSR disease: healthy, early-stage, late-stage, and dead trees. A concentric disk segmentation was applied to tree crown segmentation at the sub-plant scale, and recursive feature elimination was used for feature selection. The results revealed that the classification performance for the early-stage trees is maximum at the specific tree crown segments, and only a few spectral bands in the red-edge region are sufficient to classify the infection categories. These findings will be useful for future UAV-based multispectral imaging to efficiently cover a wide area of oil palm plantations for the early detection of BSR disease.},
DOI = {10.3390/rs14030799}
}



@Article{rs14040909,
AUTHOR = {Junttila, Samuli and Näsi, Roope and Koivumäki, Niko and Imangholiloo, Mohammad and Saarinen, Ninni and Raisio, Juha and Holopainen, Markus and Hyyppä, Hannu and Hyyppä, Juha and Lyytikäinen-Saarenmaa, Päivi and Vastaranta, Mikko and Honkavaara, Eija},
TITLE = {Multispectral Imagery Provides Benefits for Mapping Spruce Tree Decline Due to Bark Beetle Infestation When Acquired Late in the Season},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {909},
URL = {https://www.mdpi.com/2072-4292/14/4/909},
ISSN = {2072-4292},
ABSTRACT = {Climate change is increasing pest insects&rsquo; ability to reproduce as temperatures rise, resulting in vast tree mortality globally. Early information on pest infestation is urgently needed for timely decisions to mitigate the damage. We investigated the mapping of trees that were in decline due to European spruce bark beetle infestation using multispectral unmanned aerial vehicles (UAV)-based imagery collected in spring and fall in four study areas in Helsinki, Finland. We used the Random Forest machine learning to classify trees based on their symptoms during both occasions. Our approach achieved an overall classification accuracy of 78.2% and 84.5% for healthy, declined and dead trees for spring and fall datasets, respectively. The results suggest that fall or the end of summer provides the most accurate tree vitality classification results. We also investigated the transferability of Random Forest classifiers between different areas, resulting in overall classification accuracies ranging from 59.3% to 84.7%. The findings of this study indicate that multispectral UAV-based imagery is capable of classifying tree decline in Norway spruce trees during a bark beetle infestation.},
DOI = {10.3390/rs14040909}
}



@Article{rs14040998,
AUTHOR = {Liang, Min-Chih and Tfwala, Samkele S. and Chen, Su-Chin},
TITLE = {The Evaluation of Color Spaces for Large Woody Debris Detection in Rivers Using XGBoost Algorithm},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {998},
URL = {https://www.mdpi.com/2072-4292/14/4/998},
ISSN = {2072-4292},
ABSTRACT = {Large woody debris (LWD) strongly influences river systems, especially in forested and mountainous catchments. In Taiwan, LWD are mainly from typhoons and extreme torrential events. To effectively manage the LWD, it is necessary to conduct regular surveys on river systems. Simple, low cost, and accurate tools are therefore necessary. The proposed methodology applies image processing and machine learning (XGBoost classifier) to quantify LWD distribution, location, and volume in river channels. XGBoost algorithm was selected due to its scalability and faster execution speeds. Nishueibei River, located in Taitung County, was used as the area of investigation. Unmanned aerial vehicles (UAVs) were used to capture the terrain and LWD. Structure from Motion (SfM) was used to build high-resolution orthophotos and digital elevation models (DEM), after which machine learning and different color spaces were used to recognize LWD. Finally, the volume of LWD in the river was estimated. The findings show that RGB color space as LWD recognition factor suffers serious collinearity problems, and it is easy to lose some LWD information; thus, it is not suitable for LWD recognition. On the contrary, the combination of different factors in different color spaces enhances the results, and most of the factors are related to the YCbCr color space. The CbCr factor in the YCbCr color space was best for identifying LWD. LWD volume was then estimated from the identified LWD using manual, field, and automatic measurements. The results indicate that the manual measurement method was the best (R2 = 0.88) to identify field LWD volume. Moreover, automatic measurement (R2 = 0.72) can also obtain LWD volume to save time and workforce.},
DOI = {10.3390/rs14040998}
}



@Article{rs14041023,
AUTHOR = {Guan, Yunyi and Grote, Katherine and Schott, Joel and Leverett, Kelsi},
TITLE = {Prediction of Soil Water Content and Electrical Conductivity Using Random Forest Methods with UAV Multispectral and Ground-Coupled Geophysical Data},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {1023},
URL = {https://www.mdpi.com/2072-4292/14/4/1023},
ISSN = {2072-4292},
ABSTRACT = {The volumetric water content (VWC) of soil is a critical parameter in agriculture, as VWC strongly influences crop yield, provides nutrients to plants, and maintains the microbes that are needed for the biological health of the soil. Measuring VWC is difficult, as it is spatially and temporally heterogeneous, and most agricultural producers use point measurements that cannot fully capture this parameter. Electrical conductivity (EC) is another soil parameter that is useful in agriculture, since it can be used to indicate soil salinity, soil texture, and plant nutrient availability. Soil EC is also very heterogeneous; measuring EC using conventional soil sampling techniques is very time consuming and often fails to capture the variability in EC at a site. In contrast to the point-based methods used to measure VWC and EC, multispectral data acquired with unmanned aerial vehicles (UAV) can cover large areas with high resolution. In agriculture, multispectral data are often used to calculate vegetation indices (VIs). In this research UAV-acquired VIs and raw multispectral data were used to predict soil VWC and EC. High-resolution geophysical methods were used to acquire more than 41,000 measurements of VWC and 8000 measurements of EC in 18 traverses across a field that contained 56 experimental plots. The plots varied by crop type (corn, soybeans, and alfalfa) and drainage (no drainage, moderate drainage, high drainage). Machine learning was performed using the random forest method to predict VWC and EC using VIs and multispectral data. Prediction accuracy was determined for several scenarios that assumed different levels of knowledge about crop type or drainage. Results showed that multispectral data improved prediction of VWC and EC, and the best predictions occurred when both the crop type and degree of drainage were known, but drainage was a more important input than crop type. Predictions were most accurate in drier soil, which may be due to the lower overall variability of VWC and EC under these conditions. An analysis of which multispectral data were most important showed that NDRE, VARI, and blue band data improved predictions the most. The final conclusions of this study are that inexpensive UAV-based multispectral data can be used to improve estimation of heterogenous soil properties, such as VWC and EC in active agricultural fields. In this study, the best estimates of these properties were obtained when the agriculture parameters in a field were fairly homogeneous (one crop type and the same type of drainage throughout the field), although improvements were observed even when these conditions were not met. The multispectral data that were most useful for prediction were those that penetrated deeper into the soil canopy or were sensitive to bare soil.},
DOI = {10.3390/rs14041023}
}



@Article{rs14051140,
AUTHOR = {Narmilan, Amarasingam and Gonzalez, Felipe and Salgadoe, Arachchige Surantha Ashan and Kumarasiri, Unupen Widanelage Lahiru Madhushanka and Weerasinghe, Hettiarachchige Asiri Sampageeth and Kulasekara, Buddhika Rasanjana},
TITLE = {Predicting Canopy Chlorophyll Content in Sugarcane Crops Using Machine Learning Algorithms and Spectral Vegetation Indices Derived from UAV Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1140},
URL = {https://www.mdpi.com/2072-4292/14/5/1140},
ISSN = {2072-4292},
ABSTRACT = {The use of satellite-based Remote Sensing (RS) is a well-developed field of research. RS techniques have been successfully utilized to evaluate the chlorophyll content for the monitoring of sugarcane crops. This research provides a new framework for inferring the chlorophyll content in sugarcane crops at the canopy level using unmanned aerial vehicles (UAVs) and spectral vegetation indices processed with multiple machine learning algorithms. Studies were conducted in a sugarcane field located in Sugarcane Research Institute (SRI, Uda Walawe, Sri Lanka), with various fertilizer applications over the entire growing season from 2020 to 2021. An UAV with multispectral camera was used to collect the aerial images to generate the vegetation indices. Ground measurements of leaf chlorophyll were used as indications for fertilizer status in the sugarcane field. Different machine learning (ML) algorithms were used ground-truthing data of chlorophyll content and spectral vegetation indices to forecast sugarcane chlorophyll content. Several machine learning algorithms such as MLR, RF, DT, SVR, XGB, KNN and ANN were applied in two ways: before feature selection (BFS) by training the algorithms with all twenty-four (24) vegetation indices with five (05) spectral bands and after feature selection (AFS) by training algorithms with fifteen (15) vegetation indices. All the algorithms with both BFS and AFS methods were compared with an estimated coefficient of determination (R2) and root mean square error (RMSE). Spectral indices such as RVI and DVI were shown to be the most reliable indices for estimating chlorophyll content in sugarcane fields, with coefficients of determination (R2) of 0.94 and 0.93, respectively. XGB model shows the highest validation score (R2) and lowest RMSE in both methods of BFS (0.96 and 0.14) and AFS (0.98 and 0.78), respectively. However, KNN and SVR algorithms show the lowest validation accuracy than other models. According to the results, the AFS validation score is higher than BFS in MLR, SVR, XGB and KNN. Even though, validation score of the ANN model is decreased in AFS. The findings demonstrated that the use of multispectral UAV could be utilized to estimate chlorophyll content and measure crop health status over a larger sugarcane field. This methodology will aid in real-time crop nutrition management in sugarcane plantations by reducing the need for conventional measurement of sugarcane chlorophyll content.},
DOI = {10.3390/rs14051140}
}



@Article{rs14051251,
AUTHOR = {Wang, Falv and Yang, Mao and Ma, Longfei and Zhang, Tong and Qin, Weilong and Li, Wei and Zhang, Yinghua and Sun, Zhencai and Wang, Zhimin and Li, Fei and Yu, Kang},
TITLE = {Estimation of Above-Ground Biomass of Winter Wheat Based on Consumer-Grade Multi-Spectral UAV},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1251},
URL = {https://www.mdpi.com/2072-4292/14/5/1251},
ISSN = {2072-4292},
ABSTRACT = {One of the problems of optical remote sensing of crop above-ground biomass (AGB) is that vegetation indices (VIs) often saturate from the middle to late growth stages. This study focuses on combining VIs acquired by a consumer-grade multiple-spectral UAV and machine learning regression techniques to (i) determine the optimal time window for AGB estimation of winter wheat and to (ii) determine the optimal combination of multi-spectral VIs and regression algorithms. UAV-based multi-spectral data and manually measured AGB of winter wheat, under five nitrogen rates, were obtained from the jointing stage until 25 days after flowering in the growing season 2020/2021. Forty-four multi-spectral VIs were used in the linear regression (LR), partial least squares regression (PLSR), and random forest (RF) models in this study. Results of LR models showed that the heading stage was the most suitable stage for AGB prediction, with R2 values varying from 0.48 to 0.93. Three PLSR models based on different datasets performed differently in estimating AGB in the training dataset (R2 = 0.74~0.92, RMSE = 0.95~2.87 t/ha, MAE = 0.75~2.18 t/ha, and RPD = 2.00~3.67) and validation dataset (R2 = 0.50~0.75, RMSE = 1.56~2.57 t/ha, MAE = 1.44~2.05 t/ha, RPD = 1.45~1.89). Compared with PLSR models, the performance of the RF models was more stable in the prediction of AGB in the training dataset (R2 = 0.95~0.97, RMSE = 0.58~1.08 t/ha, MAE = 0.46~0.89 t/ha, and RPD = 3.95~6.35) and validation dataset (R2 = 0.83~0.93, RMSE = 0.93~2.34 t/ha, MAE = 0.72~2.01 t/ha, RPD = 1.36~3.79). Monitoring AGB prior to flowering was found to be more effective than post-flowering. Moreover, this study demonstrates that it is feasible to estimate AGB for multiple growth stages of winter wheat by combining the optimal VIs and PLSR and RF models, which overcomes the saturation problem of using individual VI-based linear regression models.},
DOI = {10.3390/rs14051251}
}



@Article{f13030418,
AUTHOR = {Xu, Zhanghua and Zhang, Qi and Xiang, Songyang and Li, Yifan and Huang, Xuying and Zhang, Yiwei and Zhou, Xin and Li, Zenglu and Yao, Xiong and Li, Qiaosi and Guo, Xiaoyu},
TITLE = {Monitoring the Severity of Pantana phyllostachysae Chao Infestation in Moso Bamboo Forests Based on UAV Multi-Spectral Remote Sensing Feature Selection},
JOURNAL = {Forests},
VOLUME = {13},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {418},
URL = {https://www.mdpi.com/1999-4907/13/3/418},
ISSN = {1999-4907},
ABSTRACT = {In recent years, the rapid development of unmanned aerial vehicle (UAV) remote sensing technology has provided a new means to efficiently monitor forest resources and effectively prevent and control pests and diseases. This study aims to develop a detection model to study the damage caused to Moso bamboo forests by Pantana phyllostachysae Chao (PPC), a major leaf-eating pest, at 5 cm resolution. Damage sensitive features were extracted from multispectral images acquired by UAVs and used to train detection models based on support vector machines (SVM), random forests (RF), and extreme gradient boosting tree (XGBoost) machine learning algorithms. The overall detection accuracy (OA) and Kappa coefficient of SVM, RF, and XGBoost were 81.95%, 0.733, 85.71%, 0.805, and 86.47%, 0.811, respectively. Meanwhile, the detection accuracies of SVM, RF, and XGBoost were 78.26%, 76.19%, and 80.95% for healthy, 75.00%, 83.87%, and 79.17% for mild damage, 83.33%, 86.49%, and 85.00% for moderate damage, and 82.5%, 90.91%, and 93.75% for severe damage Moso bamboo, respectively. Overall, XGBoost exhibited the best detection performance, followed by RF and SVM. Thus, the study findings provide a technical reference for the regional monitoring and control of PPC in Moso bamboo.},
DOI = {10.3390/f13030418}
}



@Article{s22052049,
AUTHOR = {Douklias, Athanasios and Karagiannidis, Lazaros and Misichroni, Fay and Amditis, Angelos},
TITLE = {Design and Implementation of a UAV-Based Airborne Computing Platform for Computer Vision and Machine Learning Applications},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {2049},
URL = {https://www.mdpi.com/1424-8220/22/5/2049},
PubMedID = {35271196},
ISSN = {1424-8220},
ABSTRACT = {Visual sensing of the environment is crucial for flying an unmanned aerial vehicle (UAV) and is a centerpiece of many related applications. The ability to run computer vision and machine learning algorithms onboard an unmanned aerial system (UAS) is becoming more of a necessity in an effort to alleviate the communication burden of high-resolution video streaming, to provide flying aids, such as obstacle avoidance and automated landing, and to create autonomous machines. Thus, there is a growing interest on the part of many researchers in developing and validating solutions that are suitable for deployment on a UAV system by following the general trend of edge processing and airborne computing, which transforms UAVs from moving sensors into intelligent nodes that are capable of local processing. In this paper, we present, in a rigorous way, the design and implementation of a 12.85 kg UAV system equipped with the necessary computational power and sensors to serve as a testbed for image processing and machine learning applications, explain the rationale behind our decisions, highlight selected implementation details, and showcase the usefulness of our system by providing an example of how a sample computer vision application can be deployed on our platform.},
DOI = {10.3390/s22052049}
}



@Article{rs14061337,
AUTHOR = {Guo, Yahui and Chen, Shouzhi and Li, Xinxi and Cunha, Mario and Jayavelu, Senthilnath and Cammarano, Davide and Fu, Yongshuo},
TITLE = {Machine Learning-Based Approaches for Predicting SPAD Values of Maize Using Multi-Spectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1337},
URL = {https://www.mdpi.com/2072-4292/14/6/1337},
ISSN = {2072-4292},
ABSTRACT = {Precisely monitoring the growth condition and nutritional status of maize is crucial for optimizing agronomic management and improving agricultural production. Multi-spectral sensors are widely applied in ecological and agricultural domains. However, the images collected under varying weather conditions on multiple days show a lack of data consistency. In this study, the Mini MCA 6 Camera from UAV platform was used to collect images covering different growth stages of maize. The empirical line calibration method was applied to establish generic equations for radiometric calibration. The coefficient of determination (R2) of the reflectance from calibrated images and ASD Handheld-2 ranged from 0.964 to 0.988 (calibration), and from 0.874 to 0.927 (validation), respectively. Similarly, the root mean square errors (RMSE) were 0.110, 0.089, and 0.102% for validation using data of 5 August, 21 September, and both days in 2019, respectively. The soil and plant analyzer development (SPAD) values were measured and applied to build the linear regression relationships with spectral and textural indices of different growth stages. The Stepwise regression model (SRM) was applied to identify the optimal combination of spectral and textural indices for estimating SPAD values. The support vector machine (SVM) and random forest (RF) models were independently applied for estimating SPAD values based on the optimal combinations. SVM performed better than RF in estimating SPAD values with R2 (0.81) and RMSE (0.14), respectively. This study contributed to the retrieval of SPAD values based on both spectral and textural indices extracted from multi-spectral images using machine learning methods.},
DOI = {10.3390/rs14061337}
}



@Article{computation10030042,
AUTHOR = {Vasilopoulos, Emmanuel and Vosinakis, Georgios and Krommyda, Maria and Karagiannidis, Lazaros and Ouzounoglou, Eleftherios and Amditis, Angelos},
TITLE = {A Comparative Study of Autonomous Object Detection Algorithms in the Maritime Environment Using a UAV Platform},
JOURNAL = {Computation},
VOLUME = {10},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {42},
URL = {https://www.mdpi.com/2079-3197/10/3/42},
ISSN = {2079-3197},
ABSTRACT = {Maritime operations rely heavily on surveillance and require reliable and timely data that can inform decisions and planning. Critical information in such cases includes the exact location of objects in the water, such as vessels, persons, and others. Due to the unique characteristics of the maritime environment, the location of even inert objects changes through time, depending on the weather conditions, water currents, etc. Unmanned aerial vehicles (UAVs) can be used to support maritime operations by providing live video streams and images from the area of operations. Machine learning algorithms can be developed, trained, and used to automatically detect and track objects of specific types and characteristics. EFFECTOR is an EU-funded project, developing an Interoperability Framework for maritime surveillance. Within the project, we developed an embedded system that employs machine learning algorithms, allowing a UAV to autonomously detect objects in the water and keep track of their changing position through time. Using the on-board computation unit of the UAV, we ran and present the results of a series of comparative tests among possible architecture sizes and training datasets for the detection and tracking of objects in the maritime environment. We tested architectures based on their efficiency, accuracy, and speed. A combined solution for training the datasets is suggested, providing optimal efficiency and accuracy.},
DOI = {10.3390/computation10030042}
}



@Article{rs14061453,
AUTHOR = {Nasiri, Vahid and Darvishsefat, Ali Asghar and Arefi, Hossein and Griess, Verena C. and Sadeghi, Seyed Mohammad Moein and Borz, Stelian Alexandru},
TITLE = {Modeling Forest Canopy Cover: A Synergistic Use of Sentinel-2, Aerial Photogrammetry Data, and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1453},
URL = {https://www.mdpi.com/2072-4292/14/6/1453},
ISSN = {2072-4292},
ABSTRACT = {Forest canopy cover (FCC) is an important ecological parameter of forest ecosystems, and is correlated with forest characteristics, including plant growth, regeneration, biodiversity, light regimes, and hydrological properties. Here, we present an approach of combining Sentinel-2 data, high-resolution aerial images, and machine learning (ML) algorithms to model FCC in the Hyrcanian mixed temperate forest, Northern Iran. Sentinel-2 multispectral bands and vegetation indices were used as variables for modeling and mapping FCC based on UAV ground truth to a wider spatial extent. Random forest (RF), support-vector machine (SVM), elastic net (ENET), and extreme gradient boosting (XGBoost) were the ML algorithms used to learn and generalize on the remotely sensed variables. Evaluation of variable importance indicated that vegetation indices including NDVI, NDVI-A, NDRE, and NDI45 were the dominant predictors in most of the models. Model accuracy estimation results showed that among the tested models, RF (R2 = 0.67, RMSE = 18.87%, MAE = 15.35%) and ENET (R2 = 0.63, RMSE = 20.04%, MAE = 16.44%) showed the best and the worst performance, respectively. In conclusion, it was possible to prove the suitability of integrating UAV-obtained RGB images, Sentinel-2 data, and ML models for the estimation of FCC, intended for precise and fast mapping at landscape-level scale.},
DOI = {10.3390/rs14061453}
}



@Article{rs14061474,
AUTHOR = {Bian, Chaofa and Shi, Hongtao and Wu, Suqin and Zhang, Kefei and Wei, Meng and Zhao, Yindi and Sun, Yaqin and Zhuang, Huifu and Zhang, Xuewei and Chen, Shuo},
TITLE = {Prediction of Field-Scale Wheat Yield Using Machine Learning Method and Multi-Spectral UAV Data},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1474},
URL = {https://www.mdpi.com/2072-4292/14/6/1474},
ISSN = {2072-4292},
ABSTRACT = {Accurate prediction of food crop yield is of great significance for global food security and regional trade stability. Since remote sensing data collected from unmanned aerial vehicle (UAV) platforms have the features of flexibility and high resolution, these data can be used as samples to develop regional regression models for accurate prediction of crop yield at a field scale. The primary objective of this study was to construct regional prediction models for winter wheat yield based on multi-spectral UAV data and machine learning methods. Six machine learning methods including Gaussian process regression (GPR), support vector machine regression (SVR) and random forest regression (RFR) were used for the construction of the yield prediction models. Ten vegetation indices (VIs) extracted from canopy spectral images of winter wheat acquired from a multi-spectral UAV at five key growth stages in Xuzhou City, Jiangsu Province, China in 2021 were selected as the variables of the models. In addition, in situ measurements of wheat yield were obtained in a destructive sampling manner for prediction algorithm modeling and validation. Prediction results of single growth stages showed that the optimal model was GPR constructed from extremely strong correlated VIs (ESCVIs) at the filling stage (R2 = 0.87, RMSE = 49.22 g/m2, MAE = 42.74 g/m2). The results of multiple stages showed GPR achieved the highest accuracy (R2 = 0.88, RMSE = 49.18 g/m2, MAE = 42.57 g/m2) when the ESCVIs of the flowering and filling stages were used. Larger sampling plots were adopted to verify the accuracy of yield prediction; the results indicated that the GPR model has strong adaptability at different scales. These findings suggest that using machine learning methods and multi-spectral UAV data can accurately predict crop yield at the field scale and deliver a valuable application reference for farm-scale field crop management.},
DOI = {10.3390/rs14061474}
}



@Article{ijgi11040222,
AUTHOR = {Ren, Simiao and Malof, Jordan and Fetter, Rob and Beach, Robert and Rineer, Jay and Bradbury, Kyle},
TITLE = {Utilizing Geospatial Data for Assessing Energy Security: Mapping Small Solar Home Systems Using Unmanned Aerial Vehicles and Deep Learning},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {11},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {222},
URL = {https://www.mdpi.com/2220-9964/11/4/222},
ISSN = {2220-9964},
ABSTRACT = {Solar home systems (SHS), a cost-effective solution for rural communities far from the grid in developing countries, are small solar panels and associated equipment that provides power to a single household. A crucial resource for targeting further investment of public and private resources, as well as tracking the progress of universal electrification goals, is shared access to high-quality data on individual SHS installations including information such as location and power capacity. Though recent studies utilizing satellite imagery and machine learning to detect solar panels have emerged, they struggle to accurately locate many SHS due to limited image resolution (some small solar panels only occupy several pixels in satellite imagery). In this work, we explore the viability and cost-performance tradeoff of using automatic SHS detection on unmanned aerial vehicle (UAV) imagery as an alternative to satellite imagery. More specifically, we explore three questions: (i) what is the detection performance of SHS using drone imagery; (ii) how expensive is the drone data collection, compared to satellite imagery; and (iii) how well does drone-based SHS detection perform in real-world scenarios? To examine these questions, we collect and publicly-release a dataset of high-resolution drone imagery encompassing SHS imaged under a variety of real-world conditions and use this dataset and a dataset of imagery from Rwanda to evaluate the capabilities of deep learning models to recognize SHS, including those that are too small to be reliably recognized in satellite imagery. The results suggest that UAV imagery may be a viable alternative to identify very small SHS from perspectives of both detection accuracy and financial costs of data collection. UAV-based data collection may be a practical option for supporting electricity access planning strategies for achieving sustainable development goals and for monitoring the progress towards those goals.},
DOI = {10.3390/ijgi11040222}
}



@Article{s22072762,
AUTHOR = {Bae, Jaehoon and Lee, Jonghoon and Jang, Arum and Ju, Young K. and Park, Min Jae},
TITLE = {SMART SKY EYE System for Preliminary Structural Safety Assessment of Buildings Using Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {2762},
URL = {https://www.mdpi.com/1424-8220/22/7/2762},
ISSN = {1424-8220},
ABSTRACT = {The development of unmanned aerial vehicles (UAVs) is expected to become one of the most commercialized research areas in the world over the next decade. Globally, unmanned aircraft have been increasingly used for safety surveillance in the construction industry and civil engineering fields. This paper presents an aerial image-based approach using UAVs to inspect cracks and deformations in buildings. A state-of-the-art safety evaluation method termed SMART SKY EYE (Smart building safety assessment system using UAV) is introduced; this system utilizes an unmanned airplane equipped with a thermal camera and programmed with various surveying efficiency improvement methods, such as thermography, machine-learning algorithms, and 3D point cloud modeling. Using this method, crack maps, crack depths, and the deformations of structures can be obtained. Error rates are compared between the proposed and conventional methods.},
DOI = {10.3390/s22072762}
}



