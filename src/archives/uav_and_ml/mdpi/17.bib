
@Article{act10120323,
AUTHOR = {Yang, Pu and Wang, Zixin and Zhang, Zhiqing and Hu, Xukai},
TITLE = {Sliding Mode Fault Tolerant Control for a Quadrotor with Varying Load and Actuator Fault},
JOURNAL = {Actuators},
VOLUME = {10},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2076-0825/10/12/323},
ISSN = {2076-0825},
ABSTRACT = {In this paper, an adaptive sliding mode fault-tolerant control scheme based on prescribed performance control and neural networks is developed for an Unmanned Aerial Vehicle (UAV) quadrotor carrying a load to deal with actuator faults. First, a nonsingular fast terminal sliding mode (NFTSM) control strategy is presented. In virtue of the proposed strategy, fast convergence and high robustness can be guaranteed without stimulating chattering. Secondly, to obtain correct fault magnitudes and compensate the failures actively, a radial basis function neural network-based fault estimation scheme is proposed. By combining the proposed fault estimation strategy and the NFTSM controller, an active fault-tolerant control algorithm is established. Then, the uncertainties caused by load variation are explicitly considered and compensated by the presented adaptive laws. Moreover, by synthesizing the proposed sliding mode control and prescribed performance control (PPC), an output error transformation is defined to deal with state constraints and provide better tracking performance. From the Lyapunov stability analysis, the overall system is proven to be uniformly asymptotically stable. Finally, numerical simulation based on a quadrotor helicopter is carried out to validate the effectiveness and superiority of the proposed algorithm.},
DOI = {10.3390/act10120323}
}



@Article{rs13244968,
AUTHOR = {Chakhar, Amal and Hernández-López, David and Ballesteros, Rocío and Moreno, Miguel A.},
TITLE = {Improvement of the Soil Moisture Retrieval Procedure Based on the Integration of UAV Photogrammetry and Satellite Remote Sensing Information},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {4968},
URL = {https://www.mdpi.com/2072-4292/13/24/4968},
ISSN = {2072-4292},
ABSTRACT = {In countries characterized by arid and semi-arid climates, a precise determination of soil moisture conditions on the field scale is critically important, especially in the first crop growth stages, to schedule irrigation and to avoid wasting water. The objective of this study was to apply the operative methodology that allowed surface soil moisture (SSM) content in a semi-arid environment to be estimated. SSM retrieval was carried out by combining two scattering models (IEM and WCM), supplied by backscattering coefficients at the VV polarization obtained from the C-band Synthetic Aperture Radar (SAR), a vegetation descriptor NDVI obtained from the optical sensor, among other essential parameters. The inversion of these models was performed by Neural Networks (NN). The combined models were calibrated by the Sentinel 1 and Sentinel 2 data collected on bare soil, and in cereal, pea and onion crop fields. To retrieve SSM, these scattering models need accurate measurements of the roughness surface parameters, standard deviation of the surface height (hrms) and correlation length (L). This work used a photogrammetric acquisition system carried on Unmanned Aerial Vehicles (UAV) to reconstruct digital surface models (DSM), which allowed these soil roughness parameters to be acquired in a large portion of the studied fields. The obtained results showed that the applied improved methodology effectively estimated SSM on bare and cultivated soils in the principal early growth stages. The bare soil experimentation yielded an R2 = 0.74 between the estimated and observed SSMs. For the cereal field, the relation between the estimated and measured SSMs yielded R2 = 0.71. In the experimental pea fields, the relation between the estimated and measured SSMs revealed R2 = 0.72 and 0.78, respectively, for peas 1 and peas 2. For the onion experimentation, the highest R2 equaled 0.5 in the principal growth stage (leaf development), but the crop R2 drastically decreased to 0.08 in the completed growth phase. The acquired results showed that the applied improved methodology proves to be an effective tool for estimating the SSM on bare and cultivated soils in the principal early growth stages.},
DOI = {10.3390/rs13244968}
}



@Article{land10121365,
AUTHOR = {Agapiou, Athos and Vionis, Athanasios and Papantoniou, Giorgos},
TITLE = {Detection of Archaeological Surface Ceramics Using Deep Learning Image-Based Methods and Very High-Resolution UAV Imageries},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {1365},
URL = {https://www.mdpi.com/2073-445X/10/12/1365},
ISSN = {2073-445X},
ABSTRACT = {Mapping surface ceramics through systematic pedestrian archaeological survey is considered a consistent method to recover the cultural biography of sites within a micro-region. Archaeologists nowadays conduct surface survey equipped with navigation devices counting, documenting, and collecting surface archaeological potsherds within a set of plotted grids. Recent advancements in unmanned aerial vehicles (UAVs) and image processing analysis can be utilised to support such surface archaeological investigations. In this study, we have implemented two different artificial intelligence image processing methods over two areas of interest near the present-day village of Kophinou in Cyprus, in the Xeros River valley. We have applied a random forest classifier through the Google Earth Engine big data cloud platform and a Single Shot Detector neural network in the ArcGIS Pro environment. For the first case study, the detection was based on red&ndash;green&ndash;blue (RGB) high-resolution orthophotos. In contrast, a multispectral camera covering both the visible and the near-infrared parts of the spectrum was used in the second area of investigation. The overall results indicate that such an approach can be used in the future as part of ongoing archaeological pedestrian surveys to detect scattered potsherds in areas of archaeological interest, even if pottery shares a very high spectral similarity with the surface.},
DOI = {10.3390/land10121365}
}



@Article{e23121678,
AUTHOR = {Yang, Shubo and Luo, Yang and Miao, Wang and Ge, Changhao and Sun, Wenjian and Luo, Chunbo},
TITLE = {RF Signal-Based UAV Detection and Mode Classification: A Joint Feature Engineering Generator and Multi-Channel Deep Neural Network Approach},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {1678},
URL = {https://www.mdpi.com/1099-4300/23/12/1678},
PubMedID = {34945985},
ISSN = {1099-4300},
ABSTRACT = {With the proliferation of Unmanned Aerial Vehicles (UAVs) to provide diverse critical services, such as surveillance, disaster management, and medicine delivery, the accurate detection of these small devices and the efficient classification of their flight modes are of paramount importance to guarantee their safe operation in our sky. Among the existing approaches, Radio Frequency (RF) based methods are less affected by complex environmental factors. The similarities between UAV RF signals and the diversity of frequency components make accurate detection and classification a particularly difficult task. To bridge this gap, we propose a joint Feature Engineering Generator (FEG) and Multi-Channel Deep Neural Network (MC-DNN) approach. Specifically, in FEG, data truncation and normalization separate different frequency components, the moving average filter reduces the outliers in the RF signal, and the concatenation fully exploits the details of the dataset. In addition, the multi-channel input in MC-DNN separates multiple frequency components and reduces the interference between them. A novel dataset that contains ten categories of RF signals from three types of UAVs is used to verify the effectiveness. Experiments show that the proposed method outperforms the state-of-the-art UAV detection and classification approaches in terms of 98.4% and F1 score of 98.3%.},
DOI = {10.3390/e23121678}
}



@Article{machines9120360,
AUTHOR = {Yang, Pu and Wen, Chenwan and Geng, Huilin and Liu, Peng},
TITLE = {Intelligent Fault Diagnosis Method for Blade Damage of Quad-Rotor UAV Based on Stacked Pruning Sparse Denoising Autoencoder and Convolutional Neural Network},
JOURNAL = {Machines},
VOLUME = {9},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {360},
URL = {https://www.mdpi.com/2075-1702/9/12/360},
ISSN = {2075-1702},
ABSTRACT = {This paper introduces a new intelligent fault diagnosis method based on stack pruning sparse denoising autoencoder and convolutional neural network (sPSDAE-CNN). This method processes the original input data by using a stack denoising autoencoder. Different from the traditional autoencoder, stack pruning sparse denoising autoencoder includes a fully connected autoencoding network, the features extracted from the front layer of the network are used for the operation of the subsequent layer, which means that some new connections will appear between the front and rear layers of the network, reduce the loss of information, and obtain more effective features. Firstly, a one-dimensional sliding window is introduced for data enhancement. In addition, transforming one-dimensional time-domain data into the two-dimensional gray image can further improve the deep learning (DL) ability of models. At the same time, pruning operation is introduced to improve the training efficiency and accuracy of the network. The convolutional neural network model with sPSDAE has a faster training speed, strong adaptability to noise interference signals, and can also suppress the over-fitting problem of the convolutional neural network to a certain extent. Actual experiments show that for the fault of unmanned aerial vehicle (UAV) blade damage, the sPSDAE-CNN model we use has better stability and reliable prediction accuracy than traditional convolutional neural networks. At the same time, For noise signals, better results can be obtained. The experimental results show that the sPSDAE-CNN model still has a good diagnostic accuracy rate in a high-noise environment. In the case of a signal-to-noise ratio of &minus;4, it still has an accuracy rate of 90%.},
DOI = {10.3390/machines9120360}
}



@Article{rs14010046,
AUTHOR = {Wei, Lele and Luo, Yusen and Xu, Lizhang and Zhang, Qian and Cai, Qibing and Shen, Mingjun},
TITLE = {Deep Convolutional Neural Network for Rice Density Prescription Map at Ripening Stage Using Unmanned Aerial Vehicle-Based Remotely Sensed Images},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {46},
URL = {https://www.mdpi.com/2072-4292/14/1/46},
ISSN = {2072-4292},
ABSTRACT = {In this paper, UAV (unmanned aerial vehicle, DJI Phantom4RTK) and YOLOv4 (You Only Look Once) target detection deep neural network methods were employed to collected mature rice images and detect rice ears to produce a rice density prescription map. The YOLOv4 model was used for rice ear quick detection of rice images captured by a UAV. The Kriging interpolation algorithm was used in ArcGIS to make rice density prescription maps. Mature rice images collected by a UAV were marked manually and used to build the training and testing datasets. The resolution of the images was 300 &times; 300 pixels. The batch size was 2, and the initial learning rate was 0.01, and the mean average precision (mAP) of the best trained model was 98.84%. Exceptionally, the network ability to detect rice in different health states was also studied with a mAP of 95.42% in the no infection rice images set, 98.84% in the mild infection rice images set, 94.35% in the moderate infection rice images set, and 93.36% in the severe infection rice images set. According to the severity of rice sheath blight, which can cause rice leaves to wither and turn yellow, the blighted grain percentage increased and the thousand-grain weight decreased, the rice images were divided into these four infection levels. The ability of the network model (R2 = 0.844) was compared with traditional image processing segmentation methods (R2 = 0.396) based on color and morphology features and machine learning image segmentation method (Support Vector Machine, SVM R2 = 0.0817, and K-means R2 = 0.1949) for rice ear counting. The results highlight that the CNN has excellent robustness, and can generate a wide range of rice density prescription maps.},
DOI = {10.3390/rs14010046}
}



@Article{rs14010050,
AUTHOR = {He, Haiqing and Yu, Jing and Cheng, Penggen and Wang, Yuqian and Zhu, Yufeng and Lin, Taiqing and Dai, Guoqiang},
TITLE = {Automatic, Multiview, Coplanar Extraction for CityGML Building Model Texture Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2072-4292/14/1/50},
ISSN = {2072-4292},
ABSTRACT = {Most 3D CityGML building models in street-view maps (e.g., Google, Baidu) lack texture information, which is generally used to reconstruct real-scene 3D models by photogrammetric techniques, such as unmanned aerial vehicle (UAV) mapping. However, due to its simplified building model and inaccurate location information, the commonly used photogrammetric method using a single data source cannot satisfy the requirement of texture mapping for the CityGML building model. Furthermore, a single data source usually suffers from several problems, such as object occlusion. We proposed a novel approach to achieve CityGML building model texture mapping by multiview coplanar extraction from UAV remotely sensed or terrestrial images to alleviate these problems. We utilized a deep convolutional neural network to filter out object occlusion (e.g., pedestrians, vehicles, and trees) and obtain building-texture distribution. Point-line-based features are extracted to characterize multiview coplanar textures in 2D space under the constraint of a homography matrix, and geometric topology is subsequently conducted to optimize the boundary of textures by using a strategy combining Hough-transform and iterative least-squares methods. Experimental results show that the proposed approach enables texture mapping for building fa&ccedil;ades to use 2D terrestrial images without the requirement of exterior orientation information; that is, different from the photogrammetric method, a collinear equation is not an essential part to capture texture information. In addition, the proposed approach can significantly eliminate blurred and distorted textures of building models, so it is suitable for automatic and rapid texture updates.},
DOI = {10.3390/rs14010050}
}



@Article{drones6010005,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Heravi, Amirhossein and Thaheem, Muhammad Jamaluddin and Maqsoom, Ahsen},
TITLE = {Inspecting Buildings Using Drones and Computer Vision: A Machine Learning Approach to Detect Cracks and Damages},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {5},
URL = {https://www.mdpi.com/2504-446X/6/1/5},
ISSN = {2504-446X},
ABSTRACT = {Manual inspection of infrastructure damages such as building cracks is difficult due to the objectivity and reliability of assessment and high demands of time and costs. This can be automated using unmanned aerial vehicles (UAVs) for aerial imagery of damages. Numerous computer vision-based approaches have been applied to address the limitations of crack detection but they have their limitations that can be overcome by using various hybrid approaches based on artificial intelligence (AI) and machine learning (ML) techniques. The convolutional neural networks (CNNs), an application of the deep learning (DL) method, display remarkable potential for automatically detecting image features such as damages and are less sensitive to image noise. A modified deep hierarchical CNN architecture has been used in this study for crack detection and damage assessment in civil infrastructures. The proposed architecture is based on 16 convolution layers and a cycle generative adversarial network (CycleGAN). For this study, the crack images were collected using UAVs and open-source images of mid to high rise buildings (five stories and above) constructed during 2000 in Sydney, Australia. Conventionally, a CNN network only utilizes the last layer of convolution. However, our proposed network is based on the utility of multiple layers. Another important component of the proposed CNN architecture is the application of guided filtering (GF) and conditional random fields (CRFs) to refine the predicted outputs to get reliable results. Benchmarking data (600 images) of Sydney-based buildings damages was used to test the proposed architecture. The proposed deep hierarchical CNN architecture produced superior performance when evaluated using five methods: GF method, Baseline (BN) method, Deep-Crack BN, Deep-Crack GF, and SegNet. Overall, the GF method outperformed all other methods as indicated by the global accuracy (0.990), class average accuracy (0.939), mean intersection of the union overall classes (IoU) (0.879), precision (0.838), recall (0.879), and F-score (0.8581) values. Overall, the proposed CNN architecture provides the advantages of reduced noise, highly integrated supervision of features, adequate learning, and aggregation of both multi-scale and multilevel features during the training procedure along with the refinement of the overall output predictions.},
DOI = {10.3390/drones6010005}
}



@Article{rs14010157,
AUTHOR = {Jiang, Zongchen and Zhang, Jie and Ma, Yi and Mao, Xingpeng},
TITLE = {Hyperspectral Remote Sensing Detection of Marine Oil Spills Using an Adaptive Long-Term Moment Estimation Optimizer},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {157},
URL = {https://www.mdpi.com/2072-4292/14/1/157},
ISSN = {2072-4292},
ABSTRACT = {Marine oil spills can damage marine ecosystems, economic development, and human health. It is important to accurately identify the type of oil spills and detect the thickness of oil films on the sea surface to obtain the amount of oil spill for on-site emergency responses and scientific decision-making. Optical remote sensing is an important method for marine oil-spill detection and identification. In this study, hyperspectral images of five types of oil spills were obtained using unmanned aerial vehicles (UAV). To address the poor spectral separability between different types of light oils and weak spectral differences in heavy oils with different thicknesses, we propose the adaptive long-term moment estimation (ALTME) optimizer, which cumulatively learns the spectral characteristics and then builds a marine oil-spill detection model based on a one-dimensional convolutional neural network. The results of the detection experiment show that the ALTME optimizer can store in memory multiple batches of long-term oil-spill spectral information, accurately identify the type of oil spills, and detect different thicknesses of oil films. The overall detection accuracy is larger than 98.09%, and the Kappa coefficient is larger than 0.970. The F1-score for the recognition of light-oil types is larger than 0.971, and the F1-score for detecting films of heavy oils with different film thicknesses is larger than 0.980. The proposed optimizer also performs well on a public hyperspectral dataset. We further carried out a feasibility study on oil-spill detection using UAV thermal infrared remote sensing technology, and the results show its potential for oil-spill detection in strong sunlight.},
DOI = {10.3390/rs14010157}
}



@Article{f13010048,
AUTHOR = {Kamarulzaman, Aisyah Marliza Muhmad and Wan Mohd Jaafar, Wan Shafrina and Abdul Maulud, Khairul Nizam and Saad, Siti Nor Maizah and Omar, Hamdan and Mohan, Midhun},
TITLE = {Integrated Segmentation Approach with Machine Learning Classifier in Detecting and Mapping Post Selective Logging Impacts Using UAV Imagery},
JOURNAL = {Forests},
VOLUME = {13},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {48},
URL = {https://www.mdpi.com/1999-4907/13/1/48},
ISSN = {1999-4907},
ABSTRACT = {Selective logging can cause significant impacts on the residual stands, affecting biodiversity and leading to environmental changes. Proper monitoring and mapping of the impacts from logging activities, such as the stumps, felled logs, roads, skid trails, and forest canopy gaps, are crucial for sustainable forest management operations. The purpose of this study is to assess the indicators of selective logging impacts by detecting the individual stumps as the main indicators, evaluating the performance of classification methods to assess the impacts and identifying forest gaps from selective logging activities. The combination of forest inventory field plots and unmanned aerial vehicle (UAV) RGB and overlapped imaged were used in this study to assess these impacts. The study area is located in Ulu Jelai Forest Reserve in the central part of Peninsular Malaysia, covering an experimental study area of 48 ha. The study involved the integration of template matching (TM), object-based image analysis (OBIA), and machine learning classification&mdash;support vector machine (SVM) and artificial neural network (ANN). Forest features and tree stumps were classified, and the canopy height model was used for detecting forest canopy gaps in the post selective logging region. Stump detection using the integration of TM and OBIA produced an accuracy of 75.8% when compared with the ground data. Forest classification using SVM and ANN methods were adopted to extract other impacts from logging activities such as skid trails, felled logs, roads and forest canopy gaps. These methods provided an overall accuracy of 85% and kappa coefficient value of 0.74 when compared with conventional classifier. The logging operation also caused an 18.6% loss of canopy cover. The result derived from this study highlights the potential use of UAVs for efficient post logging impact analysis and can be used to complement conventional forest inventory practices.},
DOI = {10.3390/f13010048}
}



@Article{jmse10010051,
AUTHOR = {Li, Jiqiang and Zhang, Guoqing and Li, Bo},
TITLE = {Robust Adaptive Neural Cooperative Control for the USV-UAV Based on the LVS-LVA Guidance Principle},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {10},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {51},
URL = {https://www.mdpi.com/2077-1312/10/1/51},
ISSN = {2077-1312},
ABSTRACT = {Around the cooperative path-following control for the underactuated surface vessel (USV) and the unmanned aerial vehicle (UAV), a logic virtual ship-logic virtual aircraft (LVS-LVA) guidance principle is developed to generate the reference heading signals for the USV-UAV system by using the &ldquo;virtual ship&rdquo; and the &ldquo;virtual aircraft&rdquo;, which is critical to establish an effective correlation between the USV and the UAV. Taking the steerable variables (the main engine speed and the rudder angle of the USV, and the rotor angular velocities of the UAV) as the control input, a robust adaptive neural cooperative control algorithm was designed by employing the dynamic surface control (DSC), radial basic function neural networks (RBF-NNs) and the event-triggered technique. In the proposed algorithm, the reference roll angle and pitch angle for the UAV can be calculated from the position control loop by virtue of the nonlinear decouple technique. In addition, the system uncertainties were approximated through the RBF-NNs and the transmission burden from the controller to the actuators was reduced for merits of the event-triggered technique. Thus, the derived control law is superior in terms of the concise form, low transmission burden and robustness. Furthermore, the tracking errors of the USV-UAV cooperative control system can converge to a small compact set through adjusting the designed control parameters appropriately, and it can be also guaranteed that all the signals are the semi-global uniformly ultimately bounded (SGUUB). Finally, the effectiveness of the proposed algorithm has been verified via numerical simulations in the presence of the time-varying disturbances.},
DOI = {10.3390/jmse10010051}
}



@Article{rs14010223,
AUTHOR = {Hernández, Daniel and Cecilia, José M. and Cano, Juan-Carlos and Calafate, Carlos T.},
TITLE = {Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {223},
URL = {https://www.mdpi.com/2072-4292/14/1/223},
ISSN = {2072-4292},
ABSTRACT = {With the proliferation of unmanned aerial vehicles (UAVs) in different contexts and application areas, efforts are being made to endow these devices with enough intelligence so as to allow them to perform complex tasks with full autonomy. In particular, covering scenarios such as disaster areas may become particularly difficult due to infrastructure shortage in some areas, often impeding a cloud-based analysis of the data in near-real time. Enabling AI techniques at the edge is therefore fundamental so that UAVs themselves can both capture and process information to gain an understanding of their context, and determine the appropriate course of action in an independent manner. Towards this goal, in this paper, we take determined steps towards UAV autonomy in a disaster scenario such as a flood. In particular, we use a dataset of UAV images relative to different floods taking place in Spain, and then use an AI-based approach that relies on three widely used deep neural networks (DNNs) for semantic segmentation of images, to automatically determine the regions more affected by rains (flooded areas). The targeted algorithms are optimized for GPU-based edge computing platforms, so that the classification can be carried out on the UAVs themselves, and only the algorithm output is uploaded to the cloud for real-time tracking of the flooded areas. This way, we are able to reduce dependency on infrastructure, and to reduce network resource consumption, making the overall process greener and more robust to connection disruptions. Experimental results using different types of hardware and different architectures show that it is feasible to perform advanced real-time processing of UAV images using sophisticated DNN-based solutions.},
DOI = {10.3390/rs14010223}
}



@Article{s22020546,
AUTHOR = {Yu, Xinyang and Chang, Chunyan and Song, Jiaxuan and Zhuge, Yuping and Wang, Ailing},
TITLE = {Precise Monitoring of Soil Salinity in China&rsquo;s Yellow River Delta Using UAV-Borne Multispectral Imagery and a Soil Salinity Retrieval Index},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {546},
URL = {https://www.mdpi.com/1424-8220/22/2/546},
PubMedID = {35062514},
ISSN = {1424-8220},
ABSTRACT = {Monitoring salinity information of salinized soil efficiently and precisely using the unmanned aerial vehicle (UAV) is critical for the rational use and sustainable development of arable land resources. The sensitive parameter and a precise retrieval method of soil salinity, however, remain unknown. This study strived to explore the sensitive parameter and construct an optimal method for retrieving soil salinity. The UAV-borne multispectral image in China&rsquo;s Yellow River Delta was acquired to extract band reflectance, compute vegetation indexes and soil salinity indexes. Soil samples collected from 120 different study sites were used for laboratory salt content measurements. Grey correlation analysis and Pearson correlation coefficient methods were employed to screen sensitive band reflectance and indexes. A new soil salinity retrieval index (SSRI) was then proposed based on the screened sensitive reflectance. The Partial Least Squares Regression (PLSR), Multivariable Linear Regression (MLR), Back Propagation Neural Network (BPNN), Support Vector Machine (SVM), and Random Forest (RF) methods were employed to construct retrieval models based on the sensitive indexes. The results found that green, red, and near-infrared (NIR) bands were sensitive to soil salinity, which can be used to build SSRI. The SSRI-based RF method was the optimal method for accurately retrieving the soil salinity. Its modeling determination coefficient (R2) and Root Mean Square Error (RMSE) were 0.724 and 1.764, respectively; and the validation R2, RMSE, and Residual Predictive Deviation (RPD) were 0.745, 1.879, and 2.211.},
DOI = {10.3390/s22020546}
}



@Article{drones6010019,
AUTHOR = {Kundid Vasić, Mirela and Papić, Vladan},
TITLE = {Improving the Model for Person Detection in Aerial Image Sequences Using the Displacement Vector: A Search and Rescue Scenario},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {19},
URL = {https://www.mdpi.com/2504-446X/6/1/19},
ISSN = {2504-446X},
ABSTRACT = {Recent results in person detection using deep learning methods applied to aerial images gathered by Unmanned Aerial Vehicles (UAVs) have demonstrated the applicability of this approach in scenarios such as Search and Rescue (SAR) operations. In this paper, the continuation of our previous research is presented. The main goal is to further improve detection results, especially in terms of reducing the number of false positive detections and consequently increasing the precision value. We present a new approach that, as input to the multimodel neural network architecture, uses sequences of consecutive images instead of only one static image. Since successive images overlap, the same object of interest needs to be detected in more than one image. The correlation between successive images was calculated, and detected regions in one image were translated to other images based on the displacement vector. The assumption is that an object detected in more than one image has a higher probability of being a true positive detection because it is unlikely that the detection model will find the same false positive detections in multiple images. Based on this information, three different algorithms for rejecting detections and adding detections from one image to other images in the sequence are proposed. All of them achieved precision value about 80% which is increased by almost 20% compared to the current state-of-the-art methods.},
DOI = {10.3390/drones6010019}
}



@Article{s22020601,
AUTHOR = {Sharma, Prakriti and Leigh, Larry and Chang, Jiyul and Maimaitijiang, Maitiniyazi and Caffé, Melanie},
TITLE = {Above-Ground Biomass Estimation in Oats Using UAV Remote Sensing and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {601},
URL = {https://www.mdpi.com/1424-8220/22/2/601},
PubMedID = {35062559},
ISSN = {1424-8220},
ABSTRACT = {Current strategies for phenotyping above-ground biomass in field breeding nurseries demand significant investment in both time and labor. Unmanned aerial vehicles (UAV) can be used to derive vegetation indices (VIs) with high throughput and could provide an efficient way to predict forage yield with high accuracy. The main objective of the study is to investigate the potential of UAV-based multispectral data and machine learning approaches in the estimation of oat biomass. UAV equipped with a multispectral sensor was flown over three experimental oat fields in Volga, South Shore, and Beresford, South Dakota, USA, throughout the pre- and post-heading growth phases of oats in 2019. A variety of vegetation indices (VIs) derived from UAV-based multispectral imagery were employed to build oat biomass estimation models using four machine-learning algorithms: partial least squares (PLS), support vector machine (SVM), Artificial neural network (ANN), and random forest (RF). The results showed that several VIs derived from the UAV collected images were significantly positively correlated with dry biomass for Volga and Beresford (r = 0.2&ndash;0.65), however, in South Shore, VIs were either not significantly or weakly correlated with biomass. For Beresford, approximately 70% of the variance was explained by PLS, RF, and SVM validation models using data collected during the post-heading phase. Likewise for Volga, validation models had lower coefficient of determination (R2 = 0.20&ndash;0.25) and higher error (RMSE = 700&ndash;800 kg/ha) than training models (R2 = 0.50&ndash;0.60; RMSE = 500&ndash;690 kg/ha). In South Shore, validation models were only able to explain approx. 15&ndash;20% of the variation in biomass, which is possibly due to the insignificant correlation values between VIs and biomass. Overall, this study indicates that airborne remote sensing with machine learning has potential for above-ground biomass estimation in oat breeding nurseries. The main limitation was inconsistent accuracy in model prediction across locations. Multiple-year spectral data, along with the inclusion of textural features like crop surface model (CSM) derived height and volumetric indicators, should be considered in future studies while estimating biophysical parameters like biomass.},
DOI = {10.3390/s22020601}
}



@Article{rs14020382,
AUTHOR = {Jing, Yafei and Ren, Yuhuan and Liu, Yalan and Wang, Dacheng and Yu, Linjun},
TITLE = {Automatic Extraction of Damaged Houses by Earthquake Based on Improved YOLOv5: A Case Study in Yangbi},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {382},
URL = {https://www.mdpi.com/2072-4292/14/2/382},
ISSN = {2072-4292},
ABSTRACT = {Efficiently and automatically acquiring information on earthquake damage through remote sensing has posed great challenges because the classical methods of detecting houses damaged by destructive earthquakes are often both time consuming and low in accuracy. A series of deep-learning-based techniques have been developed and recent studies have demonstrated their high intelligence for automatic target extraction for natural and remote sensing images. For the detection of small artificial targets, current studies show that You Only Look Once (YOLO) has a good performance in aerial and Unmanned Aerial Vehicle (UAV) images. However, less work has been conducted on the extraction of damaged houses. In this study, we propose a YOLOv5s-ViT-BiFPN-based neural network for the detection of rural houses. Specifically, to enhance the feature information of damaged houses from the global information of the feature map, we introduce the Vision Transformer into the feature extraction network. Furthermore, regarding the scale differences for damaged houses in UAV images due to the changes in flying height, we apply the Bi-Directional Feature Pyramid Network (BiFPN) for multi-scale feature fusion to aggregate features with different resolutions and test the model. We took the 2021 Yangbi earthquake with a surface wave magnitude (Ms) of 6.4 in Yunan, China, as an example; the results show that the proposed model presents a better performance, with the average precision (AP) being increased by 9.31% and 1.23% compared to YOLOv3 and YOLOv5s, respectively, and a detection speed of 80 FPS, which is 2.96 times faster than YOLOv3. In addition, the transferability test for five other areas showed that the average accuracy was 91.23% and the total processing time was 4 min, while 100 min were needed for professional visual interpreters. The experimental results demonstrate that the YOLOv5s-ViT-BiFPN model can automatically detect damaged rural houses due to destructive earthquakes in UAV images with a good performance in terms of accuracy and timeliness, as well as being robust and transferable.},
DOI = {10.3390/rs14020382}
}



@Article{s22030891,
AUTHOR = {Yuan, Songhe and Ota, Kaoru and Dong, Mianxiong and Zhao, Jianghai},
TITLE = {A Path Planning Method with Perception Optimization Based on Sky Scanning for UAVs},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {891},
URL = {https://www.mdpi.com/1424-8220/22/3/891},
PubMedID = {35161639},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are frequently adopted in disaster management. The vision they provide is extremely valuable for rescuers. However, they face severe problems in their stability in actual disaster scenarios, as the images captured by the on-board sensors cannot consistently give enough information for deep learning models to make accurate decisions. In many cases, UAVs have to capture multiple images from different views to output final recognition results. In this paper, we desire to formulate the fly path task for UAVs, considering the actual perception needs. A convolutional neural networks (CNNs) model is proposed to detect and localize the objects, such as the buildings, as well as an optimization method to find the optimal flying path to accurately recognize as many objects as possible with a minimum time cost. The simulation results demonstrate that the proposed method is effective and efficient, and can address the actual scene understanding and path planning problems for UAVs in the real world well.},
DOI = {10.3390/s22030891}
}



@Article{rs14030663,
AUTHOR = {Ding, Jiujie and Zhang, Jiahuan and Zhan, Zongqian and Tang, Xiaofang and Wang, Xin},
TITLE = {A Precision Efficient Method for Collapsed Building Detection in Post-Earthquake UAV Images Based on the Improved NMS Algorithm and Faster R-CNN},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {663},
URL = {https://www.mdpi.com/2072-4292/14/3/663},
ISSN = {2072-4292},
ABSTRACT = {The results of collapsed building detection act as an important reference for damage assessment after an earthquake, which is crucial for governments in order to efficiently determine the affected area and execute emergency rescue. For this task, unmanned aerial vehicle (UAV) images are often used as the data sources due to the advantages of high flexibility regarding data acquisition time and flying requirements and high resolution. However, collapsed buildings are typically distributed in both connected and independent pieces and with arbitrary shapes, and these are generally more obvious in the UAV images with high resolution; therefore, the corresponding detection is restricted by using conventional convolutional neural networks (CNN) and the detection results are difficult to evaluate. In this work, based on faster region-based convolutional neural network (Faster R-CNN), deformable convolution was used to improve the adaptability to the arbitrarily shaped collapsed buildings. In addition, inspired by the idea of pixelwise semantic segmentation, in contrast to the intersection over union (IoU), a new method which estimates the intersected proportion of objects (IPO) is proposed to describe the degree of the intersection of bounding boxes, leading to two improvements: first, the traditional non-maximum suppression (NMS) algorithm is improved by integration with the IPO to effectively suppress the redundant bounding boxes; second, the IPO is utilized as a new indicator to determine positive and negative bounding boxes, and is introduced as a new strategy for precision and recall estimation, which can be considered a more reasonable measurement of the degree of similarity between the detected bounding boxes and ground truth bounding boxes. Experiments show that compared with other models, our work can obtain better precision and recall for detecting collapsed buildings for which an F1 score of 0.787 was achieved, and the evaluation results from the suggested IPO are qualitatively closer to the ground truth. In conclusion, the improved NMS with the IPO and Faster R-CNN in this paper is feasible and efficient for the detection of collapsed buildings in UAV images, and the suggested IPO strategy is more suitable for the corresponding detection result&rsquo;s evaluation.},
DOI = {10.3390/rs14030663}
}



@Article{electronics11030441,
AUTHOR = {Wang, Min and Chen, Peng and Cao, Zhenxin and Chen, Yun},
TITLE = {Reinforcement Learning-Based UAVs Resource Allocation for Integrated Sensing and Communication (ISAC) System},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {441},
URL = {https://www.mdpi.com/2079-9292/11/3/441},
ISSN = {2079-9292},
ABSTRACT = {Due to the limited ability of a single unmanned aerial vehicle (UAV), group unmanned aerial vehicles (UAVs) have attracted more attention in communication and radar fields. The use of an integrated sensing and communication (ISAC) system can make communication and radar modules share a radar module&rsquo;s resources, coupled with efficient resource allocation methods. It can effectively solve the problem of inadequate UAV resources and the low utilization rate of resources. In this paper, the resource allocation problem is addressed for group UAVs to achieve a trade-off between the detection and communication performance, where the ISAC system is equipped in group UAVs. The resource allocation problem is described by an optimization problem, but with group UAVs, the problem is complex and cannot be solved efficiently. Compared with the traditional resource allocation scheme, which needs a lot of calculation or sample set problems, a novel reinforcement-learning-based method is proposed. We formulate a new reward function by combining mutual information (MI) and the communication rate (CR). The MI describes the radar detection performance, and the CR is for wireless communication. Simulation results show that compared with the traditional Kuhn Munkres (KM) or the deep neural network (DNN) methods, this method has better performance with the increase in problem complexity. Additionally, the execution time of this scheme is close to that of the DNN scheme, and it is better than the KM algorithm.},
DOI = {10.3390/electronics11030441}
}



@Article{buildings12020156,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Shahzad, Danish and Heravi, Amirhossein and Qayyum, Siddra and Akram, Junaid},
TITLE = {Civil Infrastructure Damage and Corrosion Detection: An Application of Machine Learning},
JOURNAL = {Buildings},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {156},
URL = {https://www.mdpi.com/2075-5309/12/2/156},
ISSN = {2075-5309},
ABSTRACT = {Automatic detection of corrosion and associated damages to civil infrastructures such as bridges, buildings, and roads, from aerial images captured by an Unmanned Aerial Vehicle (UAV), helps one to overcome the challenges and shortcomings (objectivity and reliability) associated with the manual inspection methods. Deep learning methods have been widely reported in the literature for civil infrastructure corrosion detection. Among them, convolutional neural networks (CNNs) display promising applicability for the automatic detection of image features less affected by image noises. Therefore, in the current study, we propose a modified version of deep hierarchical CNN architecture, based on 16 convolution layers and cycle generative adversarial network (CycleGAN), to predict pixel-wise segmentation in an end-to-end manner using the images of Bolte Bridge and sky rail areas in Victoria (Melbourne). The convolutedly designed model network proposed in the study is based on learning and aggregation of multi-scale and multilevel features while moving from the low convolutional layers to the high-level layers, thus reducing the consistency loss in images due to the inclusion of CycleGAN. The standard approaches only use the last convolutional layer, but our proposed architecture differs from these approaches and uses multiple layers. Moreover, we have used guided filtering and Conditional Random Fields (CRFs) methods to refine the prediction results. Additionally, the effectiveness of the proposed architecture was assessed using benchmarking data of 600 images of civil infrastructure. Overall, the results show that the deep hierarchical CNN architecture based on 16 convolution layers produced advanced performances when evaluated for different methods, including the baseline, PSPNet, DeepLab, and SegNet. Overall, the extended method displayed the Global Accuracy (GA); Class Average Accuracy (CAC); mean Intersection Of the Union (IOU); Precision (P); Recall (R); and F-score values of 0.989, 0.931, 0.878, 0.849, 0.818 and 0.833, respectively.},
DOI = {10.3390/buildings12020156}
}



@Article{rs14030733,
AUTHOR = {Varela, Sebastian and Pederson, Taylor L. and Leakey, Andrew D. B.},
TITLE = {Implementing Spatio-Temporal 3D-Convolution Neural Networks and UAV Time Series Imagery to Better Predict Lodging Damage in Sorghum},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/14/3/733},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV)-based remote sensing is gaining momentum in a variety of agricultural and environmental applications. Very-high-resolution remote sensing image sets collected repeatedly throughout a crop growing season are becoming increasingly common. Analytical methods able to learn from both spatial and time dimensions of the data may allow for an improved estimation of crop traits, as well as the effects of genetics and the environment on these traits. Multispectral and geometric time series imagery was collected by UAV on 11 dates, along with ground-truth data, in a field trial of 866 genetically diverse biomass sorghum accessions. We compared the performance of Convolution Neural Network (CNN) architectures that used image data from single dates (two spatial dimensions, 2D) versus multiple dates (two spatial dimensions + temporal dimension, 3D) to estimate lodging detection and severity. Lodging was detected with 3D-CNN analysis of time series imagery with 0.88 accuracy, 0.92 Precision, and 0.83 Recall. This outperformed the best 2D-CNN on a single date with 0.85 accuracy, 0.84 Precision, and 0.76 Recall. The variation in lodging severity was estimated by the best 3D-CNN analysis with 9.4% mean absolute error (MAE), 11.9% root mean square error (RMSE), and goodness-of-fit (R2) of 0.76. This was a significant improvement over the best 2D-CNN analysis with 11.84% MAE, 14.91% RMSE, and 0.63 R2. The success of the improved 3D-CNN analysis approach depended on the inclusion of &ldquo;before and after&rdquo; data, i.e., images collected on dates before and after the lodging event. The integration of geometric and spectral features with 3D-CNN architecture was also key to the improved assessment of lodging severity, which is an important and difficult-to-assess phenomenon in bioenergy feedstocks such as biomass sorghum. This demonstrates that spatio-temporal CNN architectures based on UAV time series imagery have significant potential to enhance plant phenotyping capabilities in crop breeding and Precision agriculture applications.},
DOI = {10.3390/rs14030733}
}



@Article{en15031185,
AUTHOR = {Chemali, Ephrem and Kollmeyer, Phillip J. and Preindl, Matthias and Fahmy, Youssef and Emadi, Ali},
TITLE = {A Convolutional Neural Network Approach for Estimation of Li-Ion Battery State of Health from Charge Profiles},
JOURNAL = {Energies},
VOLUME = {15},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {1185},
URL = {https://www.mdpi.com/1996-1073/15/3/1185},
ISSN = {1996-1073},
ABSTRACT = {Intelligent and pragmatic state-of-health (SOH) estimation is critical for the safe and reliable operation of Li-ion batteries, which recently have become ubiquitous for applications such as electrified vehicles, smart grids, smartphones, as well as manned and unmanned aerial vehicles. This paper introduces a convolutional neural network (CNN)-based framework for directly estimating SOH from voltage, current, and temperature measured while the battery is charging. The CNN is trained with data from as many as 28 cells, which were aged at two temperatures using randomized usage profiles. CNNs with between 1 and 6 layers and between 32 and 256 neurons were investigated, and the training data was augmented with noise and error as well to improve accuracy. Importantly, the algorithm was validated for partial charges, as would be common for many applications. Full charges starting between 0 and 95% SOC as well as for multiple ranges ending at less than 100% SOC were tested. The proposed CNN SOH estimation framework achieved a mean average error (MAE) as low as 0.8% over the life of the battery, and still achieved a reasonable MAE of 1.6% when a very small charge window of 85% to 97% SOC was used. While the CNN algorithm is shown to estimate SOH very accurately with partial charge data and two temperatures, further studies could also investigate a wider temperature range and multiple different charge currents or constant power charging.},
DOI = {10.3390/en15031185}
}



@Article{drones6020047,
AUTHOR = {Pádua, Luís and Antão-Geraldes, Ana M. and Sousa, Joaquim J. and Rodrigues, Manuel Ângelo and Oliveira, Verónica and Santos, Daniela and Miguens, Maria Filomena P. and Castro, João Paulo},
TITLE = {Water Hyacinth (Eichhornia crassipes) Detection Using Coarse and High Resolution Multispectral Data},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {47},
URL = {https://www.mdpi.com/2504-446X/6/2/47},
ISSN = {2504-446X},
ABSTRACT = {Efficient detection and monitoring procedures of invasive plant species are required. It is of crucial importance to deal with such plants in aquatic ecosystems, since they can affect biodiversity and, ultimately, ecosystem function and services. In this study, it is intended to detect water hyacinth (Eichhornia crassipes) using multispectral data with different spatial resolutions. For this purpose, high-resolution data (&lt;0.1 m) acquired from an unmanned aerial vehicle (UAV) and coarse-resolution data (10 m) from Sentinel-2 MSI were used. Three areas with a high incidence of water hyacinth located in the Lower Mondego region (Portugal) were surveyed. Different classifiers were used to perform a pixel-based detection of this invasive species in both datasets. From the different classifiers used, the results were achieved by the random forest classifiers stand-out (overall accuracy (OA): 0.94). On the other hand, support vector machine performed worst (OA: 0.87), followed by Gaussian naive Bayes (OA: 0.88), k-nearest neighbours (OA: 0.90), and artificial neural networks (OA: 0.91). The higher spatial resolution from UAV-based data enabled us to detect small amounts of water hyacinth, which could not be detected in Sentinel-2 data. However, and despite the coarser resolution, satellite data analysis enabled us to identify water hyacinth coverage, compared well with a UAV-based survey. Combining both datasets and even considering the different resolutions, it was possible to observe the temporal and spatial evolution of water hyacinth. This approach proved to be an effective way to assess the effects of the mitigation/control measures taken in the study areas. Thus, this approach can be applied to detect invasive species in aquatic environments and to monitor their changes over time.},
DOI = {10.3390/drones6020047}
}



@Article{rs14051125,
AUTHOR = {Sun, Yusen and Jin, Xingji and Pukkala, Timo and Li, Fengri},
TITLE = {Predicting Individual Tree Diameter of Larch (Larix olgensis) from UAV-LiDAR Data Using Six Different Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1125},
URL = {https://www.mdpi.com/2072-4292/14/5/1125},
ISSN = {2072-4292},
ABSTRACT = {Individual tree detection is an increasing trend in LiDAR-based forest inventories. The locations, heights, and crown areas of the detected trees can be estimated rather directly from the LiDAR data by using the LiDAR-based canopy height model and segmentation methods to delineate the tree crowns. However, the most important tree variable is the diameter of the tree stem at the breast height (DBH) which can seldom be interpreted directly from the LiDAR data. Therefore, the use of individually detected trees in forest planning calculations requires predictions for the DBH. This study tested six methods for predicting the DBH from laser scanning data collected by an unmanned aerial vehicle from Larix olgensis plantations located in northeast China. The tested methods were the linear regression model (LM), a linear model with ridge regularization (LMR), support vector regression (SVR), random forest (RF), artificial neural network (ANN), and the k-nearest neighbors (KNN) method. Both tree-level and stand-level metrics derived from the LiDAR point cloud data (for instance percentiles of the height distribution of the echoes) were used as potential predictors of DBH. Compared to the LM, all other methods improved the accuracy of the predictions. On the other hand, all methods tended to underestimate the DBH of the largest trees, which could be due to the inability of the methods to sufficiently describe nonlinear relationships unless different transformations of the LiDAR metrics are used as predictors. The support vector regression was evaluated to be the best method for predicting individual tree diameters from LiDAR data. The benefits of the methods tested in this study can be expected to be the highest in the case of little prior knowledge on the relationships between the predicted variable and predictors, a high number of potential predictors, and strong mutual correlations among the potential predictors.},
DOI = {10.3390/rs14051125}
}



@Article{en15051763,
AUTHOR = {Rao, Jinjun and Li, Bo and Zhang, Zhen and Chen, Dongdong and Giernacki, Wojciech},
TITLE = {Position Control of Quadrotor UAV Based on Cascade Fuzzy Neural Network},
JOURNAL = {Energies},
VOLUME = {15},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1763},
URL = {https://www.mdpi.com/1996-1073/15/5/1763},
ISSN = {1996-1073},
ABSTRACT = {In this article, a cascade fuzzy neural network (FNN) control approach is proposed for position control of quadrotor unmanned aerial vehicle (UAV) system with high coupling and underactuated. For the attitude loop with limited range, the FNN controller parameters were trained offline using flight data, whereas for the position loop, the method based on FNN compensation proportional-integral-derivative (PID) was adopted to tune the system online adaptively. This method not only combined the advantages of fuzzy systems and neural networks but also reduced the amount of calculation for cascade neural network control. Simulations of fixed set point flight and spiral and square trajectory tracking flight were then conducted. The comparison of the results showed that our method had advantages in terms of minimizing overshoot and settling time. Finally, flight experiments were carried out on a DJI Tello quadrotor UAV. The experimental results showed that the proposed controller had good performance in position control.},
DOI = {10.3390/en15051763}
}



@Article{sym14030498,
AUTHOR = {Zheng, Bochao and Wu, Yuewen and Li, Hui and Chen, Zhipeng},
TITLE = {Adaptive Sliding Mode Attitude Control of Quadrotor UAVs Based on the Delta Operator Framework},
JOURNAL = {Symmetry},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {498},
URL = {https://www.mdpi.com/2073-8994/14/3/498},
ISSN = {2073-8994},
ABSTRACT = {In this paper, a novel adaptive sliding-mode control algorithm is proposed for the attitude control of quadrotor unmanned aerial vehicles (UAVs) under the delta operator framework. First, the delta operator technique is used to discretize the attitude control systems of a quadrotor UAV. Then, based on the linear matrix inequality technique, a linear sliding surface is designed to ensure the asymptotical stability of the quadrotor UAV attitude control system during the sliding motion process. Second, by the estimated external disturbance using a radical basis function (RBF) neural network, an adaptive sliding-mode attitude controller is designed such that the states of the quadrotor UAV attitude systems can be driven towards the desired sliding surface, and thus the attitude control objective of the qudarotor UAV is achieved. Compared with the traditional adaptive sliding-mode control algorithm, the proposed adaptive sliding-mode control algorithm can effectively realize the attitude control of a quadrotor UAV subject to strong disturbances and couplings. Finally, comparisons of the simulation results verify the effectiveness and superiority of the control algorithm proposed in this paper.},
DOI = {10.3390/sym14030498}
}



@Article{rs14051231,
AUTHOR = {Zhang, Shimin and Li, Xiuhua and Ba, Yuxuan and Lyu, Xuegang and Zhang, Muqing and Li, Minzan},
TITLE = {Banana Fusarium Wilt Disease Detection by Supervised and Unsupervised Methods from UAV-Based Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1231},
URL = {https://www.mdpi.com/2072-4292/14/5/1231},
ISSN = {2072-4292},
ABSTRACT = {Banana Fusarium wilt (BFW) is a devastating disease with no effective cure methods. Timely and effective detection of the disease and evaluation of its spreading trend will help farmers in making right decisions on plantation management. The main purpose of this study was to find the spectral features of the BFW-infected canopy and build the optimal BFW classification models for different stages of infection. A RedEdge-MX camera mounted on an unmanned aerial vehicle (UAV) was used to collect multispectral images of a banana plantation infected with BFW in July and August 2020. Three types of spectral features were used as the inputs of classification models, including three-visible-band images, five-multispectral-band images, and vegetation indices (VIs). Four supervised methods including Support Vector Machine (SVM), Random Forest (RF), Back Propagation Neural Networks (BPNN) and Logistic Regression (LR), and two unsupervised methods including Hotspot Analysis (HA) and Iterative Self-Organizing Data Analysis Technique Algorithm (ISODATA) were adopted to detect the BFW-infected canopies. Comparing to the healthy canopies, the BFW-infected canopies had higher reflectance in the visible region, but lower reflectance in the NIR region. The classification results showed that most of the supervised and unsupervised methods reached excellent accuracies. Among all the supervised methods, RF based on the five-multispectral-band was considered as the optimal model, with higher overall accuracy (OA) of 97.28% and faster running time of 22 min. For the unsupervised methods, HA reached high and balanced OAs of more than 95% based on the selected VIs derived from the red and NIR band, especially for WDRVI, NDVI, and TDVI. By comprehensively evaluating the classification results of different metrics, the unsupervised method HA was recommended for BFW recognition, especially in the late stage of infection; the supervised method RF was recommended in the early stage of infection to reach a slightly higher accuracy. The results found in this study could give advice for banana plantation management and provide approaches for plant disease detection.},
DOI = {10.3390/rs14051231}
}



@Article{s22052068,
AUTHOR = {Gromada, Krzysztof and Siemiątkowska, Barbara and Stecz, Wojciech and Płochocki, Krystian and Woźniak, Karol},
TITLE = {Real-Time Object Detection and Classification by UAV Equipped With SAR},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {2068},
URL = {https://www.mdpi.com/1424-8220/22/5/2068},
PubMedID = {35271213},
ISSN = {1424-8220},
ABSTRACT = {The article presents real-time object detection and classification methods by unmanned aerial vehicles (UAVs) equipped with a synthetic aperture radar (SAR). Two algorithms have been extensively tested: classic image analysis and convolutional neural networks (YOLOv5). The research resulted in a new method that combines YOLOv5 with post-processing using classic image analysis. It is shown that the new system improves both the classification accuracy and the location of the identified object. The algorithms were implemented and tested on a mobile platform installed on a military-class UAV as the primary unit for online image analysis. The usage of objective low-computational complexity detection algorithms on SAR scans can reduce the size of the scans sent to the ground control station.},
DOI = {10.3390/s22052068}
}



@Article{s22062354,
AUTHOR = {Yu, Ruonan and Li, Hongguang and Jiang, Yalong and Zhang, Baochang and Wang, Yufeng},
TITLE = {Tiny Vehicle Detection for Mid-to-High Altitude UAV Images Based on Visual Attention and Spatial-Temporal Information},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {2354},
URL = {https://www.mdpi.com/1424-8220/22/6/2354},
PubMedID = {35336525},
ISSN = {1424-8220},
ABSTRACT = {Mid-to-high altitude Unmanned Aerial Vehicle (UAV) imagery can provide important remote sensing information between satellite and low altitude platforms, and vehicle detection in mid-to-high altitude UAV images plays a crucial role in land monitoring and disaster relief. However, the high background complexity of images and limited pixels of objects challenge the performance of tiny vehicle detection. Traditional methods suffer from poor adaptation ability to complex backgrounds, while deep neural networks (DNNs) have inherent defects in feature extraction of tiny objects with finite pixels. To address the issue above, this paper puts forward a vehicle detection method combining the DNNs-based and traditional methods for mid-to-high altitude UAV images. We first employ the deep segmentation network to exploit the co-occurrence of the road and vehicles, then detect tiny vehicles based on visual attention mechanism with spatial-temporal constraint information. Experimental results show that the proposed method achieves effective detection of tiny vehicles in complex backgrounds. In addition, ablation experiments are performed to inspect the effectiveness of each component, and comparative experiments on tinier objects are carried out to prove the superior generalization performance of our method in detecting vehicles with a limited size of 5 &times; 5 pixels or less.},
DOI = {10.3390/s22062354}
}



@Article{s22072711,
AUTHOR = {Li, Xiuhua and Ba, Yuxuan and Zhang, Muqing and Nong, Mengling and Yang, Ce and Zhang, Shimin},
TITLE = {Sugarcane Nitrogen Concentration and Irrigation Level Prediction Based on UAV Multispectral Imagery},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {2711},
URL = {https://www.mdpi.com/1424-8220/22/7/2711},
ISSN = {1424-8220},
ABSTRACT = {Sugarcane is the main industrial crop for sugar production, and its growth status is closely related to fertilizer, water, and light input. Unmanned aerial vehicle (UAV)-based multispectral imagery is widely used for high-throughput phenotyping, since it can rapidly predict crop vigor at field scale. This study focused on the potential of drone multispectral images in predicting canopy nitrogen concentration (CNC) and irrigation levels for sugarcane. An experiment was carried out in a sugarcane field with three irrigation levels and five fertilizer levels. Multispectral images at an altitude of 40 m were acquired during the elongating stage. Partial least square (PLS), backpropagation neural network (BPNN), and extreme learning machine (ELM) were adopted to establish CNC prediction models based on various combinations of band reflectance and vegetation indices. The simple ratio pigment index (SRPI), normalized pigment chlorophyll index (NPCI), and normalized green-blue difference index (NGBDI) were selected as model inputs due to their higher grey relational degree with the CNC and lower correlation between one another. The PLS model based on the five-band reflectance and the three vegetation indices achieved the best accuracy (Rv = 0.79, RMSEv = 0.11). Support vector machine (SVM) and BPNN were then used to classify the irrigation levels based on five spectral features which had high correlations with irrigation levels. SVM reached a higher accuracy of 80.6%. The results of this study demonstrated that high resolution multispectral images could provide effective information for CNC prediction and water irrigation level recognition for sugarcane crop.},
DOI = {10.3390/s22072711}
}



