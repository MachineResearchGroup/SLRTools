
@Article{s17112488,
AUTHOR = {Poblete, Tomas and Ortega-Farías, Samuel and Moreno, Miguel Angel and Bardeen, Matthew},
TITLE = {Artificial Neural Network to Predict Vine Water Status Spatial Variability Using Multispectral Information Obtained from an Unmanned Aerial Vehicle (UAV)},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {2488},
URL = {https://www.mdpi.com/1424-8220/17/11/2488},
ISSN = {1424-8220},
ABSTRACT = {Water stress, which affects yield and wine quality, is often evaluated using the midday stem water potential (Ψstem). However, this measurement is acquired on a per plant basis and does not account for the assessment of vine water status spatial variability. The use of multispectral cameras mounted on unmanned aerial vehicle (UAV) is capable to capture the variability of vine water stress in a whole field scenario. It has been reported that conventional multispectral indices (CMI) that use information between 500–800 nm, do not accurately predict plant water status since they are not sensitive to water content. The objective of this study was to develop artificial neural network (ANN) models derived from multispectral images to predict the Ψstem spatial variability of a drip-irrigated Carménère vineyard in Talca, Maule Region, Chile. The coefficient of determination (R2) obtained between ANN outputs and ground-truth measurements of Ψstem were between 0.56–0.87, with the best performance observed for the model that included the bands 550, 570, 670, 700 and 800 nm. Validation analysis indicated that the ANN model could estimate Ψstem with a mean absolute error (MAE) of 0.1 MPa, root mean square error (RMSE) of 0.12 MPa, and relative error (RE) of −9.1%. For the validation of the CMI, the MAE, RMSE and RE values were between 0.26–0.27 MPa, 0.32–0.34 MPa and −24.2–25.6%, respectively.},
DOI = {10.3390/s17112488}
}



@Article{drones2040039,
AUTHOR = {Csillik, Ovidiu and Cherbini, John and Johnson, Robert and Lyons, Andy and Kelly, Maggi},
TITLE = {Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks},
JOURNAL = {Drones},
VOLUME = {2},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2504-446X/2/4/39},
ISSN = {2504-446X},
ABSTRACT = {Remote sensing is important to precision agriculture and the spatial resolution provided by Unmanned Aerial Vehicles (UAVs) is revolutionizing precision agriculture workflows for measurement crop condition and yields over the growing season, for identifying and monitoring weeds and other applications. Monitoring of individual trees for growth, fruit production and pest and disease occurrence remains a high research priority and the delineation of each tree using automated means as an alternative to manual delineation would be useful for long-term farm management. In this paper, we detected citrus and other crop trees from UAV images using a simple convolutional neural network (CNN) algorithm, followed by a classification refinement using superpixels derived from a Simple Linear Iterative Clustering (SLIC) algorithm. The workflow performed well in a relatively complex agricultural environment (multiple targets, multiple size trees and ages, etc.) achieving high accuracy (overall accuracy = 96.24%, Precision (positive predictive value) = 94.59%, Recall (sensitivity) = 97.94%). To our knowledge, this is the first time a CNN has been used with UAV multi-spectral imagery to focus on citrus trees. More of these individual cases are needed to develop standard automated workflows to help agricultural managers better incorporate large volumes of high resolution UAV imagery into agricultural management operations.},
DOI = {10.3390/drones2040039}
}



@Article{s19071486,
AUTHOR = {Gebrehiwot, Asmamaw and Hashemi-Beni, Leila and Thompson, Gary and Kordjamshidi, Parisa and Langan, Thomas E.},
TITLE = {Deep Convolutional Neural Network for Flood Extent Mapping Using Unmanned Aerial Vehicles Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1486},
URL = {https://www.mdpi.com/1424-8220/19/7/1486},
ISSN = {1424-8220},
ABSTRACT = {Flooding is one of the leading threats of natural disasters to human life and property, especially in densely populated urban areas. Rapid and precise extraction of the flooded areas is key to supporting emergency-response planning and providing damage assessment in both spatial and temporal measurements. Unmanned Aerial Vehicles (UAV) technology has recently been recognized as an efficient photogrammetry data acquisition platform to quickly deliver high-resolution imagery because of its cost-effectiveness, ability to fly at lower altitudes, and ability to enter a hazardous area. Different image classification methods including SVM (Support Vector Machine) have been used for flood extent mapping. In recent years, there has been a significant improvement in remote sensing image classification using Convolutional Neural Networks (CNNs). CNNs have demonstrated excellent performance on various tasks including image classification, feature extraction, and segmentation. CNNs can learn features automatically from large datasets through the organization of multi-layers of neurons and have the ability to implement nonlinear decision functions. This study investigates the potential of CNN approaches to extract flooded areas from UAV imagery. A VGG-based fully convolutional network (FCN-16s) was used in this research. The model was fine-tuned and a k-fold cross-validation was applied to estimate the performance of the model on the new UAV imagery dataset. This approach allowed FCN-16s to be trained on the datasets that contained only one hundred training samples, and resulted in a highly accurate classification. Confusion matrix was calculated to estimate the accuracy of the proposed method. The image segmentation results obtained from FCN-16s were compared from the results obtained from FCN-8s, FCN-32s and SVMs. Experimental results showed that the FCNs could extract flooded areas precisely from UAV images compared to the traditional classifiers such as SVMs. The classification accuracy achieved by FCN-16s, FCN-8s, FCN-32s, and SVM for the water class was 97.52%, 97.8%, 94.20% and 89%, respectively.},
DOI = {10.3390/s19071486}
}



@Article{su11092580,
AUTHOR = {Guimarães, Tainá T. and Veronez, Maurício R. and Koste, Emilie C. and Souza, Eniuce M. and Brum, Diego and Gonzaga, Luiz and Mauad, Frederico F.},
TITLE = {Evaluation of Regression Analysis and Neural Networks to Predict Total Suspended Solids in Water Bodies from Unmanned Aerial Vehicle Images},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2580},
URL = {https://www.mdpi.com/2071-1050/11/9/2580},
ISSN = {2071-1050},
ABSTRACT = {The concentration of suspended solids in water is one of the quality parameters that can be recovered using remote sensing data. This paper investigates the data obtained using a sensor coupled to an unmanned aerial vehicle (UAV) in order to estimate the concentration of suspended solids in a lake in southern Brazil based on the relation of spectral images and limnological data. The water samples underwent laboratory analysis to determine the concentration of total suspended solids (TSS). The images obtained using the UAV were orthorectified and georeferenced so that the values referring to the near, green, and blue infrared channels were collected at each sampling point to relate with the laboratory data. The prediction of the TSS concentration was performed using regression analysis and artificial neural networks. The obtained results were important for two main reasons. First, although regression methods have been used in remote sensing applications, they may not be adequate to capture the linear and/or non-linear relationships of interest. Second, results show that the integration of UAV in the mapping of water bodies together with the application of neural networks in the data analysis is a promising approach to predict TSS as well as their temporal and spatial variations.},
DOI = {10.3390/su11092580}
}



@Article{app9152976,
AUTHOR = {Luo, Cai and Zhao, Weikang and Du, Zhenpeng and Yu, Leijian},
TITLE = {A Neural Network Based Landing Method for an Unmanned Aerial Vehicle with Soft Landing Gears},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2976},
URL = {https://www.mdpi.com/2076-3417/9/15/2976},
ISSN = {2076-3417},
ABSTRACT = {This paper presents the design, implementation, and testing of a soft landing gear together with a neural network-based control method for replicating avian landing behavior on non-flat surfaces. With full consideration of unmanned aerial vehicles and landing gear requirements, a quadrotor helicopter, comprised of one flying unit and one landing assistance unit, is employed. Considering the touchdown speed and posture, a novel design of a soft mechanism for non-flat surfaces is proposed, in order to absorb the remaining landing impact. The framework of the control strategy is designed based on a derived dynamic model. A neural network-based backstepping controller is applied to achieve the desired trajectory. The simulation and outdoor testing results attest to the effectiveness and reliability of the proposed control method.},
DOI = {10.3390/app9152976}
}



@Article{electronics8090931,
AUTHOR = {Luo, Cai and Du, Zhenpeng and Yu, Leijian},
TITLE = {Neural Network Control Design for an Unmanned Aerial Vehicle with a Suspended Payload},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {931},
URL = {https://www.mdpi.com/2079-9292/8/9/931},
ISSN = {2079-9292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) demonstrate excellent manoeuvrability in cluttered environments, which makes them a suitable platform as a data collection and parcel delivering system. In this work, the attitude and position control challenges for a drone with a package connected by a wire is analysed. During the delivering task, it is very difficult to eliminate the external unpredictable disturbances. A robust neural network-based backstepping sliding mode control method is designed, which is capable of monitoring the drone&rsquo;s flight path and desired attitude with a suspended cable attached. The convergence of the position and attitude errors together with the Lyapunov function are employed to attest to the robustness of the nonlinear transportation platform. The proposed control system is tested with a simulation and in an outdoor environment. The simulation and open field test results for the UAV transportation platform verify the controllers&rsquo; reliability.},
DOI = {10.3390/electronics8090931}
}



@Article{s19194115,
AUTHOR = {Li, Yuxia and Peng, Bo and He, Lei and Fan, Kunlong and Li, Zhenxu and Tong, Ling},
TITLE = {Road Extraction from Unmanned Aerial Vehicle Remote Sensing Images Based on Improved Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4115},
URL = {https://www.mdpi.com/1424-8220/19/19/4115},
ISSN = {1424-8220},
ABSTRACT = {Roads are vital components of infrastructure, the extraction of which has become a topic of significant interest in the field of remote sensing. Because deep learning has been a popular method in image processing and information extraction, researchers have paid more attention to extracting road using neural networks. This article proposes the improvement of neural networks to extract roads from Unmanned Aerial Vehicle (UAV) remote sensing images. D-Linknet was first considered for its high performance; however, the huge scale of the net reduced computational efficiency. With a focus on the low computational efficiency problem of the popular D-LinkNet, this article made some improvements: (1) Replace the initial block with a stem block. (2) Rebuild the entire network based on ResNet units with a new structure, allowing for the construction of an improved neural network D-Linknetplus. (3) Add a 1 &times; 1 convolution layer before DBlock to reduce the input feature maps, reducing parameters and improving computational efficiency. Add another 1 &times; 1 convolution layer after DBlock to recover the required number of output channels. Accordingly, another improved neural network B-D-LinknetPlus was built. Comparisons were performed between the neural nets, and the verification were made with the Massachusetts Roads Dataset. The results show improved neural networks are helpful in reducing the network size and developing the precision needed for road extraction.},
DOI = {10.3390/s19194115}
}



@Article{s19204484,
AUTHOR = {García Rubio, Víctor and Rodrigo Ferrán, Juan Antonio and Menéndez García, Jose Manuel and Sánchez Almodóvar, Nuria and Lalueza Mayordomo, José María and Álvarez, Federico},
TITLE = {Automatic Change Detection System over Unmanned Aerial Vehicle Video Sequences Based on Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4484},
URL = {https://www.mdpi.com/1424-8220/19/20/4484},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the use of unmanned aerial vehicles (UAVs) for surveillance tasks has increased considerably. This technology provides a versatile and innovative approach to the field. However, the automation of tasks such as object recognition or change detection usually requires image processing techniques. In this paper we present a system for change detection in video sequences acquired by moving cameras. It is based on the combination of image alignment techniques with a deep learning model based on convolutional neural networks (CNNs). This approach covers two important topics. Firstly, the capability of our system to be adaptable to variations in the UAV flight. In particular, the difference of height between flights, and a slight modification of the camera&rsquo;s position or movement of the UAV because of natural conditions such as the effect of wind. These modifications can be produced by multiple factors, such as weather conditions, security requirements or human errors. Secondly, the precision of our model to detect changes in diverse environments, which has been compared with state-of-the-art methods in change detection. This has been measured using the Change Detection 2014 dataset, which provides a selection of labelled images from different scenarios for training change detection algorithms. We have used images from dynamic background, intermittent object motion and bad weather sections. These sections have been selected to test our algorithm&rsquo;s robustness to changes in the background, as in real flight conditions. Our system provides a precise solution for these scenarios, as the mean F-measure score from the image analysis surpasses 97%, and a significant precision in the intermittent object motion category, where the score is above 99%.},
DOI = {10.3390/s19204484}
}



@Article{s20082213,
AUTHOR = {Xu, Hongyang and Fang, Guicai and Fan, Yonghua and Xu, Bin and Yan, Jie},
TITLE = {Universal Adaptive Neural Network Predictive Algorithm for Remotely Piloted Unmanned Combat Aerial Vehicle in Wireless Sensor Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2213},
URL = {https://www.mdpi.com/1424-8220/20/8/2213},
ISSN = {1424-8220},
ABSTRACT = {Remotely piloted unmanned combat aerial vehicle (UCAV) will be a prospective mode of air fight in the future, which can remove the physical restraint of the pilot, maximize the performance of the fighter and effectively reduce casualties. However, it has two difficulties in this mode: (1) There is greater time delay in the network of pilot-wireless sensor-UCAV, which can degrade the piloting performance. (2) Designing of a universal predictive method is very important to pilot different UCAVs remotely, even if the model of the control augmentation system of the UCAV is totally unknown. Considering these two issues, this paper proposes a novel universal modeling method, and establishes a universal nonlinear uncertain model which uses the pilot&rsquo;s remotely piloted command as input and the states of the UCAV with a control augmentation system as output. To deal with the nonlinear uncertainty of the model, a neural network observer is proposed to identify the nonlinear dynamics model online. Meanwhile, to guarantee the stability of the overall observer system, an adaptive law is designed to adjust the neural network weights. To solve the greater transmission time delay existing in the pilot-wireless sensor-UCAV closed-loop system, a time-varying delay state predictor is designed based on the identified nonlinear dynamics model to predict the time delay states. Moreover, the overall observer-predictor system is proved to be uniformly ultimately bounded (UUB). Finally, two simulations verify the effectiveness and universality of the proposed method. The results indicate that the proposed method has desirable performance of accurately compensating the time delay and has universality of remotely piloting two different UCAVs.},
DOI = {10.3390/s20082213}
}



@Article{rs12101668,
AUTHOR = {Feng, Quanlong and Yang, Jianyu and Liu, Yiming and Ou, Cong and Zhu, Dehai and Niu, Bowen and Liu, Jiantao and Li, Baoguo},
TITLE = {Multi-Temporal Unmanned Aerial Vehicle Remote Sensing for Vegetable Mapping Using an Attention-Based Recurrent Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1668},
URL = {https://www.mdpi.com/2072-4292/12/10/1668},
ISSN = {2072-4292},
ABSTRACT = {Vegetable mapping from remote sensing imagery is important for precision agricultural activities such as automated pesticide spraying. Multi-temporal unmanned aerial vehicle (UAV) data has the merits of both very high spatial resolution and useful phenological information, which shows great potential for accurate vegetable classification, especially under complex and fragmented agricultural landscapes. In this study, an attention-based recurrent convolutional neural network (ARCNN) has been proposed for accurate vegetable mapping from multi-temporal UAV red-green-blue (RGB) imagery. The proposed model firstly utilizes a multi-scale deformable CNN to learn and extract rich spatial features from UAV data. Afterwards, the extracted features are fed into an attention-based recurrent neural network (RNN), from which the sequential dependency between multi-temporal features could be established. Finally, the aggregated spatial-temporal features are used to predict the vegetable category. Experimental results show that the proposed ARCNN yields a high performance with an overall accuracy of 92.80%. When compared with mono-temporal classification, the incorporation of multi-temporal UAV imagery could significantly boost the accuracy by 24.49% on average, which justifies the hypothesis that the low spectral resolution of RGB imagery could be compensated by the inclusion of multi-temporal observations. In addition, the attention-based RNN in this study outperforms other feature fusion methods such as feature-stacking. The deformable convolution operation also yields higher classification accuracy than that of a standard convolution unit. Results demonstrate that the ARCNN could provide an effective way for extracting and aggregating discriminative spatial-temporal features for vegetable mapping from multi-temporal UAV RGB imagery.},
DOI = {10.3390/rs12101668}
}



@Article{agriculture10070277,
AUTHOR = {García-Martínez, Héctor and Flores-Magdaleno, Héctor and Ascencio-Hernández, Roberto and Khalil-Gardezi, Abdul and Tijerina-Chávez, Leonardo and Mancilla-Villa, Oscar R. and Vázquez-Peña, Mario A.},
TITLE = {Corn Grain Yield Estimation from Vegetation Indices, Canopy Cover, Plant Density, and a Neural Network Using Multispectral and RGB Images Acquired with Unmanned Aerial Vehicles},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {277},
URL = {https://www.mdpi.com/2077-0472/10/7/277},
ISSN = {2077-0472},
ABSTRACT = {Corn yields vary spatially and temporally in the plots as a result of weather, altitude, variety, plant density, available water, nutrients, and planting date; these are the main factors that influence crop yield. In this study, different multispectral and red-green-blue (RGB) vegetation indices were analyzed, as well as the digitally estimated canopy cover and plant density, in order to estimate corn grain yield using a neural network model. The relative importance of the predictor variables was also analyzed. An experiment was established with five levels of nitrogen fertilization (140, 200, 260, 320, and 380 kg/ha) and four replicates, in a completely randomized block design, resulting in 20 experimental polygons. Crop information was captured using two sensors (Parrot Sequoia_4.9, and DJI FC6310_8.8) mounted on an unmanned aerial vehicle (UAV) for two flight dates at 47 and 79 days after sowing (DAS). The correlation coefficient between the plant density, obtained through the digital count of corn plants, and the corn grain yield was 0.94; this variable was the one with the highest relative importance in the yield estimation according to Garson&rsquo;s algorithm. The canopy cover, digitally estimated, showed a correlation coefficient of 0.77 with respect to the corn grain yield, while the relative importance of this variable in the yield estimation was 0.080 and 0.093 for 47 and 79 DAS, respectively. The wide dynamic range vegetation index (WDRVI), plant density, and canopy cover showed the highest correlation coefficient and the smallest errors (R = 0.99, mean absolute error (MAE) = 0.028 t ha&minus;1, root mean square error (RMSE) = 0.125 t ha&minus;1) in the corn grain yield estimation at 47 DAS, with the WDRVI index and the density being the variables with the highest relative importance for this crop development date. For the 79 DAS flight, the combination of the normalized difference vegetation index (NDVI), normalized difference red edge (NDRE), WDRVI, excess green (EXG), triangular greenness index (TGI), and visible atmospherically resistant index (VARI), as well as plant density and canopy cover, generated the highest correlation coefficient and the smallest errors (R = 0.97, MAE = 0.249 t ha&minus;1, RMSE = 0.425 t ha&minus;1) in the corn grain yield estimation, where the density and the NDVI were the variables with the highest relative importance, with values of 0.295 and 0.184, respectively. However, the WDRVI, plant density, and canopy cover estimated the corn grain yield with acceptable precision (R = 0.96, MAE = 0.209 t ha&minus;1, RMSE = 0.449 t ha&minus;1). The generated neural network models provided a high correlation coefficient between the estimated and the observed corn grain yield, and also showed acceptable errors in the yield estimation. The spectral information registered through remote sensors mounted on unmanned aerial vehicles and its processing in vegetation indices, canopy cover, and plant density allowed the characterization and estimation of corn grain yield. Such information is very useful for decision-making and agricultural activities planning.},
DOI = {10.3390/agriculture10070277}
}



@Article{app10144991,
AUTHOR = {Villaseñor, Carlos and Gallegos, Alberto A. and Gomez-Avila, Javier and López-González, Gehová and Rios, Jorge D. and Arana-Daniel, Nancy},
TITLE = {Environment Classification for Unmanned Aerial Vehicle Using Convolutional Neural Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4991},
URL = {https://www.mdpi.com/2076-3417/10/14/4991},
ISSN = {2076-3417},
ABSTRACT = {Environment classification is one of the most critical tasks for Unmanned Aerial Vehicles (UAV). Since water accumulation may destabilize UAV, clouds must be detected and avoided. In a previous work presented by the authors, Superpixel Segmentation (SPS) descriptors with low computational cost are used to classify ground, sky, and clouds. In this paper, an enhanced approach to classify the environment in those three classes is presented. The proposed scheme consists of a Convolutional Neural Network (CNN) trained with a dataset generated by both, an human expert and a Support Vector Machine (SVM) to capture context and precise localization. The advantage of using this approach is that the CNN classifies each pixel, instead of a cluster like in SPS, which improves the resolution of the classification, also, is less tedious for the human expert to generate a few training samples instead of the normal amount that it is required. This proposal is implemented for images obtained from video and photographic cameras mounted on a UAV facing in the same direction of the vehicle flight. Experimental results and comparison with other approaches are shown to demonstrate the effectiveness of the algorithm.},
DOI = {10.3390/app10144991}
}



@Article{agronomy11020398,
AUTHOR = {Seo, Min-Guk and Shin, Hyo-Sang and Tsourdos, Antonios},
TITLE = {Soil Moisture Retrieval Model Design with Multispectral and Infrared Images from Unmanned Aerial Vehicles Using Convolutional Neural Network},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {398},
URL = {https://www.mdpi.com/2073-4395/11/2/398},
ISSN = {2073-4395},
ABSTRACT = {This paper deals with a soil moisture retrieval model design with airborne measurements for remote monitoring of soil moisture level in large crop fields. A small quadrotor unmanned aerial vehicle (UAV) is considered as a remote sensing platform for high spatial resolutions of airborne images and easy operations. A combination of multispectral and infrared (IR) sensors is applied to overcome the effects of canopies convering the field on the sensor measurements. Convolutional neural network (CNN) is utilized to take the measurement images directly as inputs for the soil moisture retrieval model without loss of information. The procedures to obtain an input image corresponding to a certain soil moisture level measurement point are addressed, and the overall structure of the proposed CNN-based model is suggested with descriptions. Training and testing of the proposed soil moisture retrieval model are conducted to verify and validate its performance and address the effects of input image sizes and errors on input images. The soil moisture level estimation performance decreases when the input image size increases as the ratio of the pixel corresponding to the point to estimate soil moisture level to the total number of pixels in the input image, whereas the input image size should be large enough to include this pixel under the errors in input images. The comparative study shows that the proposed CNN-based algorithm is advantageous on estimation performance by maintaining spatial information of pixels on the input images.},
DOI = {10.3390/agronomy11020398}
}



@Article{app11094084,
AUTHOR = {Hua, Lianghao and Zhang, Jianfeng and Li, Dejie and Xi, Xiaobo},
TITLE = {Fault-Tolerant Active Disturbance Rejection Control of Plant Protection of Unmanned Aerial Vehicles Based on a Spatio-Temporal RBF Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4084},
URL = {https://www.mdpi.com/2076-3417/11/9/4084},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a fault-tolerant flight control method for a multi-rotor UAV under actuator failure and external wind disturbances. The control method is based on an active disturbance rejection controller (ADRC) and spatio-temporal radial basis function neural networks, which can be used to achieve the stable control of the system when the parameters of the UAV mathematical model change. Firstly, an active disturbance rejection controller with an optimized parameter design is designed for rthe obust control of a multi-rotor vehicle. Secondly, a spatio-temporal radial basis function neural network with a new adaptive kernel is designed. In addition, the output of the novel radial basis function neural network is used to estimate fusion parameters containing actuator faults and model uncertainties and, consequently, to design an active fault-tolerant controller for a multi-rotor vehicle. Finally, fault injection experiments are carried out with the Qball-X4 quadrotor UAV as a specific research object, and the experimental results show the effectiveness of the proposed self-tolerant, fault-tolerant control method.},
DOI = {10.3390/app11094084}
}



@Article{rs13132643,
AUTHOR = {Pedro, Dário and Matos-Carvalho, João P. and Fonseca, José M. and Mora, André},
TITLE = {Collision Avoidance on Unmanned Aerial Vehicles Using Neural Network Pipelines and Flow Clustering Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2643},
URL = {https://www.mdpi.com/2072-4292/13/13/2643},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Autonomous Vehicles (UAV), while not a recent invention, have recently acquired a prominent position in many industries, and they are increasingly used not only by avid customers, but also in high-demand technical use-cases, and will have a significant societal effect in the coming years. However, the use of UAVs is fraught with significant safety threats, such as collisions with dynamic obstacles (other UAVs, birds, or randomly thrown objects). This research focuses on a safety problem that is often overlooked due to a lack of technology and solutions to address it: collisions with non-stationary objects. A novel approach is described that employs deep learning techniques to solve the computationally intensive problem of real-time collision avoidance with dynamic objects using off-the-shelf commercial vision sensors. The suggested approach’s viability was corroborated by multiple experiments, firstly in simulation, and afterward in a concrete real-world case, that consists of dodging a thrown ball. A novel video dataset was created and made available for this purpose, and transfer learning was also tested, with positive results.},
DOI = {10.3390/rs13132643}
}



@Article{electronics10222764,
AUTHOR = {Hassan, Syed-Ali and Rahim, Tariq and Shin, Soo-Young},
TITLE = {An Improved Deep Convolutional Neural Network-Based Autonomous Road Inspection Scheme Using Unmanned Aerial Vehicles},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {2764},
URL = {https://www.mdpi.com/2079-9292/10/22/2764},
ISSN = {2079-9292},
ABSTRACT = {Recent advancements in the field of machine learning (ML) provide opportunity to conduct research on autonomous devices for a variety of applications. Intelligent decision-making is a critical task for self-driving systems. An attempt is made in this study to use a deep learning (DL) approach for the early detection of road cracks, potholes, and the yellow lane. The accuracy is not sufficient after training with the default model. To enhance accuracy, a convolutional neural network (CNN) model with 13 convolutional layers, a softmax layer as an output layer, and two fully connected layers (FCN) are constructed. In order to achieve the deeper propagation and to prevent saturation in the training phase, mish activation is employed in the first 12 layers with a rectified linear unit (ReLU) activation function. The upgraded CNN model performs better than the default CNN model in terms of accuracy. For the varied situation, a revised and enriched dataset for road cracks, potholes, and the yellow lane is created. The yellow lane is detected and tracked in order to move the unmanned aerial vehicle (UAV) autonomously by following yellow lane. After identifying a yellow lane, the UAV performs autonomous navigation while concurrently detecting road cracks and potholes using the robot operating system within the UAV. The performance model is benchmarked using performance measures, such as accuracy, sensitivity, F1-score, F2-score, and dice-coefficient, which demonstrate that the suggested technique produces better outcomes.},
DOI = {10.3390/electronics10222764}
}



@Article{rs14010046,
AUTHOR = {Wei, Lele and Luo, Yusen and Xu, Lizhang and Zhang, Qian and Cai, Qibing and Shen, Mingjun},
TITLE = {Deep Convolutional Neural Network for Rice Density Prescription Map at Ripening Stage Using Unmanned Aerial Vehicle-Based Remotely Sensed Images},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {46},
URL = {https://www.mdpi.com/2072-4292/14/1/46},
ISSN = {2072-4292},
ABSTRACT = {In this paper, UAV (unmanned aerial vehicle, DJI Phantom4RTK) and YOLOv4 (You Only Look Once) target detection deep neural network methods were employed to collected mature rice images and detect rice ears to produce a rice density prescription map. The YOLOv4 model was used for rice ear quick detection of rice images captured by a UAV. The Kriging interpolation algorithm was used in ArcGIS to make rice density prescription maps. Mature rice images collected by a UAV were marked manually and used to build the training and testing datasets. The resolution of the images was 300 &times; 300 pixels. The batch size was 2, and the initial learning rate was 0.01, and the mean average precision (mAP) of the best trained model was 98.84%. Exceptionally, the network ability to detect rice in different health states was also studied with a mAP of 95.42% in the no infection rice images set, 98.84% in the mild infection rice images set, 94.35% in the moderate infection rice images set, and 93.36% in the severe infection rice images set. According to the severity of rice sheath blight, which can cause rice leaves to wither and turn yellow, the blighted grain percentage increased and the thousand-grain weight decreased, the rice images were divided into these four infection levels. The ability of the network model (R2 = 0.844) was compared with traditional image processing segmentation methods (R2 = 0.396) based on color and morphology features and machine learning image segmentation method (Support Vector Machine, SVM R2 = 0.0817, and K-means R2 = 0.1949) for rice ear counting. The results highlight that the CNN has excellent robustness, and can generate a wide range of rice density prescription maps.},
DOI = {10.3390/rs14010046}
}



