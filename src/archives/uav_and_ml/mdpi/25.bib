
@Article{rs13193919,
AUTHOR = {Mo, Jiawei and Lan, Yubin and Yang, Dongzi and Wen, Fei and Qiu, Hongbin and Chen, Xin and Deng, Xiaoling},
TITLE = {Deep Learning-Based Instance Segmentation Method of Litchi Canopy from UAV-Acquired Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3919},
URL = {https://www.mdpi.com/2072-4292/13/19/3919},
ISSN = {2072-4292},
ABSTRACT = {Instance segmentation of fruit tree canopies from images acquired by unmanned aerial vehicles (UAVs) is of significance for the precise management of orchards. Although deep learning methods have been widely used in the fields of feature extraction and classification, there are still phenomena of complex data and strong dependence on software performances. This paper proposes a deep learning-based instance segmentation method of litchi trees, which has a simple structure and lower requirements for data form. Considering that deep learning models require a large amount of training data, a labor-friendly semi-auto method for image annotation is introduced. The introduction of this method allows for a significant improvement in the efficiency of data pre-processing. Facing the high requirement of a deep learning method for computing resources, a partition-based method is presented for the segmentation of high-resolution digital orthophoto maps (DOMs). Citrus data is added to the training set to alleviate the lack of diversity of the original litchi dataset. The average precision (AP) is selected to evaluate the metric of the proposed model. The results show that with the help of training with the litchi-citrus datasets, the best AP on the test set reaches 96.25%.},
DOI = {10.3390/rs13193919}
}



@Article{s21196540,
AUTHOR = {Pan, Qian and Gao, Maofang and Wu, Pingbo and Yan, Jingwen and Li, Shilei},
TITLE = {A Deep-Learning-Based Approach for Wheat Yellow Rust Disease Recognition from Unmanned Aerial Vehicle Images},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6540},
URL = {https://www.mdpi.com/1424-8220/21/19/6540},
PubMedID = {34640873},
ISSN = {1424-8220},
ABSTRACT = {Yellow rust is a disease with a wide range that causes great damage to wheat. The traditional method of manually identifying wheat yellow rust is very inefficient. To improve this situation, this study proposed a deep-learning-based method for identifying wheat yellow rust from unmanned aerial vehicle (UAV) images. The method was based on the pyramid scene parsing network (PSPNet) semantic segmentation model to classify healthy wheat, yellow rust wheat, and bare soil in small-scale UAV images, and to investigate the spatial generalization of the model. In addition, it was proposed to use the high-accuracy classification results of traditional algorithms as weak samples for wheat yellow rust identification. The recognition accuracy of the PSPNet model in this study reached 98%. On this basis, this study used the trained semantic segmentation model to recognize another wheat field. The results showed that the method had certain generalization ability, and its accuracy reached 98%. In addition, the high-accuracy classification result of a support vector machine was used as a weak label by weak supervision, which better solved the labeling problem of large-size images, and the final recognition accuracy reached 94%. Therefore, the present study method facilitated timely control measures to reduce economic losses.},
DOI = {10.3390/s21196540}
}



@Article{drones5040111,
AUTHOR = {Avola, Danilo and Pannone, Daniele},
TITLE = {MAGI: Multistream Aerial Segmentation of Ground Images with Small-Scale Drones},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {111},
URL = {https://www.mdpi.com/2504-446X/5/4/111},
ISSN = {2504-446X},
ABSTRACT = {In recent years, small-scale drones have been used in heterogeneous tasks, such as border control, precision agriculture, and search and rescue. This is mainly due to their small size that allows for easy deployment, their low cost, and their increasing computing capability. The latter aspect allows for researchers and industries to develop complex machine- and deep-learning algorithms for several challenging tasks, such as object classification, object detection, and segmentation. Focusing on segmentation, this paper proposes a novel deep-learning model for semantic segmentation. The model follows a fully convolutional multistream approach to perform segmentation on different image scales. Several streams perform convolutions by exploiting kernels of different sizes, making segmentation tasks robust to flight altitude changes. Extensive experiments were performed on the UAV Mosaicking and Change Detection (UMCD) dataset, highlighting the effectiveness of the proposed method.},
DOI = {10.3390/drones5040111}
}



@Article{s21206826,
AUTHOR = {Yang, Baohua and Zhu, Yue and Zhou, Shuaijun},
TITLE = {Accurate Wheat Lodging Extraction from Multi-Channel UAV Images Using a Lightweight Network Model},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {6826},
URL = {https://www.mdpi.com/1424-8220/21/20/6826},
PubMedID = {34696038},
ISSN = {1424-8220},
ABSTRACT = {The extraction of wheat lodging is of great significance to post-disaster agricultural production management, disaster assessment and insurance subsidies. At present, the recognition of lodging wheat in the actual complex field environment still has low accuracy and poor real-time performance. To overcome this gap, first, four-channel fusion images, including RGB and DSM (digital surface model), as well as RGB and ExG (excess green), were constructed based on the RGB image acquired from unmanned aerial vehicle (UAV). Second, a Mobile U-Net model that combined a lightweight neural network with a depthwise separable convolution and U-Net model was proposed. Finally, three data sets (RGB, RGB + DSM and RGB + ExG) were used to train, verify, test and evaluate the proposed model. The results of the experiment showed that the overall accuracy of lodging recognition based on RGB + DSM reached 88.99%, which is 11.8% higher than that of original RGB and 6.2% higher than that of RGB + ExG. In addition, our proposed model was superior to typical deep learning frameworks in terms of model parameters, processing speed and segmentation accuracy. The optimized Mobile U-Net model reached 9.49 million parameters, which was 27.3% and 33.3% faster than the FCN and U-Net models, respectively. Furthermore, for RGB + DSM wheat lodging extraction, the overall accuracy of Mobile U-Net was improved by 24.3% and 15.3% compared with FCN and U-Net, respectively. Therefore, the Mobile U-Net model using RGB + DSM could extract wheat lodging with higher accuracy, fewer parameters and stronger robustness.},
DOI = {10.3390/s21206826}
}



@Article{su132011359,
AUTHOR = {Aliyari, Mostafa and Droguett, Enrique Lopez and Ayele, Yonas Zewdu},
TITLE = {UAV-Based Bridge Inspection via Transfer Learning},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {11359},
URL = {https://www.mdpi.com/2071-1050/13/20/11359},
ISSN = {2071-1050},
ABSTRACT = {As bridge inspection becomes more advanced and more ubiquitous, artificial intelligence (AI) techniques, such as machine and deep learning, could offer suitable solutions to the nation’s problems of overdue bridge inspections. AI coupling with various data that can be captured by unmanned aerial vehicles (UAVs) enables fully automated bridge inspections. The key to the success of automated bridge inspection is a model capable of detecting failures from UAV data like images and films. In this context, this paper investigates the performances of state-of-the-art convolutional neural networks (CNNs) through transfer learning for crack detection in UAV-based bridge inspection. The performance of different CNN models is evaluated via UAV-based inspection of Skodsberg Bridge, located in eastern Norway. The low-level features are extracted in the last layers of the CNN models and these layers are trained using 19,023 crack and non-crack images. There is always a trade-off between the number of trainable parameters that CNN models need to learn for each specific task and the number of non-trainable parameters that come from transfer learning. Therefore, selecting the optimized amount of transfer learning is a challenging task and, as there is not enough research in this area, it will be studied in this paper. Moreover, UAV-based bridge inception images require specific attention to establish a suitable dataset as the input of CNN models that are trained on homogenous images. However, in the real implementation of CNN models in UAV-based bridge inspection images, there are always heterogeneities and noises, such as natural and artificial effects like different luminosities, spatial positions, and colors of the elements in an image. In this study, the effects of such heterogeneities on the performance of CNN models via transfer learning are examined. The results demonstrate that with a simplified image cropping technique and with minimum effort to preprocess images, CNN models can identify crack elements from non-crack elements with 81% accuracy. Moreover, the results show that heterogeneities inherent in UAV-based bridge inspection data significantly affect the performance of CNN models with an average 32.6% decrease of accuracy of the CNN models. It is also found that deeper CNN models do not provide higher accuracy compared to the shallower CNN models when the number of images for adoption to a specific task, in this case crack detection, is not large enough; in this study, 19,023 images and shallower models outperform the deeper models.},
DOI = {10.3390/su132011359}
}



@Article{rs13214196,
AUTHOR = {Koay, Hong Vin and Chuah, Joon Huang and Chow, Chee-Onn and Chang, Yang-Lang and Yong, Keh Kok},
TITLE = {YOLO-RTUAV: Towards Real-Time Vehicle Detection through Aerial Images with Low-Cost Edge Devices},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {4196},
URL = {https://www.mdpi.com/2072-4292/13/21/4196},
ISSN = {2072-4292},
ABSTRACT = {Object detection in aerial images has been an active research area thanks to the vast availability of unmanned aerial vehicles (UAVs). Along with the increase of computational power, deep learning algorithms are commonly used for object detection tasks. However, aerial images have large variations, and the object sizes are usually small, rendering lower detection accuracy. Besides, real-time inferencing on low-cost edge devices remains an open-ended question. In this work, we explored the usage of state-of-the-art deep learning object detection on low-cost edge hardware. We propose YOLO-RTUAV, an improved version of YOLOv4-Tiny, as the solution. We benchmarked our proposed models with various state-of-the-art models on the VAID and COWC datasets. Our proposed model can achieve higher mean average precision (mAP) and frames per second (FPS) than other state-of-the-art tiny YOLO models, especially on a low-cost edge device such as the Jetson Nano 2 GB. It was observed that the Jetson Nano 2 GB can achieve up to 12.8 FPS with a model size of only 5.5 MB.},
DOI = {10.3390/rs13214196}
}



@Article{drones5040127,
AUTHOR = {Raza, Wamiq and Osman, Anas and Ferrini, Francesco and Natale, Francesco De},
TITLE = {Energy-Efficient Inference on the Edge Exploiting TinyML Capabilities for UAVs},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {127},
URL = {https://www.mdpi.com/2504-446X/5/4/127},
ISSN = {2504-446X},
ABSTRACT = {In recent years, the proliferation of unmanned aerial vehicles (UAVs) has increased dramatically. UAVs can accomplish complex or dangerous tasks in a reliable and cost-effective way but are still limited by power consumption problems, which pose serious constraints on the flight duration and completion of energy-demanding tasks. The possibility of providing UAVs with advanced decision-making capabilities in an energy-effective way would be extremely beneficial. In this paper, we propose a practical solution to this problem that exploits deep learning on the edge. The developed system integrates an OpenMV microcontroller into a DJI Tello Micro Aerial Vehicle (MAV). The microcontroller hosts a set of machine learning-enabled inference tools that cooperate to control the navigation of the drone and complete a given mission objective. The goal of this approach is to leverage the new opportunistic features of TinyML through OpenMV including offline inference, low latency, energy efficiency, and data security. The approach is successfully validated on a practical application consisting of the onboard detection of people wearing protection masks in a crowded environment.},
DOI = {10.3390/drones5040127}
}



@Article{rs13214377,
AUTHOR = {Sun, Long and Chen, Jie and Feng, Dazheng and Xing, Mengdao},
TITLE = {Parallel Ensemble Deep Learning for Real-Time Remote Sensing Video Multi-Target Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {4377},
URL = {https://www.mdpi.com/2072-4292/13/21/4377},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) is one of the main means of information warfare, such as in battlefield cruises, reconnaissance, and military strikes. Rapid detection and accurate recognition of key targets in UAV images are the basis of subsequent military tasks. The UAV image has characteristics of high resolution and small target size, and in practical application, the detection speed is often required to be fast. Existing algorithms are not able to achieve an effective trade-off between detection accuracy and speed. Therefore, this paper proposes a parallel ensemble deep learning framework for unmanned aerial vehicle video multi-target detection, which is a global and local joint detection strategy. It combines a deep learning target detection algorithm with template matching to make full use of image information. It also integrates multi-process and multi-threading mechanisms to speed up processing. Experiments show that the system has high detection accuracy for targets with focal lengths varying from one to ten times. At the same time, the real-time and stable display of detection results is realized by aiming at the moving UAV video image.},
DOI = {10.3390/rs13214377}
}



@Article{s21217307,
AUTHOR = {Li, Mingjun and Cai, Zhihao and Zhao, Jiang and Wang, Yibo and Wang, Yingxun and Lu, Kelin},
TITLE = {MNNMs Integrated Control for UAV Autonomous Tracking Randomly Moving Target Based on Learning Method},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7307},
URL = {https://www.mdpi.com/1424-8220/21/21/7307},
PubMedID = {34770614},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we investigate the problem of unmanned aerial vehicles (UAVs) autonomous tracking moving target with only an airborne camera sensor. We proposed a novel integrated controller framework for this problem based on multi-neural-network modules (MNNMs). In this framework, two neural networks are designed for target perception and guidance control, respectively. The deep learning method and reinforcement learning method are applied to train the integrated controller. The training result demonstrates that the integrated controller can be trained more quickly and efficiently than the end-to-end controller trained by the deep reinforcement learning method. The flight tests with the integrated controller are implemented in simulated and realistic environments, the results show that the integrated controller trained in simulation can easily be transferred to the realistic environment and achieve the UAV tracking randomly moving target, which has a faster motion velocity. The integrated controller based on the MNNMs structure has a better performance on an autonomous tracking target than the control mode that combines with a perception network and a proportional integral derivative controller.},
DOI = {10.3390/s21217307}
}



@Article{rs13214445,
AUTHOR = {Nazeri, Behrokh and Crawford, Melba},
TITLE = {Detection of Outliers in LiDAR Data Acquired by Multiple Platforms over Sorghum and Maize},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {4445},
URL = {https://www.mdpi.com/2072-4292/13/21/4445},
ISSN = {2072-4292},
ABSTRACT = {High-resolution point cloud data acquired with a laser scanner from any platform contain random noise and outliers. Therefore, outlier detection in LiDAR data is often necessary prior to analysis. Applications in agriculture are particularly challenging, as there is typically no prior knowledge of the statistical distribution of points, plant complexity, and local point densities, which are crop-dependent. The goals of this study were first to investigate approaches to minimize the impact of outliers on LiDAR acquired over agricultural row crops, and specifically for sorghum and maize breeding experiments, by an unmanned aerial vehicle (UAV) and a wheel-based ground platform; second, to evaluate the impact of existing outliers in the datasets on leaf area index (LAI) prediction using LiDAR data. Two methods were investigated to detect and remove the outliers from the plant datasets. The first was based on surface fitting to noisy point cloud data via normal and curvature estimation in a local neighborhood. The second utilized the PointCleanNet deep learning framework. Both methods were applied to individual plants and field-based datasets. To evaluate the method, an F-score was calculated for synthetic data in the controlled conditions, and LAI, the variable being predicted, was computed both before and after outlier removal for both scenarios. Results indicate that the deep learning method for outlier detection is more robust than the geometric approach to changes in point densities, level of noise, and shapes. The prediction of LAI was also improved for the wheel-based vehicle data based on the coefficient of determination (R2) and the root mean squared error (RMSE) of the residuals before and after the removal of outliers.},
DOI = {10.3390/rs13214445}
}



@Article{app112210595,
AUTHOR = {Zhao, Wenlong and Meng, Zhijun and Wang, Kaipeng and Zhang, Jiahui and Lu, Shaoze},
TITLE = {Hierarchical Active Tracking Control for UAVs via Deep Reinforcement Learning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {10595},
URL = {https://www.mdpi.com/2076-3417/11/22/10595},
ISSN = {2076-3417},
ABSTRACT = {Active tracking control is essential for UAVs to perform autonomous operations in GPS-denied environments. In the active tracking task, UAVs take high-dimensional raw images as input and execute motor actions to actively follow the dynamic target. Most research focuses on three-stage methods, which entail perception first, followed by high-level decision-making based on extracted spatial information of the dynamic target, and then UAV movement control, using a low-level dynamic controller. Perception methods based on deep neural networks are powerful but require considerable effort for manual ground truth labeling. Instead, we unify the perception and decision-making stages using a high-level controller and then leverage deep reinforcement learning to learn the mapping from raw images to the high-level action commands in the V-REP-based environment, where simulation data are infinite and inexpensive. This end-to-end method also has the advantages of a small parameter size and reduced effort requirements for parameter turning in the decision-making stage. The high-level controller, which has a novel architecture, explicitly encodes the spatial and temporal features of the dynamic target. Auxiliary segmentation and motion-in-depth losses are introduced to generate denser training signals for the high-level controller’s fast and stable training. The high-level controller and a conventional low-level PID controller constitute our hierarchical active tracking control framework for the UAVs’ active tracking task. Simulation experiments show that our controller trained with several augmentation techniques sufficiently generalizes dynamic targets with random appearances and velocities, and achieves significantly better performance, compared with three-stage methods.},
DOI = {10.3390/app112210595}
}



@Article{electronics10222764,
AUTHOR = {Hassan, Syed-Ali and Rahim, Tariq and Shin, Soo-Young},
TITLE = {An Improved Deep Convolutional Neural Network-Based Autonomous Road Inspection Scheme Using Unmanned Aerial Vehicles},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {2764},
URL = {https://www.mdpi.com/2079-9292/10/22/2764},
ISSN = {2079-9292},
ABSTRACT = {Recent advancements in the field of machine learning (ML) provide opportunity to conduct research on autonomous devices for a variety of applications. Intelligent decision-making is a critical task for self-driving systems. An attempt is made in this study to use a deep learning (DL) approach for the early detection of road cracks, potholes, and the yellow lane. The accuracy is not sufficient after training with the default model. To enhance accuracy, a convolutional neural network (CNN) model with 13 convolutional layers, a softmax layer as an output layer, and two fully connected layers (FCN) are constructed. In order to achieve the deeper propagation and to prevent saturation in the training phase, mish activation is employed in the first 12 layers with a rectified linear unit (ReLU) activation function. The upgraded CNN model performs better than the default CNN model in terms of accuracy. For the varied situation, a revised and enriched dataset for road cracks, potholes, and the yellow lane is created. The yellow lane is detected and tracked in order to move the unmanned aerial vehicle (UAV) autonomously by following yellow lane. After identifying a yellow lane, the UAV performs autonomous navigation while concurrently detecting road cracks and potholes using the robot operating system within the UAV. The performance model is benchmarked using performance measures, such as accuracy, sensitivity, F1-score, F2-score, and dice-coefficient, which demonstrate that the suggested technique produces better outcomes.},
DOI = {10.3390/electronics10222764}
}



@Article{rs13224606,
AUTHOR = {Eide, Austin and Koparan, Cengiz and Zhang, Yu and Ostlie, Michael and Howatt, Kirk and Sun, Xin},
TITLE = {UAV-Assisted Thermal Infrared and Multispectral Imaging of Weed Canopies for Glyphosate Resistance Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {4606},
URL = {https://www.mdpi.com/2072-4292/13/22/4606},
ISSN = {2072-4292},
ABSTRACT = {The foundation of contemporary weed management practices in many parts of the world is glyphosate. However, dependency on the effectiveness of herbicide practices has led to overuse through continuous growth of crops resistant to a single mode of action. In order to provide a cost-effective weed management strategy that does not promote glyphosate-resistant weed biotypes, differences between resistant and susceptible biotypes have to be identified accurately in the field conditions. Unmanned Aerial Vehicle (UAV)-assisted thermal and multispectral remote sensing has potential for detecting biophysical characteristics of weed biotypes during the growing season, which includes distinguishing glyphosate-susceptible and glyphosate-resistant weed populations based on canopy temperature and deep learning driven weed identification algorithms. The objective of this study was to identify herbicide resistance after glyphosate application in true field conditions by analyzing the UAV-acquired thermal and multispectral response of kochia, waterhemp, redroot pigweed, and common ragweed. The data were processed in ArcGIS for raster classification as well as spectral comparison of glyphosate-resistant and glyphosate-susceptible weeds. The classification accuracy between the sensors and classification methods of maximum likelihood, random trees, and Support Vector Machine (SVM) were compared. The random trees classifier performed the best at 4 days after application (DAA) for kochia with 62.9% accuracy. The maximum likelihood classifier provided the highest performing result out of all classification methods with an accuracy of 75.2%. A commendable classification was made at 8 DAA where the random trees classifier attained an accuracy of 87.2%. However, thermal reflectance measurements as a predictor for glyphosate resistance within weed populations in field condition was unreliable due to its susceptibility to environmental conditions. Normalized Difference Vegetation Index (NDVI) and a composite reflectance of 842 nm, 705 nm, and 740 nm wavelength managed to provide better classification results than thermal in most cases.},
DOI = {10.3390/rs13224606}
}



@Article{rs13224632,
AUTHOR = {Teodoro, Paulo Eduardo and Teodoro, Larissa Pereira Ribeiro and Baio, Fábio Henrique Rojo and da Silva Junior, Carlos Antonio and dos Santos, Regimar Garcia and Ramos, Ana Paula Marques and Pinheiro, Mayara Maezano Faita and Osco, Lucas Prado and Gonçalves, Wesley Nunes and Carneiro, Alexsandro Monteiro and Junior, José Marcato and Pistori, Hemerson and Shiratsuchi, Luciano Shozo},
TITLE = {Predicting Days to Maturity, Plant Height, and Grain Yield in Soybean: A Machine and Deep Learning Approach Using Multispectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {4632},
URL = {https://www.mdpi.com/2072-4292/13/22/4632},
ISSN = {2072-4292},
ABSTRACT = {In soybean, there is a lack of research aiming to compare the performance of machine learning (ML) and deep learning (DL) methods to predict more than one agronomic variable, such as days to maturity (DM), plant height (PH), and grain yield (GY). As these variables are important to developing an overall precision farming model, we propose a machine learning approach to predict DM, PH, and GY for soybean cultivars based on multispectral bands. The field experiment considered 524 genotypes of soybeans in the 2017/2018 and 2018/2019 growing seasons and a multitemporal–multispectral dataset collected by embedded sensor in an unmanned aerial vehicle (UAV). We proposed a multilayer deep learning regression network, trained during 2000 epochs using an adaptive subgradient method, a random Gaussian initialization, and a 50% dropout in the first hidden layer for regularization. Three different scenarios, including only spectral bands, only vegetation indices, and spectral bands plus vegetation indices, were adopted to infer each variable (PH, DM, and GY). The DL model performance was compared against shallow learning methods such as random forest (RF), support vector machine (SVM), and linear regression (LR). The results indicate that our approach has the potential to predict soybean-related variables using multispectral bands only. Both DL and RF models presented a strong (r surpassing 0.77) prediction capacity for the PH variable, regardless of the adopted input variables group. Our results demonstrated that the DL model (r = 0.66) was superior to predict DM when the input variable was the spectral bands. For GY, all machine learning models evaluated presented similar performance (r ranging from 0.42 to 0.44) for each tested scenario. In conclusion, this study demonstrated an efficient approach to a computational solution capable of predicting multiple important soybean crop variables based on remote sensing data. Future research could benefit from the information presented here and be implemented in subsequent processes related to soybean cultivars or other types of agronomic crops.},
DOI = {10.3390/rs13224632}
}



@Article{telecom2040027,
AUTHOR = {Singh, Simran and Kumbhar, Abhaykumar and Güvenç, İsmail and Sichitiu, Mihail L.},
TITLE = {Intelligent Interference Management in UAV-Based HetNets},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {472--488},
URL = {https://www.mdpi.com/2673-4001/2/4/27},
ISSN = {2673-4001},
ABSTRACT = {Unmanned aerial vehicles (UAVs) can play a key role in meeting certain demands of cellular networks. UAVs can be used not only as user equipment (UE) in cellular networks but also as mobile base stations (BSs) wherein they can either augment conventional BSs by adapting their position to serve the changing traffic and connectivity demands or temporarily replace BSs that are damaged due to natural disasters. The flexibility of UAVs allows them to provide coverage to UEs in hot-spots, at cell-edges, in coverage holes, or regions with scarce cellular infrastructure. In this work, we study how UAV locations and other cellular parameters may be optimized in such scenarios to maximize the spectral efficiency (SE) of the network. We compare the performance of machine learning (ML) techniques with conventional optimization approaches. We found that, on an average, a double deep Q learning approach can achieve 93.46% of the optimal median SE and 95.83% of the optimal mean SE. A simple greedy approach, which tunes the parameters of each BS and UAV independently, performed very well in all the cases that we tested. These computationally efficient approaches can be utilized to enhance the network performance in existing cellular networks.},
DOI = {10.3390/telecom2040027}
}



@Article{app112311193,
AUTHOR = {Yang, Yuting and Mei, Gang},
TITLE = {Deep Transfer Learning Approach for Identifying Slope Surface Cracks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {11193},
URL = {https://www.mdpi.com/2076-3417/11/23/11193},
ISSN = {2076-3417},
ABSTRACT = {Geohazards such as landslides, which are often accompanied by surface cracks, have caused great harm to public safety and property. If these surface cracks could be identified in time, this would be of great significance for the monitoring and early warning of geohazards. Currently, the most common method for crack identification is manual detection, which has low efficiency and accuracy. In this paper, a deep transfer learning approach is proposed to effectively and efficiently identify slope surface cracks for the sake of fast monitoring and early warning of geohazards, such as landslides. The essential idea is to employ transfer learning by training (a) a large sample dataset of concrete cracks and (b) a small sample dataset of soil and rock masses&rsquo; cracks. In the proposed approach, (1) pretrained crack identification models are constructed based on a large sample dataset of concrete cracks; (2) refined crack identification models are further constructed based on a small sample dataset of soil and rock masses&rsquo; cracks. The proposed approach could be applied to conduct UAV surveys on high and steep slopes to provide monitoring and early warning of landslides to ensure the safety of people and property.},
DOI = {10.3390/app112311193}
}



@Article{rs13234853,
AUTHOR = {Wei, Dawei and Xi, Ning and Ma, Jianfeng and He, Lei},
TITLE = {UAV-Assisted Privacy-Preserving Online Computation Offloading for Internet of Things},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {4853},
URL = {https://www.mdpi.com/2072-4292/13/23/4853},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) plays a more and more important role in Internet of Things (IoT) for remote sensing and device interconnecting. Due to the limitation of computing capacity and energy, the UAV cannot handle complex tasks. Recently, computation offloading provides a promising way for the UAV to handle complex tasks by deep reinforcement learning (DRL)-based methods. However, existing DRL-based computation offloading methods merely protect usage pattern privacy and location privacy. In this paper, we consider a new privacy issue in UAV-assisted IoT, namely computation offloading preference leakage, which lacks through study. To cope with this issue, we propose a novel privacy-preserving online computation offloading method for UAV-assisted IoT. Our method integrates the differential privacy mechanism into deep reinforcement learning (DRL), which can protect UAV&rsquo;s offloading preference. We provide the formal analysis on security and utility loss of our method. Extensive real-world experiments are conducted. Results demonstrate that, compared with baseline methods, our method can learn cost-efficient computation offloading policy without preference leakage and a priori knowledge of the wireless channel model.},
DOI = {10.3390/rs13234853}
}



@Article{robotics10040131,
AUTHOR = {Takayama, Yusuke and Ratsamee, Photchara and Mashita, Tomohiro},
TITLE = {Reduced Simulation: Real-to-Sim Approach toward Collision Detection in Narrowly Confined Environments},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {131},
URL = {https://www.mdpi.com/2218-6581/10/4/131},
ISSN = {2218-6581},
ABSTRACT = {Recently, several deep-learning based navigation methods have been achieved because of a high quality dataset collected from high-quality simulated environments. However, the cost of creating high-quality simulated environments is high. In this paper, we present a concept of the reduced simulation, which can serve as a simplified version of a simulated environment yet be efficient enough for training deep-learning based UAV collision avoidance approaches. Our approach deals with the reality gap between a reduced simulation dataset and real world dataset and can provide a clear guideline for reduced simulation design. Our experimental result confirmed that the reduction in visual features provided by textures and lighting does not affect operating performance with the user study. Moreover, by conducting collision detection experiments, we verified that our reduced simulation outperforms the conventional cost-effective simulations in adaptation capability with respect to realistic simulation and real-world scenario.},
DOI = {10.3390/robotics10040131}
}



@Article{machines9120360,
AUTHOR = {Yang, Pu and Wen, Chenwan and Geng, Huilin and Liu, Peng},
TITLE = {Intelligent Fault Diagnosis Method for Blade Damage of Quad-Rotor UAV Based on Stacked Pruning Sparse Denoising Autoencoder and Convolutional Neural Network},
JOURNAL = {Machines},
VOLUME = {9},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {360},
URL = {https://www.mdpi.com/2075-1702/9/12/360},
ISSN = {2075-1702},
ABSTRACT = {This paper introduces a new intelligent fault diagnosis method based on stack pruning sparse denoising autoencoder and convolutional neural network (sPSDAE-CNN). This method processes the original input data by using a stack denoising autoencoder. Different from the traditional autoencoder, stack pruning sparse denoising autoencoder includes a fully connected autoencoding network, the features extracted from the front layer of the network are used for the operation of the subsequent layer, which means that some new connections will appear between the front and rear layers of the network, reduce the loss of information, and obtain more effective features. Firstly, a one-dimensional sliding window is introduced for data enhancement. In addition, transforming one-dimensional time-domain data into the two-dimensional gray image can further improve the deep learning (DL) ability of models. At the same time, pruning operation is introduced to improve the training efficiency and accuracy of the network. The convolutional neural network model with sPSDAE has a faster training speed, strong adaptability to noise interference signals, and can also suppress the over-fitting problem of the convolutional neural network to a certain extent. Actual experiments show that for the fault of unmanned aerial vehicle (UAV) blade damage, the sPSDAE-CNN model we use has better stability and reliable prediction accuracy than traditional convolutional neural networks. At the same time, For noise signals, better results can be obtained. The experimental results show that the sPSDAE-CNN model still has a good diagnostic accuracy rate in a high-noise environment. In the case of a signal-to-noise ratio of &minus;4, it still has an accuracy rate of 90%.},
DOI = {10.3390/machines9120360}
}



@Article{s21248455,
AUTHOR = {Queirós Pokee, Diana and Barbosa Pereira, Carina and Mösch, Lucas and Follmann, Andreas and Czaplik, Michael},
TITLE = {Consciousness Detection on Injured Simulated Patients Using Manual and Automatic Classification via Visible and Infrared Imaging},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {8455},
URL = {https://www.mdpi.com/1424-8220/21/24/8455},
PubMedID = {34960551},
ISSN = {1424-8220},
ABSTRACT = {In a disaster scene, triage is a key principle for effectively rescuing injured people according to severity level. One main parameter of the used triage algorithm is the patient&rsquo;s consciousness. Unmanned aerial vehicles (UAV) have been investigated toward (semi-)automatic triage. In addition to vital parameters, such as heart and respiratory rate, UAVs should detect victims&rsquo; mobility and consciousness from the video data. This paper presents an algorithm combining deep learning with image processing techniques to detect human bodies for further (un)consciousness classification. The algorithm was tested in a 20-subject group in an outside environment with static (RGB and thermal) cameras where participants performed different limb movements in different body positions and angles between the cameras and the bodies&rsquo; longitudinal axis. The results verified that the algorithm performed better in RGB. For the most probable case of 0 degrees, RGB data obtained the following results: Mathews correlation coefficient (MMC) of 0.943, F1-score of 0.951, and precision-recall area under curve AUC (PRC) score of 0.968. For the thermal data, the MMC was 0.913, F1-score averaged 0.923, and AUC (PRC) was 0.960. Overall, the algorithm may be promising along with others for a complete contactless triage assessment in disaster events during day and night.},
DOI = {10.3390/s21248455}
}



@Article{drones6010005,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Heravi, Amirhossein and Thaheem, Muhammad Jamaluddin and Maqsoom, Ahsen},
TITLE = {Inspecting Buildings Using Drones and Computer Vision: A Machine Learning Approach to Detect Cracks and Damages},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {5},
URL = {https://www.mdpi.com/2504-446X/6/1/5},
ISSN = {2504-446X},
ABSTRACT = {Manual inspection of infrastructure damages such as building cracks is difficult due to the objectivity and reliability of assessment and high demands of time and costs. This can be automated using unmanned aerial vehicles (UAVs) for aerial imagery of damages. Numerous computer vision-based approaches have been applied to address the limitations of crack detection but they have their limitations that can be overcome by using various hybrid approaches based on artificial intelligence (AI) and machine learning (ML) techniques. The convolutional neural networks (CNNs), an application of the deep learning (DL) method, display remarkable potential for automatically detecting image features such as damages and are less sensitive to image noise. A modified deep hierarchical CNN architecture has been used in this study for crack detection and damage assessment in civil infrastructures. The proposed architecture is based on 16 convolution layers and a cycle generative adversarial network (CycleGAN). For this study, the crack images were collected using UAVs and open-source images of mid to high rise buildings (five stories and above) constructed during 2000 in Sydney, Australia. Conventionally, a CNN network only utilizes the last layer of convolution. However, our proposed network is based on the utility of multiple layers. Another important component of the proposed CNN architecture is the application of guided filtering (GF) and conditional random fields (CRFs) to refine the predicted outputs to get reliable results. Benchmarking data (600 images) of Sydney-based buildings damages was used to test the proposed architecture. The proposed deep hierarchical CNN architecture produced superior performance when evaluated using five methods: GF method, Baseline (BN) method, Deep-Crack BN, Deep-Crack GF, and SegNet. Overall, the GF method outperformed all other methods as indicated by the global accuracy (0.990), class average accuracy (0.939), mean intersection of the union overall classes (IoU) (0.879), precision (0.838), recall (0.879), and F-score (0.8581) values. Overall, the proposed CNN architecture provides the advantages of reduced noise, highly integrated supervision of features, adequate learning, and aggregation of both multi-scale and multilevel features during the training procedure along with the refinement of the overall output predictions.},
DOI = {10.3390/drones6010005}
}



@Article{s22010270,
AUTHOR = {Domingo, Mari Carmen},
TITLE = {Power Allocation and Energy Cooperation for UAV-Enabled MmWave Networks: A Multi-Agent Deep Reinforcement Learning Approach},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {270},
URL = {https://www.mdpi.com/1424-8220/22/1/270},
PubMedID = {35009812},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicle (UAV)-assisted cellular networks over the millimeter-wave (mmWave) frequency band can meet the requirements of a high data rate and flexible coverage in next-generation communication networks. However, higher propagation loss and the use of a large number of antennas in mmWave networks give rise to high energy consumption and UAVs are constrained by their low-capacity onboard battery. Energy harvesting (EH) is a viable solution to reduce the energy cost of UAV-enabled mmWave networks. However, the random nature of renewable energy makes it challenging to maintain robust connectivity in UAV-assisted terrestrial cellular networks. Energy cooperation allows UAVs to send their excessive energy to other UAVs with reduced energy. In this paper, we propose a power allocation algorithm based on energy harvesting and energy cooperation to maximize the throughput of a UAV-assisted mmWave cellular network. Since there is channel-state uncertainty and the amount of harvested energy can be treated as a stochastic process, we propose an optimal multi-agent deep reinforcement learning algorithm (DRL) named Multi-Agent Deep Deterministic Policy Gradient (MADDPG) to solve the renewable energy resource allocation problem for throughput maximization. The simulation results show that the proposed algorithm outperforms the Random Power (RP), Maximal Power (MP) and value-based Deep Q-Learning (DQL) algorithms in terms of network throughput.},
DOI = {10.3390/s22010270}
}



@Article{app12010395,
AUTHOR = {Wang, Ying and Koo, Ki-Young},
TITLE = {Vegetation Removal on 3D Point Cloud Reconstruction of Cut-Slopes Using U-Net},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {395},
URL = {https://www.mdpi.com/2076-3417/12/1/395},
ISSN = {2076-3417},
ABSTRACT = {The 3D point cloud reconstruction from photos taken by an unmanned aerial vehicle (UAV) is a promising tool for monitoring and managing risks of cut-slopes. However, surface changes on cut-slopes are likely to be hidden by seasonal vegetation variations on the cut-slopes. This paper proposes a vegetation removal method for 3D reconstructed point clouds using (1) a 2D image segmentation deep learning model and (2) projection matrices available from photogrammetry. For a given point cloud, each 3D point of it is reprojected into the image coordinates by the projection matrices to determine if it belongs to vegetation or not using the 2D image segmentation model. The 3D points belonging to vegetation in the 2D images are deleted from the point cloud. The effort to build a 2D image segmentation model was significantly reduced by using U-Net with the dataset prepared by the colour index method complemented by manual trimming. The proposed method was applied to a cut-slope in Doam Dam in South Korea, and showed that vegetation from the two point clouds of the cut-slope at winter and summer was removed successfully. The M3C2 distance between the two vegetation-removed point clouds showed a feasibility of the proposed method as a tool to reveal actual change of cut-slopes without the effect of vegetation.},
DOI = {10.3390/app12010395}
}



@Article{rs14020265,
AUTHOR = {Wang, Yanjun and Li, Shaochun and Teng, Fei and Lin, Yunhao and Wang, Mengjie and Cai, Hengfan},
TITLE = {Improved Mask R-CNN for Rural Building Roof Type Recognition from UAV High-Resolution Images: A Case Study in Hunan Province, China},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {265},
URL = {https://www.mdpi.com/2072-4292/14/2/265},
ISSN = {2072-4292},
ABSTRACT = {Accurate roof information of buildings can be obtained from UAV high-resolution images. The large-scale accurate recognition of roof types (such as gabled, flat, hipped, complex and mono-pitched roofs) of rural buildings is crucial for rural planning and construction. At present, most UAV high-resolution optical images only have red, green and blue (RGB) band information, which aggravates the problems of inter-class similarity and intra-class variability of image features. Furthermore, the different roof types of rural buildings are complex, spatially scattered, and easily covered by vegetation, which in turn leads to the low accuracy of roof type identification by existing methods. In response to the above problems, this paper proposes a method for identifying roof types of complex rural buildings based on visible high-resolution remote sensing images from UAVs. First, the fusion of deep learning networks with different visual features is investigated to analyze the effect of the different feature combinations of the visible difference vegetation index (VDVI) and Sobel edge detection features and UAV visible images on model recognition of rural building roof types. Secondly, an improved Mask R-CNN model is proposed to learn more complex features of different types of images of building roofs by using the ResNet152 feature extraction network with migration learning. After we obtained roof type recognition results in two test areas, we evaluated the accuracy of the results using the confusion matrix and obtained the following conclusions: (1) the model with RGB images incorporating Sobel edge detection features has the highest accuracy and enables the model to recognize more and more accurately the roof types of different morphological rural buildings, and the model recognition accuracy (Kappa coefficient (KC)) compared to that of RGB images is on average improved by 0.115; (2) compared with the original Mask R-CNN, U-Net, DeeplabV3 and PSPNet deep learning models, the improved Mask R-CNN model has the highest accuracy in recognizing the roof types of rural buildings, with F1-score, KC and OA averaging 0.777, 0.821 and 0.905, respectively. The method can obtain clear and accurate profiles and types of rural building roofs, and can be extended for green roof suitability evaluation, rooftop solar potential assessment, and other building roof surveys, management and planning.},
DOI = {10.3390/rs14020265}
}



@Article{pr10010131,
AUTHOR = {Luo, Wei and Han, Wenlong and Fu, Ping and Wang, Huijuan and Zhao, Yunfeng and Liu, Ke and Liu, Yuyan and Zhao, Zihui and Zhu, Mengxu and Xu, Ruopeng and Wei, Guosheng},
TITLE = {A Water Surface Contaminants Monitoring Method Based on Airborne Depth Reasoning},
JOURNAL = {Processes},
VOLUME = {10},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {131},
URL = {https://www.mdpi.com/2227-9717/10/1/131},
ISSN = {2227-9717},
ABSTRACT = {Water surface plastic pollution turns out to be a global issue, having aroused rising attention worldwide. How to monitor water surface plastic waste in real time and accurately collect and analyze the relevant numerical data has become a hotspot in water environment research. (1) Background: Over the past few years, unmanned aerial vehicles (UAVs) have been progressively adopted to conduct studies on the monitoring of water surface plastic waste. On the whole, the monitored data are stored in the UAVS to be subsequently retrieved and analyzed, thereby probably causing the loss of real-time information and hindering the whole monitoring process from being fully automated. (2) Methods: An investigation was conducted on the relationship, function and relevant mechanism between various types of plastic waste in the water surface system. On that basis, this study built a deep learning-based lightweight water surface plastic waste detection model, which was capable of automatically detecting and locating different water surface plastic waste. Moreover, a UAV platform-based edge computing architecture was built. (3) Results: The delay of return task data and UAV energy consumption were effectively reduced, and computing and network resources were optimally allocated. (4) Conclusions: The UAV platform based on airborne depth reasoning is expected to be the mainstream means of water environment monitoring in the future.},
DOI = {10.3390/pr10010131}
}



@Article{rs14020295,
AUTHOR = {Yu, Kunyong and Hao, Zhenbang and Post, Christopher J. and Mikhailova, Elena A. and Lin, Lili and Zhao, Gejin and Tian, Shangfeng and Liu, Jian},
TITLE = {Comparison of Classical Methods and Mask R-CNN for Automatic Tree Detection and Mapping Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {295},
URL = {https://www.mdpi.com/2072-4292/14/2/295},
ISSN = {2072-4292},
ABSTRACT = {Detecting and mapping individual trees accurately and automatically from remote sensing images is of great significance for precision forest management. Many algorithms, including classical methods and deep learning techniques, have been developed and applied for tree crown detection from remote sensing images. However, few studies have evaluated the accuracy of different individual tree detection (ITD) algorithms and their data and processing requirements. This study explored the accuracy of ITD using local maxima (LM) algorithm, marker-controlled watershed segmentation (MCWS), and Mask Region-based Convolutional Neural Networks (Mask R-CNN) in a young plantation forest with different test images. Manually delineated tree crowns from UAV imagery were used for accuracy assessment of the three methods, followed by an evaluation of the data processing and application requirements for three methods to detect individual trees. Overall, Mask R-CNN can best use the information in multi-band input images for detecting individual trees. The results showed that the Mask R-CNN model with the multi-band combination produced higher accuracy than the model with a single-band image, and the RGB band combination achieved the highest accuracy for ITD (F1 score = 94.68%). Moreover, the Mask R-CNN models with multi-band images are capable of providing higher accuracies for ITD than the LM and MCWS algorithms. The LM algorithm and MCWS algorithm also achieved promising accuracies for ITD when the canopy height model (CHM) was used as the test image (F1 score = 87.86% for LM algorithm, F1 score = 85.92% for MCWS algorithm). The LM and MCWS algorithms are easy to use and lower computer computational requirements, but they are unable to identify tree species and are limited by algorithm parameters, which need to be adjusted for each classification. It is highlighted that the application of deep learning with its end-to-end-learning approach is very efficient and capable of deriving the information from multi-layer images, but an additional training set is needed for model training, robust computer resources are required, and a large number of accurate training samples are necessary. This study provides valuable information for forestry practitioners to select an optimal approach for detecting individual trees.},
DOI = {10.3390/rs14020295}
}



@Article{drones6010019,
AUTHOR = {Kundid Vasić, Mirela and Papić, Vladan},
TITLE = {Improving the Model for Person Detection in Aerial Image Sequences Using the Displacement Vector: A Search and Rescue Scenario},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {19},
URL = {https://www.mdpi.com/2504-446X/6/1/19},
ISSN = {2504-446X},
ABSTRACT = {Recent results in person detection using deep learning methods applied to aerial images gathered by Unmanned Aerial Vehicles (UAVs) have demonstrated the applicability of this approach in scenarios such as Search and Rescue (SAR) operations. In this paper, the continuation of our previous research is presented. The main goal is to further improve detection results, especially in terms of reducing the number of false positive detections and consequently increasing the precision value. We present a new approach that, as input to the multimodel neural network architecture, uses sequences of consecutive images instead of only one static image. Since successive images overlap, the same object of interest needs to be detected in more than one image. The correlation between successive images was calculated, and detected regions in one image were translated to other images based on the displacement vector. The assumption is that an object detected in more than one image has a higher probability of being a true positive detection because it is unlikely that the detection model will find the same false positive detections in multiple images. Based on this information, three different algorithms for rejecting detections and adding detections from one image to other images in the sequence are proposed. All of them achieved precision value about 80% which is increased by almost 20% compared to the current state-of-the-art methods.},
DOI = {10.3390/drones6010019}
}



@Article{rs14020382,
AUTHOR = {Jing, Yafei and Ren, Yuhuan and Liu, Yalan and Wang, Dacheng and Yu, Linjun},
TITLE = {Automatic Extraction of Damaged Houses by Earthquake Based on Improved YOLOv5: A Case Study in Yangbi},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {382},
URL = {https://www.mdpi.com/2072-4292/14/2/382},
ISSN = {2072-4292},
ABSTRACT = {Efficiently and automatically acquiring information on earthquake damage through remote sensing has posed great challenges because the classical methods of detecting houses damaged by destructive earthquakes are often both time consuming and low in accuracy. A series of deep-learning-based techniques have been developed and recent studies have demonstrated their high intelligence for automatic target extraction for natural and remote sensing images. For the detection of small artificial targets, current studies show that You Only Look Once (YOLO) has a good performance in aerial and Unmanned Aerial Vehicle (UAV) images. However, less work has been conducted on the extraction of damaged houses. In this study, we propose a YOLOv5s-ViT-BiFPN-based neural network for the detection of rural houses. Specifically, to enhance the feature information of damaged houses from the global information of the feature map, we introduce the Vision Transformer into the feature extraction network. Furthermore, regarding the scale differences for damaged houses in UAV images due to the changes in flying height, we apply the Bi-Directional Feature Pyramid Network (BiFPN) for multi-scale feature fusion to aggregate features with different resolutions and test the model. We took the 2021 Yangbi earthquake with a surface wave magnitude (Ms) of 6.4 in Yunan, China, as an example; the results show that the proposed model presents a better performance, with the average precision (AP) being increased by 9.31% and 1.23% compared to YOLOv3 and YOLOv5s, respectively, and a detection speed of 80 FPS, which is 2.96 times faster than YOLOv3. In addition, the transferability test for five other areas showed that the average accuracy was 91.23% and the total processing time was 4 min, while 100 min were needed for professional visual interpreters. The experimental results demonstrate that the YOLOv5s-ViT-BiFPN model can automatically detect damaged rural houses due to destructive earthquakes in UAV images with a good performance in terms of accuracy and timeliness, as well as being robust and transferable.},
DOI = {10.3390/rs14020382}
}



@Article{rs14020396,
AUTHOR = {Shi, Yue and Han, Liangxiu and Kleerekoper, Anthony and Chang, Sheng and Hu, Tongle},
TITLE = {Novel CropdocNet Model for Automated Potato Late Blight Disease Detection from Unmanned Aerial Vehicle-Based Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/2072-4292/14/2/396},
ISSN = {2072-4292},
ABSTRACT = {The accurate and automated diagnosis of potato late blight disease, one of the most destructive potato diseases, is critical for precision agricultural control and management. Recent advances in remote sensing and deep learning offer the opportunity to address this challenge. This study proposes a novel end-to-end deep learning model (CropdocNet) for accurate and automated late blight disease diagnosis from UAV-based hyperspectral imagery. The proposed method considers the potential disease-specific reflectance radiation variance caused by the canopy&rsquo;s structural diversity and introduces multiple capsule layers to model the part-to-whole relationship between spectral&ndash;spatial features and the target classes to represent the rotation invariance of the target classes in the feature space. We evaluate the proposed method with real UAV-based HSI data under controlled and natural field conditions. The effectiveness of the hierarchical features is quantitatively assessed and compared with the existing representative machine learning/deep learning methods on both testing and independent datasets. The experimental results show that the proposed model significantly improves accuracy when considering the hierarchical structure of spectral&ndash;spatial features, with average accuracies of 98.09% for the testing dataset and 95.75% for the independent dataset, respectively.},
DOI = {10.3390/rs14020396}
}



@Article{s22030891,
AUTHOR = {Yuan, Songhe and Ota, Kaoru and Dong, Mianxiong and Zhao, Jianghai},
TITLE = {A Path Planning Method with Perception Optimization Based on Sky Scanning for UAVs},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {891},
URL = {https://www.mdpi.com/1424-8220/22/3/891},
PubMedID = {35161639},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are frequently adopted in disaster management. The vision they provide is extremely valuable for rescuers. However, they face severe problems in their stability in actual disaster scenarios, as the images captured by the on-board sensors cannot consistently give enough information for deep learning models to make accurate decisions. In many cases, UAVs have to capture multiple images from different views to output final recognition results. In this paper, we desire to formulate the fly path task for UAVs, considering the actual perception needs. A convolutional neural networks (CNNs) model is proposed to detect and localize the objects, such as the buildings, as well as an optimization method to find the optimal flying path to accurately recognize as many objects as possible with a minimum time cost. The simulation results demonstrate that the proposed method is effective and efficient, and can address the actual scene understanding and path planning problems for UAVs in the real world well.},
DOI = {10.3390/s22030891}
}



@Article{rs14030592,
AUTHOR = {Reedha, Reenul and Dericquebourg, Eric and Canals, Raphael and Hafiane, Adel},
TITLE = {Transformer Neural Network for Weed and Crop Classification of High Resolution UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {592},
URL = {https://www.mdpi.com/2072-4292/14/3/592},
ISSN = {2072-4292},
ABSTRACT = {Monitoring crops and weeds is a major challenge in agriculture and food production today. Weeds compete directly with crops for moisture, nutrients, and sunlight. They therefore have a significant negative impact on crop yield if not sufficiently controlled. Weed detection and mapping is an essential step in weed control. Many existing research studies recognize the importance of remote sensing systems and machine learning algorithms in weed management. Deep learning approaches have shown good performance in many agriculture-related remote sensing tasks, such as plant classification, disease detection, etc. However, despite the success of these approaches, they still face many challenges such as high computation cost, the need of large labelled datasets, intra-class discrimination (in growing phase weeds and crops share many attributes similarity as color, texture, and shape), etc. This paper aims to show that the attention-based deep network is a promising approach to address the forementioned problems, in the context of weeds and crops recognition with drone system. The specific objective of this study was to investigate visual transformers (ViT) and apply them to plant classification in Unmanned Aerial Vehicles (UAV) images. Data were collected using a high-resolution camera mounted on a UAV, which was deployed in beet, parsley and spinach fields. The acquired data were augmented to build larger dataset, since ViT requires large sample sets for better performance, we also adopted the transfer learning strategy. Experiments were set out to assess the effect of training and validation dataset size, as well as the effect of increasing the test set while reducing the training set. The results show that with a small labeled training dataset, the ViT models outperform state-of-the-art models such as EfficientNet and ResNet. The results of this study are promising and show the potential of ViT to be applied to a wide range of remote sensing image analysis tasks.},
DOI = {10.3390/rs14030592}
}



@Article{buildings12020156,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Shahzad, Danish and Heravi, Amirhossein and Qayyum, Siddra and Akram, Junaid},
TITLE = {Civil Infrastructure Damage and Corrosion Detection: An Application of Machine Learning},
JOURNAL = {Buildings},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {156},
URL = {https://www.mdpi.com/2075-5309/12/2/156},
ISSN = {2075-5309},
ABSTRACT = {Automatic detection of corrosion and associated damages to civil infrastructures such as bridges, buildings, and roads, from aerial images captured by an Unmanned Aerial Vehicle (UAV), helps one to overcome the challenges and shortcomings (objectivity and reliability) associated with the manual inspection methods. Deep learning methods have been widely reported in the literature for civil infrastructure corrosion detection. Among them, convolutional neural networks (CNNs) display promising applicability for the automatic detection of image features less affected by image noises. Therefore, in the current study, we propose a modified version of deep hierarchical CNN architecture, based on 16 convolution layers and cycle generative adversarial network (CycleGAN), to predict pixel-wise segmentation in an end-to-end manner using the images of Bolte Bridge and sky rail areas in Victoria (Melbourne). The convolutedly designed model network proposed in the study is based on learning and aggregation of multi-scale and multilevel features while moving from the low convolutional layers to the high-level layers, thus reducing the consistency loss in images due to the inclusion of CycleGAN. The standard approaches only use the last convolutional layer, but our proposed architecture differs from these approaches and uses multiple layers. Moreover, we have used guided filtering and Conditional Random Fields (CRFs) methods to refine the prediction results. Additionally, the effectiveness of the proposed architecture was assessed using benchmarking data of 600 images of civil infrastructure. Overall, the results show that the deep hierarchical CNN architecture based on 16 convolution layers produced advanced performances when evaluated for different methods, including the baseline, PSPNet, DeepLab, and SegNet. Overall, the extended method displayed the Global Accuracy (GA); Class Average Accuracy (CAC); mean Intersection Of the Union (IOU); Precision (P); Recall (R); and F-score values of 0.989, 0.931, 0.878, 0.849, 0.818 and 0.833, respectively.},
DOI = {10.3390/buildings12020156}
}



@Article{s22031200,
AUTHOR = {Jang, Younghoon and Raza, Syed M. and Kim, Moonseong and Choo, Hyunseung},
TITLE = {Proactive Handover Decision for UAVs with Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {1200},
URL = {https://www.mdpi.com/1424-8220/22/3/1200},
PubMedID = {35161945},
ISSN = {1424-8220},
ABSTRACT = {The applications of Unmanned Aerial Vehicles (UAVs) are rapidly growing in domains such as surveillance, logistics, and entertainment and require continuous connectivity with cellular networks to ensure their seamless operations. However, handover policies in current cellular networks are primarily designed for ground users, and thus are not appropriate for UAVs due to frequent fluctuations of signal strength in the air. This paper presents a novel handover decision scheme deploying Deep Reinforcement Learning (DRL) to prevent unnecessary handovers while maintaining stable connectivity. The proposed DRL framework takes the UAV state as an input for a proximal policy optimization algorithm and develops a Received Signal Strength Indicator (RSSI) based on a reward function for the online learning of UAV handover decisions. The proposed scheme is evaluated in a 3D-emulated UAV mobility environment where it reduces up to 76 and 73% of unnecessary handovers compared to greedy and Q-learning-based UAV handover decision schemes, respectively. Furthermore, this scheme ensures reliable communication with the UAV by maintaining the RSSI above &minus;75 dBm more than 80% of the time.},
DOI = {10.3390/s22031200}
}



@Article{agriculture12020248,
AUTHOR = {Du, Lei and Sun, Yaqin and Chen, Shuo and Feng, Jiedong and Zhao, Yindi and Yan, Zhigang and Zhang, Xuewei and Bian, Yuchen},
TITLE = {A Novel Object Detection Model Based on Faster R-CNN for Spodoptera frugiperda According to Feeding Trace of Corn Leaves},
JOURNAL = {Agriculture},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {248},
URL = {https://www.mdpi.com/2077-0472/12/2/248},
ISSN = {2077-0472},
ABSTRACT = {The conventional method for crop insect detection based on visual judgment of the field is time-consuming, laborious, subjective, and error prone. The early detection and accurate localization of agricultural insect pests can significantly improve the effectiveness of pest control as well as reduce the costs, which has become an urgent demand for crop production. Maize Spodoptera frugiperda is a migratory agricultural pest that has severely decreased the yield of maize, rice, and other kinds of crops worldwide. To monitor the occurrences of maize Spodoptera frugiperda in a timely manner, an end-to-end Spodoptera frugiperda detection model termed the Pest Region-CNN (Pest R-CNN) was proposed based on the Faster Region-CNN (Faster R-CNN) model. Pest R-CNN was carried out according to the feeding traces of maize leaves by Spodoptera frugiperda. The proposed model was trained and validated using high-spatial-resolution red&ndash;green&ndash;blue (RGB) ortho-images acquired by an unmanned aerial vehicle (UAV). On the basis of the severity of feeding, the degree of Spodoptera frugiperda invasion severity was classified into the four classes of juvenile, minor, moderate, and severe. The degree of severity and specific feed location of S. frugiperda infestation can be determined and depicted in the frame forms using the proposed model. A mean average precision (mAP) of 43.6% was achieved by the proposed model on the test dataset, showing the great potential of deep learning object detection in pest monitoring. Compared with the Faster R-CNN and YOLOv5 model, the detection accuracy of the proposed model increased by 12% and 19%, respectively. Further ablation studies showed the effectives of channel and spatial attention, group convolution, deformable convolution, and the multi-scale aggregation strategy in the aspect of improving the accuracy of detection. The design methods of the object detection architecture could provide reference for other research. This is the first step in applying deep-learning object detection to S. frugiperda feeding trace, enabling the application of high-spatial-resolution RGB images obtained by UAVs to S. frugiperda-infested object detection. The proposed model will be beneficial with respect to S. frugiperda pest stress monitoring to realize precision pest control.},
DOI = {10.3390/agriculture12020248}
}



@Article{rs14040838,
AUTHOR = {Xu, Chuan and Liu, Chang and Li, Hongli and Ye, Zhiwei and Sui, Haigang and Yang, Wei},
TITLE = {Multiview Image Matching of Optical Satellite and UAV Based on a Joint Description Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {838},
URL = {https://www.mdpi.com/2072-4292/14/4/838},
ISSN = {2072-4292},
ABSTRACT = {Matching aerial and satellite optical images with large dip angles is a core technology and is essential for target positioning and dynamic monitoring in sensitive areas. However, due to the long distances and large dip angle observations of the aerial platform, there are significant perspective, radiation, and scale differences between heterologous space-sky images, which seriously affect the accuracy and robustness of feature matching. In this paper, a multiview satellite and unmanned aerial vehicle (UAV) image matching method based on deep learning is proposed to solve this problem. The main innovation of this approach is to propose a joint descriptor consisting of soft descriptions and hard descriptions. Hard descriptions are used as the main description to ensure matching accuracy. Soft descriptions are used not only as auxiliary descriptions but also for the process of network training. Experiments on several problems show that the proposed method ensures matching efficiency and achieves better matching accuracy for multiview satellite and UAV images than other traditional methods. In addition, the matching accuracy of our method in optical satellite and UAV images is within 3 pixels, and can nearly reach 2 pixels, which meets the requirements of relevant UAV missions.},
DOI = {10.3390/rs14040838}
}



@Article{app12041953,
AUTHOR = {Sultonov, Furkat and Park, Jun-Hyun and Yun, Sangseok and Lim, Dong-Woo and Kang, Jae-Mo},
TITLE = {Mixer U-Net: An Improved Automatic Road Extraction from UAV Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {1953},
URL = {https://www.mdpi.com/2076-3417/12/4/1953},
ISSN = {2076-3417},
ABSTRACT = {Automatic road extraction from unmanned aerial vehicle (UAV) imagery has been one of the major research topics in the area of remote sensing analysis due to its importance in a wide range of applications such as urban planning, road monitoring, intelligent transportation systems, and automatic road navigation. Thanks to the recent advances in Deep Learning (DL), the tedious manual segmentation of roads can be automated. However, the majority of these models are computationally heavy and, thus, are not suitable for UAV remote-sensing tasks with limited resources. To alleviate this bottleneck, we propose two lightweight models based on depthwise separable convolutions and ConvMixer inception block. Both models take the advantage of computational efficiency of depthwise separable convolutions and multi-scale processing of inception module and combine them in an encoder&ndash;decoder architecture of U-Net. Specifically, we substitute standard convolution layers used in U-Net for ConvMixer layers. Furthermore, in order to learn images on different scales, we apply ConvMixer layer into Inception module. Finally, we incorporate pathway networks along the skip connections to minimize the semantic gap between encoder and decoder. In order to validate the performance and effectiveness of the models, we adopt Massachusetts roads dataset. One incarnation of our models is able to beat the U-Net&rsquo;s performance with 10&times; fewer parameters, and DeepLabV3&rsquo;s performance with 12&times; fewer parameters in terms of mean intersection over union (mIoU) metric. For further validation, we have compared our models against four baselines in total and used additional metrics such as precision (P), recall (R), and F1 score.},
DOI = {10.3390/app12041953}
}



@Article{app12052336,
AUTHOR = {Yu, Qianqian and Fan, Keqi and Wang, Yiyang and Zheng, Yuhui},
TITLE = {Faster MDNet for Visual Object Tracking},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {2336},
URL = {https://www.mdpi.com/2076-3417/12/5/2336},
ISSN = {2076-3417},
ABSTRACT = {With the rapid development of deep learning techniques, new breakthroughs have been made in deep learning-based object tracking methods. Although many approaches have achieved state-of-the-art results, existing methods still cannot fully satisfy practical needs. A robust tracker should perform well in three aspects: tracking accuracy, speed, and resource consumption. Considering this notion, we propose a novel model, Faster MDNet, to strike a better balance among these factors. To improve the tracking accuracy, a channel attention module is introduced to our method. We also design domain adaptation components to obtain more generic features. Simultaneously, we implement an adaptive, spatial pyramid pooling layer for reducing model complexity and accelerating the tracking speed. The experiments illustrate the promising performance of our tracker on OTB100, VOT2018, TrackingNet, UAV123, and NfS.},
DOI = {10.3390/app12052336}
}



@Article{rs14051234,
AUTHOR = {Li, Xianjiang and He, Boyong and Ding, Kaiwen and Guo, Weijie and Huang, Bo and Wu, Liaoni},
TITLE = {Wide-Area and Real-Time Object Search System of UAV},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1234},
URL = {https://www.mdpi.com/2072-4292/14/5/1234},
ISSN = {2072-4292},
ABSTRACT = {The method of collecting aerial images or videos by unmanned aerial vehicles (UAVs) for object search has the advantages of high flexibility and low cost, and has been widely used in various fields, such as pipeline inspection, disaster rescue, and forest fire prevention. However, in the case of object search in a wide area, the scanning efficiency and real-time performance of UAV are often difficult to satisfy at the same time, which may lead to missing the best time to perform the task. In this paper, we design a wide-area and real-time object search system of UAV based on deep learning for this problem. The system first solves the problem of area scanning efficiency by controlling the high-resolution camera in order to collect aerial images with a large field of view. For real-time requirements, we adopted three strategies to accelerate the system, as follows: design a parallel system, simplify the object detection algorithm, and use TensorRT on the edge device to optimize the object detection model. We selected the NVIDIA Jetson AGX Xavier edge device as the central processor and verified the feasibility and practicability of the system through the actual application of suspicious vehicle search in the grazing area of the prairie. Experiments have proved that the parallel design of the system can effectively meet the real-time requirements. For the most time-consuming image object detection link, with a slight loss of precision, most algorithms can reach the 400% inference speed of the benchmark in total, after algorithm simplification, and corresponding model&rsquo;s deployment by TensorRT.},
DOI = {10.3390/rs14051234}
}



@Article{rs14061400,
AUTHOR = {Zhang, Xin and Han, Liangxiu and Sobeih, Tam and Lappin, Lewis and Lee, Mark A. and Howard, Andew and Kisdi, Aron},
TITLE = {The Self-Supervised Spectral&ndash;Spatial Vision Transformer Network for Accurate Prediction of Wheat Nitrogen Status from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1400},
URL = {https://www.mdpi.com/2072-4292/14/6/1400},
ISSN = {2072-4292},
ABSTRACT = {Nitrogen (N) fertilizer is routinely applied by farmers to increase crop yields. At present, farmers often over-apply N fertilizer in some locations or at certain times because they do not have high-resolution crop N status data. N-use efficiency can be low, with the remaining N lost to the environment, resulting in higher production costs and environmental pollution. Accurate and timely estimation of N status in crops is crucial to improving cropping systems&rsquo; economic and environmental sustainability. Destructive approaches based on plant tissue analysis are time consuming and impractical over large fields. Recent advances in remote sensing and deep learning have shown promise in addressing the aforementioned challenges in a non-destructive way. In this work, we propose a novel deep learning framework: a self-supervised spectral&ndash;spatial attention-based vision transformer (SSVT). The proposed SSVT introduces a Spectral Attention Block (SAB) and a Spatial Interaction Block (SIB), which allows for simultaneous learning of both spatial and spectral features from UAV digital aerial imagery, for accurate N status prediction in wheat fields. Moreover, the proposed framework introduces local-to-global self-supervised learning to help train the model from unlabelled data. The proposed SSVT has been compared with five state-of-the-art models including: ResNet, RegNet, EfficientNet, EfficientNetV2, and the original vision transformer on both testing and independent datasets. The proposed approach achieved high accuracy (0.96) with good generalizability and reproducibility for wheat N status estimation.},
DOI = {10.3390/rs14061400}
}



@Article{rs14061523,
AUTHOR = {Ye, Zhangxi and Wei, Jiahao and Lin, Yuwei and Guo, Qian and Zhang, Jian and Zhang, Houxi and Deng, Hui and Yang, Kaijie},
TITLE = {Extraction of Olive Crown Based on UAV Visible Images and the U2-Net Deep Learning Model},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {1523},
URL = {https://www.mdpi.com/2072-4292/14/6/1523},
ISSN = {2072-4292},
ABSTRACT = {Olive trees, which are planted widely in China, are economically significant. Timely and accurate acquisition of olive tree crown information is vital in monitoring olive tree growth and accurately predicting its fruit yield. The advent of unmanned aerial vehicles (UAVs) and deep learning (DL) provides an opportunity for rapid monitoring parameters of the olive tree crown. In this study, we propose a method of automatically extracting olive crown information (crown number and area of olive tree), combining visible-light images captured by consumer UAV and a new deep learning model, U2-Net, with a deeply nested structure. Firstly, a data set of an olive tree crown (OTC) images was constructed, which was further processed by the ESRGAN model to enhance the image resolution and was augmented (geometric transformation and spectral transformation) to enlarge the data set to increase the generalization ability of the model. Secondly, four typical subareas (A&ndash;D) in the study area were selected to evaluate the performance of the U2-Net model in olive crown extraction in different scenarios, and the U2-Net model was compared with three current mainstream deep learning models (i.e., HRNet, U-Net, and DeepLabv3+) in remote sensing image segmentation effect. The results showed that the U2-Net model achieved high accuracy in the extraction of tree crown numbers in the four subareas with a mean of intersection over union (IoU), overall accuracy (OA), and F1-Score of 92.27%, 95.19%, and 95.95%, respectively. Compared with the other three models, the IoU, OA, and F1-Score of the U2-Net model increased by 14.03&ndash;23.97 percentage points, 7.57&ndash;12.85 percentage points, and 8.15&ndash;14.78 percentage points, respectively. In addition, the U2-Net model had a high consistency between the predicted and measured area of the olive crown, and compared with the other three deep learning models, it had a lower error rate with a root mean squared error (RMSE) of 4.78, magnitude of relative error (MRE) of 14.27%, and a coefficient of determination (R2) higher than 0.93 in all four subareas, suggesting that the U2-Net model extracted the best crown profile integrity and was most consistent with the actual situation. This study indicates that the method combining UVA RGB images with the U2-Net model can provide a highly accurate and robust extraction result for olive tree crowns and is helpful in the dynamic monitoring and management of orchard trees.},
DOI = {10.3390/rs14061523}
}



@Article{rs14071561,
AUTHOR = {Hao, Zhenbang and Post, Christopher J. and Mikhailova, Elena A. and Lin, Lili and Liu, Jian and Yu, Kunyong},
TITLE = {How Does Sample Labeling and Distribution Affect the Accuracy and Efficiency of a Deep Learning Model for Individual Tree-Crown Detection and Delineation},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {1561},
URL = {https://www.mdpi.com/2072-4292/14/7/1561},
ISSN = {2072-4292},
ABSTRACT = {Monitoring and assessing vegetation using deep learning approaches has shown promise in forestry applications. Sample labeling to represent forest complexity is the main limitation for deep learning approaches for remote sensing vegetation classification applications, and few studies have focused on the impact of sample labeling methods on model performance and model training efficiency. This study is the first-of-its-kind that uses Mask region-based convolutional neural networks (Mask R-CNN) to evaluate the influence of sample labeling methods (including sample size and sample distribution) on individual tree-crown detection and delineation. A flight was conducted over a plantation with Fokienia hodginsii as the main tree species using a Phantom4-Multispectral (P4M) to obtain UAV imagery, and a total of 2061 manually and accurately delineated tree crowns were used for training and validating (1689) and testing (372). First, the model performance of three pre-trained backbones (ResNet-34, ResNet-50, and ResNet-101) was evaluated. Second, random deleting and clumped deleting methods were used to repeatedly delete 10% from the original sample set to reduce the training and validation set, to simulate two different sample distributions (the random sample set and the clumped sample set). Both RGB image and Multi-band images derived from UAV flights were used to evaluate model performance. Each model&rsquo;s average per-epoch training time was calculated to evaluate the model training efficiency. The results showed that ResNet-50 yielded a more robust network than ResNet-34 and ResNet-101 when the same parameters were used for Mask R-CNN. The sample size determined the influence of sample labeling methods on the model performance. Random sample labeling had lower requirements for sample size compared to clumped sample labeling, and unlabeled trees in random sample labeling had no impact on model training. Additionally, the model with clumped samples provides a shorter average per-epoch training time than the model with random samples. This study demonstrates that random sample labeling can greatly reduce the requirement of sample size, and it is not necessary to accurately label each sample in the image during the sample labeling process.},
DOI = {10.3390/rs14071561}
}



@Article{ijgi11040222,
AUTHOR = {Ren, Simiao and Malof, Jordan and Fetter, Rob and Beach, Robert and Rineer, Jay and Bradbury, Kyle},
TITLE = {Utilizing Geospatial Data for Assessing Energy Security: Mapping Small Solar Home Systems Using Unmanned Aerial Vehicles and Deep Learning},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {11},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {222},
URL = {https://www.mdpi.com/2220-9964/11/4/222},
ISSN = {2220-9964},
ABSTRACT = {Solar home systems (SHS), a cost-effective solution for rural communities far from the grid in developing countries, are small solar panels and associated equipment that provides power to a single household. A crucial resource for targeting further investment of public and private resources, as well as tracking the progress of universal electrification goals, is shared access to high-quality data on individual SHS installations including information such as location and power capacity. Though recent studies utilizing satellite imagery and machine learning to detect solar panels have emerged, they struggle to accurately locate many SHS due to limited image resolution (some small solar panels only occupy several pixels in satellite imagery). In this work, we explore the viability and cost-performance tradeoff of using automatic SHS detection on unmanned aerial vehicle (UAV) imagery as an alternative to satellite imagery. More specifically, we explore three questions: (i) what is the detection performance of SHS using drone imagery; (ii) how expensive is the drone data collection, compared to satellite imagery; and (iii) how well does drone-based SHS detection perform in real-world scenarios? To examine these questions, we collect and publicly-release a dataset of high-resolution drone imagery encompassing SHS imaged under a variety of real-world conditions and use this dataset and a dataset of imagery from Rwanda to evaluate the capabilities of deep learning models to recognize SHS, including those that are too small to be reliably recognized in satellite imagery. The results suggest that UAV imagery may be a viable alternative to identify very small SHS from perspectives of both detection accuracy and financial costs of data collection. UAV-based data collection may be a practical option for supporting electricity access planning strategies for achieving sustainable development goals and for monitoring the progress towards those goals.},
DOI = {10.3390/ijgi11040222}
}



@Article{rs14071574,
AUTHOR = {Guo, Mingqiang and Zhang, Zeyuan and Liu, Heng and Huang, Ying},
TITLE = {NDSRGAN: A Novel Dense Generative Adversarial Network for Real Aerial Imagery Super-Resolution Reconstruction},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {1574},
URL = {https://www.mdpi.com/2072-4292/14/7/1574},
ISSN = {2072-4292},
ABSTRACT = {In recent years, more and more researchers have used deep learning methods for super-resolution reconstruction and have made good progress. However, most of the existing super-resolution reconstruction models generate low-resolution images for training by downsampling high-resolution images through bicubic interpolation, and the models trained from these data have poor reconstruction results on real-world low-resolution images. In the field of unmanned aerial vehicle (UAV) aerial photography, the use of existing super-resolution reconstruction models in reconstructing real-world low-resolution aerial images captured by UAVs is prone to producing some artifacts, texture detail distortion and other problems, due to compression and fusion processing of the aerial images, thereby resulting in serious loss of texture detail in the obtained low-resolution aerial images. To address this problem, this paper proposes a novel dense generative adversarial network for real aerial imagery super-resolution reconstruction (NDSRGAN), and we produce image datasets with paired high- and low-resolution real aerial remote sensing images. In the generative network, we use a multilevel dense network to connect the dense connections in a residual dense block. In the discriminative network, we use a matrix mean discriminator that can discriminate the generated images locally, no longer discriminating the whole input image using a single value but instead in chunks of regions. We also use smoothL1 loss instead of the L1 loss used in most existing super-resolution models, to accelerate the model convergence and reach the global optimum faster. Compared with traditional models, our model can better utilise the feature information in the original image and discriminate the image in patches. A series of experiments is conducted with real aerial imagery datasets, and the results show that our model achieves good performance on quantitative metrics and visual perception.},
DOI = {10.3390/rs14071574}
}



@Article{su14074034,
AUTHOR = {Zong, Shuya and Chen, Sikai and Alinizzi, Majed and Labi, Samuel},
TITLE = {Leveraging UAV Capabilities for Vehicle Tracking and Collision Risk Assessment at Road Intersections},
JOURNAL = {Sustainability},
VOLUME = {14},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {4034},
URL = {https://www.mdpi.com/2071-1050/14/7/4034},
ISSN = {2071-1050},
ABSTRACT = {Transportation agencies continue to pursue crash reduction. Initiatives include the design of safer facilities, promotion of safe behaviors, and assessments of collision risk as a precursor to the identification of proactive countermeasures. Collision risk assessment includes reliable prediction of vehicle trajectories. Unfortunately, in using traditional tracking equipment, such prediction can be impaired by occlusion. It has been suggested in recent literature that unmanned aerial vehicles (UAVs) can be deployed to address this issue successfully, given their wide visual field and movement flexibility. This paper presents a methodology that integrates UAVs to track the movement of road users and to assess potential collisions at intersections. The proposed methodology includes an existing deep-learning-based algorithm to identify road users, extract trajectories, and calculate collision risk. The methodology was applied using a case study, and the results show that the methodology can provide beneficial information for the purpose of measuring and analyzing the infrastructure performance. Based on vehicle movements it observes, the UAV can communicate its collision risk to each vehicle so that the vehicle can undertake proactive driving decisions. Finally, the proposed framework can serve as a valuable tool for urban road agencies to develop measures to reduce crash risks.},
DOI = {10.3390/su14074034}
}



@Article{rs14071710,
AUTHOR = {Onishi, Masanori and Watanabe, Shuntaro and Nakashima, Tadashi and Ise, Takeshi},
TITLE = {Practicality and Robustness of Tree Species Identification Using UAV RGB Image and Deep Learning in Temperate Forest in Japan},
JOURNAL = {Remote Sensing},
VOLUME = {14},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {1710},
URL = {https://www.mdpi.com/2072-4292/14/7/1710},
ISSN = {2072-4292},
ABSTRACT = {Identifying tree species from the air has long been desired for forest management. Recently, combination of UAV RGB image and deep learning has shown high performance for tree identification in limited conditions. In this study, we evaluated the practicality and robustness of the tree identification system using UAVs and deep learning. We sampled training and test data from three sites in temperate forests in Japan. The objective tree species ranged across 56 species, including dead trees and gaps. When we evaluated the model performance on the dataset obtained from the same time and same tree crowns as the training dataset, it yielded a Kappa score of 0.97, and 0.72, respectively, for the performance on the dataset obtained from the same time but with different tree crowns. When we evaluated the dataset obtained from different times and sites from the training dataset, which is the same condition as the practical one, the Kappa scores decreased to 0.47. Though coniferous trees and representative species of stands showed a certain stable performance regarding identification, some misclassifications occurred between: (1) trees that belong to phylogenetically close species, (2) tree species with similar leaf shapes, and (3) tree species that prefer the same environment. Furthermore, tree types such as coniferous and broadleaved or evergreen and deciduous do not always guarantee common features between the different trees belonging to the tree type. Our findings promote the practicalization of identification systems using UAV RGB images and deep learning.},
DOI = {10.3390/rs14071710}
}



