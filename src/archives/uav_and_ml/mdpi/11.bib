
@Article{s18113921,
AUTHOR = {Boonpook, Wuttichai and Tan, Yumin and Ye, Yinghua and Torteeka, Peerapong and Torsri, Kritanai and Dong, Shengxian},
TITLE = {A Deep Learning Approach on Building Detection from Unmanned Aerial Vehicle-Based Images in Riverbank Monitoring},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {11},
ARTICLE-NUMBER = {3921},
URL = {https://www.mdpi.com/1424-8220/18/11/3921},
ISSN = {1424-8220},
ABSTRACT = {Buildings along riverbanks are likely to be affected by rising water levels, therefore the acquisition of accurate building information has great importance not only for riverbank environmental protection but also for dealing with emergency cases like flooding. UAV-based photographs are flexible and cloud-free compared to satellite images and can provide very high-resolution images up to centimeter level, while there exist great challenges in quickly and accurately detecting and extracting building from UAV images because there are usually too many details and distortions on UAV images. In this paper, a deep learning (DL)-based approach is proposed for more accurately extracting building information, in which the network architecture, SegNet, is used in the semantic segmentation after the network training on a completely labeled UAV image dataset covering multi-dimension urban settlement appearances along a riverbank area in Chongqing. The experiment results show that an excellent performance has been obtained in the detection of buildings from untrained locations with an average overall accuracy more than 90%. To verify the generality and advantage of the proposed method, the procedure is further evaluated by training and testing with another two open standard datasets which have a variety of building patterns and styles, and the final overall accuracies of building extraction are more than 93% and 95%, respectively.},
DOI = {10.3390/s18113921}
}



@Article{rs11040410,
AUTHOR = {Ampatzidis, Yiannis and Partel, Victor},
TITLE = {UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {410},
URL = {https://www.mdpi.com/2072-4292/11/4/410},
ISSN = {2072-4292},
ABSTRACT = {Traditional plant breeding evaluation methods are time-consuming, labor-intensive, and costly. Accurate and rapid phenotypic trait data acquisition and analysis can improve genomic selection and accelerate cultivar development. In this work, a technique for data acquisition and image processing was developed utilizing small unmanned aerial vehicles (UAVs), multispectral imaging, and deep learning convolutional neural networks to evaluate phenotypic characteristics on citrus crops. This low-cost and automated high-throughput phenotyping technique utilizes artificial intelligence (AI) and machine learning (ML) to: (i) detect, count, and geolocate trees and tree gaps; (ii) categorize trees based on their canopy size; (iii) develop individual tree health indices; and (iv) evaluate citrus varieties and rootstocks. The proposed remote sensing technique was able to detect and count citrus trees in a grove of 4,931 trees, with precision and recall of 99.9% and 99.7%, respectively, estimate their canopy size with overall accuracy of 85.5%, and detect, count, and geolocate tree gaps with a precision and recall of 100% and 94.6%, respectively. This UAV-based technique provides a consistent, more direct, cost-effective, and rapid method to evaluate phenotypic characteristics of citrus varieties and rootstocks.},
DOI = {10.3390/rs11040410}
}



@Article{rs11080925,
AUTHOR = {Zhu, Jiasong and Chen, Siyuan and Tu, Wei and Sun, Ke},
TITLE = {Tracking and Simulating Pedestrian Movements at Intersections Using Unmanned Aerial Vehicles},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {925},
URL = {https://www.mdpi.com/2072-4292/11/8/925},
ISSN = {2072-4292},
ABSTRACT = {For a city to be livable and walkable is the ultimate goal of future cities. However, conflicts among pedestrians, vehicles, and cyclists at traffic intersections are becoming severe in high-density urban transportation areas, especially in China. Correspondingly, the transit time at intersections is becoming prolonged, and pedestrian safety is becoming endangered. Simulating pedestrian movements at complex traffic intersections is necessary to optimize the traffic organization. We propose an unmanned aerial vehicle (UAV)-based method for tracking and simulating pedestrian movements at intersections. Specifically, high-resolution videos acquired by a UAV are used to recognize and position moving targets, including pedestrians, cyclists, and vehicles, using the convolutional neural network. An improved social force-based motion model is proposed, considering the conflicts among pedestrians, cyclists, and vehicles. In addition, maximum likelihood estimation is performed to calibrate an improved social force model. UAV videos of intersections in Shenzhen are analyzed to demonstrate the performance of the presented approach. The results demonstrate that the proposed social force-based motion model can effectively simulate the movement of pedestrians and cyclists at road intersections. The presented approach provides an alternative method to track and simulate pedestrian movements, thus benefitting the organization of pedestrian flow and traffic signals controlling the intersections.},
DOI = {10.3390/rs11080925}
}



@Article{rs11091128,
AUTHOR = {Rahnemoonfar, Maryam and Dobbs, Dugan and Yari, Masoud and Starek, Michael J.},
TITLE = {DisCountNet: Discriminating and Counting Network for Real-Time Counting and Localization of Sparse Objects in High-Resolution UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1128},
URL = {https://www.mdpi.com/2072-4292/11/9/1128},
ISSN = {2072-4292},
ABSTRACT = {Recent deep-learning counting techniques revolve around two distinct features of data&mdash;sparse data, which favors detection networks, or dense data where density map networks are used. Both techniques fail to address a third scenario, where dense objects are sparsely located. Raw aerial images represent sparse distributions of data in most situations. To address this issue, we propose a novel and exceedingly portable end-to-end model, DisCountNet, and an example dataset to test it on. DisCountNet is a two-stage network that uses theories from both detection and heat-map networks to provide a simple yet powerful design. The first stage, DiscNet, operates on the theory of coarse detection, but does so by converting a rich and high-resolution image into a sparse representation where only important information is encoded. Following this, CountNet operates on the dense regions of the sparse matrix to generate a density map, which provides fine locations and count predictions on densities of objects. Comparing the proposed network to current state-of-the-art networks, we find that we can maintain competitive performance while using a fraction of the computational complexity, resulting in a real-time solution.},
DOI = {10.3390/rs11091128}
}



@Article{s19122775,
AUTHOR = {Munaye, Yirga Yayeh and Lin, Hsin-Piao and Adege, Abebe Belay and Tarekegn, Getaneh Berie},
TITLE = {UAV Positioning for Throughput Maximization Using Deep Learning Approaches},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2775},
URL = {https://www.mdpi.com/1424-8220/19/12/2775},
ISSN = {1424-8220},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) as a communication platform has great practical importance for future wireless networks, especially for on-demand deployment for temporary and emergency conditions. The user throughput estimation in a wireless system depends on the data traffic load and the available capacity to support that load. In UAV-assisted communication, the position of the UAV is one major factor that affects the capacity available to the data flows being served. This study applies multi-layer perceptron (MLP) and long short term memory (LSTM) approaches to determine the position of a UAV that maximizes the overall system performance and user throughput. To analyze and evaluate the system performance, we apply the hybrid of MLP-LSTM for classification regression tasks and K-means algorithms for automatic clustering of classes. The implementation of our work is done through TensorFlow packages. The performance of our proposed system is compared with other approaches to give accurate and novel results for both classification and regression tasks of the user throughput maximization and UAV positioning. According to the results, 98% of the user throughput maximization accuracy is correctly classified. Moreover, the UAV positioning provides accuracy levels of 94.73%, 98.33%, and 99.53% for original datasets (scenario 1), reduced features on the estimated values of user throughput at each grid point (scenario 2), and reduced feature datasets collected on different days and grid points achieved maximum throughput (scenario 3), respectively.},
DOI = {10.3390/s19122775}
}



@Article{s19143106,
AUTHOR = {Zhou, Chengquan and Ye, Hongbao and Hu, Jun and Shi, Xiaoyan and Hua, Shan and Yue, Jibo and Xu, Zhifu and Yang, Guijun},
TITLE = {Automated Counting of Rice Panicle by Applying Deep Learning Model to Images from Unmanned Aerial Vehicle Platform},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3106},
URL = {https://www.mdpi.com/1424-8220/19/14/3106},
ISSN = {1424-8220},
ABSTRACT = {The number of panicles per unit area is a common indicator of rice yield and is of great significance to yield estimation, breeding, and phenotype analysis. Traditional counting methods have various drawbacks, such as long delay times and high subjectivity, and they are easily perturbed by noise. To improve the accuracy of rice detection and counting in the field, we developed and implemented a panicle detection and counting system that is based on improved region-based fully convolutional networks, and we use the system to automate rice-phenotype measurements. The field experiments were conducted in target areas to train and test the system and used a rotor light unmanned aerial vehicle equipped with a high-definition RGB camera to collect images. The trained model achieved a precision of 0.868 on a held-out test set, which demonstrates the feasibility of this approach. The algorithm can deal with the irregular edge of the rice panicle, the significantly different appearance between the different varieties and growing periods, the interference due to color overlapping between panicle and leaves, and the variations in illumination intensity and shading effects in the field. The result is more accurate and efficient recognition of rice-panicles, which facilitates rice breeding. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a global scale.},
DOI = {10.3390/s19143106}
}



@Article{geosciences9070323,
AUTHOR = {Jakovljevic, Gordana and Govedarica, Miro and Alvarez-Taboada, Flor and Pajic, Vladimir},
TITLE = {Accuracy Assessment of Deep Learning Based Classification of LiDAR and UAV Points Clouds for DTM Creation and Flood Risk Mapping},
JOURNAL = {Geosciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2076-3263/9/7/323},
ISSN = {2076-3263},
ABSTRACT = {Digital elevation model (DEM) has been frequently used for the reduction and management of flood risk. Various classification methods have been developed to extract DEM from point clouds. However, the accuracy and computational efficiency need to be improved. The objectives of this study were as follows: (1) to determine the suitability of a new method to produce DEM from unmanned aerial vehicle (UAV) and light detection and ranging (LiDAR) data, using a raw point cloud classification and ground point filtering based on deep learning and neural networks (NN); (2) to test the convenience of rebalancing datasets for point cloud classification; (3) to evaluate the effect of the land cover class on the algorithm performance and the elevation accuracy; and (4) to assess the usability of the LiDAR and UAV structure from motion (SfM) DEM in flood risk mapping. In this paper, a new method of raw point cloud classification and ground point filtering based on deep learning using NN is proposed and tested on LiDAR and UAV data. The NN was trained on approximately 6 million points from which local and global geometric features and intensity data were extracted. Pixel-by-pixel accuracy assessment and visual inspection confirmed that filtering point clouds based on deep learning using NN is an appropriate technique for ground classification and producing DEM, as for the test and validation areas, both ground and non-ground classes achieved high recall (&gt;0.70) and high precision values (&gt;0.85), which showed that the two classes were well handled by the model. The type of method used for balancing the original dataset did not have a significant influence in the algorithm accuracy, and it was suggested not to use any of them unless the distribution of the generated and real data set will remain the same. Furthermore, the comparisons between true data and LiDAR and a UAV structure from motion (UAV SfM) point clouds were analyzed, as well as the derived DEM. The root mean square error (RMSE) and the mean average error (MAE) of the DEM were 0.25 m and 0.05 m, respectively, for LiDAR data, and 0.59 m and &ndash;0.28 m, respectively, for UAV data. For all land cover classes, the UAV DEM overestimated the elevation, whereas the LIDAR DEM underestimated it. The accuracy was not significantly different in the LiDAR DEM for the different vegetation classes, while for the UAV DEM, the RMSE increased with the height of the vegetation class. The comparison of the inundation areas derived from true LiDAR and UAV data for different water levels showed that in all cases, the largest differences were obtained for the lowest water level tested, while they performed best for very high water levels. Overall, the approach presented in this work produced DEM from LiDAR and UAV data with the required accuracy for flood mapping according to European Flood Directive standards. Although LiDAR is the recommended technology for point cloud acquisition, a suitable alternative is also UAV SfM in hilly areas.},
DOI = {10.3390/geosciences9070323}
}



@Article{s19163542,
AUTHOR = {Lygouras, Eleftherios and Santavas, Nicholas and Taitzoglou, Anastasios and Tarchanidis, Konstantinos and Mitropoulos, Athanasios and Gasteratos, Antonios},
TITLE = {Unsupervised Human Detection with an Embedded Vision System on a Fully Autonomous UAV for Search and Rescue Operations},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3542},
URL = {https://www.mdpi.com/1424-8220/19/16/3542},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play a primary role in a plethora of technical and scientific fields owing to their wide range of applications. In particular, the provision of emergency services during the occurrence of a crisis event is a vital application domain where such aerial robots can contribute, sending out valuable assistance to both distressed humans and rescue teams. Bearing in mind that time constraints constitute a crucial parameter in search and rescue (SAR) missions, the punctual and precise detection of humans in peril is of paramount importance. The paper in hand deals with real-time human detection onboard a fully autonomous rescue UAV. Using deep learning techniques, the implemented embedded system was capable of detecting open water swimmers. This allowed the UAV to provide assistance accurately in a fully unsupervised manner, thus enhancing first responder operational capabilities. The novelty of the proposed system is the combination of global navigation satellite system (GNSS) techniques and computer vision algorithms for both precise human detection and rescue apparatus release. Details about hardware configuration as well as the system&rsquo;s performance evaluation are fully discussed.},
DOI = {10.3390/s19163542}
}



@Article{rs11172046,
AUTHOR = {Ghorbanzadeh, Omid and Meena, Sansar Raj and Blaschke, Thomas and Aryal, Jagannath},
TITLE = {UAV-Based Slope Failure Detection Using Deep-Learning Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2046},
URL = {https://www.mdpi.com/2072-4292/11/17/2046},
ISSN = {2072-4292},
ABSTRACT = {Slope failures occur when parts of a slope collapse abruptly under the influence of gravity, often triggered by a rainfall event or earthquake. The resulting slope failures often cause problems in mountainous or hilly regions, and the detection of slope failure is therefore an important topic for research. Most of the methods currently used for mapping and modelling slope failures rely on classification algorithms or feature extraction, but the spatial complexity of slope failures, the uncertainties inherent in expert knowledge, and problems in transferability, all combine to inhibit slope failure detection. In an attempt to overcome some of these problems we have analyzed the potential of deep learning convolutional neural networks (CNNs) for slope failure detection, in an area along a road section in the northern Himalayas, India. We used optical data from unmanned aerial vehicles (UAVs) over two separate study areas. Different CNN designs were used to produce eight different slope failure distribution maps, which were then compared with manually extracted slope failure polygons using different accuracy assessment metrics such as the precision, F-score, and mean intersection-over-union (mIOU). A slope failure inventory data set was produced for each of the study areas using a frequency-area distribution (FAD). The CNN approach that was found to perform best (precision accuracy assessment of almost 90% precision, F-score 85%, mIOU 74%) was one that used a window size of 64 &times; 64 pixels for the sample patches, and included slope data as an additional input layer. The additional information from the slope data helped to discriminate between slope failure areas and roads, which had similar spectral characteristics in the optical imagery. We concluded that the effectiveness of CNNs for slope failure detection was strongly dependent on their design (i.e., the window size selected for the sample patch, the data used, and the training strategies), but that CNNs are currently only designed by trial and error. While CNNs can be powerful tools, such trial and error strategies make it difficult to explain why a particular pooling or layer numbering works better than any other.},
DOI = {10.3390/rs11172046}
}



@Article{s19194332,
AUTHOR = {Opromolla, Roberto and Inchingolo, Giuseppe and Fasano, Giancarmine},
TITLE = {Airborne Visual Detection and Tracking of Cooperative UAVs Exploiting Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4332},
URL = {https://www.mdpi.com/1424-8220/19/19/4332},
ISSN = {1424-8220},
ABSTRACT = {The performance achievable by using Unmanned Aerial Vehicles (UAVs) for a large variety of civil and military applications, as well as the extent of applicable mission scenarios, can significantly benefit from the exploitation of formations of vehicles able to fly in a coordinated manner (swarms). In this respect, visual cameras represent a key instrument to enable coordination by giving each UAV the capability to visually monitor the other members of the formation. Hence, a related technological challenge is the development of robust solutions to detect and track cooperative targets through a sequence of frames. In this framework, this paper proposes an innovative approach to carry out this task based on deep learning. Specifically, the You Only Look Once (YOLO) object detection system is integrated within an original processing architecture in which the machine-vision algorithms are aided by navigation hints available thanks to the cooperative nature of the formation. An experimental flight test campaign, involving formations of two multirotor UAVs, is conducted to collect a database of images suitable to assess the performance of the proposed approach. Results demonstrate high-level accuracy, and robustness against challenging conditions in terms of illumination, background and target-range variability.},
DOI = {10.3390/s19194332}
}



@Article{s19214794,
AUTHOR = {Rodriguez-Ramos, Alejandro and Alvarez-Fernandez, Adrian and Bavle, Hriday and Campoy, Pascual and How, Jonathan P.},
TITLE = {Vision-Based Multirotor Following Using Synthetic Learning Techniques},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4794},
URL = {https://www.mdpi.com/1424-8220/19/21/4794},
ISSN = {1424-8220},
ABSTRACT = {Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights).},
DOI = {10.3390/s19214794}
}



@Article{s19224851,
AUTHOR = {Zhou, Jun and Tian, Yichen and Yuan, Chao and Yin, Kai and Yang, Guang and Wen, Meiping},
TITLE = {Improved UAV Opium Poppy Detection Using an Updated YOLOv3 Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4851},
URL = {https://www.mdpi.com/1424-8220/19/22/4851},
ISSN = {1424-8220},
ABSTRACT = {Rapid detection of illicit opium poppy plants using UAV (unmanned aerial vehicle) imagery has become an important means to prevent and combat crimes related to drug cultivation. However, current methods rely on time-consuming visual image interpretation. Here, the You Only Look Once version 3 (YOLOv3) network structure was used to assess the influence that different backbone networks have on the average precision and detection speed of an UAV-derived dataset of poppy imagery, with MobileNetv2 (MN) selected as the most suitable backbone network. A Spatial Pyramid Pooling (SPP) unit was introduced and Generalized Intersection over Union (GIoU) was used to calculate the coordinate loss. The resulting SPP-GIoU-YOLOv3-MN model improved the average precision by 1.62% (from 94.75% to 96.37%) without decreasing speed and achieved an average precision of 96.37%, with a detection speed of 29 FPS using an RTX 2080Ti platform. The sliding window method was used for detection in complete UAV images, which took approximately 2.2 sec/image, approximately 10&times; faster than visual interpretation. The proposed technique significantly improved the efficiency of poppy detection in UAV images while also maintaining a high detection accuracy. The proposed method is thus suitable for the rapid detection of illicit opium poppy cultivation in residential areas and farmland where UAVs with ordinary visible light cameras can be operated at low altitudes (relative height &lt; 200 m).},
DOI = {10.3390/s19224851}
}



@Article{rs12010182,
AUTHOR = {Meng, Lingxuan and Peng, Zhixing and Zhou, Ji and Zhang, Jirong and Lu, Zhenyu and Baumann, Andreas and Du, Yan},
TITLE = {Real-Time Detection of Ground Objects Based on Unmanned Aerial Vehicle Remote Sensing with Deep Learning: Application in Excavator Detection for Pipeline Safety},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {182},
URL = {https://www.mdpi.com/2072-4292/12/1/182},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) remote sensing and deep learning provide a practical approach to object detection. However, most of the current approaches for processing UAV remote-sensing data cannot carry out object detection in real time for emergencies, such as firefighting. This study proposes a new approach for integrating UAV remote sensing and deep learning for the real-time detection of ground objects. Excavators, which usually threaten pipeline safety, are selected as the target object. A widely used deep-learning algorithm, namely You Only Look Once V3, is first used to train the excavator detection model on a workstation and then deployed on an embedded board that is carried by a UAV. The recall rate of the trained excavator detection model is 99.4%, demonstrating that the trained model has a very high accuracy. Then, the UAV for an excavator detection system (UAV-ED) is further constructed for operational application. UAV-ED is composed of a UAV Control Module, a UAV Module, and a Warning Module. A UAV experiment with different scenarios was conducted to evaluate the performance of the UAV-ED. The whole process from the UAV observation of an excavator to the Warning Module (350 km away from the testing area) receiving the detection results only lasted about 1.15 s. Thus, the UAV-ED system has good performance and would benefit the management of pipeline safety.},
DOI = {10.3390/rs12010182}
}



@Article{rs12020245,
AUTHOR = {Senthilnath, J. and Varia, Neelanshi and Dokania, Akanksha and Anand, Gaotham and Benediktsson, Jón Atli},
TITLE = {Deep TEC: Deep Transfer Learning with Ensemble Classifier for Road Extraction from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {245},
URL = {https://www.mdpi.com/2072-4292/12/2/245},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) remote sensing has a wide area of applications and in this paper, we attempt to address one such problem&mdash;road extraction from UAV-captured RGB images. The key challenge here is to solve the road extraction problem using the UAV multiple remote sensing scene datasets that are acquired with different sensors over different locations. We aim to extract the knowledge from a dataset that is available in the literature and apply this extracted knowledge on our dataset. The paper focuses on a novel method which consists of deep TEC (deep transfer learning with ensemble classifier) for road extraction using UAV imagery. The proposed deep TEC performs road extraction on UAV imagery in two stages, namely, deep transfer learning and ensemble classifier. In the first stage, with the help of deep learning methods, namely, the conditional generative adversarial network, the cycle generative adversarial network and the fully convolutional network, the model is pre-trained on the benchmark UAV road extraction dataset that is available in the literature. With this extracted knowledge (based on the pre-trained model) the road regions are then extracted on our UAV acquired images. Finally, for the road classified images, ensemble classification is carried out. In particular, the deep TEC method has an average quality of 71%, which is 10% higher than the next best standard deep learning methods. Deep TEC also shows a higher level of performance measures such as completeness, correctness and F1 score measures. Therefore, the obtained results show that the deep TEC is efficient in extracting road networks in an urban region.},
DOI = {10.3390/rs12020245}
}



@Article{s20020563,
AUTHOR = {Lobo Torres, Daliana and Queiroz Feitosa, Raul and Nigri Happ, Patrick and Elena Cué La Rosa, Laura and Marcato Junior, José and Martins, José and Olã Bressan, Patrik and Gonçalves, Wesley Nunes and Liesenberg, Veraldo},
TITLE = {Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {563},
URL = {https://www.mdpi.com/1424-8220/20/2/563},
ISSN = {1424-8220},
ABSTRACT = {This study proposes and evaluates five deep fully convolutional networks (FCNs) for the semantic segmentation of a single tree species: SegNet, U-Net, FC-DenseNet, and two DeepLabv3+ variants. The performance of the FCN designs is evaluated experimentally in terms of classification accuracy and computational load. We also verify the benefits of fully connected conditional random fields (CRFs) as a post-processing step to improve the segmentation maps. The analysis is conducted on a set of images captured by an RGB camera aboard a UAV flying over an urban area. The dataset also contains a mask that indicates the occurrence of an endangered species called Dipteryx alata Vogel, also known as cumbaru, taken as the species to be identified. The experimental analysis shows the effectiveness of each design and reports average overall accuracy ranging from 88.9% to 96.7%, an F1-score between 87.0% and 96.1%, and IoU from 77.1% to 92.5%. We also realize that CRF consistently improves the performance, but at a high computational cost.},
DOI = {10.3390/s20020563}
}



@Article{rs12030354,
AUTHOR = {Park, Seula and Song, Ahram},
TITLE = {Discrepancy Analysis for Detecting Candidate Parcels Requiring Update of Land Category in Cadastral Map Using Hyperspectral UAV Images: A Case Study in Jeonju, South Korea},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {354},
URL = {https://www.mdpi.com/2072-4292/12/3/354},
ISSN = {2072-4292},
ABSTRACT = {The non-spatial information of cadastral maps must be repeatedly updated to monitor recent changes in land property and to detect illegal land registrations by tax evaders. Since non-spatial information, such as land category, is usually updated by field-based surveys, it is time-consuming and only a limited area can be updated at a time. Although land categories can be updated by remote sensing techniques, the update is typically performed through manual analysis, namely through a visually interpreted comparison between the newly generated land information and the existing cadastral maps. A cost-effective, fast alternative to the current surveying methods would improve the efficiency of land management. For this purpose, the present study analyzes the discrepancy between the existing cadastral map and the actual land use. Our proposed method operates in two steps. First, an up-to-date land cover map is generated from hyperspectral unmanned aerial vehicle (UAV) images. These images are effectively classified by a hybrid two- and three-dimensional convolutional neural network. Second, a discrepancy map, which contains the ratio of the area that is being used differently from the registered land use in each parcel, is constructed through a three-stage inconsistency comparison. As a case study, the proposed method was evaluated using hyperspectral UAV images acquired at two sites of Jeonju in South Korea. The overall classification accuracies of six land classes at Sites 1 and 2 were 99.93% and 99.75% and those at Sites 1 and 2 are 39.4% and 34.4%, respectively, which had discrepancy ratios of 50% or higher. Finally, discrepancy maps between the land cover maps and existing cadastral maps were generated and visualized. The method automatically reveals the inconsistent parcels requiring updates of their land category. Although the performance of the proposed method depends on the classification results obtained from UAV imagery, the method allows a flexible modification of the matching criteria between the land categories and land coverage. Therefore, it is generalizable to various cadastral systems and the discrepancy ratios will provide practical information and significantly reduce the time and effort for land monitoring and field surveying.},
DOI = {10.3390/rs12030354}
}



@Article{rs12030402,
AUTHOR = {Wang, Linhui and Yue, Xuejun and Wang, Huihui and Ling, Kangjie and Liu, Yongxin and Wang, Jian and Hong, Jinbao and Pen, Wen and Song, Houbing},
TITLE = {Dynamic Inversion of Inland Aquaculture Water Quality Based on UAVs-WSN Spectral Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {402},
URL = {https://www.mdpi.com/2072-4292/12/3/402},
ISSN = {2072-4292},
ABSTRACT = {The inland aquaculture environment is an artificial ecosystem, where the water quality is a key factor which is closely related to the economic benefits of inland aquaculture and the quality of aquatic products. Compared with marine aquaculture, inland aquaculture is normally smaller and susceptible to pollution, with poor self-purification capacity. Considering its low cost and large-scale monitoring ability, many researches have developed spectrum sensor on-board satellite platforms to allow remote monitoring of inland water surface. However, there remain many problems, such as low image resolution, poor flexible data acquisition, and anti-interference. Apart from that, the conventional forecasting model is of weak generalization ability and low accuracy. In our study, we combine unmanned aerial vehicles system (UAVs) with the wireless sensor network (WSN) to design a new ground water quality parameter and drone spectrum information acquisition approach, and to propose a novel dynamic network surgery-deep neural networks (DNS-DNNs) model based on multi-source feature fusion to forecast the distribution of dissolved oxygen (DO) and turbidity (TUB) in inland aquaculture areas. The result of using fused features, including characteristic spectrum, Gray-level co-occurrence matrix (GLCM) texture feature, and convolutional neural network (CNN) texture feature to build a model is that the characteristic spectrum+ CNN texture fusion features were the best input items for DNS-DNNs when forecasting DO, with the determination coefficient      R 2      of the vertical set arriving at 0.8741, while the characteristic spectrum+ GLCM texture+ CNN texture fusion features were the best for TUB, with the      R 2      reaching 0.8531. Compared with a variety of conventional models, our model had a better performance in the inversion of DO and TUB, and there was a strong correlation between predicted and real values:      R 2      reached 0.8042 and 0.8346, whereas the root mean square error (RMSE) were only 0.1907 and 0.1794, separately. Our study provides a new insight about using remote sensing to rapidly monitor water quality in inland aquaculture regions.},
DOI = {10.3390/rs12030402}
}



@Article{ijgi9020099,
AUTHOR = {Sang, Xuejia and Xue, Linfu and Ran, Xiangjin and Li, Xiaoshun and Liu, Jiwen and Liu, Zeyu},
TITLE = {Intelligent High-Resolution Geological Mapping Based on SLIC-CNN},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {99},
URL = {https://www.mdpi.com/2220-9964/9/2/99},
ISSN = {2220-9964},
ABSTRACT = {High-resolution geological mapping is an important supporting condition for mineral and energy exploration. However, high-resolution geological mapping work still faces many problems. At present, high-resolution geological mapping is still generated by expert interpretation of survey lines, compasses, and field data. The work in the field is constrained by the weather, terrain, and personnel, and the working methods need to be improved. This paper proposes a new method for high-resolution mapping using Unmanned Aerial Vehicle (UAV) and deep learning algorithms. This method uses the UAV to collect high-resolution remote sensing images, cooperates with some groundwork to anchor the lithology, and then completes most of the mapping work on high-resolution remote sensing images. This method transfers a large amount of field work into the room and provides an automatic mapping process based on the Simple Linear Iterative Clustering-Convolutional Neural Network (SLIC-CNN) algorithm. It uses the convolutional neural network (CNN) to identify the image content and confirms the lithologic distribution, the simple linear iterative cluster (SLIC) algorithm can be used to outline the boundary of the rock mass and determine the contact interface of the rock mass, and the mode and expert decision method is used to clarify the results of the fusion and mapping. The mapping method was applied to the Taili waterfront in Xingcheng City, Liaoning Province, China. In this study, the Area Under the Curve (AUC) of the mapping method was 0.937. The Kappa test result was k = 0.8523, and a high-resolution geological map was obtained.},
DOI = {10.3390/ijgi9020099}
}



@Article{rs12040633,
AUTHOR = {Yang, Ming-Der and Tseng, Hsin-Hung and Hsu, Yu-Chun and Tsai, Hui Ping},
TITLE = {Semantic Segmentation Using Deep Learning with Vegetation Indices for Rice Lodging Identification in Multi-date UAV Visible Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {633},
URL = {https://www.mdpi.com/2072-4292/12/4/633},
ISSN = {2072-4292},
ABSTRACT = {A rapid and precise large-scale agricultural disaster survey is a basis for agricultural disaster relief and insurance but is labor-intensive and time-consuming. This study applies Unmanned Aerial Vehicles (UAVs) images through deep-learning image processing to estimate the rice lodging in paddies over a large area. This study establishes an image semantic segmentation model employing two neural network architectures, FCN-AlexNet, and SegNet, whose effects are explored in the interpretation of various object sizes and computation efficiency. Commercial UAVs imaging rice paddies in high-resolution visible images are used to calculate three vegetation indicators to improve the applicability of visible images. The proposed model was trained and tested on a set of UAV images in 2017 and was validated on a set of UAV images in 2019. For the identification of rice lodging on the 2017 UAV images, the F1-score reaches 0.80 and 0.79 for FCN-AlexNet and SegNet, respectively. The F1-score of FCN-AlexNet using RGB + ExGR combination also reaches 0.78 in the 2019 images for validation. The proposed model adopting semantic segmentation networks is proven to have better efficiency, approximately 10 to 15 times faster, and a lower misinterpretation rate than that of the maximum likelihood method.},
DOI = {10.3390/rs12040633}
}



@Article{rs12040640,
AUTHOR = {Wan, Kaifang and Gao, Xiaoguang and Hu, Zijian and Wu, Gaofeng},
TITLE = {Robust Motion Control for UAV in Dynamic Uncertain Environments Using Deep Reinforcement Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {640},
URL = {https://www.mdpi.com/2072-4292/12/4/640},
ISSN = {2072-4292},
ABSTRACT = {In this paper, a novel deep reinforcement learning (DRL) method, and robust deep deterministic policy gradient (Robust-DDPG), is proposed for developing a controller that allows robust flying of an unmanned aerial vehicle (UAV) in dynamic uncertain environments. This technique is applicable in many fields, such as penetration and remote surveillance. The learning-based controller is constructed with an actor-critic framework, and can perform a dual-channel continuous control (roll and speed) of the UAV. To overcome the fragility and volatility of original DDPG, three critical learning tricks are introduced in Robust-DDPG: (1) Delayed-learning trick, providing stable learnings, while facing dynamic environments; (2) adversarial attack trick, improving policy&rsquo;s adaptability to uncertain environments; (3) mixed exploration trick, enabling faster convergence of the model. The training experiments show great improvement in its convergence speed, convergence effect, and stability. The exploiting experiments demonstrate high efficiency in providing the UAV a shorter and smoother path. While, the generalization experiments verify its better adaptability to complicated, dynamic and uncertain environments, comparing to Deep Q Network (DQN) and DDPG algorithms.},
DOI = {10.3390/rs12040640}
}



@Article{rs12071085,
AUTHOR = {Zhang, Weixing and Liljedahl, Anna K. and Kanevskiy, Mikhail and Epstein, Howard E. and Jones, Benjamin M. and Jorgenson, M. Torre and Kent, Kelcy},
TITLE = {Transferability of the Deep Learning Mask R-CNN Model for Automated Mapping of Ice-Wedge Polygons in High-Resolution Satellite and UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1085},
URL = {https://www.mdpi.com/2072-4292/12/7/1085},
ISSN = {2072-4292},
ABSTRACT = {State-of-the-art deep learning technology has been successfully applied to relatively small selected areas of very high spatial resolution (0.15 and 0.25 m) optical aerial imagery acquired by a fixed-wing aircraft to automatically characterize ice-wedge polygons (IWPs) in the Arctic tundra. However, any mapping of IWPs at regional to continental scales requires images acquired on different sensor platforms (particularly satellite) and a refined understanding of the performance stability of the method across sensor platforms through reliable evaluation assessments. In this study, we examined the transferability of a deep learning Mask Region-Based Convolutional Neural Network (R-CNN) model for mapping IWPs in satellite remote sensing imagery (~0.5 m) covering 272 km2 and unmanned aerial vehicle (UAV) (0.02 m) imagery covering 0.32 km2. Multi-spectral images were obtained from the WorldView-2 satellite sensor and pan-sharpened to ~0.5 m, and a 20 mp CMOS sensor camera onboard a UAV, respectively. The training dataset included 25,489 and 6022 manually delineated IWPs from satellite and fixed-wing aircraft aerial imagery near the Arctic Coastal Plain, northern Alaska. Quantitative assessments showed that individual IWPs were correctly detected at up to 72% and 70%, and delineated at up to 73% and 68% F1 score accuracy levels for satellite and UAV images, respectively. Expert-based qualitative assessments showed that IWPs were correctly detected at good (40&ndash;60%) and excellent (80&ndash;100%) accuracy levels for satellite and UAV images, respectively, and delineated at excellent (80&ndash;100%) level for both images. We found that (1) regardless of spatial resolution and spectral bands, the deep learning Mask R-CNN model effectively mapped IWPs in both remote sensing satellite and UAV images; (2) the model achieved a better accuracy in detection with finer image resolution, such as UAV imagery, yet a better accuracy in delineation with coarser image resolution, such as satellite imagery; (3) increasing the number of training data with different resolutions between the training and actual application imagery does not necessarily result in better performance of the Mask R-CNN in IWPs mapping; (4) and overall, the model underestimates the total number of IWPs particularly in terms of disjoint/incomplete IWPs.},
DOI = {10.3390/rs12071085}
}



@Article{s20071890,
AUTHOR = {Hu, Zijian and Wan, Kaifang and Gao, Xiaoguang and Zhai, Yiwei and Wang, Qianglong},
TITLE = {Deep Reinforcement Learning Approach with Multiple Experience Pools for UAV’s Autonomous Motion Planning in Complex Unknown Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1890},
URL = {https://www.mdpi.com/1424-8220/20/7/1890},
ISSN = {1424-8220},
ABSTRACT = {Autonomous motion planning (AMP) of unmanned aerial vehicles (UAVs) is aimed at enabling a UAV to safely fly to the target without human intervention. Recently, several emerging deep reinforcement learning (DRL) methods have been employed to address the AMP problem in some simplified environments, and these methods have yielded good results. This paper proposes a multiple experience pools (MEPs) framework leveraging human expert experiences for DRL to speed up the learning process. Based on the deep deterministic policy gradient (DDPG) algorithm, a MEP&ndash;DDPG algorithm was designed using model predictive control and simulated annealing to generate expert experiences. On applying this algorithm to a complex unknown simulation environment constructed based on the parameters of the real UAV, the training experiment results showed that the novel DRL algorithm resulted in a performance improvement exceeding 20% as compared with the state-of-the-art DDPG. The results of the experimental testing indicate that UAVs trained using MEP&ndash;DDPG can stably complete a variety of tasks in complex, unknown environments.},
DOI = {10.3390/s20071890}
}



@Article{s20082320,
AUTHOR = {Çetin, Ender and Barrado, Cristina and Pastor, Enric},
TITLE = {Counter a Drone in a Complex Neighborhood Area by Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2320},
URL = {https://www.mdpi.com/1424-8220/20/8/2320},
ISSN = {1424-8220},
ABSTRACT = {Counter-drone technology by using artificial intelligence (AI) is an emerging technology and it is rapidly developing. Considering the recent advances in AI, counter-drone systems with AI can be very accurate and efficient to fight against drones. The time required to engage with the target can be less than other methods based on human intervention, such as bringing down a malicious drone by a machine-gun. Also, AI can identify and classify the target with a high precision in order to prevent a false interdiction with the targeted object. We believe that counter-drone technology with AI will bring important advantages to the threats coming from some drones and will help the skies to become safer and more secure. In this study, a deep reinforcement learning (DRL) architecture is proposed to counter a drone with another drone, the learning drone, which will autonomously avoid all kind of obstacles inside a suburban neighborhood environment. The environment in a simulator that has stationary obstacles such as trees, cables, parked cars, and houses. In addition, another non-malicious third drone, acting as moving obstacle inside the environment was also included. In this way, the learning drone is trained to detect stationary and moving obstacles, and to counter and catch the target drone without crashing with any other obstacle inside the neighborhood. The learning drone has a front camera and it can capture continuously depth images. Every depth image is part of the state used in DRL architecture. There are also scalar state parameters such as velocities, distances to the target, distances to some defined geofences and track, and elevation angles. The state image and scalars are processed by a neural network that joints the two state parts into a unique flow. Moreover, transfer learning is tested by using the weights of the first full-trained model. With transfer learning, one of the best jump-starts achieved higher mean rewards (close to 35 more) at the beginning of training. Transfer learning also shows that the number of crashes during training can be reduced, with a total number of crashed episodes reduced by 65%, when all ground obstacles are included.},
DOI = {10.3390/s20082320}
}



@Article{rs12091515,
AUTHOR = {Jakovljevic, Gordana and Govedarica, Miro and Alvarez-Taboada, Flor},
TITLE = {A Deep Learning Model for Automatic Plastic Mapping Using Unmanned Aerial Vehicle (UAV) Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1515},
URL = {https://www.mdpi.com/2072-4292/12/9/1515},
ISSN = {2072-4292},
ABSTRACT = {Although plastic pollution is one of the most noteworthy environmental issues nowadays, there is still a knowledge gap in terms of monitoring the spatial distribution of plastics, which is needed to prevent its negative effects and to plan mitigation actions. Unmanned Aerial Vehicles (UAVs) can provide suitable data for mapping floating plastic, but most of the methods require visual interpretation and manual labeling. The main goals of this paper are to determine the suitability of deep learning algorithms for automatic floating plastic extraction from UAV orthophotos, testing the possibility of differentiating plastic types, and exploring the relationship between spatial resolution and detectable plastic size, in order to define a methodology for UAV surveys to map floating plastic. Two study areas and three datasets were used to train and validate the models. An end-to-end semantic segmentation algorithm based on U-Net architecture using the ResUNet50 provided the highest accuracy to map different plastic materials (F1-score: Oriented Polystyrene (OPS): 0.86; Nylon: 0.88; Polyethylene terephthalate (PET): 0.92; plastic (in general): 0.78), showing its ability to identify plastic types. The classification accuracy decreased with the decrease in spatial resolution, performing best on 4 mm resolution images for all kinds of plastic. The model provided reliable estimates of the area and volume of the plastics, which is crucial information for a cleaning campaign.},
DOI = {10.3390/rs12091515}
}



@Article{agriengineering2020019,
AUTHOR = {Deng, Xiaoling and Tong, Zejing and Lan, Yubin and Huang, Zixiao},
TITLE = {Detection and Location of Dead Trees with Pine Wilt Disease Based on Deep Learning and UAV Remote Sensing},
JOURNAL = {AgriEngineering},
VOLUME = {2},
YEAR = {2020},
NUMBER = {2},
PAGES = {294--307},
URL = {https://www.mdpi.com/2624-7402/2/2/19},
ISSN = {2624-7402},
ABSTRACT = {Pine wilt disease causes huge economic losses to pine wood forestry because of its destructiveness and rapid spread. This paper proposes a detection and location method of pine wood nematode disease at a large scale adopting UAV (Unmanned Aerial Vehicle) remote sensing and artificial intelligence technology. The UAV remote sensing images were enhanced by computer vision tools. A Faster-RCNN (Faster Region Convolutional Neural Networks) deep learning framework based on a RPN (Region Proposal Network) network and the ResNet residual neural network were used to train the pine wilt diseased dead tree detection model. The loss function and the anchors in the RPN of the convolutional neural network were optimized. Finally, the location of pine wood nematode dead tree was conducted, which generated the geographic information on the detection results. The results show that ResNet101 performed better than VGG16 (Visual Geometry Group 16) convolutional neural network. The detection accuracy was improved and reached to about 90% after a series of optimizations to the network, meaning that the optimization methods proposed in this paper are feasible to pine wood nematode dead tree detection.},
DOI = {10.3390/agriengineering2020019}
}



@Article{s20143856,
AUTHOR = {Seidaliyeva, Ulzhalgas and Akhmetov, Daryn and Ilipbayeva, Lyazzat and Matson, Eric T.},
TITLE = {Real-Time and Accurate Drone Detection in a Video with a Static Background},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3856},
URL = {https://www.mdpi.com/1424-8220/20/14/3856},
ISSN = {1424-8220},
ABSTRACT = {With the increasing number of drones, the danger of their illegal use has become relevant. This has necessitated the creation of automatic drone protection systems. One of the important tasks solved by these systems is the reliable detection of drones near guarded objects. This problem can be solved using various methods. From the point of view of the price&ndash;quality ratio, the use of video cameras for a drone detection is of great interest. However, drone detection using visual information is hampered by the large similarity of drones to other objects, such as birds or airplanes. In addition, drones can reach very high speeds, so detection should be done in real time. This paper addresses the problem of real-time drone detection with high accuracy. We divided the drone detection task into two separate tasks: the detection of moving objects and the classification of the detected object into drone, bird, and background. The moving object detection is based on background subtraction, while classification is performed using a convolutional neural network (CNN). The experimental results showed that the proposed approach can achieve an accuracy comparable to existing approaches at high processing speed. We also concluded that the main limitation of our detector is the dependence of its performance on the presence of a moving background.},
DOI = {10.3390/s20143856}
}



@Article{informatics7030023,
AUTHOR = {Ciaburro, Giuseppe and Iannace, Gino},
TITLE = {Improving Smart Cities Safety Using Sound Events Detection Based on Deep Neural Network Algorithms},
JOURNAL = {Informatics},
VOLUME = {7},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2227-9709/7/3/23},
ISSN = {2227-9709},
ABSTRACT = {In recent years, security in urban areas has gradually assumed a central position, focusing increasing attention on citizens, institutions and political forces. Security problems have a different nature&mdash;to name a few, we can think of the problems deriving from citizens&rsquo; mobility, then move on to microcrime, and end up with the ever-present risk of terrorism. Equipping a smart city with an infrastructure of sensors capable of alerting security managers about a possible risk becomes crucial for the safety of citizens. The use of unmanned aerial vehicles (UAVs) to manage citizens&rsquo; needs is now widespread, to highlight the possible risks to public safety. These risks were then increased using these devices to carry out terrorist attacks in various places around the world. Detecting the presence of drones is not a simple procedure given the small size and the presence of only rotating parts. This study presents the results of studies carried out on the detection of the presence of UAVs in outdoor/indoor urban sound environments. For the detection of UAVs, sensors capable of measuring the sound emitted by UAVs and algorithms based on deep neural networks capable of identifying their spectral signature that were used. The results obtained suggest the adoption of this methodology for improving the safety of smart cities.},
DOI = {10.3390/informatics7030023}
}



@Article{s20174739,
AUTHOR = {Walker, Ory and Vanegas, Fernando and Gonzalez, Felipe},
TITLE = {A Framework for Multi-Agent UAV Exploration and Target-Finding in GPS-Denied and Partially Observable Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4739},
URL = {https://www.mdpi.com/1424-8220/20/17/4739},
ISSN = {1424-8220},
ABSTRACT = {The problem of multi-agent remote sensing for the purposes of finding survivors or surveying points of interest in GPS-denied and partially observable environments remains a challenge. This paper presents a framework for multi-agent target-finding using a combination of online POMDP based planning and Deep Reinforcement Learning based control. The framework is implemented considering planning and control as two separate problems. The planning problem is defined as a decentralised multi-agent graph search problem and is solved using a modern online POMDP solver. The control problem is defined as a local continuous-environment exploration problem and is solved using modern Deep Reinforcement Learning techniques. The proposed framework combines the solution to both of these problems and testing shows that it enables multiple agents to find a target within large, simulated test environments in the presence of unknown obstacles and obstructions. The proposed approach could also be extended or adapted to a number of time sensitive remote-sensing problems, from searching for multiple survivors during a disaster to surveying points of interest in a hazardous environment by adjusting the individual model definitions.},
DOI = {10.3390/s20174739}
}



@Article{rs12172863,
AUTHOR = {Dang, L. Minh and Wang, Hanxiang and Li, Yanfen and Min, Kyungbok and Kwak, Jin Tae and Lee, O. New and Park, Hanyong and Moon, Hyeonjoon},
TITLE = {Fusarium Wilt of Radish Detection Using RGB and Near Infrared Images from Unmanned Aerial Vehicles},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2863},
URL = {https://www.mdpi.com/2072-4292/12/17/2863},
ISSN = {2072-4292},
ABSTRACT = {The radish is a delicious, healthy vegetable and an important ingredient to many side dishes and main recipes. However, climate change, pollinator decline, and especially Fusarium wilt cause a significant reduction in the cultivation area and the quality of the radish yield. Previous studies on plant disease identification have relied heavily on extracting features manually from images, which is time-consuming and inefficient. In addition to Red-Green-Blue (RGB) images, the development of near-infrared (NIR) sensors has enabled a more effective way to monitor the diseases and evaluate plant health based on multispectral imagery. Thus, this study compares two distinct approaches in detecting radish wilt using RGB images and NIR images taken by unmanned aerial vehicles (UAV). The main research contributions include (1) a high-resolution RGB and NIR radish field dataset captured by drone from low to high altitudes, which can serve several research purposes; (2) implementation of a superpixel segmentation method to segment captured radish field images into separated segments; (3) a customized deep learning-based radish identification framework for the extracted segmented images, which achieved remarkable performance in terms of accuracy and robustness with the highest accuracy of 96%; (4) the proposal for a disease severity analysis that can detect different stages of the wilt disease; (5) showing that the approach based on NIR images is more straightforward and effective in detecting wilt disease than the learning approach based on the RGB dataset.},
DOI = {10.3390/rs12172863}
}



@Article{s20185240,
AUTHOR = {Koubaa, Anis and Ammar, Adel and Alahdab, Mahmoud and Kanhouch, Anas and Azar, Ahmad Taher},
TITLE = {DeepBrain: Experimental Evaluation of Cloud-Based Computation Offloading and Edge Computing in the Internet-of-Drones for Deep Learning Applications},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5240},
URL = {https://www.mdpi.com/1424-8220/20/18/5240},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) have been very effective in collecting aerial images data for various Internet-of-Things (IoT)/smart cities applications such as search and rescue, surveillance, vehicle detection, counting, intelligent transportation systems, to name a few. However, the real-time processing of collected data on edge in the context of the Internet-of-Drones remains an open challenge because UAVs have limited energy capabilities, while computer vision techniquesconsume excessive energy and require abundant resources. This fact is even more critical when deep learning algorithms, such as convolutional neural networks (CNNs), are used for classification and detection. In this paper, we first propose a system architecture of computation offloading for Internet-connected drones. Then, we conduct a comprehensive experimental study to evaluate the performance in terms of energy, bandwidth, and delay of the cloud computation offloading approach versus the edge computing approach of deep learning applications in the context of UAVs. In particular, we investigate the tradeoff between the communication cost and the computation of the two candidate approaches experimentally. The main results demonstrate that the computation offloading approach allows us to provide much higher throughput (i.e., frames per second) as compared to the edge computing approach, despite the larger communication delays.},
DOI = {10.3390/s20185240}
}



@Article{rs12183015,
AUTHOR = {Machefer, Mélissande and Lemarchand, François and Bonnefond, Virginie and Hitchins, Alasdair and Sidiropoulos, Panagiotis},
TITLE = {Mask R-CNN Refitting Strategy for Plant Counting and Sizing in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3015},
URL = {https://www.mdpi.com/2072-4292/12/18/3015},
ISSN = {2072-4292},
ABSTRACT = {This work introduces a method that combines remote sensing and deep learning into a framework that is tailored for accurate, reliable and efficient counting and sizing of plants in aerial images. The investigated task focuses on two low-density crops, potato and lettuce. This double objective of counting and sizing is achieved through the detection and segmentation of individual plants by fine-tuning an existing deep learning architecture called Mask R-CNN. This paper includes a thorough discussion on the optimal parametrisation to adapt the Mask R-CNN architecture to this novel task. As we examine the correlation of the Mask R-CNN performance to the annotation volume and granularity (coarse or refined) of remotely sensed images of plants, we conclude that transfer learning can be effectively used to reduce the required amount of labelled data. Indeed, a previously trained Mask R-CNN on a low-density crop can improve performances after training on new crops. Once trained for a given crop, the Mask R-CNN solution is shown to outperform a manually-tuned computer vision algorithm. Model performances are assessed using intuitive metrics such as Mean Average Precision (mAP) from Intersection over Union (IoU) of the masks for individual plant segmentation and Multiple Object Tracking Accuracy (MOTA) for detection. The presented model reaches an mAP of 0.418 for potato plants and 0.660 for lettuces for the individual plant segmentation task. In detection, we obtain a MOTA of 0.781 for potato plants and 0.918 for lettuces.},
DOI = {10.3390/rs12183015}
}



@Article{rs12203318,
AUTHOR = {Na, Jiaming and Xue, Kaikai and Xiong, Liyang and Tang, Guoan and Ding, Hu and Strobl, Josef and Pfeifer, Norbert},
TITLE = {UAV-Based Terrain Modeling under Vegetation in the Chinese Loess Plateau: A Deep Learning and Terrain Correction Ensemble Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3318},
URL = {https://www.mdpi.com/2072-4292/12/20/3318},
ISSN = {2072-4292},
ABSTRACT = {Accurate topographic mapping is a critical task for various environmental applications because elevation affects hydrodynamics and vegetation distributions. UAV photogrammetry is popular in terrain modelling because of its lower cost compared to laser scanning. However, this method is restricted in vegetation area with a complex terrain, due to reduced ground visibility and lack of robust and automatic filtering algorithms. To solve this problem, this work proposed an ensemble method of deep learning and terrain correction. First, image matching point cloud was generated by UAV photogrammetry. Second, vegetation points were identified based on U-net deep learning network. After that, ground elevation was corrected by estimating vegetation height to generate the digital terrain model (DTM). Two scenarios, namely, discrete and continuous vegetation areas were considered. The vegetation points in the discrete area were directly removed and then interpolated, and terrain correction was applied for the points in the continuous areas. Case studies were conducted in three different landforms in the loess plateau of China, and accuracy assessment indicated that the overall accuracy of vegetation detection was 95.0%, and the MSE (Mean Square Error) of final DTM (Digital Terrain Model) was 0.024 m.},
DOI = {10.3390/rs12203318}
}



@Article{app10207132,
AUTHOR = {Deng, Jizhong and Zhong, Zhaoji and Huang, Huasheng and Lan, Yubin and Han, Yuxing and Zhang, Yali},
TITLE = {Lightweight Semantic Segmentation Network for Real-Time Weed Mapping Using Unmanned Aerial Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7132},
URL = {https://www.mdpi.com/2076-3417/10/20/7132},
ISSN = {2076-3417},
ABSTRACT = {The timely and efficient generation of weed maps is essential for weed control tasks and precise spraying applications. Based on the general concept of site-specific weed management (SSWM), many researchers have used unmanned aerial vehicle (UAV) remote sensing technology to monitor weed distributions, which can provide decision support information for precision spraying. However, image processing is mainly conducted offline, as the time gap between image collection and spraying significantly limits the applications of SSWM. In this study, we conducted real-time image processing onboard a UAV to reduce the time gap between image collection and herbicide treatment. First, we established a hardware environment for real-time image processing that integrates map visualization, flight control, image collection, and real-time image processing onboard a UAV based on secondary development. Second, we exploited the proposed model design to develop a lightweight network architecture for weed mapping tasks. The proposed network architecture was evaluated and compared with mainstream semantic segmentation models. Results demonstrate that the proposed network outperform contemporary networks in terms of efficiency with competitive accuracy. We also conducted optimization during the inference process. Precision calibration was applied to both the desktop and embedded devices and the precision was reduced from FP32 to FP16. Experimental results demonstrate that this precision calibration further improves inference speed while maintaining reasonable accuracy. Our modified network architecture achieved an accuracy of 80.9% on the testing samples and its inference speed was 4.5 fps on a Jetson TX2 module (Nvidia Corporation, Santa Clara, CA, USA), which demonstrates its potential for practical agricultural monitoring and precise spraying applications.},
DOI = {10.3390/app10207132}
}



@Article{rs12213533,
AUTHOR = {Pedro, Dário and Matos-Carvalho, João P. and Azevedo, Fábio and Sacoto-Martins, Ricardo and Bernardo, Luís and Campos, Luís and Fonseca, José M. and Mora, André},
TITLE = {FFAU—Framework for Fully Autonomous UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3533},
URL = {https://www.mdpi.com/2072-4292/12/21/3533},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), although hardly a new technology, have recently gained a prominent role in many industries being widely used not only among enthusiastic consumers, but also in high demanding professional situations, and will have a massive societal impact over the coming years. However, the operation of UAVs is fraught with serious safety risks, such as collisions with dynamic obstacles (birds, other UAVs, or randomly thrown objects). These collision scenarios are complex to analyze in real-time, sometimes being computationally impossible to solve with existing State of the Art (SoA) algorithms, making the use of UAVs an operational hazard and therefore significantly reducing their commercial applicability in urban environments. In this work, a conceptual framework for both stand-alone and swarm (networked) UAVs is introduced, with a focus on the architectural requirements of the collision avoidance subsystem to achieve acceptable levels of safety and reliability. The SoA principles for collision avoidance against stationary objects are reviewed and a novel approach is described, using deep learning techniques to solve the computational intensive problem of real-time collision avoidance with dynamic objects. The proposed framework includes a web-interface allowing the full control of UAVs as remote clients with a supervisor cloud-based platform. The feasibility of the proposed approach was demonstrated through experimental tests using a UAV, developed from scratch using the proposed framework. Test flight results are presented for an autonomous UAV monitored from multiple countries across the world.},
DOI = {10.3390/rs12213533}
}



@Article{agronomy10111762,
AUTHOR = {Zhao, Biquan and Li, Jiating and Baenziger, P. Stephen and Belamkar, Vikas and Ge, Yufeng and Zhang, Jian and Shi, Yeyin},
TITLE = {Automatic Wheat Lodging Detection and Mapping in Aerial Imagery to Support High-Throughput Phenotyping and In-Season Crop Management},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1762},
URL = {https://www.mdpi.com/2073-4395/10/11/1762},
ISSN = {2073-4395},
ABSTRACT = {Latest advances in unmanned aerial vehicle (UAV) technology and convolutional neural networks (CNNs) allow us to detect crop lodging in a more precise and accurate way. However, the performance and generalization of a model capable of detecting lodging when the plants may show different spectral and morphological signatures have not been investigated much. This study investigated and compared the performance of models trained using aerial imagery collected at two growth stages of winter wheat with different canopy phenotypes. Specifically, three CNN-based models were trained with aerial imagery collected at early grain filling stage only, at physiological maturity only, and at both stages. Results show that the multi-stage model trained by images from both growth stages outperformed the models trained by images from individual growth stages on all testing data. The mean accuracy of the multi-stage model was 89.23% for both growth stages, while the mean of the other two models were 52.32% and 84.9%, respectively. This study demonstrates the importance of diversity of training data in big data analytics, and the feasibility of developing a universal decision support system for wheat lodging detection and mapping multi-growth stages with high-resolution remote sensing imagery.},
DOI = {10.3390/agronomy10111762}
}



@Article{rs12223789,
AUTHOR = {Li, Bo and Gan, Zhigang and Chen, Daqing and Sergey Aleksandrovich, Dyachenko},
TITLE = {UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2072-4292/12/22/3789},
ISSN = {2072-4292},
ABSTRACT = {This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.},
DOI = {10.3390/rs12223789}
}



@Article{ijgi9120728,
AUTHOR = {Zhou, Dongbo and Liu, Shuangjian and Yu, Jie and Li, Hao},
TITLE = {A High-Resolution Spatial and Time-Series Labeled Unmanned Aerial Vehicle Image Dataset for Middle-Season Rice},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {728},
URL = {https://www.mdpi.com/2220-9964/9/12/728},
ISSN = {2220-9964},
ABSTRACT = {The existing remote sensing image datasets target the identification of objects, features, or man-made targets but lack the ability to provide the date and spatial information for the same feature in the time-series images. The spatial and temporal information is important for machine learning methods so that networks can be trained to support precision classification, particularly for agricultural applications of specific crops with distinct phenological growth stages. In this paper, we built a high-resolution unmanned aerial vehicle (UAV) image dataset for middle-season rice. We scheduled the UAV data acquisition in five villages of Hubei Province for three years, including 11 or 13 growing stages in each year that were accompanied by the annual agricultural surveying business. We investigated the accuracy of the vector maps for each field block and the precise information regarding the crops in the field by surveying each village and periodically arranging the UAV flight tasks on a weekly basis during the phenological stages. Subsequently, we developed a method to generate the samples automatically. Finally, we built a high-resolution UAV image dataset, including over 500,000 samples with the location and phenological growth stage information, and employed the imagery dataset in several machine learning algorithms for classification. We performed two exams to test our dataset. First, we used four classical deep learning networks for the fine classification of spatial and temporal information. Second, we used typical models to test the land cover on our dataset and compared this with the UCMerced Land Use Dataset and RSSCN7 Dataset. The results showed that the proposed image dataset supported typical deep learning networks in the classification task to identify the location and time of middle-season rice and achieved high accuracy with the public image dataset.},
DOI = {10.3390/ijgi9120728}
}



@Article{rs12234000,
AUTHOR = {Nevavuori, Petteri and Narra, Nathaniel and Linna, Petri and Lipping, Tarmo},
TITLE = {Crop Yield Prediction Using Multitemporal UAV Data and Spatio-Temporal Deep Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {4000},
URL = {https://www.mdpi.com/2072-4292/12/23/4000},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51% mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17% MAPE with four weekly frames from the beginning of the season.},
DOI = {10.3390/rs12234000}
}



@Article{rs12244193,
AUTHOR = {Tilon, Sofia and Nex, Francesco and Kerle, Norman and Vosselman, George},
TITLE = {Post-Disaster Building Damage Detection from Earth Observation Imagery Using Unsupervised and Transferable Anomaly Detecting Generative Adversarial Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4193},
URL = {https://www.mdpi.com/2072-4292/12/24/4193},
ISSN = {2072-4292},
ABSTRACT = {We present an unsupervised deep learning approach for post-disaster building damage detection that can transfer to different typologies of damage or geographical locations. Previous advances in this direction were limited by insufficient qualitative training data. We propose to use a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN) because it only requires pre-event imagery of buildings in their undamaged state. This approach aids the post-disaster response phase because the model can be developed in the pre-event phase and rapidly deployed in the post-event phase. We used the xBD dataset, containing pre- and post- event satellite imagery of several disaster-types, and a custom made Unmanned Aerial Vehicle (UAV) dataset, containing post-earthquake imagery. Results showed that models trained on UAV-imagery were capable of detecting earthquake-induced damage. The best performing model for European locations obtained a recall, precision and F1-score of 0.59, 0.97 and 0.74, respectively. Models trained on satellite imagery were capable of detecting damage on the condition that the training dataset was void of vegetation and shadows. In this manner, the best performing model for (wild)fire events yielded a recall, precision and F1-score of 0.78, 0.99 and 0.87, respectively. Compared to other supervised and/or multi-epoch approaches, our results are encouraging. Moreover, in addition to image classifications, we show how contextual information can be used to create detailed damage maps without the need of a dedicated multi-task deep learning framework. Finally, we formulate practical guidelines to apply this single-epoch and unsupervised method to real-world applications.},
DOI = {10.3390/rs12244193}
}



@Article{rs13020162,
AUTHOR = {Qin, Jun and Wang, Biao and Wu, Yanlan and Lu, Qi and Zhu, Haochen},
TITLE = {Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {162},
URL = {https://www.mdpi.com/2072-4292/13/2/162},
ISSN = {2072-4292},
ABSTRACT = {Pine nematode is a highly contagious disease that causes great damage to the world&rsquo;s pine forest resources. Timely and accurate identification of pine nematode disease can help to control it. At present, there are few research on pine nematode disease identification, and it is difficult to accurately identify and locate nematode disease in a single pine by existing methods. This paper proposes a new network, SCANet (spatial-context-attention network), to identify pine nematode disease based on unmanned aerial vehicle (UAV) multi-spectral remote sensing images. In this method, a spatial information retention module is designed to reduce the loss of spatial information; it preserves the shallow features of pine nematode disease and expands the receptive field to enhance the extraction of deep features through a context information module. SCANet reached an overall accuracy of 79% and a precision and recall of around 0.86, and 0.91, respectively. In addition, 55 disease points among 59 known disease points were identified, which is better than other methods (DeepLab V3+, DenseNet, and HRNet). This paper presents a fast, precise, and practical method for identifying nematode disease and provides reliable technical support for the surveillance and control of pine wood nematode disease.},
DOI = {10.3390/rs13020162}
}



@Article{s21020471,
AUTHOR = {Kentsch, Sarah and Cabezas, Mariano and Tomhave, Luca and Groß, Jens and Burkhard, Benjamin and Lopez Caceres, Maximo Larry and Waki, Katsushi and Diez, Yago},
TITLE = {Analysis of UAV-Acquired Wetland Orthomosaics Using GIS, Computer Vision, Computational Topology and Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {471},
URL = {https://www.mdpi.com/1424-8220/21/2/471},
PubMedID = {33440797},
ISSN = {1424-8220},
ABSTRACT = {Invasive blueberry species endanger the sensitive environment of wetlands and protection laws call for management measures. Therefore, methods are needed to identify blueberry bushes, locate them, and characterise their distribution and properties with a minimum of disturbance. UAVs (Unmanned Aerial Vehicles) and image analysis have become important tools for classification and detection approaches. In this study, techniques, such as GIS (Geographical Information Systems) and deep learning, were combined in order to detect invasive blueberry species in wetland environments. Images that were collected by UAV were used to produce orthomosaics, which were analysed to produce maps of blueberry location, distribution, and spread in each study site, as well as bush height and area information. Deep learning networks were used with transfer learning and unfrozen weights in order to automatically detect blueberry bushes reaching True Positive Values (TPV) of 93.83% and an Overall Accuracy (OA) of 98.83%. A refinement of the result masks reached a Dice of 0.624. This study provides an efficient and effective methodology to study wetlands while using different techniques.},
DOI = {10.3390/s21020471}
}



@Article{rs13020260,
AUTHOR = {Nguyen, Ha Trang and Lopez Caceres, Maximo Larry and Moritake, Koma and Kentsch, Sarah and Shu, Hase and Diez, Yago},
TITLE = {Individual Sick Fir Tree (Abies mariesii) Identification in Insect Infested Forests by Means of UAV Images and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {260},
URL = {https://www.mdpi.com/2072-4292/13/2/260},
ISSN = {2072-4292},
ABSTRACT = {Insect outbreaks are a recurrent natural phenomenon in forest ecosystems expected to increase due to climate change. Recent advances in Unmanned Aerial Vehicles (UAV) and Deep Learning (DL) Networks provide us with tools to monitor them. In this study we used nine orthomosaics and normalized Digital Surface Models (nDSM) to detect and classify healthy and sick Maries fir trees as well as deciduous trees. This study aims at automatically classifying treetops by means of a novel computer vision treetops detection algorithm and the adaptation of existing DL architectures. Considering detection alone, the accuracy results showed 85.70% success. In terms of detection and classification, we were able to detect/classify correctly 78.59% of all tree classes (39.64% for sick fir). However, with data augmentation, detection/classification percentage of the sick fir class rose to 73.01% at the cost of the result accuracy of all tree classes that dropped 63.57%. The implementation of UAV, computer vision and DL techniques contribute to the development of a new approach to evaluate the impact of insect outbreaks in forest.},
DOI = {10.3390/rs13020260}
}



@Article{s21041076,
AUTHOR = {Yan, Peng and Jia, Tao and Bai, Chengchao},
TITLE = {Searching and Tracking an Unknown Number of Targets: A Learning-Based Method Enhanced with Maps Merging},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1076},
URL = {https://www.mdpi.com/1424-8220/21/4/1076},
PubMedID = {33557359},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have been widely used in search and rescue (SAR) missions due to their high flexibility. A key problem in SAR missions is to search and track moving targets in an area of interest. In this paper, we focus on the problem of Cooperative Multi-UAV Observation of Multiple Moving Targets (CMUOMMT). In contrast to the existing literature, we not only optimize the average observation rate of the discovered targets, but we also emphasize the fairness of the observation of the discovered targets and the continuous exploration of the undiscovered targets, under the assumption that the total number of targets is unknown. To achieve this objective, a deep reinforcement learning (DRL)-based method is proposed under the Partially Observable Markov Decision Process (POMDP) framework, where each UAV maintains four observation history maps, and maps from different UAVs within a communication range can be merged to enhance UAVs’ awareness of the environment. A deep convolutional neural network (CNN) is used to process the merged maps and generate the control commands to UAVs. The simulation results show that our policy can enable UAVs to balance between giving the discovered targets a fair observation and exploring the search region compared with other methods.},
DOI = {10.3390/s21041076}
}



@Article{s21041385,
AUTHOR = {Feng, Yurong and Tse, Kwaiwa and Chen, Shengyang and Wen, Chih-Yung and Li, Boyang},
TITLE = {Learning-Based Autonomous UAV System for Electrical and Mechanical (E&amp;M) Device Inspection},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1385},
URL = {https://www.mdpi.com/1424-8220/21/4/1385},
PubMedID = {33669478},
ISSN = {1424-8220},
ABSTRACT = {The inspection of electrical and mechanical (E&amp;M) devices using unmanned aerial vehicles (UAVs) has become an increasingly popular choice in the last decade due to their flexibility and mobility. UAVs have the potential to reduce human involvement in visual inspection tasks, which could increase efficiency and reduce risks. This paper presents a UAV system for autonomously performing E&amp;M device inspection. The proposed system relies on learning-based detection for perception, multi-sensor fusion for localization, and path planning for fully autonomous inspection. The perception method utilizes semantic and spatial information generated by a 2-D object detector. The information is then fused with depth measurements for object state estimation. No prior knowledge about the location and category of the target device is needed. The system design is validated by flight experiments using a quadrotor platform. The result shows that the proposed UAV system enables the inspection mission autonomously and ensures a stable and collision-free flight.},
DOI = {10.3390/s21041385}
}



@Article{electronics10050543,
AUTHOR = {Jung, Soyi and Yun, Won Joon and Kim, Joongheon and Kim, Jae-Hyun},
TITLE = {Coordinated Multi-Agent Deep Reinforcement Learning for Energy-Aware UAV-Based Big-Data Platforms},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {543},
URL = {https://www.mdpi.com/2079-9292/10/5/543},
ISSN = {2079-9292},
ABSTRACT = {This paper proposes a novel coordinated multi-agent deep reinforcement learning (MADRL) algorithm for energy sharing among multiple unmanned aerial vehicles (UAVs) in order to conduct big-data processing in a distributed manner. For realizing UAV-assisted aerial surveillance or flexible mobile cellular services, robust wireless charging mechanisms are essential for delivering energy sources from charging towers (i.e., charging infrastructure) to their associated UAVs for seamless operations of autonomous UAVs in the sky. In order to actively and intelligently manage the energy resources in charging towers, a MADRL-based coordinated energy management system is desired and proposed for energy resource sharing among charging towers. When the required energy for charging UAVs is not enough in charging towers, the energy purchase from utility company (i.e., energy source provider in local energy market) is desired, which takes high costs. Therefore, the main objective of our proposed coordinated MADRL-based energy sharing learning algorithm is minimizing energy purchase from external utility companies to minimize system-operational costs. Finally, our performance evaluation results verify that the proposed coordinated MADRL-based algorithm achieves desired performance improvements.},
DOI = {10.3390/electronics10050543}
}



@Article{app11052185,
AUTHOR = {Nakama, Justin and Parada, Ricky and Matos-Carvalho, João P. and Azevedo, Fábio and Pedro, Dário and Campos, Luís},
TITLE = {Autonomous Environment Generator for UAV-Based Simulation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2185},
URL = {https://www.mdpi.com/2076-3417/11/5/2185},
ISSN = {2076-3417},
ABSTRACT = {The increased demand for Unmanned Aerial Vehicles (UAV) has also led to higher demand for realistic and efficient UAV testing environments. The current use of simulated environments has been shown to be a relatively inexpensive, safe, and repeatable way to evaluate UAVs before real-world use. However, the use of generic environments and manually-created custom scenarios leaves more to be desired. In this paper, we propose a new testbed that utilizes machine learning algorithms to procedurally generate, scale, and place 3D models to create a realistic environment. These environments are additionally based on satellite images, thus providing users with a more robust example of real-world UAV deployment. Although certain graphical improvements could be made, this paper serves as a proof of concept for an novel autonomous and relatively-large scale environment generator. Such a testbed could allow for preliminary operational planning and testing worldwide, without the need for on-site evaluation or data collection in the future.},
DOI = {10.3390/app11052185}
}



@Article{rs13050965,
AUTHOR = {Kraft, Marek and Piechocki, Mateusz and Ptak, Bartosz and Walas, Krzysztof},
TITLE = {Autonomous, Onboard Vision-Based Trash and Litter Detection in Low Altitude Aerial Images Collected by an Unmanned Aerial Vehicle},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {965},
URL = {https://www.mdpi.com/2072-4292/13/5/965},
ISSN = {2072-4292},
ABSTRACT = {Public littering and discarded trash are, despite the effort being put to limit it, still a serious ecological, aesthetic, and social problem. The problematic waste is usually localised and picked up by designated personnel, which is a tiresome, time-consuming task. This paper proposes a low-cost solution enabling the localisation of trash and litter objects in low altitude imagery collected by an unmanned aerial vehicle (UAV) during an autonomous patrol mission. The objects of interest are detected in the acquired images and put on the global map using a set of onboard sensors commonly found in typical UAV autopilots. The core object detection algorithm is based on deep, convolutional neural networks. Since the task is domain-specific, a dedicated dataset of images containing objects of interest was collected and annotated. The dataset is made publicly available, and its description is contained in the paper. The dataset was used to test a range of embedded devices enabling the deployment of deep neural networks for inference onboard the UAV. The results of measurements in terms of detection accuracy and processing speed are enclosed, and recommendations for the neural network model and hardware platform are given based on the obtained values. The complete system can be put together using inexpensive, off-the-shelf components, and perform autonomous localisation of discarded trash, relieving human personnel of this burdensome task, and enabling automated pickup planning.},
DOI = {10.3390/rs13050965}
}



@Article{s21061960,
AUTHOR = {Fotouhi, Azade and Ding, Ming and Hassan, Mahbub},
TITLE = {Deep Q-Learning for Two-Hop Communications of Drone Base Stations},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1960},
URL = {https://www.mdpi.com/1424-8220/21/6/1960},
PubMedID = {33799546},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we address the application of the flying Drone Base Stations (DBS) in order to improve the network performance. Given the high degrees of freedom of a DBS, it can change its position and adapt its trajectory according to the users movements and the target environment. A two-hop communication model, between an end-user and a macrocell through a DBS, is studied in this work. We propose Q-learning and Deep Q-learning based solutions to optimize the drone’s trajectory. Simulation results show that, by employing our proposed models, the drone can autonomously fly and adapts its mobility according to the users’ movements. Additionally, the Deep Q-learning model outperforms the Q-learning model and can be applied in more complex environments.},
DOI = {10.3390/s21061960}
}



@Article{aerospace8030079,
AUTHOR = {Swinney, Carolyn J. and Woods, John C.},
TITLE = {Unmanned Aerial Vehicle Operating Mode Classification Using Deep Residual Learning Feature Extraction},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {79},
URL = {https://www.mdpi.com/2226-4310/8/3/79},
ISSN = {2226-4310},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) undoubtedly pose many security challenges. We need only look to the December 2018 Gatwick Airport incident for an example of the disruption UAVs can cause. In total, 1000 flights were grounded for 36 h over the Christmas period which was estimated to cost over 50 million pounds. In this paper, we introduce a novel approach which considers UAV detection as an imagery classification problem. We consider signal representations Power Spectral Density (PSD); Spectrogram, Histogram and raw IQ constellation as graphical images presented to a deep Convolution Neural Network (CNN) ResNet50 for feature extraction. Pre-trained on ImageNet, transfer learning is utilised to mitigate the requirement for a large signal dataset. We evaluate performance through machine learning classifier Logistic Regression. Three popular UAVs are classified in different modes; switched on; hovering; flying; flying with video; and no UAV present, creating a total of 10 classes. Our results, validated with 5-fold cross validation and an independent dataset, show PSD representation to produce over 91% accuracy for 10 classifications. Our paper treats UAV detection as an imagery classification problem by presenting signal representations as images to a ResNet50, utilising the benefits of transfer learning and outperforming previous work in the field.},
DOI = {10.3390/aerospace8030079}
}



@Article{electronics10060724,
AUTHOR = {Yavariabdi, Amir and Kusetogullari, Huseyin and Celik, Turgay and Cicek, Hasan},
TITLE = {FastUAV-NET: A Multi-UAV Detection Algorithm for Embedded Platforms},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {724},
URL = {https://www.mdpi.com/2079-9292/10/6/724},
ISSN = {2079-9292},
ABSTRACT = {In this paper, a real-time deep learning-based framework for detecting and tracking Unmanned Aerial Vehicles (UAVs) in video streams captured by a fixed-wing UAV is proposed. The proposed framework consists of two steps, namely intra-frame multi-UAV detection and the inter-frame multi-UAV tracking. In the detection step, a new multi-scale UAV detection Convolutional Neural Network (CNN) architecture based on a shallow version of You Only Look Once version 3 (YOLOv3-tiny) widened by Inception blocks is designed to extract local and global features from input video streams. Here, the widened multi-UAV detection network architecture is termed as FastUAV-NET and aims to improve UAV detection accuracy while preserving computing time of one-step deep detection algorithms in the context of UAV-UAV tracking. To detect UAVs, the FastUAV-NET architecture uses five inception units and adopts a feature pyramid network to detect UAVs. To obtain a high frame rate, the proposed method is applied to every nth frame and then the detected UAVs are tracked in intermediate frames using scalable Kernel Correlation Filter algorithm. The results on the generated UAV-UAV dataset illustrate that the proposed framework obtains 0.7916 average precision with 29 FPS performance on Jetson-TX2. The results imply that the widening of CNN network is a much more effective way than increasing the depth of CNN and leading to a good trade-off between accurate detection and real-time performance. The FastUAV-NET model will be publicly available to the research community to further advance multi-UAV-UAV detection algorithms.},
DOI = {10.3390/electronics10060724}
}



