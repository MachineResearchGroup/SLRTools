TY  - EJOU
AU  - Han, Seongkyun
AU  - Yoo, Jisang
AU  - Kwon, Soonchul
TI  - Real-Time Vehicle-Detection Method in Bird-View Unmanned-Aerial-Vehicle Imagery
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 18
SN  - 1424-8220

AB  - Vehicle detection is an important research area that provides background information for the diversity of unmanned-aerial-vehicle (UAV) applications. In this paper, we propose a vehicle-detection method using a convolutional-neural-network (CNN)-based object detector. We design our method, DRFBNet300, with a Deeper Receptive Field Block (DRFB) module that enhances the expressiveness of feature maps to detect small objects in the UAV imagery. We also propose the UAV-cars dataset that includes the composition and angular distortion of vehicles in UAV imagery to train our DRFBNet300. Lastly, we propose a Split Image Processing (SIP) method to improve the accuracy of the detection model. Our DRFBNet300 achieves 21 mAP with 45 FPS in the MS COCO metric, which is the highest score compared to other lightweight single-stage methods running in real time. In addition, DRFBNet300, trained on the UAV-cars dataset, obtains the highest AP score at altitudes of 20&ndash;50 m. The gap of accuracy improvement by applying the SIP method became larger when the altitude increases. The DRFBNet300 trained on the UAV-cars dataset with SIP method operates at 33 FPS, enabling real-time vehicle detection.
KW  - vehicle detection
KW  - object detection
KW  - UAV imagery
KW  - convolutional neural network
DO  - 10.3390/s19183958
TY  - EJOU
AU  - Wang, Jie
AU  - Simeonova, Sandra
AU  - Shahbazi, Mozhdeh
TI  - Orientation- and Scale-Invariant Multi-Vehicle Detection and Tracking from Unmanned Aerial Videos
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 18
SN  - 2072-4292

AB  - Along with the advancement of light-weight sensing and processing technologies, unmanned aerial vehicles (UAVs) have recently become popular platforms for intelligent traffic monitoring and control. UAV-mounted cameras can capture traffic-flow videos from various perspectives providing a comprehensive insight into road conditions. To analyze the traffic flow from remotely captured videos, a reliable and accurate vehicle detection-and-tracking approach is required. In this paper, we propose a deep-learning framework for vehicle detection and tracking from UAV videos for monitoring traffic flow in complex road structures. This approach is designed to be invariant to significant orientation and scale variations in the videos. The detection procedure is performed by fine-tuning a state-of-the-art object detector, You Only Look Once (YOLOv3), using several custom-labeled traffic datasets. Vehicle tracking is conducted following a tracking-by-detection paradigm, where deep appearance features are used for vehicle re-identification, and Kalman filtering is used for motion estimation. The proposed methodology is tested on a variety of real videos collected by UAVs under various conditions, e.g., in late afternoons with long vehicle shadows, in dawn with vehicles lights being on, over roundabouts and interchange roads where vehicle directions change considerably, and from various viewpoints where vehicles&rsquo; appearance undergo substantial perspective distortions. The proposed tracking-by-detection approach performs efficiently at 11 frames per second on color videos of 2720p resolution. Experiments demonstrated that high detection accuracy could be achieved with an average F1-score of 92.1%. Besides, the tracking technique performs accurately, with an average multiple-object tracking accuracy (MOTA) of 81.3%. The proposed approach also addressed the shortcomings of the state-of-the-art in multi-object tracking regarding frequent identity switching, resulting in a total of only one identity switch over every 305 tracked vehicles.
KW  - traffic monitoring
KW  - vehicle detection
KW  - multi-vehicle tracking
KW  - vehicle re-identification
KW  - unmanned aerial vehicles
KW  - deep convolutional neural network.
DO  - 10.3390/rs11182155
TY  - EJOU
AU  - Wang, Dezhi
AU  - Wan, Bo
AU  - Qiu, Penghua
AU  - Zuo, Zejun
AU  - Wang, Run
AU  - Wu, Xincai
TI  - Mapping Height and Aboveground Biomass of Mangrove Forests on Hainan Island Using UAV-LiDAR Sampling
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 18
SN  - 2072-4292

AB  - Hainan Island is the second-largest island in China and has the most species-diverse mangrove forests in the country. To date, the height and aboveground ground biomass (AGB) of the mangrove forests on Hainan Island are unknown, partly as a result of the challenges faced during extensive field sampling in mangrove habitats (intertidal mudflats inundated by periodic seawater). Therefore, this study used a low-cost UAV-LiDAR (light detection and ranging sensor mounted on an unmanned aerial vehicle) system as a sampling tool and Sentinel-2 imagery as auxiliary data to estimate and map the mangrove height and AGB on Hainan Island. Hainan Island has 3697.02 hectares of mangrove forests with an average patch area of approximately 1 ha. The results show that the mangroves on whole Hainan Island have an average height of 6.99 m, a total AGB of 474,199.31 Mg and an AGB density of 128.27 Mg ha&minus;1. The AGB hot spots are located in Qinglan Harbor and the south of Dongzhai Harbor. The proposed height model LiDAR-S2 performed well with an R2 of 0.67 and an RMSE (root mean square error) of 1.90 m; the proposed AGB model G~LiDAR~S2 performed better (an R2 of 0.62 and an RMSE of 50.36 Mg ha&minus;1) than the traditional AGB model G~S2 that directly related ground plots and Sentinel-2 data. The results also indicate that the LiDAR metrics describing the canopy&rsquo;s thickness and its top and bottom characteristics are the most important variables for mangrove AGB estimation. For the Sentinel-2 indices, the red-edge and shortwave infrared features, especially the red-edge 1 and shortwave infrared Band 11 features, play the most important roles in estimating mangrove AGB and height. In conclusion, this paper presents the first mangrove height and AGB maps of Hainan Island and demonstrates the feasibility of using UAV-LiDAR as a sampling tool for mangrove forests.
KW  - mangroves
KW  - Hainan island
KW  - aboveground biomass
KW  - UAV-LiDAR sampling
KW  - Sentinel-2
KW  - random forest
KW  - feature importance
DO  - 10.3390/rs11182156
TY  - EJOU
AU  - Zou, Xiaodan
AU  - Liang, Anjie
AU  - Wu, Bizhi
AU  - Su, Jun
AU  - Zheng, Renhua
AU  - Li, Jian
TI  - UAV-Based High-Throughput Approach for Fast Growing Cunninghamia lanceolata (Lamb.) Cultivar Screening by Machine Learning
T2  - Forests

PY  - 2019
VL  - 10
IS  - 9
SN  - 1999-4907

AB  - Obtaining accurate measurements of tree height and diameter at breast height (DBH) in forests to evaluate the growth rate of cultivars is still a significant challenge, even when using light detection and ranging (LiDAR) and three-dimensional (3-D) modeling. As an alternative, we provide a novel high-throughput strategy for predicting the biomass of forests in the field by vegetation indices. This study proposes an integrated pipeline methodology to measure the biomass of different tree cultivars in plantation forests with high crown density, which combines unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Using a planation of Cunninghamia lanceolate, which is commonly known as Chinese fir, in Fujian, China, images were collected while using a hyperspectral camera. Vegetation indices and modeling were processed in Python using decision trees, random forests, support vector machine, and eXtreme Gradient Boosting (XGBoost) third-party libraries. The tree height and DBH of 2880 samples were manually measured and clustered into three groups&mdash;&ldquo;Fast&rdquo;, &ldquo;median&rdquo;, and &ldquo;normal&rdquo; growth groups&mdash;and 19 vegetation indices from 12,000 pixels were abstracted as the input of features for the modeling. After modeling and cross-validation, the classifier that was generated by random forests had the best prediction accuracy when compared to other algorithms (75%). This framework can be applied to other tree species to make management and business decisions.
KW  - Cunninghamia lanceolate
KW  - UAVs
KW  - hyperspectral camera
KW  - machine learning
KW  - random forests
KW  - XGBoost
DO  - 10.3390/f10090815
TY  - EJOU
AU  - Phuc, Le T.
AU  - Jeon, HyeJun
AU  - Truong, Nguyen T.
AU  - Hak, Jung J.
TI  - Applying the Haar-cascade Algorithm for Detecting Safety Equipment in Safety Management Systems for Multiple Working Environments
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 10
SN  - 2079-9292

AB  - There are many ways to maintain the safety of workers on a working site, such as using a human supervisor, computer supervisor, and smoke&ndash;flame detecting system. In order to create a safety warning system for the working site, the machine-learning algorithm&mdash;Haar-cascade classifier&mdash;was used to build four different classes for safety equipment recognition. Then a proposed algorithm was applied to calculate a score to determine the dangerousness of the current working environment based on the safety equipment and working environment. With this data, the system decides whether it is necessary to give a warning signal. For checking the efficiency of this project, three different situations were installed with this system. Generally, with the promising outcome, this application can be used in maintaining, supervising, and controlling the safety of a worker.
KW  - Haar–cascade algorithm
KW  - safety equipment
KW  - safety management system
DO  - 10.3390/electronics8101079
TY  - EJOU
AU  - Li, Yuxia
AU  - Peng, Bo
AU  - He, Lei
AU  - Fan, Kunlong
AU  - Li, Zhenxu
AU  - Tong, Ling
TI  - Road Extraction from Unmanned Aerial Vehicle Remote Sensing Images Based on Improved Neural Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 19
SN  - 1424-8220

AB  - Roads are vital components of infrastructure, the extraction of which has become a topic of significant interest in the field of remote sensing. Because deep learning has been a popular method in image processing and information extraction, researchers have paid more attention to extracting road using neural networks. This article proposes the improvement of neural networks to extract roads from Unmanned Aerial Vehicle (UAV) remote sensing images. D-Linknet was first considered for its high performance; however, the huge scale of the net reduced computational efficiency. With a focus on the low computational efficiency problem of the popular D-LinkNet, this article made some improvements: (1) Replace the initial block with a stem block. (2) Rebuild the entire network based on ResNet units with a new structure, allowing for the construction of an improved neural network D-Linknetplus. (3) Add a 1 &times; 1 convolution layer before DBlock to reduce the input feature maps, reducing parameters and improving computational efficiency. Add another 1 &times; 1 convolution layer after DBlock to recover the required number of output channels. Accordingly, another improved neural network B-D-LinknetPlus was built. Comparisons were performed between the neural nets, and the verification were made with the Massachusetts Roads Dataset. The results show improved neural networks are helpful in reducing the network size and developing the precision needed for road extraction.
KW  - road
KW  - UAV sensors
KW  - image processing
KW  - convolutional neural net
DO  - 10.3390/s19194115
TY  - EJOU
AU  - Jiao, Leilei
AU  - Sun, Weiwei
AU  - Yang, Gang
AU  - Ren, Guangbo
AU  - Liu, Yinnian
TI  - A Hierarchical Classification Framework of Satellite Multispectral/Hyperspectral Images for Mapping Coastal Wetlands
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 19
SN  - 2072-4292

AB  - Mapping different land cover types with satellite remote sensing data is significant for restoring and protecting natural resources and ecological services in coastal wetlands. In this paper, we propose a hierarchical classification framework (HCF) that implements two levels of classification scheme to identify different land cover types of coastal wetlands. The first level utilizes the designed decision tree to roughly group land covers into four rough classes and the second level combines multiple features (i.e., spectral feature, texture feature and geometric feature) of each class to distinguish different subtypes of land covers in each rough class. Two groups of classification experiments on Landsat and Sentinel multispectral data and China Gaofen (GF)-5 hyperspectral data are carried out in order to testify the classification behaviors of two famous coastal wetlands of China, that is, Yellow River Estuary and Yancheng coastal wetland. Experimental results on Landsat data show that the proposed HCF performs better than support vector machine and random forest in classifying land covers of coastal wetlands. Moreover, HCF is suitable for both multispectral data and hyperspectral data and the GF-5 data is superior to Landsat-8 and Sentinel-2 multispectral data in obtaining fine classification results of coastal wetlands.
KW  - hierarchical classification framework
KW  - coastal wetlands
KW  - Landsat images
KW  - hyperspectral imagery
KW  - Sentinel-2
KW  - GF-5
DO  - 10.3390/rs11192238
TY  - EJOU
AU  - Hassler, Samuel C.
AU  - Baysal-Gurel, Fulya
TI  - Unmanned Aircraft System (UAS) Technology and Applications in Agriculture
T2  - Agronomy

PY  - 2019
VL  - 9
IS  - 10
SN  - 2073-4395

AB  - Numerous sensors have been developed over time for precision agriculture; though, only recently have these sensors been incorporated into the new realm of unmanned aircraft systems (UAS). This UAS technology has allowed for a more integrated and optimized approach to various farming tasks such as field mapping, plant stress detection, biomass estimation, weed management, inventory counting, and chemical spraying, among others. These systems can be highly specialized depending on the particular goals of the researcher or farmer, yet many aspects of UAS are similar. All systems require an underlying platform&mdash;or unmanned aerial vehicle (UAV)&mdash;and one or more peripherals and sensing equipment such as imaging devices (RGB, multispectral, hyperspectral, near infra-red, RGB depth), gripping tools, or spraying equipment. Along with these wide-ranging peripherals and sensing equipment comes a great deal of data processing. Common tools to aid in this processing include vegetation indices, point clouds, machine learning models, and statistical methods. With any emerging technology, there are also a few considerations that need to be analyzed like legal constraints, economic trade-offs, and ease of use. This review then concludes with a discussion on the pros and cons of this technology, along with a brief outlook into future areas of research regarding UAS technology in agriculture.
KW  - unmanned aircraft system (UAS)
KW  - unmanned aerial vehicle (UAV)
KW  - precision agriculture
KW  - remote sensing
KW  - aerial imaging
DO  - 10.3390/agronomy9100618
TY  - EJOU
AU  - Zhang, Jianyong
AU  - Zhao, Yanling
AU  - Abbott, A. L.
AU  - Wynne, Randolph H.
AU  - Hu, Zhenqi
AU  - Zou, Yuzhu
AU  - Tian, Shuaishuai
TI  - Automated Mapping of Typical Cropland Strips in the North China Plain Using Small Unmanned Aircraft Systems (sUAS) Photogrammetry
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - Accurate mapping of agricultural fields is needed for many purposes, including irrigation decisions and cadastral management. This paper is concerned with the automated mapping of cropland strips that are common in the North China Plain. These strips are commonly 3&ndash;8 m in width and 50&ndash;300 m in length, and are separated by small ridges that assist with irrigation. Conventional surveying methods are labor-intensive and time-consuming for this application, and only limited performance is possible with very high resolution satellite images. Small Unmanned Aircraft System (sUAS) images could provide an alternative approach to ridge detection and strip mapping. This paper presents a novel method for detecting cropland strips, utilizing centimeter spatial resolution imagery captured by sUAS flying at low altitude (60 m). Using digital surface models (DSM) and ortho-rectified imagery from sUAS data, this method extracts candidate ridge locations by surface roughness segmentation in combination with geometric constraints. This method then exploits vegetation removal and morphological operations to refine candidate ridge elements, leading to polyline-based representations of cropland strip boundaries. This procedure has been tested using sUAS data from four typical cropland plots located approximately 60 km west of Jinan, China. The plots contained early winter wheat. The results indicated an ability to detect ridges with comparatively high recall and precision (96.8% and 95.4%, respectively). Cropland strips were extracted with over 98.9% agreement relative to ground truth, with kappa coefficients over 97.4%. To our knowledge, this method is the first to attempt cropland strip mapping using centimeter spatial resolution sUAS images. These results have demonstrated that sUAS mapping is a viable approach for data collection to assist in agricultural land management in the North China Plain.
KW  - automated extraction
KW  - ridge detection
KW  - strip mapping
KW  - small unmanned aircraft systems (sUAS)
KW  - surface roughness
KW  - North China Plain
DO  - 10.3390/rs11202343
TY  - EJOU
AU  - Zhang, Dongyan
AU  - Wang, Daoyong
AU  - Gu, Chunyan
AU  - Jin, Ning
AU  - Zhao, Haitao
AU  - Chen, Gao
AU  - Liang, Hongyi
AU  - Liang, Dong
TI  - Using Neural Network to Identify the Severity of Wheat Fusarium Head Blight in the Field Environment
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - Fusarium head blight (FHB), one of the most important diseases of wheat, mainly occurs in the ear. Given that the severity of the disease cannot be accurately identified, the cost of pesticide application increases every year, and the agricultural ecological environment is also polluted. In this study, a neural network (NN) method was proposed based on the red-green-blue (RGB) image to segment wheat ear and disease spot in the field environment, and then to determine the disease grade. Firstly, a segmentation dataset of single wheat ear was constructed to provide a benchmark for the segmentation of the wheat ear. Secondly, a segmentation model of single wheat ear based on the fully convolutional network (FCN) was established to effectively realize the segmentation of the wheat ear in the field environment. An FHB segmentation algorithm was proposed based on a pulse-coupled neural network (PCNN) with K-means clustering of the improved artificial bee colony (IABC) to segment the diseased spot of wheat ear by automatic optimization of PCNN parameters. Finally, the disease grade was calculated using the ratio of the disease spot to the whole wheat ear. The experimental results show that: (1) the accuracy of the segmentation model for single wheat ear constructed in this study is 0.981. The segmentation time is less than 1 s, indicating that the model can quickly and accurately segment wheat ear in the field environment; (2) the segmentation method of the disease spot performed under each evaluation indicator is improved compared with the traditional segmentation methods, and the accuracy is 0.925 in the disease severity identification. These research results can provide important reference value for grading wheat FHB in the field environment, which also can be beneficial for real-time monitoring of other crops&rsquo; diseases under near-Earth remote sensing.
KW  - Fusarium head blight
KW  - fully convolutional network
KW  - pulse coupled neural network
KW  - artificial bee colony
KW  - disease grading
DO  - 10.3390/rs11202375
TY  - EJOU
AU  - Shuai, Guanyuan
AU  - Martinez-Feria, Rafael A.
AU  - Zhang, Jinshui
AU  - Li, Shiming
AU  - Price, Richard
AU  - Basso, Bruno
TI  - Capturing Maize Stand Heterogeneity Across Yield-Stability Zones Using Unmanned Aerial Vehicles (UAV)
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 20
SN  - 1424-8220

AB  - Despite the new equipment capabilities, uneven crop stands are still common occurrences in crop fields, mainly due to spatial heterogeneity in soil conditions, seedling mortality due to herbivore predation and disease, or human error. Non-uniform plant stands may reduce grain yield in crops like maize. Thus, detecting signs of variability in crop stand density early in the season provides critical information for management decisions and crop yield forecasts. Processing techniques applied on images captured by unmanned aerial vehicles (UAVs) has been used successfully to identify crop rows and estimate stand density and, most recently, to estimate plant-to-plant interval distance. Here, we further test and apply an image processing algorithm on UAV images collected from yield-stability zones in a commercial crop field. Our objective was to implement the algorithm to compare variation of plant-spacing intervals to test whether yield differences within these zones are related to differences in crop stand characteristics. Our analysis indicates that the algorithm can be reliably used to estimate plant counts (precision &gt;95% and recall &gt;97%) and plant distance interval (R2 ~0.9 and relative error &lt;10%). Analysis of the collected data indicated that plant spacing variability differences were small among plots with large yield differences, suggesting that it was not a major cause of yield variability across zones with distinct yield history. This analysis provides an example of how plant-detection algorithms can be applied to improve the understanding of patterns of spatial and temporal yield variability.
KW  - UAV
KW  - plant detection
KW  - distance estimation
KW  - field experiments
KW  - yield stability
DO  - 10.3390/s19204446
TY  - EJOU
AU  - García Rubio, Víctor
AU  - Rodrigo Ferrán, Juan A.
AU  - Menéndez García, Jose M.
AU  - Sánchez Almodóvar, Nuria
AU  - Lalueza Mayordomo, José M.
AU  - Álvarez, Federico
TI  - Automatic Change Detection System over Unmanned Aerial Vehicle Video Sequences Based on Convolutional Neural Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 20
SN  - 1424-8220

AB  - In recent years, the use of unmanned aerial vehicles (UAVs) for surveillance tasks has increased considerably. This technology provides a versatile and innovative approach to the field. However, the automation of tasks such as object recognition or change detection usually requires image processing techniques. In this paper we present a system for change detection in video sequences acquired by moving cameras. It is based on the combination of image alignment techniques with a deep learning model based on convolutional neural networks (CNNs). This approach covers two important topics. Firstly, the capability of our system to be adaptable to variations in the UAV flight. In particular, the difference of height between flights, and a slight modification of the camera&rsquo;s position or movement of the UAV because of natural conditions such as the effect of wind. These modifications can be produced by multiple factors, such as weather conditions, security requirements or human errors. Secondly, the precision of our model to detect changes in diverse environments, which has been compared with state-of-the-art methods in change detection. This has been measured using the Change Detection 2014 dataset, which provides a selection of labelled images from different scenarios for training change detection algorithms. We have used images from dynamic background, intermittent object motion and bad weather sections. These sections have been selected to test our algorithm&rsquo;s robustness to changes in the background, as in real flight conditions. Our system provides a precise solution for these scenarios, as the mean F-measure score from the image analysis surpasses 97%, and a significant precision in the intermittent object motion category, where the score is above 99%.
KW  - change detection
KW  - convolutional neural networks
KW  - moving camera
KW  - image alignment
KW  - UAV
DO  - 10.3390/s19204484
TY  - EJOU
AU  - Freimuth, Henk
AU  - König, Markus
TI  - A Framework for Automated Acquisition and Processing of As-Built Data with Autonomous Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 20
SN  - 1424-8220

AB  - Planning and scheduling in construction heavily depend on current information about the state of construction processes. However, the acquisition process for visual data requires human personnel to take photographs of construction objects. We propose using unmanned aerial vehicle (UAVs) for automated creation of images and point cloud data of particular construction objects. The method extracts locations of objects that require inspection from Four Dimensional Building Information Modelling (4D-BIM). With this information at hand viable flight missions around the known structures of the construction site are computed. During flight, the UAV uses stereo cameras to detect and avoid any obstacles that are not known to the model, for example moving humans or machinery. The combination of pre-computed waypoint missions and reactive avoidance ensures deterministic routing from takeoff to landing and operational safety for humans and machines. During flight, an additional software component compares the captured point cloud data with the model data, enabling automatic per-object completion checking or reconstruction. The prototype is developed in the Robot Operating System (ROS) and evaluated in Software-In-The-Loop (SITL) simulations for the sake of being executable on real UAVs.
KW  - as-built
KW  - data acquisition
KW  - BIM
KW  - UAV
KW  - point cloud
KW  - octree
KW  - ROS
KW  - depth camera
KW  - obstacle avoidance
DO  - 10.3390/s19204513
TY  - EJOU
AU  - Mesas-Carrascosa, Francisco J.
AU  - Pérez Porras, Fernando
AU  - Triviño-Tarradas, Paula
AU  - Meroño de Larriva, Jose E.
AU  - García-Ferrer, Alfonso
TI  - Project-Based Learning Applied to Unmanned Aerial Systems and Remote Sensing
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - The development of unmanned aerial vehicle (UAV) technology and the miniaturization of sensors have changed the way remote sensing (RS) is used, popularizing this geoscientific discipline in other fields, such as precision agriculture. This makes it necessary to implement the use of these technologies in teaching RS alongside the classical platforms (satellite and manned aircraft). This manuscript describes how The Higher Technical School of Agricultural Engineering at the University of C&oacute;rdoba (Spain) has introduced UAV RS into the academic program by way of project-based learning (PBL). It also presents the basic characteristics of PBL, the design of the subject, the description of the teacher-guided and self-directed activities, as well as the degree of student satisfaction. The teaching and learning objectives of the subject are to learn how to determine the vigor, temperature, and water stress of a crop through the use of RGB, multispectral, and thermographic sensors onboard a UAV platform. From the onset, students are motivated, actively participate in the tasks related to the realization of UAV flights, and subsequent processing and analysis of the registered images. Students report that PBL is more engaging and allows them to develop a better understanding of RS.
KW  - educational assessment
KW  - motivation
KW  - unmanned aerial vehicle
KW  - agriculture
DO  - 10.3390/rs11202413
TY  - EJOU
AU  - Woodget, Amy S.
AU  - Dietrich, James T.
AU  - Wilson, Robin T.
TI  - Quantifying Below-Water Fluvial Geomorphic Change: The Implications of Refraction Correction, Water Surface Elevations, and Spatially Variable Error
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - Much of the geomorphic work of rivers occurs underwater. As a result, high resolutionquantification of geomorphic change in these submerged areas is important. Currently, to quantify thischange, multiple methods are required to get high resolution data for both the exposed and submergedareas. Remote sensing methods are often limited to the exposed areas due to the challenges imposedby the water, and those remote sensing methods for below the water surface require the collection ofextensive calibration data in-channel, which is time-consuming, labour-intensive, and sometimesprohibitive in dicult-to-access areas. Within this paper, we pioneer a novel approach for quantifyingabove- and below-water geomorphic change using Structure-from-Motion photogrammetry andinvestigate the implications of water surface elevations, refraction correction measures, and thespatial variability of topographic errors. We use two epochs of imagery from a site on the River Teme,Herefordshire, UK, collected using a remotely piloted aircraft system (RPAS) and processed usingStructure-from-Motion (SfM) photogrammetry. For the first time, we show that: (1) Quantification ofsubmerged geomorphic change to levels of accuracy commensurate with exposed areas is possiblewithout the need for calibration data or a dierent method from exposed areas; (2) there is minimaldierence in results produced by dierent refraction correction procedures using predominantlynadir imagery (small angle vs. multi-view), allowing users a choice of software packages/processingcomplexity; (3) improvements to our estimations of water surface elevations are critical for accuratetopographic estimation in submerged areas and can reduce mean elevation error by up to 73%;and (4) we can use machine learning, in the form of multiple linear regressions, and a Gaussian Na&iuml;veBayes classifier, based on the relationship between error and 11 independent variables, to generate ahigh resolution, spatially continuous model of geomorphic change in submerged areas, constrained byspatially variable error estimates. Our multiple regression model is capable of explaining up to 54%of magnitude and direction of topographic error, with accuracies of less than 0.04 m. With on-goingtesting and improvements, this machine learning approach has potential for routine application inspatially variable error estimation within the RPAS&ndash;SfM workflow.
KW  - fluvial
KW  - geomorphology
KW  - change detection
KW  - remotely piloted aircraft system
KW  - refraction correction
KW  - structure-from-motion photogrammetry
KW  - water surface elevation
KW  - topographic error
KW  - machine learning
DO  - 10.3390/rs11202415
TY  - EJOU
AU  - Kwak, Jeonghoon
AU  - Sung, Yunsick
TI  - End-To-End Controls Using K-Means Algorithm for 360-Degree Video Control Method on Omnidirectional Camera-Equipped Autonomous Micro Unmanned Aircraft Systems
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 20
SN  - 2076-3417

AB  - Micro unmanned aircraft systems (micro UAS)-related technical research is important because micro UAS has the advantage of being able to perform missions remotely. When an omnidirectional camera is mounted, it captures all surrounding areas of the micro UAS. Normal field of view (NFoV) refers to a view presented as an image to a user in a 360-degree video. The 360-degree video is controlled using an end-to-end controls method to automatically provide the user with NFoVs without the user controlling the 360-degree video. When using the end-to-end controls method that controls 360-degree video, if there are various signals that control the 360-degree video, the training of the deep learning model requires a considerable amount of training data. Therefore, there is a need for a method of autonomously determining the signals to reduce the number of signals for controlling the 360-degree video. This paper proposes a method to autonomously determine the output to be used for end-to-end control-based deep learning model to control 360-degree video for micro UAS controllers. The output of the deep learning model to control 360-degree video is automatically determined using the K-means algorithm. Using a trained deep learning model, the user is presented with NFoVs in a 360-degree video. The proposed method was experimentally verified by providing NFoVs wherein the signals that control the 360-degree video were set by the proposed method and by user definition. The results of training the convolution neural network (CNN) model using the signals to provide NFoVs were compared, and the proposed method provided NFoVs similar to NFoVs of existing user with 24.4% more similarity compared to a user-defined approach.
KW  - micro unmanned aircraft systems
KW  - surveillance
KW  - 360-degree videos
KW  - deep learning
KW  - normal field of view
KW  - end-to-end controls
DO  - 10.3390/app9204431
TY  - EJOU
AU  - Ghaffarian, Saman
AU  - Kerle, Norman
AU  - Pasolli, Edoardo
AU  - Jokar Arsanjani, Jamal
TI  - Post-Disaster Building Database Updating Using Automated Deep Learning: An Integration of Pre-Disaster OpenStreetMap and Multi-Temporal Satellite Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - First responders and recovery planners need accurate and quickly derived information about the status of buildings as well as newly built ones to both help victims and to make decisions for reconstruction processes after a disaster. Deep learning and, in particular, convolutional neural network (CNN)-based approaches have recently become state-of-the-art methods to extract information from remote sensing images, in particular for image-based structural damage assessment. However, they are predominantly based on manually extracted training samples. In the present study, we use pre-disaster OpenStreetMap building data to automatically generate training samples to train the proposed deep learning approach after the co-registration of the map and the satellite images. The proposed deep learning framework is based on the U-net design with residual connections, which has been shown to be an effective method to increase the efficiency of CNN-based models. The ResUnet is followed by a Conditional Random Field (CRF) implementation to further refine the results. Experimental analysis was carried out on selected very high resolution (VHR) satellite images representing various scenarios after the 2013 Super Typhoon Haiyan in both the damage and the recovery phases in Tacloban, the Philippines. The results show the robustness of the proposed ResUnet-CRF framework in updating the building map after a disaster for both damage and recovery situations by producing an overall F1-score of 84.2%.
KW  - post-disaster
KW  - building database update
KW  - damage assessment
KW  - recovery assessment
KW  - OpenStreetMap
KW  - deep learning
KW  - convolutional neural network
KW  - multi-temporal satellite imagery
KW  - U-Net
KW  - Super Typhoon Haiyan
DO  - 10.3390/rs11202427
TY  - EJOU
AU  - Almadhoun, Randa
AU  - Abduldayem, Abdullah
AU  - Taha, Tarek
AU  - Seneviratne, Lakmal
AU  - Zweiri, Yahya
TI  - Guided Next Best View for 3D Reconstruction of Large Complex Structures
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - In this paper, a Next Best View (NBV) approach with a profiling stage and a novel utility function for 3D reconstruction using an Unmanned Aerial Vehicle (UAV) is proposed. The proposed approach performs an initial scan in order to build a rough model of the structure that is later used to improve coverage completeness and reduce flight time. Then, a more thorough NBV process is initiated, utilizing the rough model in order to create a dense 3D reconstruction of the structure of interest. The proposed approach exploits the reflectional symmetry feature if it exists in the initial scan of the structure. The proposed NBV approach is implemented with a novel utility function, which consists of four main components: information theory, model density, traveled distance, and predictive measures based on symmetries in the structure. This system outperforms classic information gain approaches with a higher density, entropy reduction and coverage completeness. Simulated and real experiments were conducted and the results show the effectiveness and applicability of the proposed approach.
KW  - autonomous exploration
KW  - coverage planning
KW  - aerial robots
KW  - UAV
KW  - view planning
KW  - navigation
KW  - 3D reconstruction
DO  - 10.3390/rs11202440
TY  - EJOU
AU  - He, Zhi
AU  - He, Dan
AU  - Mei, Xiangqin
AU  - Hu, Saihan
TI  - Wetland Classification Based on a New Efficient Generative Adversarial Network and Jilin-1 Satellite Image
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 20
SN  - 2072-4292

AB  - Recent studies have shown that deep learning methods provide useful tools for wetland classification. However, it is difficult to perform species-level classification with limited labeled samples. In this paper, we propose a semi-supervised method for wetland species classification by using a new efficient generative adversarial network (GAN) and Jilin-1 satellite image. The main contributions of this paper are twofold. First, the proposed method, namely ShuffleGAN, requires only a small number of labeled samples. ShuffleGAN is composed of two neural networks (i.e., generator and discriminator), which perform an adversarial game in the training phase and ShuffleNet units are added in both generator and discriminator to obtain speed-accuracy tradeoff. Second, ShuffleGAN can perform species-level wetland classification. In addition to distinguishing the wetland areas from non-wetlands, different tree species located in the wetland are also identified, thus providing a more detailed distribution of the wetland land-covers. Experiments are conducted on the Haizhu Lake wetland data acquired by the Jilin-1 satellite. Compared with existing GAN, the improvement in overall accuracy (OA) of the proposed ShuffleGAN is more than 2%. This work can not only deepen the application of deep learning in wetland classification but also promote the study of fine classification of wetland land-covers.
KW  - wetland
KW  - classification
KW  - remote sensing
KW  - deep learning
KW  - generative adversarial networks
DO  - 10.3390/rs11202455
TY  - EJOU
AU  - Bohnenkamp, David
AU  - Behmann, Jan
AU  - Mahlein, Anne-Katrin
TI  - In-Field Detection of Yellow Rust in Wheat on the Ground Canopy and UAV Scale
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - The application of hyperspectral imaging technology for plant disease detection in the field is still challenging. Existing equipment and analysis algorithms are adapted to highly controlled environmental conditions in the laboratory. However, only real time information from the field scale is able to guide plant protection measures and to optimize the use of resources. At the field scale, many parameters such as the optimal measurement distance, informative feature sets, and suitable algorithms have not been investigated. In this study, the hyperspectral detection and quantification of yellow rust in wheat was evaluated using two measurement platforms: a ground-based vehicle and an unmanned aerial vehicle (UAV). Different disease development stages and disease severities were provided in a plot-based field experiment. Measurements were performed weekly during the vegetation period. Data analysis was performed by three prediction algorithms with a focus on the selection of optimal feature sets. In this context, the across-scale application of optimized feature sets, an approach of information transfer between scales, was also evaluated. Relevant aspects for an on-line disease assessment in the field integrating affordable sensor technology, sensor spatial resolution, compact analysis models, and fast evaluation have been outlined and reflected upon. For the first time, a hyperspectral imaging observation experiment of a plant disease was comparatively performed at two scales, ground canopy and UAV.
KW  - feature selection
KW  - spectral angle mapper
KW  - support vector machine
KW  - support vector regression
KW  - hyperspectral imaging
KW  - UAV
KW  - cross-scale
KW  - yellow rust
KW  - spatial resolution
KW  - winter wheat
DO  - 10.3390/rs11212495
TY  - EJOU
AU  - Xin, Jiang
AU  - Zhang, Xinchang
AU  - Zhang, Zhiqiang
AU  - Fang, Wu
TI  - Road Extraction of High-Resolution Remote Sensing Images Derived from DenseUNet
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - Road network extraction is one of the significant assignments for disaster emergency response, intelligent transportation systems, and real-time updating road network. Road extraction base on high-resolution remote sensing images has become a hot topic. Presently, most of the researches are based on traditional machine learning algorithms, which are complex and computational because of impervious surfaces such as roads and buildings that are discernible in the images. Given the above problems, we propose a new method to extract the road network from remote sensing images using a DenseUNet model with few parameters and robust characteristics. DenseUNet consists of dense connection units and skips connections, which strengthens the fusion of different scales by connections at various network layers. The performance of the advanced method is validated on two datasets of high-resolution images by comparison with three classical semantic segmentation methods. The experimental results show that the method can be used for road extraction in complex scenes.
KW  - high-resolution remote sensing imagery
KW  - multi-scale
KW  - road extraction
KW  - machine learning
KW  - DenseUNet
DO  - 10.3390/rs11212499
TY  - EJOU
AU  - Crommelinck, Sophie
AU  - Koeva, Mila
AU  - Yang, Michael Y.
AU  - Vosselman, George
TI  - Application of Deep Learning for Delineation of Visible Cadastral Boundaries from Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - Cadastral boundaries are often demarcated by objects that are visible in remote sensing imagery. Indirect surveying relies on the delineation of visible parcel boundaries from such images. Despite advances in automated detection and localization of objects from images, indirect surveying is rarely automated and relies on manual on-screen delineation. We have previously introduced a boundary delineation workflow, comprising image segmentation, boundary classification and interactive delineation that we applied on Unmanned Aerial Vehicle (UAV) data to delineate roads. In this study, we improve each of these steps. For image segmentation, we remove the need to reduce the image resolution and we limit over-segmentation by reducing the number of segment lines by 80% through filtering. For boundary classification, we show how Convolutional Neural Networks (CNN) can be used for boundary line classification, thereby eliminating the previous need for Random Forest (RF) feature generation and thus achieving 71% accuracy. For interactive delineation, we develop additional and more intuitive delineation functionalities that cover more application cases. We test our approach on more varied and larger data sets by applying it to UAV and aerial imagery of 0.02&ndash;0.25 m resolution from Kenya, Rwanda and Ethiopia. We show that it is more effective in terms of clicks and time compared to manual delineation for parcels surrounded by visible boundaries. Strongest advantages are obtained for rural scenes delineated from aerial imagery, where the delineation effort per parcel requires 38% less time and 80% fewer clicks compared to manual delineation.
KW  - cadastral mapping
KW  - indirect surveying
KW  - boundary extraction
KW  - boundary delineation
KW  - machine learning
KW  - deep learning
KW  - image analysis
KW  - CNN
KW  - RF
DO  - 10.3390/rs11212505
TY  - EJOU
AU  - Kerle, Norman
AU  - Ghaffarian, Saman
AU  - Nawrotzki, Raphael
AU  - Leppert, Gerald
AU  - Lech, Malte
TI  - Evaluating Resilience-Centered Development Interventions with Remote Sensing
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - Natural disasters are projected to increase in number and severity, in part due to climate change. At the same time a growing number of disaster risk reduction (DRR) and climate change adaptation measures are being implemented by governmental and non-governmental organizations, and substantial post-disaster donations are frequently pledged. At the same time there has been increasing demand for transparency and accountability, and thus evidence of those measures having a positive effect. We hypothesized that resilience-enhancing interventions should result in less damage during a hazard event, or at least quicker recovery. In this study we assessed recovery over a 3 year period of seven municipalities in the central Philippines devastated by Typhoon Haiyan in 2013. We used very high resolution optical images (&lt;1 m), and created detailed land cover and land use maps for four epochs before and after the event, using a machine learning approach with extreme gradient boosting. The spatially and temporally highly variable recovery maps were then statistically related to detailed questionnaire data acquired by DEval in 2012 and 2016, whose principal aim was to assess the impact of a 10 year land-planning intervention program by the German agency for technical cooperation (GIZ). The survey data allowed very detailed insights into DRR-related perspectives, motivations and drivers of the affected population. To some extent they also helped to overcome the principal limitation of remote sensing, which can effectively describe but not explain the reasons for differential recovery. However, while a number of causal links between intervention parameters and reconstruction was found, the common notion that a resilient community should recover better and more quickly could not be confirmed. The study also revealed a number of methodological limitations, such as the high cost for commercial image data not matching the spatially extensive but also detailed scale of field evaluations, the remote sensing analysis likely overestimating damage and thus providing incorrect recovery metrics, and image data catalogues especially for more remote communities often being incomplete. Nevertheless, the study provides a valuable proof of concept for the synergies resulting from an integration of socio-economic survey data and remote sensing imagery for recovery assessment.
KW  - disaster
KW  - resilience
KW  - impact
KW  - evaluation
KW  - Philippines
KW  - Haiyan
KW  - machine learning
KW  - gradient boosting
KW  - land use planning
KW  - German development cooperation
DO  - 10.3390/rs11212511
TY  - EJOU
AU  - Xia, Wei
AU  - Ma, Caihong
AU  - Liu, Jianbo
AU  - Liu, Shibin
AU  - Chen, Fu
AU  - Yang, Zhi
AU  - Duan, Jianbo
TI  - High-Resolution Remote Sensing Imagery Classification of Imbalanced Data Using Multistage Sampling Method and Deep Neural Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - Class imbalance is a key issue for the application of deep learning for remote sensing image classification because a model generated by imbalanced samples training has low classification accuracy for minority classes. In this study, an accurate classification approach using the multistage sampling method and deep neural networks was proposed to classify imbalanced data. We first balance samples by multistage sampling to obtain the training sets. Then, a state-of-the-art model is adopted by combining the advantages of atrous spatial pyramid pooling (ASPP) and Encoder-Decoder for pixel-wise classification, which are two different types of fully convolutional networks (FCNs) that can obtain contextual information of multiple levels in the Encoder stage. The details and spatial dimensions of targets are restored using such information during the Decoder stage. We employ four deep learning-based classification algorithms (basic FCN, FCN-8S, ASPP, and Encoder-Decoder with ASPP of our approach) on multistage training sets (original, MUS1, and MUS2) of WorldView-3 images in southeastern Qinghai-Tibet Plateau and GF-2 images in northeastern Beijing for comparison. The experiments show that, compared with existing sets (original, MUS1, and identical) and existing method (cost weighting), the MUS2 training set of multistage sampling significantly enhance the classification performance for minority classes. Our approach shows distinct advantages for imbalanced data.
KW  - high-resolution remote sensing image
KW  - classification
KW  - deep learning
KW  - imbalanced data
KW  - multistage sampling
KW  - ASPP
KW  - encoder-decoder
DO  - 10.3390/rs11212523
TY  - EJOU
AU  - Tavakkoli Piralilou, Sepideh
AU  - Shahabi, Hejar
AU  - Jarihani, Ben
AU  - Ghorbanzadeh, Omid
AU  - Blaschke, Thomas
AU  - Gholamnia, Khalil
AU  - Meena, Sansar R.
AU  - Aryal, Jagannath
TI  - Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - Landslides represent a severe hazard in many areas of the world. Accurate landslide maps are needed to document the occurrence and extent of landslides and to investigate their distribution, types, and the pattern of slope failures. Landslide maps are also crucial for determining landslide susceptibility and risk. Satellite data have been widely used for such investigations—next to data from airborne or unmanned aerial vehicle (UAV)-borne campaigns and Digital Elevation Models (DEMs). We have developed a methodology that incorporates object-based image analysis (OBIA) with three machine learning (ML) methods, namely, the multilayer perceptron neural network (MLP-NN) and random forest (RF), for landslide detection. We identified the optimal scale parameters (SP) and used them for multi-scale segmentation and further analysis. We evaluated the resulting objects using the object pureness index (OPI), object matching index (OMI), and object fitness index (OFI) measures. We then applied two different methods to optimize the landslide detection task: (a) an ensemble method of stacking that combines the different ML methods for improving the performance, and (b) Dempster–Shafer theory (DST), to combine the multi-scale segmentation and classification results. Through the combination of three ML methods and the multi-scale approach, the framework enhanced landslide detection when it was tested for detecting earthquake-triggered landslides in Rasuwa district, Nepal. PlanetScope optical satellite images and a DEM were used, along with the derived landslide conditioning factors. Different accuracy assessment measures were used to compare the results against a field-based landslide inventory. All ML methods yielded the highest overall accuracies ranging from 83.3% to 87.2% when using objects with the optimal SP compared to other SPs. However, applying DST to combine the multi-scale results of each ML method significantly increased the overall accuracies to almost 90%. Overall, the integration of OBIA with ML methods resulted in appropriate landslide detections, but using the optimal SP and ML method is crucial for success.
KW  - landslide mapping
KW  - object-based image analysis (OBIA)
KW  - scale parameter (SP)
KW  - Dempster–Shafer theory (DST)
KW  - Planetscope
DO  - 10.3390/rs11212575
TY  - EJOU
AU  - Rodriguez-Ramos, Alejandro
AU  - Alvarez-Fernandez, Adrian
AU  - Bavle, Hriday
AU  - Campoy, Pascual
AU  - How, Jonathan P.
TI  - Vision-Based Multirotor Following Using Synthetic Learning Techniques
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 21
SN  - 1424-8220

AB  - Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights).
KW  - multirotor
KW  - UAV
KW  - following
KW  - synthetic learning
KW  - reinforcement learning
KW  - deep learning
DO  - 10.3390/s19214794
TY  - EJOU
AU  - Fromm, Michael
AU  - Schubert, Matthias
AU  - Castilla, Guillermo
AU  - Linke, Julia
AU  - McDermid, Greg
TI  - Automated Detection of Conifer Seedlings in Drone Imagery Using Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 21
SN  - 2072-4292

AB  - Monitoring tree regeneration in forest areas disturbed by resource extraction is a requirement for sustainably managing the boreal forest of Alberta, Canada. Small remotely piloted aircraft systems (sRPAS, a.k.a. drones) have the potential to decrease the cost of field surveys drastically, but produce large quantities of data that will require specialized processing techniques. In this study, we explored the possibility of using convolutional neural networks (CNNs) on this data for automatically detecting conifer seedlings along recovering seismic lines: a common legacy footprint from oil and gas exploration. We assessed three different CNN architectures, of which faster region-CNN (R-CNN) performed best (mean average precision 81%). Furthermore, we evaluated the effects of training-set size, season, seedling size, and spatial resolution on the detection performance. Our results indicate that drone imagery analyzed by artificial intelligence can be used to detect conifer seedling in regenerating sites with high accuracy, which increases with the size in pixels of the seedlings. By using a pre-trained network, the size of the training dataset can be reduced to a couple hundred seedlings without any significant loss of accuracy. Furthermore, we show that combining data from different seasons yields the best results. The proposed method is a first step towards automated monitoring of forest restoration/regeneration.
KW  - convolutional neural networks
KW  - forest restoration
KW  - regeneration surveys
KW  - seedling detection
KW  - UAVs
KW  - sRPAS
DO  - 10.3390/rs11212585
TY  - EJOU
AU  - Ham, Sungil
AU  - Im, Junhyuck
AU  - Kim, Minjun
AU  - Cho, Kuk
TI  - Construction and Verification of a High-Precision Base Map for an Autonomous Vehicle Monitoring System
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 11
SN  - 2220-9964

AB  - For autonomous driving, a control system that supports precise road maps is required to monitor the operation status of autonomous vehicles in the research stage. Such a system is also required for research related to automobile engineering, sensors, and artificial intelligence. The design of Google Maps and other map services is limited to the provision of map support at 20 levels of high-resolution precision. An ideal map should include information on roads, autonomous vehicles, and Internet of Things (IOT) facilities that support autonomous driving. The aim of this study was to design a map suitable for the control of autonomous vehicles in Gyeonggi Province in Korea. This work was part of the project &ldquo;Building a Testbed for Pilot Operations of Autonomous Vehicles&rdquo;. The map design scheme was redesigned for an autonomous vehicle control system based on the &ldquo;Easy Map&rdquo; developed by the National Geography Center, which provides free design schema. In addition, a vector-based precision map, including roads, sidewalks, and road markings, was produced to provide content suitable for 20 levels. A hybrid map that combines the vector layer of the road and an unmanned aerial vehicle (UAV) orthographic map was designed to facilitate vehicle identification. A control system that can display vehicle and sensor information based on the designed map was developed, and an environment to monitor the operation of autonomous vehicles was established. Finally, the high-precision map was verified through an accuracy test and driving data from autonomous vehicles.
KW  - high-precision map
KW  - schema design
KW  - autonomous vehicle
DO  - 10.3390/ijgi8110501
TY  - EJOU
AU  - Zhou, Jun
AU  - Tian, Yichen
AU  - Yuan, Chao
AU  - Yin, Kai
AU  - Yang, Guang
AU  - Wen, Meiping
TI  - Improved UAV Opium Poppy Detection Using an Updated YOLOv3 Model
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 22
SN  - 1424-8220

AB  - Rapid detection of illicit opium poppy plants using UAV (unmanned aerial vehicle) imagery has become an important means to prevent and combat crimes related to drug cultivation. However, current methods rely on time-consuming visual image interpretation. Here, the You Only Look Once version 3 (YOLOv3) network structure was used to assess the influence that different backbone networks have on the average precision and detection speed of an UAV-derived dataset of poppy imagery, with MobileNetv2 (MN) selected as the most suitable backbone network. A Spatial Pyramid Pooling (SPP) unit was introduced and Generalized Intersection over Union (GIoU) was used to calculate the coordinate loss. The resulting SPP-GIoU-YOLOv3-MN model improved the average precision by 1.62% (from 94.75% to 96.37%) without decreasing speed and achieved an average precision of 96.37%, with a detection speed of 29 FPS using an RTX 2080Ti platform. The sliding window method was used for detection in complete UAV images, which took approximately 2.2 sec/image, approximately 10&times; faster than visual interpretation. The proposed technique significantly improved the efficiency of poppy detection in UAV images while also maintaining a high detection accuracy. The proposed method is thus suitable for the rapid detection of illicit opium poppy cultivation in residential areas and farmland where UAVs with ordinary visible light cameras can be operated at low altitudes (relative height &lt; 200 m).
KW  - UAV
KW  - opium poppy
KW  - object detection
KW  - YOLOv3 model
KW  - deep learning
KW  - CNN
KW  - spatial pyramid pooling
KW  - GIoU
DO  - 10.3390/s19224851
TY  - EJOU
AU  - Shahabi, Hejar
AU  - Jarihani, Ben
AU  - Tavakkoli Piralilou, Sepideh
AU  - Chittleborough, David
AU  - Avand, Mohammadtaghi
AU  - Ghorbanzadeh, Omid
TI  - A Semi-Automated Object-Based Gully Networks Detection Using Different Machine Learning Models: A Case Study of Bowen Catchment, Queensland, Australia
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 22
SN  - 1424-8220

AB  - Gully erosion is a dominant source of sediment and particulates to the Great Barrier Reef (GBR) World Heritage area. We selected the Bowen catchment, a tributary of the Burdekin Basin, as our area of study; the region is associated with a high density of gully networks. We aimed to use a semi-automated object-based gully networks detection process using a combination of multi-source and multi-scale remote sensing and ground-based data. An advanced approach was employed by integrating geographic object-based image analysis (GEOBIA) with current machine learning (ML) models. These included artificial neural networks (ANN), support vector machines (SVM), and random forests (RF), and an ensemble ML model of stacking to deal with the spatial scaling problem in gully networks detection. Spectral indices such as the normalized difference vegetation index (NDVI) and topographic conditioning factors, such as elevation, slope, aspect, topographic wetness index (TWI), slope length (SL), and curvature, were generated from Sentinel 2A images and the ALOS 12-m digital elevation model (DEM), respectively. For image segmentation, the ESP2 tool was used to obtain three optimal scale factors. On using object pureness index (OPI), object matching index (OMI), and object fitness index (OFI), the accuracy of each scale in image segmentation was evaluated. The scale parameter of 45 with OFI of 0.94, which is a combination of OPI and OMI indices, proved to be the optimal scale parameter for image segmentation. Furthermore, segmented objects based on scale 45 were overlaid with 70% and 30% of a prepared gully inventory map to select the ML models&rsquo; training and testing objects, respectively. The quantitative accuracy assessment methods of Precision, Recall, and an F1 measure were used to evaluate the model&rsquo;s performance. Integration of GEOBIA with the stacking model using a scale of 45 resulted in the highest accuracy in detection of gully networks with an F1 measure value of 0.89. Here, we conclude that the adoption of optimal scale object definition in the GEOBIA and application of the ensemble stacking of ML models resulted in higher accuracy in the detection of gully networks.
KW  - geographic object-based image analysis (GEOBIA)
KW  - gully erosion
KW  - optimal scale detection
KW  - stacking model
KW  - Bowen catchment
DO  - 10.3390/s19224893
TY  - EJOU
AU  - Tian, Furui
AU  - Zhao, Ying
AU  - Che, Xiangqian
AU  - Zhao, Yagebai
AU  - Xin, Dabo
TI  - Concrete Crack Identification and Image Mosaic Based on Image Processing
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 22
SN  - 2076-3417

AB  - Crack assessment is an essential process in bridge detection. In general, most non-contact crack detection techniques are not suitable for widespread use. The reason for this is that they all need to position the ruler at the inspection site in advance or calibrate the camera unit pixel size at a certain distance in a very intricate process. However, the object distance method in this paper can complete the calculation using only the crack image and the working distance, which are provided by an acquisition system equipped with a camera and laser range finder. First, the object distance method and the scale method are compared by calculating the crack width, and the results show that the object distance method is the more accurate method. Then, a double edge pixel statistical method is proposed to calculate the crack length, which solves the problem of redundant and missing pixels. In addition, the conventional mosaic algorithm is improved to realize an image mosaic for the more efficient splicing of crack images. Finally, a series of laboratory tests were conducted to verify the proposed approach. The experiments showed that the precision of crack length extraction can reach 92%, and the improved algorithm stitching precision can reach 98%.
KW  - concrete crack
KW  - edge detection
KW  - image identification
KW  - minimum width extraction
KW  - double edge pixel statistics
KW  - image mosaic algorithm
DO  - 10.3390/app9224826
TY  - EJOU
AU  - Tsouros, Dimosthenis C.
AU  - Bibi, Stamatia
AU  - Sarigiannidis, Panagiotis G.
TI  - A Review on UAV-Based Applications for Precision Agriculture
T2  - Information

PY  - 2019
VL  - 10
IS  - 11
SN  - 2078-2489

AB  - Emerging technologies such as Internet of Things (IoT) can provide significant potential in Smart Farming and Precision Agriculture applications, enabling the acquisition of real-time environmental data. IoT devices such as Unmanned Aerial Vehicles (UAVs) can be exploited in a variety of applications related to crops management, by capturing high spatial and temporal resolution images. These technologies are expected to revolutionize agriculture, enabling decision-making in days instead of weeks, promising significant reduction in cost and increase in the yield. Such decisions enable the effective application of farm inputs, supporting the four pillars of precision agriculture, i.e., apply the right practice, at the right place, at the right time and with the right quantity. However, the actual proliferation and exploitation of UAVs in Smart Farming has not been as robust as expected mainly due to the challenges confronted when selecting and deploying the relevant technologies, including the data acquisition and image processing methods. The main problem is that still there is no standardized workflow for the use of UAVs in such applications, as it is a relatively new area. In this article, we review the most recent applications of UAVs for Precision Agriculture. We discuss the most common applications, the types of UAVs exploited and then we focus on the data acquisition methods and technologies, appointing the benefits and drawbacks of each one. We also point out the most popular processing methods of aerial imagery and discuss the outcomes of each method and the potential applications of each one in the farming operations.
KW  - remote sensing
KW  - IoT
KW  - UAV
KW  - UAS
KW  - Unmanned Aerial Vehicle
KW  - Unmanned Aerial System
KW  - image processing
KW  - Precision Agriculture
KW  - Smart Farming
KW  - review
DO  - 10.3390/info10110349
TY  - EJOU
AU  - Riid, Andri
AU  - Lõuk, Roland
AU  - Pihlak, Rene
AU  - Tepljakov, Aleksei
AU  - Vassiljeva, Kristina
TI  - Pavement Distress Detection with Deep Learning Using the Orthoframes Acquired by a Mobile Mapping System
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 22
SN  - 2076-3417

AB  - The subject matter of this research article is automatic detection of pavement distress on highway roads using computer vision algorithms. Specifically, deep learning convolutional neural network models are employed towards the implementation of the detector. Source data for training the detector come in the form of orthoframes acquired by a mobile mapping system. Compared to our previous work, the orthoframes are generally of better quality, but more importantly, in this work, we introduce a manual preprocessing step: sets of orthoframes are carefully selected for training and manually digitized to ensure adequate performance of the detector. Pretrained convolutional neural networks are then fine-tuned for the problem of pavement distress detection. Corresponding experimental results are provided and analyzed and indicate a successful implementation of the detector.
KW  - pavement distress
KW  - defect detection
KW  - image recognition
KW  - image processing
KW  - deep neural network
DO  - 10.3390/app9224829
TY  - EJOU
AU  - Zhao, Longcai
AU  - Li, Qiangzi
AU  - Zhang, Yuan
AU  - Wang, Hongyan
AU  - Du, Xin
TI  - Integrating the Continuous Wavelet Transform and a Convolutional Neural Network to Identify Vineyard Using Time Series Satellite Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 22
SN  - 2072-4292

AB  - Grape is an economic crop of great importance and is widely cultivated in China. With the development of remote sensing, abundant data sources strongly guarantee that researchers can identify crop types and map their spatial distributions. However, to date, only a few studies have been conducted to identify vineyards using satellite image data. In this study, a vineyard is identified using satellite images, and a new approach is proposed that integrates the continuous wavelet transform (CWT) and a convolutional neural network (CNN). Specifically, the original time series of the normalized difference vegetation index (NDVI), enhanced vegetation index (EVI), and green chlorophyll vegetation index (GCVI) are reconstructed by applying an iterated Savitzky-Golay (S-G) method to form a daily time series for a full year; then, the CWT is applied to three reconstructed time series to generate corresponding scalograms; and finally, CNN technology is used to identify vineyards based on the stacked scalograms. In addition to our approach, a traditional and common approach that uses a random forest (RF) to identify crop types based on multi-temporal images is selected as the control group. The experimental results demonstrated the following: (i) the proposed approach was comprehensively superior to the RF approach; it improved the overall accuracy by 9.87% (up to 89.66%); (ii) the CWT had a stable and effective influence on the reconstructed time series, and the scalograms fully represented the unique time-related frequency pattern of each of the planting conditions; and (iii) the convolution and max pooling processing of the CNN captured the unique and subtle distribution patterns of the scalograms to distinguish vineyards from other crops. Additionally, the proposed approach is considered as able to be applied to other practical scenarios, such as using time series data to identify crop types, map landcover/land use, and is recommended to be tested in future practical applications.
KW  - vineyard
KW  - identification
KW  - CWT
KW  - CNN
KW  - Sentinel-2
KW  - remote sensing
DO  - 10.3390/rs11222641
TY  - EJOU
AU  - Tang, Jun
AU  - Ma, Xiang
AU  - Gu, Ren
AU  - Yang, Zhichao
AU  - Li, Shi
AU  - Yang, Chen
AU  - Yang, Bo
TI  - Cost Consensus Algorithm Applications for EV Charging Station Participating in AGC of Interconnected Power Grid
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 22
SN  - 2076-3417

AB  - In order to more effectively reduce the regulation costs of power grids and to improve the automatic generation control (AGC) performance, an optimization mathematical model of generation command dispatch for AGC with an electric vehicle (EV) charging station is proposed in this paper, in which a cost consensus algorithm for AGC is adopted. Particularly, virtual consensus variables are applied to exchange information among different AGC units. At the same time, the actual consensus variables are utilized to determine the generation command, upon which the flexibility of the proposed algorithm can be significantly enhanced. Furthermore, the implement feasibility of such an algorithm is verified through a series simulation experiments on the Hainan power grid in southern China, where the results demonstrate that the proposed algorithm can effectively realize an autonomous frequency regulation of EVs participating in AGC.
KW  - electric vehicle charging station
KW  - cost consensus algorithm
KW  - automatic generation control
KW  - autonomous frequency regulation
DO  - 10.3390/app9224886
TY  - EJOU
AU  - Shin, Jung-il
AU  - Seo, Won-woo
AU  - Kim, Taejung
AU  - Park, Joowon
AU  - Woo, Choong-shik
TI  - Using UAV Multispectral Images for Classification of Forest Burn Severity—A Case Study of the 2019 Gangneung Forest Fire
T2  - Forests

PY  - 2019
VL  - 10
IS  - 11
SN  - 1999-4907

AB  - Unmanned aerial vehicle (UAV)-based remote sensing has limitations in acquiring images before a forest fire, although burn severity can be analyzed by comparing images before and after a fire. Determining the burned surface area is a challenging class in the analysis of burn area severity because it looks unburned in images from aircraft or satellites. This study analyzes the availability of multispectral UAV images that can be used to classify burn severity, including the burned surface class. RedEdge multispectral UAV image was acquired after a forest fire, which was then processed into a mosaic reflectance image. Hundreds of samples were collected for each burn severity class, and they were used as training and validation samples for classification. Maximum likelihood (MLH), spectral angle mapper (SAM), and thresholding of a normalized difference vegetation index (NDVI) were used as classifiers. In the results, all classifiers showed high overall accuracy. The classifiers also showed high accuracy for classification of the burned surface, even though there was some confusion among spectrally similar classes, unburned pine, and unburned deciduous. Therefore, multispectral UAV images can be used to analyze burn severity after a forest fire. Additionally, NDVI thresholding can also be an easy and accurate method, although thresholds should be generalized in the future.
KW  - UAV
KW  - multispectral image
KW  - forest fire
KW  - burn severity
KW  - classification
DO  - 10.3390/f10111025
TY  - EJOU
AU  - Arshad, Bilal
AU  - Ogie, Robert
AU  - Barthelemy, Johan
AU  - Pradhan, Biswajeet
AU  - Verstaevel, Nicolas
AU  - Perez, Pascal
TI  - Computer Vision and IoT-Based Sensors in Flood Monitoring and Mapping: A Systematic Review
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 22
SN  - 1424-8220

AB  - Floods are amongst the most common and devastating of all natural hazards. The alarming number of flood-related deaths and financial losses suffered annually across the world call for improved response to flood risks. Interestingly, the last decade has presented great opportunities with a series of scholarly activities exploring how camera images and wireless sensor data from Internet-of-Things (IoT) networks can improve flood management. This paper presents a systematic review of the literature regarding IoT-based sensors and computer vision applications in flood monitoring and mapping. The paper contributes by highlighting the main computer vision techniques and IoT sensor approaches utilised in the literature for real-time flood monitoring, flood modelling, mapping and early warning systems including the estimation of water level. The paper further contributes by providing recommendations for future research. In particular, the study recommends ways in which computer vision and IoT sensor techniques can be harnessed to better monitor and manage coastal lagoons&mdash;an aspect that is under-explored in the literature.
KW  - remote sensing
KW  - flood
KW  - disaster management
KW  - coastal
KW  - environmental sensor network (ESN)
KW  - IoT
KW  - drones
KW  - UAV
KW  - computer vision
KW  - wireless sensor network
DO  - 10.3390/s19225012
TY  - EJOU
AU  - Wang, Wantian
AU  - Tang, Ziyue
AU  - Chen, Yichang
AU  - Zhang, Yuanpeng
AU  - Sun, Yongjian
TI  - Aircraft Target Classification for Conventional Narrow-Band Radar with Multi-Wave Gates Sparse Echo Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 22
SN  - 2072-4292

AB  - For a conventional narrow-band radar system, the detectable information of the target is limited, and it is difficult for the radar to accurately identify the target type. In particular, the classification probability will further decrease when part of the echo data is missed. By extracting the target features in time and frequency domains from multi-wave gates sparse echo data, this paper presents a classification algorithm in conventional narrow-band radar to identify three different types of aircraft target, i.e., helicopter, propeller and jet. Firstly, the classical sparse reconstruction algorithm is utilized to reconstruct the target frequency spectrum with single-wave gate sparse echo data. Then, the micro-Doppler effect caused by rotating parts of different targets is analyzed, and the micro-Doppler based features, such as amplitude deviation coefficient, time domain waveform entropy and frequency domain waveform entropy, are extracted from reconstructed echo data to identify targets. Thirdly, the target features extracted from multi-wave gates reconstructed echo data are weighted and fused to improve the accuracy of classification. Finally, the fused feature vectors are fed into a support vector machine (SVM) model for classification. By contrast with the conventional algorithm of aircraft target classification, the proposed algorithm can effectively process sparse echo data and achieve higher classification probability via weighted features fusion of multi-wave gates echo data. The experiments on synthetic data are carried out to validate the effectiveness of the proposed algorithm.
KW  - narrow-band radar
KW  - target classification
KW  - signal reconstruction
KW  - features extraction
KW  - weighted features fusion
DO  - 10.3390/rs11222700
TY  - EJOU
AU  - Sun, Ying
AU  - Huang, Jianfeng
AU  - Ao, Zurui
AU  - Lao, Dazhao
AU  - Xin, Qinchuan
TI  - Deep Learning Approaches for the Mapping of Tree Species Diversity in a Tropical Wetland Using Airborne LiDAR and High-Spatial-Resolution Remote Sensing Images
T2  - Forests

PY  - 2019
VL  - 10
IS  - 11
SN  - 1999-4907

AB  - The monitoring of tree species diversity is important for forest or wetland ecosystem service maintenance or resource management. Remote sensing is an efficient alternative to traditional field work to map tree species diversity over large areas. Previous studies have used light detection and ranging (LiDAR) and imaging spectroscopy (hyperspectral or multispectral remote sensing) for species richness prediction. The recent development of very high spatial resolution (VHR) RGB images has enabled detailed characterization of canopies and forest structures. In this study, we developed a three-step workflow for mapping tree species diversity, the aim of which was to increase knowledge of tree species diversity assessment using deep learning in a tropical wetland (Haizhu Wetland) in South China based on VHR-RGB images and LiDAR points. Firstly, individual trees were detected based on a canopy height model (CHM, derived from LiDAR points) by the local-maxima-based method in the FUSION software (Version 3.70, Seattle, USA). Then, tree species at the individual tree level were identified via a patch-based image input method, which cropped the RGB images into small patches (the individually detected trees) based on the tree apexes detected. Three different deep learning methods (i.e., AlexNet, VGG16, and ResNet50) were modified to classify the tree species, as they can make good use of the spatial context information. Finally, four diversity indices, namely, the Margalef richness index, the Shannon&ndash;Wiener diversity index, the Simpson diversity index, and the Pielou evenness index, were calculated from the fixed subset with a size of 30 &times; 30 m for assessment. In the classification phase, VGG16 had the best performance, with an overall accuracy of 73.25% for 18 tree species. Based on the classification results, mapping of tree species diversity showed reasonable agreement with field survey data (R2Margalef = 0.4562, root-mean-square error RMSEMargalef = 0.5629; R2Shannon&ndash;Wiener = 0.7948, RMSEShannon&ndash;Wiener = 0.7202; R2Simpson = 0.7907, RMSESimpson = 0.1038; and R2Pielou = 0.5875, RMSEPielou = 0.3053). While challenges remain for individual tree detection and species classification, the deep-learning-based solution shows potential for mapping tree species diversity.
KW  - tree species diversity
KW  - tropical wetland
KW  - high-resolution remote sensing images
KW  - LiDAR
KW  - individual tree level
KW  - deep learning
DO  - 10.3390/f10111047
TY  - EJOU
AU  - Bithas, Petros S.
AU  - Michailidis, Emmanouel T.
AU  - Nomikos, Nikolaos
AU  - Vouyioukas, Demosthenes
AU  - Kanatas, Athanasios G.
TI  - A Survey on Machine-Learning Techniques for UAV-Based Communications
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 23
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) will be an integral part of the next generation wireless communication networks. Their adoption in various communication-based applications is expected to improve coverage and spectral efficiency, as compared to traditional ground-based solutions. However, this new degree of freedom that will be included in the network will also add new challenges. In this context, the machine-learning (ML) framework is expected to provide solutions for the various problems that have already been identified when UAVs are used for communication purposes. In this article, we provide a detailed survey of all relevant research works, in which ML techniques have been used on UAV-based communications for improving various design and functional aspects such as channel modeling, resource management, positioning, and security.
KW  - 5G networks
KW  - air-to-ground communications
KW  - machine-learning
KW  - unmanned aerial vehicles (UAVs)
KW  - cellular networks
DO  - 10.3390/s19235170
TY  - EJOU
AU  - Xuan-Mung, Nguyen
AU  - Hong, Sung K.
TI  - Robust Backstepping Trajectory Tracking Control of a Quadrotor with Input Saturation via Extended State Observer
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 23
SN  - 2076-3417

AB  - Quadrotor unmanned aerial vehicles have become increasingly popular in several applications, and the improvement of their control performance has been documented in several studies. Nevertheless, the design of a high-performance tracking controller for aerial vehicles that reliably functions in the simultaneous presence of model uncertainties, external disturbances, and control input saturation still remains a challenge. In this paper, we present a robust backstepping trajectory tracking control of a quadrotor with input saturation. The controller design accounts for both parameterized uncertainties and external disturbances, whereas a new auxiliary system is proposed to cope with control input saturation. Taking into account that only the position and attitude of the quadrotor are measurable, we devise an extended state observer to supply the estimations of unmeasured states, model uncertainties, and external disturbances. We strictly prove the stability of the closed-loop system by using the Lyapunov theory and demonstrate the effectiveness of the proposed algorithm through numerical simulations.
KW  - robust control
KW  - trajectory tracking control
KW  - backstepping
KW  - quadrotor
KW  - extended state observer
KW  - input saturation.
DO  - 10.3390/app9235184
TY  - EJOU
AU  - Zhang, Heng
AU  - Wu, Jiayu
AU  - Liu, Yanli
AU  - Yu, Jia
TI  - VaryBlock: A Novel Approach for Object Detection in Remote Sensed Images
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 23
SN  - 1424-8220

AB  - In recent years, the research on optical remote sensing images has received greater and greater attention. Object detection, as one of the most challenging tasks in the area of remote sensing, has been remarkably promoted by convolutional neural network (CNN)-based methods like You Only Look Once (YOLO) and Faster R-CNN. However, due to the complexity of backgrounds and the distinctive object distribution, directly applying these general object detection methods to the remote sensing object detection usually renders poor performance. To tackle this problem, a highly efficient and robust framework based on YOLO is proposed. We devise and integrate VaryBlock to the architecture which effectively offsets some of the information loss caused by downsampling. In addition, some techniques are utilized to facilitate the performance and to avoid overfitting. Experimental results show that our proposed method can enormously improve the mean average precision by a large margin on the NWPU VHR-10 dataset.
KW  - remote sensing
KW  - object detection
KW  - YOLO
KW  - VaryBlock
DO  - 10.3390/s19235284
TY  - EJOU
AU  - Moreno-Armendáriz, Marco A.
AU  - Calvo, Hiram
AU  - Duchanoy, Carlos A.
AU  - López-Juárez, Anayantzin P.
AU  - Vargas-Monroy, Israel A.
AU  - Suarez-Castañon, Miguel S.
TI  - Deep Green Diagnostics: Urban Green Space Analysis Using Deep Learning and Drone Images
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 23
SN  - 1424-8220

AB  - Nowadays, more than half of the world’s population lives in urban areas, and this number continues increasing. Consequently, there are more and more scientific publications that analyze health problems of people associated with living in these highly urbanized locations. In particular, some of the recent work has focused on relating people’s health to the quality and quantity of urban green areas. In this context, and considering the huge amount of land area in large cities that must be supervised, our work seeks to develop a deep learning-based solution capable of determining the level of health of the land and to assess whether it is contaminated. The main purpose is to provide health institutions with software capable of creating updated maps that indicate where these phenomena are presented, as this information could be very useful to guide public health goals in large cities. Our software is released as open source code, and the data used for the experiments presented in this paper are also freely available.
KW  - deep learning (for social good)
KW  - remote sensing
KW  - biomass analysis
DO  - 10.3390/s19235287
TY  - EJOU
AU  - Kayad, Ahmed
AU  - Sozzi, Marco
AU  - Gatto, Simone
AU  - Marinello, Francesco
AU  - Pirotti, Francesco
TI  - Monitoring Within-Field Variability of Corn Yield using Sentinel-2 and Machine Learning Techniques
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 23
SN  - 2072-4292

AB  - Monitoring and prediction of within-field crop variability can support farmers to make the right decisions in different situations. The current advances in remote sensing and the availability of high resolution, high frequency, and free Sentinel-2 images improve the implementation of Precision Agriculture (PA) for a wider range of farmers. This study investigated the possibility of using vegetation indices (VIs) derived from Sentinel-2 images and machine learning techniques to assess corn (Zea mays) grain yield spatial variability within the field scale. A 22-ha study field in North Italy was monitored between 2016 and 2018; corn yield was measured and recorded by a grain yield monitor mounted on the harvester machine recording more than 20,000 georeferenced yield observation points from the study field for each season. VIs from a total of 34 Sentinel-2 images at different crop ages were analyzed for correlation with the measured yield observations. Multiple regression and two different machine learning approaches were also tested to model corn grain yield. The three main results were the following: (i) the Green Normalized Difference Vegetation Index (GNDVI) provided the highest R2 value of 0.48 for monitoring within-field variability of corn grain yield; (ii) the most suitable period for corn yield monitoring was a crop age between 105 and 135 days from the planting date (R4&ndash;R6); (iii) Random Forests was the most accurate machine learning approach for predicting within-field variability of corn yield, with an R2 value of almost 0.6 over an independent validation set of half of the total observations. Based on the results, within-field variability of corn yield for previous seasons could be investigated from archived Sentinel-2 data with GNDVI at crop stage (R4&ndash;R6).
KW  - Sentinel-2
KW  - precision agriculture
KW  - machine learning
KW  - vegetation indices
KW  - corn yield
KW  - within-field variability
KW  - digital farming
DO  - 10.3390/rs11232873
TY  - EJOU
AU  - Liu, Wei
AU  - Yang, MengYuan
AU  - Xie, Meng
AU  - Guo, Zihui
AU  - Li, ErZhu
AU  - Zhang, Lianpeng
AU  - Pei, Tao
AU  - Wang, Dong
TI  - Accurate Building Extraction from Fused DSM and UAV Images Using a Chain Fully Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 24
SN  - 2072-4292

AB  - Accurate extraction of buildings using high spatial resolution imagery is essential to a wide range of urban applications. However, it is difficult to extract semantic features from a variety of complex scenes (e.g., suburban, urban and urban village areas) because various complex man-made objects usually appear heterogeneous with large intra-class and low inter-class variations. The automatic extraction of buildings is thus extremely challenging. The fully convolutional neural networks (FCNs) developed in recent years have performed well in the extraction of urban man-made objects due to their ability to learn state-of-the-art features and to label pixels end-to-end. One of the most successful FCNs used in building extraction is U-net. However, the commonly used skip connection and feature fusion refinement modules in U-net often ignore the problem of feature selection, and the ability to extract smaller buildings and refine building boundaries needs to be improved. In this paper, we propose a trainable chain fully convolutional neural network (CFCN), which fuses high spatial resolution unmanned aerial vehicle (UAV) images and the digital surface model (DSM) for building extraction. Multilevel features are obtained from the fusion data, and an improved U-net is used for the coarse extraction of the building. To solve the problem of incomplete extraction of building boundaries, a U-net network is introduced by chain, which is used for the introduction of a coarse building boundary constraint, hole filling, and "speckle" removal. Typical areas such as suburban, urban, and urban villages were selected for building extraction experiments. The results show that the CFCN achieved recall of 98.67%, 98.62%, and 99.52% and intersection over union (IoU) of 96.23%, 96.43%, and 95.76% in suburban, urban, and urban village areas, respectively. Considering the IoU in conjunction with the CFCN and U-net resulted in improvements of 6.61%, 5.31%, and 6.45% in suburban, urban, and urban village areas, respectively. The proposed method can extract buildings with higher accuracy and with clearer and more complete boundaries.
KW  - building extraction
KW  - digital surface model
KW  - unmanned aerial vehicle images
KW  - chain full convolution neural network
KW  - fusion
DO  - 10.3390/rs11242912
TY  - EJOU
AU  - Prado Osco, Lucas
AU  - Marques Ramos, Ana P.
AU  - Roberto Pereira, Danilo
AU  - Akemi Saito Moriya, Érika
AU  - Nobuhiro Imai, Nilton
AU  - Takashi Matsubara, Edson
AU  - Estrabis, Nayara
AU  - de Souza, Maurício
AU  - Marcato Junior, José
AU  - Gonçalves, Wesley N.
AU  - Li, Jonathan
AU  - Liesenberg, Veraldo
AU  - Eduardo Creste, José
TI  - Predicting Canopy Nitrogen Content in Citrus-Trees Using Random Forest Algorithm Associated to Spectral Vegetation Indices from UAV-Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 24
SN  - 2072-4292

AB  - The traditional method of measuring nitrogen content in plants is a time-consuming and labor-intensive task. Spectral vegetation indices extracted from unmanned aerial vehicle (UAV) images and machine learning algorithms have been proved effective in assisting nutritional analysis in plants. Still, this analysis has not considered the combination of spectral indices and machine learning algorithms to predict nitrogen in tree-canopy structures. This paper proposes a new framework to infer the nitrogen content in citrus-tree at a canopy-level using spectral vegetation indices processed with the random forest algorithm. A total of 33 spectral indices were estimated from multispectral images acquired with a UAV-based sensor. Leaf samples were gathered from different planting-fields and the leaf nitrogen content (LNC) was measured in the laboratory, and later converted into the canopy nitrogen content (CNC). To evaluate the robustness of the proposed framework, we compared it with other machine learning algorithms. We used 33,600 citrus trees to evaluate the performance of the machine learning models. The random forest algorithm had higher performance in predicting CNC than all models tested, reaching an R2 of 0.90, MAE of 0.341 g&middot;kg&minus;1 and MSE of 0.307 g&middot;kg&minus;1. We demonstrated that our approach is able to reduce the need for chemical analysis of the leaf tissue and optimizes citrus orchard CNC monitoring.
KW  - UAV multispectral imagery
KW  - spectral vegetation indices
KW  - machine learning
KW  - plant nutrition
DO  - 10.3390/rs11242925
TY  - EJOU
AU  - Zhou, Yu
AU  - Wu, Chunxue
AU  - Wu, Qunhui
AU  - Eli, Zelda M.
AU  - Xiong, Naixue
AU  - Zhang, Sheng
TI  - Design and Analysis of Refined Inspection of Field Conditions of Oilfield Pumping Wells Based on Rotorcraft UAV Technology
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 12
SN  - 2079-9292

AB  - The traditional oil well monitoring method relies on manual acquisition and various high-precision sensors. Using the indicator diagram to judge the working condition of the well is not only difficult to establish but also consumes huge manpower and financial resources. This paper proposes the use of computer vision in the detection of working conditions in oil extraction. Combined with the advantages of an unmanned aerial vehicle (UAV), UAV aerial photography images are used to realize real-time detection of on-site working conditions by real-time tracking of the working status of the head working and other related parts of the pumping unit. Considering the real-time performance of working condition detection, this paper proposes a framework that combines You only look once version 3 (YOLOv3) and a sort algorithm to complete multi-target tracking in the form of tracking by detection. The quality of the target detection in the framework is the key factor affecting the tracking effect. The experimental results show that a good detector makes the tracking speed achieve the real-time effect and provides help for the real-time detection of the working condition, which has a strong practical application.
KW  - computer vision
KW  - oil well working condition
KW  - real-time detection
KW  - sort
KW  - unmanned aerial vehicle (UAV)
KW  - YOLOv3
DO  - 10.3390/electronics8121504
TY  - EJOU
AU  - Barbedo, Jayme G.
AU  - Koenigkan, Luciano V.
AU  - Santos, Thiago T.
AU  - Santos, Patrícia M.
TI  - A Study on the Detection of Cattle in UAV Images Using Deep Learning
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 24
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) are being increasingly viewed as valuable tools to aid the management of farms. This kind of technology can be particularly useful in the context of extensive cattle farming, as production areas tend to be expansive and animals tend to be more loosely monitored. With the advent of deep learning, and convolutional neural networks (CNNs) in particular, extracting relevant information from aerial images has become more effective. Despite the technological advancements in drone, imaging and machine learning technologies, the application of UAVs for cattle monitoring is far from being thoroughly studied, with many research gaps still remaining. In this context, the objectives of this study were threefold: (1) to determine the highest possible accuracy that could be achieved in the detection of animals of the Canchim breed, which is visually similar to the Nelore breed (Bos taurus indicus); (2) to determine the ideal ground sample distance (GSD) for animal detection; (3) to determine the most accurate CNN architecture for this specific problem. The experiments involved 1853 images containing 8629 samples of animals, and 15 different CNN architectures were tested. A total of 900 models were trained (15 CNN architectures &times; 3 spacial resolutions &times; 2 datasets &times; 10-fold cross validation), allowing for a deep analysis of the several aspects that impact the detection of cattle using aerial images captured using UAVs. Results revealed that many CNN architectures are robust enough to reliably detect animals in aerial images even under far from ideal conditions, indicating the viability of using UAVs for cattle monitoring.
KW  - unmanned aerial vehicles
KW  - drones
KW  - canchim breed
KW  - nelore breed
KW  - convolutional neural networks
DO  - 10.3390/s19245436
TY  - EJOU
AU  - Siebring, Jasper
AU  - Valente, João
AU  - Domingues Franceschini, Marston H.
AU  - Kamp, Jan
AU  - Kooistra, Lammert
TI  - Object-Based Image Analysis Applied to Low Altitude Aerial Imagery for Potato Plant Trait Retrieval and Pathogen Detection
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 24
SN  - 1424-8220

AB  - There is a growing demand in both food quality and quantity, but as of now, one-third of all food produced for human consumption is lost due to pests and other pathogens accounting for roughly 40% of pre-harvest loss in potatoes. Pathogens in potato plants, like the Erwinia bacteria and the PVYNTN virus for example, exhibit symptoms of varying severity that are not easily captured by pixel-based classes (as these ignore shape, texture, and context in general). The aim of this research is to develop an object-based image analysis (OBIA) method for trait retrieval of individual potato plants that maximizes information output from Unmanned Aerial Vehicle (UAV) RGB very high resolution (VHR) imagery and its derivatives, to be used for disease detection of the Solanum tuberosum. The approach proposed can be split in two steps: (1) object-based mapping of potato plants using an optimized implementation of large scale mean-shift segmentation (LSMSS), and (2) classification of disease using a random forest (RF) model for a set of morphological traits computed from their associative objects. The approach was proven viable as the associative RF model detected presence of Erwinia and PVY pathogens with a maximum F1 score of 0.75 and an average Matthews Correlation Coefficient (MCC) score of 0.47. It also shows that low-altitude imagery acquired with a commercial UAV is a viable off-the-shelf tool for precision farming, and potato pathogen detection.
KW  - OBIA
KW  - VHR
KW  - erwinia bacteria
KW  - PVY virus
KW  - disease detection
KW  - color spaces
DO  - 10.3390/s19245477
TY  - EJOU
AU  - He, Xiaoxing
AU  - Hua, Xianghong
AU  - Montillet, Jean-Philippe
AU  - Yu, Kegen
AU  - Zou, Jingui
AU  - Xiang, Dong
AU  - Zhu, Huiping
AU  - Zhang, Di
AU  - Huang, Zhengkai
AU  - Zhao, Bufan
TI  - An Innovative Virtual Simulation Teaching Platform on Digital Mapping with Unmanned Aerial Vehicle for Remote Sensing Education
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 24
SN  - 2072-4292

AB  - This work mainly discusses an innovative teaching platform on Unmanned Aerial Vehicle digital mapping for Remote Sensing (RS) education at Wuhan University, underlining the fast development of RS technology. Firstly, we introduce and discuss the future development of the Virtual Simulation Experiment Teaching Platform for Unmanned Aerial Vehicle (VSETP-UAV). It includes specific topics such as the Systems and function Design, teaching and learning strategies, and experimental methods. This study shows that VSETP-UAV expands the usual content and training methods related to RS education, and creates a good synergy between teaching and research. The results also show that the VSETP-UAV platform is of high teaching quality producing excellent engineers, with high international standards and innovative skills in the RS field. In particular, it develops students&rsquo; practical skills with technical manipulations of dedicated hardware and software equipment (e.g., UAV) in order to assimilate quickly this particular topic. Therefore, students report that this platform is more accessible from an educational point-of-view than theoretical programs, with a quick way of learning basic concepts of RS. Finally, the proposed VSETP-UAV platform achieves a high social influence, expanding the practical content and training methods of UAV based experiments, and providing a platform for producing high-quality national talents with internationally recognized topics related to emerging engineering education.
KW  - Unmanned Aerial Vehicle
KW  - undergraduate education
KW  - Remote Sensing
KW  - surveying and mapping
DO  - 10.3390/rs11242993
TY  - EJOU
AU  - Ahn, Hyojung
AU  - Choi, Han-Lim
AU  - Kang, Minguk
AU  - Moon, SungTae
TI  - Learning-Based Anomaly Detection and Monitoring for Swarm Drone Flights
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 24
SN  - 2076-3417

AB  - This paper addresses anomaly detection and monitoring for swarm drone flights. While the current practice of swarm flight typically relies on the operator&rsquo;s naked eyes to monitor health of the multiple vehicles, this work proposes a machine learning-based framework to enable detection of abnormal behavior of a large number of flying drones on the fly. The method works in two steps: a sequence of two unsupervised learning procedures reduces the dimensionality of the real flight test data and labels them as normal and abnormal cases; then, a deep neural network classifier with one-dimensional convolution layers followed by fully connected multi-layer perceptron extracts the associated features and distinguishes the anomaly from normal conditions. The proposed anomaly detection scheme is validated on the real flight test data, highlighting its capability of online implementation.
KW  - swarm drone
KW  - anomaly detection
KW  - clustering
KW  - labeling
KW  - classification
DO  - 10.3390/app9245477
TY  - EJOU
AU  - Kocur-Bera, Katarzyna
AU  - Dawidowicz, Agnieszka
TI  - Land Use versus Land Cover: Geo-Analysis of National Roads and Synchronisation Algorithms
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 24
SN  - 2072-4292

AB  - Technological progress in Earth surface observation provides a vast range of information on the land and methods of its use. This enables property owners, users and administrators to monitor the state of the boundaries of the land they own/administer. The land cover, monitored directly on the ground, is not always consistent with the land use entered in the Land and Property Registry (LPR). Discrepancies between these data are often found in former communist countries. One of the reasons for this was the rapid process of land privatisation, which took place in Poland, without updating information on the plot geodetic boundaries. The study examined and compared the land use (entered in the LPR) with the land cover (on the ground) for national roads (acr. LU-LC). The most frequent discrepancies were selected, using CLC2018, digital orthophotomaps (using the Web Map Service (WMS) browsing service compliant with Open Geospatial Consortium (OGC) standards), cadastral data, statistical modelling and an updated survey of the right-of-way. Subsequently, six algorithms were proposed to synchronise the land use and land cover when the right-of-way was used by unauthorised persons, and two algorithms for cases of unauthorised use of land by the road administrator. Currently, it is difficult to synchronise the land cover with the land use from the administrative, legal and social points of view. The results of analyses show that full synchronisation of land use and land cover is complicated and time-consuming, although desired.
KW  - spatial analysis
KW  - updating data
KW  - land use/cover (acr. LU-LC)
KW  - cadastral data
KW  - synchronisation algorithms
DO  - 10.3390/rs11243053
TY  - EJOU
AU  - Chamran, Mohammad K.
AU  - Yau, Kok-Lim A.
AU  - Noor, Rafidah M. D.
AU  - Wong, Richard
TI  - A Distributed Testbed for 5G Scenarios: An Experimental Study
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 1
SN  - 1424-8220

AB  - This paper demonstrates the use of Universal Software Radio Peripheral (USRP), together with Raspberry Pi3 B+ (RP3) as the brain (or the decision making engine), to develop a distributed wireless network in which nodes can communicate with other nodes independently and make decision autonomously. In other words, each USRP node (i.e., sensor) is embedded with separate processing units (i.e., RP3), which has not been investigated in the literature, so that each node can make independent decisions in a distributed manner. The proposed testbed in this paper is compared with the traditional distributed testbed, which has been widely used in the literature. In the traditional distributed testbed, there is a single processing unit (i.e., a personal computer) that makes decisions in a centralized manner, and each node (i.e., USRP) is connected to the processing unit via a switch. The single processing unit exchanges control messages with nodes via the switch, while the nodes exchange data packets among themselves using a wireless medium in a distributed manner. The main disadvantage of the traditional testbed is that, despite the network being distributed in nature, decisions are made in a centralized manner. Hence, the response delay of the control message exchange is always neglected. The use of such testbed is mainly due to the limited hardware and monetary cost to acquire a separate processing unit for each node. The experiment in our testbed has shown the increase of end-to-end delay and decrease of packet delivery ratio due to software and hardware delays. The observed multihop transmission is performed using device-to-device (D2D) communication, which has been enabled in 5G. Therefore, nodes can either communicate with other nodes via: (a) a direct communication with the base station at the macrocell, which helps to improve network performance; or (b) D2D that improve spectrum efficiency, whereby traffic is offloaded from macrocell to small cells. Our testbed is the first of its kind in this scale, and it uses RP3 as the distributed decision-making engine incorporated into the USRP/GNU radio platform. This work provides an insight to the development of a 5G network.
KW  - D2D communication
KW  - 5G
KW  - sensor network
KW  - sensor
KW  - end-to-end delay
KW  - USRP
KW  - distributed mechanism
KW  - Raspberry Pi
DO  - 10.3390/s20010018
TY  - EJOU
AU  - Khan, Muhammad F.
AU  - Yau, Kok-Lim A.
AU  - Noor, Rafidah M.
AU  - Imran, Muhammad A.
TI  - Routing Schemes in FANETs: A Survey
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 1
SN  - 1424-8220

AB  - Flying ad hoc network (FANET) is a self-organizing wireless network that enables inexpensive, flexible, and easy-to-deploy flying nodes, such as unmanned aerial vehicles (UAVs), to communicate among themselves in the absence of fixed network infrastructure. FANET is one of the emerging networks that has an extensive range of next-generation applications. Hence, FANET plays a significant role in achieving application-based goals. Routing enables the flying nodes to collaborate and coordinate among themselves and to establish routes to radio access infrastructure, particularly FANET base station (BS). With a longer route lifetime, the effects of link disconnections and network partitions reduce. Routing must cater to two main characteristics of FANETs that reduce the route lifetime. Firstly, the collaboration nature requires the flying nodes to exchange messages and to coordinate among themselves, causing high energy consumption. Secondly, the mobility pattern of the flying nodes is highly dynamic in a three-dimensional space and they may be spaced far apart, causing link disconnection. In this paper, we present a comprehensive survey of the limited research work of routing schemes in FANETs. Different aspects, including objectives, challenges, routing metrics, characteristics, and performance measures, are covered. Furthermore, we present open issues.
KW  - ad hoc networks
KW  - FANETs
KW  - routing
KW  - network topology
DO  - 10.3390/s20010038
TY  - EJOU
AU  - He, Qimin
AU  - Zhang, Kefei
AU  - Wu, Suqin
AU  - Zhao, Qingzhi
AU  - Wang, Xiaoming
AU  - Shen, Zhen
AU  - Li, Longjiang
AU  - Wan, Moufeng
AU  - Liu, Xiaoyang
TI  - Real-Time GNSS-Derived PWV for Typhoon Characterizations: A Case Study for Super Typhoon Mangkhut in Hong Kong
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 1
SN  - 2072-4292

AB  - Typhoons can be serious natural disasters for the sustainability and development of society. The development of a typhoon usually involves a pre-existing weather disturbance, warm tropical oceans, and a large amount of moisture. This implies that a large variation in the atmospheric water vapor over the path of a typhoon can be used to study the characteristics of the typhoon. This is the reason that the variation in precipitable water vapor (PWV) is often used to capture the signature of a typhoon in meteorology. This study investigates the usability of real-time PWV retrieved from global navigation satellite systems (GNSS) for typhoons&rsquo; characterizations, and especially, the following aspects were investigated: (1) The correlation between PWV and atmospheric parameters including pressure, temperature, precipitation, and wind speed; (2) water vapor transportation during a typhoon period; and (3) the correlation between the movement of a typhoon and the transportation of water vapor. The case study selected for this research was Super Typhoon Mangkhut that occurred in mid-September 2018 in Hong Kong. The PWV time series were obtained from a conversion of GNSS-derived zenith total delays (ZTDs) using observations at 10 stations selected from the Hong Kong GNSS continuously operating reference stations (CORS) network, which are also located along the path of the typhoon. The Bernese GNSS Software (ver. 5.2) was used to obtain the ZTDs; and the root mean square (RMS) of the differences between the GNSS-ZTDs and International GNSS Service post-processed ZTDs time series was less than 8 mm. The RMS of the differences between the GNSS-PWVs (i.e., the ZTDs converted PWVs) and radiosonde-derived PWVs (RS-PWVs) time series was less than 2 mm. The changes in PWV reflect the variation in wind speed during the typhoon period to a certain degree, and their correlation coefficient was 0.76, meaning a significant positive correlation. In addition, a new approach was proposed to estimate the direction and speed of a typhoon&rsquo;s movement using the time difference of PWV arrival at different sites. The direction and speed estimated agreed well with the ones published by the China Meteorological Administration. These results suggest that GNSS-derived PWV has a great potential for the monitoring and even prediction of typhoon events, especially for near real-time warnings.
KW  - typhoon
KW  - global navigation satellite systems
KW  - precipitable water vapor
KW  - radiosonde
KW  - Super Typhoon Mangkhut
DO  - 10.3390/rs12010104
TY  - EJOU
AU  - Dong, Xinyu
AU  - Zhang, Zhichao
AU  - Yu, Ruiyang
AU  - Tian, Qingjiu
AU  - Zhu, Xicun
TI  - Extraction of Information about Individual Trees from High-Spatial-Resolution UAV-Acquired Images of an Orchard
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 1
SN  - 2072-4292

AB  - The extraction of information about individual trees is essential to supporting the growing of fruit in orchard management. Data acquired from spectral sensors mounted on unmanned aerial vehicles (UAVs) have very high spatial and temporal resolution. However, an efficient and reliable method for extracting information about individual trees with irregular tree-crown shapes and a complicated background is lacking. In this study, we developed and tested the performance of an approach, based on UAV imagery, to extracting information about individual trees in an orchard with a complicated background that includes apple trees (Plot 1) and pear trees (Plot 2). The workflow involves the construction of a digital orthophoto map (DOM), digital surface models (DSMs), and digital terrain models (DTMs) using the Structure from Motion (SfM) and Multi-View Stereo (MVS) approaches, as well as the calculation of the Excess Green minus Excess Red Index (ExGR) and the selection of various thresholds. Furthermore, a local-maxima filter method and marker-controlled watershed segmentation were used for the detection and delineation, respectively, of individual trees. The accuracy of the proposed method was evaluated by comparing its results with manual estimates of the numbers of trees and the areas and diameters of tree-crowns, all three of which parameters were obtained from the DOM. The results of the proposed method are in good agreement with these manual estimates: The F-scores for the estimated numbers of individual trees were 99.0% and 99.3% in Plot 1 and Plot 2, respectively, while the Producer&rsquo;s Accuracy (PA) and User&rsquo;s Accuracy (UA) for the delineation of individual tree-crowns were above 95% for both of the plots. For the area of individual tree-crowns, root-mean-square error (RMSE) values of 0.72 m2 and 0.48 m2 were obtained for Plot 1 and Plot 2, respectively, while for the diameter of individual tree-crowns, RMSE values of 0.39 m and 0.26 m were obtained for Plot 1 (339 trees correctly identified) and Plot 2 (203 trees correctly identified), respectively. Both the areas and diameters of individual tree-crowns were overestimated to varying degrees.
KW  - fruit tree growing
KW  - orchard
KW  - image processing
KW  - remote sensing
KW  - unmanned aerial vehicles
KW  - digital height model
KW  - image segmentation
DO  - 10.3390/rs12010133
TY  - EJOU
AU  - Mohamed, Hassan
AU  - Nadaoka, Kazuo
AU  - Nakamura, Takashi
TI  - Towards Benthic Habitat 3D Mapping Using Machine Learning Algorithms and Structures from Motion Photogrammetry
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 1
SN  - 2072-4292

AB  - The accurate classification and 3D mapping of benthic habitats in coastal ecosystems are vital for developing management strategies for these valuable shallow water environments. However, both automatic and semiautomatic approaches for deriving ecologically significant information from a towed video camera system are quite limited. In the current study, we demonstrate a semiautomated framework for high-resolution benthic habitat classification and 3D mapping using Structure from Motion and Multi View Stereo (SfM-MVS) algorithms and automated machine learning classifiers. The semiautomatic classification of benthic habitats was performed using several attributes extracted automatically from labeled examples by a human annotator using raw towed video camera image data. The Bagging of Features (BOF), Hue Saturation Value (HSV), and Gray Level Co-occurrence Matrix (GLCM) methods were used to extract these attributes from 3000 images. Three machine learning classifiers (k-nearest neighbor (k-NN), support vector machine (SVM), and bagging (BAG)) were trained by using these attributes, and their outputs were assembled by the fuzzy majority voting (FMV) algorithm. The correctly classified benthic habitat images were then geo-referenced using a differential global positioning system (DGPS). Finally, SfM-MVS techniques used the resulting classified geo-referenced images to produce high spatial resolution digital terrain models and orthophoto mosaics for each category. The framework was tested for the identification and 3D mapping of seven habitats in a portion of the Shiraho area in Japan. These seven habitats were corals (Acropora and Porites), blue corals (H. coerulea), brown algae, blue algae, soft sand, hard sediments (pebble, cobble, and boulders), and seagrass. Using the FMV algorithm, we achieved an overall accuracy of 93.5% in the semiautomatic classification of the seven habitats.
KW  - machine learning algorithms
KW  - benthic 3D mapping
KW  - towed underwater video camera
KW  - hybrid classifiers
KW  - structure from motion
DO  - 10.3390/rs12010127
TY  - EJOU
AU  - Meng, Lingxuan
AU  - Peng, Zhixing
AU  - Zhou, Ji
AU  - Zhang, Jirong
AU  - Lu, Zhenyu
AU  - Baumann, Andreas
AU  - Du, Yan
TI  - Real-Time Detection of Ground Objects Based on Unmanned Aerial Vehicle Remote Sensing with Deep Learning: Application in Excavator Detection for Pipeline Safety
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 1
SN  - 2072-4292

AB  - Unmanned aerial vehicle (UAV) remote sensing and deep learning provide a practical approach to object detection. However, most of the current approaches for processing UAV remote-sensing data cannot carry out object detection in real time for emergencies, such as firefighting. This study proposes a new approach for integrating UAV remote sensing and deep learning for the real-time detection of ground objects. Excavators, which usually threaten pipeline safety, are selected as the target object. A widely used deep-learning algorithm, namely You Only Look Once V3, is first used to train the excavator detection model on a workstation and then deployed on an embedded board that is carried by a UAV. The recall rate of the trained excavator detection model is 99.4%, demonstrating that the trained model has a very high accuracy. Then, the UAV for an excavator detection system (UAV-ED) is further constructed for operational application. UAV-ED is composed of a UAV Control Module, a UAV Module, and a Warning Module. A UAV experiment with different scenarios was conducted to evaluate the performance of the UAV-ED. The whole process from the UAV observation of an excavator to the Warning Module (350 km away from the testing area) receiving the detection results only lasted about 1.15 s. Thus, the UAV-ED system has good performance and would benefit the management of pipeline safety.
KW  - YOLOv3
KW  - deep learning
KW  - real-time detection
KW  - unmanned aerial vehicle (UAV)
KW  - remote sensing
DO  - 10.3390/rs12010182
TY  - EJOU
AU  - Li, Yaxin
AU  - Li, Wenbin
AU  - Tang, Shengjun
AU  - Darwish, Walid
AU  - Hu, Yuling
AU  - Chen, Wu
TI  - Automatic Indoor as-Built Building Information Models Generation by Using Low-Cost RGB-D Sensors
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 1
SN  - 1424-8220

AB  - To generate indoor as-built building information models (AB BIMs) automatically and economically is a great technological challenge. Many approaches have been developed to address this problem in recent years, but it is far from being settled, particularly for the point cloud segmentation and the extraction of the relationship among different elements due to the complicated indoor environment. This is even more difficult for the low-quality point cloud generated by low-cost scanning equipment. This paper proposes an automatic as-built BIMs generation framework that transforms the noisy 3D point cloud produced by a low-cost RGB-D sensor (about 708 USD for data collection equipment, 379 USD for the Structure sensor and 329 USD for iPad) to the as-built BIMs, without any manual intervention. The experiment results show that the proposed method has competitive robustness and accuracy, compared to the high-quality Terrestrial Lidar System (TLS), with the element extraction accuracy of 100%, mean dimension reconstruction accuracy of 98.6% and mean area reconstruction accuracy of 93.6%. Also, the proposed framework makes the BIM generation workflows more efficient in both data collection and data processing. In the experiments, the time consumption of data collection for a typical room, with an area of 45&ndash;67      m 2     , is reduced to 4&ndash;6 min with an RGB-D sensor from 50&ndash;60 min with TLS. The processing time to generate BIM models is about half minutes automatically, from around 10 min with a conventional semi-manual method.
KW  - as-built BIMs
KW  - automatic
KW  - RGB-D sensors
DO  - 10.3390/s20010293
TY  - EJOU
AU  - Zhang, Xiuwei
AU  - Jin, Jiaojiao
AU  - Lan, Zeze
AU  - Li, Chunjiang
AU  - Fan, Minhao
AU  - Wang, Yafei
AU  - Yu, Xin
AU  - Zhang, Yanning
TI  - ICENET: A Semantic Segmentation Deep Network for River Ice by Fusing Positional and Channel-Wise Attentive Features
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 2
SN  - 2072-4292

AB  - River ice monitoring is of great significance for river management, ship navigation and ice hazard forecasting in cold-regions. Accurate ice segmentation is one most important pieces of technology in ice monitoring research. It can provide the prerequisite information for the calculation of ice cover density, drift ice speed, ice cover distribution, change detection and so on. Unmanned aerial vehicle (UAV) aerial photography has the advantages of higher spatial and temporal resolution. As UAV technology has become more popular and cheaper, it has been widely used in ice monitoring. So, we focused on river ice segmentation based on UAV remote sensing images. In this study, the NWPU_YRCC dataset was built for river ice segmentation, in which all images were captured by different UAVs in the region of the Yellow River, the most difficult river to manage in the world. To the best of our knowledge, this is the first public UAV image dataset for river ice segmentation. Meanwhile, a semantic segmentation deep convolution neural network by fusing positional and channel-wise attentive features is proposed for river ice semantic segmentation, named ICENET. Experiments demonstrated that the proposed ICENET outperforms the state-of-the-art methods, achieving a superior result on the NWPU_YRCC dataset.
KW  - river ice
KW  - position attention
KW  - channel-wise attention
KW  - deep convolutional neural network
KW  - semantic segmentation
DO  - 10.3390/rs12020221
TY  - EJOU
AU  - Wang, Ju
AU  - Wang, Guoqiang
AU  - Hu, Xiaoxuan
AU  - Luo, He
AU  - Xu, Haiqing
TI  - Cooperative Transmission Tower Inspection with a Vehicle and a UAV in Urban Areas
T2  - Energies

PY  - 2020
VL  - 13
IS  - 2
SN  - 1996-1073

AB  - To reduce the workload of inspectors and improve the inspection efficiency of urban transmission towers, a new inspection method is proposed in this paper, in which an unmanned aerial vehicle (UAV) and vehicle cooperate with each other. We investigate the cooperative path planning problem of a UAV and a vehicle for transmission tower inspection and develop a new 0&ndash;1 integer programming model to address the problem. An odd-even layered genetic algorithm (O-ELGA) is proposed to efficiently solve the model. Finally, the effectiveness of the algorithm is further verified by simulation experiments.
KW  - vehicle-carried UAV
KW  - TSP-D
KW  - transmission tower inspection
KW  - path planning
KW  - genetic algorithm
DO  - 10.3390/en13020326
TY  - EJOU
AU  - Senthilnath, J.
AU  - Varia, Neelanshi
AU  - Dokania, Akanksha
AU  - Anand, Gaotham
AU  - Benediktsson, Jón A.
TI  - Deep TEC: Deep Transfer Learning with Ensemble Classifier for Road Extraction from UAV Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 2
SN  - 2072-4292

AB  - Unmanned aerial vehicle (UAV) remote sensing has a wide area of applications and in this paper, we attempt to address one such problem&mdash;road extraction from UAV-captured RGB images. The key challenge here is to solve the road extraction problem using the UAV multiple remote sensing scene datasets that are acquired with different sensors over different locations. We aim to extract the knowledge from a dataset that is available in the literature and apply this extracted knowledge on our dataset. The paper focuses on a novel method which consists of deep TEC (deep transfer learning with ensemble classifier) for road extraction using UAV imagery. The proposed deep TEC performs road extraction on UAV imagery in two stages, namely, deep transfer learning and ensemble classifier. In the first stage, with the help of deep learning methods, namely, the conditional generative adversarial network, the cycle generative adversarial network and the fully convolutional network, the model is pre-trained on the benchmark UAV road extraction dataset that is available in the literature. With this extracted knowledge (based on the pre-trained model) the road regions are then extracted on our UAV acquired images. Finally, for the road classified images, ensemble classification is carried out. In particular, the deep TEC method has an average quality of 71%, which is 10% higher than the next best standard deep learning methods. Deep TEC also shows a higher level of performance measures such as completeness, correctness and F1 score measures. Therefore, the obtained results show that the deep TEC is efficient in extracting road networks in an urban region.
KW  - UAV
KW  - remote sensing
KW  - road extraction
KW  - deep learning
KW  - transfer learning
KW  - ensemble classifier
DO  - 10.3390/rs12020245
TY  - EJOU
AU  - Rokhsaritalemi, Somaiieh
AU  - Sadeghi-Niaraki, Abolghasem
AU  - Choi, Soo-Mi
TI  - A Review on Mixed Reality: Current Trends, Challenges and Prospects
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 2
SN  - 2076-3417

AB  - Currently, new technologies have enabled the design of smart applications that are used as decision-making tools in the problems of daily life. The key issue in designing such an application is the increasing level of user interaction. Mixed reality (MR) is an emerging technology that deals with maximum user interaction in the real world compared to other similar technologies. Developing an MR application is complicated, and depends on the different components that have been addressed in previous literature. In addition to the extraction of such components, a comprehensive study that presents a generic framework comprising all components required to develop MR applications needs to be performed. This review studies intensive research to obtain a comprehensive framework for MR applications. The suggested framework comprises five layers: the first layer considers system components; the second and third layers focus on architectural issues for component integration; the fourth layer is the application layer that executes the architecture; and the fifth layer is the user interface layer that enables user interaction. The merits of this study are as follows: this review can act as a proper resource for MR basic concepts, and it introduces MR development steps and analytical models, a simulation toolkit, system types, and architecture types, in addition to practical issues for stakeholders such as considering MR different domains.
KW  - mixed reality
KW  - review
KW  - trend
KW  - challenge
KW  - future prospect
DO  - 10.3390/app10020636
TY  - EJOU
AU  - Jung, Daekyo
AU  - Tran Tuan, Vu
AU  - Quoc Tran, Dai
AU  - Park, Minsoo
AU  - Park, Seunghee
TI  - Conceptual Framework of an Intelligent Decision Support System for Smart City Disaster Management
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 2
SN  - 2076-3417

AB  - In order to protect human lives and infrastructure, as well as to minimize the risk of damage, it is important to predict and respond to natural disasters in advance. However, currently, the standardized disaster response system in South Korea still needs further advancement, and the response phase systems need to be improved to ensure that they are properly equipped to cope with natural disasters. Existing studies on intelligent disaster management systems (IDSSs) in South Korea have focused only on storms, floods, and earthquakes, and they have not used past data. This research proposes a new conceptual framework of an IDSS for disaster management, with particular attention paid to wildfires and cold/heat waves. The IDSS uses big data collected from open application programming interface (API) and artificial intelligence (AI) algorithms to help decision-makers make faster and more accurate decisions. In addition, a simple example of the use of a convolutional neural network (CNN) to detect fire in surveillance video has been developed, which can be used for automatic fire detection and provide an appropriate response. The system will also consider connecting to open source intelligence (OSINT) to identify vulnerabilities, mitigate risks, and develop more robust security policies than those currently in place to prevent cyber-attacks.
KW  - decision support system
KW  - big data
KW  - artificial intelligence
KW  - Internet of Things
KW  - disaster management
DO  - 10.3390/app10020666
TY  - EJOU
AU  - Zha, Yufei
AU  - Wu, Min
AU  - Qiu, Zhuling
AU  - Sun, Jingxian
AU  - Zhang, Peng
AU  - Huang, Wei
TI  - Online Semantic Subspace Learning with Siamese Network for UAV Tracking
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 2
SN  - 2072-4292

AB  - In urban environment monitoring, visual tracking on unmanned aerial vehicles (UAVs) can produce more applications owing to the inherent advantages, but it also brings new challenges for existing visual tracking approaches (such as complex background clutters, rotation, fast motion, small objects, and realtime issues due to camera motion and viewpoint changes). Based on the Siamese network, tracking can be conducted efficiently in recent UAV datasets. Unfortunately, the learned convolutional neural network (CNN) features are not discriminative when identifying the target from the background/clutter, In particular for the distractor, and cannot capture the appearance variations temporally. Additionally, occlusion and disappearance are also reasons for tracking failure. In this paper, a semantic subspace module is designed to be integrated into the Siamese network tracker to encode the local fine-grained details of the target for UAV tracking. More specifically, the target&rsquo;s semantic subspace is learned online to adapt to the target in the temporal domain. Additionally, the pixel-wise response of the semantic subspace can be used to detect occlusion and disappearance of the target, and this enables reasonable updating to relieve model drifting. Substantial experiments conducted on challenging UAV benchmarks illustrate that the proposed method can obtain competitive results in both accuracy and efficiency when they are applied to UAV videos.
KW  - UAV tracking
KW  - semantic subspace
KW  - siamese network
KW  - occlusion detection
DO  - 10.3390/rs12020325
TY  - EJOU
AU  - Lobo Torres, Daliana
AU  - Queiroz Feitosa, Raul
AU  - Nigri Happ, Patrick
AU  - Elena Cué La Rosa, Laura
AU  - Marcato Junior, José
AU  - Martins, José
AU  - Olã Bressan, Patrik
AU  - Gonçalves, Wesley N.
AU  - Liesenberg, Veraldo
TI  - Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 2
SN  - 1424-8220

AB  - This study proposes and evaluates five deep fully convolutional networks (FCNs) for the semantic segmentation of a single tree species: SegNet, U-Net, FC-DenseNet, and two DeepLabv3+ variants. The performance of the FCN designs is evaluated experimentally in terms of classification accuracy and computational load. We also verify the benefits of fully connected conditional random fields (CRFs) as a post-processing step to improve the segmentation maps. The analysis is conducted on a set of images captured by an RGB camera aboard a UAV flying over an urban area. The dataset also contains a mask that indicates the occurrence of an endangered species called Dipteryx alata Vogel, also known as cumbaru, taken as the species to be identified. The experimental analysis shows the effectiveness of each design and reports average overall accuracy ranging from 88.9% to 96.7%, an F1-score between 87.0% and 96.1%, and IoU from 77.1% to 92.5%. We also realize that CRF consistently improves the performance, but at a high computational cost.
KW  - deep learning
KW  - fully convolution neural networks
KW  - semantic segmentation
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/s20020563
TY  - EJOU
AU  - Zhang, Yishan
AU  - Wu, Lun
AU  - Ren, Huazhong
AU  - Liu, Yu
AU  - Zheng, Yongqian
AU  - Liu, Yaowen
AU  - Dong, Jiaji
TI  - Mapping Water Quality Parameters in Urban Rivers from Hyperspectral Images Using a New Self-Adapting Selection of Multiple Artificial Neural Networks
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 2
SN  - 2072-4292

AB  - Protection of water environments is an important part of overall environmental protection; hence, many people devote their efforts to monitoring and improving water quality. In this study, a self-adapting selection method of multiple artificial neural networks (ANNs) using hyperspectral remote sensing and ground-measured water quality data is proposed to quantitatively predict water quality parameters, including phosphorus, nitrogen, biochemical oxygen demand (BOD), chemical oxygen demand (COD), and chlorophyll a. Seventy-nine ground measured data samples are used as training data in the establishment of the proposed model, and 30 samples are used as testing data. The proposed method based on traditional ANNs of numerical prediction involves feature selection of bands, self-adapting selection based on multiple selection criteria, stepwise backtracking, and combined weighted correlation. Water quality parameters are estimated with coefficient of determination      R 2      ranging from 0.93 (phosphorus) to 0.98 (nitrogen), which is higher than the value (0.7 to 0.8) obtained by traditional ANNs. MPAE (mean percent of absolute error) values ranging from 5% to 11% are used rather than root mean square error to evaluate the predicting precision of the proposed model because the magnitude of each water quality parameter considerably differs, thereby providing reasonable and interpretable results. Compared with other ANNs with backpropagation, this study proposes an auto-adapting method assisted by the above-mentioned methods to select the best model with all settings, such as the number of hidden layers, number of neurons in each hidden layer, choice of optimizer, and activation function. Different settings for ANNS with backpropagation are important to improve precision and compatibility for different data. Furthermore, the proposed method is applied to hyperspectral remote sensing images collected using an unmanned aerial vehicle for monitoring the water quality in the Shiqi River, Zhongshan City, Guangdong Province, China. Obtained results indicate the locations of pollution sources.
KW  - self-adapting
KW  - deep learning
KW  - multiple neural network
KW  - hyperspectral image
KW  - water quality monitoring
DO  - 10.3390/rs12020336
TY  - EJOU
AU  - Alganci, Ugur
AU  - Soydas, Mehmet
AU  - Sertel, Elif
TI  - Comparative Research on Deep Learning Approaches for Airplane Detection from Very High-Resolution Satellite Images
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 3
SN  - 2072-4292

AB  - Object detection from satellite images has been a challenging problem for many years. With the development of effective deep learning algorithms and advancement in hardware systems, higher accuracies have been achieved in the detection of various objects from very high-resolution (VHR) satellite images. This article provides a comparative evaluation of the state-of-the-art convolutional neural network (CNN)-based object detection models, which are Faster R-CNN, Single Shot Multi-box Detector (SSD), and You Look Only Once-v3 (YOLO-v3), to cope with the limited number of labeled data and to automatically detect airplanes in VHR satellite images. Data augmentation with rotation, rescaling, and cropping was applied on the test images to artificially increase the number of training data from satellite images. Moreover, a non-maximum suppression algorithm (NMS) was introduced at the end of the SSD and YOLO-v3 flows to get rid of the multiple detection occurrences near each detected object in the overlapping areas. The trained networks were applied to five independent VHR test images that cover airports and their surroundings to evaluate their performance objectively. Accuracy assessment results of the test regions proved that Faster R-CNN architecture provided the highest accuracy according to the F1 scores, average precision (AP) metrics, and visual inspection of the results. The YOLO-v3 ranked as second, with a slightly lower performance but providing a balanced trade-off between accuracy and speed. The SSD provided the lowest detection performance, but it was better in object localization. The results were also evaluated in terms of the object size and detection accuracy manner, which proved that large- and medium-sized airplanes were detected with higher accuracy.
KW  - convolutional neural networks (CNNs)
KW  - end-to-end detection
KW  - transfer learning
KW  - remote sensing
KW  - single shot multi-box detector (SSD)
KW  - You Look Only Once-v3 (YOLO-v3)
KW  - Faster RCNN
DO  - 10.3390/rs12030458
TY  - EJOU
AU  - Chang, Zhilu
AU  - Du, Zhen
AU  - Zhang, Fan
AU  - Huang, Faming
AU  - Chen, Jiawu
AU  - Li, Wenbin
AU  - Guo, Zizheng
TI  - Landslide Susceptibility Prediction Based on Remote Sensing Images and GIS: Comparisons of Supervised and Unsupervised Machine Learning Models
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 3
SN  - 2072-4292

AB  - Landslide susceptibility prediction (LSP) has been widely and effectively implemented by machine learning (ML) models based on remote sensing (RS) images and Geographic Information System (GIS). However, comparisons of the applications of ML models for LSP from the perspectives of supervised machine learning (SML) and unsupervised machine learning (USML) have not been explored. Hence, this study aims to compare the LSP performance of these SML and USML models, thus further to explore the advantages and disadvantages of these ML models and to realize a more accurate and reliable LSP result. Two representative SML models (support vector machine (SVM) and CHi-squared Automatic Interaction Detection (CHAID)) and two representative USML models (K-means and Kohonen models) are respectively used to scientifically predict the landslide susceptibility indexes, and then these prediction results are discussed. Ningdu County with 446 recorded landslides obtained through field investigations is introduced as case study. A total of 12 conditioning factors are obtained through procession of Landsat TM 8 images and high-resolution aerial images, topographical and hydrological spatial analysis of Digital Elevation Modeling in GIS software, and government reports. The area value under the curve of receiver operating features (AUC) is applied for evaluating the prediction accuracy of SML models, and the frequency ratio (FR) accuracy is then introduced to compare the remarkable prediction performance differences between SML and USML models. Overall, the receiver operation curve (ROC) results show that the AUC of the SVM is 0.892 and is slightly greater than the AUC of the CHAID model (0.872). The FR accuracy results show that the SVM model has the highest accuracy for LSP (77.80%), followed by the CHAID model (74.50%), the Kohonen model (72.8%) and the K-means model (69.7%), which indicates that the SML models can reach considerably better prediction capability than the USML models. It can be concluded that selecting recorded landslides as prior knowledge to train and test the LSP models is the key reason for the higher prediction accuracy of the SML models, while the lack of a priori knowledge and target guidance is an important reason for the low LSP accuracy of the USML models. Nevertheless, the USML models can also be used to implement LSP due to their advantages of efficient modeling processes, dimensionality reduction and strong scalability.
KW  - landslide susceptibility prediction
KW  - supervised machine learning
KW  - unsupervised machine learning
KW  - remote sensing
KW  - Geographic Information System
DO  - 10.3390/rs12030502
TY  - EJOU
AU  - Sang, Xuejia
AU  - Xue, Linfu
AU  - Ran, Xiangjin
AU  - Li, Xiaoshun
AU  - Liu, Jiwen
AU  - Liu, Zeyu
TI  - Intelligent High-Resolution Geological Mapping Based on SLIC-CNN
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 2
SN  - 2220-9964

AB  - High-resolution geological mapping is an important supporting condition for mineral and energy exploration. However, high-resolution geological mapping work still faces many problems. At present, high-resolution geological mapping is still generated by expert interpretation of survey lines, compasses, and field data. The work in the field is constrained by the weather, terrain, and personnel, and the working methods need to be improved. This paper proposes a new method for high-resolution mapping using Unmanned Aerial Vehicle (UAV) and deep learning algorithms. This method uses the UAV to collect high-resolution remote sensing images, cooperates with some groundwork to anchor the lithology, and then completes most of the mapping work on high-resolution remote sensing images. This method transfers a large amount of field work into the room and provides an automatic mapping process based on the Simple Linear Iterative Clustering-Convolutional Neural Network (SLIC-CNN) algorithm. It uses the convolutional neural network (CNN) to identify the image content and confirms the lithologic distribution, the simple linear iterative cluster (SLIC) algorithm can be used to outline the boundary of the rock mass and determine the contact interface of the rock mass, and the mode and expert decision method is used to clarify the results of the fusion and mapping. The mapping method was applied to the Taili waterfront in Xingcheng City, Liaoning Province, China. In this study, the Area Under the Curve (AUC) of the mapping method was 0.937. The Kappa test result was k = 0.8523, and a high-resolution geological map was obtained.
KW  - geological survey
KW  - geological mapping
KW  - deep learning
KW  - SLIC-CNN
KW  - UAV
KW  - ductile shear zone
DO  - 10.3390/ijgi9020099
TY  - EJOU
AU  - Maxwell, Aaron E.
AU  - Pourmohammadi, Pariya
AU  - Poyner, Joey D.
TI  - Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 3
SN  - 2072-4292

AB  - Modern elevation-determining remote sensing technologies such as light-detection and ranging (LiDAR) produce a wealth of topographic information that is increasingly being used in a wide range of disciplines, including archaeology and geomorphology. However, automated methods for mapping topographic features have remained a significant challenge. Deep learning (DL) mask regional-convolutional neural networks (Mask R-CNN), which provides context-based instance mapping, offers the potential to overcome many of the difficulties of previous approaches to topographic mapping. We therefore explore the application of Mask R-CNN to extract valley fill faces (VFFs), which are a product of mountaintop removal (MTR) coal mining in the Appalachian region of the eastern United States. LiDAR-derived slopeshades are provided as the only predictor variable in the model. Model generalization is evaluated by mapping multiple study sites outside the training data region. A range of assessment methods, including precision, recall, and F1 score, all based on VFF counts, as well as area- and a fuzzy area-based user&rsquo;s and producer&rsquo;s accuracy, indicate that the model was successful in mapping VFFs in new geographic regions, using elevation data derived from different LiDAR sensors. Precision, recall, and F1-score values were above 0.85 using VFF counts while user&rsquo;s and producer&rsquo;s accuracy were above 0.75 and 0.85 when using the area- and fuzzy area-based methods, respectively, when averaged across all study areas characterized with LiDAR data. Due to the limited availability of LiDAR data until relatively recently, we also assessed how well the model generalizes to terrain data created using photogrammetric methods that characterize past terrain conditions. Unfortunately, the model was not sufficiently general to allow successful mapping of VFFs using photogrammetrically-derived slopeshades, as all assessment metrics were lower than 0.60; however, this may partially be attributed to the quality of the photogrammetric data. The overall results suggest that the combination of Mask R-CNN and LiDAR has great potential for mapping anthropogenic and natural landscape features. To realize this vision, however, research on the mapping of other topographic features is needed, as well as the development of large topographic training datasets including a variety of features for calibrating and testing new methods.
KW  - light detection and ranging
KW  - LiDAR
KW  - deep learning
KW  - convolutional neural networks
KW  - CNNs
KW  - mask regional-convolutional neural networks
KW  - mask R-CNN
KW  - digital terrain analysis
KW  - resource extraction
DO  - 10.3390/rs12030547
TY  - EJOU
AU  - Du, Hao
AU  - Wang, Wei
AU  - Xu, Chaowen
AU  - Xiao, Ran
AU  - Sun, Changyin
TI  - Real-Time Onboard 3D State Estimation of an Unmanned Aerial Vehicle in Multi-Environments Using Multi-Sensor Data Fusion
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 3
SN  - 1424-8220

AB  - The question of how to estimate the state of an unmanned aerial vehicle (UAV) in real time in multi-environments remains a challenge. Although the global navigation satellite system (GNSS) has been widely applied, drones cannot perform position estimation when a GNSS signal is not available or the GNSS is disturbed. In this paper, the problem of state estimation in multi-environments is solved by employing an Extended Kalman Filter (EKF) algorithm to fuse the data from multiple heterogeneous sensors (MHS), including an inertial measurement unit (IMU), a magnetometer, a barometer, a GNSS receiver, an optical flow sensor (OFS), Light Detection and Ranging (LiDAR), and an RGB-D camera. Finally, the robustness and effectiveness of the multi-sensor data fusion system based on the EKF algorithm are verified by field flights in unstructured, indoor, outdoor, and indoor and outdoor transition scenarios.
KW  - multi-sensor data fusion
KW  - multi-environments
KW  - state estimation
KW  - unmanned aerial vehicle
DO  - 10.3390/s20030919
TY  - EJOU
AU  - Perera, Asanka G.
AU  - Khanam, Fatema-Tuz-Zohra
AU  - Al-Naji, Ali
AU  - Chahl, Javaan
TI  - Detection and Localisation of Life Signs from the Air Using Image Registration and Spatio-Temporal Filtering
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 3
SN  - 2072-4292

AB  - In search and rescue operations, it is crucial to rapidly identify those people who are alive from those who are not. If this information is known, emergency teams can prioritize their operations to save more lives. However, in some natural disasters the people may be lying on the ground covered with dust, debris, or ashes making them difficult to detect by video analysis that is tuned to human shapes. We present a novel method to estimate the locations of people from aerial video using image and signal processing designed to detect breathing movements. We have shown that this method can successfully detect clearly visible people and people who are fully occluded by debris. First, the aerial videos were stabilized using the key points of adjacent image frames. Next, the stabilized video was decomposed into tile videos and the temporal frequency bands of interest were motion magnified while the other frequencies were suppressed. Image differencing and temporal filtering were performed on each tile video to detect potential breathing signals. Finally, the detected frequencies were remapped to the image frame creating a life signs map that indicates possible human locations. The proposed method was validated with both aerial and ground recorded videos in a controlled environment. Based on the dataset, the results showed good reliability for aerial videos and no errors for ground recorded videos where the average precision measures for aerial videos and ground recorded videos were 0.913 and 1 respectively.
KW  - search and rescue
KW  - drone
KW  - breathing detection
KW  - human detection
DO  - 10.3390/rs12030577
TY  - EJOU
AU  - Agapiou, Athos
TI  - Evaluation of Landsat 8 OLI/TIRS Level-2 and Sentinel 2 Level-1C Fusion Techniques Intended for Image Segmentation of Archaeological Landscapes and Proxies
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 3
SN  - 2072-4292

AB  - The use of medium resolution, open access, and freely distributed satellite images, such as those of Landsat, is still understudied in the domain of archaeological research, mainly due to restrictions of spatial resolution. This investigation aims to showcase how the synergistic use of Landsat and Sentinel optical sensors can efficiently support archaeological research through object-based image analysis (OBIA), a relatively new scientific trend, as highlighted in the relevant literature, in the domain of remote sensing archaeology. Initially, the fusion of a 30 m spatial resolution Landsat 8 OLI/TIRS Level-2 and a 10 m spatial resolution Sentinel 2 Level-1C optical images, over the archaeological site of &ldquo;Nea Paphos&rdquo; in Cyprus, are evaluated in order to improve the spatial resolution of the Landsat image. At this step, various known fusion models are implemented and evaluated, namely Gram&ndash;Schmidt, Brovey, principal component analysis (PCA), and hue-saturation-value (HSV) algorithms. In addition, all four 10 m available spectral bands of the Sentinel 2 sensor, namely the blue, green, red, and near-infrared bands (Bands 2 to 4 and Band 8, respectively) were assessed for each of the different fusion models. On the basis of these findings, the next step of the study, focused on the image segmentation process, through the evaluation of different scale factors. The segmentation process is an important step moving from pixel-based to object-based image analysis. The overall results show that the Gram&ndash;Schmidt fusion method based on the near-infrared band of the Sentinel 2 (Band 8) at a range of scale factor segmentation to 70 are the optimum parameters for the detection of standing visible monuments, monitoring excavated areas, and detecting buried archaeological remains, without any significant spectral distortion of the original Landsat image. The new 10 m fused Landsat 8 image provides further spatial details of the archaeological site and depicts, through the segmentation process, important details within the landscape under examination.
KW  - fusion
KW  - image segmentation
KW  - archaeological landscapes
KW  - archaeological proxies
KW  - Landsat 8
KW  - Sentinel 2
KW  - object-based image analysis (OBIA)
DO  - 10.3390/rs12030579
TY  - EJOU
AU  - Wan, Kaifang
AU  - Gao, Xiaoguang
AU  - Hu, Zijian
AU  - Wu, Gaofeng
TI  - Robust Motion Control for UAV in Dynamic Uncertain Environments Using Deep Reinforcement Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 4
SN  - 2072-4292

AB  - In this paper, a novel deep reinforcement learning (DRL) method, and robust deep deterministic policy gradient (Robust-DDPG), is proposed for developing a controller that allows robust flying of an unmanned aerial vehicle (UAV) in dynamic uncertain environments. This technique is applicable in many fields, such as penetration and remote surveillance. The learning-based controller is constructed with an actor-critic framework, and can perform a dual-channel continuous control (roll and speed) of the UAV. To overcome the fragility and volatility of original DDPG, three critical learning tricks are introduced in Robust-DDPG: (1) Delayed-learning trick, providing stable learnings, while facing dynamic environments; (2) adversarial attack trick, improving policy&rsquo;s adaptability to uncertain environments; (3) mixed exploration trick, enabling faster convergence of the model. The training experiments show great improvement in its convergence speed, convergence effect, and stability. The exploiting experiments demonstrate high efficiency in providing the UAV a shorter and smoother path. While, the generalization experiments verify its better adaptability to complicated, dynamic and uncertain environments, comparing to Deep Q Network (DQN) and DDPG algorithms.
KW  - UAV
KW  - robust motion control
KW  - deep reinforcement learning
KW  - adversarial attack
KW  - delayed learning
KW  - mixed exploration
DO  - 10.3390/rs12040640
TY  - EJOU
AU  - Hufkens, Koen
AU  - de Haulleville, Thalès
AU  - Kearsley, Elizabeth
AU  - Jacobsen, Kim
AU  - Beeckman, Hans
AU  - Stoffelen, Piet
AU  - Vandelook, Filip
AU  - Meeus, Sofie
AU  - Amara, Michael
AU  - Van Hirtum, Leen
AU  - Van den Bulcke, Jan
AU  - Verbeeck, Hans
AU  - Wingate, Lisa
TI  - Historical Aerial Surveys Map Long-Term Changes of Forest Cover and Structure in the Central Congo Basin
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 4
SN  - 2072-4292

AB  - Given the impact of tropical forest disturbances on atmospheric carbon emissions, biodiversity, and ecosystem productivity, accurate long-term reporting of Land-Use and Land-Cover (LULC) change in the pre-satellite era (&lt;1972) is an imperative. Here, we used a combination of historical (1958) aerial photography and contemporary remote sensing data to map long-term changes in the extent and structure of the tropical forest surrounding Yangambi (DR Congo) in the central Congo Basin. Our study leveraged structure-from-motion and a convolutional neural network-based LULC classifier, using synthetic landscape-based image augmentation to map historical forest cover across a large orthomosaic (~93,431 ha) geo-referenced to ~4.7 ± 4.3 m at submeter resolution. A comparison with contemporary LULC data showed a shift from previously highly regular industrial deforestation of large areas to discrete smallholder farming clearing, increasing landscape fragmentation and providing opportunties for substantial forest regrowth. We estimated aboveground carbon gains through reforestation to range from 811 to 1592 Gg C, partially offsetting historical deforestation (2416 Gg C), in our study area. Efforts to quantify long-term canopy texture changes and their link to aboveground carbon had limited to no success. Our analysis provides methods and insights into key spatial and temporal patterns of deforestation and reforestation at a multi-decadal scale, providing a historical context for past and ongoing forest research in the area.
KW  - aerial survey
KW  - data recovery
KW  - CNN
KW  - deep learning
KW  - SfM
KW  - Congo Basin
DO  - 10.3390/rs12040638
TY  - EJOU
AU  - Lin, Yan-Ting
AU  - Yang, Ming-Der
AU  - Han, Jen-Yu
AU  - Su, Yuan-Fong
AU  - Jang, Jiun-Huei
TI  - Quantifying Flood Water Levels Using Image-Based Volunteered Geographic Information
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 4
SN  - 2072-4292

AB  - Many people use smartphone cameras to record their living environments through captured images, and share aspects of their daily lives on social networks, such as Facebook, Instagram, and Twitter. These platforms provide volunteered geographic information (VGI), which enables the public to know where and when events occur. At the same time, image-based VGI can also indicate environmental changes and disaster conditions, such as flooding ranges and relative water levels. However, little image-based VGI has been applied for the quantification of flooding water levels because of the difficulty of identifying water lines in image-based VGI and linking them to detailed terrain models. In this study, flood detection has been achieved through image-based VGI obtained by smartphone cameras. Digital image processing and a photogrammetric method were presented to determine the water levels. In digital image processing, the random forest classification was applied to simplify ambient complexity and highlight certain aspects of flooding regions, and the HT-Canny method was used to detect the flooding line of the classified image-based VGI. Through the photogrammetric method and a fine-resolution digital elevation model based on the unmanned aerial vehicle mapping technique, the detected flooding lines were employed to determine water levels. Based on the results of image-based VGI experiments, the proposed approach identified water levels during an urban flood event in Taipei City for demonstration. Notably, classified images were produced using random forest supervised classification for a total of three classes with an average overall accuracy of 88.05%. The quantified water levels with a resolution of centimeters (&lt;3-cm difference on average) can validate flood modeling so as to extend point-basis observations to area-basis estimations. Therefore, the limited performance of image-based VGI quantification has been improved to help in flood disasters. Consequently, the proposed approach using VGI images provides a reliable and effective flood-monitoring technique for disaster management authorities.
KW  - volunteered geographic information (VGI)
KW  - social network
KW  - random forest
KW  - water level detection
KW  - image processing
KW  - smartphones
DO  - 10.3390/rs12040706
TY  - EJOU
AU  - Cao, Qingjie
AU  - Shi, Zaifeng
AU  - Wang, Pumeng
AU  - Gao, Yang
TI  - A Seamless Image-Stitching Method Based on Human Visual Discrimination and Attention
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 4
SN  - 2076-3417

AB  - Stitching gaps and misalignments in mosaic images can severely degrade the human visual perception of mosaic effects. Image stitching plays a key role in eliminating these unpleasant defects. In this paper, an image-stitching method for mosaic images with invisible seams is proposed, according to the research on the human visual system (HVS). By quantifying the human visual attention of images and visual discrimination about luminance difference and fine dislocations, each pixel in the stitching region is given a priority value for tracing a stitching line. Coupled with the processing of an optimal stitching line locating method and the multi-band blending algorithm, the pixels of discontinuous items in mosaic images decrease significantly and the stitching line is almost invisible. This study provides a new insight into the image-stitching field, and the experiments show that the results of the proposed method are more consistent with the human visual system in creating high-quality image mosaics.
KW  - image mosaic
KW  - seamless image stitching
KW  - human visual system
KW  - stitching line detection
DO  - 10.3390/app10041462
TY  - EJOU
AU  - Spachos, Petros
TI  - Towards a Low-Cost Precision Viticulture System Using Internet of Things Devices
T2  - IoT

PY  - 2020
VL  - 1
IS  - 1
SN  - 2624-831X

AB  - Precision Agriculture (PA) is an ever-expanding field that takes modern technological advancements and applies it to farming practices to reduce waste and increase output. One advancement that can play a significant role in achieving precision agriculture is wireless technology, and specifically the Internet of Things (IoT) devices. Small, inch scale and low-cost devices can be used to monitor great agricultural areas. In this paper, a system for precision viticulture which uses IoT devices for real-time monitoring is proposed. The different components of the system are programmed properly and the interconnection between them is designed to minimize energy consumption. Wireless sensor nodes measure soil moisture and soil temperature in the field and transmit the information to a base station. If the conditions are optimal for a disease or pest to occur, a drone flies towards the area. When the drone is over the node, pictures are captured and then it returns to the base station for further processing. The feasibility of the system is examined through experimentation in a realistic scenario.
KW  - precision viticulture
KW  - Internet of Things
KW  - sensors and instrumentation
KW  - smart agriculture
DO  - 10.3390/iot1010002
TY  - EJOU
AU  - Nogueira, Keiller
AU  - L. S. Machado, Gabriel
AU  - H. T. Gama, Pedro
AU  - C. V. da Silva, Caio
AU  - Balaniuk, Remis
AU  - A. dos Santos, Jefersson
TI  - Facing Erosion Identification in Railway Lines Using Pixel-Wise Deep-Based Approaches
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 4
SN  - 2072-4292

AB  - Soil erosion is considered one of the most expensive natural hazards with a high impact on several infrastructure assets. Among them, railway lines are one of the most likely constructions for the appearance of erosion and, consequently, one of the most troublesome due to the maintenance costs, risks of derailments, and so on. Therefore, it is fundamental to identify and monitor erosion in railway lines to prevent major consequences. Currently, erosion identification is manually performed by humans using huge image sets, a time-consuming and slow task. Hence, automatic machine learning methods appear as an appealing alternative. A crucial step for automatic erosion identification is to create a good feature representation. Towards such objective, deep learning can learn data-driven features and classifiers. In this paper, we propose a novel deep learning-based framework capable of performing erosion identification in railway lines. Six techniques were evaluated and the best one, Dynamic Dilated ConvNet, was integrated into this framework that was then encapsulated into a new ArcGIS plugin to facilitate its use by non-programmer users. To analyze such techniques, we also propose a new dataset, composed of almost 2000 high-resolution images.
KW  - deep learning
KW  - remote sensing
KW  - erosion identification
KW  - high-resolution images
DO  - 10.3390/rs12040739
TY  - EJOU
AU  - Lu, Heng
AU  - Ma, Lei
AU  - Fu, Xiao
AU  - Liu, Chao
AU  - Wang, Zhi
AU  - Tang, Min
AU  - Li, Naiwen
TI  - Landslides Information Extraction Using Object-Oriented Image Analysis Paradigm Based on Deep Learning and Transfer Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 5
SN  - 2072-4292

AB  - How to acquire landslide disaster information quickly and accurately has become the focus and difficulty of disaster prevention and relief by remote sensing. Landslide disasters are generally featured by sudden occurrence, proposing high demand for emergency data acquisition. The low-altitude Unmanned Aerial Vehicle (UAV) remote sensing technology is widely applied to acquire landslide disaster data, due to its convenience, high efficiency, and ability to fly at low altitude under cloud. However, the spectrum information of UAV images is generally deficient and manual interpretation is difficult for meeting the need of quick acquisition of emergency data. Based on this, UAV images of high-occurrence areas of landslide disaster in Wenchuan County and Baoxing County in Sichuan Province, China were selected for research in the paper. Firstly, the acquired UAV images were pre-processed to generate orthoimages. Subsequently, multi-resolution segmentation was carried out to obtain image objects, and the barycenter of each object was calculated to generate a landslide sample database (including positive and negative samples) for deep learning. Next, four landslide feature models of deep learning and transfer learning, namely Histograms of Oriented Gradients (HOG), Bag of Visual Word (BOVW), Convolutional Neural Network (CNN), and Transfer Learning (TL) were compared, and it was found that the TL model possesses the best feature extraction effect, so a landslide extraction method based on the TL model and object-oriented image analysis (TLOEL) was proposed; finally, the TLOEL method was compared with the object-oriented nearest neighbor classification (NNC) method. The research results show that the accuracy of the TLOEL method is higher than the NNC method, which can not only achieve the edge extraction of large landslides, but also detect and extract middle and small landslides accurately that are scatteredly distributed.
KW  - landslides information extraction
KW  - unmanned aerial vehicle imagery
KW  - convolutional neural network
KW  - transfer learning
KW  - object-oriented image analysis
DO  - 10.3390/rs12050752
TY  - EJOU
AU  - Polvara, Riccardo
AU  - Patacchiola, Massimiliano
AU  - Hanheide, Marc
AU  - Neumann, Gerhard
TI  - Sim-to-Real Quadrotor Landing via Sequential Deep Q-Networks and Domain Randomization
T2  - Robotics

PY  - 2020
VL  - 9
IS  - 1
SN  - 2218-6581

AB  - The autonomous landing of an Unmanned Aerial Vehicle (UAV) on a marker is one of the most challenging problems in robotics. Many solutions have been proposed, with the best results achieved via customized geometric features and external sensors. This paper discusses for the first time the use of deep reinforcement learning as an end-to-end learning paradigm to find a policy for UAVs autonomous landing. Our method is based on a divide-and-conquer paradigm that splits a task into sequential sub-tasks, each one assigned to a Deep Q-Network (DQN), hence the name Sequential Deep Q-Network (SDQN). Each DQN in an SDQN is activated by an internal trigger, and it represents a component of a high-level control policy, which can navigate the UAV towards the marker. Different technical solutions have been implemented, for example combining vanilla and double DQNs, and the introduction of a partitioned buffer replay to address the problem of sample efficiency. One of the main contributions of this work consists in showing how an SDQN trained in a simulator via domain randomization, can effectively generalize to real-world scenarios of increasing complexity. The performance of SDQNs is comparable with a state-of-the-art algorithm and human pilots while being quantitatively better in noisy conditions.
KW  - deep reinforcement learning
KW  - aerial vehicles
KW  - Sim-to-Real
DO  - 10.3390/robotics9010008
TY  - EJOU
AU  - Ding, Hu
AU  - Liu, Kai
AU  - Chen, Xiaozheng
AU  - Xiong, Liyang
AU  - Tang, Guoan
AU  - Qiu, Fang
AU  - Strobl, Josef
TI  - Optimized Segmentation Based on the Weighted Aggregation Method for Loess Bank Gully Mapping
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 5
SN  - 2072-4292

AB  - The Chinese Loess Plateau suffers severe gully erosion. Gully mapping is a fundamental task for gully erosion monitoring in this region. Among the different gully types in the Loess Plateau, the bank gully is usually regarded as the most important source for the generation of sediment. However, approaches for bank gully extraction are still limited. This study put forward an integrated framework, including segmentation optimization, evaluation and Extreme Gradient Boosting (XGBoost)-based classification, for the bank gully mapping of Zhifanggou catchment in the Chinese Loess Plateau. The approach was conducted using a 1-m resolution digital elevation model (DEM), based on unmanned aerial vehicle (UAV) photogrammetry and WorldView-3 imagery. The methodology first divided the study area into different watersheds. Then, segmentation by weighted aggregation (SWA) was implemented to generate multi-level segments. For achieving an optimum segmentation, area-weighted variance (WV) and Moran&rsquo;s I (MI) were adopted and calculated within each sub-watershed. After that, a new discrepancy metric, the area-number index (ANI), was developed for evaluating the segmentation results, and the results were compared with the multi-resolution segmentation (MRS) algorithm. Finally, bank gully mappings were obtained based on the XGBoost model after fine-tuning. The experiment results demonstrate that the proposed method can achieve superior segmentation compared to MRS. Moreover, the overall accuracy of the bank gully extraction results was 78.57%. The proposed approach provides a credible tool for mapping bank gullies, which could be useful for the catchment-scale gully erosion process.
KW  - object-based image analysis
KW  - gully mapping
KW  - segmentation optimization
KW  - unmanned aerial vehicle (UAV)
KW  - XGBoost
DO  - 10.3390/rs12050793
TY  - EJOU
AU  - Lee, Yong-Suk
AU  - Lee, Sunmin
AU  - Baek, Won-Kyung
AU  - Jung, Hyung-Sup
AU  - Park, Sung-Hwan
AU  - Lee, Moung-Jin
TI  - Mapping Forest Vertical Structure in Jeju Island from Optical and Radar Satellite Images Using Artificial Neural Network
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 5
SN  - 2072-4292

AB  - Recently, due to the acceleration of global warming, an accurate understanding and management of forest carbon stocks, such as forest aboveground biomass, has become very important. The vertical structure of the forest, which is the internal structure of the forest, was mainly investigated by field surveys that are labor intensive. Recently, remote sensing techniques have been actively used to explore large and inaccessible areas. In addition, machine learning techniques that could classify and analyze large amounts of data are being used in various fields. Thus, this study aims to analyze the forest vertical structure (number of tree layers) to estimate forest aboveground biomass in Jeju Island from optical and radar satellite images using artificial neural networks (ANN). For this purpose, the eight input neurons of the forest related layers, based on remote sensing data, were prepared: normalized difference vegetation index (NDVI), normalized difference water index (NDWI), NDVI texture, NDWI texture, average canopy height, standard deviation canopy height and two types of coherence maps were created using the Kompsat-3 optical image, L-band ALOS PALSAR-1 radar images, digital surface model (DSM), and digital terrain model (DTM). The forest vertical structure data, based on field surveys, was divided into the training/validation and test data and the hyper-parameters of ANN were trained using the training/validation data. The forest vertical classification result from ANN was evaluated by comparison to the test data. It showed about a 65.7% overall accuracy based on the error matrix. This result shows that the forest vertical structure map can be effectively generated from optical and radar satellite images and existing DEM and DTM using the ANN approach, especially for national scale mapping.
KW  - forest vertical structure
KW  - KOMPSAT-3
KW  - ALOS PALSAR-1
KW  - artificial neural network
DO  - 10.3390/rs12050797
TY  - EJOU
AU  - Baur, Jasper
AU  - Steinberg, Gabriel
AU  - Nikulin, Alex
AU  - Chiu, Kenneth
AU  - de Smet, Timothy S.
TI  - Applying Deep Learning to Automate UAV-Based Detection of Scatterable Landmines
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 5
SN  - 2072-4292

AB  - Recent advances in unmanned-aerial-vehicle- (UAV-) based remote sensing utilizing lightweight multispectral and thermal infrared sensors allow for rapid wide-area landmine contamination detection and mapping surveys. We present results of a study focused on developing and testing an automated technique of remote landmine detection and identification of scatterable antipersonnel landmines in wide-area surveys. Our methodology is calibrated for the detection of scatterable plastic landmines which utilize a liquid explosive encapsulated in a polyethylene or plastic body in their design. We base our findings on analysis of multispectral and thermal datasets collected by an automated UAV-survey system featuring scattered PFM-1-type landmines as test objects and present results of an effort to automate landmine detection, relying on supervised learning algorithms using a Faster Regional-Convolutional Neural Network (Faster R-CNN). The RGB visible light Faster R-CNN demo yielded a 99.3% testing accuracy for a partially withheld testing set and 71.5% testing accuracy for a completely withheld testing set. Across multiple test environments, using centimeter scale accurate georeferenced datasets paired with Faster R-CNN, allowed for accurate automated detection of test PFM-1 landmines. This method can be calibrated to other types of scatterable antipersonnel mines in future trials to aid humanitarian demining initiatives. With millions of remnant PFM-1 and similar scatterable plastic mines across post-conflict regions and considerable stockpiles of these landmines posing long-term humanitarian and economic threats to impacted communities, our methodology could considerably aid in efforts to demine impacted regions.
KW  - landmines
KW  - UXO
KW  - UAV
KW  - CNN
KW  - neural networks
DO  - 10.3390/rs12050859
TY  - EJOU
AU  - Gao, Demin
AU  - Sun, Quan
AU  - Hu, Bin
AU  - Zhang, Shuo
TI  - A Framework for Agricultural Pest and Disease Monitoring Based on Internet-of-Things and Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 5
SN  - 1424-8220

AB  - With the development of information technology, Internet-of-Things (IoT) and low-altitude remote-sensing technology represented by Unmanned Aerial Vehicles (UAVs) are widely used in environmental monitoring fields. In agricultural modernization, IoT and UAV can monitor the incidence of crop diseases and pests from the ground micro and air macro perspectives, respectively. IoT technology can collect real-time weather parameters of the crop growth by means of numerous inexpensive sensor nodes. While depending on spectral camera technology, UAVs can capture the images of farmland, and these images can be utilize for analyzing the occurrence of pests and diseases of crops. In this work, we attempt to design an agriculture framework for providing profound insights into the specific relationship between the occurrence of pests/diseases and weather parameters. Firstly, considering that most farms are usually located in remote areas and far away from infrastructure, making it hard to deploy agricultural IoT devices due to limited energy supplement, a sun tracker device is designed to adjust the angle automatically between the solar panel and the sunlight for improving the energy-harvesting rate. Secondly, for resolving the problem of short flight time of UAV, a flight mode is introduced to ensure the maximum utilization of wind force and prolong the fight time. Thirdly, the images captured by UAV are transmitted to the cloud data center for analyzing the degree of damage of pests and diseases based on spectrum analysis technology. Finally, the agriculture framework is deployed in the Yangtze River Zone of China and the results demonstrate that wheat is susceptible to disease when the temperature is between 14 &deg;C and 16 &deg;C, and high rainfall decreases the spread of wheat powdery mildew.
KW  - agricultural pests and diseases
KW  - internet of things
KW  - unmanned aerial vehicle
DO  - 10.3390/s20051487
TY  - EJOU
AU  - Zhu, Xiaobo
AU  - He, Honglin
AU  - Ma, Mingguo
AU  - Ren, Xiaoli
AU  - Zhang, Li
AU  - Zhang, Fawei
AU  - Li, Yingnian
AU  - Shi, Peili
AU  - Chen, Shiping
AU  - Wang, Yanfen
AU  - Xin, Xiaoping
AU  - Ma, Yaoming
AU  - Zhang, Yu
AU  - Du, Mingyuan
AU  - Ge, Rong
AU  - Zeng, Na
AU  - Li, Pan
AU  - Niu, Zhongen
AU  - Zhang, Liyun
AU  - Lv, Yan
AU  - Song, Zengjing
AU  - Gu, Qing
TI  - Estimating Ecosystem Respiration in the Grasslands of Northern China Using Machine Learning: Model Evaluation and Comparison
T2  - Sustainability

PY  - 2020
VL  - 12
IS  - 5
SN  - 2071-1050

AB  - While a number of machine learning (ML) models have been used to estimate RE, systematic evaluation and comparison of these models are still limited. In this study, we developed three traditional ML models and a deep learning (DL) model, stacked autoencoders (SAE), to estimate RE in northern China&rsquo;s grasslands. The four models were trained with two strategies: training for all of northern China&rsquo;s grasslands and separate training for the alpine and temperate grasslands. Our results showed that all four ML models estimated RE in northern China&rsquo;s grasslands fairly well, while the SAE model performed best (R2 = 0.858, RMSE = 0.472 gC m&minus;2 d&minus;1, MAE = 0.304 gC m&minus;2 d&minus;1). Models trained with the two strategies had almost identical performances. The enhanced vegetation index and soil organic carbon density (SOCD) were the two most important environmental variables for estimating RE in the grasslands of northern China. Air temperature (Ta) was more important than the growing season land surface water index (LSWI) in the alpine grasslands, while the LSWI was more important than Ta in the temperate grasslands. These findings may promote the application of DL models and the inclusion of SOCD for RE estimates with increased accuracy.
KW  - ecosystem respiration
KW  - machine learning
KW  - deep learning
KW  - grasslands
KW  - northern China
DO  - 10.3390/su12052099
TY  - EJOU
AU  - Liu, Haojie
AU  - Liao, Kang
AU  - Lin, Chunyu
AU  - Zhao, Yao
AU  - Liu, Meiqin
TI  - PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 6
SN  - 1424-8220

AB  - LiDAR sensors can provide dependable 3D spatial information at a low frequency (around 10 Hz) and have been widely applied in the field of autonomous driving and unmanned aerial vehicle (UAV). However, the camera with a higher frequency (around 20 Hz) has to be decreased so as to match with LiDAR in a multi-sensor system. In this paper, we propose a novel Pseudo-LiDAR interpolation network (PLIN) to increase the frequency of LiDAR sensor data. PLIN can generate temporally and spatially high-quality point cloud sequences to match the high frequency of cameras. To achieve this goal, we design a coarse interpolation stage guided by consecutive sparse depth maps and motion relationship. We also propose a refined interpolation stage guided by the realistic scene. Using this coarse-to-fine cascade structure, our method can progressively perceive multi-modal information and generate accurate intermediate point clouds. To the best of our knowledge, this is the first deep framework for Pseudo-LiDAR point cloud interpolation, which shows appealing applications in navigation systems equipped with LiDAR and cameras. Experimental results demonstrate that PLIN achieves promising performance on the KITTI dataset, significantly outperforming the traditional interpolation method and the state-of-the-art video interpolation technique.
KW  - 3D point cloud
KW  - pseudo-LiDAR interpolation
KW  - convolutional neural networks
KW  - depth completion
KW  - video interpolation
DO  - 10.3390/s20061573
TY  - EJOU
AU  - Osco, Lucas P.
AU  - Ramos, Ana P.
AU  - Faita Pinheiro, Mayara M.
AU  - Moriya, Érika A.
AU  - Imai, Nilton N.
AU  - Estrabis, Nayara
AU  - Ianczyk, Felipe
AU  - Araújo, Fábio F.
AU  - Liesenberg, Veraldo
AU  - Jorge, Lúcio A.
AU  - Li, Jonathan
AU  - Ma, Lingfei
AU  - Gonçalves, Wesley N.
AU  - Marcato Junior, José
AU  - Eduardo Creste, José
TI  - A Machine Learning Framework to Predict Nutrient Content in Valencia-Orange Leaf Hyperspectral Measurements
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 6
SN  - 2072-4292

AB  - This paper presents a framework based on machine learning algorithms to predict nutrient content in leaf hyperspectral measurements. This is the first approach to evaluate macro- and micronutrient content with both machine learning and reflectance/first-derivative data. For this, citrus-leaves collected at a Valencia-orange orchard were used. Their spectral data was measured with a Fieldspec ASD FieldSpec&reg; HandHeld 2 spectroradiometer and the surface reflectance and first-derivative spectra from the spectral range of 380 to 1020 nm (640 spectral bands) was evaluated. A total of 320 spectral signatures were collected, and the leaf-nutrient content (N, P, K, Mg, S, Cu, Fe, Mn, and Zn) was associated with them. For this, 204,800 (320 &times; 640) combinations were used. The following machine learning algorithms were used in this framework: k-Nearest Neighbor (kNN), Lasso Regression, Ridge Regression, Support Vector Machine (SVM), Artificial Neural Network (ANN), Decision Tree (DT), and Random Forest (RF). The training methods were assessed based on Cross-Validation and Leave-One-Out. The Relief-F metric of the algorithms&rsquo; prediction was used to determine the most contributive wavelength or spectral region associated with each nutrient. This approach was able to return, with high predictions (R2), nutrients like N (0.912), Mg (0.832), Cu (0.861), Mn (0.898), and Zn (0.855), and, to a lesser extent, P (0.771), K (0.763), and S (0.727). These accuracies were obtained with different algorithms, but RF was the most suitable to model most of them. The results indicate that, for the Valencia-orange leaves, surface reflectance data is more suitable to predict macronutrients, while first-derivative spectra is better linked to micronutrients. A final contribution of this study is the identification of the wavelengths responsible for contributing to these predictions.
KW  - spectroscopy
KW  - proximal sensor
KW  - macronutrient
KW  - micronutrient
KW  - artificial intelligence
DO  - 10.3390/rs12060906
TY  - EJOU
AU  - Pashaei, Mohammad
AU  - Kamangir, Hamid
AU  - Starek, Michael J.
AU  - Tissot, Philippe
TI  - Review and Evaluation of Deep Learning Architectures for Efficient Land Cover Mapping with UAS Hyper-Spatial Imagery: A Case Study Over a Wetland
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 6
SN  - 2072-4292

AB  - Deep learning has already been proved as a powerful state-of-the-art technique for many image understanding tasks in computer vision and other applications including remote sensing (RS) image analysis. Unmanned aircraft systems (UASs) offer a viable and economical alternative to a conventional sensor and platform for acquiring high spatial and high temporal resolution data with high operational flexibility. Coastal wetlands are among some of the most challenging and complex ecosystems for land cover prediction and mapping tasks because land cover targets often show high intra-class and low inter-class variances. In recent years, several deep convolutional neural network (CNN) architectures have been proposed for pixel-wise image labeling, commonly called semantic image segmentation. In this paper, some of the more recent deep CNN architectures proposed for semantic image segmentation are reviewed, and each model&rsquo;s training efficiency and classification performance are evaluated by training it on a limited labeled image set. Training samples are provided using the hyper-spatial resolution UAS imagery over a wetland area and the required ground truth images are prepared by manual image labeling. Experimental results demonstrate that deep CNNs have a great potential for accurate land cover prediction task using UAS hyper-spatial resolution images. Some simple deep learning architectures perform comparable or even better than complex and very deep architectures with remarkably fewer training epochs. This performance is especially valuable when limited training samples are available, which is a common case in most RS applications.
KW  - coastal wetland
KW  - land cover mapping
KW  - semantic image segmentation
KW  - machine learning
KW  - deep learning
KW  - convolutional neural networks
KW  - transfer learning
KW  - unmanned aircraft systems
DO  - 10.3390/rs12060959
TY  - EJOU
AU  - Sonobe, Rei
AU  - Hirono, Yuhei
AU  - Oi, Ayako
TI  - Non-Destructive Detection of Tea Leaf Chlorophyll Content Using Hyperspectral Reflectance and Machine Learning Algorithms
T2  - Plants

PY  - 2020
VL  - 9
IS  - 3
SN  - 2223-7747

AB  - Tea trees are kept in shaded locations to increase their chlorophyll content, which influences green tea quality. Therefore, monitoring change in chlorophyll content under low light conditions is important for managing tea trees and producing high-quality green tea. Hyperspectral remote sensing is one of the most frequently used methods for estimating chlorophyll content. Numerous studies based on data collected under relatively low-stress conditions and many hyperspectral indices and radiative transfer models show that shade-grown tea performs poorly. The performance of four machine learning algorithms&mdash;random forest, support vector machine, deep belief nets, and kernel-based extreme learning machine (KELM)&mdash;in evaluating data collected from tea leaves cultivated under different shade treatments was tested. KELM performed best with a root-mean-square error of 8.94 &plusmn; 3.05 &mu;g cm&minus;2 and performance to deviation values from 1.70 to 8.04 for the test data. These results suggest that a combination of hyperspectral reflectance and KELM has the potential to trace changes in the chlorophyll content of shaded tea leaves.
KW  - deep belief nets
KW  - extreme learning machine
KW  - first derivative spectra
KW  - random forest
KW  - shade-grown tea
KW  - support vector machine
DO  - 10.3390/plants9030368
TY  - EJOU
AU  - Garcia Millan, Virginia E.
AU  - Rankine, Cassidy
AU  - Sanchez-Azofeifa, G. A.
TI  - Crop Loss Evaluation Using Digital Surface Models from Unmanned Aerial Vehicles Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 6
SN  - 2072-4292

AB  - Precision agriculture and Unmanned Aerial Vehicles (UAV) are revolutionizing agriculture management methods. Remote sensing data, image analysis and Digital Surface Models derived from Structure from Motion and Multi-View Stereopsis offer new and fast methods to detect the needs of crops, greatly improving crops efficiency. In this study, we present a tool to detect and estimate crop damage after a disturbance (i.e., weather event, wildlife attacks or fires). The types of damage that are addressed in this study affect crop structure (i.e., plants are bent or gone), in the shape of depressions in the crop canopy. The aim of this study was to evaluate the performance of four unsupervised methods based on terrain analyses, for the detection of damaged crops in UAV 3D models: slope detection, variance analysis, geomorphology classification and cloth simulation filter. A full workflow was designed and described in this article that involves the postprocessing of the raw results from the terrain analyses, for a refinement in the detection of damages. Our results show that all four methods performed similarly well after postprocessing&ndash;&ndash;reaching an accuracy above to 90%&ndash;&ndash;in the detection of severe crop damage, without the need of training data. The results of this study suggest that the used methods are effective and independent of the crop type, crop damage and growth stage. However, only severe damages were detected with this workflow. Other factors such as data volume, processing time, number of processing steps and spatial distribution of targets and errors are discussed in this article for the selection of the most appropriate method. Among the four tested methods, slope analysis involves less processing steps, generates the smallest data volume, is the fastest of methods and resulted in best spatial distribution of matches. Thus, it was selected as the most efficient method for crop damage detection.
KW  - Unmanned Aerial Vehicles
KW  - structure from motion
KW  - precision agriculture
KW  - digital surface models
KW  - crop loss estimation
DO  - 10.3390/rs12060981
TY  - EJOU
AU  - Martin-Abadal, Miguel
AU  - Ruiz-Frau, Ana
AU  - Hinz, Hilmar
AU  - Gonzalez-Cid, Yolanda
TI  - Jellytoring: Real-Time Jellyfish Monitoring Based on Deep Learning Object Detection
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 6
SN  - 1424-8220

AB  - During the past decades, the composition and distribution of marine species have changed due to multiple anthropogenic pressures. Monitoring these changes in a cost-effective manner is of high relevance to assess the environmental status and evaluate the effectiveness of management measures. In particular, recent studies point to a rise of jellyfish populations on a global scale, negatively affecting diverse marine sectors like commercial fishing or the tourism industry. Past monitoring efforts using underwater video observations tended to be time-consuming and costly due to human-based data processing. In this paper, we present Jellytoring, a system to automatically detect and quantify different species of jellyfish based on a deep object detection neural network, allowing us to automatically record jellyfish presence during long periods of time. Jellytoring demonstrates outstanding performance on the jellyfish detection task, reaching an F1 score of 95.2%; and also on the jellyfish quantification task, as it correctly quantifies the number and class of jellyfish on a real-time processed video sequence up to a 93.8% of its duration. The results of this study are encouraging and provide the means towards a efficient way to monitor jellyfish, which can be used for the development of a jellyfish early-warning system, providing highly valuable information for marine biologists and contributing to the reduction of jellyfish impacts on humans.
KW  - deep learning
KW  - object detection
KW  - jellyfish quantification
KW  - jellyfish monitoring
DO  - 10.3390/s20061708
TY  - EJOU
AU  - Mandlburger, Gottfried
AU  - Pfennigbauer, Martin
AU  - Schwarz, Roland
AU  - Flöry, Sebastian
AU  - Nussbaumer, Lukas
TI  - Concept and Performance Evaluation of a Novel UAV-Borne Topo-Bathymetric LiDAR Sensor
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 6
SN  - 2072-4292

AB  - We present the sensor concept and first performance and accuracy assessment results of a novel lightweight topo-bathymetric laser scanner designed for integration on Unmanned Aerial Vehicles (UAVs), light aircraft, and helicopters. The instrument is particularly well suited for capturing river bathymetry in high spatial resolution as a consequence of (i) the low nominal flying altitude of 50&ndash;150 m above ground level resulting in a laser footprint diameter on the ground of typically 10&ndash;30 cm and (ii) the high pulse repetition rate of up to 200 kHz yielding a point density on the ground of approximately 20&ndash;50 points/m2. The instrument features online waveform processing and additionally stores the full waveform within the entire range gate for waveform analysis in post-processing. The sensor was tested in a real-world environment by acquiring data from two freshwater ponds and a 500 m section of the pre-Alpine Pielach River (Lower Austria). The captured underwater points featured a maximum penetration of two times the Secchi depth. On dry land, the 3D point clouds exhibited (i) a measurement noise in the range of 1&ndash;3 mm; (ii) a fitting precision of redundantly captured flight strips of 1 cm; and (iii) an absolute accuracy of 2&ndash;3 cm compared to terrestrially surveyed checkerboard targets. A comparison of the refraction corrected LiDAR point cloud with independent underwater checkpoints exhibited a maximum deviation of 7.8 cm and revealed a systematic depth-dependent error when using a refraction coefficient of n = 1.36 for time-of-flight correction. The bias is attributed to multi-path effects in the turbid water column (Secchi depth: 1.1 m) caused by forward scattering of the laser signal at suspended particles. Due to the high spatial resolution, good depth performance, and accuracy, the sensor shows a high potential for applications in hydrology, fluvial morphology, and hydraulic engineering, including flood simulation, sediment transport modeling, and habitat mapping.
KW  - UAV LiDAR
KW  - airborne laser bathymetry
KW  - full waveform processing
KW  - performance assessment
KW  - high resolution hydro-mapping
DO  - 10.3390/rs12060986
TY  - EJOU
AU  - Francisco Ribeiro, Priscilla
AU  - Camargo Rodriguez, Anyela V.
TI  - Emerging Advanced Technologies to Mitigate the Impact of Climate Change in Africa
T2  - Plants

PY  - 2020
VL  - 9
IS  - 3
SN  - 2223-7747

AB  - Agriculture remains critical to Africa&rsquo;s socioeconomic development, employing 65% of the work force and contributing 32% of GDP (Gross Domestic Product). Low productivity, which characterises food production in many Africa countries, remains a major concern. Compounded by the effects of climate change and lack of technical expertise, recent reports suggest that the impacts of climate change on agriculture and food systems in African countries may have further-reaching consequences than previously anticipated. Thus, it has become imperative that African scientists and farmers adopt new technologies which facilitate their research and provide smart agricultural solutions to mitigating current and future climate change-related challenges. Advanced technologies have been developed across the globe to facilitate adaptation to climate change in the agriculture sector. Clustered regularly interspaced short palindromic repeats (CRISPR)-CRISPR-associated protein 9 (Cas9), synthetic biology, and genomic selection, among others, constitute examples of some of these technologies. In this work, emerging advanced technologies with the potential to effectively mitigate climate change in Africa are reviewed. The authors show how these technologies can be utilised to enhance knowledge discovery for increased production in a climate change-impacted environment. We conclude that the application of these technologies could empower African scientists to explore agricultural strategies more resilient to the effects of climate change. Additionally, we conclude that support for African scientists from the international community in various forms is necessary to help Africans avoid the full undesirable effects of climate change.
KW  - climate change
KW  - bioinformatics
KW  - Africa
KW  - OMICS
DO  - 10.3390/plants9030381
TY  - EJOU
AU  - Tomaszewski, Michał
AU  - Michalski, Paweł
AU  - Osuchowski, Jakub
TI  - Evaluation of Power Insulator Detection Efficiency with the Use of Limited Training Dataset
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 6
SN  - 2076-3417

AB  - This article presents an analysis of the effectiveness of object detection in digital images with the application of a limited quantity of input. The possibility of using a limited set of learning data was achieved by developing a detailed scenario of the task, which strictly defined the conditions of detector operation in the considered case of a convolutional neural network. The described solution utilizes known architectures of deep neural networks in the process of learning and object detection. The article presents comparisons of results from detecting the most popular deep neural networks while maintaining a limited training set composed of a specific number of selected images from diagnostic video. The analyzed input material was recorded during an inspection flight conducted along high-voltage lines. The object detector was built for a power insulator. The main contribution of the presented papier is the evidence that a limited training set (in our case, just 60 training frames) could be used for object detection, assuming an outdoor scenario with low variability of environmental conditions. The decision of which network will generate the best result for such a limited training set is not a trivial task. Conducted research suggests that the deep neural networks will achieve different levels of effectiveness depending on the amount of training data. The most beneficial results were obtained for two convolutional neural networks: the faster region-convolutional neural network (faster R-CNN) and the region-based fully convolutional network (R-FCN). Faster R-CNN reached the highest AP (average precision) at a level of 0.8 for 60 frames. The R-FCN model gained a worse AP result; however, it can be noted that the relationship between the number of input samples and the obtained results has a significantly lower influence than in the case of other CNN models, which, in the authors&rsquo; assessment, is a desired feature in the case of a limited training set.
KW  - convolutional neural network
KW  - deep neural network
KW  - insulator detection
KW  - efficiency evaluation
KW  - power system maintenance
DO  - 10.3390/app10062104
TY  - EJOU
AU  - Xu, Jin
AU  - Wang, Haixia
AU  - Cui, Can
AU  - Zhao, Baigang
AU  - Li, Bo
TI  - Oil Spill Monitoring of Shipborne Radar Image Features Using SVM and Local Adaptive Threshold
T2  - Algorithms

PY  - 2020
VL  - 13
IS  - 3
SN  - 1999-4893

AB  - In the case of marine accidents, monitoring marine oil spills can provide an important basis for identifying liabilities and assessing the damage. Shipborne radar can ensure large-scale, real-time monitoring, in all weather, with high-resolution. It therefore has the potential for broad applications in oil spill monitoring. Considering the original gray-scale image from the shipborne radar acquired in the case of the Dalian 7.16 oil spill accident, a complete oil spill detection method is proposed. Firstly, the co-frequency interferences and speckles in the original image are eliminated by preprocessing. Secondly, the wave information is classified using a support vector machine (SVM), and the effective wave monitoring area is generated according to the gray distribution matrix. Finally, oil spills are detected by a local adaptive threshold and displayed on an electronic chart based on geographic information system (GIS). The results show that the SVM can extract the effective wave information from the original shipborne radar image, and the local adaptive threshold method has strong applicability for oil film segmentation. This method can provide a technical basis for real-time cleaning and liability determination in oil spill accidents.
KW  - oil spill
KW  - SVM
KW  - real-time monitoring
KW  - shipborne radar
KW  - remote sensing
KW  - image processing
KW  - GIS
DO  - 10.3390/a13030069
TY  - EJOU
AU  - Nguyen, Truong L.
AU  - Han, DongYeob
TI  - Detection of Road Surface Changes from Multi-Temporal Unmanned Aerial Vehicle Images Using a Convolutional Siamese Network
T2  - Sustainability

PY  - 2020
VL  - 12
IS  - 6
SN  - 2071-1050

AB  - Road quality commonly decreases due to aging and deterioration of road surfaces. As the number of roads that need to be surveyed increases, general maintenance&mdash;particularly surveillance&mdash;can be quite costly if carried out using traditional methods. Therefore, using unmanned aerial vehicles (UAVs) and deep learning to detect changes via surveys is a promising strategy. This study proposes a method for detecting changes on road surfaces using pairs of UAV images captured at different times. First, a convolutional Siamese network is introduced to extract the features of an image pair and a Euclidean distance function is applied to calculate the distance between two features. Then, a contrastive loss function is used to enlarge the distance between changed feature pairs and reduce the distance between unchanged feature pairs. Finally, the initial change map is improved based on the preliminary differences between the two input images. Our experimental results confirm the effectiveness of this approach.
KW  - change detection
KW  - convolutional Siamese network
KW  - unmanned aerial vehicles
KW  - image processing
DO  - 10.3390/su12062482
TY  - EJOU
AU  - Elmes, Arthur
AU  - Alemohammad, Hamed
AU  - Avery, Ryan
AU  - Caylor, Kelly
AU  - Eastman, J. R.
AU  - Fishgold, Lewis
AU  - Friedl, Mark A.
AU  - Jain, Meha
AU  - Kohli, Divyani
AU  - Laso Bayas, Juan C.
AU  - Lunga, Dalton
AU  - McCarty, Jessica L.
AU  - Pontius, Robert G.
AU  - Reinmann, Andrew B.
AU  - Rogan, John
AU  - Song, Lei
AU  - Stoynova, Hristiana
AU  - Ye, Su
AU  - Yi, Zhuang-Fang
AU  - Estes, Lyndon
TI  - Accounting for Training Data Error in Machine Learning Applied to Earth Observations
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 6
SN  - 2072-4292

AB  - Remote sensing, or Earth Observation (EO), is increasingly used to understand Earth system dynamics and create continuous and categorical maps of biophysical properties and land cover, especially based on recent advances in machine learning (ML). ML models typically require large, spatially explicit training datasets to make accurate predictions. Training data (TD) are typically generated by digitizing polygons on high spatial-resolution imagery, by collecting in situ data, or by using pre-existing datasets. TD are often assumed to accurately represent the truth, but in practice almost always have error, stemming from (1) sample design, and (2) sample collection errors. The latter is particularly relevant for image-interpreted TD, an increasingly commonly used method due to its practicality and the increasing training sample size requirements of modern ML algorithms. TD errors can cause substantial errors in the maps created using ML algorithms, which may impact map use and interpretation. Despite these potential errors and their real-world consequences for map-based decisions, TD error is often not accounted for or reported in EO research. Here we review the current practices for collecting and handling TD. We identify the sources of TD error, and illustrate their impacts using several case studies representing different EO applications (infrastructure mapping, global surface flux estimates, and agricultural monitoring), and provide guidelines for minimizing and accounting for TD errors. To harmonize terminology, we distinguish TD from three other classes of data that should be used to create and assess ML models: training reference data, used to assess the quality of TD during data generation; validation data, used to iteratively improve models; and map reference data, used only for final accuracy assessment. We focus primarily on TD, but our advice is generally applicable to all four classes, and we ground our review in established best practices for map accuracy assessment literature. EO researchers should start by determining the tolerable levels of map error and appropriate error metrics. Next, TD error should be minimized during sample design by choosing a representative spatio-temporal collection strategy, by using spatially and temporally relevant imagery and ancillary data sources during TD creation, and by selecting a set of legend definitions supported by the data. Furthermore, TD error can be minimized during the collection of individual samples by using consensus-based collection strategies, by directly comparing interpreted training observations against expert-generated training reference data to derive TD error metrics, and by providing image interpreters with thorough application-specific training. We strongly advise that TD error is incorporated in model outputs, either directly in bias and variance estimates or, at a minimum, by documenting the sources and implications of error. TD should be fully documented and made available via an open TD repository, allowing others to replicate and assess its use. To guide researchers in this process, we propose three tiers of TD error accounting standards. Finally, we advise researchers to clearly communicate the magnitude and impacts of TD error on map outputs, with specific consideration given to the likely map audience.
KW  - training data
KW  - machine learning
KW  - map accuracy
KW  - error propagation
DO  - 10.3390/rs12061034
TY  - EJOU
AU  - Zhang, Weixing
AU  - Liljedahl, Anna K.
AU  - Kanevskiy, Mikhail
AU  - Epstein, Howard E.
AU  - Jones, Benjamin M.
AU  - Jorgenson, M. T.
AU  - Kent, Kelcy
TI  - Transferability of the Deep Learning Mask R-CNN Model for Automated Mapping of Ice-Wedge Polygons in High-Resolution Satellite and UAV Images
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - State-of-the-art deep learning technology has been successfully applied to relatively small selected areas of very high spatial resolution (0.15 and 0.25 m) optical aerial imagery acquired by a fixed-wing aircraft to automatically characterize ice-wedge polygons (IWPs) in the Arctic tundra. However, any mapping of IWPs at regional to continental scales requires images acquired on different sensor platforms (particularly satellite) and a refined understanding of the performance stability of the method across sensor platforms through reliable evaluation assessments. In this study, we examined the transferability of a deep learning Mask Region-Based Convolutional Neural Network (R-CNN) model for mapping IWPs in satellite remote sensing imagery (~0.5 m) covering 272 km2 and unmanned aerial vehicle (UAV) (0.02 m) imagery covering 0.32 km2. Multi-spectral images were obtained from the WorldView-2 satellite sensor and pan-sharpened to ~0.5 m, and a 20 mp CMOS sensor camera onboard a UAV, respectively. The training dataset included 25,489 and 6022 manually delineated IWPs from satellite and fixed-wing aircraft aerial imagery near the Arctic Coastal Plain, northern Alaska. Quantitative assessments showed that individual IWPs were correctly detected at up to 72% and 70%, and delineated at up to 73% and 68% F1 score accuracy levels for satellite and UAV images, respectively. Expert-based qualitative assessments showed that IWPs were correctly detected at good (40&ndash;60%) and excellent (80&ndash;100%) accuracy levels for satellite and UAV images, respectively, and delineated at excellent (80&ndash;100%) level for both images. We found that (1) regardless of spatial resolution and spectral bands, the deep learning Mask R-CNN model effectively mapped IWPs in both remote sensing satellite and UAV images; (2) the model achieved a better accuracy in detection with finer image resolution, such as UAV imagery, yet a better accuracy in delineation with coarser image resolution, such as satellite imagery; (3) increasing the number of training data with different resolutions between the training and actual application imagery does not necessarily result in better performance of the Mask R-CNN in IWPs mapping; (4) and overall, the model underestimates the total number of IWPs particularly in terms of disjoint/incomplete IWPs.
KW  - ice-wedge polygons
KW  - Arctic
KW  - deep learning
KW  - Mask R-CNN
KW  - WorldView-2
KW  - UAV
DO  - 10.3390/rs12071085
TY  - EJOU
AU  - Bao, Hanqing
AU  - Ming, Dongping
AU  - Guo, Ya
AU  - Zhang, Kui
AU  - Zhou, Keqi
AU  - Du, Shigao
TI  - DFCNN-Based Semantic Recognition of Urban Functional Zones by Integrating Remote Sensing Data and POI Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - The urban functional zone, as a special fundamental unit of the city, helps to understand the complex interaction between human space activities and environmental changes. Based on the recognition of physical and social semantics of buildings, combining remote sensing data and social sensing data is an effective way to quickly and accurately comprehend urban functional zone patterns. From the object level, this paper proposes a novel object-wise recognition strategy based on very high spatial resolution images (VHSRI) and social sensing data. First, buildings are extracted according to the physical semantics of objects; second, remote sensing and point of interest (POI) data are combined to comprehend the spatial distribution and functional semantics in the social function context; finally, urban functional zones are recognized and determined by building with physical and social functional semantics. When it comes to building geometrical information extraction, this paper, given the importance of building boundary information, introduces the deeper edge feature map (DEFM) into the segmentation and classification, and improves the result of building boundary recognition. Given the difficulty in understanding deeper semantics and spatial information and the limitation of traditional convolutional neural network (CNN) models in feature extraction, we propose the Deeper-Feature Convolutional Neural Network (DFCNN), which is able to extract more and deeper features for building semantic recognition. Experimental results conducted on a Google Earth image of Shenzhen City show that the proposed method and model are able to effectively, quickly, and accurately recognize urban functional zones by combining building physical semantics and social functional semantics, and are able to ensure the accuracy of urban functional zone recognition.
KW  - urban functional zones
KW  - semantic recognition
KW  - stratified scale estimation
KW  - deeper-feature CNN (DFCNN)
KW  - POIs
DO  - 10.3390/rs12071088
TY  - EJOU
AU  - Song, Ahram
AU  - Kim, Yongil
TI  - Transfer Change Rules from Recurrent Fully Convolutional Networks for Hyperspectral Unmanned Aerial Vehicle Images without Ground Truth Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - Change detection (CD) networks based on supervised learning have been used in diverse CD tasks. However, such supervised CD networks require a large amount of data and only use information from current images. In addition, it is time consuming to manually acquire the ground truth data for newly obtained images. Here, we proposed a novel method for CD in case of a lack of training data in an area near by another one with the available ground truth data. The proposed method automatically entails generating training data and fine-tuning the CD network. To detect changes in target images without ground truth data, the difference images were generated using spectral similarity measure, and the training data were selected via fuzzy c-means clustering. Recurrent fully convolutional networks with multiscale three-dimensional filters were used to extract objects of various sizes from unmanned aerial vehicle (UAV) images. The CD network was pre-trained on labeled source domain data; then, the network was fine-tuned on target images using generated training data. Two further CD networks were trained with a combined weighted loss function. The training data in the target domain were iteratively updated using he prediction map of the CD network. Experiments on two hyperspectral UAV datasets confirmed that the proposed method is capable of transferring change rules and improving CD results based on training data extracted in an unsupervised way.
KW  - change detection
KW  - hyperspectral unmanned aerial vehicle
KW  - spectral similarity measures
DO  - 10.3390/rs12071099
TY  - EJOU
AU  - Yuzugullu, Onur
AU  - Lorenz, Frank
AU  - Fröhlich, Peter
AU  - Liebisch, Frank
TI  - Understanding Fields by Remote Sensing: Soil Zoning and Property Mapping
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - Precision agriculture aims to optimize field management to increase agronomic yield, reduce environmental impact, and potentially foster soil carbon sequestration. In 2015, the Copernicus mission, with Sentinel-1 and -2, opened a new era by providing freely available high spatial and temporal resolution satellite data. Since then, many studies have been conducted to understand, monitor and improve agricultural systems. This paper presents results from the SolumScire project, focusing on the prediction of the spatial distribution of soil zones and topsoil properties, such as pH, soil organic matter (SOM) and clay content in agricultural fields through random forest algorithms. For this purpose, samples from 120 fields were investigated. The zoning and soil property prediction has an accuracy greater than 90%. This is supported by a high agreement of the derived zones with farmer&rsquo;s observations. The trained models revealed a prediction accuracy of 94%, 89% and 96% for pH, SOM and clay content, respectively. The obtained models for soil properties can support precision field management, the improvement of soil sampling and fertilization strategies, and eventually the management of soil properties such as SOM.
KW  - soil property prediction
KW  - pH
KW  - soil organic matter
KW  - soil clay content
KW  - precision agriculture
KW  - Copernicus mission
KW  - Sentinel
KW  - multi-spectral imagery
KW  - synthetic aperture radar imagery
KW  - machine learning
KW  - random forest
DO  - 10.3390/rs12071116
TY  - EJOU
AU  - Farhood, Helia
AU  - Perry, Stuart
AU  - Cheng, Eva
AU  - Kim, Juno
TI  - Enhanced 3D Point Cloud from a Light Field Image
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - The importance of three-dimensional (3D) point cloud technologies in the field of agriculture environmental research has increased in recent years. Obtaining dense and accurate 3D reconstructions of plants and urban areas provide useful information for remote sensing. In this paper, we propose a novel strategy for the enhancement of 3D point clouds from a single 4D light field (LF) image. Using a light field camera in this way creates an easy way for obtaining 3D point clouds from one snapshot and enabling diversity in monitoring and modelling applications for remote sensing. Considering an LF image and associated depth map as an input, we first apply histogram equalization and histogram stretching to enhance the separation between depth planes. We then apply multi-modal edge detection by using feature matching and fuzzy logic from the central sub-aperture LF image and the depth map. These two steps of depth map enhancement are significant parts of our novelty for this work. After combing the two previous steps and transforming the point&ndash;plane correspondence, we can obtain the 3D point cloud. We tested our method with synthetic and real world image databases. To verify the accuracy of our method, we compared our results with two different state-of-the-art algorithms. The results showed that our method can reliably mitigate noise and had the highest level of detail compared to other existing methods.
KW  - 3D point cloud
KW  - light field camera
KW  - 3D reconstruction
KW  - 3D modelling
KW  - three-dimensional data
KW  - enhanced depth map
DO  - 10.3390/rs12071125
TY  - EJOU
AU  - Kislov, Dmitry E.
AU  - Korznikov, Kirill A.
TI  - Automatic Windthrow Detection Using Very-High-Resolution Satellite Imagery and Deep Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - Wind disturbances are significant phenomena in forest spatial structure and succession dynamics. They cause changes in biodiversity, impact on forest ecosystems at different spatial scales, and have a strong influence on economics and human beings. The reliable recognition and mapping of windthrow areas are of high importance from the perspective of forest management and nature conservation. Recent research in artificial intelligence and computer vision has demonstrated the incredible potential of neural networks in addressing image classification problems. The most efficient algorithms are based on artificial neural networks of nested and complex architecture (e.g., convolutional neural networks (CNNs)), which are usually referred to by a common term&mdash;deep learning. Deep learning provides powerful algorithms for the precise segmentation of remote sensing data. We developed an algorithm based on a U-Net-like CNN, which was trained to recognize windthrow areas in Kunashir Island, Russia. We used satellite imagery of very-high spatial resolution (0.5 m/pixel) as source data. We performed a grid search among 216 parameter combinations defining different U-Net-like architectures. The best parameter combination allowed us to achieve an overall accuracy for recognition of windthrow sites of up to 94% for forested landscapes by coniferous and mixed coniferous forests. We found that the false-positive decisions of our algorithm correspond to either seashore logs, which may look similar to fallen tree trunks, or leafless forest stands. While the former can be rectified by applying a forest mask, the latter requires the usage of additional information, which is not always provided by satellite imagery.
KW  - convolutional neural network
KW  - deep learning
KW  - image segmentation
KW  - machine learning
KW  - forest disturbance
KW  - windthrow
DO  - 10.3390/rs12071145
TY  - EJOU
AU  - Deng, Lu
AU  - Chu, Hong-Hu
AU  - Shi, Peng
AU  - Wang, Wei
AU  - Kong, Xuan
TI  - Region-Based CNN Method with Deformable Modules for Visually Classifying Concrete Cracks
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 7
SN  - 2076-3417

AB  - Cracks are often the most intuitive indicators for assessing the condition of in-service structures. Intelligent detection methods based on regular convolutional neural networks (CNNs) have been widely applied to the field of crack detection in recently years; however, these methods exhibit unsatisfying performance on the detection of out-of-plane cracks. To overcome this drawback, a new type of region-based CNN (R-CNN) crack detector with deformable modules is proposed in the present study. The core idea of the method is to replace the traditional regular convolution and pooling operation with a deformable convolution operation and a deformable pooling operation. The idea is implemented on three different regular detectors, namely the Faster R-CNN, region-based fully convolutional networks (R-FCN), and feature pyramid network (FPN)-based Faster R-CNN. To examine the advantages of the proposed method, the results obtained from the proposed detector and corresponding regular detectors are compared. The results show that the addition of deformable modules improves the mean average precisions (mAPs) achieved by the Faster R-CNN, R-FCN, and FPN-based Faster R-CNN for crack detection. More importantly, adding deformable modules enables these detectors to detect the out-of-plane cracks that are difficult for regular detectors to detect.
KW  - structural health monitoring (SHM)
KW  - deep learning
KW  - convolutional neural network
KW  - deformable convolution
KW  - concrete cracks
KW  - out-of-plane crack
DO  - 10.3390/app10072528
TY  - EJOU
AU  - Feng, Chuncheng
AU  - Zhang, Hua
AU  - Wang, Haoran
AU  - Wang, Shuang
AU  - Li, Yonglong
TI  - Automatic Pixel-Level Crack Detection on Dam Surface Using Deep Convolutional Network
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 7
SN  - 1424-8220

AB  - Crack detection on dam surfaces is an important task for safe inspection of hydropower stations. More and more object detection methods based on deep learning are being applied to crack detection. However, most of the methods can only achieve the classification and rough location of cracks. Pixel-level crack detection can provide more intuitive and accurate detection results for dam health assessment. To realize pixel-level crack detection, a method of crack detection on dam surface (CDDS) using deep convolution network is proposed. First, we use an unmanned aerial vehicle (UAV) to collect dam surface images along a predetermined trajectory. Second, raw images are cropped. Then crack regions are manually labelled on cropped images to create the crack dataset, and the architecture of CDDS network is designed. Finally, the CDDS network is trained, validated and tested using the crack dataset. To validate the performance of the CDDS network, the predicted results are compared with ResNet152-based, SegNet, UNet and fully convolutional network (FCN). In terms of crack segmentation, the recall, precision, F-measure and IoU are 80.45%, 80.31%, 79.16%, and 66.76%. The results on test dataset show that the CDDS network has better performance for crack detection of dam surfaces.
KW  - crack detection
KW  - dam surface
KW  - UAV
KW  - pixel-level
KW  - deep convolutional network
DO  - 10.3390/s20072069
TY  - EJOU
AU  - Rahman, Md. M.
AU  - Avtar, Ram
AU  - Yunus, Ali P.
AU  - Dou, Jie
AU  - Misra, Prakhar
AU  - Takeuchi, Wataru
AU  - Sahu, Netrananda
AU  - Kumar, Pankaj
AU  - Johnson, Brian A.
AU  - Dasgupta, Rajarshi
AU  - Kharrazi, Ali
AU  - Chakraborty, Shamik
AU  - Agustiono Kurniawan, Tonni
TI  - Monitoring Effect of Spatial Growth on Land Surface Temperature in Dhaka
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - Spatial urban growth and its impact on land surface temperature (LST) is a high priority environmental issue for urban policy. Although the impact of horizontal spatial growth of cities on LST is well studied, the impact of the vertical spatial distribution of buildings on LST is under-investigated. This is particularly true for cities in sub-tropical developing countries. In this study, TerraSAR-X add-on for Digital Elevation Measurement (TanDEM-XDEM), Advanced Spaceborne Thermal Emission and Reflection (ASTER)-Global Digital Elevation Model (GDEM), and ALOS World 3D-30m (AW3D30) based Digital Surface Model (DSM) data were used to investigate the vertical growth of the Dhaka Metropolitan Area (DMA) in Bangladesh. Thermal Infrared (TIR) data (10.6-11.2&micro;m) of Landsat-8 were used to investigate the seasonal variations in LST. Thereafter, the impact of horizontal and vertical spatial growth on LST was studied. The result showed that: (a) TanDEM-X DSM derived building height had a higher accuracy as compared to other existing DSM that reveals mean building height of the Dhaka city is approximately 10 m, (b) built-up areas were estimated to cover approximately 94%, 88%, and 44% in Dhaka South City Corporation (DSCC), Dhaka North City Corporation (DNCC), and Fringe areas, respectively, of DMA using a Support Vector Machine (SVM) classification method, (c) the built-up showed a strong relationship with LST (Kendall tau coefficient of 0.625 in summer and 0.483 in winter) in comparison to vertical growth (Kendall tau coefficient of 0.156 in the summer and 0.059 in the winter), and (d) the &lsquo;low height-high density&rsquo; areas showed high LST in both seasons. This study suggests that vertical development is better than horizontal development for providing enough open spaces, green spaces, and preserving natural features. This study provides city planners with a better understating of sustainable urban planning and can promote the formulation of action plans for appropriate urban development policies.
KW  - spatial growth
KW  - LST
KW  - TanDEM-X -DSM
KW  - building height
KW  - urban planning
DO  - 10.3390/rs12071191
TY  - EJOU
AU  - Zhu, Yaohui
AU  - Yang, Guijun
AU  - Yang, Hao
AU  - Wu, Jintao
AU  - Lei, Lei
AU  - Zhao, Fa
AU  - Fan, Lingling
AU  - Zhao, Chunjiang
TI  - Identification of Apple Orchard Planting Year Based on Spatiotemporally Fused Satellite Images and Clustering Analysis of Foliage Phenophase
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - The planting year of apple orchard not only determines the fruit output but also provides information for the governmental management of the fruit industry. However, considering that different orchards use different management and cultivation methods, this may result in some trees having similar outlines but different planting years, and it is, therefore, difficult to effectively determine the actual planting year based on textural or structural characteristics. Therefore, the monitoring method provided in this paper is not to monitor the growing year positively from the planting of orchard seedlings but to use time series remote sensing data to reverse determine the continuous growth age of each existing orchard. The city of Qixia, Shandong Province, China, was used as a case study. Firstly, the spatial distribution of apple orchards was accurately extracted using the Sentinel-2 normalized difference vegetation index (NDVI) spatiotemporally fused images and phenological vegetation information. Secondly, using region of interest (ROI) data for different vegetation types obtained from a field survey, NDVI time series were extracted from the Sentinel-2 NDVI spatiotemporally fused image. Among them, three characteristic phenological periods were selected, and the NDVI time series for apple orchards was used as a template to extract the apple orchard distribution area from 2000 to 2017. Then, the distribution area of apple orchards was defined as the area of interest in the planting year, combined with the Landsat NDVI time series image composed of three characteristic phenological periods each year from 2000 to 2017, and the apple orchard phenological curve. Subsequently, a Euclidean distance (ED) method was used to calculate the distribution area of apple orchards for each year between 2000 and 2017. Finally, a pixel-by-pixel inverse time series calculation method was used to obtain the planting year of apple orchards in the study area. This study provides a new way to accurately identify the planting year of apple orchards using satellite remote sensing images.
KW  - apple orchard
KW  - planting year
KW  - remote sensing
KW  - spatiotemporal fusion
KW  - phenology
DO  - 10.3390/rs12071199
TY  - EJOU
AU  - Raza, Muhammad M.
AU  - Harding, Chris
AU  - Liebman, Matt
AU  - Leandro, Leonor F.
TI  - Exploring the Potential of High-Resolution Satellite Imagery for the Detection of Soybean Sudden Death Syndrome
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 7
SN  - 2072-4292

AB  - Sudden death syndrome (SDS) is one of the major yield-limiting soybean diseases in the Midwestern United States. Effective management for SDS requires accurate detection in soybean fields. Since traditional scouting methods are time-consuming, labor-intensive, and often destructive, alternative methods to monitor SDS in large soybean fields are needed. This study explores the potential of using high-resolution (3 m) PlanetScope satellite imagery for detection of SDS using the random forest classification algorithm. Image data from blue, green, red, and near-infrared (NIR) spectral bands, the calculated normalized difference vegetation index (NDVI), and crop rotation information were used to detect healthy and SDS-infected quadrats in a soybean field experiment with different rotation treatments, located in Boone County, Iowa. Datasets collected during the 2016, 2017, and 2018 soybean growing seasons were analyzed. The results indicate that spectral features, when combined with ground-based information, can detect areas in soybean plots that are at risk for disease, even before foliar symptoms develop. The classification of healthy and diseased soybean quadrats was &gt;75% accurate and the area under the receiver operating characteristic curve (AUROC) was &gt;70%. Our results indicate that high-resolution satellite imagery and random forest analyses have the potential to detect SDS in soybean fields, and that this approach may facilitate large-scale monitoring of SDS (and possibly other economically important soybean diseases). It may also be useful for guiding recommendations for site-specific management in current and future seasons.
KW  - soybean disease
KW  - sudden death syndrome
KW  - disease detection
KW  - remote sensing
KW  - PlanetScope
KW  - satellite imagery
KW  - random forest
DO  - 10.3390/rs12071213
TY  - EJOU
AU  - Silveira Kupssinskü, Lucas
AU  - Thomassim Guimarães, Tainá
AU  - Menezes de Souza, Eniuce
AU  - C. Zanotta, Daniel
AU  - Roberto Veronez, Mauricio
AU  - Gonzaga, Luiz
AU  - Mauad, Frederico F.
TI  - A Method for Chlorophyll-a and Suspended Solids Prediction through Remote Sensing and Machine Learning
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 7
SN  - 1424-8220

AB  - Total Suspended Solids (TSS) and chlorophyll-a concentration are two critical parameters to monitor water quality. Since directly collecting samples for laboratory analysis can be expensive, this paper presents a methodology to estimate this information through remote sensing and Machine Learning (ML) techniques. TSS and chlorophyll-a are optically active components, therefore enabling measurement by remote sensing. Two study cases in distinct water bodies are performed, and those cases use different spatial resolution data from Sentinel-2 spectral images and unmanned aerial vehicles together with laboratory analysis data. In consonance with the methodology, supervised ML algorithms are trained to predict the concentration of TSS and chlorophyll-a. The predictions are evaluated separately in both study areas, where both TSS and chlorophyll-a models achieved R-squared values above 0.8.
KW  - chlorophyll-a
KW  - total suspended solids
KW  - remote sensing
KW  - machine learning
KW  - artificial neural networks
KW  - random forest
KW  - K nearest neighbors
KW  - water quality
DO  - 10.3390/s20072125
TY  - EJOU
AU  - Barbedo, Jayme G.
AU  - Koenigkan, Luciano V.
AU  - Santos, Patrícia M.
AU  - Ribeiro, Andrea R.
TI  - Counting Cattle in UAV Images—Dealing with Clustered Animals and Animal/Background Contrast Changes
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 7
SN  - 1424-8220

AB  - The management of livestock in extensive production systems may be challenging, especially in large areas. Using Unmanned Aerial Vehicles (UAVs) to collect images from the area of interest is quickly becoming a viable alternative, but suitable algorithms for extraction of relevant information from the images are still rare. This article proposes a method for counting cattle which combines a deep learning model for rough animal location, color space manipulation to increase contrast between animals and background, mathematical morphology to isolate the animals and infer the number of individuals in clustered groups, and image matching to take into account image overlap. Using Nelore and Canchim breeds as a case study, the proposed approach yields accuracies over 90% under a wide variety of conditions and backgrounds.
KW  - unmanned aerial vehicles
KW  - Canchim breed
KW  - Nelore breed
KW  - convolutional neural networks
KW  - mathematical morphology
DO  - 10.3390/s20072126
TY  - EJOU
AU  - Xu, Zhiqiang
AU  - Chen, Yumin
AU  - Yang, Fan
AU  - Chu, Tianyou
AU  - Zhou, Hongyan
TI  - A Postearthquake Multiple Scene Recognition Model Based on Classical SSD Method and Transfer Learning
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 4
SN  - 2220-9964

AB  - The recognition of postearthquake scenes plays an important role in postearthquake rescue and reconstruction. To overcome the over-reliance on expert visual interpretation and the poor recognition performance of traditional machine learning in postearthquake scene recognition, this paper proposes a postearthquake multiple scene recognition (PEMSR) model based on the classical deep learning Single Shot MultiBox Detector (SSD) method. In this paper, a labeled postearthquake scenes dataset is constructed by segmenting acquired remote sensing images, which are classified into six categories: landslide, houses, ruins, trees, clogged and ponding. Due to the insufficiency and imbalance of the original dataset, transfer learning and a data augmentation and balancing strategy are utilized in the PEMSR model. To evaluate the PEMSR model, the evaluation metrics of precision, recall and F1 score are used in the experiment. Multiple experimental test results demonstrate that the PEMSR model shows a stronger performance in postearthquake scene recognition. The PEMSR model improves the detection accuracy of each scene compared with SSD by transfer learning and data augmentation strategy. In addition, the average detection time of the PEMSR model only needs 0.4565s, which is far less than the 8.3472s of the traditional Histogram of Oriented Gradient + Support Vector Machine (HOG+SVM) method.
KW  - earthquake disasters
KW  - scene recognition
KW  - deep learning
KW  - classical SSD method
KW  - transfer learning
DO  - 10.3390/ijgi9040238
TY  - EJOU
AU  - Pan, Xinliang
AU  - Jiang, Tao
AU  - Zhang, Zhen
AU  - Sui, Baikai
AU  - Liu, Chenxi
AU  - Zhang, Linjing
TI  - A New Method for Extracting Laver Culture Carriers Based on Inaccurate Supervised Classification with FCN-CRF
T2  - Journal of Marine Science and Engineering

PY  - 2020
VL  - 8
IS  - 4
SN  - 2077-1312

AB  - Timely monitoring of marine aquaculture has considerable significance for marine ecological protection and maritime safety and security. Considering that supervised learning needs to rely on a large number of training samples and the characteristics of intensive and regular distribution of the laver aquaculture zone, in this paper, an inaccurate supervised classification model based on fully convolutional neural network and conditional random filed (FCN-CRF) is designed for the study of a laver aquaculture zone in Lianyungang, Jiangsu Province. The proposed model can extract the aquaculture zone and calculate the area and quantity of laver aquaculture net simultaneously. The FCN is used to extract the laver aquaculture zone by roughly making the training label. Then, the CRF is used to extract the isolated laver aquaculture net with high precision. The results show that the     k a p p a     coefficient of the proposed model is 0.984, the      F 1      is 0.99, and the recognition effect is outstanding. For label production, the fault tolerance rate is high and does not affect the final classification accuracy, thereby saving more label production time. The findings provide a data basis for future aquaculture yield estimation and offshore resource planning as well as technical support for marine ecological supervision and marine traffic management.
KW  - laver culture
KW  - FCN
KW  - conditional random filed
KW  - inaccurate supervised classification
DO  - 10.3390/jmse8040274
TY  - EJOU
AU  - Šarlah, Nikolaj
AU  - Podobnikar, Tomaž
AU  - Ambrožič, Tomaž
AU  - Mušič, Branko
TI  - Application of Kinematic GPR-TPS Model with High 3D Georeference Accuracy for Underground Utility Infrastructure Mapping: A Case Study from Urban Sites in Celje, Slovenia
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 8
SN  - 2072-4292

AB  - This paper describes in detail the applicability of the developed ground-penetrating radar (GPR) model with a kinematic GPR and self-tracking (robotic) terrestrial positioning system (TPS) surveying setup (GPR-TPS model) for the acquisition, processing and visualisation of underground utility infrastructure (UUI) in a real urban environment. The integration of GPR with TPS can significantly improve the accuracy of UUI positioning in a real urban environment by means of efficient control of GPR trajectories. Two areas in the urban part of Celje in Slovenia were chosen. The accuracy of the kinematic GPR-TPS model was analysed by comparing the three-dimensional (3D) position of UUI given as reference values (true 3D position) from the officially consolidated cadastre of utility infrastructure in the Republic of Slovenia and those obtained by the GPR-TPS method. To determine the reference 3D position of the GPR antenna and UUI, the same positional and height geodetic network was used. Small unmanned aerial vehicles (UAV) were used for recording to provide a better spatial display of the results of UUI obtained with the GPR-TPS method. As demonstrated by the results, the kinematic GPR-TPS model for data acquisition can achieve an accuracy of fewer than 15 centimetres in a real urban environment.
KW  - kinematic GPR-TPS model
KW  - self-tracking terrestrial positioning system
KW  - underground utility infrastructure
KW  - unmanned aerial vehicle
KW  - horizontal accuracy
KW  - vertical accuracy
KW  - real urban environment
DO  - 10.3390/rs12081228
TY  - EJOU
AU  - El-Kadi, Omar
AU  - El-Shazly, Adel
AU  - Nassar, Khaled
TI  - Robust In-Plane Structures Oscillation Monitoring by Terrestrial Photogrammetry
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 8
SN  - 1424-8220

AB  - Oscillation monitoring commonly requires complex setups integrating various types of sensors associated with intensive computations to achieve an adequate rate of observations and accuracy. This research presents a simple, cost-effective approach that allows two-dimensional oscillation monitoring by terrestrial photogrammetry using non-metric cameras. Tedious camera calibration procedures are eliminated by using a grid target that allows geometric correction to be performed to the frame&rsquo;s region of interest at which oscillations are monitored. Region-based convolutional neural networks (Faster R-CNN) techniques are adopted to minimize the light exposure limitations, commonly constraining applications of terrestrial photogrammetry. The proposed monitoring procedure is tested at outdoor conditions to check its reliability and accuracy and examining the effect of using Faster R-CNN on monitoring results. The proposed artificial intelligence (AI) aided oscillation monitoring allowed sub-millimeter accuracy monitoring with observation rates up to 60 frames per second and gained the benefit of high optical zoom offered by market available bridge cameras to monitor oscillation of targets 100 m apart with high accuracy.
KW  - automation
KW  - deep learning
KW  - deformation monitoring
KW  - Faster R-CNN
KW  - image processing
KW  - oscillation monitoring
KW  - non-metric camera
KW  - terrestrial photogrammetry
KW  - tensor flow
KW  - video processing
DO  - 10.3390/s20082223
TY  - EJOU
AU  - Böhler, Jonas E.
AU  - Schaepman, Michael E.
AU  - Kneubühler, Mathias
TI  - Crop Separability from Individual and Combined Airborne Imaging Spectroscopy and UAV Multispectral Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 8
SN  - 2072-4292

AB  - Crop species separation is essential for a wide range of agricultural applications&mdash;in particular, when seasonal information is needed. In general, remote sensing can provide such information with high accuracy, but in small structured agricultural areas, very high spatial resolution data (VHR) are required. We present a study involving spectral and textural features derived from near-infrared (NIR) Red Green Blue (NIR-RGB) band datasets, acquired using an unmanned aerial vehicle (UAV), and an imaging spectroscopy (IS) dataset acquired by the Airborne Prism EXperiment (APEX). Both the single usage and combination of these datasets were analyzed using a random forest-based method for crop separability. In addition, different band reduction methods based on feature factor loading were analyzed. The most accurate crop separation results were achieved using both the IS dataset and the two combined datasets with an average accuracy (AA) of &gt;92%. In addition, we conclude that, in the case of a reduced number of IS features (i.e., wavelengths), the accuracy can be compensated by using additional NIR-RGB texture features (AA &gt; 90%).
KW  - crop separability
KW  - imaging spectroscopy
KW  - multispectral drone data
KW  - random forest
KW  - band reduction
DO  - 10.3390/rs12081256
TY  - EJOU
AU  - Miyoshi, Gabriela T.
AU  - Arruda, Mauro D.
AU  - Osco, Lucas P.
AU  - Marcato Junior, José
AU  - Gonçalves, Diogo N.
AU  - Imai, Nilton N.
AU  - Tommaselli, Antonio M.
AU  - Honkavaara, Eija
AU  - Gonçalves, Wesley N.
TI  - A Novel Deep Learning Method to Identify Single Tree Species in UAV-Based Hyperspectral Images
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 8
SN  - 2072-4292

AB  - Deep neural networks are currently the focus of many remote sensing approaches related to forest management. Although they return satisfactory results in most tasks, some challenges related to hyperspectral data remain, like the curse of data dimensionality. In forested areas, another common problem is the highly-dense distribution of trees. In this paper, we propose a novel deep learning approach for hyperspectral imagery to identify single-tree species in highly-dense areas. We evaluated images with 25 spectral bands ranging from 506 to 820 nm taken over a semideciduous forest of the Brazilian Atlantic biome. We included in our network&rsquo;s architecture a band combination selection phase. This phase learns from multiple combinations between bands which contributed the most for the tree identification task. This is followed by a feature map extraction and a multi-stage model refinement of the confidence map to produce accurate results of a highly-dense target. Our method returned an f-measure, precision and recall values of 0.959, 0.973, and 0.945, respectively. The results were superior when compared with a principal component analysis (PCA) approach. Compared to other learning methods, ours estimate a combination of hyperspectral bands that most contribute to the mentioned task within the network&rsquo;s architecture. With this, the proposed method achieved state-of-the-art performance for detecting and geolocating individual tree-species in UAV-based hyperspectral images in a complex forest.
KW  - high-density object
KW  - data-reduction
KW  - band selection
KW  - convolutional neural network
KW  - tree species identification
DO  - 10.3390/rs12081294
TY  - EJOU
AU  - Kouhdaragh, Vahid
AU  - Verde, Francesco
AU  - Gelli, Giacinto
AU  - Abouei, Jamshid
TI  - On the Application of Machine Learning to the Design of UAV-Based 5G Radio Access Networks
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 4
SN  - 2079-9292

AB  - A groundbreaking design of radio access networks (RANs) is needed to fulfill 5G traffic requirements. To this aim, a cost-effective and flexible strategy consists of complementing terrestrial RANs with unmanned aerial vehicles (UAVs). However, several problems must be solved in order to effectively deploy such UAV-based RANs (U-RANs). Indeed, due to the high complexity and heterogeneity of these networks, model-based design approaches, often relying on restrictive assumptions and constraints, exhibit severe limitation in real-world scenarios. Moreover, design of a set of appropriate protocols for such U-RANs is a highly sophisticated task. In this context, machine learning (ML) emerges as a useful tool to obtain practical and effective solutions. In this paper, we discuss why, how, and which types of ML methods are useful for designing U-RANs, by focusing in particular on supervised and reinforcement learning strategies.
KW  - 5G and beyond systems
KW  - machine learning
KW  - radio access networks
KW  - reinforcement learning
KW  - supervised learning
KW  - unmanned aerial vehicles (UAVs)
DO  - 10.3390/electronics9040689
TY  - EJOU
AU  - Alsharif, Mohammed H.
AU  - Kelechi, Anabi H.
AU  - Albreem, Mahmoud A.
AU  - Chaudhry, Shehzad A.
AU  - Zia, M. S.
AU  - Kim, Sunghwan
TI  - Sixth Generation (6G) Wireless Networks: Vision, Research Activities, Challenges and Potential Solutions
T2  - Symmetry

PY  - 2020
VL  - 12
IS  - 4
SN  - 2073-8994

AB  - The standardization activities of the fifth generation communications are clearly over and deployment has commenced globally. To sustain the competitive edge of wireless networks, industrial and academia synergy have begun to conceptualize the next generation of wireless communication systems (namely, sixth generation, (6G)) aimed at laying the foundation for the stratification of the communication needs of the 2030s. In support of this vision, this study highlights the most promising lines of research from the recent literature in common directions for the 6G project. Its core contribution involves exploring the critical issues and key potential features of 6G communications, including: (i) vision and key features; (ii) challenges and potential solutions; and (iii) research activities. These controversial research topics were profoundly examined in relation to the motivation of their various sub-domains to achieve a precise, concrete, and concise conclusion. Thus, this article will contribute significantly to opening new horizons for future research directions.
KW  - wireless networks
KW  - beyond 5G
KW  - 6G
KW  - 6G mobile communication
KW  - terahertz communications
KW  - holographic communications
KW  - terahertz spectrum
KW  - visible-light communications
DO  - 10.3390/sym12040676
TY  - EJOU
AU  - Chen, Ting
AU  - He, Haiqing
AU  - Li, Dajun
AU  - An, Puyang
AU  - Hui, Zhenyang
TI  - Damage Signature Generation of Revetment Surface along Urban Rivers Using UAV-Based Mapping
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 4
SN  - 2220-9964

AB  - The all-embracing inspection of geometry structures of revetments along urban rivers using the conventional field visual inspection is technically complex and time-consuming. In this study, an approach using dense point clouds derived from low-cost unmanned aerial vehicle (UAV) photogrammetry is proposed to automatically and efficiently recognize the signatures of revetment damage. To quickly and accurately recover the finely detailed surface of a revetment, an object space-based dense matching approach, that is, region growing coupled with semi-global matching, is exploited to generate pixel-by-pixel dense point clouds for characterizing the signatures of revetment damage. Then, damage recognition is conducted using a proposed operator, that is, a self-adaptive and multiscale gradient operator, which is designed to extract the damaged regions with different sizes in the slope intensity image of the revetment. A revetment with slope protection along urban rivers is selected to evaluate the performance of damage recognition. Results indicate that the proposed approach can be considered an effective alternative to field visual inspection for revetment damage recognition along urban rivers because our method not only recovers the finely detailed surface of the revetment but also remarkably improves the accuracy of revetment damage recognition.
KW  - revetment
KW  - damage signature
KW  - dense point clouds
KW  - unmanned aerial vehicle (UAV)
KW  - gradient operator
DO  - 10.3390/ijgi9040283
TY  - EJOU
AU  - Tang, Ziyang
AU  - Liu, Xiang
AU  - Chen, Hanlin
AU  - Hupy, Joseph
AU  - Yang, Baijian
TI  - Deep Learning Based Wildfire Event Object Detection from 4K Aerial Images Acquired by UAS
T2  - AI

PY  - 2020
VL  - 1
IS  - 2
SN  - 2673-2688

AB  - Unmanned Aerial Systems, hereafter referred to as UAS, are of great use in hazard events such as wildfire due to their ability to provide high-resolution video imagery over areas deemed too dangerous for manned aircraft and ground crews. This aerial perspective allows for identification of ground-based hazards such as spot fires and fire lines, and to communicate this information with fire fighting crews. Current technology relies on visual interpretation of UAS imagery, with little to no computer-assisted automatic detection. With the help of big labeled data and the significant increase of computing power, deep learning has seen great successes on object detection with fixed patterns, such as people and vehicles. However, little has been done for objects, such as spot fires, with amorphous and irregular shapes. Additional challenges arise when data are collected via UAS as high-resolution aerial images or videos; an ample solution must provide reasonable accuracy with low delays. In this paper, we examined 4K (    3840 × 2160    ) videos collected by UAS from a controlled burn and created a set of labeled video sets to be shared for public use. We introduce a coarse-to-fine framework to auto-detect wildfires that are sparse, small, and irregularly-shaped. The coarse detector adaptively selects the sub-regions that are likely to contain the objects of interest while the fine detector passes only the details of the sub-regions, rather than the entire 4K region, for further scrutiny. The proposed two-phase learning therefore greatly reduced time overhead and is capable of maintaining high accuracy. Compared against the real-time one-stage object backbone of YoloV3, the proposed methods improved the mean average precision(mAP) from     0 . 29     to     0 . 67    , with an average inference speed of 7.44 frames per second. Limitations and future work are discussed with regard to the design and the experiment results.
KW  - wildfire detection
KW  - deep learning
KW  - unmanned aerial systems
KW  - high resolution images
KW  - dataset
DO  - 10.3390/ai1020010
TY  - EJOU
AU  - Zhang, Jian
AU  - Xie, Tianjin
AU  - Yang, Chenghai
AU  - Song, Huaibo
AU  - Jiang, Zhao
AU  - Zhou, Guangsheng
AU  - Zhang, Dongyan
AU  - Feng, Hui
AU  - Xie, Jing
TI  - Segmenting Purple Rapeseed Leaves in the Field from UAV RGB Imagery Using Deep Learning as an Auxiliary Means for Nitrogen Stress Detection
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 9
SN  - 2072-4292

AB  - Crop leaf purpling is a common phenotypic change when plants are subject to some biotic and abiotic stresses during their growth. The extraction of purple leaves can monitor crop stresses as an apparent trait and meanwhile contributes to crop phenotype analysis, monitoring, and yield estimation. Due to the complexity of the field environment as well as differences in size, shape, texture, and color gradation among the leaves, purple leaf segmentation is difficult. In this study, we used a U-Net model for segmenting purple rapeseed leaves during the seedling stage based on unmanned aerial vehicle (UAV) RGB imagery at the pixel level. With the limited spatial resolution of rapeseed images acquired by UAV and small object size, the input patch size was carefully selected. Experiments showed that the U-Net model with the patch size of 256 &times; 256 pixels obtained better and more stable results with a F-measure of 90.29% and an Intersection of Union (IoU) of 82.41%. To further explore the influence of image spatial resolution, we evaluated the performance of the U-Net model with different image resolutions and patch sizes. The U-Net model performed better compared with four other commonly used image segmentation approaches comprising support vector machine, random forest, HSeg, and SegNet. Moreover, regression analysis was performed between the purple rapeseed leaf ratios and the measured N content. The negative exponential model had a coefficient of determination (R&sup2;) of 0.858, thereby explaining much of the rapeseed leaf purpling in this study. This purple leaf phenotype could be an auxiliary means for monitoring crop growth status so that crops could be managed in a timely and effective manner when nitrogen stress occurs. Results demonstrate that the U-Net model is a robust method for purple rapeseed leaf segmentation and that the accurate segmentation of purple leaves provides a new method for crop nitrogen stress monitoring.
KW  - purple rapeseed leaves
KW  - unmanned aerial vehicle
KW  - U-Net
KW  - plant segmentation
KW  - nitrogen stress
DO  - 10.3390/rs12091403
TY  - EJOU
AU  - Mazzia, Vittorio
AU  - Comba, Lorenzo
AU  - Khaliq, Aleem
AU  - Chiaberge, Marcello
AU  - Gay, Paolo
TI  - UAV and Machine Learning Based Refinement of a Satellite-Driven Vegetation Index for Precision Agriculture
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 9
SN  - 1424-8220

AB  - Precision agriculture is considered to be a fundamental approach in pursuing a low-input, high-efficiency, and sustainable kind of agriculture when performing site-specific management practices. To achieve this objective, a reliable and updated description of the local status of crops is required. Remote sensing, and in particular satellite-based imagery, proved to be a valuable tool in crop mapping, monitoring, and diseases assessment. However, freely available satellite imagery with low or moderate resolutions showed some limits in specific agricultural applications, e.g., where crops are grown by rows. Indeed, in this framework, the satellite&rsquo;s output could be biased by intra-row covering, giving inaccurate information about crop status. This paper presents a novel satellite imagery refinement framework, based on a deep learning technique which exploits information properly derived from high resolution images acquired by unmanned aerial vehicle (UAV) airborne multispectral sensors. To train the convolutional neural network, only a single UAV-driven dataset is required, making the proposed approach simple and cost-effective. A vineyard in Serralunga d&rsquo;Alba (Northern Italy) was chosen as a case study for validation purposes. Refined satellite-driven normalized difference vegetation index (NDVI) maps, acquired in four different periods during the vine growing season, were shown to better describe crop status with respect to raw datasets by correlation analysis and ANOVA. In addition, using a K-means based classifier, 3-class vineyard vigor maps were profitably derived from the NDVI maps, which are a valuable tool for growers.
KW  - precision agriculture
KW  - remote sensing
KW  - moderate resolution satellite imagery
KW  - UAV
KW  - convolutional neural network
DO  - 10.3390/s20092530
TY  - EJOU
AU  - Abdollahi, Abolfazl
AU  - Pradhan, Biswajeet
AU  - Shukla, Nagesh
AU  - Chakraborty, Subrata
AU  - Alamri, Abdullah
TI  - Deep Learning Approaches Applied to Remote Sensing Datasets for Road Extraction: A State-Of-The-Art Review
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 9
SN  - 2072-4292

AB  - One of the most challenging research subjects in remote sensing is feature extraction, such as road features, from remote sensing images. Such an extraction influences multiple scenes, including map updating, traffic management, emergency tasks, road monitoring, and others. Therefore, a systematic review of deep learning techniques applied to common remote sensing benchmarks for road extraction is conducted in this study. The research is conducted based on four main types of deep learning methods, namely, the GANs model, deconvolutional networks, FCNs, and patch-based CNNs models. We also compare these various deep learning models applied to remote sensing datasets to show which method performs well in extracting road parts from high-resolution remote sensing images. Moreover, we describe future research directions and research gaps. Results indicate that the largest reported performance record is related to the deconvolutional nets applied to remote sensing images, and the F1 score metric of the generative adversarial network model, DenseNet method, and FCN-32 applied to UAV and Google Earth images are high: 96.08%, 95.72%, and 94.59%, respectively.
KW  - road extraction
KW  - common benchmarks
KW  - machine learning
KW  - deep learning
KW  - remote sensing
DO  - 10.3390/rs12091444
TY  - EJOU
AU  - Gorkin, Robert
AU  - Adams, Kye
AU  - Berryman, Matthew J.
AU  - Aubin, Sam
AU  - Li, Wanqing
AU  - Davis, Andrew R.
AU  - Barthelemy, Johan
TI  - Sharkeye: Real-Time Autonomous Personal Shark Alerting via Aerial Surveillance
T2  - Drones

PY  - 2020
VL  - 4
IS  - 2
SN  - 2504-446X

AB  - While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens.
KW  - UAV
KW  - blimp
KW  - shark spotting
KW  - machine learning
KW  - wearables
DO  - 10.3390/drones4020018
TY  - EJOU
AU  - Messina, Gaetano
AU  - Modica, Giuseppe
TI  - Applications of UAV Thermal Imagery in Precision Agriculture: State of the Art and Future Research Outlook
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 9
SN  - 2072-4292

AB  - Low-altitude remote sensing (RS) using unmanned aerial vehicles (UAVs) is a powerful tool in precision agriculture (PA). In that context, thermal RS has many potential uses. The surface temperature of plants changes rapidly under stress conditions, which makes thermal RS a useful tool for real-time detection of plant stress conditions. Current applications of UAV thermal RS include monitoring plant water stress, detecting plant diseases, assessing crop yield estimation, and plant phenotyping. However, the correct use and interpretation of thermal data are based on basic knowledge of the nature of thermal radiation. Therefore, aspects that are related to calibration and ground data collection, in which the use of reference panels is highly recommended, as well as data processing, must be carefully considered. This paper aims to review the state of the art of UAV thermal RS in agriculture, outlining an overview of the latest applications and providing a future research outlook.
KW  - unmanned aerial vehicles (UAVs)
KW  - remote sensing (RS)
KW  - thermal UAV RS
KW  - thermal infrared (TIR)
KW  - precision agriculture (PA)
KW  - crop water stress monitoring
KW  - plant disease detection
KW  - yield estimation
KW  - vegetation status monitoring
DO  - 10.3390/rs12091491
TY  - EJOU
AU  - Jakovljevic, Gordana
AU  - Govedarica, Miro
AU  - Alvarez-Taboada, Flor
TI  - A Deep Learning Model for Automatic Plastic Mapping Using Unmanned Aerial Vehicle (UAV) Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 9
SN  - 2072-4292

AB  - Although plastic pollution is one of the most noteworthy environmental issues nowadays, there is still a knowledge gap in terms of monitoring the spatial distribution of plastics, which is needed to prevent its negative effects and to plan mitigation actions. Unmanned Aerial Vehicles (UAVs) can provide suitable data for mapping floating plastic, but most of the methods require visual interpretation and manual labeling. The main goals of this paper are to determine the suitability of deep learning algorithms for automatic floating plastic extraction from UAV orthophotos, testing the possibility of differentiating plastic types, and exploring the relationship between spatial resolution and detectable plastic size, in order to define a methodology for UAV surveys to map floating plastic. Two study areas and three datasets were used to train and validate the models. An end-to-end semantic segmentation algorithm based on U-Net architecture using the ResUNet50 provided the highest accuracy to map different plastic materials (F1-score: Oriented Polystyrene (OPS): 0.86; Nylon: 0.88; Polyethylene terephthalate (PET): 0.92; plastic (in general): 0.78), showing its ability to identify plastic types. The classification accuracy decreased with the decrease in spatial resolution, performing best on 4 mm resolution images for all kinds of plastic. The model provided reliable estimates of the area and volume of the plastics, which is crucial information for a cleaning campaign.
KW  - deep learning
KW  - mapping plastic
KW  - automatic detection
KW  - AI
KW  - remote sensing
KW  - UAV
KW  - segmentation
DO  - 10.3390/rs12091515
TY  - EJOU
AU  - Rodoshi, Rehenuma T.
AU  - Kim, Taewoon
AU  - Choi, Wooyeol
TI  - Resource Management in Cloud Radio Access Network: Conventional and New Approaches
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 9
SN  - 1424-8220

AB  - Cloud radio access network (C-RAN) is a promising mobile wireless sensor network architecture to address the challenges of ever-increasing mobile data traffic and network costs. C-RAN is a practical solution to the strict energy-constrained wireless sensor nodes, often found in Internet of Things (IoT) applications. Although this architecture can provide energy efficiency and reduce cost, it is a challenging task in C-RAN to utilize the resources efficiently, considering the dynamic real-time environment. Several research works have proposed different methodologies for effective resource management in C-RAN. This study performs a comprehensive survey on the state-of-the-art resource management techniques that have been proposed recently for this architecture. The resource management techniques are categorized into computational resource management (CRM) and radio resource management (RRM) techniques. Then both of the techniques are further classified and analyzed based on the strategies used in the studies. Remote radio head (RRH) clustering schemes used in CRM techniques are discussed extensively. In this research work, the investigated performance metrics and their validation techniques are critically analyzed. Moreover, other important challenges and open research issues for efficient resource management in C-RAN are highlighted to provide future research direction.
KW  - cloud radio access network
KW  - resource management
KW  - RRM
KW  - CRM
KW  - RRH clustering
DO  - 10.3390/s20092708
TY  - EJOU
AU  - Khaki, Saeed
AU  - Pham, Hieu
AU  - Han, Ye
AU  - Kuhl, Andy
AU  - Kent, Wade
AU  - Wang, Lizhi
TI  - Convolutional Neural Networks for Image-Based Corn Kernel Detection and Counting
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 9
SN  - 1424-8220

AB  - Precise in-season corn grain yield estimates enable farmers to make real-time accurate harvest and grain marketing decisions minimizing possible losses of profitability. A well developed corn ear can have up to 800 kernels, but manually counting the kernels on an ear of corn is labor-intensive, time consuming and prone to human error. From an algorithmic perspective, the detection of the kernels from a single corn ear image is challenging due to the large number of kernels at different angles and very small distance among the kernels. In this paper, we propose a kernel detection and counting method based on a sliding window approach. The proposed method detects and counts all corn kernels in a single corn ear image taken in uncontrolled lighting conditions. The sliding window approach uses a convolutional neural network (CNN) for kernel detection. Then, a non-maximum suppression (NMS) is applied to remove overlapping detections. Finally, windows that are classified as kernel are passed to another CNN regression model for finding the     ( x , y )     coordinates of the center of kernel image patches. Our experiments indicate that the proposed method can successfully detect the corn kernels with a low detection error and is also able to detect kernels on a batch of corn ears positioned at different angles.
KW  - corn kernel counting
KW  - object detection
KW  - convolutional neural networks
KW  - digital agriculture
DO  - 10.3390/s20092721
TY  - EJOU
AU  - Jiang, Ling
AU  - Hu, Yang
AU  - Xia, Xilin
AU  - Liang, Qiuhua
AU  - Soltoggio, Andrea
AU  - Kabir, Syed R.
TI  - A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for Reconstructing High-Resolution Urban DEMs
T2  - Water

PY  - 2020
VL  - 12
IS  - 5
SN  - 2073-4441

AB  - The scarcity of high-resolution urban digital elevation model (DEM) datasets, particularly in certain developing countries, has posed a challenge for many water-related applications such as flood risk management. A solution to address this is to develop effective approaches to reconstruct high-resolution DEMs from their low-resolution equivalents that are more widely available. However, the current high-resolution DEM reconstruction approaches mainly focus on natural topography. Few attempts have been made for urban topography, which is typically an integration of complex artificial and natural features. This study proposed a novel multi-scale mapping approach based on convolutional neural network (CNN) to deal with the complex features of urban topography and to reconstruct high-resolution urban DEMs. The proposed multi-scale CNN model was firstly trained using urban DEMs that contained topographic features at different resolutions, and then used to reconstruct the urban DEM at a specified (high) resolution from a low-resolution equivalent. A two-level accuracy assessment approach was also designed to evaluate the performance of the proposed urban DEM reconstruction method, in terms of numerical accuracy and morphological accuracy. The proposed DEM reconstruction approach was applied to a 121 km2 urbanized area in London, United Kingdom. Compared with other commonly used methods, the current CNN-based approach produced superior results, providing a cost-effective innovative method to acquire high-resolution DEMs in other data-scarce regions.
KW  - urban DEM
KW  - high resolution
KW  - deep learning
KW  - convolutional neural network
KW  - multiple scales
KW  - flood modeling
DO  - 10.3390/w12051369
TY  - EJOU
AU  - Azimi, Mohsen
AU  - Eslamlou, Armin D.
AU  - Pekcan, Gokhan
TI  - Data-Driven Structural Health Monitoring and Damage Detection through Deep Learning: State-of-the-Art Review
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 10
SN  - 1424-8220

AB  - Data-driven methods in structural health monitoring (SHM) is gaining popularity due to recent technological advancements in sensors, as well as high-speed internet and cloud-based computation. Since the introduction of deep learning (DL) in civil engineering, particularly in SHM, this emerging and promising tool has attracted significant attention among researchers. The main goal of this paper is to review the latest publications in SHM using emerging DL-based methods and provide readers with an overall understanding of various SHM applications. After a brief introduction, an overview of various DL methods (e.g., deep neural networks, transfer learning, etc.) is presented. The procedure and application of vibration-based, vision-based monitoring, along with some of the recent technologies used for SHM, such as sensors, unmanned aerial vehicles (UAVs), etc. are discussed. The review concludes with prospects and potential limitations of DL-based methods in SHM applications.
KW  - deep learning
KW  - machine learning
KW  - structural health monitoring
KW  - crack detection
KW  - damage detection
KW  - data science
KW  - computer vision
DO  - 10.3390/s20102778
TY  - EJOU
AU  - Chun, Pang-jo
AU  - Yamane, Tatsuro
AU  - Izumi, Shota
AU  - Kuramoto, Naoya
TI  - Development of a Machine Learning-Based Damage Identification Method Using Multi-Point Simultaneous Acceleration Measurement Results
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 10
SN  - 1424-8220

AB  - It is necessary to assess damage properly for the safe use of a structure and for the development of an appropriate maintenance strategy. Although many efforts have been made to measure the vibration of a structure to determine the degree of damage, the accuracy of evaluation is not high enough, so it is difficult to say that a damage evaluation based on vibrations in a structure has not been put to practical use. In this study, we propose a method to evaluate damage by measuring the acceleration of a structure at multiple points and interpreting the results with a Random Forest, which is a kind of supervised machine learning. The proposed method uses the maximum response acceleration, standard deviation, logarithmic decay rate, and natural frequency to improve the accuracy of damage assessment. We propose a three-step Random Forest method to evaluate various damage types based on the results of these many measurements. Then, the accuracy of the proposed method is verified based on the results of a cross-validation and a vibration test of an actual damaged specimen.
KW  - artificial intelligence
KW  - machine learning
KW  - Random Forest
KW  - vibration
KW  - damage detection
KW  - damage evaluation
DO  - 10.3390/s20102780
TY  - EJOU
AU  - Hong, Suk-Ju
AU  - Kim, Sang-Yeon
AU  - Kim, Eungchan
AU  - Lee, Chang-Hyup
AU  - Lee, Jung-Sup
AU  - Lee, Dong-Soo
AU  - Bang, Jiwoong
AU  - Kim, Ghiseok
TI  - Moth Detection from Pheromone Trap Images Using Deep Learning Object Detectors
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 5
SN  - 2077-0472

AB  - Diverse pheromones and pheromone-based traps, as well as images acquired from insects captured by pheromone-based traps, have been studied and developed to monitor the presence and abundance of pests and to protect plants. The purpose of this study is to construct models that detect three species of pest moths in pheromone trap images using deep learning object detection methods and compare their speed and accuracy. Moth images in pheromone traps were collected for training and evaluation of deep learning detectors. Collected images were then subjected to a labeling process that defines the ground truths of target objects for their box locations and classes. Because there were a few negative objects in the dataset, non-target insects were labeled as unknown class and images of non-target insects were added to the dataset. Moreover, data augmentation methods were applied to the training process, and parameters of detectors that were pre-trained with the COCO dataset were used as initial parameter values. Seven detectors&mdash;Faster R-CNN ResNet 101, Faster R-CNN ResNet 50, Faster R-CNN Inception v.2, R-FCN ResNet 101, Retinanet ResNet 50, Retinanet Mobile v.2, and SSD Inception v.2 were trained and evaluated. Faster R-CNN ResNet 101 detector exhibited the highest accuracy (mAP as 90.25), and seven different detector types showed different accuracy and speed. Furthermore, when unexpected insects were included in the collected images, a four-class detector with an unknown class (non-target insect) showed lower detection error than a three-class detector.
KW  - pheromone trap
KW  - pest
KW  - moth
KW  - deep learning
KW  - horticulture
KW  - insect detection
DO  - 10.3390/agriculture10050170
TY  - EJOU
AU  - Zhang, Yishan
AU  - Wu, Lun
AU  - Ren, Huazhong
AU  - Deng, Licui
AU  - Zhang, Pengcheng
TI  - Retrieval of Water Quality Parameters from Hyperspectral Images Using Hybrid Bayesian Probabilistic Neural Network
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 10
SN  - 2072-4292

AB  - The protection of water resources is of paramount importance to human beings&rsquo; practical lives. Monitoring and improving water quality nowadays has become an important topic. In this study, a novel Bayesian probabilistic neural network (BPNN) improved from ordinary Bayesian probability methods has been developed to quantitatively predict water quality parameters including phosphorus, nitrogen, chemical oxygen demand (COD), biochemical oxygen demand (BOD), and chlorophyll a. The proposed method, based on conventional Bayesian probability methods, involves feature engineering and deep neural networks. Additionally, it extracts significant information for each endmember from combinations of spectra by feature extraction, with spectral unmixing based on mathematical and statistical analysis, and calculates each of the water quality parameters. The experimental results show the great performance of the proposed model with all coefficient of determination      R 2      over 0.9 greater than the values (0.6&ndash;0.8) from conventional methods, which are greater than ordinary Bayesian probability analysis. The mean percent of absolute error (MPAE) is taken into account as an important statistical criterion to evaluate model performance, and our results show that MPAE ranges from 4% (nitrogen) to 10% (COD). The root mean squared errors (RMSEs) of phosphorus, nitrogen, COD, BOD, and chlorophyll-a (Chla) are 0.03 mg/L, 0.28 mg/L, 3.28 mg/L, 0.49 mg/L, and 0.75 &mu;g/L, respectively. In comparison with other deep learning methods, this study takes a relatively small amount of data as training data to train the proposed model and the proposed model is then tested on the same amount of testing data, achieving a greater performance. Thus, the proposed method is time-saving and more effective. This study proposes a more compatible and effective method to assist with decomposing combinations of hyperspectral signatures in order to calculate the content level of each water quality parameter. Moreover, the proposed method is practically applied to hyperspectral image data on board an unmanned aerial vehicle in order to monitor the water quality on a large scale and trace the location of pollution sources in the Maozhou River, Guangdong Province of China, obtaining well-explained and significant results.
KW  - Bayesian neural network
KW  - deep learning
KW  - spectral unmixing
KW  - hyperspectral remote sensing
KW  - water quality monitoring
KW  - machine learning
DO  - 10.3390/rs12101567
TY  - EJOU
AU  - Pan, Zhuokun
AU  - Xu, Jiashu
AU  - Guo, Yubin
AU  - Hu, Yueming
AU  - Wang, Guangxing
TI  - Deep Learning Segmentation and Classification for Urban Village Using a Worldview Satellite Image Based on U-Net
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 10
SN  - 2072-4292

AB  - Unplanned urban settlements exist worldwide. The geospatial information of these areas is critical for urban management and reconstruction planning but usually unavailable. Automatically characterizing individual buildings in the unplanned urban village using remote sensing imagery is very challenging due to complex landscapes and high-density settlements. The newly emerging deep learning method provides the potential to characterize individual buildings in a complex urban village. This study proposed an urban village mapping paradigm based on U-net deep learning architecture. The study area is located in Guangzhou City, China. The Worldview satellite image with eight pan-sharpened bands at a 0.5-m spatial resolution and building boundary vector file were used as research purposes. There are ten sites of the urban villages included in this scene of the Worldview image. The deep neural network model was trained and tested based on the selected six and four sites of the urban village, respectively. Models for building segmentation and classification were both trained and tested. The results indicated that the U-net model reached overall accuracy over 86% for building segmentation and over 83% for the classification. The F1-score ranged from 0.9 to 0.98 for the segmentation, and from 0.63 to 0.88 for the classification. The Interaction over Union reached over 90% for the segmentation and 86% for the classification. The superiority of the deep learning method has been demonstrated through comparison with Random Forest and object-based image analysis. This study fully showed the feasibility, efficiency, and potential of the deep learning in delineating individual buildings in the high-density urban village. More importantly, this study implied that through deep learning methods, mapping unplanned urban settlements could further characterize individual buildings with considerable accuracy.
KW  - deep learning
KW  - urban village settlement
KW  - Worldview imagery
KW  - U-net
KW  - segmentation
KW  - Guangzhou
DO  - 10.3390/rs12101574
TY  - EJOU
AU  - Cholula, Uriel
AU  - da Silva, Jorge A.
AU  - Marconi, Thiago
AU  - Thomasson, J. A.
AU  - Solorzano, Jorge
AU  - Enciso, Juan
TI  - Forecasting Yield and Lignocellulosic Composition of Energy Cane Using Unmanned Aerial Systems
T2  - Agronomy

PY  - 2020
VL  - 10
IS  - 5
SN  - 2073-4395

AB  - Crop monitoring and appropriate agricultural management practices of elite germplasm will enhance bioenergy&rsquo;s efficiency. Unmanned aerial systems (UAS) may be a useful tool for this purpose. The objective of this study was to assess the use of UAS with true color and multispectral imagery to predict the yield and total cellulosic content (TCC) of newly created energy cane germplasm. A trial was established in the growing season of 2016 at the Texas A&amp;M AgriLife Research Center in Weslaco, Texas, where 15 energy cane elite lines and three checks were grown on experimental plots, arranged in a complete block design and replicated four times. Four flights were executed at different growth stages in 2018, at the first ratoon crop, using two multi-rotor UAS: the DJI Phantom 4 Pro equipped with RGB camera and the DJI Matrice 100, equipped with multispectral sensor (SlantRange 3p). Canopy cover, canopy height, NDVI (Normalized Difference Vegetation Index), and ExG (Excess Green Index) were extracted from the images and used to perform a stepwise regression to obtain the yield and TCC models. The results showed a good agreement between the predicted and the measured yields (R2 = 0.88); however, a low coefficient of determination was found between the predicted and the observed TCC (R2 = 0.30). This study demonstrated the potential application of UAS to estimate energy cane yield with high accuracy, enabling plant breeders to phenotype larger populations and make selections with higher confidence.
KW  - energy cane
KW  - NDVI
KW  - ExG
KW  - yield
KW  - total cellulosic content
DO  - 10.3390/agronomy10050718
TY  - EJOU
AU  - Deng, Xiaoling
AU  - Tong, Zejing
AU  - Lan, Yubin
AU  - Huang, Zixiao
TI  - Detection and Location of Dead Trees with Pine Wilt Disease Based on Deep Learning and UAV Remote Sensing
T2  - AgriEngineering

PY  - 2020
VL  - 2
IS  - 2
SN  - 2624-7402

AB  - Pine wilt disease causes huge economic losses to pine wood forestry because of its destructiveness and rapid spread. This paper proposes a detection and location method of pine wood nematode disease at a large scale adopting UAV (Unmanned Aerial Vehicle) remote sensing and artificial intelligence technology. The UAV remote sensing images were enhanced by computer vision tools. A Faster-RCNN (Faster Region Convolutional Neural Networks) deep learning framework based on a RPN (Region Proposal Network) network and the ResNet residual neural network were used to train the pine wilt diseased dead tree detection model. The loss function and the anchors in the RPN of the convolutional neural network were optimized. Finally, the location of pine wood nematode dead tree was conducted, which generated the geographic information on the detection results. The results show that ResNet101 performed better than VGG16 (Visual Geometry Group 16) convolutional neural network. The detection accuracy was improved and reached to about 90% after a series of optimizations to the network, meaning that the optimization methods proposed in this paper are feasible to pine wood nematode dead tree detection.
KW  - pine wilt disease
KW  - Faster-RCNN
KW  - UAV remote sensing
KW  - deep learning
KW  - geographical information
DO  - 10.3390/agriengineering2020019
TY  - EJOU
AU  - Ciaburro, Giuseppe
AU  - Iannace, Gino
AU  - Trematerra, Amelia
TI  - Research for the Presence of Unmanned Aerial Vehicle inside Closed Environments with Acoustic Measurements
T2  - Buildings

PY  - 2020
VL  - 10
IS  - 5
SN  - 2075-5309

AB  - Small UAVs (unmanned aerial vehicle) can be used in many sectors such as the acquisition of images or the transport of objects. Small UAVs have also been used for terrorist activities or to disturb the flight of airplanes. Due to the small size and the presence of only rotating parts, drones escape traditional controls and therefore represent a danger. This paper reports a methodology for identifying the presence of small UAVs inside a closed environment by measuring the noise emitted during the flight. Acoustic measurements of the noise emitted by a drone inside a large environment (12.0 &times; 30.0 &times; 12.0 m) were performed. The noise was measured with a sound level meter placed at different distances (5, 10, and 15 m), to characterize the noise in the absence of anthropic noise. In this configuration, a typical tonal component of drone noise is highlighted at the frequency of one-third of an octave at 5000 Hz due to the rotation of the blades. This component is also present 15 m away from the source point. Subsequent measurements were performed by introducing into the environment, through a loudspeaker, the anthropogenic noise produced by the buzz of people and background music. It is possible to distinguish the typical tonal component of UAV noise at the frequency of 5000 Hz even when the level of recording of anthropogenic noise emitted by the loudspeaker is at the maximum power tested. It is therefore possible to search for the presence of small UAVs inside a specific closed environment with only acoustic measurements, paying attention to the typical frequency of noise emission equal to 5000 Hz.
KW  - UAV
KW  - frequency
KW  - acoustic measurements
KW  - noise
KW  - closed environments
DO  - 10.3390/buildings10050096
TY  - EJOU
AU  - Dai, Chenguang
AU  - Zhang, Zhenchao
AU  - Lin, Dong
TI  - An Object-Based Bidirectional Method for Integrated Building Extraction and Change Detection between Multimodal Point Clouds
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 10
SN  - 2072-4292

AB  - Building extraction and change detection are two important tasks in the remote sensing domain. Change detection between airborne laser scanning data and photogrammetric data is vulnerable to dense matching errors, mis-alignment errors and data gaps. This paper proposes an unsupervised object-based method for integrated building extraction and change detection. Firstly, terrain, roofs and vegetation are extracted from the precise laser point cloud, based on &ldquo;bottom-up&rdquo; segmentation and clustering. Secondly, change detection is performed in an object-based bidirectional manner: Heightened buildings and demolished buildings are detected by taking the laser scanning data as reference, while newly-built buildings are detected by taking the dense matching data as reference. Experiments on two urban data sets demonstrate its effectiveness and robustness. The object-based change detection achieves a recall rate of 92.31% and a precision rate of 88.89% for the Rotterdam dataset; it achieves a recall rate of 85.71% and a precision rate of 100% for the Enschede dataset. It can not only extract unchanged building footprints, but also assign heightened or demolished labels to the changed buildings.
KW  - change detection
KW  - building extraction
KW  - point clouds
KW  - object-based
KW  - airborne laser scanning
KW  - dense image matching
DO  - 10.3390/rs12101680
TY  - EJOU
AU  - d’Oliveira, Marcus V. N.
AU  - Broadbent, Eben N.
AU  - Oliveira, Luis C.
AU  - Almeida, Danilo R. A.
AU  - Papa, Daniel A.
AU  - Ferreira, Manuel E.
AU  - Zambrano, Angelica M. Almeyda
AU  - Silva, Carlos A.
AU  - Avino, Felipe S.
AU  - Prata, Gabriel A.
AU  - Mello, Ricardo A.
AU  - Figueiredo, Evandro O.
AU  - Jorge, Lúcio A. de Castro
AU  - Junior, Leomar
AU  - Albuquerque, Rafael W.
AU  - Brancalion, Pedro H. S.
AU  - Wilkinson, Ben
AU  - Oliveira-da-Costa, Marcelo
TI  - Aboveground Biomass Estimation in Amazonian Tropical Forests: a Comparison of Aircraft- and GatorEye UAV-borne LiDAR Data in the Chico Mendes Extractive Reserve in Acre, Brazil
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 11
SN  - 2072-4292

AB  - Tropical forests are often located in difficult-to-access areas, which make high-quality forest structure information difficult and expensive to obtain by traditional field-based approaches. LiDAR (acronym for Light Detection And Ranging) data have been used throughout the world to produce time-efficient and wall-to-wall structural parameter estimates for monitoring in native and commercial forests. In this study, we compare products and aboveground biomass (AGB) estimations from LiDAR data acquired using an aircraft-borne system in 2015 and data collected by the unmanned aerial vehicle (UAV)-based GatorEye Unmanned Flying Laboratory in 2017 for ten forest inventory plots located in the Chico Mendes Extractive Reserve in Acre state, southwestern Brazilian Amazon. The LiDAR products were similar and comparable among the two platforms and sensors. Principal differences between derived products resulted from the GatorEye system flying lower and slower and having increased returns per second than the aircraft, resulting in a much higher point density overall (11.3 ± 1.8 vs. 381.2 ± 58 pts/m2). Differences in ground point density, however, were much smaller among the systems, due to the larger pulse area and increased number of returns per pulse of the aircraft system, with the GatorEye showing an approximately 50% higher ground point density (0.27 ± 0.09 vs. 0.42 ± 0.09). The LiDAR models produced by both sensors presented similar results for digital elevation models and estimated AGB. Our results validate the ability for UAV-borne LiDAR sensors to accurately quantify AGB in dense high-leaf-area tropical forests in the Amazon. We also highlight new possibilities using the dense point clouds of UAV-borne systems for analyses of detailed crown structure and leaf area density distribution of the forest interior.
KW  - forest inventory
KW  - forest monitoring
KW  - forest structure
KW  - remote sensing
DO  - 10.3390/rs12111754
TY  - EJOU
AU  - Piccinini, Fabio
AU  - Pierdicca, Roberto
AU  - Malinverni, Eva S.
TI  - A Relational Conceptual Model in GIS for the Management of Photovoltaic Systems
T2  - Energies

PY  - 2020
VL  - 13
IS  - 11
SN  - 1996-1073

AB  - The aim of this manuscript is to define an operational pipeline of work, from data acquisition to the report creation, for the smart management of PV plants. To achieve such an ambitious result, we exploit the implementation of a conceptual model, deployed through a relational database to retrieve any kind of information related to the PV plant. The motivation that drove this research is due to the increasing construction of PV plants. In fact, following European and international investments that heavily stimulated the use of clean energy, the need to maintain PV plants in their maximum efficiency for their whole lifecycle emerged, to bring about benefits from both the ecological and the economic points of view. While the research community focuses on finding new and automatic ways to detect faults automatically, few efforts have been made considering the so-called Operation and Maintenance (O&amp;M). A relational conceptual model may facilitate the management of heterogeneous sources of information, which are common in complex PV plants. The purpose of the present study is to provide companies and insiders with a GIS-based tool to maintain the energy efficiency of a PV plant. Indeed, it is a common practice used by companies dealing with O&amp;M of PV plants to create technical reports about the health status of the plants. This operation, made manually, is very time consuming and error prone. To overcome this latter drawback, this work attempts to encourage the use of GIS in the PV plants O&amp;M, which proves to be efficient to deal with fault management and to assure a good level of energy production. The developed conceptual model, tested on two real case studies, proved to be complete, cost-effective and efficient to be replicated in other existing plants.
KW  - photovoltaics
KW  - PV plants
KW  - PV faults
KW  - GIS
KW  - relational data base
KW  - operation and maintenance
DO  - 10.3390/en13112860
TY  - EJOU
AU  - Zhang, Zhao
AU  - Flores, Paulo
AU  - Igathinathane, C.
AU  - L. Naik, Dayakar
AU  - Kiran, Ravi
AU  - Ransom, Joel K.
TI  - Wheat Lodging Detection from UAS Imagery Using Machine Learning Algorithms
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 11
SN  - 2072-4292

AB  - The current mainstream approach of using manual measurements and visual inspections for crop lodging detection is inefficient, time-consuming, and subjective. An innovative method for wheat lodging detection that can overcome or alleviate these shortcomings would be welcomed. This study proposed a systematic approach for wheat lodging detection in research plots (372 experimental plots), which consisted of using unmanned aerial systems (UAS) for aerial imagery acquisition, manual field evaluation, and machine learning algorithms to detect the occurrence or not of lodging. UAS imagery was collected on three different dates (23 and 30 July 2019, and 8 August 2019) after lodging occurred. Traditional machine learning and deep learning were evaluated and compared in this study in terms of classification accuracy and standard deviation. For traditional machine learning, five types of features (i.e. gray level co-occurrence matrix, local binary pattern, Gabor, intensity, and Hu-moment) were extracted and fed into three traditional machine learning algorithms (i.e., random forest (RF), neural network, and support vector machine) for detecting lodged plots. For the datasets on each imagery collection date, the accuracies of the three algorithms were not significantly different from each other. For any of the three algorithms, accuracies on the first and last date datasets had the lowest and highest values, respectively. Incorporating standard deviation as a measurement of performance robustness, RF was determined as the most satisfactory. Regarding deep learning, three different convolutional neural networks (simple convolutional neural network, VGG-16, and GoogLeNet) were tested. For any of the single date datasets, GoogLeNet consistently had superior performance over the other two methods. Further comparisons between RF and GoogLeNet demonstrated that the detection accuracies of the two methods were not significantly different from each other (p &gt; 0.05); hence, the choice of any of the two would not affect the final detection accuracies. However, considering the fact that the average accuracy of GoogLeNet (93%) was larger than RF (91%), it was recommended to use GoogLeNet for wheat lodging detection. This research demonstrated that UAS RGB imagery, coupled with the GoogLeNet machine learning algorithm, can be a novel, reliable, objective, simple, low-cost, and effective (accuracy &gt; 90%) tool for wheat lodging detection.
KW  - precision agriculture
KW  - field crops
KW  - machine learning
KW  - deep learning
KW  - image processing
KW  - textural features
DO  - 10.3390/rs12111838
TY  - EJOU
AU  - de la Fuente Castillo, Víctor
AU  - Díaz-Álvarez, Alberto
AU  - Manso-Callejo, Miguel-Ángel
AU  - Serradilla García, Francisco
TI  - Grammar Guided Genetic Programming for Network Architecture Search and Road Detection on Aerial Orthophotography
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 11
SN  - 2076-3417

AB  - Photogrammetry involves aerial photography of the Earth&rsquo;s surface and subsequently processing the images to provide a more accurate depiction of the area (Orthophotography). It is used by the Spanish Instituto Geogr&aacute;fico Nacional to update road cartography but requires a significant amount of manual labor due to the need to perform visual inspection of all tiled images. Deep learning techniques (artificial neural networks with more than one hidden layer) can perform road detection but it is still unclear how to find the optimal network architecture. Our main goal is the automatic design of deep neural network architectures with grammar-guided genetic programming. In this kind of evolutive algorithm, all the population individuals (here candidate network architectures) are constrained to rules specified by a grammar that defines valid and useful structural patterns to guide the search process. Grammar used includes well-known complex structures (e.g., Inception-like modules) combined with a custom designed mutation operator (dynamically links the mutation probability to structural diversity). Pilot results show that the system is able to design models for road detection that obtain test accuracies similar to that reached by state-of-the-art models when evaluated over a dataset from the Spanish National Aerial Orthophotography Plan.
KW  - grammar evolution
KW  - deep learning
KW  - network architecture search
KW  - grammar-guided genetic programming
DO  - 10.3390/app10113953
TY  - EJOU
AU  - Zhang, Tianyao
AU  - Hu, Xiaoguang
AU  - Xiao, Jin
AU  - Zhang, Guofeng
TI  - A Machine Learning Method for Vision-Based Unmanned Aerial Vehicle Systems to Understand Unknown Environments
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 11
SN  - 1424-8220

AB  - What makes unmanned aerial vehicles (UAVs) intelligent is their capability of sensing and understanding new unknown environments. Some studies utilize computer vision algorithms like Visual Simultaneous Localization and Mapping (VSLAM) and Visual Odometry (VO) to sense the environment for pose estimation, obstacles avoidance and visual servoing. However, understanding the new environment (i.e., make the UAV recognize generic objects) is still an essential scientific problem that lacks a solution. Therefore, this paper takes a step to understand the items in an unknown environment. The aim of this research is to enable the UAV with basic understanding capability for a high-level UAV flock application in the future. Specially, firstly, the proposed understanding method combines machine learning and traditional algorithm to understand the unknown environment through RGB images; secondly, the You Only Look Once (YOLO) object detection system is integrated (based on TensorFlow) in a smartphone to perceive the position and category of 80 classes of objects in the images; thirdly, the method makes the UAV more intelligent and liberates the operator from labor; fourthly, detection accuracy and latency in working condition are quantitatively evaluated, and properties of generality (can be used in various platforms), transportability (easily deployed from one platform to another) and scalability (easily updated and maintained) for UAV flocks are qualitatively discussed. The experiments suggest that the method has enough accuracy to recognize various objects with high computational speed, and excellent properties of generality, transportability and scalability.
KW  - UAV
KW  - visual RGB
KW  - real-time
KW  - YOLOv3
KW  - color detection
KW  - object detection
KW  - machine learning system
DO  - 10.3390/s20113245
TY  - EJOU
AU  - Tang, Ta-Wei
AU  - Kuo, Wei-Han
AU  - Lan, Jauh-Hsiang
AU  - Ding, Chien-Fang
AU  - Hsu, Hakiem
AU  - Young, Hong-Tsu
TI  - Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 12
SN  - 1424-8220

AB  - Recently, researchers have been studying methods to introduce deep learning into automated optical inspection (AOI) systems to reduce labor costs. However, the integration of deep learning in the industry may encounter major challenges such as sample imbalance (defective products that only account for a small proportion). Therefore, in this study, an anomaly detection neural network, dual auto-encoder generative adversarial network (DAGAN), was developed to solve the problem of sample imbalance. With skip-connection and dual auto-encoder architecture, the proposed method exhibited excellent image reconstruction ability and training stability. Three datasets, namely public industrial detection training set, MVTec AD, with mobile phone screen glass and wood defect detection datasets, were used to verify the inspection ability of DAGAN. In addition, training with a limited amount of data was proposed to verify its detection ability. The results demonstrated that the areas under the curve (AUCs) of DAGAN were better than previous generative adversarial network-based anomaly detection models in 13 out of 17 categories in these datasets, especially in categories with high variability or noise. The maximum AUC improvement was 0.250 (toothbrush). Moreover, the proposed method exhibited better detection ability than the U-Net auto-encoder, which indicates the function of discriminator in this application. Furthermore, the proposed method had a high level of AUCs when using only a small amount of training data. DAGAN can significantly reduce the time and cost of collecting and labeling data when it is applied to industrial detection.
KW  - automated optical inspection (AOI)
KW  - anomaly detection (AD)
KW  - defect detection
KW  - generative adversarial network (GAN)
KW  - dual auto-encoder generative adversarial network (DAGAN)
DO  - 10.3390/s20123336
TY  - EJOU
AU  - Wagle, Nimisha
AU  - Acharya, Tri D.
TI  - Past and Present Practices of Topographic Base Map Database Update in Nepal
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 6
SN  - 2220-9964

AB  - Topographic Base Maps (TBMs) are those maps that portray ground relief as the form of contour lines and show planimetric details. Various other maps like geomorphological maps, contour maps, and land use planning maps are derived from topographical maps. In this constantly changing world, the update of TBMs is indispensable. In Nepal, their update and maintenance are done by the Survey Department (SD) as a national mapping agency. This paper presents the history of topographical mapping and the reasons for the lack of updates. Currently, the SD is updating the TBM database using panchromatic and multispectral images from the Zi Yuan-3 (ZY-3) satellite with a resolution of 2.1 and 5.8 m, respectively. The updated methodology includes the orthorectification of images, the pansharpening of images, field data collection, digitization, change detection, and updating, the overlay of vector data and field verification, data quality control, and printing map production. A TBM in the Dang district of Nepal is presented as casework to show the changes in the area and issues faced during the update. Though the present digitizing procedure is time-consuming and labor-intensive, the use of high-resolution imagery has made mapping accurate and has produced high-quality maps. However, audit and automation can be introduced from the experiences of other countries for accurate and frequent updates of the TBM database in Nepal.
KW  - topographic base map
KW  - national mapping agency
KW  - updates
KW  - history
KW  - ZY-3
KW  - Nepal
DO  - 10.3390/ijgi9060397
TY  - EJOU
AU  - Abaspur Kazerouni, Iman
AU  - Dooly, Gerard
AU  - Toal, Daniel
TI  - Underwater Image Enhancement and Mosaicking System Based on A-KAZE Feature Matching
T2  - Journal of Marine Science and Engineering

PY  - 2020
VL  - 8
IS  - 6
SN  - 2077-1312

AB  - Feature extraction and matching is a key component in image stitching and a critical step in advancing image reconstructions, machine vision and robotic perception algorithms. This paper presents a fast and robust underwater image mosaicking system based on (2D)2PCA and A-KAZE key-points extraction and optimal seam-line methods. The system utilizes image enhancement as a preprocessing step to improve quality and allow for greater keyframe extraction and matching performance, leading to better quality mosaicking. The application focus of this paper is underwater imaging and it demonstrates the suitability of the developed system in advanced underwater reconstructions. The results show that the proposed method can address the problems of noise, mismatching and quality issues which are typically found in underwater image datasets. The results demonstrate the proposed method as scale-invariant and show improvements in terms of processing speed and system robustness over other methods found in the literature.
KW  - underwater image
KW  - feature extraction
KW  - image matching
KW  - stitching
KW  - image mosaic
DO  - 10.3390/jmse8060449
TY  - EJOU
AU  - Zhang, Xueyan
TI  - Village-Level Homestead and Building Floor Area Estimates Based on UAV Imagery and U-Net Algorithm
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 6
SN  - 2220-9964

AB  - China&rsquo;s rural population has declined markedly with the acceleration of urbanization and industrialization, but the area under rural homesteads has continued to expand. Proper rural land use and management require large-scale, efficient, and low-cost rural residential surveys; however, such surveys are time-consuming and difficult to accomplish. Unmanned aerial vehicle (UAV) technology coupled with a deep learning architecture and 3D modelling can provide a potential alternative to traditional surveys for gathering rural homestead information. In this study, a method to estimate the village-level homestead area, a 3D-based building height model (BHM), and the number of building floors based on UAV imagery and the U-net algorithm was developed, and the respective estimation accuracies were found to be 0.92, 0.99, and 0.89. This method is rapid and inexpensive compared to the traditional time-consuming and costly household surveys, and, thus, it is of great significance to the ongoing use and management of rural homestead information, especially with regards to the confirmation of homestead property rights in China. Further, the proposed combination of UAV imagery and U-net technology may have a broader application in rural household surveys, as it can provide more information for decision-makers to grasp the current state of the rural socio-economic environment.
KW  - rural homestead
KW  - household survey
KW  - U-net
KW  - UAV
KW  - China
DO  - 10.3390/ijgi9060403
TY  - EJOU
AU  - Zhong, Kefeng
AU  - Teng, Shuai
AU  - Liu, Gen
AU  - Chen, Gongfa
AU  - Cui, Fangsen
TI  - Structural Damage Features Extracted by Convolutional Neural Networks from Mode Shapes
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 12
SN  - 2076-3417

AB  - This paper aims to locate damaged rods in a three-dimensional (3D) steel truss and reveals some internal working mechanisms of the convolutional neural network (CNN), which is based on the first-order modal parameters and CNN. The CNN training samples (including a large number of damage scenarios) are created by ABAQUS and PYTHON scripts. The mode shapes and mode curvature differences are taken as the inputs of the CNN training samples, respectively, and the damage locating accuracy of the CNN is investigated. Finally, the features extracted from each convolutional layer of the CNN are checked to reveal some internal working mechanisms of the CNN and explain the specific meanings of some features. The results show that the CNN-based damage detection method using mode shapes as the inputs has a higher locating accuracy for all damage degrees, while the method using mode curvature differences as the inputs has a lower accuracy for the targets with a low damage degree; however, with the increase of the target damage degree, it gradually achieves the same good locating accuracy as mode shapes. The features extracted from each convolutional layer show that the CNN can obtain the difference between the sample to be classified and the average of training samples in shallow layers, and then amplify the difference in the subsequent convolutional layer, which is similar to a power function, finally it produces a distinguishable peak signal at the damage location. Then a damage locating method is derived from the feature extraction of the CNN. All of these results indicate that the CNN using first-order modal parameters not only has a powerful damage location ability, but also opens up a new way to extract damage features from the measurement data.
KW  - structural state detection
KW  - convolutional neural networks
KW  - mode shapes
KW  - mode curvature differences
KW  - feature extraction
DO  - 10.3390/app10124247
TY  - EJOU
AU  - Wagner, Matthias P.
AU  - Oppelt, Natascha
TI  - Deep Learning and Adaptive Graph-Based Growing Contours for Agricultural Field Extraction
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 12
SN  - 2072-4292

AB  - Field mapping and information on agricultural landscapes is of increasing importance for many applications. Monitoring schemes and national cadasters provide a rich source of information but their maintenance and regular updating is costly and labor-intensive. Automatized mapping of fields based on remote sensing imagery may aid in this task and allow for a faster and more regular observation. Although remote sensing has seen extensive use in agricultural research topics, such as plant health monitoring, crop type classification, yield prediction, and irrigation, field delineation and extraction has seen comparatively little research interest. In this study, we present a field boundary detection technique based on deep learning and a variety of image features, and combine it with the graph-based growing contours (GGC) method to extract agricultural fields in a study area in northern Germany. The boundary detection step only requires red, green, and blue (RGB) data and is therefore largely independent of the sensor used. We compare different image features based on color and luminosity information and evaluate their usefulness for the task of field boundary detection. A model based on texture metrics, gradient information, Hessian matrix eigenvalues, and local statistics showed good results with accuracies up to 88.2%, an area under the ROC curve (AUC) of up to 0.94, and F1 score of up to 0.88. The exclusive use of these universal image features may also facilitate transferability to other regions. We further present modifications to the GGC method intended to aid in upscaling of the method through process acceleration with a minimal effect on results. We combined the boundary detection results with the GGC method for field polygon extraction. Results were promising, with the new GGC version performing similarly or better than the original version while experiencing an acceleration of 1.3&times; to 2.3&times; on different subsets and input complexities. Further research may explore other applications of the GGC method outside agricultural remote sensing and field extraction.
KW  - field extraction
KW  - field boundary detection
KW  - deep learning
KW  - multilayer perceptron
KW  - active contours
KW  - growing snakes
KW  - graph-based growing contours
DO  - 10.3390/rs12121990
TY  - EJOU
AU  - Panagiotou, Emmanouil
AU  - Chochlakis, Georgios
AU  - Grammatikopoulos, Lazaros
AU  - Charou, Eleni
TI  - Generating Elevation Surface from a Single RGB Remotely Sensed Image Using Deep Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 12
SN  - 2072-4292

AB  - Generating Digital Elevation Models (DEM) from satellite imagery or other data sources constitutes an essential tool for a plethora of applications and disciplines, ranging from 3D flight planning and simulation, autonomous driving and satellite navigation, such as GPS, to modeling water flow, precision farming and forestry. The task of extracting this 3D geometry from a given surface hitherto requires a combination of appropriately collected corresponding samples and/or specialized equipment, as inferring the elevation from single image data is out of reach for contemporary approaches. On the other hand, Artificial Intelligence (AI) and Machine Learning (ML) algorithms have experienced unprecedented growth in recent years as they can extrapolate rules in a data-driven manner and retrieve convoluted, nonlinear one-to-one mappings, such as an approximate mapping from satellite imagery to DEMs. Therefore, we propose an end-to-end Deep Learning (DL) approach to construct this mapping and to generate an absolute or relative point cloud estimation of a DEM given a single RGB satellite (Sentinel-2 imagery in this work) or drone image. The model has been readily extended to incorporate available information from the non-visible electromagnetic spectrum. Unlike existing methods, we only exploit one image for the production of the elevation data, rendering our approach less restrictive and constrained, but suboptimal compared to them at the same time. Moreover, recent advances in software and hardware allow us to make the inference and the generation extremely fast, even on moderate hardware. We deploy Conditional Generative Adversarial networks (CGAN), which are the state-of-the-art approach to image-to-image translation. We expect our work to serve as a springboard for further development in this field and to foster the integration of such methods in the process of generating, updating and analyzing DEMs.
KW  - deep learning
KW  - remote sensing
KW  - digital elevation models
KW  - generative adversarial networks
KW  - 3D point cloud
KW  - satellite imagery
KW  - drones
KW  - height maps
DO  - 10.3390/rs12122002
TY  - EJOU
AU  - Seydi, Seyd T.
AU  - Hasanlou, Mahdi
AU  - Amani, Meisam
TI  - A New End-to-End Multi-Dimensional CNN Framework for Land Cover/Land Use Change Detection in Multi-Source Remote Sensing Datasets
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 12
SN  - 2072-4292

AB  - The diversity of change detection (CD) methods and the limitations in generalizing these techniques using different types of remote sensing datasets over various study areas have been a challenge for CD applications. Additionally, most CD methods have been implemented in two intensive and time-consuming steps: (a) predicting change areas, and (b) decision on predicted areas. In this study, a novel CD framework based on the convolutional neural network (CNN) is proposed to not only address the aforementioned problems but also to considerably improve the level of accuracy. The proposed CNN-based CD network contains three parallel channels: the first and second channels, respectively, extract deep features on the original first- and second-time imagery and the third channel focuses on the extraction of change deep features based on differencing and staking deep features. Additionally, each channel includes three types of convolution kernels: 1D-, 2D-, and 3D-dilated-convolution. The effectiveness and reliability of the proposed CD method are evaluated using three different types of remote sensing benchmark datasets (i.e., multispectral, hyperspectral, and Polarimetric Synthetic Aperture RADAR (PolSAR)). The results of the CD maps are also evaluated both visually and statistically by calculating nine different accuracy indices. Moreover, the results of the CD using the proposed method are compared to those of several state-of-the-art CD algorithms. All the results prove that the proposed method outperforms the other remote sensing CD techniques. For instance, considering different scenarios, the Overall Accuracies (OAs) and Kappa Coefficients (KCs) of the proposed CD method are better than 95.89% and 0.805, respectively, and the Miss Detection (MD) and the False Alarm (FA) rates are lower than 12% and 3%, respectively.
KW  - remote sensing
KW  - change detection
KW  - deep learning
KW  - CNN
KW  - hyperspectral
KW  - multispectral
KW  - polarimetric SAR
DO  - 10.3390/rs12122010
TY  - EJOU
AU  - Kucharczyk, Maja
AU  - Hay, Geoffrey J.
AU  - Ghaffarian, Salar
AU  - Hugenholtz, Chris H.
TI  - Geographic Object-Based Image Analysis: A Primer and Future Directions
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 12
SN  - 2072-4292

AB  - Geographic object-based image analysis (GEOBIA) is a remote sensing image analysis paradigm that defines and examines image-objects: groups of neighboring pixels that represent real-world geographic objects. Recent reviews have examined methodological considerations and highlighted how GEOBIA improves upon the 30+ year pixel-based approach, particularly for H-resolution imagery. However, the literature also exposes an opportunity to improve guidance on the application of GEOBIA for novice practitioners. In this paper, we describe the theoretical foundations of GEOBIA and provide a comprehensive overview of the methodological workflow, including: (i) software-specific approaches (open-source and commercial); (ii) best practices informed by research; and (iii) the current status of methodological research. Building on this foundation, we then review recent research on the convergence of GEOBIA with deep convolutional neural networks, which we suggest is a new form of GEOBIA. Specifically, we discuss general integrative approaches and offer recommendations for future research. Overall, this paper describes the past, present, and anticipated future of GEOBIA in a novice-accessible format, while providing innovation and depth to experienced practitioners.
KW  - geographic object-based image analysis
KW  - GEOBIA
KW  - object-based image analysis
KW  - OBIA
KW  - machine learning
KW  - deep learning
KW  - convolutional neural network
KW  - CNN
KW  - GEOCNN
DO  - 10.3390/rs12122012
TY  - EJOU
AU  - Zhang, Qichen
AU  - Zhu, Meiqiang
AU  - Zou, Liang
AU  - Li, Ming
AU  - Zhang, Yong
TI  - Learning Reward Function with Matching Network for Mapless Navigation
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 13
SN  - 1424-8220

AB  - Deep reinforcement learning (DRL) has been successfully applied in mapless navigation. An important issue in DRL is to design a reward function for evaluating actions of agents. However, designing a robust and suitable reward function greatly depends on the designer&rsquo;s experience and intuition. To address this concern, we consider employing reward shaping from trajectories on similar navigation tasks without human supervision, and propose a general reward function based on matching network (MN). The MN-based reward function is able to gain the experience by pre-training through trajectories on different navigation tasks and accelerate the training speed of DRL in new tasks. The proposed reward function keeps the optimal strategy of DRL unchanged. The simulation results on two static maps show that the DRL converge with less iterations via the learned reward function than the state-of-the-art mapless navigation methods. The proposed method performs well in dynamic maps with partially moving obstacles. Even when test maps are different from training maps, the proposed strategy is able to complete the navigation tasks without additional training.
KW  - deep reinforcement learning
KW  - reward shaping
KW  - matching network
KW  - navigation
DO  - 10.3390/s20133664
TY  - EJOU
AU  - Ghaffarian, Saman
AU  - Rezaie Farhadabad, Ali
AU  - Kerle, Norman
TI  - Post-Disaster Recovery Monitoring with Google Earth Engine
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 13
SN  - 2076-3417

AB  - Post-disaster recovery is a complex process in terms of measuring its progress after a disaster and understanding its components and influencing factors. During this process, disaster planners and governments need reliable information to make decisions towards building the affected region back to normal (pre-disaster), or even improved, conditions. Hence, it is essential to use methods to understand the dynamics/variables of the post-disaster recovery process, and rapid and cost-effective data and tools to monitor the process. Google Earth Engine (GEE) provides free access to vast amounts of remote sensing (RS) data and a powerful computing environment in a cloud platform, making it an attractive tool to analyze earth surface data. In this study we assessed the suitability of GEE to analyze and track recovery. To do so, we employed GEE to assess the recovery process over a three-year period after Typhoon Haiyan, which struck Leyte island, in the Philippines, in 2013. We developed an approach to (i) generate cloud and shadow-free image composites from Landsat 7 and 8 satellite imagery and produce land cover classification data using the Random Forest method, and (ii) generate damage and recovery maps based on post-classification change analysis. The method produced land cover maps with accuracies &gt;88%. We used the model to produce damage and three time-step recovery maps for 62 municipalities on Leyte island. The results showed that most of the municipalities had recovered after three years in terms of returning to the pre-disaster situation based on the selected land cover change analysis. However, more analysis (e.g., functional assessment) based on detailed data (e.g., land use maps) is needed to evaluate the more complex and subtle socio-economic aspects of the recovery. The study showed that GEE has good potential for monitoring the recovery process for extensive regions. However, the most important limitation is the lack of very-high-resolution RS data that are critical to assess the process in detail, in particular in complex urban environments.
KW  - disaster
KW  - damage
KW  - recovery
KW  - monitoring
KW  - assessment
KW  - remote sensing
KW  - satellite imagery
KW  - Landsat
KW  - Google Earth Engine
KW  - Typhoon Haiyan
KW  - cloud computing
DO  - 10.3390/app10134574
TY  - EJOU
AU  - Veeranampalayam Sivakumar, Arun N.
AU  - Li, Jiating
AU  - Scott, Stephen
AU  - Psota, Eric
AU  - J. Jhala, Amit
AU  - Luck, Joe D.
AU  - Shi, Yeyin
TI  - Comparison of Object Detection and Patch-Based Classification Deep Learning Models on Mid- to Late-Season Weed Detection in UAV Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 13
SN  - 2072-4292

AB  - Mid- to late-season weeds that escape from the routine early-season weed management threaten agricultural production by creating a large number of seeds for several future growing seasons. Rapid and accurate detection of weed patches in field is the first step of site-specific weed management. In this study, object detection-based convolutional neural network models were trained and evaluated over low-altitude unmanned aerial vehicle (UAV) imagery for mid- to late-season weed detection in soybean fields. The performance of two object detection models, Faster RCNN and the Single Shot Detector (SSD), were evaluated and compared in terms of weed detection performance using mean Intersection over Union (IoU) and inference speed. It was found that the Faster RCNN model with 200 box proposals had similar good weed detection performance to the SSD model in terms of precision, recall, f1 score, and IoU, as well as a similar inference time. The precision, recall, f1 score and IoU were 0.65, 0.68, 0.66 and 0.85 for Faster RCNN with 200 proposals, and 0.66, 0.68, 0.67 and 0.84 for SSD, respectively. However, the optimal confidence threshold of the SSD model was found to be much lower than that of the Faster RCNN model, which indicated that SSD might have lower generalization performance than Faster RCNN for mid- to late-season weed detection in soybean fields using UAV imagery. The performance of the object detection model was also compared with patch-based CNN model. The Faster RCNN model yielded a better weed detection performance than the patch-based CNN with and without overlap. The inference time of Faster RCNN was similar to patch-based CNN without overlap, but significantly less than patch-based CNN with overlap. Hence, Faster RCNN was found to be the best model in terms of weed detection performance and inference time among the different models compared in this study. This work is important in understanding the potential and identifying the algorithms for an on-farm, near real-time weed detection and management.
KW  - CNN
KW  - Faster RCNN
KW  - SSD
KW  - Inception v2
KW  - patch-based CNN
KW  - MobileNet v2
KW  - detection performance
KW  - inference time
DO  - 10.3390/rs12132136
TY  - EJOU
AU  - Cao, Dalu
AU  - Bai, Guangchen
TI  - A Study on Aeroengine Conceptual Design Considering Multi-Mission Performance Reliability
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 13
SN  - 2076-3417

AB  - Owing to the realization of multi-mission adaptability requires more complex mechanical structure, the candidates of future aviation propulsion are confronted with more overall reliability problems than that of the conventional gas turbine engine. This situation is challenging to a traditional aeroengine deterministic design method. To overcome this challenge, the Reliability-based Multi-Design Point Methodology is proposed for aeroengine conceptual design. The presented methodology adopted an unconventional approach of engaging the reliability prediction by artificial neural network (ANN) surrogate models rather than the time-consuming Monte Carlo (MC) simulation. Based on the Adaptive Particle swarm optimization, the utilization of the pre-training technique optimizes the initial network parameters to acquire better-conditioned initial network, which is sited closer to designated optimum so that contributes to the convergence property. Moreover, a new hybrid algorithm is presented to integrate the pre-training technique into neural network training procedure in order to enhance the ANN performance. The proposed methodology is applied to the cycle design of a turbofan engine with uncertainty component performance. The testing results certify that the prediction accuracy of pre-trained ANN is improved with negligible computational cost, which only spent nearly one-millionth as much time as the MC-based probabilistic analysis (0.1267 s vs. 95,262 s, for 20 testing samples). The MC simulation results substantiate that optimal cycle parameters precisely improve the engine overall performance to simultaneously reach expected reliability (&ge;98.9%) in multiple operating conditions without unnecessary performance redundancy, which verifies the efficiency of the presented methodology. The presented efforts provide a novel approach for aeroengine cycle design, and enrich reliability design theory as well.
KW  - aircraft engine
KW  - performance reliability
KW  - cycle design
KW  - artificial neural network
KW  - pre-training technique
DO  - 10.3390/app10134668
TY  - EJOU
AU  - Arce, Samuel
AU  - Vernon, Cory A.
AU  - Hammond, Joshua
AU  - Newell, Valerie
AU  - Janson, Joseph
AU  - Franke, Kevin W.
AU  - Hedengren, John D.
TI  - Automated 3D Reconstruction Using Optimized View-Planning Algorithms for Iterative Development of Structure-from-Motion Models
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 13
SN  - 2072-4292

AB  - Unsupervised machine learning algorithms (clustering, genetic, and principal component analysis) automate Unmanned Aerial Vehicle (UAV) missions as well as the creation and refinement of iterative 3D photogrammetric models with a next best view (NBV) approach. The novel approach uses Structure-from-Motion (SfM) to achieve convergence to a specified orthomosaic resolution by identifying edges in the point cloud and planning cameras that &ldquo;view&rdquo; the holes identified by edges without requiring an initial model. This iterative UAV photogrammetric method successfully runs in various Microsoft AirSim environments. Simulated ground sampling distance (GSD) of models reaches as low as     3.4     cm per pixel, and generally, successive iterations improve resolution. Besides analogous application in simulated environments, a field study of a retired municipal water tank illustrates the practical application and advantages of automated UAV iterative inspection of infrastructure using     63 %     fewer photographs than a comparable manual flight with analogous density point clouds obtaining a GSD of less than 3 cm per pixel. Each iteration qualitatively increases resolution according to a logarithmic regression, reduces holes in models, and adds details to model edges.
KW  - Structure-from-Motion
KW  - Unmanned Aerial Vehicles
KW  - iterative inspection
KW  - automated inspection
KW  - multi-scale
KW  - view-planning
KW  - unsupervised machine learning
KW  - autonomous flight
KW  - iterative optimization
DO  - 10.3390/rs12132169
TY  - EJOU
AU  - García-Martínez, Héctor
AU  - Flores-Magdaleno, Héctor
AU  - Ascencio-Hernández, Roberto
AU  - Khalil-Gardezi, Abdul
AU  - Tijerina-Chávez, Leonardo
AU  - Mancilla-Villa, Oscar R.
AU  - Vázquez-Peña, Mario A.
TI  - Corn Grain Yield Estimation from Vegetation Indices, Canopy Cover, Plant Density, and a Neural Network Using Multispectral and RGB Images Acquired with Unmanned Aerial Vehicles
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 7
SN  - 2077-0472

AB  - Corn yields vary spatially and temporally in the plots as a result of weather, altitude, variety, plant density, available water, nutrients, and planting date; these are the main factors that influence crop yield. In this study, different multispectral and red-green-blue (RGB) vegetation indices were analyzed, as well as the digitally estimated canopy cover and plant density, in order to estimate corn grain yield using a neural network model. The relative importance of the predictor variables was also analyzed. An experiment was established with five levels of nitrogen fertilization (140, 200, 260, 320, and 380 kg/ha) and four replicates, in a completely randomized block design, resulting in 20 experimental polygons. Crop information was captured using two sensors (Parrot Sequoia_4.9, and DJI FC6310_8.8) mounted on an unmanned aerial vehicle (UAV) for two flight dates at 47 and 79 days after sowing (DAS). The correlation coefficient between the plant density, obtained through the digital count of corn plants, and the corn grain yield was 0.94; this variable was the one with the highest relative importance in the yield estimation according to Garson&rsquo;s algorithm. The canopy cover, digitally estimated, showed a correlation coefficient of 0.77 with respect to the corn grain yield, while the relative importance of this variable in the yield estimation was 0.080 and 0.093 for 47 and 79 DAS, respectively. The wide dynamic range vegetation index (WDRVI), plant density, and canopy cover showed the highest correlation coefficient and the smallest errors (R = 0.99, mean absolute error (MAE) = 0.028 t ha&minus;1, root mean square error (RMSE) = 0.125 t ha&minus;1) in the corn grain yield estimation at 47 DAS, with the WDRVI index and the density being the variables with the highest relative importance for this crop development date. For the 79 DAS flight, the combination of the normalized difference vegetation index (NDVI), normalized difference red edge (NDRE), WDRVI, excess green (EXG), triangular greenness index (TGI), and visible atmospherically resistant index (VARI), as well as plant density and canopy cover, generated the highest correlation coefficient and the smallest errors (R = 0.97, MAE = 0.249 t ha&minus;1, RMSE = 0.425 t ha&minus;1) in the corn grain yield estimation, where the density and the NDVI were the variables with the highest relative importance, with values of 0.295 and 0.184, respectively. However, the WDRVI, plant density, and canopy cover estimated the corn grain yield with acceptable precision (R = 0.96, MAE = 0.209 t ha&minus;1, RMSE = 0.449 t ha&minus;1). The generated neural network models provided a high correlation coefficient between the estimated and the observed corn grain yield, and also showed acceptable errors in the yield estimation. The spectral information registered through remote sensors mounted on unmanned aerial vehicles and its processing in vegetation indices, canopy cover, and plant density allowed the characterization and estimation of corn grain yield. Such information is very useful for decision-making and agricultural activities planning.
KW  - vegetation indices
KW  - UAV
KW  - neural network
KW  - corn plant density
KW  - corn canopy cover
KW  - yield prediction
DO  - 10.3390/agriculture10070277
TY  - EJOU
AU  - Kong, Weiren
AU  - Zhou, Deyun
AU  - Yang, Zhen
AU  - Zhao, Yiyang
AU  - Zhang, Kai
TI  - UAV Autonomous Aerial Combat Maneuver Strategy Generation with Observation Error Based on State-Adversarial Deep Deterministic Policy Gradient and Inverse Reinforcement Learning
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 7
SN  - 2079-9292

AB  - With the development of unmanned aerial vehicle (UAV) and artificial intelligence (AI) technology, Intelligent UAV will be widely used in future autonomous aerial combat. Previous researches on autonomous aerial combat within visual range (WVR) have limitations due to simplifying assumptions, limited robustness, and ignoring sensor errors. In this paper, in order to consider the error of the aircraft sensors, we model the aerial combat WVR as a state-adversarial Markov decision process (SA-MDP), which introduce the small adversarial perturbations on state observations and these perturbations do not alter the environment directly, but can mislead the agent into making suboptimal decisions. Meanwhile, we propose a novel autonomous aerial combat maneuver strategy generation algorithm with high-performance and high-robustness based on state-adversarial deep deterministic policy gradient algorithm (SA-DDPG), which add a robustness regularizers related to an upper bound on performance loss at the actor-network. At the same time, a reward shaping method based on maximum entropy (MaxEnt) inverse reinforcement learning algorithm (IRL) is proposed to improve the aerial combat strategy generation algorithm&rsquo;s efficiency. Finally, the efficiency of the aerial combat strategy generation algorithm and the performance and robustness of the resulting aerial combat strategy is verified by simulation experiments. Our main contributions are three-fold. First, to introduce the observation errors of UAV, we are modeling air combat as SA-MDP. Second, to make the strategy network of air combat maneuver more robust in the presence of observation errors, we introduce regularizers into the policy gradient. Third, to solve the problem that air combat&rsquo;s reward function is too sparse, we use MaxEnt IRL to design a shaping reward to accelerate the convergence of SA-DDPG.
KW  - aerial combat
KW  - reinforcement learning
KW  - robustness
KW  - sensor errors
KW  - network training
KW  - UAV
DO  - 10.3390/electronics9071121
TY  - EJOU
AU  - Shahmoradi, Javad
AU  - Talebi, Elaheh
AU  - Roghanchi, Pedram
AU  - Hassanalian, Mostafa
TI  - A Comprehensive Review of Applications of Drone Technology in the Mining Industry
T2  - Drones

PY  - 2020
VL  - 4
IS  - 3
SN  - 2504-446X

AB  - This paper aims to provide a comprehensive review of the current state of drone technology and its applications in the mining industry. The mining industry has shown increased interest in the use of drones for routine operations. These applications include 3D mapping of the mine environment, ore control, rock discontinuities mapping, postblast rock fragmentation measurements, and tailing stability monitoring, to name a few. The article offers a review of drone types, specifications, and applications of commercially available drones for mining applications. Finally, the research needs for the design and implementation of drones for underground mining applications are discussed.
KW  - drones
KW  - remote sensing
KW  - surface mining
KW  - underground mining
KW  - abandoned mining
DO  - 10.3390/drones4030034
TY  - EJOU
AU  - Akçay, Hüseyin G.
AU  - Kabasakal, Bekir
AU  - Aksu, Duygugül
AU  - Demir, Nusret
AU  - Öz, Melih
AU  - Erdoğan, Ali
TI  - Automated Bird Counting with Deep Learning for Regional Bird Distribution Mapping
T2  - Animals

PY  - 2020
VL  - 10
IS  - 7
SN  - 2076-2615

AB  - A challenging problem in the field of avian ecology is deriving information on bird population movement trends. This necessitates the regular counting of birds which is usually not an easily-achievable task. A promising attempt towards solving the bird counting problem in a more consistent and fast way is to predict the number of birds in different regions from their photos. For this purpose, we exploit the ability of computers to learn from past data through deep learning which has been a leading sub-field of AI for image understanding. Our data source is a collection of on-ground photos taken during our long run of birding activity. We employ several state-of-the-art generic object-detection algorithms to learn to detect birds, each being a member of one of the 38 identified species, in natural scenes. The experiments revealed that computer-aided counting outperformed the manual counting with respect to both accuracy and time. As a real-world application of image-based bird counting, we prepared the spatial bird order distribution and species diversity maps of Turkey by utilizing the geographic information system (GIS) technology. Our results suggested that deep learning can assist humans in bird monitoring activities and increase citizen scientists&rsquo; participation in large-scale bird surveys.
KW  - computer vision
KW  - machine learning
KW  - deep learning
KW  - bird detection
KW  - bird counting
KW  - bird monitoring
KW  - bird population mapping
KW  - bird diversity
KW  - GIS
KW  - citizen science
DO  - 10.3390/ani10071207
TY  - EJOU
AU  - Klemmt, Hans-Joachim
AU  - Seitz, Rudolf
AU  - Straub, Christoph
TI  - Application of Haralick’s Texture Features for Rapid Detection of Windthrow Hotspots in Orthophotos
T2  - Forests

PY  - 2020
VL  - 11
IS  - 7
SN  - 1999-4907

AB  - Windthrow and storm damage are crucial issues in practical forestry. We propose a method for rapid detection of windthrow hotspots in airborne digital orthophotos. Therefore, we apply Haralick&rsquo;s texture features on 50 &times; 50 m cells of the orthophotos and classify the cells with a random forest algorithm. We apply the classification results from a training data set on a validation set. The overall classification accuracy of the proposed method varies between 76% for fine distinction of the cells and 96% for a distinction level that tried to detect only severe damaged cells. The proposed method enables the rapid detection of windthrow hotspots in forests immediately after their occurrence in single-date data. It is not adequate for the determination of areas with only single fallen trees. Future research will investigate the possibilities and limitations when applying the method on other data sources (e.g., optical satellite data).
KW  - aerial image
KW  - airborne orthophoto
KW  - Haralick
KW  - storm damage
KW  - texture feature
KW  - windthrow
DO  - 10.3390/f11070763
TY  - EJOU
AU  - Ahmed, Habib
AU  - La, Hung M.
AU  - Gucunski, Nenad
TI  - Review of Non-Destructive Civil Infrastructure Evaluation for Bridges: State-of-the-Art Robotic Platforms, Sensors and Algorithms
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 14
SN  - 1424-8220

AB  - The non-destructive evaluation (NDE) of civil infrastructure has been an active area of research in recent decades. The traditional inspection of civil infrastructure mostly relies on visual inspection using human inspectors. To facilitate this process, different sensors for data collection and techniques for data analyses have been used to effectively carry out this task in an automated fashion. This review-based study will examine some of the recent developments in the field of autonomous robotic platforms for NDE and the structural health monitoring (SHM) of bridges. Some of the salient features of this review-based study will be discussed in the light of the existing surveys and reviews that have been published in the recent past, which will enable the clarification regarding the novelty of the present review-based study. The review methodology will be discussed in sufficient depth, which will provide insights regarding some of the primary aspects of the review methodology followed by this review-based study. In order to provide an in-depth examination of the state-of-the-art, the current research will examine the three major research streams. The first stream relates to technological robotic platforms developed for NDE of bridges. The second stream of literature examines myriad sensors used for the development of robotic platforms for the NDE of bridges. The third stream of literature highlights different algorithms for the surface- and sub-surface-level analysis of bridges that have been developed by studies in the past. A number of challenges towards the development of robotic platforms have also been discussed.
KW  - non-destructive evaluation (NDE)
KW  - structural health monitoring (SHM)
KW  - electric resistivity (ER) sensors
KW  - ground-penetrating radar (GPR)
KW  - infrared (IR) thermography
KW  - impact echo (IE)
KW  - NDE sensor fusion
KW  - convolutional neural network (CNNs)
KW  - concrete crack detection
KW  - rebar detection and localization
DO  - 10.3390/s20143954
TY  - EJOU
AU  - Muhadi, Nur A.
AU  - Abdullah, Ahmad F.
AU  - Bejo, Siti K.
AU  - Mahadi, Muhammad R.
AU  - Mijic, Ana
TI  - The Use of LiDAR-Derived DEM in Flood Applications: A Review
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 14
SN  - 2072-4292

AB  - Flood occurrence is increasing due to escalated urbanization and extreme climate change; hence, various studies on this issue and methods of flood monitoring and mapping are also increasing to reduce the severe impacts of flood disasters. The advancement of current technologies such as light detection and ranging (LiDAR) systems facilitated and improved flood applications. In a LiDAR system, a laser emits light that travels to the ground and reflects off objects like buildings and trees. The reflected light energy returns to the sensor, whereby the time interval is recorded. Since the conventional methods cannot produce high-resolution digital elevation model (DEM) data, which results in low accuracy of flood simulation results, LiDAR data are extensively used as an alternative. This review aims to study the potential and the applications of LiDAR-derived DEM in flood studies. It also provides insight into the operating principles of different LiDAR systems, system components, and advantages and disadvantages of each system. This paper discusses several topics relevant to flood studies from a LiDAR-derived DEM perspective. Furthermore, the challenges and future perspectives regarding DEM LiDAR data for flood mapping and assessment are also reviewed. This study demonstrates that LiDAR-derived data are useful in flood risk management, especially in the future assessment of flood-related problems.
KW  - airborne LiDAR
KW  - DEM
KW  - flood inundation
KW  - flood map
KW  - flood model
KW  - LiDAR
KW  - terrestrial LiDAR
DO  - 10.3390/rs12142308
TY  - EJOU
AU  - El Mahrad, Badr
AU  - Newton, Alice
AU  - Icely, John D.
AU  - Kacimi, Ilias
AU  - Abalansa, Samuel
AU  - Snoussi, Maria
TI  - Contribution of Remote Sensing Technologies to a Holistic Coastal and Marine Environmental Management Framework: A Review
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 14
SN  - 2072-4292

AB  - Coastal and marine management require the evaluation of multiple environmental threats and issues. However, there are gaps in the necessary data and poor access or dissemination of existing data in many countries around the world. This research identifies how remote sensing can contribute to filling these gaps so that environmental agencies, such as the United Nations Environmental Programme, European Environmental Agency, and International Union for Conservation of Nature, can better implement environmental directives in a cost-effective manner. Remote sensing (RS) techniques generally allow for uniform data collection, with common acquisition and reporting methods, across large areas. Furthermore, these datasets are sometimes open-source, mainly when governments finance satellite missions. Some of these data can be used in holistic, coastal and marine environmental management frameworks, such as the DAPSI(W)R(M) framework (Drivers–Activities–Pressures–State changes–Impacts (on Welfare)–Responses (as Measures), an updated version of Drivers–Pressures–State–Impact–Responses. The framework is a useful and holistic problem-structuring framework that can be used to assess the causes, consequences, and responses to change in the marine environment. Six broad classifications of remote data collection technologies are reviewed for their potential contribution to integrated marine management, including Satellite-based Remote Sensing, Aerial Remote Sensing, Unmanned Aerial Vehicles, Unmanned Surface Vehicles, Unmanned Underwater Vehicles, and Static Sensors. A significant outcome of this study is practical inputs into each component of the DAPSI(W)R(M) framework. The RS applications are not expected to be all-inclusive; rather, they provide insight into the current use of the framework as a foundation for developing further holistic resource technologies for management strategies in the future. A significant outcome of this research will deliver practical insights for integrated coastal and marine management and demonstrate the usefulness of RS to support the implementation of environmental goals, descriptors, targets, and policies, such as the Water Framework Directive, Marine Strategy Framework Directive, Ocean Health Index, and United Nations Sustainable Development Goals. Additionally, the opportunities and challenges of these technologies are discussed.
KW  - remote sensing
KW  - DPSIR
KW  - coastal and marine management
KW  - environmental policies and directives
KW  - WFD
KW  - MSFD
KW  - ocean health index
KW  - sustainable development goals
DO  - 10.3390/rs12142313
TY  - EJOU
AU  - Vidal, Vinicius F.
AU  - Honório, Leonardo M.
AU  - Dias, Felipe M.
AU  - Pinto, Milena F.
AU  - Carvalho, Alexandre L.
AU  - Marcato, Andre L. M.
TI  - Sensors Fusion and Multidimensional Point Cloud Analysis for Electrical Power System Inspection
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 14
SN  - 1424-8220

AB  - Thermal inspection is a powerful tool that enables the diagnosis of several components at its early stages. One critical aspect that influences thermal inspection outputs is the infrared reflection from external sources. This situation may change the readings, demanding that an expert correctly define the camera position, which is a time consuming and expensive operation. To mitigate this problem, this work proposes an autonomous system capable of identifying infrared reflections by filtering and fusing data obtained from both stereo and thermal cameras. The process starts by acquiring readings from multiples Observation Points (OPs) where, at each OP, the system processes the 3D point cloud and thermal image by fusing them together. The result is a dense point cloud where each point has its spatial position and temperature. Considering that each point&rsquo;s information is acquired from multiple poses, it is possible to generate a temperature profile of each spatial point and filter undesirable readings caused by interference and other phenomena. To deploy and test this approach, a Directional Robotic System (DRS) is mounted over a traditional human-operated service vehicle. In that way, the DRS autonomously tracks and inspects any desirable equipment as the service vehicle passes them by. To demonstrate the results, this work presents the algorithm workflow, a proof of concept, and a real application result, showing improved performance in real-life conditions.
KW  - thermal inspection
KW  - multidimensional point cloud
KW  - sensor fusion
KW  - autonomous inspection
KW  - infrared noise filtering
DO  - 10.3390/s20144042
TY  - EJOU
AU  - Qiu, Zhengjun
AU  - Zhao, Nan
AU  - Zhou, Lei
AU  - Wang, Mengcen
AU  - Yang, Liangliang
AU  - Fang, Hui
AU  - He, Yong
AU  - Liu, Yufei
TI  - Vision-Based Moving Obstacle Detection and Tracking in Paddy Field Using Improved Yolov3 and Deep SORT
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 15
SN  - 1424-8220

AB  - Using intelligent agricultural machines in paddy fields has received great attention. An obstacle avoidance system is required with the development of agricultural machines. In order to make the machines more intelligent, detecting and tracking obstacles, especially the moving obstacles in paddy fields, is the basis of obstacle avoidance. To achieve this goal, a red, green and blue (RGB) camera and a computer were used to build a machine vision system, mounted on a transplanter. A method that combined the improved You Only Look Once version 3 (Yolov3) and deep Simple Online and Realtime Tracking (deep SORT) was used to detect and track typical moving obstacles, and figure out the center point positions of the obstacles in paddy fields. The improved Yolov3 has 23 residual blocks and upsamples only once, and has new loss calculation functions. Results showed that the improved Yolov3 obtained mean intersection over union (mIoU) score of 0.779 and was 27.3% faster in processing speed than standard Yolov3 on a self-created test dataset of moving obstacles (human and water buffalo) in paddy fields. An acceptable performance for detecting and tracking could be obtained in a real paddy field test with an average processing speed of 5&ndash;7 frames per second (FPS), which satisfies actual work demands. In future research, the proposed system could support the intelligent agriculture machines more flexible in autonomous navigation.
KW  - machine vision
KW  - deep learning
KW  - detecting and tracking
KW  - moving obstacles
KW  - paddy field
DO  - 10.3390/s20154082
TY  - EJOU
AU  - Fang, Peng
AU  - Zhang, Xiwang
AU  - Wei, Panpan
AU  - Wang, Yuanzheng
AU  - Zhang, Huiyi
AU  - Liu, Feng
AU  - Zhao, Jun
TI  - The Classification Performance and Mechanism of Machine Learning Algorithms in Winter Wheat Mapping Using Sentinel-2 10 m Resolution Imagery
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 15
SN  - 2076-3417

AB  - Machine learning algorithms are crucial for crop identification and mapping. However, many works only focus on the identification results of these algorithms, but pay less attention to their classification performance and mechanism. In this paper, based on Google Earth Engine (GEE), Sentinel-2 10 m resolution images during a specific phenological period of winter wheat were obtained. Then, support vector machine (SVM), random forest (RF), and classification and regression tree (CART) machine learning algorithms were employed to identify and map winter wheat in a large-scale area. The hyperparameters of the three machine learning algorithms were tuned by grid search and the 5-fold cross-validation method. The classification performance of the three machine learning algorithms were compared, the results of which demonstrate that SVM achieves best performance in identifying winter wheat, and its overall accuracy (OA), user&rsquo;s accuracy (UA), producer&rsquo;s accuracy (PA), and kappa coefficient (Kappa) are 0.94, 0.95, 0.95, and 0.92, respectively. Moreover, 50 various combinations of training and validation sets were used to analyze the generalization ability of the algorithms, and the results show that the average OA of SVM, RF, and CART are 0.93, 0.92, and 0.88, respectively, thus indicating that SVM and RF are more robust than CART. To further explore the sensitivity of SVM, RF, and CART to variations of the algorithm parameters&mdash;namely, (C and gamma), (tree and split), and (maxD and minSP)&mdash;we employed the grid search method to iterate these parameters, respectively, and to analyze the effect of these parameters on the accuracy scores and classification residuals. It was found that with the change of (C and gamma) in (0.01~1000), SVM&rsquo;s maximum variation of accuracy score is up to 0.63, and the maximum variation of residuals is 76,215 km2. We concluded that SVM is sensitive to the parameters (C and gamma) and presents a positive correlation. When the parameters (tree and split) change between (100~600) and (1~6), respectively, the RF&rsquo;s maximum variation of accuracy score is 0.08, and the maximum variation of residuals is 1157 km2, indicating that RF is low in sensitivity toward the parameters (tree and split). When the parameters (maxD and minSP) are between (10~60), the maximum accuracy change value is 0.06, and the maximum variation of residuals is 6943 km2. Therefore, compared to RF, CART is sensitive to the parameters (maxD and minSP) and has poor robustness. In general, under the conditions of the hyperparameters, SVM and RF exhibit optimal classification performance, while CART has relatively inferior performance. Meanwhile, SVM, RF, and CART have different sensitivities toward the algorithm parameters; that is, SVM and CART are more sensitive to the algorithm parameters, while RF has low sensitivity toward changes in the algorithm parameters. The different parameters cause great changes in the accuracy scores and residuals, so it is necessary to determine the algorithm hyperparameters. Generally, default parameters can be used to achieve crop classification, but we recommend the enumeration method, similar to grid search, as a practical way to improve the classification performance of the algorithm if the best classification effect is expected.
KW  - machine learning algorithms
KW  - classification performance
KW  - winter wheat mapping
KW  - Sentinel-2
KW  - large-scale
DO  - 10.3390/app10155075
TY  - EJOU
AU  - Pulido, Dagoberto
AU  - Salas, Joaquín
AU  - Rös, Matthias
AU  - Puettmann, Klaus
AU  - Karaman, Sertac
TI  - Assessment of Tree Detection Methods in Multispectral Aerial Images
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 15
SN  - 2072-4292

AB  - Detecting individual trees and quantifying their biomass is crucial for carbon accounting procedures at the stand, landscape, and national levels. A significant challenge for many organizations is the amount of effort necessary to document carbon storage levels, especially in terms of human labor. To advance towards the goal of efficiently assessing the carbon content of forest, we evaluate methods to detect trees from high-resolution images taken from unoccupied aerial systems (UAS). In the process, we introduce the Digital Elevated Vegetation Model (DEVM), a representation that combines multispectral images, digital surface models, and digital terrain models. We show that the DEVM facilitates the development of refined synthetic data to detect individual trees using deep learning-based approaches. We carried out experiments in two tree fields located in different countries. Simultaneously, we perform comparisons among an array of classical and deep learning-based methods highlighting the precision and reliability of the DEVM.
KW  - tree detection
KW  - convolutional neural networks
KW  - unocuppied aerial systems
KW  - digital elevated vegetation model
KW  - synthetic data set
DO  - 10.3390/rs12152379
TY  - EJOU
AU  - Prosekov, Alexander
AU  - Kuznetsov, Alexander
AU  - Rada, Artem
AU  - Ivanova, Svetlana
TI  - Methods for Monitoring Large Terrestrial Animals in the Wild
T2  - Forests

PY  - 2020
VL  - 11
IS  - 8
SN  - 1999-4907

AB  - Reliable information about wildlife is absolutely important for making informed management decisions. The issues with the effectiveness of the control and monitoring of both large and small wild animals are relevant to assess and protect the world&rsquo;s biodiversity. Monitoring becomes part of the methods in wildlife ecology for observation, assessment, and forecasting of the human environment. World practice reveals the potential of the joint application of both proven traditional and modern technologies using specialized equipment to organize environmental control and management processes. Monitoring large terrestrial animals require an individual approach due to their low density and larger habitat. Elk/moose are such animals. This work aims to evaluate the methods for monitoring large wild animals, suitable for controlling the number of elk/moose in the framework of nature conservation activities. Using different models allows determining the population size without affecting the animals and without significant financial costs. Although, the accuracy of each model is determined by its postulates implementation and initial conditions that need statistical data. Depending on the geographical, climatic, and economic conditions in each territory, it is possible to use different tools and equipment (e.g., cameras, GPS sensors, and unmanned aerial vehicles), a flexible variation of which will allow reaching the golden mean between the desires and capabilities of researchers.
KW  - monitoring methods
KW  - large wild animals
KW  - elk
KW  - moose
KW  - hunting
KW  - unmanned aerial vehicles
DO  - 10.3390/f11080808
TY  - EJOU
AU  - Schlosser, Aletta D.
AU  - Szabó, Gergely
AU  - Bertalan, László
AU  - Varga, Zsolt
AU  - Enyedi, Péter
AU  - Szabó, Szilárd
TI  - Building Extraction Using Orthophotos and Dense Point Cloud Derived from Visual Band Aerial Imagery Based on Machine Learning and Segmentation
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 15
SN  - 2072-4292

AB  - Urban sprawl related increase of built-in areas requires reliable monitoring methods and remote sensing can be an efficient technique. Aerial surveys, with high spatial resolution, provide detailed data for building monitoring, but archive images usually have only visible bands. We aimed to reveal the efficiency of visible orthophotographs and photogrammetric dense point clouds in building detection with segmentation-based machine learning (with five algorithms) using visible bands, texture information, and spectral and morphometric indices in different variable sets. Usually random forest (RF) had the best (99.8%) and partial least squares the worst overall accuracy (~60%). We found that &gt;95% accuracy can be gained even in class level. Recursive feature elimination (RFE) was an efficient variable selection tool, its result with six variables was like when we applied all the available 31 variables. Morphometric indices had 82% producer&rsquo;s and 85% user&rsquo;s Accuracy (PA and UA, respectively) and combining them with spectral and texture indices, it had the largest contribution in the improvement. However, morphometric indices are not always available but by adding texture and spectral indices to red-green-blue (RGB) bands the PA improved with 12% and the UA with 6%. Building extraction from visual aerial surveys can be accurate, and archive images can be involved in the time series of a monitoring.
KW  - photogrammetry
KW  - RGB indices
KW  - image texture
KW  - morphometric indices
KW  - recursive feature elimination
KW  - random forest
KW  - support vector machine
KW  - multiple adaptive regression spline
KW  - partial least squares
DO  - 10.3390/rs12152397
TY  - EJOU
AU  - Zwęgliński, Tomasz
TI  - The Use of Drones in Disaster Aerial Needs Reconnaissance and Damage Assessment – Three-Dimensional Modeling and Orthophoto Map Study
T2  - Sustainability

PY  - 2020
VL  - 12
IS  - 15
SN  - 2071-1050

AB  - The aim of this research is to provide disaster managers with the results of testing three-dimensional modeling and orthophoto mapping, so as to add value to aerial assessments of flood-related needs and damages. The relevant testing of solutions concerning the real needs of disaster managers is an essential part of the pre-disaster phase. As such, providing evidence-based results of the solutions&rsquo; performance is critical with regard to purchasing them and their successful implementation for disaster management purposes. Since disaster response is mostly realized in complex and dynamic, rather than repetitive, environments, it requires pertinent testing methods. A quasi-experimental approach, applied in a form of a full-scale trial meets disaster manager&rsquo;s requirements as well as addressing limitations resulting from the disaster environment&rsquo;s characteristics. Three-dimensional modeling and orthophoto mapping have already proven their potential in many professional fields; however, they have not yet been broadly tested for disaster response purposes. Therefore, the objective here is to verify the technologies regarding their applicability in aerial reconnaissance in sudden-onset disasters. The hypothesis assumes that they will improve the efficiency (e.g., time) and effectiveness (e.g., accuracy of revealed data) of this process. The research verifies that the technologies have a potential to facilitate disaster managers with more precise damage assessment; however, their effectivity was less than expected in terms of needs reconnaissance. Secondly, the overall assessment process is heavily burdened by data processing time, however, the technologies allow a reduction of analytical work.
KW  - civil protection
KW  - disaster management
KW  - flood response
KW  - aerial
KW  - drone
KW  - needs reconnaissance
KW  - damage assessment
KW  - three-dimensional modeling
KW  - orthophoto map
DO  - 10.3390/su12156080
TY  - EJOU
AU  - Pleșoianu, Alin-Ionuț
AU  - Stupariu, Mihai-Sorin
AU  - Șandric, Ionuț
AU  - Pătru-Stupariu, Ileana
AU  - Drăguț, Lucian
TI  - Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 15
SN  - 2072-4292

AB  - Traditional methods for individual tree-crown (ITC) detection (image classification, segmentation, template matching, etc.) applied to very high-resolution remote sensing imagery have been shown to struggle in disparate landscape types or image resolutions due to scale problems and information complexity. Deep learning promised to overcome these shortcomings due to its superior performance and versatility, proven with reported detection rates of ~90%. However, such models still find their limits in transferability across study areas, because of different tree conditions (e.g., isolated trees vs. compact forests) and/or resolutions of the input data. This study introduces a highly replicable deep learning ensemble design for ITC detection and species classification based on the established single shot detector (SSD) model. The ensemble model design is based on varying the input data for the SSD models, coupled with a voting strategy for the output predictions. Very high-resolution unmanned aerial vehicles (UAV), aerial remote sensing imagery and elevation data are used in different combinations to test the performance of the ensemble models in three study sites with highly contrasting spatial patterns. The results show that ensemble models perform better than any single SSD model, regardless of the local tree conditions or image resolution. The detection performance and the accuracy rates improved by 3&ndash;18% with only as few as two participant single models, regardless of the study site. However, when more than two models were included, the performance of the ensemble models only improved slightly and even dropped.
KW  - tree-crown detection
KW  - deep learning
KW  - ensemble model
KW  - object detection
KW  - single shot detector
DO  - 10.3390/rs12152426
TY  - EJOU
AU  - Cai, Yiming
AU  - Ding, Yalin
AU  - Zhang, Hongwen
AU  - Xiu, Jihong
AU  - Liu, Zhiming
TI  - Geo-Location Algorithm for Building Targets in Oblique Remote Sensing Images Based on Deep Learning and Height Estimation
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 15
SN  - 2072-4292

AB  - To improve the accuracy of the geographic positioning of a single aerial remote sensing image, the height information of a building in the image must be considered. Oblique remote sensing images are essentially two-dimensional images and produce a large positioning error if a traditional positioning algorithm is used to locate the building directly. To address this problem, this study uses a convolutional neural network to automatically detect the location of buildings in remote sensing images. Moreover, it optimizes an automatic building recognition algorithm for oblique aerial remote sensing images based on You Only Look Once V4 (YOLO V4). This study also proposes a positioning algorithm for the building target, which uses the imaging angle to estimate the height of a building, and combines the spatial coordinate transformation matrix to calculate high-accuracy geo-location of target buildings. Simulation analysis shows that the traditional positioning algorithm inevitably leads to large errors in the positioning of building targets. When the target height is 50 m and the imaging angle is 70&deg;, the positioning error is 114.89 m. Flight tests show that the algorithm established in this study can improve the positioning accuracy of building targets by approximately 20%&ndash;50% depending on the difference in target height.
KW  - remote sensing
KW  - target geo-location
KW  - building target
KW  - elevation error
KW  - deep learning
KW  - error analysis
DO  - 10.3390/rs12152427
TY  - EJOU
AU  - Li, Hongchen
AU  - Yang, Zhong
AU  - Han, Jiaming
AU  - Lai, Shangxiang
AU  - Zhang, Qiuyan
AU  - Zhang, Chi
AU  - Fang, Qianhui
AU  - Hu, Guoxiong
TI  - TL-Net: A Novel Network for Transmission Line Scenes Classification
T2  - Energies

PY  - 2020
VL  - 13
IS  - 15
SN  - 1996-1073

AB  - With the development of unmanned aerial vehicle (UAV) control technology, one of the recent trends in this research domain is to utilize UAVs to perform non-contact transmission line inspection. The RGB camera mounted on UAVs collects large numbers of images during the transmission line inspection, but most of them contain no critical components of transmission lines. Hence, it is a momentous task to adopt image classification algorithms to distinguish key images from all aerial images. In this work, we propose a novel classification method to remove redundant data and retain informative images. A novel transmission line scene dataset, namely TLS_dataset, is built to evaluate the classification performance of networks. Then, we propose a novel convolutional neural network (CNN), namely TL-Net, to classify transmission line scenes. In comparison to other typical deep learning networks, TL-Nets gain better classification accuracy and less memory consumption. The experimental results show that TL-Net101 gains 99.68% test accuracy on the TLS_dataset.
KW  - transmission lines inspection
KW  - unmanned aerial vehicle
KW  - image classification
KW  - deep neural network
KW  - voting classification strategy
DO  - 10.3390/en13153910
TY  - EJOU
AU  - Nielsen, Mikkel S.
AU  - Nikolov, Ivan
AU  - Kruse, Emil K.
AU  - Garnæs, Jørgen
AU  - Madsen, Claus B.
TI  - High-Resolution Structure-from-Motion for Quantitative Measurement of Leading-Edge Roughness
T2  - Energies

PY  - 2020
VL  - 13
IS  - 15
SN  - 1996-1073

AB  - Over time, erosion of the leading edge of wind turbine blades increases the leading-edge roughness (LER). This may reduce the aerodynamic performance of the blade and hence the annual energy production of the wind turbine. As early detection is key for cost-effective maintenance, inspection methods are needed to quantify the LER of the blade. The aim of this proof-of-principle study is to determine whether high-resolution Structure-from-Motion (SfM) has the sufficient resolution and accuracy for quantitative inspection of LER. SfM provides 3D reconstruction of an object geometry using overlapping images of the object acquired with an RGB camera. Using information of the camera positions and orientations, absolute scale of the reconstruction can be achieved. Combined with a UAV platform, SfM has the potential for remote blade inspections with a reduced downtime. The tip of a decommissioned blade with an artificially enhanced erosion was used for the measurements. For validation, replica molding was used to transfer areas-of-interest to the lab for reference measurements using confocal microscopy. The SfM reconstruction resulted in a spatial resolution of 1 mm as well as a sub-mm accuracy in both the RMS surface roughness and the size of topographic features. In conclusion, high-resolution SfM demonstrated a successful quantitative reconstruction of LER.
KW  - structure from motion
KW  - surface analysis
KW  - leading-edge roughness
KW  - blade inspection
KW  - quantitative 3D reconstruction
KW  - photogrammetry
DO  - 10.3390/en13153916
TY  - EJOU
AU  - Su, Wen-Hao
TI  - Advanced Machine Learning in Point Spectroscopy, RGB- and Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review
T2  - Smart Cities

PY  - 2020
VL  - 3
IS  - 3
SN  - 2624-6511

AB  - Crop productivity is readily reduced by competition from weeds. It is particularly important to control weeds early to prevent yield losses. Limited herbicide choices and increasing costs of weed management are threatening the profitability of crops. Smart agriculture can use intelligent technology to accurately measure the distribution of weeds in the field and perform weed control tasks in selected areas, which cannot only improve the effectiveness of pesticides, but also increase the economic benefits of agricultural products. The most important thing for an automatic system to remove weeds within crop rows is to utilize reliable sensing technology to achieve accurate differentiation of weeds and crops at specific locations in the field. In recent years, there have been many significant achievements involving the differentiation of crops and weeds. These studies are related to the development of rapid and non-destructive sensors, as well as the analysis methods for the data obtained. This paper presents a review of the use of three sensing methods including spectroscopy, color imaging, and hyperspectral imaging in the discrimination of crops and weeds. Several algorithms of machine learning have been employed for data analysis such as convolutional neural network (CNN), artificial neural network (ANN), and support vector machine (SVM). Successful applications include the weed detection in grain crops (such as maize, wheat, and soybean), vegetable crops (such as tomato, lettuce, and radish), and fiber crops (such as cotton) with unsupervised or supervised learning. This review gives a brief introduction into proposed sensing and machine learning methods, then provides an overview of instructive examples of these techniques for weed/crop discrimination. The discussion describes the recent progress made in the development of automated technology for accurate plant identification as well as the challenges and future prospects. It is believed that this review is of great significance to those who study automatic plant care in crops using intelligent technology.
KW  - smart farm
KW  - sensing
KW  - machine learning
KW  - plant identification
KW  - weed control
DO  - 10.3390/smartcities3030039
TY  - EJOU
AU  - Kim, Dong-Hyun
AU  - Go, Yong-Guk
AU  - Choi, Soo-Mi
TI  - An Aerial Mixed-Reality Environment for First-Person-View Drone Flying
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 16
SN  - 2076-3417

AB  - A drone be able to fly without colliding to preserve the surroundings and its own safety. In addition, it must also incorporate numerous features of interest for drone users. In this paper, an aerial mixed-reality environment for first-person-view drone flying is proposed to provide an immersive experience and a safe environment for drone users by creating additional virtual obstacles when flying a drone in an open area. The proposed system is effective in perceiving the depth of obstacles, and enables bidirectional interaction between real and virtual worlds using a drone equipped with a stereo camera based on human binocular vision. In addition, it synchronizes the parameters of the real and virtual cameras to effectively and naturally create virtual objects in a real space. Based on user studies that included both general and expert users, we confirm that the proposed system successfully creates a mixed-reality environment using a flying drone by quickly recognizing real objects and stably combining them with virtual objects.
KW  - aerial mixed-reality
KW  - drones
KW  - stereo cameras
KW  - first-person-view
KW  - head-mounted display
DO  - 10.3390/app10165436
TY  - EJOU
AU  - Blaya-Ros, Pedro J.
AU  - Blanco, Víctor
AU  - Domingo, Rafael
AU  - Soto-Valles, Fulgencio
AU  - Torres-Sánchez, Roque
TI  - Feasibility of Low-Cost Thermal Imaging for Monitoring Water Stress in Young and Mature Sweet Cherry Trees
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 16
SN  - 2076-3417

AB  - Infrared thermography has been introduced as an affordable tool for plant water status monitoring, especially in regions where water availability is the main limiting factor in agricultural production. This paper outlines the potential applications of low-cost thermal imaging devices to evaluate the water status of young and mature sweet cherry trees (Prunus avium L.) submitted to water stress. Two treatments per plot were assayed: (i) a control treatment irrigated to ensure non-limiting soil water conditions; and (ii) a water-stress treatment. The seasonal evolution of the temperature of the canopy (Tc) and the difference between Tc and air temperature (&Delta;T) were compared and three thermal indices were calculated: crop water stress index (CWSI), degrees above control treatment (DAC) and degrees above non-water-stressed baseline (DANS). Midday stem water potential (&Psi;stem) was used as the reference indicator of water stress and linear relationships of Tc, &Delta;T, CWSI, DAC and DANS with &Psi;stem were discussed in order to assess their sensitivity to quantify water stress. CWSI and DANS exhibited strong relationships with &Psi;stem and two regression lines to young and mature trees were found. The promising results obtained highlight that using low-cost infrared thermal devices can be used to determine the plant water status in sweet cherry trees.
KW  - water stress
KW  - Prunus avium L.
KW  - stem water potential
KW  - low-cost thermography
KW  - thermal indexes
KW  - canopy temperature
KW  - non-water-stressed baselines
KW  - non-transpiration baseline
DO  - 10.3390/app10165461
TY  - EJOU
AU  - Billet, Kévin
AU  - Malinowska, Magdalena A.
AU  - Munsch, Thibaut
AU  - Unlubayir, Marianne
AU  - Adler, Sophie
AU  - Delanoue, Guillaume
AU  - Lanoue, Arnaud
TI  - Semi-Targeted Metabolomics to Validate Biomarkers of Grape Downy Mildew Infection Under Field Conditions
T2  - Plants

PY  - 2020
VL  - 9
IS  - 8
SN  - 2223-7747

AB  - Grape downy mildew is a devastating disease worldwide and new molecular phenotyping tools are required to detect metabolic changes associated to plant disease symptoms. In this purpose, we used UPLC-DAD-MS-based semi-targeted metabolomics to screen downy mildew symptomatic leaves that expressed oil spots (6 dpi, days post-infection) and necrotic lesions (15 dpi) under natural infections in the field. Leaf extract analyses enabled the identification of 47 metabolites belonging to the primary metabolism including 6 amino acids and 1 organic acid, as well as an important diversity of specialized metabolites including 9 flavonols, 11 flavan-3-ols, 3 phenolic acids, and stilbenoids with various degree of polymerization (DP) including 4 stilbenoids DP1, 8 stilbenoids DP2, and 4 stilbenoids DP3. Principal component analysis (PCA) was applied as unsupervised multivariate statistical analysis method to reveal metabolic variables that were affected by the infection status. Univariate and multivariate statistics revealed 33 and 27 metabolites as relevant infection biomarkers at 6 and 15 dpi, respectively. Correlation-based networks highlighted a general decrease of flavonoid-related metabolites, whereas stilbenoid DP1 and DP2 concentrations increased upon downy mildew infection. Stilbenoids DP3 were identified only in necrotic lesions representing late biomarkers of downy mildew infection.
KW  - grape
KW  - downy mildew
KW  - semi-targeted metabolomics
KW  - infection biomarkers
KW  - polyphenols
KW  - stilbenoids
KW  - correlation network
DO  - 10.3390/plants9081008
TY  - EJOU
AU  - de Bem, Pablo P.
AU  - de Carvalho Júnior, Osmar A.
AU  - de Carvalho, Osmar L.
AU  - Gomes, Roberto A.
AU  - Fontes Guimarães, Renato
TI  - Performance Analysis of Deep Convolutional Autoencoders with Different Patch Sizes for Change Detection from Burnt Areas
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 16
SN  - 2072-4292

AB  - Fire is one of the primary sources of damages to natural environments globally. Estimates show that approximately 4 million km2 of land burns yearly. Studies have shown that such estimates often underestimate the real extent of burnt land, which highlights the need to find better, state-of-the-art methods to detect and classify these areas. This study aimed to analyze the use of deep convolutional Autoencoders in the classification of burnt areas, considering different sample patch sizes. A simple Autoencoder and the U-Net and ResUnet architectures were evaluated. We collected Landsat 8 OLI+ data from three scenes in four consecutive dates to detect the changes specifically in the form of burnt land. The data were sampled according to four different sampling strategies to evaluate possible performance changes related to sampling window sizes. The training stage used two scenes, while the validation stage used the remaining scene. The ground truth change mask was created using the Normalized Burn Ratio (NBR) spectral index through a thresholding approach. The classifications were evaluated according to the F1 index, Kappa index, and mean Intersection over Union (mIoU) value. Results have shown that the U-Net and ResUnet architectures offered the best classifications with average F1, Kappa, and mIoU values of approximately 0.96, representing excellent classification results. We have also verified that a sampling window size of 256 by 256 pixels offered the best results.
KW  - deep learning
KW  - CNN
KW  - classification
KW  - fire
KW  - multitemporal image
DO  - 10.3390/rs12162576
TY  - EJOU
AU  - Wei, Marcelo C.
AU  - Molin, José P.
TI  - Soybean Yield Estimation and Its Components: A Linear Regression Approach
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 8
SN  - 2077-0472

AB  - Soybean yield estimation is either based on yield monitors or agro-meteorological and satellite imagery data, but they present several limiting factors regarding on-farm decision level. Aware that machine learning approaches have been largely applied to estimate soybean yield and the availability of data regarding soybean yield and its components (number of grains (NG) and thousand grains weight (TGW)), there is an opportunity to study their relationships. The objective was to explore the relationships between soybean yield and its components, generate equations to estimate yield and evaluate its prediction accuracy. The training dataset was composed of soybean yield and its components&rsquo; data from 2010 to 2019. Linear regression models based on NG, TGW and yield were fitted on the training dataset and applied to a validation dataset composed of 58 on-field collected samples. It was found that globally TGW and NG presented weak (r = 0.50) and strong (r = 0.92) linear relationships with yield, respectively. In addition to that, applying the fitted models to the validation dataset, model based on NG presented the highest accuracy, coefficient of determination (R2) of 0.70, mean absolute error (MAE) of 639.99 kg ha&minus;1 and root mean squared error (RMSE) of 726.67 kg ha&minus;1.
KW  - hundred grains weight
KW  - machine learning
KW  - number of grains
KW  - precision agriculture
KW  - thousand grains weight
DO  - 10.3390/agriculture10080348
TY  - EJOU
AU  - Li, Daoliang
AU  - Zhang, Pan
AU  - Chen, Tao
AU  - Qin, Wei
TI  - Recent Development and Challenges in Spectroscopy and Machine Vision Technologies for Crop Nitrogen Diagnosis: A Review
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 16
SN  - 2072-4292

AB  - Recent development of non-destructive optical techniques, such as spectroscopy and machine vision technologies, have laid a good foundation for real-time monitoring and precise management of crop N status. However, their advantages and disadvantages have not been systematically summarized and evaluated. Here, we reviewed the state-of-the-art of non-destructive optical methods for monitoring the N status of crops, and summarized their advantages and disadvantages. We mainly focused on the contribution of spectral and machine vision technology to the accurate diagnosis of crop N status from three aspects: system selection, data processing, and estimation methods. Finally, we discussed the opportunities and challenges of the application of these technologies, followed by recommendations for future work to address the challenges.
KW  - crops
KW  - nitrogen status
KW  - diagnosis
KW  - spectroscopy
KW  - vision
DO  - 10.3390/rs12162578
TY  - EJOU
AU  - Ding, Kaimeng
AU  - Liu, Yueming
AU  - Xu, Qin
AU  - Lu, Fuqiang
TI  - A Subject-Sensitive Perceptual Hash Based on MUM-Net for the Integrity Authentication of High Resolution Remote Sensing Images
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 8
SN  - 2220-9964

AB  - Data security technology is of great significance to the application of high resolution remote sensing image (HRRS) images. As an important data security technology, perceptual hash overcomes the shortcomings of cryptographic hashing that is not robust and can achieve integrity authentication of HRRS images based on perceptual content. However, the existing perceptual hash does not take into account whether the user focuses on certain types of information of the HRRS image. In this paper, we introduce the concept of subject-sensitive perceptual hash, which can be seen as a special case of conventional perceptual hash, for the integrity authentication of HRRS image. To achieve subject-sensitive perceptual hash, we propose a new deep convolutional neural network architecture, named MUM-Net, for extracting robust features of HRRS images. MUM-Net is the core of perceptual hash algorithm, and it uses focal loss as the loss function to overcome the imbalance between the positive and negative samples in the training samples. The robust features extracted by MUM-Net are further compressed and encoded to obtain the perceptual hash sequence of HRRS image. Experiments show that our algorithm has higher tamper sensitivity to subject-related malicious tampering, and the robustness is improved by about 10% compared to the existing U-net-based algorithm; compared to other deep learning-based algorithms, this algorithm achieves a better balance between robustness and tampering sensitivity, and has better overall performance.
KW  - perceptual hash
KW  - integrity authentication
KW  - subject-sensitive
KW  - HRRS image
KW  - deep learning
DO  - 10.3390/ijgi9080485
TY  - EJOU
AU  - Gonçalves, Gil
AU  - Andriolo, Umberto
AU  - Gonçalves, Luísa
AU  - Sobral, Paula
AU  - Bessa, Filipa
TI  - Quantifying Marine Macro Litter Abundance on a Sandy Beach Using Unmanned Aerial Systems and Object-Oriented Machine Learning Methods
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 16
SN  - 2072-4292

AB  - Unmanned aerial systems (UASs) have recently been proven to be valuable remote sensing tools for detecting marine macro litter (MML), with the potential of supporting pollution monitoring programs on coasts. Very low altitude images, acquired with a low-cost RGB camera onboard a UAS on a sandy beach, were used to characterize the abundance of stranded macro litter. We developed an object-oriented classification strategy for automatically identifying the marine macro litter items on a UAS-based orthomosaic. A comparison is presented among three automated object-oriented machine learning (OOML) techniques, namely random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN). Overall, the detection was satisfactory for the three techniques, with mean F-scores of 65% for KNN, 68% for SVM, and 72% for RF. A comparison with manual detection showed that the RF technique was the most accurate OOML macro litter detector, as it returned the best overall detection quality (F-score) with the lowest number of false positives. Because the number of tuning parameters varied among the three automated machine learning techniques and considering that the three generated abundance maps correlated similarly with the abundance map produced manually, the simplest KNN classifier was preferred to the more complex RF. This work contributes to advances in remote sensing marine litter surveys on coasts, optimizing the automated detection on UAS-derived orthomosaics. MML abundance maps, produced by UAS surveys, assist coastal managers and authorities through environmental pollution monitoring programs. In addition, they contribute to search and evaluation of the mitigation measures and improve clean-up operations on coastal environments.
KW  - drone
KW  - anthropogenic debris
KW  - OBIA
KW  - random forest
KW  - support vector machine
KW  - k-nearest neighbor
DO  - 10.3390/rs12162599
TY  - EJOU
AU  - Zhao, Weiwei
AU  - Chu, Hairong
AU  - Miao, Xikui
AU  - Guo, Lihong
AU  - Shen, Honghai
AU  - Zhu, Chenhao
AU  - Zhang, Feng
AU  - Liang, Dongxin
TI  - Research on the Multiagent Joint Proximal Policy Optimization Algorithm Controlling Cooperative Fixed-Wing UAV Obstacle Avoidance
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 16
SN  - 1424-8220

AB  - Multiple unmanned aerial vehicle (UAV) collaboration has great potential. To increase the intelligence and environmental adaptability of multi-UAV control, we study the application of deep reinforcement learning algorithms in the field of multi-UAV cooperative control. Aiming at the problem of a non-stationary environment caused by the change of learning agent strategy in reinforcement learning in a multi-agent environment, the paper presents an improved multiagent reinforcement learning algorithm&mdash;the multiagent joint proximal policy optimization (MAJPPO) algorithm with the centralized learning and decentralized execution. This algorithm uses the moving window averaging method to make each agent obtain a centralized state value function, so that the agents can achieve better collaboration. The improved algorithm enhances the collaboration and increases the sum of reward values obtained by the multiagent system. To evaluate the performance of the algorithm, we use the MAJPPO algorithm to complete the task of multi-UAV formation and the crossing of multiple-obstacle environments. To simplify the control complexity of the UAV, we use the six-degree of freedom and 12-state equations of the dynamics model of the UAV with an attitude control loop. The experimental results show that the MAJPPO algorithm has better performance and better environmental adaptability.
KW  - reinforcement learning
KW  - proximal policy optimization (PPO)
KW  - the joint state-value function
KW  - multiagent cooperative
KW  - multiple unmanned aerial vehicles (multi-UAV) formation
KW  - obstacle avoidance
DO  - 10.3390/s20164546
TY  - EJOU
AU  - Vlachopoulos, Odysseas
AU  - Leblon, Brigitte
AU  - Wang, Jinfei
AU  - Haddadi, Ataollah
AU  - LaRocque, Armand
AU  - Patterson, Greg
TI  - Delineation of Crop Field Areas and Boundaries from UAS Imagery Using PBIA and GEOBIA with Random Forest Classification
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 16
SN  - 2072-4292

AB  - Unmanned aircraft systems (UAS) have been proven cost- and time-effective remote-sensing platforms for precision agriculture applications. This study presents a method for automatic delineation of field areas and boundaries that uses UAS multispectral orthomosaics acquired over 7 vegetated fields having a variety of crops in Prince Edward Island (PEI). This information is needed by crop insurance agencies and growers for an accurate determination of crop insurance premiums. The field areas and boundaries were delineated by applying both a pixel-based and an object-based supervised random forest (RF) classifier applied to reflectance and vegetation index images, followed by a vectorization pipeline. Both methodologies performed exceptionally well, resulting in a mean area goodness of fit (AGoF) for the field areas greater than 98% and a mean boundary mean positional error (BMPE) lower than 0.8 m for the seven surveyed fields.
KW  - UAS
KW  - UAV
KW  - random forests
KW  - PBIA
KW  - GEOBIA
KW  - multispectral
KW  - optical
KW  - crop
DO  - 10.3390/rs12162640
TY  - EJOU
AU  - Zhang, Shiyu
AU  - Zhuo, Li
AU  - Zhang, Hui
AU  - Li, Jiafeng
TI  - Object Tracking in Unmanned Aerial Vehicle Videos via Multifeature Discrimination and Instance-Aware Attention Network
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 16
SN  - 2072-4292

AB  - Visual object tracking in unmanned aerial vehicle (UAV) videos plays an important role in a variety of fields, such as traffic data collection, traffic monitoring, as well as film and television shooting. However, it is still challenging to track the target robustly in UAV vision task due to several factors such as appearance variation, background clutter, and severe occlusion. In this paper, we propose a novel two-stage UAV tracking framework, which includes a target detection stage based on multifeature discrimination and a bounding-box estimation stage based on the instance-aware attention network. In the target detection stage, we explore a feature representation scheme for a small target that integrates handcrafted features, low-level deep features, and high-level deep features. Then, the correlation filter is used to roughly predict target location. In the bounding-box estimation stage, an instance-aware intersection over union (IoU)-Net is integrated together with an instance-aware attention network to estimate the target size based on the bounding-box proposals generated in the target detection stage. Extensive experimental results on the UAV123 and UAVDT datasets show that our tracker, running at over 25 frames per second (FPS), has superior performance as compared with state-of-the-art UAV visual tracking approaches.
KW  - visual tracking
KW  - two-stage framework
KW  - instance-aware network
KW  - unmanned aerial vehicle (UAV) videos
DO  - 10.3390/rs12162646
TY  - EJOU
AU  - Adamopoulos, Efstathios
AU  - Rinaudo, Fulvio
TI  - UAS-Based Archaeological Remote Sensing: Review, Meta-Analysis and State-of-the-Art
T2  - Drones

PY  - 2020
VL  - 4
IS  - 3
SN  - 2504-446X

AB  - Over the last decade, we have witnessed momentous technological developments in unmanned aircraft systems (UAS) and in lightweight sensors operating at various wavelengths, at and beyond the visible spectrum, which can be integrated with unmanned aerial platforms. These innovations have made feasible close-range and high-resolution remote sensing for numerous archaeological applications, including documentation, prospection, and monitoring bridging the gap between satellite, high-altitude airborne, and terrestrial sensing of historical sites and landscapes. In this article, we track the progress made so far, by systematically reviewing the literature relevant to the combined use of UAS platforms with visible, infrared, multi-spectral, hyper-spectral, laser, and radar sensors to reveal archaeological features otherwise invisible to archaeologists with applied non-destructive techniques. We review, specific applications and their global distribution, as well as commonly used platforms, sensors, and data-processing workflows. Furthermore, we identify the contemporary state-of-the-art and discuss the challenges that have already been overcome, and those that have not, to propose suggestions for future research.
KW  - UAS
KW  - lightweight sensors
KW  - near-infrared sensors
KW  - thermal sensors
KW  - multi-spectral sensors
KW  - hyperspectral sensors
KW  - LiDAR
KW  - remote sensing
KW  - archaeology
KW  - prospection
DO  - 10.3390/drones4030046
TY  - EJOU
AU  - Wang, Bin
AU  - Gu, Yinjuan
TI  - An Improved FBPN-Based Detection Network for Vehicles in Aerial Images
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 17
SN  - 1424-8220

AB  - With the development of artificial intelligence and big data analytics, an increasing number of researchers have tried to use deep-learning technology to train neural networks and achieved great success in the field of vehicle detection. However, as a special domain of object detection, vehicle detection in aerial images still has made limited progress because of low resolution, complex backgrounds and rotating objects. In this paper, an improved feature-balanced pyramid network (FBPN) has been proposed to enhance the network&rsquo;s ability to detect small objects. By combining FBPN with modified faster region convolutional neural network (faster-RCNN), a vehicle detection framework for aerial images is proposed. The focal loss function is adopted in the proposed framework to reduce the imbalance between easy and hard samples. The experimental results based on the VEDIA, USCAS-AOD, and DOTA datasets show that the proposed framework outperforms other state-of-the-art vehicle detection algorithms for aerial images.
KW  - vehicle detection
KW  - aerial images
KW  - Feature Balanced Pyramid Network
DO  - 10.3390/s20174709
TY  - EJOU
AU  - Yang, Jianxiu
AU  - Xie, Xuemei
AU  - Shi, Guangming
AU  - Yang, Wenzhe
TI  - A Feature-Enhanced Anchor-Free Network for UAV Vehicle Detection
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 17
SN  - 2072-4292

AB  - Vehicle detection based on unmanned aerial vehicle (UAV) images is a challenging task. One reason is that the objects are small size, low-resolution, and large scale variations, resulting in weak feature representation. Another reason is the imbalance between positive and negative examples. In this paper, we propose a novel architecture for UAV vehicle detection to solve above problems. In detail, we use anchor-free mechanism to eliminate predefined anchors, which can reduce complicated computation and relieve the imbalance between positive and negative samples. Meanwhile, to enhance the features for vehicles, we design a multi-scale semantic enhancement block (MSEB) and an effective 49-layer backbone which is based on the DetNet59. The proposed network offers appropriate receptive fields that match the small-sized vehicles, and involves precise localization information provided by the contexts with high resolution. The MSEB strengthens discriminative feature representation at various scales, without reducing the spatial resolution of prediction layers. Experiments show that the proposed method achieves the state-of-the-art performance. Particularly, the main part of vehicles, much smaller ones, the accuracy is about 2% higher than other existing methods.
KW  - feature-enhanced
KW  - anchor-free network
KW  - multi-scale
KW  - unmanned aerial vehicle
KW  - object detection
DO  - 10.3390/rs12172729
TY  - EJOU
AU  - Abdulridha, Jaafar
AU  - Ampatzidis, Yiannis
AU  - Qureshi, Jawwad
AU  - Roberts, Pamela
TI  - Laboratory and UAV-Based Identification and Classification of Tomato Yellow Leaf Curl, Bacterial Spot, and Target Spot Diseases in Tomato Utilizing Hyperspectral Imaging and Machine Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 17
SN  - 2072-4292

AB  - Tomato crops are susceptible to multiple diseases, several of which may be present during the same season. Therefore, rapid disease identification could enhance crop management consequently increasing the yield. In this study, nondestructive methods were developed to detect diseases that affect tomato crops, such as bacterial spot (BS), target spot (TS), and tomato yellow leaf curl (TYLC) for two varieties of tomato (susceptible and tolerant to TYLC only) by using hyperspectral sensing in two conditions: a) laboratory (benchtop scanning), and b) in field using an unmanned aerial vehicle (UAV-based). The stepwise discriminant analysis (STDA) and the radial basis function were applied to classify the infected plants and distinguish them from noninfected or healthy (H) plants. Multiple vegetation indices (VIs) and the M statistic method were utilized to distinguish and classify the diseased plants. In general, the classification results between healthy and diseased plants were highly accurate for all diseases; for instance, when comparing H vs. BS, TS, and TYLC in the asymptomatic stage and laboratory conditions, the classification rates were 94%, 95%, and 100%, respectively. Similarly, in the symptomatic stage, the classification rates between healthy and infected plants were 98% for BS, and 99&ndash;100% for TS and TYLC diseases. The classification results in the field conditions also showed high values of 98%, 96%, and 100%, for BS, TS, and TYLC, respectively. The VIs that could best identify these diseases were the renormalized difference vegetation index (RDVI), and the modified triangular vegetation index 1 (MTVI 1) in both laboratory and field. The results were promising and suggest the possibility to identify these diseases using remote sensing.
KW  - hyperspectral
KW  - artificial intelligence
KW  - spectral analysis
KW  - UAV
KW  - disease detection
KW  - classification
KW  - machine learning
DO  - 10.3390/rs12172732
TY  - EJOU
AU  - Devadoss, Jashvina
AU  - Falco, Nicola
AU  - Dafflon, Baptiste
AU  - Wu, Yuxin
AU  - Franklin, Maya
AU  - Hermes, Anna
AU  - Hinckley, Eve-Lyn S.
AU  - Wainwright, Haruko
TI  - Remote Sensing-Informed Zonation for Understanding Snow, Plant and Soil Moisture Dynamics within a Mountain Ecosystem
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 17
SN  - 2072-4292

AB  - In the headwater catchments of the Rocky Mountains, plant productivity and its dynamics are largely dependent upon water availability, which is influenced by changing snowmelt dynamics associated with climate change. Understanding and quantifying the interactions between snow, plants and soil moisture is challenging, since these interactions are highly heterogeneous in mountainous terrain, particularly as they are influenced by microtopography within a hillslope. Recent advances in satellite remote sensing have created an opportunity for monitoring snow and plant dynamics at high spatiotemporal resolutions that can capture microtopographic effects. In this study, we investigate the relationships among topography, snowmelt, soil moisture and plant dynamics in the East River watershed, Crested Butte, Colorado, based on a time series of 3-meter resolution PlanetScope normalized difference vegetation index (NDVI) images. To make use of a large volume of high-resolution time-lapse images (17 images total), we use unsupervised machine learning methods to reduce the dimensionality of the time lapse images by identifying spatial zones that have characteristic NDVI time series. We hypothesize that each zone represents a set of similar snowmelt and plant dynamics that differ from other identified zones and that these zones are associated with key topographic features, plant species and soil moisture. We compare different distance measures (Ward and complete linkage) to understand the effects of their influence on the zonation map. Results show that the identified zones are associated with particular microtopographic features; highly productive zones are associated with low slopes and high topographic wetness index, in contrast with zones of low productivity, which are associated with high slopes and low topographic wetness index. The zones also correspond to particular plant species distributions; higher forb coverage is associated with zones characterized by higher peak productivity combined with rapid senescence in low moisture conditions, while higher sagebrush coverage is associated with low productivity and similar senescence patterns between high and low moisture conditions. In addition, soil moisture probe and sensor data confirm that each zone has a unique soil moisture distribution. This cluster-based analysis can tractably analyze high-resolution time-lapse images to examine plant-soil-snow interactions, guide sampling and sensor placements and identify areas likely vulnerable to ecological change in the future.
KW  - soil moisture
KW  - plant productivity
KW  - water availability
KW  - spatiotemporal dynamics
KW  - NDVI
KW  - water-limited ecosystem
KW  - microtopography
KW  - unsupervised machine learning
DO  - 10.3390/rs12172733
TY  - EJOU
AU  - Yu, Hang
AU  - Gong, Jiulu
AU  - Chen, Derong
TI  - Object Detection Using Multi-Scale Balanced Sampling
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 17
SN  - 2076-3417

AB  - Detecting small objects and objects with large scale variants are always challenging for deep learning based object detection approaches. Many efforts have been made to solve these problems such as adopting more effective network structures, image features, loss functions, etc. However, for both small objects detection and detecting objects with various scale in single image, the first thing should be solve is the matching mechanism between anchor boxes and ground-truths. In this paper, an approach based on multi-scale balanced sampling(MB-RPN) is proposed for the difficult matching of small objects and detecting multi-scale objects. According to the scale of the anchor boxes, different positive and negative sample IOU discriminate thresholds are adopted to improve the probability of matching the small object area with the anchor boxes so that more small object samples are included in the training process. Moreover, the balanced sampling method is proposed for the collected samples, the samples are further divided and uniform sampling to ensure the diversity of samples in training process. Several datasets are adopted to evaluate the MB-RPN, the experimental results show that compare with the similar approach, MB-RPN improves detection performances effectively.
KW  - object detection
KW  - small object
KW  - multi-scale sampling
KW  - balanced sampling
DO  - 10.3390/app10176053
TY  - EJOU
AU  - Arabameri, Alireza
AU  - Asadi Nalivan, Omid
AU  - Chandra Pal, Subodh
AU  - Chakrabortty, Rabin
AU  - Saha, Asish
AU  - Lee, Saro
AU  - Pradhan, Biswajeet
AU  - Tien Bui, Dieu
TI  - Novel Machine Learning Approaches for Modelling the Gully Erosion Susceptibility
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 17
SN  - 2072-4292

AB  - The extreme form of land degradation caused by the formation of gullies is a major challenge for the sustainability of land resources. This problem is more vulnerable in the arid and semi-arid environment and associated damage to agriculture and allied economic activities. Appropriate modeling of such erosion is therefore needed with optimum accuracy for estimating vulnerable regions and taking appropriate initiatives. The Golestan Dam has faced an acute problem of gully erosion over the last decade and has adversely affected society. Here, the artificial neural network (ANN), general linear model (GLM), maximum entropy (MaxEnt), and support vector machine (SVM) machine learning algorithm with 90/10, 80/20, 70/30, 60/40, and 50/50 random partitioning of training and validation samples was selected purposively for estimating the gully erosion susceptibility. The main objective of this work was to predict the susceptible zone with the maximum possible accuracy. For this purpose, random partitioning approaches were implemented. For this purpose, 20 gully erosion conditioning factors were considered for predicting the susceptible areas by considering the multi-collinearity test. The variance inflation factor (VIF) and tolerance (TOL) limit were considered for multi-collinearity assessment for reducing the error of the models and increase the efficiency of the outcome. The ANN with 50/50 random partitioning of the sample is the most optimal model in this analysis. The area under curve (AUC) values of receiver operating characteristics (ROC) in ANN (50/50) for the training and validation data are 0.918 and 0.868, respectively. The importance of the causative factors was estimated with the help of the Jackknife test, which reveals that the most important factor is the topography position index (TPI). Apart from this, the prioritization of all predicted models was estimated taking into account the training and validation data set, which should help future researchers to select models from this perspective. This type of outcome should help planners and local stakeholders to implement appropriate land and water conservation measures.
KW  - land degradation
KW  - gully erosion
KW  - random partitioning approaches
KW  - machine learning algorithm
KW  - jackknife test
DO  - 10.3390/rs12172833
TY  - EJOU
AU  - Liu, Jiantao
AU  - Feng, Quanlong
AU  - Wang, Ying
AU  - Batsaikhan, Bayartungalag
AU  - Gong, Jianhua
AU  - Li, Yi
AU  - Liu, Chunting
AU  - Ma, Yin
TI  - Urban Green Plastic Cover Mapping Based on VHR Remote Sensing Images and a Deep Semi-Supervised Learning Framework
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 9
SN  - 2220-9964

AB  - With the rapid process of both urban sprawl and urban renewal, large numbers of old buildings have been demolished in China, leading to wide spread construction sites, which could cause severe dust contamination. To alleviate the accompanied dust pollution, green plastic mulch has been widely used by local governments of China. Therefore, timely and accurate mapping of urban green plastic covered regions is of great significance to both urban environmental management and the understanding of urban growth status. However, the complex spatial patterns of the urban landscape make it challenging to accurately identify these areas of green plastic cover. To tackle this issue, we propose a deep semi-supervised learning framework for green plastic cover mapping using very high resolution (VHR) remote sensing imagery. Specifically, a multi-scale deformable convolution neural network (CNN) was exploited to learn representative and discriminative features under complex urban landscapes. Afterwards, a semi-supervised learning strategy was proposed to integrate the limited labeled data and massive unlabeled data for model co-training. Experimental results indicate that the proposed method could accurately identify green plastic-covered regions in Jinan with an overall accuracy (OA) of 91.63%. An ablation study indicated that, compared with supervised learning, the semi-supervised learning strategy in this study could increase the OA by 6.38%. Moreover, the multi-scale deformable CNN outperforms several classic CNN models in the computer vision field. The proposed method is the first attempt to map urban green plastic-covered regions based on deep learning, which could serve as a baseline and useful reference for future research.
KW  - green plastic cover
KW  - semi-supervised learning
KW  - deep learning
KW  - urban land cover mapping
DO  - 10.3390/ijgi9090527
TY  - EJOU
AU  - Meng , Kaitao
AU  - Li , Deshi
AU  - He , Xiaofan
AU  - Liu , Mingliu
AU  - Song , Weitao
TI  - Real-Time Compact Environment Representation for UAV Navigation
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 17
SN  - 1424-8220

AB  - Recently, unmanned aerial vehicles (UAVs) have attracted much attention due to their on-demand deployment, high mobility, and low cost. For UAVs navigating in an unknown environment, efficient environment representation is needed due to the storage limitation of the UAVs. Nonetheless, building an accurate and compact environment representation model is highly non-trivial because of the unknown shape of the obstacles and the time-consuming operations such as finding and eliminating the environmental details. To overcome these challenges, a novel vertical strip extraction algorithm is proposed to analyze the probability density function characteristics of the normalized disparity value and segment the obstacles through an adaptive size sliding window. In addition, a plane adjustment algorithm is proposed to represent the obstacle surfaces as polygonal prism profiles while minimizing the redundant obstacle information. By combining these two proposed algorithms, the depth sensor data can be converted into the multi-layer polygonal prism models in real time. Besides, a drone platform equipped with a depth sensor is developed to build the compact environment representation models in the real world. Experimental results demonstrate that the proposed scheme achieves better performance in terms of precision and storage as compared to the baseline.
KW  - unmanned aerial vehicle
KW  - obstacle sensing
KW  - compact environment representation
KW  - kernel density estimation
DO  - 10.3390/s20174976
TY  - EJOU
AU  - Ren, Yongfeng
AU  - Yu, Yongtao
AU  - Guan, Haiyan
TI  - DA-CapsUNet: A Dual-Attention Capsule U-Net for Road Extraction from Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - The up-to-date and information-accurate road database plays a significant role in many applications. Recently, with the improvement in image resolutions and quality, remote sensing images have provided an important data source for road extraction tasks. However, due to the topology variations, spectral diversities, and complex scenarios, it is still challenging to realize fully automated and highly accurate road extractions from remote sensing images. This paper proposes a novel dual-attention capsule U-Net (DA-CapsUNet) for road region extraction by combining the advantageous properties of capsule representations and the powerful features of attention mechanisms. By constructing a capsule U-Net architecture, the DA-CapsUNet can extract and fuse multiscale capsule features to recover a high-resolution and semantically strong feature representation. By designing the multiscale context-augmentation and two types of feature attention modules, the DA-CapsUNet can exploit multiscale contextual properties at a high-resolution perspective and generate an informative and class-specific feature encoding. Quantitative evaluations on a large dataset showed that the DA-CapsUNet provides a competitive road extraction performance with a precision of 0.9523, a recall of 0.9486, and an F-score of 0.9504, respectively. Comparative studies with eight recently developed deep learning methods also confirmed the applicability and superiority or compatibility of the DA-CapsUNet in road extraction tasks.
KW  - road extraction
KW  - capsule network
KW  - capsule U-Net
KW  - capsule attention network
KW  - remote sensing imagery
KW  - deep learning
DO  - 10.3390/rs12182866
