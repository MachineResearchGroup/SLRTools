TY  - EJOU
AU  - Hung, Calvin
AU  - Xu, Zhe
AU  - Sukkarieh, Salah
TI  - Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV
T2  - Remote Sensing

PY  - 2014
VL  - 6
IS  - 12
SN  - 2072-4292

AB  - The development of low-cost unmanned aerial vehicles (UAVs) and light weight imaging sensors has resulted in significant interest in their use for remote sensing applications. While significant attention has been paid to the collection, calibration, registration and mosaicking of data collected from small UAVs, the interpretation of these data into semantically meaningful information can still be a laborious task. A standard data collection and classification work-flow requires significant manual effort for segment size tuning, feature selection and rule-based classifier design. In this paper, we propose an alternative learning-based approach using feature learning to minimise the manual effort required. We apply this system to the classification of invasive weed species. Small UAVs are suited to this application, as they can collect data at high spatial resolutions, which is essential for the classification of small or localised weed outbreaks. In this paper, we apply feature learning to generate a bank of image filters that allows for the extraction of features that discriminate between the weeds of interest and background objects. These features are pooled to summarise the image statistics and form the input to a texton-based linear classifier that classifies an image patch as weed or background. We evaluated our approach to weed classification on three weeds of significance in Australia: water hyacinth, tropical soda apple and serrated tussock. Our results showed that collecting images at 5–10 m resulted in the highest classifier accuracy, indicated by F1 scores of up to 94%.
KW  - weed classification
KW  - UAV remote sensing
KW  - serrated tussock
KW  - tropical soda apple
KW  - water hyacinth
DO  - 10.3390/rs61212037
TY  - EJOU
AU  - Gökçe, Fatih
AU  - Üçoluk, Göktürk
AU  - Şahin, Erol
AU  - Kalkan, Sinan
TI  - Vision-Based Detection and Distance Estimation of Micro Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2015
VL  - 15
IS  - 9
SN  - 1424-8220

AB  - Detection and distance estimation of micro unmanned aerial vehicles (mUAVs) is crucial for (i) the detection of intruder mUAVs in protected environments; (ii) sense and avoid purposes on mUAVs or on other aerial vehicles and (iii) multi-mUAV control scenarios, such as environmental monitoring, surveillance and exploration. In this article, we evaluate vision algorithms as alternatives for detection and distance estimation of mUAVs, since other sensing modalities entail certain limitations on the environment or on the distance. For this purpose, we test Haar-like features, histogram of gradients (HOG) and local binary patterns (LBP) using cascades of boosted classifiers. Cascaded boosted classifiers allow fast processing by performing detection tests at multiple stages, where only candidates passing earlier simple stages are processed at the preceding more complex stages. We also integrate a distance estimation method with our system utilizing geometric cues with support vector regressors. We evaluated each method on indoor and outdoor videos that are collected in a systematic way and also on videos having motion blur. Our experiments show that, using boosted cascaded classifiers with LBP, near real-time detection and distance estimation of mUAVs are possible in about 60 ms indoors (1032 × 778 resolution) and 150 ms outdoors (1280 × 720 resolution) per frame, with a detection rate of 0.96 F-score. However, the cascaded classifiers using Haar-like features lead to better distance estimation since they can position the bounding boxes on mUAVs more accurately. On the other hand, our time analysis yields that the cascaded classifiers using HOG train and run faster than the other algorithms.
KW  - UAV
KW  - micro UAV
KW  - vision
KW  - detection
KW  - distance estimation
KW  - cascaded classifiers
DO  - 10.3390/s150923805
TY  - EJOU
AU  - Kim, Sungho
AU  - Song, Woo-Jin
AU  - Kim, So-Hyun
TI  - Robust Ground Target Detection by SAR and IR Sensor Fusion Using Adaboost-Based Feature Selection
T2  - Sensors

PY  - 2016
VL  - 16
IS  - 7
SN  - 1424-8220

AB  - Long-range ground targets are difficult to detect in a noisy cluttered environment using either synthetic aperture radar (SAR) images or infrared (IR) images. SAR-based detectors can provide a high detection rate with a high false alarm rate to background scatter noise. IR-based approaches can detect hot targets but are affected strongly by the weather conditions. This paper proposes a novel target detection method by decision-level SAR and IR fusion using an Adaboost-based machine learning scheme to achieve a high detection rate and low false alarm rate. The proposed method consists of individual detection, registration, and fusion architecture. This paper presents a single framework of a SAR and IR target detection method using modified Boolean map visual theory (modBMVT) and feature-selection based fusion. Previous methods applied different algorithms to detect SAR and IR targets because of the different physical image characteristics. One method that is optimized for IR target detection produces unsuccessful results in SAR target detection. This study examined the image characteristics and proposed a unified SAR and IR target detection method by inserting a median local average filter (MLAF, pre-filter) and an asymmetric morphological closing filter (AMCF, post-filter) into the BMVT. The original BMVT was optimized to detect small infrared targets. The proposed modBMVT can remove the thermal and scatter noise by the MLAF and detect extended targets by attaching the AMCF after the BMVT. Heterogeneous SAR and IR images were registered automatically using the proposed RANdom SAmple Region Consensus (RANSARC)-based homography optimization after a brute-force correspondence search using the detected target centers and regions. The final targets were detected by feature-selection based sensor fusion using Adaboost. The proposed method showed good SAR and IR target detection performance through feature selection-based decision fusion on a synthetic database generated by OKTAL-SE.
KW  - synthetic aperture radar
KW  - infrared
KW  - target detection
KW  - sensor fusion
KW  - machine learning
KW  - feature selection
KW  - OKTAL-SE
DO  - 10.3390/s16071117
TY  - EJOU
AU  - Li, Weijia
AU  - Fu, Haohuan
AU  - Yu, Le
AU  - Cracknell, Arthur
TI  - Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 1
SN  - 2072-4292

AB  - Oil palm trees are important economic crops in Malaysia and other tropical areas. The number of oil palm trees in a plantation area is important information for predicting the yield of palm oil, monitoring the growing situation of palm trees and maximizing their productivity, etc. In this paper, we propose a deep learning based framework for oil palm tree detection and counting using high-resolution remote sensing images for Malaysia. Unlike previous palm tree detection studies, the trees in our study area are more crowded and their crowns often overlap. We use a number of manually interpreted samples to train and optimize the convolutional neural network (CNN), and predict labels for all the samples in an image dataset collected through the sliding window technique. Then, we merge the predicted palm coordinates corresponding to the same palm tree into one palm coordinate and obtain the final palm tree detection results. Based on our proposed method, more than 96% of the oil palm trees in our study area can be detected correctly when compared with the manually interpreted ground truth, and this is higher than the accuracies of the other three tree detection methods used in this study.
KW  - oil palm trees
KW  - deep learning
KW  - convolutional neural network (CNN)
KW  - object detection
DO  - 10.3390/rs9010022
TY  - EJOU
AU  - Tang, Tianyu
AU  - Zhou, Shilin
AU  - Deng, Zhipeng
AU  - Zou, Huanxin
AU  - Lei, Lin
TI  - Vehicle Detection in Aerial Images Based on Region Convolutional Neural Networks and Hard Negative Example Mining
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 2
SN  - 1424-8220

AB  - Detecting vehicles in aerial imagery plays an important role in a wide range of applications. The current vehicle detection methods are mostly based on sliding-window search and handcrafted or shallow-learning-based features, having limited description capability and heavy computational costs. Recently, due to the powerful feature representations, region convolutional neural networks (CNN) based detection methods have achieved state-of-the-art performance in computer vision, especially Faster R-CNN. However, directly using it for vehicle detection in aerial images has many limitations: (1) region proposal network (RPN) in Faster R-CNN has poor performance for accurately locating small-sized vehicles, due to the relatively coarse feature maps; and (2) the classifier after RPN cannot distinguish vehicles and complex backgrounds well. In this study, an improved detection method based on Faster R-CNN is proposed in order to accomplish the two challenges mentioned above. Firstly, to improve the recall, we employ a hyper region proposal network (HRPN) to extract vehicle-like targets with a combination of hierarchical feature maps. Then, we replace the classifier after RPN by a cascade of boosted classifiers to verify the candidate regions, aiming at reducing false detection by negative example mining. We evaluate our method on the Munich vehicle dataset and the collected vehicle dataset, with improvements in accuracy and robustness compared to existing methods.
KW  - vehicle detection
KW  - hyper region proposal network
KW  - convolutional neural networks
KW  - hard negative example mining
DO  - 10.3390/s17020336
TY  - EJOU
AU  - Ammour, Nassim
AU  - Alhichri, Haikel
AU  - Bazi, Yakoub
AU  - Benjdira, Bilel
AU  - Alajlan, Naif
AU  - Zuair, Mansour
TI  - Deep Learning Approach for Car Detection in UAV Imagery
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 4
SN  - 2072-4292

AB  - This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.
KW  - UAV imagery
KW  - car counting
KW  - deep learning
KW  - convolutional neural networks (CNNs)
KW  - support vector machines (SVM)
KW  - mean-shift segmentation
DO  - 10.3390/rs9040312
TY  - EJOU
AU  - Liu, Weifeng
AU  - Zhang, Zhenqing
AU  - Li, Shuying
AU  - Tao, Dapeng
TI  - Road Detection by Using a Generalized  Hough Transform
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 6
SN  - 2072-4292

AB  - Road detection plays key roles for remote sensing image analytics. Hough transform (HT) is one very typical method for road detection, especially for straight line road detection. Although many variants of Hough transform have been reported, it is still a great challenge to develop a low computational complexity and time-saving Hough transform algorithm. In this paper, we propose a generalized Hough transform (i.e., Radon transform) implementation for road detection in remote sensing images. Specifically, we present a dictionary learning method to approximate the Radon transform. The proposed approximation method treats a Radon transform as a linear transform, which then facilitates parallel implementation of the Radon transform for multiple images. To evaluate the proposed algorithm, we conduct extensive experiments on the popular RSSCN7 database for straight road detection. The experimental results demonstrate that our method is superior to the traditional algorithms in terms of accuracy and computing complexity.
KW  - Hough transform
KW  - dictionary learning
KW  - road detection
KW  - Radon transform
DO  - 10.3390/rs9060590
TY  - EJOU
AU  - Ampatzidis, Yiannis
AU  - De Bellis, Luigi
AU  - Luvisi, Andrea
TI  - iPathology: Robotic Applications and Management of Plants and Plant Diseases
T2  - Sustainability

PY  - 2017
VL  - 9
IS  - 6
SN  - 2071-1050

AB  - The rapid development of new technologies and the changing landscape of the online world (e.g., Internet of Things (IoT), Internet of All, cloud-based solutions) provide a unique opportunity for developing automated and robotic systems for urban farming, agriculture, and forestry. Technological advances in machine vision, global positioning systems, laser technologies, actuators, and mechatronics have enabled the development and implementation of robotic systems and intelligent technologies for precision agriculture. Herein, we present and review robotic applications on plant pathology and management, and emerging agricultural technologies for intra-urban agriculture. Greenhouse advanced management systems and technologies have been greatly developed in the last years, integrating IoT and WSN (Wireless Sensor Network). Machine learning, machine vision, and AI (Artificial Intelligence) have been utilized and applied in agriculture for automated and robotic farming. Intelligence technologies, using machine vision/learning, have been developed not only for planting, irrigation, weeding (to some extent), pruning, and harvesting, but also for plant disease detection and identification. However, plant disease detection still represents an intriguing challenge, for both abiotic and biotic stress. Many recognition methods and technologies for identifying plant disease symptoms have been successfully developed; still, the majority of them require a controlled environment for data acquisition to avoid false positives. Machine learning methods (e.g., deep and transfer learning) present promising results for improving image processing and plant symptom identification. Nevertheless, diagnostic specificity is a challenge for microorganism control and should drive the development of mechatronics and robotic solutions for disease management.
KW  - machine vision
KW  - machine learning
KW  - vertical farming systems
KW  - mechatronics
KW  - smart machines
KW  - smart city
DO  - 10.3390/su9061010
TY  - EJOU
AU  - Yang, Yurong
AU  - Gong, Huajun
AU  - Wang, Xinhua
AU  - Sun, Peng
TI  - Aerial Target Tracking Algorithm Based on Faster R-CNN Combined with Frame Differencing
T2  - Aerospace

PY  - 2017
VL  - 4
IS  - 2
SN  - 2226-4310

AB  - We propose a robust approach to detecting and tracking moving objects for a naval unmanned aircraft system (UAS) landing on an aircraft carrier. The frame difference algorithm follows a simple principle to achieve real-time tracking, whereas Faster Region-Convolutional Neural Network (R-CNN) performs highly precise detection and tracking characteristics. We thus combine Faster R-CNN with the frame difference method, which is demonstrated to exhibit robust and real-time detection and tracking performance. In our UAS landing experiments, two cameras placed on both sides of the runway are used to capture the moving UAS. When the UAS is captured, the joint algorithm uses frame difference to detect the moving target (UAS). As soon as the Faster R-CNN algorithm accurately detects the UAS, the detection priority is given to Faster R-CNN. In this manner, we also perform motion segmentation and object detection in the presence of changes in the environment, such as illumination variation or “walking persons”. By combining the 2 algorithms we can accurately detect and track objects with a tracking accuracy rate of up to 99% and a frame per second of up to 40 Hz. Thus, a solid foundation is laid for subsequent landing guidance.
KW  - deep learning
KW  - Faster R-CNN
KW  - UAS landing
KW  - object detection
DO  - 10.3390/aerospace4020032
TY  - EJOU
AU  - Nguyen, Phong Ha
AU  - Kim, Ki Wan
AU  - Lee, Young Won
AU  - Park, Kang Ryoung
TI  - Remote Marker-Based Tracking for UAV Landing Using Visible-Light Camera Sensor
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 9
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs), which are commonly known as drones, have proved to be useful not only on the battlefields where manned flight is considered too risky or difficult, but also in everyday life purposes such as surveillance, monitoring, rescue, unmanned cargo, aerial video, and photography. More advanced drones make use of global positioning system (GPS) receivers during the navigation and control loop which allows for smart GPS features of drone navigation. However, there are problems if the drones operate in heterogeneous areas with no GPS signal, so it is important to perform research into the development of UAVs with autonomous navigation and landing guidance using computer vision. In this research, we determined how to safely land a drone in the absence of GPS signals using our remote maker-based tracking algorithm based on the visible light camera sensor. The proposed method uses a unique marker designed as a tracking target during landing procedures. Experimental results show that our method significantly outperforms state-of-the-art object trackers in terms of both accuracy and processing time, and we perform test on an embedded system in various environments.
KW  - unmanned aerial vehicle (UAV)
KW  - remote marker-based tracking
KW  - visible light camera sensor
KW  - UAV landing
DO  - 10.3390/s17091987
TY  - EJOU
AU  - Kim, Hyunjun
AU  - Lee, Junhwa
AU  - Ahn, Eunjong
AU  - Cho, Soojin
AU  - Shin, Myoungsu
AU  - Sim, Sung-Han
TI  - Concrete Crack Identification Using a UAV Incorporating Hybrid Image Processing
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 9
SN  - 1424-8220

AB  - Crack assessment is an essential process in the maintenance of concrete structures. In general, concrete cracks are inspected by manual visual observation of the surface, which is intrinsically subjective as it depends on the experience of inspectors. Further, it is time-consuming, expensive, and often unsafe when inaccessible structural members are to be assessed. Unmanned aerial vehicle (UAV) technologies combined with digital image processing have recently been applied to crack assessment to overcome the drawbacks of manual visual inspection. However, identification of crack information in terms of width and length has not been fully explored in the UAV-based applications, because of the absence of distance measurement and tailored image processing. This paper presents a crack identification strategy that combines hybrid image processing with UAV technology. Equipped with a camera, an ultrasonic displacement sensor, and a WiFi module, the system provides the image of cracks and the associated working distance from a target structure on demand. The obtained information is subsequently processed by hybrid image binarization to estimate the crack width accurately while minimizing the loss of the crack length information. The proposed system has shown to successfully measure cracks thicker than 0.1 mm with the maximum length estimation error of 7.3%.
KW  - concrete structure
KW  - crack identification
KW  - digital image processing
KW  - structural health monitoring
KW  - unmanned aerial vehicle
DO  - 10.3390/s17092052
TY  - EJOU
AU  - Tian, Zhe
AU  - Liu, Fushun
AU  - Li, Zhixiong
AU  - Malekian, Reza
AU  - Xie, Yingchun
TI  - The Development of Key Technologies in Applications of Vessels Connected to the Internet
T2  - Symmetry

PY  - 2017
VL  - 9
IS  - 10
SN  - 2073-8994

AB  - With the development of science and technology, traffic perception, communication, information processing, artificial intelligence and the shipping information system have become important in supporting the realization of intelligent shipping transportation. Against this background, the Internet of Vessels (IoV) is proposed to integrate all these advanced technologies into a platform to meet the requirements of international and regional transportations. The purpose of this paper is to analyze how to benefit from the Internet of Vessels to improve the efficiency and safety of shipping, and promote the development of world transportation. In this paper, the IoV is introduced and its main architectures are outlined. Furthermore, the characteristics of the Internet of Vessels are described. Several important applications that illustrate the interaction of the Internet of Vessels’ components are proposed. Due to the development of the Internet of Vessels still being in its primary stage, challenges and prospects are identified and addressed. Finally, the main conclusions are drawn and future research priorities are provided for reference and as professional suggestions for future researchers in this field.
KW  - ship intelligence
KW  - big data
KW  - internet of vessels
KW  - information fusion
DO  - 10.3390/sym9100211
TY  - EJOU
AU  - Liu, Yuan
AU  - Wang, Jun
AU  - Song, Jingwei
AU  - Song, Zihui
TI  - Globally Consistent Indoor Mapping via a Decoupling Rotation and Translation Algorithm Applied to RGB-D Camera Output
T2  - ISPRS International Journal of Geo-Information

PY  - 2017
VL  - 6
IS  - 11
SN  - 2220-9964

AB  - This paper presents a novel RGB-D 3D reconstruction algorithm for the indoor environment. The method can produce globally-consistent 3D maps for potential GIS applications. As the consumer RGB-D camera provides a noisy depth image, the proposed algorithm decouples the rotation and translation for a more robust camera pose estimation, which makes full use of the information, but also prevents inaccuracies caused by noisy depth measurements. The uncertainty in the image depth is not only related to the camera device, but also the environment; hence, a novel uncertainty model for depth measurements was developed using Gaussian mixture applied to multi-windows. The plane features in the indoor environment contain valuable information about the global structure, which can guide the convergence of camera pose solutions, and plane and feature point constraints are incorporated in the proposed optimization framework. The proposed method was validated using publicly-available RGB-D benchmarks and obtained good quality trajectory and 3D models, which are difficult for traditional 3D reconstruction algorithms.
KW  - RGB-D
KW  - 3D map
KW  - indoor mapping
KW  - SLAM
KW  - 3D reconstruction
DO  - 10.3390/ijgi6110323
TY  - EJOU
AU  - Yamamoto, Kyosuke
AU  - Togami, Takashi
AU  - Yamaguchi, Norio
TI  - Super-Resolution of Plant Disease Images for the Acceleration of Image-based Phenotyping and Vigor Diagnosis in Agriculture
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs or drones) are a very promising branch of technology, and they have been utilized in agriculture—in cooperation with image processing technologies—for phenotyping and vigor diagnosis. One of the problems in the utilization of UAVs for agricultural purposes is the limitation in flight time. It is necessary to fly at a high altitude to capture the maximum number of plants in the limited time available, but this reduces the spatial resolution of the captured images. In this study, we applied a super-resolution method to the low-resolution images of tomato diseases to recover detailed appearances, such as lesions on plant organs. We also conducted disease classification using high-resolution, low-resolution, and super-resolution images to evaluate the effectiveness of super-resolution methods in disease classification. Our results indicated that the super-resolution method outperformed conventional image scaling methods in spatial resolution enhancement of tomato disease images. The results of disease classification showed that the accuracy attained was also better by a large margin with super-resolution images than with low-resolution images. These results indicated that our approach not only recovered the information lost in low-resolution images, but also exerted a beneficial influence on further image analysis. The proposed approach will accelerate image-based phenotyping and vigor diagnosis in the field, because it not only saves time to capture images of a crop in a cultivation field but also secures the accuracy of these images for further analysis.
KW  - super-resolution
KW  - deep learning
KW  - convolutional neural network
KW  - disease classification
KW  - agriculture
DO  - 10.3390/s17112557
TY  - EJOU
AU  - Tang, Tianyu
AU  - Zhou, Shilin
AU  - Deng, Zhipeng
AU  - Lei, Lin
AU  - Zou, Huanxin
TI  - Arbitrary-Oriented Vehicle Detection in Aerial Imagery with Single Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 11
SN  - 2072-4292

AB  - Vehicle detection with orientation estimation in aerial images has received widespread interest as it is important for intelligent traffic management. This is a challenging task, not only because of the complex background and relatively small size of the target, but also the various orientations of vehicles in aerial images captured from the top view. The existing methods for oriented vehicle detection need several post-processing steps to generate final detection results with orientation, which are not efficient enough. Moreover, they can only get discrete orientation information for each target. In this paper, we present an end-to-end single convolutional neural network to generate arbitrarily-oriented detection results directly. Our approach, named Oriented_SSD (Single Shot MultiBox Detector, SSD), uses a set of default boxes with various scales on each feature map location to produce detection bounding boxes. Meanwhile, offsets are predicted for each default box to better match the object shape, which contain the angle parameter for oriented bounding boxes’ generation. Evaluation results on the public DLR Vehicle Aerial dataset and Vehicle Detection in Aerial Imagery (VEDAI) dataset demonstrate that our method can detect both the location and orientation of the vehicle with high accuracy and fast speed. For test images in the DLR Vehicle Aerial dataset with a size of     5616 × 3744    , our method achieves 76.1% average precision (AP) and 78.7% correct direction classification at 5.17 s on an NVIDIA GTX-1060.
KW  - arbitrary-oriented
KW  - vehicle detection
KW  - single convolutional neural networks (CNN)
KW  - aerial images
KW  - near-real-time
DO  - 10.3390/rs9111170
TY  - EJOU
AU  - Zhong, Jiandan
AU  - Lei, Tao
AU  - Yao, Guangle
TI  - Robust Vehicle Detection in Aerial Images Based on Cascaded Convolutional Neural Networks
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 12
SN  - 1424-8220

AB  - Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.
KW  - vehicle detection
KW  - convolutional neural network
KW  - aerial image
KW  - deep learning
DO  - 10.3390/s17122720
TY  - EJOU
AU  - Su, Jinya
AU  - Yi, Dewei
AU  - Liu, Cunjia
AU  - Guo, Lei
AU  - Chen, Wen-Hua
TI  - Dimension Reduction Aided Hyperspectral Image Classification with a Small-sized Training Dataset: Experimental Comparisons
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 12
SN  - 1424-8220

AB  - Hyperspectral images (HSI) provide rich information which may not be captured by other sensing technologies and therefore gradually find a wide range of applications. However, they also generate a large amount of irrelevant or redundant data for a specific task. This causes a number of issues including significantly increased computation time, complexity and scale of prediction models mapping the data to semantics (e.g., classification), and the need of a large amount of labelled data for training. Particularly, it is generally difficult and expensive for experts to acquire sufficient training samples in many applications. This paper addresses these issues by exploring a number of classical dimension reduction algorithms in machine learning communities for HSI classification. To reduce the size of training dataset, feature selection (e.g., mutual information, minimal redundancy maximal relevance) and feature extraction (e.g., Principal Component Analysis (PCA), Kernel PCA) are adopted to augment a baseline classification method, Support Vector Machine (SVM). The proposed algorithms are evaluated using a real HSI dataset. It is shown that PCA yields the most promising performance in reducing the number of features or spectral bands. It is observed that while significantly reducing the computational complexity, the proposed method can achieve better classification results over the classic SVM on a small training dataset, which makes it suitable for real-time applications or when only limited training data are available. Furthermore, it can also achieve performances similar to the classic SVM on large datasets but with much less computing time.
KW  - feature extraction/selection
KW  - image classification
KW  - Hyperspectral image
KW  - PCA
KW  - SVM
DO  - 10.3390/s17122726
TY  - EJOU
AU  - Chen, Suting
AU  - Li, Xin
AU  - Zhang, Yanyan
AU  - Feng, Rui
AU  - Zhang, Chuang
TI  - Local Deep Hashing Matching of Aerial Images Based on Relative Distance and Absolute Distance Constraints
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 12
SN  - 2072-4292

AB  - Aerial images have features of high resolution, complex background, and usually require large amounts of calculation, however, most algorithms used in matching of aerial images adopt the shallow hand-crafted features expressed as floating-point descriptors (e.g., SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features)), which may suffer from poor matching speed and are not well represented in the literature. Here, we propose a novel Local Deep Hashing Matching (LDHM) method for matching of aerial images with large size and with lower complexity or fast matching speed. The basic idea of the proposed algorithm is to utilize the deep network model in the local area of the aerial images, and study the local features, as well as the hash function of the images. Firstly, according to the course overlap rate of aerial images, the algorithm extracts the local areas for matching to avoid the processing of redundant information. Secondly, a triplet network structure is proposed to mine the deep features of the patches of the local image, and the learned features are imported to the hash layer, thus obtaining the representation of a binary hash code. Thirdly, the constraints of the positive samples to the absolute distance are added on the basis of the triplet loss, a new objective function is constructed to optimize the parameters of the network and enhance the discriminating capabilities of image patch features. Finally, the obtained deep hash code of each image patch is used for the similarity comparison of the image patches in the Hamming space to complete the matching of aerial images. The proposed LDHM algorithm evaluates the UltraCam-D dataset and a set of actual aerial images, simulation result demonstrates that it may significantly outperform the state-of-the-art algorithm in terms of the efficiency and performance.
KW  - aerial matching
KW  - overlap rate
KW  - deep learning
KW  - local features
KW  - hash learning
KW  - absolute distance constraints
DO  - 10.3390/rs9121244
TY  - EJOU
AU  - Qadir, Junaid
AU  - Sathiaseelan, Arjuna
AU  - Farooq, Umar B.
AU  - Usama, Muhammad
AU  - Imran, Muhammad A.
AU  - Shafique, Muhammad
TI  - Approximate Networking for Universal Internet Access
T2  - Future Internet

PY  - 2017
VL  - 9
IS  - 4
SN  - 1999-5903

AB  - Despite the best efforts of networking researchers and practitioners, an ideal Internet experience is inaccessible to an overwhelming majority of people the world over, mainly due to the lack of cost-efficient ways of provisioning high-performance, global Internet. In this paper, we argue that instead of an exclusive focus on a utopian goal of universally accessible “ideal networking” (in which we have a high throughput and quality of service as well as low latency and congestion), we should consider providing “approximate networking” through the adoption of context-appropriate trade-offs. In this regard, we propose to leverage the advances in the emerging trend of “approximate computing” that rely on relaxing the bounds of precise/exact computing to provide new opportunities for improving the area, power, and performance efficiency of systems by orders of magnitude by embracing output errors in resilient applications. Furthermore, we propose to extend the dimensions of approximate computing towards various knobs available at network layers. Approximate networking can be used to provision “Global Access to the Internet for All” (GAIA) in a pragmatically tiered fashion, in which different users around the world are provided a different context-appropriate (but still contextually functional) Internet experience.
KW  - universal Internet access
KW  - approximate networking
KW  - Global Access to the Internet for All (GAIA)
DO  - 10.3390/fi9040094
TY  - EJOU
AU  - Chen, Weitao
AU  - Li, Xianju
AU  - He, Haixia
AU  - Wang, Lizhe
TI  - A Review of Fine-Scale Land Use and Land Cover Classification in Open-Pit Mining Areas by Remote Sensing Techniques
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - Over recent decades, fine-scale land use and land cover classification in open-pit mine areas (LCCMA) has become very important for understanding the influence of mining activities on the regional geo-environment, and for environmental impact assessment procedure. This research reviews advances in fine-scale LCCMA from the following aspects. Firstly, it analyzes and proposes classification thematic resolution for LCCMA. Secondly, remote sensing data sources, features, feature selection methods, and classification algorithms for LCCMA are summarized. Thirdly, three major factors that affect LCCMA are discussed: significant three-dimensional terrain features, strong LCCMA feature variability, and homogeneity of spectral-spatial features. Correspondingly, three key scientific issues that limit the accuracy of LCCMA are presented. Finally, several future research directions are discussed: (1) unitization of new sensors, particularly those with stereo survey ability; (2) procurement of sensitive features by new sensors and combinations of sensitive features using novel feature selection methods; (3) development of robust and self-adjusted classification algorithms, such as ensemble learning and deep learning for LCCMA; and (4) application of fine-scale mining information for regularity and management of mines.
KW  - land cover classification
KW  - open-pit mining area
KW  - remote sensing
KW  - review
KW  - fine-scale
DO  - 10.3390/rs10010015
TY  - EJOU
AU  - Li, Hongguang
AU  - Shi, Yang
AU  - Zhang, Baochang
AU  - Wang, Yufeng
TI  - Superpixel-Based Feature for Aerial Image Scene Recognition
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 1
SN  - 1424-8220

AB  - Image scene recognition is a core technology for many aerial remote sensing applications. Different landforms are inputted as different scenes in aerial imaging, and all landform information is regarded as valuable for aerial image scene recognition. However, the conventional features of the Bag-of-Words model are designed using local points or other related information and thus are unable to fully describe landform areas. This limitation cannot be ignored when the aim is to ensure accurate aerial scene recognition. A novel superpixel-based feature is proposed in this study to characterize aerial image scenes. Then, based on the proposed feature, a scene recognition method of the Bag-of-Words model for aerial imaging is designed. The proposed superpixel-based feature that utilizes landform information establishes top-task superpixel extraction of landforms to bottom-task expression of feature vectors. This characterization technique comprises the following steps: simple linear iterative clustering based superpixel segmentation, adaptive filter bank construction, Lie group-based feature quantification, and visual saliency model-based feature weighting. Experiments of image scene recognition are carried out using real image data captured by an unmanned aerial vehicle (UAV). The recognition accuracy of the proposed superpixel-based feature is 95.1%, which is higher than those of scene recognition algorithms based on other local features.
KW  - superpixel-based feature
KW  - image scene recognition
KW  - aerial remote sensing
DO  - 10.3390/s18010156
TY  - EJOU
AU  - Kim, Sungho
AU  - Song, Woo-Jin
AU  - Kim, So-Hyun
TI  - Double Weight-Based SAR and Infrared Sensor Fusion for Automatic Ground Target Recognition with Deep Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - This paper presents a novel double weight-based synthetic aperture radar (SAR) and infrared (IR) sensor fusion method (DW-SIF) for automatic ground target recognition (ATR). IR-based ATR can provide accurate recognition because of its high image resolution but it is affected by the weather conditions. On the other hand, SAR-based ATR shows a low recognition rate due to the noisy low resolution but can provide consistent performance regardless of the weather conditions. The fusion of an active sensor (SAR) and a passive sensor (IR) can lead to upgraded performance. This paper proposes a doubly weighted neural network fusion scheme at the decision level. The first weight (   α   ) can measure the offline sensor confidence per target category based on the classification rate for an evaluation set. The second weight (   β   ) can measure the online sensor reliability based on the score distribution for a test target image. The LeNet architecture-based deep convolution network (14 layers) is used as an individual classifier. Doubly weighted sensor scores are fused by two types of fusion schemes, such as the sum-based linear fusion scheme (    α β    -sum) and neural network-based nonlinear fusion scheme (    α β    -NN). The experimental results confirmed the proposed linear fusion method (    α β    -sum) to have the best performance among the linear fusion schemes available (SAR-CNN, IR-CNN,    α   -sum,    β   -sum,     α β    -sum, and Bayesian fusion). In addition, the proposed nonlinear fusion method (    α β    -NN) showed superior target recognition performance to linear fusion on the OKTAL-SE-based synthetic database.
KW  - SAR
KW  - IR
KW  - fusion
KW  - double weights
KW  - linear
KW  - nonlinear
KW  - deep learning
KW  - OKTAL-SE
DO  - 10.3390/rs10010072
TY  - EJOU
AU  - Qu, Yufu
AU  - Huang, Jianyu
AU  - Zhang, Xuan
TI  - Rapid 3D Reconstruction for Image Sequence Acquired from UAV Camera
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 1
SN  - 1424-8220

AB  - In order to reconstruct three-dimensional (3D) structures from an image sequence captured by unmanned aerial vehicles’ camera (UAVs) and improve the processing speed, we propose a rapid 3D reconstruction method that is based on an image queue, considering the continuity and relevance of UAV camera images. The proposed approach first compresses the feature points of each image into three principal component points by using the principal component analysis method. In order to select the key images suitable for 3D reconstruction, the principal component points are used to estimate the interrelationships between images. Second, these key images are inserted into a fixed-length image queue. The positions and orientations of the images are calculated, and the 3D coordinates of the feature points are estimated using weighted bundle adjustment. With this structural information, the depth maps of these images can be calculated. Next, we update the image queue by deleting some of the old images and inserting some new images into the queue, and a structural calculation of all the images can be performed by repeating the previous steps. Finally, a dense 3D point cloud can be obtained using the depth–map fusion method. The experimental results indicate that when the texture of the images is complex and the number of images exceeds 100, the proposed method can improve the calculation speed by more than a factor of four with almost no loss of precision. Furthermore, as the number of images increases, the improvement in the calculation speed will become more noticeable.
KW  - UAV camera
KW  - multi-view stereo
KW  - structure from motion
KW  - 3D reconstruction
KW  - point cloud
DO  - 10.3390/s18010225
TY  - EJOU
AU  - Mahabir, Ron
AU  - Croitoru, Arie
AU  - Crooks, Andrew T.
AU  - Agouris, Peggy
AU  - Stefanidis, Anthony
TI  - A Critical Review of High and Very High-Resolution Remote Sensing Approaches for Detecting and Mapping Slums: Trends, Challenges and Emerging Opportunities
T2  - Urban Science

PY  - 2018
VL  - 2
IS  - 1
SN  - 2413-8851

AB  - Slums are a global urban challenge, with less developed countries being particularly impacted. To adequately detect and map them, data is needed on their location, spatial extent and evolution. High- and very high-resolution remote sensing imagery has emerged as an important source of data in this regard. The purpose of this paper is to critically review studies that have used such data to detect and map slums. Our analysis shows that while such studies have been increasing over time, they tend to be concentrated to a few geographical areas and often focus on the use of a single approach (e.g., image texture and object-based image analysis), thus limiting generalizability to understand slums, their population, and evolution within the global context. We argue that to develop a more comprehensive framework that can be used to detect and map slums, other emerging sourcing of geospatial data should be considered (e.g., volunteer geographic information) in conjunction with growing trends and advancements in technology (e.g., geosensor networks). Through such data integration and analysis we can then create a benchmark for determining the most suitable methods for mapping slums in a given locality, thus fostering the creation of new approaches to address this challenge.
KW  - high-and very high-resolution imagery
KW  - remote sensing
KW  - slums
KW  - volunteer geographic information
KW  - geosensor networks
KW  - image analysis
DO  - 10.3390/urbansci2010008
TY  - EJOU
AU  - Mueller, Markus S.
AU  - Jutzi, Boris
TI  - UAS Navigation with SqueezePoseNet—Accuracy Boosting for Pose Regression by Data Augmentation
T2  - Drones

PY  - 2018
VL  - 2
IS  - 1
SN  - 2504-446X

AB  - The navigation of Unmanned Aerial Vehicles (UAVs) nowadays is mostly based on Global Navigation Satellite Systems (GNSSs). Drawbacks of satellite-based navigation are failures caused by occlusions or multi-path interferences. Therefore, alternative methods have been developed in recent years. Visual navigation methods such as Visual Odometry (VO) or visual Simultaneous Localization and Mapping (SLAM) aid global navigation solutions by closing trajectory gaps or performing loop closures. However, if the trajectory estimation is interrupted or not available, a re-localization is mandatory. Furthermore, the latest research has shown promising results on pose regression in 6 Degrees of Freedom (DoF) based on Convolutional Neural Networks (CNNs). Additionally, existing navigation methods can benefit from these networks. In this article, a method for GNSS-free and fast image-based pose regression by utilizing a small Convolutional Neural Network is presented. Therefore, a small CNN (SqueezePoseNet) is utilized, transfer learning is applied and the network is tuned for pose regression. Furthermore, recent drawbacks are overcome by applying data augmentation on a training dataset utilizing simulated images. Experiments with small CNNs show promising results for GNSS-free and fast localization compared to larger networks. By training a CNN with an extended data set including simulated images, the accuracy on pose regression is improved up to 61.7% for position and up to 76.0% for rotation compared to training on a standard not-augmented data set.
KW  - convolutional neural networks
KW  - data augmentation
KW  - image-based navigation
KW  - pose estimation
DO  - 10.3390/drones2010007
TY  - EJOU
AU  - Guo, Qiangliang
AU  - Xiao, Jin
AU  - Hu, Xiaoguang
TI  - New Keypoint Matching Method Using Local Convolutional Features for Power Transmission Line Icing Monitoring
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - Power transmission line icing (PTLI) problems, which cause tremendous damage to the power grids, has drawn much attention. Existing three-dimensional measurement methods based on binocular stereo vision was recently introduced to measure the ice thickness in PTLI, but failed to meet requirements of practical applications due to inefficient keypoint matching in the complex PTLI scene. In this paper, a new keypoint matching method is proposed based on the local multi-layer convolutional neural network (CNN) features, termed Local Convolutional Features (LCFs). LCFs are deployed to extract more discriminative features than the conventional CNNs. Particularly in LCFs, a multi-layer features fusion scheme is exploited to boost the matching performance. Together with a location constraint method, the correspondence of neighboring keypoints is further refined. Our approach achieves 1.5%, 5.3%, 13.1%, 27.3% improvement in the average matching precision compared with SIFT, SURF, ORB and MatchNet on the public Middlebury dataset, and the measurement accuracy of ice thickness can reach 90.9% compared with manual measurement on the collected PTLI dataset.
KW  - power transmission line icing
KW  - keypoint matching
KW  - convolutional neural network
KW  - feature fusion
KW  - location constraint
DO  - 10.3390/s18030698
TY  - EJOU
AU  - Zhao, Yi
AU  - Ma, Jiale
AU  - Li, Xiaohui
AU  - Zhang, Jie
TI  - Saliency Detection and Deep Learning-Based Wildfire Identification in UAV Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - An unmanned aerial vehicle (UAV) equipped with global positioning systems (GPS) can provide direct georeferenced imagery, mapping an area with high resolution. So far, the major difficulty in wildfire image classification is the lack of unified identification marks, the fire features of color, shape, texture (smoke, flame, or both) and background can vary significantly from one scene to another. Deep learning (e.g., DCNN for Deep Convolutional Neural Network) is very effective in high-level feature learning, however, a substantial amount of training images dataset is obligatory in optimizing its weights value and coefficients. In this work, we proposed a new saliency detection algorithm for fast location and segmentation of core fire area in aerial images. As the proposed method can effectively avoid feature loss caused by direct resizing; it is used in data augmentation and formation of a standard fire image dataset ‘UAV_Fire’. A 15-layered self-learning DCNN architecture named ‘Fire_Net’ is then presented as a self-learning fire feature exactor and classifier. We evaluated different architectures and several key parameters (drop out ratio, batch size, etc.) of the DCNN model regarding its validation accuracy. The proposed architecture outperformed previous methods by achieving an overall accuracy of 98%. Furthermore, ‘Fire_Net’ guarantied an average processing speed of 41.5 ms per image for real-time wildfire inspection. To demonstrate its practical utility, Fire_Net is tested on 40 sampled images in wildfire news reports and all of them have been accurately identified.
KW  - UAV
KW  - wildfire
KW  - deep learning
KW  - saliency detection
DO  - 10.3390/s18030712
TY  - EJOU
AU  - Czúni, László
AU  - Rashad, Metwally
TI  - Lightweight Active Object Retrieval with Weak Classifiers
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - In the last few years, there has been a steadily growing interest in autonomous vehicles and robotic systems. While many of these agents are expected to have limited resources, these systems should be able to dynamically interact with other objects in their environment. We present an approach where lightweight sensory and processing techniques, requiring very limited memory and processing power, can be successfully applied to the task of object retrieval using sensors of different modalities. We use the Hough framework to fuse optical and orientation information of the different views of the objects. In the presented spatio-temporal perception technique, we apply active vision, where, based on the analysis of initial measurements, the direction of the next view is determined to increase the hit-rate of retrieval. The performance of the proposed methods is shown on three datasets loaded with heavy noise.
KW  - object retrieval
KW  - Hough transformation
KW  - sensor fusion
KW  - active vision
DO  - 10.3390/s18030801
TY  - EJOU
AU  - Zhang, Duona
AU  - Ding, Wenrui
AU  - Zhang, Baochang
AU  - Xie, Chunyu
AU  - Li, Hongguang
AU  - Liu, Chunhui
AU  - Han, Jungong
TI  - Automatic Modulation Classification Based on Deep Learning for Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - Deep learning has recently attracted much attention due to its excellent performance in processing audio, image, and video data. However, few studies are devoted to the field of automatic modulation classification (AMC). It is one of the most well-known research topics in communication signal recognition and remains challenging for traditional methods due to complex disturbance from other sources. This paper proposes a heterogeneous deep model fusion (HDMF) method to solve the problem in a unified framework. The contributions include the following: (1) a convolutional neural network (CNN) and long short-term memory (LSTM) are combined by two different ways without prior knowledge involved; (2) a large database, including eleven types of single-carrier modulation signals with various noises as well as a fading channel, is collected with various signal-to-noise ratios (SNRs) based on a real geographical environment; and (3) experimental results demonstrate that HDMF is very capable of coping with the AMC problem, and achieves much better performance when compared with the independent network.
KW  - deep learning
KW  - automatic modulation classification
KW  - classifier fusion
KW  - convolutional neural network
KW  - long short-term memory
DO  - 10.3390/s18030924
TY  - EJOU
AU  - Sandino, Juan
AU  - Pegg, Geoff
AU  - Gonzalez, Felipe
AU  - Smith, Grant
TI  - Aerial Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral Sensors, and Artificial Intelligence
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 4
SN  - 1424-8220

AB  - The environmental and economic impacts of exotic fungal species on natural and plantation forests have been historically catastrophic. Recorded surveillance and control actions are challenging because they are costly, time-consuming, and hazardous in remote areas. Prolonged periods of testing and observation of site-based tests have limitations in verifying the rapid proliferation of exotic pathogens and deterioration rates in hosts. Recent remote sensing approaches have offered fast, broad-scale, and affordable surveys as well as additional indicators that can complement on-ground tests. This paper proposes a framework that consolidates site-based insights and remote sensing capabilities to detect and segment deteriorations by fungal pathogens in natural and plantation forests. This approach is illustrated with an experimentation case of myrtle rust (Austropuccinia psidii) on paperbark tea trees (Melaleuca quinquenervia) in New South Wales (NSW), Australia. The method integrates unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Imagery is acquired using a Headwall Nano-Hyperspec     ®     camera, orthorectified in Headwall SpectralView     ®    , and processed in Python programming language using eXtreme Gradient Boosting (XGBoost), Geospatial Data Abstraction Library (GDAL), and Scikit-learn third-party libraries. In total, 11,385 samples were extracted and labelled into five classes: two classes for deterioration status and three classes for background objects. Insights reveal individual detection rates of 95% for healthy trees, 97% for deteriorated trees, and a global multiclass detection rate of 97%. The methodology is versatile to be applied to additional datasets taken with different image sensors, and the processing of large datasets with freeware tools.
KW  - Austropuccinia psidii
KW  - drones
KW  - hyperspectral camera
KW  - machine learning
KW  - Melaleuca quinquenervia
KW  - myrtle rust
KW  - non-invasive assessment
KW  - paperbark
KW  - unmanned aerial vehicles (UAV)
KW  - xgboost
DO  - 10.3390/s18040944
TY  - EJOU
AU  - Pang, Jingyue
AU  - Liu, Datong
AU  - Peng, Yu
AU  - Peng, Xiyuan
TI  - Optimize the Coverage Probability of Prediction Interval for Anomaly Detection of Sensor-Based Monitoring Series
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 4
SN  - 1424-8220

AB  - Effective anomaly detection of sensing data is essential for identifying potential system failures. Because they require no prior knowledge or accumulated labels, and provide uncertainty presentation, the probability prediction methods (e.g., Gaussian process regression (GPR) and relevance vector machine (RVM)) are especially adaptable to perform anomaly detection for sensing series. Generally, one key parameter of prediction models is coverage probability (CP), which controls the judging threshold of the testing sample and is generally set to a default value (e.g., 90% or 95%). There are few criteria to determine the optimal CP for anomaly detection. Therefore, this paper designs a graphic indicator of the receiver operating characteristic curve of prediction interval (ROC-PI) based on the definition of the ROC curve which can depict the trade-off between the PI width and PI coverage probability across a series of cut-off points. Furthermore, the Youden index is modified to assess the performance of different CPs, by the minimization of which the optimal CP is derived by the simulated annealing (SA) algorithm. Experiments conducted on two simulation datasets demonstrate the validity of the proposed method. Especially, an actual case study on sensing series from an on-orbit satellite illustrates its significant performance in practical application.
KW  - satellite
KW  - anomaly detection
KW  - coverage probability
KW  - prediction interval
KW  - Gaussian process regression
KW  - relevance vector machine
DO  - 10.3390/s18040967
TY  - EJOU
AU  - Kong, Xiangxiong
AU  - Li, Jian
TI  - Image Registration-Based Bolt Loosening Detection of Steel Joints
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 4
SN  - 1424-8220

AB  - Self-loosening of bolts caused by repetitive loads and vibrations is one of the common defects that can weaken the structural integrity of bolted steel joints in civil structures. Many existing approaches for detecting loosening bolts are based on physical sensors and, hence, require extensive sensor deployment, which limit their abilities to cost-effectively detect loosened bolts in a large number of steel joints. Recently, computer vision-based structural health monitoring (SHM) technologies have demonstrated great potential for damage detection due to the benefits of being low cost, easy to deploy, and contactless. In this study, we propose a vision-based non-contact bolt loosening detection method that uses a consumer-grade digital camera. Two images of the monitored steel joint are first collected during different inspection periods and then aligned through two image registration processes. If the bolt experiences rotation between inspections, it will introduce differential features in the registration errors, serving as a good indicator for bolt loosening detection. The performance and robustness of this approach have been validated through a series of experimental investigations using three laboratory setups including a gusset plate on a cross frame, a column flange, and a girder web. The bolt loosening detection results are presented for easy interpretation such that informed decisions can be made about the detected loosened bolts.
KW  - bolt loosening detection
KW  - intensity-based image registration
KW  - feature matching
KW  - structural health monitoring
KW  - structural inspection
KW  - superpixel
KW  - civil structures
KW  - steel joints
KW  - feature tracking
DO  - 10.3390/s18041000
TY  - EJOU
AU  - Zhuo, Xiangyu
AU  - Fraundorfer, Friedrich
AU  - Kurz, Franz
AU  - Reinartz, Peter
TI  - Optimization of OpenStreetMap Building Footprints Based on Semantic Information of Oblique UAV Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - Building footprint information is vital for 3D building modeling. Traditionally, in remote sensing, building footprints are extracted and delineated from aerial imagery and/or LiDAR point cloud. Taking a different approach, this paper is dedicated to the optimization of OpenStreetMap (OSM) building footprints exploiting the contour information, which is derived from deep learning-based semantic segmentation of oblique images acquired by the Unmanned Aerial Vehicle (UAV). First, a simplified 3D building model of Level of Detail 1 (LoD 1) is initialized using the footprint information from OSM and the elevation information from Digital Surface Model (DSM). In parallel, a deep neural network for pixel-wise semantic image segmentation is trained in order to extract the building boundaries as contour evidence. Subsequently, an optimization integrating the contour evidence from multi-view images as a constraint results in a refined 3D building model with optimized footprints and height. Our method is leveraged to optimize OSM building footprints for four datasets with different building types, demonstrating robust performance for both individual buildings and multiple buildings regardless of image resolution. Finally, we compare our result with reference data from German Authority Topographic-Cartographic Information System (ATKIS). Quantitative and qualitative evaluations reveal that the original OSM building footprints have large offset, but can be significantly improved from meter level to decimeter level after optimization.
KW  - building footprint
KW  - oblique UAV images
KW  - semantic segmentation
KW  - deep neural network
DO  - 10.3390/rs10040624
TY  - EJOU
AU  - Zhang, Yongjun
AU  - Wang, Xiang
AU  - Xie, Xunwei
AU  - Li, Yansheng
TI  - Salient Object Detection via Recursive Sparse Representation
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - Object-level saliency detection is an attractive research field which is useful for many content-based computer vision and remote-sensing tasks. This paper introduces an efficient unsupervised approach to salient object detection from the perspective of recursive sparse representation. The reconstruction error determined by foreground and background dictionaries other than common local and global contrasts is used as the saliency indication, by which the shortcomings of the object integrity can be effectively improved. The proposed method consists of the following four steps: (1) regional feature extraction; (2) background and foreground dictionaries extraction according to the initial saliency map and image boundary constraints; (3) sparse representation and saliency measurement; and (4) recursive processing with a current saliency map updating the initial saliency map in step 2 and repeating step 3. This paper also presents the experimental results of the proposed method compared with seven state-of-the-art saliency detection methods using three benchmark datasets, as well as some satellite and unmanned aerial vehicle remote-sensing images, which confirmed that the proposed method was more effective than current methods and could achieve more favorable performance in the detection of multiple objects as well as maintaining the integrity of the object area.
KW  - salient object detection
KW  - sparse representation
KW  - reconstruction error
KW  - recursive processing
DO  - 10.3390/rs10040652
TY  - EJOU
AU  - Krylov, Vladimir A.
AU  - Kenny, Eamonn
AU  - Dahyot, Rozenn
TI  - Automatic Discovery and Geotagging of Objects from Street View Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Many applications, such as autonomous navigation, urban planning, and asset monitoring, rely on the availability of accurate information about objects and their geolocations. In this paper, we propose the automatic detection and computation of the coordinates of recurring stationary objects of interest using street view imagery. Our processing pipeline relies on two fully convolutional neural networks: the first segments objects in the images, while the second estimates their distance from the camera. To geolocate all the detected objects coherently we propose a novel custom Markov random field model to estimate the objects&rsquo; geolocation. The novelty of the resulting pipeline is the combined use of monocular depth estimation and triangulation to enable automatic mapping of complex scenes with the simultaneous presence of multiple, visually similar objects of interest. We validate experimentally the effectiveness of our approach on two object classes: traffic lights and telegraph poles. The experiments report high object recall rates and position precision of approximately 2 m, which is approaching the precision of single-frequency GPS receivers.
KW  - object geolocation
KW  - object mapping
KW  - street view imagery
KW  - Markov random fields
KW  - traffic lights
KW  - telecom assets
KW  - GPS estimation
DO  - 10.3390/rs10050661
TY  - EJOU
AU  - Moy de Vitry, Matthew
AU  - Schindler, Konrad
AU  - Rieckermann, Jörg
AU  - Leitão, João P.
TI  - Sewer Inlet Localization in UAV Image Clouds: Improving Performance with Multiview Detection
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Sewer and drainage infrastructure are often not as well catalogued as they should be, considering the immense investment they represent. In this work, we present a fully automatic framework for localizing sewer inlets from image clouds captured from an unmanned aerial vehicle (UAV). The framework exploits the high image overlap of UAV imaging surveys with a multiview approach to improve detection performance. The framework uses a Viola–Jones classifier trained to detect sewer inlets in aerial images with a ground sampling distance of 3–3.5 cm/pixel. The detections are then projected into three-dimensional space where they are clustered and reclassified to discard false positives. The method is evaluated by cross-validating results from an image cloud of 252 UAV images captured over a 0.57-km2 study area with 228 sewer inlets. Compared to an equivalent single-view detector, the multiview approach improves both recall and precision, increasing average precision from 0.65 to 0.73. The source code and case study data are publicly available for reuse.
KW  - infrastructure mapping
KW  - multiview
KW  - object detection
KW  - unmanned aerial vehicle
KW  - urban drainage
KW  - asset management
DO  - 10.3390/rs10050706
TY  - EJOU
AU  - Deng, Zhipeng
AU  - Sun, Hao
AU  - Zhou, Shilin
TI  - Semi-Supervised Ground-to-Aerial Adaptation with Heterogeneous Features Learning for Scene Classification
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 5
SN  - 2220-9964

AB  - Currently, huge quantities of remote sensing images (RSIs) are becoming available. Nevertheless, the scarcity of labeled samples hinders the semantic understanding of RSIs. Fortunately, many ground-level image datasets with detailed semantic annotations have been collected in the vision community. In this paper, we attempt to exploit the abundant labeled ground-level images to build discriminative models for overhead-view RSI classification. However, images from the ground-level and overhead view are represented by heterogeneous features with different distributions; how to effectively combine multiple features and reduce the mismatch of distributions are two key problems in this scene-model transfer task. Specifically, a semi-supervised manifold-regularized multiple-kernel-learning (SMRMKL) algorithm is proposed for solving these problems. We employ multiple kernels over several features to learn an optimal combined model automatically. Multi-kernel Maximum Mean Discrepancy (MK-MMD) is utilized to measure the data mismatch. To make use of unlabeled target samples, a manifold regularized semi-supervised learning process is incorporated into our framework. Extensive experimental results on both cross-view and aerial-to-satellite scene datasets demonstrate that: (1) SMRMKL has an appealing extension ability to effectively fuse different types of visual features; and (2) manifold regularization can improve the adaptation performance by utilizing unlabeled target samples.
KW  - remote sensing
KW  - scene classification
KW  - heterogeneous domain adaptation
KW  - cross-view
KW  - multiple kernel learning
DO  - 10.3390/ijgi7050182
TY  - EJOU
AU  - Ahmad Yousef, Khalil M.
AU  - AlMajali, Anas
AU  - Ghalyon, Salah A.
AU  - Dweik, Waleed
AU  - Mohd, Bassam J.
TI  - Analyzing Cyber-Physical Threats on Robotic Platforms
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 5
SN  - 1424-8220

AB  - Robots are increasingly involved in our daily lives. Fundamental to robots are the communication link (or stream) and the applications that connect the robots to their clients or users. Such communication link and applications are usually supported through client/server network connection. This networking system is amenable of being attacked and vulnerable to the security threats. Ensuring security and privacy for robotic platforms is thus critical, as failures and attacks could have devastating consequences. In this paper, we examine several cyber-physical security threats that are unique to the robotic platforms; specifically the communication link and the applications. Threats target integrity, availability and confidential security requirements of the robotic platforms, which use MobileEyes/arnlServer client/server applications. A robot attack tool (RAT) was developed to perform specific security attacks. An impact-oriented approach was adopted to analyze the assessment results of the attacks. Tests and experiments of attacks were conducted in simulation environment and physically on the robot. The simulation environment was based on MobileSim; a software tool for simulating, debugging and experimenting on MobileRobots/ActivMedia platforms and their environments. The robot platform PeopleBotTM was used for physical experiments. The analysis and testing results show that certain attacks were successful at breaching the robot security. Integrity attacks modified commands and manipulated the robot behavior. Availability attacks were able to cause Denial-of-Service (DoS) and the robot was not responsive to MobileEyes commands. Integrity and availability attacks caused sensitive information on the robot to be hijacked. To mitigate security threats, we provide possible mitigation techniques and suggestions to raise awareness of threats on the robotic platforms, especially when the robots are involved in critical missions or applications.
KW  - mobile robot
KW  - robotic platform
KW  - cyper-physical systems
KW  - threats
KW  - security
KW  - risk management
KW  - risk assessment
DO  - 10.3390/s18051643
TY  - EJOU
AU  - Lyu, Hai-Min
AU  - Xu, Ye-Shuang
AU  - Cheng, Wen-Chieh
AU  - Arulrajah, Arul
TI  - Flooding Hazards across Southern China and Prospective Sustainability Measures
T2  - Sustainability

PY  - 2018
VL  - 10
IS  - 5
SN  - 2071-1050

AB  - The Yangtze River Basin and Huaihe River Basin in Southern China experienced severe floods 1998 and 2016. The reasons for the flooding hazards include the following two factors: hazardous weather conditions and degradation of the hydrological environment due to anthropogenic activities. This review work investigated the weather conditions based on recorded data, which showed that both 1998 and 2016 were in El Nino periods. Human activities include the degradations of rivers and lakes and the effects caused by the building of the Three Gorges Dam. In addition, the flooding in 2016 had a lower hazard scale than that in 1998 but resulted in larger economic losses than that of 1998. To mitigate urban waterlogging caused by flooding hazards, China proposed a new strategy named Spongy City (SPC) in 2014. SPC promotes sustainable city development so that a city has the resilience to adapt to climate change, to mitigate the impacts of waterlogging caused by extreme rainfall events. The countermeasures used to tackle the SPC construction-related problems, such as local inundation, water resource shortage, storm water usage, and water pollution control, are proposed for city management to improve the environment.
KW  - flooding hazards
KW  - hazardous weather conditions
KW  - urban waterlogging
KW  - SPC
KW  - sustainable development
DO  - 10.3390/su10051682
TY  - EJOU
AU  - Liu, Xiaofei
AU  - Yang, Tao
AU  - Li, Jing
TI  - Real-Time Ground Vehicle Detection in Aerial Infrared Imagery Based on Convolutional Neural Network
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 6
SN  - 2079-9292

AB  - An infrared sensor is a commonly used imaging device. Unmanned aerial vehicles, the most promising moving platform, each play a vital role in their own field, respectively. However, the two devices are seldom combined in automatic ground vehicle detection tasks. Therefore, how to make full use of them&mdash;especially in ground vehicle detection based on aerial imagery&ndash;has aroused wide academic concern. However, due to the aerial imagery&rsquo;s low-resolution and the vehicle detection&rsquo;s complexity, how to extract remarkable features and handle pose variations, view changes as well as surrounding radiation remains a challenge. In fact, these typical abstract features extracted by convolutional neural networks are more recognizable than the engineering features, and those complex conditions involved can be learned and memorized before. In this paper, a novel approach towards ground vehicle detection in aerial infrared images based on a convolutional neural network is proposed. The UAV and the infrared sensor used in this application are firstly introduced. Then, a novel aerial moving platform is built and an aerial infrared vehicle dataset is unprecedentedly constructed. We publicly release this dataset (NPU_CS_UAV_IR_DATA), which can be used for the following research in this field. Next, an end-to-end convolutional neural network is built. With large amounts of recognized features being iteratively learned, a real-time ground vehicle model is constructed. It has the unique ability to detect both the stationary vehicles and moving vehicles in real urban environments. We evaluate the proposed algorithm on some low&ndash;resolution aerial infrared images. Experiments on the NPU_CS_UAV_IR_DATA dataset demonstrate that the proposed method is effective and efficient to recognize the ground vehicles. Moreover it can accomplish the task in real-time while achieving superior performances in leak and false alarm ratio.
KW  - aerial infrared imagery
KW  - real-time ground vehicle detection
KW  - convolutional neural network
KW  - unmanned aerial vehicle
DO  - 10.3390/electronics7060078
TY  - EJOU
AU  - Nguyen, Phong H.
AU  - Arsalan, Muhammad
AU  - Koo, Ja H.
AU  - Naqvi, Rizwan A.
AU  - Truong, Noi Q.
AU  - Park, Kang R.
TI  - LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - Autonomous landing of an unmanned aerial vehicle or a drone is a challenging problem for the robotics research community. Previous researchers have attempted to solve this problem by combining multiple sensors such as global positioning system (GPS) receivers, inertial measurement unit, and multiple camera systems. Although these approaches successfully estimate an unmanned aerial vehicle location during landing, many calibration processes are required to achieve good detection accuracy. In addition, cases where drones operate in heterogeneous areas with no GPS signal should be considered. To overcome these problems, we determined how to safely land a drone in a GPS-denied environment using our remote-marker-based tracking algorithm based on a single visible-light-camera sensor. Instead of using hand-crafted features, our algorithm includes a convolutional neural network named lightDenseYOLO to extract trained features from an input image to predict a marker&rsquo;s location by visible light camera sensor on drone. Experimental results show that our method significantly outperforms state-of-the-art object trackers both using and not using convolutional neural network in terms of both accuracy and processing time.
KW  - unmanned aerial vehicle
KW  - autonomous landing
KW  - real-time marker detection
KW  - lightDenseYOLO
KW  - visible light camera sensor on drone
DO  - 10.3390/s18061703
TY  - EJOU
AU  - Zhu, Jiasong
AU  - Sun, Ke
AU  - Jia, Sen
AU  - Lin, Weidong
AU  - Hou, Xianxu
AU  - Liu, Bozhi
AU  - Qiu, Guoping
TI  - Bidirectional Long Short-Term Memory Network for Vehicle Behavior Recognition
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 6
SN  - 2072-4292

AB  - Vehicle behavior recognition is an attractive research field which is useful for many computer vision and intelligent traffic analysis tasks. This paper presents an all-in-one behavior recognition framework for moving vehicles based on the latest deep learning techniques. Unlike traditional traffic analysis methods which rely on low-resolution videos captured by road cameras, we capture 4K (    3840 × 2178    ) traffic videos at a busy road intersection of a modern megacity by flying a unmanned aerial vehicle (UAV) during the rush hours. We then manually annotate locations and types of road vehicles. The proposed method consists of the following three steps: (1) vehicle detection and type recognition based on deep neural networks; (2) vehicle tracking by data association and vehicle trajectory modeling; (3) vehicle behavior recognition by nearest neighbor search and by bidirectional long short-term memory network, respectively. This paper also presents experimental results of the proposed framework in comparison with state-of-the-art approaches on the 4K testing traffic video, which demonstrated the effectiveness and superiority of the proposed method.
KW  - unmanned aerial vehicles (UAVs)
KW  - deep neural networks
KW  - vehicle detection
KW  - vehicle tracking
KW  - behavior recognition
KW  - long short-term memory
DO  - 10.3390/rs10060887
TY  - EJOU
AU  - Kim, In-Ho
AU  - Jeon, Haemin
AU  - Baek, Seung-Chan
AU  - Hong, Won-Hwa
AU  - Jung, Hyung-Jo
TI  - Application of Crack Identification Techniques for an Aging Concrete Bridge Inspection Using an Unmanned Aerial Vehicle
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - Bridge inspection using unmanned aerial vehicles (UAV) with high performance vision sensors has received considerable attention due to its safety and reliability. As bridges become obsolete, the number of bridges that need to be inspected increases, and they require much maintenance cost. Therefore, a bridge inspection method based on UAV with vision sensors is proposed as one of the promising strategies to maintain bridges. In this paper, a crack identification method by using a commercial UAV with a high resolution vision sensor is investigated in an aging concrete bridge. First, a point cloud-based background model is generated in the preliminary flight. Then, cracks on the structural surface are detected with the deep learning algorithm, and their thickness and length are calculated. In the deep learning method, region with convolutional neural networks (R-CNN)-based transfer learning is applied. As a result, a new network for the 384 collected crack images of 256 &times; 256 pixel resolution is generated from the pre-trained network. A field test is conducted to verify the proposed approach, and the experimental results proved that the UAV-based bridge inspection is effective at identifying and quantifying the cracks on the structures.
KW  - crack identification
KW  - deep learning
KW  - unmanned aerial vehicle (UAV)
KW  - computer vision
KW  - spatial information
DO  - 10.3390/s18061881
TY  - EJOU
AU  - Rivas, Alberto
AU  - Chamoso, Pablo
AU  - González-Briones, Alfonso
AU  - Corchado, Juan M.
TI  - Detection of Cattle Using Drones and Convolutional Neural Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Multirotor drones have been one of the most important technological advances of the last decade. Their mechanics are simple compared to other types of drones and their possibilities in flight are greater. For example, they can take-off vertically. Their capabilities have therefore brought progress to many professional activities. Moreover, advances in computing and telecommunications have also broadened the range of activities in which drones may be used. Currently, artificial intelligence and information analysis are the main areas of research in the field of computing. The case study presented in this article employed artificial intelligence techniques in the analysis of information captured by drones. More specifically, the camera installed in the drone took images which were later analyzed using Convolutional Neural Networks (CNNs) to identify the objects captured in the images. In this research, a CNN was trained to detect cattle, however the same training process could be followed to develop a CNN for the detection of any other object. This article describes the design of the platform for real-time analysis of information and its performance in the detection of cattle.
KW  - cattle detection
KW  - convolutional neural network
KW  - multirotor
KW  - drone
KW  - Unmanned Aerial Vehicle
DO  - 10.3390/s18072048
TY  - EJOU
AU  - Guerra, Edmundo
AU  - Munguía, Rodrigo
AU  - Grau, Antoni
TI  - UAV Visual and Laser Sensors Fusion for Detection and Positioning in Industrial Applications
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - This work presents a solution to localize Unmanned Autonomous Vehicles with respect to pipes and other cylindrical elements found in inspection and maintenance tasks both in industrial and civilian infrastructures. The proposed system exploits the different features of vision and laser based sensors, combining them to obtain accurate positioning of the robot with respect to the cylindrical structures. A probabilistic (RANSAC-based) procedure is used to segment possible cylinders found in the laser scans, and this is used as a seed to accurately determine the robot position through a computer vision system. The priors obtained from the laser scan registration help to solve the problem of determining the apparent contour of the cylinders. In turn this apparent contour is used in a degenerate quadratic conic estimation, enabling to visually estimate the pose of the cylinder.
KW  - Unmanned Autonomous Vehicle
KW  - pose determination
KW  - LiDAR registration
KW  - apparent contour
DO  - 10.3390/s18072071
TY  - EJOU
AU  - Adege, Abebe B.
AU  - Lin, Hsin-Piao
AU  - Tarekegn, Getaneh B.
AU  - Jeng, Shiann-Shiun
TI  - Applying Deep Neural Network (DNN) for Robust Indoor Localization in Multi-Building Environment
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 7
SN  - 2076-3417

AB  - In the Internet of Things (IoT) era, indoor localization plays a vital role in academia and industry. Wi-Fi is a promising scheme for indoor localization as it is easy and free of charge, even for private networks. However, Wi-Fi has signal fluctuation problems because of dynamic changes of environments and shadowing effects. In this paper, we propose to use a deep neural network (DNN) to achieve accurate localization in Wi-Fi environments. In the localization process, we primarily construct a database having all reachable received signal strengths (RSSs), and basic service set identifiers (BSSIDs). Secondly, we fill the missed RSS values using regression, and then apply linear discriminant analysis (LDA) to reduce features. Thirdly, the 5-BSSIDs having the strongest RSS values are appended with reduced RSS vector. Finally, a DNN is applied for localizing Wi-Fi users. The proposed system is evaluated in the classification and regression schemes using the python programming language. The results show that 99.15% of the localization accuracy is correctly classified. Moreover, the coordinate-based localization provides 50%, 75%, and 93.10% accuracies for errors less than 0.50 m, 0.75 m, and 0.90 m respectively. The proposed method is compared with other algorithms, and our method provides motivated results. The simulation results also show that the proposed method can robustly localize Wi-Fi users in hierarchical and complex wireless environments.
KW  - deep neural network
KW  - Internet of Things
KW  - linear discriminant analysis
KW  - Wi-Fi based indoor localization
DO  - 10.3390/app8071062
TY  - EJOU
AU  - Huang, Huasheng
AU  - Lan, Yubin
AU  - Deng, Jizhong
AU  - Yang, Aqing
AU  - Deng, Xiaoling
AU  - Zhang, Lei
AU  - Wen, Sheng
TI  - A Semantic Labeling Approach for Accurate Weed Mapping of High Resolution UAV Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Weed control is necessary in rice cultivation, but the excessive use of herbicide treatments has led to serious agronomic and environmental problems. Suitable site-specific weed management (SSWM) is a solution to address this problem while maintaining the rice production quality and quantity. In the context of SSWM, an accurate weed distribution map is needed to provide decision support information for herbicide treatment. UAV remote sensing offers an efficient and effective platform to monitor weeds thanks to its high spatial resolution. In this work, UAV imagery was captured in a rice field located in South China. A semantic labeling approach was adopted to generate the weed distribution maps of the UAV imagery. An ImageNet pre-trained CNN with residual framework was adapted in a fully convolutional form, and transferred to our dataset by fine-tuning. Atrous convolution was applied to extend the field of view of convolutional filters; the performance of multi-scale processing was evaluated; and a fully connected conditional random field (CRF) was applied after the CNN to further refine the spatial details. Finally, our approach was compared with the pixel-based-SVM and the classical FCN-8s. Experimental results demonstrated that our approach achieved the best performance in terms of accuracy. Especially for the detection of small weed patches in the imagery, our approach significantly outperformed other methods. The mean intersection over union (mean IU), overall accuracy, and Kappa coefficient of our method were 0.7751, 0.9445, and 0.9128, respectively. The experiments showed that our approach has high potential in accurate weed mapping of UAV imagery.
KW  - UAV
KW  - remote sensing
KW  - weed mapping
KW  - Deep Fully Convolutional Network
KW  - semantic labeling
DO  - 10.3390/s18072113
TY  - EJOU
AU  - Buscombe, Daniel
AU  - Ritchie, Andrew C.
TI  - Landscape Classification with Deep Neural Networks
T2  - Geosciences

PY  - 2018
VL  - 8
IS  - 7
SN  - 2076-3263

AB  - The application of deep learning, specifically deep convolutional neural networks (DCNNs), to the classification of remotely-sensed imagery of natural landscapes has the potential to greatly assist in the analysis and interpretation of geomorphic processes. However, the general usefulness of deep learning applied to conventional photographic imagery at a landscape scale is, at yet, largely unproven. If DCNN-based image classification is to gain wider application and acceptance within the geoscience community, demonstrable successes need to be coupled with accessible tools to retrain deep neural networks to discriminate landforms and land uses in landscape imagery. Here, we present an efficient approach to train/apply DCNNs with/on sets of photographic images, using a powerful graphical method called a conditional random field (CRF), to generate DCNN training and testing data using minimal manual supervision. We apply the method to several sets of images of natural landscapes, acquired from satellites, aircraft, unmanned aerial vehicles, and fixed camera installations. We synthesize our findings to examine the general effectiveness of transfer learning to landscape-scale image classification. Finally, we show how DCNN predictions on small regions of images might be used in conjunction with a CRF for highly accurate pixel-level classification of images.
KW  - image classification
KW  - image segmentation
KW  - land use
KW  - land cover
KW  - landforms
KW  - deep learning
KW  - machine learning
KW  - unmanned aerial systems
KW  - aerial imagery
KW  - remote sensing
DO  - 10.3390/geosciences8070244
TY  - EJOU
AU  - Näsi, Roope
AU  - Viljanen, Niko
AU  - Kaivosoja, Jere
AU  - Alhonoja, Katja
AU  - Hakala, Teemu
AU  - Markelin, Lauri
AU  - Honkavaara, Eija
TI  - Estimating Biomass and Nitrogen Amount of Barley and Grass Using UAV and Aircraft Based Spectral and Photogrammetric 3D Features
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 7
SN  - 2072-4292

AB  - The timely estimation of crop biomass and nitrogen content is a crucial step in various tasks in precision agriculture, for example in fertilization optimization. Remote sensing using drones and aircrafts offers a feasible tool to carry out this task. Our objective was to develop and assess a methodology for crop biomass and nitrogen estimation, integrating spectral and 3D features that can be extracted using airborne miniaturized multispectral, hyperspectral and colour (RGB) cameras. We used the Random Forest (RF) as the estimator, and in addition Simple Linear Regression (SLR) was used to validate the consistency of the RF results. The method was assessed with empirical datasets captured of a barley field and a grass silage trial site using a hyperspectral camera based on the Fabry-P&eacute;rot interferometer (FPI) and a regular RGB camera onboard a drone and an aircraft. Agricultural reference measurements included fresh yield (FY), dry matter yield (DMY) and amount of nitrogen. In DMY estimation of barley, the Pearson Correlation Coefficient (PCC) and the normalized Root Mean Square Error (RMSE%) were at best 0.95% and 33.2%, respectively; and in the grass DMY estimation, the best results were 0.79% and 1.9%, respectively. In the nitrogen amount estimations of barley, the PCC and RMSE% were at best 0.97% and 21.6%, respectively. In the biomass estimation, the best results were obtained when integrating hyperspectral and 3D features, but the integration of RGB images and 3D features also provided results that were almost as good. In nitrogen content estimation, the hyperspectral camera gave the best results. We concluded that the integration of spectral and high spatial resolution 3D features and radiometric calibration was necessary to optimize the accuracy.
KW  - hyperspectral
KW  - photogrammetry
KW  - UAV
KW  - drone
KW  - machine learning
KW  - random forest
KW  - regression
KW  - precision agriculture
KW  - biomass
KW  - nitrogen
DO  - 10.3390/rs10071082
TY  - EJOU
AU  - Bachmann, Daniel
AU  - Weichert, Frank
AU  - Rinkenauer, Gerhard
TI  - Review of Three-Dimensional Human-Computer Interaction with Focus on the Leap Motion Controller
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Modern hardware and software development has led to an evolution of user interfaces from command-line to natural user interfaces for virtual immersive environments. Gestures imitating real-world interaction tasks increasingly replace classical two-dimensional interfaces based on Windows/Icons/Menus/Pointers (WIMP) or touch metaphors. Thus, the purpose of this paper is to survey the state-of-the-art Human-Computer Interaction (HCI) techniques with a focus on the special field of three-dimensional interaction. This includes an overview of currently available interaction devices, their applications of usage and underlying methods for gesture design and recognition. Focus is on interfaces based on the Leap Motion Controller (LMC) and corresponding methods of gesture design and recognition. Further, a review of evaluation methods for the proposed natural user interfaces is given.
KW  - human-computer interaction
KW  - contact-free input devices
KW  - three-dimensional interaction
KW  - natural user interfaces
KW  - leap motion controller
DO  - 10.3390/s18072194
TY  - EJOU
AU  - Petrellis, Nikos
TI  - A Review of Image Processing Techniques Common in Human and Plant Disease Diagnosis
T2  - Symmetry

PY  - 2018
VL  - 10
IS  - 7
SN  - 2073-8994

AB  - Image processing has been extensively used in various (human, animal, plant) disease diagnosis approaches, assisting experts to select the right treatment. It has been applied to both images captured from cameras of visible light and from equipment that captures information in invisible wavelengths (magnetic/ultrasonic sensors, microscopes, etc.). In most of the referenced diagnosis applications, the image is enhanced by various filtering methods and segmentation follows isolating the regions of interest. Classification of the input image is performed at the final stage. The disease diagnosis approaches based on these steps and the common methods are described. The features extracted from a plant/skin disease diagnosis framework developed by the author are used here to demonstrate various techniques adopted in the literature. The various metrics along with the available experimental conditions and results presented in the referenced approaches are also discussed. The accuracy achieved in the diagnosis methods that are based on image processing is often higher than 90%. The motivation for this review is to highlight the most common and efficient methods that have been employed in various disease diagnosis approaches and suggest how they can be used in similar or different applications.
KW  - image processing
KW  - disease diagnosis
KW  - plant disease
KW  - segmentation
KW  - classification
KW  - image filtering
DO  - 10.3390/sym10070270
TY  - EJOU
AU  - De Oliveira, Diulhio C.
AU  - Wehrmeister, Marco A.
TI  - Using Deep Learning and Low-Cost RGB and Thermal Cameras to Detect Pedestrians in Aerial Images Captured by Multirotor UAV
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - The use of Unmanned Aerial Vehicles (UAV) has been increasing over the last few years in many sorts of applications due mainly to the decreasing cost of this technology. One can see the use of the UAV in several civilian applications such as surveillance and search and rescue. Automatic detection of pedestrians in aerial images is a challenging task. The computing vision system must deal with many sources of variability in the aerial images captured with the UAV, e.g., low-resolution images of pedestrians, images captured at distinct angles due to the degrees of freedom that a UAV can move, the camera platform possibly experiencing some instability while the UAV flies, among others. In this work, we created and evaluated different implementations of Pattern Recognition Systems (PRS) aiming at the automatic detection of pedestrians in aerial images captured with multirotor UAV. The main goal is to assess the feasibility and suitability of distinct PRS implementations running on top of low-cost computing platforms, e.g., single-board computers such as the Raspberry Pi or regular laptops without a GPU. For that, we used four machine learning techniques in the feature extraction and classification steps, namely Haar cascade, LBP cascade, HOG + SVM and Convolutional Neural Networks (CNN). In order to improve the system performance (especially the processing time) and also to decrease the rate of false alarms, we applied the Saliency Map (SM) and Thermal Image Processing (TIP) within the segmentation and detection steps of the PRS. The classification results show the CNN to be the best technique with 99.7% accuracy, followed by HOG + SVM with 92.3%. In situations of partial occlusion, the CNN showed 71.1% sensitivity, which can be considered a good result in comparison with the current state-of-the-art, since part of the original image data is missing. As demonstrated in the experiments, by combining TIP with CNN, the PRS can process more than two frames per second (fps), whereas the PRS that combines TIP with HOG + SVM was able to process 100 fps. It is important to mention that our experiments show that a trade-off analysis must be performed during the design of a pedestrian detection PRS. The faster implementations lead to a decrease in the PRS accuracy. For instance, by using HOG + SVM with TIP, the PRS presented the best performance results, but the obtained accuracy was 35 percentage points lower than the CNN. The obtained results indicate that the best detection technique (i.e., the CNN) requires more computational resources to decrease the PRS computation time. Therefore, this work shows and discusses the pros/cons of each technique and trade-off situations, and hence, one can use such an analysis to improve and tailor the design of a PRS to detect pedestrians in aerial images.
KW  - pedestrian detection
KW  - aerial images
KW  - Unmanned Aerial Vehicle (UAV)
KW  - thermal camera
KW  - deep learning
KW  - convolutional neural network
KW  - pattern recognition system
KW  - performance assessment
DO  - 10.3390/s18072244
TY  - EJOU
AU  - Xiang, Xuezhi
AU  - Lv, Ning
AU  - Guo, Xinli
AU  - Wang, Shuai
AU  - El Saddik, Abdulmotaleb
TI  - Engineering Vehicles Detection Based on Modified Faster R-CNN for Power Grid Surveillance
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Engineering vehicles intrusion detection is a key problem for the security of power grid operation, which can warn of the regional invasion and prevent external damage from architectural construction. In this paper, we propose an intelligent surveillance method based on the framework of Faster R-CNN for locating and identifying the invading engineering vehicles. In our detection task, the type of the objects is varied and the monitoring scene is large and complex. In order to solve these challenging problems, we modify the network structure of the object detection model by adjusting the position of the ROI pooling layer. The convolutional layer is added to the feature classification part to improve the accuracy of the detection model. We verify that increasing the depth of the feature classification part is effective for detecting engineering vehicles in realistic transmission lines corridors. We also collect plenty of scene images taken from the monitor site and label the objects to create a fine-tuned dataset. We train the modified deep detection model based on the technology of transfer learning and conduct training and test on the newly labeled dataset. Experimental results show that the proposed intelligent surveillance method can detect engineering vehicles with high accuracy and a low false alarm rate, which can be used for the early warning of power grid surveillance.
KW  - power grid surveillance
KW  - external damage
KW  - engineering vehicles
KW  - faster R-CNN
KW  - transfer learning
DO  - 10.3390/s18072258
TY  - EJOU
AU  - Feduck, Corey
AU  - McDermid, Gregory J.
AU  - Castilla, Guillermo
TI  - Detection of Coniferous Seedlings in UAV Imagery
T2  - Forests

PY  - 2018
VL  - 9
IS  - 7
SN  - 1999-4907

AB  - Rapid assessment of forest regeneration using unmanned aerial vehicles (UAVs) is likely to decrease the cost of establishment surveys in a variety of resource industries. This research tests the feasibility of using UAVs to rapidly identify coniferous seedlings in replanted forest-harvest areas in Alberta, Canada. In developing our protocols, we gave special consideration to creating a workflow that could perform in an operational context, avoiding comprehensive wall-to-wall surveys and complex photogrammetric processing in favor of an efficient sampling-based approach, consumer-grade cameras, and straightforward image handling. Using simple spectral decision rules from a red, green, and blue (RGB) camera, we documented a seedling detection rate of 75.8 % (n = 149), on the basis of independent test data. While moderate imbalances between the omission and commission errors suggest that our workflow has a tendency to underestimate the seedling density in a harvest block, the plot-level associations with ground surveys were very high (Pearson&rsquo;s r = 0.98; n = 14). Our results were promising enough to suggest that UAVs can be used to detect coniferous seedlings in an operational capacity with standard RGB cameras alone, although our workflow relies on seasonal leaf-off windows where seedlings are visible and spectrally distinct from their surroundings. In addition, the differential errors between the pine seedlings and spruce seedlings suggest that operational workflows could benefit from multiple decision rules designed to handle diversity in species and other sources of spectral variability.
KW  - unmanned aerial vehicles
KW  - seedling detection
KW  - forest regeneration
KW  - reforestation
KW  - establishment survey
KW  - machine learning
KW  - multispectral classification
DO  - 10.3390/f9070432
TY  - EJOU
AU  - Gopalakrishnan, Kasthurirangan
TI  - Deep Learning in Data-Driven Pavement Image Analysis and Automated Distress Detection: A Review
T2  - Data

PY  - 2018
VL  - 3
IS  - 3
SN  - 2306-5729

AB  - Deep learning, more specifically deep convolutional neural networks, is fast becoming a popular choice for computer vision-based automated pavement distress detection. While pavement image analysis has been extensively researched over the past three decades or so, recent ground-breaking achievements of deep learning algorithms in the areas of machine translation, speech recognition, and computer vision has sparked interest in the application of deep learning to automated detection of distresses in pavement images. This paper provides a narrative review of recently published studies in this field, highlighting the current achievements and challenges. A comparison of the deep learning software frameworks, network architecture, hyper-parameters employed by each study, and crack detection performance is provided, which is expected to provide a good foundation for driving further research on this important topic in the context of smart pavement or asset management systems. The review concludes with potential avenues for future research; especially in the application of deep learning to not only detect, but also characterize the type, extent, and severity of distresses from 2D and 3D pavement images.
KW  - pavement cracking
KW  - pavement management
KW  - pavement imaging
KW  - 3D image
KW  - deep learning
KW  - TensorFlow
KW  - deep convolutional neural networks
DO  - 10.3390/data3030028
TY  - EJOU
AU  - Chabot, Dominique
AU  - Dillon, Christopher
AU  - Shemrock, Adam
AU  - Weissflog, Nicholas
AU  - Sager, Eric P. S.
TI  - An Object-Based Image Analysis Workflow for Monitoring Shallow-Water Aquatic Vegetation in Multispectral Drone Imagery
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 8
SN  - 2220-9964

AB  - High-resolution drone aerial surveys combined with object-based image analysis are transforming our capacity to monitor and manage aquatic vegetation in an era of invasive species. To better exploit the potential of these technologies, there is a need to develop more efficient and accessible analysis workflows and focus more efforts on the distinct challenge of mapping submerged vegetation. We present a straightforward workflow developed to monitor emergent and submerged invasive water soldier (Stratiotes aloides) in shallow waters of the Trent-Severn Waterway in Ontario, Canada. The main elements of the workflow are: (1) collection of radiometrically calibrated multispectral imagery including a near-infrared band; (2) multistage segmentation of the imagery involving an initial separation of above-water from submerged features; and (3) automated classification of features with a supervised machine-learning classifier. The approach yielded excellent classification accuracy for emergent features (overall accuracy = 92%; kappa = 88%; water soldier producer&rsquo;s accuracy = 92%; user&rsquo;s accuracy = 91%) and good accuracy for submerged features (overall accuracy = 84%; kappa = 75%; water soldier producer&rsquo;s accuracy = 71%; user&rsquo;s accuracy = 84%). The workflow employs off-the-shelf graphical software tools requiring no programming or coding, and could therefore be used by anyone with basic GIS and image analysis skills for a potentially wide variety of aquatic vegetation monitoring operations.
KW  - environmental monitoring
KW  - freshwater ecosystems
KW  - OBIA
KW  - random forests
KW  - remote sensing
KW  - rivers
KW  - unmanned aircraft
KW  - UAS
KW  - UAV
KW  - wetlands
DO  - 10.3390/ijgi7080294
TY  - EJOU
AU  - Feng, Chen-Chieh
AU  - Guo, Zhou
TI  - Automating Parameter Learning for Classifying Terrestrial LiDAR Point Cloud Using 2D Land Cover Maps
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - The automating classification of point clouds capturing urban scenes is critical for supporting applications that demand three-dimensional (3D) models. Achieving this goal, however, is met with challenges because of the varying densities of the point clouds and the complexity of the 3D data. In order to increase the level of automation in the point cloud classification, this study proposes a segment-based parameter learning method that incorporates a two-dimensional (2D) land cover map, in which a strategy of fusing the 2D land cover map and the 3D points is first adopted to create labelled samples, and a formalized procedure is then implemented to automatically learn the following parameters of point cloud classification: the optimal scale of the neighborhood for segmentation, optimal feature set, and the training classifier. It comprises four main steps, namely: (1) point cloud segmentation; (2) sample selection; (3) optimal feature set selection; and (4) point cloud classification. Three datasets containing the point cloud data were used in this study to validate the efficiency of the proposed method. The first two datasets cover two areas of the National University of Singapore (NUS) campus while the third dataset is a widely used benchmark point cloud dataset of Oakland, Pennsylvania. The classification parameters were learned from the first dataset consisting of a terrestrial laser-scanning data and a 2D land cover map, and were subsequently used to classify both of the NUS datasets. The evaluation of the classification results showed overall accuracies of 94.07% and 91.13%, respectively, indicating that the transition of the knowledge learned from one dataset to another was satisfactory. The classification of the Oakland dataset achieved an overall accuracy of 97.08%, which further verified the transferability of the proposed approach. An experiment of the point-based classification was also conducted on the first dataset and the result was compared to that of the segment-based classification. The evaluation revealed that the overall accuracy of the segment-based classification is indeed higher than that of the point-based classification, demonstrating the advantage of the segment-based approaches.
KW  - point cloud classification
KW  - 2D map
KW  - segment-based
KW  - neighborhood scale
KW  - sample selection
DO  - 10.3390/rs10081192
TY  - EJOU
AU  - Zhang, Weixing
AU  - Witharana, Chandi
AU  - Li, Weidong
AU  - Zhang, Chuanrong
AU  - Li, Xiaojiang
AU  - Parent, Jason
TI  - Using Deep Learning to Identify Utility Poles with Crossarms and Estimate Their Locations from Google Street View Images
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - Traditional methods of detecting and mapping utility poles are inefficient and costly because of the demand for visual interpretation with quality data sources or intense field inspection. The advent of deep learning for object detection provides an opportunity for detecting utility poles from side-view optical images. In this study, we proposed using a deep learning-based method for automatically mapping roadside utility poles with crossarms (UPCs) from Google Street View (GSV) images. The method combines the state-of-the-art DL object detection algorithm (i.e., the RetinaNet object detection algorithm) and a modified brute-force-based line-of-bearing (LOB, a LOB stands for the ray towards the location of the target [UPC at here] from the original location of the sensor [GSV mobile platform]) measurement method to estimate the locations of detected roadside UPCs from GSV. Experimental results indicate that: (1) both the average precision (AP) and the overall accuracy (OA) are around 0.78 when the intersection-over-union (IoU) threshold is greater than 0.3, based on the testing of 500 GSV images with a total number of 937 objects; and (2) around 2.6%, 47%, and 79% of estimated locations of utility poles are within 1 m, 5 m, and 10 m buffer zones, respectively, around the referenced locations of utility poles. In general, this study indicates that even in a complex background, most utility poles can be detected with the use of DL, and the LOB measurement method can estimate the locations of most UPCs.
KW  - deep learning
KW  - utility pole
KW  - infrastructure mapping
KW  - Google Street View
KW  - line-of-bearing measurement
KW  - object detection
DO  - 10.3390/s18082484
TY  - EJOU
AU  - Zhao, Qi
AU  - Zhang, Boxue
AU  - Lyu, Shuchang
AU  - Zhang, Hong
AU  - Sun, Daniel
AU  - Li, Guoqiang
AU  - Feng, Wenquan
TI  - A CNN-SIFT Hybrid Pedestrian Navigation Method Based on First-Person Vision
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - The emergence of new wearable technologies, such as action cameras and smart glasses, has driven the use of the first-person perspective in computer applications. This field is now attracting the attention and investment of researchers aiming to develop methods to process first-person vision (FPV) video. The current approaches present particular combinations of different image features and quantitative methods to accomplish specific objectives, such as object detection, activity recognition, user&ndash;machine interaction, etc. FPV-based navigation is necessary in some special areas, where Global Position System (GPS) or other radio-wave strength methods are blocked, and is especially helpful for visually impaired people. In this paper, we propose a hybrid structure with a convolutional neural network (CNN) and local image features to achieve FPV pedestrian navigation. A novel end-to-end trainable global pooling operator, called AlphaMEX, has been designed to improve the scene classification accuracy of CNNs. A scale-invariant feature transform (SIFT)-based tracking algorithm is employed for movement estimation and trajectory tracking of the person through each frame of FPV images. Experimental results demonstrate the effectiveness of the proposed method. The top-1 error rate of the proposed AlphaMEX-ResNet outperforms the original ResNet (k = 12) by 1.7% on the ImageNet dataset. The CNN-SIFT hybrid pedestrian navigation system reaches 0.57 m average absolute error, which is an adequate accuracy for pedestrian navigation. Both positions and movements can be well estimated by the proposed pedestrian navigation algorithm with a single wearable camera.
KW  - navigation
KW  - first-person vision
KW  - CNN
KW  - SIFT
KW  - movement estimation
DO  - 10.3390/rs10081229
TY  - EJOU
AU  - Gray, Patrick C.
AU  - Ridge, Justin T.
AU  - Poulin, Sarah K.
AU  - Seymour, Alexander C.
AU  - Schwantes, Amanda M.
AU  - Swenson, Jennifer J.
AU  - Johnston, David W.
TI  - Integrating Drone Imagery into High Resolution Satellite Remote Sensing Assessments of Estuarine Environments
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - Very high-resolution satellite imagery (&le;5 m resolution) has become available on a spatial and temporal scale appropriate for dynamic wetland management and conservation across large areas. Estuarine wetlands have the potential to be mapped at a detailed habitat scale with a frequency that allows immediate monitoring after storms, in response to human disturbances, and in the face of sea-level rise. Yet mapping requires significant fieldwork to run modern classification algorithms and estuarine environments can be difficult to access and are environmentally sensitive. Recent advances in unoccupied aircraft systems (UAS, or drones), coupled with their increased availability, present a solution. UAS can cover a study site with ultra-high resolution (&lt;5 cm) imagery allowing visual validation. In this study we used UAS imagery to assist training a Support Vector Machine to classify WorldView-3 and RapidEye satellite imagery of the Rachel Carson Reserve in North Carolina, USA. UAS and field-based accuracy assessments were employed for comparison across validation methods. We created and examined an array of indices and layers including texture, NDVI, and a LiDAR DEM. Our results demonstrate classification accuracy on par with previous extensive fieldwork campaigns (93% UAS and 93% field for WorldView-3; 92% UAS and 87% field for RapidEye). Examining change between 2004 and 2017, we found drastic shoreline change but general stability of emergent wetlands. Both WorldView-3 and RapidEye were found to be valuable sources of imagery for habitat classification with the main tradeoff being WorldView&rsquo;s fine spatial resolution versus RapidEye&rsquo;s temporal frequency. We conclude that UAS can be highly effective in training and validating satellite imagery.
KW  - drones
KW  - unoccupied aircraft systems
KW  - RapidEye
KW  - WorldView-3
KW  - estuarine
KW  - wetland
KW  - change detection
KW  - LiDAR
KW  - NERR
KW  - habitat mapping
DO  - 10.3390/rs10081257
TY  - EJOU
AU  - Gallo, Mariano
AU  - De Luca, Giuseppina
TI  - Spatial Extension of Road Traffic Sensor Data with Artificial Neural Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - This paper proposes a method for estimating traffic flows on some links of a road network knowing the data on other links that are monitored with sensors. In this way, it is possible to obtain more information on traffic conditions without increasing the number of monitored links. The proposed method is based on artificial neural networks (ANNs), wherein the input data are the traffic flows on some monitored road links and the output data are the traffic flows on some unmonitored links. We have implemented and tested several single-layer feed-forward ANNs that differ in the number of neurons and the method of generating datasets for training. The proposed ANNs were trained with a supervised learning approach where input and output example datasets were generated through traffic simulation techniques. The proposed method was tested on a real-scale network and gave very good results if the travel demand patterns were known and used for generating example datasets, and promising results if the demand patterns were not considered in the procedure. Numerical results have underlined that the ANNs with few neurons were more effective than the ones with many neurons in this specific problem.
KW  - traffic sensors
KW  - smart roads
KW  - artificial neural networks
KW  - ITS
DO  - 10.3390/s18082640
TY  - EJOU
AU  - Liakos, Konstantinos G.
AU  - Busato, Patrizia
AU  - Moshou, Dimitrios
AU  - Pearson, Simon
AU  - Bochtis, Dionysis
TI  - Machine Learning in Agriculture: A Review
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.
KW  - crop management
KW  - water management
KW  - soil management
KW  - livestock management
KW  - artificial intelligence
KW  - planning
KW  - precision agriculture
DO  - 10.3390/s18082674
TY  - EJOU
AU  - Hidayat, Sarip
AU  - MATSUOKA, Masayuki
AU  - Baja, Sumbangan
AU  - Rampisela, Dorothea A.
TI  - Object-Based Image Analysis for Sago Palm Classification: The Most Important Features from High-Resolution Satellite Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - Sago palm (Metroxylon sagu) is a palm tree species originating in Indonesia. In the future, this starch-producing tree will play an important role in food security and biodiversity. Local governments have begun to emphasize the sustainable development of sago palm plantations; therefore, they require near-real-time geospatial information on palm stands. We developed a semi-automated classification scheme for mapping sago palm using machine learning within an object-based image analysis framework with Pleiades-1A imagery. In addition to spectral information, arithmetic, geometric, and textural features were employed to enhance the classification accuracy. Recursive feature elimination was applied to samples to rank the importance of 26 input features. A support vector machine (SVM) was used to perform classifications and resulted in the highest overall accuracy of 85.00% after inclusion of the eight most important features, including three spectral features, three arithmetic features, and two textural features. The SVM classifier showed normal fitting up to the eighth most important feature. According to the McNemar test results, using the top seven to 14 features provided a better classification accuracy. The significance of this research is the revelation of the most important features in recognizing sago palm among other similar tree species.
KW  - sago palm
KW  - OBIA
KW  - machine learning
KW  - textural features
KW  - image segmentation
KW  - feature selection
KW  - classification
DO  - 10.3390/rs10081319
TY  - EJOU
AU  - Hu, Jie
AU  - Wu, Zhongli
AU  - Qin, Xiongzhen
AU  - Geng, Huangzheng
AU  - Gao, Zhangbin
TI  - An Extended Kalman Filter and Back Propagation Neural Network Algorithm Positioning Method Based on Anti-lock Brake Sensor and Global Navigation Satellite System Information
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - Telematics box (T-Box) chip-level Global Navigation Satellite System (GNSS) receiver modules usually suffer from GNSS information failure or noise in urban environments. In order to resolve this issue, this paper presents a real-time positioning method for Extended Kalman Filter (EKF) and Back Propagation Neural Network (BPNN) algorithms based on Antilock Brake System (ABS) sensor and GNSS information. Experiments were performed using an assembly in the vehicle with a T-Box. The T-Box firstly use automotive kinematical Pre-EKF to fuse the four wheel speed, yaw rate and steering wheel angle data from the ABS sensor to obtain a more accurate vehicle speed and heading angle velocity. In order to reduce the noise of the GNSS information, After-EKF fusion vehicle speed, heading angle velocity and GNSS data were used and low-noise positioning data were obtained. The heading angle speed error is extracted as target and part of low-noise positioning data were used as input for training a BPNN model. When the positioning is invalid, the well-trained BPNN corrected heading angle velocity output and vehicle speed add the synthesized relative displacement to the previous absolute position to realize a new position. With the data of high-precision real-time kinematic differential positioning equipment as the reference, the use of the dual EKF can reduce the noise range of GNSS information and concentrate good-positioning signals of the road within 5 m (i.e. the positioning status is valid). When the GNSS information was shielded (making the positioning status invalid), and the previous data was regarded as a training sample, it is found that the vehicle achieved 15 minutes position without GNSS information on the recycling line. The results indicated this new position method can reduce the vehicle positioning noise when GNSS information is valid and determine the position during long periods of invalid GNSS information.
KW  - ABS sensor
KW  - neural network
KW  - EKF
KW  - GNSS
KW  - T-Box
DO  - 10.3390/s18092753
TY  - EJOU
AU  - Liu, Shuo
AU  - Ding, Wenrui
AU  - Liu, Chunhui
AU  - Liu, Yu
AU  - Wang, Yufeng
AU  - Li, Hongguang
TI  - ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved.
KW  - CNN
KW  - deep learning
KW  - edge loss reinforced network
KW  - remote sensing
KW  - semantic segmentation
DO  - 10.3390/rs10091339
TY  - EJOU
AU  - Chen, Ting
AU  - Pennisi, Andrea
AU  - Li, Zhi
AU  - Zhang, Yanning
AU  - Sahli, Hichem
TI  - A Hierarchical Association Framework for Multi-Object Tracking in Airborne Videos
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - Multi-Object Tracking (MOT) in airborne videos is a challenging problem due to the uncertain airborne vehicle motion, vibrations of the mounted camera, unreliable detections, changes of size, appearance and motion of the moving objects and occlusions caused by the interaction between moving and static objects in the scene. To deal with these problems, this work proposes a four-stage hierarchical association framework for multiple object tracking in airborne video. The proposed framework combines Data Association-based Tracking (DAT) methods and target tracking using a compressive tracking approach, to robustly track objects in complex airborne surveillance scenes. In each association stage, different sets of tracklets and detections are associated to efficiently handle local tracklet generation, local trajectory construction, global drifting tracklet correction and global fragmented tracklet linking. Experiments with challenging airborne videos show significant tracking improvement compared to existing state-of-the-art methods.
KW  - multiple object tracking
KW  - airborne video
KW  - tracklet confidence
KW  - hierarchical association framework
DO  - 10.3390/rs10091347
TY  - EJOU
AU  - Kung, Chien-Chun
TI  - Study on Consulting Air Combat Simulation of Cluster UAV Based on Mixed Parallel Computing Framework of Graphics Processing Unit
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 9
SN  - 2079-9292

AB  - This paper combines matrix game theory with negotiating theory and uses U-solution to study the framework of the consulting air combat of UAV cluster. The processes to determine the optimal strategy in this paper follow three points: first, the UAV cluster are grouped into fleets; second, the best paring for the joint operations of the fleet member with the enemy fleet members are calculated; thirdly, consultations within the fleet are conducted to discuss the problems of optimal tactic, roles of main/assistance, and situational assessment within the fleet. In order to improve the computing efficiency of the framework, this article explores the use of the NVIDIA graphics processor programmed through MATLAB mixed C++/CUDA toolkit to accelerate the calculations of equations of motion of unmanned aerial vehicles, the prediction of superiority values and U values, computations of consultation, the evaluation of situational assessment and the optimal strategies. The effectiveness evaluation of GPGPU and CPU can be observed by the simulation results. When the number of team air combat is small, the CPU alone has better efficiency; however, when the number of air combat clusters exceeds 6 to 6, the architecture presented in this article can provide higher performance improvements and run faster than optimized CPU-only code.
KW  - GPGPU
KW  - MATLAB/CUDA
KW  - matrix game
KW  - consulting air combat
KW  - UAV cluster
KW  - maneuver decision-making
DO  - 10.3390/electronics7090160
TY  - EJOU
AU  - Lee, Jungshin
AU  - Bang, Hyochoong
TI  - A Robust Terrain Aided Navigation Using the Rao-Blackwellized Particle Filter Trained by Long Short-Term Memory Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - Terrain-aided navigation (TAN) is a technology that estimates the position of the vehicle by comparing the altitude measured by an altimeter and height from the digital elevation model (DEM). The particle filter (PF)-based TAN has been commonly used to obtain stable real-time navigation solutions in cases where the unmanned aerial vehicle (UAV) operates at a high altitude. Even though TAN performs well on rough and unique terrains, its performance degrades in flat and repetitive terrains. In particular, in the case of PF-based TAN, there has been no verified technique for deciding its terrain validity. Therefore, this study designed a Rao-Blackwellized PF (RBPF)-based TAN, used long short-term memory (LSTM) networks to endure flat and repetitive terrains, and trained the noise covariances and measurement model of RBPF. LSTM is a modified recurrent neural network (RNN), which is an artificial neural network that recognizes patterns from time series data. Using this, this study tuned the noise covariances and measurement model of RBPF to minimize the navigation errors in various flight trajectories. This paper designed a TAN algorithm based on combining RBPF and LSTM and confirmed that it can enable a more precise navigation performance than conventional RBPF based TAN through simulations.
KW  - terrain-aided navigation (TAN)
KW  - Rao-Blackwellized particle filter (RBPF)
KW  - long short-term memory (LSTM)
KW  - terrain validity check
KW  - digital elevation model (DEM)
KW  - inertial navigation system (INS)
DO  - 10.3390/s18092886
TY  - EJOU
AU  - Sa, Inkyu
AU  - Popović, Marija
AU  - Khanna, Raghav
AU  - Chen, Zetao
AU  - Lottes, Philipp
AU  - Liebisch, Frank
AU  - Nieto, Juan
AU  - Stachniss, Cyrill
AU  - Walter, Achim
AU  - Siegwart, Roland
TI  - WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB (red, green, and blue) inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.
KW  - precision farming
KW  - weed management
KW  - multispectral imaging
KW  - semantic segmentation
KW  - deep neural network
KW  - unmanned aerial vehicle
KW  - remote sensing
DO  - 10.3390/rs10091423
TY  - EJOU
AU  - Choi, Jongseong
AU  - Yeum, Chul M.
AU  - Dyke, Shirley J.
AU  - Jahanshahi, Mohammad R.
TI  - Computer-Aided Approach for Rapid Post-Event Visual Evaluation of a Building Façade
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - After a disaster strikes an urban area, damage to the fa&ccedil;ades of a building may produce dangerous falling hazards that jeopardize pedestrians and vehicles. Thus, building fa&ccedil;ades must be rapidly inspected to prevent potential loss of life and property damage. Harnessing the capacity to use new vision sensors and associated sensing platforms, such as unmanned aerial vehicles (UAVs) would expedite this process and alleviate spatial and temporal limitations typically associated with human-based inspection in high-rise buildings. In this paper, we have developed an approach to perform rapid and accurate visual inspection of building fa&ccedil;ades using images collected from UAVs. An orthophoto corresponding to any reasonably flat region on the building (e.g., a fa&ccedil;ade or building side) is automatically constructed using a structure-from-motion (SfM) technique, followed by image stitching and blending. Based on the geometric relationship between the collected images and the constructed orthophoto, high-resolution region-of-interest are automatically extracted from the collected images, enabling efficient visual inspection. We successfully demonstrate the capabilities of the technique using an abandoned building of which a fa&ccedil;ade has damaged building components (e.g., window panes or external drainage pipes).
KW  - post-event visual evaluation
KW  - image localization
KW  - orthophoto generation
KW  - unmanned aerial vehicle
DO  - 10.3390/s18093017
TY  - EJOU
AU  - Xu, Yongyang
AU  - Xie, Zhong
AU  - Feng, Yaxing
AU  - Chen, Zhanlong
TI  - Road Extraction from High-Resolution Remote Sensing Imagery Using Deep Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - The road network plays an important role in the modern traffic system; as development occurs, the road structure changes frequently. Owing to the advancements in the field of high-resolution remote sensing, and the success of semantic segmentation success using deep learning in computer version, extracting the road network from high-resolution remote sensing imagery is becoming increasingly popular, and has become a new tool to update the geospatial database. Considering that the training dataset of the deep convolutional neural network will be clipped to a fixed size, which lead to the roads run through each sample, and that different kinds of road types have different widths, this work provides a segmentation model that was designed based on densely connected convolutional networks (DenseNet) and introduces the local and global attention units. The aim of this work is to propose a novel road extraction method that can efficiently extract the road network from remote sensing imagery with local and global information. A dataset from Google Earth was used to validate the method, and experiments showed that the proposed deep convolutional neural network can extract the road network accurately and effectively. This method also achieves a harmonic mean of precision and recall higher than other machine learning and deep learning methods.
KW  - road network extraction
KW  - deep learning
KW  - pyramid attention
KW  - global attention
KW  - high resolution
DO  - 10.3390/rs10091461
TY  - EJOU
AU  - Duarte-Carvajalino, Julio M.
AU  - Alzate, Diego F.
AU  - Ramirez, Andrés A.
AU  - Santa-Sepulveda, Juan D.
AU  - Fajardo-Rojas, Alexandra E.
AU  - Soto-Suárez, Mauricio
TI  - Evaluating Late Blight Severity in Potato Crops Using Unmanned Aerial Vehicles and Machine Learning Algorithms
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - This work presents quantitative prediction of severity of the disease caused by Phytophthora infestans in potato crops using machine learning algorithms such as multilayer perceptron, deep learning convolutional neural networks, support vector regression, and random forests. The machine learning algorithms are trained using datasets extracted from multispectral data captured at the canopy level with an unmanned aerial vehicle, carrying an inexpensive digital camera. The results indicate that deep learning convolutional neural networks, random forests and multilayer perceptron using band differences can predict the level of Phytophthora infestans affectation on potato crops with acceptable accuracy.
KW  - UAV
KW  - remote sensing
KW  - Phytophthora infestans
KW  - multispectral
KW  - neural networks
KW  - deep learning
DO  - 10.3390/rs10101513
