TY  - EJOU
AU  - Wu, Zhangnan
AU  - Chen, Yajun
AU  - Zhao, Bo
AU  - Kang, Xiaobing
AU  - Ding, Yuanyuan
TI  - Review of Weed Detection Methods Based on Computer Vision
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 11
SN  - 1424-8220

AB  - Weeds are one of the most important factors affecting agricultural production. The waste and pollution of farmland ecological environment caused by full-coverage chemical herbicide spraying are becoming increasingly evident. With the continuous improvement in the agricultural production level, accurately distinguishing crops from weeds and achieving precise spraying only for weeds are important. However, precise spraying depends on accurately identifying and locating weeds and crops. In recent years, some scholars have used various computer vision methods to achieve this purpose. This review elaborates the two aspects of using traditional image-processing methods and deep learning-based methods to solve weed detection problems. It provides an overview of various methods for weed detection in recent years, analyzes the advantages and disadvantages of existing methods, and introduces several related plant leaves, weed datasets, and weeding machinery. Lastly, the problems and difficulties of the existing weed detection methods are analyzed, and the development trend of future research is prospected.
KW  - weed detection
KW  - computer vision
KW  - image processing
KW  - deep learning
KW  - machine learning
DO  - 10.3390/s21113647
ER  -
TY  - EJOU
AU  - Khan, Noman
AU  - Muhammad, Khan
AU  - Hussain, Tanveer
AU  - Nasir, Mansoor
AU  - Munsif, Muhammad
AU  - Imran, Ali S.
AU  - Sajjad, Muhammad
TI  - An Adaptive Game-Based Learning Strategy for Children Road Safety Education and Practice in Virtual Space
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 11
SN  - 1424-8220

AB  - Virtual reality (VR) has been widely used as a tool to assist people by letting them learn and simulate situations that are too dangerous and risky to practice in real life, and one of these is road safety training for children. Traditional video- and presentation-based road safety training has average output results as it lacks physical practice and the involvement of children during training, without any practical testing examination to check the learned abilities of a child before their exposure to real-world environments. Therefore, in this paper, we propose a 3D realistic open-ended VR and Kinect sensor-based training setup using the Unity game engine, wherein children are educated and involved in road safety exercises. The proposed system applies the concepts of VR in a game-like setting to let the children learn about traffic rules and practice them in their homes without any risk of being exposed to the outside environment. Thus, with our interactive and immersive training environment, we aim to minimize road accidents involving children and contribute to the generic domain of healthcare. Furthermore, the proposed framework evaluates the overall performance of the students in a virtual environment (VE) to develop their road-awareness skills. To ensure safety, the proposed system has an extra examination layer for children’s abilities evaluation, whereby a child is considered fit for real-world practice in cases where they fulfil certain criteria by achieving set scores. To show the robustness and stability of the proposed system, we conduct four types of subjective activities by involving a group of ten students with average grades in their classes. The experimental results show the positive effect of the proposed system in improving the road crossing behavior of the children.
KW  - education
KW  - human–computer interaction
KW  - road safety
KW  - sensor
KW  - technology
KW  - virtual reality
DO  - 10.3390/s21113661
ER  -
TY  - EJOU
AU  - Teodoro, Ana
AU  - Santos, Patrícia
AU  - Espinha Marques, Jorge
AU  - Ribeiro, Joana
AU  - Mansilha, Catarina
AU  - Melo, Armindo
AU  - Duarte, Lia
AU  - Rodrigues de Almeida, Cátia
AU  - Flores, Deolinda
TI  - An Integrated Multi-Approach to Environmental Monitoring of a Self-Burning Coal Waste Pile: The São Pedro da Cova Mine (Porto, Portugal) Study Case
T2  - Environments

PY  - 2021
VL  - 8
IS  - 6
SN  - 2076-3298

AB  - The São Pedro da Cova waste pile (Porto, Portugal) is composed of coal mining residues that have been self-burning since 2005 and is located close to an inhabited area and social infrastructures, further adding to effects on the environment and human health. Therefore, there is a great interest in the environmental monitoring of this waste pile. This work describes an integrative multi-approach that allows the environmental monitoring of several parameters of the waste pile, applying several technologies. The temperature measurements were obtained by a thermal infrared (TIR) sensor on board an unmanned aerial vehicle (UAV) and supplemented with field measurements. In order to evaluate the altimetric variations, for each flight, a digital elevation model (DEM) was generated considering a multispectral sensor also on board the UAV. The hydrogeochemical characterization was performed through the analysis of groundwater and surface water samples, with and without the influence of mine drainage. The soil monitoring included the analysis of waste material as well as the surface soil in the surrounding area of the waste pile. All the data were analyzed and integrated in a geographical information system (GIS) open-source application. The adopted multi-approach methodology, given its intrinsic interdisciplinary character, has proven to be an effective way of encompassing the complexity of this type of environmental problem.
KW  - temperature
KW  - soils
KW  - water quality
KW  - geographical information system
KW  - unmanned aerial vehicles
KW  - land use land cover
DO  - 10.3390/environments8060048
ER  -
TY  - EJOU
AU  - Fetai, Bujar
AU  - Račič, Matej
AU  - Lisec, Anka
TI  - Deep Learning for Detection of Visible Land Boundaries from UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - Current efforts aim to accelerate cadastral mapping through innovative and automated approaches and can be used to both create and update cadastral maps. This research aims to automate the detection of visible land boundaries from unmanned aerial vehicle (UAV) imagery using deep learning. In addition, we wanted to evaluate the advantages and disadvantages of programming-based deep learning compared to commercial software-based deep learning. For the first case, we used the convolutional neural network U-Net, implemented in Keras, written in Python using the TensorFlow library. For commercial software-based deep learning, we used ENVINet5. UAV imageries from different areas were used to train the U-Net model, which was performed in Google Collaboratory and tested in the study area in Odranci, Slovenia. The results were compared with the results of ENVINet5 using the same datasets. The results showed that both models achieved an overall accuracy of over 95%. The high accuracy is due to the problem of unbalanced classes, which is usually present in boundary detection tasks. U-Net provided a recall of 0.35 and a precision of 0.68 when the threshold was set to 0.5. A threshold can be viewed as a tool for filtering predicted boundary maps and balancing recall and precision. For equitable comparison with ENVINet5, the threshold was increased. U-Net provided more balanced results, a recall of 0.65 and a precision of 0.41, compared to ENVINet5 recall of 0.84 and a precision of 0.35. Programming-based deep learning provides a more flexible yet complex approach to boundary mapping than software-based, which is rigid and does not require programming. The predicted visible land boundaries can be used both to speed up the creation of cadastral maps and to automate the revision of existing cadastral maps and define areas where updates are needed. The predicted boundaries cannot be considered final at this stage but can be used as preliminary cadastral boundaries.
KW  - land
KW  - cadastral mapping
KW  - visible boundary
KW  - UAV
KW  - deep learning
DO  - 10.3390/rs13112077
ER  -
TY  - EJOU
AU  - Dronova, Iryna
AU  - Kislik, Chippie
AU  - Dinh, Zack
AU  - Kelly, Maggi
TI  - A Review of Unoccupied Aerial Vehicle Use in Wetland Applications: Emerging Opportunities in Approach, Technology, and Data
T2  - Drones

PY  - 2021
VL  - 5
IS  - 2
SN  - 2504-446X

AB  - Recent developments in technology and data processing for Unoccupied Aerial Vehicles (UAVs) have revolutionized the scope of ecosystem monitoring, providing novel pathways to fill the critical gap between limited-scope field surveys and limited-customization satellite and piloted aerial platforms. These advances are especially ground-breaking for supporting management, restoration, and conservation of landscapes with limited field access and vulnerable ecological systems, particularly wetlands. This study presents a scoping review of the current status and emerging opportunities in wetland UAV applications, with particular emphasis on ecosystem management goals and remaining research, technology, and data needs to even better support these goals in the future. Using 122 case studies from 29 countries, we discuss which wetland monitoring and management objectives are most served by this rapidly developing technology, and what workflows were employed to analyze these data. This review showcases many ways in which UAVs may help reduce or replace logistically demanding field surveys and can help improve the efficiency of UAV-based workflows to support longer-term monitoring in the face of wetland environmental challenges and management constraints. We also highlight several emerging trends in applications, technology, and data and offer insights into future needs.
KW  - wetland
KW  - unoccupied aerial vehicle
KW  - UAV
KW  - UAS
KW  - drone
KW  - management
KW  - conservation
KW  - restoration
KW  - monitoring
KW  - high spatial resolution
DO  - 10.3390/drones5020045
ER  -
TY  - EJOU
AU  - Ahmed, Shibbir
AU  - Qiu, Baijing
AU  - Ahmad, Fiaz
AU  - Kong, Chun-Wei
AU  - Xin, Huang
TI  - A State-of-the-Art Analysis of Obstacle Avoidance Methods from the Perspective of an Agricultural Sprayer UAV’s Operation Scenario
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 6
SN  - 2073-4395

AB  - Over the last decade, Unmanned Aerial Vehicles (UAVs), also known as drones, have been broadly utilized in various agricultural fields, such as crop management, crop monitoring, seed sowing, and pesticide spraying. Nonetheless, autonomy is still a crucial limitation faced by the Internet of Things (IoT) UAV systems, especially when used as sprayer UAVs, where data needs to be captured and preprocessed for robust real-time obstacle detection and collision avoidance. Moreover, because of the objective and operational difference between general UAVs and sprayer UAVs, not every obstacle detection and collision avoidance method will be sufficient for sprayer UAVs. In this regard, this article seeks to review the most relevant developments on all correlated branches of the obstacle avoidance scenarios for agricultural sprayer UAVs, including a UAV sprayer’s structural details. Furthermore, the most relevant open challenges for current UAV sprayer solutions are enumerated, thus paving the way for future researchers to define a roadmap for devising new-generation, affordable autonomous sprayer UAV solutions. Agricultural UAV sprayers require data-intensive algorithms for the processing of the images acquired, and expertise in the field of autonomous flight is usually needed. The present study concludes that UAV sprayers are still facing obstacle detection challenges due to their dynamic operating and loading conditions.
KW  - agricultural sprayer UAVs
KW  - Internet of Things
KW  - obstacles on farmland
KW  - operation pattern
KW  - obstacle detection
KW  - collision avoidance
KW  - path planning
KW  - spray coverage
DO  - 10.3390/agronomy11061069
ER  -
TY  - EJOU
AU  - Greifeneder, Felix
AU  - Notarnicola, Claudia
AU  - Wagner, Wolfgang
TI  - A Machine Learning-Based Approach for Surface Soil Moisture Estimations with Google Earth Engine
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - Due to its relation to the Earth’s climate and weather and phenomena like drought, flooding, or landslides, knowledge of the soil moisture content is valuable to many scientific and professional users. Remote-sensing offers the unique possibility for continuous measurements of this variable. Especially for agriculture, there is a strong demand for high spatial resolution mapping. However, operationally available soil moisture products exist with medium to coarse spatial resolution only (≥1 km). This study introduces a machine learning (ML)—based approach for the high spatial resolution (50 m) mapping of soil moisture based on the integration of Landsat-8 optical and thermal images, Copernicus Sentinel-1 C-Band SAR images, and modelled data, executable in the Google Earth Engine. The novelty of this approach lies in applying an entirely data-driven ML concept for global estimation of the surface soil moisture content. Globally distributed in situ data from the International Soil Moisture Network acted as an input for model training. Based on the independent validation dataset, the resulting overall estimation accuracy, in terms of Root-Mean-Squared-Error and R², was 0.04 m3·m−3 and 0.81, respectively. Beyond the retrieval model itself, this article introduces a framework for collecting training data and a stand-alone Python package for soil moisture mapping. The Google Earth Engine Python API facilitates the execution of data collection and retrieval which is entirely cloud-based. For soil moisture retrieval, it eliminates the requirement to download or preprocess any input datasets.
KW  - soil moisture
KW  - Sentinel-1 SAR
KW  - Landsat-8 optical/thermal data
KW  - machine learning
KW  - cloud-based approach
KW  - Google Earth Engine
DO  - 10.3390/rs13112099
ER  -
TY  - EJOU
AU  - Choudhury, MD A.
AU  - Marcheggiani, Ernesto
AU  - Galli, Andrea
AU  - Modica, Giuseppe
AU  - Somers, Ben
TI  - Mapping the Urban Atmospheric Carbon Stock by LiDAR and WorldView-3 Data
T2  - Forests

PY  - 2021
VL  - 12
IS  - 6
SN  - 1999-4907

AB  - Currently, the worsening impacts of urbanizations have been impelled to the importance of monitoring and management of existing urban trees, securing sustainable use of the available green spaces. Urban tree species identification and evaluation of their roles in atmospheric Carbon Stock (CS) are still among the prime concerns for city planners regarding initiating a convenient and easily adaptive urban green planning and management system. A detailed methodology on the urban tree carbon stock calibration and mapping was conducted in the urban area of Brussels, Belgium. A comparative analysis of the mapping outcomes was assessed to define the convenience and efficiency of two different remote sensing data sources, Light Detection and Ranging (LiDAR) and WorldView-3 (WV-3), in a unique urban area. The mapping results were validated against field estimated carbon stocks. At the initial stage, dominant tree species were identified and classified using the high-resolution WorldView3 image, leading to the final carbon stock mapping based on the dominant species. An object-based image analysis approach was employed to attain an overall accuracy (OA) of 71% during the classification of the dominant species. The field estimations of carbon stock for each plot were done utilizing an allometric model based on the field tree dendrometric data. Later based on the correlation among the field data and the variables (i.e., Normalized Difference Vegetation Index, NDVI and Crown Height Model, CHM) extracted from the available remote sensing data, the carbon stock mapping and validation had been done in a GIS environment. The calibrated NDVI and CHM had been used to compute possible carbon stock in either case of the WV-3 image and LiDAR data, respectively. A comparative discussion has been introduced to bring out the issues, especially for the developing countries, where WV-3 data could be a better solution over the hardly available LiDAR data. This study could assist city planners in understanding and deciding the applicability of remote sensing data sources based on their availability and the level of expediency, ensuring a sustainable urban green management system.
KW  - urban trees
KW  - Geospatial Object-Based Image Analysis (GEOBIA)
KW  - Carbon Stock (CS) mapping
KW  - allometric model
KW  - WorldView-3 (WV-3) imagery
KW  - aerial Light Detection and Ranging (LiDAR) data
DO  - 10.3390/f12060692
ER  -
TY  - EJOU
AU  - Aeberli, Aaron
AU  - Johansen, Kasper
AU  - Robson, Andrew
AU  - Lamb, David W.
AU  - Phinn, Stuart
TI  - Detection of Banana Plants Using Multi-Temporal Multispectral UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - Unoccupied aerial vehicles (UAVs) have become increasingly commonplace in aiding planning and management decisions in agricultural and horticultural crop production. The ability of UAV-based sensing technologies to provide high spatial (&lt;1 m) and temporal (on-demand) resolution data facilitates monitoring of individual plants over time and can provide essential information about health, yield, and growth in a timely and quantifiable manner. Such applications would be beneficial for cropped banana plants due to their distinctive growth characteristics. Limited studies have employed UAV data for mapping banana crops and to our knowledge only one other investigation features multi-temporal detection of banana crowns. The purpose of this study was to determine the suitability of multiple-date UAV-captured multi-spectral data for the automated detection of individual plants using convolutional neural network (CNN), template matching (TM), and local maximum filter (LMF) methods in a geographic object-based image analysis (GEOBIA) software framework coupled with basic classification refinement. The results indicate that CNN returns the highest plant detection accuracies, with the developed rule set and model providing greater transferability between dates (F-score ranging between 0.93 and 0.85) than TM (0.86–0.74) and LMF (0.86–0.73) approaches. The findings provide a foundation for UAV-based individual banana plant counting and crop monitoring, which may be used for precision agricultural applications to monitor health, estimate yield, and to inform on fertilizer, pesticide, and other input requirements for optimized farm management.
KW  - unoccupied aerial vehicle
KW  - UAV
KW  - banana plant
KW  - geographic object-based image analysis
KW  - convolutional neural network
KW  - CNN
KW  - template matching
KW  - local maximum filter
DO  - 10.3390/rs13112123
ER  -
TY  - EJOU
AU  - de Castro, Ana I.
AU  - Shi, Yeyin
AU  - Maja, Joe M.
AU  - Peña, Jose M.
TI  - UAVs for Vegetation Monitoring: Overview and Recent Scientific Contributions
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - This paper reviewed a set of twenty-one original and innovative papers included in a special issue on UAVs for vegetation monitoring, which proposed new methods and techniques applied to diverse agricultural and forestry scenarios. Three general categories were considered: (1) sensors and vegetation indices used, (2) technological goals pursued, and (3) agroforestry applications. Some investigations focused on issues related to UAV flight operations, spatial resolution requirements, and computation and data analytics, while others studied the ability of UAVs for characterizing relevant vegetation features (mainly canopy cover and crop height) or for detecting different plant/crop stressors, such as nutrient content/deficiencies, water needs, weeds, and diseases. The general goal was proposing UAV-based technological solutions for a better use of agricultural and forestry resources and more efficient production with relevant economic and environmental benefits.
KW  - drone
KW  - RGB
KW  - multispectral
KW  - hyperspectral
KW  - thermal
KW  - machine learning
KW  - water stress
KW  - nutrient deficiency
KW  - weed detection
KW  - disease diagnosis
KW  - plant trails
DO  - 10.3390/rs13112139
ER  -
TY  - EJOU
AU  - Zhang, Peng
AU  - Hu, Shougeng
AU  - Li, Weidong
AU  - Zhang, Chuanrong
AU  - Cheng, Peikun
TI  - Improving Parcel-Level Mapping of Smallholder Crops from VHSR Imagery: An Ensemble Machine-Learning-Based Framework
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - Explicit spatial information about crop types on smallholder farms is important for the development of local precision agriculture. However, due to highly fragmented and heterogeneous cropland landscapes, fine-scale mapping of smallholder crops, based on low- and medium-resolution satellite images and relying on a single machine learning (ML) classifier, generally fails to achieve satisfactory performance. This paper develops an ensemble ML-based framework to improve the accuracy of parcel-level smallholder crop mapping from very high spatial resolution (VHSR) images. A typical smallholder agricultural area in central China covered by WorldView-2 images is selected to demonstrate our approach. This approach involves the task of distinguishing eight crop-level agricultural land use types. To this end, six widely used individual ML classifiers are evaluated. We further improved their performance by independently implementing bagging and stacking ensemble learning (EL) techniques. The results show that the bagging models improved the performance of unstable classifiers, but these improvements are limited. In contrast, the stacking models perform better, and the Stacking #2 model (overall accuracy = 83.91%, kappa = 0.812), which integrates the three best-performing individual classifiers, performs the best of all of the built models and improves the classwise accuracy of almost all of the land use types. Since classification performance can be significantly improved without adding costly data collection, stacking-ensemble mapping approaches are valuable for the spatial management of complex agricultural areas. We also demonstrate that using geometric and textural features extracted from VHSR images can improve the accuracy of parcel-level smallholder crop mapping. The proposed framework shows the great potential of combining EL technology with VHSR imagery for accurate mapping of smallholder crops, which could facilitate the development of parcel-level crop identification systems in countries dominated by smallholder agriculture.
KW  - crop classification
KW  - smallholder farms
KW  - land parcel
KW  - geographic object-based image analysis (GEOBIA)
KW  - machine learning
KW  - stacking
KW  - bagging
KW  - WorldView-2
DO  - 10.3390/rs13112146
ER  -
TY  - EJOU
AU  - Budholiya, Sejal
AU  - Bhat, Aayush
AU  - Raj, S. A.
AU  - Hameed Sultan, Mohamed T.
AU  - Md Shah, Ain U.
AU  - A. Basri, Adi
TI  - State of the Art Review about Bio-Inspired Design and Applications: An Aerospace Perspective
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 11
SN  - 2076-3417

AB  - The field of bio-inspired design has tremendously transitioned into newer automated methods, yet there are methods being discovered which can elucidate underlying principles in design, materials, and manufacturing. Bio-inspired design aims to translate knowledge from the natural world to the current trends in industry. The recent growth in additive manufacturing (AM)methods has fueled the tremendous growth of bio-inspired products. It has enabled the production of intricate and complicated features notably used in the aerospace industry. Numerous methodologies were adopted to analyse the process of bio-inspired material selection, manufacturing methods, design, and applications. In the current review, different approaches are implemented to utilize bio-inspired designs that have revolutionized the aerospace industry, focusing on AM methods.
KW  - bio-inspireddesign
KW  - additive manufacturing
KW  - aerospace
DO  - 10.3390/app11115054
ER  -
TY  - EJOU
AU  - Bai, Hao
AU  - Bai, Tingzhu
AU  - Li, Wei
AU  - Liu, Xun
TI  - A Building Segmentation Network Based on Improved Spatial Pyramid in Remote Sensing Images
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 11
SN  - 2076-3417

AB  - Building segmentation is widely used in urban planning, disaster prevention, human flow monitoring and environmental monitoring. However, due to the complex landscapes and highdensity settlements, automatically characterizing building in the urban village or cities using remote sensing images is very challenging. Inspired by the rencent deep learning methods, this paper proposed a novel end-to-end building segmentation network for segmenting buildings from remote sensing images. The network includes two branches: one branch uses Widely Adaptive Spatial Pyramid (WASP) structure to extract multi-scale features, and the other branch uses a deep residual network combined with a sub-pixel up-sampling structure to enhance the detail of building boundaries. We compared our proposed method with three state-of-the-art networks: DeepLabv3+, ENet, ESPNet. Experiments were performed using the publicly available Inria Aerial Image Labelling dataset (Inria aerial dataset) and the Satellite dataset II(East Asia). The results showed that our method outperformed the other networks in the experiments, with Pixel Accuracy reaching 0.8421 and 0.8738, respectively and with mIoU reaching 0.9034 and 0.8936 respectively. Compared with the basic network, it has increased by about 25% or more. It can not only extract building footprints, but also especially small building objects.
KW  - CNN
KW  - semantic segmentation
KW  - super resolution
KW  - remote sensing
KW  - spatial pyramid
KW  - ResNet
DO  - 10.3390/app11115069
ER  -
TY  - EJOU
AU  - Lee, Seunghyeon
AU  - Song, Youngkeun
AU  - Kil, Sung-Ho
TI  - Feasibility Analyses of Real-Time Detection of Wildlife Using UAV-Derived Thermal and RGB Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - Wildlife monitoring is carried out for diverse reasons, and monitoring methods have gradually advanced through technological development. Direct field investigations have been replaced by remote monitoring methods, and unmanned aerial vehicles (UAVs) have recently become the most important tool for wildlife monitoring. Many previous studies on detecting wild animals have used RGB images acquired from UAVs, with most of the analyses depending on machine learning–deep learning (ML–DL) methods. These methods provide relatively accurate results, and when thermal sensors are used as a supplement, even more accurate detection results can be obtained through complementation with RGB images. However, because most previous analyses were based on ML–DL methods, a lot of time was required to generate training data and train detection models. This drawback makes ML–DL methods unsuitable for real-time detection in the field. To compensate for the disadvantages of the previous methods, this paper proposes a real-time animal detection method that generates a total of six applicable input images depending on the context and uses them for detection. The proposed method is based on the Sobel edge algorithm, which is simple but can detect edges quickly based on change values. The method can detect animals in a single image without training data. The fastest detection time per image was 0.033 s, and all frames of a thermal video could be analyzed. Furthermore, because of the synchronization of the properties of the thermal and RGB images, the performance of the method was above average in comparison with previous studies. With target images acquired at heights below 100 m, the maximum detection precision and detection recall of the most accurate input image were 0.804 and 0.699, respectively. However, the low resolution of the thermal sensor and its shooting height limitation were hindrances to wildlife detection. The aim of future research will be to develop a detection method that can improve these shortcomings.
KW  - thermal sensing
KW  - unmanned aerial vehicle
KW  - object-based animal detection
KW  - instant and automated detection
KW  - mixed image analysis
KW  - wildlife monitoring
KW  - multiple height shooting
DO  - 10.3390/rs13112169
ER  -
TY  - EJOU
AU  - Burdziakowski, Pawel
AU  - Zima, Piotr
AU  - Wielgat, Pawel
AU  - Kalinowska, Dominika
TI  - Tracking Fluorescent Dye Dispersion from an Unmanned Aerial Vehicle
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 11
SN  - 1424-8220

AB  - Commercial unmanned aerial vehicles continue to gain popularity and their use for collecting image data and recording new phenomena is becoming more frequent. This study presents an effective method for measuring the concentration of fluorescent dyes (fluorescein and Rhodamine WT) for the purpose of providing a mathematical dispersion model. Image data obtained using a typical visible-light camera was used to measure the concentration of the dye floating on water. The reference measurement was taken using a laboratory fluorometer. The article presents the details of three extensive measurement sessions and presents elements of a newly developed method for measuring fluorescent tracer concentrations. The said method provides tracer concentration maps presented on the example of an orthophoto within a 2 × 2 m discrete grid.
KW  - UAV
KW  - camera
KW  - fluorimeters
KW  - dispersion
KW  - fluorescein
KW  - rhodamine
DO  - 10.3390/s21113905
ER  -
TY  - EJOU
AU  - Yang, Xiaoyu
AU  - Bao, Nisha
AU  - Li, Wenwen
AU  - Liu, Shanjun
AU  - Fu, Yanhua
AU  - Mao, Yachun
TI  - Soil Nutrient Estimation and Mapping in Farmland Based on UAV Imaging Spectrometry
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 11
SN  - 1424-8220

AB  - Soil nutrient is one of the most important properties for improving farmland quality and product. Imaging spectrometry has the potential for rapid acquisition and real-time monitoring of soil characteristics. This study aims to explore the preprocessing and modeling methods of hyperspectral images obtained from an unmanned aerial vehicle (UAV) platform for estimating the soil organic matter (SOM) and soil total nitrogen (STN) in farmland. The results showed that: (1) Multiplicative Scattering Correction (MSC) performed better in reducing image scattering noise than Standard Normal Variate (SNV) transformation or spectral derivatives, and it yielded a result with higher correlation and lower signal-to-noise ratio; (2) The proposed feature selection method combining Successive Projections Algorithm (SPA) and Competitive Adaptive Reweighted Sampling algorithm (CARS), could provide selective preference for hyperspectral bands. Exploiting this method, 24 and 22 feature bands were selected for SOM and STN estimation, respectively; (3) The particle swarm optimization (PSO) algorithm was employed to obtain optimized input weights and bias values of the extreme learning machine (ELM) model for more accurate prediction of SOM and STN. The improved PSO-ELM model based on the selected preference bands achieved higher prediction accuracy (R2 of 0.73 and RPD of 1.91 for SOM, R2 of 0.63, and RPD of 1.53 for STN) than support vector machine (SVM), partial least squares regression (PLSR), and the ELM model. This study provides an important guideline for monitoring soil nutrient for precision agriculture with imaging spectrometry.
KW  - unmanned aerial vehicle
KW  - hyperspectral image
KW  - extreme learning machine
KW  - soil nutrient estimation
KW  - feature selection
DO  - 10.3390/s21113919
ER  -
TY  - EJOU
AU  - Paturkar, Abhipray
AU  - Sen Gupta, Gourab
AU  - Bailey, Donald
TI  - Making Use of 3D Models for Plant Physiognomic Analysis: A Review
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 11
SN  - 2072-4292

AB  - Use of 3D sensors in plant phenotyping has increased in the last few years. Various image acquisition, 3D representations, 3D model processing and analysis techniques exist to help the researchers. However, a review of approaches, algorithms, and techniques used for 3D plant physiognomic analysis is lacking. In this paper, we investigate the techniques and algorithms used at various stages of processing and analysing 3D models of plants, and identify their current limiting factors. This review will serve potential users as well as new researchers in this field. The focus is on exploring studies monitoring the plant growth of single plants or small scale canopies as opposed to large scale monitoring in the field.
KW  - plant phenotyping
KW  - plant growth monitoring
KW  - point cloud processing
KW  - 3D point cloud
KW  - SfM
KW  - structural parameter
KW  - 3D measurements
DO  - 10.3390/rs13112232
ER  -
TY  - EJOU
AU  - Hara, Patryk
AU  - Piekutowska, Magdalena
AU  - Niedbała, Gniewko
TI  - Selection of Independent Variables for Crop Yield Prediction Using Artificial Neural Network Models with Remote Sensing Data
T2  - Land

PY  - 2021
VL  - 10
IS  - 6
SN  - 2073-445X

AB  - Knowing the expected crop yield in the current growing season provides valuable information for farmers, policy makers, and food processing plants. One of the main benefits of using reliable forecasting tools is generating more income from grown crops. Information on the amount of crop yielding before harvesting helps to guide the adoption of an appropriate strategy for managing agricultural products. The difficulty in creating forecasting models is related to the appropriate selection of independent variables. Their proper selection requires a perfect knowledge of the research object. The following article presents and discusses the most commonly used independent variables in agricultural crop yield prediction modeling based on artificial neural networks (ANNs). Particular attention is paid to environmental variables, such as climatic data, air temperature, total precipitation, insolation, and soil parameters. The possibility of using plant productivity indices and vegetation indices, which are valuable predictors obtained due to the application of remote sensing techniques, are analyzed in detail. The paper emphasizes that the increasingly common use of remote sensing and photogrammetric tools enables the development of precision agriculture. In addition, some limitations in the application of certain input variables are specified, as well as further possibilities for the development of non-linear modeling, using artificial neural networks as a tool supporting the practical use of and improvement in precision farming techniques.
KW  - crop yield prediction
KW  - independent variables
KW  - ANN
KW  - remote sensing
DO  - 10.3390/land10060609
ER  -
TY  - EJOU
AU  - Al-amri, Redhwan
AU  - Murugesan, Raja K.
AU  - Man, Mustafa
AU  - Abdulateef, Alaa F.
AU  - Al-Sharafi, Mohammed A.
AU  - Alkahtani, Ammar A.
TI  - A Review of Machine Learning and Deep Learning Techniques for Anomaly Detection in IoT Data
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 12
SN  - 2076-3417

AB  - Anomaly detection has gained considerable attention in the past couple of years. Emerging technologies, such as the Internet of Things (IoT), are known to be among the most critical sources of data streams that produce massive amounts of data continuously from numerous applications. Examining these collected data to detect suspicious events can reduce functional threats and avoid unseen issues that cause downtime in the applications. Due to the dynamic nature of the data stream characteristics, many unresolved problems persist. In the existing literature, methods have been designed and developed to evaluate certain anomalous behaviors in IoT data stream sources. However, there is a lack of comprehensive studies that discuss all the aspects of IoT data processing. Thus, this paper attempts to fill this gap by providing a complete image of various state-of-the-art techniques on the major problems and core challenges in IoT data. The nature of data, anomaly types, learning mode, window model, datasets, and evaluation criteria are also presented. Research challenges related to data evolving, feature-evolving, windowing, ensemble approaches, nature of input data, data complexity and noise, parameters selection, data visualizations, heterogeneity of data, accuracy, and large-scale and high-dimensional data are investigated. Finally, the challenges that require substantial research efforts and future directions are summarized.
KW  - anomaly detection
KW  - data stream
KW  - deep learning
KW  - Internet of Things
KW  - machine learning
DO  - 10.3390/app11125320
ER  -
TY  - EJOU
AU  - de Oliveira, Gabriel S.
AU  - Marcato Junior, José
AU  - Polidoro, Caio
AU  - Osco, Lucas P.
AU  - Siqueira, Henrique
AU  - Rodrigues, Lucas
AU  - Jank, Liana
AU  - Barrios, Sanzio
AU  - Valle, Cacilda
AU  - Simeão, Rosângela
AU  - Carromeu, Camilo
AU  - Silveira, Eloise
AU  -  André de Castro Jorge, Lúcio
AU  - Gonçalves, Wesley
AU  - Santos, Mateus
AU  - Matsubara, Edson
TI  - Convolutional Neural Networks to Estimate Dry Matter Yield in a Guineagrass Breeding Program Using UAV Remote Sensing
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 12
SN  - 1424-8220

AB  - Forage dry matter is the main source of nutrients in the diet of ruminant animals. Thus, this trait is evaluated in most forage breeding programs with the objective of increasing the yield. Novel solutions combining unmanned aerial vehicles (UAVs) and computer vision are crucial to increase the efficiency of forage breeding programs, to support high-throughput phenotyping (HTP), aiming to estimate parameters correlated to important traits. The main goal of this study was to propose a convolutional neural network (CNN) approach using UAV-RGB imagery to estimate dry matter yield traits in a guineagrass breeding program. For this, an experiment composed of 330 plots of full-sib families and checks conducted at Embrapa Beef Cattle, Brazil, was used. The image dataset was composed of images obtained with an RGB sensor embedded in a Phantom 4 PRO. The traits leaf dry matter yield (LDMY) and total dry matter yield (TDMY) were obtained by conventional agronomic methodology and considered as the ground-truth data. Different CNN architectures were analyzed, such as AlexNet, ResNeXt50, DarkNet53, and two networks proposed recently for related tasks named MaCNN and LF-CNN. Pretrained AlexNet and ResNeXt50 architectures were also studied. Ten-fold cross-validation was used for training and testing the model. Estimates of DMY traits by each CNN architecture were considered as new HTP traits to compare with real traits. Pearson correlation coefficient r between real and HTP traits ranged from 0.62 to 0.79 for LDMY and from 0.60 to 0.76 for TDMY; root square mean error (RSME) ranged from 286.24 to 366.93 kg·ha−1 for LDMY and from 413.07 to 506.56 kg·ha−1 for TDMY. All the CNNs generated heritable HTP traits, except LF-CNN for LDMY and AlexNet for TDMY. Genetic correlations between real and HTP traits were high but varied according to the CNN architecture. HTP trait from ResNeXt50 pretrained achieved the best results for indirect selection regardless of the dry matter trait. This demonstrates that CNNs with remote sensing data are highly promising for HTP for dry matter yield traits in forage breeding programs.
KW  - deep learning
KW  - forage dry matter yield
KW  - high-throughput phenotyping
KW  - Brazilian pasture
DO  - 10.3390/s21123971
ER  -
TY  - EJOU
AU  - Lazzeri, Giacomo
AU  - Frodella, William
AU  - Rossi, Guglielmo
AU  - Moretti, Sandro
TI  - Multitemporal Mapping of Post-Fire Land Cover Using Multiplatform PRISMA Hyperspectral and Sentinel-UAV Multispectral Data: Insights from Case Studies in Portugal and Italy
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 12
SN  - 1424-8220

AB  - Wildfires have affected global forests and the Mediterranean area with increasing recurrency and intensity in the last years, with climate change resulting in reduced precipitations and higher temperatures. To assess the impact of wildfires on the environment, burned area mapping has become progressively more relevant. Initially carried out via field sketches, the advent of satellite remote sensing opened new possibilities, reducing the cost uncertainty and safety of the previous techniques. In the present study an experimental methodology was adopted to test the potential of advanced remote sensing techniques such as multispectral Sentinel-2, PRISMA hyperspectral satellite, and UAV (unmanned aerial vehicle) remotely-sensed data for the multitemporal mapping of burned areas by soil–vegetation recovery analysis in two test sites in Portugal and Italy. In case study one, innovative multiplatform data classification was performed with the correlation between Sentinel-2 RBR (relativized burn ratio) fire severity classes and the scene hyperspectral signature, performed with a pixel-by-pixel comparison leading to a converging classification. In the adopted methodology, RBR burned area analysis and vegetation recovery was tested for accordance with biophysical vegetation parameters (LAI, fCover, and fAPAR). In case study two, a UAV-sensed NDVI index was adopted for high-resolution mapping data collection. At a large scale, the Sentinel-2 RBR index proved to be efficient for burned area analysis, from both fire severity and vegetation recovery phenomena perspectives. Despite the elapsed time between the event and the acquisition, PRISMA hyperspectral converging classification based on Sentinel-2 was able to detect and discriminate different spectral signatures corresponding to different fire severity classes. At a slope scale, the UAV platform proved to be an effective tool for mapping and characterizing the burned area, giving clear advantage with respect to filed GPS mapping. Results highlighted that UAV platforms, if equipped with a hyperspectral sensor and used in a synergistic approach with PRISMA, would create a useful tool for satellite acquired data scene classification, allowing for the acquisition of a ground truth.
KW  - remote sensing
KW  - hyperspectral
KW  - multispectral
KW  - vegetation recovery
KW  - burn severity
KW  - soil charring
KW  - drone
KW  - scene classification
KW  - RBR
DO  - 10.3390/s21123982
ER  -
TY  - EJOU
AU  - Zheng, Ke
AU  - Jia, Guozhu
AU  - Yang, Linchao
AU  - Wang, Jiaqing
TI  - A Compound Fault Labeling and Diagnosis Method Based on Flight Data and BIT Record of UAV
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 12
SN  - 2076-3417

AB  - In the process of Unmanned Aerial Vehicle (UAV) flight testing, plenty of compound faults exist, which could be composed of concurrent single faults or over-limit states alarmed by Built-In-Test (BIT) equipment. At present, there still lacks a suitable automatic labeling approach for UAV flight data, effectively utilizing the information of the BIT record. The performance of the originally employed flight data-driven fault diagnosis models based on machine learning needs to be improved as well. A compound fault labeling and diagnosis method based on actual flight data and the BIT record of the UAV during flight test phase is proposed, through labeling the flight data with compound fault modes corresponding to concurrent single faults recorded by the BIT system, and upgrading the original diagnosis model based on Gradient Boosting Decision Tree (GBDT) and Fully Convolutional Network (FCNN), to eXtreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and modified Convolutional Neural Network (CNN). The experimental results based on actual test flight data show that the proposed method could effectively label the flight data and obtain a significant improvement in diagnostic performance, appearing to be practical in the UAV test flight process.
KW  - fault diagnosis
KW  - data labeling
KW  - UAV
KW  - flight data and BIT record
KW  - machine learning
DO  - 10.3390/app11125410
ER  -
TY  - EJOU
AU  - Sun, Fengjie
AU  - Wang, Xianchang
AU  - Zhang, Rui
TI  - Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment
T2  - Entropy

PY  - 2021
VL  - 23
IS  - 6
SN  - 1099-4300

AB  - An Unmanned Aerial Vehicle (UAV) can greatly reduce manpower in the agricultural plant protection such as watering, sowing, and pesticide spraying. It is essential to develop a Decision-making Support System (DSS) for UAVs to help them choose the correct action in states according to the policy. In an unknown environment, the method of formulating rules for UAVs to help them choose actions is not applicable, and it is a feasible solution to obtain the optimal policy through reinforcement learning. However, experiments show that the existing reinforcement learning algorithms cannot get the optimal policy for a UAV in the agricultural plant protection environment. In this work we propose an improved Q-learning algorithm based on similar state matching, and we prove theoretically that there has a greater probability for UAV choosing the optimal action according to the policy learned by the algorithm we proposed than the classic Q-learning algorithm in the agricultural plant protection environment. This proposed algorithm is implemented and tested on datasets that are evenly distributed based on real UAV parameters and real farm information. The performance evaluation of the algorithm is discussed in detail. Experimental results show that the algorithm we proposed can efficiently learn the optimal policy for UAVs in the agricultural plant protection environment.
KW  - decision-making support system
KW  - reinforcement learning
KW  - Q-learning
DO  - 10.3390/e23060737
ER  -
TY  - EJOU
AU  - Brandoli, Bruno
AU  - de Geus, André R.
AU  - Souza, Jefferson R.
AU  - Spadon, Gabriel
AU  - Soares, Amilcar
AU  - Rodrigues, Jose F.
AU  - Komorowski, Jerzy
AU  - Matwin, Stan
TI  - Aircraft Fuselage Corrosion Detection Using Artificial Intelligence
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 12
SN  - 1424-8220

AB  - Corrosion identification and repair is a vital task in aircraft maintenance to ensure continued structural integrity. Regarding fuselage lap joints, typically, visual inspections are followed by non-destructive methodologies, which are time-consuming. The visual inspection of large areas suffers not only from subjectivity but also from the variable probability of corrosion detection, which is aggravated by the multiple layers used in fuselage construction. In this paper, we propose a methodology for automatic image-based corrosion detection of aircraft structures using deep neural networks. For machine learning, we use a dataset that consists of D-Sight Aircraft Inspection System (DAIS) images from different lap joints of Boeing and Airbus aircrafts. We also employ transfer learning to overcome the shortage of aircraft corrosion images. With precision of over 93%, we demonstrate that our approach detects corrosion with a precision comparable to that of trained operators, aiding to reduce the uncertainties related to operator fatigue or inadequate training. Our results indicate that our methodology can support specialists and engineers in corrosion monitoring in the aerospace industry, potentially contributing to the automation of condition-based maintenance protocols.
KW  - aircraft corrosion inspection
KW  - automatic corrosion detection
KW  - material fatigue
KW  - corrosion science
KW  - rust detection
KW  - aviation maintenance
KW  - deep learning
DO  - 10.3390/s21124026
ER  -
TY  - EJOU
AU  - Burdziakowski, Pawel
TI  - Polymodal Method of Improving the Quality of Photogrammetric Images and Models
T2  - Energies

PY  - 2021
VL  - 14
IS  - 12
SN  - 1996-1073

AB  - Photogrammetry using unmanned aerial vehicles has become very popular and is already commonly used. The most frequent photogrammetry products are an orthoimage, digital terrain model and a 3D object model. When executing measurement flights, it may happen that there are unsuitable lighting conditions, and the flight itself is fast and not very stable. As a result, noise and blur appear on the images, and the images themselves can have too low of a resolution to satisfy the quality requirements for a photogrammetric product. In such cases, the obtained images are useless or will significantly reduce the quality of the end-product of low-level photogrammetry. A new polymodal method of improving measurement image quality has been proposed to avoid such issues. The method discussed in this article removes degrading factors from the images and, as a consequence, improves the geometric and interpretative quality of a photogrammetric product. The author analyzed 17 various image degradation cases, developed 34 models based on degraded and recovered images, and conducted an objective analysis of the quality of the recovered images and models. As evidenced, the result was a significant improvement in the interpretative quality of the images themselves and a better geometry model.
KW  - UAV
KW  - neural networks
KW  - deblur
KW  - denoise
KW  - super resolution
KW  - neural network
DO  - 10.3390/en14123457
ER  -
TY  - EJOU
AU  - Zhang, Wentao
AU  - Liu, Yucheng
AU  - Zhang, Shaohui
AU  - Long, Tuzhi
AU  - Liang, Jinglun
TI  - Error Fusion of Hybrid Neural Networks for Mechanical Condition Dynamic Prediction
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 12
SN  - 1424-8220

AB  - It is important for equipment to operate safely and reliably so that the working state of mechanical parts pushes forward an immense influence. Therefore, in order to enhance the dependability and security of mechanical equipment, to accurately predict the changing trend of mechanical components in advance plays a significant role. This paper introduces a novel condition prediction method, named error fusion of hybrid neural networks (EFHNN), by combining the error fusion of multiple sparse auto-encoders with convolutional neural networks for predicting the mechanical condition. First, to improve prediction accuracy, we can use the error fusion of multiple sparse auto-encoders to collect multi-feature information, and obtain a trend curve representing machine condition as well as a threshold line that can indicate the beginning of mechanical failure by computing the square prediction error (SPE). Then, convolutional neural networks predict the state of the machine according to the original data when the SPE value exceeds the threshold line. It can be seen from this result that the EFHNN method in the prediction of mechanical fault time series is available and superior.
KW  - mechanical equipment
KW  - error fusion of multiple SAEs (EFMSAE)
KW  - convolutional neural networks (CNN)
KW  - prediction
DO  - 10.3390/s21124043
ER  -
TY  - EJOU
AU  - Lema, Darío G.
AU  - Pedrayes, Oscar D.
AU  - Usamentiaga, Rubén
AU  - García, Daniel F.
AU  - Alonso, Ángela
TI  - Cost-Performance Evaluation of a Recognition Service of Livestock Activity Using Aerial Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 12
SN  - 2072-4292

AB  - The recognition of livestock activity is essential to be eligible for subsides, to automatically supervise critical activities and to locate stray animals. In recent decades, research has been carried out into animal detection, but this paper also analyzes the detection of other key elements that can be used to verify the presence of livestock activity in a given terrain: manure piles, feeders, silage balls, silage storage areas, and slurry pits. In recent years, the trend is to apply Convolutional Neuronal Networks (CNN) as they offer significantly better results than those obtained by traditional techniques. To implement a livestock activity detection service, the following object detection algorithms have been evaluated: YOLOv2, YOLOv4, YOLOv5, SSD, and Azure Custom Vision. Since YOLOv5 offers the best results, producing a mean average precision (mAP) of 0.94, this detector is selected for the creation of a livestock activity recognition service. In order to deploy the service in the best infrastructure, the performance/cost ratio of various Azure cloud infrastructures are analyzed and compared with a local solution. The result is an efficient and accurate service that can help to identify the presence of livestock activity in a specified terrain.
KW  - livestock activity recognition
KW  - Azure
KW  - cloud service deployment and cost-performance evaluation
KW  - aerial images
KW  - CNNs
KW  - YOLOv2
KW  - YOLOv4
KW  - YOLOv5
KW  - SSD
KW  - Azure Custom Vision
DO  - 10.3390/rs13122318
ER  -
TY  - EJOU
AU  - Kim, Jingyeom
AU  - Lee, Joohyung
AU  - Kim, Taeyeon
TI  - AdaMM: Adaptive Object Movement and Motion Tracking in Hierarchical Edge Computing System
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 12
SN  - 1424-8220

AB  - This paper presents a novel adaptive object movement and motion tracking (AdaMM) framework in a hierarchical edge computing system for achieving GPU memory footprint reduction of deep learning (DL)-based video surveillance services. DL-based object movement and motion tracking requires a significant amount of resources, such as (1) GPU processing power for the inference phase and (2) GPU memory for model loading. Despite the absence of an object in the video, if the DL model is loaded, the GPU memory must be kept allocated for the loaded model. Moreover, in several cases, video surveillance tries to capture events that rarely occur (e.g., abnormal object behaviors); therefore, such standby GPU memory might be easily wasted. To alleviate this problem, the proposed AdaMM framework categorizes the tasks used for the object movement and motion tracking procedure in an increasing order of the required processing and memory resources as task (1) frame difference calculation, task (2) object detection, and task (3) object motion and movement tracking. The proposed framework aims to adaptively release the unnecessary standby object motion and movement tracking model to save GPU memory by utilizing light tasks, such as frame difference calculation and object detection in a hierarchical manner. Consequently, object movement and motion tracking are adaptively triggered if the object is detected within the specified threshold time; otherwise, the GPU memory for the model of task (3) can be released. Moreover, object detection is also adaptively performed if the frame difference over time is greater than the specified threshold. We implemented the proposed AdaMM framework using commercial edge devices by considering a three-tier system, such as the 1st edge node for both tasks (1) and (2), the 2nd edge node for task (3), and the cloud for sending a push alarm. A measurement-based experiment reveals that the proposed framework achieves a maximum GPU memory reduction of 76.8% compared to the baseline system, while requiring a 2680 ms delay for loading the model for object movement and motion tracking.
KW  - EdgeAI
KW  - hierarchical edge computing
KW  - deep learning
KW  - object detection and tracking
KW  - software implementation
DO  - 10.3390/s21124089
ER  -
TY  - EJOU
AU  - Kakavas, Maria P.
AU  - Nikolakopoulos, Konstantinos G.
TI  - Digital Elevation Models of Rockfalls and Landslides: A Review and Meta-Analysis
T2  - Geosciences

PY  - 2021
VL  - 11
IS  - 6
SN  - 2076-3263

AB  - The scope of this paper is to summarize previous research pertaining to the use of digital elevation models (DEMs) and digital terrain models (DTMs) in the study of rockfalls and landslides. Research from 1983 to 2020 was surveyed in order to understand how the spatial resolution of DEMs and DTMs affects landslide detection, validation, and mapping. Another major question examined was the relationship between the DEM resolution and the extent of the rockfall or landslide event. It emerged from the study that, for landslides, the majority of researchers used DEMs with a spatial resolution of between 10 m and 30 m, while for rockfalls, they used DEMs with a spatial resolution of between 5 m and 20 m. We concluded that DEMs with a very high resolution (less than 5 m) are suitable for local-scale occurrences, while medium-resolution (from 20 m to 30 m) DEMs are suitable for regional-scale events. High resolution is associated with high accuracy and detailed structural characteristics, while medium accuracy better illustrates the topographic features. A low pixel size (more than 90 m) is not recommended for this type of research. Susceptibility maps, inventory maps, hazard risk zones, and vulnerability assessments are some of the main tools used in landslide/rockfall investigations, and topographic indexes, methods, models, and software optimize the reliability of the results. All of these parameters are closely related to DEMs and DTMs as the cell size affects the credibility of the final outcome.
KW  - landslides
KW  - rockfalls
KW  - DEM
KW  - DTM
KW  - spatial resolution
DO  - 10.3390/geosciences11060256
ER  -
TY  - EJOU
AU  - Geng, Liying
AU  - Che, Tao
AU  - Ma, Mingguo
AU  - Tan, Junlei
AU  - Wang, Haibo
TI  - Corn Biomass Estimation by Integrating Remote Sensing and Long-Term Observation Data Based on Machine Learning Techniques
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 12
SN  - 2072-4292

AB  - The accurate and timely estimation of regional crop biomass at different growth stages is of great importance in guiding crop management decision making. The recent availability of long time series of remote sensing data offers opportunities for crop monitoring. In this paper, four machine learning models, namely random forest (RF), support vector machine (SVM), artificial neural network (ANN), and extreme gradient boosting (XGBoost) were adopted to estimate the seasonal corn biomass based on field observation data and moderate resolution imaging spectroradiometer (MODIS) reflectance data from 2012 to 2019 in the middle reaches of the Heihe River basin, China. Nine variables were selected with the forward feature selection approach from among twenty-seven variables potentially influencing corn biomass: soil-adjusted total vegetation index (SATVI), green ratio vegetation index (GRVI), Nadir_B7 (2105–2155 nm), Nadir_B6 (1628–1652 nm), land surface water index (LSWI), normalized difference vegetation index (NDVI), Nadir_B4 (545–565 nm), and Nadir_B3 (459–479 nm). The results indicated that the corn biomass was suitably estimated (the coefficient of determination (R2) was between 0.72 and 0.78) with the four machine learning models. The XGBoost model performed better than the other three models (R2 = 0.78, root mean squared error (RMSE) = 2.86 t/ha and mean absolute error (MAE) = 1.86 t/ha). Moreover, the RF model was an effective method (R2 = 0.77, RMSE = 2.91 t/ha and MAE = 1.91 t/ha), with a performance comparable to that of the XGBoost model. This study provides a reference for estimating crop biomass from MOD43A4 datasets. In addition, the research demonstrates the potential of machine learning techniques to achieve a relatively accurate estimation of daily corn biomass at a large scale.
KW  - corn
KW  - biomass
KW  - field data
KW  - MODIS
KW  - machine learning models
DO  - 10.3390/rs13122352
ER  -
TY  - EJOU
AU  - Zeng, Linglin
AU  - Hu, Yuchao
AU  - Wang, Rui
AU  - Zhang, Xiang
AU  - Peng, Guozhang
AU  - Huang, Zhenyu
AU  - Zhou, Guoqing
AU  - Xiang, Daxiang
AU  - Meng, Ran
AU  - Wu, Weixiong
AU  - Hu, Shun
TI  - 8-Day and Daily Maximum and Minimum Air Temperature Estimation via Machine Learning Method on a Climate Zone to Global Scale
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 12
SN  - 2072-4292

AB  - Air temperature (Ta) is a required input in a wide range of applications, e.g., agriculture. Land Surface Temperature (LST) products from Moderate Resolution Imaging Spectroradiometer (MODIS) are widely used to estimate Ta. Previous studies of these products in Ta estimation, however, were generally applied in small areas and with a small number of meteorological stations. This study designed both temporal and spatial experiments to estimate 8-day and daily maximum and minimum Ta (Tmax and Tmin) on three spatial scales: climate zone, continental and global scales from 2009 to 2018, using the Random Forest (RF) method based on MODIS LST products and other auxiliary data. Factors contributing to the relation between LST and Ta were determined based on physical models and equations. Temporal and spatial experiments were defined by the rules of dividing the training and validation datasets for the RF method, in which the stations selected in the training dataset were all included or not in the validation dataset. The RF model was first trained and validated on each spatial scale, respectively. On a global scale, model accuracy with a determination coefficient (R2) &gt; 0.96 and root mean square error (RMSE) &lt; 1.96 °C and R2 &gt; 0.95 and RMSE &lt; 2.55 °C was achieved for 8-day and daily Ta estimations, respectively, in both temporal and spatial experiments. Then the model was trained and cross-validated on each spatial scale. The results showed that the data size and station distribution of the study area were the main factors influencing the model performance at different spatial scales. Finally, the spatial patterns of the model performance and variable importance were analyzed. Both daytime and nighttime LST had a significant contribution in the 8-day Tmax estimation on all the three spatial scales; while their contribution in daily Tmax estimation varied over different continents or climate zones. This study was expected to improve our understanding of Ta estimation in terms of accuracy variations and influencing variables on different spatial and temporal scales. The future work mainly includes identifying underlying mechanisms of estimation errors and the uncertainty sources of Ta estimation from a local to a global scale.
KW  - MODIS
KW  - air temperature estimation
KW  - remote sensing
KW  - land surface temperature
KW  - nighttime LST
DO  - 10.3390/rs13122355
ER  -
TY  - EJOU
AU  - Planke, Lars J.
AU  - Gardi, Alessandro
AU  - Sabatini, Roberto
AU  - Kistan, Trevor
AU  - Ezer, Neta
TI  - Online Multimodal Inference of Mental Workload for Cognitive Human Machine Systems
T2  - Computers

PY  - 2021
VL  - 10
IS  - 6
SN  - 2073-431X

AB  - With increasingly higher levels of automation in aerospace decision support systems, it is imperative that the human operator maintains the required level of situational awareness in different operational conditions and a central role in the decision-making process. While current aerospace systems and interfaces are limited in their adaptability, a Cognitive Human Machine System (CHMS) aims to perform dynamic, real-time system adaptation by estimating the cognitive states of the human operator. Nevertheless, to reliably drive system adaptation of current and emerging aerospace systems, there is a need to accurately and repeatably estimate cognitive states, particularly for Mental Workload (MWL), in real-time. As part of this study, two sessions were performed during a Multi-Attribute Task Battery (MATB) scenario, including a session for offline calibration and validation and a session for online validation of eleven multimodal inference models of MWL. The multimodal inference model implemented included an Adaptive Neuro Fuzzy Inference System (ANFIS), which was used in different configurations to fuse data from an Electroencephalogram (EEG) model’s output, four eye activity features and a control input feature. The online validation of the ANFIS models produced good results, while the best performing model (containing all four eye activity features and the control input feature) showed an average Mean Absolute Error (MAE) = 0.67 ± 0.18 and Correlation Coefficient (CC) = 0.71 ± 0.15. The remaining six ANFIS models included data from the EEG model’s output, which had an offset discrepancy. This resulted in an equivalent offset for the online multimodal fusion. Nonetheless, the efficacy of these ANFIS models could be confirmed by the pairwise correlation with the task level, where one model demonstrated a CC = 0.77 ± 0.06, which was the highest among all of the ANFIS models tested. Hence, this study demonstrates the suitability for online multimodal fusion of features extracted from EEG signals, eye activity and control inputs to produce an accurate and repeatable inference of MWL.
KW  - mental workload
KW  - EEG
KW  - eye tracking
KW  - control inputs
KW  - closed loop system adaptation
KW  - adaptive automation
KW  - multimodal data fusion
KW  - machine learning
KW  - ANFIS
DO  - 10.3390/computers10060081
ER  -
TY  - EJOU
AU  - Linaza, Maria T.
AU  - Posada, Jorge
AU  - Bund, Jürgen
AU  - Eisert, Peter
AU  - Quartulli, Marco
AU  - Döllner, Jürgen
AU  - Pagani, Alain
AU  - G. Olaizola, Igor
AU  - Barriguinha, Andre
AU  - Moysiadis, Theocharis
AU  - Lucat, Laurent
TI  - Data-Driven Artificial Intelligence Applications for Sustainable Precision Agriculture
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 6
SN  - 2073-4395

AB  - One of the main challenges for the implementation of artificial intelligence (AI) in agriculture includes the low replicability and the corresponding difficulty in systematic data gathering, as no two fields are exactly alike. Therefore, the comparison of several pilot experiments in different fields, weather conditions and farming techniques enhances the collective knowledge. Thus, this work provides a summary of the most recent research activities in the form of research projects implemented and validated by the authors in several European countries, with the objective of presenting the already achieved results, the current investigations and the still open technical challenges. As an overall conclusion, it can be mentioned that even though in their primary stages in some cases, AI technologies improve decision support at farm level, monitoring conditions and optimizing production to allow farmers to apply the optimal number of inputs for each crop, thereby boosting yields and reducing water use and greenhouse gas emissions. Future extensions of this work will include new concepts based on autonomous and intelligent robots for plant and soil sample retrieval, and effective livestock management.
KW  - agriculture
KW  - artificial intelligence
KW  - data analysis
KW  - computer vision
KW  - robotics
DO  - 10.3390/agronomy11061227
ER  -
TY  - EJOU
AU  - Siemiatkowska, Barbara
AU  - Stecz, Wojciech
TI  - A Framework for Planning and Execution of Drone Swarm Missions in a Hostile Environment
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 12
SN  - 1424-8220

AB  - This article presents a framework for planning a drone swarm mission in a hostile environment. Elements of the planning framework are discussed in detail, including methods of planning routes for drone swarms using mixed integer linear programming (MILP) and methods of detecting potentially dangerous objects using EO/IR camera images and synthetic aperture radar (SAR). Methods of detecting objects in the field are used in the mission planning process to re-plan the swarm’s flight paths. The route planning model is discussed using the example of drone formations managed by one UAV that communicates through another UAV with the ground control station (GCS). This article presents practical examples of using algorithms for detecting dangerous objects for re-planning of swarm routes. A novelty in the work is the development of these algorithms in such a way that they can be implemented on mobile computers used by UAVs and integrated with MILP tasks. The methods of detection and classification of objects in real time by UAVs equipped with SAR and EO/IR are presented. Different sensors require different methods to detect objects. In the case of infrared or optoelectronic sensors, a convolutional neural network is used. For SAR images, a rule-based system is applied. The experimental results confirm that the stream of images can be analyzed in real-time.
KW  - mission planning
KW  - UAV swarms
KW  - object detection
KW  - CNN
KW  - SAR
KW  - EO
DO  - 10.3390/s21124150
ER  -
TY  - EJOU
AU  - Lee, Thomas
AU  - Mckeever, Susan
AU  - Courtney, Jane
TI  - Flying Free: A Research Overview of Deep Learning in Drone Navigation Autonomy
T2  - Drones

PY  - 2021
VL  - 5
IS  - 2
SN  - 2504-446X

AB  - With the rise of Deep Learning approaches in computer vision applications, significant strides have been made towards vehicular autonomy. Research activity in autonomous drone navigation has increased rapidly in the past five years, and drones are moving fast towards the ultimate goal of near-complete autonomy. However, while much work in the area focuses on specific tasks in drone navigation, the contribution to the overall goal of autonomy is often not assessed, and a comprehensive overview is needed. In this work, a taxonomy of drone navigation autonomy is established by mapping the definitions of vehicular autonomy levels, as defined by the Society of Automotive Engineers, to specific drone tasks in order to create a clear definition of autonomy when applied to drones. A top–down examination of research work in the area is conducted, focusing on drone navigation tasks, in order to understand the extent of research activity in each area. Autonomy levels are cross-checked against the drone navigation tasks addressed in each work to provide a framework for understanding the trajectory of current research. This work serves as a guide to research in drone autonomy with a particular focus on Deep Learning-based solutions, indicating key works and areas of opportunity for development of this area in the future.
KW  - artificial intelligence
KW  - deep learning
KW  - neural networks
KW  - artificial neural networks
KW  - multi-layer neural network
KW  - neural network hardware
KW  - autonomous systems
KW  - internet of things
KW  - machine vision
KW  - unmanned autonomous vehicles
KW  - unmanned aerial vehicles
DO  - 10.3390/drones5020052
ER  -
TY  - EJOU
AU  - An, Kang
AU  - Chen, Yixin
AU  - Wang, Suhong
AU  - Xiao, Zhifeng
TI  - RCBi-CenterNet: An Absolute Pose Policy for 3D Object Detection in Autonomous Driving
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 12
SN  - 2076-3417

AB  - 3D Object detection is a critical mission of the perception system of a self-driving vehicle. Existing bounding box-based methods are hard to train due to the need to remove duplicated detections in the post-processing stage. In this paper, we propose a center point-based deep neural network (DNN) architecture named RCBi-CenterNet that predicts the absolute pose for each detected object in the 3D world space. RCBi-CenterNet is composed of a recursive composite network with a dual-backbone feature extractor and a bi-directional feature pyramid network (BiFPN) for cross-scale feature fusion. In the detection head, we predict a confidence heatmap that is used to determine the position of detected objects. The other pose information, including depth and orientation, is regressed. We conducted extensive experiments on the Peking University/Baidu-Autonomous Driving dataset, which contains more than 60,000 labeled 3D vehicle instances from 5277 real-world images, and each vehicle object is annotated with the absolute pose described by the six degrees of freedom (6DOF). We validated the design choices of various data augmentation methods and the backbone options. Through an ablation study and an overall comparison with the state-of-the-art (SOTA), namely CenterNet, we showed that the proposed RCBi-CenterNet presents performance gains of 2.16%, 2.76%, and 5.24% in Top 1, Top 3, and Top 10 mean average precision (mAP). The model and the result could serve as a credible benchmark for future research in center point-based object detection.
KW  - object detection
KW  - CenterNet
KW  - absolute pose
KW  - feature fusion
KW  - autonomous driving
KW  - feature pyramid network
DO  - 10.3390/app11125621
ER  -
TY  - EJOU
AU  - Marin, Ivana
AU  - Mladenović, Saša
AU  - Gotovac, Sven
AU  - Zaharija, Goran
TI  - Deep-Feature-Based Approach to Marine Debris Classification
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 12
SN  - 2076-3417

AB  - The global community has recognized an increasing amount of pollutants entering oceans and other water bodies as a severe environmental, economic, and social issue. In addition to prevention, one of the key measures in addressing marine pollution is the cleanup of debris already present in marine environments. Deployment of machine learning (ML) and deep learning (DL) techniques can automate marine waste removal, making the cleanup process more efficient. This study examines the performance of six well-known deep convolutional neural networks (CNNs), namely VGG19, InceptionV3, ResNet50, Inception-ResNetV2, DenseNet121, and MobileNetV2, utilized as feature extractors according to three different extraction schemes for the identification and classification of underwater marine debris. We compare the performance of a neural network (NN) classifier trained on top of deep CNN feature extractors when the feature extractor is (1) fixed; (2) fine-tuned on the given task; (3) fixed during the first phase of training and fine-tuned afterward. In general, fine-tuning resulted in better-performing models but is much more computationally expensive. The overall best NN performance showed the fine-tuned Inception-ResNetV2 feature extractor with an accuracy of 91.40% and F1-score 92.08%, followed by fine-tuned InceptionV3 extractor. Furthermore, we analyze conventional ML classifiers’ performance when trained on features extracted with deep CNNs. Finally, we show that replacing NN with a conventional ML classifier, such as support vector machine (SVM) or logistic regression (LR), can further enhance the classification performance on new data.
KW  - deep learning
KW  - marine litter classification
KW  - feature vectors
KW  - transfer learning
KW  - computer vision
DO  - 10.3390/app11125644
ER  -
TY  - EJOU
AU  - Albuquerque, Rafael W.
AU  - Ferreira, Manuel E.
AU  - Olsen, Søren I.
AU  - Tymus, Julio R.
AU  - Balieiro, Cintia P.
AU  - Mansur, Hendrik
AU  - Moura, Ciro J.
AU  - Costa, João V.
AU  - Branco, Maurício R.
AU  - Grohmann, Carlos H.
TI  - Forest Restoration Monitoring Protocol with a Low-Cost Remotely Piloted Aircraft: Lessons Learned from a Case Study in the Brazilian Atlantic Forest
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 12
SN  - 2072-4292

AB  - Traditional forest restoration (FR) monitoring methods employ spreadsheets and photos taken at the ground level. Since remotely piloted aircraft (RPA) generate a panoramic high resolution and georeferenced view of the entire area of interest, this technology has high potential to improve the traditional FR monitoring methods. This study evaluates how low-cost RPA data may contribute to FR monitoring of the Brazilian Atlantic Forest by the automatic remote measurement of Tree Density, Tree Height, Vegetation Cover (area covered by trees), and Grass Infestation. The point cloud data was processed to map the Tree Density, Tree Height, and Vegetation Cover parameters. The orthomosaic was used for a Random Forest classification that considered trees and grasses as a single land cover class. The Grass Infestation parameter was mapped by the difference between this land cover class (which considered trees and grasses) and the Vegetation Cover results (obtained by the point cloud data processing). Tree Density, Vegetation Cover, and Grass Infestation parameters presented F_scores of 0.92, 0.85, and 0.64, respectively. Tree Height accuracy was indicated by the Error Percentage considering the traditional fieldwork and the RPA results. The Error Percentage was equal to 0.13 and was considered accurate because it estimated a 13% shorter height for trees that averaged 1.93 m tall. Thus, this study showed that the FR structural parameters were accurately measured by the low-cost RPA, a technology that contributes to FR monitoring. Despite accurately measuring the structural parameters, this study reinforced the challenge of measuring the Biodiversity parameter via remote sensing because the classification of tree species was not possible. After all, the Brazilian Atlantic Forest is a biodiversity hotspot, and thus different species have similar spectral responses in the visible spectrum and similar geometric forms. Therefore, until improved automatic classification methods become available for tree species, traditional fieldwork remains necessary for a complete FR monitoring diagnostic.
KW  - Atlantic Forest
KW  - drones
KW  - SfM-MVS
KW  - structural parameters
KW  - unmanned aerial vehicle
DO  - 10.3390/rs13122401
ER  -
TY  - EJOU
AU  - Perich, Gregor
AU  - Aasen, Helge
AU  - Verrelst, Jochem
AU  - Argento, Francesco
AU  - Walter, Achim
AU  - Liebisch, Frank
TI  - Crop Nitrogen Retrieval Methods for Simulated Sentinel-2 Data Using In-Field Spectrometer Data
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 12
SN  - 2072-4292

AB  - Nitrogen (N) is one of the key nutrients supplied in agricultural production worldwide. Over-fertilization can have negative influences on the field and the regional level (e.g., agro-ecosystems). Remote sensing of the plant N of field crops presents a valuable tool for the monitoring of N flows in agro-ecosystems. Available data for validation of satellite-based remote sensing of N is scarce. Therefore, in this study, field spectrometer measurements were used to simulate data of the Sentinel-2 (S2) satellites developed for vegetation monitoring by the ESA. The prediction performance of normalized ratio indices (NRIs), random forest regression (RFR) and Gaussian processes regression (GPR) for plant-N-related traits was assessed on a diverse real-world dataset including multiple crops, field sites and years. The plant N traits included the mass-based N measure, N concentration in the biomass (Nconc), and an area-based N measure approximating the plant N uptake (NUP). Spectral indices such as normalized ratio indices (NRIs) performed well, but the RFR and GPR methods outperformed the NRIs. Key spectral bands for each trait were identified using the RFR variable importance measure and the Gaussian processes regression band analysis tool (GPR-BAT), highlighting the importance of the short-wave infrared (SWIR) region for estimation of plant Nconc—and to a lesser extent the NUP. The red edge (RE) region was also important. The GPR-BAT showed that five bands were sufficient for plant N trait and leaf area index (LAI) estimation and that a surplus of bands effectively reduced prediction performance. A global sensitivity analysis (GSA) was performed on all traits simultaneously, showing the dominance of the LAI in the mixed remote sensing signal. To delineate the plant-N-related traits from this signal, regional and/or national data collection campaigns producing large crop spectral libraries (CSL) are needed. An improved database will likely enable the mapping of N at the agro-ecosystem level or for use in precision farming by farmers in the future.
KW  - nitrogen
KW  - chlorophyll
KW  - leaf area index
KW  - agro-ecosystem monitoring
KW  - spectral indices
KW  - random forest
KW  - gaussian processes regression
KW  - ARTMO toolbox
DO  - 10.3390/rs13122404
ER  -
TY  - EJOU
AU  - Tian, Luo
AU  - Qu, Yonghua
AU  - Qi, Jianbo
TI  - Estimation of Forest LAI Using Discrete Airborne LiDAR: A Review
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 12
SN  - 2072-4292

AB  - The leaf area index (LAI) is an essential input parameter for quantitatively studying the energy and mass balance in soil-vegetation-atmosphere transfer systems. As an active remote sensing technology, light detection and ranging (LiDAR) provides a new method to describe forest canopy LAI. This paper reviewed the primary LAI retrieval methods using point cloud data (PCD) obtained by discrete airborne LiDAR scanner (DALS), its validation scheme, and its limitations. There are two types of LAI retrieval methods based on DALS PCD, i.e., the empirical regression and the gap fraction (GF) model. In the empirical model, tree height-related variables, LiDAR penetration indexes (LPIs), and canopy cover are the most widely used proxy variables. The height-related proxies are used most frequently; however, the LPIs proved the most efficient proxy. The GF model based on the Beer-Lambert law has been proven useful to estimate LAI; however, the suitability of LPIs is site-, tree species-, and LiDAR system-dependent. In the local validation in previous studies, poor scalability of both empirical and GF models in time, space, and across different DALS systems was observed, which means that field measurements are still needed to calibrate both types of models. The method to correct the impact from the clumping effect and woody material using DALS PCD and the saturation effect for both empirical and GF models still needs further exploration. Of most importance, further work is desired to emphasize assessing the transferability of published methods to new geographic contexts, different DALS sensors, and survey characteristics, based on figuring out the influence of each factor on the LAI retrieval process using DALS PCD. In addition, from a methodological perspective, taking advantage of DALS PCD in characterizing the 3D structure of the canopy, making full use of the ability of machine learning methods in the fusion of multisource data, developing a spatiotemporal scalable model of canopy structure parameters including LAI, and using multisource and heterogeneous data are promising areas of research.
KW  - leaf area index (LAI)
KW  - airborne laser scanner (ALS)
KW  - discrete airborne LiDAR scanner (DALS)
KW  - LiDAR
KW  - LiDAR penetration index (LPI)
DO  - 10.3390/rs13122408
ER  -
TY  - EJOU
AU  - Li, Minhui
AU  - Shamshiri, Redmond R.
AU  - Schirrmann, Michael
AU  - Weltzien, Cornelia
TI  - Impact of Camera Viewing Angle for Estimating Leaf Parameters of Wheat Plants from 3D Point Clouds
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 6
SN  - 2077-0472

AB  - Estimation of plant canopy using low-altitude imagery can help monitor the normal growth status of crops and is highly beneficial for various digital farming applications such as precision crop protection. However, extracting 3D canopy information from raw images requires studying the effect of sensor viewing angle by taking into accounts the limitations of the mobile platform routes inside the field. The main objective of this research was to estimate wheat (Triticum aestivum L.) leaf parameters, including leaf length and width, from the 3D model representation of the plants. For this purpose, experiments with different camera viewing angles were conducted to find the optimum setup of a mono-camera system that would result in the best 3D point clouds. The angle-control analytical study was conducted on a four-row wheat plot with a row spacing of 0.17 m and with two seeding densities and growth stages as factors. Nadir and six oblique view image datasets were acquired from the plot with 88% overlapping and were then reconstructed to point clouds using Structure from Motion (SfM) and Multi-View Stereo (MVS) methods. Point clouds were first categorized into three classes as wheat canopy, soil background, and experimental plot. The wheat canopy class was then used to extract leaf parameters, which were then compared with those values from manual measurements. The comparison between results showed that (i) multiple-view dataset provided the best estimation for leaf length and leaf width, (ii) among the single-view dataset, canopy, and leaf parameters were best modeled with angles vertically at −45° and horizontally at 0° (VA −45, HA 0), while (iii) in nadir view, fewer underlying 3D points were obtained with a missing leaf rate of 70%. It was concluded that oblique imagery is a promising approach to effectively estimate wheat canopy 3D representation with SfM-MVS using a single camera platform for crop monitoring. This study contributes to the improvement of the proximal sensing platform for crop health assessment.
KW  - digital agriculture
KW  - 3D photogrammetry
KW  - response surface methodology
KW  - structure from motion (SfM)
KW  - multi-view stereo (MVS)
DO  - 10.3390/agriculture11060563
ER  -
TY  - EJOU
AU  - Chen, Fang
TI  - Comparing Methods for Segmenting Supra-Glacial Lakes and Surface Features in the Mount Everest Region of the Himalayas Using Chinese GaoFen-3 SAR Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Glaciers and numerous glacial lakes that are produced by glacier melting are key indicators of climate change. Often overlooked, supra-glacial lakes develop in the melting area in the low-lying part of a glacier and appear to be highly variable in their size, shape, and location. The lifespan of these lakes is thought to be quite transient, since the lakes may be completely filled by water and burst out within several weeks. Changes in supra-glacial lake outlines and other surface features such as supra-glacial rivers and crevasses on the glaciers are useful indicators for the direct monitoring of glacier changes. Synthetic aperture radar (SAR) is not affected by weather and climate, and is an effective tool for study of glaciated areas. The development of the Chinese GaoFen-3 (GF-3) SAR, which has high spatial and temporal resolution and high-precision observation performance, has made it possible to obtain dynamic information about glaciers in more detail. In this paper, the classical Canny operator, the variational B-spline level-set method, and U-Net-based deep-learning model were applied and compared to extract glacial lake outlines and other surface features using different modes and Chinese GF-3 SAR imagery in the Mount Everest Region of the Himalayas. Particularly, the U-Net-based deep-learning method, which was independent of auxiliary data and had a high degree of automation, was used for the first time in this context. The experimental results showed that the U-Net-based deep-learning model worked best in the segmentation of supra-glacial lakes in terms of accuracy (Precision = 98.45% and Recall = 95.82%) and segmentation efficiency, and was good at detecting small, elongated, and ice-covered supra-glacial lakes. We also found that it was useful for accurately identifying the location of supra-glacial streams and ice crevasses on glaciers, and quantifying their width. Finally, based on the time series of the mapping results, the spatial characteristics and temporal evolution of these features over the glaciers were comprehensively analyzed. Overall, this study presents a novel approach to improve the detection accuracy of glacier elements that could be leveraged for dynamic monitoring in future research.
KW  - GF-3
KW  - supra-glacial lakes
KW  - supra-glacial streams
KW  - ice crevasses
KW  - segmentation
KW  - digital disaster reduction
DO  - 10.3390/rs13132429
ER  -
TY  - EJOU
AU  - Calamita, Federico
AU  - Imran, Hafiz A.
AU  - Vescovo, Loris
AU  - Mekhalfi, Mohamed L.
AU  - La Porta, Nicola
TI  - Early Identification of Root Rot Disease by Using Hyperspectral Reflectance: The Case of Pathosystem Grapevine/Armillaria
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Armillaria genus represents one of the most common causes of chronic root rot disease in woody plants. Prompt recognition of diseased plants is crucial to control the pathogen. However, the current disease detection methods are limited at a field scale. Therefore, an alternative approach is needed. In this study, we investigated the potential of hyperspectral techniques to identify fungi-infected vs. healthy plants of Vitis vinifera. We used the hyperspectral imaging sensor Specim-IQ to acquire leaves’ reflectance data of the Teroldego Rotaliano grapevine cultivar. We analyzed three different groups of plants: healthy, asymptomatic, and diseased. Highly significant differences were found in the near-infrared (NIR) spectral region with a decreasing pattern from healthy to diseased plants attributable to the leaf mesophyll changes. Asymptomatic plants emerged from the other groups due to a lower reflectance in the red edge spectrum (around 705 nm), ascribable to an accumulation of secondary metabolites involved in plant defense strategies. Further significant differences were observed in the wavelengths close to 550 nm in diseased vs. asymptomatic plants. We evaluated several machine learning paradigms to differentiate the plant groups. The Naïve Bayes (NB) algorithm, combined with the most discriminant variables among vegetation indices and spectral narrow bands, provided the best results with an overall accuracy of 90% and 75% in healthy vs. diseased and healthy vs. asymptomatic plants, respectively. To our knowledge, this study represents the first report on the possibility of using hyperspectral data for root rot disease diagnosis in woody plants. Although further validation studies are required, it appears that the spectral reflectance technique, possibly implemented on unmanned aerial vehicles (UAVs), could be a promising tool for a cost-effective, non-invasive method of Armillaria disease diagnosis and mapping in-field, contributing to a significant step forward in precision viticulture.
KW  - agriculture 4.0
KW  - chlorophyll
KW  - early diagnosis
KW  - fungal tree pathogens
KW  - mycology
KW  - plant disease
KW  - plant pathology
KW  - smart viticulture
KW  - vegetation indices
KW  - wine grapes
DO  - 10.3390/rs13132436
ER  -
TY  - EJOU
AU  - Martos, Vanesa
AU  - Ahmad, Ali
AU  - Cartujo, Pedro
AU  - Ordoñez, Javier
TI  - Ensuring Agricultural Sustainability through Remote Sensing in the Era of Agriculture 5.0
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 13
SN  - 2076-3417

AB  - Timely and reliable information about crop management, production, and yield is considered of great utility by stakeholders (e.g., national and international authorities, farmers, commercial units, etc.) to ensure food safety and security. By 2050, according to Food and Agriculture Organization (FAO) estimates, around 70% more production of agricultural products will be needed to fulfil the demands of the world population. Likewise, to meet the Sustainable Development Goals (SDGs), especially the second goal of “zero hunger”, potential technologies like remote sensing (RS) need to be efficiently integrated into agriculture. The application of RS is indispensable today for a highly productive and sustainable agriculture. Therefore, the present study draws a general overview of RS technology with a special focus on the principal platforms of this technology, i.e., satellites and remotely piloted aircrafts (RPAs), and the sensors used, in relation to the 5th industrial revolution. Nevertheless, since 1957, RS technology has found applications, through the use of satellite imagery, in agriculture, which was later enriched by the incorporation of remotely piloted aircrafts (RPAs), which is further pushing the boundaries of proficiency through the upgrading of sensors capable of higher spectral, spatial, and temporal resolutions. More prominently, wireless sensor technologies (WST) have streamlined real time information acquisition and programming for respective measures. Improved algorithms and sensors can, not only add significant value to crop data acquisition, but can also devise simulations on yield, harvesting and irrigation periods, metrological data, etc., by making use of cloud computing. The RS technology generates huge sets of data that necessitate the incorporation of artificial intelligence (AI) and big data to extract useful products, thereby augmenting the adeptness and efficiency of agriculture to ensure its sustainability. These technologies have made the orientation of current research towards the estimation of plant physiological traits rather than the structural parameters possible. Futuristic approaches for benefiting from these cutting-edge technologies are discussed in this study. This study can be helpful for researchers, academics, and young students aspiring to play a role in the achievement of sustainable agriculture.
KW  - agriculture 5.0
KW  - drones
KW  - remotely piloted aircrafts (RPAs)
KW  - precision agriculture
KW  - remote sensing
KW  - Internet of Things (IoT)
KW  - digital agriculture
KW  - sustainable development goals
KW  - sensors
KW  - agricultural robots
DO  - 10.3390/app11135911
ER  -
TY  - EJOU
AU  - Zamboni, Pedro
AU  - Junior, José M.
AU  - Silva, Jonathan D.
AU  - Miyoshi, Gabriela T.
AU  - Matsubara, Edson T.
AU  - Nogueira, Keiller
AU  - Gonçalves, Wesley N.
TI  - Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 × 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications.
KW  - object detection
KW  - convolutional neural network
KW  - remote sensing
DO  - 10.3390/rs13132482
ER  -
TY  - EJOU
AU  - Ouhami, Maryam
AU  - Hafiane, Adel
AU  - Es-Saady, Youssef
AU  - El Hajji, Mohamed
AU  - Canals, Raphael
TI  - Computer Vision, IoT and Data Fusion for Crop Disease Detection Using Machine Learning: A Survey and Ongoing Research
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Crop diseases constitute a serious issue in agriculture, affecting both quality and quantity of agriculture production. Disease control has been a research object in many scientific and technologic domains. Technological advances in sensors, data storage, computing resources and artificial intelligence have shown enormous potential to control diseases effectively. A growing body of literature recognizes the importance of using data from different types of sensors and machine learning approaches to build models for detection, prediction, analysis, assessment, etc. However, the increasing number and diversity of research studies requires a literature review for further developments and contributions in this area. This paper reviews state-of-the-art machine learning methods that use different data sources, applied to plant disease detection. It lists traditional and deep learning methods associated with the main data acquisition modalities, namely IoT, ground imaging, unmanned aerial vehicle imaging and satellite imaging. In addition, this study examines the role of data fusion for ongoing research in the context of disease detection. It highlights the advantage of intelligent data fusion techniques, from heterogeneous data sources, to improve plant health status prediction and presents the main challenges facing this field. The study concludes with a discussion of several current issues and research trends.
KW  - plant disease
KW  - machine learning
KW  - remote sensing
KW  - intelligent sensors
KW  - data fusion
DO  - 10.3390/rs13132486
ER  -
TY  - EJOU
AU  - Nabwire, Shona
AU  - Suh, Hyun-Kwon
AU  - Kim, Moon S.
AU  - Baek, Insuck
AU  - Cho, Byoung-Kwan
TI  - Review: Application of Artificial Intelligence in Phenomics
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 13
SN  - 1424-8220

AB  - Plant phenomics has been rapidly advancing over the past few years. This advancement is attributed to the increased innovation and availability of new technologies which can enable the high-throughput phenotyping of complex plant traits. The application of artificial intelligence in various domains of science has also grown exponentially in recent years. Notably, the computer vision, machine learning, and deep learning aspects of artificial intelligence have been successfully integrated into non-invasive imaging techniques. This integration is gradually improving the efficiency of data collection and analysis through the application of machine and deep learning for robust image analysis. In addition, artificial intelligence has fostered the development of software and tools applied in field phenotyping for data collection and management. These include open-source devices and tools which are enabling community driven research and data-sharing, thereby availing the large amounts of data required for the accurate study of phenotypes. This paper reviews more than one hundred current state-of-the-art papers concerning AI-applied plant phenotyping published between 2010 and 2020. It provides an overview of current phenotyping technologies and the ongoing integration of artificial intelligence into plant phenotyping. Lastly, the limitations of the current approaches/methods and future directions are discussed.
KW  - artificial intelligence
KW  - deep learning
KW  - plant phenomics
KW  - field phenotyping
KW  - high throughput phenotyping
KW  - image-based phenotyping
DO  - 10.3390/s21134363
ER  -
TY  - EJOU
AU  - Shrestha, Rakesh
AU  - Omidkar, Atefeh
AU  - Roudi, Sajjad A.
AU  - Abbas, Robert
AU  - Kim, Shiho
TI  - Machine-Learning-Enabled Intrusion Detection System for Cellular Connected UAV Networks
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 13
SN  - 2079-9292

AB  - The recent development and adoption of unmanned aerial vehicles (UAVs) is due to its wide variety of applications in public and private sector from parcel delivery to wildlife conservation. The integration of UAVs, 5G, and satellite technologies has prompted telecommunication networks to evolve to provide higher-quality and more stable service to remote areas. However, security concerns with UAVs are growing as UAV nodes are becoming attractive targets for cyberattacks due to enormously growing volumes and poor and weak inbuilt security. In this paper, we propose a UAV- and satellite-based 5G-network security model that can harness machine learning to effectively detect of vulnerabilities and cyberattacks. The solution is divided into two main parts: the model creation for intrusion detection using various machine learning (ML) algorithms and the implementation of ML-based model into terrestrial or satellite gateways. The system identifies various attack types using realistic CSE-CIC IDS-2018 network datasets published by Canadian Establishment for Cybersecurity (CIC). It consists of seven different types of new and contemporary attack types. This paper demonstrates that ML algorithms can be used to classify benign or malicious packets in UAV networks to enhance security. Finally, the tested ML algorithms are compared for effectiveness in terms of accuracy rate, precision, recall, F1-score, and false-negative rate. The decision tree algorithm performed well by obtaining a maximum accuracy rate of 99.99% and a minimum false negative rate of 0% in detecting various attacks as compared to all other types of ML classifiers.
KW  - UAV
KW  - machine learning
KW  - intrusion detection system
KW  - cybersecurity attacks
KW  - software-defined security
DO  - 10.3390/electronics10131549
ER  -
TY  - EJOU
AU  - Khoroshevsky, Faina
AU  - Khoroshevsky, Stanislav
AU  - Bar-Hillel, Aharon
TI  - Parts-per-Object Count in Agricultural Images: Solving Phenotyping Problems via a Single Deep Neural Network
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Solving many phenotyping problems involves not only automatic detection of objects in an image, but also counting the number of parts per object. We propose a solution in the form of a single deep network, tested for three agricultural datasets pertaining to bananas-per-bunch, spikelets-per-wheat-spike, and berries-per-grape-cluster. The suggested network incorporates object detection, object resizing, and part counting as modules in a single deep network, with several variants tested. The detection module is based on a Retina-Net architecture, whereas for the counting modules, two different architectures are examined: the first based on direct regression of the predicted count, and the other on explicit parts detection and counting. The results are promising, with the mean relative deviation between estimated and visible part count in the range of 9.2% to 11.5%. Further inference of count-based yield related statistics is considered. For banana bunches, the actual banana count (including occluded bananas) is inferred from the count of visible bananas. For spikelets-per-wheat-spike, robust estimation methods are employed to get the average spikelet count across the field, which is an effective yield estimator.
KW  - phenotyping problems
KW  - deep learning
KW  - parts-per-object count
KW  - object detection
KW  - robust estimation
DO  - 10.3390/rs13132496
ER  -
TY  - EJOU
AU  - Ukaegbu, Uchechi F.
AU  - Tartibu, Lagouge K.
AU  - Okwu, Modestus O.
AU  - Olayode, Isaac O.
TI  - Development of a Light-Weight Unmanned Aerial Vehicle for Precision Agriculture
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 13
SN  - 1424-8220

AB  - This paper describes the development of a modular unmanned aerial vehicle for the detection and eradication of weeds on farmland. Precision agriculture entails solving the problem of poor agricultural yield due to competition for nutrients by weeds and provides a faster approach to eliminating the problematic weeds using emerging technologies. This research has addressed the aforementioned problem. A quadcopter was built, and components were assembled with light-weight materials. The system consists of the electric motor, electronic speed controller, propellers, frame, lithium polymer (li-po) battery, flight controller, a global positioning system (GPS), and receiver. A sprayer module which consists of a relay, Raspberry Pi 3, spray pump, 12 V DC source, water hose, and the tank was built. It operated in such a way that when a weed is detected based on the deep learning algorithms deployed on the Raspberry Pi, general purpose input/output (GPIO) 17 or GPIO 18 (of the Raspberry Pi) were activated to supply 3.3 V, which turned on a DC relay to spray herbicides accordingly. The sprayer module was mounted on the quadcopter and from the test-running operation conducted, broadleaf and grass weeds were accurately detected and the spraying of herbicides according to the weed type occurred in less than a second.
KW  - unmanned aerial vehicle (UAV)
KW  - deep learning
KW  - Raspberry Pi 3
KW  - industry 4.0
KW  - precision agriculture
DO  - 10.3390/s21134417
ER  -
TY  - EJOU
AU  - Al-Nuaimi, Mohammed
AU  - Wibowo, Sapto
AU  - Qu, Hongyang
AU  - Aitken, Jonathan
AU  - Veres, Sandor
TI  - Hybrid Verification Technique for Decision-Making of Self-Driving Vehicles
T2  - Journal of Sensor and Actuator Networks

PY  - 2021
VL  - 10
IS  - 3
SN  - 2224-2708

AB  - The evolution of driving technology has recently progressed from active safety features and ADAS systems to fully sensor-guided autonomous driving. Bringing such a vehicle to market requires not only simulation and testing but formal verification to account for all possible traffic scenarios. A new verification approach, which combines the use of two well-known model checkers: model checker for multi-agent systems (MCMAS) and probabilistic model checker (PRISM), is presented for this purpose. The overall structure of our autonomous vehicle (AV) system consists of: (1) A perception system of sensors that feeds data into (2) a rational agent (RA) based on a belief–desire–intention (BDI) architecture, which uses a model of the environment and is connected to the RA for verification of decision-making, and (3) a feedback control systems for following a self-planned path. MCMAS is used to check the consistency and stability of the BDI agent logic during design-time. PRISM is used to provide the RA with the probability of success while it decides to take action during run-time operation. This allows the RA to select movements of the highest probability of success from several generated alternatives. This framework has been tested on a new AV software platform built using the robot operating system (ROS) and virtual reality (VR) Gazebo Simulator. It also includes a parking lot scenario to test the feasibility of this approach in a realistic environment. A practical implementation of the AV system was also carried out on the experimental testbed.
KW  - self-driving vehicle
KW  - formal verification
KW  - model checking
KW  - rational agent
KW  - decision-making
KW  - ROS
DO  - 10.3390/jsan10030042
ER  -
TY  - EJOU
AU  - Niu, Zijie
AU  - Deng, Juntao
AU  - Zhang, Xu
AU  - Zhang, Jun
AU  - Pan, Shijia
AU  - Mu, Haotian
TI  - Identifying the Branch of Kiwifruit Based on Unmanned Aerial Vehicle (UAV) Images Using Deep Learning Method
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 13
SN  - 1424-8220

AB  - It is important to obtain accurate information about kiwifruit vines to monitoring their physiological states and undertake precise orchard operations. However, because vines are small and cling to trellises, and have branches laying on the ground, numerous challenges exist in the acquisition of accurate data for kiwifruit vines. In this paper, a kiwifruit canopy distribution prediction model is proposed on the basis of low-altitude unmanned aerial vehicle (UAV) images and deep learning techniques. First, the location of the kiwifruit plants and vine distribution are extracted from high-precision images collected by UAV. The canopy gradient distribution maps with different noise reduction and distribution effects are generated by modifying the threshold and sampling size using the resampling normalization method. The results showed that the accuracies of the vine segmentation using PSPnet, support vector machine, and random forest classification were 71.2%, 85.8%, and 75.26%, respectively. However, the segmentation image obtained using depth semantic segmentation had a higher signal-to-noise ratio and was closer to the real situation. The average intersection over union of the deep semantic segmentation was more than or equal to 80% in distribution maps, whereas, in traditional machine learning, the average intersection was between 20% and 60%. This indicates the proposed model can quickly extract the vine distribution and plant position, and is thus able to perform dynamic monitoring of orchards to provide real-time operation guidance.
KW  - deep learning
KW  - unmanned aerial vehicle
KW  - kiwifruit
KW  - image segmentation
DO  - 10.3390/s21134442
ER  -
TY  - EJOU
AU  - Kong, Deheng
AU  - Wu, Faquan
AU  - Saroglou, Charalampos
AU  - Sha, Peng
AU  - Li, Bo
TI  - In-Situ Block Characterization of Jointed Rock Exposures Based on a 3D Point Cloud Model
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - The importance of in-situ rock block characterization has been realized for decades in rock mechanics and engineering, yet how to reliably measure and characterize the geometrical properties of blocks in varied forms of exposures and patterns of jointing is still a challenging task. Using a point cloud model (PCM) of rock exposures generated from remote sensing techniques, we developed a consistent and comprehensive method for rock block characterization that is composed of two different procedures and a block indicator system. A semi-automatic procedure towards the robust extraction of in-situ rock blocks created by the deterministic discontinuity network on rock exposures (PCM-DDN) was developed. A 3D stochastic discrete fracture network (DFN) simulation (PCM-SDS) procedure was built based on the statistically valid representation of the discontinuity network geometry. A multi-dimensional block indicator system, i.e., the block size, shape, orientation, and spatial distribution pattern for systematic and objective block characterization, was then established. The developed method was applied to a synthetic model of cardboard boxes and three different rock engineering scenarios, including a road cut slope from Spain and two open-pit mining slopes from China. Compared with existing empirical methods, the proposed procedures and the block indicator system are dependable and practically feasible, which can help enhance our understanding of block geometry characteristics in related applications.
KW  - rock block characterization
KW  - remote sensing
KW  - point cloud
KW  - discontinuity network
KW  - block size
DO  - 10.3390/rs13132540
ER  -
TY  - EJOU
AU  - Habibi, Luthfan N.
AU  - Watanabe, Tomoya
AU  - Matsui, Tsutomu
AU  - Tanaka, Takashi S. T.
TI  - Machine Learning Techniques to Predict Soybean Plant Density Using UAV and Satellite-Based Remote Sensing
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - The plant density of soybean is a critical factor affecting plant canopy structure and yield. Predicting the spatial variability of plant density would be valuable for improving agronomic practices. The objective of this study was to develop a model for plant density measurement using several data sets with different spatial resolutions, including unmanned aerial vehicle (UAV) imagery, PlanetScope satellite imagery, and climate data. The model establishment process includes (1) performing the high-throughput measurement of actual plant density from UAV imagery with the You Only Look Once version 3 (YOLOv3) object detection algorithm, which was further treated as a response variable of the estimation models in the next step, and (2) developing regression models to estimate plant density in the extended areas using various combinations of predictors derived from PlanetScope imagery and climate data. Our results showed that the YOLOv3 model can accurately measure actual soybean plant density from UAV imagery data with a root mean square error (RMSE) value of 0.96 plants m−2. Furthermore, the two regression models, partial least squares and random forest (RF), successfully expanded the plant density prediction areas with RMSE values ranging from 1.78 to 3.67 plant m−2. Model improvement was conducted using the variable importance feature in RF, which improved prediction accuracy with an RMSE value of 1.72 plant m−2. These results demonstrated that the established model had an acceptable prediction accuracy for estimating plant density. Although the model could not often evaluate the within-field spatial variability of soybean plant density, the predicted values were sufficient for informing the field-specific status.
KW  - PlanetScope
KW  - random forest
KW  - partial least squares regression
KW  - spatial variation
KW  - spectral reflectance
KW  - YOLOv3
DO  - 10.3390/rs13132548
ER  -
TY  - EJOU
AU  - Yoosefzadeh-Najafabadi, Mohsen
AU  - Tulpan, Dan
AU  - Eskandari, Milad
TI  - Using Hybrid Artificial Intelligence and Evolutionary Optimization Algorithms for Estimating Soybean Yield and Fresh Biomass Using Hyperspectral Vegetation Indices
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Recent advanced high-throughput field phenotyping combined with sophisticated big data analysis methods have provided plant breeders with unprecedented tools for a better prediction of important agronomic traits, such as yield and fresh biomass (FBIO), at early growth stages. This study aimed to demonstrate the potential use of 35 selected hyperspectral vegetation indices (HVI), collected at the R5 growth stage, for predicting soybean seed yield and FBIO. Two artificial intelligence algorithms, ensemble-bagging (EB) and deep neural network (DNN), were used to predict soybean seed yield and FBIO using HVI. Considering HVI as input variables, the coefficients of determination (R2) of 0.76 and 0.77 for yield and 0.91 and 0.89 for FBIO were obtained using DNN and EB, respectively. In this study, we also used hybrid DNN-SPEA2 to estimate the optimum HVI values in soybeans with maximized yield and FBIO productions. In addition, to identify the most informative HVI in predicting yield and FBIO, the feature recursive elimination wrapper method was used and the top ranking HVI were determined to be associated with red, 670 nm and near-infrared, 800 nm, regions. Overall, this study introduced hybrid DNN-SPEA2 as a robust mathematical tool for optimizing and using informative HVI for estimating soybean seed yield and FBIO at early growth stages, which can be employed by soybean breeders for discriminating superior genotypes in large breeding populations.
KW  - high-throughput phenotyping
KW  - machine learning
KW  - multi-objective optimization algorithm
KW  - radial basis function
KW  - random forest
KW  - support vector regression
KW  - SPEA2
DO  - 10.3390/rs13132555
ER  -
TY  - EJOU
AU  - Matin, Sahar S.
AU  - Pradhan, Biswajeet
TI  - Earthquake-Induced Building-Damage Mapping Using Explainable AI (XAI)
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 13
SN  - 1424-8220

AB  - Building-damage mapping using remote sensing images plays a critical role in providing quick and accurate information for the first responders after major earthquakes. In recent years, there has been an increasing interest in generating post-earthquake building-damage maps automatically using different artificial intelligence (AI)-based frameworks. These frameworks in this domain are promising, yet not reliable for several reasons, including but not limited to the site-specific design of the methods, the lack of transparency in the AI-model, the lack of quality in the labelled image, and the use of irrelevant descriptor features in building the AI-model. Using explainable AI (XAI) can lead us to gain insight into identifying these limitations and therefore, to modify the training dataset and the model accordingly. This paper proposes the use of SHAP (Shapley additive explanation) to interpret the outputs of a multilayer perceptron (MLP)—a machine learning model—and analyse the impact of each feature descriptor included in the model for building-damage assessment to examine the reliability of the model. In this study, a post-event satellite image from the 2018 Palu earthquake was used. The results show that MLP can classify the collapsed and non-collapsed buildings with an overall accuracy of 84% after removing the redundant features. Further, spectral features are found to be more important than texture features in distinguishing the collapsed and non-collapsed buildings. Finally, we argue that constructing an explainable model would help to understand the model’s decision to classify the buildings as collapsed and non-collapsed and open avenues to build a transferable AI model.
KW  - building-damage mapping
KW  - feature analysis
KW  - explainable AI
KW  - machine learning
KW  - remote sensing
DO  - 10.3390/s21134489
ER  -
TY  - EJOU
AU  - Mbiydzenyuy, Gideon
AU  - Nowaczyk, Sławomir
AU  - Knutsson, Håkan
AU  - Vanhoudt, Dirk
AU  - Brage, Jens
AU  - Calikus, Ece
TI  - Opportunities for Machine Learning in District Heating
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 13
SN  - 2076-3417

AB  - The district heating (DH) industry is facing an important transformation towards more efficient networks that utilise significantly lower water temperatures to distribute the heat. This change requires taking advantage of new technologies, and Machine Learning (ML) is a popular direction. In the last decade, we have witnessed an extreme growth in the number of published research papers that focus on applying ML techniques to the DH domain. However, based on our experience in the field, and an extensive review of the state-of-the-art, we perceive a mismatch between the most popular research directions, such as forecasting, and the challenges faced by the DH industry. In this work, we present our findings, explain and demonstrate the key gaps between the two communities and suggest a road-map ahead towards increasing the impact of ML research in the DH industry.
KW  - Machine Learning
KW  - district heating
KW  - review
KW  - road-map
KW  - research opportunities
DO  - 10.3390/app11136112
ER  -
TY  - EJOU
AU  - Ackerson, Joseph M.
AU  - Dave, Rushit
AU  - Seliya, Naeem
TI  - Applications of Recurrent Neural Network for Biometric Authentication &amp; Anomaly Detection
T2  - Information

PY  - 2021
VL  - 12
IS  - 7
SN  - 2078-2489

AB  - Recurrent Neural Networks are powerful machine learning frameworks that allow for data to be saved and referenced in a temporal sequence. This opens many new possibilities in fields such as handwriting analysis and speech recognition. This paper seeks to explore current research being conducted on RNNs in four very important areas, being biometric authentication, expression recognition, anomaly detection, and applications to aircraft. This paper reviews the methodologies, purpose, results, and the benefits and drawbacks of each proposed method below. These various methodologies all focus on how they can leverage distinct RNN architectures such as the popular Long Short-Term Memory (LSTM) RNN or a Deep-Residual RNN. This paper also examines which frameworks work best in certain situations, and the advantages and disadvantages of each proposed model.
KW  - recurrent neural network
KW  - biometric authentication
KW  - expression recognition
KW  - anomaly detection
KW  - smartphone authentication
KW  - mouse-based authentication
KW  - aircraft trajectory prediction
DO  - 10.3390/info12070272
ER  -
TY  - EJOU
AU  - Swinney, Carolyn J.
AU  - Woods, John C.
TI  - The Effect of Real-World Interference on CNN Feature Extraction and Machine Learning Classification of Unmanned Aerial Systems
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 7
SN  - 2226-4310

AB  - Small unmanned aerial systems (UASs) present many potential solutions and enhancements to industry today but equally pose a significant security challenge. We only need to look at the levels of disruption caused by UASs at airports in recent years. The accuracy of UAS detection and classification systems based on radio frequency (RF) signals can be hindered by other interfering signals present in the same frequency band, such as Bluetooth and Wi-Fi devices. In this paper, we evaluate the effect of real-world interference from Bluetooth and Wi-Fi signals concurrently on convolutional neural network (CNN) feature extraction and machine learning classification of UASs. We assess multiple UASs that operate using different transmission systems: Wi-Fi, Lightbridge 2.0, OcuSync 1.0, OcuSync 2.0 and the recently released OcuSync 3.0. We consider 7 popular UASs, evaluating 2 class UAS detection, 8 class UAS type classification and 21 class UAS flight mode classification. Our results show that the process of CNN feature extraction using transfer learning and machine learning classification is fairly robust in the presence of real-world interference. We also show that UASs that are operating using the same transmission system can be distinguished. In the presence of interference from both Bluetooth and Wi-Fi signals, our results show 100% accuracy for UAV detection (2 classes), 98.1% (+/−0.4%) for UAV type classification (8 classes) and 95.4% (+/−0.3%) for UAV flight mode classification (21 classes).
KW  - unmanned aerial vehicles
KW  - unmanned aerial systems
KW  - interference
KW  - UAS detection
KW  - RF spectrum analysis
KW  - machine learning classification
KW  - deep learning
KW  - convolutional neural network
KW  - transfer learning
KW  - signal analysis
DO  - 10.3390/aerospace8070179
ER  -
TY  - EJOU
AU  - Kaczorowska, Monika
AU  - Karczmarek, Paweł
AU  - Plechawska-Wójcik, Małgorzata
AU  - Tokovarov, Mikhail
TI  - On the Improvement of Eye Tracking-Based Cognitive Workload Estimation Using Aggregation Functions
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 13
SN  - 1424-8220

AB  - Cognitive workload, being a quantitative measure of mental effort, draws significant interest of researchers, as it allows to monitor the state of mental fatigue. Estimation of cognitive workload becomes especially important for job positions requiring outstanding engagement and responsibility, e.g., air-traffic dispatchers, pilots, car or train drivers. Cognitive workload estimation finds its applications also in the field of education material preparation. It allows to monitor the difficulty degree for specific tasks enabling to adjust the level of education materials to typical abilities of students. In this study, we present the results of research conducted with the goal of examining the influence of various fuzzy or non-fuzzy aggregation functions upon the quality of cognitive workload estimation. Various classic machine learning models were successfully applied to the problem. The results of extensive in-depth experiments with over 2000 aggregation operators shows the applicability of the approach based on the aggregation functions. Moreover, the approach based on aggregation process allows for further improvement of classification results. A wide range of aggregation functions is considered and the results suggest that the combination of classical machine learning models and aggregation methods allows to achieve high quality of cognitive workload level recognition preserving low computational cost.
KW  - aggregation
KW  - generalized Choquet integral
KW  - fuzzy measure
KW  - classical machine learning
KW  - cognitive workload
DO  - 10.3390/s21134542
ER  -
TY  - EJOU
AU  - Mohan, Midhun
AU  - Richardson, Gabriella
AU  - Gopan, Gopika
AU  - Aghai, Matthew M.
AU  - Bajaj, Shaurya
AU  - Galgamuwa, G. A. Pabodha
AU  - Vastaranta, Mikko
AU  - Arachchige, Pavithra S. Pitumpe
AU  - Amorós, Lot
AU  - Corte, Ana P.
AU  - de-Miguel, Sergio
AU  - Leite, Rodrigo V.
AU  - Kganyago, Mahlatse
AU  - Broadbent, Eben N.
AU  - Doaemo, Willie
AU  - Shorab, Mohammed A.
AU  - Cardil, Adrian
TI  - UAV-Supported Forest Regeneration: Current Trends, Challenges and Implications
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Replanting trees helps with avoiding desertification, reducing the chances of soil erosion and flooding, minimizing the risks of zoonotic disease outbreaks, and providing ecosystem services and livelihood to the indigenous people, in addition to sequestering carbon dioxide for mitigating climate change. Consequently, it is important to explore new methods and technologies that are aiming to upscale and fast-track afforestation and reforestation (A/R) endeavors, given that many of the current tree planting strategies are not cost effective over large landscapes, and suffer from constraints associated with time, energy, manpower, and nursery-based seedling production. UAV (unmanned aerial vehicle)-supported seed sowing (UAVsSS) can promote rapid A/R in a safe, cost-effective, fast and environmentally friendly manner, if performed correctly, even in otherwise unsafe and/or inaccessible terrains, supplementing the overall manual planting efforts globally. In this study, we reviewed the recent literature on UAVsSS, to analyze the current status of the technology. Primary UAVsSS applications were found to be in areas of post-wildfire reforestation, mangrove restoration, forest restoration after degradation, weed eradication, and desert greening. Nonetheless, low survival rates of the seeds, future forest diversity, weather limitations, financial constraints, and seed-firing accuracy concerns were determined as major challenges to operationalization. Based on our literature survey and qualitative analysis, twelve recommendations—ranging from the need for publishing germination results to linking UAVsSS operations with carbon offset markets—are provided for the advancement of UAVsSS applications.
KW  - planting trees with drones
KW  - seed pods
KW  - unmanned aerial system (UAS)
KW  - seed spraying drones
KW  - forestry applications of UAVs
KW  - afforestation and reforestation using UAVs
DO  - 10.3390/rs13132596
ER  -
TY  - EJOU
AU  - Bahrami, Hazhir
AU  - Homayouni, Saeid
AU  - Safari, Abdolreza
AU  - Mirzaei, Sayeh
AU  - Mahdianpari, Masoud
AU  - Reisi-Gahrouei, Omid
TI  - Deep Learning-Based Estimation of Crop Biophysical Parameters Using Multi-Source and Multi-Temporal Remote Sensing Observations
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 7
SN  - 2073-4395

AB  - Remote sensing data are considered as one of the primary data sources for precise agriculture. Several studies have demonstrated the excellent capability of radar and optical imagery for crop mapping and biophysical parameter estimation. This paper aims at modeling the crop biophysical parameters, e.g., Leaf Area Index (LAI) and biomass, using a combination of radar and optical Earth observations. We extracted several radar features from polarimetric Synthetic Aperture Radar (SAR) data and Vegetation Indices (VIs) from optical images to model crops’ LAI and dry biomass. Then, the mutual correlations between these features and Random Forest feature importance were calculated. We considered two scenarios to estimate crop parameters. First, Machine Learning (ML) algorithms, e.g., Support Vector Regression (SVR), Random Forest (RF), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB), were utilized to estimate two crop biophysical parameters. To this end, crops’ dry biomass and LAI were estimated using three input data; (1) SAR polarimetric features; (2) spectral VIs; (3) integrating both SAR and optical features. Second, a deep artificial neural network was created. These input data were fed to the mentioned algorithms and evaluated using the in-situ measurements. These observations of three cash crops, including soybean, corn, and canola, have been collected over Manitoba, Canada, during the Soil Moisture Active Validation Experimental 2012 (SMAPVEX-12) campaign. The results showed that GB and XGB have great potential in parameter estimation and remarkably improved accuracy. Our results also demonstrated a significant improvement in the dry biomass and LAI estimation compared to the previous studies. For LAI, the validation Root Mean Square Error (RMSE) was reported as 0.557 m2/m2 for canola using GB, and 0.298 m2/m2 for corn using GB, 0.233 m2/m2 for soybean using XGB. RMSE was reported for dry biomass as 26.29 g/m2 for canola utilizing SVR, 57.97 g/m2 for corn using RF, and 5.00 g/m2 for soybean using GB. The results revealed that the deep artificial neural network had a better potential to estimate crop parameters than the ML algorithms.
KW  - crop biomass
KW  - Leaf Area Index
KW  - Earth observations
KW  - Synthetic Aperture Radar
KW  - optical images
KW  - machine learning algorithms
KW  - SMAPVEX-12
DO  - 10.3390/agronomy11071363
ER  -
TY  - EJOU
AU  - Jang, Hyoseon
AU  - Kim, Sangkyun
AU  - Yoo, Suhong
AU  - Han, Soohee
AU  - Sohn, Hong-Gyoo
TI  - Feature Matching Combining Radiometric and Geometric Characteristics of Images, Applied to Oblique- and Nadir-Looking Visible and TIR Sensors of UAV Imagery
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 13
SN  - 1424-8220

AB  - A large amount of information needs to be identified and produced during the process of promoting projects of interest. Thermal infrared (TIR) images are extensively used because they can provide information that cannot be extracted from visible images. In particular, TIR oblique images facilitate the acquisition of information of a building’s facade that is challenging to obtain from a nadir image. When a TIR oblique image and the 3D information acquired from conventional visible nadir imagery are combined, a great synergy for identifying surface information can be created. However, it is an onerous task to match common points in the images. In this study, a robust matching method of image pairs combined with different wavelengths and geometries (i.e., visible nadir-looking vs. TIR oblique, and visible oblique vs. TIR nadir-looking) is proposed. Three main processes of phase congruency, histogram matching, and Image Matching by Affine Simulation (IMAS) were adjusted to accommodate the radiometric and geometric differences of matched image pairs. The method was applied to Unmanned Aerial Vehicle (UAV) images of building and non-building areas. The results were compared with frequently used matching techniques, such as scale-invariant feature transform (SIFT), speeded-up robust features (SURF), synthetic aperture radar–SIFT (SAR–SIFT), and Affine SIFT (ASIFT). The method outperforms other matching methods in root mean square error (RMSE) and matching performance (matched and not matched). The proposed method is believed to be a reliable solution for pinpointing surface information through image matching with different geometries obtained via TIR and visible sensors.
KW  - thermal infrared (TIR) oblique image
KW  - geometry
KW  - wavelength
KW  - phase congruency
KW  - histogram matching
KW  - Image Matching by Affine Simulation (IMAS)
KW  - Unmanned Aerial Vehicle (UAV)
DO  - 10.3390/s21134587
ER  -
TY  - EJOU
AU  - Moura, Marks M.
AU  - de Oliveira, Luiz E.
AU  - Sanquetta, Carlos R.
AU  - Bastos, Alexis
AU  - Mohan, Midhun
AU  - Corte, Ana P.
TI  - Towards Amazon Forest Restoration: Automatic Detection of Species from UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Precise assessments of forest species’ composition help analyze biodiversity patterns, estimate wood stocks, and improve carbon stock estimates. Therefore, the objective of this work was to evaluate the use of high-resolution images obtained from Unmanned Aerial Vehicle (UAV) for the identification of forest species in areas of forest regeneration in the Amazon. For this purpose, convolutional neural networks (CNN) were trained using the Keras–Tensorflow package with the faster_rcnn_inception_v2_pets model. Samples of six forest species were used to train CNN. From these, attempts were made with the number of thresholds, which is the cutoff value of the function; any value below this output is considered 0, and values above are treated as an output 1; that is, values above the value stipulated in the Threshold are considered as identified species. The results showed that the reduction in the threshold decreases the accuracy of identification, as well as the overlap of the polygons of species identification. However, in comparison with the data collected in the field, it was observed that there exists a high correlation between the trees identified by the CNN and those observed in the plots. The statistical metrics used to validate the classification results showed that CNN are able to identify species with accuracy above 90%. Based on our results, which demonstrate good accuracy and precision in the identification of species, we conclude that convolutional neural networks are an effective tool in classifying objects from UAV images.
KW  - deep learning
KW  - drone
KW  - forest identification
KW  - unmanned aerial vehicles
DO  - 10.3390/rs13132627
ER  -
TY  - EJOU
AU  - Grybas, Heather
AU  - Congalton, Russell G.
TI  - A Comparison of Multi-Temporal RGB and Multispectral UAS Imagery for Tree Species Classification in Heterogeneous New Hampshire Forests
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 13
SN  - 2072-4292

AB  - Unmanned aerial systems (UASs) have recently become an affordable means to map forests at the species level, but research into the performance of different classification methodologies and sensors is necessary so users can make informed choices that maximize accuracy. This study investigated whether multi-temporal UAS data improved the classified accuracy of 14 species examined the optimal time-window for data collection, and compared the performance of a consumer-grade RGB sensor to that of a multispectral sensor. A time series of UAS data was collected from early spring to mid-summer and a sequence of mono-temporal and multi-temporal classifications were carried out. Kappa comparisons were conducted to ascertain whether the multi-temporal classifications significantly improved accuracy and whether there were significant differences between the RGB and multispectral classifications. The multi-temporal classification approach significantly improved accuracy; however, there was no significant benefit when more than three dates were used. Mid- to late spring imagery produced the highest accuracies, potentially due to high spectral heterogeneity between species and homogeneity within species during this time. The RGB sensor exhibited significantly higher accuracies, probably due to the blue band, which was found to be very important for classification accuracy and lacking in the multispectral sensor employed here.
KW  - remote sensing
KW  - forests
KW  - New Hampshire
KW  - UAS
KW  - multi-temporal
KW  - species level
KW  - OBIA
DO  - 10.3390/rs13132631
ER  -
TY  - EJOU
AU  - Wang, Hao
AU  - Ren, Yaxin
AU  - Meng, Zhijun
TI  - A Farm Management Information System for Semi-Supervised Path Planning and Autonomous Vehicle Control
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 13
SN  - 2071-1050

AB  - This paper presents a farm management information system targeting improvements in the ease of use and sustainability of robot farming systems. The system integrates the functionalities of field survey, path planning, monitoring, and controlling agricultural vehicles in real time. Firstly, a Grabcut-based semi-supervised field registration method is proposed for arable field detection from the orthoimage taken by the drone with an RGB camera. It partitions a complex field into simple geometric entities with simple user interaction. The average Mean Intersection over Union is about 0.95 when the field size ranges from 2.74 ha to 5.06 ha. In addition, a desktop software and a web application are developed as the entity of an FMIS. Compared to existing FMISs, this system provides more advanced features in robot farming, while providing simpler user interaction and better results. It allows clients to invoke web services and receive responses independent of programming language and platforms. Moreover, the system is compatible with other services, users, and devices following the open-source access protocol. We have evaluated the system by controlling 5 robot tractors with a 2 Hz communication frequency. The communication protocols will be publicly available to protentional users.
KW  - smart agriculture
KW  - image segmentation
KW  - agricultural robot
KW  - field registration
DO  - 10.3390/su13137497
ER  -
TY  - EJOU
AU  - Munawar, Hafiz S.
AU  - Ullah, Fahim
AU  - Qayyum, Siddra
AU  - Khan, Sara I.
AU  - Mojtahedi, Mohammad
TI  - UAVs in Disaster Management: Application of Integrated Aerial Imagery and Convolutional Neural Network for Flood Detection
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 14
SN  - 2071-1050

AB  - Floods have been a major cause of destruction, instigating fatalities and massive damage to the infrastructure and overall economy of the affected country. Flood-related devastation results in the loss of homes, buildings, and critical infrastructure, leaving no means of communication or travel for the people stuck in such disasters. Thus, it is essential to develop systems that can detect floods in a region to provide timely aid and relief to stranded people, save their livelihoods, homes, and buildings, and protect key city infrastructure. Flood prediction and warning systems have been implemented in developed countries, but the manufacturing cost of such systems is too high for developing countries. Remote sensing, satellite imagery, global positioning system, and geographical information systems are currently used for flood detection to assess the flood-related damages. These techniques use neural networks, machine learning, or deep learning methods. However, unmanned aerial vehicles (UAVs) coupled with convolution neural networks have not been explored in these contexts to instigate a swift disaster management response to minimize damage to infrastructure. Accordingly, this paper uses UAV-based aerial imagery as a flood detection method based on Convolutional Neural Network (CNN) to extract flood-related features from the images of the disaster zone. This method is effective in assessing the damage to local infrastructures in the disaster zones. The study area is based on a flood-prone region of the Indus River in Pakistan, where both pre-and post-disaster images are collected through UAVs. For the training phase, 2150 image patches are created by resizing and cropping the source images. These patches in the training dataset train the CNN model to detect and extract the regions where a flood-related change has occurred. The model is tested against both pre-and post-disaster images to validate it, which has positive flood detection results with an accuracy of 91%. Disaster management organizations can use this model to assess the damages to critical city infrastructure and other assets worldwide to instigate proper disaster responses and minimize the damages. This can help with the smart governance of the cities where all emergent disasters are addressed promptly.
KW  - convolutional neural network (CNN)
KW  - disaster management
KW  - aerial imagery
KW  - flood detection
KW  - unmanned aerial vehicles (UAVs)
DO  - 10.3390/su13147547
ER  -
TY  - EJOU
AU  - Jozdani, Shahab
AU  - Chen, Dongmei
AU  - Chen, Wenjun
AU  - Leblanc, Sylvain G.
AU  - Prévost, Christian
AU  - Lovitt, Julie
AU  - He, Liming
AU  - Johnson, Brian A.
TI  - Leveraging Deep Neural Networks to Map Caribou Lichen in High-Resolution Satellite Images Based on a Small-Scale, Noisy UAV-Derived Map
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - Lichen is an important food source for caribou in Canada. Lichen mapping using remote sensing (RS) images could be a challenging task, however, as lichens generally appear in unevenly distributed, small patches, and could resemble surficial features. Moreover, collecting lichen labeled data (reference data) is expensive, which restricts the application of many robust supervised classification models that generally demand a large quantity of labeled data. The goal of this study was to investigate the potential of using a very-high-spatial resolution (1-cm) lichen map of a small sample site (e.g., generated based on a single UAV scene and using field data) to train a subsequent classifier to map caribou lichen over a much larger area (~0.04 km2 vs. ~195 km2) and a lower spatial resolution image (in this case, a 50-cm WorldView-2 image). The limited labeled data from the sample site were also partially noisy due to spatial and temporal mismatching issues. For this, we deployed a recently proposed Teacher-Student semi-supervised learning (SSL) approach (based on U-Net and U-Net++ networks) involving unlabeled data to assist with improving the model performance. Our experiments showed that it was possible to scale-up the UAV-derived lichen map to the WorldView-2 scale with reasonable accuracy (overall accuracy of 85.28% and F1-socre of 84.38%) without collecting any samples directly in the WorldView-2 scene. We also found that our noisy labels were partially beneficial to the SSL robustness because they improved the false positive rate compared to the use of a cleaner training set directly collected within the same area in the WorldView-2 image. As a result, this research opens new insights into how current very high-resolution, small-scale caribou lichen maps can be used for generating more accurate large-scale caribou lichen maps from high-resolution satellite imagery.
KW  - remote sensing
KW  - lichen mapping
KW  - deep learning
KW  - semi-supervised learning
KW  - teacher-student learning
KW  - WorldView-2
KW  - unmanned aerial vehicle
DO  - 10.3390/rs13142658
ER  -
TY  - EJOU
AU  - Mirzazade, Ali
AU  - Popescu, Cosmin
AU  - Blanksvärd, Thomas
AU  - Täljsten, Björn
TI  - Workflow for Off-Site Bridge Inspection Using Automatic Damage Detection-Case Study of the Pahtajokk Bridge
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - For the inspection of structures, particularly bridges, it is becoming common to replace humans with autonomous systems that use unmanned aerial vehicles (UAV). In this paper, a framework for autonomous bridge inspection using a UAV is proposed with a four-step workflow: (a) data acquisition with an efficient UAV flight path, (b) computer vision comprising training, testing and validation of convolutional neural networks (ConvNets), (c) point cloud generation using intelligent hierarchical dense structure from motion (DSfM), and (d) damage quantification. This workflow starts with planning the most efficient flight path that allows for capturing of the minimum number of images required to achieve the maximum accuracy for the desired defect size, then followed by bridge and damage recognition. Three types of autonomous detection are used: masking the background of the images, detecting areas of potential damage, and pixel-wise damage segmentation. Detection of bridge components by masking extraneous parts of the image, such as vegetation, sky, roads or rivers, can improve the 3D reconstruction in the feature detection and matching stages. In addition, detecting damaged areas involves the UAV capturing close-range images of these critical regions, and damage segmentation facilitates damage quantification using 2D images. By application of DSfM, a denser and more accurate point cloud can be generated for these detected areas, and aligned to the overall point cloud to create a digital model of the bridge. Then, this generated point cloud is evaluated in terms of outlier noise, and surface deviation. Finally, damage that has been detected is quantified and verified, based on the point cloud generated using the Terrestrial Laser Scanning (TLS) method. The results indicate this workflow for autonomous bridge inspection has potential.
KW  - bridge inspection
KW  - computer vision
KW  - intelligent hierarchical DSfM
KW  - bridge 3D modeling
KW  - damage detection
KW  - damage segmentation
KW  - damage assessment
KW  - unmanned inspections
KW  - UAV
DO  - 10.3390/rs13142665
ER  -
TY  - EJOU
AU  - Łabędź, Piotr
AU  - Skabek, Krzysztof
AU  - Ozimek, Paweł
AU  - Nytko, Mateusz
TI  - Histogram Adjustment of Images for Improving Photogrammetric Reconstruction
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 14
SN  - 1424-8220

AB  - The accuracy of photogrammetric reconstruction depends largely on the acquisition conditions and on the quality of input photographs. This paper proposes methods of improving raster images that increase photogrammetric reconstruction accuracy. These methods are based on modifying color image histograms. Special emphasis was placed on the selection of channels of the RGB and CIE L*a*b* color models for further improvement of the reconstruction process. A methodology was proposed for assessing the quality of reconstruction based on premade reference models using positional statistics. The analysis of the influence of image enhancement on reconstruction was carried out for various types of objects. The proposed methods can significantly improve the quality of reconstruction. The superiority of methods based on the luminance channel of the L*a*b* model was demonstrated. Our studies indicated high efficiency of the histogram equalization method (HE), although these results were not highly distinctive for all performed tests.
KW  - photogrammetry
KW  - preprocessing
KW  - enhancement
KW  - point cloud
KW  - 3D reconstruction
KW  - image processing
KW  - image histogram
DO  - 10.3390/s21144654
ER  -
TY  - EJOU
AU  - Ge, Haixiao
AU  - Ma, Fei
AU  - Li, Zhenwang
AU  - Tan, Zhengzheng
AU  - Du, Changwen
TI  - Improved Accuracy of Phenological Detection in Rice Breeding by Using Ensemble Models of Machine Learning Based on UAV-RGB Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - Accurate and timely detection of phenology at plot scale in rice breeding trails is crucial for understanding the heterogeneity of varieties and guiding field management. Traditionally, remote sensing studies of phenology detection have heavily relied on the time-series vegetation index (VI) data. However, the methodology based on time-series VI data was often limited by the temporal resolution. In this study, three types of ensemble models including hard voting (majority voting), soft voting (weighted majority voting) and model stacking, were proposed to identify the principal phenological stages of rice based on unmanned aerial vehicle (UAV) RGB imagery. These ensemble models combined RGB-VIs, color space (e.g., RGB and HSV) and textures derived from UAV-RGB imagery, and five machine learning algorithms (random forest; k-nearest neighbors; Gaussian naïve Bayes; support vector machine and logistic regression) as base models to estimate phenological stages in rice breeding. The phenological estimation models were trained on the dataset of late-maturity cultivars and tested independently on the dataset of early-medium-maturity cultivars. The results indicated that all ensemble models outperform individual machine learning models in all datasets. The soft voting strategy provided the best performance for identifying phenology with the overall accuracy of 90% and 93%, and the mean F1-scores of 0.79 and 0.81, respectively, in calibration and validation datasets, which meant that the overall accuracy and mean F1-scores improved by 5% and 7%, respectively, in comparison with those of the best individual model (GNB), tested in this study. Therefore, the ensemble models demonstrated great potential in improving the accuracy of phenology detection in rice breeding.
KW  - UAV
KW  - machine learning
KW  - ensemble models
KW  - phenology
KW  - breeding
DO  - 10.3390/rs13142678
ER  -
TY  - EJOU
AU  - Herzig, Paul
AU  - Borrmann, Peter
AU  - Knauer, Uwe
AU  - Klück, Hans-Christian
AU  - Kilias, David
AU  - Seiffert, Udo
AU  - Pillen, Klaus
AU  - Maurer, Andreas
TI  - Evaluation of RGB and Multispectral Unmanned Aerial Vehicle (UAV) Imagery for High-Throughput Phenotyping and Yield Prediction in Barley Breeding
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - With advances in plant genomics, plant phenotyping has become a new bottleneck in plant breeding and the need for reliable high-throughput plant phenotyping techniques has emerged. In the face of future climatic challenges, it does not seem appropriate to continue to solely select for grain yield and a few agronomically important traits. Therefore, new sensor-based high-throughput phenotyping has been increasingly used in plant breeding research, with the potential to provide non-destructive, objective and continuous plant characterization that reveals the formation of the final grain yield and provides insights into the physiology of the plant during the growth phase. In this context, we present the comparison of two sensor systems, Red-Green-Blue (RGB) and multispectral cameras, attached to unmanned aerial vehicles (UAV), and investigate their suitability for yield prediction using different modelling approaches in a segregating barley introgression population at three environments with weekly data collection during the entire vegetation period. In addition to vegetation indices, morphological traits such as canopy height, vegetation cover and growth dynamics traits were used for yield prediction. Repeatability analyses and genotype association studies of sensor-based traits were compared with reference values from ground-based phenotyping to test the use of conventional and new traits for barley breeding. The relative height estimation of the canopy by UAV achieved high precision (up to r = 0.93) and repeatability (up to R2 = 0.98). In addition, we found a great overlap of detected significant genotypes between the reference heights and sensor-based heights. The yield prediction accuracy of both sensor systems was at the same level and reached a maximum prediction accuracy of r2 = 0.82 with a continuous increase in precision throughout the entire vegetation period. Due to the lower costs and the consumer-friendly handling of image acquisition and processing, the RGB imagery seems to be more suitable for yield prediction in this study.
KW  - barley (Hordeum vulgare ssp. vulgare)
KW  - remote sensing
KW  - unmanned aerial vehicle (UAV)
KW  - multi-spectral imagery
KW  - RGB imagery
KW  - crop height modelling
KW  - vegetation cover modelling
KW  - growth dynamics
KW  - yield prediction
KW  - genotype association study
DO  - 10.3390/rs13142670
ER  -
TY  - EJOU
AU  - Liu, Zhi
AU  - Yang, Shuyuan
AU  - Feng, Zhixi
AU  - Gao, Quanwei
AU  - Wang, Min
TI  - Fast SAR Autofocus Based on Ensemble Convolutional Extreme Learning Machine
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - Inaccurate Synthetic Aperture Radar (SAR) navigation information will lead to unknown phase errors in SAR data. Uncompensated phase errors can blur the SAR images. Autofocus is a technique that can automatically estimate phase errors from data. However, existing autofocus algorithms either have poor focusing quality or a slow focusing speed. In this paper, an ensemble learning-based autofocus method is proposed. Convolutional Extreme Learning Machine (CELM) is constructed and utilized to estimate the phase error. However, the performance of a single CELM is poor. To overcome this, a novel, metric-based combination strategy is proposed, combining multiple CELMs to further improve the estimation accuracy. The proposed model is trained with the classical bagging-based ensemble learning method. The training and testing process is non-iterative and fast. Experimental results conducted on real SAR data show that the proposed method has a good trade-off between focusing quality and speed.
KW  - synthetic aperture radar
KW  - autofocus
KW  - ensemble learning
KW  - extreme learning machine
KW  - convolutional neural network
DO  - 10.3390/rs13142683
ER  -
TY  - EJOU
AU  - Dawid, Wojciech
AU  - Pokonieczny, Krzysztof
TI  - Methodology of Using Terrain Passability Maps for Planning the Movement of Troops and Navigation of Unmanned Ground Vehicles
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 14
SN  - 1424-8220

AB  - The determination of the route of movement is a key factor which enables navigation. In this article, the authors present the methodology of using different resolution terrain passability maps to generate graphs, which allow for the determination of the optimal route between two points. The routes are generated with the use of two commonly used pathfinding algorithms: Dijkstra’s and A-star. The proposed methodology allows for the determination of routes in various variants—A more secure route that avoids all terrain obstacles with a wide curve, or a shorter route, which is, however, more difficult to pass. In order to achieve that, two functions that modify the value of the index of passability (IOP), which is assigned to the primary fields that the passability map consists of, have been used. These functions have a β parameter that augments or reduces the impact of the applied function on IOP values. The paper also shows the possibilities of implementation of the methodology for the movement of single vehicles or unmanned ground vehicles (UGVs) by using detailed maps as well as for determining routes for large military operational units moving in a 1 km wide corridor. The obtained results show that the change in β value causes the change of a course of the route as expected and that Dijkstra’s algorithm is more stable and slightly faster than A-star. The area of application of the presented methodology is very wide because, except for planning the movement of unmanned ground vehicles or military units of different sizes, it can be used in crisis management, where the possibility of reaching the area outside the road network can be of key importance for the success of the salvage operation.
KW  - maps of passability
KW  - pathfinding
KW  - UGV navigation
KW  - cross country movement
DO  - 10.3390/s21144682
ER  -
TY  - EJOU
AU  - Huang, Shenjin
AU  - Han, Wenting
AU  - Chen, Haipeng
AU  - Li, Guang
AU  - Tang, Jiandong
TI  - Recognizing Zucchinis Intercropped with Sunflowers in UAV Visible Images Using an Improved Method Based on OCRNet
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - An improved semantic segmentation method based on object contextual representations network (OCRNet) is proposed to accurately identify zucchinis intercropped with sunflowers from unmanned aerial vehicle (UAV) visible images taken over Hetao Irrigation District, Inner Mongolia, China. The proposed method improves on the performance of OCRNet in two respects. First, based on the object region context extraction structure of the OCRNet, a branch that uses the channel attention module was added in parallel to rationally use channel feature maps with different weights and reduce the noise of invalid channel features. Secondly, Lovász-Softmax loss was introduced to improve the accuracy of the object region representation in the OCRNet and optimize the final segmentation result at the object level. We compared the proposed method with extant advanced semantic segmentation methods (PSPNet, DeepLabV3+, DNLNet, and OCRNet) in two test areas to test its effectiveness. The results showed that the proposed method achieved the best semantic segmentation effect in the two test areas. More specifically, our method performed better in processing image details, segmenting field edges, and identifying intercropping fields. The proposed method has significant advantages for crop classification and intercropping recognition based on UAV visible images, and these advantages are more substantive in object-level evaluation metrics (mIoU and intercropping IoU).
KW  - intercropping identification
KW  - UAV remote sensing
KW  - semantic segmentation
KW  - OCRNet
DO  - 10.3390/rs13142706
ER  -
TY  - EJOU
AU  - Bui, Quang-Thanh
AU  - Chou, Tien-Yin
AU  - Hoang, Thanh-Van
AU  - Fang, Yao-Min
AU  - Mu, Ching-Yun
AU  - Huang, Pi-Hui
AU  - Pham, Vu-Dong
AU  - Nguyen, Quoc-Huy
AU  - Anh, Do T.
AU  - Pham, Van-Manh
AU  - Meadows, Michael E.
TI  - Gradient Boosting Machine and Object-Based CNN for Land Cover Classification
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - In regular convolutional neural networks (CNN), fully-connected layers act as classifiers to estimate the probabilities for each instance in classification tasks. The accuracy of CNNs can be improved by replacing fully connected layers with gradient boosting algorithms. In this regard, this study investigates three robust classifiers, namely XGBoost, LightGBM, and Catboost, in combination with a CNN for a land cover study in Hanoi, Vietnam. The experiments were implemented using SPOT7 imagery through (1) image segmentation and extraction of features, including spectral information and spatial metrics, (2) normalization of attribute values and generation of graphs, and (3) using graphs as the input dataset to the investigated models for classifying six land cover classes, namely House, Bare land, Vegetation, Water, Impervious Surface, and Shadow. The results show that CNN-based XGBoost (Overall accuracy = 0.8905), LightGBM (0.8956), and CatBoost (0.8956) outperform the other methods used for comparison. It can be seen that the combination of object-based image analysis and CNN-based gradient boosting algorithms significantly improves classification accuracies and can be considered as alternative methods for land cover analysis.
KW  - object-based image analysis
KW  - gradient boosting
KW  - convolutional neural network
KW  - land cover
DO  - 10.3390/rs13142709
ER  -
TY  - EJOU
AU  - Abdollahi, Abolfazl
AU  - Pradhan, Biswajeet
TI  - Urban Vegetation Mapping from Aerial Imagery Using Explainable AI (XAI)
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 14
SN  - 1424-8220

AB  - Urban vegetation mapping is critical in many applications, i.e., preserving biodiversity, maintaining ecological balance, and minimizing the urban heat island effect. It is still challenging to extract accurate vegetation covers from aerial imagery using traditional classification approaches, because urban vegetation categories have complex spatial structures and similar spectral properties. Deep neural networks (DNNs) have shown a significant improvement in remote sensing image classification outcomes during the last few years. These methods are promising in this domain, yet unreliable for various reasons, such as the use of irrelevant descriptor features in the building of the models and lack of quality in the labeled image. Explainable AI (XAI) can help us gain insight into these limits and, as a result, adjust the training dataset and model as needed. Thus, in this work, we explain how an explanation model called Shapley additive explanations (SHAP) can be utilized for interpreting the output of the DNN model that is designed for classifying vegetation covers. We want to not only produce high-quality vegetation maps, but also rank the input parameters and select appropriate features for classification. Therefore, we test our method on vegetation mapping from aerial imagery based on spectral and textural features. Texture features can help overcome the limitations of poor spectral resolution in aerial imagery for vegetation mapping. The model was capable of obtaining an overall accuracy (OA) of 94.44% for vegetation cover mapping. The conclusions derived from SHAP plots demonstrate the high contribution of features, such as Hue, Brightness, GLCM_Dissimilarity, GLCM_Homogeneity, and GLCM_Mean to the output of the proposed model for vegetation mapping. Therefore, the study indicates that existing vegetation mapping strategies based only on spectral characteristics are insufficient to appropriately classify vegetation covers.
KW  - XAI
KW  - deep neural network
KW  - remote sensing
KW  - SHAP
KW  - vegetation mapping
DO  - 10.3390/s21144738
ER  -
TY  - EJOU
AU  - Jumaah, Huda J.
AU  - Kalantar, Bahareh
AU  - Halin, Alfian A.
AU  - Mansor, Shattri
AU  - Ueda, Naonori
AU  - Jumaah, Sarah J.
TI  - Development of UAV-Based PM2.5 Monitoring System
T2  - Drones

PY  - 2021
VL  - 5
IS  - 3
SN  - 2504-446X

AB  - This paper proposes a UAV-based PM2.5 air quality and temperature-humidity monitoring system. The system includes an air quality detector comprising four Arduino sensor modules. Specifically, it includes a dust (DSM501A) sensor and a temperature and humidity (DHT11) sensor. The NEO-6M GPS module and DS3231 real-time module are also included for input visualization. A DIY SD card logging shield and memory module is also available for data recording purposes. The Arduino-based board houses multiple sensors and all are programmable using the Arduino integrated development environment (IDE) coding tool. Measurements conducted in a vertical flight path show promise where comparisons with ground truth references data showed good similarity. Overall, the results point to the idea that a light-weight and portable system can be used for accurate and reliable remote sensing data collection (in this case, PM2.5 concentration data and environmental data).
KW  - particulate matter
KW  - UAV
KW  - air quality
KW  - sensor
KW  - Arduino
DO  - 10.3390/drones5030060
ER  -
TY  - EJOU
AU  - Wengert, Matthias
AU  - Piepho, Hans-Peter
AU  - Astor, Thomas
AU  - Graß, Rüdiger
AU  - Wijesingha, Jayan
AU  - Wachendorf, Michael
TI  - Assessing Spatial Variability of Barley Whole Crop Biomass Yield and Leaf Area Index in Silvoarable Agroforestry Systems Using UAV-Borne Remote Sensing
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - Agroforestry systems (AFS) can provide positive ecosystem services while at the same time stabilizing yields under increasingly common drought conditions. The effect of distance to trees in alley cropping AFS on yield-related crop parameters has predominantly been studied using point data from transects. Unmanned aerial vehicles (UAVs) offer a novel possibility to map plant traits with high spatial resolution and coverage. In the present study, UAV-borne red, green, blue (RGB) and multispectral imagery was utilized for the prediction of whole crop dry biomass yield (DM) and leaf area index (LAI) of barley at three different conventionally managed silvoarable alley cropping agroforestry sites located in Germany. DM and LAI were modelled using random forest regression models with good accuracies (DM: R² 0.62, nRMSEp 14.9%, LAI: R² 0.92, nRMSEp 7.1%). Important variables for prediction included normalized reflectance, vegetation indices, texture and plant height. Maps were produced from model predictions for spatial analysis, showing significant effects of distance to trees on DM and LAI. Spatial patterns differed greatly between the sampled sites and suggested management and soil effects overriding tree effects across large portions of 96 m wide crop alleys, thus questioning alleged impacts of AFS tree rows on yield distribution in intensively managed barley populations. Models based on UAV-borne imagery proved to be a valuable novel tool for prediction of DM and LAI at high accuracies, revealing spatial variability in AFS with high spatial resolution and coverage.
KW  - UAV
KW  - agroforestry
KW  - multispectral
KW  - barley
KW  - alley cropping
KW  - predictive modelling
KW  - SFM
DO  - 10.3390/rs13142751
ER  -
TY  - EJOU
AU  - Bu, Liangtao
AU  - Du, Guoqiang
AU  - Hou, Qi
TI  - Prediction of the Compressive Strength of Recycled Aggregate Concrete Based on Artificial Neural Network
T2  - Materials

PY  - 2021
VL  - 14
IS  - 14
SN  - 1996-1944

AB  - Recycled aggregate concrete (RAC), due to its high porosity and the residual cement and mortar on its surface, exhibits weaker strength than common concrete. To guarantee the safe use of RAC, a compressive strength prediction model based on artificial neural network (ANN) was built in this paper, which can be applied to predict the RAC compressive strength for 28 days. A data set containing 88 data points was obtained by relative tests with different mix proportion designs. The data set was used to develop an ANN, whose optimal structure was determined using the trial-and-error method by taking cement content (C), sand content (S), natural coarse aggregate content (NCA), recycled coarse aggregate content (RCA), water content (W), water–colloid ratio (WCR), sand content rate (SR), and replacement rate of recycled aggregate (RRCA) as input parameters. On the basis of different numbers of hidden layers, numbers of hidden layer neurons, and transfer functions, a total of 840 different back propagation neural network (BPNN) models were developed using MATLAB software, which were then sorted according to the correlation coefficient R2. In addition, the optimal BPNN structure was finally determined to be 8–12–8–1. For the training set, the correlation coefficient R2 = 0.97233 and RMSE = 2.01, and for the testing set, the correlation coefficient R2 = 0.96650 and RMSE = 2.42. The model prediction deviations of the two were both less than 15%, and the results show that the ANN achieved pretty accurate prediction on the compressive strength of RAC. Finally, a sensitivity analysis was carried out, through which the impact of the input parameters on the predicted compressive strength of the RAC was obtained.
KW  - recycled aggregate concrete
KW  - artificial neural network
KW  - compressive strength
KW  - mixture ratio
KW  - sensitivity analysis
DO  - 10.3390/ma14143921
ER  -
TY  - EJOU
AU  - Anderson, Nicholas T.
AU  - Walsh, Kerry B.
AU  - Wulfsohn, Dvoralai
TI  - Technologies for Forecasting Tree Fruit Load and Harvest Timing—From Ground, Sky and Time
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 7
SN  - 2073-4395

AB  - The management and marketing of fruit requires data on expected numbers, size, quality and timing. Current practice estimates orchard fruit load based on the qualitative assessment of fruit number per tree and historical orchard yield, or manually counting a subsample of trees. This review considers technological aids assisting these estimates, in terms of: (i) improving sampling strategies by the number of units to be counted and their selection; (ii) machine vision for the direct measurement of fruit number and size on the canopy; (iii) aerial or satellite imagery for the acquisition of information on tree structural parameters and spectral indices, with the indirect assessment of fruit load; (iv) models extrapolating historical yield data with knowledge of tree management and climate parameters, and (v) technologies relevant to the estimation of harvest timing such as heat units and the proximal sensing of fruit maturity attributes. Machine vision is currently dominating research outputs on fruit load estimation, while the improvement of sampling strategies has potential for a widespread impact. Techniques based on tree parameters and modeling offer scalability, but tree crops are complicated (perennialism). The use of machine vision for flowering estimates, fruit sizing, external quality evaluation is also considered. The potential synergies between technologies are highlighted.
KW  - yield
KW  - estimation
KW  - machine vision
KW  - remote sensing
KW  - correlative
KW  - models
KW  - fruit
KW  - tree
KW  - review
DO  - 10.3390/agronomy11071409
ER  -
TY  - EJOU
AU  - Li, Jingbo
AU  - Li, Changchun
AU  - Fei, Shuaipeng
AU  - Ma, Chunyan
AU  - Chen, Weinan
AU  - Ding, Fan
AU  - Wang, Yilin
AU  - Li, Yacong
AU  - Shi, Jinjin
AU  - Xiao, Zhen
TI  - Wheat Ear Recognition Based on RetinaNet and Transfer Learning
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 14
SN  - 1424-8220

AB  - The number of wheat ears is an essential indicator for wheat production and yield estimation, but accurately obtaining wheat ears requires expensive manual cost and labor time. Meanwhile, the characteristics of wheat ears provide less information, and the color is consistent with the background, which can be challenging to obtain the number of wheat ears required. In this paper, the performance of Faster regions with convolutional neural networks (Faster R-CNN) and RetinaNet to predict the number of wheat ears for wheat at different growth stages under different conditions is investigated. The results show that using the Global WHEAT dataset for recognition, the RetinaNet method, and the Faster R-CNN method achieve an average accuracy of 0.82 and 0.72, with the RetinaNet method obtaining the highest recognition accuracy. Secondly, using the collected image data for recognition, the R2 of RetinaNet and Faster R-CNN after transfer learning is 0.9722 and 0.8702, respectively, indicating that the recognition accuracy of the RetinaNet method is higher on different data sets. We also tested wheat ears at both the filling and maturity stages; our proposed method has proven to be very robust (the R2 is above 90). This study provides technical support and a reference for automatic wheat ear recognition and yield estimation.
KW  - RetinaNet
KW  - deep learning
KW  - transfer learning
KW  - wheat ears
KW  - Global WHEAT
DO  - 10.3390/s21144845
ER  -
TY  - EJOU
AU  - Jiang, Fugen
AU  - Chen, Chuanshi
AU  - Li, Chengjie
AU  - Kutia, Mykola
AU  - Sun, Hua
TI  - A Novel Spatial Simulation Method for Mapping the Urban Forest Carbon Density in Southern China by the Google Earth Engine
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - Urban forest is an important component of terrestrial ecosystems and is highly related to global climate change. However, because of complex city landscapes, deriving the spatial distribution of urban forest carbon density and conducting accuracy assessments are difficult. This study proposes a novel spatial simulation method, optimized geographically weighted logarithm regression (OGWLR), using Landsat 8 data acquired by the Google Earth Engine (GEE) and field survey data to map the forest carbon density of Shenzhen city in southern China. To verify the effectiveness of the novel method, multiple linear regression (MLR), k-nearest neighbors (kNN), random forest (RF) and geographically weighted regression (GWR) models were established for comparison. The results showed that OGWLR achieved the highest coefficient of determination (R2 = 0.54) and the lowest root mean square error (RMSE = 13.28 Mg/ha) among all estimation models. In addition, OGWLR achieved a more consistent spatial distribution of carbon density with the actual situation. The carbon density of the forests in the study area was large in the central and western regions and coastal areas and small in the building and road areas. Therefore, this method can provide a new reference for urban forest carbon density estimation and mapping.
KW  - forest carbon density
KW  - Landsat 8
KW  - GEE
KW  - geographically weighted regression
DO  - 10.3390/rs13142792
ER  -
TY  - EJOU
AU  - Ran, Shuhao
AU  - Gao, Xianjun
AU  - Yang, Yuanwei
AU  - Li, Shaohua
AU  - Zhang, Guangbin
AU  - Wang, Ping
TI  - Building Multi-Feature Fusion Refined Network for Building Extraction from High-Resolution Remote Sensing Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - Deep learning approaches have been widely used in building automatic extraction tasks and have made great progress in recent years. However, the missing detection and wrong detection causing by spectrum confusion is still a great challenge. The existing fully convolutional networks (FCNs) cannot effectively distinguish whether the feature differences are from one building or the building and its adjacent non-building objects. In order to overcome the limitations, a building multi-feature fusion refined network (BMFR-Net) was presented in this paper to extract buildings accurately and completely. BMFR-Net is based on an encoding and decoding structure, mainly consisting of two parts: the continuous atrous convolution pyramid (CACP) module and the multiscale output fusion constraint (MOFC) structure. The CACP module is positioned at the end of the contracting path and it effectively minimizes the loss of effective information in multiscale feature extraction and fusion by using parallel continuous small-scale atrous convolution. To improve the ability to aggregate semantic information from the context, the MOFC structure performs predictive output at each stage of the expanding path and integrates the results into the network. Furthermore, the multilevel joint weighted loss function effectively updates parameters well away from the output layer, enhancing the learning capacity of the network for low-level abstract features. The experimental results demonstrate that the proposed BMFR-Net outperforms the other five state-of-the-art approaches in both visual interpretation and quantitative evaluation.
KW  - high-resolution remote sensing images
KW  - building extraction
KW  - multiscale features
KW  - aggregate semantic information
KW  - feature pyramid
DO  - 10.3390/rs13142794
ER  -
TY  - EJOU
AU  - Vásquez, Felipe
AU  - Cravero, Ania
AU  - Castro, Manuel
AU  - Acevedo, Patricio
TI  - Decision Support System Development of Wildland Fire: A Systematic Mapping
T2  - Forests

PY  - 2021
VL  - 12
IS  - 7
SN  - 1999-4907

AB  - Wildland fires have been a rising problem on the worldwide level, generating ecological and economic losses. Specifically, between wildland fire types, uncontrolled fires are critical due to the potential damage to the ecosystem and their effects on the soil, and, in the last decade, different technologies have been applied to fight them. Selecting a specific technology and Decision Support Systems (DSS) is fundamental, since the results and validity of this could drastically oscillate according to the different environmental and geographic factors of the terrain to be studied. Given the above, a systematic mapping was realized, with the purpose of recognizing the most-used DSS and context where they have been applied. One hundred and eighty-three studies were found that used different types of DSS to solve problems of detection, prediction, prevention, monitoring, simulation, administration, and access to routes. The concepts key to the type of solution are related to the use or development of systems or Information and Communication Technologies (ICT) in the computer science area. Although the use of BA and Big Data has increased in recent years, there are still many challenges to face, such as staff training, the friendly environment of DSS, and real-time decision-making.
KW  - wildland fire
KW  - forest fire
KW  - decision support systems
KW  - systematic mapping
DO  - 10.3390/f12070943
ER  -
TY  - EJOU
AU  - Lin, Zhe
AU  - Guo, Wenxuan
TI  - Cotton Stand Counting from Unmanned Aerial System Imagery Using MobileNet and CenterNet Deep Learning Models
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 14
SN  - 2072-4292

AB  - An accurate stand count is a prerequisite to determining the emergence rate, assessing seedling vigor, and facilitating site-specific management for optimal crop production. Traditional manual counting methods in stand assessment are labor intensive and time consuming for large-scale breeding programs or production field operations. This study aimed to apply two deep learning models, the MobileNet and CenterNet, to detect and count cotton plants at the seedling stage with unmanned aerial system (UAS) images. These models were trained with two datasets containing 400 and 900 images with variations in plant size and soil background brightness. The performance of these models was assessed with two testing datasets of different dimensions, testing dataset 1 with 300 by 400 pixels and testing dataset 2 with 250 by 1200 pixels. The model validation results showed that the mean average precision (mAP) and average recall (AR) were 79% and 73% for the CenterNet model, and 86% and 72% for the MobileNet model with 900 training images. The accuracy of cotton plant detection and counting was higher with testing dataset 1 for both CenterNet and MobileNet models. The results showed that the CenterNet model had a better overall performance for cotton plant detection and counting with 900 training images. The results also indicated that more training images are required when applying object detection models on images with different dimensions from training datasets. The mean absolute percentage error (MAPE), coefficient of determination (R2), and the root mean squared error (RMSE) values of the cotton plant counting were 0.07%, 0.98 and 0.37, respectively, with testing dataset 1 for the CenterNet model with 900 training images. Both MobileNet and CenterNet models have the potential to accurately and timely detect and count cotton plants based on high-resolution UAS images at the seedling stage. This study provides valuable information for selecting the right deep learning tools and the appropriate number of training images for object detection projects in agricultural applications.
KW  - cotton stand count
KW  - unmanned aerial systems
KW  - deep learning
KW  - remote sensing
KW  - MobileNet
KW  - CenterNet
KW  - Python
KW  - Tensorflow
DO  - 10.3390/rs13142822
ER  -
TY  - EJOU
AU  - Tanwar, Monika
AU  - Park, Hyunseok
AU  - Raghavan, Nagarajan
TI  - Multistate Diagnosis and Prognosis of Lubricating Oil Degradation Using Sticky Hierarchical Dirichlet Process–Hidden Markov Model Framework
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 14
SN  - 2076-3417

AB  - In this study, we present a state-based diagnostic and prognostic methodology for lubricating oil degradation based on a nonparametric Bayesian approach, i.e., sticky hierarchical Dirichlet process–hidden Markov model (HDP-HMM). An accurate health state-space assessment for diagnostics and prognostics has always been unobservable and hypothetical in the past. The lubrication condition monitoring (LCM) data is generally segregated as “healthy or unhealthy”, representing a binary state-based perspective to the problem. This two-state performance-based formulation poses limitations to the precision and accuracy of the diagnosis and prognosis for real data wherein there may be multiple states of discrete performance that are characteristic of the system functionality. In particular, the reversible and nonlinear time-series trends of degradation data increase the complexity of state-based modeling. We propose a multistate diagnostic and prognostic framework for LCM data in the wear-out phase (i.e., the unhealthy portion of degradation data), accounting for irregular oil replenishment and oil change effects (i.e., nonlinearity in the degradation signal). The LCM data is simulated for an elementary mechanical system with four components. The sticky HDP sets the prior for the HMM parameters. The unsupervised learning over infinite observations and emission reveals four discrete health states and helps estimate the associated state transition probabilities. The inferred state sequence provides information relating to the state dynamics, which provides further guidance to maintenance decision making. The decision making is further backed by prognostics based on the conditional reliability function and mean residual life estimation.
KW  - sticky HDP-HMM
KW  - Dirichlet process
KW  - health state estimation
KW  - hidden Markov model
KW  - diagnostics
KW  - prognostics
KW  - lubricating oil
DO  - 10.3390/app11146603
ER  -
TY  - EJOU
AU  - Che’Ya, Nik N.
AU  - Dunwoody, Ernest
AU  - Gupta, Madan
TI  - Assessment of Weed Classification Using Hyperspectral Reflectance and Optimal Multispectral UAV Imagery
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 7
SN  - 2073-4395

AB  - Weeds compete with crops and are hard to differentiate and identify due to their similarities in color, shape, and size. In this study, the weed species present in sorghum (sorghum bicolor (L.) Moench) fields, such as amaranth (Amaranthus macrocarpus), pigweed (Portulaca oleracea), mallow weed (Malva sp.), nutgrass (Cyperus rotundus), liver seed grass (Urochoa panicoides), and Bellive (Ipomea plebeian), were discriminated using hyperspectral data and were detected and analyzed using multispectral images. Discriminant analysis (DA) was used to identify the most significant spectral bands in order to discriminate weeds from sorghum using hyperspectral data. The results demonstrated good separation accuracy for Amaranthus macrocarpus, Urochoa panicoides, Malva sp., Cyperus rotundus, and Sorghum bicolor (L.) Moench at 440, 560, 680, 710, 720, and 850 nm. Later, the multispectral images of these six bands were collected to detect weeds in the sorghum crop fields using object-based image analysis (OBIA). The results showed that the differences between sorghum and weed species were detectable using the six selected bands, with data collected using an unmanned aerial vehicle. Here, the highest spatial resolution had the highest accuracy for weed detection. It was concluded that each weed was successfully discriminated using hyperspectral data and was detectable using multispectral data with higher spatial resolution.
KW  - weed classification
KW  - hyperspectral reflectance
KW  - discriminant analysis
KW  - weed species
KW  - weed mapping
KW  - site-specific weed management
DO  - 10.3390/agronomy11071435
ER  -
TY  - EJOU
AU  - Liu, Jingjing
AU  - Liu, Chuanyang
AU  - Wu, Yiquan
AU  - Xu, Huajie
AU  - Sun, Zuo
TI  - An Improved Method Based on Deep Learning for Insulator Fault Detection in Diverse Aerial Images
T2  - Energies

PY  - 2021
VL  - 14
IS  - 14
SN  - 1996-1073

AB  - Insulators play a significant role in high-voltage transmission lines, and detecting insulator faults timely and accurately is important for the safe and stable operation of power grids. Since insulator faults are extremely small and the backgrounds of aerial images are complex, insulator fault detection is a challenging task for automatically inspecting transmission lines. In this paper, a method based on deep learning is proposed for insulator fault detection in diverse aerial images. Firstly, to provide sufficient insulator fault images for training, a novel insulator fault dataset named “InSF-detection” is constructed. Secondly, an improved YOLOv3 model is proposed to reuse features and prevent feature loss. To improve the accuracy of insulator fault detection, SPP-networks and a multi-scale prediction network are employed for the improved YOLOv3 model. Finally, the improved YOLOv3 model and the compared models are trained and tested on the “InSF-detection”. The average precision (AP) of the improved YOLOv3 model is superior to YOLOv3 and YOLOv3-dense models, and just a little (1.2%) lower than that of CSPD-YOLO model; more importantly, the memory usage of the improved YOLOv3 model is 225 MB, which is the smallest between the four compared models. The experimental results and analysis validate that the improved YOLOv3 model achieves good performance for insulator fault detection in aerial images with diverse backgrounds.
KW  - insulator fault detection
KW  - aerial image
KW  - deep learning
KW  - YOLO
KW  - DenseNet
KW  - complex backgrounds
DO  - 10.3390/en14144365
ER  -
TY  - EJOU
AU  - Hallee, Mitchell J.
AU  - Napolitano, Rebecca K.
AU  - Reinhart, Wesley F.
AU  - Glisic, Branko
TI  - Crack Detection in Images of Masonry Using CNNs
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 14
SN  - 1424-8220

AB  - While there is a significant body of research on crack detection by computer vision methods in concrete and asphalt, less attention has been given to masonry. We train a convolutional neural network (CNN) on images of brick walls built in a laboratory environment and test its ability to detect cracks in images of brick-and-mortar structures both in the laboratory and on real-world images taken from the internet. We also compare the performance of the CNN to a variety of simpler classifiers operating on handcrafted features. We find that the CNN performed better on the domain adaptation from laboratory to real-world images than these simple models. However, we also find that performance is significantly better in performing the reverse domain adaptation task, where the simple classifiers are trained on real-world images and tested on the laboratory images. This work demonstrates the ability to detect cracks in images of masonry using a variety of machine learning methods and provides guidance for improving the reliability of such models when performing domain adaptation for crack detection in masonry.
KW  - computer vision
KW  - crack detection
KW  - structural health monitoring
KW  - masonry
KW  - machine learning
KW  - convolutional neural network
DO  - 10.3390/s21144929
ER  -
TY  - EJOU
AU  - Wei, Baoquan
AU  - Zuo, Yong
AU  - Liu, Yande
AU  - Luo, Wei
AU  - Wen, Kaiyun
AU  - Deng, Fangming
TI  - Novel MOA Fault Detection Technology Based on Small Sample Infrared Image
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 15
SN  - 2079-9292

AB  - This paper proposes a novel metal oxide arrester (MOA) fault detection technology based on a small sample infrared image. The research is carried out from the detection process and data enhancement. A lightweight MOA identification and location algorithm is designed at the edge, which can not only reduce the amount of data uploaded, but also reduce the search space of cloud algorithm. In order to improve the accuracy and generalization ability of the defect detection model under the condition of small samples, a multi-model fusion detection algorithm is proposed. Different features of the image are extracted by multiple convolutional neural networks, and then multiple classifiers are trained. Finally, the weighted voting strategy is used for fault diagnosis. In addition, the extended model of fault samples is constructed by transfer learning and deep convolutional generative adversarial networks (DCGAN) to solve the problem of unbalanced training data sets. The experimental results show that the proposed method can realize the accurate location of arrester under the condition of small samples, and after the data expansion, the recognition rate of arrester anomalies can be improved from 83% to 85%, showing high effectiveness and reliability.
KW  - metal oxide arrester
KW  - deep learning
KW  - edge computing
KW  - condition monitoring
DO  - 10.3390/electronics10151748
ER  -
TY  - EJOU
AU  - Zhang, Ziyuan
AU  - Hua, Zexi
AU  - Tang, Yongchuan
AU  - Zhang, Yunjia
AU  - Lu, Weijun
AU  - Dai, Congfei
TI  - Recognition Method of Digital Meter Readings in Substation Based on Connected Domain Analysis Algorithm
T2  - Actuators

PY  - 2021
VL  - 10
IS  - 8
SN  - 2076-0825

AB  - Aiming at the problem that the number and decimal point of digital instruments in substations are prone to misdetection and missed detection, a method of digital meter readings in a substation based on connected domain analysis algorithm is proposed. This method uses Faster R-CNN (Faster Region Convolutional Neural Network) as a positioning network to localize the dial area, and after acquiring the partial image, it enhances the useful information of the digital area. YOLOv4 (You Only Look Once) convolutional neural network is used as the detector to detect the digital area. The purpose is to distinguish the numbers and obtain the digital area that may contain a decimal point or no decimal point at the tail. Combined with the connected domain analysis algorithm, the difference between the number of connected domain categories and the area ratio of the digital area is analyzed, and the judgment of the decimal point is realized. The method reduces the problem of mutual interference among categories when detecting YOLOv4. The experimental results show that the method improves the detection accuracy of the algorithm.
KW  - deep learning
KW  - YOLOv4
KW  - target detection
KW  - connected domain
KW  - digital meter readings
DO  - 10.3390/act10080170
ER  -
TY  - EJOU
AU  - Ammar, Adel
AU  - Koubaa, Anis
AU  - Benjdira, Bilel
TI  - Deep-Learning-Based Automated Palm Tree Counting and Geolocation in Large Farms from Aerial Geotagged Images
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 8
SN  - 2073-4395

AB  - In this paper, we propose an original deep learning framework for the automated counting and geolocation of palm trees from aerial images using convolutional neural networks. For this purpose, we collected aerial images from two different regions in Saudi Arabia, using two DJI drones, and we built a dataset of around 11,000 instances of palm trees. Then, we applied several recent convolutional neural network models (Faster R-CNN, YOLOv3, YOLOv4, and EfficientDet) to detect palms and other trees, and we conducted a complete comparative evaluation in terms of average precision and inference speed. YOLOv4 and EfficientDet-D5 yielded the best trade-off between accuracy and speed (up to 99% mean average precision and 7.4 FPS). Furthermore, using the geotagged metadata of aerial images, we used photogrammetry concepts and distance corrections to automatically detect the geographical location of detected palm trees. This geolocation technique was tested on two different types of drones (DJI Mavic Pro and Phantom 4 pro) and was assessed to provide an average geolocation accuracy that attains 1.6 m. This GPS tagging allows us to uniquely identify palm trees and count their number from a series of drone images, while correctly dealing with the issue of image overlapping. Moreover, this innovative combination between deep learning object detection and geolocalization can be generalized to any other objects in UAV images.
KW  - unmanned aerial vehicles
KW  - convolutional neural networks
KW  - Faster R-CNN
KW  - You Only Look Once (YOLO)
DO  - 10.3390/agronomy11081458
ER  -
TY  - EJOU
AU  - Isheyskiy, Valentin
AU  - Martinyskin, Evgeny
AU  - Smirnov, Sergey
AU  - Vasilyev, Anton
AU  - Knyazev, Kirill
AU  - Fatyanov, Timur
TI  - Specifics of MWD Data Collection and Verification during Formation of Training Datasets
T2  - Minerals

PY  - 2021
VL  - 11
IS  - 8
SN  - 2075-163X

AB  - This paper presents a structured analysis in the area of measurement while drilling (MWD) data processing and verification methods, as well as describes the main nuances and certain specifics of “clean” data selection in order to build a “parent” training database for subsequent use in machine learning algorithms. The main purpose of the authors is to create a trainable machine learning algorithm, which, based on the available “clean” input data associated with specific conditions, could correlate, process and select parameters obtained from the drilling rig and use them for further estimation of various rock characteristics, prediction of optimal drilling and blasting parameters, and blasting results. The paper is a continuation of a series of publications devoted to the prospects of using MWD technology for the quality management of drilling and blasting operations at mining enterprises.
KW  - measurement while drilling
KW  - drilling monitoring
KW  - drilling parameters
KW  - rock properties
KW  - blasting
KW  - data verification
DO  - 10.3390/min11080798
ER  -
TY  - EJOU
AU  - Walambe, Rahee
AU  - Marathe, Aboli
AU  - Kotecha, Ketan
TI  - Multiscale Object Detection from Drone Imagery Using Ensemble Transfer Learning
T2  - Drones

PY  - 2021
VL  - 5
IS  - 3
SN  - 2504-446X

AB  - Object detection in uncrewed aerial vehicle (UAV) images has been a longstanding challenge in the field of computer vision. Specifically, object detection in drone images is a complex task due to objects of various scales such as humans, buildings, water bodies, and hills. In this paper, we present an implementation of ensemble transfer learning to enhance the performance of the base models for multiscale object detection in drone imagery. Combined with a test-time augmentation pipeline, the algorithm combines different models and applies voting strategies to detect objects of various scales in UAV images. The data augmentation also presents a solution to the deficiency of drone image datasets. We experimented with two specific datasets in the open domain: the VisDrone dataset and the AU-AIR Dataset. Our approach is more practical and efficient due to the use of transfer learning and two-level voting strategy ensemble instead of training custom models on entire datasets. The experimentation shows significant improvement in the mAP for both VisDrone and AU-AIR datasets by employing the ensemble transfer learning method. Furthermore, the utilization of voting strategies further increases the 3reliability of the ensemble as the end-user can select and trace the effects of the mechanism for bounding box predictions.
KW  - drone imagery
KW  - 2D object detection
KW  - ensemble techniques
KW  - voting strategies
DO  - 10.3390/drones5030066
ER  -
TY  - EJOU
AU  - Wang, Jingrui
AU  - Wang, Shuqing
AU  - Zou, Dongxiao
AU  - Chen, Huimin
AU  - Zhong, Run
AU  - Li, Hanliang
AU  - Zhou, Wei
AU  - Yan, Kai
TI  - Social Network and Bibliometric Analysis of Unmanned Aerial Vehicle Remote Sensing Applications from 2010 to 2021
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - Unmanned Aerial Vehicle (UAV) Remote sensing (RS) has unique advantages over traditional satellite RS, including convenience, high resolution, affordability and fast acquisition speed, making it widely used in many fields. To provide an overview of the development of UAV RS applications during the past decade, we screened related publications from the Web of Science core database from 2010 to 2021, built co-author networks, a discipline interaction network, a keywords timeline view, a co-citation cluster, and detected burst citations using bibliometrics and social network analysis. Our results show that: (1) The number of UAV RS publications had an increasing trend, with explosive growth in the past five years. The number of papers published by China and the United States (US) is far ahead in this field; (2) The US has currently the greatest influence in this field through the largest number of international cooperations. Cooperation is mainly concentrated in countries and institutions with a large number of publications but is not widely distributed. (3) The application of UAV RS involves multiple interdisciplinary subjects, among which “Environmental Science and Ecology” ranks first; (4) Future research trends of UAV RS are expected to be related to artificial intelligence (e.g., artificial neural networks-based research). This paper provides a scientific basis and guidance for future developments of UAV RS applications, which can help the research community to better grasp the developments of this field.
KW  - Unmanned Aerial Vehicle (UAV)
KW  - Remote Sensing (RS)
KW  - Bibliometric
KW  - Scientometric
KW  - visualization
DO  - 10.3390/rs13152912
ER  -
TY  - EJOU
AU  - Wei, Lifei
AU  - Wang, Kun
AU  - Lu, Qikai
AU  - Liang, Yajing
AU  - Li, Haibo
AU  - Wang, Zhengxiang
AU  - Wang, Run
AU  - Cao, Liqin
TI  - Crops Fine Classification in Airborne Hyperspectral Imagery Based on Multi-Feature Fusion and Deep Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - Hyperspectral imagery has been widely used in precision agriculture due to its rich spectral characteristics. With the rapid development of remote sensing technology, the airborne hyperspectral imagery shows detailed spatial information and temporal flexibility, which open a new way to accurate agricultural monitoring. To extract crop types from the airborne hyperspectral images, we propose a fine classification method based on multi-feature fusion and deep learning. In this research, the morphological profiles, GLCM texture and endmember abundance features are leveraged to exploit the spatial information of the hyperspectral imagery. Then, the multiple spatial information is fused with the original spectral information to generate classification result by using the deep neural network with conditional random field (DNN+CRF) model. Specifically, the deep neural network (DNN) is a deep recognition model which can extract depth features and mine the potential information of data. As a discriminant model, conditional random field (CRF) considers both spatial and contextual information to reduce the misclassification noises while keeping the object boundaries. Moreover, three multiple feature fusion approaches, namely feature stacking, decision fusion and probability fusion, are taken into account. In the experiments, two airborne hyperspectral remote sensing datasets (Honghu dataset and Xiong’an dataset) are used. The experimental results show that the classification performance of the proposed method is satisfactory, where the salt and pepper noise is decreased, and the boundary of the ground object is preserved.
KW  - hyperspectral imagery
KW  - crops fine classification
KW  - multi-feature fusion
KW  - deep neural network
KW  - conditional random field
DO  - 10.3390/rs13152917
ER  -
TY  - EJOU
AU  - Banerjee, Bikram P.
AU  - Sharma, Vikas
AU  - Spangenberg, German
AU  - Kant, Surya
TI  - Machine Learning Regression Analysis for Estimation of Crop Emergence Using Multispectral UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - Optimal crop emergence is an important trait in crop breeding for genotypic screening and for achieving potential growth and yield. Emergence is conventionally quantified manually by counting the sub-sections of field plots or scoring; these are less reliable, laborious and inefficient. Remote sensing technology is being increasingly used for high-throughput estimation of agronomic traits in field crops. This study developed a method for estimating wheat seedlings using multispectral images captured from an unmanned aerial vehicle. A machine learning regression (MLR) analysis was used by combining spectral and morphological information extracted from the multispectral images. The approach was tested on diverse wheat genotypes varying in seedling emergence. In this study, three supervised MLR models including regression trees, support vector regression and Gaussian process regression (GPR) were evaluated for estimating wheat seedling emergence. The GPR model was the most effective compared to the other methods, with R2 = 0.86, RMSE = 4.07 and MAE = 3.21 when correlated to the manual seedling count. In addition, imagery data collected at multiple flight altitudes and different wheat growth stages suggested that 10 m altitude and 20 days after sowing were desirable for optimal spatial resolution and image analysis. The method is deployable on larger field trials and other crops for effective and reliable seedling emergence estimates.
KW  - field trials
KW  - plant count
KW  - plant phenotyping
KW  - wheat
DO  - 10.3390/rs13152918
ER  -
TY  - EJOU
AU  - Behjati, Mehran
AU  - Mohd Noh, Aishah B.
AU  - Alobaidy, Haider A. H.
AU  - Zulkifley, Muhammad A.
AU  - Nordin, Rosdiadee
AU  - Abdullah, Nor F.
TI  - LoRa Communications as an Enabler for Internet of Drones towards Large-Scale Livestock Monitoring in Rural Farms
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 15
SN  - 1424-8220

AB  - Currently, smart farming is considered an effective solution to enhance the productivity of farms; thereby, it has recently received broad interest from service providers to offer a wide range of applications, from pest identification to asset monitoring. Although the emergence of digital technologies, such as the Internet of Things (IoT) and low-power wide-area networks (LPWANs), has led to significant advances in the smart farming industry, farming operations still need more efficient solutions. On the other hand, the utilization of unmanned aerial vehicles (UAVs), also known as drones, is growing rapidly across many civil application domains. This paper aims to develop a farm monitoring system that incorporates UAV, LPWAN, and IoT technologies to transform the current farm management approach and aid farmers in obtaining actionable data from their farm operations. In this regard, an IoT-based water quality monitoring system was developed because water is an essential aspect in livestock development. Then, based on the Long-Range Wide-Area Network (LoRaWAN®) technology, a multi-channel LoRaWAN® gateway was developed and integrated into a vertical takeoff and landing drone to convey collected data from the sensors to the cloud for further analysis. In addition, to develop LoRaWAN®-based aerial communication, a series of measurements and simulations were performed under different configurations and scenarios. Finally, to enhance the efficiency of aerial-based data collection, the UAV path planning was optimized. Measurement results showed that the maximum achievable LoRa coverage when operating on-air via the drone is about 10 km, and the Longley–Rice irregular terrain model provides the most suitable path loss model for the scenario of large-scale farms, and a multi-channel gateway with a spreading factor of 12 provides the most reliable communication link at a high drone speed (up to 95 km/h). Simulation results showed that the developed system can overcome the coverage limitation of LoRaWAN® and it can establish a reliable communication link over large-scale wireless sensor networks. In addition, it was shown that by optimizing flight paths, aerial data collection could be performed in a much shorter time than industrial mission planning (up to four times in our case).
KW  - unmanned aircraft vehicle (UAV)
KW  - drone
KW  - long range (LoRa)
KW  - wireless sensor network
KW  - Internet of Things (IoT)
KW  - remote sensing
KW  - smart farming
KW  - path planning
DO  - 10.3390/s21155044
ER  -
TY  - EJOU
AU  - Zeng, Linglin
AU  - Peng, Guozhang
AU  - Meng, Ran
AU  - Man, Jianguo
AU  - Li, Weibo
AU  - Xu, Binyuan
AU  - Lv, Zhengang
AU  - Sun, Rui
TI  - Wheat Yield Prediction Based on Unmanned Aerial Vehicles-Collected Red–Green–Blue Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - Unmanned aerial vehicles-collected (UAVs) digital red–green–blue (RGB) images provided a cost-effective method for precision agriculture applications regarding yield prediction. This study aims to fully explore the potential of UAV-collected RGB images in yield prediction of winter wheat by comparing it to multi-source observations, including thermal, structure, volumetric metrics, and ground-observed leaf area index (LAI) and chlorophyll content under the same level or across different levels of nitrogen fertilization. Color indices are vegetation indices calculated by the vegetation reflectance at visible bands (i.e., red, green, and blue) derived from RGB images. The results showed that some of the color indices collected at the jointing, flowering, and early maturity stages had high correlation (R2 = 0.76–0.93) with wheat grain yield. They gave the highest prediction power (R2 = 0.92–0.93) under four levels of nitrogen fertilization at the flowering stage. In contrast, the other measurements including canopy temperature, volumetric metrics, and ground-observed chlorophyll content showed lower correlation (R2 = 0.52–0.85) to grain yield. In addition, thermal information as well as volumetric metrics generally had little contribution to the improvement of grain yield prediction when combining them with color indices derived from digital images. Especially, LAI had inferior performance to color indices in grain yield prediction within the same level of nitrogen fertilization at the flowering stage (R2 = 0.00–0.40 and R2 = 0.55–0.68), and color indices provided slightly better prediction of yield than LAI at the flowering stage (R2 = 0.93, RMSE = 32.18 g/m2 and R2 = 0.89, RMSE = 39.82 g/m2) under all levels of nitrogen fertilization. This study highlights the capabilities of color indices in wheat yield prediction across genotypes, which also indicates the potential of precision agriculture application using many other flexible, affordable, and easy-to-handle devices such as mobile phones and near surface digital cameras in the future.
KW  - red–green–blue (RGB) imageries
KW  - yield prediction
KW  - nitrogen fertilization
KW  - vegetation index
DO  - 10.3390/rs13152937
ER  -
TY  - EJOU
AU  - Fernández, Claudio I.
AU  - Leblon, Brigitte
AU  - Wang, Jinfei
AU  - Haddadi, Ata
AU  - Wang, Keri
TI  - Detecting Infected Cucumber Plants with Close-Range Multispectral Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - This study used close-range multispectral imagery over cucumber plants inside a commercial greenhouse to detect powdery mildew due to Podosphaera xanthii. It was collected using a MicaSense® RedEdge camera at 1.5 m over the top of the plant. Image registration was performed using Speeded-Up Robust Features (SURF) with an affine geometric transformation. The image background was removed using a binary mask created with the aligned NIR band of each image, and the illumination was corrected using Cheng et al.’s algorithm. Different features were computed, including RGB, image reflectance values, and several vegetation indices. For each feature, a fine Gaussian Support Vector Machines algorithm was trained and validated to classify healthy and infected pixels. The data set to train and validate the SVM was composed of 1000 healthy and 1000 infected pixels, split 70–30% into training and validation datasets, respectively. The overall validation accuracy was 89, 73, 82, 51, and 48%, respectively, for blue, green, red, red-edge, and NIR band image. With the RGB images, we obtained an overall validation accuracy of 89%, while the best vegetation index image was the PMVI-2 image which produced an overall accuracy of 81%. Using the five bands together, overall accuracy dropped from 99% in the training to 57% in the validation dataset. While the results of this work are promising, further research should be considered to increase the number of images to achieve better training and validation datasets.
KW  - speeded-up robust features
KW  - SURF features
KW  - support vector machines
KW  - image alignment
KW  - powdery mildew
DO  - 10.3390/rs13152948
ER  -
TY  - EJOU
AU  - Ghaffarian, Saman
AU  - Valente, João
AU  - van der Voort, Mariska
AU  - Tekinerdogan, Bedir
TI  - Effect of Attention Mechanism in Deep Learning-Based Remote Sensing Image Processing: A Systematic Literature Review
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - Machine learning, particularly deep learning (DL), has become a central and state-of-the-art method for several computer vision applications and remote sensing (RS) image processing. Researchers are continually trying to improve the performance of the DL methods by developing new architectural designs of the networks and/or developing new techniques, such as attention mechanisms. Since the attention mechanism has been proposed, regardless of its type, it has been increasingly used for diverse RS applications to improve the performances of the existing DL methods. However, these methods are scattered over different studies impeding the selection and application of the feasible approaches. This study provides an overview of the developed attention mechanisms and how to integrate them with different deep learning neural network architectures. In addition, it aims to investigate the effect of the attention mechanism on deep learning-based RS image processing. We identified and analyzed the advances in the corresponding attention mechanism-based deep learning (At-DL) methods. A systematic literature review was performed to identify the trends in publications, publishers, improved DL methods, data types used, attention types used, overall accuracies achieved using At-DL methods, and extracted the current research directions, weaknesses, and open problems to provide insights and recommendations for future studies. For this, five main research questions were formulated to extract the required data and information from the literature. Furthermore, we categorized the papers regarding the addressed RS image processing tasks (e.g., image classification, object detection, and change detection) and discussed the results within each group. In total, 270 papers were retrieved, of which 176 papers were selected according to the defined exclusion criteria for further analysis and detailed review. The results reveal that most of the papers reported an increase in overall accuracy when using the attention mechanism within the DL methods for image classification, image segmentation, change detection, and object detection using remote sensing images.
KW  - remote sensing
KW  - image processing
KW  - attention mechanism
KW  - spatial attention
KW  - channel attention
KW  - deep learning
KW  - CNN
DO  - 10.3390/rs13152965
ER  -
TY  - EJOU
AU  - Hajjar, Chantal
AU  - Ghattas, Ghassan
AU  - Sarkis, Maya K.
AU  - Chamoun, Yolla G.
TI  - Vine Identification and Characterization in Goblet-Trained Vineyards Using Remotely Sensed Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - This paper proposes a novel approach for living and missing vine identification and vine characterization in goblet-trained vine plots using aerial images. Given the periodic structure of goblet vineyards, the RGB color coded parcel image is analyzed using proper processing techniques in order to determine the locations of living and missing vines. Vine characterization is achieved by implementing the marker-controlled watershed transform where the centers of the living vines serve as object markers. As a result, a precise mortality rate is calculated for each parcel. Moreover, all vines, even the overlapping ones, are fully recognized providing information about their size, shape, and green color intensity. The presented approach is fully automated and yields accuracy values exceeding 95% when the obtained results are assessed with ground-truth data. This unsupervised and automated approach can be applied to any type of plots presenting similar spatial patterns requiring only the image as input.
KW  - vine characterization
KW  - missing and living vine identification
KW  - goblet vineyards
KW  - Hough transform
KW  - watershed transform
KW  - remote sensing
KW  - semantic segmentation
DO  - 10.3390/rs13152992
ER  -
TY  - EJOU
AU  - Wang, Yang
AU  - Tian, Yongzhong
AU  - Cao, Yan
TI  - Dam Siting: A Review
T2  - Water

PY  - 2021
VL  - 13
IS  - 15
SN  - 2073-4441

AB  - Dams can effectively regulate the spatial and temporal distribution of water resources, where the rationality of dam siting determines whether the role of dams can be effectively performed. This paper reviews the research literature on dam siting in the past 20 years, discusses the methods used for dam siting, focuses on the factors influencing dam siting, and assesses the impact of different dam functions on siting factors. The results show the following: (1) Existing siting methods can be categorized into three types—namely, GIS/RS-based siting, MCDM- and MCDM-GIS-based siting, and machine learning-based siting. GIS/RS emphasizes the ability to capture and analyze data, MCDM has the advantage of weighing the importance of the relationship between multiple factors, and machine learning methods have a strong ability to learn and process complex data. (2) Site selection factors vary greatly, depending on the function of the dam. For dams with irrigation and water supply as the main purpose, the site selection is more focused on the evaluation of water quality. For dams with power generation as the main purpose, the hydrological factors characterizing the power generation potential are the most important. For dams with flood control as the main purpose, the topography and geological conditions are more important. (3) The integration of different siting methods and the siting of new functional dams in the existing research is not sufficient. Future research should focus on the integration of different methods and disciplines, in order to explore the siting of new types of dams.
KW  - dam siting
KW  - multi-criteria decision-making
KW  - geographic information systems
KW  - machine learning
KW  - siting factors
DO  - 10.3390/w13152080
ER  -
TY  - EJOU
AU  - Wang, Hao
AU  - Lyu, Suxing
AU  - Ren, Yaxin
TI  - Paddy Rice Imagery Dataset for Panicle Segmentation
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 8
SN  - 2073-4395

AB  - Accurate panicle identification is a key step in rice-field phenotyping. Deep learning methods based on high-spatial-resolution images provide a high-throughput and accurate solution of panicle segmentation. Panicle segmentation tasks require costly annotations to train an accurate and robust deep learning model. However, few public datasets are available for rice-panicle phenotyping. We present a semi-supervised deep learning model training process, which greatly assists the annotation and refinement of training datasets. The model learns the panicle features with limited annotations and localizes more positive samples in the datasets, without further interaction. After the dataset refinement, the number of annotations increased by 40.6%. In addition, we trained and tested modern deep learning models to show how the dataset is beneficial to both detection and segmentation tasks. Results of our comparison experiments can inspire others in dataset preparation and model selection.
KW  - image segmentation
KW  - panicle detection
KW  - deep learning
KW  - smart agriculture
KW  - unmanned aerial vehicle platform
DO  - 10.3390/agronomy11081542
ER  -
TY  - EJOU
AU  - Xiao, Guoquan
AU  - Tong, Chao
AU  - Wang, Yue
AU  - Guan, Shuaishuai
AU  - Hong, Xiaobin
AU  - Shang, Bin
TI  - CFD Simulation of the Safety of Unmanned Ship Berthing under the Influence of Various Factors
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 15
SN  - 2076-3417

AB  - The safety of unmanned ship berthing is of paramount importance. In order to explore the influence of wind and wave coupling, a berthing computational fluid dynamics (CFD) model was established, and the characteristics of speed field, pressure field, and vortex have been obtained under different speed, wind direction, and the quay wall distances. The results show that the total resistance of the hull against the current can be about 1.60 times higher compared to the downstream resistance, water flow resistance is the dominant factor, accounting for more than 80% of the total resistance. When changing the distance between ship and shore at fixed speed, the results found that the torque is small, but the growth rate is very large when driving below 2 m/s, and the torque growth rate is stable above 2 m/s. Based on the established coupling model, a multi-factor berthing safety study is carried out on an actual unmanned ship. The results show that when the speed increases from 4 m/s to 12 m/s, the curve slope is small, the resistance increases from 3666 N to 18,056 N, and the rear slope increases. The pressure increases with the speed, and when the speed is 24 m/s, the maximum pressure is up to 238,869 Pa. When the wind speed is fixed, the vertical force of the unmanned ship increases first and then decreases to zero and then reverses the same law change, and the maximum resistance is about 425 N at the wind angle of about 45 degrees; At 90 degrees, the maximum lateral force on an unmanned boat is about 638 N. The above results can provide control strategy for unmanned ship berthing safety, and provide theoretical basis for unmanned ship route planning and obstacle avoidance, safety design, etc.
KW  - unmanned ship
KW  - CFD coupling model
KW  - berthing safety
KW  - autonomous berthing algorithm
DO  - 10.3390/app11157102
ER  -
TY  - EJOU
AU  - Saitoh, Tomoko
AU  - Kobayashi, Moyu
TI  - Appropriate Drone Flight Altitude for Horse Behavioral Observation
T2  - Drones

PY  - 2021
VL  - 5
IS  - 3
SN  - 2504-446X

AB  - Recently, drone technology advanced, and its safety and operability markedly improved, leading to its increased application in animal research. This study demonstrated drone application in livestock management, using its technology to observe horse behavior and verify the appropriate horse–drone distance for aerial behavioral observations. Recordings were conducted from September to October 2017 on 11 horses using the Phantom 4 Pro drone. Four flight altitudes were tested (60, 50, 40, and 30 m) to investigate the reactions of the horses to the drones and observe their behavior; the recording time at each altitude was 5 min. None of the horses displayed avoidance behavior at any flight altitude, and the observer was able to distinguish between any two horses. Recorded behaviors were foraging, moving, standing, recumbency, avoidance, and others. Foraging was the most common behavior observed both directly and in the drone videos. The correlation coefficients of all behavioral data from direct and drone video observations at all altitudes were significant (p &lt; 0.01). These results indicate that horse behavior can be discerned with equal accuracy by both direct and recorded drone video observations. In conclusion, drones can be useful for recording and analyzing horse behavior.
KW  - horse
KW  - drone
KW  - behavioral observation
KW  - grazing
KW  - flight altitude
DO  - 10.3390/drones5030071
ER  -
TY  - EJOU
AU  - Ma, Huiqin
AU  - Huang, Wenjiang
AU  - Dong, Yingying
AU  - Liu, Linyi
AU  - Guo, Anting
TI  - Using UAV-Based Hyperspectral Imagery to Detect Winter Wheat Fusarium Head Blight
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 15
SN  - 2072-4292

AB  - Fusarium head blight (FHB) is a major winter wheat disease in China. The accurate and timely detection of wheat FHB is vital to scientific field management. By combining three types of spectral features, namely, spectral bands (SBs), vegetation indices (VIs), and wavelet features (WFs), in this study, we explore the potential of using hyperspectral imagery obtained from an unmanned aerial vehicle (UAV), to detect wheat FHB. First, during the wheat filling period, two UAV-based hyperspectral images were acquired. SBs, VIs, and WFs that were sensitive to wheat FHB were extracted and optimized from the two images. Subsequently, a field-scale wheat FHB detection model was formulated, based on the optimal spectral feature combination of SBs, VIs, and WFs (SBs + VIs + WFs), using a support vector machine. Two commonly used data normalization algorithms were utilized before the construction of the model. The single WFs, and the spectral feature combination of optimal SBs and VIs (SBs + VIs), were respectively used to formulate models for comparison and testing. The results showed that the detection model based on the normalized SBs + VIs + WFs, using min–max normalization algorithm, achieved the highest R2 of 0.88 and the lowest RMSE of 2.68% among the three models. Our results suggest that UAV-based hyperspectral imaging technology is promising for the field-scale detection of wheat FHB. Combining traditional SBs and VIs with WFs can improve the detection accuracy of wheat FHB effectively.
KW  - crop disease
KW  - remote sensing detection
KW  - hyperspectral imaging
KW  - spectral feature combination
KW  - data normalization
DO  - 10.3390/rs13153024
ER  -
TY  - EJOU
AU  - Sharma, Meenakshi
AU  - Kaushik, Prashant
AU  - Chawade, Aakash
TI  - Frontiers in the Solicitation of Machine Learning Approaches in Vegetable Science Research
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 15
SN  - 2071-1050

AB  - Along with essential nutrients and trace elements, vegetables provide raw materials for the food processing industry. Despite this, plant diseases and unfavorable weather patterns continue to threaten the delicate balance between vegetable production and consumption. It is critical to utilize machine learning (ML) in this setting because it provides context for decision-making related to breeding goals. Cutting-edge technologies for crop genome sequencing and phenotyping, combined with advances in computer science, are currently fueling a revolution in vegetable science and technology. Additionally, various ML techniques such as prediction, classification, and clustering are frequently used to forecast vegetable crop production in the field. In the vegetable seed industry, machine learning algorithms are used to assess seed quality before germination and have the potential to improve vegetable production with desired features significantly; whereas, in plant disease detection and management, the ML approaches can improve decision-support systems that assist in converting massive amounts of data into valuable recommendations. On similar lines, in vegetable breeding, ML approaches are helpful in predicting treatment results, such as what will happen if a gene is silenced. Furthermore, ML approaches can be a saviour to insufficient coverage and noisy data generated using various omics platforms. This article examines ML models in the field of vegetable sciences, which encompasses breeding, biotechnology, and genome sequencing.
KW  - machine learning
KW  - vegetables
KW  - models
KW  - predictions
KW  - breeding
KW  - biotechnology
KW  - genomics
DO  - 10.3390/su13158600
ER  -
TY  - EJOU
AU  - Sheu, Ming-Hwa
AU  - Jhang, Yu-Syuan
AU  - Morsalin, S M.
AU  - Huang, Yao-Fong
AU  - Sun, Chi-Chia
AU  - Lai, Shin-Chi
TI  - UAV Object Tracking Application Based on Patch Color Group Feature on Embedded System
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 15
SN  - 2079-9292

AB  - The discriminative object tracking system for unmanned aerial vehicles (UAVs) is widely used in numerous applications. While an ample amount of research has been carried out in this domain, implementing a low computational cost algorithm on a UAV onboard embedded system is still challenging. To address this issue, we propose a low computational complexity discriminative object tracking system for UAVs approach using the patch color group feature (PCGF) framework in this work. The tracking object is separated into several non-overlapping local image patches then the features are extracted into the PCGFs, which consist of the Gaussian mixture model (GMM). The object location is calculated by the similar PCGFs comparison from the previous frame and current frame. The background PCGFs of the object are removed by four directions feature scanning and dynamic threshold comparison, which improve the performance accuracy. In the terms of speed execution, the proposed algorithm accomplished 32.5 frames per second (FPS) on the x64 CPU platform without a GPU accelerator and 17 FPS in Raspberry Pi 4. Therefore, this work could be considered as a good solution for achieving a low computational complexity PCGF algorithm on a UAV onboard embedded system to improve flight times.
KW  - unmanned aerial vehicle (UAV)
KW  - UAV object tracking
KW  - Gaussian mixture model (GMM)
KW  - patch color group feature (PCGF)
KW  - embedded system
DO  - 10.3390/electronics10151864
ER  -
TY  - EJOU
AU  - Zhang, Jing
AU  - Li, Jiwu
AU  - Yang, Hongwei
AU  - Feng, Xin
AU  - Sun, Geng
TI  - Complex Environment Path Planning for Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 15
SN  - 1424-8220

AB  - Flying safely in complex urban environments is a challenge for unmanned aerial vehicles because path planning in urban environments with many narrow passages and few dynamic flight obstacles is difficult. The path planning problem is decomposed into global path planning and local path adjustment in this paper. First, a branch-selected rapidly-exploring random tree (BS-RRT) algorithm is proposed to solve the global path planning problem in environments with narrow passages. A cyclic pruning algorithm is proposed to shorten the length of the planned path. Second, the GM(1,1) model is improved with optimized background value named RMGM(1,1) to predict the flight path of dynamic obstacles. Herein, the local path adjustment is made by analyzing the prediction results. BS-RRT demonstrated a faster convergence speed and higher stability in narrow passage environments when compared with RRT, RRT-Connect, P-RRT, 1-0 Bg-RRT, and RRT*. In addition, the path planned by BS-RRT through the use of the cyclic pruning algorithm was the shortest. The prediction error of RMGM(1,1) was compared with those of ECGM(1,1), PCGM(1,1), GM(1,1), MGM(1,1), and GDF. The trajectory predicted by RMGM(1,1) was closer to the actual trajectory. Finally, we use the two methods to realize path planning in urban environments.
KW  - unmanned aerial vehicles
KW  - narrow passages
KW  - path planning
KW  - pruning
KW  - trajectory prediction
DO  - 10.3390/s21155250
ER  -
TY  - EJOU
AU  - Lee, Dong-Ho
AU  - Kim, Hyeon-Jin
AU  - Park, Jong-Hwa
TI  - UAV, a Farm Map, and Machine Learning Technology Convergence Classification Method of a Corn Cultivation Area
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 8
SN  - 2073-4395

AB  - South Korea’s agriculture is characterized by a mixture of various cultivated crops. In such an agricultural environment, convergence technology for ICT (information, communications, and technology) and AI (artificial intelligence) as well as agriculture is required to classify objects and predict yields. In general, the classification of paddy fields and field boundaries takes a lot of time and effort. The Farm Map was developed to clearly demarcate and classify the boundaries of paddy fields and fields in Korea. Therefore, this study tried to minimize the time and effort required to divide paddy fields and fields through the application of the Farm Map. To improve the fact that UAV image processing for a wide area requires a lot of time and effort to classify objects, we suggest a method for optimizing cultivated crop recognition. This study aimed to evaluate the applicability and effectiveness of machine learning classification techniques using a Farm Map in object-based mapping of agricultural land using unmanned aerial vehicles (UAVs). In this study, the advanced function selection method for object classification is to improve classification accuracy by using two types of classifiers, support vector machine (SVM) and random forest (RF). As a result of classification by applying a Farm Map-based SVM algorithm to wide-area UAV images, producer’s accuracy (PA) was 81.68%, user’s accuracy (UA) was 75.09%, the Kappa coefficient was 0.77, and the F-measure was 0.78. The results of classification by the Farm Map-based RF algorithm were as follows: PA of 96.58%, UA of 92.27%, a Kappa coefficient of 0.94, and the F-measure of 0.94. In the cultivation environment in which various crops were mixed, the corn cultivation area was estimated to be 96.54 ha by SVM, showing an accuracy of 90.27%. RF provided an estimate of 98.77 ha and showed an accuracy of 92.36%, which was higher than that of SVM. As a result of using the Farm Map for the object-based classification method, the agricultural land classification showed a higher efficiency in terms of time than the existing object classification method. Most importantly, it was confirmed that the efficiency of data processing can be increased by minimizing the possibility of misclassification in the obtained results. The obtained results confirmed that rapid and reliable analysis is possible when the cultivated area of crops is identified using UAV images, a Farm Map, and machine learning.
KW  - unmanned aerial vehicles
KW  - Farm Map
KW  - support vector machines
KW  - random forest
DO  - 10.3390/agronomy11081554
ER  -
TY  - EJOU
AU  - Perroy, Ryan L.
AU  - Sullivan, Timo
AU  - Benitez, David
AU  - Hughes, R. F.
AU  - Keith, Lisa M.
AU  - Brill, Eva
AU  - Kissinger, Karma
AU  - Duda, Daniel
TI  - Spatial Patterns of ‘Ōhi‘a Mortality Associated with Rapid ‘Ōhi‘a Death and Ungulate Presence
T2  - Forests

PY  - 2021
VL  - 12
IS  - 8
SN  - 1999-4907

AB  - Effective forest management, particularly during forest disturbance events, requires timely and accurate monitoring information at appropriate spatial scales. In Hawai‘i, widespread ‘ōhi‘a (Metrosideros polymorpha Gaud.) mortality associated with introduced fungal pathogens affects forest stands across the archipelago, further impacting native ecosystems already under threat from invasive species. Here, we share results from an integrated monitoring program based on high resolution (&lt;5 cm) aerial imagery, field sampling, and confirmatory laboratory testing to detect and monitor ‘ōhi‘a mortality at the individual tree level across four representative sites on Hawai‘i island. We developed a custom imaging system for helicopter operations to map thousands of hectares (ha) per flight, a more useful scale than the ten to hundreds of ha typically covered using small, unoccupied aerial systems. Based on collected imagery, we developed a rating system of canopy condition to identify ‘ōhi‘a trees suspected of infection by the fungal pathogens responsible for rapid ‘ōhi‘a death (ROD); we used this system to quickly generate and share suspect tree candidate locations with partner agencies to rapidly detect new mortality outbreaks and prioritize field sampling efforts. In three of the four sites, 98% of laboratory samples collected from suspect trees assigned a high confidence rating (n = 50) and 89% of those assigned a medium confidence rating (n = 117) returned positive detections for the fungal pathogens responsible for ROD. The fourth site, which has a history of unexplained ‘ōhi‘a mortality, exhibited much lower positive detection rates: only 6% of sampled trees assigned a high confidence rating (n = 16) and 0% of the sampled suspect trees assigned a medium confidence rating (n = 20) were found to be positive for the pathogen. The disparity in positive detection rates across study sites illustrates challenges to definitively determine the cause of ‘ōhi‘a mortality from aerial imagery alone. Spatial patterns of ROD-associated ‘ōhi‘a mortality were strongly affected by ungulate presence or absence as measured by the density of suspected ROD trees in fenced (i.e., ungulate-free) and unfenced (i.e., ungulate present) areas. Suspected ROD tree densities in neighboring areas containing ungulates were two to 69 times greater than those found in ungulate-free zones. In one study site, a fence line breach occurred during the study period, and feral ungulates entered an area that was previously ungulate-free. Following the breach, suspect ROD tree densities in this area rose from 0.02 to 2.78 suspect trees/ha, highlighting the need for ungulate control to protect ‘ōhi‘a stands from Ceratocystis-induced mortality and repeat monitoring to detect forest changes and resource threats.
KW  - Hawai‘i
KW  - Metrosideros polymorpha
KW  - Ceratocystis lukuohia
KW  - remote sensing
KW  - helicopter
KW  - visible imagery
DO  - 10.3390/f12081035
ER  -
TY  - EJOU
AU  - Pikalov, Simon
AU  - Azaria, Elisha
AU  - Sonnenberg, Shaya
AU  - Ben-Moshe, Boaz
AU  - Azaria, Amos
TI  - Vision-Less Sensing for Autonomous Micro-Drones
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - This work presents a concept of intelligent vision-less micro-drones, which are motivated by flying animals such as insects, birds, and bats. The presented micro-drone (named BAT: Blind Autonomous Tiny-drone) can perform bio-inspired complex tasks without the use of cameras. The BAT uses LIDARs and self-emitted optical-flow in order to perform obstacle avoiding and maze-solving. The controlling algorithms were implemented on an onboard micro-controller, allowing the BAT to be fully autonomous. We further present a method for using the information collected by the drone to generate a detailed mapping of the environment. A complete model of the BAT was implemented and tested using several scenarios both in simulation and field experiments, in which it was able to explore and map complex building autonomously even in total darkness.
KW  - autonomous micro-drones
KW  - sensor fusion
KW  - indoor mapping
KW  - bio-inspired micro-robotics
DO  - 10.3390/s21165293
ER  -
TY  - EJOU
AU  - Qi, Guanghui
AU  - Chang, Chunyan
AU  - Yang, Wei
AU  - Gao, Peng
AU  - Zhao, Gengxing
TI  - Soil Salinity Inversion in Coastal Corn Planting Areas by the Satellite-UAV-Ground Integration Approach
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - Soil salinization is a significant factor affecting corn growth in coastal areas. How to use multi-source remote sensing data to achieve the target of rapid, efficient and accurate soil salinity monitoring in a large area is worth further study. In this research, using Kenli District of the Yellow River Delta as study area, the inversion of soil salinity in a corn planting area was carried out based on the integration of ground imaging hyperspectral, unmanned aerial vehicles (UAV) multispectral and Sentinel-2A satellite multispectral images. The UAV and ground images were fused, and the partial least squares inversion model was constructed by the fused UAV image. Then, inversion model was scaled up to the satellite by the TsHARP method, and finally, the accuracy of the satellite-UAV-ground inversion model and results was verified. The results show that the band fusion of UAV and ground images effectively enrich the spectral information of the UAV image. The accuracy of the inversion model constructed based on the fused UAV images was improved. The inversion results of soil salinity based on the integration of satellite-UAV-ground were highly consistent with the measured soil salinity (R2 = 0.716 and RMSE = 0.727), and the inversion model had excellent universal applicability. This research integrated the advantages of multi-source data to establish a unified satellite-UAV-ground model, which improved the ability of large-scale remote sensing data to finely indicate soil salinity.
KW  - Sentinel-2A
KW  - UAV
KW  - ground imaging hyperspectral
KW  - multi-source remote sensing data
KW  - soil salinity
DO  - 10.3390/rs13163100
ER  -
TY  - EJOU
AU  - Sinaice, Brian B.
AU  - Owada, Narihiro
AU  - Saadat, Mahdi
AU  - Toriya, Hisatoshi
AU  - Inagaki, Fumiaki
AU  - Bagai, Zibisani
AU  - Kawamura, Youhei
TI  - Coupling NCA Dimensionality Reduction with Machine Learning in Multispectral Rock Classification Problems
T2  - Minerals

PY  - 2021
VL  - 11
IS  - 8
SN  - 2075-163X

AB  - Though multitudes of industries depend on the mining industry for resources, this industry has taken hits in terms of declining mineral ore grades and its current use of traditional, time-consuming and computationally costly rock and mineral identification methods. Therefore, this paper proposes integrating Hyperspectral Imaging, Neighbourhood Component Analysis (NCA) and Machine Learning (ML) as a combined system that can identify rocks and minerals. Modestly put, hyperspectral imaging gathers electromagnetic signatures of the rocks in hundreds of spectral bands. However, this data suffers from what is termed the ‘dimensionality curse’, which led to our employment of NCA as a dimensionality reduction technique. NCA, in turn, highlights the most discriminant feature bands, number of which being dependent on the intended application(s) of this system. Our envisioned application is rock and mineral classification via unmanned aerial vehicle (UAV) drone technology. In this study, we performed a 204-hyperspectral to 5-band multispectral reduction, because current production drones are limited to five multispectral bands sensors. Based on these bands, we applied ML to identify and classify rocks, thereby proving our hypothesis, reducing computational costs, attaining an ML classification accuracy of 71%, and demonstrating the potential mining industry optimisations attainable through this integrated system.
KW  - hyperspectral imaging
KW  - multispectral imaging
KW  - dimensionality reduction
KW  - neighbourhood component analysis
KW  - artificial intelligence
KW  - machine learning
DO  - 10.3390/min11080846
ER  -
TY  - EJOU
AU  - Jembre, Yalew Z.
AU  - Nugroho, Yuniarto W.
AU  - Khan, Muhammad T.
AU  - Attique, Muhammad
AU  - Paul, Rajib
AU  - Shah, Syed H.
AU  - Kim, Beomjoon
TI  - Evaluation of Reinforcement and Deep Learning Algorithms in Controlling Unmanned Aerial Vehicles
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 16
SN  - 2076-3417

AB  - Unmanned Aerial Vehicles (UAVs) are abundantly becoming a part of society, which is a trend that is expected to grow even further. The quadrotor is one of the drone technologies that is applicable in many sectors and in both military and civilian activities, with some applications requiring autonomous flight. However, stability, path planning, and control remain significant challenges in autonomous quadrotor flights. Traditional control algorithms, such as proportional-integral-derivative (PID), have deficiencies, especially in tuning. Recently, machine learning has received great attention in flying UAVs to desired positions autonomously. In this work, we configure the quadrotor to fly autonomously by using agents (the machine learning schemes being used to fly the quadrotor autonomously) to learn about the virtual physical environment. The quadrotor will fly from an initial to a desired position. When the agent brings the quadrotor closer to the desired position, it is rewarded; otherwise, it is punished. Two reinforcement learning models, Q-learning and SARSA, and a deep learning deep Q-network network are used as agents. The simulation is conducted by integrating the robot operating system (ROS) and Gazebo, which allowed for the implementation of the learning algorithms and the physical environment, respectively. The result has shown that the Deep Q-network network with Adadelta optimizer is the best setting to fly the quadrotor from the initial to desired position.
KW  - reinforcement learning
KW  - UAV
KW  - quadrotor
KW  - flight control
KW  - intelligent control
DO  - 10.3390/app11167240
ER  -
TY  - EJOU
AU  - Kim, Yongsu
AU  - Kang, Hyoeun
AU  - Suryanto, Naufal
AU  - Larasati, Harashta T.
AU  - Mukaroh, Afifatul
AU  - Kim, Howon
TI  - Extended Spatially Localized Perturbation GAN (eSLP-GAN) for Robust Adversarial Camouflage Patches
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - Deep neural networks (DNNs), especially those used in computer vision, are highly vulnerable to adversarial attacks, such as adversarial perturbations and adversarial patches. Adversarial patches, often considered more appropriate for a real-world attack, are attached to the target object or its surroundings to deceive the target system. However, most previous research employed adversarial patches that are conspicuous to human vision, making them easy to identify and counter. Previously, the spatially localized perturbation GAN (SLP-GAN) was proposed, in which the perturbation was only added to the most representative area of the input images, creating a spatially localized adversarial camouflage patch that excels in terms of visual fidelity and is, therefore, difficult to detect by human vision. In this study, the use of the method called eSLP-GAN was extended to deceive classifiers and object detection systems. Specifically, the loss function was modified for greater compatibility with an object-detection model attack and to increase robustness in the real world. Furthermore, the applicability of the proposed method was tested on the CARLA simulator for a more authentic real-world attack scenario.
KW  - adversarial patch
KW  - generative adversarial networks
KW  - camouflage
DO  - 10.3390/s21165323
ER  -
TY  - EJOU
AU  - Ramalingam, Balakrishnan
AU  - Tun, Thein
AU  - Mohan, Rajesh E.
AU  - Gómez, Braulio F.
AU  - Cheng, Ruoxi
AU  - Balakrishnan, Selvasundari
AU  - Mohan Rayaguru, Madan
AU  - Hayat, Abdullah A.
TI  - AI Enabled IoRT Framework for Rodent Activity Monitoring in a False Ceiling Environment
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - Routine rodent inspection is essential to curbing rat-borne diseases and infrastructure damages within the built environment. Rodents find false ceilings to be a perfect spot to seek shelter and construct their habitats. However, a manual false ceiling inspection for rodents is laborious and risky. This work presents an AI-enabled IoRT framework for rodent activity monitoring inside a false ceiling using an in-house developed robot called “Falcon”. The IoRT serves as a bridge between the users and the robots, through which seamless information sharing takes place. The shared images by the robots are inspected through a Faster RCNN ResNet 101 object detection algorithm, which is used to automatically detect the signs of rodent inside a false ceiling. The efficiency of the rodent activity detection algorithm was tested in a real-world false ceiling environment, and detection accuracy was evaluated with the standard performance metrics. The experimental results indicate that the algorithm detects rodent signs and 3D-printed rodents with a good confidence level.
KW  - rodent detection
KW  - faster RCNN
KW  - deep learning
KW  - object detection
KW  - IoRT
KW  - inspection robot
DO  - 10.3390/s21165326
ER  -
TY  - EJOU
AU  - Chao, Zhenhua
AU  - Fang, Xuan
AU  - Na, Jiaming
AU  - Che, Mingliang
TI  - A Collaborative Sensing System for Farmland Water Conservancy Project Maintenance through Integrating Satellite, Aerial, and Ground Observations
T2  - Water

PY  - 2021
VL  - 13
IS  - 16
SN  - 2073-4441

AB  - More and more attention has been paid to farmland water conservancy project (FWCP) maintenance in China, which can reallocate water resources in a more rational and efficient manner. Compared with the traditional survey such as field survey, FWCP maintenance can be improved efficiently with geospatial technology. To improve the level of FWCP maintenance in China, a collaborative sensing system framework by integrating satellite, aerial, and ground remote sensing is put forward. The structure of the system framework includes three sections, namely the data acquisition, the operational work, and the application and service. Through the construction and operation of such collaborative sensing system, it will break through the limitation of any single remote sensing platform and provide all-around and real-time information on FWCP. The collaborative monitoring schemes for the designed FWCP maintenance can engage ditch riders to maintain more effectively, which will enable them to communicate more specifically with smallholders in the process of irrigation. Only when ditch riders and farmers are fully involved, irrigation efficiency will be improved. Furthermore, the collaborative sensing system needs feasible standards for multi-source remote sensing data processing and intelligent information extraction such as data fusion, data assimilation, and data mining. In a way, this will promote the application of remote sensing in the field of agricultural irrigation and water saving. On the whole, it will be helpful to improve the traditional maintenance problems and is also the guarantee for establishing a long-term scientific management mechanism of FWCP maintenance in developing countries, especially in China.
KW  - unmanned aerial vehicle
KW  - hydroelectric conversion coefficient of pumping station
KW  - irrigation pump station
KW  - routine maintenance
KW  - emergency maintenance
KW  - irrigation performance
DO  - 10.3390/w13162163
ER  -
TY  - EJOU
AU  - Zheng, Yuemin
AU  - Tao, Jin
AU  - Sun, Hao
AU  - Sun, Qinglin
AU  - Chen, Zengqiang
AU  - Dehmer, Matthias
AU  - Zhou, Quan
TI  - Load Frequency Active Disturbance Rejection Control for Multi-Source Power System Based on Soft Actor-Critic
T2  - Energies

PY  - 2021
VL  - 14
IS  - 16
SN  - 1996-1073

AB  - To ensure the safe operation of an interconnected power system, it is necessary to maintain the stability of the frequency and the tie-line exchanged power. This is one of the hottest issues in the power system field and is usually called load frequency control. To overcome the influences of load disturbances on multi-source power systems containing thermal power plants, hydropower plants, and gas turbine plants, we design a linear active disturbance rejection control (LADRC) based on the tie-line bias control mode. For LADRC, the parameter selection of the controller directly affects the response performance of the entire system, and it is usually not feasible to manually adjust parameters. Therefore, to obtain the optimal controller parameters, we use the Soft Actor-Critic algorithm in reinforcement learning to obtain the controller parameters in real time, and we design the reward function according to the needs of the power system. We carry out simulation experiments to verify the effectiveness of the proposed method. Compared with the results of other proportional–integral–derivative control techniques using optimization algorithms and LADRC with constant parameters, the proposed method shows significant advantages in terms of overshoot, undershoot, and settling time. In addition, by adding different disturbances to different areas of the multi-source power system, we demonstrate the robustness of the proposed control strategy.
KW  - load frequency control
KW  - linear active disturbance rejection control
KW  - soft actor-critic
KW  - multi-source power system
KW  - reinforcement learning
DO  - 10.3390/en14164804
ER  -
TY  - EJOU
AU  - Monteiro, António
AU  - Santos, Sérgio
AU  - Gonçalves, Pedro
TI  - Precision Agriculture for Crop and Livestock Farming—Brief Review
T2  - Animals

PY  - 2021
VL  - 11
IS  - 8
SN  - 2076-2615

AB  - In the last few decades, agriculture has played an important role in the worldwide economy. The need to produce more food for a rapidly growing population is creating pressure on crop and animal production and a negative impact to the environment. On the other hand, smart farming technologies are becoming increasingly common in modern agriculture to assist in optimizing agricultural and livestock production and minimizing the wastes and costs. Precision agriculture (PA) is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. Precision livestock farming (PLF), relying on the automatic monitoring of individual animals, is used for animal growth, milk production, and the detection of diseases as well as to monitor animal behavior and their physical environment, among others. This study aims to briefly review recent scientific and technological trends in PA and their application in crop and livestock farming, serving as a simple research guide for the researcher and farmer in the application of technology to agriculture. The development and operation of PA applications involve several steps and techniques that need to be investigated further to make the developed systems accurate and implementable in commercial environments.
KW  - crop and animal production
KW  - smart farming technologies
KW  - precision agriculture
KW  - precision livestock farming
KW  - trends
DO  - 10.3390/ani11082345
ER  -
TY  - EJOU
AU  - Zhang, Qiang
AU  - Sun, Banyong
AU  - Cheng, Yaxiong
AU  - Li, Xijie
TI  - Residual Self-Calibration and Self-Attention Aggregation Network for Crop Disease Recognition
T2  - International Journal of Environmental Research and Public Health

PY  - 2021
VL  - 18
IS  - 16
SN  - 1660-4601

AB  - The correct diagnosis and recognition of crop diseases play an important role in ensuring crop yields and preventing food safety. The existing methods for crop disease recognition mainly focus on accuracy while ignoring the algorithm’s robustness. In practice, the acquired images are often accompanied by various noises. These noises lead to a huge challenge for improving the robustness and accuracy of the recognition algorithm. In order to solve this problem, this paper proposes a residual self-calibration and self-attention aggregation network (RCAA-Net) for crop disease recognition in actual scenarios. The proposed RCAA-Net is composed of three main modules: (1) multi-scale residual module, (2) feedback self-calibration module, and (3) self-attention aggregation module. Specifically, the multi-scale residual module is designed to learn multi-scale features and provide both global and local information for the appearance of the disease to improve the performance of the model. The feedback self-calibration is proposed to improve the robustness of the model by suppressing the background noise in the original deep features. The self-attention aggregation module is introduced to further improve the robustness and accuracy of the model by capturing multi-scale information in different semantic spaces. The experimental results on the challenging 2018ai_challenger crop disease recognition dataset show that the proposed RCAA-Net achieves state-of-the-art performance on robustness and accuracy for crop disease recognition in actual scenarios.
KW  - crop disease recognition
KW  - self-calibration
KW  - self-attention
KW  - residual
DO  - 10.3390/ijerph18168404
ER  -
TY  - EJOU
AU  - Abro, Ghulam E.
AU  - Zulkifli, Saiful Azrin B. M.
AU  - Asirvadam, Vijanth S.
AU  - Ali, Zain A.
TI  - Model-Free-Based Single-Dimension Fuzzy SMC Design for Underactuated Quadrotor UAV
T2  - Actuators

PY  - 2021
VL  - 10
IS  - 8
SN  - 2076-0825

AB  - The underactuated quadrotor unmanned aerial vehicle (UAV) is one of the nonlinear systems that have few actuators as compared to the degree of freedom (DOF); thus, it is a strenuous task to stabilize its attitude and positions. Moreover, an induction of unmodelled dynamic factors and uncertainties make it more difficult to control its maneuverability. In this paper, a model-free based single-dimension fuzzy sliding mode control (MFSDF-SMC) is proposed to control the attitude and positions of underactuated quadrotor UAV. The paper discusses the kinematic and dynamic models with unmodelled dynamic factors and unknown external disturbances. These unmodelled factors and disturbances may lead the quadrotor towards failure in tracking specific trajectory and may also generate some serious transient and steady-state issues. Furthermore, to avoid the problem of gimbal lock, the model is amalgamated with hyperbolic function to resolve the singularity issues dully developed due to Newton Euler’s dynamic modeling. The simulation results performed for MFSDF-SMC using MATLAB software R2020a are compared with conventional sliding mode control, fuzzy-based sliding control and single-dimension fuzzy-based sliding mode control without a model-free approach. The design and implementation of the model-free single dimension-based fuzzy sliding mode control (MFSDF-SMC) with an updated Lyapunov stability theorem is presented in this work. It is observed that MFSDF-SMC produces robust trajectory performance therefore, and the manuscript suggests the experimental setup to test the proposed algorithm in a noisy environment keeping the same conditions. The verification of the equipment used and its effective demonstration is also available for the reader within the manuscript.
KW  - model-free approach
KW  - quadrotor
KW  - single-dimension fuzzy
KW  - sliding mode control
KW  - unmodelled dynamics
KW  - underactuated system
DO  - 10.3390/act10080191
ER  -
TY  - EJOU
AU  - Yu, Bo
AU  - Chen, Fang
AU  - Xu, Chong
AU  - Wang, Lei
AU  - Wang, Ning
TI  - Matrix SegNet: A Practical Deep Learning Framework for Landslide Mapping from Images of Different Areas with Different Spatial Resolutions
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - Practical landslide inventory maps covering large-scale areas are essential in emergency response and geohazard analysis. Recently proposed techniques in landslide detection generally focused on landslides in pure vegetation backgrounds and image radiometric correction. There are still challenges in regard to robust methods that automatically detect landslides from images with multiple platforms and without radiometric correction. It is a significant issue in practical application. In order to detect landslides from images over different large-scale areas with different spatial resolutions, this paper proposes a two-branch Matrix SegNet to semantically segment input images by change detection. The Matrix SegNet learns landslide features in multiple scales and aspect ratios. The pre- and post- event images are captured directly from Google Earth, without radiometric correction. To evaluate the proposed framework, we conducted landslide detection in four study areas with two different spatial resolutions. Moreover, two other widely used frameworks: U-Net and SegNet, were adapted to detect landslides via the same data by change detection. The experiments show that our model improves the performance largely in terms of recall, precision, F1-score, and IOU. It is a good starting point to develop a practical, deep learning landslide detection framework for large scale application, using images from different areas, with different spatial resolutions.
KW  - landslide detection
KW  - Matrix nets
KW  - different spatial resolutions
DO  - 10.3390/rs13163158
ER  -
TY  - EJOU
AU  - Vargas, Jorge
AU  - Alsweiss, Suleiman
AU  - Toker, Onur
AU  - Razdan, Rahul
AU  - Santos, Joshua
TI  - An Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - Autonomous vehicles (AVs) rely on various types of sensor technologies to perceive the environment and to make logical decisions based on the gathered information similar to humans. Under ideal operating conditions, the perception systems (sensors onboard AVs) provide enough information to enable autonomous transportation and mobility. In practice, there are still several challenges that can impede the AV sensors’ operability and, in turn, degrade their performance under more realistic conditions that actually occur in the physical world. This paper specifically addresses the effects of different weather conditions (precipitation, fog, lightning, etc.) on the perception systems of AVs. In this work, the most common types of AV sensors and communication modules are included, namely: RADAR, LiDAR, ultrasonic, camera, and global navigation satellite system (GNSS). A comprehensive overview of their physical fundamentals, electromagnetic spectrum, and principle of operation is used to quantify the effects of various weather conditions on the performance of the selected AV sensors. This quantification will lead to several advantages in the simulation world by creating more realistic scenarios and by properly fusing responses from AV sensors in any object identification model used in AVs in the physical world. Moreover, it will assist in selecting the appropriate fading or attenuation models to be used in any X-in-the-loop (XIL, e.g., hardware-in-the-loop, software-in-the-loop, etc.) type of experiments to test and validate the manner AVs perceive the surrounding environment under certain conditions.
KW  - autonomous vehicles
KW  - sensors
KW  - perception
KW  - weather
KW  - camera
KW  - LiDAR
KW  - RADAR
KW  - GNSS
DO  - 10.3390/s21165397
ER  -
TY  - EJOU
AU  - Li, Linlin
AU  - Xu, Shufang
AU  - Nie, Hua
AU  - Mao, Yingchi
AU  - Yu, Shun
TI  - Collaborative Target Search Algorithm for UAV Based on Chaotic Disturbance Pigeon-Inspired Optimization
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 16
SN  - 2076-3417

AB  - Unmanned aerial vehicles (UAVs) have shown their superiority in military and civilian missions. In the face of complex tasks, many UAVs are usually needed to cooperate with each other. Therefore, multi-UAV cooperative target search has attracted more and more scholars’ attention. At present, there are many bionic algorithms for solving the cooperative search problem of multi-UAVs, including particle swarm optimization algorithm (PSO) and differential evolution (DE). Pigeon-inspired optimization (PIO) is a new swarm intelligence optimization algorithm proposed in recent years. It has great advantages over other algorithms in convergence, robustness, and accuracy, and has few parameters to be adjusted. Aiming at the shortcomings of the standard pigeon colony algorithm, such as poor population diversity, slow convergence speed, and the ease of falling into local optimum, we have proposed chaotic disturbance pigeon-inspired optimization (CDPIO) algorithm. The improved tent chaotic map was used to initialize the population and increase the diversity of the population. The disturbance factor is introduced in the iterative update stage of the algorithm to generate new individuals, replace the individuals with poor performance, and carry out disturbance to increase the optimization accuracy. Benchmark functions and UAV target search model were used to test the algorithm performance. The results show that the CDPIO had faster convergence speed, better optimization precision, better robustness, and better performance than PIO.
KW  - pigeon-inspired optimization
KW  - tent map
KW  - disturbance mechanism
KW  - cooperative target search
DO  - 10.3390/app11167358
ER  -
TY  - EJOU
AU  - Haider, Tazeem
AU  - Farid, Muhammad S.
AU  - Mahmood, Rashid
AU  - Ilyas, Areeba
AU  - Khan, Muhammad H.
AU  - Haider, Sakeena T.
AU  - Chaudhry, Muhammad H.
AU  - Gul, Mehreen
TI  - A Computer-Vision-Based Approach for Nitrogen Content Estimation in Plant Leaves
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 8
SN  - 2077-0472

AB  - Nitrogen is an essential nutrient element required for optimum crop growth and yield. If a specific amount of nitrogen is not applied to crops, their yield is affected. Estimation of nitrogen level in crops is momentous to decide the nitrogen fertilization in crops. The amount of nitrogen in crops is measured through different techniques, including visual inspection of leaf color and texture and by laboratory analysis of plant leaves. Laboratory analysis-based techniques are more accurate than visual inspection, but they are costly, time-consuming, and require skilled laboratorian and precise equipment. Therefore, computer-based systems are required to estimate the amount of nitrogen in field crops. In this paper, a computer vision-based solution is introduced to solve this problem as well as to help farmers by providing an easier, cheaper, and faster approach for measuring nitrogen deficiency in crops. The system takes an image of the crop leaf as input and estimates the amount of nitrogen in it. The image is captured by placing the leaf on a specially designed slate that contains the reference green and yellow colors for that crop. The proposed algorithm automatically extracts the leaf from the image and computes its color similarity with the reference colors. In particular, we define a green color value (GCV) index from this analysis, which serves as a nitrogen indicator. We also present an evaluation of different color distance models to find a model able to accurately capture the color differences. The performance of the proposed system is evaluated on a Spinacia oleracea dataset. The results of the proposed system and laboratory analysis are highly correlated, which shows the effectiveness of the proposed system.
KW  - nitrogen estimation
KW  - image processing
KW  - leaf contents
KW  - crop yield
KW  - color distance models
DO  - 10.3390/agriculture11080766
ER  -
TY  - EJOU
AU  - Takechi, Hitoshi
AU  - Aragaki, Shunsuke
AU  - Irie, Mitsuteru
TI  - Differentiation of River Sediments Fractions in UAV Aerial Images by Convolution Neural Network
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - Riverbed material has multiple functions in river ecosystems, such as habitats, feeding grounds, spawning grounds, and shelters for aquatic organisms, and particle size of riverbed material reflects the tractive force of the channel flow. Therefore, regular surveys of riverbed material are conducted for environmental protection and river flood control projects. The field method is the most conventional riverbed material survey. However, conventional surveys of particle size of riverbed material require much labor, time, and cost to collect material on site. Furthermore, its spatial representativeness is also a problem because of the limited survey area against a wide riverbank. As a further solution to these problems, in this study, we tried an automatic classification of riverbed conditions using aerial photography with an unmanned aerial vehicle (UAV) and image recognition with artificial intelligence (AI) to improve survey efficiency. Due to using AI for image processing, a large number of images can be handled regardless of whether they are of fine or coarse particles. We tried a classification of aerial riverbed images that have the difference of particle size characteristics with a convolutional neural network (CNN). GoogLeNet, Alexnet, VGG-16 and ResNet, the common pre-trained networks, were retrained to perform the new task with the 70 riverbed images using transfer learning. Among the networks tested, GoogleNet showed the best performance for this study. The overall accuracy of the image classification reached 95.4%. On the other hand, it was supposed that shadows of the gravels caused the error of the classification. The network retrained with the images taken in the uniform temporal period gives higher accuracy for classifying the images taken in the same period as the training data. The results suggest the potential of evaluating riverbed materials using aerial photography with UAV and image recognition with CNN.
KW  - channel bed condition
KW  - particle size
KW  - convolution neural network
KW  - UAV
DO  - 10.3390/rs13163188
ER  -
TY  - EJOU
AU  - Li, Kai-Yun
AU  - Burnside, Niall G.
AU  - de Lima, Raul S.
AU  - Peciña, Miguel V.
AU  - Sepp, Karli
AU  - Cabral Pinheiro, Victor H.
AU  - de Lima, Bruno R.
AU  - Yang, Ming-Der
AU  - Vain, Ants
AU  - Sepp, Kalev
TI  - An Automated Machine Learning Framework in Unmanned Aircraft Systems: New Insights into Agricultural Management Practices Recognition Approaches
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - The recent trend of automated machine learning (AutoML) has been driving further significant technological innovation in the application of artificial intelligence from its automated algorithm selection and hyperparameter optimization of the deployable pipeline model for unraveling substance problems. However, a current knowledge gap lies in the integration of AutoML technology and unmanned aircraft systems (UAS) within image-based data classification tasks. Therefore, we employed a state-of-the-art (SOTA) and completely open-source AutoML framework, Auto-sklearn, which was constructed based on one of the most widely used ML systems: Scikit-learn. It was combined with two novel AutoML visualization tools to focus particularly on the recognition and adoption of UAS-derived multispectral vegetation indices (VI) data across a diverse range of agricultural management practices (AMP). These include soil tillage methods (STM), cultivation methods (CM), and manure application (MA), and are under the four-crop combination fields (i.e., red clover-grass mixture, spring wheat, pea-oat mixture, and spring barley). Furthermore, they have currently not been efficiently examined and accessible parameters in UAS applications are absent for them. We conducted the comparison of AutoML performance using three other common machine learning classifiers, namely Random Forest (RF), support vector machine (SVM), and artificial neural network (ANN). The results showed AutoML achieved the highest overall classification accuracy numbers after 1200 s of calculation. RF yielded the second-best classification accuracy, and SVM and ANN were revealed to be less capable among some of the given datasets. Regarding the classification of AMPs, the best recognized period for data capture occurred in the crop vegetative growth stage (in May). The results demonstrated that CM yielded the best performance in terms of classification, followed by MA and STM. Our framework presents new insights into plant–environment interactions with capable classification capabilities. It further illustrated the automatic system would become an important tool in furthering the understanding for future sustainable smart farming and field-based crop phenotyping research across a diverse range of agricultural environmental assessment and management applications.
KW  - unmanned aircraft system
KW  - automated machine learning
KW  - agricultural management practices
KW  - image classification
KW  - precision agriculture
KW  - variety performance trials
KW  - crop breeding
KW  - crop phenotyping
KW  - agriculture decision-making
DO  - 10.3390/rs13163190
ER  -
TY  - EJOU
AU  - Ezzy, Haitham
AU  - Charter, Motti
AU  - Bonfante, Antonello
AU  - Brook, Anna
TI  - How the Small Object Detection via Machine Learning and UAS-Based Remote-Sensing Imagery Can Support the Achievement of SDG2: A Case Study of Vole Burrows
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - Small mammals, and particularly rodents, are common inhabitants of farmlands, where they play key roles in the ecosystem, but when overabundant, they can be major pests, able to reduce crop production and farmers’ incomes, with tangible effects on the achievement of Sustainable Development Goals no 2 (SDG2, Zero Hunger) of the United Nations. Farmers do not currently have a standardized, accurate method of detecting the presence, abundance, and locations of rodents in their fields, and hence do not have environmentally efficient methods of rodent control able to promote sustainable agriculture oriented to reduce the environmental impacts of cultivation. New developments in unmanned aerial system (UAS) platforms and sensor technology facilitate cost-effective data collection through simultaneous multimodal data collection approaches at very high spatial resolutions in environmental and agricultural contexts. Object detection from remote-sensing images has been an active research topic over the last decade. With recent increases in computational resources and data availability, deep learning-based object detection methods are beginning to play an important role in advancing remote-sensing commercial and scientific applications. However, the performance of current detectors on various UAS-based datasets, including multimodal spatial and physical datasets, remains limited in terms of small object detection. In particular, the ability to quickly detect small objects from a large observed scene (at field scale) is still an open question. In this paper, we compare the efficiencies of applying one- and two-stage detector models to a single UAS-based image and a processed (via Pix4D mapper photogrammetric program) UAS-based orthophoto product to detect rodent burrows, for agriculture/environmental applications as to support farmer activities in the achievements of SDG2. Our results indicate that the use of multimodal data from low-cost UASs within a self-training YOLOv3 model can provide relatively accurate and robust detection for small objects (mAP of 0.86 and an F1-score of 93.39%), and can deliver valuable insights for field management with high spatial precision able to reduce the environmental costs of crop production in the direction of precision agriculture management.
KW  - small object detection
KW  - UAS
KW  - YOLOv3
KW  - Faster R-CNN
KW  - EfficientNet
KW  - RetinaNet
DO  - 10.3390/rs13163191
ER  -
TY  - EJOU
AU  - Zhang, Binghan
AU  - Yang, Bin
AU  - Wang, Congjun
AU  - Wang, Zhichen
AU  - Liu, Boda
AU  - Fang, Tengwei
TI  - Computer Vision-Based Construction Process Sensing for Cyber–Physical Systems: A Review
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - Cyber–physical systems (CPSs) are generally considered to be the next generation of engineered systems. However, the actual application of CPSs in the Architecture, Engineering and Construction (AEC) industry is still at a low level. The sensing method in the construction process plays a very important role in the establishment of CPSs. Therefore, the purpose of this paper is to discuss the application potential of computer vision-based sensing methods and provide practical suggestions through a literature review. This paper provides a review of the current application of CPSs in the AEC industry, summarizes the current knowledge gaps, and discusses the problems with the current construction site sensing approach. Considering the unique advantages of the computer vision (CV) method at the construction site, the application of CV for different construction entities was reviewed and summarized to achieve a CV-based construction site sensing approach for construction process CPSs. The potential of CPS can be further stimulated by providing rich information from on-site sensing using CV methods. According to the review, this approach has unique advantages in the specific environment of the construction site. Based on the current knowledge gap identified in the literature review, this paper proposes a novel concept of visual-based construction site sensing method for CPS application, and an architecture for CV-based CPS is proposed as an implementation of this concept. The main contribution of this paper is to propose a CPS architecture using computer vision as the main information acquisition method based on the literature review. This architecture innovatively introduces computer vision as a sensing method of construction sites, and realizes low-cost and non-invasive information acquisition in complex construction scenarios. This method can be used as an important supplement to on-site sensing to further promote the automation and intelligence of the construction process.
KW  - computer vision
KW  - cyber–physical systems
KW  - sensing system
KW  - review
DO  - 10.3390/s21165468
ER  -
TY  - EJOU
AU  - Wang, Rui
AU  - Zou, Jialing
AU  - Wen, James Z.
TI  - SFA-MDEN: Semantic-Feature-Aided Monocular Depth Estimation Network Using Dual Branches
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - Monocular depth estimation based on unsupervised learning has attracted great attention due to the rising demand for lightweight monocular vision sensors. Inspired by multi-task learning, semantic information has been used to improve the monocular depth estimation models. However, multi-task learning is still limited by multi-type annotations. As far as we know, there are scarcely any large public datasets that provide all the necessary information. Therefore, we propose a novel network architecture Semantic-Feature-Aided Monocular Depth Estimation Network (SFA-MDEN) to extract multi-resolution depth features and semantic features, which are merged and fed into the decoder, with the goal of predicting depth with the support of semantics. Instead of using loss functions to relate the semantics and depth, the fusion of feature maps for semantics and depth is employed to predict the monocular depth. Therefore, two accessible datasets with similar topics for depth estimation and semantic segmentation can meet the requirements of SFA-MDEN for training sets. We explored the performance of the proposed SFA-MDEN with experiments on different datasets, including KITTI, Make3D, and our own dataset BHDE-v1. The experimental results demonstrate that SFA-MDEN achieves competitive accuracy and generalization capacity compared to state-of-the-art methods.
KW  - monocular depth estimation
KW  - semantic segmentation
KW  - feature fusion
KW  - multi-task deep learning
DO  - 10.3390/s21165476
ER  -
TY  - EJOU
AU  - Munawar, Hafiz S.
AU  - Hammad, Ahmed W. A.
AU  - Haddad, Assed
AU  - Soares, Carlos A.
AU  - Waller, S. T.
TI  - Image-Based Crack Detection Methods: A Review
T2  - Infrastructures

PY  - 2021
VL  - 6
IS  - 8
SN  - 2412-3811

AB  - Annually, millions of dollars are spent to carry out defect detection in key infrastructure including roads, bridges, and buildings. The aftermath of natural disasters like floods and earthquakes leads to severe damage to the urban infrastructure. Maintenance operations that follow for the damaged infrastructure often involve a visual inspection and assessment of their state to ensure their functional and physical integrity. Such damage may appear in the form of minor or major cracks, which gradually spread, leading to ultimate collapse or destruction of the structure. Crack detection is a very laborious task if performed via manual visual inspection. Many infrastructure elements need to be checked regularly and it is therefore not feasible as it will require significant human resources. This may also result in cases where cracks go undetected. A need, therefore, exists for performing automatic defect detection in infrastructure to ensure its effectiveness and reliability. Using image processing techniques, the captured or scanned images of the infrastructure parts can be analyzed to identify any possible defects. Apart from image processing, machine learning methods are being increasingly applied to ensure better performance outcomes and robustness in crack detection. This paper provides a review of image-based crack detection techniques which implement image processing and/or machine learning. A total of 30 research articles have been collected for the review which is published in top tier journals and conferences in the past decade. A comprehensive analysis and comparison of these methods are performed to highlight the most promising automated approaches for crack detection.
KW  - crack detection
KW  - machine learning
KW  - artificial intelligence
KW  - image processing
DO  - 10.3390/infrastructures6080115
ER  -
TY  - EJOU
AU  - Shen, Shengyu
AU  - Chen, Jiasheng
AU  - Zhang, Shaoyi
AU  - Cheng, Dongbing
AU  - Wang, Zhigang
AU  - Zhang, Tong
TI  - Deep Fusion of DOM and DSM Features for Benggang Discovery
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 8
SN  - 2220-9964

AB  - Benggang is a typical erosional landform in southern and southeastern China. Since benggang poses significant risks to local ecological environments and economic infrastructure, it is vital to accurately detect benggang-eroded areas. Relying only on remote sensing imagery for benggang detection cannot produce satisfactory results. In this study, we propose integrating high-resolution Digital Orthophoto Map (DOM) and Digital Surface Model (DSM) data for efficient and automatic benggang discovery. The fusion of complementary rich information hidden in both DOM and DSM data is realized by a two-stream convolutional neural network (CNN), which integrates aggregated terrain and activation image features that are both extracted by supervised deep learning. We aggregate local low-level geomorphic features via a supervised diffusion-convolutional embedding branch for expressive representations of benggang terrain variations. Activation image features are obtained from an image-oriented convolutional neural network branch. The two sources of information (DOM and DSM) are fused via a gated neural network, which learns the most discriminative features for the detection of benggang. The evaluation of a challenging benggang dataset demonstrates that our method exceeds several baselines, even with limited training examples. The results show that the fusion of DOM and DSM data is beneficial for benggang detection via supervised convolutional and deep fusion networks.
KW  - benggang
KW  - deep learning
KW  - fusion
KW  - CNN
KW  - DOM
KW  - DSM
DO  - 10.3390/ijgi10080556
ER  -
TY  - EJOU
AU  - Tina, Giuseppe M.
AU  - Ventura, Cristina
AU  - Ferlito, Sergio
AU  - De Vito, Saverio
TI  - A State-of-Art-Review on Machine-Learning Based Methods for PV
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 16
SN  - 2076-3417

AB  - In the current era, Artificial Intelligence (AI) is becoming increasingly pervasive with applications in several applicative fields effectively changing our daily life. In this scenario, machine learning (ML), a subset of AI techniques, provides machines with the ability to programmatically learn from data to model a system while adapting to new situations as they learn more by data they are ingesting (on-line training). During the last several years, many papers have been published concerning ML applications in the field of solar systems. This paper presents the state of the art ML models applied in solar energy’s forecasting field i.e., for solar irradiance and power production forecasting (both point and interval or probabilistic forecasting), electricity price forecasting and energy demand forecasting. Other applications of ML into the photovoltaic (PV) field taken into account are the modelling of PV modules, PV design parameter extraction, tracking the maximum power point (MPP), PV systems efficiency optimization, PV/Thermal (PV/T) and Concentrating PV (CPV) system design parameters’ optimization and efficiency improvement, anomaly detection and energy management of PV’s storage systems. While many review papers already exist in this regard, they are usually focused only on one specific topic, while in this paper are gathered all the most relevant applications of ML for solar systems in many different fields. The paper gives an overview of the most recent and promising applications of machine learning used in the field of photovoltaic systems.
KW  - machine learning
KW  - solar energy
KW  - forecast
KW  - diagnostic
KW  - electricity markets
DO  - 10.3390/app11167550
ER  -
TY  - EJOU
AU  - Wang, Kaixuan
AU  - Zhang, Jiaqiao
AU  - Ni, Hongjun
AU  - Ren, Fuji
TI  - Thermal Defect Detection for Substation Equipment Based on Infrared Image Using Convolutional Neural Network
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 16
SN  - 2079-9292

AB  - Thermal defects of substation equipment have a great impact on the stability of power systems. Temperature is crucial for thermal defect detection in infrared images. The traditional detection methods, which have low efficiency and poor accuracy, record the temperature of infrared images manually. In this study, a thermal defect detection method based on infrared images using a convolutional neural network (CNN) is proposed. Firstly, the improved pre-processing method is applied to reduce background information, and the region of interest is located according to the contour and position information, hence improving the quality of images. Then, the temperature values are segmented to establish the dataset (T-IR11), which contains 11 labels. Finally, the CNN model is constructed to extract features, and the support vector machine is trained for classification. To verify the effectiveness of the proposed method, precision, recall, and F1 score are adopted and 10-fold cross-validation is employed on the T-IR11 dataset. The results demonstrate that the accuracy of the proposed method is 99.50%, and the performance is superior to that of previous methods in terms of infrared images. The proposed method can realize automatic temperature recognition and equipment with thermal defects can be recorded systematically, which has significant practical value for defect detection in substation equipment.
KW  - infrared image
KW  - substation equipment
KW  - thermal defect detection
KW  - adaptive binarization
KW  - character recognition
KW  - convolutional neural network
DO  - 10.3390/electronics10161986
ER  -
TY  - EJOU
AU  - Liu, Zhijie
AU  - Guo, Pengju
AU  - Liu, Heng
AU  - Fan, Pan
AU  - Zeng, Pengzong
AU  - Liu, Xiangyang
AU  - Feng, Ce
AU  - Wang, Wang
AU  - Yang, Fuzeng
TI  - Gradient Boosting Estimation of the Leaf Area Index of Apple Orchards in UAV Remote Sensing
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - The leaf area index (LAI) is a key parameter for describing the canopy structure of apple trees. This index is also employed in evaluating the amount of pesticide sprayed per unit volume of apple trees. Hence, numerous manual and automatic methods have been explored for LAI estimation. In this work, the leaf area indices for different types of apple trees are obtained in terms of multispectral remote-sensing data collected with an unmanned aerial vehicle (UAV), along with simultaneous measurements of apple orchards. The proposed approach was tested on apple trees of the “Fuji”, “Golden Delicious”, and “Ruixue” types, which were planted in the Apple Experimental Station of the Northwest Agriculture and Forestry University in Baishui County, Shaanxi Province, China. Five vegetation indices of strong correlation with the apple leaf area index were selected and used to train models of support vector regression (SVR) and gradient-boosting decision trees (GBDT) for predicting the leaf area index of apple trees. The best model was selected based on the metrics of the coefficient of determination (R2) and the root-mean-square error (RMSE). The experimental results showed that the gradient-boosting decision tree model achieved the best performance with an R2 of 0.846, an RMSE of 0.356, and a spatial efficiency (SPAEF) of 0.57. This demonstrates the feasibility of our approach for fast and accurate remote-sensing-based estimation of the leaf area index of apple trees.
KW  - leaf area index
KW  - gradient-boosting decision trees
KW  - UAV remote sensing
KW  - apple orchards
KW  - vegetation index
DO  - 10.3390/rs13163263
ER  -
TY  - EJOU
AU  - Žuraulis, Vidas
AU  - Sivilevičius, Henrikas
AU  - Šabanovič, Eldar
AU  - Ivanov, Valentin
AU  - Skrickij, Viktor
TI  - Variability of Gravel Pavement Roughness: An Analysis of the Impact on Vehicle Dynamic Response and Driving Comfort
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 16
SN  - 2076-3417

AB  - Gravel pavement has lower construction costs but poorer performance than asphalt surfaces on roads. It also emits dust and deforms under the impact of vehicle loads and ambient air factors; the resulting ripples and ruts constantly deepen, and therefore increase vehicle vibrations and fuel consumption, and reduce safe driving speed and comfort. In this study, existing pavement quality evaluation indexes are analysed, and a methodology for adapting them for roads with gravel pavement is proposed. We report the measured wave depth and length of gravel pavement profile using the straightedge method on a 160 m long road section at three stages of road utilization. The measured pavement elevation was processed according to ISO 8608, and the frequency response of a vehicle was investigated using simulations in MATLAB/Simulink. The international roughness index (IRI) analysis showed that a speed of 30–45 km/h instead of 80 km/h provided the objective results of the IRI calculation on the flexible pavement due to the decreasing velocity of a vehicle’s unsprung mass on a more deteriorated road pavement state. The influence of the corrugation phenomenon of gravel pavement was explored, identifying specific driving safety and comfort cases. Finally, an increase in the dynamic load coefficient (DLC) at a low speed of 30 km/h on the most deteriorated pavement and a high speed of 90 km/h on the middle-quality pavement demonstrated the demand for timely gravel pavement maintenance and the complicated prediction of a safe driving speed for drivers. The main relevant objectives of this study are the adaptation of a road roughness indicator to gravel pavement, including the evaluation of vehicle dynamic responses at different speeds and pavement deterioration states.
KW  - gravel pavement
KW  - roughness
KW  - straightedge
KW  - power spectral density
KW  - international roughness index
KW  - vehicle response
KW  - driving comfort
DO  - 10.3390/app11167582
ER  -
TY  - EJOU
AU  - Ivošević, Bojana
AU  - Lugonja, Predrag
AU  - Brdar, Sanja
AU  - Radulović, Mirjana
AU  - Vujić, Ante
AU  - Valente, João
TI  - UAV-Based Land Cover Classification for Hoverfly (Diptera: Syrphidae) Habitat Condition Assessment: A Case Study on Mt. Stara Planina (Serbia)
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - Habitat degradation, mostly caused by human impact, is one of the key drivers of biodiversity loss. This is a global problem, causing a decline in the number of pollinators, such as hoverflies. In the process of digitalizing ecological studies in Serbia, remote-sensing-based land cover classification has become a key component for both current and future research. Object-based land cover classification, using machine learning algorithms of very high resolution (VHR) imagery acquired by an unmanned aerial vehicle (UAV) was carried out in three different study sites on Mt. Stara Planina, Eastern Serbia. UAV land cover classified maps with seven land cover classes (trees, shrubs, meadows, road, water, agricultural land, and forest patches) were studied. Moreover, three different classification algorithms—support vector machine (SVM), random forest (RF), and k-NN (k-nearest neighbors)—were compared. This study shows that the random forest classifier performs better with respect to the other classifiers in all three study sites, with overall accuracy values ranging from 0.87 to 0.96. The overall results are robust to changes in labeling ground truth subsets. The obtained UAV land cover classified maps were compared with the Map of the Natural Vegetation of Europe (EPNV) and used to quantify habitat degradation and assess hoverfly species richness. It was concluded that the percentage of habitat degradation is primarily caused by anthropogenic pressure, thus affecting the richness of hoverfly species in the study sites. In order to enable research reproducibility, the datasets used in this study are made available in a public repository.
KW  - unmanned aerial vehicle
KW  - object-based image analysis
KW  - Orfeo ToolBox
KW  - QGIS
KW  - random forest
KW  - hoverfly
KW  - Map of the Natural Vegetation of Europe
DO  - 10.3390/rs13163272
ER  -
TY  - EJOU
AU  - Ulhaq, Anwaar
AU  - Adams, Peter
AU  - Cox, Tarnya E.
AU  - Khan, Asim
AU  - Low, Tom
AU  - Paul, Manoranjan
TI  - Automated Detection of Animals in Low-Resolution Airborne Thermal Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 16
SN  - 2072-4292

AB  - Detecting animals to estimate abundance can be difficult, particularly when the habitat is dense or the target animals are fossorial. The recent surge in the use of thermal imagers in ecology and their use in animal detections can increase the accuracy of population estimates and improve the subsequent implementation of management programs. However, the use of thermal imagers results in many hours of captured flight videos which require manual review for confirmation of species detection and identification. Therefore, the perceived cost and efficiency trade-off often restricts the use of these systems. Additionally, for many off-the-shelf systems, the exported imagery can be quite low resolution (&lt;9 Hz), increasing the difficulty of using automated detections algorithms to streamline the review process. This paper presents an animal species detection system that utilises the cost-effectiveness of these lower resolution thermal imagers while harnessing the power of transfer learning and an enhanced small object detection algorithm. We have proposed a distant object detection algorithm named Distant-YOLO (D-YOLO) that utilises YOLO (You Only Look Once) and improves its training and structure for the automated detection of target objects in thermal imagery. We trained our system on thermal imaging data of rabbits, their active warrens, feral pigs, and kangaroos collected by thermal imaging researchers in New South Wales and Western Australia. This work will enhance the visual analysis of animal species while performing well on low, medium and high-resolution thermal imagery.
KW  - invasive species
KW  - thermal imaging
KW  - habitat identification
KW  - deep learning
KW  - drone
DO  - 10.3390/rs13163276
ER  -
TY  - EJOU
AU  - Shan, Donghui
AU  - Lei, Tian
AU  - Yin, Xiaohong
AU  - Luo, Qin
AU  - Gong, Lei
TI  - Extracting Key Traffic Parameters from UAV Video with On-Board Vehicle Data Validation
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - The advantages of UAV video in flexibility, traceability, easy-operation, and abundant information make it a popular and powerful aerial tool applied in traffic monitoring in recent years. This paper proposed a systematic approach to detect and track vehicles based on the YOLO v3 model and the deep SORT algorithm for further extracting key traffic parameters. A field experiment was implemented to provide data for model training and validation to ensure the accuracy of the proposed approach. In the experiment, 5400 frame images and 1192 speed points were collected from two test vehicles equipped with high-precision GNSS-RTK and onboard OBD after completion of seven experimental groups with a different height (150 m to 500 m) and operating speed (40 km/h to 90 km/h). The results indicate that the proposed approach exhibits strong robustness and reliability, due to the 90.88% accuracy of object detection and 98.9% precision of tracking vehicle. Moreover, the absolute and relative error of extracted speed falls within ±3 km/h and 2%, respectively. The overall accuracy of the extracted parameters reaches up to 98%.
KW  - UAV video
KW  - traffic information extraction
KW  - vehicle detection and tracking
KW  - validation experiment
KW  - accuracy
DO  - 10.3390/s21165620
ER  -
TY  - EJOU
AU  - Li, Xuanye
AU  - Li, Hongguang
AU  - Jiang, Yalong
AU  - Wang, Meng
TI  - Lightweight Detection Network Based on Sub-Pixel Convolution and Objectness-Aware Structure for UAV Images
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 16
SN  - 1424-8220

AB  - Unmanned Aerial Vehicles (UAVs) can serve as an ideal mobile platform in various situations. Real-time object detection with on-board apparatus provides drones with increased flexibility as well as a higher intelligence level. In order to achieve good detection results in UAV images with complex ground scenes, small object size and high object density, most of the previous work introduced models with higher computational burdens, making deployment on mobile platforms more difficult.This paper puts forward a lightweight object detection framework. Besides being anchor-free, the framework is based on a lightweight backbone and a simultaneous up-sampling and detection module to form a more efficient detection architecture. Meanwhile, we add an objectness branch to assist the multi-class center point prediction, which notably improves the detection accuracy and only takes up very little computing resources. The results of the experiment indicate that the computational cost of this paper is 92.78% lower than the CenterNet with ResNet18 backbone, and the mAP is 2.8 points higher on the Visdrone-2018-VID dataset. A frame rate of about 220 FPS is achieved. Additionally, we perform ablation experiments to check on the validity of each part, and the method we propose is compared with other representative lightweight object detection methods on UAV image datasets.
KW  - lightweight convolutional neural network
KW  - object detection
KW  - UAV images
DO  - 10.3390/s21165656
ER  -
TY  - EJOU
AU  - Huang, Xin
AU  - Dong, Xiaoya
AU  - Ma, Jing
AU  - Liu, Kuan
AU  - Ahmed, Shibbir
AU  - Lin, Jinlong
AU  - Qiu, Baijing
TI  - The Improved A* Obstacle Avoidance Algorithm for the Plant Protection UAV with Millimeter Wave Radar and Monocular Camera Data Fusion
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 17
SN  - 2072-4292

AB  - To enhance obstacle avoidance abilities of the plant protection UAV in unstructured farmland, this article improved the traditional A* algorithms through dynamic heuristic functions, search point optimization, and inflection point optimization based on millimeter wave radar and monocular camera data fusion. Obstacle information extraction experiments were carried out. The performance between the improved algorithm and traditional algorithm was compared. Additionally, obstacle avoidance experiments were also carried out. The results show that the maximum error in distance measurement of data fusion method was 8.2%. Additionally, the maximum error in obstacle width and height measurement were 27.3% and 18.5%, respectively. The improved algorithm is more useful in path planning, significantly reduces data processing time, search grid, and turning points. The algorithm at most increases path length by 2.0%, at least reduces data processing time by 68.4%, search grid by 74.9%, and turning points by 20.7%. The maximum trajectory offset error was proportional to the flight speed, with a maximum trajectory offset of 1.4 m. The distance between the UAV and obstacle was inversely proportional to flight speed, with a minimum distance of 1.6 m. This method can provide a new idea for obstacle avoidance of the plant protection UAV.
KW  - the plant protection UAV
KW  - obstacle avoidance
KW  - improved A* algorithm
KW  - millimeter wave radar
KW  - monocular camera
KW  - data fusion
DO  - 10.3390/rs13173364
ER  -
TY  - EJOU
AU  - Ansari, Emaad
AU  - Akhtar, Mohammad N.
AU  - Abdullah, Mohamad N.
AU  - Othman, Wan A.
AU  - Bakar, Elmi A.
AU  - Hawary, Ahmad F.
AU  - Alhady, Syed S.
TI  - Image Processing of UAV Imagery for River Feature Recognition of Kerian River, Malaysia
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 17
SN  - 2071-1050

AB  - The impact of floods is the most severe among the natural calamities occurring in Malaysia. The knock of floods is consistent and annually forces thousands of Malaysians to relocate. The lack of information from the Ministry of Environment and Water, Malaysia is the foremost obstacle in upgrading the flood mapping. With the expeditious evolution of computer techniques, processing of satellite and unmanned aerial vehicle (UAV) images for river hydromorphological feature detection and flood management have gathered pace in the last two decades. Different image processing algorithms—structure from motion (SfM), multi-view stereo (MVS), gradient vector flow (GVF) snake algorithm, etc.—and artificial neural networks are implemented for the monitoring and classification of river features. This paper presents the application of the k-means algorithm along with image thresholding to quantify variation in river surface flow areas and vegetation growth along Kerian River, Malaysia. The river characteristic recognition directly or indirectly assists in studying river behavior and flood monitoring. Dice similarity coefficient and Jaccard index are numerated between thresholded images that are clustered using the k-means algorithm and manually segmented images. Based on quantitative evaluation, a dice similarity coefficient and Jaccard index of up to 97.86% and 94.36% were yielded for flow area and vegetation calculation. Thus, the present technique is functional in evaluating river characteristics with reduced errors. With minimum errors, the present technique can be utilized for quantifying agricultural areas and urban areas around the river basin.
KW  - image processing
KW  - unmanned aerial vehicle
KW  - feature recognition
KW  - image segmentation
KW  - color space
KW  - floods
KW  - sediment
DO  - 10.3390/su13179568
ER  -
TY  - EJOU
AU  - Grau, Joan
AU  - Liang, Kang
AU  - Ogilvie, Jae
AU  - Arp, Paul
AU  - Li, Sheng
AU  - Robertson, Bonnie
AU  - Meng, Fan-Rui
TI  - Using Unmanned Aerial Vehicle and LiDAR-Derived DEMs to Estimate Channels of Small Tributary Streams
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 17
SN  - 2072-4292

AB  - Defining stream channels in a watershed is important for assessing freshwater habitat availability, complexity, and quality. However, mapping channels of small tributary streams becomes challenging due to frequent channel change and dense vegetation coverage. In this study, we used an Unmanned Aerial Vehicle (UAV) and photogrammetry method to obtain a 3D Digital Surface Model (DSM) to estimate the total in-stream channel and channel width within grazed riparian pastures. We used two methods to predict the stream channel boundary: the Slope Gradient (SG) and Vertical Slope Position (VSP). As a comparison, the same methods were also applied using low-resolution DEM, obtained with traditional photogrammetry (coarse resolution) and two more LiDAR-derived DEMs with different resolution. When using the SG method, the higher-resolution, UAV-derived DEM provided the best agreement with the field-validated area followed by the high-resolution LiDAR DEM, with Mean Squared Errors (MSE) of 1.81 m and 1.91 m, respectively. The LiDAR DEM collected at low resolution was able to predict the stream channel with a MSE of 3.33 m. Finally, the coarse DEM did not perform accurately and the MSE obtained was 26.76 m. On the other hand, when the VSP method was used we found that low-resolution LiDAR DEM performed the best followed by high-resolution LiDAR, with MSE values of 9.70 and 11.45 m, respectively. The MSE for the UAV-derived DEM was 15.12 m and for the coarse DEM was 20.78 m. We found that the UAV-derived DEM could be used to identify steep bank which could be used for mapping the hydrogeomorphology of lower order streams. Therefore, UAVs could be applied to efficiently map small stream channels in order to monitor the dynamic changes occurring in these ecosystems at a local scale. However, the VSP method should be used to map stream channels in small watersheds when high resolution DEM data is not available.
KW  - DEM
KW  - LiDAR
KW  - UAV
KW  - stream bank
KW  - VSP
KW  - Slope Gradient
DO  - 10.3390/rs13173380
ER  -
TY  - EJOU
AU  - Qader, Sarchil H.
AU  - Dash, Jadu
AU  - Alegana, Victor A.
AU  - Khwarahm, Nabaz R.
AU  - Tatem, Andrew J.
AU  - Atkinson, Peter M.
TI  - The Role of Earth Observation in Achieving Sustainable Agricultural Production in Arid and Semi-Arid Regions of the World
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 17
SN  - 2072-4292

AB  - Crop production is a major source of food and livelihood for many people in arid and semi-arid (ASA) regions across the world. However, due to irregular climatic events, ASA regions are affected commonly by frequent droughts that can impact food production. In addition, ASA regions in the Middle East and Africa are often characterised by political instability, which can increase population vulnerability to hunger and ill health. Remote sensing (RS) provides a platform to improve the spatial prediction of crop production and food availability, with the potential to positively impact populations. This paper, firstly, describes some of the important characteristics of agriculture in ASA regions that require monitoring to improve their management. Secondly, it demonstrates how freely available RS data can support decision-making through a cost-effective monitoring system that complements traditional approaches for collecting agricultural data. Thirdly, it illustrates the challenges of employing freely available RS data for mapping and monitoring crop area, crop status and forecasting crop yield in these regions. Finally, existing approaches used in these applications are evaluated, and the challenges associated with their use and possible future improvements are discussed. We demonstrate that agricultural activities can be monitored effectively and both crop area and crop yield can be predicted in advance using RS data. We also discuss the future challenges associated with maintaining food security in ASA regions and explore some recent advances in RS that can be used to monitor cropland and forecast crop production and yield.
KW  - agriculture
KW  - arid and semi-arid regions
KW  - crop monitoring
KW  - remote sensing
KW  - crop yield
DO  - 10.3390/rs13173382
ER  -
TY  - EJOU
AU  - Ali, Wasiq
AU  - Li, Yaan
AU  - Raja, Muhammad A.
AU  - Khan, Wasim U.
AU  - He, Yigang
TI  - State Estimation of an Underwater Markov Chain Maneuvering Target Using Intelligent Computing
T2  - Entropy

PY  - 2021
VL  - 23
IS  - 9
SN  - 1099-4300

AB  - In this study, an application of deep learning-based neural computing is proposed for efficient real-time state estimation of the Markov chain underwater maneuvering object. The designed intelligent strategy is exploiting the strength of nonlinear autoregressive with an exogenous input (NARX) network model, which has the capability for estimating the dynamics of the systems that follow the discrete-time Markov chain. Nonlinear Bayesian filtering techniques are often applied for underwater maneuvering state estimation applications by following state-space methodology. The robustness and precision of NARX neural network are efficiently investigated for accurate state prediction of the passive Markov chain highly maneuvering underwater target. A continuous coordinated turning trajectory of an underwater maneuvering object is modeled for analyzing the performance of the neural computing paradigm. State estimation modeling is developed in the context of bearings only tracking technology in which the efficiency of the NARX neural network is investigated for ideal and complex ocean environments. Real-time position and velocity of maneuvering object are computed for five different cases by varying standard deviations of white Gaussian measured noise. Sufficient Monte Carlo simulation results validate the competence of NARX neural computing over conventional generalized pseudo-Bayesian filtering algorithms like an interacting multiple model extended Kalman filter and an interacting multiple model unscented Kalman filter.
KW  - neural computing
KW  - state estimation
KW  - Markov chain
KW  - turning trajectory
KW  - bearings only tracking
KW  - maneuvering object
DO  - 10.3390/e23091124
ER  -
TY  - EJOU
AU  - Specht, Mariusz
AU  - Stateczny, Andrzej
AU  - Specht, Cezary
AU  - Widźgowski, Szymon
AU  - Lewicka, Oktawia
AU  - Wiśniewska, Marta
TI  - Concept of an Innovative Autonomous Unmanned System for Bathymetric Monitoring of Shallow Waterbodies (INNOBAT System)
T2  - Energies

PY  - 2021
VL  - 14
IS  - 17
SN  - 1996-1073

AB  - Bathymetry is a subset of hydrography, aimed at measuring the depth of waterbodies and waterways. Measurements are taken inter alia to detect natural obstacles or other navigational obstacles that endanger the safety of navigation, to examine the navigability conditions, anchorages, waterways and other commercial waterbodies, and to determine the parameters of the safe depth of waterbodies in the vicinity of ports, etc. Therefore, it is necessary to produce precise and reliable seabed maps, so that any hazards that may occur, particularly in shallow waterbodies, can be prevented, including the high dynamics of hydromorphological changes. This publication is aimed at developing a concept of an innovative autonomous unmanned system for bathymetric monitoring of shallow waterbodies. A bathymetric and topographic system will use autonomous unmanned aerial and surface vehicles to study the seabed relief in the littoral zone (even at depths of less than 1 m), in line with the requirements set out for the most stringent International Hydrographic Organization (IHO) order—exclusive. Unlike other existing solutions, the INNOBAT system will enable the coverage of the entire surveyed area with measurements, which will allow a comprehensive assessment of the hydrographic and navigation situation in the waterbody to be conducted.
KW  - unmanned surface vehicle (USV)
KW  - unmanned aerial vehicle (UAV)
KW  - bathymetric monitoring system
KW  - shallow waterbody
KW  - hydrography
DO  - 10.3390/en14175370
ER  -
TY  - EJOU
AU  - Aghababaei, Masoumeh
AU  - Ebrahimi, Ataollah
AU  - Naghipour, Ali A.
AU  - Asadi, Esmaeil
AU  - Verrelst, Jochem
TI  - Classification of Plant Ecological Units in Heterogeneous Semi-Steppe Rangelands: Performance Assessment of Four Classification Algorithms
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 17
SN  - 2072-4292

AB  - Plant Ecological Unit’s (PEUs) are the abstraction of vegetation communities that occur on a site which similarly respond to management actions and natural disturbances. Identification and monitoring of PEUs in a heterogeneous landscape is the most difficult task in medium resolution satellite images datasets. The main objective of this study is to compare pixel-based classification versus object-based classification for accurately classifying PEUs with four selected different algorithms across heterogeneous rangelands in Central Zagros, Iran. We used images of Landsat-8 OLI that were pan-sharpened to 15 m to classify four PEU classes based on a random dataset collected in the field (40%). In the first stage, we applied the following classification algorithms to distinguish PEUs: Minimum Distance (MD), Maximum Likelihood Classification (MLC), Neural Network-Multi Layer Perceptron (NN-MLP) and Classification Tree Analysis (CTA) for pixel based method and object based method. Then, by using the most accurate classification approach, in the second stage auxiliary data (Principal Component Analysis (PCA)) was incorporated to improve the accuracy of the PEUs classification process. At the end, test data (60%) were used for accuracy assessment of the resulting maps. Object-based maps clearly outperformed pixel-based maps, especially with CTA, NN-MLP and MD algorithms with overall accuracies of 86%, 72% and 59%, respectively. The MLC algorithm did not reveal any significant difference between the object-based and pixel-based analyses. Finally, complementing PCA auxiliary bands to the CTA algorithms offered the most successful PEUs classification strategy, with the highest overall accuracy (89%). The results clearly underpin the importance of object-based classification with the CTA classifier together with PCA auxiliary data to optimize identification of PEU classes.
KW  - object-based classification
KW  - machine learning algorithms
KW  - principal component analysis
KW  - plant ecological units mapping
DO  - 10.3390/rs13173433
ER  -
TY  - EJOU
AU  - Qi, Yuan
AU  - Dong, Xuhua
AU  - Chen, Pengchao
AU  - Lee, Kyeong-Hwan
AU  - Lan, Yubin
AU  - Lu, Xiaoyang
AU  - Jia, Ruichang
AU  - Deng, Jizhong
AU  - Zhang, Yali
TI  - Canopy Volume Extraction of Citrus reticulate Blanco cv. Shatangju Trees Using UAV Image-Based Point Cloud Deep Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 17
SN  - 2072-4292

AB  - Automatic acquisition of the canopy volume parameters of the Citrus reticulate Blanco cv. Shatangju tree is of great significance to precision management of the orchard. This research combined the point cloud deep learning algorithm with the volume calculation algorithm to segment the canopy of the Citrus reticulate Blanco cv. Shatangju trees. The 3D (Three-Dimensional) point cloud model of a Citrus reticulate Blanco cv. Shatangju orchard was generated using UAV tilt photogrammetry images. The segmentation effects of three deep learning models, PointNet++, MinkowskiNet and FPConv, on Shatangju trees and the ground were compared. The following three volume algorithms: convex hull by slices, voxel-based method and 3D convex hull were applied to calculate the volume of Shatangju trees. Model accuracy was evaluated using the coefficient of determination (R2) and Root Mean Square Error (RMSE). The results show that the overall accuracy of the MinkowskiNet model (94.57%) is higher than the other two models, which indicates the best segmentation effect. The 3D convex hull algorithm received the highest R2 (0.8215) and the lowest RMSE (0.3186 m3) for the canopy volume calculation, which best reflects the real volume of Citrus reticulate Blanco cv. Shatangju trees. The proposed method is capable of rapid and automatic acquisition for the canopy volume of Citrus reticulate Blanco cv. Shatangju trees.
KW  - canopy volume
KW  - UAV tilt photogrammetry
KW  - point cloud
KW  - deep learning
KW  - Citrus reticulate Blanco cv. Shatangju trees
DO  - 10.3390/rs13173437
ER  -
TY  - EJOU
AU  - Grigusova, Paulina
AU  - Larsen, Annegret
AU  - Achilles, Sebastian
AU  - Klug, Alexander
AU  - Fischer, Robin
AU  - Kraus, Diana
AU  - Übernickel, Kirstin
AU  - Paulino, Leandro
AU  - Pliscoff, Patricio
AU  - Brandl, Roland
AU  - Farwig, Nina
AU  - Bendix, Jörg
TI  - Area-Wide Prediction of Vertebrate and Invertebrate Hole Density and Depth across a Climate Gradient in Chile Based on UAV and Machine Learning
T2  - Drones

PY  - 2021
VL  - 5
IS  - 3
SN  - 2504-446X

AB  - Burrowing animals are important ecosystem engineers affecting soil properties, as their burrowing activity leads to the redistribution of nutrients and soil carbon sequestration. The magnitude of these effects depends on the spatial density and depth of such burrows, but a method to derive this type of spatially explicit data is still lacking. In this study, we test the potential of using consumer-oriented UAV RGB imagery to determine the density and depth of holes created by burrowing animals at four study sites along a climate gradient in Chile, by combining UAV data with empirical field plot observations and machine learning techniques. To enhance the limited spectral information in RGB imagery, we derived spatial layers representing vegetation type and height and used landscape textures and diversity to predict hole parameters. Across-site models for hole density generally performed better than those for depth, where the best-performing model was for the invertebrate hole density (R2 = 0.62). The best models at individual study sites were obtained for hole density in the arid climate zone (R2 = 0.75 and 0.68 for invertebrates and vertebrates, respectively). Hole depth models only showed good to fair performance. Regarding predictor importance, the models heavily relied on vegetation height, texture metrics, and diversity indices.
KW  - UAV
KW  - machine learning
KW  - burrowing animals
KW  - climate gradient
KW  - Chile
KW  - vegetation patterns
KW  - heterogeneity
DO  - 10.3390/drones5030086
ER  -
TY  - EJOU
AU  - Liu, Chengqi
AU  - Zhou, Han
AU  - Cao, Jing
AU  - Guo, Xuchao
AU  - Su, Jie
AU  - Wang, Longhe
AU  - Lu, Shuhan
AU  - Li, Lin
TI  - Behavior Trajectory Tracking of Piglets Based on DLC-KPCA
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 9
SN  - 2077-0472

AB  - Tracking the behavior trajectories in pigs in group is becoming increasingly important for welfare feeding. A novel method was proposed in this study to accurately track individual trajectories of pigs in group and analyze their behavior characteristics. First, a multi-pig trajectory tracking model was established based on DeepLabCut (DLC) to realize the daily trajectory tracking of piglets. Second, a high-dimensional spatiotemporal feature model was established based on kernel principal component analysis (KPCA) to achieve nonlinear trajectory optimal clustering. At the same time, the abnormal trajectory correction model was established from five dimensions (semantic, space, angle, time, and velocity) to avoid trajectory loss and drift. Finally, the thermal map of the track distribution was established to analyze the four activity areas of the piggery (resting, drinking, excretion, and feeding areas). Experimental results show that the trajectory tracking accuracy of our method reaches 96.88%, the tracking speed is 350 fps, and the loss value is 0.002. Thus, the method based on DLC–KPCA can meet the requirements of identification of piggery area and tracking of piglets’ behavior. This study is helpful for automatic monitoring of animal behavior and provides data support for breeding.
KW  - piglets
KW  - behavior tracking
KW  - trajectory correction
KW  - DeepLabCut
KW  - KPCA
DO  - 10.3390/agriculture11090843
ER  -
TY  - EJOU
AU  - Roy Choudhury, Malini
AU  - Das, Sumanta
AU  - Christopher, Jack
AU  - Apan, Armando
AU  - Chapman, Scott
AU  - Menzies, Neal W.
AU  - Dang, Yash P.
TI  - Improving Biomass and Grain Yield Prediction of Wheat Genotypes on Sodic Soil Using Integrated High-Resolution Multispectral, Hyperspectral, 3D Point Cloud, and Machine Learning Techniques
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 17
SN  - 2072-4292

AB  - Sodic soils adversely affect crop production over extensive areas of rain-fed cropping worldwide, with particularly large areas in Australia. Crop phenotyping may assist in identifying cultivars tolerant to soil sodicity. However, studies to identify the most appropriate traits and reliable tools to assist crop phenotyping on sodic soil are limited. Hence, this study evaluated the ability of multispectral, hyperspectral, 3D point cloud, and machine learning techniques to improve estimation of biomass and grain yield of wheat genotypes grown on a moderately sodic (MS) and highly sodic (HS) soil sites in northeastern Australia. While a number of studies have reported using different remote sensing approaches and crop traits to quantify crop growth, stress, and yield variation, studies are limited using the combination of these techniques including machine learning to improve estimation of genotypic biomass and yield, especially in constrained sodic soil environments. At close to flowering, unmanned aerial vehicle (UAV) and ground-based proximal sensing was used to obtain remote and/or proximal sensing data, while biomass yield and crop heights were also manually measured in the field. Grain yield was machine-harvested at maturity. UAV remote and/or proximal sensing-derived spectral vegetation indices (VIs), such as normalized difference vegetation index, optimized soil adjusted vegetation index, and enhanced vegetation index and crop height were closely corresponded to wheat genotypic biomass and grain yields. UAV multispectral VIs more closely associated with biomass and grain yields compared to proximal sensing data. The red-green-blue (RGB) 3D point cloud technique was effective in determining crop height, which was slightly better correlated with genotypic biomass and grain yield than ground-measured crop height data. These remote sensing-derived crop traits (VIs and crop height) and wheat biomass and grain yields were further simulated using machine learning algorithms (multitarget linear regression, support vector machine regression, Gaussian process regression, and artificial neural network) with different kernels to improve estimation of biomass and grain yield. The artificial neural network predicted biomass yield (R2 = 0.89; RMSE = 34.8 g/m2 for the MS and R2 = 0.82; RMSE = 26.4 g/m2 for the HS site) and grain yield (R2 = 0.88; RMSE = 11.8 g/m2 for the MS and R2 = 0.74; RMSE = 16.1 g/m2 for the HS site) with slightly less error than the others. Wheat genotypes Mitch, Corack, Mace, Trojan, Lancer, and Bremer were identified as more tolerant to sodic soil constraints than Emu Rock, Janz, Flanker, and Gladius. The study improves our ability to select appropriate traits and techniques in accurate estimation of wheat genotypic biomass and grain yields on sodic soils. This will also assist farmers in identifying cultivars tolerant to sodic soil constraints.
KW  - phenotyping
KW  - vegetation indices
KW  - crop height
KW  - machine learning
KW  - biomass and grain yields
KW  - sodic soil
DO  - 10.3390/rs13173482
ER  -
TY  - EJOU
AU  - Zhang, Zhen
AU  - Wang, Leilei
AU  - Xue, Naiting
AU  - Du, Zhiheng
TI  - Spatiotemporal Analysis of Active Fires in the Arctic Region during 2001–2019 and a Fire Risk Assessment Model
T2  - Fire

PY  - 2021
VL  - 4
IS  - 3
SN  - 2571-6255

AB  - The increasing frequency of active fires worldwide has caused significant impacts on terrestrial, aquatic, and atmospheric systems. Polar regions have received little attention due to their sparse populations, but active fires in the Arctic cause carbon losses from peatlands, which affects the global climate system. Therefore, it is necessary to focus on the spatiotemporal variations in active fires in the Arctic and to assess the fire risk. We used MODIS C6 data from 2001 to 2019 and VIIRS V1 data from 2012 to 2019 to analyse the spatiotemporal characteristics of active fires and establish a fire risk assessment model based on logistic regression. The trends in active fire frequency based on MODIS C6 and VIIRS V1 data are consistent. Throughout the Arctic, the fire frequency appears to be fluctuating and overall increasing. Fire occurrence has obvious seasonality, being concentrated in summer (June–August) and highest in July, when lightning is most frequent. The frequency of active fires is related to multiple factors, such as vegetation type, NDVI, elevation, slope, air temperature, precipitation, wind speed, and distances from roads and settlements. A risk assessment model was constructed based on logistic regression and found to be accurate. The results are helpful in understanding the risk of fires in the Arctic under climate change and provide a scientific basis for fire prediction and control and for reducing fire-related carbon emissions.
KW  - active fires
KW  - fire risk assessment
KW  - arctic
KW  - MODIS
KW  - VIIRS
DO  - 10.3390/fire4030057
ER  -
TY  - EJOU
AU  - Vrochidou, Eleni
AU  - Bazinas, Christos
AU  - Manios, Michail
AU  - Papakostas, George A.
AU  - Pachidis, Theodore P.
AU  - Kaburlasos, Vassilis G.
TI  - Machine Vision for Ripeness Estimation in Viticulture Automation
T2  - Horticulturae

PY  - 2021
VL  - 7
IS  - 9
SN  - 2311-7524

AB  - Ripeness estimation of fruits and vegetables is a key factor for the optimization of field management and the harvesting of the desired product quality. Typical ripeness estimation involves multiple manual samplings before harvest followed by chemical analyses. Machine vision has paved the way for agricultural automation by introducing quicker, cost-effective, and non-destructive methods. This work comprehensively surveys the most recent applications of machine vision techniques for ripeness estimation. Due to the broad area of machine vision applications in agriculture, this review is limited only to the most recent techniques related to grapes. The aim of this work is to provide an overview of the state-of-the-art algorithms by covering a wide range of applications. The potential of current machine vision techniques for specific viticulture applications is also analyzed. Problems, limitations of each technique, and future trends are discussed. Moreover, the integration of machine vision algorithms in grape harvesting robots for real-time in-field maturity assessment is additionally examined.
KW  - machine vision
KW  - grape ripeness estimation
KW  - image analysis
KW  - precision agriculture
KW  - agrobots
KW  - harvesting robot
DO  - 10.3390/horticulturae7090282
ER  -
TY  - EJOU
AU  - Typiak, Rafał
AU  - Rykała, Łukasz
AU  - Typiak, Andrzej
TI  - Configuring a UWB Based Location System for a UGV Operating in a Follow-Me Scenario
T2  - Energies

PY  - 2021
VL  - 14
IS  - 17
SN  - 1996-1073

AB  - Unmanned Ground Vehicles (UGV) are devices capable of performing basic working movements without the operator being in their immediate working environment. Their capabilities include but are not limited to the perception of the environment with the use of sensors, determining the platform’s position, and planning and executing its movement. Ultra Wideband (UWB) is one of the wireless communication technologies which is increasingly used in location systems. This article presents the use of UWB technology in developing a guide localization system for a UGV (one of the stages of implementing a follow-me system). The article describes tests carried out on the developed testbed. Their aim was to determine the hardware configuration of the anchor arrangement characterized by the minimum number of lost data packets during operation. In order to determine the influence of the analysed variables on the output values, the method of global sensitivity analysis for neural networks was used.
KW  - Ultra Wideband
KW  - follow-me
KW  - Unmanned Ground Vehicles
KW  - global sensitivity analysis
KW  - neural networks
DO  - 10.3390/en14175517
ER  -
TY  - EJOU
AU  - Fuentes, Sigfredo
AU  - Tongson, Eden
AU  - Unnithan, Ranjith R.
AU  - Gonzalez Viejo, Claudia
TI  - Early Detection of Aphid Infestation and Insect-Plant Interaction Assessment in Wheat Using a Low-Cost Electronic Nose (E-Nose), Near-Infrared Spectroscopy and Machine Learning Modeling
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 17
SN  - 1424-8220

AB  - Advances in early insect detection have been reported using digital technologies through camera systems, sensor networks, and remote sensing coupled with machine learning (ML) modeling. However, up to date, there is no cost-effective system to monitor insect presence accurately and insect-plant interactions. This paper presents results on the implementation of near-infrared spectroscopy (NIR) and a low-cost electronic nose (e-nose) coupled with machine learning. Several artificial neural network (ANN) models were developed based on classification to detect the level of infestation and regression to predict insect numbers for both e-nose and NIR inputs, and plant physiological response based on e-nose to predict photosynthesis rate (A), transpiration (E) and stomatal conductance (gs). Results showed high accuracy for classification models ranging within 96.5–99.3% for NIR and between 94.2–99.2% using e-nose data as inputs. For regression models, high correlation coefficients were obtained for physiological parameters (gs, E and A) using e-nose data from all samples as inputs (R = 0.86) and R = 0.94 considering only control plants (no insect presence). Finally, R = 0.97 for NIR and R = 0.99 for e-nose data as inputs were obtained to predict number of insects. Performances for all models developed showed no signs of overfitting. In this paper, a field-based system using unmanned aerial vehicles with the e-nose as payload was proposed and described for deployment of ML models to aid growers in pest management practices.
KW  - remote sensing
KW  - volatile compounds
KW  - artificial neural networks
KW  - photosynthesis modeling
KW  - plant water status modeling
DO  - 10.3390/s21175948
ER  -
TY  - EJOU
AU  - Ma, Lei
AU  - Zhu, Xiaoxiang
AU  - Qiu, Chunping
AU  - Blaschke, Thomas
AU  - Li, Manchun
TI  - Advances of Local Climate Zone Mapping and Its Practice Using Object-Based Image Analysis
T2  - Atmosphere

PY  - 2021
VL  - 12
IS  - 9
SN  - 2073-4433

AB  - In the context of climate change and urban heat islands, the concept of local climate zones (LCZ) aims for consistent and comparable mapping of urban surface structure and cover across cities. This study provides a timely survey of remote sensing-based applications of LCZ mapping considering the recent increase in publications. We analyze and evaluate several aspects that affect the performance of LCZ mapping, including mapping units/scale, transferability, sample dataset, low accuracy, and classification schemes. Since current LCZ analysis and mapping are based on per-pixel approaches, this study implements an object-based image analysis (OBIA) method and tests it for two cities in Germany using Sentinel 2 data. A comparison with a per-pixel method yields promising results. This study shall serve as a blueprint for future object-based remotely sensed LCZ mapping approaches.
KW  - local climate zones
KW  - remote sensing
KW  - mapping unit
KW  - transferability
KW  - object-based image analysis
DO  - 10.3390/atmos12091146
ER  -
TY  - EJOU
AU  - Du, Chunyu
AU  - Fan, Wenyi
AU  - Ma, Ye
AU  - Jin, Hung-Il
AU  - Zhen, Zhen
TI  - The Effect of Synergistic Approaches of Features and Ensemble Learning Algorithms on Aboveground Biomass Estimation of Natural Secondary Forests Based on ALS and Landsat 8
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 17
SN  - 1424-8220

AB  - Although the combination of Airborne Laser Scanning (ALS) data and optical imagery and machine learning algorithms were proved to improve the estimation of aboveground biomass (AGB), the synergistic approaches of different data and ensemble learning algorithms have not been fully investigated, especially for natural secondary forests (NSFs) with complex structures. This study aimed to explore the effects of the two factors on AGB estimation of NSFs based on ALS data and Landsat 8 imagery. The synergistic method of extracting novel features (i.e., COLI1 and COLI2) using optimal Landsat 8 features and the best-performing ALS feature (i.e., elevation mean) yielded higher accuracy of AGB estimation than either optical-only or ALS-only features. However, both of them failed to improve the accuracy compared to the simple combination of the untransformed features that generated them. The convolutional neural networks (CNN) model was much superior to other classic machine learning algorithms no matter of features. The stacked generalization (SG) algorithms, a kind of ensemble learning algorithms, greatly improved the accuracies compared to the corresponding base model, and the SG with the CNN meta-model performed best. This study provides technical support for a wall-to-wall AGB mapping of NSFs of northeastern China using efficient features and algorithms.
KW  - ensemble learning
KW  - machine learning
KW  - feature extraction
KW  - AGB
KW  - NSFs
DO  - 10.3390/s21175974
ER  -
TY  - EJOU
AU  - Koeva, Mila
AU  - Gasuku, Oscar
AU  - Lengoiboni, Monica
AU  - Asiama, Kwabena
AU  - Bennett, Rohan M.
AU  - Potel, Jossam
AU  - Zevenbergen, Jaap
TI  - Remote Sensing for Property Valuation: A Data Source Comparison in Support of Fair Land Taxation in Rwanda
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 18
SN  - 2072-4292

AB  - Remotely sensed data is increasingly applied across many domains, including fit-for-purpose land administration (FFPLA), where the focus is on fast, affordable, and accurate property information collection. Property valuation, as one of the main functions of land administration systems, is influenced by locational, physical, legal, and economic factors. Despite the importance of property valuation to economic development, there are often no standardized rules or strict data requirements for property valuation for taxation in developing contexts, such as Rwanda. This study aims at assessing different remote sensing data in support of developing a new approach for property valuation for taxation in Rwanda; one that aligns with the FFPLA philosophy. Three different remote sensing technologies, (i) aerial images acquired with a digital camera, (ii) WorldView2 satellite images, and (iii) unmanned aerial vehicle (UAV) images obtained with a DJI Phantom 2 Vision Plus quadcopter, are compared and analyzed in terms of their fitness to fulfil the requirements for valuation for taxation purposes. Quantitative and qualitative methods are applied for the comparative analysis. Prior to the field visit, the fundamental concepts of property valuation for taxation and remote sensing were reviewed. In the field, reference data using high precision GNSS (Leica) was collected and used for quantitative assessment. Primary data was further collected via semi-structured interviews and focus group discussions. The results show that UAVs have the highest potential for collecting data to support property valuation for taxation. The main reasons are the prime need for accurate-enough and up-to-date information. The comparison of the different remote sensing techniques and the provided new approach can support land valuers and professionals in the field in bottom-up activities following the FFPLA principles and maintaining the temporal quality of data needed for fair taxation.
KW  - property valuation
KW  - property taxation
KW  - remote sensing
KW  - land
KW  - UAV
DO  - 10.3390/rs13183563
ER  -
TY  - EJOU
AU  - Roslim, Muhammad H.
AU  - Juraimi, Abdul S.
AU  - Che’Ya, Nik N.
AU  - Sulaiman, Nursyazyla
AU  - Manaf, Muhammad N.
AU  - Ramli, Zaid
AU  - Motmainna, Mst.
TI  - Using Remote Sensing and an Unmanned Aerial System for Weed Management in Agricultural Crops: A Review
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 9
SN  - 2073-4395

AB  - Weeds are unwanted plants that can reduce crop yields by competing for water, nutrients, light, space, and carbon dioxide, which need to be controlled to meet future food production requirements. The integration of drones, artificial intelligence, and various sensors, which include hyperspectral, multi-spectral, and RGB (red-green-blue), ensure the possibility of a better outcome in managing weed problems. Most of the major or minor challenges caused by weed infestation can be faced by implementing remote sensing systems in various agricultural tasks. It is a multi-disciplinary science that includes spectroscopy, optics, computer, photography, satellite launching, electronics, communication, and several other fields. Future challenges, including food security, sustainability, supply and demand, climate change, and herbicide resistance, can also be overcome by those technologies based on machine learning approaches. This review provides an overview of the potential and practical use of unmanned aerial vehicle and remote sensing techniques in weed management practices and discusses how they overcome future challenges.
KW  - weeds
KW  - artificial intelligence
KW  - hyperspectral
KW  - multi-spectral
KW  - weeds management
DO  - 10.3390/agronomy11091809
ER  -
TY  - EJOU
AU  - Xia, Lang
AU  - Zhang, Ruirui
AU  - Chen, Liping
AU  - Li, Longlong
AU  - Yi, Tongchuan
AU  - Wen, Yao
AU  - Ding, Chenchen
AU  - Xie, Chunchun
TI  - Evaluation of Deep Learning Segmentation Models for Detection of Pine Wilt Disease in Unmanned Aerial Vehicle Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 18
SN  - 2072-4292

AB  - Pine wilt disease (PWD) is a serious threat to pine forests. Combining unmanned aerial vehicle (UAV) images and deep learning (DL) techniques to identify infected pines is the most efficient method to determine the potential spread of PWD over a large area. In particular, image segmentation using DL obtains the detailed shape and size of infected pines to assess the disease’s degree of damage. However, the performance of such segmentation models has not been thoroughly studied. We used a fixed-wing UAV to collect images from a pine forest in Laoshan, Qingdao, China, and conducted a ground survey to collect samples of infected pines and construct prior knowledge to interpret the images. Then, training and test sets were annotated on selected images, and we obtained 2352 samples of infected pines annotated over different backgrounds. Finally, high-performance DL models (e.g., fully convolutional networks for semantic segmentation, DeepLabv3+, and PSPNet) were trained and evaluated. The results demonstrated that focal loss provided a higher accuracy and a finer boundary than Dice loss, with the average intersection over union (IoU) for all models increasing from 0.656 to 0.701. From the evaluated models, DeepLLabv3+ achieved the highest IoU and an F1 score of 0.720 and 0.832, respectively. Also, an atrous spatial pyramid pooling module encoded multiscale context information, and the encoder–decoder architecture recovered location/spatial information, being the best architecture for segmenting trees infected by the PWD. Furthermore, segmentation accuracy did not improve as the depth of the backbone network increased, and neither ResNet34 nor ResNet50 was the appropriate backbone for most segmentation models.
KW  - deep learning
KW  - image segmentation
KW  - pine wilt disease
KW  - infected pine DeepLabv3+
KW  - focal loss
DO  - 10.3390/rs13183594
ER  -
TY  - EJOU
AU  - Zdziebko, Paweł
AU  - Holak, Krzysztof
TI  - Synthetic Image Generation Using the Finite Element Method and Blender Graphics Program for Modeling of Vision-Based Measurement Systems
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 18
SN  - 1424-8220

AB  - Computer vision is a frequently used approach in static and dynamic measurements of various mechanical structures. Sometimes, however, conducting a large number of experiments is time-consuming and may require significant financial and human resources. On the contrary, the authors propose a simulation approach for performing experiments to synthetically generate vision data. Synthetic images of mechanical structures subjected to loads are generated in the following way. The finite element method is adopted to compute deformations of the studied structure, and next, the Blender graphics program is used to render images presenting that structure. As a result of the proposed approach, it is possible to obtain synthetic images that reliably reflect static and dynamic experiments. This paper presents the results of the application of the proposed approach in the analysis of a complex-shaped structure for which experimental validation was carried out. In addition, the second example of the process of 3D reconstruction of the examined structure (in a multicamera system) is provided. The results for the structure with damage (cantilever beam) are also presented. The obtained results allow concluding that the proposed approach reliably imitates the images captured during real experiments. In addition, the method can become a tool supporting the vision system configuration process before conducting final experimental research.
KW  - image-based measurement
KW  - vision sensor modeling
KW  - vision system simulation
KW  - image-based reconstruction
KW  - finite element method
KW  - physics-based computer graphics
DO  - 10.3390/s21186046
ER  -
TY  - EJOU
AU  - Lytridis, Chris
AU  - Kaburlasos, Vassilis G.
AU  - Pachidis, Theodore
AU  - Manios, Michalis
AU  - Vrochidou, Eleni
AU  - Kalampokas, Theofanis
AU  - Chatzistamatis, Stamatis
TI  - An Overview of Cooperative Robotics in Agriculture
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 9
SN  - 2073-4395

AB  - Agricultural robotics has been a popular subject in recent years from an academic as well as a commercial point of view. This is because agricultural robotics addresses critical issues such as seasonal shortages in manual labor, e.g., during harvest, as well as the increasing concern regarding environmentally friendly practices. On one hand, several individual agricultural robots have already been developed for specific tasks (e.g., for monitoring, spraying, harvesting, transport, etc.) with varying degrees of effectiveness. On the other hand, the use of cooperative teams of agricultural robots in farming tasks is not as widespread; yet, it is an emerging trend. This paper presents a comprehensive overview of the work carried out so far in the area of cooperative agricultural robotics and identifies the state-of-the-art. This paper also outlines challenges to be addressed in fully automating agricultural production; the latter is promising for sustaining an increasingly vast human population, especially in cases of pandemics such as the recent COVID-19 pandemic.
KW  - agricultural robots
KW  - agriculture 4.0/5.0
KW  - cooperative robots
KW  - farming automation
DO  - 10.3390/agronomy11091818
ER  -
TY  - EJOU
AU  - Wada, Daichi
AU  - Araujo-Estrada, Sergio A.
AU  - Windsor, Shane
TI  - Unmanned Aerial Vehicle Pitch Control under Delay Using Deep Reinforcement Learning with Continuous Action in Wind Tunnel Test
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 9
SN  - 2226-4310

AB  - Nonlinear flight controllers for fixed-wing unmanned aerial vehicles (UAVs) can potentially be developed using deep reinforcement learning. However, there is often a reality gap between the simulation models used to train these controllers and the real world. This study experimentally investigated the application of deep reinforcement learning to the pitch control of a UAV in wind tunnel tests, with a particular focus of investigating the effect of time delays on flight controller performance. Multiple neural networks were trained in simulation with different assumed time delays and then wind tunnel tested. The neural networks trained with shorter delays tended to be susceptible to delay in the real tests and produce fluctuating behaviour. The neural networks trained with longer delays behaved more conservatively and did not produce oscillations but suffered steady state errors under some conditions due to unmodeled frictional effects. These results highlight the importance of performing physical experiments to validate controller performance and how the training approach used with reinforcement learning needs to be robust to reality gaps between simulation and the real world.
KW  - attitude control
KW  - deep reinforcement learning
KW  - fixed-wing aircraft
KW  - unmanned aerial vehicle
KW  - wind tunnel test
DO  - 10.3390/aerospace8090258
ER  -
TY  - EJOU
AU  - Fourlas, George K.
AU  - Karras, George C.
TI  - A Survey on Fault Diagnosis and Fault-Tolerant Control Methods for Unmanned Aerial Vehicles
T2  - Machines

PY  - 2021
VL  - 9
IS  - 9
SN  - 2075-1702

AB  - The continuous evolution of modern technology has led to the creation of increasingly complex and advanced systems. This has been also reflected in the technology of Unmanned Aerial Vehicles (UAVs), where the growing demand for more reliable performance necessitates the development of sophisticated techniques that provide fault diagnosis and fault tolerance in a timely and accurate manner. Typically, a UAV consists of three types of subsystems: actuators, main structure and sensors. Therefore, a fault-monitoring system must be specifically designed to supervise and debug each of these subsystems, so that any faults can be addressed before they lead to disastrous consequences. In this survey article, we provide a detailed overview of recent advances and studies regarding fault diagnosis, Fault-Tolerant Control (FTC) and anomaly detection for UAVs. Concerning fault diagnosis, our interest is mainly focused on sensors and actuators, as these subsystems are mostly prone to faults, while their healthy operation usually ensures the smooth and reliable performance of the aerial vehicle.
KW  - fault diagnosis
KW  - fault tolerant control
KW  - anomaly detection
KW  - unmanned aerial vehicles
DO  - 10.3390/machines9090197
ER  -
TY  - EJOU
AU  - Liu, Wenyao
AU  - Meng, Qingfeng
AU  - Li, Zhen
AU  - Hu, Xin
TI  - Applications of Computer Vision in Monitoring the Unsafe Behavior of Construction Workers: Current Status and Challenges
T2  - Buildings

PY  - 2021
VL  - 11
IS  - 9
SN  - 2075-5309

AB  - The unsafe behavior of construction workers is one of the main causes of safety accidents at construction sites. To reduce the incidence of construction accidents and improve the safety performance of construction projects, there is a need to identify risky factors by monitoring the behavior of construction workers. Computer vision (CV) technology, which is a powerful and automated tool used for extracting images and video information from construction sites, has been recognized and adopted as an effective construction site monitoring technology for the identification of risky factors resulting from the unsafe behavior of construction workers. In this article, we introduce the research background of this field and conduct a systematic statistical analysis of the relevant literature in this field through the bibliometric analysis method. Thereafter, we adopt a content-based analysis method to depict the historical explorations in the field. On this basis, the limitations and challenges in this field are identified, and future research directions are proposed. It is found that CV technology can effectively monitor the unsafe behaviors of construction workers. The research findings can enhance people’s understanding of construction safety management.
KW  - computer vision
KW  - construction workers
KW  - monitoring
KW  - unsafe behavior
KW  - literature review
DO  - 10.3390/buildings11090409
ER  -
TY  - EJOU
AU  - Liu, Shenzhou
AU  - Zeng, Wenzhi
AU  - Wu, Lifeng
AU  - Lei, Guoqing
AU  - Chen, Haorui
AU  - Gaiser, Thomas
AU  - Srivastava, Amit K.
TI  - Simulating the Leaf Area Index of Rice from Multispectral Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 18
SN  - 2072-4292

AB  - Accurate estimation of the leaf area index (LAI) is essential for crop growth simulations and agricultural management. This study conducted a field experiment with rice and measured the LAI in different rice growth periods. The multispectral bands (B) including red edge (RE, 730 nm ± 16 nm), near-infrared (NIR, 840 nm ± 26 nm), green (560 nm ± 16 nm), red (650 nm ± 16 nm), blue (450 nm ± 16 nm), and visible light (RGB) were also obtained by an unmanned aerial vehicle (UAV) with multispectral sensors (DJI-P4M, SZ DJI Technology Co., Ltd.). Based on the bands, five vegetation indexes (VI) including Green Normalized Difference Vegetation Index (GNDVI), Leaf Chlorophyll Index (LCI), Normalized Difference Red Edge Index (NDRE), Normalized Difference Vegetation Index (NDVI), and Optimization Soil-Adjusted Vegetation Index (OSAVI) were calculated. The semi-empirical model (SEM), the random forest model (RF), and the Extreme Gradient Boosting model (XGBoost) were used to estimate rice LAI based on multispectral bands, VIs, and their combinations, respectively. The results indicated that the GNDVI had the highest accuracy in the SEM (R2 = 0.78, RMSE = 0.77). For the single band, NIR had the highest accuracy in both RF (R2 = 0.73, RMSE = 0.98) and XGBoost (R2 = 0.77, RMSE = 0.88). Band combination of NIR + red improved the estimation accuracy in both RF (R2 = 0.87, RMSE = 0.65) and XGBoost (R2 = 0.88, RMSE = 0.63). NDRE and LCI were the first two single VIs for LAI estimation using both RF and XGBoost. However, putting more than one VI together could only increase the LAI estimation accuracy slightly. Meanwhile, the bands + VIs combinations could improve the accuracy in both RF and XGBoost. Our study recommended estimating rice LAI by a combination of red + NIR + OSAVI + NDVI + GNDVI + LCI + NDRE (2B + 5V) with XGBoost to obtain high accuracy and overcome the potential over-fitting issue (R2 = 0.91, RMSE = 0.54).
KW  - leaf area index (LAI)
KW  - rice
KW  - multispectral images
KW  - random forest (RF)
KW  - Extreme Gradient Boosting model (XGBoost)
DO  - 10.3390/rs13183663
ER  -
TY  - EJOU
AU  - Li, Wangbin
AU  - Sun, Kaimin
AU  - Du, Zhuotong
AU  - Hu, Xiuqing
AU  - Li, Wenzhuo
AU  - Wei, Jinjiang
AU  - Gao, Song
TI  - PCNet: Cloud Detection in FY-3D True-Color Imagery Using Multi-Scale Pyramid Contextual Information
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 18
SN  - 2072-4292

AB  - Cloud, one of the poor atmospheric conditions, significantly reduces the usability of optical remote-sensing data and hampers follow-up applications. Thus, the identification of cloud remains a priority for various remote-sensing activities, such as product retrieval, land-use/cover classification, object detection, and especially for change detection. However, the complexity of clouds themselves make it difficult to detect thin clouds and small isolated clouds. To accurately detect clouds in satellite imagery, we propose a novel neural network named the Pyramid Contextual Network (PCNet). Considering the limited applicability of a regular convolution kernel, we employed a Dilated Residual Block (DRB) to extend the receptive field of the network, which contains a dilated convolution and residual connection. To improve the detection ability for thin clouds, the proposed new model, pyramid contextual block (PCB), was used to generate global information at different scales. FengYun-3D MERSI-II remote-sensing images covering China with 14,165 × 24,659 pixels, acquired on 17 July 2019, are processed to conduct cloud-detection experiments. Experimental results show that the overall precision rates of the trained network reach 97.1% and the overall recall rates reach 93.2%, which performs better both in quantity and quality than U-Net, UNet++, UNet3+, PSPNet and DeepLabV3+.
KW  - cloud detection
KW  - FY-3D remote-sensing images
KW  - pyramid contextual
KW  - deep learning
DO  - 10.3390/rs13183670
ER  -
TY  - EJOU
AU  - Daranagama, Samitha
AU  - Witayangkurn, Apichon
TI  - Automatic Building Detection with Polygonizing and Attribute Extraction from High-Resolution Images
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 9
SN  - 2220-9964

AB  - Buildings can be introduced as a fundamental element for forming a city. Therefore, up-to-date building maps have become vital for many applications, including urban mapping and urban expansion analysis. With the development of deep learning, segmenting building footprints from high-resolution remote sensing imagery has become a subject of intense study. Here, a modified version of the U-Net architecture with a combination of pre- and post-processing techniques was developed to extract building footprints from high-resolution aerial imagery and unmanned aerial vehicle (UAV) imagery. Data pre-processing with the logarithmic correction image enhancing algorithm showed the most significant improvement in the building detection accuracy for aerial images; meanwhile, the CLAHE algorithm improved the most concerning UAV images. This study developed a post-processing technique using polygonizing and polygon smoothing called the Douglas–Peucker algorithm, which made the building output directly ready to use for different applications. The attribute information, land use data, and population count data were applied using two open datasets. In addition, the building area and perimeter of each building were calculated as geometric attributes.
KW  - deep learning
KW  - building extraction
KW  - UAV images
KW  - aerial images
KW  - semantic segmentation
KW  - transfer learning
KW  - polygonizing
KW  - polygon smoothing
KW  - attribute extraction
DO  - 10.3390/ijgi10090606
ER  -
TY  - EJOU
AU  - Lee, Donghee
AU  - Park, Wooryong
AU  - Nam, Woochul
TI  - Autonomous Landing of Micro Unmanned Aerial Vehicles with Landing-Assistive Platform and Robust Spherical Object Detection
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 18
SN  - 2076-3417

AB  - Autonomous unmanned aerial vehicle (UAV) landing can be useful in multiple applications. Precise landing is a difficult task because of the significant navigation errors of the global positioning system (GPS). To overcome these errors and to realize precise landing control, various sensors have been installed on UAVs. However, this approach can be challenging for micro UAVs (MAVs) because strong thrust forces are required to carry multiple sensors. In this study, a new autonomous MAV landing system is proposed, in which a landing platform actively assists vehicle landing. In addition to the vision system of the UAV, a camera was installed on the platform to precisely control the MAV near the landing area. The platform was also designed with various types of equipment to assist the MAV in searching, approaching, alignment, and landing. Furthermore, a novel algorithm was developed for robust spherical object detection under different illumination conditions. To validate the proposed landing system and detection algorithm, 80 flight experiments were conducted using a DJI TELLO drone, which successfully landed on the platform in every trial with a small landing position average error of 2.7 cm.
KW  - micro unmanned aerial vehicle
KW  - autonomous landing
KW  - landing-assistive platform
KW  - spherical object detection
DO  - 10.3390/app11188555
ER  -
TY  - EJOU
AU  - Krause, Johannes R.
AU  - Hinojosa-Corona, Alejandro
AU  - Gray, Andrew B.
AU  - Burke Watson, Elizabeth
TI  - Emerging Sensor Platforms Allow for Seagrass Extent Mapping in a Turbid Estuary and from the Meadow to Ecosystem Scale
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 18
SN  - 2072-4292

AB  - Seagrass meadows are globally important habitats, protecting shorelines, providing nursery areas for fish, and sequestering carbon. However, both anthropogenic and natural environmental stressors have led to a worldwide reduction seagrass habitats. For purposes of management and restoration, it is essential to produce accurate maps of seagrass meadows over a variety of spatial scales, resolutions, and at temporal frequencies ranging from months to years. Satellite remote sensing has been successfully employed to produce maps of seagrass in the past, but turbid waters and difficulty in obtaining low-tide scenes pose persistent challenges. This study builds on an increased availability of affordable high temporal frequency imaging platforms, using seasonal unmanned aerial vehicle (UAV) surveys of seagrass extent at the meadow scale, to inform machine learning classifications of satellite imagery of a 40 km2 bay. We find that object-based image analysis is suitable to detect seasonal trends in seagrass extent from UAV imagery and find that trends vary between individual meadows at our study site Bahía de San Quintín, Baja California, México, during our study period in 2019. We further suggest that compositing multiple satellite imagery classifications into a seagrass probability map allows for an estimation of seagrass extent in turbid waters and report that in 2019, seagrass covered 2324 ha of Bahía de San Quintín, indicating a recovery from losses reported for previous decades.
KW  - seagrass
KW  - unmanned aerial vehicle (UAV)
KW  - object-based image analysis
KW  - planet
KW  - machine learning
KW  - turbid
KW  - estuary
DO  - 10.3390/rs13183681
ER  -
TY  - EJOU
AU  - Espinoza-Fraire, Tadeo
AU  - Saenz, Armando
AU  - Salas, Francisco
AU  - Juarez, Raymundo
AU  - Giernacki, Wojciech
TI  - Trajectory Tracking with Adaptive Robust Control for Quadrotor
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 18
SN  - 2076-3417

AB  - This work proposes three robust mechanisms based on the MIT rule and the sliding-mode techniques. These robust mechanisms have to tune the gains of an adaptive Proportional-Derivative controller to steer a quadrotor in a predefined trajectory. The adaptive structure is a model reference adaptive control (MRAC). The robust mechanisms proposed to achieve the control objective (trajectory tracking) are MIT rule, MIT rule with sliding mode (MIT-SM), MIT rule with twisting (MIT-Twisting), and MIT rule with high order sliding mode (MIT-HOSM).
KW  - adaptive control
KW  - MIT rule
KW  - sliding mode
KW  - trajectory following
DO  - 10.3390/app11188571
ER  -
TY  - EJOU
AU  - Abdollahi, Abolfazl
AU  - Pradhan, Biswajeet
AU  - Shukla, Nagesh
AU  - Chakraborty, Subrata
AU  - Alamri, Abdullah
TI  - Multi-Object Segmentation in Complex Urban Scenes from High-Resolution Remote Sensing Data
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 18
SN  - 2072-4292

AB  - Terrestrial features extraction, such as roads and buildings from aerial images using an automatic system, has many usages in an extensive range of fields, including disaster management, change detection, land cover assessment, and urban planning. This task is commonly tough because of complex scenes, such as urban scenes, where buildings and road objects are surrounded by shadows, vehicles, trees, etc., which appear in heterogeneous forms with lower inter-class and higher intra-class contrasts. Moreover, such extraction is time-consuming and expensive to perform by human specialists manually. Deep convolutional models have displayed considerable performance for feature segmentation from remote sensing data in the recent years. However, for the large and continuous area of obstructions, most of these techniques still cannot detect road and building well. Hence, this work’s principal goal is to introduce two novel deep convolutional models based on UNet family for multi-object segmentation, such as roads and buildings from aerial imagery. We focused on buildings and road networks because these objects constitute a huge part of the urban areas. The presented models are called multi-level context gating UNet (MCG-UNet) and bi-directional ConvLSTM UNet model (BCL-UNet). The proposed methods have the same advantages as the UNet model, the mechanism of densely connected convolutions, bi-directional ConvLSTM, and squeeze and excitation module to produce the segmentation maps with a high resolution and maintain the boundary information even under complicated backgrounds. Additionally, we implemented a basic efficient loss function called boundary-aware loss (BAL) that allowed a network to concentrate on hard semantic segmentation regions, such as overlapping areas, small objects, sophisticated objects, and boundaries of objects, and produce high-quality segmentation maps. The presented networks were tested on the Massachusetts building and road datasets. The MCG-UNet improved the average F1 accuracy by 1.85%, and 1.19% and 6.67% and 5.11% compared with UNet and BCL-UNet for road and building extraction, respectively. Additionally, the presented MCG-UNet and BCL-UNet networks were compared with other state-of-the-art deep learning-based networks, and the results proved the superiority of the networks in multi-object segmentation tasks.
KW  - building extraction
KW  - boundary-aware loss
KW  - deep learning
KW  - remote sensing
KW  - road extraction
DO  - 10.3390/rs13183710
ER  -
TY  - EJOU
AU  - Jamali, Ali
AU  - Mahdianpari, Masoud
TI  - A Cloud-Based Framework for Large-Scale Monitoring of Ocean Plastics Using Multi-Spectral Satellite Imagery and Generative Adversarial Network
T2  - Water

PY  - 2021
VL  - 13
IS  - 18
SN  - 2073-4441

AB  - Marine debris is considered a threat to the inhabitants, as well as the marine environments. Accumulation of marine debris, besides climate change factors, including warming water, sea-level rise, and changes in oceans’ chemistry, are causing the potential collapse of the marine environment’s health. Due to the increase of marine debris, including plastics in coastlines, ocean and sea surfaces, and even in deep ocean layers, there is a need for developing new advanced technology for the detection of large-sized marine pollution (with sizes larger than 1 m) using state-of-the-art remote sensing and machine learning tools. Therefore, we developed a cloud-based framework for large-scale marine pollution detection with the integration of Sentinel-2 satellite imagery and advanced machine learning tools on the Sentinel Hub cloud application programming interface (API). Moreover, we evaluated the performance of two shallow machine learning algorithms of random forest (RF) and support vector machine (SVM), as well as the deep learning method of the generative adversarial network-random forest (GAN-RF) for the detection of ocean plastics in the pilot site of Mytilene Island, Greece. Based on the obtained results, the shallow algorithms of RF and SVM achieved an overall accuracy of 88% and 84%, respectively, with available training data of plastic debris. The GAN-RF classifier improved the detection of ocean plastics of the RF method by 8%, achieving an overall accuracy of 96% by generating several synthetic ocean plastic samples.
KW  - ocean plastics
KW  - support vector machine
KW  - random forest
KW  - marine debris
KW  - marine pollution
KW  - Sentinel Hub
KW  - generative adversarial network
DO  - 10.3390/w13182553
ER  -
TY  - EJOU
AU  - Richardson, Galen
AU  - Leblanc, Sylvain G.
AU  - Lovitt, Julie
AU  - Rajaratnam, Krishan
AU  - Chen, Wenjun
TI  - Leveraging AI to Estimate Caribou Lichen in UAV Orthomosaics from Ground Photo Datasets
T2  - Drones

PY  - 2021
VL  - 5
IS  - 3
SN  - 2504-446X

AB  - Relating ground photographs to UAV orthomosaics is a key linkage required for accurate multi-scaled lichen mapping. Conventional methods of multi-scaled lichen mapping, such as random forest models and convolutional neural networks, heavily rely on pixel DN values for classification. However, the limited spectral range of ground photos requires additional characteristics to differentiate lichen from spectrally similar objects, such as bright logs. By applying a neural network to tiles of a UAV orthomosaics, additional characteristics, such as surface texture and spatial patterns, can be used for inferences. Our methodology used a neural network (UAV LiCNN) trained on ground photo mosaics to predict lichen in UAV orthomosaic tiles. The UAV LiCNN achieved mean user and producer accuracies of 85.84% and 92.93%, respectively, in the high lichen class across eight different orthomosaics. We compared the known lichen percentages found in 77 vegetation microplots with the predicted lichen percentage calculated from the UAV LiCNN, resulting in a R2 relationship of 0.6910. This research shows that AI models trained on ground photographs effectively classify lichen in UAV orthomosaics. Limiting factors include the misclassification of spectrally similar objects to lichen in the RGB bands and dark shadows cast by vegetation.
KW  - image classification
KW  - lichen mapping
KW  - orthomosaics
KW  - artificial intelligence
KW  - UAV
DO  - 10.3390/drones5030099
ER  -
TY  - EJOU
AU  - Kim, Eric J.
AU  - Perez, Ruben E.
TI  - Neuroevolutionary Control for Autonomous Soaring
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 9
SN  - 2226-4310

AB  - The energy efficiency and flight endurance of small unmanned aerial vehicles (SUAVs) can be improved through the implementation of autonomous soaring strategies. Biologically inspired flight techniques such as dynamic and thermal soaring offer significant energy savings through the exploitation of naturally occurring wind phenomena for thrustless flight. Recent interest in the application of artificial intelligence algorithms for autonomous soaring has been motivated by the pursuit of instilling generalized behavior in control systems, centered around the use of neural networks. However, the topology of such networks is usually predetermined, restricting the search space of potential solutions, while often resulting in complex neural networks that can pose implementation challenges for the limited hardware onboard small-scale autonomous vehicles. In exploring a novel method of generating neurocontrollers, this paper presents a neural network-based soaring strategy to extend flight times and advance the potential operational capability of SUAVs. In this study, the Neuroevolution of Augmenting Topologies (NEAT) algorithm is used to train efficient and effective neurocontrollers that can control a simulated aircraft along sustained dynamic and thermal soaring trajectories. The proposed approach evolves interpretable neural networks in a way that preserves simplicity while maximizing performance without requiring extensive training datasets. As a result, the combined trajectory planning and aircraft control strategy is suitable for real-time implementation on SUAV platforms.
KW  - dynamic soaring
KW  - thermal soaring
KW  - neurocontrol
KW  - artificial neural network
KW  - neuroevolution
KW  - unmanned aerial vehicle
KW  - flight trajectory
KW  - aircraft control
KW  - trajectory optimization
KW  - optimal control
DO  - 10.3390/aerospace8090267
ER  -
TY  - EJOU
AU  - Li, Daoliang
AU  - Du, Ling
TI  - AUV Trajectory Tracking Models and Control Strategies: A Review
T2  - Journal of Marine Science and Engineering

PY  - 2021
VL  - 9
IS  - 9
SN  - 2077-1312

AB  - Autonomous underwater vehicles (AUVs) have been widely used to perform underwater tasks. Due to the environmental disturbances, underactuated problems, system constraints, and system coupling, AUV trajectory tracking control is challenging. Thus, further investigation of dynamic characteristics and trajectory tracking control methods of the AUV motion system will be of great importance to improve underwater task performance. An AUV controller must be able to cope with various challenges with the underwater vehicle, adaptively update the reference model, and overcome unexpected deviations. In order to identify modeling strategies and the best control practices, this paper presents an overview of the main factors of control-oriented models and control strategies for AUVs. In modeling, two fields are considered: (i) models that come from simplifications of Fossen’s equations; and (ii) system identification models. For each category, a brief description of the control-oriented modeling strategies is given. In the control field, three relevant aspects are considered: (i) significance of AUV trajectory tracking control, (ii) control strategies; and (iii) control performance. For each aspect, the most important features are explained. Furthermore, in the aspect of control strategies, mathematical modeling study and physical experiment study are introduced in detail. Finally, with the aim of establishing the acceptability of the reported modeling and control techniques, as well as challenges that remain open, a discussion and a case study are presented. The literature review shows the development of new control-oriented models, the research in the estimation of unknown inputs, and the development of more innovative control strategies for AUV trajectory tracking systems are still open problems that must be addressed in the short term.
KW  - autonomous underwater vehicle
KW  - trajectory tracking
KW  - modeling
KW  - control strategies
DO  - 10.3390/jmse9091020
ER  -
TY  - EJOU
AU  - Kim, Chang J.
AU  - Jeong, Won T.
AU  - Kyung, Kee S.
AU  - Lee, Hee-Dong
AU  - Kim, Danbi
AU  - Song, Ho S.
AU  - Kang, Younkoo
AU  - Noh, Hyun H.
TI  - Dissipation and Distribution of Picarbutrazox Residue Following Spraying with an Unmanned Aerial Vehicle on Chinese Cabbage (Brassica campestris var. pekinensis)
T2  - Molecules

PY  - 2021
VL  - 26
IS  - 18
SN  - 1420-3049

AB  - We assessed the residual distribution and temporal trend of picarbutrazox sprayed by agricultural multicopters on Chinese cabbage and considered fortification levels and flying speeds. In plot 2, 14 days after the last spraying, the residues decreased by ~91.3% compared with those in the samples on day 0. The residues in the crops decreased by ~40.8% of the initial concentration owing to growth (dilution effect) and by ~50.6% after excluding the dilution effect. As the flight speed increased, picarbutrazox residues decreased (p &lt; 0.05, least significant deviation [LSD]). At 2 m s−1 flight speed, the residual distribution differed from the dilution rate of the spraying solution. The average range of picarbutrazox residues at all sampling points was 0.007 to 0.486, below the limit of quantitation −0.395, 0.005–0.316, and 0.005–0.289 mg kg−1 in plots 1, 2, 3, and 4, respectively, showing significant differences (p &lt; 0.05, LSD). These results indicated that the residual distribution of picarbutrazox sprayed by using a multicopter on the Chinese cabbages was not uniform. However, the residues were less than the maximum residue limit in all plots. Accordingly, picarbutrazox was considered to have a low risk to human health if it was sprayed on cabbage according to the recommended spraying conditions.
KW  - pesticide residue
KW  - Chinese cabbage
KW  - liquid chromatography
KW  - tandem mass spectrometry
KW  - QuEChERS
KW  - unmanned aerial vehicle
KW  - multicopter
KW  - picarbutrazox
KW  - spraying condition
DO  - 10.3390/molecules26185671
ER  -
TY  - EJOU
AU  - Zhang, Xupei
AU  - He, Zhanzhuang
AU  - Ma, Zhong
AU  - Jun, Peng
AU  - Yang, Kun
TI  - VIAE-Net: An End-to-End Altitude Estimation through Monocular Vision and Inertial Feature Fusion Neural Networks for UAV Autonomous Landing
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 18
SN  - 1424-8220

AB  - Altitude estimation is one of the fundamental tasks of unmanned aerial vehicle (UAV) automatic navigation, where it aims to accurately and robustly estimate the relative altitude between the UAV and specific areas. However, most methods rely on auxiliary signal reception or expensive equipment, which are not always available, or applicable owing to signal interference, cost or power-consuming limitations in real application scenarios. In addition, fixed-wing UAVs have more complex kinematic models than vertical take-off and landing UAVs. Therefore, an altitude estimation method which can be robustly applied in a GPS denied environment for fixed-wing UAVs must be considered. In this paper, we present a method for high-precision altitude estimation that combines the vision information from a monocular camera and poses information from the inertial measurement unit (IMU) through a novel end-to-end deep neural network architecture. Our method has numerous advantages over existing approaches. First, we utilize the visual-inertial information and physics-based reasoning to build an ideal altitude model that provides general applicability and data efficiency for neural network learning. A further advantage is that we have designed a novel feature fusion module to simplify the tedious manual calibration and synchronization of the camera and IMU, which are required for the standard visual or visual-inertial methods to obtain the data association for altitude estimation modeling. Finally, the proposed method was evaluated, and validated using real flight data obtained during a fixed-wing UAV landing phase. The results show the average estimation error of our method is less than 3% of the actual altitude, which vastly improves the altitude estimation accuracy compared to other visual and visual-inertial based methods.
KW  - altitude estimation
KW  - visual-inertial data fusion
KW  - self attention
KW  - UAV autonomous landing
DO  - 10.3390/s21186302
ER  -
TY  - EJOU
AU  - Benbouzid, Mohamed
AU  - Berghout, Tarek
AU  - Sarma, Nur
AU  - Djurović, Siniša
AU  - Wu, Yueqi
AU  - Ma, Xiandong
TI  - Intelligent Condition Monitoring of Wind Power Systems: State of the Art Review
T2  - Energies

PY  - 2021
VL  - 14
IS  - 18
SN  - 1996-1073

AB  - Modern wind turbines operate in continuously transient conditions, with varying speed, torque, and power based on the stochastic nature of the wind resource. This variability affects not only the operational performance of the wind power system, but can also affect its integrity under service conditions. Condition monitoring continues to play an important role in achieving reliable and economic operation of wind turbines. This paper reviews the current advances in wind turbine condition monitoring, ranging from conventional condition monitoring and signal processing tools to machine-learning-based condition monitoring and usage of big data mining for predictive maintenance. A systematic review is presented of signal-based and data-driven modeling methodologies using intelligent and machine learning approaches, with the view to providing a critical evaluation of the recent developments in this area, and their applications in diagnosis, prognosis, health assessment, and predictive maintenance of wind turbines and farms.
KW  - wind turbines
KW  - condition monitoring
KW  - diagnosis
KW  - prognosis
KW  - machine learning
KW  - data mining
KW  - health management
KW  - operations and maintenance
DO  - 10.3390/en14185967
ER  -
TY  - EJOU
AU  - Barber, Nastassia
AU  - Alvarado, Ernesto
AU  - Kane, Van R.
AU  - Mell, William E.
AU  - Moskal, L. M.
TI  - Estimating Fuel Moisture in Grasslands Using UAV-Mounted Infrared and Visible Light Sensors
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 19
SN  - 1424-8220

AB  - Predicting wildfire behavior is a complex task that has historically relied on empirical models. Physics-based fire models could improve predictions and have broad applicability, but these models require more detailed inputs, including spatially explicit estimates of fuel characteristics. One of the most critical of these characteristics is fuel moisture. Obtaining moisture measurements with traditional destructive sampling techniques can be prohibitively time-consuming and extremely limited in spatial resolution. This study seeks to assess how effectively moisture in grasses can be estimated using reflectance in six wavelengths in the visible and infrared ranges. One hundred twenty 1 m-square field samples were collected in a western Washington grassland as well as overhead imagery in six wavelengths for the same area. Predictive models of vegetation moisture using existing vegetation indices and components from principal component analysis of the wavelengths were generated and compared. The best model, a linear model based on principal components and biomass, showed modest predictive power (r² = 0.45). This model performed better for the plots with both dominant grass species pooled than it did for each species individually. The presence of this correlation, especially given the limited moisture range of this study, suggests that further research using samples across the entire fire season could potentially produce effective models for estimating moisture in this type of ecosystem using unmanned aerial vehicles, even when more than one major species of grass is present. This approach would be a fast and flexible approach compared to traditional moisture measurements.
KW  - unmanned aerial vehicle
KW  - fuel moisture content
KW  - wildfire
KW  - grassland
DO  - 10.3390/s21196350
ER  -
TY  - EJOU
AU  - Yañez-Badillo, Hugo
AU  - Beltran-Carbajal, Francisco
AU  - Tapia-Olvera, Ruben
AU  - Favela-Contreras, Antonio
AU  - Sotelo, Carlos
AU  - Sotelo, David
TI  - Adaptive Robust Motion Control of Quadrotor Systems Using Artificial Neural Networks and Particle Swarm Optimization
T2  - Mathematics

PY  - 2021
VL  - 9
IS  - 19
SN  - 2227-7390

AB  - Most of the mechanical dynamic systems are subjected to parametric uncertainty, unmodeled dynamics, and undesired external vibrating disturbances while are motion controlled. In this regard, new adaptive and robust, advanced control theories have been developed to efficiently regulate the motion trajectories of these dynamic systems while dealing with several kinds of variable disturbances. In this work, a novel adaptive robust neural control design approach for efficient motion trajectory tracking control tasks for a considerably disturbed non-linear under-actuated quadrotor system is introduced. Self-adaptive disturbance signal modeling based on Taylor-series expansions to handle dynamic uncertainty is adopted. Dynamic compensators of planned motion tracking errors are then used for designing a baseline controller with adaptive capabilities provided by three layers B-spline artificial neural networks (Bs-ANN). In the presented adaptive robust control scheme, measurements of position signals are only required. Moreover, real-time accurate estimation of time-varying disturbances and time derivatives of error signals are unnecessary. Integral reconstructors of velocity error signals are properly integrated in the output error signal feedback control scheme. In addition, the appropriate combination of several mathematical tools, such as particle swarm optimization (PSO), Bézier polynomials, artificial neural networks, and Taylor-series expansions, are advantageously exploited in the proposed control design perspective. In this fashion, the present contribution introduces a new adaptive desired motion tracking control solution based on B-spline neural networks, along with dynamic tracking error compensators for quadrotor non-linear systems. Several numeric experiments were performed to assess and highlight the effectiveness of the adaptive robust motion tracking control for a quadrotor unmanned aerial vehicle while subjected to undesired vibrating disturbances. Experiments include important scenarios that commonly face the quadrotors as path and trajectory tracking, take-off and landing, variations of the quadrotor nominal mass and basic navigation. Obtained results evidence a satisfactory quadrotor motion control while acceptable attenuation levels of vibrating disturbances are exhibited.
KW  - quadrotor UAV
KW  - artificial neural networks
KW  - robust control
KW  - Taylor series
KW  - B-splines
KW  - particle swarm optimization
DO  - 10.3390/math9192367
ER  -
TY  - EJOU
AU  - Wen, Xiang
AU  - Li, Xing
AU  - Zhang, Ce
AU  - Han, Wenquan
AU  - Li, Erzhu
AU  - Liu, Wei
AU  - Zhang, Lianpeng
TI  - ME-Net: A Multi-Scale Erosion Network for Crisp Building Edge Detection from Very High Resolution Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - The detection of building edges from very high resolution (VHR) remote sensing imagery is essential to various geo-related applications, including surveying and mapping, urban management, etc. Recently, the rapid development of deep convolutional neural networks (DCNNs) has achieved remarkable progress in edge detection; however, there has always been the problem of edge thickness due to the large receptive field of DCNNs. In this paper, we proposed a multi-scale erosion network (ME-Net) for building edge detection to crisp the building edge through two innovative approaches: (1) embedding an erosion module (EM) in the network to crisp the edge and (2) adding the Dice coefficient and local cross entropy of edge neighbors into the loss function to increase its sensitivity to the receptive field. In addition, a new metric, Ene, to measure the crispness of the predicted building edge was proposed. The experiment results show that ME-Net not only detects the clearest and crispest building edges, but also achieves the best OA of 98.75%, 95.00% and 95.51% on three building edge datasets, and exceeds other edge detection networks 3.17% and 0.44% at least in strict F1-score and Ene. In a word, the proposed ME-Net is an effective and practical approach for detecting crisp building edges from VHR remote sensing imagery.
KW  - building edge detection
KW  - deep convolutional neural network
KW  - erosion module
KW  - very high resolution remote sensing imagery
DO  - 10.3390/rs13193826
ER  -
TY  - EJOU
AU  - Deng, Hongjie
AU  - Ergu, Daji
AU  - Liu, Fangyao
AU  - Ma, Bo
AU  - Cai, Ying
TI  - An Embeddable Algorithm for Automatic Garbage Detection Based on Complex Marine Environment
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 19
SN  - 1424-8220

AB  - With the continuous development of artificial intelligence, embedding object detection algorithms into autonomous underwater detectors for marine garbage cleanup has become an emerging application area. Considering the complexity of the marine environment and the low resolution of the images taken by underwater detectors, this paper proposes an improved algorithm based on Mask R-CNN, with the aim of achieving high accuracy marine garbage detection and instance segmentation. First, the idea of dilated convolution is introduced in the Feature Pyramid Network to enhance feature extraction ability for small objects. Secondly, the spatial-channel attention mechanism is used to make features learn adaptively. It can effectively focus attention on detection objects. Third, the re-scoring branch is added to improve the accuracy of instance segmentation by scoring the predicted masks based on the method of Generalized Intersection over Union. Finally, we train the proposed algorithm in this paper on the Transcan dataset, evaluating its effectiveness by various metrics and comparing it with existing algorithms. The experimental results show that compared to the baseline provided by the Transcan dataset, the algorithm in this paper improves the mAP indexes on the two tasks of garbage detection and instance segmentation by 9.6 and 5.0, respectively, which significantly improves the algorithm performance. Thus, it can be better applied in the marine environment and achieve high precision object detection and instance segmentation.
KW  - deep learning
KW  - object detection
KW  - instance segmentation
KW  - marine ecology
KW  - the attentional mechanism
KW  - dilated convolution
DO  - 10.3390/s21196391
ER  -
TY  - EJOU
AU  - Neupane, Krishna
AU  - Baysal-Gurel, Fulya
TI  - Automatic Identification and Monitoring of Plant Diseases Using Unmanned Aerial Vehicles: A Review
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - Disease diagnosis is one of the major tasks for increasing food production in agriculture. Although precision agriculture (PA) takes less time and provides a more precise application of agricultural activities, the detection of disease using an Unmanned Aerial System (UAS) is a challenging task. Several Unmanned Aerial Vehicles (UAVs) and sensors have been used for this purpose. The UAVs’ platforms and their peripherals have their own limitations in accurately diagnosing plant diseases. Several types of image processing software are available for vignetting and orthorectification. The training and validation of datasets are important characteristics of data analysis. Currently, different algorithms and architectures of machine learning models are used to classify and detect plant diseases. These models help in image segmentation and feature extractions to interpret results. Researchers also use the values of vegetative indices, such as Normalized Difference Vegetative Index (NDVI), Crop Water Stress Index (CWSI), etc., acquired from different multispectral and hyperspectral sensors to fit into the statistical models to deliver results. There are still various drifts in the automatic detection of plant diseases as imaging sensors are limited by their own spectral bandwidth, resolution, background noise of the image, etc. The future of crop health monitoring using UAVs should include a gimble consisting of multiple sensors, large datasets for training and validation, the development of site-specific irradiance systems, and so on. This review briefly highlights the advantages of automatic detection of plant diseases to the growers.
KW  - UAS
KW  - UAVs
KW  - plant disease detection
KW  - plant monitoring
KW  - convolutional neural networks (CNNs)
KW  - deep learning
KW  - machine learning
DO  - 10.3390/rs13193841
ER  -
TY  - EJOU
AU  - Czarnecki, Joby M. Prince
AU  - Samiappan, Sathishkumar
AU  - Zhou, Meilun
AU  - McCraine, Cary D.
AU  - Wasson, Louis L.
TI  - Real-Time Automated Classification of Sky Conditions Using Deep Learning and Edge Computing
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - The radiometric quality of remotely sensed imagery is crucial for precision agriculture applications because estimations of plant health rely on the underlying quality. Sky conditions, and specifically shadowing from clouds, are critical determinants in the quality of images that can be obtained from low-altitude sensing platforms. In this work, we first compare common deep learning approaches to classify sky conditions with regard to cloud shadows in agricultural fields using a visible spectrum camera. We then develop an artificial-intelligence-based edge computing system to fully automate the classification process. Training data consisting of 100 oblique angle images of the sky were provided to a convolutional neural network and two deep residual neural networks (ResNet18 and ResNet34) to facilitate learning two classes, namely (1) good image quality expected, and (2) degraded image quality expected. The expectation of quality stemmed from the sky condition (i.e., density, coverage, and thickness of clouds) present at the time of the image capture. These networks were tested using a set of 13,000 images. Our results demonstrated that ResNet18 and ResNet34 classifiers produced better classification accuracy when compared to a convolutional neural network classifier. The best overall accuracy was obtained by ResNet34, which was 92% accurate, with a Kappa statistic of 0.77. These results demonstrate a low-cost solution to quality control for future autonomous farming systems that will operate without human intervention and supervision.
KW  - autonomous systems
KW  - cloud detection
KW  - low-altitude remote sensing
KW  - ResNet
KW  - UAS image quality
DO  - 10.3390/rs13193859
ER  -
TY  - EJOU
AU  - Sharma, Vinamra B.
AU  - Tewari, Saurabh
AU  - Biswas, Susham
AU  - Lohani, Bharat
AU  - Dwivedi, Umakant D.
AU  - Dwivedi, Deepak
AU  - Sharma, Ashutosh
AU  - Jung, Jae P.
TI  - Recent Advancements in AI-Enabled Smart Electronics Packaging for Structural Health Monitoring
T2  - Metals

PY  - 2021
VL  - 11
IS  - 10
SN  - 2075-4701

AB  - Real-time health monitoring of civil infrastructures is performed to maintain their structural integrity, sustainability, and serviceability for a longer time. With smart electronics and packaging technology, large amounts of complex monitoring data are generated, requiring sophisticated artificial intelligence (AI) techniques for their processing. With the advancement of technology, more complex AI models have been applied, from simple models to sophisticated deep learning (DL) models, for structural health monitoring (SHM). In this article, a comprehensive review is performed, primarily on the applications of AI models for SHM to maintain the sustainability of diverse civil infrastructures. Three smart data capturing methods of SHM, namely, camera-based, smartphone-based, and unmanned aerial vehicle (UAV)-based methods, are also discussed, having made the utilization of intelligent paradigms easier. UAV is found to be the most promising smart data acquisition technology, whereas convolution neural networks are the most impressive DL model reported for SHM. Furthermore, current challenges and future perspectives of AI-based SHM systems are also described separately. Moreover, the Internet of Things (IoT) and smart city concepts are explained to elaborate on the contributions of intelligent SHM systems. The integration of SHM with IoT and cloud-based computing is leading us towards the evolution of future smart cities.
KW  - electronics packaging
KW  - lead-free solders
KW  - structural health monitoring
KW  - civil infrastructure
KW  - damage detection
KW  - pipeline leakage detection
DO  - 10.3390/met11101537
ER  -
TY  - EJOU
AU  - Montgomery, Joshua
AU  - Mahoney, Craig
AU  - Brisco, Brian
AU  - Boychuk, Lyle
AU  - Cobbaert, Danielle
AU  - Hopkinson, Chris
TI  - Remote Sensing of Wetlands in the Prairie Pothole Region of North America
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - The Prairie Pothole Region (PPR) of North America is an extremely important habitat for a diverse range of wetland ecosystems that provide a wealth of socio-economic value. This paper describes the ecological characteristics and importance of PPR wetlands and the use of remote sensing for mapping and monitoring applications. While there are comprehensive reviews for wetland remote sensing in recent publications, there is no comprehensive review about the use of remote sensing in the PPR. First, the PPR is described, including the wetland classification systems that have been used, the water regimes that control the surface water and water levels, and the soil and vegetation characteristics of the region. The tools and techniques that have been used in the PPR for analyses of geospatial data for wetland applications are described. Field observations for ground truth data are critical for good validation and accuracy assessment of the many products that are produced. Wetland classification approaches are reviewed, including Decision Trees, Machine Learning, and object versus pixel-based approaches. A comprehensive description of the remote sensing systems and data that have been employed by various studies in the PPR is provided. A wide range of data can be used for various applications, including passive optical data like aerial photographs or satellite-based, Earth-observation data. Both airborne and spaceborne lidar studies are described. A detailed description of Synthetic Aperture RADAR (SAR) data and research are provided. The state of the art is the use of multi-source data to achieve higher accuracies and hybrid approaches. Digital Surface Models are also being incorporated in geospatial analyses to separate forest and shrub and emergent systems based on vegetation height. Remote sensing provides a cost-effective mechanism for mapping and monitoring PPR wetlands, especially with the logistical difficulties and cost of field-based methods. The wetland characteristics of the PPR dictate the need for high resolution in both time and space, which is increasingly possible with the numerous and increasing remote sensing systems available and the trend to open-source data and tools. The fusion of multi-source remote sensing data via state-of-the-art machine learning is recommended for wetland applications in the PPR. The use of such data promotes flexibility for sensor addition, subtraction, or substitution as a function of application needs and potential cost restrictions. This is important in the PPR because of the challenges related to the highly dynamic nature of this unique region.
KW  - prairie pothole region
KW  - wetland
KW  - remote sensing
KW  - monitoring
KW  - classification
KW  - ecology
DO  - 10.3390/rs13193878
ER  -
TY  - EJOU
AU  - Gouiaa, Rafik
AU  - Akhloufi, Moulay A.
AU  - Shahbazi, Mozhdeh
TI  - Advances in Convolution Neural Networks Based Crowd Counting and Density Estimation
T2  - Big Data and Cognitive Computing

PY  - 2021
VL  - 5
IS  - 4
SN  - 2504-2289

AB  - Automatically estimating the number of people in unconstrained scenes is a crucial yet challenging task in different real-world applications, including video surveillance, public safety, urban planning, and traffic monitoring. In addition, methods developed to estimate the number of people can be adapted and applied to related tasks in various fields, such as plant counting, vehicle counting, and cell microscopy. Many challenges and problems face crowd counting, including cluttered scenes, extreme occlusions, scale variation, and changes in camera perspective. Therefore, in the past few years, tremendous research efforts have been devoted to crowd counting, and numerous excellent techniques have been proposed. The significant progress in crowd counting methods in recent years is mostly attributed to advances in deep convolution neural networks (CNNs) as well as to public crowd counting datasets. In this work, we review the papers that have been published in the last decade and provide a comprehensive survey of the recent CNNs based crowd counting techniques. We briefly review detection-based, regression-based, and traditional density estimation based approaches. Then, we delve into detail regarding the deep learning based density estimation approaches and recently published datasets. In addition, we discuss the potential applications of crowd counting and in particular its applications using unmanned aerial vehicle (UAV) images.
KW  - density estimation
KW  - crowd counting
KW  - deep learning
KW  - CNN
KW  - UAV
DO  - 10.3390/bdcc5040050
ER  -
TY  - EJOU
AU  - Zhang, Tianxiang
AU  - Xu, Zhiyong
AU  - Su, Jinya
AU  - Yang, Zhifang
AU  - Liu, Cunjia
AU  - Chen, Wen-Hua
AU  - Li, Jiangyun
TI  - Ir-UNet: Irregular Segmentation U-Shape Network for Wheat Yellow Rust Detection by UAV Multispectral Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - Crop disease is widely considered as one of the most pressing challenges for food crops, and therefore an accurate crop disease detection algorithm is highly desirable for its sustainable management. The recent use of remote sensing and deep learning is drawing increasing research interests in wheat yellow rust disease detection. However, current solutions on yellow rust detection are generally addressed by RGB images and the basic semantic segmentation algorithms (e.g., UNet), which do not consider the irregular and blurred boundary problems of yellow rust area therein, restricting the disease segmentation performance. Therefore, this work aims to develop an automatic yellow rust disease detection algorithm to cope with these boundary problems. An improved algorithm entitled Ir-UNet by embedding irregular encoder module (IEM), irregular decoder module (IDM) and content-aware channel re-weight module (CCRM) is proposed and compared against the basic UNet while with various input features. The recently collected dataset by DJI M100 UAV equipped with RedEdge multispectral camera is used to evaluate the algorithm performance. Comparative results show that the Ir-UNet with five raw bands outperforms the basic UNet, achieving the highest overall accuracy (OA) score (97.13%) among various inputs. Moreover, the use of three selected bands, Red-NIR-RE, in the proposed Ir-UNet can obtain a comparable result (OA: 96.83%) while with fewer spectral bands and less computation load. It is anticipated that this study by seamlessly integrating the Ir-UNet network and UAV multispectral images can pave the way for automated yellow rust detection at farmland scales.
KW  - deep learning
KW  - Ir-UNet
KW  - crop disease detection
KW  - multispectral imagery
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/rs13193892
ER  -
TY  - EJOU
AU  - Nooralishahi, Parham
AU  - Ibarra-Castanedo, Clemente
AU  - Deane, Shakeb
AU  - López, Fernando
AU  - Pant, Shashank
AU  - Genest, Marc
AU  - Avdelidis, Nicolas P.
AU  - Maldague, Xavier P. V.
TI  - Drone-Based Non-Destructive Inspection of Industrial Sites: A Review and Case Studies
T2  - Drones

PY  - 2021
VL  - 5
IS  - 4
SN  - 2504-446X

AB  - Using aerial platforms for Non-Destructive Inspection (NDI) of large and complex structures is a growing field of interest in various industries. Infrastructures such as: buildings, bridges, oil and gas, etc. refineries require regular and extensive inspections. The inspection reports are used to plan and perform required maintenance, ensuring their structural health and the safety of the workers. However, performing these inspections can be challenging due to the size of the facility, the lack of easy access, the health risks for the inspectors, or several other reasons, which has convinced companies to invest more in drones as an alternative solution to overcome these challenges. The autonomous nature of drones can assist companies in reducing inspection time and cost. Moreover, the employment of drones can lower the number of required personnel for inspection and can increase personnel safety. Finally, drones can provide a safe and reliable solution for inspecting hard-to-reach or hazardous areas. Despite the recent developments in drone-based NDI to reliably detect defects, several limitations and challenges still need to be addressed. In this paper, a brief review of the history of unmanned aerial vehicles, along with a comprehensive review of studies focused on UAV-based NDI of industrial and commercial facilities, are provided. Moreover, the benefits of using drones in inspections as an alternative to conventional methods are discussed, along with the challenges and open problems of employing drones in industrial inspections, are explored. Finally, some of our case studies conducted in different industrial fields in the field of Non-Destructive Inspection are presented.
KW  - unmanned aerial vehicle
KW  - thermography
KW  - non-destructive testing (NDT)
KW  - aerial inspection
DO  - 10.3390/drones5040106
ER  -
TY  - EJOU
AU  - Ma, Weidong
AU  - Jia, Wei
AU  - Su, Peng
AU  - Feng, Xingyun
AU  - Liu, Fenggui
AU  - Wang, Jing’ai
TI  - Mapping Highland Barley on the Qinghai–Tibet Combing Landsat OLI Data and Object-Oriented Classification Method
T2  - Land

PY  - 2021
VL  - 10
IS  - 10
SN  - 2073-445X

AB  - In this paper, we use the extraction method of multi-factors fusion to extract the Highland barley cultivation area on Qinghai–Tibet Plateau. The study results indicate that: (1) the method (extracting through multi-factors fusion) is efficient during the extracting process and is highly accurate in extraction results. This extraction scheme allows for not only the spatial heterogeneity of different physical geographic units, but also the impact of multi-factors on crop cultivation; (2) according to our research, the total Highland barley cultivation area on Qinghai–Tibet Plateau is about 2.74 × 105 ha. Based on the statistics, we draw the first distribution map of the Highland barley cultivation area on Qinghai–Tibet Plateau, which upgrades its spatial distribution pattern from administrative unit to patch unit; (3) Highland barley in various divisions has a distinct spatial heterogeneity in elevation. On the whole, the Highland barley on the plateau is planted at an elevation of 2500–4500 m, up to 5200 m. Due to the impact of topography diversity, temperature, moisture, light, arable land and irrigation conditions, its cultivation area at the same elevation varies in different divisions.
KW  - Highland barley
KW  - Qinghai–Tibet Plateau
KW  - cultivation area extraction
KW  - object-oriented extraction method
KW  - spatial distribution of Highland barley
DO  - 10.3390/land10101022
ER  -
TY  - EJOU
AU  - Xia, Shuang
AU  - Zhang, Xiangyin
TI  - Constrained Path Planning for Unmanned Aerial Vehicle in 3D Terrain Using Modified Multi-Objective Particle Swarm Optimization
T2  - Actuators

PY  - 2021
VL  - 10
IS  - 10
SN  - 2076-0825

AB  - This paper considered the constrained unmanned aerial vehicle (UAV) path planning problem as the multi-objective optimization problem, in which both costs and constraints are treated as the objective functions. A novel multi-objective particle swarm optimization algorithm based on the Gaussian distribution and the Q-Learning technique (GMOPSO-QL) is proposed and applied to determine the feasible and optimal path for UAV. In GMOPSO-QL, the Gaussian distribution based updating operator is adopted to generate new particles, and the exploration and exploitation modes are introduced to enhance population diversity and convergence speed, respectively. Moreover, the Q-Learning based mode selection logic is introduced to balance the global search with the local search in the evolution process. Simulation results indicate that our proposed GMOPSO-QL can deal with the constrained UAV path planning problem and is superior to existing optimization algorithms in terms of efficiency and robustness.
KW  - 3D path planning
KW  - multi-objective particle swarm optimization
KW  - unmanned aerial vehicle
KW  - Q-Learning
DO  - 10.3390/act10100255
ER  -
TY  - EJOU
AU  - Saddik, Amine
AU  - Latif, Rachid
AU  - El Ouardi, Abdelhafid
TI  - Low-Power FPGA Architecture Based Monitoring Applications in Precision Agriculture
T2  - Journal of Low Power Electronics and Applications

PY  - 2021
VL  - 11
IS  - 4
SN  - 2079-9268

AB  - Today’s on-chip systems technology has grounded impressive advances in computing power and energy consumption. The choice of the right architecture depends on the application. In our case, we were studying vegetation monitoring algorithms in precision agriculture. This study presents a system based on a monitoring algorithm for agricultural fields, an electronic architecture based on a CPU-FPGA SoC system and the OpenCL parallel programming paradigm. We focused our study on our own dataset of agricultural fields to validate the results. The fields studied in our case are in the Guelmin-Oued noun region in the south of Morocco. These fields are divided into two areas, with a total surface of 3.44 Ha2 for the first field and 3.73 Ha2 for the second. The images were collected using a DJI-type unmanned aerial vehicle and an RGB camera. Performance evaluation showed that the system could process up to 86 fps versus 12 fps or 20 fps in C/C++ and OpenMP implementations, respectively. Software optimizations have increased the performance to 107 fps, which meets real-time constraints.
KW  - CPU-FPGA SoC
KW  - on-chip systems
KW  - embedded systems
KW  - precision agriculture
DO  - 10.3390/jlpea11040039
ER  -
TY  - EJOU
AU  - Zawadzka, Joanna
AU  - Truckell, Ian
AU  - Khouakhi, Abdou
AU  - Rivas Casado, Mónica
TI  - Detection of Flood Damage in Urban Residential Areas Using Object-Oriented UAV Image Analysis Coupled with Tree-Based Classifiers
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - Timely clearing-up interventions are essential for effective recovery of flood-damaged housing, however, time-consuming door-to-door inspections for insurance purposes need to take place before major repairs can be done to adequately assess the losses caused by flooding. With the increased probability of flooding, there is a heightened need for rapid flood damage assessment methods. High resolution imagery captured by unmanned aerial vehicles (UAVs) offers an opportunity for accelerating the time needed for inspections, either through visual interpretation or automated image classification. In this study, object-oriented image segmentation coupled with tree-based classifiers was implemented on a 10 cm resolution RGB orthoimage, captured over the English town of Cockermouth a week after a flood triggered by storm Desmond, to automatically detect debris associated with damages predominantly to residential housing. Random forests algorithm achieved a good level of overall accuracy of 74%, with debris being correctly classified at the rate of 58%, and performing well for small debris (67%) and skips (64%). The method was successful at depicting brightly-colored debris, however, was prone to misclassifications with brightly-colored vehicles. Consequently, in the current stage, the methodology could be used to facilitate visual interpretation of UAV images. Methods to improve accuracy have been identified and discussed.
KW  - urban flood damage
KW  - UAV
KW  - object-oriented image analysis
DO  - 10.3390/rs13193913
ER  -
TY  - EJOU
AU  - Korchagin, Sergey A.
AU  - Gataullin, Sergey T.
AU  - Osipov, Aleksey V.
AU  - Smirnov, Mikhail V.
AU  - Suvorov, Stanislav V.
AU  - Serdechnyi, Denis V.
AU  - Bublikov, Konstantin V.
TI  - Development of an Optimal Algorithm for Detecting Damaged and Diseased Potato Tubers Moving along a Conveyor Belt Using Computer Vision Systems
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 10
SN  - 2073-4395

AB  - The article discusses the problem of detecting sick or mechanically damaged potatoes using machine learning methods. We proposed an algorithm and developed a system for the rapid detection of damaged tubers. The system can be installed on a conveyor belt in a vegetable store, and it consists of a laptop computer and an action camera, synchronized with a flashlight system. The algorithm consists of two phases. The first phase uses the Viola-Jones algorithm, applied to the filtered action camera image, so it aims to detect separate potato tubers on the conveyor belt. The second phase is the application of a method that we choose based on video capturing conditions. To isolate potatoes infected with certain types of diseases (dry rot, for example), we use the Scale Invariant Feature Transform (SIFT)—Support Vector Machine (SVM) method. In case of inconsistent or weak lighting, the histogram of oriented gradients (HOG)—Bag-of-Visual-Words (BOVW)—neural network (BPNN) method is used. Otherwise, Otsu’s threshold binarization—a convolutional neural network (CNN) method is used. The first phase’s result depends on the conveyor’s speed, the density of tubers on the conveyor, and the accuracy of the video system. With the optimal setting, the result reaches 97%. The second phase’s outcome depends on the method and varies from 80% to 97%. When evaluating the performance of the system, it was found that it allows to detect and classify up to 100 tubers in one second, which significantly exceeds the performance of most similar systems.
KW  - neural networks
KW  - defects detection
KW  - crop
KW  - potato disease
KW  - potato classification
KW  - fast detection
KW  - machine learning
DO  - 10.3390/agronomy11101980
ER  -
TY  - EJOU
AU  - Lu, Qikai
AU  - Si, Wei
AU  - Wei, Lifei
AU  - Li, Zhongqiang
AU  - Xia, Zhihong
AU  - Ye, Song
AU  - Xia, Yu
TI  - Retrieval of Water Quality from UAV-Borne Hyperspectral Imagery: A Comparative Study of Machine Learning Algorithms
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 19
SN  - 2072-4292

AB  - The rapidly increasing world population and human activities accelerate the crisis of the limited freshwater resources. Water quality must be monitored for the sustainability of freshwater resources. Unmanned aerial vehicle (UAV)-borne hyperspectral data can capture fine features of water bodies, which have been widely used for monitoring water quality. In this study, nine machine learning algorithms are systematically evaluated for the inversion of water quality parameters including chlorophyll-a (Chl-a) and suspended solids (SS) with UAV-borne hyperspectral data. In comparing the experimental results of the machine learning model on the water quality parameters, we can observe that the prediction performance of the Catboost regression (CBR) model is the best. However, the prediction performances of the Multi-layer Perceptron regression (MLPR) and Elastic net (EN) models are very unsatisfactory, indicating that the MLPR and EN models are not suitable for the inversion of water quality parameters. In addition, the water quality distribution map is generated, which can be used to identify polluted areas of water bodies.
KW  - water quality parameters inversion
KW  - machine learning
KW  - UAV-borne hyperspectral data
KW  - water quality mapping
DO  - 10.3390/rs13193928
ER  -
TY  - EJOU
AU  - Berghout, Tarek
AU  - Benbouzid, Mohamed
AU  - Bentrcia, Toufik
AU  - Ma, Xiandong
AU  - Djurović, Siniša
AU  - Mouss, Leïla-Hayet
TI  - Machine Learning-Based Condition Monitoring for PV Systems: State of the Art and Future Prospects
T2  - Energies

PY  - 2021
VL  - 14
IS  - 19
SN  - 1996-1073

AB  - To ensure the continuity of electric power generation for photovoltaic systems, condition monitoring frameworks are subject to major enhancements. The continuous uniform delivery of electric power depends entirely on a well-designed condition maintenance program. A just-in-time task to deal with several naturally occurring faults can be correctly undertaken via the cooperation of effective detection, diagnosis, and prognostic analyses. Therefore, the present review first outlines different failure modes to which all photovoltaic systems are subjected, in addition to the essential integrated detection methods and technologies. Then, data-driven paradigms, and their contribution to solving this prediction problem, are also explored. Accordingly, this review primarily investigates the different learning architectures used (i.e., ordinary, hybrid, and ensemble) in relation to their learning frameworks (i.e., traditional and deep learning). It also discusses the extension of machine learning to knowledge-driven approaches, including generative models such as adversarial networks and transfer learning. Finally, this review provides insights into different works to highlight various operating conditions and different numbers and types of failures, and provides links to some publicly available datasets in the field. The clear organization of the abundant information on this subject may result in rigorous guidelines for the trends adopted in the future.
KW  - photovoltaic systems
KW  - machine learning
KW  - deep learning
KW  - condition monitoring
KW  - faults diagnosis
KW  - fault detection
KW  - open source datasets
DO  - 10.3390/en14196316
ER  -
