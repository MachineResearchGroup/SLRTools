TY  - EJOU
AU  - Hung, Calvin
AU  - Xu, Zhe
AU  - Sukkarieh, Salah
TI  - Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV
T2  - Remote Sensing

PY  - 2014
VL  - 6
IS  - 12
SN  - 2072-4292

AB  - The development of low-cost unmanned aerial vehicles (UAVs) and light weight imaging sensors has resulted in significant interest in their use for remote sensing applications. While significant attention has been paid to the collection, calibration, registration and mosaicking of data collected from small UAVs, the interpretation of these data into semantically meaningful information can still be a laborious task. A standard data collection and classification work-flow requires significant manual effort for segment size tuning, feature selection and rule-based classifier design. In this paper, we propose an alternative learning-based approach using feature learning to minimise the manual effort required. We apply this system to the classification of invasive weed species. Small UAVs are suited to this application, as they can collect data at high spatial resolutions, which is essential for the classification of small or localised weed outbreaks. In this paper, we apply feature learning to generate a bank of image filters that allows for the extraction of features that discriminate between the weeds of interest and background objects. These features are pooled to summarise the image statistics and form the input to a texton-based linear classifier that classifies an image patch as weed or background. We evaluated our approach to weed classification on three weeds of significance in Australia: water hyacinth, tropical soda apple and serrated tussock. Our results showed that collecting images at 5–10 m resulted in the highest classifier accuracy, indicated by F1 scores of up to 94%.
KW  - weed classification
KW  - UAV remote sensing
KW  - serrated tussock
KW  - tropical soda apple
KW  - water hyacinth
DO  - 10.3390/rs61212037
TY  - EJOU
AU  - Gökçe, Fatih
AU  - Üçoluk, Göktürk
AU  - Şahin, Erol
AU  - Kalkan, Sinan
TI  - Vision-Based Detection and Distance Estimation of Micro Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2015
VL  - 15
IS  - 9
SN  - 1424-8220

AB  - Detection and distance estimation of micro unmanned aerial vehicles (mUAVs) is crucial for (i) the detection of intruder mUAVs in protected environments; (ii) sense and avoid purposes on mUAVs or on other aerial vehicles and (iii) multi-mUAV control scenarios, such as environmental monitoring, surveillance and exploration. In this article, we evaluate vision algorithms as alternatives for detection and distance estimation of mUAVs, since other sensing modalities entail certain limitations on the environment or on the distance. For this purpose, we test Haar-like features, histogram of gradients (HOG) and local binary patterns (LBP) using cascades of boosted classifiers. Cascaded boosted classifiers allow fast processing by performing detection tests at multiple stages, where only candidates passing earlier simple stages are processed at the preceding more complex stages. We also integrate a distance estimation method with our system utilizing geometric cues with support vector regressors. We evaluated each method on indoor and outdoor videos that are collected in a systematic way and also on videos having motion blur. Our experiments show that, using boosted cascaded classifiers with LBP, near real-time detection and distance estimation of mUAVs are possible in about 60 ms indoors (1032 × 778 resolution) and 150 ms outdoors (1280 × 720 resolution) per frame, with a detection rate of 0.96 F-score. However, the cascaded classifiers using Haar-like features lead to better distance estimation since they can position the bounding boxes on mUAVs more accurately. On the other hand, our time analysis yields that the cascaded classifiers using HOG train and run faster than the other algorithms.
KW  - UAV
KW  - micro UAV
KW  - vision
KW  - detection
KW  - distance estimation
KW  - cascaded classifiers
DO  - 10.3390/s150923805
TY  - EJOU
AU  - Ali, Iftikhar
AU  - Greifeneder, Felix
AU  - Stamenkovic, Jelena
AU  - Neumann, Maxim
AU  - Notarnicola, Claudia
TI  - Review of Machine Learning Approaches for Biomass and Soil Moisture Retrievals from Remote Sensing Data
T2  - Remote Sensing

PY  - 2015
VL  - 7
IS  - 12
SN  - 2072-4292

AB  - The enormous increase of remote sensing data from airborne and space-borne platforms, as well as ground measurements has directed the attention of scientists towards new and efficient retrieval methodologies. Of particular importance is the consideration of the large extent and the high dimensionality (spectral, temporal and spatial) of remote sensing data. Moreover, the launch of the Sentinel satellite family will increase the availability of data, especially in the temporal domain, at no cost to the users. To analyze these data and to extract relevant features, such as essential climate variables (ECV), specific methodologies need to be exploited. Among these, greater attention is devoted to machine learning methods due to their flexibility and the capability to process large number of inputs and to handle non-linear problems. The main objective of this paper is to provide a review of research that is being carried out to retrieve two critically important terrestrial biophysical quantities (vegetation biomass and soil moisture) from remote sensing data using machine learning methods.
KW  - remote sensing
KW  - soil moisture
KW  - biomass
KW  - retrieval algorithms
KW  - machine learning
KW  - artificial neural networks
KW  - SVM
KW  - regression
KW  - biophysical parameters
DO  - 10.3390/rs71215841
TY  - EJOU
AU  - Fremont, Vincent
AU  - Bui, Manh T.
AU  - Boukerroui, Djamal
AU  - Letort, Pierrick
TI  - Vision-Based People Detection System for Heavy Machine Applications
T2  - Sensors

PY  - 2016
VL  - 16
IS  - 1
SN  - 1424-8220

AB  - This paper presents a vision-based people detection system for improving safety in heavy machines. We propose a perception system composed of a monocular fisheye camera and a LiDAR. Fisheye cameras have the advantage of a wide field-of-view, but the strong distortions that they create must be handled at the detection stage. Since people detection in fisheye images has not been well studied, we focus on investigating and quantifying the impact that strong radial distortions have on the appearance of people, and we propose approaches for handling this specificity, adapted from state-of-the-art people detection approaches. These adaptive approaches nevertheless have the drawback of high computational cost and complexity. Consequently, we also present a framework for harnessing the LiDAR modality in order to enhance the detection algorithm for different camera positions. A sequential LiDAR-based fusion architecture is used, which addresses directly the problem of reducing false detections and computational cost in an exclusively vision-based system. A heavy machine dataset was built, and different experiments were carried out to evaluate the performance of the system. The results are promising, in terms of both processing speed and performance.
KW  - heavy machines
KW  - sensor fusion
KW  - pedestrian detection
KW  - deformable part model
KW  - fisheye images
KW  - histogram of oriented gradients
DO  - 10.3390/s16010128
TY  - EJOU
AU  - Yue, Bo
AU  - Wang, Shuang
AU  - Liang, Xuefeng
AU  - Jiao, Licheng
AU  - Xu, Caijin
TI  - Joint Prior Learning for Visual Sensor Network Noisy Image Super-Resolution
T2  - Sensors

PY  - 2016
VL  - 16
IS  - 3
SN  - 1424-8220

AB  - The visual sensor network (VSN), a new type of wireless sensor network composed of low-cost wireless camera nodes, is being applied for numerous complex visual analyses in wild environments, such as visual surveillance, object recognition, etc. However, the captured images/videos are often low resolution with noise. Such visual data cannot be directly delivered to the advanced visual analysis. In this paper, we propose a joint-prior image super-resolution (JPISR) method using expectation maximization (EM) algorithm to improve VSN image quality. Unlike conventional methods that only focus on upscaling images, JPISR alternatively solves upscaling mapping and denoising in the E-step and M-step. To meet the requirement of the M-step, we introduce a novel non-local group-sparsity image filtering method to learn the explicit prior and induce the geometric duality between images to learn the implicit prior. The EM algorithm inherently combines the explicit prior and implicit prior by joint learning. Moreover, JPISR does not rely on large external datasets for training, which is much more practical in a VSN. Extensive experiments show that JPISR outperforms five state-of-the-art methods in terms of both PSNR, SSIM and visual perception.
KW  - visual sensor network
KW  - image super-resolution
KW  - image denoising
KW  - prior learning
KW  - EM algorithm
DO  - 10.3390/s16030288
TY  - EJOU
AU  - Kim, Sungho
AU  - Song, Woo-Jin
AU  - Kim, So-Hyun
TI  - Robust Ground Target Detection by SAR and IR Sensor Fusion Using Adaboost-Based Feature Selection
T2  - Sensors

PY  - 2016
VL  - 16
IS  - 7
SN  - 1424-8220

AB  - Long-range ground targets are difficult to detect in a noisy cluttered environment using either synthetic aperture radar (SAR) images or infrared (IR) images. SAR-based detectors can provide a high detection rate with a high false alarm rate to background scatter noise. IR-based approaches can detect hot targets but are affected strongly by the weather conditions. This paper proposes a novel target detection method by decision-level SAR and IR fusion using an Adaboost-based machine learning scheme to achieve a high detection rate and low false alarm rate. The proposed method consists of individual detection, registration, and fusion architecture. This paper presents a single framework of a SAR and IR target detection method using modified Boolean map visual theory (modBMVT) and feature-selection based fusion. Previous methods applied different algorithms to detect SAR and IR targets because of the different physical image characteristics. One method that is optimized for IR target detection produces unsuccessful results in SAR target detection. This study examined the image characteristics and proposed a unified SAR and IR target detection method by inserting a median local average filter (MLAF, pre-filter) and an asymmetric morphological closing filter (AMCF, post-filter) into the BMVT. The original BMVT was optimized to detect small infrared targets. The proposed modBMVT can remove the thermal and scatter noise by the MLAF and detect extended targets by attaching the AMCF after the BMVT. Heterogeneous SAR and IR images were registered automatically using the proposed RANdom SAmple Region Consensus (RANSARC)-based homography optimization after a brute-force correspondence search using the detected target centers and regions. The final targets were detected by feature-selection based sensor fusion using Adaboost. The proposed method showed good SAR and IR target detection performance through feature selection-based decision fusion on a synthetic database generated by OKTAL-SE.
KW  - synthetic aperture radar
KW  - infrared
KW  - target detection
KW  - sensor fusion
KW  - machine learning
KW  - feature selection
KW  - OKTAL-SE
DO  - 10.3390/s16071117
TY  - EJOU
AU  - Gong, Wenjuan
AU  - Zhang, Xuena
AU  - Gonzàlez, Jordi
AU  - Sobral, Andrews
AU  - Bouwmans, Thierry
AU  - Tu, Changhe
AU  - Zahzah, El-hadi
TI  - Human Pose Estimation from Monocular Images: A Comprehensive Survey
T2  - Sensors

PY  - 2016
VL  - 16
IS  - 12
SN  - 1424-8220

AB  - Human pose estimation refers to the estimation of the location of body parts and how they are connected in an image. Human pose estimation from monocular images has wide applications (e.g., image indexing). Several surveys on human pose estimation can be found in the literature, but they focus on a certain category; for example, model-based approaches or human motion analysis, etc. As far as we know, an overall review of this problem domain has yet to be provided. Furthermore, recent advancements based on deep learning have brought novel algorithms for this problem. In this paper, a comprehensive survey of human pose estimation from monocular images is carried out including milestone works and recent advancements. Based on one standard pipeline for the solution of computer vision problems, this survey splits the problem into several modules: feature extraction and description, human body models, and modeling methods. Problem modeling methods are approached based on two means of categorization in this survey. One way to categorize includes top-down and bottom-up methods, and another way includes generative and discriminative methods. Considering the fact that one direct application of human pose estimation is to provide initialization for automatic video surveillance, there are additional sections for motion-related methods in all modules: motion features, motion models, and motion-based methods. Finally, the paper also collects 26 publicly available data sets for validation and provides error measurement methods that are frequently used.
KW  - human pose estimation
KW  - human body models
KW  - generative methods
KW  - discriminative methods
KW  - top-down methods
KW  - bottom-up methods
DO  - 10.3390/s16121966
TY  - EJOU
AU  - Li, Weijia
AU  - Fu, Haohuan
AU  - Yu, Le
AU  - Cracknell, Arthur
TI  - Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 1
SN  - 2072-4292

AB  - Oil palm trees are important economic crops in Malaysia and other tropical areas. The number of oil palm trees in a plantation area is important information for predicting the yield of palm oil, monitoring the growing situation of palm trees and maximizing their productivity, etc. In this paper, we propose a deep learning based framework for oil palm tree detection and counting using high-resolution remote sensing images for Malaysia. Unlike previous palm tree detection studies, the trees in our study area are more crowded and their crowns often overlap. We use a number of manually interpreted samples to train and optimize the convolutional neural network (CNN), and predict labels for all the samples in an image dataset collected through the sliding window technique. Then, we merge the predicted palm coordinates corresponding to the same palm tree into one palm coordinate and obtain the final palm tree detection results. Based on our proposed method, more than 96% of the oil palm trees in our study area can be detected correctly when compared with the manually interpreted ground truth, and this is higher than the accuracies of the other three tree detection methods used in this study.
KW  - oil palm trees
KW  - deep learning
KW  - convolutional neural network (CNN)
KW  - object detection
DO  - 10.3390/rs9010022
TY  - EJOU
AU  - Ramon Soria, Pablo
AU  - Arrue, Begoña C.
AU  - Ollero, Anibal
TI  - Detection, Location and Grasping Objects Using a Stereo Sensor on UAV in Outdoor Environments
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 1
SN  - 1424-8220

AB  - The article presents a vision system for the autonomous grasping of objects with Unmanned Aerial Vehicles (UAVs) in real time. Giving UAVs the capability to manipulate objects vastly extends their applications, as they are capable of accessing places that are difficult to reach or even unreachable for human beings. This work is focused on the grasping of known objects based on feature models. The system runs in an on-board computer on a UAV equipped with a stereo camera and a robotic arm. The algorithm learns a feature-based model in an offline stage, then it is used online for detection of the targeted object and estimation of its position. This feature-based model was proved to be robust to both occlusions and the presence of outliers. The use of stereo cameras improves the learning stage, providing 3D information and helping to filter features in the online stage. An experimental system was derived using a rotary-wing UAV and a small manipulator for final proof of concept. The robotic arm is designed with three degrees of freedom and is lightweight due to payload limitations of the UAV. The system has been validated with different objects, both indoors and outdoors.
KW  - UAV
KW  - grasping
KW  - outdoors
DO  - 10.3390/s17010103
TY  - EJOU
AU  - Tang, Tianyu
AU  - Zhou, Shilin
AU  - Deng, Zhipeng
AU  - Zou, Huanxin
AU  - Lei, Lin
TI  - Vehicle Detection in Aerial Images Based on Region Convolutional Neural Networks and Hard Negative Example Mining
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 2
SN  - 1424-8220

AB  - Detecting vehicles in aerial imagery plays an important role in a wide range of applications. The current vehicle detection methods are mostly based on sliding-window search and handcrafted or shallow-learning-based features, having limited description capability and heavy computational costs. Recently, due to the powerful feature representations, region convolutional neural networks (CNN) based detection methods have achieved state-of-the-art performance in computer vision, especially Faster R-CNN. However, directly using it for vehicle detection in aerial images has many limitations: (1) region proposal network (RPN) in Faster R-CNN has poor performance for accurately locating small-sized vehicles, due to the relatively coarse feature maps; and (2) the classifier after RPN cannot distinguish vehicles and complex backgrounds well. In this study, an improved detection method based on Faster R-CNN is proposed in order to accomplish the two challenges mentioned above. Firstly, to improve the recall, we employ a hyper region proposal network (HRPN) to extract vehicle-like targets with a combination of hierarchical feature maps. Then, we replace the classifier after RPN by a cascade of boosted classifiers to verify the candidate regions, aiming at reducing false detection by negative example mining. We evaluate our method on the Munich vehicle dataset and the collected vehicle dataset, with improvements in accuracy and robustness compared to existing methods.
KW  - vehicle detection
KW  - hyper region proposal network
KW  - convolutional neural networks
KW  - hard negative example mining
DO  - 10.3390/s17020336
TY  - EJOU
AU  - Ammour, Nassim
AU  - Alhichri, Haikel
AU  - Bazi, Yakoub
AU  - Benjdira, Bilel
AU  - Alajlan, Naif
AU  - Zuair, Mansour
TI  - Deep Learning Approach for Car Detection in UAV Imagery
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 4
SN  - 2072-4292

AB  - This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.
KW  - UAV imagery
KW  - car counting
KW  - deep learning
KW  - convolutional neural networks (CNNs)
KW  - support vector machines (SVM)
KW  - mean-shift segmentation
DO  - 10.3390/rs9040312
TY  - EJOU
AU  - Song, Zhiguo
AU  - Sun, Jifeng
AU  - Yu, Jialin
TI  - Object Tracking by a Combination of Discriminative Global and Generative Multi-Scale Local Models
T2  - Information

PY  - 2017
VL  - 8
IS  - 2
SN  - 2078-2489

AB  - Object tracking is a challenging task in many computer vision applications due to occlusion, scale variation and background clutter, etc. In this paper, we propose a tracking algorithm by combining discriminative global and generative multi-scale local models. In the global model, we teach a classifier with sparse discriminative features to separate the target object from the background based on holistic templates. In the multi-scale local model, the object is represented by multi-scale local sparse representation histograms, which exploit the complementary partial and spatial information of an object across different scales. Finally, a collaborative similarity score of one candidate target is input into a Bayesian inference framework to estimate the target state sequentially during tracking. Experimental results on the various challenging video sequences show that the proposed method performs favorably compared to several state-of-the-art trackers.
KW  - object tracking
KW  - sparse representation
KW  - Bayesian inference
KW  - discriminative global model
KW  - generative multi-scale local model
DO  - 10.3390/info8020043
TY  - EJOU
AU  - Li, Feimo
AU  - Li, Shuxiao
AU  - Zhu, Chengfei
AU  - Lan, Xiaosong
AU  - Chang, Hongxing
TI  - Cost-Effective Class-Imbalance Aware CNN for Vehicle Localization and Categorization in High Resolution Aerial Images
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 5
SN  - 2072-4292

AB  - Joint vehicle localization and categorization in high resolution aerial images can provide useful information for applications such as traffic flow structure analysis. To maintain sufficient features to recognize small-scaled vehicles, a regions with convolutional neural network features (R-CNN) -like detection structure is employed. In this setting, cascaded localization error can be averted by equally treating the negatives and differently typed positives as a multi-class classification task, but the problem of class-imbalance remains. To address this issue, a cost-effective network extension scheme is proposed. In it, the correlated convolution and connection costs during extension are reduced by feature map selection and bi-partite main-side network construction, which are realized with the assistance of a novel feature map class-importance measurement and a new class-imbalance sensitive main-side loss function. By using an image classification dataset established from a set of traditional real-colored aerial images with 0.13 m ground sampling distance which are taken from the height of 1000 m by an imaging system composed of non-metric cameras, the effectiveness of the proposed network extension is verified by comparing with its similarly shaped strong counter-parts. Experiments show an equivalent or better performance, while requiring the least parameter and memory overheads are required.
KW  - vehicle localization
KW  - vehicle classification
KW  - high resolution
KW  - aerial image
KW  - convolutional neural network (CNN)
KW  - class imbalance
DO  - 10.3390/rs9050494
TY  - EJOU
AU  - Ampatzidis, Yiannis
AU  - De Bellis, Luigi
AU  - Luvisi, Andrea
TI  - iPathology: Robotic Applications and Management of Plants and Plant Diseases
T2  - Sustainability

PY  - 2017
VL  - 9
IS  - 6
SN  - 2071-1050

AB  - The rapid development of new technologies and the changing landscape of the online world (e.g., Internet of Things (IoT), Internet of All, cloud-based solutions) provide a unique opportunity for developing automated and robotic systems for urban farming, agriculture, and forestry. Technological advances in machine vision, global positioning systems, laser technologies, actuators, and mechatronics have enabled the development and implementation of robotic systems and intelligent technologies for precision agriculture. Herein, we present and review robotic applications on plant pathology and management, and emerging agricultural technologies for intra-urban agriculture. Greenhouse advanced management systems and technologies have been greatly developed in the last years, integrating IoT and WSN (Wireless Sensor Network). Machine learning, machine vision, and AI (Artificial Intelligence) have been utilized and applied in agriculture for automated and robotic farming. Intelligence technologies, using machine vision/learning, have been developed not only for planting, irrigation, weeding (to some extent), pruning, and harvesting, but also for plant disease detection and identification. However, plant disease detection still represents an intriguing challenge, for both abiotic and biotic stress. Many recognition methods and technologies for identifying plant disease symptoms have been successfully developed; still, the majority of them require a controlled environment for data acquisition to avoid false positives. Machine learning methods (e.g., deep and transfer learning) present promising results for improving image processing and plant symptom identification. Nevertheless, diagnostic specificity is a challenge for microorganism control and should drive the development of mechatronics and robotic solutions for disease management.
KW  - machine vision
KW  - machine learning
KW  - vertical farming systems
KW  - mechatronics
KW  - smart machines
KW  - smart city
DO  - 10.3390/su9061010
TY  - EJOU
AU  - Radovic, Matija
AU  - Adarkwa, Offei
AU  - Wang, Qiaosong
TI  - Object Recognition in Aerial Images Using Convolutional Neural Networks
T2  - Journal of Imaging

PY  - 2017
VL  - 3
IS  - 2
SN  - 2313-433X

AB  - There are numerous applications of unmanned aerial vehicles (UAVs) in the management of civil infrastructure assets. A few examples include routine bridge inspections, disaster management, power line surveillance and traffic surveying. As UAV applications become widespread, increased levels of autonomy and independent decision-making are necessary to improve the safety, efficiency, and accuracy of the devices. This paper details the procedure and parameters used for the training of convolutional neural networks (CNNs) on a set of aerial images for efficient and automated object recognition. Potential application areas in the transportation field are also highlighted. The accuracy and reliability of CNNs depend on the network’s training and the selection of operational parameters. This paper details the CNN training procedure and parameter selection. The object recognition results show that by selecting a proper set of parameters, a CNN can detect and classify objects with a high level of accuracy (97.5%) and computational efficiency. Furthermore, using a convolutional neural network implemented in the “YOLO” (“You Only Look Once”) platform, objects can be tracked, detected (“seen”), and classified (“comprehended”) from video feeds supplied by UAVs in real-time.
KW  - convolutional neural networks
KW  - Unmanned Aerial Vehicle (UAV)
KW  - object recognition and detection
DO  - 10.3390/jimaging3020021
TY  - EJOU
AU  - Yang, Yurong
AU  - Gong, Huajun
AU  - Wang, Xinhua
AU  - Sun, Peng
TI  - Aerial Target Tracking Algorithm Based on Faster R-CNN Combined with Frame Differencing
T2  - Aerospace

PY  - 2017
VL  - 4
IS  - 2
SN  - 2226-4310

AB  - We propose a robust approach to detecting and tracking moving objects for a naval unmanned aircraft system (UAS) landing on an aircraft carrier. The frame difference algorithm follows a simple principle to achieve real-time tracking, whereas Faster Region-Convolutional Neural Network (R-CNN) performs highly precise detection and tracking characteristics. We thus combine Faster R-CNN with the frame difference method, which is demonstrated to exhibit robust and real-time detection and tracking performance. In our UAS landing experiments, two cameras placed on both sides of the runway are used to capture the moving UAS. When the UAS is captured, the joint algorithm uses frame difference to detect the moving target (UAS). As soon as the Faster R-CNN algorithm accurately detects the UAS, the detection priority is given to Faster R-CNN. In this manner, we also perform motion segmentation and object detection in the presence of changes in the environment, such as illumination variation or “walking persons”. By combining the 2 algorithms we can accurately detect and track objects with a tracking accuracy rate of up to 99% and a frame per second of up to 40 Hz. Thus, a solid foundation is laid for subsequent landing guidance.
KW  - deep learning
KW  - Faster R-CNN
KW  - UAS landing
KW  - object detection
DO  - 10.3390/aerospace4020032
TY  - EJOU
AU  - Jiang, Hao
AU  - Chen, Shuisen
AU  - Li, Dan
AU  - Wang, Chongyang
AU  - Yang, Ji
TI  - Papaya Tree Detection with UAV Images Using a GPU-Accelerated Scale-Space Filtering Method
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 7
SN  - 2072-4292

AB  - The use of unmanned aerial vehicles (UAV) can allow individual tree detection for forest inventories in a cost-effective way. The scale-space filtering (SSF) algorithm is commonly used and has the capability of detecting trees of different crown sizes. In this study, we made two improvements with regard to the existing method and implementations. First, we incorporated SSF with a Lab color transformation to reduce over-detection problems associated with the original luminance image. Second, we ported four of the most time-consuming processes to the graphics processing unit (GPU) to improve computational efficiency. The proposed method was implemented using PyCUDA, which enabled access to NVIDIA’s compute unified device architecture (CUDA) through high-level scripting of the Python language. Our experiments were conducted using two images captured by the DJI Phantom 3 Professional and a most recent NVIDIA GPU GTX1080. The resulting accuracy was high, with an F-measure larger than 0.94. The speedup achieved by our parallel implementation was 44.77 and 28.54 for the first and second test image, respectively. For each 4000 × 3000 image, the total runtime was less than 1 s, which was sufficient for real-time performance and interactive application.
KW  - compute unified device architecture (CUDA)
KW  - graphics processing units (GPU)
KW  - PyCUDA
KW  - scale-space
KW  - tree detection
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/rs9070721
TY  - EJOU
AU  - Ahmed, Asmau M.
AU  - Duran, Olga
AU  - Zweiri, Yahya
AU  - Smith, Mike
TI  - Hybrid Spectral Unmixing: Using Artificial Neural Networks for Linear/Non-Linear Switching
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 8
SN  - 2072-4292

AB  - Spectral unmixing is a key process in identifying spectral signature of materials and quantifying their spatial distribution over an image. The linear model is expected to provide acceptable results when two assumptions are satisfied: (1) The mixing process should occur at macroscopic level and (2) Photons must interact with single material before reaching the sensor. However, these assumptions do not always hold and more complex nonlinear models are required. This study proposes a new hybrid method for switching between linear and nonlinear spectral unmixing of hyperspectral data based on artificial neural networks. The neural networks was trained with parameters within a window of the pixel under consideration. These parameters are computed to represent the diversity of the neighboring pixels and are based on the Spectral Angular Distance, Covariance and a non linearity parameter. The endmembers were extracted using Vertex Component Analysis while the abundances were estimated using the method identified by the neural networks (Vertex Component Analysis, Fully Constraint Least Square Method, Polynomial Post Nonlinear Mixing Model or Generalized Bilinear Model). Results show that the hybrid method performs better than each of the individual techniques with high overall accuracy, while the abundance estimation error is significantly lower than that obtained using the individual methods. Experiments on both synthetic dataset and real hyperspectral images demonstrated that the proposed hybrid switch method is efficient for solving spectral unmixing of hyperspectral images as compared to individual algorithms.
KW  - hyperspectral image
KW  - spectral unmixing
KW  - endmembers
KW  - artificial neural networks
KW  - hybrid switch method
DO  - 10.3390/rs9080775
TY  - EJOU
AU  - Yan, Yiming
AU  - Tan, Zhichao
AU  - Su, Nan
AU  - Zhao, Chunhui
TI  - Building Extraction Based on an Optimized Stacked Sparse Autoencoder of Structure and Training Samples Using LIDAR DSM and Optical Images
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 9
SN  - 1424-8220

AB  - In this paper, a building extraction method is proposed based on a stacked sparse autoencoder with an optimized structure and training samples. Building extraction plays an important role in urban construction and planning. However, some negative effects will reduce the accuracy of extraction, such as exceeding resolution, bad correction and terrain influence. Data collected by multiple sensors, as light detection and ranging (LIDAR), optical sensor etc., are used to improve the extraction. Using digital surface model (DSM) obtained from LIDAR data and optical images, traditional method can improve the extraction effect to a certain extent, but there are some defects in feature extraction. Since stacked sparse autoencoder (SSAE) neural network can learn the essential characteristics of the data in depth, SSAE was employed to extract buildings from the combined DSM data and optical image. A better setting strategy of SSAE network structure is given, and an idea of setting the number and proportion of training samples for better training of SSAE was presented. The optical data and DSM were combined as input of the optimized SSAE, and after training by an optimized samples, the appropriate network structure can extract buildings with great accuracy and has good robustness.
KW  - stacked sparse autoencoder
KW  - LIDAR DSM
KW  - remote sensing image
KW  - building extraction
DO  - 10.3390/s17091957
TY  - EJOU
AU  - Nguyen, Phong Ha
AU  - Kim, Ki Wan
AU  - Lee, Young Won
AU  - Park, Kang Ryoung
TI  - Remote Marker-Based Tracking for UAV Landing Using Visible-Light Camera Sensor
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 9
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs), which are commonly known as drones, have proved to be useful not only on the battlefields where manned flight is considered too risky or difficult, but also in everyday life purposes such as surveillance, monitoring, rescue, unmanned cargo, aerial video, and photography. More advanced drones make use of global positioning system (GPS) receivers during the navigation and control loop which allows for smart GPS features of drone navigation. However, there are problems if the drones operate in heterogeneous areas with no GPS signal, so it is important to perform research into the development of UAVs with autonomous navigation and landing guidance using computer vision. In this research, we determined how to safely land a drone in the absence of GPS signals using our remote maker-based tracking algorithm based on the visible light camera sensor. The proposed method uses a unique marker designed as a tracking target during landing procedures. Experimental results show that our method significantly outperforms state-of-the-art object trackers in terms of both accuracy and processing time, and we perform test on an embedded system in various environments.
KW  - unmanned aerial vehicle (UAV)
KW  - remote marker-based tracking
KW  - visible light camera sensor
KW  - UAV landing
DO  - 10.3390/s17091987
TY  - EJOU
AU  - Kim, Hyunjun
AU  - Lee, Junhwa
AU  - Ahn, Eunjong
AU  - Cho, Soojin
AU  - Shin, Myoungsu
AU  - Sim, Sung-Han
TI  - Concrete Crack Identification Using a UAV Incorporating Hybrid Image Processing
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 9
SN  - 1424-8220

AB  - Crack assessment is an essential process in the maintenance of concrete structures. In general, concrete cracks are inspected by manual visual observation of the surface, which is intrinsically subjective as it depends on the experience of inspectors. Further, it is time-consuming, expensive, and often unsafe when inaccessible structural members are to be assessed. Unmanned aerial vehicle (UAV) technologies combined with digital image processing have recently been applied to crack assessment to overcome the drawbacks of manual visual inspection. However, identification of crack information in terms of width and length has not been fully explored in the UAV-based applications, because of the absence of distance measurement and tailored image processing. This paper presents a crack identification strategy that combines hybrid image processing with UAV technology. Equipped with a camera, an ultrasonic displacement sensor, and a WiFi module, the system provides the image of cracks and the associated working distance from a target structure on demand. The obtained information is subsequently processed by hybrid image binarization to estimate the crack width accurately while minimizing the loss of the crack length information. The proposed system has shown to successfully measure cracks thicker than 0.1 mm with the maximum length estimation error of 7.3%.
KW  - concrete structure
KW  - crack identification
KW  - digital image processing
KW  - structural health monitoring
KW  - unmanned aerial vehicle
DO  - 10.3390/s17092052
TY  - EJOU
AU  - Meng, Xiaoli
AU  - Wang, Heng
AU  - Liu, Bingbing
TI  - A Robust Vehicle Localization Approach Based on GNSS/IMU/DMI/LiDAR Sensor Fusion for Autonomous Vehicles
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 9
SN  - 1424-8220

AB  - Precise and robust localization in a large-scale outdoor environment is essential for an autonomous vehicle. In order to improve the performance of the fusion of GNSS (Global Navigation Satellite System)/IMU (Inertial Measurement Unit)/DMI (Distance-Measuring Instruments), a multi-constraint fault detection approach is proposed to smooth the vehicle locations in spite of GNSS jumps. Furthermore, the lateral localization error is compensated by the point cloud-based lateral localization method proposed in this paper. Experiment results have verified the algorithms proposed in this paper, which shows that the algorithms proposed in this paper are capable of providing precise and robust vehicle localization.
KW  - sensor fusion
KW  - Unscented Kalman Filter (UKF)
KW  - vehicle localization
DO  - 10.3390/s17092140
TY  - EJOU
AU  - Ejofodomi, O’tega
AU  - Ofualagba, Godswill
TI  - Detection and Classification of Land Crude Oil Spills Using Color Segmentation and Texture Analysis
T2  - Journal of Imaging

PY  - 2017
VL  - 3
IS  - 4
SN  - 2313-433X

AB  - Crude oil spills have negative consequences on the economy, environment, health and society in which they occur, and the severity of the consequences depends on how quickly these spills are detected once they begin. Several methods have been employed for spill detection, including real time remote surveillance by flying aircrafts with surveillance teams. Other methods employ various sensors, including visible sensors. This paper presents an algorithm to automatically detect the presence of crude oil spills in images acquired using visible light sensors. Images of crude oil spills used in the development of the algorithm were obtained from the Shell Petroleum Development Company (SPDC) Nigeria website The major steps of the detection algorithm are image preprocessing, crude oil color segmentation, sky elimination segmentation, Region of Interest (ROI) extraction, ROI texture feature extraction, and ROI texture feature analysis and classification. The algorithm was developed using 25 sample images containing crude oil spills and demonstrated a sensitivity of 92% and an FPI of 1.43. The algorithm was further tested on a set of 56 case images and demonstrated a sensitivity of 82% and an FPI of 0.66. This algorithm can be incorporated into spill detection systems that utilize visible sensors for early detection of crude oil spills.
KW  - crude oil spills
KW  - crude oil spill detection
KW  - image segmentation
KW  - texture feature extraction
KW  - automated spill detection
DO  - 10.3390/jimaging3040047
TY  - EJOU
AU  - Liu, Yuan
AU  - Wang, Jun
AU  - Song, Jingwei
AU  - Song, Zihui
TI  - Globally Consistent Indoor Mapping via a Decoupling Rotation and Translation Algorithm Applied to RGB-D Camera Output
T2  - ISPRS International Journal of Geo-Information

PY  - 2017
VL  - 6
IS  - 11
SN  - 2220-9964

AB  - This paper presents a novel RGB-D 3D reconstruction algorithm for the indoor environment. The method can produce globally-consistent 3D maps for potential GIS applications. As the consumer RGB-D camera provides a noisy depth image, the proposed algorithm decouples the rotation and translation for a more robust camera pose estimation, which makes full use of the information, but also prevents inaccuracies caused by noisy depth measurements. The uncertainty in the image depth is not only related to the camera device, but also the environment; hence, a novel uncertainty model for depth measurements was developed using Gaussian mixture applied to multi-windows. The plane features in the indoor environment contain valuable information about the global structure, which can guide the convergence of camera pose solutions, and plane and feature point constraints are incorporated in the proposed optimization framework. The proposed method was validated using publicly-available RGB-D benchmarks and obtained good quality trajectory and 3D models, which are difficult for traditional 3D reconstruction algorithms.
KW  - RGB-D
KW  - 3D map
KW  - indoor mapping
KW  - SLAM
KW  - 3D reconstruction
DO  - 10.3390/ijgi6110323
TY  - EJOU
AU  - Hoshiba, Kotaro
AU  - Washizaki, Kai
AU  - Wakabayashi, Mizuho
AU  - Ishiki, Takahiro
AU  - Kumon, Makoto
AU  - Bando, Yoshiaki
AU  - Gabriel, Daniel
AU  - Nakadai, Kazuhiro
AU  - Okuno, Hiroshi G.
TI  - Design of UAV-Embedded Microphone Array System for Sound Source Localization in Outdoor Environments
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 11
SN  - 1424-8220

AB  - In search and rescue activities, unmanned aerial vehicles (UAV) should exploit sound information to compensate for poor visual information. This paper describes the design and implementation of a UAV-embedded microphone array system for sound source localization in outdoor environments. Four critical development problems included water-resistance of the microphone array, efficiency in assembling, reliability of wireless communication, and sufficiency of visualization tools for operators. To solve these problems, we developed a spherical microphone array system (SMAS) consisting of a microphone array, a stable wireless network communication system, and intuitive visualization tools. The performance of SMAS was evaluated with simulated data and a demonstration in the field. Results confirmed that the SMAS provides highly accurate localization, water resistance, prompt assembly, stable wireless communication, and intuitive information for observers and operators.
KW  - robot audition
KW  - sound source localization
KW  - multiple signal classification
KW  - outdoor-environment measurement
KW  - real-time measurement
KW  - unmanned aerial vehicle
DO  - 10.3390/s17112535
TY  - EJOU
AU  - Yamamoto, Kyosuke
AU  - Togami, Takashi
AU  - Yamaguchi, Norio
TI  - Super-Resolution of Plant Disease Images for the Acceleration of Image-based Phenotyping and Vigor Diagnosis in Agriculture
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs or drones) are a very promising branch of technology, and they have been utilized in agriculture—in cooperation with image processing technologies—for phenotyping and vigor diagnosis. One of the problems in the utilization of UAVs for agricultural purposes is the limitation in flight time. It is necessary to fly at a high altitude to capture the maximum number of plants in the limited time available, but this reduces the spatial resolution of the captured images. In this study, we applied a super-resolution method to the low-resolution images of tomato diseases to recover detailed appearances, such as lesions on plant organs. We also conducted disease classification using high-resolution, low-resolution, and super-resolution images to evaluate the effectiveness of super-resolution methods in disease classification. Our results indicated that the super-resolution method outperformed conventional image scaling methods in spatial resolution enhancement of tomato disease images. The results of disease classification showed that the accuracy attained was also better by a large margin with super-resolution images than with low-resolution images. These results indicated that our approach not only recovered the information lost in low-resolution images, but also exerted a beneficial influence on further image analysis. The proposed approach will accelerate image-based phenotyping and vigor diagnosis in the field, because it not only saves time to capture images of a crop in a cultivation field but also secures the accuracy of these images for further analysis.
KW  - super-resolution
KW  - deep learning
KW  - convolutional neural network
KW  - disease classification
KW  - agriculture
DO  - 10.3390/s17112557
TY  - EJOU
AU  - Li, Chang
AU  - Ma, Yong
AU  - Mei, Xiaoguang
AU  - Fan, Fan
AU  - Huang, Jun
AU  - Ma, Jiayi
TI  - Sparse Unmixing of Hyperspectral Data with Noise Level Estimation
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 11
SN  - 2072-4292

AB  - Recently, sparse unmixing has received particular attention in the analysis of hyperspectral images (HSIs). However, traditional sparse unmixing ignores the different noise levels in different bands of HSIs, making such methods sensitive to different noise levels. To overcome this problem, the noise levels at different bands are assumed to be different in this paper, and a general sparse unmixing method based on noise level estimation (SU-NLE) under the sparse regression framework is proposed. First, the noise in each band is estimated on the basis of the multiple regression theory in hyperspectral applications, given that neighboring spectral bands are usually highly correlated. Second, the noise weighting matrix can be obtained from the estimated noise. Third, the noise weighting matrix is integrated into the sparse regression unmixing framework, which can alleviate the impact of different noise levels at different bands. Finally, the proposed SU-NLE is solved by the alternative direction method of multipliers. Experiments on synthetic datasets show that the signal-to-reconstruction error of the proposed SU-NLE is considerably higher than those of the corresponding traditional sparse regression unmixing methods without noise level estimation, which demonstrates the efficiency of integrating noise level estimation into the sparse regression unmixing framework. The proposed SU-NLE also shows promising results in real HSIs.
KW  - alternative direction method of multipliers (ADMM)
KW  - hyperspectral image (HSI)
KW  - sparse unmixing method based on noise level estimation (SU-NLE)
DO  - 10.3390/rs9111166
TY  - EJOU
AU  - Tang, Tianyu
AU  - Zhou, Shilin
AU  - Deng, Zhipeng
AU  - Lei, Lin
AU  - Zou, Huanxin
TI  - Arbitrary-Oriented Vehicle Detection in Aerial Imagery with Single Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 11
SN  - 2072-4292

AB  - Vehicle detection with orientation estimation in aerial images has received widespread interest as it is important for intelligent traffic management. This is a challenging task, not only because of the complex background and relatively small size of the target, but also the various orientations of vehicles in aerial images captured from the top view. The existing methods for oriented vehicle detection need several post-processing steps to generate final detection results with orientation, which are not efficient enough. Moreover, they can only get discrete orientation information for each target. In this paper, we present an end-to-end single convolutional neural network to generate arbitrarily-oriented detection results directly. Our approach, named Oriented_SSD (Single Shot MultiBox Detector, SSD), uses a set of default boxes with various scales on each feature map location to produce detection bounding boxes. Meanwhile, offsets are predicted for each default box to better match the object shape, which contain the angle parameter for oriented bounding boxes’ generation. Evaluation results on the public DLR Vehicle Aerial dataset and Vehicle Detection in Aerial Imagery (VEDAI) dataset demonstrate that our method can detect both the location and orientation of the vehicle with high accuracy and fast speed. For test images in the DLR Vehicle Aerial dataset with a size of     5616 × 3744    , our method achieves 76.1% average precision (AP) and 78.7% correct direction classification at 5.17 s on an NVIDIA GTX-1060.
KW  - arbitrary-oriented
KW  - vehicle detection
KW  - single convolutional neural networks (CNN)
KW  - aerial images
KW  - near-real-time
DO  - 10.3390/rs9111170
TY  - EJOU
AU  - Malek, Salim
AU  - Melgani, Farid
AU  - Mekhalfi, Mohamed L.
AU  - Bazi, Yakoub
TI  - Real-Time Indoor Scene Description for the Visually Impaired Using Autoencoder Fusion Strategies with Visible Cameras
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 11
SN  - 1424-8220

AB  - This paper describes three coarse image description strategies, which are meant to promote a rough perception of surrounding objects for visually impaired individuals, with application to indoor spaces. The described algorithms operate on images (grabbed by the user, by means of a chest-mounted camera), and provide in output a list of objects that likely exist in his context across the indoor scene. In this regard, first, different colour, texture, and shape-based feature extractors are generated, followed by a feature learning step by means of AutoEncoder (AE) models. Second, the produced features are fused and fed into a multilabel classifier in order to list the potential objects. The conducted experiments point out that fusing a set of AE-learned features scores higher classification rates with respect to using the features individually. Furthermore, with respect to reference works, our method: (i) yields higher classification accuracies, and (ii) runs (at least four times) faster, which enables a potential full real-time application.
KW  - assistive technologies
KW  - visible cameras
KW  - visually impaired (VI) people
KW  - coarse scene description
KW  - multiobject recognition
KW  - deep learning
KW  - feature fusion
KW  - image representation
DO  - 10.3390/s17112641
TY  - EJOU
AU  - Cai, Bowen
AU  - Jiang, Zhiguo
AU  - Zhang, Haopeng
AU  - Zhao, Danpei
AU  - Yao, Yuan
TI  - Airport Detection Using End-to-End Convolutional Neural Network with Hard Example Mining
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 11
SN  - 2072-4292

AB  - Deep convolutional neural network (CNN) achieves outstanding performance in the field of target detection. As one of the most typical targets in remote sensing images (RSIs), airport has attracted increasing attention in recent years. However, the essential challenge for using deep CNN to detect airport is the great imbalance between the number of airports and background examples in large-scale RSIs, which may lead to over-fitting. In this paper, we develop a hard example mining and weight-balanced strategy to construct a novel end-to-end convolutional neural network for airport detection. The initial motivation of the proposed method is that backgrounds contain an overwhelming number of easy examples and a few hard examples. Therefore, we design a hard example mining layer to automatically select hard examples by their losses, and implement a new weight-balanced loss function to optimize CNN. Meanwhile, the cascade design of proposal extraction and object detection in our network releases the constraint on input image size and reduces spurious false positives. Compared with geometric characteristics and low-level manually designed features, the hard example mining based network could extract high-level features, which is more robust for airport detection in complex environment. The proposed method is validated on a multi-scale dataset with complex background collected from Google Earth. The experimental results demonstrate that our proposed method is robust, and superior to the state-of-the-art airport detection models.
KW  - airport detection
KW  - hard example mining
KW  - convolutional neural network
KW  - region proposal network
DO  - 10.3390/rs9111198
TY  - EJOU
AU  - Zhong, Jiandan
AU  - Lei, Tao
AU  - Yao, Guangle
TI  - Robust Vehicle Detection in Aerial Images Based on Cascaded Convolutional Neural Networks
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 12
SN  - 1424-8220

AB  - Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.
KW  - vehicle detection
KW  - convolutional neural network
KW  - aerial image
KW  - deep learning
DO  - 10.3390/s17122720
TY  - EJOU
AU  - Su, Jinya
AU  - Yi, Dewei
AU  - Liu, Cunjia
AU  - Guo, Lei
AU  - Chen, Wen-Hua
TI  - Dimension Reduction Aided Hyperspectral Image Classification with a Small-sized Training Dataset: Experimental Comparisons
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 12
SN  - 1424-8220

AB  - Hyperspectral images (HSI) provide rich information which may not be captured by other sensing technologies and therefore gradually find a wide range of applications. However, they also generate a large amount of irrelevant or redundant data for a specific task. This causes a number of issues including significantly increased computation time, complexity and scale of prediction models mapping the data to semantics (e.g., classification), and the need of a large amount of labelled data for training. Particularly, it is generally difficult and expensive for experts to acquire sufficient training samples in many applications. This paper addresses these issues by exploring a number of classical dimension reduction algorithms in machine learning communities for HSI classification. To reduce the size of training dataset, feature selection (e.g., mutual information, minimal redundancy maximal relevance) and feature extraction (e.g., Principal Component Analysis (PCA), Kernel PCA) are adopted to augment a baseline classification method, Support Vector Machine (SVM). The proposed algorithms are evaluated using a real HSI dataset. It is shown that PCA yields the most promising performance in reducing the number of features or spectral bands. It is observed that while significantly reducing the computational complexity, the proposed method can achieve better classification results over the classic SVM on a small training dataset, which makes it suitable for real-time applications or when only limited training data are available. Furthermore, it can also achieve performances similar to the classic SVM on large datasets but with much less computing time.
KW  - feature extraction/selection
KW  - image classification
KW  - Hyperspectral image
KW  - PCA
KW  - SVM
DO  - 10.3390/s17122726
TY  - EJOU
AU  - Guirado, Emilio
AU  - Tabik, Siham
AU  - Alcaraz-Segura, Domingo
AU  - Cabello, Javier
AU  - Herrera, Francisco
TI  - Deep-learning Versus OBIA for Scattered Shrub Detection with Google Earth Imagery: Ziziphus lotus as Case Study
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 12
SN  - 2072-4292

AB  - There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been traditionally performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image often cannot be extrapolated to a different image. Recently, deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in computer vision and are offering promising results in land cover mapping. This paper analyzes the potential of CNN-based methods for detection of plant species of conservation concern using free high-resolution Google Earth     TM     images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. Compared to the best performing OBIA-method, the best CNN-detector achieved up to 12% better precision, up to 30% better recall and up to 20% better balance between precision and recall. Besides, the knowledge that CNNs acquired in the first image can be re-utilized in other regions, which makes the detection process very fast. A natural conclusion of this work is that including CNN-models as classifiers, e.g., ResNet-classifier, could further improve OBIA methods. The provided methodology can be systematically reproduced for other species detection using our codes available through (https://github.com/EGuirado/CNN-remotesensing).
KW  - Ziziphus lotus
KW  - plant species detection
KW  - land cover mapping
KW  - Convolutional Neural Networks (CNNs)
KW  - Object-Based Image Analysis (OBIA)
KW  - remote sensing
DO  - 10.3390/rs9121220
TY  - EJOU
AU  - Zhang, Wei
AU  - Wei, Shilin
AU  - Teng, Yanbin
AU  - Zhang, Jianku
AU  - Wang, Xiufang
AU  - Yan, Zheping
TI  - Dynamic Obstacle Avoidance for Unmanned Underwater Vehicles Based on an Improved Velocity Obstacle Method
T2  - Sensors

PY  - 2017
VL  - 17
IS  - 12
SN  - 1424-8220

AB  - In view of a dynamic obstacle environment with motion uncertainty, we present a dynamic collision avoidance method based on the collision risk assessment and improved velocity obstacle method. First, through the fusion optimization of forward-looking sonar data, the redundancy of the data is reduced and the position, size and velocity information of the obstacles are obtained, which can provide an accurate decision-making basis for next-step collision avoidance. Second, according to minimum meeting time and the minimum distance between the obstacle and unmanned underwater vehicle (UUV), this paper establishes the collision risk assessment model, and screens key obstacles to avoid collision. Finally, the optimization objective function is established based on the improved velocity obstacle method, and a UUV motion characteristic is used to calculate the reachable velocity sets. The optimal collision speed of UUV is searched in velocity space. The corresponding heading and speed commands are calculated, and outputted to the motion control module. The above is the complete dynamic obstacle avoidance process. The simulation results show that the proposed method can obtain a better collision avoidance effect in the dynamic environment, and has good adaptability to the unknown dynamic environment.
KW  - unmanned underwater vehicle
KW  - velocity obstacle method
KW  - dynamic collision avoidance
KW  - forward-looking sonar
DO  - 10.3390/s17122742
TY  - EJOU
AU  - Yu, Huai
AU  - Yang, Wen
AU  - Hua, Guang
AU  - Ru, Hui
AU  - Huang, Pingping
TI  - Change Detection Using High Resolution Remote Sensing Images Based on Active Learning and Markov Random Fields
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 12
SN  - 2072-4292

AB  - Change detection has been widely used in remote sensing, such as for disaster assessment and urban expansion detection. Although it is convenient to use unsupervised methods to detect changes from multi-temporal images, the results could be further improved. In supervised methods, heavy data labelling tasks are needed, and the sample annotation process with real categories is tedious and costly. To relieve the burden of labelling and to obtain satisfactory results, we propose an interactive change detection framework based on active learning and Markov random field (MRF). More specifically, a limited number of representative objects are found in an unsupervised way at the beginning. Then, the very limited samples are labelled as “change” or “no change” to train a simple binary classification model, i.e., a Gaussian process model. By using this model, we then select and label the most informative samples by “the easiest” sample selection strategy to update the former weak classification model until the detection results do not change notably. Finally, the maximum a posteriori (MAP) change detection is efficiently computed via the min-cut-based integer optimization algorithm. The time consuming and laborious manual labelling process can be reduced substantially, and a desirable detection result can be obtained. The experiments on several WorldView-2 images demonstrate the effectiveness of the proposed method.
KW  - high resolution remote sensing images
KW  - change detection
KW  - active learning
KW  - gaussian processes
KW  - Markov random fields
DO  - 10.3390/rs9121233
TY  - EJOU
AU  - Chen, Suting
AU  - Li, Xin
AU  - Zhang, Yanyan
AU  - Feng, Rui
AU  - Zhang, Chuang
TI  - Local Deep Hashing Matching of Aerial Images Based on Relative Distance and Absolute Distance Constraints
T2  - Remote Sensing

PY  - 2017
VL  - 9
IS  - 12
SN  - 2072-4292

AB  - Aerial images have features of high resolution, complex background, and usually require large amounts of calculation, however, most algorithms used in matching of aerial images adopt the shallow hand-crafted features expressed as floating-point descriptors (e.g., SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features)), which may suffer from poor matching speed and are not well represented in the literature. Here, we propose a novel Local Deep Hashing Matching (LDHM) method for matching of aerial images with large size and with lower complexity or fast matching speed. The basic idea of the proposed algorithm is to utilize the deep network model in the local area of the aerial images, and study the local features, as well as the hash function of the images. Firstly, according to the course overlap rate of aerial images, the algorithm extracts the local areas for matching to avoid the processing of redundant information. Secondly, a triplet network structure is proposed to mine the deep features of the patches of the local image, and the learned features are imported to the hash layer, thus obtaining the representation of a binary hash code. Thirdly, the constraints of the positive samples to the absolute distance are added on the basis of the triplet loss, a new objective function is constructed to optimize the parameters of the network and enhance the discriminating capabilities of image patch features. Finally, the obtained deep hash code of each image patch is used for the similarity comparison of the image patches in the Hamming space to complete the matching of aerial images. The proposed LDHM algorithm evaluates the UltraCam-D dataset and a set of actual aerial images, simulation result demonstrates that it may significantly outperform the state-of-the-art algorithm in terms of the efficiency and performance.
KW  - aerial matching
KW  - overlap rate
KW  - deep learning
KW  - local features
KW  - hash learning
KW  - absolute distance constraints
DO  - 10.3390/rs9121244
TY  - EJOU
AU  - Latif, Siddique
AU  - Qadir, Junaid
AU  - Farooq, Shahzad
AU  - Imran, Muhammad A.
TI  - How 5G Wireless (and Concomitant Technologies) Will Revolutionize Healthcare?
T2  - Future Internet

PY  - 2017
VL  - 9
IS  - 4
SN  - 1999-5903

AB  - The need to have equitable access to quality healthcare is enshrined in the United Nations (UN) Sustainable Development Goals (SDGs), which defines the developmental agenda of the UN for the next 15 years. In particular, the third SDG focuses on the need to “ensure healthy lives and promote well-being for all at all ages”. In this paper, we build the case that 5G wireless technology, along with concomitant emerging technologies (such as IoT, big data, artificial intelligence and machine learning), will transform global healthcare systems in the near future. Our optimism around 5G-enabled healthcare stems from a confluence of significant technical pushes that are already at play: apart from the availability of high-throughput low-latency wireless connectivity, other significant factors include the democratization of computing through cloud computing; the democratization of Artificial Intelligence (AI) and cognitive computing (e.g., IBM Watson); and the commoditization of data through crowdsourcing and digital exhaust. These technologies together can finally crack a dysfunctional healthcare system that has largely been impervious to technological innovations. We highlight the persistent deficiencies of the current healthcare system and then demonstrate how the 5G-enabled healthcare revolution can fix these deficiencies. We also highlight open technical research challenges, and potential pitfalls, that may hinder the development of such a 5G-enabled health revolution.
KW  - healthcare
KW  - 5G
KW  - Internet of Things
KW  - big data analytics
KW  - artificial intelligence and machine learning
DO  - 10.3390/fi9040093
TY  - EJOU
AU  - Weinmann, Martin
AU  - Weinmann, Michael
TI  - Geospatial Computer Vision Based on Multi-Modal Data—How Valuable Is Shape Information for the Extraction of Semantic Information?
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - In this paper, we investigate the value of different modalities and their combination for the analysis of geospatial data of low spatial resolution. For this purpose, we present a framework that allows for the enrichment of geospatial data with additional semantics based on given color information, hyperspectral information, and shape information. While the different types of information are used to define a variety of features, classification based on these features is performed using a random forest classifier. To draw conclusions about the relevance of different modalities and their combination for scene analysis, we present and discuss results which have been achieved with our framework on the MUUFL Gulfport Hyperspectral and LiDAR Airborne Data Set.
KW  - geospatial computer vision
KW  - multi-modal data
KW  - 3D point cloud
KW  - shape information
KW  - hyperspectral imagery
KW  - feature extraction
KW  - semantic classification
KW  - semantic information
DO  - 10.3390/rs10010002
TY  - EJOU
AU  - Fan, Junqing
AU  - Yan, Jining
AU  - Ma, Yan
AU  - Wang, Lizhe
TI  - Big Data Integration in Remote Sensing across a Distributed Metadata-Based Spatial Infrastructure
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - Since Landsat-1 first started to deliver volumes of pixels in 1972, the volumes of archived data in remote sensing data centers have increased continuously. Due to various satellite orbit parameters and the specifications of different sensors, the storage formats, projections, spatial resolutions, and revisit periods of these archived data are vastly different. In addition, the remote sensing data received continuously by each data center arrives at a faster code rate; it is best to ingest and archive the newly received data to ensure users have access to the latest data retrieval and distribution services. Hence, an excellent data integration, organization, and management program is urgently needed. However, the multi-source, massive, heterogeneous, and distributed storage features of remote sensing data have not only caused difficulties for integration across distributed data center spatial infrastructures, but have also resulted in the current modes of data organization and management being unable meet the rapid retrieval and access requirements of users. Hence, this paper proposes an object-oriented data technology (OODT) and SolrCloud-based remote sensing data integration and management framework across a distributed data center spatial infrastructure. In this framework, all of the remote sensing metadata in the distributed sub-centers are transformed into the International Standardization Organization (ISO) 19115-based unified format, and then ingested and transferred to the main center by OODT components, continuously or at regular intervals. In the main data center, in order to improve the efficiency of massive data retrieval, we proposed a logical segmentation indexing (LSI) model-based data organization approach, and took SolrCloud to realize the distributed index and retrieval of massive metadata. Finally, a series of distributed data integration, retrieval, and comparative experiments showed that our proposed distributed data integration and management program is effective and promises superior results. Specifically, the LSI model-based data organization and the SolrCloud-based distributed indexing schema was able to effectively improve the efficiency of massive data retrieval.
KW  - multi-sourced remote sensing big data
KW  - data integration
KW  - data management
KW  - distributed data centers
KW  - OODT
DO  - 10.3390/rs10010007
TY  - EJOU
AU  - Chen, Weitao
AU  - Li, Xianju
AU  - He, Haixia
AU  - Wang, Lizhe
TI  - A Review of Fine-Scale Land Use and Land Cover Classification in Open-Pit Mining Areas by Remote Sensing Techniques
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - Over recent decades, fine-scale land use and land cover classification in open-pit mine areas (LCCMA) has become very important for understanding the influence of mining activities on the regional geo-environment, and for environmental impact assessment procedure. This research reviews advances in fine-scale LCCMA from the following aspects. Firstly, it analyzes and proposes classification thematic resolution for LCCMA. Secondly, remote sensing data sources, features, feature selection methods, and classification algorithms for LCCMA are summarized. Thirdly, three major factors that affect LCCMA are discussed: significant three-dimensional terrain features, strong LCCMA feature variability, and homogeneity of spectral-spatial features. Correspondingly, three key scientific issues that limit the accuracy of LCCMA are presented. Finally, several future research directions are discussed: (1) unitization of new sensors, particularly those with stereo survey ability; (2) procurement of sensitive features by new sensors and combinations of sensitive features using novel feature selection methods; (3) development of robust and self-adjusted classification algorithms, such as ensemble learning and deep learning for LCCMA; and (4) application of fine-scale mining information for regularity and management of mines.
KW  - land cover classification
KW  - open-pit mining area
KW  - remote sensing
KW  - review
KW  - fine-scale
DO  - 10.3390/rs10010015
TY  - EJOU
AU  - Ji, Shunping
AU  - Zhang, Chi
AU  - Xu, Anjian
AU  - Shi, Yun
AU  - Duan, Yulin
TI  - 3D Convolutional Neural Networks for Crop Classification with Multi-Temporal Remote Sensing Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - This study describes a novel three-dimensional (3D) convolutional neural networks (CNN) based method that automatically classifies crops from spatio-temporal remote sensing images. First, 3D kernel is designed according to the structure of multi-spectral multi-temporal remote sensing data. Secondly, the 3D CNN framework with fine-tuned parameters is designed for training 3D crop samples and learning spatio-temporal discriminative representations, with the full crop growth cycles being preserved. In addition, we introduce an active learning strategy to the CNN model to improve labelling accuracy up to a required threshold with the most efficiency. Finally, experiments are carried out to test the advantage of the 3D CNN, in comparison to the two-dimensional (2D) CNN and other conventional methods. Our experiments show that the 3D CNN is especially suitable in characterizing the dynamics of crop growth and outperformed the other mainstream methods.
KW  - 3D convolution
KW  - convolutional neural networks
KW  - crop classification
KW  - multi-temporal remote sensing images
KW  - active learning
DO  - 10.3390/rs10010075
TY  - EJOU
AU  - Li, Hongguang
AU  - Shi, Yang
AU  - Zhang, Baochang
AU  - Wang, Yufeng
TI  - Superpixel-Based Feature for Aerial Image Scene Recognition
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 1
SN  - 1424-8220

AB  - Image scene recognition is a core technology for many aerial remote sensing applications. Different landforms are inputted as different scenes in aerial imaging, and all landform information is regarded as valuable for aerial image scene recognition. However, the conventional features of the Bag-of-Words model are designed using local points or other related information and thus are unable to fully describe landform areas. This limitation cannot be ignored when the aim is to ensure accurate aerial scene recognition. A novel superpixel-based feature is proposed in this study to characterize aerial image scenes. Then, based on the proposed feature, a scene recognition method of the Bag-of-Words model for aerial imaging is designed. The proposed superpixel-based feature that utilizes landform information establishes top-task superpixel extraction of landforms to bottom-task expression of feature vectors. This characterization technique comprises the following steps: simple linear iterative clustering based superpixel segmentation, adaptive filter bank construction, Lie group-based feature quantification, and visual saliency model-based feature weighting. Experiments of image scene recognition are carried out using real image data captured by an unmanned aerial vehicle (UAV). The recognition accuracy of the proposed superpixel-based feature is 95.1%, which is higher than those of scene recognition algorithms based on other local features.
KW  - superpixel-based feature
KW  - image scene recognition
KW  - aerial remote sensing
DO  - 10.3390/s18010156
TY  - EJOU
AU  - Kim, Sungho
AU  - Song, Woo-Jin
AU  - Kim, So-Hyun
TI  - Double Weight-Based SAR and Infrared Sensor Fusion for Automatic Ground Target Recognition with Deep Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 1
SN  - 2072-4292

AB  - This paper presents a novel double weight-based synthetic aperture radar (SAR) and infrared (IR) sensor fusion method (DW-SIF) for automatic ground target recognition (ATR). IR-based ATR can provide accurate recognition because of its high image resolution but it is affected by the weather conditions. On the other hand, SAR-based ATR shows a low recognition rate due to the noisy low resolution but can provide consistent performance regardless of the weather conditions. The fusion of an active sensor (SAR) and a passive sensor (IR) can lead to upgraded performance. This paper proposes a doubly weighted neural network fusion scheme at the decision level. The first weight (   α   ) can measure the offline sensor confidence per target category based on the classification rate for an evaluation set. The second weight (   β   ) can measure the online sensor reliability based on the score distribution for a test target image. The LeNet architecture-based deep convolution network (14 layers) is used as an individual classifier. Doubly weighted sensor scores are fused by two types of fusion schemes, such as the sum-based linear fusion scheme (    α β    -sum) and neural network-based nonlinear fusion scheme (    α β    -NN). The experimental results confirmed the proposed linear fusion method (    α β    -sum) to have the best performance among the linear fusion schemes available (SAR-CNN, IR-CNN,    α   -sum,    β   -sum,     α β    -sum, and Bayesian fusion). In addition, the proposed nonlinear fusion method (    α β    -NN) showed superior target recognition performance to linear fusion on the OKTAL-SE-based synthetic database.
KW  - SAR
KW  - IR
KW  - fusion
KW  - double weights
KW  - linear
KW  - nonlinear
KW  - deep learning
KW  - OKTAL-SE
DO  - 10.3390/rs10010072
TY  - EJOU
AU  - Qu, Yufu
AU  - Huang, Jianyu
AU  - Zhang, Xuan
TI  - Rapid 3D Reconstruction for Image Sequence Acquired from UAV Camera
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 1
SN  - 1424-8220

AB  - In order to reconstruct three-dimensional (3D) structures from an image sequence captured by unmanned aerial vehicles’ camera (UAVs) and improve the processing speed, we propose a rapid 3D reconstruction method that is based on an image queue, considering the continuity and relevance of UAV camera images. The proposed approach first compresses the feature points of each image into three principal component points by using the principal component analysis method. In order to select the key images suitable for 3D reconstruction, the principal component points are used to estimate the interrelationships between images. Second, these key images are inserted into a fixed-length image queue. The positions and orientations of the images are calculated, and the 3D coordinates of the feature points are estimated using weighted bundle adjustment. With this structural information, the depth maps of these images can be calculated. Next, we update the image queue by deleting some of the old images and inserting some new images into the queue, and a structural calculation of all the images can be performed by repeating the previous steps. Finally, a dense 3D point cloud can be obtained using the depth–map fusion method. The experimental results indicate that when the texture of the images is complex and the number of images exceeds 100, the proposed method can improve the calculation speed by more than a factor of four with almost no loss of precision. Furthermore, as the number of images increases, the improvement in the calculation speed will become more noticeable.
KW  - UAV camera
KW  - multi-view stereo
KW  - structure from motion
KW  - 3D reconstruction
KW  - point cloud
DO  - 10.3390/s18010225
TY  - EJOU
AU  - Mahabir, Ron
AU  - Croitoru, Arie
AU  - Crooks, Andrew T.
AU  - Agouris, Peggy
AU  - Stefanidis, Anthony
TI  - A Critical Review of High and Very High-Resolution Remote Sensing Approaches for Detecting and Mapping Slums: Trends, Challenges and Emerging Opportunities
T2  - Urban Science

PY  - 2018
VL  - 2
IS  - 1
SN  - 2413-8851

AB  - Slums are a global urban challenge, with less developed countries being particularly impacted. To adequately detect and map them, data is needed on their location, spatial extent and evolution. High- and very high-resolution remote sensing imagery has emerged as an important source of data in this regard. The purpose of this paper is to critically review studies that have used such data to detect and map slums. Our analysis shows that while such studies have been increasing over time, they tend to be concentrated to a few geographical areas and often focus on the use of a single approach (e.g., image texture and object-based image analysis), thus limiting generalizability to understand slums, their population, and evolution within the global context. We argue that to develop a more comprehensive framework that can be used to detect and map slums, other emerging sourcing of geospatial data should be considered (e.g., volunteer geographic information) in conjunction with growing trends and advancements in technology (e.g., geosensor networks). Through such data integration and analysis we can then create a benchmark for determining the most suitable methods for mapping slums in a given locality, thus fostering the creation of new approaches to address this challenge.
KW  - high-and very high-resolution imagery
KW  - remote sensing
KW  - slums
KW  - volunteer geographic information
KW  - geosensor networks
KW  - image analysis
DO  - 10.3390/urbansci2010008
TY  - EJOU
AU  - Feng, Yu
AU  - Sester, Monika
TI  - Extraction of Pluvial Flood Relevant Volunteered Geographic Information (VGI) by Deep Learning from User Generated Texts and Photos
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 2
SN  - 2220-9964

AB  - In recent years, pluvial floods caused by extreme rainfall events have occurred frequently. Especially in urban areas, they lead to serious damages and endanger the citizens’ safety. Therefore, real-time information about such events is desirable. With the increasing popularity of social media platforms, such as Twitter or Instagram, information provided by voluntary users becomes a valuable source for emergency response. Many applications have been built for disaster detection and flood mapping using crowdsourcing. Most of the applications so far have merely used keyword filtering or classical language processing methods to identify disaster relevant documents based on user generated texts. As the reliability of social media information is often under criticism, the precision of information retrieval plays a significant role for further analyses. Thus, in this paper, high quality eyewitnesses of rainfall and flooding events are retrieved from social media by applying deep learning approaches on user generated texts and photos. Subsequently, events are detected through spatiotemporal clustering and visualized together with these high quality eyewitnesses in a web map application. Analyses and case studies are conducted during flooding events in Paris, London and Berlin.
KW  - social media
KW  - crowdsourcing
KW  - volunteered geographic information
KW  - multimedia information retrieval
KW  - convolutional neural network
KW  - transfer learning
KW  - word embedding
KW  - flood mapping
DO  - 10.3390/ijgi7020039
TY  - EJOU
AU  - Mueller, Markus S.
AU  - Jutzi, Boris
TI  - UAS Navigation with SqueezePoseNet—Accuracy Boosting for Pose Regression by Data Augmentation
T2  - Drones

PY  - 2018
VL  - 2
IS  - 1
SN  - 2504-446X

AB  - The navigation of Unmanned Aerial Vehicles (UAVs) nowadays is mostly based on Global Navigation Satellite Systems (GNSSs). Drawbacks of satellite-based navigation are failures caused by occlusions or multi-path interferences. Therefore, alternative methods have been developed in recent years. Visual navigation methods such as Visual Odometry (VO) or visual Simultaneous Localization and Mapping (SLAM) aid global navigation solutions by closing trajectory gaps or performing loop closures. However, if the trajectory estimation is interrupted or not available, a re-localization is mandatory. Furthermore, the latest research has shown promising results on pose regression in 6 Degrees of Freedom (DoF) based on Convolutional Neural Networks (CNNs). Additionally, existing navigation methods can benefit from these networks. In this article, a method for GNSS-free and fast image-based pose regression by utilizing a small Convolutional Neural Network is presented. Therefore, a small CNN (SqueezePoseNet) is utilized, transfer learning is applied and the network is tuned for pose regression. Furthermore, recent drawbacks are overcome by applying data augmentation on a training dataset utilizing simulated images. Experiments with small CNNs show promising results for GNSS-free and fast localization compared to larger networks. By training a CNN with an extended data set including simulated images, the accuracy on pose regression is improved up to 61.7% for position and up to 76.0% for rotation compared to training on a standard not-augmented data set.
KW  - convolutional neural networks
KW  - data augmentation
KW  - image-based navigation
KW  - pose estimation
DO  - 10.3390/drones2010007
TY  - EJOU
AU  - Yang, Liping
AU  - MacEachren, Alan M.
AU  - Mitra, Prasenjit
AU  - Onorati, Teresa
TI  - Visually-Enabled Active Deep Learning for (Geo) Text and Image Classification: A Review
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 2
SN  - 2220-9964

AB  - This paper investigates recent research on active learning for (geo) text and image classification, with an emphasis on methods that combine visual analytics and/or deep learning. Deep learning has attracted substantial attention across many domains of science and practice, because it can find intricate patterns in big data; but successful application of the methods requires a big set of labeled data. Active learning, which has the potential to address the data labeling challenge, has already had success in geospatial applications such as trajectory classification from movement data and (geo) text and image classification. This review is intended to be particularly relevant for extension of these methods to GISience, to support work in domains such as geographic information retrieval from text and image repositories, interpretation of spatial language, and related geo-semantics challenges. Specifically, to provide a structure for leveraging recent advances, we group the relevant work into five categories: active learning, visual analytics, active learning with visual analytics, active deep learning, plus GIScience and Remote Sensing (RS) using active learning and active deep learning. Each category is exemplified by recent influential work. Based on this framing and our systematic review of key research, we then discuss some of the main challenges of integrating active learning with visual analytics and deep learning, and point out research opportunities from technical and application perspectives—for application-based opportunities, with emphasis on those that address big data with geospatial components.
KW  - visual analytics
KW  - human-centered computing
KW  - active learning
KW  - deep learning
KW  - machine learning
KW  - multi-class classification
KW  - multi-label classification
KW  - text classification
KW  - image classification
KW  - geographic information retrieval
DO  - 10.3390/ijgi7020065
TY  - EJOU
AU  - Bashmal, Laila
AU  - Bazi, Yakoub
AU  - AlHichri, Haikel
AU  - AlRahhal, Mohamad M.
AU  - Ammour, Nassim
AU  - Alajlan, Naif
TI  - Siamese-GAN: Learning Invariant Representations for Aerial Vehicle Image Categorization
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 2
SN  - 2072-4292

AB  - In this paper, we present a new algorithm for cross-domain classification in aerial vehicle images based on generative adversarial networks (GANs). The proposed method, called Siamese-GAN, learns invariant feature representations for both labeled and unlabeled images coming from two different domains. To this end, we train in an adversarial manner a Siamese encoder–decoder architecture coupled with a discriminator network. The encoder–decoder network has the task of matching the distributions of both domains in a shared space regularized by the reconstruction ability, while the discriminator seeks to distinguish between them. After this phase, we feed the resulting encoded labeled and unlabeled features to another network composed of two fully-connected layers for training and classification, respectively. Experiments on several cross-domain datasets composed of extremely high resolution (EHR) images acquired by manned/unmanned aerial vehicles (MAV/UAV) over the cities of Vaihingen, Toronto, Potsdam, and Trento are reported and discussed.
KW  - manned/unmanned aerial vehicles (MAV/UAV)
KW  - extremely high resolution (EHR) images
KW  - distribution mismatch
KW  - generative adversarial networks (GANs)
KW  - Siamese encoder–decoder
DO  - 10.3390/rs10020351
TY  - EJOU
AU  - Guo, Qiangliang
AU  - Xiao, Jin
AU  - Hu, Xiaoguang
TI  - New Keypoint Matching Method Using Local Convolutional Features for Power Transmission Line Icing Monitoring
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - Power transmission line icing (PTLI) problems, which cause tremendous damage to the power grids, has drawn much attention. Existing three-dimensional measurement methods based on binocular stereo vision was recently introduced to measure the ice thickness in PTLI, but failed to meet requirements of practical applications due to inefficient keypoint matching in the complex PTLI scene. In this paper, a new keypoint matching method is proposed based on the local multi-layer convolutional neural network (CNN) features, termed Local Convolutional Features (LCFs). LCFs are deployed to extract more discriminative features than the conventional CNNs. Particularly in LCFs, a multi-layer features fusion scheme is exploited to boost the matching performance. Together with a location constraint method, the correspondence of neighboring keypoints is further refined. Our approach achieves 1.5%, 5.3%, 13.1%, 27.3% improvement in the average matching precision compared with SIFT, SURF, ORB and MatchNet on the public Middlebury dataset, and the measurement accuracy of ice thickness can reach 90.9% compared with manual measurement on the collected PTLI dataset.
KW  - power transmission line icing
KW  - keypoint matching
KW  - convolutional neural network
KW  - feature fusion
KW  - location constraint
DO  - 10.3390/s18030698
TY  - EJOU
AU  - Zhao, Yi
AU  - Ma, Jiale
AU  - Li, Xiaohui
AU  - Zhang, Jie
TI  - Saliency Detection and Deep Learning-Based Wildfire Identification in UAV Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - An unmanned aerial vehicle (UAV) equipped with global positioning systems (GPS) can provide direct georeferenced imagery, mapping an area with high resolution. So far, the major difficulty in wildfire image classification is the lack of unified identification marks, the fire features of color, shape, texture (smoke, flame, or both) and background can vary significantly from one scene to another. Deep learning (e.g., DCNN for Deep Convolutional Neural Network) is very effective in high-level feature learning, however, a substantial amount of training images dataset is obligatory in optimizing its weights value and coefficients. In this work, we proposed a new saliency detection algorithm for fast location and segmentation of core fire area in aerial images. As the proposed method can effectively avoid feature loss caused by direct resizing; it is used in data augmentation and formation of a standard fire image dataset ‘UAV_Fire’. A 15-layered self-learning DCNN architecture named ‘Fire_Net’ is then presented as a self-learning fire feature exactor and classifier. We evaluated different architectures and several key parameters (drop out ratio, batch size, etc.) of the DCNN model regarding its validation accuracy. The proposed architecture outperformed previous methods by achieving an overall accuracy of 98%. Furthermore, ‘Fire_Net’ guarantied an average processing speed of 41.5 ms per image for real-time wildfire inspection. To demonstrate its practical utility, Fire_Net is tested on 40 sampled images in wildfire news reports and all of them have been accurately identified.
KW  - UAV
KW  - wildfire
KW  - deep learning
KW  - saliency detection
DO  - 10.3390/s18030712
TY  - EJOU
AU  - Cao, Xiaoguang
AU  - Wang, Peng
AU  - Meng, Cai
AU  - Bai, Xiangzhi
AU  - Gong, Guoping
AU  - Liu, Miaoming
AU  - Qi, Jun
TI  - Region Based CNN for Foreign Object Debris Detection on Airfield Pavement
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - In this paper, a novel algorithm based on convolutional neural network (CNN) is proposed to detect foreign object debris (FOD) based on optical imaging sensors. It contains two modules, the improved region proposal network (RPN) and spatial transformer network (STN) based CNN classifier. In the improved RPN, some extra select rules are designed and deployed to generate high quality candidates with fewer numbers. Moreover, the efficiency of CNN detector is significantly improved by introducing STN layer. Compared to faster R-CNN and single shot multiBox detector (SSD), the proposed algorithm achieves better result for FOD detection on airfield pavement in the experiment.
KW  - foreign object debris
KW  - object detection
KW  - convolutional neural network
KW  - vehicular imaging sensors
DO  - 10.3390/s18030737
TY  - EJOU
AU  - Li, Jiaojiao
AU  - Xi, Bobo
AU  - Li, Yunsong
AU  - Du, Qian
AU  - Wang, Keyan
TI  - Hyperspectral Classification Based on Texture Feature Enhancement and Deep Belief Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 3
SN  - 2072-4292

AB  - With success of Deep Belief Networks (DBNs) in computer vision, DBN has attracted great attention in hyperspectral classification. Many deep learning based algorithms have been focused on deep feature extraction for classification improvement. Multi-features, such as texture feature, are widely utilized in classification process to enhance classification accuracy greatly. In this paper, a novel hyperspectral classification framework based on an optimal DBN and a novel texture feature enhancement (TFE) is proposed. Through band grouping, sample band selection and guided filtering, the texture features of hyperspectral data are improved. After TFE, the optimal DBN is employed on the hyperspectral reconstructed data for feature extraction and classification. Experimental results demonstrate that the proposed classification framework outperforms some state-of-the-art classification algorithms, and it can achieve outstanding hyperspectral classification performance. Furthermore, our proposed TFE method can play a significant role in improving classification accuracy.
KW  - deep belief networks
KW  - deep learning
KW  - texture feature enhancement
KW  - hyperspectral classification
KW  - band grouping
DO  - 10.3390/rs10030396
TY  - EJOU
AU  - Jin, Xue-Bo
AU  - Su, Ting-Li
AU  - Kong, Jian-Lei
AU  - Bai, Yu-Ting
AU  - Miao, Bei-Bei
AU  - Dou, Chao
TI  - State-of-the-Art Mobile Intelligence: Enabling Robots to Move Like Humans by Estimating Mobility with Artificial Intelligence
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 3
SN  - 2076-3417

AB  - Mobility is a significant robotic task. It is the most important function when robotics is applied to domains such as autonomous cars, home service robots, and autonomous underwater vehicles. Despite extensive research on this topic, robots still suffer from difficulties when moving in complex environments, especially in practical applications. Therefore, the ability to have enough intelligence while moving is a key issue for the success of robots. Researchers have proposed a variety of methods and algorithms, including navigation and tracking. To help readers swiftly understand the recent advances in methodology and algorithms for robot movement, we present this survey, which provides a detailed review of the existing methods of navigation and tracking. In particular, this survey features a relation-based architecture that enables readers to easily grasp the key points of mobile intelligence. We first outline the key problems in robot systems and point out the relationship among robotics, navigation, and tracking. We then illustrate navigation using different sensors and the fusion methods and detail the state estimation and tracking models for target maneuvering. Finally, we address several issues of deep learning as well as the mobile intelligence of robots as suggested future research topics. The contributions of this survey are threefold. First, we review the literature of navigation according to the applied sensors and fusion method. Second, we detail the models for target maneuvering and the existing tracking based on estimation, such as the Kalman filter and its series developed form, according to their model-construction mechanisms: linear, nonlinear, and non-Gaussian white noise. Third, we illustrate the artificial intelligence approach—especially deep learning methods—and discuss its combination with the estimation method.
KW  - mobile intelligence
KW  - navigation
KW  - tracking
KW  - Kalman filter
KW  - estimation
KW  - tracking models
KW  - interacting multiple model
KW  - adaptive model
KW  - deep learning
DO  - 10.3390/app8030379
TY  - EJOU
AU  - Palazzolo, Emanuele
AU  - Stachniss, Cyrill
TI  - Effective Exploration for MAVs Based on the Expected Information Gain
T2  - Drones

PY  - 2018
VL  - 2
IS  - 1
SN  - 2504-446X

AB  - Micro aerial vehicles (MAVs) are an excellent platform for autonomous exploration. Most MAVs rely mainly on cameras for buliding a map of the 3D environment. Therefore, vision-based MAVs require an efficient exploration algorithm to select viewpoints that provide informative measurements. In this paper, we propose an exploration approach that selects in real time the next-best-view that maximizes the expected information gain of new measurements. In addition, we take into account the cost of reaching a new viewpoint in terms of distance and predictability of the flight path for a human observer. Finally, our approach selects a path that reduces the risk of crashes when the expected battery life comes to an end, while still maximizing the information gain in the process. We implemented and thoroughly tested our approach and the experiments show that it offers an improved performance compared to other state-of-the-art algorithms in terms of precision of the reconstruction, execution time, and smoothness of the path.
KW  - exploration
KW  - information gain
KW  - vision
DO  - 10.3390/drones2010009
TY  - EJOU
AU  - Liu, Tao
AU  - Abd-Elrahman, Amr
TI  - An Object-Based Image Analysis Method for Enhancing Classification of Land Covers Using Fully Convolutional Networks and Multi-View Images of Small Unmanned Aerial System
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 3
SN  - 2072-4292

AB  - Fully Convolutional Networks (FCN) has shown better performance than other classifiers like Random Forest (RF), Support Vector Machine (SVM) and patch-based Deep Convolutional Neural Network (DCNN), for object-based classification using orthoimage only in previous studies; however, for further improving deep learning algorithm performance, multi-view data should be considered for training data enrichment, which has not been investigated for FCN. The present study developed a novel OBIA classification using FCN and multi-view data extracted from small Unmanned Aerial System (UAS) for mapping landcovers. Specifically, this study proposed three methods to automatically generate multi-view training samples from orthoimage training datasets to conduct multi-view object-based classification using FCN, and compared their performances with each other and also with RF, SVM, and DCNN classifiers. The first method does not consider the object surrounding information, while the other two utilized object context information. We demonstrated that all the three versions of FCN multi-view object-based classification outperformed their counterparts utilizing orthoimage data only. Furthermore, the results also showed that when multi-view training samples were prepared with consideration of object surroundings, FCN trained with these samples gave much better accuracy than FCN classification trained without context information. Similar accuracies were achieved from the two methods utilizing object surrounding information, although sample preparation was conducted using two different ways. When comparing FCN with RF, SVM, DCNN implies that FCN generally produced better accuracy than the other classifiers, regardless of using orthoimage or multi-view data.
KW  - FCN
KW  - deep learning
KW  - object-based
KW  - OBIA
KW  - UAS
KW  - multi-view data
KW  - wetland
DO  - 10.3390/rs10030457
TY  - EJOU
AU  - Sung, Yunsick
AU  - Jin, Yong
AU  - Kwak, Jeonghoon
AU  - Lee, Sang-Geol
AU  - Cho, Kyungeun
TI  - Advanced Camera Image Cropping Approach for CNN-Based End-to-End Controls on Sustainable Computing
T2  - Sustainability

PY  - 2018
VL  - 10
IS  - 3
SN  - 2071-1050

AB  - Recent research on deep learning has been applied to a diversity of fields. In particular, numerous studies have been conducted on self-driving vehicles using end-to-end approaches based on images captured by a single camera. End-to-end controls learn the output vectors of output devices directly from the input vectors of available input devices. In other words, an end-to-end approach learns not by analyzing the meaning of input vectors, but by extracting optimal output vectors based on input vectors. Generally, when end-to-end control is applied to self-driving vehicles, the steering wheel and pedals are controlled autonomously by learning from the images captured by a camera. However, high-resolution images captured from a car cannot be directly used as inputs to Convolutional Neural Networks (CNNs) owing to memory limitations; the image size needs to be efficiently reduced. Therefore, it is necessary to extract features from captured images automatically and to generate input images by merging the parts of the images that contain the extracted features. This paper proposes a learning method for end-to-end control that generates input images for CNNs by extracting road parts from input images, identifying the edges of the extracted road parts, and merging the parts of the images that contain the detected edges. In addition, a CNN model for end-to-end control is introduced. Experiments involving the Open Racing Car Simulator (TORCS), a sustainable computing environment for cars, confirmed the effectiveness of the proposed method for self-driving by comparing the accumulated difference in the angle of the steering wheel in the images generated by it with those of resized images containing the entire captured area and cropped images containing only a part of the captured area. The results showed that the proposed method reduced the accumulated difference by 0.839% and 0.850% compared to those yielded by the resized images and cropped images, respectively.
KW  - self-driving
KW  - convolution neural network
KW  - end-to-end control
DO  - 10.3390/su10030816
TY  - EJOU
AU  - Zhang, Zhen
AU  - Li, Yibing
AU  - Jin, Shanshan
AU  - Zhang, Zhaoyue
AU  - Wang, Hui
AU  - Qi, Lin
AU  - Zhou, Ruolin
TI  - Modulation Signal Recognition Based on Information Entropy and Ensemble Learning
T2  - Entropy

PY  - 2018
VL  - 20
IS  - 3
SN  - 1099-4300

AB  - In this paper, information entropy and ensemble learning based signal recognition theory and algorithms have been proposed. We have extracted 16 kinds of entropy features out of 9 types of modulated signals. The types of information entropy used are numerous, including Rényi entropy and energy entropy based on S Transform and Generalized S Transform. We have used three feature selection algorithms, including sequence forward selection (SFS), sequence forward floating selection (SFFS) and RELIEF-F to select the optimal feature subset from 16 entropy features. We use five classifiers, including k-nearest neighbor (KNN), support vector machine (SVM), Adaboost, Gradient Boosting Decision Tree (GBDT) and eXtreme Gradient Boosting (XGBoost) to classify the original feature set and the feature subsets selected by different feature selection algorithms. The simulation results show that the feature subsets selected by SFS and SFFS algorithms are the best, with a 48% increase in recognition rate over the original feature set when using KNN classifier and a 34% increase when using SVM classifier. For the other three classifiers, the original feature set can achieve the best recognition performance. The XGBoost classifier has the best recognition performance, the overall recognition rate is 97.74% and the recognition rate can reach 82% when the signal to noise ratio (SNR) is −10 dB.
KW  - entropy feature
KW  - feature selection
KW  - ensemble learning
KW  - radar
DO  - 10.3390/e20030198
TY  - EJOU
AU  - Zhang, Duona
AU  - Ding, Wenrui
AU  - Zhang, Baochang
AU  - Xie, Chunyu
AU  - Li, Hongguang
AU  - Liu, Chunhui
AU  - Han, Jungong
TI  - Automatic Modulation Classification Based on Deep Learning for Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 3
SN  - 1424-8220

AB  - Deep learning has recently attracted much attention due to its excellent performance in processing audio, image, and video data. However, few studies are devoted to the field of automatic modulation classification (AMC). It is one of the most well-known research topics in communication signal recognition and remains challenging for traditional methods due to complex disturbance from other sources. This paper proposes a heterogeneous deep model fusion (HDMF) method to solve the problem in a unified framework. The contributions include the following: (1) a convolutional neural network (CNN) and long short-term memory (LSTM) are combined by two different ways without prior knowledge involved; (2) a large database, including eleven types of single-carrier modulation signals with various noises as well as a fading channel, is collected with various signal-to-noise ratios (SNRs) based on a real geographical environment; and (3) experimental results demonstrate that HDMF is very capable of coping with the AMC problem, and achieves much better performance when compared with the independent network.
KW  - deep learning
KW  - automatic modulation classification
KW  - classifier fusion
KW  - convolutional neural network
KW  - long short-term memory
DO  - 10.3390/s18030924
TY  - EJOU
AU  - Sandino, Juan
AU  - Pegg, Geoff
AU  - Gonzalez, Felipe
AU  - Smith, Grant
TI  - Aerial Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral Sensors, and Artificial Intelligence
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 4
SN  - 1424-8220

AB  - The environmental and economic impacts of exotic fungal species on natural and plantation forests have been historically catastrophic. Recorded surveillance and control actions are challenging because they are costly, time-consuming, and hazardous in remote areas. Prolonged periods of testing and observation of site-based tests have limitations in verifying the rapid proliferation of exotic pathogens and deterioration rates in hosts. Recent remote sensing approaches have offered fast, broad-scale, and affordable surveys as well as additional indicators that can complement on-ground tests. This paper proposes a framework that consolidates site-based insights and remote sensing capabilities to detect and segment deteriorations by fungal pathogens in natural and plantation forests. This approach is illustrated with an experimentation case of myrtle rust (Austropuccinia psidii) on paperbark tea trees (Melaleuca quinquenervia) in New South Wales (NSW), Australia. The method integrates unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Imagery is acquired using a Headwall Nano-Hyperspec     ®     camera, orthorectified in Headwall SpectralView     ®    , and processed in Python programming language using eXtreme Gradient Boosting (XGBoost), Geospatial Data Abstraction Library (GDAL), and Scikit-learn third-party libraries. In total, 11,385 samples were extracted and labelled into five classes: two classes for deterioration status and three classes for background objects. Insights reveal individual detection rates of 95% for healthy trees, 97% for deteriorated trees, and a global multiclass detection rate of 97%. The methodology is versatile to be applied to additional datasets taken with different image sensors, and the processing of large datasets with freeware tools.
KW  - Austropuccinia psidii
KW  - drones
KW  - hyperspectral camera
KW  - machine learning
KW  - Melaleuca quinquenervia
KW  - myrtle rust
KW  - non-invasive assessment
KW  - paperbark
KW  - unmanned aerial vehicles (UAV)
KW  - xgboost
DO  - 10.3390/s18040944
TY  - EJOU
AU  - Gallego, Antonio-Javier
AU  - Pertusa, Antonio
AU  - Gil, Pablo
TI  - Automatic Ship Classification from Optical Aerial Images with Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - The automatic classification of ships from aerial images is a considerable challenge. Previous works have usually applied image processing and computer vision techniques to extract meaningful features from visible spectrum images in order to use them as the input for traditional supervised classifiers. We present a method for determining if an aerial image of visible spectrum contains a ship or not. The proposed architecture is based on Convolutional Neural Networks (CNN), and it combines neural codes extracted from a CNN with a k-Nearest Neighbor method so as to improve performance. The kNN results are compared to those obtained with the CNN Softmax output. Several CNN models have been configured and evaluated in order to seek the best hyperparameters, and the most suitable setting for this task was found by using transfer learning at different levels. A new dataset (named MASATI) composed of aerial imagery with more than 6000 samples has also been created to train and evaluate our architecture. The experimentation shows a success rate of over 99% for our approach, in contrast with the 79% obtained with traditional methods in classification of ship images, also outperforming other methods based on CNNs. A dataset of images (MWPU VHR-10) used in previous works was additionally used to evaluate the proposed approach. Our best setup achieves a success ratio of 86% with these data, significantly outperforming previous state-of-the-art ship classification methods.
KW  - deep learning
KW  - aerial image classification
KW  - ships classification
KW  - maritime surveillance
KW  - optical remote sensing
KW  - convolutional neural networks
DO  - 10.3390/rs10040511
TY  - EJOU
AU  - Kong, Xiangxiong
AU  - Li, Jian
TI  - Image Registration-Based Bolt Loosening Detection of Steel Joints
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 4
SN  - 1424-8220

AB  - Self-loosening of bolts caused by repetitive loads and vibrations is one of the common defects that can weaken the structural integrity of bolted steel joints in civil structures. Many existing approaches for detecting loosening bolts are based on physical sensors and, hence, require extensive sensor deployment, which limit their abilities to cost-effectively detect loosened bolts in a large number of steel joints. Recently, computer vision-based structural health monitoring (SHM) technologies have demonstrated great potential for damage detection due to the benefits of being low cost, easy to deploy, and contactless. In this study, we propose a vision-based non-contact bolt loosening detection method that uses a consumer-grade digital camera. Two images of the monitored steel joint are first collected during different inspection periods and then aligned through two image registration processes. If the bolt experiences rotation between inspections, it will introduce differential features in the registration errors, serving as a good indicator for bolt loosening detection. The performance and robustness of this approach have been validated through a series of experimental investigations using three laboratory setups including a gusset plate on a cross frame, a column flange, and a girder web. The bolt loosening detection results are presented for easy interpretation such that informed decisions can be made about the detected loosened bolts.
KW  - bolt loosening detection
KW  - intensity-based image registration
KW  - feature matching
KW  - structural health monitoring
KW  - structural inspection
KW  - superpixel
KW  - civil structures
KW  - steel joints
KW  - feature tracking
DO  - 10.3390/s18041000
TY  - EJOU
AU  - Zhu, Xiaolin
AU  - Cai, Fangyi
AU  - Tian, Jiaqi
AU  - Williams, Trecia K.
TI  - Spatiotemporal Fusion of Multisource Remote Sensing Data: Literature Survey, Taxonomy, Principles, Applications, and Future Directions
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - Satellite time series with high spatial resolution is critical for monitoring land surface dynamics in heterogeneous landscapes. Although remote sensing technologies have experienced rapid development in recent years, data acquired from a single satellite sensor are often unable to satisfy our demand. As a result, integrated use of data from different sensors has become increasingly popular in the past decade. Many spatiotemporal data fusion methods have been developed to produce synthesized images with both high spatial and temporal resolutions from two types of satellite images, frequent coarse-resolution images, and sparse fine-resolution images. These methods were designed based on different principles and strategies, and therefore show different strengths and limitations. This diversity brings difficulties for users to choose an appropriate method for their specific applications and data sets. To this end, this review paper investigates literature on current spatiotemporal data fusion methods, categorizes existing methods, discusses the principal laws underlying these methods, summarizes their potential applications, and proposes possible directions for future studies in this field.
KW  - spatiotemporal data fusion
KW  - data blending
KW  - spatial resolution
KW  - temporal resolution
KW  - satellite images
DO  - 10.3390/rs10040527
TY  - EJOU
AU  - Cardim, Guilherme P.
AU  - Silva, Erivaldo A.
AU  - Dias, Mauricio A.
AU  - Bravo, Ignácio
AU  - Gardel, Alfredo
TI  - Statistical Evaluation and Analysis of Road Extraction Methodologies Using a Unique Dataset from Remote Sensing
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - In the scientific literature, multiple studies address the application of road extraction methodologies to a particular cartographic dataset. However, it is difficult for any study to perform a more reliable comparison among road extraction methodologies when their results come from different cartographic datasets. Therefore, aiming to enable a more reliable comparison among different road extraction methodologies from the scientific literature, this study proposed a statistical evaluation and analysis of road extraction methodologies using a common image dataset. To achieve this goal, we setup a dataset containing remote sensing images of three different road types, highways, cities network and rural paths, and a group of images from the ISPRS (International Society for Photogrammetry and Remote Sensing) dataset. Furthermore, three road extraction methodologies were selected from the literature, in accordance with their availability, to be processed and evaluated using well-known statistical metrics. The achieved results are encouraging and indicate that the proposed statistical evaluation and analysis can allow researchers to evaluate and compare road extraction methodologies using this common dataset extracting similar characteristics to obtain a more reliable comparison among them.
KW  - road network extraction
KW  - remote sensing images
KW  - methodologies review
KW  - image dataset
KW  - evaluation metrics
DO  - 10.3390/rs10040620
TY  - EJOU
AU  - Zhuo, Xiangyu
AU  - Fraundorfer, Friedrich
AU  - Kurz, Franz
AU  - Reinartz, Peter
TI  - Optimization of OpenStreetMap Building Footprints Based on Semantic Information of Oblique UAV Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - Building footprint information is vital for 3D building modeling. Traditionally, in remote sensing, building footprints are extracted and delineated from aerial imagery and/or LiDAR point cloud. Taking a different approach, this paper is dedicated to the optimization of OpenStreetMap (OSM) building footprints exploiting the contour information, which is derived from deep learning-based semantic segmentation of oblique images acquired by the Unmanned Aerial Vehicle (UAV). First, a simplified 3D building model of Level of Detail 1 (LoD 1) is initialized using the footprint information from OSM and the elevation information from Digital Surface Model (DSM). In parallel, a deep neural network for pixel-wise semantic image segmentation is trained in order to extract the building boundaries as contour evidence. Subsequently, an optimization integrating the contour evidence from multi-view images as a constraint results in a refined 3D building model with optimized footprints and height. Our method is leveraged to optimize OSM building footprints for four datasets with different building types, demonstrating robust performance for both individual buildings and multiple buildings regardless of image resolution. Finally, we compare our result with reference data from German Authority Topographic-Cartographic Information System (ATKIS). Quantitative and qualitative evaluations reveal that the original OSM building footprints have large offset, but can be significantly improved from meter level to decimeter level after optimization.
KW  - building footprint
KW  - oblique UAV images
KW  - semantic segmentation
KW  - deep neural network
DO  - 10.3390/rs10040624
TY  - EJOU
AU  - Tripodi, Pasquale
AU  - Massa, Daniele
AU  - Venezia, Accursio
AU  - Cardi, Teodoro
TI  - Sensing Technologies for Precision Phenotyping in Vegetable Crops: Current Status and Future Challenges
T2  - Agronomy

PY  - 2018
VL  - 8
IS  - 4
SN  - 2073-4395

AB  - Increasing the ability to investigate plant functions and structure through non-invasive methods with high accuracy has become a major target in plant breeding and precision agriculture. Emerging approaches in plant phenotyping play a key role in unraveling quantitative traits responsible for growth, production, quality, and resistance to various stresses. Beyond fully automatic phenotyping systems, several promising technologies can help accurately characterize a wide range of plant traits at affordable costs and with high-throughput. In this review, we revisit the principles of proximal and remote sensing, describing the application of non-invasive devices for precision phenotyping applied to the protected horticulture. Potentiality and constraints of big data management and integration with &ldquo;omics&rdquo; disciplines will also be discussed.
KW  - digital imaging
KW  - genomics
KW  - phenomics
KW  - plant breeding
KW  - greenhouse horticulture
KW  - advanced crop management
KW  - automation
KW  - vegetation indices
KW  - optical sensors
KW  - fluorescence
DO  - 10.3390/agronomy8040057
TY  - EJOU
AU  - Ayrey, Elias
AU  - Hayes, Daniel J.
TI  - The Use of Three-Dimensional Convolutional Neural Networks to Interpret LiDAR for Forest Inventory
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - As light detection and ranging (LiDAR) technology becomes more available, it has become common to use these datasets to generate remotely sensed forest inventories across landscapes. Traditional methods for generating these inventories employ the use of height and proportion metrics to measure LiDAR returns and relate these back to field data using predictive models. Here, we employ a three-dimensional convolutional neural network (CNN), a deep learning technique that scans the LiDAR data and automatically generates useful features for predicting forest attributes. We test the accuracy in estimating forest attributes using the three-dimensional implementations of different CNN models commonly used in the field of image recognition. Using the best performing model architecture, we compared CNN performance to models developed using traditional height metrics. The results of this comparison show that CNNs produced 12% less prediction error when estimating biomass, 6% less in estimating tree count, and 2% less when estimating the percentage of needleleaf trees. We conclude that using CNNs can be a more accurate means of interpreting LiDAR data for forest inventories compared to standard approaches.
KW  - deep learning
KW  - artificial neural network
KW  - machine learning
KW  - ALS
KW  - LiDAR
KW  - enhanced forest inventory
KW  - area-based
DO  - 10.3390/rs10040649
TY  - EJOU
AU  - Guo, Siqiu
AU  - Zhang, Tao
AU  - Song, Yulong
AU  - Qian, Feng
TI  - Color Feature-Based Object Tracking through Particle Swarm Optimization with Improved Inertia Weight
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 4
SN  - 1424-8220

AB  - This paper presents a particle swarm tracking algorithm with improved inertia weight based on color features. The weighted color histogram is used as the target feature to reduce the contribution of target edge pixels in the target feature, which makes the algorithm insensitive to the target non-rigid deformation, scale variation, and rotation. Meanwhile, the influence of partial obstruction on the description of target features is reduced. The particle swarm optimization algorithm can complete the multi-peak search, which can cope well with the object occlusion tracking problem. This means that the target is located precisely where the similarity function appears multi-peak. When the particle swarm optimization algorithm is applied to the object tracking, the inertia weight adjustment mechanism has some limitations. This paper presents an improved method. The concept of particle maturity is introduced to improve the inertia weight adjustment mechanism, which could adjust the inertia weight in time according to the different states of each particle in each generation. Experimental results show that our algorithm achieves state-of-the-art performance in a wide range of scenarios.
KW  - object tracking
KW  - particle swarm optimization
KW  - particle maturity
KW  - inertia weight
KW  - color feature
DO  - 10.3390/s18041292
TY  - EJOU
AU  - Zhang, Yongjun
AU  - Wang, Xiang
AU  - Xie, Xunwei
AU  - Li, Yansheng
TI  - Salient Object Detection via Recursive Sparse Representation
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 4
SN  - 2072-4292

AB  - Object-level saliency detection is an attractive research field which is useful for many content-based computer vision and remote-sensing tasks. This paper introduces an efficient unsupervised approach to salient object detection from the perspective of recursive sparse representation. The reconstruction error determined by foreground and background dictionaries other than common local and global contrasts is used as the saliency indication, by which the shortcomings of the object integrity can be effectively improved. The proposed method consists of the following four steps: (1) regional feature extraction; (2) background and foreground dictionaries extraction according to the initial saliency map and image boundary constraints; (3) sparse representation and saliency measurement; and (4) recursive processing with a current saliency map updating the initial saliency map in step 2 and repeating step 3. This paper also presents the experimental results of the proposed method compared with seven state-of-the-art saliency detection methods using three benchmark datasets, as well as some satellite and unmanned aerial vehicle remote-sensing images, which confirmed that the proposed method was more effective than current methods and could achieve more favorable performance in the detection of multiple objects as well as maintaining the integrity of the object area.
KW  - salient object detection
KW  - sparse representation
KW  - reconstruction error
KW  - recursive processing
DO  - 10.3390/rs10040652
TY  - EJOU
AU  - Krylov, Vladimir A.
AU  - Kenny, Eamonn
AU  - Dahyot, Rozenn
TI  - Automatic Discovery and Geotagging of Objects from Street View Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Many applications, such as autonomous navigation, urban planning, and asset monitoring, rely on the availability of accurate information about objects and their geolocations. In this paper, we propose the automatic detection and computation of the coordinates of recurring stationary objects of interest using street view imagery. Our processing pipeline relies on two fully convolutional neural networks: the first segments objects in the images, while the second estimates their distance from the camera. To geolocate all the detected objects coherently we propose a novel custom Markov random field model to estimate the objects&rsquo; geolocation. The novelty of the resulting pipeline is the combined use of monocular depth estimation and triangulation to enable automatic mapping of complex scenes with the simultaneous presence of multiple, visually similar objects of interest. We validate experimentally the effectiveness of our approach on two object classes: traffic lights and telegraph poles. The experiments report high object recall rates and position precision of approximately 2 m, which is approaching the precision of single-frequency GPS receivers.
KW  - object geolocation
KW  - object mapping
KW  - street view imagery
KW  - Markov random fields
KW  - traffic lights
KW  - telecom assets
KW  - GPS estimation
DO  - 10.3390/rs10050661
TY  - EJOU
AU  - Al Shidi, Rashid H.
AU  - Kumar, Lalit
AU  - Al-Khatri, Salim A. H.
AU  - Albahri, Malik M.
AU  - Alaufi, Mohammed S.
TI  - Relationship of Date Palm Tree Density to Dubas Bug Ommatissus lybicus Infestation in Omani Orchards
T2  - Agriculture

PY  - 2018
VL  - 8
IS  - 5
SN  - 2077-0472

AB  - Date palm trees, Phoenix dactylifera, are the primary crop in Oman. Most date palm cultivation is under the traditional agricultural system. The plants are usually under dense planting, which makes them prone to pest infestation. The main pest attacking date palm crops in Oman is the Dubas bug Ommatissus lybicus. This study integrated modern technology, remote sensing and geographic information systems to determine the number of date palm trees in traditional agriculture locations to find the relationship between date palm tree density and O. lybicus infestation. A local maxima method for tree identification was used to determine the number of date palm trees from high spatial resolution satellite imagery captured by WorldView-3 satellite. Window scale sizes of 3, 5 and 7 m were tested and the results showed that the best window size for date palm trees number detection was 7 m, with an overall estimation accuracy 88.2%. Global regression ordinary least square (OLS) and local geographic weighted regression (GWR) were used to test the relationship between infestation intensity and tree density. The GWR model showed a good positive significant relationship between infestation and tree density in the spring season with R2 = 0.59 and medium positive significant relationship in the autumn season with R2 = 0.30. In contrast, the OLS model results showed a weak positive significant relationship in the spring season with R2 = 0.02, p &lt; 0.05 and insignificant relationship in the autumn season with R2 = 0.01, p &gt; 0.05. The results indicated that there was a geographic effect on the infestation of O. lybicus, which had a greater impact on infestation severity, and that the impact of tree density was higher in the spring season than in autumn season.
KW  - Dubas bug Ommatissus lybicus
KW  - date palm Phoenix dactylifera
KW  - remote sensing
KW  - multispectral image
KW  - local maxima
DO  - 10.3390/agriculture8050064
TY  - EJOU
AU  - Moy de Vitry, Matthew
AU  - Schindler, Konrad
AU  - Rieckermann, Jörg
AU  - Leitão, João P.
TI  - Sewer Inlet Localization in UAV Image Clouds: Improving Performance with Multiview Detection
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Sewer and drainage infrastructure are often not as well catalogued as they should be, considering the immense investment they represent. In this work, we present a fully automatic framework for localizing sewer inlets from image clouds captured from an unmanned aerial vehicle (UAV). The framework exploits the high image overlap of UAV imaging surveys with a multiview approach to improve detection performance. The framework uses a Viola–Jones classifier trained to detect sewer inlets in aerial images with a ground sampling distance of 3–3.5 cm/pixel. The detections are then projected into three-dimensional space where they are clustered and reclassified to discard false positives. The method is evaluated by cross-validating results from an image cloud of 252 UAV images captured over a 0.57-km2 study area with 228 sewer inlets. Compared to an equivalent single-view detector, the multiview approach improves both recall and precision, increasing average precision from 0.65 to 0.73. The source code and case study data are publicly available for reuse.
KW  - infrastructure mapping
KW  - multiview
KW  - object detection
KW  - unmanned aerial vehicle
KW  - urban drainage
KW  - asset management
DO  - 10.3390/rs10050706
TY  - EJOU
AU  - Shamwell, E. J.
AU  - Nothwang, William D.
AU  - Perlis, Donald
TI  - An Embodied Multi-Sensor Fusion Approach to Visual Motion Estimation Using Unsupervised Deep Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 5
SN  - 1424-8220

AB  - Aimed at improving size, weight, and power (SWaP)-constrained robotic vision-aided state estimation, we describe our unsupervised, deep convolutional-deconvolutional sensor fusion network, Multi-Hypothesis DeepEfference (MHDE). MHDE learns to intelligently combine noisy heterogeneous sensor data to predict several probable hypotheses for the dense, pixel-level correspondence between a source image and an unseen target image. We show how our multi-hypothesis formulation provides increased robustness against dynamic, heteroscedastic sensor and motion noise by computing hypothesis image mappings and predictions at 76&ndash;357 Hz depending on the number of hypotheses being generated. MHDE fuses noisy, heterogeneous sensory inputs using two parallel, inter-connected architectural pathways and n (1&ndash;20 in this work) multi-hypothesis generating sub-pathways to produce n global correspondence estimates between a source and a target image. We evaluated MHDE on the KITTI Odometry dataset and benchmarked it against the vision-only DeepMatching and Deformable Spatial Pyramids algorithms and were able to demonstrate a significant runtime decrease and a performance increase compared to the next-best performing method.
KW  - deep learning
KW  - sensor fusion
KW  - optical flow
DO  - 10.3390/s18051427
TY  - EJOU
AU  - Yu, Manzhu
AU  - Yang, Chaowei
AU  - Li, Yun
TI  - Big Data in Natural Disaster Management: A Review
T2  - Geosciences

PY  - 2018
VL  - 8
IS  - 5
SN  - 2076-3263

AB  - Undoubtedly, the age of big data has opened new options for natural disaster management, primarily because of the varied possibilities it provides in visualizing, analyzing, and predicting natural disasters. From this perspective, big data has radically changed the ways through which human societies adopt natural disaster management strategies to reduce human suffering and economic losses. In a world that is now heavily dependent on information technology, the prime objective of computer experts and policy makers is to make the best of big data by sourcing information from varied formats and storing it in ways that it can be effectively used during different stages of natural disaster management. This paper aimed at making a systematic review of the literature in analyzing the role of big data in natural disaster management and highlighting the present status of the technology in providing meaningful and effective solutions in natural disaster management. The paper has presented the findings of several researchers on varied scientific and technological perspectives that have a bearing on the efficacy of big data in facilitating natural disaster management. In this context, this paper reviews the major big data sources, the associated achievements in different disaster management phases, and emerging technological topics associated with leveraging this new ecosystem of Big Data to monitor and detect natural hazards, mitigate their effects, assist in relief efforts, and contribute to the recovery and reconstruction processes.
KW  - big data
KW  - disaster management
KW  - review
DO  - 10.3390/geosciences8050165
TY  - EJOU
AU  - Chen, Guanzhou
AU  - Zhang, Xiaodong
AU  - Tan, Xiaoliang
AU  - Cheng, Yufeng
AU  - Dai, Fan
AU  - Zhu, Kun
AU  - Gong, Yuanfu
AU  - Wang, Qing
TI  - Training Small Networks for Scene Classification of Remote Sensing Images via Knowledge Distillation
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Scene classification, aiming to identify the land-cover categories of remotely sensed image patches, is now a fundamental task in the remote sensing image analysis field. Deep-learning-model-based algorithms are widely applied in scene classification and achieve remarkable performance, but these high-level methods are computationally expensive and time-consuming. Consequently in this paper, we introduce a knowledge distillation framework, currently a mainstream model compression method, into remote sensing scene classification to improve the performance of smaller and shallower network models. Our knowledge distillation training method makes the high-temperature softmax output of a small and shallow student model match the large and deep teacher model. In our experiments, we evaluate knowledge distillation training method for remote sensing scene classification on four public datasets: AID dataset, UCMerced dataset, NWPU-RESISC dataset, and EuroSAT dataset. Results show that our proposed training method was effective and increased overall accuracy (3% in AID experiments, 5% in UCMerced experiments, 1% in NWPU-RESISC and EuroSAT experiments) for small and shallow models. We further explored the performance of the student model on small and unbalanced datasets. Our findings indicate that knowledge distillation can improve the performance of small network models on datasets with lower spatial resolution images, numerous categories, as well as fewer training samples.
KW  - knowledge distillation
KW  - scene classification
KW  - convolutional neural networks (CNNs)
KW  - remote sensing
KW  - deep learning
DO  - 10.3390/rs10050719
TY  - EJOU
AU  - Deng, Zhipeng
AU  - Sun, Hao
AU  - Zhou, Shilin
TI  - Semi-Supervised Ground-to-Aerial Adaptation with Heterogeneous Features Learning for Scene Classification
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 5
SN  - 2220-9964

AB  - Currently, huge quantities of remote sensing images (RSIs) are becoming available. Nevertheless, the scarcity of labeled samples hinders the semantic understanding of RSIs. Fortunately, many ground-level image datasets with detailed semantic annotations have been collected in the vision community. In this paper, we attempt to exploit the abundant labeled ground-level images to build discriminative models for overhead-view RSI classification. However, images from the ground-level and overhead view are represented by heterogeneous features with different distributions; how to effectively combine multiple features and reduce the mismatch of distributions are two key problems in this scene-model transfer task. Specifically, a semi-supervised manifold-regularized multiple-kernel-learning (SMRMKL) algorithm is proposed for solving these problems. We employ multiple kernels over several features to learn an optimal combined model automatically. Multi-kernel Maximum Mean Discrepancy (MK-MMD) is utilized to measure the data mismatch. To make use of unlabeled target samples, a manifold regularized semi-supervised learning process is incorporated into our framework. Extensive experimental results on both cross-view and aerial-to-satellite scene datasets demonstrate that: (1) SMRMKL has an appealing extension ability to effectively fuse different types of visual features; and (2) manifold regularization can improve the adaptation performance by utilizing unlabeled target samples.
KW  - remote sensing
KW  - scene classification
KW  - heterogeneous domain adaptation
KW  - cross-view
KW  - multiple kernel learning
DO  - 10.3390/ijgi7050182
TY  - EJOU
AU  - Ma, Dandan
AU  - Yuan, Yuan
AU  - Wang, Qi
TI  - Hyperspectral Anomaly Detection via Discriminative Feature Learning with Multiple-Dictionary Sparse Representation
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Most hyperspectral anomaly detection methods directly utilize all the original spectra to recognize anomalies. However, the inherent characteristics of high spectral dimension and complex spectral correlation commonly make their detection performance unsatisfactory. Therefore, an effective feature extraction technique is necessary. To this end, this paper proposes a novel anomaly detection method via discriminative feature learning with multiple-dictionary sparse representation. Firstly, a new spectral feature selection framework based on sparse presentation is designed, which is closely guided by the anomaly detection task. Then, the representative spectra which can significantly enlarge anomaly’s deviation from background are picked out by minimizing residues between background spectrum reconstruction error and anomaly spectrum recovery error. Finally, through comprehensively considering the virtues of different groups of representative features selected from multiple dictionaries, a global multiple-view detection strategy is presented to improve the detection accuracy. The proposed method is compared with ten state-of-the-art methods including LRX, SRD, CRD, LSMAD, RSAD, BACON, BACON-target, GRX, GKRX, and PCA-GRX on three real-world hyperspectral images. Corresponding to each competitor, it has the average detection performance improvement of about     9.9 %    ,     7.4 %    ,     24.2 %    ,     10.1 %    ,     26.2 %    ,     20.1 %    ,     5.1 %    ,     19.3 %    ,     10.7 %    , and     2.0 %     respectively. Extensive experiments demonstrate its superior performance in effectiveness and efficiency.
KW  - anomaly detection
KW  - hyperspectral image
KW  - sparse representation
KW  - multiple dictionaries
KW  - feature extraction
KW  - clustering
DO  - 10.3390/rs10050745
TY  - EJOU
AU  - Tao, Yiting
AU  - Xu, Miaozhong
AU  - Lu, Zhongyuan
AU  - Zhong, Yanfei
TI  - DenseNet-Based Depth-Width Double Reinforced Deep Learning Neural Network for High-Resolution Remote Sensing Image Per-Pixel Classification
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 5
SN  - 2072-4292

AB  - Deep neural networks (DNNs) face many problems in the very high resolution remote sensing (VHRRS) per-pixel classification field. Among the problems is the fact that as the depth of the network increases, gradient disappearance influences classification accuracy and the corresponding increasing number of parameters to be learned increases the possibility of overfitting, especially when only a small amount of VHRRS labeled samples are acquired for training. Further, the hidden layers in DNNs are not transparent enough, which results in extracted features not being sufficiently discriminative and significant amounts of redundancy. This paper proposes a novel depth-width-reinforced DNN that solves these problems to produce better per-pixel classification results in VHRRS. In the proposed method, densely connected neural networks and internal classifiers are combined to build a deeper network and balance the network depth and performance. This strengthens the gradients, decreases negative effects from gradient disappearance as the network depth increases and enhances the transparency of hidden layers, making extracted features more discriminative and reducing the risk of overfitting. In addition, the proposed method uses multi-scale filters to create a wider neural network. The depth of the filters from each scale is controlled to decrease redundancy and the multi-scale filters enable utilization of joint spatio-spectral information and diverse local spatial structure simultaneously. Furthermore, the concept of network in network is applied to better fuse the deeper and wider designs, making the network operate more smoothly. The results of experiments conducted on BJ02, GF02, geoeye and quickbird satellite images verify the efficacy of the proposed method. The proposed method not only achieves competitive classification results but also proves that the network can continue to be robust and perform well even while the amount of labeled training samples is decreasing, which fits the small training samples situation faced by VHRRS per-pixel classification.
KW  - remote sensing
KW  - image per-pixel classification
KW  - densely connected neural network
KW  - internal classifier
KW  - multi-scale filters
KW  - network in network
DO  - 10.3390/rs10050779
TY  - EJOU
AU  - Karimi, Hadi
AU  - Skovsen, Søren
AU  - Dyrmann, Mads
AU  - Nyholm Jørgensen, Rasmus
TI  - A Novel Locating System for Cereal Plant Stem Emerging Points’ Detection Using a Convolutional Neural Network
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 5
SN  - 1424-8220

AB  - Determining the individual location of a plant, besides evaluating sowing performance, would make subsequent treatment for each plant across a field possible. In this study, a system for locating cereal plant stem emerging points (PSEPs) has been developed. In total, 5719 images were gathered from several cereal fields. In 212 of these images, the PSEPs of the cereal plants were marked manually and used to train a fully-convolutional neural network. In the training process, a cost function was made, which incorporates predefined penalty regions and PSEPs. The penalty regions were defined based on fault prediction of the trained model without penalty region assignment. By adding penalty regions to the training, the network&rsquo;s ability to precisely locate emergence points of the cereal plants was enhanced significantly. A coefficient of determination of about 87 percent between the predicted PSEP number of each image and the manually marked one implies the ability of the system to count PSEPs. With regard to the obtained results, it was concluded that the developed model can give a reliable clue about the quality of PSEPs&rsquo; distribution and the performance of seed drills in fields.
KW  - cereal
KW  - plants distribution
KW  - sowing performance
DO  - 10.3390/s18051611
TY  - EJOU
AU  - Ahmad Yousef, Khalil M.
AU  - AlMajali, Anas
AU  - Ghalyon, Salah A.
AU  - Dweik, Waleed
AU  - Mohd, Bassam J.
TI  - Analyzing Cyber-Physical Threats on Robotic Platforms
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 5
SN  - 1424-8220

AB  - Robots are increasingly involved in our daily lives. Fundamental to robots are the communication link (or stream) and the applications that connect the robots to their clients or users. Such communication link and applications are usually supported through client/server network connection. This networking system is amenable of being attacked and vulnerable to the security threats. Ensuring security and privacy for robotic platforms is thus critical, as failures and attacks could have devastating consequences. In this paper, we examine several cyber-physical security threats that are unique to the robotic platforms; specifically the communication link and the applications. Threats target integrity, availability and confidential security requirements of the robotic platforms, which use MobileEyes/arnlServer client/server applications. A robot attack tool (RAT) was developed to perform specific security attacks. An impact-oriented approach was adopted to analyze the assessment results of the attacks. Tests and experiments of attacks were conducted in simulation environment and physically on the robot. The simulation environment was based on MobileSim; a software tool for simulating, debugging and experimenting on MobileRobots/ActivMedia platforms and their environments. The robot platform PeopleBotTM was used for physical experiments. The analysis and testing results show that certain attacks were successful at breaching the robot security. Integrity attacks modified commands and manipulated the robot behavior. Availability attacks were able to cause Denial-of-Service (DoS) and the robot was not responsive to MobileEyes commands. Integrity and availability attacks caused sensitive information on the robot to be hijacked. To mitigate security threats, we provide possible mitigation techniques and suggestions to raise awareness of threats on the robotic platforms, especially when the robots are involved in critical missions or applications.
KW  - mobile robot
KW  - robotic platform
KW  - cyper-physical systems
KW  - threats
KW  - security
KW  - risk management
KW  - risk assessment
DO  - 10.3390/s18051643
TY  - EJOU
AU  - Lyu, Hai-Min
AU  - Xu, Ye-Shuang
AU  - Cheng, Wen-Chieh
AU  - Arulrajah, Arul
TI  - Flooding Hazards across Southern China and Prospective Sustainability Measures
T2  - Sustainability

PY  - 2018
VL  - 10
IS  - 5
SN  - 2071-1050

AB  - The Yangtze River Basin and Huaihe River Basin in Southern China experienced severe floods 1998 and 2016. The reasons for the flooding hazards include the following two factors: hazardous weather conditions and degradation of the hydrological environment due to anthropogenic activities. This review work investigated the weather conditions based on recorded data, which showed that both 1998 and 2016 were in El Nino periods. Human activities include the degradations of rivers and lakes and the effects caused by the building of the Three Gorges Dam. In addition, the flooding in 2016 had a lower hazard scale than that in 1998 but resulted in larger economic losses than that of 1998. To mitigate urban waterlogging caused by flooding hazards, China proposed a new strategy named Spongy City (SPC) in 2014. SPC promotes sustainable city development so that a city has the resilience to adapt to climate change, to mitigate the impacts of waterlogging caused by extreme rainfall events. The countermeasures used to tackle the SPC construction-related problems, such as local inundation, water resource shortage, storm water usage, and water pollution control, are proposed for city management to improve the environment.
KW  - flooding hazards
KW  - hazardous weather conditions
KW  - urban waterlogging
KW  - SPC
KW  - sustainable development
DO  - 10.3390/su10051682
TY  - EJOU
AU  - Liu, Xiaofei
AU  - Yang, Tao
AU  - Li, Jing
TI  - Real-Time Ground Vehicle Detection in Aerial Infrared Imagery Based on Convolutional Neural Network
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 6
SN  - 2079-9292

AB  - An infrared sensor is a commonly used imaging device. Unmanned aerial vehicles, the most promising moving platform, each play a vital role in their own field, respectively. However, the two devices are seldom combined in automatic ground vehicle detection tasks. Therefore, how to make full use of them&mdash;especially in ground vehicle detection based on aerial imagery&ndash;has aroused wide academic concern. However, due to the aerial imagery&rsquo;s low-resolution and the vehicle detection&rsquo;s complexity, how to extract remarkable features and handle pose variations, view changes as well as surrounding radiation remains a challenge. In fact, these typical abstract features extracted by convolutional neural networks are more recognizable than the engineering features, and those complex conditions involved can be learned and memorized before. In this paper, a novel approach towards ground vehicle detection in aerial infrared images based on a convolutional neural network is proposed. The UAV and the infrared sensor used in this application are firstly introduced. Then, a novel aerial moving platform is built and an aerial infrared vehicle dataset is unprecedentedly constructed. We publicly release this dataset (NPU_CS_UAV_IR_DATA), which can be used for the following research in this field. Next, an end-to-end convolutional neural network is built. With large amounts of recognized features being iteratively learned, a real-time ground vehicle model is constructed. It has the unique ability to detect both the stationary vehicles and moving vehicles in real urban environments. We evaluate the proposed algorithm on some low&ndash;resolution aerial infrared images. Experiments on the NPU_CS_UAV_IR_DATA dataset demonstrate that the proposed method is effective and efficient to recognize the ground vehicles. Moreover it can accomplish the task in real-time while achieving superior performances in leak and false alarm ratio.
KW  - aerial infrared imagery
KW  - real-time ground vehicle detection
KW  - convolutional neural network
KW  - unmanned aerial vehicle
DO  - 10.3390/electronics7060078
TY  - EJOU
AU  - Nguyen, Phong H.
AU  - Arsalan, Muhammad
AU  - Koo, Ja H.
AU  - Naqvi, Rizwan A.
AU  - Truong, Noi Q.
AU  - Park, Kang R.
TI  - LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - Autonomous landing of an unmanned aerial vehicle or a drone is a challenging problem for the robotics research community. Previous researchers have attempted to solve this problem by combining multiple sensors such as global positioning system (GPS) receivers, inertial measurement unit, and multiple camera systems. Although these approaches successfully estimate an unmanned aerial vehicle location during landing, many calibration processes are required to achieve good detection accuracy. In addition, cases where drones operate in heterogeneous areas with no GPS signal should be considered. To overcome these problems, we determined how to safely land a drone in a GPS-denied environment using our remote-marker-based tracking algorithm based on a single visible-light-camera sensor. Instead of using hand-crafted features, our algorithm includes a convolutional neural network named lightDenseYOLO to extract trained features from an input image to predict a marker&rsquo;s location by visible light camera sensor on drone. Experimental results show that our method significantly outperforms state-of-the-art object trackers both using and not using convolutional neural network in terms of both accuracy and processing time.
KW  - unmanned aerial vehicle
KW  - autonomous landing
KW  - real-time marker detection
KW  - lightDenseYOLO
KW  - visible light camera sensor on drone
DO  - 10.3390/s18061703
TY  - EJOU
AU  - Wang, Baoxian
AU  - Zhao, Weigang
AU  - Gao, Po
AU  - Zhang, Yufeng
AU  - Wang, Zhe
TI  - Crack Damage Detection Method via Multiple Visual Features and Efficient Multi-Task Learning Model
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - This paper proposes an effective and efficient model for concrete crack detection. The presented work consists of two modules: multi-view image feature extraction and multi-task crack region detection. Specifically, multiple visual features (such as texture, edge, etc.) of image regions are calculated, which can suppress various background noises (such as illumination, pockmark, stripe, blurring, etc.). With the computed multiple visual features, a novel crack region detector is advocated using a multi-task learning framework, which involves restraining the variability for different crack region features and emphasizing the separability between crack region features and complex background ones. Furthermore, the extreme learning machine is utilized to construct this multi-task learning model, thereby leading to high computing efficiency and good generalization. Experimental results of the practical concrete images demonstrate that the developed algorithm can achieve favorable crack detection performance compared with traditional crack detectors.
KW  - crack damage detection
KW  - multiple visual feature extraction
KW  - multi-task learning model
KW  - extreme learning machine
DO  - 10.3390/s18061796
TY  - EJOU
AU  - Zhu, Jiasong
AU  - Sun, Ke
AU  - Jia, Sen
AU  - Lin, Weidong
AU  - Hou, Xianxu
AU  - Liu, Bozhi
AU  - Qiu, Guoping
TI  - Bidirectional Long Short-Term Memory Network for Vehicle Behavior Recognition
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 6
SN  - 2072-4292

AB  - Vehicle behavior recognition is an attractive research field which is useful for many computer vision and intelligent traffic analysis tasks. This paper presents an all-in-one behavior recognition framework for moving vehicles based on the latest deep learning techniques. Unlike traditional traffic analysis methods which rely on low-resolution videos captured by road cameras, we capture 4K (    3840 × 2178    ) traffic videos at a busy road intersection of a modern megacity by flying a unmanned aerial vehicle (UAV) during the rush hours. We then manually annotate locations and types of road vehicles. The proposed method consists of the following three steps: (1) vehicle detection and type recognition based on deep neural networks; (2) vehicle tracking by data association and vehicle trajectory modeling; (3) vehicle behavior recognition by nearest neighbor search and by bidirectional long short-term memory network, respectively. This paper also presents experimental results of the proposed framework in comparison with state-of-the-art approaches on the 4K testing traffic video, which demonstrated the effectiveness and superiority of the proposed method.
KW  - unmanned aerial vehicles (UAVs)
KW  - deep neural networks
KW  - vehicle detection
KW  - vehicle tracking
KW  - behavior recognition
KW  - long short-term memory
DO  - 10.3390/rs10060887
TY  - EJOU
AU  - Mao , Keming
AU  - Lu , Duo
AU  - E , Dazhi
AU  - Tan , Zhenhua
TI  - A Case Study on Attribute Recognition of Heated Metal Mark Image Using Deep Convolutional Neural Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - Heated metal mark is an important trace to identify the cause of fire. However, traditional methods mainly focus on the knowledge of physics and chemistry for qualitative analysis and make it still a challenging problem. This paper presents a case study on attribute recognition of the heated metal mark image using computer vision and machine learning technologies. The proposed work is composed of three parts. Material is first generated. According to national standards, actual needs and feasibility, seven attributes are selected for research. Data generation and organization are conducted, and a small size benchmark dataset is constructed. A recognition model is then implemented. Feature representation and classifier construction methods are introduced based on deep convolutional neural networks. Finally, the experimental evaluation is carried out. Multi-aspect testings are performed with various model structures, data augments, training modes, optimization methods and batch sizes. The influence of parameters, recognitio efficiency and execution time are also analyzed. The results show that with a fine-tuned model, the recognition rate of attributes metal type, heating mode, heating temperature, heating duration, cooling mode, placing duration and relative humidity are 0.925, 0.908, 0.835, 0.917, 0.928, 0.805 and 0.92, respectively. The proposed method recognizes the attribute of heated metal mark with preferable effect, and it can be used in practical application.
KW  - attribute recognition
KW  - heated metal mark
KW  - convolutional neural networks
DO  - 10.3390/s18061871
TY  - EJOU
AU  - Kim, In-Ho
AU  - Jeon, Haemin
AU  - Baek, Seung-Chan
AU  - Hong, Won-Hwa
AU  - Jung, Hyung-Jo
TI  - Application of Crack Identification Techniques for an Aging Concrete Bridge Inspection Using an Unmanned Aerial Vehicle
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - Bridge inspection using unmanned aerial vehicles (UAV) with high performance vision sensors has received considerable attention due to its safety and reliability. As bridges become obsolete, the number of bridges that need to be inspected increases, and they require much maintenance cost. Therefore, a bridge inspection method based on UAV with vision sensors is proposed as one of the promising strategies to maintain bridges. In this paper, a crack identification method by using a commercial UAV with a high resolution vision sensor is investigated in an aging concrete bridge. First, a point cloud-based background model is generated in the preliminary flight. Then, cracks on the structural surface are detected with the deep learning algorithm, and their thickness and length are calculated. In the deep learning method, region with convolutional neural networks (R-CNN)-based transfer learning is applied. As a result, a new network for the 384 collected crack images of 256 &times; 256 pixel resolution is generated from the pre-trained network. A field test is conducted to verify the proposed approach, and the experimental results proved that the UAV-based bridge inspection is effective at identifying and quantifying the cracks on the structures.
KW  - crack identification
KW  - deep learning
KW  - unmanned aerial vehicle (UAV)
KW  - computer vision
KW  - spatial information
DO  - 10.3390/s18061881
TY  - EJOU
AU  - You, Ilsun
AU  - Kwon, Soonhyun
AU  - Choudhary, Gaurav
AU  - Sharma, Vishal
AU  - Seo, Jung T.
TI  - An Enhanced LoRaWAN Security Protocol for Privacy Preservation in IoT with a Case Study on a Smart Factory-Enabled Parking System
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 6
SN  - 1424-8220

AB  - The Internet of Things (IoT) utilizes algorithms to facilitate intelligent applications across cities in the form of smart-urban projects. As the majority of devices in IoT are battery operated, their applications should be facilitated with a low-power communication setup. Such facility is possible through the Low-Power Wide-Area Network (LPWAN), but at a constrained bit rate. For long-range communication over LPWAN, several approaches and protocols are adopted. One such protocol is the Long-Range Wide Area Network (LoRaWAN), which is a media access layer protocol for long-range communication between the devices and the application servers via LPWAN gateways. However, LoRaWAN comes with fewer security features as a much-secured protocol consumes more battery because of the exorbitant computational overheads. The standard protocol fails to support end-to-end security and perfect forward secrecy while being vulnerable to the replay attack that makes LoRaWAN limited in supporting applications where security (especially end-to-end security) is important. Motivated by this, an enhanced LoRaWAN security protocol is proposed, which not only provides the basic functions of connectivity between the application server and the end device, but additionally averts these listed security issues. The proposed protocol is developed with two options, the Default Option (DO) and the Security-Enhanced Option (SEO). The protocol is validated through Burrows&ndash;Abadi&ndash;Needham (BAN) logic and the Automated Validation of Internet Security Protocols and Applications (AVISPA) tool. The proposed protocol is also analyzed for overheads through system-based and low-power device-based evaluations. Further, a case study on a smart factory-enabled parking system is considered for its practical application. The results, in terms of network latency with reliability fitting and signaling overheads, show paramount improvements and better performance for the proposed protocol compared with the two handshake options, Pre-Shared Key (PSK) and Elliptic Curve Cryptography (ECC), of Datagram Transport Layer Security (DTLS).
KW  - LoRaWAN
KW  - privacy
KW  - IoT
KW  - smart parking
KW  - security
KW  - protocol
DO  - 10.3390/s18061888
TY  - EJOU
AU  - Wang, Ruihua
AU  - Xiao, Xiongwu
AU  - Guo, Bingxuan
AU  - Qin, Qianqing
AU  - Chen, Ruizhi
TI  - An Effective Image Denoising Method for UAV Images via Improved Generative Adversarial Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) are an inexpensive platform for collecting remote sensing images, but UAV images suffer from a content loss problem caused by noise. In order to solve the noise problem of UAV images, we propose a new methods to denoise UAV images. This paper introduces a novel deep neural network method based on generative adversarial learning to trace the mapping relationship between noisy and clean images. In our approach, perceptual reconstruction loss is used to establish a loss equation that continuously optimizes a min-max game theoretic model to obtain better UAV image denoising results. The generated denoised images by the proposed method enjoy clearer ground objects edges and more detailed textures of ground objects. In addition to the traditional comparison method, denoised UAV images and corresponding original clean UAV images were employed to perform image matching based on local features. At the same time, the classification experiment on the denoised images was also conducted to compare the denoising results of UAV images with others. The proposed method had achieved better results in these comparison experiments.
KW  - UAV images
KW  - image denoising
KW  - generative adversarial networks
KW  - perceptual reconstruction loss
DO  - 10.3390/s18071985
TY  - EJOU
AU  - Rivas, Alberto
AU  - Chamoso, Pablo
AU  - González-Briones, Alfonso
AU  - Corchado, Juan M.
TI  - Detection of Cattle Using Drones and Convolutional Neural Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Multirotor drones have been one of the most important technological advances of the last decade. Their mechanics are simple compared to other types of drones and their possibilities in flight are greater. For example, they can take-off vertically. Their capabilities have therefore brought progress to many professional activities. Moreover, advances in computing and telecommunications have also broadened the range of activities in which drones may be used. Currently, artificial intelligence and information analysis are the main areas of research in the field of computing. The case study presented in this article employed artificial intelligence techniques in the analysis of information captured by drones. More specifically, the camera installed in the drone took images which were later analyzed using Convolutional Neural Networks (CNNs) to identify the objects captured in the images. In this research, a CNN was trained to detect cattle, however the same training process could be followed to develop a CNN for the detection of any other object. This article describes the design of the platform for real-time analysis of information and its performance in the detection of cattle.
KW  - cattle detection
KW  - convolutional neural network
KW  - multirotor
KW  - drone
KW  - Unmanned Aerial Vehicle
DO  - 10.3390/s18072048
TY  - EJOU
AU  - Guerra, Edmundo
AU  - Munguía, Rodrigo
AU  - Grau, Antoni
TI  - UAV Visual and Laser Sensors Fusion for Detection and Positioning in Industrial Applications
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - This work presents a solution to localize Unmanned Autonomous Vehicles with respect to pipes and other cylindrical elements found in inspection and maintenance tasks both in industrial and civilian infrastructures. The proposed system exploits the different features of vision and laser based sensors, combining them to obtain accurate positioning of the robot with respect to the cylindrical structures. A probabilistic (RANSAC-based) procedure is used to segment possible cylinders found in the laser scans, and this is used as a seed to accurately determine the robot position through a computer vision system. The priors obtained from the laser scan registration help to solve the problem of determining the apparent contour of the cylinders. In turn this apparent contour is used in a degenerate quadratic conic estimation, enabling to visually estimate the pose of the cylinder.
KW  - Unmanned Autonomous Vehicle
KW  - pose determination
KW  - LiDAR registration
KW  - apparent contour
DO  - 10.3390/s18072071
TY  - EJOU
AU  - Huang, Huasheng
AU  - Lan, Yubin
AU  - Deng, Jizhong
AU  - Yang, Aqing
AU  - Deng, Xiaoling
AU  - Zhang, Lei
AU  - Wen, Sheng
TI  - A Semantic Labeling Approach for Accurate Weed Mapping of High Resolution UAV Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Weed control is necessary in rice cultivation, but the excessive use of herbicide treatments has led to serious agronomic and environmental problems. Suitable site-specific weed management (SSWM) is a solution to address this problem while maintaining the rice production quality and quantity. In the context of SSWM, an accurate weed distribution map is needed to provide decision support information for herbicide treatment. UAV remote sensing offers an efficient and effective platform to monitor weeds thanks to its high spatial resolution. In this work, UAV imagery was captured in a rice field located in South China. A semantic labeling approach was adopted to generate the weed distribution maps of the UAV imagery. An ImageNet pre-trained CNN with residual framework was adapted in a fully convolutional form, and transferred to our dataset by fine-tuning. Atrous convolution was applied to extend the field of view of convolutional filters; the performance of multi-scale processing was evaluated; and a fully connected conditional random field (CRF) was applied after the CNN to further refine the spatial details. Finally, our approach was compared with the pixel-based-SVM and the classical FCN-8s. Experimental results demonstrated that our approach achieved the best performance in terms of accuracy. Especially for the detection of small weed patches in the imagery, our approach significantly outperformed other methods. The mean intersection over union (mean IU), overall accuracy, and Kappa coefficient of our method were 0.7751, 0.9445, and 0.9128, respectively. The experiments showed that our approach has high potential in accurate weed mapping of UAV imagery.
KW  - UAV
KW  - remote sensing
KW  - weed mapping
KW  - Deep Fully Convolutional Network
KW  - semantic labeling
DO  - 10.3390/s18072113
TY  - EJOU
AU  - Le, Tuong
AU  - Hoang Son, Le
AU  - Vo, Minh T.
AU  - Lee, Mi Y.
AU  - Baik, Sung W.
TI  - A Cluster-Based Boosting Algorithm for Bankruptcy Prediction in a Highly Imbalanced Dataset
T2  - Symmetry

PY  - 2018
VL  - 10
IS  - 7
SN  - 2073-8994

AB  - Bankruptcy prediction has been a popular and challenging research topic in both computer science and economics due to its importance to financial institutions, fund managers, lenders, governments, as well as economic stakeholders in recent years. In a bankruptcy dataset, the problem of class imbalance, in which the number of bankruptcy companies is smaller than the number of normal companies, leads to a standard classification algorithm that does not work well. Therefore, this study proposes a cluster-based boosting algorithm as well as a robust framework using the CBoost algorithm and Instance Hardness Threshold (RFCI) for effective bankruptcy prediction of a financial dataset. This framework first resamples the imbalance dataset by the undersampling method using Instance Hardness Threshold (IHT), which is used to remove the noise instances having large IHT value in the majority class. Then, this study proposes a Cluster-based Boosting algorithm, namely CBoost, for dealing with the class imbalance. In this algorithm, the majority class will be clustered into a number of clusters. The distance from each sample to its closest centroid will be used to initialize its weight. This algorithm will perform several iterations for finding weak classifiers and combining them to create a strong classifier. The resample set resulting from the previous module, will be used to train CBoost, which will be used to predict bankruptcy for the validation set. The proposed framework is verified by the Korean bankruptcy dataset (KBD), which has a very small balancing ratio in both the training and the testing phases. The experimental results of this research show that the proposed framework achieves 86.8% in AUC (area under the ROC curve) and outperforms several methods for dealing with the imbalanced data problem for bankruptcy prediction such as GMBoost algorithm, the oversampling-based method using SMOTEENN, and the clustering-based undersampling method for bankruptcy prediction in the experimental dataset.
KW  - bankruptcy prediction
KW  - undersampling technique
KW  - cluster-based boosting
KW  - machine learning
DO  - 10.3390/sym10070250
TY  - EJOU
AU  - Guo, Xingjian
AU  - Shao, Quanqin
AU  - Li, Yuzhe
AU  - Wang, Yangchun
AU  - Wang, Dongliang
AU  - Liu, Jiyuan
AU  - Fan, Jiangwen
AU  - Yang, Fan
TI  - Application of UAV Remote Sensing for a Population Census of Large Wild Herbivores—Taking the Headwater Region of the Yellow River as an Example
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 7
SN  - 2072-4292

AB  - We used unmanned aerial vehicles (UAVs) to carry out a relatively complete population census of large wild herbivores in Maduo County on the Tibetan Plateau in the spring of 2017. The effective area covered by aerial surveys was 326.6 km2, and 23,784 images were acquired. Interpretation tag libraries for UAV images were created for wild animals, including Kiang (Equus kiang), Tibetan gazelle (Procapra picticaudata), and blue sheep (Pseudois nayaur), as well as livestock, including yaks and Tibetan sheep. Large wild herbivores in the survey transect were identified through manual imagery interpretation. Densities ranged from 1.15/km2 for Kiang, 0.61/km2 for Tibetan gazelle, 0.62/km2 for blue sheep, 4.12/km2 for domestic yak, and 7.34/km2 for domestic sheep. A method based on meadows in the cold and warm seasons was used for estimating the densities and numbers of large wild herbivores and livestock, and was verified against records of livestock numbers. Population estimates for Kiang, Tibetan gazelle, blue sheep, domestic yak, and domestic sheep were 17,109, 15,961, 9324, 70,846, and 102,194, respectively. Based on published consumption estimates, the results suggest that domestic stock consume 4.5 times the amount of vegetation of large wild herbivores. Compared with traditional ground survey methods, performance of UAV remote sensing surveys of large wild herbivore populations was fast, economical and reliable, providing an effective future method for surveying wild animals.
KW  - UAV remote sensing
KW  - Yellow River source area
KW  - large wild herbivores
KW  - population
KW  - distribution patterns
DO  - 10.3390/rs10071041
TY  - EJOU
AU  - Buscombe, Daniel
AU  - Ritchie, Andrew C.
TI  - Landscape Classification with Deep Neural Networks
T2  - Geosciences

PY  - 2018
VL  - 8
IS  - 7
SN  - 2076-3263

AB  - The application of deep learning, specifically deep convolutional neural networks (DCNNs), to the classification of remotely-sensed imagery of natural landscapes has the potential to greatly assist in the analysis and interpretation of geomorphic processes. However, the general usefulness of deep learning applied to conventional photographic imagery at a landscape scale is, at yet, largely unproven. If DCNN-based image classification is to gain wider application and acceptance within the geoscience community, demonstrable successes need to be coupled with accessible tools to retrain deep neural networks to discriminate landforms and land uses in landscape imagery. Here, we present an efficient approach to train/apply DCNNs with/on sets of photographic images, using a powerful graphical method called a conditional random field (CRF), to generate DCNN training and testing data using minimal manual supervision. We apply the method to several sets of images of natural landscapes, acquired from satellites, aircraft, unmanned aerial vehicles, and fixed camera installations. We synthesize our findings to examine the general effectiveness of transfer learning to landscape-scale image classification. Finally, we show how DCNN predictions on small regions of images might be used in conjunction with a CRF for highly accurate pixel-level classification of images.
KW  - image classification
KW  - image segmentation
KW  - land use
KW  - land cover
KW  - landforms
KW  - deep learning
KW  - machine learning
KW  - unmanned aerial systems
KW  - aerial imagery
KW  - remote sensing
DO  - 10.3390/geosciences8070244
TY  - EJOU
AU  - Oishi, Yu
AU  - Oguma, Hiroyuki
AU  - Tamura, Ayako
AU  - Nakamura, Ryosuke
AU  - Matsunaga, Tsuneo
TI  - Animal Detection Using Thermal Images and Its Required Observation Conditions
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 7
SN  - 2072-4292

AB  - Information about changes in the population sizes of wild animals is extremely important for conservation and management. Wild animal populations have been estimated using statistical methods, but it is difficult to apply such methods to large areas. To address this problem, we have developed several support systems for the automated detection of wild animals in remote sensing images. In this study, we applied one of the developed algorithms, the computer-aided detection of moving wild animals (DWA) algorithm, to thermal remote sensing images. We also performed several analyses to confirm that the DWA algorithm is useful for thermal images and to clarify the optimal conditions for obtaining thermal images (during predawn hours and on overcast days). We developed a method based on the algorithm to extract moving wild animals from thermal remote sensing images. Then, accuracy was evaluated by applying the method to airborne thermal images in a wide area. We found that the producer&rsquo;s accuracy of the method was approximately 77.3% and the user&rsquo;s accuracy of the method was approximately 29.3%. This means that the proposed method can reduce the person-hours required to survey moving wild animals from large numbers of thermal remote sensing images. Furthermore, we confirmed the extracted sika deer candidates in a pair of images and found 24 moving objects that were not identified by visual inspection by an expert. Therefore, the proposed method can also reduce oversight when identifying moving wild animals. The detection accuracy is expected to increase by setting suitable observation conditions for surveying moving wild animals. Accordingly, we also discuss the required observation conditions. The discussions about the required observation conditions would be extremely useful for people monitoring animal population changes using thermal remote sensing images.
KW  - movement
KW  - observation conditions
KW  - survey
KW  - thermal image
KW  - wild animal
DO  - 10.3390/rs10071050
TY  - EJOU
AU  - Silva, Wilson Ricardo Leal da
AU  - Lucena, Diogo Schwerz de
TI  - Concrete Cracks Detection Based on Deep Learning Image Classification
T2  - Proceedings

PY  - 2018
VL  - 2
IS  - 8
SN  - 2504-3900

AB  - This work aims at developing a machine learning-based model to detect cracks on concrete surfaces. Such model is intended to increase the level of automation on concrete infrastructure inspection when combined to unmanned aerial vehicles (UAV). The developed crack detection model relies on a deep learning convolutional neural network (CNN) image classification algorithm. Provided a relatively heterogeneous dataset, the use of deep learning enables the development of a concrete cracks detection system that can account for several conditions, e.g., different light, surface finish and humidity that a concrete surface might exhibit. These conditions are a limiting factor when working with computer vision systems based on conventional digital image processing methods. For this work, a dataset with 3500 images of concrete surfaces balanced between images with and without cracks was used. This dataset was divided into training and testing data at an 80/20 ratio. Since our dataset is rather small to enable a robust training of a complete deep learning model, a transfer-learning methodology was applied; in particular, the open-source model VGG16 was used as basis for the development of the model. The influence of the model’s parameters such as learning rate, number of nodes in the last fully connected layer and training dataset size were investigated. In each experiment, the model’s accuracy was recorded to identify the best result. For the dataset used in this work, the best experiment yielded a model with accuracy of 92.27%, showcasing the potential of using deep learning for concrete crack detection.
KW  - artificial intelligence
KW  - concrete cracks
KW  - deep learning
KW  - image classification
DO  - 10.3390/ICEM18-05387
TY  - EJOU
AU  - Näsi, Roope
AU  - Viljanen, Niko
AU  - Kaivosoja, Jere
AU  - Alhonoja, Katja
AU  - Hakala, Teemu
AU  - Markelin, Lauri
AU  - Honkavaara, Eija
TI  - Estimating Biomass and Nitrogen Amount of Barley and Grass Using UAV and Aircraft Based Spectral and Photogrammetric 3D Features
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 7
SN  - 2072-4292

AB  - The timely estimation of crop biomass and nitrogen content is a crucial step in various tasks in precision agriculture, for example in fertilization optimization. Remote sensing using drones and aircrafts offers a feasible tool to carry out this task. Our objective was to develop and assess a methodology for crop biomass and nitrogen estimation, integrating spectral and 3D features that can be extracted using airborne miniaturized multispectral, hyperspectral and colour (RGB) cameras. We used the Random Forest (RF) as the estimator, and in addition Simple Linear Regression (SLR) was used to validate the consistency of the RF results. The method was assessed with empirical datasets captured of a barley field and a grass silage trial site using a hyperspectral camera based on the Fabry-P&eacute;rot interferometer (FPI) and a regular RGB camera onboard a drone and an aircraft. Agricultural reference measurements included fresh yield (FY), dry matter yield (DMY) and amount of nitrogen. In DMY estimation of barley, the Pearson Correlation Coefficient (PCC) and the normalized Root Mean Square Error (RMSE%) were at best 0.95% and 33.2%, respectively; and in the grass DMY estimation, the best results were 0.79% and 1.9%, respectively. In the nitrogen amount estimations of barley, the PCC and RMSE% were at best 0.97% and 21.6%, respectively. In the biomass estimation, the best results were obtained when integrating hyperspectral and 3D features, but the integration of RGB images and 3D features also provided results that were almost as good. In nitrogen content estimation, the hyperspectral camera gave the best results. We concluded that the integration of spectral and high spatial resolution 3D features and radiometric calibration was necessary to optimize the accuracy.
KW  - hyperspectral
KW  - photogrammetry
KW  - UAV
KW  - drone
KW  - machine learning
KW  - random forest
KW  - regression
KW  - precision agriculture
KW  - biomass
KW  - nitrogen
DO  - 10.3390/rs10071082
TY  - EJOU
AU  - Bachmann, Daniel
AU  - Weichert, Frank
AU  - Rinkenauer, Gerhard
TI  - Review of Three-Dimensional Human-Computer Interaction with Focus on the Leap Motion Controller
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - Modern hardware and software development has led to an evolution of user interfaces from command-line to natural user interfaces for virtual immersive environments. Gestures imitating real-world interaction tasks increasingly replace classical two-dimensional interfaces based on Windows/Icons/Menus/Pointers (WIMP) or touch metaphors. Thus, the purpose of this paper is to survey the state-of-the-art Human-Computer Interaction (HCI) techniques with a focus on the special field of three-dimensional interaction. This includes an overview of currently available interaction devices, their applications of usage and underlying methods for gesture design and recognition. Focus is on interfaces based on the Leap Motion Controller (LMC) and corresponding methods of gesture design and recognition. Further, a review of evaluation methods for the proposed natural user interfaces is given.
KW  - human-computer interaction
KW  - contact-free input devices
KW  - three-dimensional interaction
KW  - natural user interfaces
KW  - leap motion controller
DO  - 10.3390/s18072194
TY  - EJOU
AU  - De Oliveira, Diulhio C.
AU  - Wehrmeister, Marco A.
TI  - Using Deep Learning and Low-Cost RGB and Thermal Cameras to Detect Pedestrians in Aerial Images Captured by Multirotor UAV
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 7
SN  - 1424-8220

AB  - The use of Unmanned Aerial Vehicles (UAV) has been increasing over the last few years in many sorts of applications due mainly to the decreasing cost of this technology. One can see the use of the UAV in several civilian applications such as surveillance and search and rescue. Automatic detection of pedestrians in aerial images is a challenging task. The computing vision system must deal with many sources of variability in the aerial images captured with the UAV, e.g., low-resolution images of pedestrians, images captured at distinct angles due to the degrees of freedom that a UAV can move, the camera platform possibly experiencing some instability while the UAV flies, among others. In this work, we created and evaluated different implementations of Pattern Recognition Systems (PRS) aiming at the automatic detection of pedestrians in aerial images captured with multirotor UAV. The main goal is to assess the feasibility and suitability of distinct PRS implementations running on top of low-cost computing platforms, e.g., single-board computers such as the Raspberry Pi or regular laptops without a GPU. For that, we used four machine learning techniques in the feature extraction and classification steps, namely Haar cascade, LBP cascade, HOG + SVM and Convolutional Neural Networks (CNN). In order to improve the system performance (especially the processing time) and also to decrease the rate of false alarms, we applied the Saliency Map (SM) and Thermal Image Processing (TIP) within the segmentation and detection steps of the PRS. The classification results show the CNN to be the best technique with 99.7% accuracy, followed by HOG + SVM with 92.3%. In situations of partial occlusion, the CNN showed 71.1% sensitivity, which can be considered a good result in comparison with the current state-of-the-art, since part of the original image data is missing. As demonstrated in the experiments, by combining TIP with CNN, the PRS can process more than two frames per second (fps), whereas the PRS that combines TIP with HOG + SVM was able to process 100 fps. It is important to mention that our experiments show that a trade-off analysis must be performed during the design of a pedestrian detection PRS. The faster implementations lead to a decrease in the PRS accuracy. For instance, by using HOG + SVM with TIP, the PRS presented the best performance results, but the obtained accuracy was 35 percentage points lower than the CNN. The obtained results indicate that the best detection technique (i.e., the CNN) requires more computational resources to decrease the PRS computation time. Therefore, this work shows and discusses the pros/cons of each technique and trade-off situations, and hence, one can use such an analysis to improve and tailor the design of a PRS to detect pedestrians in aerial images.
KW  - pedestrian detection
KW  - aerial images
KW  - Unmanned Aerial Vehicle (UAV)
KW  - thermal camera
KW  - deep learning
KW  - convolutional neural network
KW  - pattern recognition system
KW  - performance assessment
DO  - 10.3390/s18072244
TY  - EJOU
AU  - Feduck, Corey
AU  - McDermid, Gregory J.
AU  - Castilla, Guillermo
TI  - Detection of Coniferous Seedlings in UAV Imagery
T2  - Forests

PY  - 2018
VL  - 9
IS  - 7
SN  - 1999-4907

AB  - Rapid assessment of forest regeneration using unmanned aerial vehicles (UAVs) is likely to decrease the cost of establishment surveys in a variety of resource industries. This research tests the feasibility of using UAVs to rapidly identify coniferous seedlings in replanted forest-harvest areas in Alberta, Canada. In developing our protocols, we gave special consideration to creating a workflow that could perform in an operational context, avoiding comprehensive wall-to-wall surveys and complex photogrammetric processing in favor of an efficient sampling-based approach, consumer-grade cameras, and straightforward image handling. Using simple spectral decision rules from a red, green, and blue (RGB) camera, we documented a seedling detection rate of 75.8 % (n = 149), on the basis of independent test data. While moderate imbalances between the omission and commission errors suggest that our workflow has a tendency to underestimate the seedling density in a harvest block, the plot-level associations with ground surveys were very high (Pearson&rsquo;s r = 0.98; n = 14). Our results were promising enough to suggest that UAVs can be used to detect coniferous seedlings in an operational capacity with standard RGB cameras alone, although our workflow relies on seasonal leaf-off windows where seedlings are visible and spectrally distinct from their surroundings. In addition, the differential errors between the pine seedlings and spruce seedlings suggest that operational workflows could benefit from multiple decision rules designed to handle diversity in species and other sources of spectral variability.
KW  - unmanned aerial vehicles
KW  - seedling detection
KW  - forest regeneration
KW  - reforestation
KW  - establishment survey
KW  - machine learning
KW  - multispectral classification
DO  - 10.3390/f9070432
TY  - EJOU
AU  - Gopalakrishnan, Kasthurirangan
TI  - Deep Learning in Data-Driven Pavement Image Analysis and Automated Distress Detection: A Review
T2  - Data

PY  - 2018
VL  - 3
IS  - 3
SN  - 2306-5729

AB  - Deep learning, more specifically deep convolutional neural networks, is fast becoming a popular choice for computer vision-based automated pavement distress detection. While pavement image analysis has been extensively researched over the past three decades or so, recent ground-breaking achievements of deep learning algorithms in the areas of machine translation, speech recognition, and computer vision has sparked interest in the application of deep learning to automated detection of distresses in pavement images. This paper provides a narrative review of recently published studies in this field, highlighting the current achievements and challenges. A comparison of the deep learning software frameworks, network architecture, hyper-parameters employed by each study, and crack detection performance is provided, which is expected to provide a good foundation for driving further research on this important topic in the context of smart pavement or asset management systems. The review concludes with potential avenues for future research; especially in the application of deep learning to not only detect, but also characterize the type, extent, and severity of distresses from 2D and 3D pavement images.
KW  - pavement cracking
KW  - pavement management
KW  - pavement imaging
KW  - 3D image
KW  - deep learning
KW  - TensorFlow
KW  - deep convolutional neural networks
DO  - 10.3390/data3030028
TY  - EJOU
AU  - Chabot, Dominique
AU  - Dillon, Christopher
AU  - Shemrock, Adam
AU  - Weissflog, Nicholas
AU  - Sager, Eric P. S.
TI  - An Object-Based Image Analysis Workflow for Monitoring Shallow-Water Aquatic Vegetation in Multispectral Drone Imagery
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 8
SN  - 2220-9964

AB  - High-resolution drone aerial surveys combined with object-based image analysis are transforming our capacity to monitor and manage aquatic vegetation in an era of invasive species. To better exploit the potential of these technologies, there is a need to develop more efficient and accessible analysis workflows and focus more efforts on the distinct challenge of mapping submerged vegetation. We present a straightforward workflow developed to monitor emergent and submerged invasive water soldier (Stratiotes aloides) in shallow waters of the Trent-Severn Waterway in Ontario, Canada. The main elements of the workflow are: (1) collection of radiometrically calibrated multispectral imagery including a near-infrared band; (2) multistage segmentation of the imagery involving an initial separation of above-water from submerged features; and (3) automated classification of features with a supervised machine-learning classifier. The approach yielded excellent classification accuracy for emergent features (overall accuracy = 92%; kappa = 88%; water soldier producer&rsquo;s accuracy = 92%; user&rsquo;s accuracy = 91%) and good accuracy for submerged features (overall accuracy = 84%; kappa = 75%; water soldier producer&rsquo;s accuracy = 71%; user&rsquo;s accuracy = 84%). The workflow employs off-the-shelf graphical software tools requiring no programming or coding, and could therefore be used by anyone with basic GIS and image analysis skills for a potentially wide variety of aquatic vegetation monitoring operations.
KW  - environmental monitoring
KW  - freshwater ecosystems
KW  - OBIA
KW  - random forests
KW  - remote sensing
KW  - rivers
KW  - unmanned aircraft
KW  - UAS
KW  - UAV
KW  - wetlands
DO  - 10.3390/ijgi7080294
TY  - EJOU
AU  - Seo, Dae K.
AU  - Kim, Yong H.
AU  - Eo, Yang D.
AU  - Park, Wan Y.
TI  - Learning-Based Colorization of Grayscale Aerial Images Using Random Forest Regression
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 8
SN  - 2076-3417

AB  - Image colorization assigns colors to a grayscale image, which is an important yet difficult image-processing task encountered in various applications. In particular, grayscale aerial image colorization is a poorly posed problem that is affected by the sun elevation angle, seasons, sensor parameters, etc. Furthermore, since different colors may have the same intensity, it is difficult to solve this problem using traditional methods. This study proposes a novel method for the colorization of grayscale aerial images using random forest (RF) regression. The algorithm uses one grayscale image for input and one-color image for reference, both of which have similar seasonal features at the same location. The reference color image is then converted from the Red-Green-Blue (RGB) color space to the CIE L*a*b (Lab) color space in which the luminance is used to extract training pixels; this is done by performing change detection with the input grayscale image, and color information is used to establish color relationships. The proposed method directly establishes color relationships between features of the input grayscale image and color information of the reference color image based on the corresponding training pixels. The experimental results show that the proposed method outperforms several state-of-the-art algorithms in terms of both visual inspection and quantitative evaluation.
KW  - colorization
KW  - random forest regression
KW  - grayscale aerial image
KW  - change detection
DO  - 10.3390/app8081269
TY  - EJOU
AU  - Zhang, Weixing
AU  - Witharana, Chandi
AU  - Li, Weidong
AU  - Zhang, Chuanrong
AU  - Li, Xiaojiang
AU  - Parent, Jason
TI  - Using Deep Learning to Identify Utility Poles with Crossarms and Estimate Their Locations from Google Street View Images
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - Traditional methods of detecting and mapping utility poles are inefficient and costly because of the demand for visual interpretation with quality data sources or intense field inspection. The advent of deep learning for object detection provides an opportunity for detecting utility poles from side-view optical images. In this study, we proposed using a deep learning-based method for automatically mapping roadside utility poles with crossarms (UPCs) from Google Street View (GSV) images. The method combines the state-of-the-art DL object detection algorithm (i.e., the RetinaNet object detection algorithm) and a modified brute-force-based line-of-bearing (LOB, a LOB stands for the ray towards the location of the target [UPC at here] from the original location of the sensor [GSV mobile platform]) measurement method to estimate the locations of detected roadside UPCs from GSV. Experimental results indicate that: (1) both the average precision (AP) and the overall accuracy (OA) are around 0.78 when the intersection-over-union (IoU) threshold is greater than 0.3, based on the testing of 500 GSV images with a total number of 937 objects; and (2) around 2.6%, 47%, and 79% of estimated locations of utility poles are within 1 m, 5 m, and 10 m buffer zones, respectively, around the referenced locations of utility poles. In general, this study indicates that even in a complex background, most utility poles can be detected with the use of DL, and the LOB measurement method can estimate the locations of most UPCs.
KW  - deep learning
KW  - utility pole
KW  - infrastructure mapping
KW  - Google Street View
KW  - line-of-bearing measurement
KW  - object detection
DO  - 10.3390/s18082484
TY  - EJOU
AU  - Da Silva, Bruno
AU  - Braeken, An
AU  - Touhafi, Abdellah
TI  - FPGA-Based Architectures for Acoustic Beamforming with Microphone Arrays: Trends, Challenges and Research Opportunities
T2  - Computers

PY  - 2018
VL  - 7
IS  - 3
SN  - 2073-431X

AB  - Over the past decades, many systems composed of arrays of microphones have been developed to satisfy the quality demanded by acoustic applications. Such microphone arrays are sound acquisition systems composed of multiple microphones used to sample the sound field with spatial diversity. The relatively recent adoption of Field-Programmable Gate Arrays (FPGAs) to manage the audio data samples and to perform the signal processing operations such as filtering or beamforming has lead to customizable architectures able to satisfy the most demanding computational, power or performance acoustic applications. The presented work provides an overview of the current FPGA-based architectures and how FPGAs are exploited for different acoustic applications. Current trends on the use of this technology, pending challenges and open research opportunities on the use of FPGAs for acoustic applications using microphone arrays are presented and discussed.
KW  - FPGA
KW  - architectures
KW  - acoustics
KW  - microphone array
KW  - beamforming
DO  - 10.3390/computers7030041
TY  - EJOU
AU  - Wang, Yanjun
AU  - Chen, Qi
AU  - Liu, Lin
AU  - Li, Xiong
AU  - Sangaiah, Arun K.
AU  - Li, Kai
TI  - Systematic Comparison of Power Line Classification Methods from ALS and MLS Point Cloud Data
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - Power lines classification is important for electric power management and geographical objects extraction using LiDAR (light detection and ranging) point cloud data. Many supervised classification approaches have been introduced for the extraction of features such as ground, trees, and buildings, and several studies have been conducted to evaluate the framework and performance of such supervised classification methods in power lines applications. However, these studies did not systematically investigate all of the relevant factors affecting the classification results, including the segmentation scale, feature selection, classifier variety, and scene complexity. In this study, we examined these factors systematically using airborne laser scanning and mobile laser scanning point cloud data. Our results indicated that random forest and neural network were highly suitable for power lines classification in forest, suburban, and urban areas in terms of the precision, recall, and quality rates of the classification results. In contrast to some previous studies, random forest yielded the best results, while Na&iuml;ve Bayes was the worst classifier in most cases. Random forest was the more robust classifier with or without feature selection for various LiDAR point cloud data. Furthermore, the classification accuracies were directly related to the selection of the local neighborhood, classifier, and feature set. Finally, it was suggested that random forest should be considered in most cases for power line classification.
KW  - laser scanning data
KW  - power line classification
KW  - random forest
KW  - feature selection
KW  - classifier
DO  - 10.3390/rs10081222
TY  - EJOU
AU  - Zhao, Qi
AU  - Zhang, Boxue
AU  - Lyu, Shuchang
AU  - Zhang, Hong
AU  - Sun, Daniel
AU  - Li, Guoqiang
AU  - Feng, Wenquan
TI  - A CNN-SIFT Hybrid Pedestrian Navigation Method Based on First-Person Vision
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - The emergence of new wearable technologies, such as action cameras and smart glasses, has driven the use of the first-person perspective in computer applications. This field is now attracting the attention and investment of researchers aiming to develop methods to process first-person vision (FPV) video. The current approaches present particular combinations of different image features and quantitative methods to accomplish specific objectives, such as object detection, activity recognition, user&ndash;machine interaction, etc. FPV-based navigation is necessary in some special areas, where Global Position System (GPS) or other radio-wave strength methods are blocked, and is especially helpful for visually impaired people. In this paper, we propose a hybrid structure with a convolutional neural network (CNN) and local image features to achieve FPV pedestrian navigation. A novel end-to-end trainable global pooling operator, called AlphaMEX, has been designed to improve the scene classification accuracy of CNNs. A scale-invariant feature transform (SIFT)-based tracking algorithm is employed for movement estimation and trajectory tracking of the person through each frame of FPV images. Experimental results demonstrate the effectiveness of the proposed method. The top-1 error rate of the proposed AlphaMEX-ResNet outperforms the original ResNet (k = 12) by 1.7% on the ImageNet dataset. The CNN-SIFT hybrid pedestrian navigation system reaches 0.57 m average absolute error, which is an adequate accuracy for pedestrian navigation. Both positions and movements can be well estimated by the proposed pedestrian navigation algorithm with a single wearable camera.
KW  - navigation
KW  - first-person vision
KW  - CNN
KW  - SIFT
KW  - movement estimation
DO  - 10.3390/rs10081229
TY  - EJOU
AU  - Gray, Patrick C.
AU  - Ridge, Justin T.
AU  - Poulin, Sarah K.
AU  - Seymour, Alexander C.
AU  - Schwantes, Amanda M.
AU  - Swenson, Jennifer J.
AU  - Johnston, David W.
TI  - Integrating Drone Imagery into High Resolution Satellite Remote Sensing Assessments of Estuarine Environments
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - Very high-resolution satellite imagery (&le;5 m resolution) has become available on a spatial and temporal scale appropriate for dynamic wetland management and conservation across large areas. Estuarine wetlands have the potential to be mapped at a detailed habitat scale with a frequency that allows immediate monitoring after storms, in response to human disturbances, and in the face of sea-level rise. Yet mapping requires significant fieldwork to run modern classification algorithms and estuarine environments can be difficult to access and are environmentally sensitive. Recent advances in unoccupied aircraft systems (UAS, or drones), coupled with their increased availability, present a solution. UAS can cover a study site with ultra-high resolution (&lt;5 cm) imagery allowing visual validation. In this study we used UAS imagery to assist training a Support Vector Machine to classify WorldView-3 and RapidEye satellite imagery of the Rachel Carson Reserve in North Carolina, USA. UAS and field-based accuracy assessments were employed for comparison across validation methods. We created and examined an array of indices and layers including texture, NDVI, and a LiDAR DEM. Our results demonstrate classification accuracy on par with previous extensive fieldwork campaigns (93% UAS and 93% field for WorldView-3; 92% UAS and 87% field for RapidEye). Examining change between 2004 and 2017, we found drastic shoreline change but general stability of emergent wetlands. Both WorldView-3 and RapidEye were found to be valuable sources of imagery for habitat classification with the main tradeoff being WorldView&rsquo;s fine spatial resolution versus RapidEye&rsquo;s temporal frequency. We conclude that UAS can be highly effective in training and validating satellite imagery.
KW  - drones
KW  - unoccupied aircraft systems
KW  - RapidEye
KW  - WorldView-3
KW  - estuarine
KW  - wetland
KW  - change detection
KW  - LiDAR
KW  - NERR
KW  - habitat mapping
DO  - 10.3390/rs10081257
TY  - EJOU
AU  - Gallo, Mariano
AU  - De Luca, Giuseppina
TI  - Spatial Extension of Road Traffic Sensor Data with Artificial Neural Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - This paper proposes a method for estimating traffic flows on some links of a road network knowing the data on other links that are monitored with sensors. In this way, it is possible to obtain more information on traffic conditions without increasing the number of monitored links. The proposed method is based on artificial neural networks (ANNs), wherein the input data are the traffic flows on some monitored road links and the output data are the traffic flows on some unmonitored links. We have implemented and tested several single-layer feed-forward ANNs that differ in the number of neurons and the method of generating datasets for training. The proposed ANNs were trained with a supervised learning approach where input and output example datasets were generated through traffic simulation techniques. The proposed method was tested on a real-scale network and gave very good results if the travel demand patterns were known and used for generating example datasets, and promising results if the demand patterns were not considered in the procedure. Numerical results have underlined that the ANNs with few neurons were more effective than the ones with many neurons in this specific problem.
KW  - traffic sensors
KW  - smart roads
KW  - artificial neural networks
KW  - ITS
DO  - 10.3390/s18082640
TY  - EJOU
AU  - Liakos, Konstantinos G.
AU  - Busato, Patrizia
AU  - Moshou, Dimitrios
AU  - Pearson, Simon
AU  - Bochtis, Dionysis
TI  - Machine Learning in Agriculture: A Review
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.
KW  - crop management
KW  - water management
KW  - soil management
KW  - livestock management
KW  - artificial intelligence
KW  - planning
KW  - precision agriculture
DO  - 10.3390/s18082674
TY  - EJOU
AU  - Huh, Jun-Ho
TI  - PLC-Integrated Sensing Technology in Mountain Regions for Drone Landing Sites: Focusing on Software Technology
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 8
SN  - 1424-8220

AB  - In the Republic of Korea, one of the most widely discussed subjects related to future logistics technology is the drone-based delivery (transportation) system. Much (around 75%) of Korea&rsquo;s territory consists of mountainous areas; however, the costs of installing internet facilities for drone landing sites are very high compared to other countries. Therefore, this paper proposes the power-line communication (PLC) system introduced in the author&rsquo;s previous study as an alternative solution. For the system design, a number of lightning rods are used together with a monitoring system. The system algorithm performs substantial data analysis. Also, as the author found that instantaneous high-voltage currents were a major cause of fire incidents, a three-phase three-wire connection was used for the installation of the lightning rods (Bipolar Conventional Air Terminal). Thus, based on the PLC technology, an artificial intelligence (AI) which avoids lightning strikes at the drone landing site by interworking with a closed-circuit television (CCTV) monitoring system when a drone flies over the mountain regions is proposed in this paper. The algorithm was implemented with C++ and Unity/C#, whereas the application for the part concerning the integrated sensing was developed with Java Android.
KW  - natural hazards
KW  - lightning
KW  - drone landing site
KW  - Internet of Things (IoT)
KW  - artificial intelligence
KW  - AI
KW  - App
KW  - computer architecture
KW  - protocol
DO  - 10.3390/s18082693
TY  - EJOU
AU  - Malihi, Shirin
AU  - Valadan Zoej, Mohammad J.
AU  - Hahn, Michael
AU  - Mokhtarzade, Mehdi
TI  - Window Detection from UAS-Derived Photogrammetric Point Cloud Employing Density-Based Filtering and Perceptual Organization
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 8
SN  - 2072-4292

AB  - Point clouds with ever-increasing volume are regular data in 3D city modelling, in which building reconstruction is a significant part. The photogrammetric point cloud, generated from UAS (Unmanned Aerial System) imagery, is a novel type of data in building reconstruction. Its positive characteristics, alongside its challenging qualities, provoke discussions on this theme of research. In this paper, patch-wise detection of the points of window frames on facades and roofs are undertaken using this kind of data. A density-based multi-scale filter is devised in the feature space of normal vectors to globally handle the matter of high volume of data and to detect edges. Color information is employed for the downsized data to remove the inner clutter of the building. Perceptual organization directs the approach via grouping and the Gestalt principles, to segment the filtered point cloud and to later detect window patches. The evaluation of the approach displays a completeness of 95% and 92%, respectively, as well as a correctness of 95% and 96%, respectively, for the detection of rectangular and partially curved window frames in two big heterogeneous cluttered datasets. Moreover, most intrusions and protrusions cannot mislead the window detection approach. Several doors with glass parts and a number of parallel parts of the scaffolding are mistaken as windows when using the large-scale object detection approach due to their similar patterns with window frames. Sensitivity analysis of the input parameters demonstrates that the filter functionality depends on the radius of density calculation in the feature space. Furthermore, successfully employing the Gestalt principles in the detection of window frames is influenced by the width determination of window partitioning.
KW  - heterogeneous image-derived point cloud
KW  - filtering
KW  - perceptual organization
KW  - window extraction
KW  - UAV (Unmanned Aerial Vehicle)
KW  - edge detection
KW  - big datasets
KW  - normal vectors
KW  - clutter
DO  - 10.3390/rs10081320
TY  - EJOU
AU  - Liu, Shuo
AU  - Ding, Wenrui
AU  - Liu, Chunhui
AU  - Liu, Yu
AU  - Wang, Yufeng
AU  - Li, Hongguang
TI  - ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved.
KW  - CNN
KW  - deep learning
KW  - edge loss reinforced network
KW  - remote sensing
KW  - semantic segmentation
DO  - 10.3390/rs10091339
TY  - EJOU
AU  - Chen, Ting
AU  - Pennisi, Andrea
AU  - Li, Zhi
AU  - Zhang, Yanning
AU  - Sahli, Hichem
TI  - A Hierarchical Association Framework for Multi-Object Tracking in Airborne Videos
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - Multi-Object Tracking (MOT) in airborne videos is a challenging problem due to the uncertain airborne vehicle motion, vibrations of the mounted camera, unreliable detections, changes of size, appearance and motion of the moving objects and occlusions caused by the interaction between moving and static objects in the scene. To deal with these problems, this work proposes a four-stage hierarchical association framework for multiple object tracking in airborne video. The proposed framework combines Data Association-based Tracking (DAT) methods and target tracking using a compressive tracking approach, to robustly track objects in complex airborne surveillance scenes. In each association stage, different sets of tracklets and detections are associated to efficiently handle local tracklet generation, local trajectory construction, global drifting tracklet correction and global fragmented tracklet linking. Experiments with challenging airborne videos show significant tracking improvement compared to existing state-of-the-art methods.
KW  - multiple object tracking
KW  - airborne video
KW  - tracklet confidence
KW  - hierarchical association framework
DO  - 10.3390/rs10091347
TY  - EJOU
AU  - Kung, Chien-Chun
TI  - Study on Consulting Air Combat Simulation of Cluster UAV Based on Mixed Parallel Computing Framework of Graphics Processing Unit
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 9
SN  - 2079-9292

AB  - This paper combines matrix game theory with negotiating theory and uses U-solution to study the framework of the consulting air combat of UAV cluster. The processes to determine the optimal strategy in this paper follow three points: first, the UAV cluster are grouped into fleets; second, the best paring for the joint operations of the fleet member with the enemy fleet members are calculated; thirdly, consultations within the fleet are conducted to discuss the problems of optimal tactic, roles of main/assistance, and situational assessment within the fleet. In order to improve the computing efficiency of the framework, this article explores the use of the NVIDIA graphics processor programmed through MATLAB mixed C++/CUDA toolkit to accelerate the calculations of equations of motion of unmanned aerial vehicles, the prediction of superiority values and U values, computations of consultation, the evaluation of situational assessment and the optimal strategies. The effectiveness evaluation of GPGPU and CPU can be observed by the simulation results. When the number of team air combat is small, the CPU alone has better efficiency; however, when the number of air combat clusters exceeds 6 to 6, the architecture presented in this article can provide higher performance improvements and run faster than optimized CPU-only code.
KW  - GPGPU
KW  - MATLAB/CUDA
KW  - matrix game
KW  - consulting air combat
KW  - UAV cluster
KW  - maneuver decision-making
DO  - 10.3390/electronics7090160
TY  - EJOU
AU  - Wang, Yanzhao
AU  - Xiu, Chundi
AU  - Zhang, Xuanli
AU  - Yang, Dongkai
TI  - WiFi Indoor Localization with CSI Fingerprinting-Based Random Forest
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - WiFi fingerprinting indoor positioning systems have extensive applied prospects. However, a vast amount of data in a particular environment has to be gathered to establish a fingerprinting database. Deficiencies of these systems are the lack of universality of multipath effects and a burden of heavy workload on fingerprint storage. Thus, this paper presents a novel Random Forest fingerprinting localization (RFFP) method using channel state information (CSI), which utilizes the Random Forest model trained in the offline stage as fingerprints in order to economize memory space and possess a good anti-multipath characteristic. Furthermore, a series of specific experiments are conducted in a microwave anechoic chamber and an office to detail the localization performance of RFFP with different wireless channel circumstances, system parameters, algorithms, and input datasets. In addition, compared with other algorithms including K-Nearest-Neighbor (KNN), Weighted K-Nearest-Neighbor (WKNN), REPTree, CART, and J48, the RFFP method provides far greater classification accuracy as well as lower mean location error. The proposed method offers outstanding comprehensive performance including accuracy, robustness, low workload, and better anti-multipath-fading.
KW  - channel state information (CSI)
KW  - Random Forest
KW  - fingerprinting
KW  - indoor positioning
KW  - WiFi
DO  - 10.3390/s18092869
TY  - EJOU
AU  - Wang, Ruihua
AU  - Ma, Guorui
AU  - Qin, Qianqing
AU  - Shi, Qiang
AU  - Huang, Juntao
TI  - Blind UAV Images Deblurring Based on Discriminative Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) have become an important technology for acquiring high-resolution remote sensing images. Because most space optical imaging systems of UAVs work in environments affected by vibrations, the optical axis motion and image plane jitter caused by these vibrations easily result in blurring of UAV images. In the paper; we propose an advanced UAV image deblurring method based on a discriminative model comprising a classifier for blurred and sharp UAV images which is embedded into the maximum a posteriori framework as a regularization term that constantly optimizes ill-posed problem of blind image deblurring to obtain sharper UAV images. Compared with other methods, the results show that in image deblurring experiments using both simulated and real UAV images the proposed method delivers sharper images of various ground objects.
KW  - UAV images
KW  - image deblurring
KW  - image prior
KW  - discriminative networks
DO  - 10.3390/s18092874
TY  - EJOU
AU  - Lee, Jungshin
AU  - Bang, Hyochoong
TI  - A Robust Terrain Aided Navigation Using the Rao-Blackwellized Particle Filter Trained by Long Short-Term Memory Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - Terrain-aided navigation (TAN) is a technology that estimates the position of the vehicle by comparing the altitude measured by an altimeter and height from the digital elevation model (DEM). The particle filter (PF)-based TAN has been commonly used to obtain stable real-time navigation solutions in cases where the unmanned aerial vehicle (UAV) operates at a high altitude. Even though TAN performs well on rough and unique terrains, its performance degrades in flat and repetitive terrains. In particular, in the case of PF-based TAN, there has been no verified technique for deciding its terrain validity. Therefore, this study designed a Rao-Blackwellized PF (RBPF)-based TAN, used long short-term memory (LSTM) networks to endure flat and repetitive terrains, and trained the noise covariances and measurement model of RBPF. LSTM is a modified recurrent neural network (RNN), which is an artificial neural network that recognizes patterns from time series data. Using this, this study tuned the noise covariances and measurement model of RBPF to minimize the navigation errors in various flight trajectories. This paper designed a TAN algorithm based on combining RBPF and LSTM and confirmed that it can enable a more precise navigation performance than conventional RBPF based TAN through simulations.
KW  - terrain-aided navigation (TAN)
KW  - Rao-Blackwellized particle filter (RBPF)
KW  - long short-term memory (LSTM)
KW  - terrain validity check
KW  - digital elevation model (DEM)
KW  - inertial navigation system (INS)
DO  - 10.3390/s18092886
TY  - EJOU
AU  - Yu, Lingli
AU  - Shao, Xuanya
AU  - Wei, Yadong
AU  - Zhou, Kaijun
TI  - Intelligent Land-Vehicle Model Transfer Trajectory Planning Method Based on Deep Reinforcement Learning
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - To address the problem of model error and tracking dependence in the process of intelligent vehicle motion planning, an intelligent vehicle model transfer trajectory planning method based on deep reinforcement learning is proposed, which is able to obtain an effective control action sequence directly. Firstly, an abstract model of the real environment is extracted. On this basis, a deep deterministic policy gradient (DDPG) and a vehicle dynamic model are adopted to jointly train a reinforcement learning model, and to decide the optimal intelligent driving maneuver. Secondly, the actual scene is transferred to an equivalent virtual abstract scene using a transfer model. Furthermore, the control action and trajectory sequences are calculated according to the trained deep reinforcement learning model. Thirdly, the optimal trajectory sequence is selected according to an evaluation function in the real environment. Finally, the results demonstrate that the proposed method can deal with the problem of intelligent vehicle trajectory planning for continuous input and continuous output. The model transfer method improves the model&rsquo;s generalization performance. Compared with traditional trajectory planning, the proposed method outputs continuous rotation-angle control sequences. Moreover, the lateral control errors are also reduced.
KW  - intelligent driving vehicle
KW  - trajectory planning
KW  - end-to-end
KW  - deep reinforcement learning
KW  - model transfer
DO  - 10.3390/s18092905
TY  - EJOU
AU  - Wu, Jianwei
AU  - Yao, Wei
AU  - Polewski, Przemyslaw
TI  - Mapping Individual Tree Species and Vitality along Urban Road Corridors with LiDAR and Imaging Sensors: Point Density versus View Perspective
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - To meet a growing demand for accurate high-fidelity vegetation cover mapping in urban areas toward biodiversity conservation and assessing the impact of climate change, this paper proposes a complete approach to species and vitality classification at single tree level by synergistic use of multimodality 3D remote sensing data. So far, airborne laser scanning system(ALS or airborne LiDAR) has shown promising results in tree cover mapping for urban areas. This paper analyzes the potential of mobile laser scanning system/mobile mapping system (MLS/MMS)-based methods for recognition of urban plant species and characterization of growth conditions using ultra-dense LiDAR point clouds and provides an objective comparison with the ALS-based methods. Firstly, to solve the extremely intensive computational burden caused by the classification of ultra-dense MLS data, a new method for the semantic labeling of LiDAR data in the urban road environment is developed based on combining a conditional random field (CRF) for the context-based classification of 3D point clouds with shape priors. These priors encode geometric primitives found in the scene through sample consensus segmentation. Then, single trees are segmented from the labelled tree points using the 3D graph cuts algorithm. Multinomial logistic regression classifiers are used to determine the fine deciduous urban tree species of conversation concern and their growth vitality. Finally, the weight-of-evidence (WofE) based decision fusion method is applied to combine the probability outputs of classification results from the MLS and ALS data. The experiment results obtained in city road corridors demonstrated that point cloud data acquired from the airborne platform achieved even slightly better results in terms of tree detection rate, tree species and vitality classification accuracy, although the tree vitality distribution in the test site is less balanced compared to the species distribution. When combined with MLS data, overall accuracies of 78% and 74% for tree species and vitality classification can be achieved, which has improved by 5.7% and 4.64% respectively compared to the usage of airborne data only.
KW  - Ultra-dense MLS
KW  - ALS
KW  - tree species classification
KW  - vitality
KW  - CRF (conditional random field)
KW  - evidence fusion
DO  - 10.3390/rs10091403
TY  - EJOU
AU  - Tianyang, Dong
AU  - Jian, Zhang
AU  - Sibin, Gao
AU  - Ying, Shen
AU  - Jing, Fan
TI  - Single-Tree Detection in High-Resolution Remote-Sensing Images Based on a Cascade Neural Network
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 9
SN  - 2220-9964

AB  - Traditional single-tree detection methods usually need to set different thresholds and parameters manually according to different forest conditions. As a solution to the complicated detection process for non-professionals, this paper presents a single-tree detection method for high-resolution remote-sensing images based on a cascade neural network. In this method, we firstly calibrated the tree and non-tree samples in high-resolution remote-sensing images to train a classifier with the backpropagation (BP) neural network. Then, we analyzed the differences in the first-order statistic features, such as energy, entropy, mean, skewness, and kurtosis of the tree and non-tree samples. Finally, we used these features to correct the BP neural network model and build a cascade neural network classifier to detect a single tree. To verify the validity and practicability of the proposed method, six forestlands including two areas of oil palm in Thailand, and four areas of small seedlings, red maples, or longan trees in China were selected as test areas. The results from different methods, such as the region-growing method, template-matching method, BP neural network, and proposed cascade-neural-network method were compared considering these test areas. The experimental results show that the single-tree detection method based on the cascade neural network exhibited the highest root mean square of the matching rate (RMS_Rmat = 90%) and matching score (RMS_M = 68) in all the considered test areas.
KW  - single-tree detection
KW  - high-resolution
KW  - remote-sensing images
KW  - backpropagation network
KW  - cascade neural network
DO  - 10.3390/ijgi7090367
TY  - EJOU
AU  - Tao, Xian
AU  - Zhang, Dapeng
AU  - Ma, Wenzhi
AU  - Liu, Xilong
AU  - Xu, De
TI  - Automatic Metallic Surface Defect Detection and Recognition with Convolutional Neural Networks
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 9
SN  - 2076-3417

AB  - Automatic metallic surface defect inspection has received increased attention in relation to the quality control of industrial products. Metallic defect detection is usually performed against complex industrial scenarios, presenting an interesting but challenging problem. Traditional methods are based on image processing or shallow machine learning techniques, but these can only detect defects under specific detection conditions, such as obvious defect contours with strong contrast and low noise, at certain scales, or under specific illumination conditions. This paper discusses the automatic detection of metallic defects with a twofold procedure that accurately localizes and classifies defects appearing in input images captured from real industrial environments. A novel cascaded autoencoder (CASAE) architecture is designed for segmenting and localizing defects. The cascading network transforms the input defect image into a pixel-wise prediction mask based on semantic segmentation. The defect regions of segmented results are classified into their specific classes via a compact convolutional neural network (CNN). Metallic defects under various conditions can be successfully detected using an industrial dataset. The experimental results demonstrate that this method meets the robustness and accuracy requirements for metallic defect detection. Meanwhile, it can also be extended to other detection applications.
KW  - metallic surface
KW  - autoencoder
KW  - convolutional neural network
KW  - defect detection
DO  - 10.3390/app8091575
TY  - EJOU
AU  - Sa, Inkyu
AU  - Popović, Marija
AU  - Khanna, Raghav
AU  - Chen, Zetao
AU  - Lottes, Philipp
AU  - Liebisch, Frank
AU  - Nieto, Juan
AU  - Stachniss, Cyrill
AU  - Walter, Achim
AU  - Siegwart, Roland
TI  - WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB (red, green, and blue) inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.
KW  - precision farming
KW  - weed management
KW  - multispectral imaging
KW  - semantic segmentation
KW  - deep neural network
KW  - unmanned aerial vehicle
KW  - remote sensing
DO  - 10.3390/rs10091423
TY  - EJOU
AU  - Liu, Xuefeng
AU  - Sun, Qiaoqiao
AU  - Meng, Yue
AU  - Fu, Min
AU  - Bourennane, Salah
TI  - Hyperspectral Image Classification Based on Parameter-Optimized 3D-CNNs Combined with Transfer Learning and Virtual Samples
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - Recent research has shown that spatial-spectral information can help to improve the classification of hyperspectral images (HSIs). Therefore, three-dimensional convolutional neural networks (3D-CNNs) have been applied to HSI classification. However, a lack of HSI training samples restricts the performance of 3D-CNNs. To solve this problem and improve the classification, an improved method based on 3D-CNNs combined with parameter optimization, transfer learning, and virtual samples is proposed in this paper. Firstly, to optimize the network performance, the parameters of the 3D-CNN of the HSI to be classified (target data) are adjusted according to the single variable principle. Secondly, in order to relieve the problem caused by insufficient samples, the weights in the bottom layers of the parameter-optimized 3D-CNN of the target data can be transferred from another well trained 3D-CNN by a HSI (source data) with enough samples and the same feature space as the target data. Then, some virtual samples can be generated from the original samples of the target data to further alleviate the lack of HSI training samples. Finally, the parameter-optimized 3D-CNN with transfer learning can be trained by the training samples consisting of the virtual and the original samples. Experimental results on real-world hyperspectral satellite images have shown that the proposed method has great potential prospects in HSI classification.
KW  - remote sensing image
KW  - convolutional neural network
KW  - optimal parameter
KW  - lack of sample
KW  - tensor analysis
DO  - 10.3390/rs10091425
TY  - EJOU
AU  - Lotte, Rodolfo G.
AU  - Haala, Norbert
AU  - Karpina, Mateusz
AU  - Aragão, Luiz E.
AU  - Shimabukuro, Yosio E.
TI  - 3D Façade Labeling over Complex Scenarios: A Case Study Using Convolutional Neural Network and Structure-From-Motion
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - Urban environments are regions in which spectral variability and spatial variability are extremely high, with a huge range of shapes and sizes, and they also demand high resolution images for applications involving their study. Due to the fact that these environments can grow even more over time, applications related to their monitoring tend to turn to autonomous intelligent systems, which together with remote sensing data could help or even predict daily life situations. The task of mapping cities by autonomous operators was usually carried out by aerial optical images due to its scale and resolution; however new scientific questions have arisen, and this has led research into a new era of highly-detailed data extraction. For many years, using artificial neural models to solve complex problems such as automatic image classification was commonplace, owing much of their popularity to their ability to adapt to complex situations without needing human intervention. In spite of that, their popularity declined in the mid-2000s, mostly due to the complex and time-consuming nature of their methods and workflows. However, newer neural network architectures have brought back the interest in their application for autonomous classifiers, especially for image classification purposes. Convolutional Neural Networks (CNN) have been a trend for pixel-wise image segmentation, showing flexibility when detecting and classifying any kind of object, even in situations where humans failed to perceive differences, such as in city scenarios. In this paper, we aim to explore and experiment with state-of-the-art technologies to semantically label 3D urban models over complex scenarios. To achieve these goals, we split the problem into two main processing lines: first, how to correctly label the fa&ccedil;ade features in the 2D domain, where a supervised CNN is used to segment ground-based fa&ccedil;ade images into six feature classes, roof, window, wall, door, balcony and shop; second, a Structure-from-Motion (SfM) and Multi-View-Stereo (MVS) workflow is used to extract the geometry of the fa&ccedil;ade, wherein the segmented images in the previous stage are then used to label the generated mesh by a &ldquo;reverse&rdquo; ray-tracing technique. This paper demonstrates that the proposed methodology is robust in complex scenarios. The fa&ccedil;ade feature inferences have reached up to 93% accuracy over most of the datasets used. Although it still presents some deficiencies in unknown architectural styles and needs some improvements to be made regarding 3D-labeling, we present a consistent and simple methodology to handle the problem.
KW  - façade feature detection
KW  - 3D reconstruction
KW  - deep-learning
KW  - structure-from-motion
DO  - 10.3390/rs10091435
TY  - EJOU
AU  - Choi, Jongseong
AU  - Yeum, Chul M.
AU  - Dyke, Shirley J.
AU  - Jahanshahi, Mohammad R.
TI  - Computer-Aided Approach for Rapid Post-Event Visual Evaluation of a Building Façade
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - After a disaster strikes an urban area, damage to the fa&ccedil;ades of a building may produce dangerous falling hazards that jeopardize pedestrians and vehicles. Thus, building fa&ccedil;ades must be rapidly inspected to prevent potential loss of life and property damage. Harnessing the capacity to use new vision sensors and associated sensing platforms, such as unmanned aerial vehicles (UAVs) would expedite this process and alleviate spatial and temporal limitations typically associated with human-based inspection in high-rise buildings. In this paper, we have developed an approach to perform rapid and accurate visual inspection of building fa&ccedil;ades using images collected from UAVs. An orthophoto corresponding to any reasonably flat region on the building (e.g., a fa&ccedil;ade or building side) is automatically constructed using a structure-from-motion (SfM) technique, followed by image stitching and blending. Based on the geometric relationship between the collected images and the constructed orthophoto, high-resolution region-of-interest are automatically extracted from the collected images, enabling efficient visual inspection. We successfully demonstrate the capabilities of the technique using an abandoned building of which a fa&ccedil;ade has damaged building components (e.g., window panes or external drainage pipes).
KW  - post-event visual evaluation
KW  - image localization
KW  - orthophoto generation
KW  - unmanned aerial vehicle
DO  - 10.3390/s18093017
