TY  - EJOU
AU  - Bolfe, Édson L.
AU  - Jorge, Lúcio A.
AU  - Sanches, Ieda D.
AU  - Luchiari Júnior, Ariovaldo
AU  - da Costa, Cinthia C.
AU  - Victoria, Daniel D.
AU  - Inamasu, Ricardo Y.
AU  - Grego, Célia R.
AU  - Ferreira, Victor R.
AU  - Ramirez, Andrea R.
TI  - Precision and Digital Agriculture: Adoption of Technologies and Perception of Brazilian Farmers
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 12
SN  - 2077-0472

AB  - The rapid population growth has driven the demand for more food, fiber, energy, and water, which is associated to an increase in the need to use natural resources in a more sustainable way. The use of precision agriculture machinery and equipment since the 1990s has provided important productive gains and maximized the use of agricultural inputs. The growing connectivity in the rural environment, in addition to its greater integration with data from sensor systems, remote sensors, equipment, and smartphones have paved the way for new concepts from the so-called Agriculture 4.0 or Digital Agriculture. This article presents the results of a survey carried out with 504 Brazilian farmers about the digital technologies in use, as well as current and future applications, perceived benefits, and challenges. The questionnaire was prepared, organized, and made available to the public through the online platform LimeSurvey and was available from 17 April to 2 June 2020. The primary data obtained for each question previously defined were consolidated and analyzed statistically. The results indicate that 84% of the interviewed farmers use at least one digital technology in their production system that differs according to technological complexity level. The main perceived benefit refers to the perception of increased productivity and the main challenges are the acquisition costs of machines, equipment, software, and connectivity. It is also noteworthy that 95% of farmers would like to learn more about new technologies to strengthen the agricultural development in their properties.
KW  - agriculture 4.0
KW  - smart farming
KW  - farmer’s attitudes
KW  - Brazil
DO  - 10.3390/agriculture10120653
TY  - EJOU
AU  - Devaraja, Rahul R.
AU  - Maskeliūnas, Rytis
AU  - Damaševičius, Robertas
TI  - Design and Evaluation of Anthropomorphic Robotic Hand for Object Grasping and Shape Recognition
T2  - Computers

PY  - 2021
VL  - 10
IS  - 1
SN  - 2073-431X

AB  - We developed an anthropomorphic multi-finger artificial hand for a fine-scale object grasping task, sensing the grasped object&rsquo;s shape. The robotic hand was created using the 3D printer and has the servo bed for stand-alone finger movement. The data containing the robotic fingers&rsquo; angular position are acquired using the Leap Motion device, and a hybrid Support Vector Machine (SVM) classifier is used for object shape identification. We trained the designed robotic hand on a few monotonous convex-shaped items similar to everyday objects (ball, cylinder, and rectangular box) using supervised learning techniques. We achieve the mean accuracy of object shape recognition of 94.4%.
KW  - robot manipulator
KW  - shape recognition
KW  - supervised learning
KW  - object grasping
KW  - 3D printing
DO  - 10.3390/computers10010001
TY  - EJOU
AU  - Maung, Win S.
AU  - Sasaki, Jun
TI  - Assessing the Natural Recovery of Mangroves after Human Disturbance Using Neural Network Classification and Sentinel-2 Imagery in Wunbaik Mangrove Forest, Myanmar
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - In this study, we examined the natural recovery of mangroves in abandoned shrimp ponds located in the Wunbaik Mangrove Forest (WMF) in Myanmar using artificial neural network (ANN) classification and a change detection approach with Sentinel-2 satellite images. In 2020, we conducted various experiments related to mangrove classification by tuning input features and hyper-parameters. The selected ANN model was used with a transfer learning approach to predict the mangrove distribution in 2015. Changes were detected using classification results from 2015 and 2020. Naturally recovering mangroves were identified by extracting the change detection results of three abandoned shrimp ponds selected during field investigation. The proposed method yielded an overall accuracy of 95.98%, a kappa coefficient of 0.92, mangrove and non-mangrove precisions of 0.95 and 0.98, respectively, recalls of 0.96, and F1 scores of 0.96 for the 2020 classification. For the 2015 prediction, transfer learning improved model performance, resulting in an overall accuracy of 97.20%, a kappa coefficient of 0.94, mangrove and non-mangrove precisions of 0.98 and 0.96, respectively, recalls of 0.98 and 0.97, and F1 scores of 0.96. The change detection results showed that mangrove forests in the WMF slightly decreased between 2015 and 2020. Naturally recovering mangroves were detected at approximately 50% of each abandoned site within a short abandonment period. This study demonstrates that the ANN method using Sentinel-2 imagery and topographic and canopy height data can produce reliable results for mangrove classification. The natural recovery of mangroves presents a valuable opportunity for mangrove rehabilitation at human-disturbed sites in the WMF.
KW  - mangrove
KW  - natural recovery
KW  - artificial neural network
KW  - Sentinel-2
KW  - transfer learning
KW  - change detection
DO  - 10.3390/rs13010052
TY  - EJOU
AU  - Biffi, Leonardo J.
AU  - Mitishita, Edson
AU  - Liesenberg, Veraldo
AU  - Santos, Anderson A.
AU  - Gonçalves, Diogo N.
AU  - Estrabis, Nayara V.
AU  - Silva, Jonathan D.
AU  - Osco, Lucas P.
AU  - Ramos, Ana P.
AU  - Centeno, Jorge A.
AU  - Schimalski, Marcos B.
AU  - Rufato, Leo
AU  - Neto, Sílvio L.
AU  - Marcato Junior, José
AU  - Gonçalves, Wesley N.
TI  - ATSS Deep Learning-Based Approach to Detect Apple Fruits
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. Specifically, in fruit detection problems, several recent works were developed using Deep Learning (DL) methods applied in images acquired in different acquisition levels. However, the increasing use of anti-hail plastic net cover in commercial orchards highlights the importance of terrestrial remote sensing systems. Apples are one of the most highly-challenging fruits to be detected in images, mainly because of the target occlusion problem occurrence. Additionally, the introduction of high-density apple tree orchards makes the identification of single fruits a real challenge. To support farmers to detect apple fruits efficiently, this paper presents an approach based on the Adaptive Training Sample Selection (ATSS) deep learning method applied to close-range and low-cost terrestrial RGB images. The correct identification supports apple production forecasting and gives local producers a better idea of forthcoming management practices. The main advantage of the ATSS method is that only the center point of the objects is labeled, which is much more practicable and realistic than bounding-box annotations in heavily dense fruit orchards. Additionally, we evaluated other object detection methods such as RetinaNet, Libra Regions with Convolutional Neural Network (R-CNN), Cascade R-CNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), and High-Resolution Network (HRNet). The study area is a highly-dense apple orchard consisting of Fuji Suprema apple fruits (Malus domestica Borkh) located in a smallholder farm in the state of Santa Catarina (southern Brazil). A total of 398 terrestrial images were taken nearly perpendicularly in front of the trees by a professional camera, assuring both a good vertical coverage of the apple trees in terms of heights and overlapping between picture frames. After, the high-resolution RGB images were divided into several patches for helping the detection of small and/or occluded apples. A total of 3119, 840, and 2010 patches were used for training, validation, and testing, respectively. Moreover, the proposed method&rsquo;s generalization capability was assessed by applying simulated image corruptions to the test set images with different severity levels, including noise, blurs, weather, and digital processing. Experiments were also conducted by varying the bounding box size (80, 100, 120, 140, 160, and 180 pixels) in the image original for the proposed approach. Our results showed that the ATSS-based method slightly outperformed all other deep learning methods, between 2.4% and 0.3%. Also, we verified that the best result was obtained with a bounding box size of 160 &times; 160 pixels. The proposed method was robust regarding most of the corruption, except for snow, frost, and fog weather conditions. Finally, a benchmark of the reported dataset is also generated and publicly available.
KW  - convolutional neural network
KW  - object detection
KW  - precision agriculture
DO  - 10.3390/rs13010054
TY  - EJOU
AU  - Yin, Jian
AU  - Qiu, Yuanhong
AU  - Zhang, Bin
TI  - Identification of Poverty Areas by Remote Sensing and Machine Learning: A Case Study in Guizhou, Southwest China
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 1
SN  - 2220-9964

AB  - As an objective social phenomenon, poverty has accompanied the vicissitudes of human society, which is a chronic dilemma hindering human civilization. Remote sensing data, such as nighttime lights imagery, provides abundant poverty-related information that can be related to poverty. However, it may be insufficient to rely merely on nighttime lights data, because poverty is a comprehensive problem, and poverty identification may be affected by topography, especially in some developing countries or regions where agriculture accounts for a large proportion. Therefore, some geographical features may be necessary for supplements. With the support of the random forest machine learning method, we extracted 23 spatial features base on remote sensing including nighttime lights data and geographical data, and carried out the poverty identification in Guizhou Province, China, since 2012. Compared with the identifications using support vector machines and the artificial neural network, random forest showed a better accuracy. The results supported that nighttime lights and geographical features are better than those only by nighttime lights features. From 2012 to 2019, the identified poor counties in Guizhou Province showed obvious dynamic spatiotemporal characteristics. The number of poor counties has decreased consistently and contiguous poverty-stricken areas have fragmented; the number of poor counties in the northeast and southwest regions decreased faster than other areas. The reduction in poverty probability exhibited a pattern of spreading from the central and northern regions to the periphery parts. The poverty reduction was relatively slow in areas with large slope and large topographic relief. When poor counties are adjacent to more non-poor counties, they can get rid of poverty easier. This study provides a method for feature selection and recognition of poor counties by remote sensing images and offers new insights into poverty identification and regional sustainable development for other developing countries and areas.
KW  - poverty probability
KW  - random forest
KW  - nighttime lights
KW  - spatiotemporal characteristics
DO  - 10.3390/ijgi10010011
TY  - EJOU
AU  - Rahman, Mohammad F.
AU  - Fan, Shurui
AU  - Zhang, Yan
AU  - Chen, Lei
TI  - A Comparative Study on Application of Unmanned Aerial Vehicle Systems in Agriculture
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 1
SN  - 2077-0472

AB  - Presently in agriculture, there is much ample scope for drone and UAS (Unmanned Aircraft System) development. Because of their low cost and small size, these devices have the ability to help many developing countries with economic prosperity. The entire aggregation of financial investments in the agricultural area has increased appreciably in recent years. Sooth to say, agriculture remains a massive part of the world&rsquo;s commercial growth, and due to some complications, the agriculture fields withstand massive losses. Pets and destructive insects seem to be the primary reasons for certain degenerative diseases. It minimizes the potential productivity of the crops. For increasing the quality of the plants, fertilizers and pesticides are appropriately applied. Using UAVs (Unmanned Aerial Vehicles) for spraying pesticides and fertilizing materials is an exuberant contraption. It adequately reduces the rate of health dilemma and the number of workers, which is quite an impressive landmark. Willing producers are also adopting UAVs in agriculture to soil and field analysis, seed sowing, lessen the time and costs correlated with crop scouting, and field mapping. It is rapid, and it can sensibly diminish a farmer&rsquo;s workload, which is significantly a part of the agricultural revolution. This article aims to proportionally represent the concept of agricultural purposed UAV clear to the neophytes. First, this paper outlines the harmonic framework of the agricultural UAV, and then it abundantly illustrates the methods and materials. Finally, the article portrays the outcome.
KW  - UAV
KW  - unmanned aerial vehicle
KW  - agricultural UAV
KW  - NDVI (Normalized Difference Vegetation Index)
KW  - spraying system
KW  - livestock
KW  - agricultural monitoring
DO  - 10.3390/agriculture11010022
TY  - EJOU
AU  - Guo, Anting
AU  - Huang, Wenjiang
AU  - Dong, Yingying
AU  - Ye, Huichun
AU  - Ma, Huiqin
AU  - Liu, Bo
AU  - Wu, Wenbin
AU  - Ren, Yu
AU  - Ruan, Chao
AU  - Geng, Yun
TI  - Wheat Yellow Rust Detection Using UAV-Based Hyperspectral Technology
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - Yellow rust is a worldwide disease that poses a serious threat to the safety of wheat production. Numerous studies on near-surface hyperspectral remote sensing at the leaf scale have achieved good results for disease monitoring. The next step is to monitor the disease at the field scale, which is of great significance for disease control. In our study, an unmanned aerial vehicle (UAV) equipped with a hyperspectral sensor was used to obtain hyperspectral images at the field scale. Vegetation indices (VIs) and texture features (TFs) extracted from the UAV-based hyperspectral images and their combination were used to establish partial least-squares regression (PLSR)-based disease monitoring models in different infection periods. In addition, we resampled the original images with 1.2 cm spatial resolution to images with different spatial resolutions (3 cm, 5 cm, 7 cm, 10 cm, 15 cm, and 20 cm) to evaluate the effect of spatial resolution on disease monitoring accuracy. The findings showed that the VI-based model had the highest monitoring accuracy (R2 = 0.75) in the mid-infection period. The TF-based model could be used to monitor yellow rust at the field scale and obtained the highest R2 in the mid- and late-infection periods (0.65 and 0.82, respectively). The VI-TF-based models had the highest accuracy in each infection period and outperformed the VI-based or TF-based models. The spatial resolution had a negligible influence on the VI-based monitoring accuracy, but significantly influenced the TF-based monitoring accuracy. Furthermore, the optimal spatial resolution for monitoring yellow rust using the VI-TF-based model in each infection period was 10 cm. The findings provide a reference for accurate disease monitoring using UAV hyperspectral images.
KW  - UAV hyperspectral
KW  - wheat yellow rust
KW  - disease monitoring
KW  - vegetation index
KW  - texture
KW  - spatial resolution
DO  - 10.3390/rs13010123
TY  - EJOU
AU  - Papp, Levente
AU  - van Leeuwen, Boudewijn
AU  - Szilassi, Péter
AU  - Tobak, Zalán
AU  - Szatmári, József
AU  - Árvai, Mátyás
AU  - Mészáros, János
AU  - Pásztor, László
TI  - Monitoring Invasive Plant Species Using Hyperspectral Remote Sensing Data
T2  - Land

PY  - 2021
VL  - 10
IS  - 1
SN  - 2073-445X

AB  - The species richness and biodiversity of vegetation in Hungary are increasingly threatened by invasive plant species brought in from other continents and foreign ecosystems. These invasive plant species have spread aggressively in the natural and semi-natural habitats of Europe. Common milkweed (Asclepias syriaca) is one of the species that pose the greatest ecological menace. Therefore, the primary purpose of the present study is to map and monitor the spread of common milkweed, the most common invasive plant species in Europe. Furthermore, the possibilities to detect and validate this special invasive plant by analyzing hyperspectral remote sensing data were investigated. In combination with field reference data, high-resolution hyperspectral aerial images acquired by an unmanned aerial vehicle (UAV) platform in 138 spectral bands in areas infected by common milkweed were examined. Then, support vector machine (SVM) and artificial neural network (ANN) classification algorithms were applied to the highly accurate field reference data. As a result, common milkweed individuals were distinguished in hyperspectral images, achieving an overall accuracy of 92.95% in the case of supervised SVM classification. Using the ANN model, an overall accuracy of 99.61% was achieved. To evaluate the proposed approach, two experimental tests were conducted, and in both cases, we managed to distinguish the individual specimens within the large variety of spreading invasive species in a study area of 2 ha, based on centimeter spatial resolution hyperspectral UAV imagery.
KW  - invasive species
KW  - common milkweed
KW  - hyperspectral imaging
KW  - UAV
KW  - artificial neural networks
KW  - SVM classification
DO  - 10.3390/land10010029
TY  - EJOU
AU  - Roldán-Gómez, Juan J.
AU  - González-Gironda, Eduardo
AU  - Barrientos, Antonio
TI  - A Survey on Robotic Technologies for Forest Firefighting: Applying Drone Swarms to Improve Firefighters’ Efficiency and Safety
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 1
SN  - 2076-3417

AB  - Forest firefighting missions encompass multiple tasks related to prevention, surveillance, and extinguishing. This work presents a complete survey of firefighters on the current problems in their work and the potential technological solutions. Additionally, it reviews the efforts performed by the academy and industry to apply different types of robots in the context of firefighting missions. Finally, all this information is used to propose a concept of operation for the comprehensive application of drone swarms in firefighting. The proposed system is a fleet of quadcopters that individually are only able to visit waypoints and use payloads, but collectively can perform tasks of surveillance, mapping, monitoring, etc. Three operator roles are defined, each one with different access to information and functions in the mission: mission commander, team leaders, and team members. These operators take advantage of virtual and augmented reality interfaces to intuitively get the information of the scenario and, in the case of the mission commander, control the drone swarm.
KW  - robotics
KW  - multi-robot systems
KW  - swarms
KW  - drones
KW  - firefighting
DO  - 10.3390/app11010363
TY  - EJOU
AU  - Flores, Donovan
AU  - González-Hernández, Iván
AU  - Lozano, Rogelio
AU  - Vazquez-Nicolas, Jesus M.
AU  - Hernandez Toral, Jorge L.
TI  - Automated Agave Detection and Counting Using a Convolutional Neural Network and Unmanned Aerial Systems
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - We present an automatic agave detection method for counting plants based on aerial data from a UAV (Unmanned Aerial Vehicle). Our objective is to autonomously count the number of agave plants in an area to aid management of the yield. An orthomosaic is obtained from agave plantations, which is then used to create a database. This database is in turn used to train a Convolutional Neural Network (CNN). The proposed method is based on computer image processing, and the CNN increases the detection performance of the approach. The main contribution of the present paper is to propose a method for agave plant detection with a high level of precision. In order to test the proposed method in a real agave plantation, we develop a UAV platform, which is equipped with several sensors to reach accurate counting. Therefore, our prototype can safely track a desired path to detect and count agave plants. For comparison purposes, we perform the same application using a simpler algorithm. The result shows that our proposed algorithm has better performance reaching an F1 score of 0.96 as opposed to 0.57 for the Haar algorithm. The obtained experimental results suggest that the proposed algorithm is robust and has considerable potential to help farmers manage agave agroecosystems.
KW  - precision agriculture
KW  - plant detection
KW  - monitoring
KW  - deep learning
KW  - counting
DO  - 10.3390/drones5010004
TY  - EJOU
AU  - Yeh, Chia-Cheng
AU  - Chang, Yang-Lang
AU  - Alkhaleefah, Mohammad
AU  - Hsu, Pai-Hui
AU  - Eng, Weiyong
AU  - Koo, Voon-Chet
AU  - Huang, Bormin
AU  - Chang, Lena
TI  - YOLOv3-Based Matching Approach for Roof Region Detection from Drone Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - Due to the large data volume, the UAV image stitching and matching suffers from high computational cost. The traditional feature extraction algorithms&mdash;such as Scale-Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), and Oriented FAST Rotated BRIEF (ORB)&mdash;require heavy computation to extract and describe features in high-resolution UAV images. To overcome this issue, You Only Look Once version 3 (YOLOv3) combined with the traditional feature point matching algorithms is utilized to extract descriptive features from the drone dataset of residential areas for roof detection. Unlike the traditional feature extraction algorithms, YOLOv3 performs the feature extraction solely on the proposed candidate regions instead of the entire image, thus the complexity of the image matching is reduced significantly. Then, all the extracted features are fed into Structural Similarity Index Measure (SSIM) to identify the corresponding roof region pair between consecutive image sequences. In addition, the candidate corresponding roof pair by our architecture serves as the coarse matching region pair and limits the search range of features matching to only the detected roof region. This further improves the feature matching consistency and reduces the chances of wrong feature matching. Analytical results show that the proposed method is 13&times; faster than the traditional image matching methods with comparable performance.
KW  - image matching
KW  - deep learning
KW  - YOLOv3
KW  - roof region detection
KW  - drone images
KW  - high-performance computing
DO  - 10.3390/rs13010127
TY  - EJOU
AU  - Zhao, Xuehua
AU  - Lv, Hanfang
AU  - Wei, Yizhao
AU  - Lv, Shujin
AU  - Zhu, Xueping
TI  - Streamflow Forecasting via Two Types of Predictive Structure-Based Gated Recurrent Unit Models
T2  - Water

PY  - 2021
VL  - 13
IS  - 1
SN  - 2073-4441

AB  - Data-intelligent methods designed for forecasting the streamflow of the Fenhe River are crucial for enhancing water resource management. Herein, the gated recurrent unit (GRU) is coupled with the optimization algorithm improved grey wolf optimizer (IGWO) to design a hybrid model (IGWO-GRU) to carry out streamflow forecasting. Two types of predictive structure-based models (sequential IGWO-GRU and monthly IGWO-GRU) are compared with other models, such as the single least-squares support vector machine (LSSVM) and single extreme learning machine (ELM) models. These models incorporate the historical streamflow series as inputs of the model to forecast the future streamflow with data from January 1956 to December 2016 at the Shangjingyou station and from January 1958 to December 2016 at the Fenhe reservoir station. The IGWO-GRU model exhibited a strong ability for mapping in streamflow series when the parameters were carefully tuned. The monthly predictive structure can effectively extract the instinctive hydrological information that is more easily learned by the predictive model than the traditional sequential predictive structure. The monthly IGWO-GRU model was found to be a better forecasting tool, with an average qualification rate of 91.66% in two stations. It also showed good performance in absolute error and peak flow forecasting.
KW  - gated recurrent unit
KW  - improved grey wolf optimizer
KW  - monthly streamflow forecasting
KW  - data-driven modeling
DO  - 10.3390/w13010091
TY  - EJOU
AU  - Qin, Jun
AU  - Wang, Biao
AU  - Wu, Yanlan
AU  - Lu, Qi
AU  - Zhu, Haochen
TI  - Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Pine nematode is a highly contagious disease that causes great damage to the world&rsquo;s pine forest resources. Timely and accurate identification of pine nematode disease can help to control it. At present, there are few research on pine nematode disease identification, and it is difficult to accurately identify and locate nematode disease in a single pine by existing methods. This paper proposes a new network, SCANet (spatial-context-attention network), to identify pine nematode disease based on unmanned aerial vehicle (UAV) multi-spectral remote sensing images. In this method, a spatial information retention module is designed to reduce the loss of spatial information; it preserves the shallow features of pine nematode disease and expands the receptive field to enhance the extraction of deep features through a context information module. SCANet reached an overall accuracy of 79% and a precision and recall of around 0.86, and 0.91, respectively. In addition, 55 disease points among 59 known disease points were identified, which is better than other methods (DeepLab V3+, DenseNet, and HRNet). This paper presents a fast, precise, and practical method for identifying nematode disease and provides reliable technical support for the surveillance and control of pine wood nematode disease.
KW  - UAV remote sensing
KW  - pine wood nematode disease
KW  - deep learning
KW  - intelligent identifying
DO  - 10.3390/rs13020162
TY  - EJOU
AU  - Crusiol,  Luis G.
AU  - Nanni, Marcos R.
AU  - Furlanetto, Renato H.
AU  - Sibaldelli, Rubson N.
AU  - Cezar, Everson
AU  - Sun, Liang
AU  - Foloni, José S.
AU  - Mertz-Henning, Liliane M.
AU  - Nepomuceno, Alexandre L.
AU  - Neumaier, Norman
AU  - Farias, José R.
TI  - Classification of Soybean Genotypes Assessed Under Different Water Availability and at Different Phenological Stages Using Leaf-Based Hyperspectral Reflectance
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Monitoring of soybean genotypes is important because of intellectual property over seed technology, better management over seed genetics, and more efficient strategies for its agricultural production process. This paper aims at spectrally classifying soybean genotypes submitted to diverse water availability levels at different phenological stages using leaf-based hyperspectral reflectance. Leaf reflectance spectra were collected using a hyperspectral proximal sensor. Two experiments were conducted as field trials: one experiment was at Embrapa Soja in the 2016/2017, 2017/2018, and 2018/2019 cropping seasons, where ten soybean genotypes were grown under four water conditions; and another experiment was in the experimental farm of Unoeste University in the 2018/2019 cropping season, where nine soybean genotypes were evaluated. The spectral data collected was divided into nine spectral datasets, comprising single and multiple cropping seasons (from 2016 to 2019), and two contrasting crop-growing environments. Principal component analysis, applied as an indicator of the explained variance of the reflectance spectra among genotypes within each spectral dataset, explained over 94% of the spectral variance in the first three principal components. Linear discriminant analysis, used to obtain a model of classification of each reflectance spectra of soybean leaves into each soybean genotype, achieved accuracy between 61% and 100% in the calibration procedure and between 50% and 100% in the validation procedure. Misclassification was observed only between genotypes from the same genetic background. The results demonstrated the great potential of the spectral classification of soybean genotypes at leaf-scale, regardless of the phenological stages or water status to which plants were submitted.
KW  - Glycine max (L.) Merrill
KW  - drought stress
KW  - phenological stages
KW  - soybean varieties
KW  - spectral signature
KW  - principal component analysis
KW  - linear discriminant analysis
KW  - hyperspectral reflectance
DO  - 10.3390/rs13020172
TY  - EJOU
AU  - Koteluk, Oliwia
AU  - Wartecki, Adrian
AU  - Mazurek, Sylwia
AU  - Kołodziejczak, Iga
AU  - Mackiewicz, Andrzej
TI  - How Do Machines Learn? Artificial Intelligence as a New Era in Medicine
T2  - Journal of Personalized Medicine

PY  - 2021
VL  - 11
IS  - 1
SN  - 2075-4426

AB  - With an increased number of medical data generated every day, there is a strong need for reliable, automated evaluation tools. With high hopes and expectations, machine learning has the potential to revolutionize many fields of medicine, helping to make faster and more correct decisions and improving current standards of treatment. Today, machines can analyze, learn, communicate, and understand processed data and are used in health care increasingly. This review explains different models and the general process of machine learning and training the algorithms. Furthermore, it summarizes the most useful machine learning applications and tools in different branches of medicine and health care (radiology, pathology, pharmacology, infectious diseases, personalized decision making, and many others). The review also addresses the futuristic prospects and threats of applying artificial intelligence as an advanced, automated medicine tool.
KW  - machine learning
KW  - artificial intelligence
KW  - bioinformatics
KW  - medicine
KW  - algorithm
KW  - decision making
KW  - personalized medicine
KW  - data processing
KW  - data mining
KW  - personalized treatment
DO  - 10.3390/jpm11010032
TY  - EJOU
AU  - Zhao, Rongkun
AU  - Li, Yuechen
AU  - Ma, Mingguo
TI  - Mapping Paddy Rice with Satellite Remote Sensing: A Review
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 2
SN  - 2071-1050

AB  - Paddy rice is a staple food of three billion people in the world. Timely and accurate estimation of the paddy rice planting area and paddy rice yield can provide valuable information for the government, planners and decision makers to formulate policies. This article reviews the existing paddy rice mapping methods presented in the literature since 2010, classifies these methods, and analyzes and summarizes the basic principles, advantages and disadvantages of these methods. According to the data sources used, the methods are divided into three categories: (I) Optical mapping methods based on remote sensing; (II) Mapping methods based on microwave remote sensing; and (III) Mapping methods based on the integration of optical and microwave remote sensing. We found that the optical remote sensing data sources are mainly MODIS, Landsat, and Sentinel-2, and the emergence of Sentinel-1 data has promoted research on radar mapping methods for paddy rice. Multisource data integration further enhances the accuracy of paddy rice mapping. The best methods are phenology algorithms, paddy rice mapping combined with machine learning, and multisource data integration. Innovative methods include the time series similarity method, threshold method combined with mathematical models, and object-oriented image classification. With the development of computer technology and the establishment of cloud computing platforms, opportunities are provided for obtaining large-scale high-resolution rice maps. Multisource data integration, paddy rice mapping under different planting systems and the connection with global changes are the focus of future development priorities.
KW  - optical remote sensing
KW  - microwave remote sensing
KW  - phenology-based method
DO  - 10.3390/su13020503
TY  - EJOU
AU  - Kulsinskas, Andrius
AU  - Durdevic, Petar
AU  - Ortiz-Arroyo, Daniel
TI  - Internal Wind Turbine Blade Inspections Using UAVs: Analysis and Design Issues
T2  - Energies

PY  - 2021
VL  - 14
IS  - 2
SN  - 1996-1073

AB  - Interior and exterior wind turbine blade inspections are necessary to extend the lifetime of wind turbine generators. The use of unmanned vehicles is an alternative to exterior wind turbine blade inspections performed by technicians that require the use of cranes and ropes. Interior wind turbine blade inspections are even more challenging due to the confined spaces, lack of illumination, and the presence of potentially harmful internal structural components. Additionally, the cost of manned interior wind turbine blade inspections is a major limiting factor. This paper analyses all aspects of the viability of using manually controlled or autonomous aerial vehicles for interior wind turbine blade inspections. We discuss why the size, weight, and flight time of a vehicle, in addition to the structure of the wind turbine blade, are the main limiting factors in performing internal blade inspections. We also describe the design issues that must be considered to provide autonomy to unmanned vehicles and the control system, the sensors that can be used, and introduce some of the algorithms for localization, obstacle avoidance and path planning that are best suited for the task. Lastly, we briefly describe which non-destructive test instrumentation can be used for the purpose.
KW  - UAV
KW  - wind turbine inspection
KW  - autonomy
KW  - wind turbine blade
KW  - indoors UAV
DO  - 10.3390/en14020294
TY  - EJOU
AU  - Bigazzi, Luca
AU  - Gherardini, Stefano
AU  - Innocenti, Giacomo
AU  - Basso, Michele
TI  - Development of Non Expensive Technologies for Precise Maneuvering of Completely Autonomous Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - In this paper, solutions for precise maneuvering of an autonomous small (e.g., 350-class) Unmanned Aerial Vehicles (UAVs) are designed and implemented from smart modifications of non expensive mass market technologies. The considered class of vehicles suffers from light load, and, therefore, only a limited amount of sensors and computing devices can be installed on-board. Then, to make the prototype capable of moving autonomously along a fixed trajectory, a &ldquo;cyber-pilot&rdquo;, able on demand to replace the human operator, has been implemented on an embedded control board. This cyber-pilot overrides the commands thanks to a custom hardware signal mixer. The drone is able to localize itself in the environment without ground assistance by using a camera possibly mounted on a 3 Degrees Of Freedom (DOF) gimbal suspension. A computer vision system elaborates the video stream pointing out land markers with known absolute position and orientation. This information is fused with accelerations from a 6-DOF Inertial Measurement Unit (IMU) to generate a &ldquo;virtual sensor&rdquo; which provides refined estimates of the pose, the absolute position, the speed and the angular velocities of the drone. Due to the importance of this sensor, several fusion strategies have been investigated. The resulting data are, finally, fed to a control algorithm featuring a number of uncoupled digital PID controllers which work to bring to zero the displacement from the desired trajectory.
KW  - aircraft navigation
KW  - automatic control
KW  - computer vision
KW  - sensor fusion
KW  - unmanned aerial vehicles
DO  - 10.3390/s21020391
TY  - EJOU
AU  - Galyaev, Andrey A.
AU  - Lysenko, Pavel V.
AU  - Yakhno, Victor P.
TI  - 2D Optimal Trajectory Planning Problem in Threat Environment for UUV with Non-Uniform Radiation Pattern
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Path planning is necessary in many applications using unmanned underwater vehicles (UUVs). The main class of tasks is the planning of safe routes with minimal energy costs and/or minimal levels of emitted physical and information signals. Since the action planner is on board the UUV, the main focus is on methods and algorithms that allow it to build reference trajectories while minimizing the number of calculations. The study is devoted to the problem of the optimal route planning for a UUV with a non-uniform radiation pattern. The problem is stated in the form of two point variational problem for which necessary and sufficient optimality conditions are proved. Particular attention is paid to cases where optimality conditions are not met. These cases are directly related to found specific forms of a radiation pattern. Sufficient optimality conditions are extended on the class of two-link and multi-link motion paths. Software tools have been developed and computer simulations have been performed for various types of radiation patterns.
KW  - UUV path/trajectory planning
KW  - non-detection probability
KW  - non-uniform radiation pattern
DO  - 10.3390/s21020396
TY  - EJOU
AU  - Dirscherl, Mariel
AU  - Dietz, Andreas J.
AU  - Kneisel, Christof
AU  - Kuenzer, Claudia
TI  - A Novel Method for Automated Supraglacial Lake Mapping in Antarctica Using Sentinel-1 SAR Imagery and Deep Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Supraglacial meltwater accumulation on ice sheets can be a main driver for accelerated ice discharge, mass loss, and global sea-level-rise. With further increasing surface air temperatures, meltwater-induced hydrofracturing, basal sliding, or surface thinning will cumulate and most likely trigger unprecedented ice mass loss on the Greenland and Antarctic ice sheets. While the Greenland surface hydrological network as well as its impacts on ice dynamics and mass balance has been studied in much detail, Antarctic supraglacial lakes remain understudied with a circum-Antarctic record of their spatio-temporal development entirely lacking. This study provides the first automated supraglacial lake extent mapping method using Sentinel-1 synthetic aperture radar (SAR) imagery over Antarctica and complements the developed optical Sentinel-2 supraglacial lake detection algorithm presented in our companion paper. In detail, we propose the use of a modified U-Net for semantic segmentation of supraglacial lakes in single-polarized Sentinel-1 imagery. The convolutional neural network (CNN) is implemented with residual connections for optimized performance as well as an Atrous Spatial Pyramid Pooling (ASPP) module for multiscale feature extraction. The algorithm is trained on 21,200 Sentinel-1 image patches and evaluated in ten spatially or temporally independent test acquisitions. In addition, George VI Ice Shelf is analyzed for intra-annual lake dynamics throughout austral summer 2019/2020 and a decision-level fused Sentinel-1 and Sentinel-2 maximum lake extent mapping product is presented for January 2020 revealing a more complete supraglacial lake coverage (~770 km2) than the individual single-sensor products. Classification results confirm the reliability of the proposed workflow with an average Kappa coefficient of 0.925 and a F1-score of 93.0% for the supraglacial water class across all test regions. Furthermore, the algorithm is applied in an additional test region covering supraglacial lakes on the Greenland ice sheet which further highlights the potential for spatio-temporal transferability. Future work involves the integration of more training data as well as intra-annual analyses of supraglacial lake occurrence across the whole continent and with focus on supraglacial lake development throughout a summer melt season and into Antarctic winter.
KW  - Antarctica
KW  - Antarctic ice sheet
KW  - supraglacial lakes
KW  - ice sheet hydrology
KW  - Sentinel-1
KW  - remote sensing
KW  - machine learning
KW  - deep learning
KW  - semantic segmentation
KW  - convolutional neural network
DO  - 10.3390/rs13020197
TY  - EJOU
AU  - Korznikov, Kirill A.
AU  - Kislov, Dmitry E.
AU  - Altman, Jan
AU  - Doležal, Jiří
AU  - Vozmishcheva, Anna S.
AU  - Krestov, Pavel V.
TI  - Using U-Net-Like Deep Convolutional Neural Networks for Precise Tree Recognition in Very High Resolution RGB (Red, Green, Blue) Satellite Images
T2  - Forests

PY  - 2021
VL  - 12
IS  - 1
SN  - 1999-4907

AB  - Very high resolution satellite imageries provide an excellent foundation for precise mapping of plant communities and even single plants. We aim to perform individual tree recognition on the basis of very high resolution RGB (red, green, blue) satellite images using deep learning approaches for northern temperate mixed forests in the Primorsky Region of the Russian Far East. We used a pansharpened satellite RGB image by GeoEye-1 with a spatial resolution of 0.46 m/pixel, obtained in late April 2019. We parametrized the standard U-Net convolutional neural network (CNN) and trained it in manually delineated satellite images to solve the satellite image segmentation problem. For comparison purposes, we also applied standard pixel-based classification algorithms, such as random forest, k-nearest neighbor classifier, naive Bayes classifier, and quadratic discrimination. Pattern-specific features based on grey level co-occurrence matrices (GLCM) were computed to improve the recognition ability of standard machine learning methods. The U-Net-like CNN allowed us to obtain precise recognition of Mongolian poplar (Populus suaveolens Fisch. ex Loudon s.l.) and evergreen coniferous trees (Abies holophylla Maxim., Pinus koraiensis Siebold &amp; Zucc.). We were able to distinguish species belonging to either poplar or coniferous groups but were unable to separate species within the same group (i.e. A. holophylla and P. koraiensis were not distinguishable). The accuracy of recognition was estimated by several metrics and exceeded values obtained for standard machine learning approaches. In contrast to pixel-based recognition algorithms, the U-Net-like CNN does not lead to an increase in false-positive decisions when facing green-colored objects that are similar to trees. By means of U-Net-like CNN, we obtained a mean accuracy score of up to 0.96 in our computational experiments. The U-Net-like CNN recognizes tree crowns not as a set of pixels with known RGB intensities but as spatial objects with a specific geometry and pattern. This CNN&rsquo;s specific feature excludes misclassifications related to objects of similar colors as objects of interest. We highlight that utilization of satellite images obtained within the suitable phenological season is of high importance for successful tree recognition. The suitability of the phenological season is conceptualized as a group of conditions providing highlighting objects of interest over other components of vegetation cover. In our case, the use of satellite images captured in mid-spring allowed us to recognize evergreen fir and pine trees as the first class of objects (&ldquo;conifers&rdquo;) and poplars as the second class, which were in a leafless state among other deciduous tree species.
KW  - tree recognition
KW  - machine learning
KW  - convolutional neural network
DO  - 10.3390/f12010066
TY  - EJOU
AU  - Wang, Yutang
AU  - Wang, Jia
AU  - Chang, Shuping
AU  - Sun, Lu
AU  - An, Likun
AU  - Chen, Yuhan
AU  - Xu, Jiangqi
TI  - Classification of Street Tree Species Using UAV Tilt Photogrammetry
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - As an important component of the urban ecosystem, street trees have made an outstanding contribution to alleviating urban environmental pollution. Accurately extracting tree characteristics and species information can facilitate the monitoring and management of street trees, as well as aiding landscaping and studies of urban ecology. In this study, we selected the suburban areas of Beijing and Zhangjiakou and investigated six representative street tree species using unmanned aerial vehicle (UAV) tilt photogrammetry. We extracted five tree attributes and four combined attribute parameters and used four types of commonly-used machine learning classification algorithms as classifiers for tree species classification. The results show that random forest (RF), support vector machine (SVM), and back propagation (BP) neural network provide better classification results when using combined parameters for tree species classification, compared with those using individual tree attributes alone; however, the K-nearest neighbor (KNN) algorithm produced the opposite results. The best combination for classification is the BP neural network using combined attributes, with a classification precision of 89.1% and F-measure of 0.872, and we conclude that this approach best meets the requirements of street tree surveys. The results also demonstrate that optical UAV tilt photogrammetry combined with a machine learning classification algorithm is a low-cost, high-efficiency, and high-precision method for tree species classification.
KW  - tree species classification
KW  - street trees
KW  - UAV
KW  - machine learning
KW  - tilt photogrammetry
DO  - 10.3390/rs13020216
TY  - EJOU
AU  - Xu, Jin
AU  - Pan, Xinxiang
AU  - Jia, Baozhu
AU  - Wu, Xuerui
AU  - Liu, Peng
AU  - Li, Bo
TI  - Oil Spill Detection Using LBP Feature and K-Means Clustering in Shipborne Radar Image
T2  - Journal of Marine Science and Engineering

PY  - 2021
VL  - 9
IS  - 1
SN  - 2077-1312

AB  - Oil spill accidents have seriously harmed the marine environment. Effective oil spill monitoring can provide strong scientific and technological support for emergency response of law enforcement departments. Shipborne radar can be used to monitor oil spills immediately after the accident. In this paper, the original shipborne radar image collected by the teaching-practice ship Yukun of Dalian Maritime University during the oil spill accident of Dalian on 16 July 2010 was taken as the research data, and an oil spill detection method was proposed by using LBP texture feature and K-means algorithm. First, Laplacian operator, Otsu algorithm, and mean filter were used to suppress the co-frequency interference noises and high brightness pixels. Then the gray intensity correction matrix was used to reduce image nonuniformity. Next, using LBP texture feature and K-means clustering algorithm, the effective oil spill regions were extracted. Finally, the adaptive threshold was applied to identify the oil films. This method can automatically detect oil spills in shipborne radar image. It can provide a guarantee for real-time monitoring of oil spill accidents.
KW  - oil spill
KW  - LBP
KW  - K-means
KW  - shipborne radar
KW  - remote sensing
KW  - oil pollution
KW  - image analysis
KW  - machine learning
KW  - radar detection
DO  - 10.3390/jmse9010065
TY  - EJOU
AU  - Moeini, Mohammadreza
AU  - Shojaeizadeh, Ali
AU  - Geza, Mengistu
TI  - Supervised Machine Learning for Estimation of Total Suspended Solids in Urban Watersheds
T2  - Water

PY  - 2021
VL  - 13
IS  - 2
SN  - 2073-4441

AB  - Machine Learning (ML) algorithms provide an alternative for the prediction of pollutant concentration. We compared eight ML algorithms (Linear Regression (LR), uniform weighting k-Nearest Neighbor (UW-kNN), variable weighting k-Nearest Neighbor (VW-kNN), Support Vector Regression (SVR), Artificial Neural Network (ANN), Regression Tree (RT), Random Forest (RF), and Adaptive Boosting (AdB)) to evaluate the feasibility of ML approaches for estimation of Total Suspended Solids (TSS) using the national stormwater quality database. Six factors were used as features to train the algorithms with TSS concentration as the target parameter: Drainage area, land use, percent of imperviousness, rainfall depth, runoff volume, and antecedent dry days. Comparisons among the ML methods demonstrated a higher degree of variability in model performance, with the coefficient of determination (R2) and Nash&ndash;Sutcliffe (NSE) values ranging from 0.15 to 0.77. The Root Mean Square (RMSE) values ranged from 110 mg/L to 220 mg/L. The best fit was obtained using the AdB and RF models, with R2 values of 0.77 and 0.74 in the training step and 0.67 and 0.64 in the prediction step. The NSE values were 0.76 and 0.72 in the training step and 0.67 and 0.62 in the prediction step. The predictions from AdB were sensitive to all six factors. However, the sensitivity level was variable.
KW  - stormwater quality
KW  - urban watersheds
KW  - machine learning algorithms
KW  - total suspended solids
DO  - 10.3390/w13020147
TY  - EJOU
AU  - Yu, Tong
AU  - Wu, Wenjin
AU  - Gong, Chen
AU  - Li, Xinwu
TI  - Residual Multi-Attention Classification Network for A Forest Dominated Tropical Landscape Using High-Resolution Remote Sensing Imagery
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 1
SN  - 2220-9964

AB  - Tropical forests are of vital importance for maintaining biodiversity, regulating climate and material cycles while facing deforestation, agricultural reclamation, and managing various pressures. Remote sensing (RS) can support effective monitoring and mapping approaches for tropical forests, and to facilitate this we propose a deep neural network with an encoder&ndash;decoder architecture here to classify tropical forests and their environment. To deal with the complexity of tropical landscapes, this method utilizes a multi-scale convolution neural network (CNN) to expand the receptive field and extract multi-scale features. The model refines the features with several attention modules and fuses them through an upsampling module. A two-stage training strategy is proposed to alleviate misclassifications caused by sample imbalances. A joint loss function based on cross-entropy loss and the generalized Dice loss is applied in the first stage, and the second stage used the focal loss to fine-tune the weights. As a case study, we use Hainan tropical reserves to test the performance of this model. Compared with four state-of-the-art (SOTA) semantic segmentation networks, our network achieves the best performance with two Hainan datasets (mean intersection over union (MIoU) percentages of 85.78% and 82.85%). We also apply the new model to classify a public true color dataset which has 17 semantic classes and obtain results with an 83.75% MIoU. This further demonstrates the applicability and potential of this model in complex classification tasks.
KW  - remote sensing
KW  - deep convolution network
KW  - image analysis
KW  - land use and land cover (LULC)
KW  - tropical forest
DO  - 10.3390/ijgi10010022
TY  - EJOU
AU  - Canata, Tatiana F.
AU  - Wei, Marcelo C.
AU  - Maldaner, Leonardo F.
AU  - Molin, José P.
TI  - Sugarcane Yield Mapping Using High-Resolution Imagery Data and Machine Learning Technique
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Yield maps provide essential information to guide precision agriculture (PA) practices. Yet, on-board yield monitoring for sugarcane can be challenging. At the same time, orbital images have been widely used for indirect crop yield estimation for many crops like wheat, corn, and rice, but not for sugarcane. Due to this, the objective of this study is to explore the potential of multi-temporal imagery data as an alternative for sugarcane yield mapping. The study was based on developing predictive sugarcane yield models integrating time-series orbital imaging and a machine learning technique. A commercial sugarcane site was selected, and Sentinel-2 images were acquired from the beginning of the ratoon sprouting until harvesting of two consecutive cropping seasons. The predictive yield models RF (Random forest) and MLR (Multiple Linear Regression) were developed using orbital images and yield maps generated by a commercial sensor-system on harvesting. Original yield data were filtered and interpolated with the same spatial resolution of the orbital images. The entire dataset was divided into training and testing datasets. Spectral bands, especially the near-infrared at tillering crop stage showed greater contribution to predicting sugarcane yield than the use of derived spectral vegetation indices. The Root Mean Squared Error (RMSE) obtained for the RF regression based on multiple spectral bands was 4.63 Mg ha&minus;1 with an R2 of 0.70 for the testing dataset. Overall, the RF regression had better performance than the MLR to predict sugarcane yield.
KW  - orbital images
KW  - precision agriculture
KW  - remote sensing
KW  - vegetation index
DO  - 10.3390/rs13020232
TY  - EJOU
AU  - Wang, Le
AU  - Xiang, Lirong
AU  - Tang, Lie
AU  - Jiang, Huanyu
TI  - A Convolutional Neural Network-Based Method for Corn Stand Counting in the Field
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Accurate corn stand count in the field at early season is of great interest to corn breeders and plant geneticists. However, the commonly used manual counting method is time consuming, laborious, and prone to error. Nowadays, unmanned aerial vehicles (UAV) tend to be a popular base for plant-image-collecting platforms. However, detecting corn stands in the field is a challenging task, primarily because of camera motion, leaf fluttering caused by wind, shadows of plants caused by direct sunlight, and the complex soil background. As for the UAV system, there are mainly two limitations for early seedling detection and counting. First, flying height cannot ensure a high resolution for small objects. It is especially difficult to detect early corn seedlings at around one week after planting, because the plants are small and difficult to differentiate from the background. Second, the battery life and payload of UAV systems cannot support long-duration online counting work. In this research project, we developed an automated, robust, and high-throughput method for corn stand counting based on color images extracted from video clips. A pipeline developed based on the YoloV3 network and Kalman filter was used to count corn seedlings online. The results demonstrate that our method is accurate and reliable for stand counting, achieving an accuracy of over 98% at growth stages V2 and V3 (vegetative stages with two and three visible collars) with an average frame rate of 47 frames per second (FPS). This pipeline can also be mounted easily on manned cart, tractor, or field robotic systems for online corn counting.
KW  - deep learning
KW  - YoloV3
KW  - video tracking
KW  - corn stand counting
DO  - 10.3390/s21020507
TY  - EJOU
AU  - Lemaire, Pierre
AU  - Crispim-Junior, Carlos F.
AU  - Robinault, Lionel
AU  - Tougne, Laure
TI  - Registering Unmanned Aerial Vehicle Videos in the Long Term
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) have become a very popular way of acquiring video within contexts such as remote data acquisition or surveillance. Unfortunately, their viewpoint is often unstable, which tends to impact the automatic processing of their video flux negatively. To counteract the effects of an inconsistent viewpoint, two video processing strategies are classically adopted, namely registration and stabilization, which tend to be affected by distinct issues, namely jitter and drifting. Following our prior work, we suggest that the motion estimators used in both situations can be modeled to take into account their inherent errors. By acknowledging that drifting and jittery errors are of a different nature, we propose a combination that is able to limit their influence and build a hybrid solution for jitter-free video registration. In this work, however, our modeling was restricted to 2D-rigid transforms, which are rather limited in the case of airborne videos. In the present paper, we extend and refine the theoretical ground of our previous work. This addition allows us to show how to practically adapt our previous work to perspective transforms, which our study shows to be much more accurate for this problem. A lightweight implementation enables us to automatically register stationary UAV videos in real time. Our evaluation includes traffic surveillance recordings of up to 2 h and shows the potential of the proposed approach when paired with background subtraction tasks.
KW  - registration
KW  - stabilization
KW  - unmanned aerial vehicle
KW  - drone
DO  - 10.3390/s21020513
TY  - EJOU
AU  - Nguyen, Ha T.
AU  - Lopez Caceres, Maximo L.
AU  - Moritake, Koma
AU  - Kentsch, Sarah
AU  - Shu, Hase
AU  - Diez, Yago
TI  - Individual Sick Fir Tree (Abies mariesii) Identification in Insect Infested Forests by Means of UAV Images and Deep Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Insect outbreaks are a recurrent natural phenomenon in forest ecosystems expected to increase due to climate change. Recent advances in Unmanned Aerial Vehicles (UAV) and Deep Learning (DL) Networks provide us with tools to monitor them. In this study we used nine orthomosaics and normalized Digital Surface Models (nDSM) to detect and classify healthy and sick Maries fir trees as well as deciduous trees. This study aims at automatically classifying treetops by means of a novel computer vision treetops detection algorithm and the adaptation of existing DL architectures. Considering detection alone, the accuracy results showed 85.70% success. In terms of detection and classification, we were able to detect/classify correctly 78.59% of all tree classes (39.64% for sick fir). However, with data augmentation, detection/classification percentage of the sick fir class rose to 73.01% at the cost of the result accuracy of all tree classes that dropped 63.57%. The implementation of UAV, computer vision and DL techniques contribute to the development of a new approach to evaluate the impact of insect outbreaks in forest.
KW  - deep learning
KW  - computer vision
KW  - UAV
KW  - individual tree detection
KW  - tree classification
KW  - sick tree detection
DO  - 10.3390/rs13020260
TY  - EJOU
AU  - Wada, Daichi
AU  - Araujo-Estrada, Sergio A.
AU  - Windsor, Shane
TI  - Unmanned Aerial Vehicle Pitch Control Using Deep Reinforcement Learning with Discrete Actions in Wind Tunnel Test
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 1
SN  - 2226-4310

AB  - Deep reinforcement learning is a promising method for training a nonlinear attitude controller for fixed-wing unmanned aerial vehicles. Until now, proof-of-concept studies have demonstrated successful attitude control in simulation. However, detailed experimental investigations have not yet been conducted. This study applied deep reinforcement learning for one-degree-of-freedom pitch control in wind tunnel tests with the aim of gaining practical understandings of attitude control application. Three controllers with different discrete action choices, that is, elevator angles, were designed. The controllers with larger action rates exhibited better performance in terms of following angle-of-attack commands. The root mean square errors for tracking angle-of-attack commands decreased from 3.42&deg; to 1.99&deg; as the maximum action rate increased from 10&deg;/s to 50&deg;/s. The comparison between experimental and simulation results showed that the controller with a smaller action rate experienced the friction effect, and the controllers with larger action rates experienced fluctuating behaviors in elevator maneuvers owing to delay. The investigation of the effect of friction and delay on pitch control highlighted the importance of conducting experiments to understand actual control performances, specifically when the controllers were trained with a low-fidelity model.
KW  - attitude control
KW  - deep reinforcement learning
KW  - fixed-wing aircraft
KW  - unmanned aerial vehicle
KW  - wind tunnel test
DO  - 10.3390/aerospace8010018
TY  - EJOU
AU  - Zheng, Qiong
AU  - Ye, Huichun
AU  - Huang, Wenjiang
AU  - Dong, Yingying
AU  - Jiang, Hao
AU  - Wang, Chongyang
AU  - Li, Dan
AU  - Wang, Li
AU  - Chen, Shuisen
TI  - Integrating Spectral Information and Meteorological Data to Monitor Wheat Yellow Rust at a Regional Scale: A Case Study
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Wheat yellow rust has a severe impact on wheat production and threatens food security in China; as such, an effective monitoring method is necessary at the regional scale. We propose a model for yellow rust monitoring based on Sentinel-2 multispectral images and a series of two-stage vegetation indices and meteorological data. Sensitive spectral vegetation indices (single- and two-stage indices) and meteorological features for wheat yellow rust discrimination were selected using the random forest method. Wheat yellow rust monitoring models were established using three different classification methods: linear discriminant analysis (LDA), support vector machine (SVM), and artificial neural network (ANN). The results show that models based on two-stage indices (i.e., those calculated using images from two different days) significantly outperform single-stage index models (i.e., those calculated using an image from a single day), the overall accuracy improved from 63.2% to 78.9%. The classification accuracies of models combining a vegetation index with meteorological feature are higher than those of pure vegetation index models. Among them, the model based on two-stage vegetation indices and meteorological features performs best, with a classification accuracy exceeding 73.7%. The SVM algorithm performed best for wheat yellow rust monitoring among the three algorithms; its classification accuracy (84.2%) was ~10.5% and 5.3% greater than those of LDA and ANN, respectively. Combined with crop growth and environmental information, our model has great potential for monitoring wheat yellow rust at a regional scale. Future work will focus on regional-scale monitoring and forecasting of crop disease.
KW  - wheat yellow rust
KW  - vegetation indices
KW  - meteorological information
KW  - food security
KW  - regional remote sensing
DO  - 10.3390/rs13020278
TY  - EJOU
AU  - Zhang, Xiaomin
AU  - Zhao, Zhiyao
AU  - Wang, Zhaoyang
AU  - Wang, Xiaoyi
TI  - Fault Detection and Identification Method for Quadcopter Based on Airframe Vibration Signals
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Quadcopters are widely used in a variety of military and civilian mission scenarios. Real-time online detection of the abnormal state of the quadcopter is vital to the safety of aircraft. Existing data-driven fault detection methods generally usually require numerous sensors to collect data. However, quadcopter airframe space is limited. A large number of sensors cannot be loaded, meaning that it is difficult to use additional sensors to capture fault signals for quadcopters. In this paper, without additional sensors, a Fault Detection and Identification (FDI) method for quadcopter blades based on airframe vibration signals is proposed using the airborne acceleration sensor. This method integrates multi-axis data information and effectively detects and identifies quadcopter blade faults through Long and Short-Term Memory (LSTM) network models. Through flight experiments, the quadcopter triaxial accelerometer data are collected for airframe vibration signals at first. Then, the wavelet packet decomposition method is employed to extract data features, and the standard deviations of the wavelet packet coefficients are employed to form the feature vector. Finally, the LSTM-based FDI model is constructed for quadcopter blade FDI. The results show that the method can effectively detect and identify quadcopter blade faults with a better FDI performance and a higher model accuracy compared with the Back Propagation (BP) neural network-based FDI model.
KW  - quadcopter
KW  - fault detection and identification
KW  - wavelet packet decomposition
KW  - LSTM network
KW  - airframe vibration signals
DO  - 10.3390/s21020581
TY  - EJOU
AU  - Berger, Katja
AU  - Rivera Caicedo, Juan P.
AU  - Martino, Luca
AU  - Wocher, Matthias
AU  - Hank, Tobias
AU  - Verrelst, Jochem
TI  - A Survey of Active Learning for Quantifying Vegetation Traits from Terrestrial Earth Observation Data
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - The current exponential increase of spatiotemporally explicit data streams from satellite-based Earth observation missions offers promising opportunities for global vegetation monitoring. Intelligent sampling through active learning (AL) heuristics provides a pathway for fast inference of essential vegetation variables by means of hybrid retrieval approaches, i.e., machine learning regression algorithms trained by radiative transfer model (RTM) simulations. In this study we summarize AL theory and perform a brief systematic literature survey about AL heuristics used in the context of Earth observation regression problems over terrestrial targets. Across all relevant studies it appeared that: (i) retrieval accuracy of AL-optimized training data sets outperformed models trained over large randomly sampled data sets, and (ii) Euclidean distance-based (EBD) diversity method tends to be the most efficient AL technique in terms of accuracy and computational demand. Additionally, a case study is presented based on experimental data employing both uncertainty and diversity AL criteria. Hereby, a a simulated training data base by the PROSAIL-PRO canopy RTM is used to demonstrate the benefit of AL techniques for the estimation of total leaf carotenoid content (Cxc) and leaf water content (Cw). Gaussian process regression (GPR) was incorporated to minimize and optimize the training data set with AL. Training the GPR algorithm on optimally AL-based sampled data sets led to improved variable retrievals compared to training on full data pools, which is further demonstrated on a mapping example. From these findings we can recommend the use of AL-based sub-sampling procedures to select the most informative samples out of large training data pools. This will not only optimize regression accuracy due to exclusion of redundant information, but also speed up processing time and reduce final model size of kernel-based machine learning regression algorithms, such as GPR. With this study we want to encourage further testing and implementation of AL sampling methods for hybrid retrieval workflows. AL can contribute to the solution of regression problems within the framework of operational vegetation monitoring using satellite imaging spectroscopy data, and may strongly facilitate data processing for cloud-computing platforms.
KW  - Gaussian process regression
KW  - EnMAP
KW  - hyperspectral
KW  - query strategies
KW  - optimal experimental design
DO  - 10.3390/rs13020287
TY  - EJOU
AU  - Debella-Gilo, Misganu
AU  - Gjertsen, Arnt K.
TI  - Mapping Seasonal Agricultural Land Use Types Using Deep Learning on Sentinel-2 Image Time Series
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - The size and location of agricultural fields that are in active use and the type of use during the growing season are among the vital information that is needed for the careful planning and forecasting of agricultural production at national and regional scales. In areas where such data are not readily available, an independent seasonal monitoring method is needed. Remote sensing is a widely used tool to map land use types, although there are some limitations that can partly be circumvented by using, among others, multiple observations, careful feature selection and appropriate analysis methods. Here, we used Sentinel-2 satellite image time series (SITS) over the land area of Norway to map three agricultural land use classes: cereal crops, fodder crops (grass) and unused areas. The Multilayer Perceptron (MLP) and two variants of the Convolutional Neural Network (CNN), are implemented on SITS data of four different temporal resolutions. These enabled us to compare twelve model-dataset combinations to identify the model-dataset combination that results in the most accurate predictions. The CNN is implemented in the spectral and temporal dimensions instead of the conventional spatial dimension. Rather than using existing deep learning architectures, an autotuning procedure is implemented so that the model hyperparameters are empirically optimized during the training. The results obtained on held-out test data show that up to 94% overall accuracy and 90% Cohen&rsquo;s Kappa can be obtained when the 2D CNN is applied on the SITS data with a temporal resolution of 7 days. This is closely followed by the 1D CNN on the same dataset. However, the latter performs better than the former in predicting data outside the training set. It is further observed that cereal is predicted with the highest accuracy, followed by grass. Predicting the unused areas has been found to be difficult as there is no distinct surface condition that is common for all unused areas.
KW  - multilayer perceptron
KW  - CNN
KW  - hyperparameter tuning
KW  - cereal
KW  - grass
DO  - 10.3390/rs13020289
TY  - EJOU
AU  - Teng, Shuai
AU  - Liu, Zongchao
AU  - Chen, Gongfa
AU  - Cheng, Li
TI  - Concrete Crack Detection Based on Well-Known Feature Extractor Model and the YOLO_v2 Network
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 2
SN  - 2076-3417

AB  - This paper compares the crack detection performance (in terms of precision and computational cost) of the YOLO_v2 using 11 feature extractors, which provides a base for realizing fast and accurate crack detection on concrete structures. Cracks on concrete structures are an important indicator for assessing their durability and safety, and real-time crack detection is an essential task in structural maintenance. The object detection algorithm, especially the YOLO series network, has significant potential in crack detection, while the feature extractor is the most important component of the YOLO_v2. Hence, this paper employs 11 well-known CNN models as the feature extractor of the YOLO_v2 for crack detection. The results confirm that a different feature extractor model of the YOLO_v2 network leads to a different detection result, among which the AP value is 0.89, 0, and 0 for &lsquo;resnet18&rsquo;, &lsquo;alexnet&rsquo;, and &lsquo;vgg16&rsquo;, respectively meanwhile, the &lsquo;googlenet&rsquo; (AP = 0.84) and &lsquo;mobilenetv2&rsquo; (AP = 0.87) also demonstrate comparable AP values. In terms of computing speed, the &lsquo;alexnet&rsquo; takes the least computational time, the &lsquo;squeezenet&rsquo; and &lsquo;resnet18&rsquo; are ranked second and third respectively; therefore, the &lsquo;resnet18&rsquo; is the best feature extractor model in terms of precision and computational cost. Additionally, through the parametric study (influence on detection results of the training epoch, feature extraction layer, and testing image size), the associated parameters indeed have an impact on the detection results. It is demonstrated that: excellent crack detection results can be achieved by the YOLO_v2 detector, in which an appropriate feature extractor model, training epoch, feature extraction layer, and testing image size play an important role.
KW  - crack detection
KW  - YOLO network
KW  - feature extractor
KW  - feature extraction layer
KW  - computational cost
KW  - detection precision
DO  - 10.3390/app11020813
TY  - EJOU
AU  - Yang, Baohua
AU  - Ma, Jifeng
AU  - Yao, Xia
AU  - Cao, Weixing
AU  - Zhu, Yan
TI  - Estimation of Leaf Nitrogen Content in Wheat Based on Fusion of Spectral Features and Deep Features from Near Infrared Hyperspectral Imagery
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Nitrogen is an important indicator for monitoring wheat growth. The rapid development and wide application of non-destructive detection provide many approaches for estimating leaf nitrogen content (LNC) in wheat. Previous studies have shown that better results have been obtained in the estimation of LNC in wheat based on spectral features. However, the lack of automatically extracted features leads to poor universality of the estimation model. Therefore, a feature fusion method for estimating LNC in wheat by combining spectral features with deep features (spatial features) was proposed. The deep features were automatically obtained with a convolutional neural network model based on the PyTorch framework. The spectral features were obtained using spectral information including position features (PFs) and vegetation indices (VIs). Different models based on feature combination for evaluating LNC in wheat were constructed: partial least squares regression (PLS), gradient boosting decision tree (GBDT), and support vector regression (SVR). The results indicate that the model based on the fusion feature from near-ground hyperspectral imagery has good estimation effect. In particular, the estimation accuracy of the GBDT model is the best (R2 = 0.975 for calibration set, R2 = 0.861 for validation set). These findings demonstrate that the approach proposed in this study improved the estimation performance of LNC in wheat, which could provide technical support in wheat growth monitoring.
KW  - convolutional neural network
KW  - leaf nitrogen content
KW  - deep features
KW  - wheat
KW  - spectral features
DO  - 10.3390/s21020613
TY  - EJOU
AU  - Li, Haolu
AU  - Wang, Guojie
AU  - Dong, Zhen
AU  - Wei, Xikun
AU  - Wu, Mengjuan
AU  - Song, Huihui
AU  - Amankwah, Solomon O.
TI  - Identifying Cotton Fields from Remote Sensing Images Using Multiple Deep Learning Networks
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 1
SN  - 2073-4395

AB  - Remote sensing imageries processed through empirical and deterministic approaches help predict multiple agronomic traits throughout the growing season. Accurate identification of cotton crop from remotely sensed imageries is a significant task in precision agriculture. This study aims to utilize a deep learning-based framework for cotton crop field identification with Gaofen-1 (GF-1) high-resolution (16 m) imageries in Wei-Ku region, China. An optimized model for the pixel-wise multidimensional densely connected convolutional neural network (DenseNet) was used. Four widely-used classic convolutional neural networks (CNNs), including ResNet, VGG, SegNet, and DeepLab v3+, were also used for accuracy assessment. The results infer that DenseNet can identify cotton crop features within a relatively shorter time about 5 h for training convergence. The model performance was examined by multiple indicators (P, F1, R, and mIou) produced through the confusion matrix, and the derived cotton fields were then visualized. The DenseNet model has illustrated considerable improvements in comparison with the preceding mainstream models. The results showed that the retrieval precision was 0.948, F1 score was 0.953, and mIou was 0.911. Furthermore, its performance is relatively better in discriminating cotton crop fields&rsquo; fine structures when clouds, mountain shadows, and urban built up.
KW  - cotton identification
KW  - deep learning
KW  - DenseNet
KW  - remote sensing images
DO  - 10.3390/agronomy11010174
TY  - EJOU
AU  - Zou, Kunlin
AU  - Chen, Xin
AU  - Zhang, Fan
AU  - Zhou, Hang
AU  - Zhang, Chunlong
TI  - A Field Weed Density Evaluation Method Based on UAV Imaging and Modified U-Net
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Weeds are one of the main factors affecting the yield and quality of agricultural products. Accurate evaluation of weed density is of great significance for field management, especially precision weeding. In this paper, a weed density calculating and mapping method in the field is proposed. An unmanned aerial vehicle (UAV) was used to capture field images. The excess green minus excess red index, combined with the minimum error threshold segmentation method, was used to segment green plants and bare land. A modified U-net was used to segment crops from images. After removing the bare land and crops from the field, images of weeds were obtained. The weed density was evaluated by the ratio of weed area to total area on the segmented image. The accuracy of the green plant segmentation was 93.5%. In terms of crop segmentation, the intersection over union (IoU) was 93.40%, and the segmentation time of a single image was 35.90 ms. Finally, the determination coefficient of the UAV evaluated weed density and the manually observed weed density was 0.94, and the root mean square error was 0.03. With the proposed method, the weed density of a field can be effectively evaluated from UAV images, hence providing critical information for precision weeding.
KW  - semantic segmentation
KW  - U-net
KW  - UAV
KW  - weed density
DO  - 10.3390/rs13020310
TY  - EJOU
AU  - Ni, Ming
AU  - Wang, Hongjie
AU  - Liu, Xudong
AU  - Liao, Yilin
AU  - Fu, Lin
AU  - Wu, Qianqian
AU  - Mu, Jiong
AU  - Chen, Xiaoyan
AU  - Li, Jun
TI  - Design of Variable Spray System for Plant Protection UAV Based on CFD Simulation and Regression Analysis
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Multi-rotor unmanned aerial vehicles (UAVs) for plant protection are widely used in China&rsquo;s agricultural production. However, spray droplets often drift and distribute nonuniformly, thereby harming its utilization and the environment. A variable spray system is designed, discussed, and verified to solve this problem. The distribution characteristics of droplet deposition under different spray states (flight state, environment state, nozzle state) are obtained through computational fluid dynamics simulation. In the verification experiment, the wind velocity error of most sample points is less than 1 m/s, and the deposition ratio error is less than 10%, indicating that the simulation is reliable. A simulation data set is used to train support vector regression and back propagation neural network with multiple parameters. An optimal regression model with the root mean square error of 6.5% is selected. The UAV offset and nozzle flow of the variable spray system can be obtained in accordance with the current spray state by multi-sensor fusion and the predicted deposition distribution characteristics. The farmland experiment shows that the deposition volume error between the prediction and experiment is within 30%, thereby proving the effectiveness of the system. This article provides a reference for the improvement of UAV intelligent spray system.
KW  - aviation plant protection
KW  - downwash wind field
KW  - deposition distribution characteristic
KW  - support vector regression
KW  - back propagation neural network
KW  - farmland experiment
DO  - 10.3390/s21020638
TY  - EJOU
AU  - Kadhim, Israa
AU  - Abed, Fanar M.
TI  - The Potential of LiDAR and UAV-Photogrammetric Data Analysis to Interpret Archaeological Sites: A Case Study of Chun Castle in South-West England
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 1
SN  - 2220-9964

AB  - With the increasing demands to use remote sensing approaches, such as aerial photography, satellite imagery, and LiDAR in archaeological applications, there is still a limited number of studies assessing the differences between remote sensing methods in extracting new archaeological finds. Therefore, this work aims to critically compare two types of fine-scale remotely sensed data: LiDAR and an Unmanned Aerial Vehicle (UAV) derived Structure from Motion (SfM) photogrammetry. To achieve this, aerial imagery and airborne LiDAR datasets of Chun Castle were acquired, processed, analyzed, and interpreted. Chun Castle is one of the most remarkable ancient sites in Cornwall County (Southwest England) that had not been surveyed and explored by non-destructive techniques. The work outlines the approaches that were applied to the remotely sensed data to reveal potential remains: Visualization methods (e.g., hillshade and slope raster images), ISODATA clustering, and Support Vector Machine (SVM) algorithms. The results display various archaeological remains within the study site that have been successfully identified. Applying multiple methods and algorithms have successfully improved our understanding of spatial attributes within the landscape. The outcomes demonstrate how raster derivable from inexpensive approaches can be used to identify archaeological remains and hidden monuments, which have the possibility to revolutionize archaeological understanding.
KW  - archaeology
KW  - automatic detection
KW  - Chun Castle
KW  - drone
KW  - hidden features
KW  - Iron Age
KW  - LiDAR
KW  - SfM-photogrammetry
KW  - remote sensing
KW  - RRIMs
KW  - visualization methods
DO  - 10.3390/ijgi10010041
TY  - EJOU
AU  - Cesco, Stefano
AU  - Pii, Youry
AU  - Borruso, Luigimaria
AU  - Orzes, Guido
AU  - Lugli, Paolo
AU  - Mazzetto, Fabrizio
AU  - Genova, Giulio
AU  - Signorini, Marco
AU  - Brunetto, Gustavo
AU  - Terzano, Roberto
AU  - Vigani, Gianpiero
AU  - Mimmo, Tanja
TI  - A Smart and Sustainable Future for Viticulture Is Rooted in Soil: How to Face Cu Toxicity
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 3
SN  - 2076-3417

AB  - In recent decades, agriculture has faced the fundamental challenge of needing to increase food production and quality in order to meet the requirements of a growing global population. Similarly, viticulture has also been undergoing change. Several countries are reducing their vineyard areas, and several others are increasing them. In addition, viticulture is moving towards higher altitudes and latitudes due to climate change. Furthermore, global warming is also exacerbating the incidence of fungal diseases in vineyards, forcing farmers to apply agrochemicals to preserve production yields and quality. The repeated application of copper (Cu)-based fungicides in conventional and organic farming has caused a stepwise accumulation of Cu in vineyard soils, posing environmental and toxicological threats. High Cu concentrations in soils can have multiple impacts on agricultural systems. In fact, it can (i) alter the chemical-physical properties of soils, thus compromising their fertility; (ii) induce toxicity phenomena in plants, producing detrimental effects on growth and productivity; and (iii) affect the microbial biodiversity of soils, thereby influencing some microbial-driven soil processes. However, several indirect (e.g., management of rhizosphere processes through intercropping and/or fertilization strategies) and direct (e.g., exploitation of vine resistant genotypes) strategies have been proposed to restrain Cu accumulation in soils. Furthermore, the application of precision and smart viticulture paradigms and their related technologies could allow a timely, localized and balanced distribution of agrochemicals to achieve the required goals. The present review highlights the necessity of applying multidisciplinary approaches to meet the requisites of sustainability demanded of modern viticulture.
KW  - copper
KW  - rhizosphere
KW  - smart agriculture
KW  - microbes
KW  - vineyard
DO  - 10.3390/app11030907
TY  - EJOU
AU  - Hong, Jin
AU  - Kwon, Junseok
TI  - Visual Tracking of Small Unmanned Aerial Vehicles Based on Object Proposal Voting
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 3
SN  - 2076-3417

AB  - In this paper, we propose a novel visual tracking method for unmanned aerial vehicles (UAVs) in aerial scenery. To track the UAVs robustly, we present a new object proposal method that can accurately determine the object regions that are likely to exist. The proposed object proposal method is robust to small objects and severe background clutter. For this, we vote on candidate areas of the object and increase or decrease the weight of the area accordingly. Thus, the method can accurately propose the object areas that can be used to track small-sized UAVs with the assumption that their motion is smooth over time. Experimental results verify that UAVs are accurately tracked even when they are very small and the background is complex. The proposed method qualitatively and quantitatively delivers state-of-the-art performance in comparison with conventional object proposal-based methods.
KW  - unmanned aerial vehicles
KW  - object tracking
KW  - object proposal voting
DO  - 10.3390/app11030953
TY  - EJOU
AU  - Butcher, Paul A.
AU  - Colefax, Andrew P.
AU  - Gorkin, Robert A.
AU  - Kajiura, Stephen M.
AU  - López, Naima A.
AU  - Mourier, Johann
AU  - Purcell, Cormac R.
AU  - Skomal, Gregory B.
AU  - Tucker, James P.
AU  - Walsh, Andrew J.
AU  - Williamson, Jane E.
AU  - Raoult, Vincent
TI  - The Drone Revolution of Shark Science: A Review
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - Over the past decade, drones have become a popular tool for wildlife management and research. Drones have shown significant value for animals that were often difficult or dangerous to study using traditional survey methods. In the past five years drone technology has become commonplace for shark research with their use above, and more recently, below the water helping to minimise knowledge gaps about these cryptic species. Drones have enhanced our understanding of shark behaviour and are critically important tools, not only due to the importance and conservation of the animals in the ecosystem, but to also help minimise dangerous encounters with humans. To provide some guidance for their future use in relation to sharks, this review provides an overview of how drones are currently used with critical context for shark monitoring. We show how drones have been used to fill knowledge gaps around fundamental shark behaviours or movements, social interactions, and predation across multiple species and scenarios. We further detail the advancement in technology across sensors, automation, and artificial intelligence that are improving our abilities in data collection and analysis and opening opportunities for shark-related beach safety. An investigation of the shark-based research potential for underwater drones (ROV/AUV) is also provided. Finally, this review provides baseline observations that have been pioneered for shark research and recommendations for how drones might be used to enhance our knowledge in the future.
KW  - artificial intelligence
KW  - AUV
KW  - drones
KW  - protocols
KW  - ROV
KW  - sharks
KW  - UAV
DO  - 10.3390/drones5010008
TY  - EJOU
AU  - Nguyen, Canh
AU  - Sagan, Vasit
AU  - Maimaitiyiming, Matthew
AU  - Maimaitijiang, Maitiniyazi
AU  - Bhadra, Sourav
AU  - Kwasniewski, Misha T.
TI  - Early Detection of Plant Viral Disease Using Hyperspectral Imaging and Deep Learning
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - Early detection of grapevine viral diseases is critical for early interventions in order to prevent the disease from spreading to the entire vineyard. Hyperspectral remote sensing can potentially detect and quantify viral diseases in a nondestructive manner. This study utilized hyperspectral imagery at the plant level to identify and classify grapevines inoculated with the newly discovered DNA virus grapevine vein-clearing virus (GVCV) at the early asymptomatic stages. An experiment was set up at a test site at South Farm Research Center, Columbia, MO, USA (38.92 N, &minus;92.28 W), with two grapevine groups, namely healthy and GVCV-infected, while other conditions were controlled. Images of each vine were captured by a SPECIM IQ 400&ndash;1000 nm hyperspectral sensor (Oulu, Finland). Hyperspectral images were calibrated and preprocessed to retain only grapevine pixels. A statistical approach was employed to discriminate two reflectance spectra patterns between healthy and GVCV vines. Disease-centric vegetation indices (VIs) were established and explored in terms of their importance to the classification power. Pixel-wise (spectral features) classification was performed in parallel with image-wise (joint spatial&ndash;spectral features) classification within a framework involving deep learning architectures and traditional machine learning. The results showed that: (1) the discriminative wavelength regions included the 900&ndash;940 nm range in the near-infrared (NIR) region in vines 30 days after sowing (DAS) and the entire visual (VIS) region of 400&ndash;700 nm in vines 90 DAS; (2) the normalized pheophytization index (NPQI), fluorescence ratio index 1 (FRI1), plant senescence reflectance index (PSRI), anthocyanin index (AntGitelson), and water stress and canopy temperature (WSCT) measures were the most discriminative indices; (3) the support vector machine (SVM) was effective in VI-wise classification with smaller feature spaces, while the RF classifier performed better in pixel-wise and image-wise classification with larger feature spaces; and (4) the automated 3D convolutional neural network (3D-CNN) feature extractor provided promising results over the 2D convolutional neural network (2D-CNN) in learning features from hyperspectral data cubes with a limited number of samples.
KW  - plant disease
KW  - spectral statistics
KW  - machine learning
KW  - 2D-CNN
KW  - 3D-CNN
KW  - grapevine vein-clearing virus (GVCV)
DO  - 10.3390/s21030742
TY  - EJOU
AU  - Chen, Xinxin
AU  - Jiang, Kang
AU  - Zhu, Yushi
AU  - Wang, Xiangjun
AU  - Yun, Ting
TI  - Individual Tree Crown Segmentation Directly from UAV-Borne LiDAR Data Using the PointNet of Deep Learning
T2  - Forests

PY  - 2021
VL  - 12
IS  - 2
SN  - 1999-4907

AB  - Accurate individual tree crown (ITC) segmentation from scanned point clouds is a fundamental task in forest biomass monitoring and forest ecology management. Light detection and ranging (LiDAR) as a mainstream tool for forest survey is advancing the pattern of forest data acquisition. In this study, we performed a novel deep learning framework directly processing the forest point clouds belonging to the four forest types (i.e., the nursery base, the monastery garden, the mixed forest, and the defoliated forest) to realize the ITC segmentation. The specific steps of our approach were as follows: first, a voxelization strategy was conducted to subdivide the collected point clouds with various tree species from various forest types into many voxels. These voxels containing point clouds were taken as training samples for the PointNet deep learning framework to identify the tree crowns at the voxel scale. Second, based on the initial segmentation results, we used the height-related gradient information to accurately depict the boundaries of each tree crown. Meanwhile, the retrieved tree crown breadths of individual trees were compared with field measurements to verify the effectiveness of our approach. Among the four forest types, our results revealed the best performance for the nursery base (tree crown detection rate r = 0.90; crown breadth estimation R2 &gt; 0.94 and root mean squared error (RMSE) &lt; 0.2m). A sound performance was also achieved for the monastery garden and mixed forest, which had complex forest structures, complicated intersections of branches and different building types, with r = 0.85, R2 &gt; 0.88 and RMSE &lt; 0.6 m for the monastery garden and r = 0.80, R2 &gt; 0.85 and RMSE &lt; 0.8 m for the mixed forest. For the fourth forest plot type with the distribution of crown defoliation across the woodland, we achieved the performance with r = 0.82, R2 &gt; 0.79 and RMSE &lt; 0.7 m. Our method presents a robust framework inspired by the deep learning technology and computer graphics theory that solves the ITC segmentation problem and retrieves forest parameters under various forest conditions.
KW  - deep learning
KW  - individual tree crown segmentation
KW  - Airborne LiDAR data
KW  - computer graphics
DO  - 10.3390/f12020131
TY  - EJOU
AU  - Yang, Yuan
AU  - Huang, Yongjiang
AU  - Yang, Haoran
AU  - Zhang, Tingting
AU  - Wang, Zixuan
AU  - Liu, Xixiang
TI  - Real-Time Terrain-Following of an Autonomous Quadrotor by Multi-Sensor Fusion and Control
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 3
SN  - 2076-3417

AB  - For the application of the autonomous guidance of a quadrotor from confined undulant ground, terrain-following is the major issue for flying at a low altitude. This study has modified the open-source autopilot based on the integration of a multi-sensor receiver (a Global Navigation Satellite System (GNSS)), a Lidar-lite (a laser-range-finder device), a barometer and a low-cost inertial navigation system (INS)). These automatically control the position, attitude and height (a constant clearance above the ground) to allow terrain-following and avoid obstacles based on multi-sensors that maintain a constant height above flat ground or with obstacles. The INS/Lidar-lite integration is applied for the attitude and the height stabilization, respectively. The height control is made by the combination of an extended Kalman filter (EKF) estimator and a cascade proportional-integral-derivative (PID) controller that is designed appropriately for the noise characteristics of low accuracy sensors. The proposed terrain-following is tested by both simulations and real-world experiments. The results indicate that the quadrotor can continuously navigate and avoid obstacles at a real-time response of reliable height control with the adjustment time of the cascade PID controller improving over 50% than that of the PID controller.
KW  - quadrotor
KW  - terrain-following
KW  - dynamic models
KW  - navigation system
KW  - multi-sensor estimation
DO  - 10.3390/app11031065
TY  - EJOU
AU  - Chao, Kuei-Hsiang
AU  - Lai, Pei-Lun
TI  - A Fault Diagnosis Mechanism with Power Generation Improvement for a Photovoltaic Module Array
T2  - Energies

PY  - 2021
VL  - 14
IS  - 3
SN  - 1996-1073

AB  - This paper aims to develop an online diagnostic mechanism, doubling as a maximum power point tracking scheme, for a photovoltaic (PV) module array. In case of malfunction or shadow event occurring to a PV module, the presented diagnostic mechanism is enabled, automatically and immediately, to reconfigure a PV module array for maximum output power operation under arbitrary working conditions. Meanwhile, the malfunctioning or shaded PV module can be located instantly by this diagnostic mechanism according to the array configuration, and a PV module replacement process is made more efficient than ever before for the maintenance crew. In this manner, the intended maximum output power operation can be resumed as soon as possible in consideration of a minimum business loss. Using a particle swarm optimization (PSO)-based algorithm, the PV module array is reconfigured by means of switch manipulations between modules, such that a load is supplied with the maximum amount of output power. For compactness, the PSO-based online diagnostic algorithm is implemented herein using a TMS320F2808 digital signal processor (DSP) and is experimentally validated as successful to identify a malfunctioning PV module at the end of this work.
KW  - online diagnostic mechanism
KW  - photovoltaic module array
KW  - maximum power point tracking
KW  - particle swarm optimization
KW  - digital signal processor
DO  - 10.3390/en14030598
TY  - EJOU
AU  - Zhou, Xixuan
AU  - Yang, Liao
AU  - Wang, Weisheng
AU  - Chen, Baili
TI  - UAV Data as an Alternative to Field Sampling to Monitor Vineyards Using Machine Learning Based on UAV/Sentinel-2 Data Fusion
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - Pests and diseases affect the yield and quality of grapes directly and engender noteworthy economic losses. Diagnosing &ldquo;lesions&rdquo; on vines as soon as possible and dynamically monitoring symptoms caused by pests and diseases at a larger scale are essential to pest control. This study has appraised the capabilities of high-resolution unmanned aerial vehicle (UAV) data as an alternative to manual field sampling to obtain sampling canopy sets and to supplement satellite-based monitoring using machine learning models including partial least squared regression (PLSR), support vector regression (SVR), random forest regression (RFR), and extreme learning regression (ELR) with a new activation function. UAV data were acquired from two flights in Turpan to determine disease severity (DS) and disease incidence (DI) and compared with field visual assessments. The UAV-derived canopy structure including canopy height (CH) and vegetation fraction cover (VFC), as well as satellite-based spectral features calculated from Sentinel-2A/B data were analyzed to evaluate the potential of UAV data to replace manual sampling data and predict DI. It was found that SVR slightly outperformed the other methods with a root mean square error (RMSE) of 1.89%. Moreover, the combination of canopy structure (CS) and vegetation index (VIs) improved prediction accuracy compared with single-type features (RMSEcs of 2.86% and RMSEVIs of 1.93%). This study tested the ability of UAV sampling to replace manual sampling on a large scale and introduced opportunities and challenges of fusing different features to monitor vineyards using machine learning. Within this framework, disease incidence can be estimated efficiently and accurately for larger area monitoring operation.
KW  - unmanned aircraft system (UAS)
KW  - vineyard monitoring
KW  - machine learning
KW  - pests and diseases
KW  - Sentinel-2 data
KW  - UAV data
DO  - 10.3390/rs13030457
TY  - EJOU
AU  - Liao, Wenyue
AU  - Deng, Yingbin
AU  - Li, Miao
AU  - Sun, Meiwei
AU  - Yang, Ji
AU  - Xu, Jianhui
TI  - Extraction and Analysis of Finer Impervious Surface Classes in Urban Area
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - Impervious surfaces (IS), the most common land cover in urban areas, not only provide convenience to the city, but also exert significant negative environmental impacts, thereby affecting the ecological environment carrying capacity of urban agglomerations. Most of the current research considers IS as a single land-cover type, yet this does not fully reflect the complex physical characteristics of various IS types. Therefore, limited information for urban micro-ecology and urban fine management can be provided through one IS land-cover type. This study proposed a finer IS classification scheme and mapped the detailed IS fraction in Guangzhou City, China using Landsat imagery. The IS type was divided into seven finer classes, including blue steel, cement, asphalt, other impervious surface, and other metal, brick, and plastic. Classification results demonstrate that finer IS can be well extracted from the Landsat imagery as all root mean square errors (RMSE) are less than 15%. Specially, the accuracies of asphalt, plastic, and cement are better than other finer IS types with the RMSEs of 7.99%, 8.48%, and 9.92%, respectively. Quantitative analyses illustrate that asphalt, other impervious surface, and brick are the dominant IS types in the study area with the percentages of 9.68%, 6.27%, and 4.45%, respectively, and they are mainly located in Yuexiu, Liwan, Haizhu, and Panyu districts. These results are valuable for research into urban fine management and can support the detailed analysis of urban micro-ecology.
KW  - subpixel classification
KW  - impervious surface
KW  - urban environment
KW  - finer IS category
DO  - 10.3390/rs13030459
TY  - EJOU
AU  - Casas, Roberto
AU  - Hermosa, Arturo
AU  - Marco, Álvaro
AU  - Blanco, Teresa
AU  - Zarazaga-Soria, Francisco J.
TI  - Real-Time Extensive Livestock Monitoring Using LPWAN Smart Wearable and Infrastructure
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 3
SN  - 2076-3417

AB  - Extensive unsupervised livestock farming is a habitual technique in many places around the globe. Animal release can be done for months, in large areas and with different species packing and behaving very differently. Nevertheless, the farmer’s needs are similar: where livestock is (and where has been) and how healthy they are. The geographical areas involved usually have difficult access with harsh orography and lack of communications infrastructure. This paper presents the design of a solution for extensive livestock monitoring in these areas. Our proposal is based in a wearable equipped with inertial sensors, global positioning system and wireless communications; and a Low-Power Wide Area Network infrastructure that can run with and without internet connection. Using adaptive analysis and data compression, we provide real-time monitoring and logging of cattle’s position and activities. Hardware and firmware design achieve very low energy consumption allowing months of battery life. We have thoroughly tested the devices in different laboratory setups and evaluated the system performance in real scenarios in the mountains and in the forest.
KW  - animal monitoring
KW  - low-power wide area networks
KW  - LoRaWAN
KW  - wearable devices design
DO  - 10.3390/app11031240
TY  - EJOU
AU  - Li, Xiaoting
AU  - Hu, Tengyun
AU  - Gong, Peng
AU  - Du, Shihong
AU  - Chen, Bin
AU  - Li, Xuecao
AU  - Dai, Qi
TI  - Mapping Essential Urban Land Use Categories in Beijing with a Fast Area of Interest (AOI)-Based Method
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - Urban land use mapping is critical to understanding human activities in space. The first national mapping result of essential urban land use categories of China (EULUC-China) was released in 2019. However, the overall accuracies in some of the plain cities such as Beijing, Chengdu, and Zhengzhou were lower than 50% because many parcel-based mapping units are large with mixed land uses. To address this shortcoming, we proposed an area of interest (AOI)-based mapping approach, choosing Beijing as our study area. The mapping process includes two major steps. First, grids with different sizes (i.e., 300 m, 200 m, and 100 m) were derived from original land parcels to obtain classification units with a suitable size. Then, features within these grids were extracted from Sentinel-2 spectral data, point of interest (POI), and Tencent Easygo crowdedness data. These features were classified using a random forest (RF) classifier with AOI data, resulting in a 10-category map of EULUC. Second, we superimposed the AOIs layer on classified units to do some rectification and offer more details at the building scale. The overall accuracy of the AOI layer reached 98%, and the overall accuracy of the mapping results reached 77%. This study provides a fast method for accurate geographic sample collection, which substantially reduces the amount of fieldwork for sample collection and improves the classification accuracy compared to previous EULUC mapping. The detailed urban land use map could offer more support for urban planning and environmental policymaking.
KW  - area of interest
KW  - urban land use
KW  - sample collection
KW  - building scale
KW  - random forest
DO  - 10.3390/rs13030477
TY  - EJOU
AU  - Yang, Wanting
AU  - Zhang, Xianfeng
AU  - Luo, Peng
TI  - Transferability of Convolutional Neural Network Models for Identifying Damaged Buildings Due to Earthquake
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - The collapse of buildings caused by earthquakes can lead to a large loss of life and property. Rapid assessment of building damage with remote sensing image data can support emergency rescues. However, current studies indicate that only a limited sample set can usually be obtained from remote sensing images immediately following an earthquake. Consequently, the difficulty in preparing sufficient training samples constrains the generalization of the model in the identification of earthquake-damaged buildings. To produce a deep learning network model with strong generalization, this study adjusted four Convolutional Neural Network (CNN) models for extracting damaged building information and compared their performance. A sample dataset of damaged buildings was constructed by using multiple disaster images retrieved from the xBD dataset. Using satellite and aerial remote sensing data obtained after the 2008 Wenchuan earthquake, we examined the geographic and data transferability of the deep network model pre-trained on the xBD dataset. The result shows that the network model pre-trained with samples generated from multiple disaster remote sensing images can extract accurately collapsed building information from satellite remote sensing data. Among the adjusted CNN models tested in the study, the adjusted DenseNet121 was the most robust. Transfer learning solved the problem of poor adaptability of the network model to remote sensing images acquired by different platforms and could identify disaster-damaged buildings properly. These results provide a solution to the rapid extraction of earthquake-damaged building information based on a deep learning network model.
KW  - earthquake
KW  - disaster-damaged buildings
KW  - transfer learning
KW  - CNN
KW  - VHR images
DO  - 10.3390/rs13030504
TY  - EJOU
AU  - Sassu, Alberto
AU  - Gambella, Filippo
AU  - Ghiani, Luca
AU  - Mercenaro, Luca
AU  - Caria, Maria
AU  - Pazzona, Antonio L.
TI  - Advances in Unmanned Aerial System Remote Sensing for Precision Viticulture
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - New technologies for management, monitoring, and control of spatio-temporal crop variability in precision viticulture scenarios are numerous. Remote sensing relies on sensors able to provide useful data for the improvement of management efficiency and the optimization of inputs. unmanned aerial systems (UASs) are the newest and most versatile tools, characterized by high precision and accuracy, flexibility, and low operating costs. The work aims at providing a complete overview of the application of UASs in precision viticulture, focusing on the different application purposes, the applied equipment, the potential of technologies combined with UASs for identifying vineyards’ variability. The review discusses the potential of UASs in viticulture by distinguishing five areas of application: rows segmentation and crop features detection techniques; vineyard variability monitoring; estimation of row area and volume; disease detection; vigor and prescription maps creation. Technological innovation and low purchase costs make UASs the core tools for decision support in the customary use by winegrowers. The ability of the systems to respond to the current demands for the acquisition of digital technologies in agricultural fields makes UASs a candidate to play an increasingly important role in future scenarios of viticulture application.
KW  - UAS
KW  - vegetation index
KW  - 3D vineyard characterization
KW  - canopy height model
KW  - precision farming
KW  - precision viticulture
KW  - remote sensing
KW  - sustainability of resources
KW  - vineyard detection and segmentation
DO  - 10.3390/s21030956
TY  - EJOU
AU  - Rahman, Ehab U.
AU  - Zhang, Yihong
AU  - Ahmad, Sohail
AU  - Ahmad, Hafiz I.
AU  - Jobaer, Sayed
TI  - Autonomous Vision-Based Primary Distribution Systems Porcelain Insulators Inspection Using UAVs
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - The early detection of damaged (partially broken) outdoor insulators in primary distribution systems is of paramount importance for continuous electricity supply and public safety. Unmanned aerial vehicles (UAVs) present a safer, autonomous, and efficient way to examine the power system components without closing the power distribution system. In this work, a novel dataset is designed by capturing real images using UAVs and manually generated images collected to overcome the data insufficiency problem. A deep Laplacian pyramid-based super-resolution network is implemented to reconstruct high-resolution training images. To improve the visibility of low-light images, a low-light image enhancement technique is used for the robust exposure correction of the training images. A different fine-tuning strategy is implemented for fine-tuning the object detection model to increase detection accuracy for the specific faulty insulators. Several flight path strategies are proposed to overcome the shuttering effect of insulators, along with providing a less complex and time- and energy-efficient approach for capturing a video stream of the power system components. The performance of different object detection models is presented for selecting the most suitable one for fine-tuning on the specific faulty insulator dataset. For the detection of damaged insulators, our proposed method achieved an F1-score of 0.81 and 0.77 on two different datasets and presents a simple and more efficient flight strategy. Our approach is based on real aerial inspection of in-service porcelain insulators by extensive evaluation of several video sequences showing robust fault recognition and diagnostic capabilities. Our approach is demonstrated on data acquired by a drone in Swat, Pakistan.
KW  - primary distribution systems
KW  - transfer learning
KW  - YoloV4
KW  - porcelain insulator detection
KW  - UAVs
KW  - BRISQUE
KW  - LIME
KW  - LapSRN
KW  - YoloV5
DO  - 10.3390/s21030974
TY  - EJOU
AU  - Blasi, Luciano
AU  - Borrelli, Mauro
AU  - D’Amato, Egidio
AU  - di Grazia, Luigi E.
AU  - Mattei, Massimiliano
AU  - Notaro, Immacolata
TI  - Modeling and Control of a Modular Iron Bird
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 2
SN  - 2226-4310

AB  - This paper describes the control architecture and the control laws of a new concept of Modular Iron Bird aimed at reproducing flight loads to test mobile aerodynamic control surface actuators for small and medium size aircraft and Unmanned Aerial Vehicles. The iron bird control system must guarantee the actuation of counteracting forces. On one side, a hydraulic actuator simulates the hinge moments acting on the mobile surface due to aerodynamic and inertial effects during flight; on the other side, the actuator to be tested applies an active hinge moment to control the angular position of the same surface. Reference aerodynamic and inertial loads are generated by a flight simulation module to reproduce more realistic conditions arising during operations. The design of the control action is based on a dynamic model of the hydraulic plant used to generate loads. This system is controlled using a Proportional Integral Derivative control algorithm tuned with an optimization algorithm taking into account the closed loop dynamics of the actuator under testing, uncertainties and disturbances in the controlled plant. Numerical simulations are presented to show the effectiveness of the proposed architecture and control laws.
KW  - iron bird
KW  - hydraulic system
KW  - flight simulator
KW  - force control
KW  - PID control
DO  - 10.3390/aerospace8020039
TY  - EJOU
AU  - Papaioannou, Panagiotis
AU  - Papadopoulos, Efthymis
AU  - Nikolaidou, Anastasia
AU  - Politis, Ioannis
AU  - Basbas, Socrates
AU  - Kountouri, Eleni
TI  - Dilemma Zone: Modeling Drivers’ Decision at Signalized Intersections against Aggressiveness and Other Factors Using UAV Technology
T2  - Safety

PY  - 2021
VL  - 7
IS  - 1
SN  - 2313-576X

AB  - Intersection safety and drivers’ behavior are strongly interrelated, especially when the latter are located in dilemma zone. This paper explores, among others, the main factors affecting driver behavior, such as distance to stop line, approaching speed and acceleration/deceleration, and two additional factors, namely, driver’s aggressiveness and driver’s relative position at the onset of the yellow signal. Field data were collected using unmanned aerial vehicle (UAV) technology. Two binary choice models were developed, the first relying on observed data and the latter enriched by the latent factor drivers’ aggressiveness and the vehicles’ relative position. Drivers were classified to aggressive and non-aggressive ones using a latent class model that combined approaching speed and acceleration/deceleration data. Drivers were further grouped according to their expected reaction/decision to stop or cross the intersection in relation to their relative position. Both models equally explain drivers’ decisions adequately, but the second one offers additional explanatory power attributed to aggressiveness. Being able to identify the level of aggressiveness among the drivers enables the calculation of the probability that drivers will cross the intersection even if caught in a dilemma zone or in a zone in which the obvious decision is to stop. Such findings can be valuable when designing a signalized intersection and the traffic time settings, as well as the posted speed limit.
KW  - UAV video-observed vehicle trajectory data
KW  - driver behavior
KW  - signalized intersection
KW  - dilemma zone
KW  - choice model
KW  - latent class model
KW  - acceleration/deceleration
KW  - drivers’ aggressiveness
DO  - 10.3390/safety7010011
TY  - EJOU
AU  - Kamarudin, Mohd H.
AU  - Ismail, Zool H.
AU  - Saidi, Noor B.
TI  - Deep Learning Sensor Fusion in Plant Water Stress Assessment: A Comprehensive Review
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 4
SN  - 2076-3417

AB  - Water stress is one of the major challenges to food security, causing a significant economic loss for the nation as well for growers. Accurate assessment of water stress will enhance agricultural productivity through optimization of plant water usage, maximizing plant breeding strategies, and preventing forest wildfire for better ecosystem management. Recent advancements in sensor technologies have enabled high-throughput, non-contact, and cost-efficient plant water stress assessment through intelligence system modeling. The advanced deep learning sensor fusion technique has been reported to improve the performance of the machine learning application for processing the collected sensory data. This paper extensively reviews the state-of-the-art methods for plant water stress assessment that utilized the deep learning sensor fusion approach in their application, together with future prospects and challenges of the application domain. Notably, 37 deep learning solutions fell under six main areas, namely soil moisture estimation, soil water modelling, evapotranspiration estimation, evapotranspiration forecasting, plant water status estimation and plant water stress identification. Basically, there are eight deep learning solutions compiled for the 3D-dimensional data and plant varieties challenge, including unbalanced data that occurred due to isohydric plants, and the effect of variations that occur within the same species but cultivated from different locations.
KW  - artificial intelligence
KW  - agriculture monitoring system
KW  - modelling
KW  - plant-based water stress
KW  - smart sensor
DO  - 10.3390/app11041403
TY  - EJOU
AU  - Zenteno-Torres, Jazmín
AU  - Cieslak, Jérôme
AU  - Dávila, Jorge
AU  - Henry, David
TI  - Sliding Mode Control with Application to Fault-Tolerant Control: Assessment and Open Problems
T2  - Automation

PY  - 2021
VL  - 2
IS  - 1
SN  - 2673-4052

AB  - This paper is prepared within a collaboration between the Instituto Politécnico Nacional, which is a Mexican research institute that manages research on sliding-mode control theory, and the ARIA research team of the Intégration du Matériau au Système Lab., a French research group that engages research on model-based fault diagnosis and fault-tolerant control theories. The paper reviews the application of sliding mode control techniques to fault tolerant control and provides perspectives leading to posing some open problems. Operating principles, definitions of the basic concepts are recalled along with the control objectives and design procedures. The evolution of the sliding mode control technique through five generations (as classified by Fridman, Moreno and co-workers) is reviewed. Their respective design procedures, limitations, and robustness properties are also highlighted. The application of the five generations of sliding-mode controllers to fault-tolerant control is discussed. The focus is on some open problems that are judged to commonly be overlooked. Some applications in real-world systems are also presented.
KW  - sliding-mode control
KW  - fault-tolerant control
DO  - 10.3390/automation2010001
TY  - EJOU
AU  - Jiang, Xueqin
AU  - Fang, Shenghui
AU  - Huang, Xia
AU  - Liu, Yanghua
AU  - Guo, Linlin
TI  - Rice Mapping and Growth Monitoring Based on Time Series GF-6 Images and Red-Edge Bands
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Accurate rice mapping and growth monitoring are of great significance for ensuring food security and agricultural sustainable development. Remote sensing (RS), as an efficient observation technology, is expected to be useful for rice mapping and growth monitoring. Due to the fragmented distribution of paddy fields and the undulating terrain in Southern China, it is very difficult in rice mapping. Moreover, there are many crops with the same growth period as rice, resulting in low accuracy of rice mapping. We proposed a red-edge decision tree (REDT) method based on the combination of time series GF-6 images and red-edge bands to solve this problem. The red-edge integral and red-edge vegetation index integral were computed by using two red-edge bands derived from GF-6 images to construct the REDT. Meanwhile, the conventional method based on time series normalized difference vegetation index (NDVI), normalized difference water index (NDWI), enhanced vegetation index (EVI) (NNE) was employed to compare the effectiveness of rice mapping. The results indicated that the overall accuracy and Kappa coefficient of REDT ranged from 91%–94% and 0.82–0.87, improving about 7% and 0.15 compared with the NNE method. This proved that the proposed technology was able to efficiently solve the problem of rice mapping on a large scale and regions with fragmented landscapes. Additionally, two red-edge bands of GF-6 images were applied to monitor rice growth. It concluded that the two red-edge bands played different roles in rice growth monitoring. The red-edge bands of GF-6 images were superior in rice mapping and growth monitoring. Further study needs to develop more vegetation indices (VIs) related to the red-edge to make the best use of red-edge characteristics in precision agriculture.
KW  - rice mapping
KW  - red-edge
KW  - growth monitoring
KW  - GF-6
KW  - time series
DO  - 10.3390/rs13040579
TY  - EJOU
AU  - Horla, Dariusz
AU  - Giernacki, Wojciech
AU  - Cieślak, Jacek
AU  - Campoy, Pascual
TI  - Altitude Measurement-Based Optimization of the Landing Process of UAVs
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - The paper addresses the loop shaping problem in the altitude control of an unmanned aerial vehicle to land the flying robot with a specific landing scenario adopted. The proposed solution is optimal, in the sense of the selected performance indices, namely minimum-time, minimum-energy, and velocity-penalized related functions, achieving their minimal values, with numerous experiments conducted throughout the development and preparation to the Mohamed Bin Zayed International Robotics Challenge (MBZIRC 2020). A novel approach to generation of a reference altitude trajectory is presented, which is then tracked in a standard, though optimized, control loop. Three landing scenarios are considered, namely: minimum-time, minimum-energy, and velocity-penalized landing scenarios. The experimental results obtained with the use of the Simulink Support Package for Parrot Minidrones, and the OptiTrack motion capture system proved the effectiveness of the proposed approach.
KW  - optimization
KW  - energy
KW  - UAV
KW  - landing
DO  - 10.3390/s21041151
TY  - EJOU
AU  - Praticò, Salvatore
AU  - Solano, Francesco
AU  - Di Fazio, Salvatore
AU  - Modica, Giuseppe
TI  - Machine Learning Classification of Mediterranean Forest Habitats in Google Earth Engine Based on Seasonal Sentinel-2 Time-Series and Input Image Composition Optimisation
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - The sustainable management of natural heritage is presently considered a global strategic issue. Owing to the ever-growing availability of free data and software, remote sensing (RS) techniques have been primarily used to map, analyse, and monitor natural resources for conservation purposes. The need to adopt multi-scale and multi-temporal approaches to detect different phenological aspects of different vegetation types and species has also emerged. The time-series composite image approach allows for capturing much of the spectral variability, but presents some criticalities (e.g., time-consuming research, downloading data, and the required storage space). To overcome these issues, the Google Earth engine (GEE) has been proposed, a free cloud-based computational platform that allows users to access and process remotely sensed data at petabyte scales. The application was tested in a natural protected area in Calabria (South Italy), which is particularly representative of the Mediterranean mountain forest environment. In the research, random forest (RF), support vector machine (SVM), and classification and regression tree (CART) algorithms were used to perform supervised pixel-based classification based on the use of Sentinel-2 images. A process to select the best input image (seasonal composition strategies, statistical operators, band composition, and derived vegetation indices (VIs) information) for classification was implemented. A set of accuracy indicators, including overall accuracy (OA) and multi-class F-score (Fm), were computed to assess the results of the different classifications. GEE proved to be a reliable and powerful tool for the classification process. The best results (OA = 0.88 and Fm = 0.88) were achieved using RF with the summer image composite, adding three VIs (NDVI, EVI, and NBR) to the Sentinel-2 bands. SVM and RF produced OAs of 0.83 and 0.80, respectively.
KW  - random forest (RF)
KW  - support vector machine (SVM)
KW  - classification and regression tree (CART)
KW  - cloud platform
KW  - vegetation indices (VIs)
KW  - Natura 2000
KW  - Aspromonte National Park
DO  - 10.3390/rs13040586
TY  - EJOU
AU  - Islam, Nahina
AU  - Rashid, Md M.
AU  - Pasandideh, Faezeh
AU  - Ray, Biplob
AU  - Moore, Steven
AU  - Kadel, Rajan
TI  - A Review of Applications and Communication Technologies for Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) Based Sustainable Smart Farming
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 4
SN  - 2071-1050

AB  - To reach the goal of sustainable agriculture, smart farming is taking advantage of the Unmanned Aerial Vehicles (UAVs) and Internet of Things (IoT) paradigm. These smart farms are designed to be run by interconnected devices and vehicles. Some enormous potentials can be achieved by the integration of different IoT technologies to achieve automated operations with minimum supervision. This paper outlines some major applications of IoT and UAV in smart farming, explores the communication technologies, network functionalities and connectivity requirements for Smart farming. The connectivity limitations of smart agriculture and it’s solutions are analysed with two case studies. In case study-1, we propose and evaluate meshed Long Range Wide Area Network (LoRaWAN) gateways to address connectivity limitations of Smart Farming. While in case study-2, we explore satellite communication systems to provide connectivity to smart farms in remote areas of Australia. Finally, we conclude the paper by identifying future research challenges on this topic and outlining directions to address those challenges.
KW  - agriculture
KW  - Internet of Things (IoT)
KW  - smart farming
KW  - sustainable future
KW  - sustainable smart farming
KW  - Unmanned Aerial Vehicles (UAVs)
DO  - 10.3390/su13041821
TY  - EJOU
AU  - Zhang, Xiuwei
AU  - Zhou, Yang
AU  - Jin, Jiaojiao
AU  - Wang, Yafei
AU  - Fan, Minhao
AU  - Wang, Ning
AU  - Zhang, Yanning
TI  - ICENETv2: A Fine-Grained River Ice Semantic Segmentation Network Based on UAV Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Accurate ice segmentation is one of the most crucial techniques for intelligent ice monitoring. Compared with ice segmentation, it can provide more information for ice situation analysis, change trend prediction, and so on. Therefore, the study of ice segmentation has important practical significance. In this study, we focused on fine-grained river ice segmentation using unmanned aerial vehicle (UAV) images. This has the following difficulties: (1) The scale of river ice varies greatly in different images and even in the same image; (2) the same kind of river ice differs greatly in color, shape, texture, size, and so on; and (3) the appearances of different kinds of river ice sometimes appear similar due to the complex formation and change procedure. Therefore, to perform this study, the NWPU_YRCC2 dataset was built, in which all UAV images were collected in the Ningxia–Inner Mongolia reach of the Yellow River. Then, a novel semantic segmentation method based on deep convolution neural network, named ICENETv2, is proposed. To achieve multiscale accurate prediction, we design a multilevel features fusion framework, in which multi-scale high-level semantic features and lower-level finer features are effectively fused. Additionally, a dual attention module is adopted to highlight distinguishable characteristics, and a learnable up-sampling strategy is further used to improve the segmentation accuracy of the details. Experiments show that ICENETv2 achieves the state-of-the-art on the NWPU_YRCC2 dataset. Finally, our ICENETv2 is also applied to solve a realistic problem, calculating drift ice cover density, which is one of the most important factors to predict the freeze-up data of the river. The results demonstrate that the performance of ICENETv2 meets the actual application demand.
KW  - fine-grained river ice
KW  - position attention
KW  - channel attention
KW  - drift ice cover density
KW  - semantic segmentation
DO  - 10.3390/rs13040633
TY  - EJOU
AU  - Kopačková-Strnadová, Veronika
AU  - Koucká, Lucie
AU  - Jelének, Jan
AU  - Lhotáková, Zuzana
AU  - Oulehle, Filip
TI  - Canopy Top, Height and Photosynthetic Pigment Estimation Using Parrot Sequoia Multispectral Imagery and the Unmanned Aerial Vehicle (UAV)
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Remote sensing is one of the modern methods that have significantly developed over the last two decades and, nowadays, it provides a new means for forest monitoring. High spatial and temporal resolutions are demanded for the accurate and timely monitoring of forests. In this study, multi-spectral Unmanned Aerial Vehicle (UAV) images were used to estimate canopy parameters (definition of crown extent, top, and height, as well as photosynthetic pigment contents). The UAV images in Green, Red, Red-Edge, and Near infrared (NIR) bands were acquired by Parrot Sequoia camera over selected sites in two small catchments (Czech Republic) covered dominantly by Norway spruce monocultures. Individual tree extents, together with tree tops and heights, were derived from the Canopy Height Model (CHM). In addition, the following were tested: (i) to what extent can the linear relationship be established between selected vegetation indexes (Normalized Difference Vegetation Index (NDVI) and NDVIred edge) derived for individual trees and the corresponding ground truth (e.g., biochemically assessed needle photosynthetic pigment contents) and (ii) whether needle age selection as a ground truth and crown light conditions affect the validity of linear models. The results of the conducted statistical analysis show that the two vegetation indexes (NDVI and NDVIred edge) tested here have the potential to assess photosynthetic pigments in Norway spruce forests at a semi-quantitative level; however, the needle-age selection as a ground truth was revealed to be a very important factor. The only usable results were obtained for linear models when using the second year needle pigment contents as a ground truth. On the other hand, the illumination conditions of the crown proved to have very little effect on the model’s validity. No study was found to directly compare these results conducted on coniferous forest stands. This shows that there is a further need for studies dealing with a quantitative estimation of the biochemical variables of nature coniferous forests when employing spectral data that were acquired by the UAV platform at a very high spatial resolution.
KW  - UAV
KW  - Parrot Sequoia multispectral camera
KW  - photosynthetic pigments
KW  - Norway spruce
KW  - forest
KW  - linear models
KW  - ground truth
KW  - needle age
KW  - crown detection
DO  - 10.3390/rs13040705
TY  - EJOU
AU  - Bohlin, Jonas
AU  - Wallerman, Jörgen
AU  - Fransson, Johan E. S.
TI  - Extraction of Spectral Information from Airborne 3D Data for Assessment of Tree Species Proportions
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - With the rapid development of photogrammetric software and accessible camera technology, land surveys and other mapping organizations now provide various point cloud and digital surface model products from aerial images, often including spectral information. In this study, methods for colouring the point cloud and the importance of different metrics were compared for tree species-specific estimates at a coniferous hemi-boreal test site in southern Sweden. A total of three different data sets of aerial image-based products and one multi-spectral lidar data set were used to estimate tree species-specific proportion and stem volume using an area-based approach. Metrics were calculated for 156 field plots (10 m radius) from point cloud data and used in a Random Forest analysis. Plot level accuracy was evaluated using leave-one-out cross-validation. The results showed small differences in estimation accuracy of species-specific variables between the colouring methods. Simple averages of the spectral metrics had the highest importance and using spectral data from two seasons improved species prediction, especially deciduous proportion. Best tree species-specific proportion was estimated using multi-spectral lidar with 0.22 root mean square error (RMSE) for pine, 0.22 for spruce and 0.16 for deciduous. Corresponding RMSE for aerial images was 0.24, 0.23 and 0.20 for pine, spruce and deciduous, respectively. For the species-specific stem volume at plot level using image data, the RMSE in percent of surveyed mean was 129% for pine, 60% for spruce and 118% for deciduous.
KW  - aerial images
KW  - multi-spectral lidar
KW  - Optec Titan
KW  - photogrammetry
KW  - species-specific proportion
KW  - stem volume
KW  - UltraCam
DO  - 10.3390/rs13040720
TY  - EJOU
AU  - Liu, Feng
AU  - Dai, Shuling
AU  - Zhao, Yongjia
TI  - Learning to Have a Civil Aircraft Take Off under Crosswind Conditions by Reinforcement Learning with Multimodal Data and Preprocessing Data
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Autopilot technology in the field of aviation has developed over many years. However, it is difficult for an autopilot system to autonomously operate a civil aircraft under bad weather conditions. In this paper, we present a reinforcement learning (RL) algorithm using multimodal data and preprocessing data to have a civil aircraft take off autonomously under crosswind conditions. The multimodal data include the common flight status and visual information. The preprocessing is a new design that maps some flight data by nonlinear functions based on the general flight dynamics before these data are fed into the RL model. Extensive experiments under different crosswind conditions with a professional flight simulator demonstrate that the proposed method can effectively control a civil aircraft to take off under various crosswind conditions and achieve better performance than trials without visual information or preprocessing data.
KW  - autopilot
KW  - civil aircraft
KW  - multimodal data
KW  - reinforcement learning
KW  - preprocessing
DO  - 10.3390/s21041386
TY  - EJOU
AU  - Gao, Bowen
AU  - Chen, Ninghua
AU  - Blaschke, Thomas
AU  - Wu, Chase Q.
AU  - Chen, Jianyu
AU  - Xu, Yaochen
AU  - Yang, Xiaoping
AU  - Du, Zhenhong
TI  - Automated Characterization of Yardangs Using Deep Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - The morphological characteristics of yardangs are the direct evidence that reveals the wind and fluvial erosion for lacustrine sediments in arid areas. These features can be critical indicators in reconstructing local wind directions and environment conditions. Thus, the fast and accurate extraction of yardangs is key to studying their regional distribution and evolution process. However, the existing automated methods to characterize yardangs are of limited generalization that may only be feasible for specific types of yardangs in certain areas. Deep learning methods, which are superior in representation learning, provide potential solutions for mapping yardangs with complex and variable features. In this study, we apply Mask region-based convolutional neural networks (Mask R-CNN) to automatically delineate and classify yardangs using very high spatial resolution images from Google Earth. The yardang field in the Qaidam Basin, northwestern China is selected to conduct the experiments and the method yields mean average precisions of 0.869 and 0.671 for intersection of union (IoU) thresholds of 0.5 and 0.75, respectively. The manual validation results on images of additional study sites show an overall detection accuracy of 74%, while more than 90% of the detected yardangs can be correctly classified and delineated. We then conclude that Mask R-CNN is a robust model to characterize multi-scale yardangs of various types and allows for the research of the morphological and evolutionary aspects of aeolian landform.
KW  - aeolian landform
KW  - yardang
KW  - morphological characteristic
KW  - deep learning
KW  - Mask R-CNN
KW  - Google Earth imagery
DO  - 10.3390/rs13040733
TY  - EJOU
AU  - Blekos, Kostas
AU  - Tsakas, Anastasios
AU  - Xouris, Christos
AU  - Evdokidis, Ioannis
AU  - Alexandropoulos, Dimitris
AU  - Alexakos, Christos
AU  - Katakis, Sofoklis
AU  - Makedonas, Andreas
AU  - Theoharatos, Christos
AU  - Lalos, Aris
TI  - Analysis, Modeling and Multi-Spectral Sensing for the Predictive Management of Verticillium Wilt in Olive Groves
T2  - Journal of Sensor and Actuator Networks

PY  - 2021
VL  - 10
IS  - 1
SN  - 2224-2708

AB  - The intensification and expansion in the cultivation of olives have contributed to the significant spread of Verticillium wilt, which is the most important fungal problem affecting olive trees. Recent studies confirm that practices such as the use of innovative natural minerals (Zeoshell ZF1) and the application of beneficial microorganisms (Micosat F BS WP) restore health in infected trees. However, for their efficient implementation the above methodologies require the marking of trees in the early stages of infestation—a task that is impractical with traditional means (manual labor) but also very difficult, as early stages are difficult to perceive with the naked eye. In this paper, we present the results of the My Olive Grove Coach (MyOGC) project, which used multispectral imaging from unmanned aerial vehicles to develop an olive grove monitoring system based on the autonomous and automatic processing of the multispectral images using computer vision and machine learning techniques. The goal of the system is to monitor and assess the health of olive groves, help in the prediction of Verticillium wilt spread and implement a decision support system that guides the farmer/agronomist.
KW  - precision agriculture
KW  - intelligent management
KW  - multi-spectral sensing
KW  - multi-spectral co-registration
KW  - multi-spectral fusion of multispectral spectroscopy data
DO  - 10.3390/jsan10010015
TY  - EJOU
AU  - Li, Guoming
AU  - Huang, Yanbo
AU  - Chen, Zhiqian
AU  - Chesser, Gary D.
AU  - Purswell, Joseph L.
AU  - Linhoss, John
AU  - Zhao, Yang
TI  - Practices and Applications of Convolutional Neural Network-Based Computer Vision Systems in Animal Farming: A Review
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Convolutional neural network (CNN)-based computer vision systems have been increasingly applied in animal farming to improve animal management, but current knowledge, practices, limitations, and solutions of the applications remain to be expanded and explored. The objective of this study is to systematically review applications of CNN-based computer vision systems on animal farming in terms of the five deep learning computer vision tasks: image classification, object detection, semantic/instance segmentation, pose estimation, and tracking. Cattle, sheep/goats, pigs, and poultry were the major farm animal species of concern. In this research, preparations for system development, including camera settings, inclusion of variations for data recordings, choices of graphics processing units, image preprocessing, and data labeling were summarized. CNN architectures were reviewed based on the computer vision tasks in animal farming. Strategies of algorithm development included distribution of development data, data augmentation, hyperparameter tuning, and selection of evaluation metrics. Judgment of model performance and performance based on architectures were discussed. Besides practices in optimizing CNN-based computer vision systems, system applications were also organized based on year, country, animal species, and purposes. Finally, recommendations on future research were provided to develop and improve CNN-based computer vision systems for improved welfare, environment, engineering, genetics, and management of farm animals.
KW  - deep learning
KW  - convolutional neural network
KW  - computer vision system
KW  - animal farming
DO  - 10.3390/s21041492
TY  - EJOU
AU  - Neupane, Bipul
AU  - Horanont, Teerayut
AU  - Aryal, Jagannath
TI  - Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.
KW  - deep learning
KW  - remote sensing
KW  - review
KW  - semantic segmentation
KW  - urban image classification
DO  - 10.3390/rs13040808
TY  - EJOU
AU  - Salt Ducajú, Julián M.
AU  - Salt Llobregat, Julián J.
AU  - Cuenca, Ángel
AU  - Tomizuka, Masayoshi
TI  - Autonomous Ground Vehicle Lane-Keeping LPV Model-Based Control: Dual-Rate State Estimation and Comparison of Different Real-Time Control Strategies
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - In this contribution, we suggest two proposals to achieve fast, real-time lane-keeping control for Autonomous Ground Vehicles (AGVs). The goal of lane-keeping is to orient and keep the vehicle within a given reference path using the front wheel steering angle as the control action for a specific longitudinal velocity. While nonlinear models can describe the lateral dynamics of the vehicle in an accurate manner, they might lead to difficulties when computing some control laws such as Model Predictive Control (MPC) in real time. Therefore, our first proposal is to use a Linear Parameter Varying (LPV) model to describe the AGV’s lateral dynamics, as a trade-off between computational complexity and model accuracy. Additionally, AGV sensors typically work at different measurement acquisition frequencies so that Kalman Filters (KFs) are usually needed for sensor fusion. Our second proposal is to use a Dual-Rate Extended Kalman Filter (DREFKF) to alleviate the cost of updating the internal state of the filter. To check the validity of our proposals, an LPV model-based control strategy is compared in simulations over a circuit path to another reduced computational complexity control strategy, the Inverse Kinematic Bicycle model (IKIBI), in the presence of process and measurement Gaussian noise. The LPV-MPC controller is shown to provide a more accurate lane-keeping behavior than an IKIBI control strategy. Finally, it is seen that Dual-Rate Extended Kalman Filters (DREKFs) constitute an interesting tool for providing fast vehicle state estimation in an AGV lane-keeping application.
KW  - autonomous vehicle
KW  - dual-rate control
KW  - dual-rate EKF
KW  - MPC
KW  - LPV model
DO  - 10.3390/s21041531
TY  - EJOU
AU  - Qi, Haixia
AU  - Liang, Yu
AU  - Ding, Quanchen
AU  - Zou, Jun
TI  - Automatic Identification of Peanut-Leaf Diseases Based on Stack Ensemble
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 4
SN  - 2076-3417

AB  - Peanut is an important food crop, and diseases of its leaves can directly reduce its yield and quality. In order to solve the problem of automatic identification of peanut-leaf diseases, this paper uses a traditional machine-learning method to ensemble the output of a deep learning model to identify diseases of peanut leaves. The identification of peanut-leaf diseases included healthy leaves, rust disease on a single leaf, leaf-spot disease on a single leaf, scorch disease on a single leaf, and both rust disease and scorch disease on a single leaf. Three types of data-augmentation methods were used: image flipping, rotation, and scaling. In this experiment, the deep-learning model had a higher accuracy than the traditional machine-learning methods. Moreover, the deep-learning model achieved better performance when using data augmentation and a stacking ensemble. After ensemble by logistic regression, the accuracy of residual network with 50 layers (ResNet50) was as high as 97.59%, and the F1 score of dense convolutional network with 121 layers (DenseNet121) was as high as 90.50. The deep-learning model used in this experiment had the greatest improvement in F1 score after the logistic regression ensemble. Deep-learning networks with deeper network layers like ResNet50 and DenseNet121 performed better in this experiment. This study can provide a reference for the identification of peanut-leaf diseases.
KW  - peanut-leaf diseases
KW  - deep learning
KW  - convolutional neural network
KW  - identification
DO  - 10.3390/app11041950
TY  - EJOU
AU  - Megahed, Yasmine
AU  - Shaker, Ahmed
AU  - Yan, Wai Y.
TI  - Fusion of Airborne LiDAR Point Clouds and Aerial Images for Heterogeneous Land-Use Urban Mapping
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors.
KW  - urban land-use
KW  - LiDAR-aerial integration
KW  - LiDAR-aerial geo-registration
KW  - LiDAR classification
KW  - supervised machine learning
KW  - maximum likelihood
KW  - support vector machines
KW  - neural networks
KW  - bootstrap aggregation
KW  - k-fold cross-validation
DO  - 10.3390/rs13040814
TY  - EJOU
AU  - Bonci, Andrea
AU  - Cen Cheng, Pangcheng  David
AU  - Indri, Marina
AU  - Nabissi, Giacomo
AU  - Sibona, Fiorella
TI  - Human-Robot Perception in Industrial Environments: A Survey
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.
KW  - human-robot perception
KW  - human-robot collaboration
KW  - collision detection
KW  - human action recognition
KW  - collision avoidance
KW  - machine vision
KW  - 3D sensors
KW  - robot guidance
DO  - 10.3390/s21051571
TY  - EJOU
AU  - Bajić, Milan
AU  - Bajić, Milan
TI  - Modeling and Simulation of Very High Spatial Resolution UXOs and Landmines in a Hyperspectral Scene for UAV Survey
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - This paper presents methods for the modeling and simulation of explosive target placement in terrain spectral images (i.e., real hyperspectral 90-channel VNIR data), considering unexploded ordnances, landmines, and improvised explosive devices. The models used for landmine detection operate at sub-pixel levels. The presented research uses very fine spatial resolutions, 0.945 × 0.945 mm for targets and 1.868 × 1.868 cm for the scene, where the number of target pixels ranges from 52 to 116. While previous research has used the mean spectral value of the target, it is omitted in this paper. The model considers the probability of detection and its confidence intervals, which are derived and used in the analysis of the considered explosive targets. The detection results are better when decreased target endmembers are used to match the scene resolution, rather than using endmembers at the full resolution of the target. Unmanned aerial vehicles, as carriers of snapshot hyperspectral cameras, enable flexible target resolution selection and good area coverage.
KW  - explosive devices
KW  - hyperspectral data
KW  - simulation
KW  - Spectral Angle Mapping
KW  - UAV
DO  - 10.3390/rs13050837
TY  - EJOU
AU  - Imran
AU  - Iqbal, Naeem
AU  - Ahmad, Shabir
AU  - Kim, Do H.
TI  - Towards Mountain Fire Safety Using Fire Spread Predictive Analytics and Mountain Fire Containment in IoT Environment
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 5
SN  - 2071-1050

AB  - Mountains are popular tourist destinations due to their climate, fresh atmosphere, breathtaking sceneries, and varied topography. However, they are at times exposed to accidents, such as fire caused due to natural hazards and human activities. Such unforeseen fire accidents have a social, economic, and environmental impact on mountain towns worldwide. Protecting mountains from such fire accidents is also very challenging in terms of the high cost of fire containment resources, tracking fire spread, and evacuating the people at risk. This paper aims to fill this gap and proposes a three-fold methodology for fire safety in the mountains. The first part of the methodology is an optimization model for effective fire containment resource utilization. The second part of the methodology is a novel ensemble model based on machine learning, the heuristic approach, and principal component regression for predictive analytics of fire spread data. The final part of the methodology consists of an Internet of Things-based task orchestration approach to notify fire safety information to safety authorities. The proposed three-fold fire safety approach provides in-time information to safety authorities for making on-time decisions to minimize the damage caused by mountain fire with minimum containment cost. The performance of optimization models is evaluated in terms of execution time and cost. The particle swarm optimization-based model performs better in terms of cost, whereas the bat algorithm performs better in terms of execution time. The prediction models’ performance is evaluated in terms of root mean square error, mean absolute error, and mean absolute percentage error. The proposed ensemble-based prediction model accuracy for fire spread and burned area prediction is higher than that of the state-of-the-art algorithms. It is evident from the results that the proposed fire safety mechanism is a step towards efficient mountain fire safety management.
KW  - fire spread prediction
KW  - fire spread notification
KW  - predictive analysis
KW  - optimization
KW  - fire containment
DO  - 10.3390/su13052461
TY  - EJOU
AU  - Safonova, Anastasiia
AU  - Guirado, Emilio
AU  - Maglinets, Yuriy
AU  - Alcaraz-Segura, Domingo
AU  - Tabik, Siham
TI  - Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask R-CNN
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.
KW  - instance segmentation
KW  - machine learning
KW  - deep neural networks
KW  - olive trees
KW  - ultra-high resolution images
DO  - 10.3390/s21051617
TY  - EJOU
AU  - Kwan, Chiman
TI  - Safety Enhancement of UAVs from the Signal Processing’s Perspectives: A Bird’s Eye View
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - Unmanned air vehicles (UAVs) or drones have gained popularity in recent years. However, the US Federal Aviation Administration (FAA) is still hesitant to open up the national air space (NAS) to UAVs due to safety concerns because UAVs have several orders of magnitude of more accidents than manned aircraft. To limit the scope in this paper, we focus on large, heavy, and expensive UAVs that can be used for cargo transfer and search and rescue operations, not small radio-controlled toy drones. We first present a general architecture for enhancing the safety of UAVs. We then illustrate how signal processing technologies can help enhance the safety of UAVs. In particular, we provide a bird’s eye view of the application of signal processing algorithms on condition-based maintenance, structural health monitoring, fault diagnostics, and fault mitigation, which all play critical roles in UAV safety. Some practical applications are used to illustrate the importance of the various algorithms.
KW  - UAVs
KW  - safety
KW  - drones
KW  - signal processing
KW  - condition-based maintenance
KW  - structural health monitoring
KW  - fault diagnostics
KW  - fault mitigation
KW  - contingency planning
DO  - 10.3390/drones5010016
TY  - EJOU
AU  - Ahmad, Muhammad
AU  - Hussain, Zukhruf L.
AU  - Shah, Syed I.
AU  - Shams, Taimur A.
TI  - Estimation of Stability Parameters for Wide Body Aircraft Using Computational Techniques
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 5
SN  - 2076-3417

AB  - In this paper, we present the procedure of estimating the aerodynamic coefficients for a commercial aviation aircraft from geometric parameters at low-cruise-flight conditions using US DATCOM (United States Data Compendium) and XFLR software. The purpose of this research was to compare the stability parameters from both pieces of software to determine the efficacy of software solution for a wide-body aircraft at the stated flight conditions. During the initial phase of this project, the geometric parameters were acquired from established literature. In the next phase, stability and control coefficients of the aircraft were estimated using both pieces of software in parallel. Results obtained from both pieces of software were compared for any differences and the both pieces of software were validated with analytical correlations as presented in literature. The plots of various parameters with variations of the angle of attack or control surface deflection have also been obtained and presented. The differences between the software solutions and the analytical results can be associated with approximations of techniques used in software (the vortex lattice method is the background theory used in both DATCOM and XFLR). Additionally, from the results, it can be concluded that XFLR is more reliable than DATCOM for longitudinal, directional, and lateral stability/control coefficients. Analyses of a Boeing 747-200 (a wide-body commercial airliner) in DATCOM and XFLR for complete stability/control analysis including all modes in the longitudinal and lateral directions have been presented. DATCOM already has a sample analysis of a previous version of the Boeing 737; however, the Boeing 747-200 is much larger than the former, and complete analysis was, therefore, felt necessary to study its aerodynamics characteristics. Furthermore, in this research, it was concluded that XFLR is more reliable for various categories of aircraft alike in terms of general stability and control coefficients, and hence many aircraft can be dependably modeled and analyzed in this software.
KW  - aerodynamic coefficients
KW  - Boeing
KW  - DATCOM
KW  - stability and control
KW  - XFLR
DO  - 10.3390/app11052087
TY  - EJOU
AU  - Papić, Vladan
AU  - Šolić, Petar
AU  - Milan, Ante
AU  - Gotovac, Sven
AU  - Polić, Miljenko
TI  - High-Resolution Image Transmission from UAV to Ground Station for Search and Rescue Missions Planning
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 5
SN  - 2076-3417

AB  - Search and rescue (SAR) missions comprise search for, and provision of aid to people who are in distress or imminent danger. Providing the best possible input for the planners and search teams, up-to-date information about the terrain is of essential importance because every additional hour needed to search a person decreases probability of success. Therefore, availability of aerial images and updated terrain maps as a basis for planning and monitoring SAR missions in real-time is very important for rescuers. In this paper, we present a system for transmission of high-resolution images from an unmanned aerial vehicle (UAV) to the ground station (GS). We define and calculate data rate and transmission distance requirements between the UAV and GS in a mission scenario. Five tests were designed and carried out to confirm the viability of the proposed system architecture and modules. Test results present throughput measurements for various UAV and GS distances, antenna heights and UAV antenna yaw angles. Experimental results from the series of conducted outdoor tests show that the proposed solution using two pMDDL2450 datalinks at 2.4 GHz and a directional antenna on the receiving side can be used for a real-time transmission of high-resolution images acquired with a camera on a UAV. Achieved throughput at a UAV-GS distance of 5 km was 1.4 MB/s (11.2 Mbps). The limitations and possible improvements of the proposed system as well as future work are also discussed.
KW  - search and rescue
KW  - high-resolution images transmission
KW  - throughput
KW  - data link
DO  - 10.3390/app11052105
TY  - EJOU
AU  - Sadeghi-Tehran, Pouria
AU  - Virlet, Nicolas
AU  - Hawkesford, Malcolm J.
TI  - A Neural Network Method for Classification of Sunlit and Shaded Components of Wheat Canopies in the Field Using High-Resolution Hyperspectral Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - (1) Background: Information rich hyperspectral sensing, together with robust image analysis, is providing new research pathways in plant phenotyping. This combination facilitates the acquisition of spectral signatures of individual plant organs as well as providing detailed information about the physiological status of plants. Despite the advances in hyperspectral technology in field-based plant phenotyping, little is known about the characteristic spectral signatures of shaded and sunlit components in wheat canopies. Non-imaging hyperspectral sensors cannot provide spatial information; thus, they are not able to distinguish the spectral reflectance differences between canopy components. On the other hand, the rapid development of high-resolution imaging spectroscopy sensors opens new opportunities to investigate the reflectance spectra of individual plant organs which lead to the understanding of canopy biophysical and chemical characteristics. (2) Method: This study reports the development of a computer vision pipeline to analyze ground-acquired imaging spectrometry with high spatial and spectral resolutions for plant phenotyping. The work focuses on the critical steps in the image analysis pipeline from pre-processing to the classification of hyperspectral images. In this paper, two convolutional neural networks (CNN) are employed to automatically map wheat canopy components in shaded and sunlit regions and to determine their specific spectral signatures. The first method uses pixel vectors of the full spectral features as inputs to the CNN model and the second method integrates the dimension reduction technique known as linear discriminate analysis (LDA) along with the CNN to increase the feature discrimination and improves computational efficiency. (3) Results: The proposed technique alleviates the limitations and lack of separability inherent in existing pre-defined hyperspectral classification methods. It optimizes the use of hyperspectral imaging and ensures that the data provide information about the spectral characteristics of the targeted plant organs, rather than the background. We demonstrated that high-resolution hyperspectral imagery along with the proposed CNN model can be powerful tools for characterizing sunlit and shaded components of wheat canopies in the field. The presented method will provide significant advances in the determination and relevance of spectral properties of shaded and sunlit canopy components under natural light conditions.
KW  - hyperspectral imaging
KW  - phenotyping
KW  - hyperspectral image classification (HSI)
KW  - wheat canopies
KW  - segmentation
KW  - near infrared
DO  - 10.3390/rs13050898
TY  - EJOU
AU  - Ali, Luqman
AU  - Alnajjar, Fady
AU  - Jassmi, Hamad A.
AU  - Gocho, Munkhjargal
AU  - Khan, Wasif
AU  - Serhani, M. A.
TI  - Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.
KW  - automatic inspection
KW  - convolutional neural networks
KW  - crack detection
KW  - deep learning
KW  - transfer learning
DO  - 10.3390/s21051688
TY  - EJOU
AU  - Najafi, Payam
AU  - Feizizadeh, Bakhtiar
AU  - Navid, Hossein
TI  - A Comparative Approach of Fuzzy Object Based Image Analysis and Machine Learning Techniques Which Are Applied to Crop Residue Cover Mapping by Using Sentinel-2 Satellite and UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - Conservation tillage methods through leaving the crop residue cover (CRC) on the soil surface protect it from water and wind erosions. Hence, the percentage of the CRC on the soil surface is very critical for the evaluation of tillage intensity. The objective of this study was to develop a new methodology based on the semiautomated fuzzy object based image analysis (fuzzy OBIA) and compare its efficiency with two machine learning algorithms which include: support vector machine (SVM) and artificial neural network (ANN) for the evaluation of the previous CRC and tillage intensity. We also considered the spectral images from two remotely sensed platforms of the unmanned aerial vehicle (UAV) and Sentinel-2 satellite, respectively. The results indicated that fuzzy OBIA for multispectral Sentinel-2 image based on Gaussian membership function with overall accuracy and Cohen’s kappa of 0.920 and 0.874, respectively, surpassed machine learning algorithms and represented the useful results for the classification of tillage intensity. The results also indicated that overall accuracy and Cohen’s kappa for the classification of RGB images from the UAV using fuzzy OBIA method were 0.860 and 0.779, respectively. The semiautomated fuzzy OBIA clearly outperformed machine learning approaches in estimating the CRC and the classification of the tillage methods and also it has the potential to substitute or complement field techniques.
KW  - fuzzy object based approach
KW  - neural network
KW  - support vector machine
KW  - tillage intensity
KW  - soil erosion
DO  - 10.3390/rs13050937
TY  - EJOU
AU  - Xue, Yongan
AU  - Zhao, Jinling
AU  - Zhang, Mingmei
TI  - A Watershed-Segmentation-Based Improved Algorithm for Extracting Cultivated Land Boundaries
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - To accurately extract cultivated land boundaries based on high-resolution remote sensing imagery, an improved watershed segmentation algorithm was proposed herein based on a combination of pre- and post-improvement procedures. Image contrast enhancement was used as the pre-improvement, while the color distance of the Commission Internationale de l´Eclairage (CIE) color space, including the Lab and Luv, was used as the regional similarity measure for region merging as the post-improvement. Furthermore, the area relative error criterion (δA), the pixel quantity error criterion (δP), and the consistency criterion (Khat) were used for evaluating the image segmentation accuracy. The region merging in Red–Green–Blue (RGB) color space was selected to compare the proposed algorithm by extracting cultivated land boundaries. The validation experiments were performed using a subset of Chinese Gaofen-2 (GF-2) remote sensing image with a coverage area of 0.12 km2. The results showed the following: (1) The contrast-enhanced image exhibited an obvious gain in terms of improving the image segmentation effect and time efficiency using the improved algorithm. The time efficiency increased by 10.31%, 60.00%, and 40.28%, respectively, in the RGB, Lab, and Luv color spaces. (2) The optimal segmentation and merging scale parameters in the RGB, Lab, and Luv color spaces were C for minimum areas of 2000, 1900, and 2000, and D for a color difference of 1000, 40, and 40. (3) The algorithm improved the time efficiency of cultivated land boundary extraction in the Lab and Luv color spaces by 35.16% and 29.58%, respectively, compared to the RGB color space. The extraction accuracy was compared to the RGB color space using the δA, δP, and Khat, that were improved by 76.92%, 62.01%, and 16.83%, respectively, in the Lab color space, while they were 55.79%, 49.67%, and 13.42% in the Luv color space. (4) Through the visual comparison, time efficiency, and segmentation accuracy, the comprehensive extraction effect using the proposed algorithm was obviously better than that of RGB color-based space algorithm. The established accuracy evaluation indicators were also proven to be consistent with the visual evaluation. (5) The proposed method has a satisfying transferability by a wider test area with a coverage area of 1 km2. In addition, the proposed method, based on the image contrast enhancement, was to perform the region merging in the CIE color space according to the simulated immersion watershed segmentation results. It is a useful attempt for the watershed segmentation algorithm to extract cultivated land boundaries, which provides a reference for enhancing the watershed algorithm.
KW  - cultivated land
KW  - watershed segmentation algorithm
KW  - image contrast enhancement
KW  - region merging
KW  - CIE color space
KW  - Lab
KW  - Luv
DO  - 10.3390/rs13050939
TY  - EJOU
AU  - Müezzinoğlu, Taha
AU  - Karaköse, Mehmet
TI  - An Intelligent Human–Unmanned Aerial Vehicle Interaction Approach in Real Time Based on Machine Learning Using Wearable Gloves
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - The interactions between humans and unmanned aerial vehicles (UAVs), whose applications are increasing in the civilian field rather than for military purposes, are a popular future research area. Human–UAV interactions are a challenging problem because UAVs move in a three-dimensional space. In this paper, we present an intelligent human–UAV interaction approach in real time based on machine learning using wearable gloves. The proposed approach offers scientific contributions such as a multi-mode command structure, machine-learning-based recognition, task scheduling algorithms, real-time usage, robust and effective use, and high accuracy rates. For this purpose, two wearable smart gloves working in real time were designed. The signal data obtained from the gloves were processed with machine-learning-based methods and classified multi-mode commands were included in the human–UAV interaction process via the interface according to the task scheduling algorithm to facilitate sequential and fast operation. The performance of the proposed approach was verified on a data set created using 25 different hand gestures from 20 different people. In a test using the proposed approach on 49,000 datapoints, process time performance of a few milliseconds was achieved with approximately 98 percent accuracy.
KW  - human–UAV interaction
KW  - wearable technologies
KW  - Internet of Things (IoT)
KW  - human–computer interaction
KW  - smart systems
DO  - 10.3390/s21051766
TY  - EJOU
AU  - Kraft, Marek
AU  - Piechocki, Mateusz
AU  - Ptak, Bartosz
AU  - Walas, Krzysztof
TI  - Autonomous, Onboard Vision-Based Trash and Litter Detection in Low Altitude Aerial Images Collected by an Unmanned Aerial Vehicle
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - Public littering and discarded trash are, despite the effort being put to limit it, still a serious ecological, aesthetic, and social problem. The problematic waste is usually localised and picked up by designated personnel, which is a tiresome, time-consuming task. This paper proposes a low-cost solution enabling the localisation of trash and litter objects in low altitude imagery collected by an unmanned aerial vehicle (UAV) during an autonomous patrol mission. The objects of interest are detected in the acquired images and put on the global map using a set of onboard sensors commonly found in typical UAV autopilots. The core object detection algorithm is based on deep, convolutional neural networks. Since the task is domain-specific, a dedicated dataset of images containing objects of interest was collected and annotated. The dataset is made publicly available, and its description is contained in the paper. The dataset was used to test a range of embedded devices enabling the deployment of deep neural networks for inference onboard the UAV. The results of measurements in terms of detection accuracy and processing speed are enclosed, and recommendations for the neural network model and hardware platform are given based on the obtained values. The complete system can be put together using inexpensive, off-the-shelf components, and perform autonomous localisation of discarded trash, relieving human personnel of this burdensome task, and enabling automated pickup planning.
KW  - deep learning
KW  - object detection
KW  - image processing
KW  - trash
KW  - litter
KW  - UAV
KW  - YOLO
DO  - 10.3390/rs13050965
TY  - EJOU
AU  - Crusiol, Luís G.
AU  - Nanni, Marcos R.
AU  - Furlanetto, Renato H.
AU  - Sibaldelli, Rubson N.
AU  - Cezar, Everson
AU  - Sun, Liang
AU  - Foloni, José S.
AU  - Mertz-Henning, Liliane M.
AU  - Nepomuceno, Alexandre L.
AU  - Neumaier, Norman
AU  - Farias, José R.
TI  - Yield Prediction in Soybean Crop Grown under Different Levels of Water Availability Using Reflectance Spectroscopy and Partial Least Squares Regression
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - Soybean grain yield has regularly been impaired by drought periods, and the future climatic scenarios for soybean production might drastically impact yields worldwide. In this context, the knowledge of soybean yield is extremely important to subsidize government and corporative decisions over technical issues. This paper aimed to predict grain yield in soybean crop grown under different levels of water availability using reflectance spectroscopy and partial least square regression (PLSR). Field experiments were undertaken at Embrapa Soja (Brazilian Agricultural Research Corporation) in the 2016/2017, 2017/2018 and 2018/2019 cropping seasons. The data collected were analyzed following a split plot model in a randomized complete block design, with four blocks. The following water conditions were distributed in the field plots: irrigated (IRR), non-irrigated (NIRR) and water deficit induced at the vegetative (WDV) and reproductive stages (WDR) using rainout shelters. Soybean genotypes with different responses to water deficit were distributed in the subplots. Soil moisture and weather data were monitored daily. A total of 7216 leaf reflectance (from 400 to 2500 nm, measured by the FieldSpec 3 Jr spectroradiometer) was collected at 24 days in the three cropping seasons. The PLSR (p ≤ 0.05) was performed to predict soybean grain yield by its leaf-based reflectance spectroscopy. The results demonstrated the highest accuracy in soybean grain yield prediction at the R5 phenological stage, corresponding to the period when grains are being formed (R2 ranging from 0.731 to 0.924 and the RMSE from 334 to 403 kg ha−1—7.77 to 11.33%). Analyzing the three cropping seasons into a single PLSR model at R5 stage, R2 equal to 0.775, 0.730 and 0.688 were obtained at the calibration, cross-validation and external validation stages, with RMSE lower than 634 kg ha−1 (13.34%). The PLSR demonstrated higher accuracy in plants submitted to water deficit both at the vegetative and reproductive periods in comparison to plants under natural rainfall or irrigation.
KW  - Glycine max (L.) Merrill
KW  - drought stress
KW  - soybean genotypes
KW  - leaf-based data
KW  - hyperspectral reflectance
DO  - 10.3390/rs13050977
TY  - EJOU
AU  - Gray, Ross E. J.
AU  - Ewers, Robert M.
TI  - Monitoring Forest Phenology in a Changing World
T2  - Forests

PY  - 2021
VL  - 12
IS  - 3
SN  - 1999-4907

AB  - Plant phenology is strongly interlinked with ecosystem processes and biodiversity. Like many other aspects of ecosystem functioning, it is affected by habitat and climate change, with both global change drivers altering the timings and frequency of phenological events. As such, there has been an increased focus in recent years to monitor phenology in different biomes. A range of approaches for monitoring phenology have been developed to increase our understanding on its role in ecosystems, ranging from the use of satellites and drones to collection traps, each with their own merits and limitations. Here, we outline the trade-offs between methods (spatial resolution, temporal resolution, cost, data processing), and discuss how their use can be optimised in different environments and for different goals. We also emphasise emerging technologies that will be the focus of monitoring in the years to follow and the challenges of monitoring phenology that still need to be addressed. We conclude that there is a need to integrate studies that incorporate multiple monitoring methods, allowing the strengths of one to compensate for the weaknesses of another, with a view to developing robust methods for upscaling phenological observations from point locations to biome and global scales and reconciling data from varied sources and environments. Such developments are needed if we are to accurately quantify the impacts of a changing world on plant phenology.
KW  - drones
KW  - ecosystem change
KW  - methods
KW  - monitoring
KW  - phenology
KW  - remote sensing
KW  - UAVs
DO  - 10.3390/f12030297
TY  - EJOU
AU  - Mannino, Anna M.
AU  - Borfecchia, Flavio
AU  - Micheli, Carla
TI  - Tracking Marine Alien Macroalgae in the Mediterranean Sea: The Contribution of Citizen Science and Remote Sensing
T2  - Journal of Marine Science and Engineering

PY  - 2021
VL  - 9
IS  - 3
SN  - 2077-1312

AB  - The accelerating rate of the introduction of non-indigenous species (NIS) and the magnitude of shipping traffic make the Mediterranean Sea a hotspot of biological invasions. For the effective management of NIS, early detection and intensive monitoring over time and space are essential. Here, we present an overview of possible applications of citizen science and remote sensing in monitoring alien seaweeds in the Mediterranean Sea. Citizen science activities, involving the public (e.g., tourists, fishermen, divers) in the collection of data, have great potential for monitoring NIS. The innovative methodologies, based on remote sensing techniques coupled with in situ/laboratory advanced sampling/analysis methods for tracking such species, may be useful and effective tools for easily assessing NIS distribution patterns and monitoring the space/time changes in habitats in order to support the sustainable management of the ecosystems. The reported case studies highlight how these cost-effective systems can be useful complementary tools for monitoring NIS, especially in marine protected areas, which, despite their fundamental role in the conservation of marine biodiversity, are not immune to the introduction of NIS. To ensure effective and long-lasting management strategies, collaborations between researchers, policy makers and citizens are essential.
KW  - non-indigenous species
KW  - Mediterranean Sea
KW  - monitoring
KW  - managing
KW  - citizen science
KW  - remote sensing
KW  - Landsat 8 OLI
DO  - 10.3390/jmse9030288
TY  - EJOU
AU  - Ko, Chiung
AU  - Lee, Seunghyun
AU  - Yim, Jongsu
AU  - Kim, Donggeun
AU  - Kang, Jintaek
TI  - Comparison of Forest Inventory Methods at Plot-Level between a Backpack Personal Laser Scanning (BPLS) and Conventional Equipment in Jeju Island, South Korea
T2  - Forests

PY  - 2021
VL  - 12
IS  - 3
SN  - 1999-4907

AB  - In recent years, light detection and ranging (LiDAR) has been increasingly utilized to estimate forest resources. This study was conducted to identify the applicability of a LiDAR sensor for such estimations by comparing data on a tree’s position, height, and diameter at breast height (DBH) obtained using the sensor with those by existing forest inventory methods for a Cryptomeria japonica forest in Jeju Island, South Korea. For this purpose, a backpack personal laser scanning device (BPLS, Greenvalley International, Model D50) was employed in a protected forest, where cutting is not allowed, as a non-invasive means, simultaneously assessing the device’s field applicability. The data collected by the sensor were divided into seven different pathway variations, or “patterns” to consider the density of the sample plots and enhance the efficiency. The accuracy of estimating the variables of each tree was then assessed. The time spent acquiring and processing real-time data was also analyzed for each method, as well as total time and the time required for each measurement. The findings showed that the rate of detection of standing trees by LiDAR was 100%. Additionally, a high statistical accuracy was observed in pattern 5 (DBH: RMSE 1.22 cm, bias—0.90 cm, Height: RMSE 1.66 m, bias—1.18 m) and pattern 7 (DBH: RMSE 1.22 cm, bias—0.92 cm, Height: RMSE 1.48 m, bias—1.23 m) compared to the results from the typical inventory method. A range of 115–162.5 min/ha was required to process the data using the LiDAR, while 322.5–567.5 min was required for the typical inventory method. Thus, the application of a backpack personal LiDAR can lead to higher efficiency when conducting a forest resource inventory in a coniferous plantation with understory vegetation. Further research in various stands is necessary to confirm the efficiency of using backpack personal laser scanning.
KW  - LiDAR
KW  - BPLS
KW  - TLS
KW  - forest inventory
KW  - point cloud
DO  - 10.3390/f12030308
TY  - EJOU
AU  - Gebrehiwot, Asmamaw A.
AU  - Hashemi-Beni, Leila
TI  - Three-Dimensional Inundation Mapping Using UAV Image Segmentation and Digital Surface Model
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 3
SN  - 2220-9964

AB  - Flood occurrence is increasing due to the expansion of urbanization and extreme weather like hurricanes; hence, research on methods of inundation monitoring and mapping has increased to reduce the severe impacts of flood disasters. This research studies and compares two methods for inundation depth estimation using UAV images and topographic data. The methods consist of three main stages: (1) extracting flooded areas and create 2D inundation polygons using deep learning; (2) reconstructing 3D water surface using the polygons and topographic data; and (3) deriving a water depth map using the 3D reconstructed water surface and a pre-flood DEM. The two methods are different at reconstructing the 3D water surface (stage 2). The first method uses structure from motion (SfM) for creating a point cloud of the area from overlapping UAV images, and the water polygons resulted from stage 1 is applied for water point cloud classification. While the second method reconstructs the water surface by intersecting the water polygons and a pre-flood DEM created using the pre-flood LiDAR data. We evaluate the proposed methods for inundation depth mapping over the Town of Princeville during a flooding event during Hurricane Matthew. The methods are compared and validated using the USGS gauge water level data acquired during the flood event. The RMSEs for water depth using the SfM method and integrated method based on deep learning and DEM were 0.34m and 0.26m, respectively.
KW  - 3D inundation mapping
KW  - remote sensing
KW  - CNN
KW  - SfM
KW  - LiDAR
KW  - GFI
DO  - 10.3390/ijgi10030144
TY  - EJOU
AU  - Niu, Yaxiao
AU  - Zhang, Huihui
AU  - Han, Wenting
AU  - Zhang, Liyuan
AU  - Chen, Haipeng
TI  - A Fixed-Threshold Method for Estimating Fractional Vegetation Cover of Maize under Different Levels of Water Stress
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - Accurate estimation of fractional vegetation cover (FVC) from digital images taken by commercially available cameras is of great significance in order to monitor the vegetation growth status, especially when plants are under water stress. Two classic threshold-based methods, namely, the intersection method (T1 method) and the equal misclassification probability method (T2 method), have been widely applied to Red-Green-Blue (RGB) images. However, the high coverage and severe water stress of crops in the field make it difficult to extract FVC stably and accurately. To solve this problem, this paper proposes a fixed-threshold method based on the statistical analysis of thresholds obtained from the two classic threshold approaches. Firstly, a Gaussian mixture model (GMM), including the distributions of green vegetation and backgrounds, was fitted on four color features: excessive green index, H channel of the Hue-Saturation-Value (HSV) color space, a* channel of the CIE L*a*b* color space, and the brightness-enhanced a* channel (denoted as a*_I). Secondly, thresholds were calculated by applying the T1 and T2 methods to the GMM of each color feature. Thirdly, based on the statistical analysis of the thresholds with better performance between T1 and T2, the fixed-threshold method was proposed. Finally, the fixed-threshold method was applied to the optimal color feature a*_I to estimate FVC, and was compared with the two classic approaches. Results showed that, for some images with high reference FVC, FVC was seriously underestimated by 0.128 and 0.141 when using the T1 and T2 methods, respectively, but this problem was eliminated by the proposed fixed-threshold method. Compared with the T1 and T2 methods, for images taken in plots under severe water stress, the mean absolute error of FVC obtained by the fixed-threshold method was decreased by 0.043 and 0.193, respectively. Overall, the FVC estimation using the proposed fixed-threshold method has the advantages of robustness, accuracy, and high efficiency, with a coefficient of determination (R2) of 0.99 and root mean squared error (RMSE) of 0.02.
KW  - proximal RGB image
KW  - color feature
KW  - Gaussian mixture model
KW  - expectation-maximization algorithm
DO  - 10.3390/rs13051009
TY  - EJOU
AU  - Nemer, Ibrahim
AU  - Sheltami, Tarek
AU  - Ahmad, Irfan
AU  - Yasar, Ansar U.
AU  - Abdeen, Mohammad A. R.
TI  - RF-Based UAV Detection and Identification Using Hierarchical Learning Approach
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 6
SN  - 1424-8220

AB  - Unmanned Aerial Vehicles (UAVs) are widely available in the current market to be used either for recreation as a hobby or to serve specific industrial requirements, such as agriculture and construction. However, illegitimate and criminal usage of UAVs is also on the rise which introduces their effective identification and detection as a research challenge. This paper proposes a novel machine learning-based for efficient identification and detection of UAVs. Specifically, an improved UAV identification and detection approach is presented using an ensemble learning based on the hierarchical concept, along with pre-processing and feature extraction stages for the Radio Frequency (RF) data. Filtering is applied on the RF signals in the detection approach to improve the output. This approach consists of four classifiers and they are working in a hierarchical way. The sample will pass the first classifier to check the availability of the UAV, and then it will specify the type of the detected UAV using the second classifier. The last two classifiers will handle the sample that is related to Bebop and AR to specify their mode. Evaluation of the proposed approach with publicly available dataset demonstrates better efficiency compared to existing detection systems in the literature. It has the ability to investigate whether a UAV is flying within the area or not, and it can directly identify the type of UAV and then the flight mode of the detected UAV with accuracy around 99%.
KW  - radio frequency
KW  - unmanned aerial vehicles
KW  - machine learning
KW  - detection and identification
DO  - 10.3390/s21061947
TY  - EJOU
AU  - Dainelli, Riccardo
AU  - Toscano, Piero
AU  - Di Gennaro, Salvatore F.
AU  - Matese, Alessandro
TI  - Recent Advances in Unmanned Aerial Vehicle Forest Remote Sensing—A Systematic Review. Part I: A General Framework
T2  - Forests

PY  - 2021
VL  - 12
IS  - 3
SN  - 1999-4907

AB  - Natural, semi-natural, and planted forests are a key asset worldwide, providing a broad range of positive externalities. For sustainable forest planning and management, remote sensing (RS) platforms are rapidly going mainstream. In a framework where scientific production is growing exponentially, a systematic analysis of unmanned aerial vehicle (UAV)-based forestry research papers is of paramount importance to understand trends, overlaps and gaps. The present review is organized into two parts (Part I and Part II). Part II inspects specific technical issues regarding the application of UAV-RS in forestry, together with the pros and cons of different UAV solutions and activities where additional effort is needed, such as the technology transfer. Part I systematically analyzes and discusses general aspects of applying UAV in natural, semi-natural and artificial forestry ecosystems in the recent peer-reviewed literature (2018–mid-2020). The specific goals are threefold: (i) create a carefully selected bibliographic dataset that other researchers can draw on for their scientific works; (ii) analyze general and recent trends in RS forest monitoring (iii) reveal gaps in the general research framework where an additional activity is needed. Through double-step filtering of research items found in the Web of Science search engine, the study gathers and analyzes a comprehensive dataset (226 articles). Papers have been categorized into six main topics, and the relevant information has been subsequently extracted. The strong points emerging from this study concern the wide range of topics in the forestry sector and in particular the retrieval of tree inventory parameters often through Digital Aerial Photogrammetry (DAP), RGB sensors, and machine learning techniques. Nevertheless, challenges still exist regarding the promotion of UAV-RS in specific parts of the world, mostly in the tropical and equatorial forests. Much additional research is required for the full exploitation of hyperspectral sensors and for planning long-term monitoring.
KW  - UAV
KW  - drone
KW  - forest
KW  - precision forestry
KW  - remote sensing
KW  - meta-analysis
KW  - management
KW  - natural woodland
KW  - plantation forests
DO  - 10.3390/f12030327
TY  - EJOU
AU  - Ma, Qian
AU  - Han, Wenting
AU  - Huang, Shenjin
AU  - Dong, Shide
AU  - Li, Guang
AU  - Chen, Haipeng
TI  - Distinguishing Planting Structures of Different Complexity from UAV Multispectral Images
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 6
SN  - 1424-8220

AB  - This study explores the classification potential of a multispectral classification model for farmland with planting structures of different complexity. Unmanned aerial vehicle (UAV) remote sensing technology is used to obtain multispectral images of three study areas with low-, medium-, and high-complexity planting structures, containing three, five, and eight types of crops, respectively. The feature subsets of three study areas are selected by recursive feature elimination (RFE). Object-oriented random forest (OB-RF) and object-oriented support vector machine (OB-SVM) classification models are established for the three study areas. After training the models with the feature subsets, the classification results are evaluated using a confusion matrix. The OB-RF and OB-SVM models’ classification accuracies are 97.09% and 99.13%, respectively, for the low-complexity planting structure. The equivalent values are 92.61% and 99.08% for the medium-complexity planting structure and 88.99% and 97.21% for the high-complexity planting structure. For farmland with fragmentary plots and a high-complexity planting structure, as the planting structure complexity changed from low to high, both models’ overall accuracy levels decreased. The overall accuracy of the OB-RF model decreased by 8.1%, and that of the OB-SVM model only decreased by 1.92%. OB-SVM achieves an overall classification accuracy of 97.21%, and a single-crop extraction accuracy of at least 85.65%. Therefore, UAV multispectral remote sensing can be used for classification applications in highly complex planting structures.
KW  - UAV
KW  - multispectral remote sensing
KW  - farmland objects
KW  - classification
KW  - RF
KW  - SVM
DO  - 10.3390/s21061994
TY  - EJOU
AU  - Wang, Guoqing
AU  - Chen, He
AU  - Xie, Yizhuang
TI  - An Efficient Dual-Channel Data Storage and Access Method for Spaceborne Synthetic Aperture Radar Real-Time Processing
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 6
SN  - 2079-9292

AB  - With the development of remote sensing technology and very large-scale integrated circuit (VLSI) technology, the real-time processing of spaceborne Synthetic Aperture Radar (SAR) has greatly improved the ability of Earth observation. However, the characteristics of external memory have led to matrix transposition becoming a technical bottleneck that limits the real-time performance of the SAR imaging system. In order to solve this problem, this paper combines the optimized data mapping method and reasonable hardware architecture to implement a data controller based on the Field-Programmable Gate Array (FPGA). First of all, this paper proposes an optimized dual-channel data storage and access method, so that the two-dimensional data access efficiency can be improved. Then, a hardware architecture is designed with register manager, simplified address generator and dual-channel Double-Data-Rate Three Synchronous Dynamic Random-Access Memory (DDR3 SDRAM) access mode. Finally, the proposed data controller is implemented on the Xilinx XC7VX690T FPGA chip. The experimental results show that the reading efficiency of the data controller proposed is 80% both in the range direction and azimuth direction, and the writing efficiency is 66% both in the range direction and azimuth direction. The results of a comparison with the recent implementations show that the proposed data controller has a higher data bandwidth, is more flexible in its design, and is suitable for use in spaceborne scenarios.
KW  - Synthetic Aperture Radar (SAR)
KW  - real-time processing
KW  - data storage and access
KW  - Field Programmable Gate Array (FPGA)
KW  - dual-channel pipeline processing
DO  - 10.3390/electronics10060662
TY  - EJOU
AU  - Elsayed, Salah
AU  - El-Hendawy, Salah
AU  - Khadr, Mosaad
AU  - Elsherbiny, Osama
AU  - Al-Suhaibani, Nasser
AU  - Dewir, Yaser H.
AU  - Tahir, Muhammad U.
AU  - Mubushar, Muhammad
AU  - Darwish, Waleed
TI  - Integration of Spectral Reflectance Indices and Adaptive Neuro-Fuzzy Inference System for Assessing the Growth Performance and Yield of Potato under Different Drip Irrigation Regimes
T2  - Chemosensors

PY  - 2021
VL  - 9
IS  - 3
SN  - 2227-9040

AB  - Simultaneous and timely assessment of growth and water status-related plant traits is critical for precision irrigation management in arid regions. Here, we used proximal hyperspectral sensing tools to estimate biomass fresh weight (BFW), biomass dry weight (BDW), canopy water content (CWC), and total tuber yield (TTY) of two potato varieties irrigated with 100%, 75%, and 50% of the estimated crop evapotranspiration (ETc). Plant traits were assessed remotely using published and newly constructed vegetation and water spectral reflectance indices (SRIs). We integrated genetic algorithm (GA) and adaptive neuro-fuzzy inference system (ANFIS) models to predict the measured traits based on all SRIs. The different plant traits and SRIs varied significantly (p &lt; 0.05) between the three irrigation regimes for the two varieties. The values of plant traits and majority SRIs showed a continuous decrease from the 100% ETc to the 50% ETc. Water-SRIs performed better than vegetation-SRIs for estimating the four plant traits. Almost all indices of the two SRI types had a weak relationship with the four plant traits (R2 = 0.00–0.37) under each irrigation regime. However, the majority of vegetation-SRIs and all water-SRIs showed strong relationships with BFW, CWC, and TTY (R2 ≥ 0.65) and moderate relationships with BDW (R2 ≥ 0.40) when the data of all irrigation regimes and varieties were analyzed together for each growing season or the data of all irrigation regimes, varieties, and seasons were combined together. The ANFIS-GA model predicted plant traits with satisfactory accuracy in both calibration (R2 = 1.0) and testing (R2 = 0.72–0.97) modes. The results indicate that SRI-based ANFIS models can improve plant trait estimation. This analysis also confirmed the benefits of applying GA to ANFIS to estimate plant responses to different growth conditions.
KW  - ANFIS
KW  - biomass
KW  - data driven
KW  - genetic algorithm
KW  - proximal remote sensing
KW  - spectral indices
KW  - tuber yield
DO  - 10.3390/chemosensors9030055
TY  - EJOU
AU  - Lisowski, Józef
TI  - Synthesis of a Path-Planning Algorithm for Autonomous Robots Moving in a Game Environment during Collision Avoidance
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 6
SN  - 2079-9292

AB  - This paper describes and illustrates the optimization of a safe mobile robot control process in collision situations using the model of a multistep matrix game of many participants in the form of a dual linear programming problem. The synthesis of non-cooperative and cooperative game control software was performed in Matlab/Simulink software to determine the safe path of the robot when passing a greater number of other robots and obstacles. The operation of the game motion control algorithm of a mobile robot is illustrated by computer simulations made in the Matlab/Simulink program of two real previously recorded navigation situations while passing dozens of other autonomous mobile robots.
KW  - path planning
KW  - mobile robot
KW  - optimization
KW  - safe control
KW  - matrix game
KW  - computer simulation
DO  - 10.3390/electronics10060675
TY  - EJOU
AU  - Peng, Xingshuo
AU  - Han, Wenting
AU  - Ao, Jianyi
AU  - Wang, Yi
TI  - Assimilation of LAI Derived from UAV Multispectral Data into the SAFY Model to Estimate Maize Yield
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 6
SN  - 2072-4292

AB  - In this study, we develop a method to estimate corn yield based on remote sensing data and ground monitoring data under different water treatments. Spatially explicit information on crop yields is essential for farmers and agricultural agencies to make well-informed decisions. One approach to estimate crop yield with remote sensing is data assimilation, which integrates sequential observations of canopy development from remote sensing into model simulations of crop growth processes. We found that leaf area index (LAI) inversion based on unmanned aerial vehicle (UAV) vegetation index has a high accuracy, with R2 and root mean square error (RMSE) values of 0.877 and 0.609, respectively. Maize yield estimation based on UAV remote sensing data and simple algorithm for yield (SAFY) crop model data assimilation has different yield estimation accuracy under different water treatments. This method can be used to estimate corn yield, where R2 is 0.855 and RMSE is 692.8kg/ha. Generally, the higher the water stress, the lower the estimation accuracy. Furthermore, we perform the yield estimate mapping at 2 m spatial resolution, which has a higher spatial resolution and accuracy than satellite remote sensing. The great potential of incorporating UAV observations with crop data to monitor crop yield, and improve agricultural management is therefore indicated.
KW  - UAV
KW  - leaf area index
KW  - yield estimation
KW  - yield
KW  - SAFY model
KW  - EnKF
DO  - 10.3390/rs13061094
TY  - EJOU
AU  - Emin, Mirzat
AU  - Anwar, Erpan
AU  - Liu, Suhong
AU  - Emin, Bilal
AU  - Mamut, Maryam
AU  - Abdukeram, Abduwali
AU  - Liu, Ting
TI  - Target Detection-Based Tree Recognition in a Spruce Forest Area with a High Tree Density—Implications for Estimating Tree Numbers
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 6
SN  - 2071-1050

AB  - Here, unmanned aerial vehicle (UAV) remote sensing and machine vision were used to automatically, accurately, and efficiently count Tianshan spruce and improve the efficiency of scientific forest management, focusing on a typical Tianshan spruce forest on Tianshan Mountain, middle Asia. First, the UAV in the sampling area was cropped from the image, and a target-labeling tool was used. The Tianshan spruce trees were annotated to construct a data set, and four models were used to identify and verify them in three different areas (low, medium, and high canopy closures). Finally, the combined number of trees was calculated. The average accuracy of the detection frame, mean accuracy and precision (mAP), was used to determine the target detection accuracy. The Faster Region Convolutional Neural Network (Faster-RCNN) model achieved the highest accuracies (96.36%, 96.32%, and 95.54% under low, medium, and high canopy closures, respectively) and the highest mAP (85%). Canopy closure affected the detection and recognition accuracy; YOLOv3, YOLOv4, and Faster-RCNN all showed varying spruce recognition accuracies at different densities. The accuracy of the Faster-RCNN model decreased by at least 0.82%. Combining UAV remote sensing with target detection networks can identify and quantify statistics regarding Tianshan spruce. This solves the shortcomings of traditional monitoring methods and is significant for understanding and monitoring forest ecosystems.
KW  - Tianshan spruce
KW  - target detection
KW  - UAV
KW  - forest inventory
DO  - 10.3390/su13063279
TY  - EJOU
AU  - Hardy, Tom
AU  - Kooistra, Lammert
AU  - Domingues Franceschini, Marston
AU  - Richter, Sebastiaan
AU  - Vonk, Erwin
AU  - van den Eertwegh, Gé
AU  - van Deijl, Dion
TI  - Sen2Grass: A Cloud-Based Solution to Generate Field-Specific Grassland Information Derived from Sentinel-2 Imagery
T2  - AgriEngineering

PY  - 2021
VL  - 3
IS  - 1
SN  - 2624-7402

AB  - Grasslands are important for their ecological values and for agricultural activities such as livestock production worldwide. Efficient grassland management is vital to these values and activities, and remote sensing technologies are increasingly being used to characterize the spatiotemporal variation of grasslands to support those management practices. For this study, Sentinel-2 satellite imagery was used as an input to develop an open-source and automated monitoring system (Sen2Grass) to gain field-specific grassland information on the national and regional level for any given time range as of January 2016. This system was implemented in a cloud-computing platform (StellaSpark Nexus) designed to process large geospatial data streams from a variety of sources and was tested for a number of parcels from the Haus Riswick experimental farm in Germany. Despite outliers due to fluctuating weather conditions, vegetation index time series suggested four distinct growing cycles per growing season. Established relationships between vegetation indices and grassland yield showed poor to moderate positive trends, implying that vegetation indices could be a potential predictor for grassland biomass and chlorophyll content. However, the inclusion of larger and additional datasets such as Sentinel-1 imagery could be beneficial to developing more robust prediction models and for automatic detection of mowing events for grasslands.
KW  - sen2grass
KW  - sentinel-2
KW  - stellaspark
KW  - nexus
KW  - grassland monitoring
KW  - time series
KW  - vegetation indices
KW  - cloud cover
DO  - 10.3390/agriengineering3010008
TY  - EJOU
AU  - Yavariabdi, Amir
AU  - Kusetogullari, Huseyin
AU  - Celik, Turgay
AU  - Cicek, Hasan
TI  - FastUAV-NET: A Multi-UAV Detection Algorithm for Embedded Platforms
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 6
SN  - 2079-9292

AB  - In this paper, a real-time deep learning-based framework for detecting and tracking Unmanned Aerial Vehicles (UAVs) in video streams captured by a fixed-wing UAV is proposed. The proposed framework consists of two steps, namely intra-frame multi-UAV detection and the inter-frame multi-UAV tracking. In the detection step, a new multi-scale UAV detection Convolutional Neural Network (CNN) architecture based on a shallow version of You Only Look Once version 3 (YOLOv3-tiny) widened by Inception blocks is designed to extract local and global features from input video streams. Here, the widened multi-UAV detection network architecture is termed as FastUAV-NET and aims to improve UAV detection accuracy while preserving computing time of one-step deep detection algorithms in the context of UAV-UAV tracking. To detect UAVs, the FastUAV-NET architecture uses five inception units and adopts a feature pyramid network to detect UAVs. To obtain a high frame rate, the proposed method is applied to every nth frame and then the detected UAVs are tracked in intermediate frames using scalable Kernel Correlation Filter algorithm. The results on the generated UAV-UAV dataset illustrate that the proposed framework obtains 0.7916 average precision with 29 FPS performance on Jetson-TX2. The results imply that the widening of CNN network is a much more effective way than increasing the depth of CNN and leading to a good trade-off between accurate detection and real-time performance. The FastUAV-NET model will be publicly available to the research community to further advance multi-UAV-UAV detection algorithms.
KW  - deep learning
KW  - CNN
KW  - detection and tracking
KW  - Unmanned Aerial Vehicle
KW  - UAVs pursuit-evasion
DO  - 10.3390/electronics10060724
TY  - EJOU
AU  - Hou, Yuewu
AU  - Liu, Zhaoying
AU  - Zhang, Ting
AU  - Li, Yujian
TI  - C-UNet: Complement UNet for Remote Sensing Road Extraction
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 6
SN  - 1424-8220

AB  - Roads are important mode of transportation, which are very convenient for people’s daily work and life. However, it is challenging to accuratly extract road information from a high-resolution remote sensing image. This paper presents a road extraction method for remote sensing images with a complement UNet (C-UNet). C-UNet contains four modules. Firstly, the standard UNet is used to roughly extract road information from remote sensing images, getting the first segmentation result; secondly, a fixed threshold is utilized to erase partial extracted information; thirdly, a multi-scale dense dilated convolution UNet (MD-UNet) is introduced to discover the complement road areas in the erased masks, obtaining the second segmentation result; and, finally, we fuse the extraction results of the first and the third modules, getting the final segmentation results. Experimental results on the Massachusetts Road dataset indicate that our C-UNet gets the higher results than the state-of-the-art methods, demonstrating its effectiveness.
KW  - UNet
KW  - complementary UNet
KW  - fixed threshold
KW  - dilated convolution
KW  - remote sensing
DO  - 10.3390/s21062153
TY  - EJOU
AU  - Akumu, Clement E.
AU  - Amadi, Eze O.
AU  - Dennis, Samuel
TI  - Application of Drone and WorldView-4 Satellite Data in Mapping and Monitoring Grazing Land Cover and Pasture Quality: Pre- and Post-Flooding
T2  - Land

PY  - 2021
VL  - 10
IS  - 3
SN  - 2073-445X

AB  - Frequent flooding worldwide, especially in grazing environments, requires mapping and monitoring grazing land cover and pasture quality to support land management. Although drones, satellite, and machine learning technologies can be used to map land cover and pasture quality, there have been limited applications in grazing land environments, especially monitoring land cover change and pasture quality pre- and post-flood events. The use of high spatial resolution drone and satellite data such as WorldView-4 can provide effective mapping and monitoring in grazing land environments. The aim of this study was to utilize high spatial resolution drone and WorldView-4 satellite data to map and monitor grazing land cover change and pasture quality pre-and post-flooding. The grazing land cover was mapped pre-flooding using WorldView-4 satellite data and post-flooding using real-time drone data. The machine learning Random Forest classification algorithm was used to delineate land cover types and the normalized difference vegetation index (NDVI) was used to monitor pasture quality. This study found a seven percent (7%) increase in pasture cover and a one hundred percent (100%) increase in pasture quality post-flooding. The drone and WorldView-4 satellite data were useful to detect grazing land cover change at a finer scale.
KW  - drone and satellite data
KW  - mapping grazing land cover change
KW  - flood event
DO  - 10.3390/land10030321
TY  - EJOU
AU  - Liu, Chang
AU  - Szirányi, Tamás
TI  - Real-Time Human Detection and Gesture Recognition for On-Board UAV Rescue
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 6
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) play an important role in numerous technical and scientific fields, especially in wilderness rescue. This paper carries out work on real-time UAV human detection and recognition of body and hand rescue gestures. We use body-featuring solutions to establish biometric communications, like yolo3-tiny for human detection. When the presence of a person is detected, the system will enter the gesture recognition phase, where the user and the drone can communicate briefly and effectively, avoiding the drawbacks of speech communication. A data-set of ten body rescue gestures (i.e., Kick, Punch, Squat, Stand, Attention, Cancel, Walk, Sit, Direction, and PhoneCall) has been created by a UAV on-board camera. The two most important gestures are the novel dynamic Attention and Cancel which represent the set and reset functions respectively. When the rescue gesture of the human body is recognized as Attention, the drone will gradually approach the user with a larger resolution for hand gesture recognition. The system achieves 99.80% accuracy on testing data in body gesture data-set and 94.71% accuracy on testing data in hand gesture data-set by using the deep learning method. Experiments conducted on real-time UAV cameras confirm our solution can achieve our expected UAV rescue purpose.
KW  - unmanned aerial vehicles (UAVs)
KW  - search and rescue (SAR)
KW  - UAV human communication
KW  - body gesture recognition
KW  - hand gesture recognition
KW  - neural networks
KW  - deep learning
DO  - 10.3390/s21062180
TY  - EJOU
AU  - Liu, Bi-Yuan
AU  - Chen, Huai-Xin
AU  - Huang, Zhou
AU  - Liu, Xing
AU  - Yang, Yun-Zhi
TI  - ZoomInNet: A Novel Small Object Detector in Drone Images with Cross-Scale Knowledge Distillation
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 6
SN  - 2072-4292

AB  - Drone-based object detection has been widely applied in ground object surveillance, urban patrol, and some other fields. However, the dramatic scale changes and complex backgrounds of drone images usually result in weak feature representation of small objects, which makes it challenging to achieve high-precision object detection. Aiming to improve small objects detection, this paper proposes a novel cross-scale knowledge distillation (CSKD) method, which enhances the features of small objects in a manner similar to image enlargement, so it is termed as ZoomInNet. First, based on an efficient feature pyramid network structure, the teacher and student network are trained with images in different scales to introduce the cross-scale feature. Then, the proposed layer adaption (LA) and feature level alignment (FA) mechanisms are applied to align the feature size of the two models. After that, the adaptive key distillation point (AKDP) algorithm is used to get the crucial positions in feature maps that need knowledge distillation. Finally, the position-aware L2 loss is used to measure the difference between feature maps from cross-scale models, realizing the cross-scale information compression in a single model. Experiments on the challenging Visdrone2018 dataset show that the proposed method draws on the advantages of the image pyramid methods, while avoids the large calculation of them and significantly improves the detection accuracy of small objects. Simultaneously, the comparison with mainstream methods proves that our method has the best performance in small object detection.
KW  - small object detection
KW  - drone image
KW  - image pyramid
KW  - feature enhancement
KW  - cross-scale knowledge distillation
DO  - 10.3390/rs13061198
TY  - EJOU
AU  - Park, Kyung H.
AU  - Park, Eunji
AU  - Kim, Huy K.
TI  - Unsupervised Fault Detection on Unmanned Aerial Vehicles: Encoding and Thresholding Approach
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 6
SN  - 1424-8220

AB  - Unmanned Aerial Vehicles are expected to create enormous benefits to society, but there are safety concerns in recognizing faults at the vehicle’s control component. Prior studies proposed various fault detection approaches leveraging heuristics-based rules and supervised learning-based models, but there were several drawbacks. The rule-based approaches required an engineer to update the rules on every type of fault, and the supervised learning-based approaches necessitated the acquisition of a finely-labeled training dataset. Moreover, both prior approaches commonly include a limit that the detection model can identify the trained type of faults only, but fail to recognize the unseen type of faults. In pursuit of resolving the aforementioned drawbacks, we proposed a fault detection model utilizing a stacked autoencoder that lies under unsupervised learning. The autoencoder was trained with data from safe UAV states, and its reconstruction loss was examined to distinguish the safe states and faulty states. The key contributions of our study are, as follows. First, we presented a series of analyses to extract essential features from raw UAV flight logs. Second, we designed a fault detection model consisting of the stacked autoencoder and the classifier. Lastly, we validated our approach’s fault detection performance with two datasets consisting of different types of UAV faults.
KW  - Unmanned Aerial Vehicle
KW  - fault detection
KW  - anomaly detection
KW  - unsupervised learning
KW  - autoencoder
DO  - 10.3390/s21062208
TY  - EJOU
AU  - Passafiume, Marco
AU  - Rojhani, Neda
AU  - Collodi, Giovanni
AU  - Cidronali, Alessandro
TI  - Modeling Small UAV Micro-Doppler Signature Using Millimeter-Wave FMCW Radar
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 6
SN  - 2079-9292

AB  - With the increase in small unmanned aerial vehicle (UAV) applications in several technology areas, detection and small UAVs classification have become of interest. To cope with small radar cross-sections (RCSs), slow-flying speeds, and low flying altitudes, the micro-Doppler signature provides some of the most distinctive information to identify and classify targets in many radar systems. In this paper, we introduce an effective model for the micro-Doppler effect that is suitable for frequency-modulated continuous-wave (FMCW) radar applications, and exploit it to investigate UAV signatures. The latter depends on the number of UAV motors, which are considered vibrational sources, and their rotation speed. To demonstrate the reliability of the proposed model, it is used to build simulated FMCW radar images, which are compared with experimental data acquired by a 77 GHz FMCW multiple-input multiple-output (MIMO) cost-effective automotive radar platform. The experimental results confirm the model’s ability to estimate the class of the UAV, namely its number of motors, in different operative scenarios. In addition, the experimental results show that the motors rotation speed does not imprint a significant signature on the classification of the UAV; thus, the estimation of the number of motors represents the only viable parameter for small UAV classification using the micro-Doppler effect.
KW  - UAV classification
KW  - feature extraction
KW  - micro-Doppler signature
KW  - FMCW radar
KW  - automotive radar
DO  - 10.3390/electronics10060747
TY  - EJOU
AU  - Fan, Pan
AU  - Lang, Guodong
AU  - Yan, Bin
AU  - Lei, Xiaoyan
AU  - Guo, Pengju
AU  - Liu, Zhijie
AU  - Yang, Fuzeng
TI  - A Method of Segmenting Apples Based on Gray-Centered RGB Color Space
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 6
SN  - 2072-4292

AB  - In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. The rapid and accurate identification of apple targets in an illuminated and unstructured natural orchard is still a key challenge for the picking robot’s vision system. In this paper, by combining local image features and color information, we propose a pixel patch segmentation method based on gray-centered red–green–blue (RGB) color space to address this issue. Different from the existing methods, this method presents a novel color feature selection method that accounts for the influence of illumination and shadow in apple images. By exploring both color features and local variation in apple images, the proposed method could effectively distinguish the apple fruit pixels from other pixels. Compared with the classical segmentation methods and conventional clustering algorithms as well as the popular deep-learning segmentation algorithms, the proposed method can segment apple images more accurately and effectively. The proposed method was tested on 180 apple images. It offered an average accuracy rate of 99.26%, recall rate of 98.69%, false positive rate of 0.06%, and false negative rate of 1.44%. Experimental results demonstrate the outstanding performance of the proposed method.
KW  - fruit detection
KW  - fruit segmentation
KW  - color space
KW  - segmentation algorithm
DO  - 10.3390/rs13061211
TY  - EJOU
AU  - Li, Ke
AU  - Zhang, Kun
AU  - Zhang, Zhenchong
AU  - Liu, Zekun
AU  - Hua, Shuai
AU  - He, Jianliang
TI  - A UAV Maneuver Decision-Making Algorithm for Autonomous Airdrop Based on Deep Reinforcement Learning
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 6
SN  - 1424-8220

AB  - How to operate an unmanned aerial vehicle (UAV) safely and efficiently in an interactive environment is challenging. A large amount of research has been devoted to improve the intelligence of a UAV while performing a mission, where finding an optimal maneuver decision-making policy of the UAV has become one of the key issues when we attempt to enable the UAV autonomy. In this paper, we propose a maneuver decision-making algorithm based on deep reinforcement learning, which generates efficient maneuvers for a UAV agent to execute the airdrop mission autonomously in an interactive environment. Particularly, the training set of the learning algorithm by the Prioritized Experience Replay is constructed, that can accelerate the convergence speed of decision network training in the algorithm. It is shown that a desirable and effective maneuver decision-making policy can be found by extensive experimental results.
KW  - UAV
KW  - maneuver decision-making
KW  - autonomous airdrop
KW  - deep reinforcement learning
KW  - prioritized experience replay
DO  - 10.3390/s21062233
TY  - EJOU
AU  - Kaivosoja, Jere
AU  - Hautsalo, Juho
AU  - Heikkinen, Jaakko
AU  - Hiltunen, Lea
AU  - Ruuttunen, Pentti
AU  - Näsi, Roope
AU  - Niemeläinen, Oiva
AU  - Lemsalu, Madis
AU  - Honkavaara, Eija
AU  - Salonen, Jukka
TI  - Reference Measurements in Developing UAV Systems for Detecting Pests, Weeds, and Diseases
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 7
SN  - 2072-4292

AB  - The development of UAV (unmanned aerial vehicle) imaging technologies for precision farming applications is rapid, and new studies are published frequently. In cases where measurements are based on aerial imaging, there is the need to have ground truth or reference data in order to develop reliable applications. However, in several precision farming use cases such as pests, weeds, and diseases detection, the reference data can be subjective or relatively difficult to capture. Furthermore, the collection of reference data is usually laborious and time consuming. It also appears that it is difficult to develop generalisable solutions for these areas. This review studies previous research related to pests, weeds, and diseases detection and mapping using UAV imaging in the precision farming context, underpinning the applied reference measurement techniques. The majority of the reviewed studies utilised subjective visual observations of UAV images, and only a few applied in situ measurements. The conclusion of the review is that there is a lack of quantitative and repeatable reference data measurement solutions in the areas of mapping pests, weeds, and diseases. In addition, the results that the studies present should be reflected in the applied references. An option in the future approach could be the use of synthetic data as reference.
KW  - UAS
KW  - drone
KW  - unmanned aerial vehicle
KW  - in situ
KW  - reference data
KW  - ground truth
KW  - monitoring
KW  - precision agriculture
KW  - smart farming
DO  - 10.3390/rs13071238
TY  - EJOU
AU  - Oliveira, Luiz F. P.
AU  - Moreira, António P.
AU  - Silva, Manuel F.
TI  - Advances in Agriculture Robotics: A State-of-the-Art Review and Challenges Ahead
T2  - Robotics

PY  - 2021
VL  - 10
IS  - 2
SN  - 2218-6581

AB  - The constant advances in agricultural robotics aim to overcome the challenges imposed by population growth, accelerated urbanization, high competitiveness of high-quality products, environmental preservation and a lack of qualified labor. In this sense, this review paper surveys the main existing applications of agricultural robotic systems for the execution of land preparation before planting, sowing, planting, plant treatment, harvesting, yield estimation and phenotyping. In general, all robots were evaluated according to the following criteria: its locomotion system, what is the final application, if it has sensors, robotic arm and/or computer vision algorithm, what is its development stage and which country and continent they belong. After evaluating all similar characteristics, to expose the research trends, common pitfalls and the characteristics that hinder commercial development, and discover which countries are investing into Research and Development (R&amp;D) in these technologies for the future, four major areas that need future research work for enhancing the state of the art in smart agriculture were highlighted: locomotion systems, sensors, computer vision algorithms and communication technologies. The results of this research suggest that the investment in agricultural robotic systems allows to achieve short—harvest monitoring—and long-term objectives—yield estimation.
KW  - agricultural robots
KW  - agriculture 4.0
KW  - precision agriculture
DO  - 10.3390/robotics10020052
TY  - EJOU
AU  - Liu, Chuanyang
AU  - Wu, Yiquan
AU  - Liu, Jingjing
AU  - Sun, Zuo
TI  - Improved YOLOv3 Network for Insulator Detection in Aerial Images with Diverse Background Interference
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 7
SN  - 2079-9292

AB  - Automatic inspection of insulators from high-voltage transmission lines is of paramount importance to the safety and reliable operation of the power grid. Due to different size insulators and the complex background of aerial images, it is a difficult task to recognize insulators in aerial views. Most of the traditional image processing methods and machine learning methods cannot achieve sufficient performance for insulator detection when diverse background interference is present. In this study, a deep learning method—based on You Only Look Once (YOLO)—will be proposed, capable of detecting insulators from aerial images with complex backgrounds. Firstly, aerial images with common aerial scenes were collected by Unmanned Aerial Vehicle (UAV), and a novel insulator dataset was constructed. Secondly, to enhance feature reuse and propagation, on the basis of YOLOv3 and Dense-Blocks, the YOLOv3-dense network was utilized for insulator detection. To improve detection accuracy for different sized insulators, a structure of multiscale feature fusion was adapted to the YOLOv3-dense network. To obtain abundant semantic information of upper and lower layers, multilevel feature mapping modules were employed across the YOLOv3-dense network. Finally, the YOLOv3-dense network and compared networks were trained and tested on the testing set. The average precision of YOLOv3-dense, YOLOv3, and YOLOv2 were 94.47%, 90.31%, and 83.43%, respectively. Experimental results and analysis validate the claim that the proposed YOLOv3-dense network achieves good performance in the detection of different size insulators amid diverse background interference.
KW  - aerial image
KW  - insulator detection
KW  - YOLO
KW  - background interference
KW  - image processing
KW  - deep learning
KW  - Dense-Block
DO  - 10.3390/electronics10070771
TY  - EJOU
AU  - He, Shaokun
AU  - Gu, Lei
AU  - Tian, Jing
AU  - Deng, Lele
AU  - Yin, Jiabo
AU  - Liao, Zhen
AU  - Zeng, Ziyue
AU  - Shen, Youjiang
AU  - Hui, Yu
TI  - Machine Learning Improvement of Streamflow Simulation by Utilizing Remote Sensing Data and Potential Application in Guiding Reservoir Operation
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 7
SN  - 2071-1050

AB  - Hydro-meteorological datasets are key components for understanding physical hydrological processes, but the scarcity of observational data hinders their potential application in poorly gauged regions. Satellite-retrieved and atmospheric reanalysis products exhibit considerable advantages in filling the spatial gaps in in-situ gauging networks and are thus forced to drive the physically lumped hydrological models for long-term streamflow simulation in data-sparse regions. As machine learning (ML)-based techniques can capture the relationship between different elements, they may have potential in further exploring meteorological predictors and hydrological responses. To examine the application prospects of a physically constrained ML algorithm using earth observation data, we used a short-series hydrological observation of the Hanjiang River basin in China as a case study. In this study, the prevalent modèle du Génie Rural à 9 paramètres Journalier (GR4J-9) hydrological model was used to initially simulate streamflow, and then, the simulated series and remote sensing data were used to train the long short-term memory (LSTM) method. The results demonstrated that the advanced GR4J9–LSTM model chain effectively improves the performance of the streamflow simulation by using more remote sensing data related to the hydrological response variables. Additionally, we derived a reservoir operation model by feeding the LSTM-based simulation outputs, which further revealed the potential application of our proposed technique.
KW  - ungauged basin
KW  - machine learning
KW  - streamflow simulation
KW  - satellite precipitation
KW  - atmospheric reanalysis
DO  - 10.3390/su13073645
TY  - EJOU
AU  - Peinado, Jairo
AU  - Jiao-Wang, Liu
AU  - Olmedo, Álvaro
AU  - Santiuste, Carlos
TI  - Use of Artificial Neural Networks to Optimize Stacking Sequence in UHMWPE Protections
T2  - Polymers

PY  - 2021
VL  - 13
IS  - 7
SN  - 2073-4360

AB  - The aim of the present work is to provide a methodology to evaluate the influence of stacking sequence on the ballistic performance of ultra-high molecular weight polyethylene (UHMWPE) protections. The proposed methodology is based on the combination of experimental tests, numerical modelling, and Artificial Neural Networks (ANN). High-velocity impact experimental tests were conducted to validate the numerical model. The validated Finite Element Method (FEM) model was used to provide data to train and to validate the ANN. Finally, the ANN was used to find the best stacking sequence combining layers of three UHMWPE materials with different qualities. The results showed that the three UHMWPE materials can be properly combined to provide a solution with a better ballistic performance than using only the material with highest quality. These results imply that costs can be reduced increasing the ballistic limit of the UHMWPE protections. When the weight ratios of the three materials remain constant, the optimal results occur when the highest-performance material is placed in the back face. Furthermore, ANN simulation showed that the optimal results occur when the weight ratio of the highest-performance material is 79.2%.
KW  - UHMWPE
KW  - impact
KW  - FEM
KW  - neural networks
DO  - 10.3390/polym13071012
TY  - EJOU
AU  - López-Andreu, Francisco J.
AU  - Erena, Manuel
AU  - Dominguez-Gómez, Jose A.
AU  - López-Morales, Juan A.
TI  - Sentinel-2 Images and Machine Learning as Tool for Monitoring of the Common Agricultural Policy: Calasparra Rice as a Case Study
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 4
SN  - 2073-4395

AB  - The European Commission introduces the Control by Monitoring through new technologies to manage Common Agricultural Policy funds through the Regulation 2018/746. The advances in remote sensing have been considered one of these new technologies, mainly since the European Space Agency designed the Copernicus Programme. The Sentinel-1 (radar range) and Sentinel-2 (optical range) satellites have been designed for monitoring agricultural problems based on the characteristics they provide. The data provided by the Sentinel 2 missions, together with the emergence of different scientific disciplines in artificial intelligence —especially machine learning— offer the perfect basis for identifying and classifying any crop and its phenological state. Our research is based on developing and evaluating a pixel-based supervised classification scheme to produce accurate rice crop mapping in a smallholder agricultural zone in Calasparra, Murcia, Spain. Several models are considered to obtain the most suitable model for each element of the time series used; pixel-based classification is performed and finished with a statistical treatment. The highly accurate results obtained, especially across the most significant vegetative development dates, indicate the benefits of using Sentinel-2 data combined with Machine Learning techniques to identify rice crops. It should be noted that it was possible to locate rice crop areas with an overall accuracy of 94% and standard deviation of 1%, which could be increased to 96% (±1%) if we focus on the months of the crop’s highest development state. Thanks to the proposed methodology, the on-site inspections carried out, 5% of the files, have been replaced by remote sensing evaluations of 100% of the analyzed season files. Besides, by adjusting the model input data, it is possible to detect unproductive or abandoned plots.
KW  - multispectral remote sensing
KW  - Copernicus
KW  - sentinel
KW  - image processing
KW  - machine learning
KW  - agriculture
KW  - land cover
KW  - rice crop
KW  - common agricultural policy
DO  - 10.3390/agronomy11040621
TY  - EJOU
AU  - Sánchez, José D.
AU  - Urquiza-Aguiar, Luis
AU  - Paredes Paredes, Martha C.
TI  - Fading Channel Models for mm-Wave Communications
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 7
SN  - 2079-9292

AB  - A realistic performance assessment of any wireless communication system requires the use of a fading channel model that reflects its main characteristics. The traditional Rayleigh and Nakagami-m models have been (and still are) the basis of most theoretical research on wireless technologies today, even for emerging technologies, such as millimeter-wave communications (mm-Wave). In this article, we show that the fluctuating multiple-ray (FMR) and κ-μ shadowed models had a better fit (i.e., lowest mean square error statistical test) to field measurements in outdoor environments at 28 GHz than the conventional channel models. Therefore, these generalized models are feasible alternatives that can be used as a benchmark when evaluating communication performance in mm-Wave scenarios.
KW  - generalized fading channels
KW  - mm-Wave
KW  - κ-μ shadowed
KW  - fluctuating multiple-ray model
DO  - 10.3390/electronics10070798
TY  - EJOU
AU  - Xu, Danqing
AU  - Wu, Yiquan
TI  - FE-YOLO: A Feature Enhancement Network for Remote Sensing Target Detection
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 7
SN  - 2072-4292

AB  - In the past few decades, target detection from remote sensing images gained from aircraft or satellites has become one of the hottest topics. However, the existing algorithms are still limited by the detection of small remote sensing targets. Benefiting from the great development of computing power, deep learning has also made great breakthroughs. Due to a large number of small targets and complexity of background, the task of remote sensing target detection is still a challenge. In this work, we establish a series of feature enhancement modules for the network based on YOLO (You Only Look Once) -V3 to improve the performance of feature extraction. Therefore, we term our proposed network as FE-YOLO. In addition, to realize fast detection, the original Darknet-53 was simplified. Experimental results on remote sensing datasets show that our proposed FE-YOLO performs better than other state-of-the-art target detection models.
KW  - target detection
KW  - remote sensing images
KW  - YOLO-V3
KW  - feature enhancement
KW  - deep learning
DO  - 10.3390/rs13071311
TY  - EJOU
AU  - Ammar, Adel
AU  - Koubaa, Anis
AU  - Ahmed, Mohanned
AU  - Saad, Abdulrahman
AU  - Benjdira, Bilel
TI  - Vehicle Detection from Aerial Images Using Deep Learning: A Comparative Study
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 7
SN  - 2079-9292

AB  - This paper addresses the problem of car detection from aerial images using Convolutional Neural Networks (CNNs). This problem presents additional challenges as compared to car (or any object) detection from ground images because the features of vehicles from aerial images are more difficult to discern. To investigate this issue, we assess the performance of three state-of-the-art CNN algorithms, namely Faster R-CNN, which is the most popular region-based algorithm, as well as YOLOv3 and YOLOv4, which are known to be the fastest detection algorithms. We analyze two datasets with different characteristics to check the impact of various factors, such as the UAV’s (unmanned aerial vehicle) altitude, camera resolution, and object size. A total of 52 training experiments were conducted to account for the effect of different hyperparameter values. The objective of this work is to conduct the most robust and exhaustive comparison between these three cutting-edge algorithms on the specific domain of aerial images. By using a variety of metrics, we show that the difference between YOLOv4 and YOLOv3 on the two datasets is statistically insignificant in terms of Average Precision (AP) (contrary to what was obtained on the COCO dataset). However, both of them yield markedly better performance than Faster R-CNN in most configurations. The only exception is that both of them exhibit a lower recall when object sizes and scales in the testing dataset differ largely from those in the training dataset.
KW  - car detection
KW  - convolutional neural networks
KW  - deep learning
KW  - Faster R-CNN
KW  - unmanned aerial vehicles
KW  - YOLOv3
KW  - YOLOv4
DO  - 10.3390/electronics10070820
TY  - EJOU
AU  - Mehmood, Yasir
AU  - Aslam, Jawad
AU  - Ullah, Nasim
AU  - Chowdhury, Md. S.
AU  - Techato, Kuaanan
AU  - Alzaed, Ali N.
TI  - Adaptive Robust Trajectory Tracking Control of Multiple Quad-Rotor UAVs with Parametric Uncertainties and Disturbances
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 7
SN  - 1424-8220

AB  - Recently, formation flying of multiple unmanned aerial vehicles (UAVs) found numerous applications in various areas such as surveillance, industrial automation and disaster management. The accuracy and reliability for performing group tasks by multiple UAVs is highly dependent on the applied control strategy. The formation and trajectories of multiple UAVs are governed by two separate controllers, namely formation and trajectory tracking controllers respectively. In presence of environmental effects, disturbances due to wind and parametric uncertainties, the controller design process is a challenging task. This article proposes a robust adaptive formation and trajectory tacking control of multiple quad-rotor UAVs using super twisting sliding mode control method. In the proposed design, Lyapunov function-based adaptive disturbance estimators are used to compensate for the effects of external disturbances and parametric uncertainties. The stability of the proposed controllers is guaranteed using Lyapunov theorems. Two variants of the control schemes, namely fixed gain super twisting SMC (STSMC) and adaptive super twisting SMC (ASTSMC) are tested using numerical simulations performed in MATLAB/Simulink. From the results presented, it is verified that in presence of disturbances, the proposed ASTSMC controller exhibits enhanced robustness as compared to the fixed gain STSMC.
KW  - quad-rotor control
KW  - adaptive robust control
KW  - super twisting sliding mode control
KW  - formation control
DO  - 10.3390/s21072401
TY  - EJOU
AU  - Al-Darraji, Izzat
AU  - Piromalis, Dimitrios
AU  - Kakei, Ayad A.
AU  - Khan, Fazal Q.
AU  - Stojmenovic, Milos
AU  - Tsaramirsis, Georgios
AU  - Papageorgas, Panagiotis G.
TI  - Adaptive Robust Controller Design-Based RBF Neural Network for Aerial Robot Arm Model
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 7
SN  - 2079-9292

AB  - Aerial Robot Arms (ARAs) enable aerial drones to interact and influence objects in various environments. Traditional ARA controllers need the availability of a high-precision model to avoid high control chattering. Furthermore, in practical applications of aerial object manipulation, the payloads that ARAs can handle vary, depending on the nature of the task. The high uncertainties due to modeling errors and an unknown payload are inversely proportional to the stability of ARAs. To address the issue of stability, a new adaptive robust controller, based on the Radial Basis Function (RBF) neural network, is proposed. A three-tier approach is also followed. Firstly, a detailed new model for the ARA is derived using the Lagrange–d’Alembert principle. Secondly, an adaptive robust controller, based on a sliding mode, is designed to manipulate the problem of uncertainties, including modeling errors. Last, a higher stability controller, based on the RBF neural network, is implemented with the adaptive robust controller to stabilize the ARAs, avoiding modeling errors and unknown payload issues. The novelty of the proposed design is that it takes into account high nonlinearities, coupling control loops, high modeling errors, and disturbances due to payloads and environmental conditions. The model was evaluated by the simulation of a case study that includes the two proposed controllers and ARA trajectory tracking. The simulation results show the validation and notability of the presented control algorithm.
KW  - aerial robot arms
KW  - modeling of aerial robot arms
KW  - adaptive controller
KW  - robust controller
KW  - sliding mode controller
KW  - RBF neural network
KW  - stability of aerial robot arms
KW  - trajectory tracking
KW  - high modeling errors
DO  - 10.3390/electronics10070831
TY  - EJOU
AU  - Tokunaga, Shinya
AU  - Premachandra, Chinthaka
AU  - Premachandra, H. Waruna H.
AU  - Kawanaka, Hiroharu
AU  - Sumathipala, Sagara
AU  - Sudantha, B. S.
TI  - Autonomous Spiral Motion by a Small-Type Robot on an Obstacle-Available Surface
T2  - Micromachines

PY  - 2021
VL  - 12
IS  - 4
SN  - 2072-666X

AB  - Several robot-related studies have been conducted in recent years; however, studies on the autonomous travel of small mobile robots in small spaces are lacking. In this study, we investigate the development of autonomous travel for small robots that need to travel and cover the entire smooth surface, such as those employed for cleaning tables or solar panels. We consider an obstacle-available surface and target this travel on it by proposing a spiral motion method. To achieve the spiral motion, we focus on developing autonomous avoidance of obstacles, return to original path, and fall prevention when robots traverse a surface. The development of regular travel by a robot without an encoder is an important feature of this study. The traveled distance was measured using the traveling time. We achieved spiral motion by analyzing the data from multiple small sensors installed on the robot by introducing a new attitude-control method, and we ensured that the robot returned to the original spiral path autonomously after avoiding obstacles and without falling over the edge of the surface.
KW  - spiral robot motion
KW  - fall prevention
KW  - obstacle avoidance
KW  - sensor fusion
KW  - motion analysis
DO  - 10.3390/mi12040375
TY  - EJOU
AU  - Wang, Fei
AU  - Liu, Zhendong
AU  - Zhu, Hongchun
AU  - Wu, Pengda
AU  - Li, Chengming
TI  - An Improved Method for Stable Feature Points Selection in Structure-from-Motion Considering Image Semantic and Structural Characteristics
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 7
SN  - 1424-8220

AB  - Feature matching plays a crucial role in the process of 3D reconstruction based on the structure from motion (SfM) technique. For a large collection of oblique images, feature matching is one of the most time-consuming steps, and the matching result directly affects the accuracy of subsequent tasks. Therefore, how to extract the reasonable feature points robustly and efficiently to improve the matching speed and quality has received extensive attention from scholars worldwide. Most studies perform quantitative feature point selection based on image Difference-of-Gaussian (DoG) pyramids in practice. However, the stability and spatial distribution of feature points are not considered enough, resulting in selected feature points that may not adequately reflect the scene structures and cannot guarantee the matching rate and the aerial triangulation accuracy. To address these issues, an improved method for stable feature point selection in SfM considering image semantic and structural characteristics is proposed. First, the visible-band difference vegetation index is used to identify the vegetation areas from oblique images, and the line feature in the image is extracted by the optimized line segment detector algorithm. Second, the feature point two-tuple classification model is established, in which the vegetation area recognition result is used as the semantic constraint, the line feature extraction result is used as the structural constraint, and the feature points are divided into three types. Finally, a progressive selection algorithm for feature points is proposed, in which feature points in the DoG pyramid are selected by classes and levels until the number of feature points is satisfied. Oblique images of a 40-km2 area in Dongying city, China, were used for validation. The experimental results show that compared to the state-of-the-art method, the method proposed in this paper not only effectively reduces the number of feature points but also better reflects the scene structure. At the same time, the average reprojection error of the aerial triangulation decrease by 20%, the feature point matching rate increase by 3%, the selected feature points are more stable and reasonable.
KW  - 3D reconstruction
KW  - oblique images
KW  - feature point selection
KW  - image semantic and structural characteristics
KW  - two-tuple classification model
DO  - 10.3390/s21072416
TY  - EJOU
AU  - Lerro, Angelo
AU  - Battipede, Manuela
TI  - Safety Analysis of a Certifiable Air Data System Based on Synthetic Sensors for Flow Angle Estimation
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 7
SN  - 2076-3417

AB  - This work deals with the safety analysis of an air data system (ADS) partially based on synthetic sensors. The ADS is designed for the small aircraft transportation (SAT) community and is suitable for future unmanned aerial vehicles and urban air mobility applications. The ADS’s main innovation is based on estimation of the flow angles (angle-of-attack and angle-of-sideslip) using synthetic sensors instead of classical vanes (or sensors), whereas pressure and temperature are directly measured with Pitot and temperature probes. As the air data system is a safety-critical system, safety analyses are performed and the results are compared with the safety objectives required by the aircraft integrator. The present paper introduces the common aeronautical procedures for system safety assessment applied to a safety critical system partially based on synthetic sensors. The mean time between failures of ADS’s sub-parts are estimated on a statistical basis in order to evaluate the failure rate of the ADS’s functions. The proposed safety analysis is also useful in identifying the most critical air data system parts and sub-parts. Possible technological gaps to be filled to achieve the airworthiness safety objectives with nonredundant architectures are also identified.
KW  - angle-of-attack
KW  - flow angle
KW  - air data system
KW  - synthetic sensor
KW  - analytical redundancy
KW  - avionics
KW  - neural network
DO  - 10.3390/app11073127
TY  - EJOU
AU  - Araújo, Sara O.
AU  - Peres, Ricardo S.
AU  - Barata, José
AU  - Lidon, Fernando
AU  - Ramalho, José C.
TI  - Characterising the Agriculture 4.0 Landscape—Emerging Trends, Challenges and Opportunities
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 4
SN  - 2073-4395

AB  - Investment in technological research is imperative to stimulate the development of sustainable solutions for the agricultural sector. Advances in Internet of Things, sensors and sensor networks, robotics, artificial intelligence, big data, cloud computing, etc. foster the transition towards the Agriculture 4.0 era. This fourth revolution is currently seen as a possible solution for improving agricultural growth, ensuring the future needs of the global population in a fair, resilient and sustainable way. In this context, this article aims at characterising the current Agriculture 4.0 landscape. Emerging trends were compiled using a semi-automated process by analysing relevant scientific publications published in the past ten years. Subsequently, a literature review focusing these trends was conducted, with a particular emphasis on their applications in real environments. From the results of the study, some challenges are discussed, as well as opportunities for future research. Finally, a high-level cloud-based IoT architecture is presented, serving as foundation for designing future smart agricultural systems. It is expected that this work will positively impact the research around Agriculture 4.0 systems, providing a clear characterisation of the concept along with guidelines to assist the actors in a successful transition towards the digitalisation of the sector.
KW  - Agriculture 4.0
KW  - artificial intelligence
KW  - cloud computing
KW  - decision support system
KW  - internet of things
KW  - robotics
KW  - sensors
DO  - 10.3390/agronomy11040667
TY  - EJOU
AU  - Appeltans, Simon
AU  - Pieters, Jan G.
AU  - Mouazen, Abdul M.
TI  - Detection of Leek Rust Disease under Field Conditions Using Hyperspectral Proximal Sensing and Machine Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 7
SN  - 2072-4292

AB  - Rust disease is an important problem for leek cultivation worldwide. It reduces market value and in extreme cases destroys the entire harvest. Farmers have to resort to periodical full-field fungicide applications to prevent the spread of disease, once every 1 to 5 weeks, depending on the cultivar and weather conditions. This implies an economic cost for the farmer and an environmental cost for society. Hyperspectral sensors have been extensively used to address this issue in research, but their application in the field has been limited to a relatively low number of crops, excluding leek, due to the high investment costs and complex data gathering and analysis associated with these sensors. To fill this gap, a methodology was developed for detecting leek rust disease using hyperspectral proximal sensing data combined with supervised machine learning. First, a hyperspectral library was constructed containing 43,416 spectra with a waveband range of 400–1000 nm, measured under field conditions. Then, an extensive evaluation of 11 common classifiers was performed using the scikit-learn machine learning library in Python, combined with a variety of wavelength selection techniques and preprocessing strategies. The best performing model was a (linear) logistic regression model that was able to correctly classify rust disease with an accuracy of 98.14%, using reflectance values at 556 and 661 nm, combined with the value of the first derivative at 511 nm. This model was used to classify unlabelled hyperspectral images, confirming that the model was able to accurately classify leek rust disease symptoms. It can be concluded that the results in this work are an important step towards the mapping of leek rust disease, and that future research is needed to overcome certain challenges before variable rate fungicide applications can be adopted against leek rust disease.
KW  - hyperspectral
KW  - proximal sensing
KW  - disease detection
KW  - leek
KW  - rust
KW  - machine learning
DO  - 10.3390/rs13071341
TY  - EJOU
AU  - Vélez-Nicolás, Mercedes
AU  - García-López, Santiago
AU  - Barbero, Luis
AU  - Ruiz-Ortiz, Verónica
AU  - Sánchez-Bellón, Ángel
TI  - Applications of Unmanned Aerial Systems (UASs) in Hydrology: A Review
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 7
SN  - 2072-4292

AB  - In less than two decades, UASs (unmanned aerial systems) have revolutionized the field of hydrology, bridging the gap between traditional satellite observations and ground-based measurements and allowing the limitations of manned aircraft to be overcome. With unparalleled spatial and temporal resolutions and product-tailoring possibilities, UAS are contributing to the acquisition of large volumes of data on water bodies, submerged parameters and their interactions in different hydrological contexts and in inaccessible or hazardous locations. This paper provides a comprehensive review of 122 works on the applications of UASs in surface water and groundwater research with a purpose-oriented approach. Concretely, the review addresses: (i) the current applications of UAS in surface and groundwater studies, (ii) the type of platforms and sensors mainly used in these tasks, (iii) types of products generated from UAS-borne data, (iv) the associated advantages and limitations, and (v) knowledge gaps and future prospects of UASs application in hydrology. The first aim of this review is to serve as a reference or introductory document for all researchers and water managers who are interested in embracing this novel technology. The second aim is to unify in a single document all the possibilities, potential approaches and results obtained by different authors through the implementation of UASs.
KW  - drone applications
KW  - surface water
KW  - groundwater
KW  - photogrammetry
KW  - optical sensing
KW  - thermal infrared
DO  - 10.3390/rs13071359
TY  - EJOU
AU  - Park, Jonghyuk
AU  - Park, Jonghun
AU  - Shin, Dongmin
AU  - Choi, Yerim
TI  - A BCI Based Alerting System for Attention Recovery of UAV Operators
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 7
SN  - 1424-8220

AB  - As unmanned aerial vehicles have become popular, the number of accidents caused by an operator’s inattention have increased. To prevent such accidents, the operator should maintain an attention status. However, limited research has been conducted on the brain-computer interface (BCI)-based system with an alerting module for the operator’s attention recovery of unmanned aerial vehicles. Therefore, we introduce a detection and alerting system that prevents an unmanned aerial vehicle operator from falling into inattention status by using the operator’s electroencephalogram signal. The proposed system consists of the following three components: a signal processing module, which collects and preprocesses an electroencephalogram signal of an operator, an inattention detection module, which determines whether an inattention status occurred based on the preprocessed signal, and, lastly, an alert providing module that presents stimulus to an operator when inattention is detected. As a result of evaluating the performance with a real-world dataset, it was shown that the proposed system successfully contributed to the recovery of operator attention in the evaluating dataset, although statistical significance could not be established due to the small number of subjects.
KW  - brain computer interaction
KW  - unmanned aerial vehicle
KW  - EEG-signal
KW  - attention recovery
KW  - alerting system
KW  - graphical user interface
DO  - 10.3390/s21072447
TY  - EJOU
AU  - Wang, Junshu
AU  - Yang, Yue
AU  - Chen, Yuan
AU  - Han, Yuxing
TI  - LighterGAN: An Illumination Enhancement Method for Urban UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 7
SN  - 2072-4292

AB  - In unmanned aerial vehicle based urban observation and monitoring, the performance of computer vision algorithms is inevitably limited by the low illumination and light pollution caused degradation, therefore, the application image enhancement is a considerable prerequisite for the performance of subsequent image processing algorithms. Therefore, we proposed a deep learning and generative adversarial network based model for UAV low illumination image enhancement, named LighterGAN. The design of LighterGAN refers to the CycleGAN model with two improvements—attention mechanism and semantic consistency loss—having been proposed to the original structure. Additionally, an unpaired dataset that was captured by urban UAV aerial photography has been used to train this unsupervised learning model. Furthermore, in order to explore the advantages of the improvements, both the performance in the illumination enhancement task and the generalization ability improvement of LighterGAN were proven in the comparative experiments combining subjective and objective evaluations. In the experiments with five cutting edge image enhancement algorithms, in the test set, LighterGAN achieved the best results in both visual perception and PIQE (perception based image quality evaluator, a MATLAB build-in function, the lower the score, the higher the image quality) score of enhanced images, scores were 4.91 and 11.75 respectively, better than EnlightenGAN the state-of-the-art. In the enhancement of low illumination sub-dataset Y (containing 2000 images), LighterGAN also achieved the lowest PIQE score of 12.37, 2.85 points lower than second place. Moreover, compared with the CycleGAN, the improvement of generalization ability was also demonstrated. In the test set generated images, LighterGAN was 6.66 percent higher than CycleGAN in subjective authenticity assessment and 3.84 lower in PIQE score, meanwhile, in the whole dataset generated images, the PIQE score of LighterGAN is 11.67, 4.86 lower than CycleGAN.
KW  - UAV
KW  - unsupervised learning
KW  - LighterGAN
KW  - unpaired dataset
KW  - illumination enhancement
KW  - attention mechanism
KW  - semantic consistency loss
KW  - PIQE
KW  - generalization ability
DO  - 10.3390/rs13071371
TY  - EJOU
AU  - Fernandez-Beltran, Ruben
AU  - Baidar, Tina
AU  - Kang, Jian
AU  - Pla, Filiberto
TI  - Rice-Yield Prediction with Multi-Temporal Sentinel-2 Data and 3D CNN: A Case Study in Nepal
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 7
SN  - 2072-4292

AB  - Crop yield estimation is a major issue of crop monitoring which remains particularly challenging in developing countries due to the problem of timely and adequate data availability. Whereas traditional agricultural systems mainly rely on scarce ground-survey data, freely available multi-temporal and multi-spectral remote sensing images are excellent tools to support these vulnerable systems by accurately monitoring and estimating crop yields before harvest. In this context, we introduce the use of Sentinel-2 (S2) imagery, with a medium spatial, spectral and temporal resolutions, to estimate rice crop yields in Nepal as a case study. Firstly, we build a new large-scale rice crop database (RicePAL) composed by multi-temporal S2 and climate/soil data from the Terai districts of Nepal. Secondly, we propose a novel 3D Convolutional Neural Network (CNN) adapted to these intrinsic data constraints for the accurate rice crop yield estimation. Thirdly, we study the effect of considering different temporal, climate and soil data configurations in terms of the performance achieved by the proposed approach and several state-of-the-art regression and CNN-based yield estimation methods. The extensive experiments conducted in this work demonstrate the suitability of the proposed CNN-based framework for rice crop yield estimation in the developing country of Nepal using S2 data.
KW  - Sentinel-2
KW  - rice-yield estimation
KW  - regression
KW  - deep learning
KW  - Nepal
DO  - 10.3390/rs13071391
TY  - EJOU
AU  - Doukhi, Oualid
AU  - Lee, Deok-Jin
TI  - Deep Reinforcement Learning for End-to-End Local Motion Planning of Autonomous Aerial Robots in Unknown Outdoor Environments: Real-Time Flight Experiments
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 7
SN  - 1424-8220

AB  - Autonomous navigation and collision avoidance missions represent a significant challenge for robotics systems as they generally operate in dynamic environments that require a high level of autonomy and flexible decision-making capabilities. This challenge becomes more applicable in micro aerial vehicles (MAVs) due to their limited size and computational power. This paper presents a novel approach for enabling a micro aerial vehicle system equipped with a laser range finder to autonomously navigate among obstacles and achieve a user-specified goal location in a GPS-denied environment, without the need for mapping or path planning. The proposed system uses an actor–critic-based reinforcement learning technique to train the aerial robot in a Gazebo simulator to perform a point-goal navigation task by directly mapping the noisy MAV’s state and laser scan measurements to continuous motion control. The obtained policy can perform collision-free flight in the real world while being trained entirely on a 3D simulator. Intensive simulations and real-time experiments were conducted and compared with a nonlinear model predictive control technique to show the generalization capabilities to new unseen environments, and robustness against localization noise. The obtained results demonstrate our system’s effectiveness in flying safely and reaching the desired points by planning smooth forward linear velocity and heading rates.
KW  - autonomous navigation
KW  - collision-free
KW  - deep reinforcement learning
KW  - unmanned aerial vehicle
DO  - 10.3390/s21072534
TY  - EJOU
AU  - Speranza, Nicholas A.
AU  - Rave, Christopher J.
AU  - Pei, Yong
TI  - Energy-Efficient On-Platform Target Classification for Electric Air Transportation Systems
T2  - Electricity

PY  - 2021
VL  - 2
IS  - 2
SN  - 2673-4826

AB  - Due to the predicted rise of Unmanned Aircraft Systems (UAS) in commercial, civil, and military operations, there is a desire to make UASs more energy efficient so they can proliferate with ease of deployment and maximal life per charge. To address current limitations, a three-tiered approach is investigated to mitigate Unmanned Aerial Vehicle (UAV) hover time, reduce network datalink transmission to a ground station, and provide a real-time framework for Sense-and-Avoidance (SAA) target classification. An energy-efficient UAS architecture framework is presented, and a corresponding SAA prototype is developed using commercial hardware to validate the proposed architecture using an experimental methodology. The proposed architecture utilizes classical computer vision methods within the Detection Subsystem coupled with deeply learned Convolutional Neural Networks (CNN) within the Classification Subsystem. Real-time operations of three frames per second are realized enabling UAV hover time and associated energy consumption during SAA processing to be effectively eliminated. Additional energy improvements are not addressed in the scope of this work. Inference accuracy is improved by 19% over baseline COTS models and current non-adaptive, single-stage SAA architectures. Overall, by pushing SAA processing to the edge of the sensors, network offload transmissions and reductions in processing time and energy consumption are feasible and realistic in future battery-powered electric air transportation systems.
KW  - edge-centric
KW  - edge computing
KW  - electric transportation
KW  - energy efficiency
KW  - Sense-and-Avoidance (SAA)
KW  - Unmanned Aerial Vehicle (UAV)
DO  - 10.3390/electricity2020007
TY  - EJOU
AU  - Martínez, Anselmo
AU  - Belmonte, Lidia M.
AU  - García, Arturo S.
AU  - Fernández-Caballero, Antonio
AU  - Morales, Rafael
TI  - Facial Emotion Recognition from an Unmanned Flying Social Robot for Home Care of Dependent People
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 7
SN  - 2079-9292

AB  - This work is part of an ongoing research project to develop an unmanned flying social robot to monitor dependants at home in order to detect the person’s state and bring the necessary assistance. In this sense, this paper focuses on the description of a virtual reality (VR) simulation platform for the monitoring process of an avatar in a virtual home by a rotatory-wing autonomous unmanned aerial vehicle (UAV). This platform is based on a distributed architecture composed of three modules communicated through the message queue telemetry transport (MQTT) protocol: the UAV Simulator implemented in MATLAB/Simulink, the VR Visualiser developed in Unity, and the new emotion recognition (ER) system developed in Python. Using a face detection algorithm and a convolutional neural network (CNN), the ER System is able to detect the person’s face in the image captured by the UAV’s on-board camera and classify the emotion among seven possible ones (surprise; fear; happiness; sadness; disgust; anger; or neutral expression). The experimental results demonstrate the correct integration of this new computer vision module within the VR platform, as well as the good performance of the designed CNN, with around 85% in the F1-score, a mean of the precision and recall of the model. The developed emotion detection system can be used in the future implementation of the assistance UAV that monitors dependent people in a real environment, since the methodology used is valid for images of real people.
KW  - flying social robot
KW  - autonomous unmanned aerial vehicle (UAV)
KW  - emotion recognition
KW  - convolution neural network (CNN)
KW  - virtual reality (VR)
KW  - unity
KW  - MATLAB/Simulink
KW  - python
DO  - 10.3390/electronics10070868
TY  - EJOU
AU  - Bao, Min
AU  - Chala Urgessa, Guyo
AU  - Xing, Mengdao
AU  - Han, Liang
AU  - Chen, Rui
TI  - Toward More Robust and Real-Time Unmanned Aerial Vehicle Detection and Tracking via Cross-Scale Feature Aggregation Based on the Center Keypoint
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - Unmanned aerial vehicles (UAVs) play an essential role in various applications, such as transportation and intelligent environmental sensing. However, due to camera motion and complex environments, it can be difficult to recognize the UAV from its surroundings thus, traditional methods often miss detection of UAVs and generate false alarms. To address these issues, we propose a novel method for detecting and tracking UAVs. First, a cross-scale feature aggregation CenterNet (CFACN) is constructed to recognize the UAVs. CFACN is a free anchor-based center point estimation method that can effectively decrease the false alarm rate, the misdetection of small targets, and computational complexity. Secondly, the region of interest-scale-crop-resize (RSCR) method is utilized to merge CFACN and region-of-interest (ROI) CFACN (ROI-CFACN) further, in order to improve the accuracy at a lower computational cost. Finally, the Kalman filter is adopted to track the UAV. The effectiveness of our method is validated using a collected UAV dataset. The experimental results demonstrate that our methods can achieve higher accuracy with lower computational cost, being superior to BiFPN, CenterNet, YoLo, and their variants on the same dataset.
KW  - cross-scale feature aggregation
KW  - center point estimation
KW  - region of interest
KW  - unmanned aerial vehicle
DO  - 10.3390/rs13081416
TY  - EJOU
AU  - Tang, Mingliang
AU  - Esmaeili, Kamran
TI  - Heap Leach Pad Surface Moisture Monitoring Using Drone-Based Aerial Images and Convolutional Neural Networks: A Case Study at the El Gallo Mine, Mexico
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - An efficient metal recovery in heap leach operations relies on uniform distribution of leaching reagent solution over the heap leach pad surface. However, the current practices for heap leach pad (HLP) surface moisture monitoring often rely on manual inspection, which is labor-intensive, time-consuming, discontinuous, and intermittent. In order to complement the manual monitoring process and reduce the frequency of exposing technical manpower to the hazardous leaching reagent (e.g., dilute cyanide solution in gold leaching), this manuscript describes a case study of implementing an HLP surface moisture monitoring method based on drone-based aerial images and convolutional neural networks (CNNs). Field data collection was conducted on a gold HLP at the El Gallo mine, Mexico. A commercially available hexa-copter drone was equipped with one visible-light (RGB) camera and one thermal infrared sensor to acquire RGB and thermal images from the HLP surface. The collected data had high spatial and temporal resolutions. The high-quality aerial images were used to generate surface moisture maps of the HLP based on two CNN approaches. The generated maps provide direct visualization of the different moisture zones across the HLP surface, and such information can be used to detect potential operational issues related to distribution of reagent solution and to facilitate timely decision making in heap leach operations.
KW  - heap leach pad monitoring
KW  - convolutional neural network
KW  - surface moisture mapping
KW  - unmanned aerial vehicle
KW  - drone
KW  - gold leaching
KW  - mining
DO  - 10.3390/rs13081420
TY  - EJOU
AU  - Zhang, Xixin
AU  - Yang, Yuhang
AU  - Li, Zhiyong
AU  - Ning, Xin
AU  - Qin, Yilang
AU  - Cai, Weiwei
TI  - An Improved Encoder-Decoder Network Based on Strip Pool Method Applied to Segmentation of Farmland Vacancy Field
T2  - Entropy

PY  - 2021
VL  - 23
IS  - 4
SN  - 1099-4300

AB  - In the research of green vegetation coverage in the field of remote sensing image segmentation, crop planting area is often obtained by semantic segmentation of images taken from high altitude. This method can be used to obtain the rate of cultivated land in a region (such as a country), but it does not reflect the real situation of a particular farmland. Therefore, this paper takes low-altitude images of farmland to build a dataset. After comparing several mainstream semantic segmentation algorithms, a new method that is more suitable for farmland vacancy segmentation is proposed. Additionally, the Strip Pooling module (SPM) and the Mixed Pooling module (MPM), with strip pooling as their core, are designed and fused into the semantic segmentation network structure to better extract the vacancy features. Considering the high cost of manual data annotation, this paper uses an improved ResNet network as the backbone of signal transmission, and meanwhile uses data augmentation to improve the performance and robustness of the model. As a result, the accuracy of the proposed method in the test set is 95.6%, mIoU is 77.6%, and the error rate is 7%. Compared to the existing model, the mIoU value is improved by nearly 4%, reaching the level of practical application.
KW  - semantic segmentation
KW  - farmland vacancy segmentation
KW  - strip pooling
KW  - crop growth assessment
KW  - encoder–decoder
DO  - 10.3390/e23040435
TY  - EJOU
AU  - Miao, Qing
AU  - Wei, Juhui
AU  - Wang, Jiongqi
AU  - Chen, Yuyun
TI  - Fault Diagnosis Algorithm Based on Adjustable Nonlinear PI State Observer and Its Application in UAV Fault Diagnosis
T2  - Algorithms

PY  - 2021
VL  - 14
IS  - 4
SN  - 1999-4893

AB  - Aiming at the problem of fault diagnosis in continuous time systems, a kind of fault diagnosis algorithm based on adaptive nonlinear proportional integral (PI) observer, which can realize the effective fault identification, is studied in this paper. Firstly, the stability and stability conditions of fault diagnosis method based on the PI observer are analyzed, and the upper bound of the fault estimation error is given. Secondly, the fault diagnosis algorithm based on adjustable nonlinear PI observer is designed and constructed, it is analyzed and we proved that the upper bound of fault estimation under this algorithm is better than that of the traditional method. Finally, the L-1011 unmanned aerial vehicle (UAV) is taken as the experimental object for numerical simulation, and the fault diagnosis method based on adaptive observer factor achieves faster response speed and more accurate fault identification results.
KW  - fault diagnosis
KW  - nonlinear observer
KW  - adaptive algorithm
KW  - UAV
KW  - parameter estimation
DO  - 10.3390/a14040119
TY  - EJOU
AU  - Rodriguez-Sanchez, M. C.
AU  - Fernández-Jiménez, Luis
AU  - Jiménez, Antonio R.
AU  - Vaquero, Joaquin
AU  - Borromeo, Susana
AU  - Lázaro-Galilea, Jose L.
TI  - HelpResponder—System for the Security of First Responder Interventions
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 8
SN  - 1424-8220

AB  - Firefighter’s interventions under dense smoke and flames are hazardous and ideally need an efficient in-advance geo-located actuation plan. The existing communication and sensing technologies should be customized, optimized, and integrated to better know the conditions (flame locations, air condition) before and during the rescue team’s interventions. In this paper, we propose a firefighter intervention architecture, which consists of several sensing devices (flame detectors, carbon dioxide air content) a navigation platform (an autonomous ground wheeled robot), and a communication/localization network (BLE IoT network) that can be used before and during an intervention in rescue or fire extinguishing missions even for indoor or confined spaces. The paper’s key novelty presents our integrated solution, giving some key implementation details and an intensive experimentation campaign in two real firefighter scenarios with real controlled fires. Results carried out in these real indoor scenarios are presented to demonstrate the feasibility of the system. A fire detection system is proposed to improve fire focus in real time and moving in confined spaces with no visibility and physical references. The results obtained in the experimentation show the proposal’s effectiveness in locating the fire focus’s position and orientation reducing time and risk exposure. This kind of location-aware fire integrated systems would significantly impact the speed and security of first responder interventions.
KW  - real-time interventions
KW  - remote monitoring
KW  - indoor location
KW  - flame detection
KW  - video processing
KW  - Internet of Things
KW  - Location-Based Service
DO  - 10.3390/s21082614
TY  - EJOU
AU  - Ludwig, Christina
AU  - Hecht, Robert
AU  - Lautenbach, Sven
AU  - Schorcht, Martin
AU  - Zipf, Alexander
TI  - Mapping Public Urban Green Spaces Based on OpenStreetMap and Sentinel-2 Imagery Using Belief Functions
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 4
SN  - 2220-9964

AB  - Public urban green spaces are important for the urban quality of life. Still, comprehensive open data sets on urban green spaces are not available for most cities. As open and globally available data sets, the potential of Sentinel-2 satellite imagery and OpenStreetMap (OSM) data for urban green space mapping is high but limited due to their respective uncertainties. Sentinel-2 imagery cannot distinguish public from private green spaces and its spatial resolution of 10 m fails to capture fine-grained urban structures, while in OSM green spaces are not mapped consistently and with the same level of completeness everywhere. To address these limitations, we propose to fuse these data sets under explicit consideration of their uncertainties. The Sentinel-2 derived Normalized Difference Vegetation Index was fused with OSM data using the Dempster–Shafer theory to enhance the detection of small vegetated areas. The distinction between public and private green spaces was achieved using a Bayesian hierarchical model and OSM data. The analysis was performed based on land use parcels derived from OSM data and tested for the city of Dresden, Germany. The overall accuracy of the final map of public urban green spaces was 95% and was mainly influenced by the uncertainty of the public accessibility model.
KW  - OpenStreetMap
KW  - volunteered geographic information
KW  - remote sensing
KW  - data fusion
KW  - land use
KW  - Dempster–Shafer theory
KW  - urban areas
DO  - 10.3390/ijgi10040251
TY  - EJOU
AU  - Choi, Daegyun
AU  - Bell, William
AU  - Kim, Donghoon
AU  - Kim, Jichul
TI  - UAV-Driven Structural Crack Detection and Location Determination Using Convolutional Neural Networks
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 8
SN  - 1424-8220

AB  - Structural cracks are a vital feature in evaluating the health of aging structures. Inspectors regularly monitor structures’ health using visual information because early detection of cracks on highly trafficked structures is critical for maintaining the public’s safety. In this work, a framework for detecting cracks along with their locations is proposed. Image data provided by an unmanned aerial vehicle (UAV) is stitched using image processing techniques to overcome limitations in the resolution of cameras. This stitched image is analyzed to identify cracks using a deep learning model that makes judgements regarding the presence of cracks in the image. Moreover, cracks’ locations are determined using data from UAV sensors. To validate the system, cracks forming on an actual building are captured by a UAV, and these images are analyzed to detect and locate cracks. The proposed framework is proven as an effective way to detect cracks and to represent the cracks’ locations.
KW  - crack detection
KW  - deep learning
KW  - convolutional neural network
KW  - image processing
KW  - unmanned aerial vehicle
DO  - 10.3390/s21082650
TY  - EJOU
AU  - Yang, Yukun
AU  - Ma, Bo
AU  - Liu, Xiangdong
AU  - Zhao, Liang
AU  - Huang, Shoudong
TI  - GSAP: A Global Structure Attention Pooling Method for Graph-Based Visual Place Recognition
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - The Visual Place Recognition problem aims to use an image to recognize the location that has been visited before. In most of the scenes revisited, the appearance and view are drastically different. Most previous works focus on the 2-D image-based deep learning method. However, the convolutional features are not robust enough to the challenging scenes mentioned above. In this paper, in order to take advantage of the information that helps the Visual Place Recognition task in these challenging scenes, we propose a new graph construction approach to extract the useful information from an RGB image and a depth image and fuse them in graph data. Then, we deal with the Visual Place Recognition problem as a graph classification problem. We propose a new Global Pooling method—Global Structure Attention Pooling (GSAP), which improves the classification accuracy by improving the expression ability of the Global Pooling component. The experiments show that our GSAP method improves the accuracy of graph classification by approximately 2–5%, the graph construction method improves the accuracy of graph classification by approximately 4–6%, and that the whole Visual Place Recognition model is robust to appearance change and view change.
KW  - graph construction
KW  - graph neural networks
KW  - graph convolution
KW  - graph global pooling
KW  - visual place recognition
DO  - 10.3390/rs13081467
TY  - EJOU
AU  - Ahmed, Nafis
AU  - Pawase, Chaitali J.
AU  - Chang, KyungHi
TI  - Distributed 3-D Path Planning for Multi-UAVs with Full Area Surveillance Based on Particle Swarm Optimization
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 8
SN  - 2076-3417

AB  - Collision-free distributed path planning for the swarm of unmanned aerial vehicles (UAVs) in a stochastic and dynamic environment is an emerging and challenging subject for research in the field of a communication system. Monitoring the methods and approaches for multi-UAVs with full area surveillance is needed in both military and civilian applications, in order to protect human beings and infrastructure, as well as their social security. To perform the path planning for multiple unmanned aerial vehicles, we propose a trajectory planner based on Particle Swarm Optimization (PSO) algorithm to derive a distributed full coverage optimal path planning, and a trajectory planner is developed using a dynamic fitness function. In this paper, to obtain dynamic fitness, we implemented the PSO algorithm independently in each UAV, by maximizing the fitness function and minimizing the cost function. Simulation results show that the proposed distributed path planning algorithm generates feasible optimal trajectories and update maps for the swarm of UAVs to surveil the entire area of interest.
KW  - 3D trajectory planning
KW  - unmanned aerial vehicle
KW  - distributed path planning
KW  - PSO
KW  - area surveillance
KW  - dynamic fitness function
DO  - 10.3390/app11083417
TY  - EJOU
AU  - Marin, Diego B.
AU  - Ferraz, Gabriel A.
AU  - Guimarães, Paulo H.
AU  - Schwerz, Felipe
AU  - Santana, Lucas S.
AU  - Barbosa, Brenon D.
AU  - Barata, Rafael A.
AU  - Faria, Rafael D.
AU  - Dias, Jessica E.
AU  - Conti, Leonardo
AU  - Rossi, Giuseppe
TI  - Remotely Piloted Aircraft and Random Forest in the Evaluation of the Spatial Variability of Foliar Nitrogen in Coffee Crop
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - The development of approaches to determine the spatial variability of nitrogen (N) into coffee leaves is essential to increase productivity and reduce production costs and environmental impacts associated with excessive N applications. Thus, this study aimed to assess the potential of the Random Forest (RF) machine learning method applied to vegetation indices (VI) obtained from Remotely Piloted Aircraft (RPA) images to measure the N content in coffee plants. A total of 10 VI were obtained from multispectral images by a camera attached to a rotary-wing RPA. The RGB orthomosaic was used to determine sampling points at the crop area, which were ranked by N levels in the plants as deficient, critical, or sufficient. The chemical analysis of N content in the coffee leaves, as well as the VI values in sample points, were used as input parameters for the image training and its classification by the RF. The suggested model has shown global accuracy and a kappa coefficient of up to 0.91 and 0.86, respectively. The best results were achieved using the Green Normalized Difference Vegetation (GNDVI) and Green Optimized Soil Adjusted Vegetation Index (GOSAVI). In addition, the model enabled the evaluation of the spatial distribution of N in the coffee trees, as well as quantification of N deficiency in the crop for the whole area. The GNDVI and GOSAVI allowed the verification that 22% of the entire crop area had plants with N deficiency symptoms, which would result in a reduction of 78% in the amount of N applied by the producer.
KW  - machine learning
KW  - vegetation indices
KW  - unmanned aerial vehicle
KW  - nitrogen management
KW  - RGB camera
DO  - 10.3390/rs13081471
TY  - EJOU
AU  - Kim, Jeonghwan
AU  - Lee, Soomin
AU  - Seo, Jongwon
AU  - Lee, Dong-Eun
AU  - Choi, Hee S.
TI  - The Integration of Earthwork Design Review and Planning Using UAV-Based Point Cloud and BIM
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 8
SN  - 2076-3417

AB  - Earthwork is seemingly guesswork, but it requires a high level of accuracy and precise planning. Differences between earthwork design and finishing levels cause project delays and cost overrun due to the time-consuming nature of earthwork re-work. Therefore, error-free earthwork planning and design review is a key to the success of earthwork projects. This study utilized an integrated approach of an unmanned aerial vehicle (UAV)-based point cloud and BIM (Building Information Modeling) to verify the design and to operate the earthwork planning. The integrated approach was proposed and applied to a 420 square meters housing construction project to review an original earthwork design and create an earthwork plan for excavator work. As a result, errors in earthwork design that caused by inaccurate initial DEM was revealed, thus the earthwork design was revised with a UAV-based point cloud map. Additionally, the integrated approach was able to generate an explicit task sequence for an excavator.
KW  - earthwork
KW  - point cloud
KW  - excavation
KW  - BIM
KW  - UAV
KW  - earthwork design
DO  - 10.3390/app11083435
TY  - EJOU
AU  - Bang, Hyuntae
AU  - Min, Jiyoung
AU  - Jeon, Haemin
TI  - Deep Learning-Based Concrete Surface Damage Monitoring Method Using Structured Lights and Depth Camera
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 8
SN  - 1424-8220

AB  - Due to the increase in aging structures and the decrease in construction workforce, there is an increasing interest in automating structural damage monitoring. Surface damage on concrete structures, such as cracks, delamination, and rebar exposure, is one of the important parameters that can be used to estimate the condition of the structure. In this paper, deep learning-based detection and quantification of structural damage using structured lights and a depth camera is proposed. The proposed monitoring system is composed of four lasers and a depth camera. The lasers are projected on the surface of the structures, and the camera captures images of the structures while measuring distance. By calculating an image homography, the captured images are calibrated when the structure and sensing system are not in parallel. The Faster RCNN (Region-based Convolutional Neural Network) with Inception Resnet v2 architecture is used to detect three types of surface damage: (i) cracks; (ii) delamination; and (iii) rebar exposure. The detected damage is quantified by calculating the positions of the projected laser beams with the measured distance. The experimental results show that structural damage was detected with an F1 score of 0.83 and a median value of the quantified relative error of less than 5%.
KW  - damage detection
KW  - quantification
KW  - deep learning
KW  - structured light
KW  - depth camera
DO  - 10.3390/s21082759
TY  - EJOU
AU  - Kang, Yeseong
AU  - Nam, Jinwoo
AU  - Kim, Younggwang
AU  - Lee, Seongtae
AU  - Seong, Deokgyeong
AU  - Jang, Sihyeong
AU  - Ryu, Chanseok
TI  - Assessment of Regression Models for Predicting Rice Yield and Protein Content Using Unmanned Aerial Vehicle-Based Multispectral Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - Unmanned aerial vehicle-based multispectral imagery including five spectral bands (blue, green, red, red-edge, and near-infrared) for a rice field in the ripening stage was used to develop regression models for predicting the rice yield and protein content and to select the most suitable regression analysis method for the year-invariant model: partial least squares regression, ridge regression, and artificial neural network (ANN). The regression models developed with six vegetation indices (green normalization difference vegetation index (GNDVI), normalization difference red-edge index (NDRE), chlorophyll index red edge (CIrededge), difference NIR/Green green difference vegetation index (GDVI), green-red NDVI (GRNDVI), and medium resolution imaging spectrometer terrestrial chlorophyll index (MTCI)), calculated from the spectral bands, were applied to single years (2018, 2019, and 2020) and multiple years (2018 + 2019, 2018 + 2020, 2019 + 2020, and all years). The regression models were cross-validated through mutual prediction against the vegetation indices in nonoverlapping years, and the prediction errors were evaluated via root mean squared error of prediction (RMSEP). The ANN model was reproducible, with low and sustained prediction errors of 24.2 kg/1000 m2 ≤ RMSEP ≤ 59.1 kg/1000 m2 in rice yield and 0.14% ≤ RMSEP ≤ 0.28% in rice-protein content in all single-year and multiple-year analyses. When the importance of each vegetation index of the regression models was evaluated, only the ANN model showed the same ranking in the vegetation index of the first (MTCI in both rice yield and protein content) and second importance (CIrededge in rice yield and GRNDVI in rice-protein content). Overall, this means that the ANN model has the highest potential for developing a year-invariant model with stable RMSEP and consistent variable ranking.
KW  - multispectral imagery
KW  - mutual prediction
KW  - regression model
KW  - rice-protein content
KW  - rice yield
DO  - 10.3390/rs13081508
TY  - EJOU
AU  - Xiong, Quan
AU  - Di, Liping
AU  - Feng, Quanlong
AU  - Liu, Diyou
AU  - Liu, Wei
AU  - Zan, Xuli
AU  - Zhang, Lin
AU  - Zhu, Dehai
AU  - Liu, Zhe
AU  - Yao, Xiaochuang
AU  - Zhang, Xiaodong
TI  - Deriving Non-Cloud Contaminated Sentinel-2 Images with RGB and Near-Infrared Bands from Sentinel-1 Images Based on a Conditional Generative Adversarial Network
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - Sentinel-2 images have been widely used in studying land surface phenomena and processes, but they inevitably suffer from cloud contamination. To solve this critical optical data availability issue, it is ideal to fuse Sentinel-1 and Sentinel-2 images to create fused, cloud-free Sentinel-2-like images for facilitating land surface applications. In this paper, we propose a new data fusion model, the Multi-channels Conditional Generative Adversarial Network (MCcGAN), based on the conditional generative adversarial network, which is able to convert images from Domain A to Domain B. With the model, we were able to generate fused, cloud-free Sentinel-2-like images for a target date by using a pair of reference Sentinel-1/Sentinel-2 images and target-date Sentinel-1 images as inputs. In order to demonstrate the superiority of our method, we also compared it with other state-of-the-art methods using the same data. To make the evaluation more objective and reliable, we calculated the root-mean-square-error (RSME), R2, Kling–Gupta efficiency (KGE), structural similarity index (SSIM), spectral angle mapper (SAM), and peak signal-to-noise ratio (PSNR) of the simulated Sentinel-2 images generated by different methods. The results show that the simulated Sentinel-2 images generated by the MCcGAN have a higher quality and accuracy than those produced via the previous methods.
KW  - Sentinel-1
KW  - Sentinel-2
KW  - generative adversarial network
KW  - non-cloud contamination
KW  - data fusion
DO  - 10.3390/rs13081512
TY  - EJOU
AU  - Liu, Li-Wei
AU  - Hsieh, Sheng-Hsin
AU  - Lin, Su-Ju
AU  - Wang, Yu-Min
AU  - Lin, Wen-Shin
TI  - Rice Blast (Magnaporthe oryzae) Occurrence Prediction and the Key Factor Sensitivity Analysis by Machine Learning
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 4
SN  - 2073-4395

AB  - This study aimed to establish a machine learning (ML)-based rice blast predicting model to decrease the appreciable losses based on short-term environment data. The average, highest and lowest air temperature, average relative humidity, soil temperature and solar energy were selected for model development. The developed multilayer perceptron (MLP), support vector machine (SVM), Elman recurrent neural network (Elman RNN) and probabilistic neural network (PNN) were evaluated by F-measures. Finally, a sensitivity analysis (SA) was conducted for the factor importance assessment. The study result shows that the PNN performed best with the F-measure (β = 2) of 96.8%. The SA was conducted in the PNN model resulting in the main effect period is 10 days before the rice blast happened. The key factors found are minimum air temperature, followed by solar energy and equaled sensitivity of average relative humidity, maximum air temperature and soil temperature. The temperature phase lag in air and soil may cause a lower dew point and suitable for rice blast pathogens growth. Through this study’s results, rice blast warnings can be issued 10 days in advance, increasing the response time for farmers preparing related preventive measures, further reducing the losses caused by rice blast.
KW  - rice disease
KW  - precision agriculture
KW  - artificial neural networks (ANN)
KW  - soil temperature
KW  - confusion matrix
KW  - F-measure
DO  - 10.3390/agronomy11040771
TY  - EJOU
AU  - Hou, Xiaoyu
AU  - Zhang, Kunlin
AU  - Xu, Jihui
AU  - Huang, Wei
AU  - Yu, Xinmiao
AU  - Xu, Huaiyu
TI  - Object Detection in Drone Imagery via Sample Balance Strategies and Local Feature Enhancement
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 8
SN  - 2076-3417

AB  - With the advent of drones, new potential applications have emerged for the unconstrained analysis of images and videos from aerial view cameras. Despite the tremendous success of the generic object detection methods developed using ground-based photos, a considerable performance drop is observed when these same methods are directly applied to images captured by Unmanned Aerial Vehicles (UAVs). Usually, most of the work goes into improving the performance of the detector in aspects such as design loss, training sample selection, feature enhancement, and so forth. This paper proposes a detection framework based on an anchor-free detector with several modules, including a sample balance strategies module and super-resolved generated feature module, to improve performance. We proposed the sample balance strategies module to optimize the imbalance among training samples, especially the imbalance between positive and negative, and easy and hard samples. Due to the high frequencies and noisy representation of the small objects in images captured by drones, the detection task is extraordinarily challenging. However, when compared with other algorithms of this kind, our method achieves better results. We also propose a super-resolved generated GAN (Generative Adversarial Network) module with center-ness weights to effectively enhance the local feature map. Finally, we demonstrate our method’s effectiveness with the proposed modules by carrying out a state-of-the-art performance on Visdrone2020 benchmarks.
KW  - object detection
KW  - drones
KW  - deep learning
KW  - sample imbalance
KW  - super-resolve GAN
DO  - 10.3390/app11083547
TY  - EJOU
AU  - Xu, Gaofei
AU  - Guo, Wei
AU  - Zhao, Yang
AU  - Zhou, Yue
AU  - Zhang, Yinlong
AU  - Liu, Xinyu
AU  - Xu, Gaopeng
AU  - Li, Guangwei
TI  - Online Learning Based Underwater Robotic Thruster Fault Detection
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 8
SN  - 2076-3417

AB  - This paper presents a novel online learning-based fault detection designed for underwater robotic thruster health monitoring. In the fault detection algorithm, we build a mathematical model between the control variable and the propeller speed by fitting collected online work status data to the model. To improve the accuracy of online modeling, a multi-center PSO algorithm with memory ability is utilized to optimize the modeling parameters. Additionally, a model online update mechanism is designed to accommodate the model to the change of thruster work status and sea environment. During the operation, propeller speed of the underwater robot is predicted through the online learning-based model, and the model residuals are used for thruster health monitoring. To avoid false alarm, an adaptive fault detection strategy is established based on model online update mechanism. The proposed method has been extensively evaluated using different underwater robotics, through a sea trial data simulation, a pool test fault detection experiment and a sea trial fault detection experiment. Compared with fixed model-based method, speed prediction MAE of the online learning model is at least 37.9% lower than that of the fixed model. The online learning-based method show no misdiagnosis in experiments, while the fixed model-based method is misdiagnosed. Experimental results show that the proposed method is competitive in terms of accuracy, adaptability, and robustness.
KW  - underwater robotic
KW  - thruster system
KW  - time delay estimation
KW  - particle swarm optimization
KW  - online learning
KW  - adaptive fault detection
DO  - 10.3390/app11083586
TY  - EJOU
AU  - Coluccia, Angelo
AU  - Fascista, Alessio
AU  - Schumann, Arne
AU  - Sommer, Lars
AU  - Dimou, Anastasios
AU  - Zarpalas, Dimitrios
AU  - Méndez, Miguel
AU  - de la Iglesia, David
AU  - González, Iago
AU  - Mercier, Jean-Philippe
AU  - Gagné, Guillaume
AU  - Mitra, Arka
AU  - Rajashekar, Shobha
TI  - Drone vs. Bird Detection: Deep Learning Algorithms and Results from a Grand Challenge
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 8
SN  - 1424-8220

AB  - Adopting effective techniques to automatically detect and identify small drones is a very compelling need for a number of different stakeholders in both the public and private sectors. This work presents three different original approaches that competed in a grand challenge on the “Drone vs. Bird” detection problem. The goal is to detect one or more drones appearing at some time point in video sequences where birds and other distractor objects may be also present, together with motion in background or foreground. Algorithms should raise an alarm and provide a position estimate only when a drone is present, while not issuing alarms on birds, nor being confused by the rest of the scene. In particular, three original approaches based on different deep learning strategies are proposed and compared on a real-world dataset provided by a consortium of universities and research centers, under the 2020 edition of the Drone vs. Bird Detection Challenge. Results show that there is a range in difficulty among different test sequences, depending on the size and the shape visibility of the drone in the sequence, while sequences recorded by a moving camera and very distant drones are the most challenging ones. The performance comparison reveals that the different approaches perform somewhat complementary, in terms of correct detection rate, false alarm rate, and average precision.
KW  - drone detection
KW  - deep learning
KW  - drone vs. bird
KW  - automatic recognition
KW  - image and video signal processing
DO  - 10.3390/s21082824
TY  - EJOU
AU  - Li, Joan Y. Q.
AU  - Duce, Stephanie
AU  - Joyce, Karen E.
AU  - Xiang, Wei
TI  - SeeCucumbers: Using Deep Learning and Drone Imagery to Detect Sea Cucumbers on Coral Reef Flats
T2  - Drones

PY  - 2021
VL  - 5
IS  - 2
SN  - 2504-446X

AB  - Sea cucumbers (Holothuroidea or holothurians) are a valuable fishery and are also crucial nutrient recyclers, bioturbation agents, and hosts for many biotic associates. Their ecological impacts could be substantial given their high abundance in some reef locations and thus monitoring their populations and spatial distribution is of research interest. Traditional in situ surveys are laborious and only cover small areas but drones offer an opportunity to scale observations more broadly, especially if the holothurians can be automatically detected in drone imagery using deep learning algorithms. We adapted the object detection algorithm YOLOv3 to detect holothurians from drone imagery at Hideaway Bay, Queensland, Australia. We successfully detected 11,462 of 12,956 individuals over 2.7ha with an average density of 0.5 individual/m2. We tested a range of hyperparameters to determine the optimal detector performance and achieved 0.855 mAP, 0.82 precision, 0.83 recall, and 0.82 F1 score. We found as few as ten labelled drone images was sufficient to train an acceptable detection model (0.799 mAP). Our results illustrate the potential of using small, affordable drones with direct implementation of open-source object detection models to survey holothurians and other shallow water sessile species.
KW  - holothurian
KW  - remote sensing
KW  - UAV
KW  - machine learning
KW  - object detection
KW  - YOLOv3
KW  - Great Barrier Reef
KW  - marine ecology
KW  - ecological monitoring
KW  - FAIR data
DO  - 10.3390/drones5020028
TY  - EJOU
AU  - Kazaz, Billur
AU  - Poddar, Subhadipto
AU  - Arabi, Saeed
AU  - Perez, Michael A.
AU  - Sharma, Anuj
AU  - Whitman, J. B.
TI  - Deep Learning-Based Object Detection for Unmanned Aerial Systems (UASs)-Based Inspections of Construction Stormwater Practices
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 8
SN  - 1424-8220

AB  - Construction activities typically create large amounts of ground disturbance, which can lead to increased rates of soil erosion. Construction stormwater practices are used on active jobsites to protect downstream waterbodies from offsite sediment transport. Federal and state regulations require routine pollution prevention inspections to ensure that temporary stormwater practices are in place and performing as intended. This study addresses the existing challenges and limitations in the construction stormwater inspections and presents a unique approach for performing unmanned aerial system (UAS)-based inspections. Deep learning-based object detection principles were applied to identify and locate practices installed on active construction sites. The system integrates a post-processing stage by clustering results. The developed framework consists of data preparation with aerial inspections, model training, validation of the model, and testing for accuracy. The developed model was created from 800 aerial images and was used to detect four different types of construction stormwater practices at 100% accuracy on the Mean Average Precision (MAP) with minimal false positive detections. Results indicate that object detection could be implemented on UAS-acquired imagery as a novel approach to construction stormwater inspections and provide accurate results for site plan comparisons by rapidly detecting the quantity and location of field-installed stormwater practices.
KW  - construction stormwater management
KW  - inspections
KW  - unmanned aerial systems
KW  - photogrammetry
KW  - deep learning-based object detection
DO  - 10.3390/s21082834
TY  - EJOU
AU  - Ge, Xiangyu
AU  - Ding, Jianli
AU  - Jin, Xiuliang
AU  - Wang, Jingzhe
AU  - Chen, Xiangyue
AU  - Li, Xiaohang
AU  - Liu, Jie
AU  - Xie, Boqiang
TI  - Estimating Agricultural Soil Moisture Content through UAV-Based Hyperspectral Images in the Arid Region
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - Unmanned aerial vehicle (UAV)-based hyperspectral remote sensing is an important monitoring technology for the soil moisture content (SMC) of agroecological systems in arid regions. This technology develops precision farming and agricultural informatization. However, hyperspectral data are generally used in data mining. In this study, UAV-based hyperspectral imaging data with a resolution o 4 cm and totaling 70 soil samples (0–10 cm) were collected from farmland (2.5 × 104 m2) near Fukang City, Xinjiang Uygur Autonomous Region, China. Four estimation strategies were tested: the original image (strategy I), first- and second-order derivative methods (strategy II), the fractional-order derivative (FOD) technique (strategy III), and the optimal fractional order combined with the optimal multiband indices (strategy IV). These strategies were based on the eXtreme Gradient Boost (XGBoost) algorithm, with the aim of building the best estimation model for agricultural SMC in arid regions. The results demonstrated that FOD technology could effectively mine information (with an absolute maximum correlation coefficient of 0.768). By comparison, strategy IV yielded the best estimates out of the methods tested (R2val = 0.921, RMSEP = 1.943, and RPD = 2.736) for the SMC. The model derived from the order of 0.4 within strategy IV worked relatively well among the different derivative methods (strategy I, II, and III). In conclusion, the combination of FOD technology and the optimal multiband indices generated a highly accurate model within the XGBoost algorithm for SMC estimation. This research provided a promising data mining approach for UAV-based hyperspectral imaging data.
KW  - fractional-order derivatives
KW  - ensemble learning
KW  - hyperspectral data
KW  - precision agriculture
DO  - 10.3390/rs13081562
TY  - EJOU
AU  - Haque, Amlan
AU  - Islam, Nahina
AU  - Samrat, Nahidul H.
AU  - Dey, Shuvashis
AU  - Ray, Biplob
TI  - Smart Farming through Responsible Leadership in Bangladesh: Possibilities, Opportunities, and Beyond
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 8
SN  - 2071-1050

AB  - Smart farming has the potential to overcome the challenge of 2050 to feed 10 billion people. Both artificial intelligence (AI) and the internet of things (IoT) have become critical prerequisites to smart farming due to their high interoperability, sensors, and cutting-edge technologies. Extending the role of responsible leadership, this paper proposes an AI and IoT based smart farming system in Bangladesh. With a comprehensive literature review, this paper counsels the need to go beyond the simple application of traditional farming and irrigation practices and recommends implementing smart farming enabling responsible leadership to uphold sustainable agriculture. It contributes to the current literature of smart farming in several ways. First, this paper helps to understand the prospect and challenges of both AI and IoT and the requirement of smart farming in a nonwestern context. Second, it clarifies the interventions of responsible leadership into Bangladesh’s agriculture sector and justifies the demand for sustainable smart farming. Third, this paper is a step forward to explore future empirical studies for the effective and efficient use of AI and IoT to adopt smart farming. Finally, this paper will help policymakers to take responsible initiatives to plan and apply smart farming in a developing economy like Bangladesh.
KW  - sustainable farming
KW  - artificial intelligence (AI)
KW  - internet of things (IoT)
KW  - smart farming
KW  - responsible leadership
KW  - remote communication
DO  - 10.3390/su13084511
TY  - EJOU
AU  - Zhao, Yifan
AU  - Zhao, Xingdong
AU  - Dai, Jiajia
AU  - Yu, Wenlong
TI  - Analysis of the Surface Subsidence Induced by Mining Near-Surface Thick Lead-Zinc Deposit Based on Numerical Simulation
T2  - Processes

PY  - 2021
VL  - 9
IS  - 4
SN  - 2227-9717

AB  - This paper describes a case study of surface subsidence in the Hongling Lead-Zinc Mine. Hongling Lead-Zinc Mine is located in Inner Mongolia, China, about 240 km away from the border between China and Mongolia. There is a batch of outcrops of the near-surface thick steep-dip metamorphic orebody. The large-scale surface subsidence induced by underground excavation has brought some impact on the safety of herdsmen and their daily husbandry activities nearby. The requirements of reclamation for subsidence areas in the relevant laws and regulations, raise enormous pressure and risk on safe and economic operation. In this paper, a 3D numerical model of this mine was built by 3DMine and FLAC3D to analyse the excavation procedure and mechanism. The results of the simulation were in good agreement with the field subsidence data collected by satellites and unmanned aerial vehicles from 2009 to 2019. The analysis showed that the current mining method—an integrated underground method of stoping and caving—accelerated the surface subsidence, and some measures of monitoring, controlling and management were expected to take in order to improve economic and ecological benefits.
KW  - near-surface thick deposit
KW  - surface subsidence
KW  - numerical simulation
KW  - unmanned aerial survey
KW  - accurate model
DO  - 10.3390/pr9040717
TY  - EJOU
AU  - Gibert Martínez, Isaac
AU  - Afonso, Frederico
AU  - Rodrigues, Simão
AU  - Lau, Fernando
TI  - A Sequential Approach for Aerodynamic Shape Optimization with Topology Optimization of Airfoils
T2  - Mathematical and Computational Applications

PY  - 2021
VL  - 26
IS  - 2
SN  - 2297-8747

AB  - The objective of this work is to study the coupling of two efficient optimization techniques, Aerodynamic Shape Optimization (ASO) and Topology Optimization (TO), in 2D airfoils. To achieve such goal two open-source codes, SU2 and Calculix, are employed for ASO and TO, respectively, using the Sequential Least SQuares Programming (SLSQP) and the Bi-directional Evolutionary Structural Optimization (BESO) algorithms; the latter is well-known for allowing the addition of material in the TO which constitutes, as far as our knowledge, a novelty for this kind of application. These codes are linked by means of a script capable of reading the geometry and pressure distribution obtained from the ASO and defining the boundary conditions to be applied in the TO. The Free-Form Deformation technique is chosen for the definition of the design variables to be used in the ASO, while the densities of the inner elements are defined as design variables of the TO. As a test case, a widely used benchmark transonic airfoil, the RAE2822, is chosen here with an internal geometric constraint to simulate the wing-box of a transonic wing. First, the two optimization procedures are tested separately to gain insight and then are run in a sequential way for two test cases with available experimental data: (i) Mach 0.729 at α=2.31°; and (ii) Mach 0.730 at α=2.79°. In the ASO problem, the lift is fixed and the drag is minimized; while in the TO problem, compliance minimization is set as the objective for a prescribed volume fraction. Improvements in both aerodynamic and structural performance are found, as expected: the ASO reduced the total pressure on the airfoil surface in order to minimize drag, which resulted in lower stress values experienced by the structure.
KW  - aerodynamic shape optimization
KW  - computational fluid dynamics
KW  - topology optimization
KW  - airfoil
DO  - 10.3390/mca26020034
TY  - EJOU
AU  - Zeng, Jie
AU  - Roussis, Panayiotis C.
AU  - Mohammed, Ahmed S.
AU  - Maraveas, Chrysanthos
AU  - Fatemi, Seyed A.
AU  - Armaghani, Danial J.
AU  - Asteris, Panagiotis G.
TI  - Prediction of Peak Particle Velocity Caused by Blasting through the Combinations of Boosted-CHAID and SVM Models with Various Kernels
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 8
SN  - 2076-3417

AB  - This research examines the feasibility of hybridizing boosted Chi-Squared Automatic Interaction Detection (CHAID) with different kernels of support vector machine (SVM) techniques for the prediction of the peak particle velocity (PPV) induced by quarry blasting. To achieve this objective, a boosting-CHAID technique was applied to a big experimental database comprising six input variables. The technique identified four input parameters (distance from blast-face, stemming length, powder factor, and maximum charge per delay) as the most significant parameters affecting the prediction accuracy and utilized them to propose the SVM models with various kernels. The kernel types used in this study include radial basis function, polynomial, sigmoid, and linear. Several criteria, including mean absolute error (MAE), correlation coefficient (R), and gains, were calculated to evaluate the developed models’ accuracy and applicability. In addition, a simple ranking system was used to evaluate the models’ performance systematically. The performance of the R and MAE index of the radial basis function kernel of SVM in training and testing phases, respectively, confirm the high capability of this SVM kernel in predicting PPV values. This study successfully demonstrates that a combination of boosting-CHAID and SVM models can identify and predict with a high level of accuracy the most effective parameters affecting PPV values.
KW  - ground vibration
KW  - blasting operation
KW  - boosting-CHAID: support vector machine
KW  - input selection
DO  - 10.3390/app11083705
TY  - EJOU
AU  - Seitsonen, Oula
AU  - Ikäheimo, Janne
TI  - Detecting Archaeological Features with Airborne Laser Scanning in the Alpine Tundra of Sápmi, Northern Finland
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 8
SN  - 2072-4292

AB  - Open access airborne laser scanning (ALS) data have been available in Finland for over a decade and have been actively applied by the Finnish archaeologists in that time. The low resolution of this laser scanning 2008–2019 dataset (0.5 points/m2), however, has hindered its usability for archaeological prospection. In the summer of 2020, the situation changed markedly, when the Finnish National Land Survey started a new countrywide ALS survey with a higher resolution of 5 points/m2. In this paper we present the first results of applying this newly available ALS material for archaeological studies. Finnish LIDARK consortium has initiated the development of semi-automated approaches for visualizing, detecting, and analyzing archaeological features with this new dataset. Our first case studies are situated in the Alpine tundra environment of Sápmi in northern Finland, and the assessed archaeological features range from prehistoric sites to indigenous Sámi reindeer herding features and Second Word War-era German military structures. Already the initial analyses of the new ALS-5p data show their huge potential for locating, mapping, and assessing archaeological material. These results also suggest an imminent burst in the number of known archaeological sites, especially in the poorly accessible and little studied northern wilderness areas, when more data become available.
KW  - archaeology
KW  - airborne laser scanning
KW  - LiDAR
KW  - Finland
KW  - Lapland
KW  - Sápmi
KW  - tundra
DO  - 10.3390/rs13081599
TY  - EJOU
AU  - Hrúz, Michal
AU  - Bugaj, Martin
AU  - Novák, Andrej
AU  - Kandera, Branislav
AU  - Badánik, Benedikt
TI  - The Use of UAV with Infrared Camera and RFID for Airframe Condition Monitoring
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 9
SN  - 2076-3417

AB  - The new progressive smart technologies announced in the fourth industrial revolution in aviation—Aviation 4.0—represent new possibilities and big challenges in aircraft maintenance processes. The main benefit of these technologies is the possibility to monitor, transfer, store, and analyze huge datasets. Based on analysis outputs, there is a possibility to improve current preventive maintenance processes and implement predictive maintenance processes. These solutions lower the downtime, save manpower, and extend the components’ lifetime; thus, the maximum effectivity and safety is achieved. The article deals with the possible implementation of an unmanned aerial vehicle (UAV) with an infrared camera and Radio Frequency Identification (RFID) as two of the smart hangar technologies for airframe condition monitoring. The presented implementations of smart technologies follow up the specific results of a case study focused on trainer aircraft failure monitoring and its impact on maintenance strategy changes. The case study failure indexes show the critical parts of aircraft that are subjected to damage the most. The aim of the article was to justify the need for thorough monitoring of critical parts of the aircraft and then analyze and propose a more effective and the most suitable form of technical condition monitoring of aircraft critical parts. The article describes the whole process of visual inspection performed by an unmanned aerial vehicle (UAV) with an IR camera and its related processes; in addition, it covers the possible usage of RFID tags as a labeling tool supporting the visual inspection. The implementations criteria apply to the repair and overhaul small aircraft maintenance organization, and later, it can also increase operational efficiency. The final suggestions describe the possible usage of proposed solutions, their main benefits, and also the limitations of their implementations in maintenance of trainer aircraft.
KW  - smart technologies
KW  - smart hangar
KW  - technical condition monitoring
KW  - UAV
KW  - RFID
DO  - 10.3390/app11093737
TY  - EJOU
AU  - Yan, Bin
AU  - Fan, Pan
AU  - Lei, Xiaoyan
AU  - Liu, Zhijie
AU  - Yang, Fuzeng
TI  - A Real-Time Apple Targets Detection Method for Picking Robot Based on Improved YOLOv5
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - The apple target recognition algorithm is one of the core technologies of the apple picking robot. However, most of the existing apple detection algorithms cannot distinguish between the apples that are occluded by tree branches and occluded by other apples. The apples, grasping end-effector and mechanical picking arm of the robot are very likely to be damaged if the algorithm is directly applied to the picking robot. Based on this practical problem, in order to automatically recognize the graspable and ungraspable apples in an apple tree image, a light-weight apple targets detection method was proposed for picking robot using improved YOLOv5s. Firstly, BottleneckCSP module was improved designed to BottleneckCSP-2 module which was used to replace the BottleneckCSP module in backbone architecture of original YOLOv5s network. Secondly, SE module, which belonged to the visual attention mechanism network, was inserted to the proposed improved backbone network. Thirdly, the bonding fusion mode of feature maps, which were inputs to the target detection layer of medium size in the original YOLOv5s network, were improved. Finally, the initial anchor box size of the original network was improved. The experimental results indicated that the graspable apples, which were unoccluded or only occluded by tree leaves, and the ungraspable apples, which were occluded by tree branches or occluded by other fruits, could be identified effectively using the proposed improved network model in this study. Specifically, the recognition recall, precision, mAP and F1 were 91.48%, 83.83%, 86.75% and 87.49%, respectively. The average recognition time was 0.015 s per image. Contrasted with original YOLOv5s, YOLOv3, YOLOv4 and EfficientDet-D0 model, the mAP of the proposed improved YOLOv5s model increased by 5.05%, 14.95%, 4.74% and 6.75% respectively, the size of the model compressed by 9.29%, 94.6%, 94.8% and 15.3% respectively. The average recognition speeds per image of the proposed improved YOLOv5s model were 2.53, 1.13 and 3.53 times of EfficientDet-D0, YOLOv4 and YOLOv3 and model, respectively. The proposed method can provide technical support for the real-time accurate detection of multiple fruit targets for the apple picking robot.
KW  - artificial intelligence
KW  - convolutional neural network
KW  - YOLOv5
KW  - object detection
KW  - apple picking robot
KW  - lightweight
KW  - real-time detection
DO  - 10.3390/rs13091619
TY  - EJOU
AU  - Kwak, Geun-Ho
AU  - Park, Chan-won
AU  - Lee, Kyung-do
AU  - Na, Sang-il
AU  - Ahn, Ho-yong
AU  - Park, No-Wook
TI  - Potential of Hybrid CNN-RF Model for Early Crop Mapping with Limited Input Data
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - When sufficient time-series images and training data are unavailable for crop classification, features extracted from convolutional neural network (CNN)-based representative learning may not provide useful information to discriminate crops with similar spectral characteristics, leading to poor classification accuracy. In particular, limited input data are the main obstacles to obtain reliable classification results for early crop mapping. This study investigates the potential of a hybrid classification approach, i.e., CNN-random forest (CNN-RF), in the context of early crop mapping, that combines the automatic feature extraction capability of CNN with the superior discrimination capability of an RF classifier. Two experiments on incremental crop classification with unmanned aerial vehicle images were conducted to compare the performance of CNN-RF with that of CNN and RF with respect to the length of the time-series and training data sizes. When sufficient time-series images and training data were used for the classification, the accuracy of CNN-RF was slightly higher or comparable with that of CNN. In contrast, when fewer images and the smallest training data were used at the early crop growth stage, CNN-RF was substantially beneficial and the overall accuracy increased by maximum 6.7%p and 4.6%p in the two study areas, respectively, compared to CNN. This is attributed to its ability to discriminate crops from features with insufficient information using a more sophisticated classifier. The experimental results demonstrate that CNN-RF is an effective classifier for early crop mapping when only limited input images and training samples are available.
KW  - crop classification
KW  - convolution neural networks
KW  - random forest
KW  - hybrid model
KW  - training data
KW  - time-series images
DO  - 10.3390/rs13091629
TY  - EJOU
AU  - Ge, Haixiao
AU  - Xiang, Haitao
AU  - Ma, Fei
AU  - Li, Zhenwang
AU  - Qiu, Zhengchao
AU  - Tan, Zhengzheng
AU  - Du, Changwen
TI  - Estimating Plant Nitrogen Concentration of Rice through Fusing Vegetation Indices and Color Moments Derived from UAV-RGB Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Estimating plant nitrogen concentration (PNC) has been conducted using vegetation indices (VIs) from UAV-based imagery, but color features have been rarely considered as additional variables. In this study, the VIs and color moments (color feature) were calculated from UAV-based RGB images, then partial least square regression (PLSR) and random forest regression (RF) models were established to estimate PNC through fusing VIs and color moments. The results demonstrated that the fusion of VIs and color moments as inputs yielded higher accuracies of PNC estimation compared to VIs or color moments as input; the RF models based on the combination of VIs and color moments (R2 ranging from 0.69 to 0.91 and NRMSE ranging from 0.07 to 0.13) showed similar performances to the PLSR models (R2 ranging from 0.68 to 0.87 and NRMSE ranging from 0.10 to 0.29); Among the top five important variables in the RF models, there was at least one variable which belonged to the color moments in different datasets, indicating the significant contribution of color moments in improving PNC estimation accuracy. This revealed the great potential of combination of RGB-VIs and color moments for the estimation of rice PNC.
KW  - UAV
KW  - plant nitrogen concentration
KW  - RGB-VIs
KW  - color moments
KW  - PLSR
KW  - RF
DO  - 10.3390/rs13091620
TY  - EJOU
AU  - Azar, Ahmad T.
AU  - Koubaa, Anis
AU  - Ali Mohamed, Nada
AU  - Ibrahim, Habiba A.
AU  - Ibrahim, Zahra F.
AU  - Kazim, Muhammad
AU  - Ammar, Adel
AU  - Benjdira, Bilel
AU  - Khamis, Alaa M.
AU  - Hameed, Ibrahim A.
AU  - Casalino, Gabriella
TI  - Drone Deep Reinforcement Learning: A Review
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 9
SN  - 2079-9292

AB  - Unmanned Aerial Vehicles (UAVs) are increasingly being used in many challenging and diversified applications. These applications belong to the civilian and the military fields. To name a few; infrastructure inspection, traffic patrolling, remote sensing, mapping, surveillance, rescuing humans and animals, environment monitoring, and Intelligence, Surveillance, Target Acquisition, and Reconnaissance (ISTAR) operations. However, the use of UAVs in these applications needs a substantial level of autonomy. In other words, UAVs should have the ability to accomplish planned missions in unexpected situations without requiring human intervention. To ensure this level of autonomy, many artificial intelligence algorithms were designed. These algorithms targeted the guidance, navigation, and control (GNC) of UAVs. In this paper, we described the state of the art of one subset of these algorithms: the deep reinforcement learning (DRL) techniques. We made a detailed description of them, and we deduced the current limitations in this area. We noted that most of these DRL methods were designed to ensure stable and smooth UAV navigation by training computer-simulated environments. We realized that further research efforts are needed to address the challenges that restrain their deployment in real-life scenarios.
KW  - unmanned aerial vehicles
KW  - UAVs
KW  - guidance
KW  - navigation
KW  - control
KW  - machine learning
KW  - deep reinforcement learning (DRL)
KW  - literature review
DO  - 10.3390/electronics10090999
TY  - EJOU
AU  - Kashyap, Bhuwan
AU  - Kumar, Ratnesh
TI  - Sensing Methodologies in Agriculture for Monitoring Biotic Stress in Plants Due to Pathogens and Pests
T2  - Inventions

PY  - 2021
VL  - 6
IS  - 2
SN  - 2411-5134

AB  - Reducing agricultural losses is an effective way to sustainably increase agricultural output efficiency to meet our present and future needs for food, fiber, fodder, and fuel. Our ever-improving understanding of the ways in which plants respond to stress, biotic and abiotic, has led to the development of innovative sensing technologies for detecting crop stresses/stressors and deploying efficient measures. This article aims to present the current state of the methodologies applied in the field of agriculture towards the detection of biotic stress in crops. Key sensing methodologies for plant pathogen (or phytopathogen), as well as herbivorous insects/pests are presented, where the working principles are described, and key recent works discussed. The detection methods overviewed for phytopathogen-related stress identification include nucleic acid-based methods, immunological methods, imaging-based techniques, spectroscopic methods, phytohormone biosensing methods, monitoring methods for plant volatiles, and active remote sensing technologies. Whereas the pest-related sensing techniques include machine-vision-based methods, pest acoustic-emission sensors, and volatile organic compound-based stress monitoring methods. Additionally, Comparisons have been made between different sensing techniques as well as recently reported works, where the strengths and limitations are identified. Finally, the prospective future directions for monitoring biotic stress in crops are discussed.
KW  - biosensors
KW  - hyperspectral
KW  - thermography
KW  - electrochemical
KW  - hormones
KW  - fluorescence
KW  - acoustic
KW  - spectroscopy
KW  - remote sensing
KW  - volatile organic compounds
DO  - 10.3390/inventions6020029
TY  - EJOU
AU  - Gargees, Rasha S.
AU  - Scott, Grant J.
TI  - Large-Scale, Multiple Level-of-Detail Change Detection from Remote Sensing Imagery Using Deep Visual Feature Clustering
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - In the era of big data, where massive amounts of remotely sensed imagery can be obtained from various satellites accompanied by the rapid change in the surface of the Earth, new techniques for large-scale change detection are necessary to facilitate timely and effective human understanding of natural and human-made phenomena. In this research, we propose a chip-based change detection method that is enabled by using deep neural networks to extract visual features. These features are transformed into deep orthogonal visual features that are then clustered based on land cover characteristics. The resulting chip cluster memberships allow arbitrary level-of-detail change analysis that can also support irregular geospatial extent based agglomerations. The proposed methods naturally support cross-resolution temporal scenes without requiring normalization of the pixel resolution across scenes and without requiring pixel-level coregistration processes. This is achieved with configurable spatial locality comparisons between years, where the aperture of a unit of measure can be a single chip, a small neighborhood of chips, or a large irregular geospatial region. The performance of our proposed method has been validated using various quantitative and statistical metrics in addition to presenting the visual geo-maps and the percentage of the change. The results show that our proposed method efficiently detected the change from a large scale area.
KW  - change detection
KW  - big data
KW  - deep features
KW  - fuzzy clustering
KW  - transfer learning
KW  - land cover
DO  - 10.3390/rs13091661
TY  - EJOU
AU  - Morio, Jérôme
AU  - Levasseur, Baptiste
AU  - Bertrand, Sylvain
TI  - Drone Ground Impact Footprints with Importance Sampling: Estimation and Sensitivity Analysis
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 9
SN  - 2076-3417

AB  - This paper addresses the estimation of accurate extreme ground impact footprints and probabilistic maps due to a total loss of control of fixed-wing unmanned aerial vehicles after a main engine failure. In this paper, we focus on the ground impact footprints that contains 95%, 99% and 99.9% of the drone impacts. These regions are defined here with density minimum volume sets and may be estimated by Monte Carlo methods. As Monte Carlo approaches lead to an underestimation of extreme ground impact footprints, we consider in this article multiple importance sampling to evaluate them. Then, we perform a reliability oriented sensitivity analysis, to estimate the most influential uncertain parameters on the ground impact position. We show the results of these estimations on a realistic drone flight scenario.
KW  - UAV
KW  - probabilistic maps of impact
KW  - ground footprints
KW  - Monte Carlo
KW  - importance sampling
KW  - sensitivity analysis
DO  - 10.3390/app11093871
TY  - EJOU
AU  - Islam, Nahina
AU  - Rashid, Md M.
AU  - Wibowo, Santoso
AU  - Xu, Cheng-Yuan
AU  - Morshed, Ahsan
AU  - Wasimi, Saleh A.
AU  - Moore, Steven
AU  - Rahman, Sk M.
TI  - Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 5
SN  - 2077-0472

AB  - This paper explores the potential of machine learning algorithms for weed and crop classification from UAV images. The identification of weeds in crops is a challenging task that has been addressed through orthomosaicing of images, feature extraction and labelling of images to train machine learning algorithms. In this paper, the performances of several machine learning algorithms, random forest (RF), support vector machine (SVM) and k-nearest neighbours (KNN), are analysed to detect weeds using UAV images collected from a chilli crop field located in Australia. The evaluation metrics used in the comparison of performance were accuracy, precision, recall, false positive rate and kappa coefficient. MATLAB is used for simulating the machine learning algorithms; and the achieved weed detection accuracies are 96% using RF, 94% using SVM and 63% using KNN. Based on this study, RF and SVM algorithms are efficient and practical to use, and can be implemented easily for detecting weed from UAV images.
KW  - weed detection
KW  - smart farming
KW  - machine learning
KW  - remote sensing
KW  - image processing
DO  - 10.3390/agriculture11050387
TY  - EJOU
AU  - Song, Bonggeun
AU  - Park, Kyunghun
TI  - Comparison of Outdoor Compost Pile Detection Using Unmanned Aerial Vehicle Images and Various Machine Learning Techniques
T2  - Drones

PY  - 2021
VL  - 5
IS  - 2
SN  - 2504-446X

AB  - Since outdoor compost piles (OCPs) contain large amounts of nitrogen and phosphorus, they act as a major pollutant that deteriorates water quality, such as eutrophication and green algae, when the OCPs enter the river during rainfall. In South Korea, OCPs are frequently used, but there is a limitation that a lot of manpower and budget are consumed to investigate the current situation, so it is necessary to efficiently investigate the OCPs. This study compared the accuracy of various machine learning techniques for the efficient detection and management of outdoor compost piles (OCPs), a non-point pollution source in agricultural areas in South Korea, using unmanned aerial vehicle (UAV) images. RGB, multispectral, and thermal infrared UAV images were taken in August and October 2019. Additionally, vegetation indices (NDVI, NDRE, ENDVI, and GNDVI) and surface temperature were also considered. Four machine learning techniques, including support vector machine (SVM), decision tree (DT), random forest (RF), and k-NN, were implemented, and the machine learning technique with the highest accuracy was identified by adjusting several variables. The accuracy of all machine learning techniques was very high, reaching values of up to 0.96. Particularly, the accuracy of the RF method with the number of estimators set to 10 was highest, reaching 0.989 in August and 0.987 in October. The proposed method allows for the prediction of OCP location and area over large regions, thereby foregoing the need for OCP field measurements. Therefore, our findings provide highly useful data for the improvement of OCP management strategies and water quality.
KW  - non-point pollutant
KW  - random forest
KW  - SVM
KW  - decision tree
KW  - k-NN
KW  - python
DO  - 10.3390/drones5020031
TY  - EJOU
AU  - Elsayed, Salah
AU  - El-Hendawy, Salah
AU  - Khadr, Mosaad
AU  - Elsherbiny, Osama
AU  - Al-Suhaibani, Nasser
AU  - Alotaibi, Majed
AU  - Tahir, Muhammad U.
AU  - Darwish, Waleed
TI  - Combining Thermal and RGB Imaging Indices with Multivariate and Data-Driven Modeling to Estimate the Growth, Water Status, and Yield of Potato under Different Drip Irrigation Regimes
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Advances in proximal hyperspectral sensing tools, chemometric techniques, and data-driven modeling have enhanced precision irrigation management by facilitating the monitoring of several plant traits. This study investigated the performance of remote sensing indices derived from thermal and red-green-blue (RGB) images combined with stepwise multiple linear regression (SMLR) and an integrated adaptive neuro-fuzzy inference system with a genetic algorithm (ANFIS-GA) for monitoring the biomass fresh weight (BFW), biomass dry weight (BDW), biomass water content (BWC), and total tuber yield (TTY) of two potato varieties under 100%, 75%, and 50% of the estimated crop evapotranspiration (ETc). Results showed that the plant traits and indices varied significantly between the three irrigation regimes. Furthermore, all of the indices exhibited strong relationships with BFW, CWC, and TTY (R2 = 0.80–0.92) and moderate to weak relationships with BDW (R2 = 0.25–0.65) when considered for each variety across the irrigation regimes, for each season across the varieties and irrigation regimes, and across all data combined, but none of the indices successfully assessed any of the plant traits when considered for each irrigation regime across the two varieties. The SMLR and ANFIS-GA models gave the best predictions for the four plant traits in the calibration and testing stages, with the exception of the SMLR testing model for BDW. Thus, the use of thermal and RGB imaging indices with ANFIS-GA models could be a practical tool for managing the growth and production of potato crops under deficit irrigation regimes.
KW  - ANFIS
KW  - deficit irrigation
KW  - genetic algorithm
KW  - irrigation management
KW  - RGB digital camera
KW  - stepwise multiple linear regression
KW  - thermal camera
DO  - 10.3390/rs13091679
TY  - EJOU
AU  - Shauqee, Mohamad N.
AU  - Rajendran, Parvathy
AU  - Suhadis, Nurulasikin M.
TI  - An Explosion Based Algorithm to Solve the Optimization Problem in Quadcopter Control
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 5
SN  - 2226-4310

AB  - This paper presents an optimization algorithm named Random Explosion Algorithm (REA). The fundamental idea of this algorithm is based on a simple concept of the explosion of an object. This object is commonly known as a particle: when exploded, it will randomly disperse fragments around the particle within the explosion radius. The fragment that will be considered as a search agent will fill the local space and search that particular region for the best fitness solution. The proposed algorithm was tested on 23 benchmark test functions, and the results are validated by a comparative study with eight well-known algorithms, which are Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC), Genetic Algorithm (GA), Differential Evolution (DE), Multi-Verse Optimizer (MVO), Moth Flame Optimizer (MFO), Firefly Algorithm (FA), and Sooty Tern Optimization Algorithm (STOA). After that, the algorithm was implemented and analyzed for a quadrotor control application. Similarly, a comparative study with the other algorithms stated was done. The findings reveal that the REA can yield very competitive results. It also shows that the convergence analysis has proved that the REA can converge more quickly toward the global optimum than the other metaheuristic algorithms. For the control application result, the REA controller can better track the desired reference input with shorter rise time and settling time, lower percentage overshoot, and minimal steady-state error and root mean square error (RMSE).
KW  - random explosion
KW  - metaheuristic optimization
KW  - artificial intelligence
KW  - controller design
KW  - unimodal benchmark
KW  - multimodal benchmark
DO  - 10.3390/aerospace8050125
TY  - EJOU
AU  - Wang, Zhaojun
AU  - Wang, Jiangning
AU  - Lin, Congtian
AU  - Han, Yan
AU  - Wang, Zhaosheng
AU  - Ji, Liqiang
TI  - Identifying Habitat Elements from Bird Images Using Deep Convolutional Neural Networks
T2  - Animals

PY  - 2021
VL  - 11
IS  - 5
SN  - 2076-2615

AB  - With the rapid development of digital technology, bird images have become an important part of ornithology research data. However, due to the rapid growth of bird image data, it has become a major challenge to effectively process such a large amount of data. In recent years, deep convolutional neural networks (DCNNs) have shown great potential and effectiveness in a variety of tasks regarding the automatic processing of bird images. However, no research has been conducted on the recognition of habitat elements in bird images, which is of great help when extracting habitat information from bird images. Here, we demonstrate the recognition of habitat elements using four DCNN models trained end-to-end directly based on images. To carry out this research, an image database called Habitat Elements of Bird Images (HEOBs-10) and composed of 10 categories of habitat elements was built, making future benchmarks and evaluations possible. Experiments showed that good results can be obtained by all the tested models. ResNet-152-based models yielded the best test accuracy rate (95.52%); the AlexNet-based model yielded the lowest test accuracy rate (89.48%). We conclude that DCNNs could be efficient and useful for automatically identifying habitat elements from bird images, and we believe that the practical application of this technology will be helpful for studying the relationships between birds and habitat elements.
KW  - bird images
KW  - deep convolutional neural networks
KW  - habitat elements
DO  - 10.3390/ani11051263
TY  - EJOU
AU  - de Camargo, Tibor
AU  - Schirrmann, Michael
AU  - Landwehr, Niels
AU  - Dammer, Karl-Heinz
AU  - Pflanz, Michael
TI  - Optimized Deep Learning Model as a Basis for Fast UAV Mapping of Weed Species in Winter Wheat Crops
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Weed maps should be available quickly, reliably, and with high detail to be useful for site-specific management in crop protection and to promote more sustainable agriculture by reducing pesticide use. Here, the optimization of a deep residual convolutional neural network (ResNet-18) for the classification of weed and crop plants in UAV imagery is proposed. The target was to reach sufficient performance on an embedded system by maintaining the same features of the ResNet-18 model as a basis for fast UAV mapping. This would enable online recognition and subsequent mapping of weeds during UAV flying operation. Optimization was achieved mainly by avoiding redundant computations that arise when a classification model is applied on overlapping tiles in a larger input image. The model was trained and tested with imagery obtained from a UAV flight campaign at low altitude over a winter wheat field, and classification was performed on species level with the weed species Matricaria chamomilla L., Papaver rhoeas L., Veronica hederifolia L., and Viola arvensis ssp. arvensis observed in that field. The ResNet-18 model with the optimized image-level prediction pipeline reached a performance of 2.2 frames per second with an NVIDIA Jetson AGX Xavier on the full resolution UAV image, which would amount to about 1.78 ha h−1 area output for continuous field mapping. The overall accuracy for determining crop, soil, and weed species was 94%. There were some limitations in the detection of species unknown to the model. When shifting from 16-bit to 32-bit model precision, no improvement in classification accuracy was observed, but a strong decline in speed performance, especially when a higher number of filters was used in the ResNet-18 model. Future work should be directed towards the integration of the mapping process on UAV platforms, guiding UAVs autonomously for mapping purpose, and ensuring the transferability of the models to other crop fields.
KW  - ResNet
KW  - deep residual networks
KW  - UAV imagery
KW  - embedded systems
KW  - crop monitoring
KW  - image classification
KW  - site-specific weed management
KW  - real-time mapping
DO  - 10.3390/rs13091704
TY  - EJOU
AU  - Xu, Dandan
AU  - Wang, Haobin
AU  - Xu, Weixin
AU  - Luan, Zhaoqing
AU  - Xu, Xia
TI  - LiDAR Applications to Estimate Forest Biomass at Individual Tree Scale: Opportunities, Challenges and Future Perspectives
T2  - Forests

PY  - 2021
VL  - 12
IS  - 5
SN  - 1999-4907

AB  - Accurate forest biomass estimation at the individual tree scale is the foundation of timber industry and forest management. It plays an important role in explaining ecological issues and small-scale processes. Remotely sensed images, across a range of spatial and temporal resolutions, with their advantages of non-destructive monitoring, are widely applied in forest biomass monitoring at global, ecoregion or community scales. However, the development of remote sensing applications for forest biomass at the individual tree scale has been relatively slow due to the constraints of spatial resolution and evaluation accuracy of remotely sensed data. With the improvements in platforms and spatial resolutions, as well as the development of remote sensing techniques, the potential for forest biomass estimation at the single tree level has been demonstrated. However, a comprehensive review of remote sensing of forest biomass scaled at individual trees has not been done. This review highlights the theoretical bases, challenges and future perspectives for Light Detection and Ranging (LiDAR) applications of individual trees scaled to whole forests. We summarize research on estimating individual tree volume and aboveground biomass (AGB) using Terrestrial Laser Scanning (TLS), Airborne Laser Scanning (ALS), Unmanned Aerial Vehicle Laser Scanning (UAV-LS) and Mobile Laser Scanning (MLS, including Vehicle-borne Laser Scanning (VLS) and Backpack Laser Scanning (BLS)) data.
KW  - forest aboveground biomass
KW  - LiDAR
KW  - individual tree scale
KW  - UAV-LS
KW  - Backpack Laser Scanning
DO  - 10.3390/f12050550
TY  - EJOU
AU  - Kuzmin, Anton
AU  - Korhonen, Lauri
AU  - Kivinen, Sonja
AU  - Hurskainen, Pekka
AU  - Korpelainen, Pasi
AU  - Tanhuanpää, Topi
AU  - Maltamo, Matti
AU  - Vihervaara, Petteri
AU  - Kumpula, Timo
TI  - Detection of European Aspen (Populus tremula L.) Based on an Unmanned Aerial Vehicle Approach in Boreal Forests
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - European aspen (Populus tremula L.) is a keystone species for biodiversity of boreal forests. Large-diameter aspens maintain the diversity of hundreds of species, many of which are threatened in Fennoscandia. Due to a low economic value and relatively sparse and scattered occurrence of aspen in boreal forests, there is a lack of information of the spatial and temporal distribution of aspen, which hampers efficient planning and implementation of sustainable forest management practices and conservation efforts. Our objective was to assess identification of European aspen at the individual tree level in a southern boreal forest using high-resolution photogrammetric point cloud (PPC) and multispectral (MSP) orthomosaics acquired with an unmanned aerial vehicle (UAV). The structure-from-motion approach was applied to generate RGB imagery-based PPC to be used for individual tree-crown delineation. Multispectral data were collected using two UAV cameras: Parrot Sequoia and MicaSense RedEdge-M. Tree-crown outlines were obtained from watershed segmentation of PPC data and intersected with multispectral mosaics to extract and calculate spectral metrics for individual trees. We assessed the role of spectral data features extracted from PPC and multispectral mosaics and a combination of it, using a machine learning classifier—Support Vector Machine (SVM) to perform two different classifications: discrimination of aspen from the other species combined into one class and classification of all four species (aspen, birch, pine, spruce) simultaneously. In the first scenario, the highest classification accuracy of 84% (F1-score) for aspen and overall accuracy of 90.1% was achieved using only RGB features from PPC, whereas in the second scenario, the highest classification accuracy of 86 % (F1-score) for aspen and overall accuracy of 83.3% was achieved using the combination of RGB and MSP features. The proposed method provides a new possibility for the rapid assessment of aspen occurrence to enable more efficient forest management as well as contribute to biodiversity monitoring and conservation efforts in boreal forests.
KW  - tree species classification
KW  - European aspen
KW  - UAV
KW  - biodiversity
KW  - deciduous trees
KW  - machine learning
KW  - multispectral data
KW  - boreal forest
DO  - 10.3390/rs13091723
TY  - EJOU
AU  - Hobley, Brandon
AU  - Arosio, Riccardo
AU  - French, Geoffrey
AU  - Bremner, Julie
AU  - Dolphin, Tony
AU  - Mackiewicz, Michal
TI  - Semi-Supervised Segmentation for Coastal Monitoring Seagrass Using RPA Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Intertidal seagrass plays a vital role in estimating the overall health and dynamics of coastal environments due to its interaction with tidal changes. However, most seagrass habitats around the globe have been in steady decline due to human impacts, disturbing the already delicate balance in the environmental conditions that sustain seagrass. Miniaturization of multi-spectral sensors has facilitated very high resolution mapping of seagrass meadows, which significantly improves the potential for ecologists to monitor changes. In this study, two analytical approaches used for classifying intertidal seagrass habitats are compared—Object-based Image Analysis (OBIA) and Fully Convolutional Neural Networks (FCNNs). Both methods produce pixel-wise classifications in order to create segmented maps. FCNNs are an emerging set of algorithms within Deep Learning. Conversely, OBIA has been a prominent solution within this field, with many studies leveraging in-situ data and multiresolution segmentation to create habitat maps. This work demonstrates the utility of FCNNs in a semi-supervised setting to map seagrass and other coastal features from an optical drone survey conducted at Budle Bay, Northumberland, England. Semi-supervision is also an emerging field within Deep Learning that has practical benefits of achieving state of the art results using only subsets of labelled data. This is especially beneficial for remote sensing applications where in-situ data is an expensive commodity. For our results, we show that FCNNs have comparable performance with the standard OBIA method used by ecologists.
KW  - deep learning
KW  - computer vision
KW  - remote sensing
KW  - supervised learning
KW  - semi-supervised learning
KW  - segmentation
KW  - seagrass mapping
DO  - 10.3390/rs13091741
TY  - EJOU
AU  - Liang, Peng
AU  - Shi, Wenzhong
AU  - Ding, Yixing
AU  - Liu, Zhiqiang
AU  - Shang, Haolv
TI  - Road Extraction from High Resolution Remote Sensing Images Based on Vector Field Learning
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 9
SN  - 1424-8220

AB  - Accurate and up-to-date road network information is very important for the Geographic Information System (GIS) database, traffic management and planning, automatic vehicle navigation, emergency response and urban pollution sources investigation. In this paper, we use vector field learning to extract roads from high resolution remote sensing imaging. This method is usually used for skeleton extraction in nature image, but seldom used in road extraction. In order to improve the accuracy of road extraction, three vector fields are constructed and combined respectively with the normal road mask learning by a two-task network. The results show that all the vector fields are able to significantly improve the accuracy of road extraction, no matter the field is constructed in the road area or completely outside the road. The highest F1 score is 0.7618, increased by 0.053 compared with using only mask learning.
KW  - road extraction
KW  - vector field learning
KW  - high resolution remote sensing image
KW  - encoder-decoder
KW  - DCNN
DO  - 10.3390/s21093152
TY  - EJOU
AU  - Geng, Xiurui
AU  - Wang, Lei
AU  - Ji, Luyan
TI  - Identify Informative Bands for Hyperspectral Target Detection Using the Third-Order Statistic
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Constrained energy minimization (CEM) has been proposed and widely researched in the field of hyperspectral target detection. Generally, it selects one of the target spectra as the representative and then keeps its output constant while minimizing the average filter output energy of the data. However, it has been proven that as the number of bands (L) increases, CEM will gradually lower the average filter output energy when keeping the representative’s output constant. Unavoidably, due to the inherent spatial and temporal variation of the spectra, this will lead to an unreasonable phenomenon, i.e., if L is particularly large, when adding more bands, CEM will suppress more and more pixels, even including the target pixels. This means that the optimal solution of CEM may not correspond to the target detection result that we desire. To deal with this, in this paper, we introduce the third-order statistic (skewness) of the CEM model, served as an auxiliary index to determine whether a band is beneficial to target detection or not. Theoretically, we prove that the skewness index can always exclude the noisy bands with Gaussian distribution. In addition, experiments on several widely used remote sensing data indicate that the index can also efficiently identify informative bands for a better target detection performance.
KW  - target detection
KW  - spectral variability
KW  - third-order statistic
KW  - constrained energy minimization
DO  - 10.3390/rs13091776
TY  - EJOU
AU  - Wang, Li
AU  - Chen, Shuisen
AU  - Peng, Zhiping
AU  - Huang, Jichuan
AU  - Wang, Chongyang
AU  - Jiang, Hao
AU  - Zheng, Qiong
AU  - Li, Dan
TI  - Phenology Effects on Physically Based Estimation of Paddy Rice Canopy Traits from UAV Hyperspectral Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Radiation transform models such as PROSAIL are widely used for crop canopy reflectance simulation and biophysical parameter inversion. The PROSAIL model basically assumes that the canopy is turbid homogenous media with a bare soil background. However, the canopy structure changes when crop growth stages develop, which is more or less a departure from this assumption. In addition, a paddy rice field is inundated most of the time with flooded soil background. In this study, field-scale paddy rice leaf area index (LAI), leaf cholorphyll content (LCC), and canopy chlorophyll content (CCC) were retrieved from unmanned-aerial-vehicle-based hyperspectral images by the PROSAIL radiation transform model using a lookup table (LUT) strategy, with a special focus on the effects of growth-stage development and soil-background signature selection. Results show that involving flooded soil reflectance as background reflectance for PROSAIL could improve estimation accuracy. When using a LUT with the flooded soil reflectance signature (LUTflooded) the coefficients of determination (R2) between observed and estimation variables are 0.70, 0.11, and 0.79 for LAI, LCC, and CCC, respectively, for the entire growing season (from tillering to heading growth stages), and the corresponding mean absolute errors (MAEs) are 21.87%, 16.27%, and 12.52%. For LAI and LCC, high model bias mainly occurred in tillering growth stages. There is an obvious overestimation of LAI and underestimation of LCC for in the tillering growth stage. The estimation accuracy of CCC is relatively consistent from tillering to heading growth stages.
KW  - paddy rice
KW  - growth stages
KW  - phenology
KW  - soil background
KW  - radiative transfer models
KW  - PROSAIL
KW  - lookup tables
KW  - hyperspectral
DO  - 10.3390/rs13091792
TY  - EJOU
AU  - Feroz, Sainab
AU  - Abu Dabous, Saleh
TI  - UAV-Based Remote Sensing Applications for Bridge Condition Assessment
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Deterioration of bridge infrastructure is a serious concern to transport and government agencies as it declines serviceability and reliability of bridges and jeopardizes public safety. Maintenance and rehabilitation needs of bridge infrastructure are periodically monitored and assessed, typically every two years. Existing inspection techniques, such as visual inspection, are time-consuming, subjective, and often incomplete. Non-destructive testing (NDT) using Unmanned Aerial Vehicles (UAVs) have been gaining momentum for bridge monitoring in the recent years, particularly due to enhanced accessibility and cost efficiency, deterrence of traffic closure, and improved safety during inspection. The primary objective of this study is to conduct a comprehensive review of the application of UAVs in bridge condition monitoring, used in conjunction with remote sensing technologies. Remote sensing technologies such as visual imagery, infrared thermography, LiDAR, and other sensors, integrated with UAVs for data acquisition are analyzed in depth. This study compiled sixty-five journal and conference papers published in the last two decades scrutinizing NDT-based UAV systems. In addition to comparison of stand-alone and integrated NDT-UAV methods, the facilitation of bridge inspection using UAVs is thoroughly discussed in the present article in terms of ease of use, accuracy, cost-efficiency, employed data collection tools, and simulation platforms. Additionally, challenges and future perspectives of the reviewed UAV-NDT technologies are highlighted.
KW  - unmanned aerial vehicles
KW  - drones
KW  - condition monitoring
KW  - remote sensing
KW  - non-destructive testing
KW  - remotely piloted aircraft
DO  - 10.3390/rs13091809
TY  - EJOU
AU  - Chen, Shuo
AU  - Zhang, Kefei
AU  - Zhao, Yindi
AU  - Sun, Yaqin
AU  - Ban, Wei
AU  - Chen, Yu
AU  - Zhuang, Huifu
AU  - Zhang, Xuewei
AU  - Liu, Jinxiang
AU  - Yang, Tao
TI  - An Approach for Rice Bacterial Leaf Streak Disease Segmentation and Disease Severity Estimation
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 5
SN  - 2077-0472

AB  - Rice bacterial leaf streak (BLS) is a serious disease in rice leaves and can seriously affect the quality and quantity of rice growth. Automatic estimation of disease severity is a crucial requirement in agricultural production. To address this, a new method (termed BLSNet) was proposed for rice and BLS leaf lesion recognition and segmentation based on a UNet network in semantic segmentation. An attention mechanism and multi-scale extraction integration were used in BLSNet to improve the accuracy of lesion segmentation. We compared the performance of the proposed network with that of DeepLabv3+ and UNet as benchmark models used in semantic segmentation. It was found that the proposed BLSNet model demonstrated higher segmentation and class accuracy. A preliminary investigation of BLS disease severity estimation was carried out based on our BLS segmentation results, and it was found that the proposed BLSNet method has strong potential to be a reliable automatic estimator of BLS disease severity.
KW  - rice bacterial leaf streak
KW  - leaf disease recognition
KW  - lesion segmentation
KW  - semantic segmentation
KW  - deep learning
KW  - convolutional neural network
KW  - disease severity estimation
DO  - 10.3390/agriculture11050420
TY  - EJOU
AU  - Bollas, Nikolaos
AU  - Kokinou, Eleni
AU  - Polychronos, Vassilios
TI  - Comparison of Sentinel-2 and UAV Multispectral Data for Use in Precision Agriculture: An Application from Northern Greece
T2  - Drones

PY  - 2021
VL  - 5
IS  - 2
SN  - 2504-446X

AB  - The scope of this work is to compare Sentinel-2 and unmanned aerial vehicles (UAV) imagery from northern Greece for use in precision agriculture by implementing statistical analysis and 2D visualization. Surveys took place on five dates with a difference between the sensing dates for the two techniques ranging from 1 to 4 days. Using the acquired images, we initially computed the maps of the Normalized Difference Vegetation Index (NDVI), then the values of this index for fifteen points and four polygons (areas). The UAV images were not resampled, aiming to compare both techniques based on their initial standards, as they are used by the farmers. Similarities between the two techniques are depicted on the trend of the NDVI means for both satellite and UAV techniques, considering the points and the polygons. The differences are in the a) mean NDVI values of the points and b) range of the NDVI values of the polygons probably because of the difference in the spatial resolution of the two techniques. The correlation coefficient of the NDVI values, considering both points and polygons, ranges between 83.5% and 98.26%. In conclusion, both techniques provide important information in precision agriculture depending on the spatial extent, resolution, and cost, as well as the requirements of the survey.
KW  - precision agriculture
KW  - NDVI
KW  - Sentinel-2
KW  - UAV
KW  - 2D visualization
DO  - 10.3390/drones5020035
TY  - EJOU
AU  - Mahmud, Md S.
AU  - Zahid, Azlan
AU  - He, Long
AU  - Martin, Phillip
TI  - Opportunities and Possibilities of Developing an Advanced Precision Spraying System for Tree Fruits
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 9
SN  - 1424-8220

AB  - Reducing risk from pesticide applications has been gaining serious attention in the last few decades due to the significant damage to human health, environment, and ecosystems. Pesticide applications are an essential part of current agriculture, enhancing cultivated crop productivity and quality and preventing losses of up to 45% of the world food supply. However, inappropriate and excessive use of pesticides is a major rising concern. Precision spraying addresses these concerns by precisely and efficiently applying pesticides to the target area and substantially reducing pesticide usage while maintaining efficacy at preventing crop losses. This review provides a systematic summary of current technologies used for precision spraying in tree fruits and highlights their potential, briefly discusses factors affecting spraying parameters, and concludes with possible solutions to reduce excessive agrochemical uses. We conclude there is a critical need for appropriate sensing techniques that can accurately detect the target. In addition, air jet velocity, travel speed, wind speed and direction, droplet size, and canopy characteristics need to be considered for successful droplet deposition by the spraying system. Assessment of terrain is important when field elevation has significant variability. Control of airflow during spraying is another important parameter that needs to be considered. Incorporation of these variables in precision spraying systems will optimize spray decisions and help reduce excessive agrochemical applications.
KW  - crop protection
KW  - canopy detection
KW  - canopy density
KW  - canopy volume
KW  - deep learning
KW  - machine vision
KW  - sensing
DO  - 10.3390/s21093262
TY  - EJOU
AU  - Jin, Xing
AU  - Tang, Ping
AU  - Zhang, Zheng
TI  - Sequence Image Datasets Construction via Deep Convolution Networks
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - Remote-sensing time-series datasets are significant for global change research and a better understanding of the Earth. However, remote-sensing acquisitions often provide sparse time series due to sensor resolution limitations and environmental factors such as cloud noise for optical data. Image transformation is the method that is often used to deal with this issue. This paper considers the deep convolution networks to learn the complex mapping between sequence images, called adaptive filter generation network (AdaFG), convolution long short-term memory network (CLSTM), and cycle-consistent generative adversarial network (CyGAN) for construction of sequence image datasets. AdaFG network uses a separable 1D convolution kernel instead of 2D kernels to capture the spatial characteristics of input sequence images and then is trained end-to-end using sequence images. CLSTM network can map between different images using the state information of multiple time-series images. CyGAN network can map an image from a source domain to a target domain without additional information. Our experiments, which were performed with unmanned aerial vehicle (UAV) and Landsat-8 datasets, show that the deep convolution networks are effective to produce high-quality time-series image datasets, and the data-driven deep convolution networks can better simulate complex and diverse nonlinear data information.
KW  - sequence image datasets
KW  - adaptive filter generation network
KW  - convolution long short-term memory network
KW  - cycle-consistent generative adversarial network
KW  - UAV dataset
KW  - Landsat-8 dataset
DO  - 10.3390/rs13091853
TY  - EJOU
AU  - Cheng, Zhenzhen
AU  - Qi, Lijun
AU  - Cheng, Yifan
TI  - Cherry Tree Crown Extraction from Natural Orchard Images with Complex Backgrounds
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 5
SN  - 2077-0472

AB  - Highly effective pesticide applications require a continual adjustment of the pesticide spray flow rate that attends to different canopy characterizations. Real-time image processing with rapid target detection and data-processing technologies is vital for precision pesticide application. However, the extant studies do not provide an efficient and reliable method of extracting individual trees with irregular tree-crown shapes and complicated backgrounds. This paper on our study proposes a Mahalanobis distance and conditional random field (CRF)-based segmentation model to extract cherry trees accurately in a natural orchard environment. This study computed Mahalanobis distance from the image’s color, brightness and location features to acquire an initial classification of the canopy and background. A CRF was then created by using the Mahalanobis distance calculations as unary potential energy and the Gaussian kernel function based on the image color and pixels distance as binary potential energy. Finally, the study completed image segmentation using mean-field approximation. The results show that the proposed method displays a higher accuracy rate than the traditional algorithms K-means and GrabCut algorithms and lower labeling and training costs than the deep learning algorithm DeepLabv3+, with 92.1%, 94.5% and 93.3% of the average P, R and F1-score, respectively. Moreover, experiments on datasets with different overlap conditions and image acquisition times, as well as in different years and seasons, show that this method performs well under complex background conditions, with an average F1-score higher than 87.7%.
KW  - agricultural computer vision
KW  - tree-crown segmentation
KW  - complex scene
KW  - natural orchard environment
DO  - 10.3390/agriculture11050431
TY  - EJOU
AU  - Bashir, Syed M.
AU  - Wang, Yi
TI  - Small Object Detection in Remote Sensing Images with Residual Feature Aggregation-Based Super-Resolution and Object Detector Network
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 9
SN  - 2072-4292

AB  - This paper deals with detecting small objects in remote sensing images from satellites or any aerial vehicle by utilizing the concept of image super-resolution for image resolution enhancement using a deep-learning-based detection method. This paper provides a rationale for image super-resolution for small objects by improving the current super-resolution (SR) framework by incorporating a cyclic generative adversarial network (GAN) and residual feature aggregation (RFA) to improve detection performance. The novelty of the method is threefold: first, a framework is proposed, independent of the final object detector used in research, i.e., YOLOv3 could be replaced with Faster R-CNN or any object detector to perform object detection; second, a residual feature aggregation network was used in the generator, which significantly improved the detection performance as the RFA network detected complex features; and third, the whole network was transformed into a cyclic GAN. The image super-resolution cyclic GAN with RFA and YOLO as the detection network is termed as SRCGAN-RFA-YOLO, which is compared with the detection accuracies of other methods. Rigorous experiments on both satellite images and aerial images (ISPRS Potsdam, VAID, and Draper Satellite Image Chronology datasets) were performed, and the results showed that the detection performance increased by using super-resolution methods for spatial resolution enhancement; for an IoU of 0.10, AP of 0.7867 was achieved for a scale factor of 16.
KW  - object detection in satellite images
KW  - image classification
KW  - vehicle detection
KW  - remote sensing
KW  - deep learning
KW  - generative adversarial networks
KW  - residual feature aggregation (RFA)
DO  - 10.3390/rs13091854
TY  - EJOU
AU  - Ioannou, Konstantinos
AU  - Myronidis, Dimitrios
TI  - Automatic Detection of Photovoltaic Farms Using Satellite Imagery and Convolutional Neural Networks
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 9
SN  - 2071-1050

AB  - The number of solar photovoltaic (PV) arrays in Greece has increased rapidly during the recent years. As a result, there is an increasing need for high quality updated information regarding the status of PV farms. This information includes the number of PV farms, power capacity and the energy generated. However, access to this data is obsolete, mainly due to the fact that there is a difficulty tracking PV investment status (from licensing to investment completion and energy production). This article presents a novel approach, which uses free access high resolution satellite imagery and a deep learning algorithm (a convolutional neural network—CNN) for the automatic detection of PV farms. Furthermore, in an effort to create an algorithm capable of generalizing better, all the current locations with installed PV farms (data provided from the Greek Energy Regulator Authority) in the Greek Territory (131,957 km2) were used. According to our knowledge this is the first time such an algorithm is used in order to determine the existence of PV farms and the results showed satisfying accuracy.
KW  - PV farms
KW  - deep learning
KW  - satellite imagery
KW  - CNN
KW  - automatic detection
DO  - 10.3390/su13095323
TY  - EJOU
AU  - Park, Gun
AU  - Lee, Jae H.
AU  - Yoon, Hyungchul
TI  - Semantic Structure from Motion for Railroad Bridges Using Deep Learning
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 10
SN  - 2076-3417

AB  - Current maintenance practices consume significant time, cost, and manpower. Thus, a new technique for maintenance is required. Construction information technologies, including building information modeling (BIM), have recently been applied to the field to carry out systematic and productive planning, design, construction, and maintenance. Although BIM is increasingly being applied to new structures, its application to existing structures has been limited. To apply BIM to an existing structure, a three-dimensional (3D) model of the structure that accurately represents the as-is status should be constructed and each structural component should be specified manually. This study proposes a method that constructs a 3D model and specifies the structural component automatically using photographic data with a camera installed on an unmanned aerial vehicle. This procedure is referred to as semantic structure from motion because it constructs a 3D point cloud model together with semantic information. A validation test was carried out on a railroad bridge to validate the performance of the proposed system. The average precision, intersection over union, and BF scores were 80.87%, 66.66%, and 56.33%, respectively. The proposed method could improve the current scan-to-BIM procedure by generating the as-is 3D point cloud model by specifying the structural component automatically.
KW  - semantic structure from motion
KW  - deep learning
KW  - structural component classification
KW  - structure from motion
KW  - scan to building information modeling
DO  - 10.3390/app11104332
TY  - EJOU
AU  - Mattivi, Pietro
AU  - Pappalardo, Salvatore E.
AU  - Nikolić, Nebojša
AU  - Mandolesi, Luca
AU  - Persichetti, Antonio
AU  - De Marchi, Massimo
AU  - Masin, Roberta
TI  - Can Commercial Low-Cost Drones and Open-Source GIS Technologies Be Suitable for Semi-Automatic Weed Mapping for Smart Farming? A Case Study in NE Italy
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 10
SN  - 2072-4292

AB  - Weed management is a crucial issue in agriculture, resulting in environmental in-field and off-field impacts. Within Agriculture 4.0, adoption of UASs combined with spatially explicit approaches may drastically reduce doses of herbicides, increasing sustainability in weed management. However, Agriculture 4.0 technologies are barely adopted in small-medium size farms. Recently, small and low-cost UASs, together with open-source software packages, may represent a low-cost spatially explicit system to map weed distribution in crop fields. The general aim is to map weed distribution by a low-cost UASs and a replicable workflow, completely based on open GIS software and algorithms: OpenDroneMap, QGIS, SAGA and OpenCV classification algorithms. Specific objectives are: (i) testing a low-cost UAS for weed mapping; (ii) assessing open-source packages for semi-automatic weed classification; (iii) performing a sustainable management scenario by prescription maps. Results showed high performances along the whole process: in orthomosaic generation at very high spatial resolution (0.01 m/pixel), in testing weed detection (Matthews Correlation Coefficient: 0.67–0.74), and in the production of prescription maps, reducing herbicide treatment to only 3.47% of the entire field. This study reveals the feasibility of low-cost UASs combined with open-source software, enabling a spatially explicit approach for weed management in small-medium size farmlands.
KW  - site-specific weed management
KW  - precision farming
KW  - OpenDroneMap
KW  - open photogrammetry
KW  - open-source mapping
DO  - 10.3390/rs13101869
TY  - EJOU
AU  - Xie, Xiuchuan
AU  - Yang, Tao
AU  - Ning, Yajia
AU  - Zhang, Fangbing
AU  - Zhang, Yanning
TI  - A Monocular Visual Odometry Method Based on Virtual-Real Hybrid Map in Low-Texture Outdoor Environment
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 10
SN  - 1424-8220

AB  - With the extensive application of robots, such as unmanned aerial vehicle (UAV) in exploring unknown environments, visual odometry (VO) algorithms have played an increasingly important role. The environments are diverse, not always textured, or low-textured with insufficient features, making them challenging for mainstream VO. However, for low-texture environment, due to the structural characteristics of man-made scene, the lines are usually abundant. In this paper, we propose a virtual-real hybrid map based monocular visual odometry algorithm. The core idea is that we reprocess line segment features to generate the virtual intersection matching points, which can be used to build the virtual map. Introducing virtual map can improve the stability of the visual odometry algorithm in low-texture environment. Specifically, we first combine unparallel matched line segments to generate virtual intersection matching points, then, based on the virtual intersection matching points, we triangulate to get a virtual map, combined with the real map built upon the ordinary point features to form a virtual-real hybrid 3D map. Finally, using the hybrid map, the continuous camera pose estimation can be solved. Extensive experimental results have demonstrated the robustness and effectiveness of the proposed method in various low-texture scenes.
KW  - visual odometry
KW  - simultaneous localization and mapping
KW  - low-texture environment
KW  - line segments
DO  - 10.3390/s21103394
TY  - EJOU
AU  - Quan, Longzhe
AU  - Wu, Bing
AU  - Mao, Shouren
AU  - Yang, Chunjie
AU  - Li, Hengda
TI  - An Instance Segmentation-Based Method to Obtain the Leaf Age and Plant Centre of Weeds in Complex Field Environments
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 10
SN  - 1424-8220

AB  - Leaf age and plant centre are important phenotypic information of weeds, and accurate identification of them plays an important role in understanding the morphological structure of weeds, guiding precise targeted spraying and reducing the use of herbicides. In this work, a weed segmentation method based on BlendMask is proposed to obtain the phenotypic information of weeds under complex field conditions. This study collected images from different angles (front, side, and top views) of three kinds of weeds (Solanum nigrum, barnyard grass (Echinochloa crus-galli), and Abutilon theophrasti Medicus) in a maize field. Two datasets (with and without data enhancement) and two backbone networks (ResNet50 and ResNet101) were replaced to improve model performance. Finally, seven evaluation indicators are used to evaluate the segmentation results of the model under different angles. The results indicated that data enhancement and ResNet101 as the backbone network could enhance the model performance. The F1 value of the plant centre is 0.9330, and the recognition accuracy of leaf age can reach 0.957. The mIOU value of the top view is 0.642. Therefore, deep learning methods can effectively identify weed leaf age and plant centre, which is of great significance for variable spraying.
KW  - weeds
KW  - phenotype
KW  - deep learning
KW  - image segmentation
DO  - 10.3390/s21103389
TY  - EJOU
AU  - Li, Jing
AU  - Liu, Yong
AU  - Zhang, Yindan
AU  - Zhang, Yang
TI  - Cascaded Attention DenseUNet (CADUNet) for Road Extraction from Very-High-Resolution Images
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 5
SN  - 2220-9964

AB  - The use of very-high-resolution images to extract urban, suburban and rural roads has important application value. However, it is still a problem to effectively extract the road area occluded by roadside tree canopy or high-rise buildings to maintain the integrity of the extracted road area, the smoothness of the sideline and the connectivity of the road network. This paper proposes an innovative Cascaded Attention DenseUNet (CADUNet) semantic segmentation model by embedding two attention modules, such as global attention and core attention modules, in the DenseUNet framework. First, a set of cascaded global attention modules are introduced to obtain the contextual information of the road; secondly, a set of cascaded core attention modules are embedded to ensure that the road information is transmitted to the greatest extent among the dense blocks in the network, and further assist the global attention module in acquiring multi-scale road information, thereby improving the connectivity of the road network while restoring the integrity of the road area shaded by the tree canopy and high-rise buildings. Based on binary cross entropy, an adaptive loss function is proposed for network parameter tuning. Experiments on the Massachusetts road dataset and the DeepGlobe-CVPR 2018 road dataset show that this semantic segmentation model can effectively extract the road area shaded by tree canopy and improve the connectivity of the road network.
KW  - deep learning
KW  - road
KW  - DenseUNet
KW  - attention module
KW  - semantic segmentation
KW  - remote sensing
DO  - 10.3390/ijgi10050329
TY  - EJOU
AU  - Jo, Yongwon
AU  - Lee, Soobin
AU  - Lee, Youngjae
AU  - Kahng, Hyungu
AU  - Park, Seonghun
AU  - Bae, Seounghun
AU  - Kim, Minkwan
AU  - Han, Sungwon
AU  - Kim, Seoungbum
TI  - Semantic Segmentation of Cabbage in the South Korea Highlands with Images by Unmanned Aerial Vehicles
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 10
SN  - 2076-3417

AB  - Identifying agricultural fields that grow cabbage in the highlands of South Korea is critical for accurate crop yield estimation. Only grown for a limited time during the summer, highland cabbage accounts for a significant proportion of South Korea’s annual cabbage production. Thus, it has a profound effect on the formation of cabbage prices. Traditionally, labor-extensive and time-consuming field surveys are manually carried out to derive agricultural field maps of the highlands. Recently, high-resolution overhead images of the highlands have become readily available with the rapid development of unmanned aerial vehicles (UAV) and remote sensing technology. In addition, deep learning-based semantic segmentation models have quickly advanced by recent improvements in algorithms and computational resources. In this study, we propose a semantic segmentation framework based on state-of-the-art deep learning techniques to automate the process of identifying cabbage cultivation fields. We operated UAVs and collected 2010 multispectral images under different spatiotemporal conditions to measure how well semantic segmentation models generalize. Next, we manually labeled these images at a pixel-level to obtain ground truth labels for training. Our results demonstrate that our framework performs well in detecting cabbage fields not only in areas included in the training data but also in unseen areas not included in the training data. Moreover, we analyzed the effects of infrared wavelengths on the performance of identifying cabbage fields. Based on the results of our framework, we expect agricultural officials to reduce time and manpower when identifying information about highlands cabbage fields by replacing field surveys.
KW  - land-cover classification
KW  - semantic segmentation
KW  - unmanned aerial vehicles
DO  - 10.3390/app11104493
TY  - EJOU
AU  - Nguyen, Dinh D.
AU  - Rohacs, Jozsef
AU  - Rohacs, Daniel
TI  - Autonomous Flight Trajectory Control System for Drones in Smart City Traffic Management
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 5
SN  - 2220-9964

AB  - With the exponential growth of numerous drone operations ranging from infrastructure monitoring to even package delivery services, the integration of UAS in the smart city transportation systems is an actual task that requires radically new, sustainable (safe, secure, with minimum environmental impact and life cycle cost) solutions. The primary objective of this proposed option is the definition of routes as desired and commanded trajectories and their autonomous execution. The airspace structure and fixed routes are given in the global GPS reference system with supporting GIS mapping. The concept application requires a series of further studies and solutions as drone trajectory (or corridor) following by an autonomous trajectory tracking control system, coupled with autonomous conflict detection, resolution, safe drone following, and formation flight options. The second part of the paper introduces such possible models and shows some results of their verification tests. Drones will be connected with the agency, designed trajectories to support them with factual information on trajectories and corridors. While the agency will use trajectory elements to design fixed or desired trajectories, drones may use the conventional GPS, infrared, acoustic, and visual sensors for positioning and advanced navigation. The accuracy can be improved by unique markers integrated into the infrastructure.
KW  - autonomous drones
KW  - UAV
KW  - autonomous flight trajectory
KW  - inverse motion simulation
KW  - smart city integration
DO  - 10.3390/ijgi10050338
TY  - EJOU
AU  - Wang, Xun
AU  - Cai, Libing
AU  - Kong, Longxing
AU  - Wang, Binfeng
AU  - Huang, Shaohua
AU  - Lin, Chengdi
TI  - Path Following and Obstacle Avoidance for Unmanned Aerial Vehicles Using a Virtual-Force-Based Guidance Law
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 10
SN  - 2076-3417

AB  - This paper presents a virtual-force-based guidance law (VFGL) for path following and obstacle avoidance in unmanned aerial vehicles. First, a virtual spring force and a virtual drag force are designed for straight-line following; then, the dynamic of the cross-track-error is equivalent to a spring mass system, which is easy to tune to acquire stability and non-overshoot convergence. Secondly, an additional virtual centripetal force is designed to counteract the influence of the curvature of the planned path so that the guidance law can accurately track a curve with a time-varying curvature. Thirdly, an extra virtual repulsive force is designed directly according to the sensor inputs; the virtual repulsive force pushes the vehicle away to move around obstacles. The use of artificial physics means the guidance law is founded on solid physical theory and is computationally simple. The physical meanings of the parameters are definite, and the VFGL has a large parameter adaptation. These make the guidance law easy to tune in application. Both the numerical and hardware-in-the-loop simulation results demonstrated the effectiveness of the proposed guidance law for path following and obstacle avoidance in unmanned aerial vehicles.
KW  - unmanned aerial vehicles
KW  - path following
KW  - obstacle avoidance
KW  - virtual-force-based guidance law
DO  - 10.3390/app11104618
TY  - EJOU
AU  - Wang, Lin
AU  - Zhou, Yuzhen
AU  - Hu, Qiao
AU  - Tang, Zhenghong
AU  - Ge, Yufeng
AU  - Smith, Adam
AU  - Awada, Tala
AU  - Shi, Yeyin
TI  - Early Detection of Encroaching Woody Juniperus virginiana and Its Classification in Multi-Species Forest Using UAS Imagery and Semantic Segmentation Algorithms
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 10
SN  - 2072-4292

AB  - Woody plant encroachment into grasslands ecosystems causes significantly ecological destruction and economic losses. Effective and efficient management largely benefits from accurate and timely detection of encroaching species at an early development stage. Recent advances in unmanned aircraft systems (UAS) enabled easier access to ultra-high spatial resolution images at a centimeter level, together with the latest machine learning based image segmentation algorithms, making it possible to detect small-sized individuals of target species at early development stage and identify them when mixed with other species. However, few studies have investigated the optimal practical spatial resolution of early encroaching species detection. Hence, we investigated the performance of four popular semantic segmentation algorithms (decision tree, DT; random forest, RF; AlexNet; and ResNet) on a multi-species forest classification case with UAS-collected RGB images in original and down-sampled coarser spatial resolutions. The objective of this study was to explore the optimal segmentation algorithm and spatial resolution for eastern redcedar (Juniperus virginiana, ERC) early detection and its classification within a multi-species forest context. To be specific, firstly, we implemented and compared the performance of the four semantic segmentation algorithms with images in the original spatial resolution (0.694 cm). The highest overall accuracy was 0.918 achieved by ResNet with a mean interaction over union at 85.0%. Secondly, we evaluated the performance of ResNet algorithm with images in down-sampled spatial resolutions (1 cm to 5 cm with 0.5 cm interval). When applied on the down-sampled images, ERC segmentation performance decreased with decreasing spatial resolution, especially for those images coarser than 3 cm spatial resolution. The UAS together with the state-of-the-art semantic segmentation algorithms provides a promising tool for early-stage detection and localization of ERC and the development of effective management strategies for mixed-species forest management.
KW  - forest classification
KW  - aggressive native species
KW  - invasive species
KW  - biodiversity
KW  - remote sensing
KW  - UAV
KW  - machine learning
KW  - deep learning
DO  - 10.3390/rs13101975
TY  - EJOU
AU  - Hamdi, Slim
AU  - Bouindour, Samir
AU  - Snoussi, Hichem
AU  - Wang, Tian
AU  - Abid, Mohamed
TI  - End-to-End Deep One-Class Learning for Anomaly Detection in UAV Video Stream
T2  - Journal of Imaging

PY  - 2021
VL  - 7
IS  - 5
SN  - 2313-433X

AB  - In recent years, the use of drones for surveillance tasks has been on the rise worldwide. However, in the context of anomaly detection, only normal events are available for the learning process. Therefore, the implementation of a generative learning method in an unsupervised mode to solve this problem becomes fundamental. In this context, we propose a new end-to-end architecture capable of generating optical flow images from original UAV images and extracting compact spatio-temporal characteristics for anomaly detection purposes. It is designed with a custom loss function as a sum of three terms, the reconstruction loss (Rl), the generation loss (Gl) and the compactness loss (Cl) to ensure an efficient classification of the “deep-one” class. In addition, we propose to minimize the effect of UAV motion in video processing by applying background subtraction on optical flow images. We tested our method on very complex datasets called the mini-drone video dataset, and obtained results surpassing existing techniques’ performances with an AUC of 85.3.
KW  - anomaly detection
KW  - UAV videos
KW  - deep one-class
DO  - 10.3390/jimaging7050090
TY  - EJOU
AU  - Liu, Chuanyang
AU  - Wu, Yiquan
AU  - Liu, Jingjing
AU  - Sun, Zuo
AU  - Xu, Huajie
TI  - Insulator Faults Detection in Aerial Images from High-Voltage Transmission Lines Based on Deep Learning Model
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 10
SN  - 2076-3417

AB  - Insulator fault detection is one of the essential tasks for high-voltage transmission lines’ intelligent inspection. In this study, a modified model based on You Only Look Once (YOLO) is proposed for detecting insulator faults in aerial images with a complex background. Firstly, aerial images with one fault or multiple faults are collected in diverse scenes, and then a novel dataset is established. Secondly, to increase feature reuse and propagation in the low-resolution feature layers, a Cross Stage Partial Dense YOLO (CSPD-YOLO) model is proposed based on YOLO-v3 and the Cross Stage Partial Network. The feature pyramid network and improved loss function are adopted to the CSPD-YOLO model, improving the accuracy of insulator fault detection. Finally, the proposed CSPD-YOLO model and compared models are trained and tested on the established dataset. The average precision of CSPD-YOLO model is 4.9% and 1.8% higher than that of YOLO-v3 and YOLO-v4, and the running time of CSPD-YOLO (0.011 s) model is slightly longer than that of YOLO-v3 (0.01 s) and YOLO-v4 (0.01 s). Compared with the excellent object detection models YOLO-v3 and YOLO-v4, the experimental results and analysis demonstrate that the proposed CSPD-YOLO model performs better in insulator fault detection from high-voltage transmission lines with a complex background.
KW  - fault detection
KW  - aerial image
KW  - complex background
KW  - deep learning
KW  - image processing
KW  - intelligent inspection
DO  - 10.3390/app11104647
TY  - EJOU
AU  - Tullu, Abera
AU  - Endale, Bedada
AU  - Wondosen, Assefinew
AU  - Hwang, Ho-Yon
TI  - Machine Learning Approach to Real-Time 3D Path Planning for Autonomous Navigation of Unmanned Aerial Vehicle
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 10
SN  - 2076-3417

AB  - The need for civilian use of Unmanned Aerial Vehicles (UAVs) has drastically increased in recent years. Their potential applications for civilian use include door-to-door package delivery, law enforcement, first aid, and emergency services in urban areas, which put the UAVs into obstacle collision risk. Therefore, UAVs are required to be equipped with sensors so as to acquire Artificial Intelligence (AI) to avoid potential risks during mission execution. The AI comes with intensive training of an on-board machine that is responsible to autonomously navigate the UAV. The training enables the UAV to develop humanoid perception of the environment it is to be navigating in. During the mission, this perception detects and localizes objects in the environment. It is based on this AI that this work proposes a real-time three-dimensional (3D) path planner that maneuvers the UAV towards destination through obstacle-free path. The proposed path planner has a heuristic sense of A⋆ algorithm, but requires no frontier nodes to be stored in a memory unlike A⋆. The planner relies on relative locations of detected objects (obstacles) and determines collision-free paths. This path planner is light-weight and hence a fast guidance method for real-time purposes. Its performance efficiency is proved through rigorous Software-In-The-Loop (SITL) simulations in constrained-environment and preliminary real flight tests.
KW  - vision-based navigation
KW  - cluttered environment
KW  - three-dimensional path planner
KW  - obstacle avoidance
KW  - machine learning
DO  - 10.3390/app11104706
TY  - EJOU
AU  - Mores, Antonia
AU  - Borrelli, Grazia M.
AU  - Laidò, Giovanni
AU  - Petruzzino, Giuseppe
AU  - Pecchioni, Nicola
AU  - Amoroso, Luca G.
AU  - Desiderio, Francesca
AU  - Mazzucotelli, Elisabetta
AU  - Mastrangelo, Anna M.
AU  - Marone, Daniela
TI  - Genomic Approaches to Identify Molecular Bases of Crop Resistance to Diseases and to Develop Future Breeding Strategies
T2  - International Journal of Molecular Sciences

PY  - 2021
VL  - 22
IS  - 11
SN  - 1422-0067

AB  - Plant diseases are responsible for substantial crop losses each year and affect food security and agricultural sustainability. The improvement of crop resistance to pathogens through breeding represents an environmentally sound method for managing disease and minimizing these losses. The challenge is to breed varieties with a stable and broad-spectrum resistance. Different approaches, from markers to recent genomic and ‘post-genomic era’ technologies, will be reviewed in order to contribute to a better understanding of the complexity of host–pathogen interactions and genes, including those with small phenotypic effects and mechanisms that underlie resistance. An efficient combination of these approaches is herein proposed as the basis to develop a successful breeding strategy to obtain resistant crop varieties that yield higher in increasing disease scenarios.
KW  - crop
KW  - disease resistance
KW  - genes
KW  - marker-assisted selection
KW  - meta-analysis
KW  - genomic selection
KW  - effectoromics
KW  - new breeding technologies
DO  - 10.3390/ijms22115423
