TY  - EJOU
AU  - Park, Minsoo
AU  - Tran, Dai Q.
AU  - Jung, Daekyo
AU  - Park, Seunghee
TI  - Wildfire-Detection Method Using DenseNet and CycleGAN Data Augmentation-Based Remote Camera Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - To minimize the damage caused by wildfires, a deep learning-based wildfire-detection technology that extracts features and patterns from surveillance camera images was developed. However, many studies related to wildfire-image classification based on deep learning have highlighted the problem of data imbalance between wildfire-image data and forest-image data. This data imbalance causes model performance degradation. In this study, wildfire images were generated using a cycle-consistent generative adversarial network (CycleGAN) to eliminate data imbalances. In addition, a densely-connected-convolutional-networks-based (DenseNet-based) framework was proposed and its performance was compared with pre-trained models. While training with a train set containing an image generated by a GAN in the proposed DenseNet-based model, the best performance result value was realized among the models with an accuracy of 98.27% and an F1 score of 98.16, obtained using the test dataset. Finally, this trained model was applied to high-quality drone images of wildfires. The experimental results showed that the proposed framework demonstrated high wildfire-detection accuracy.
KW  - wildfire detection
KW  - convolutional neural networks
KW  - densenet
KW  - generative adversarial networks
KW  - CycleGAN
KW  - data augmentation
DO  - 10.3390/rs12223715
ER  -
TY  - EJOU
AU  - Sánchez-Aparicio, María
AU  - Del Pozo, Susana
AU  - Martín-Jiménez, Jose A.
AU  - González-González, Enrique
AU  - Andrés-Anaya, Paula
AU  - Lagüela, Susana
TI  - Influence of LiDAR Point Cloud Density in the Geometric Characterization of Rooftops for Solar Photovoltaic Studies in Cities
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - The use of LiDAR (Light Detection and Ranging) data for the definition of the 3D geometry of roofs has been widely exploited in recent years for its posterior application in the field of solar energy. Point density in LiDAR data is an essential characteristic to be taken into account for the accurate estimation of roof geometry: area, orientation and slope. This paper presents a comparative study between LiDAR data of different point densities: 0.5, 1, 2 and 14 points/m2 for the measurement of the area of roofs of residential and industrial buildings. The data used for the study are the LiDAR data freely available by the Spanish Institute of Geography (IGN), which is offered according to the INSPIRE Directive. The results obtained show different behaviors for roofs with an area below and over 200 m2. While the use of low-density point clouds (0.5 point/m2) presents significant errors in the estimation of the area, the use of point clouds with higher density (1 or 2 points/m2) implies a great improvement in the area results, with no significant difference among them. The use of high-density point clouds (14 points/m2) also implies an improvement of the results, although the accuracy does not increase in the same ratio as the increase in density regarding 1 or 2 points/m2. Thus, the conclusion reached is that the geometrical characterization of roofs requires data acquisition with point density of 1 or 2 points/m2, and that higher point densities do not improve the results with the same intensity as they increase computation time.
KW  - LiDAR
KW  - density
KW  - point cloud
KW  - roofs
KW  - geometry
KW  - photovoltaic energy
KW  - solar potential
DO  - 10.3390/rs12223726
ER  -
TY  - EJOU
AU  - Stuparu, Delia-Georgiana
AU  - Ciobanu, Radu-Ioan
AU  - Dobre, Ciprian
TI  - Vehicle Detection in Overhead Satellite Images Using a One-Stage Object Detection Model
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - In order to improve the traffic in large cities and to avoid congestion, advanced methods of detecting and predicting vehicle behaviour are needed. Such methods require complex information regarding the number of vehicles on the roads, their positions, directions, etc. One way to obtain this information is by analyzing overhead images collected by satellites or drones, and extracting information from them through intelligent machine learning models. Thus, in this paper we propose and present a one-stage object detection model for finding vehicles in satellite images using the RetinaNet architecture and the Cars Overhead With Context dataset. By analyzing the results obtained by the proposed model, we show that it has a very good vehicle detection accuracy and a very low detection time, which shows that it can be employed to successfully extract data from real-time satellite or drone data.
KW  - object detection model
KW  - satellite images
KW  - vehicle detection
KW  - smart city
DO  - 10.3390/s20226485
ER  -
TY  - EJOU
AU  - Combey, Théo
AU  - Loison, António
AU  - Faucher, Maxime
AU  - Hajri, Hatem
TI  - Probabilistic Jacobian-Based Saliency Maps Attacks
T2  - Machine Learning and Knowledge Extraction

PY  - 2020
VL  - 2
IS  - 4
SN  - 2504-4990

AB  - Neural network classifiers (NNCs) are known to be vulnerable to malicious adversarial perturbations of inputs including those modifying a small fraction of the input features named sparse or L0 attacks. Effective and fast L0 attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are practical to fool NNCs but also to improve their robustness. In this paper, we show that penalising saliency maps of JSMA by the output probabilities and the input features of the NNC leads to more powerful attack algorithms that better take into account each input&rsquo;s characteristics. This leads us to introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), and demonstrate through a variety of white-box and black-box experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they are both significantly faster and more efficient than the original targeted and non-targeted versions of JSMA. Experiments also demonstrate, in some cases, very competitive results of our attacks in comparison with the Carlini-Wagner (CW) L0 attack, while remaining, like JSMA, significantly faster (WJSMA and TJSMA are more than 50 times faster than CW L0 on CIFAR-10). Therefore, our new attacks provide good trade-offs between JSMA and CW for L0 real-time adversarial testing on datasets such as the ones previously cited.
KW  - Jacobian-based Saliency Map
KW  - adversarial attacks
KW  - deep neural network classifiers
KW  - MNIST
KW  - CIFAR-10
KW  - GTSRB
DO  - 10.3390/make2040030
ER  -
TY  - EJOU
AU  - Thakker, Dhavalkumar
AU  - Mishra, Bhupesh K.
AU  - Abdullatif, Amr
AU  - Mazumdar, Suvodeep
AU  - Simpson, Sydney
TI  - Explainable Artificial Intelligence for Developing Smart Cities Solutions
T2  - Smart Cities

PY  - 2020
VL  - 3
IS  - 4
SN  - 2624-6511

AB  - Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions, Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative training datasets and features engineering and less on the available domain expertise. We argue that such an approach to solution development makes the outcome of solutions less explainable, i.e., it is often not possible to explain the results of the model. There is a growing concern among policymakers in cities with this lack of explainability of AI solutions, and this is considered a major hindrance in the wider acceptability and trust in such AI-based solutions. In this work, we survey the concept of &lsquo;explainable deep learning&rsquo; as a subset of the &lsquo;explainable AI&rsquo; problem and propose a new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring application in the context of a European Commission-funded project. Monitoring of gullies and drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc., and building a DL-based classifier to detect such objects and classify blockages based on the presence and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier, the DL component detects object presence and coverage level and semantic rules designed with close consultation with experts carry out the classification. By using the expert knowledge in the flooding context, our hybrid classifier provides the flexibility on categorising the image using objects and their coverage relationships. The experimental results demonstrated with a real-world use case showed that this hybrid approach of image classification has on average 11% improvement (F-Measure) in image classification performance compared to DL-only classifier. It also has the distinct advantage of integrating experts&rsquo; knowledge on defining the decision-making rules to represent the complex circumstances and using such knowledge to explain the results.
KW  - explainable AI
KW  - multi-object
KW  - coverage detection
KW  - semantic Rules
KW  - hybrid image classification
KW  - flood monitoring
DO  - 10.3390/smartcities3040065
ER  -
TY  - EJOU
AU  - Lu, Liang
AU  - Redondo, Carlos
AU  - Campoy, Pascual
TI  - Optimal Frontier-Based Autonomous Exploration in Unconstructed Environment Using RGB-D Sensor
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - Aerial robots are widely used in search and rescue applications because of their small size and high maneuvering. However, designing an autonomous exploration algorithm is still a challenging and open task, because of the limited payload and computing resources on board UAVs. This paper presents an autonomous exploration algorithm for the aerial robots that shows several improvements for being used in the search and rescue tasks. First of all, an RGB-D sensor is used to receive information from the environment and the OctoMap divides the environment into obstacles, free and unknown spaces. Then, a clustering algorithm is used to filter the frontiers extracted from the OctoMap, and an information gain based cost function is applied to choose the optimal frontier. At last, the feasible path is given by A* path planner and a safe corridor generation algorithm. The proposed algorithm has been tested and compared with baseline algorithms in three different environments with the map resolutions of 0.2 m, and 0.3 m. The experimental results show that the proposed algorithm has a shorter exploration path and can save more exploration time when compared with the state of the art. The algorithm has also been validated in the real flight experiments.
KW  - aerial robot
KW  - autonomous exploration
KW  - information gain
KW  - RGB-D sensor
DO  - 10.3390/s20226507
ER  -
TY  - EJOU
AU  - Park, Jungsu
AU  - Park, Jae-Hyeoung
AU  - Choi, June-Seok
AU  - Joo, Jin C.
AU  - Park, Kihak
AU  - Yoon, Hyeon C.
AU  - Park, Cheol Y.
AU  - Lee, Woo H.
AU  - Heo, Tae-Young
TI  - Ensemble Model Development for the Prediction of a Disaster Index in Water Treatment Systems
T2  - Water

PY  - 2020
VL  - 12
IS  - 11
SN  - 2073-4441

AB  - The quantitative analysis of the disaster effect on water supply systems can provide useful information for water supply system management. In this study, a total disaster index (TDI) was developed using open-source public data in 419 water treatment plants in Korea with 23 input variables. The TDI quantifies the possible effects or damage caused by three major disasters (typhoons, heavy rain, and earthquakes) on water supply systems. The four components (regional factor, risk factor, urgency factor, and response and recovery factor) were calculated using input variables to determine the disaster index (DI) of each disaster. The weight of the input variables was determined using principal component analysis (PCA), and the weights of the DI of three natural disasters and four components used to calculate the TDI were determined by the analytical hierarchy process (AHP). Specifically, two ensemble machine learning models, random forest (RF) and XGBoost (XGB), were used to develop models to predict the TDI. Both models predicted the TDI with the coefficient of determination and root-mean-square error-observations standard deviation ratio of 0.8435 and 0.3957 for the RF model and 0.8629 and 0.3703 for the XGB model, respectively. The relative importance analysis suggests that the number of input variables can be minimized, which improves the models&rsquo; practical applicability.
KW  - disaster management
KW  - ensemble model
KW  - machine learning
KW  - water supply
KW  - water treatment system
DO  - 10.3390/w12113195
ER  -
TY  - EJOU
AU  - Kim, Jung J.
AU  - Kim, Ah-Ram
AU  - Lee, Seong-Won
TI  - Artificial Neural Network-Based Automated Crack Detection and Analysis for the Inspection of Concrete Structures
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 22
SN  - 2076-3417

AB  - The damage investigation and inspection methods for infrastructures performed in small-scale (type III) facilities usually involve a visual examination by an inspector using surveying tools (e.g., cracking, crack microscope, etc.) in the field. These methods can interfere with the subjectivity of the inspector, which may reduce the objectivity and reliability of the record. Therefore, a new image analysis technique is needed to automatically detect cracks and analyze the characteristics of the cracks objectively. In this study, an image analysis technique using deep learning is developed to detect cracks and analyze characteristics (e.g., length, and width) in images for small-scale facilities. Three stages of image processing pipeline are proposed to obtain crack detection and its characteristics. In the first and second stages, two-dimensional convolutional neural networks are used for crack image detection (e.g., classification and segmentation). Based on convolution neural network for the detection, hierarchical feature learning architecture is applied into our deep learning network. After deep learning-based detection, in the third stage, thinning and tracking algorithms are applied to analyze length and width of crack in the image. The performance of the proposed method was tested using various crack images with label and the results showed good performance of crack detection and its measurement.
KW  - concrete crack
KW  - concrete structure
KW  - artificial neural network
KW  - convolution neural network
DO  - 10.3390/app10228105
ER  -
TY  - EJOU
AU  - Zhang, Peng
AU  - Du, Peijun
AU  - Lin, Cong
AU  - Wang, Xin
AU  - Li, Erzhu
AU  - Xue, Zhaohui
AU  - Bai, Xuyu
TI  - A Hybrid Attention-Aware Fusion Network (HAFNet) for Building Extraction from High-Resolution Imagery and LiDAR Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Automated extraction of buildings from earth observation (EO) data has long been a fundamental but challenging research topic. Combining data from different modalities (e.g., high-resolution imagery (HRI) and light detection and ranging (LiDAR) data) has shown great potential in building extraction. Recent studies have examined the role that deep learning (DL) could play in both multimodal data fusion and urban object extraction. However, DL-based multimodal fusion networks may encounter the following limitations: (1) the individual modal and cross-modal features, which we consider both useful and important for final prediction, cannot be sufficiently learned and utilized and (2) the multimodal features are fused by a simple summation or concatenation, which appears ambiguous in selecting cross-modal complementary information. In this paper, we address these two limitations by proposing a hybrid attention-aware fusion network (HAFNet) for building extraction. It consists of RGB-specific, digital surface model (DSM)-specific, and cross-modal streams to sufficiently learn and utilize both individual modal and cross-modal features. Furthermore, an attention-aware multimodal fusion block (Att-MFBlock) was introduced to overcome the fusion problem by adaptively selecting and combining complementary features from each modality. Extensive experiments conducted on two publicly available datasets demonstrated the effectiveness of the proposed HAFNet for building extraction.
KW  - building extraction
KW  - high-resolution imagery (HRI)
KW  - light detection and ranging (LiDAR)
KW  - multimodal data fusion
KW  - deep learning
KW  - attention mechanism
DO  - 10.3390/rs12223764
ER  -
TY  - EJOU
AU  - Liu, Huan
AU  - Li, Shiyong
AU  - Sun, Wei
TI  - Resource Allocation for Edge Computing without Using Cloud Center in Smart Home Environment: A Pricing Approach
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - Recently, more and more smart homes have become one of important parts of home infrastructure. However, most of the smart home applications are not interconnected and remain isolated. They use the cloud center as the control platform, which increases the risk of link congestion and data security. Thus, in the future, smart homes based on edge computing without using cloud center become an important research area. In this paper, we assume that all applications in a smart home environment are composed of edge nodes and users. In order to maximize the utility of users, we assume that all users and edge nodes are placed in a market and formulate a pricing resource allocation model with utility maximization. We apply the Lagrangian method to analyze the model, so an edge node (provider in the market) allocates its resources to a user (customer in the market) based on the prices of resources and the utility related to the preference of users. To obtain the optimal resource allocation, we propose a pricing-based resource allocation algorithm by using low-pass filtering scheme and conform that the proposed algorithm can achieve an optimum within reasonable convergence times through some numerical examples.
KW  - edge computing
KW  - smart homes
KW  - resource pricing
KW  - resource allocation
KW  - utility optimization
DO  - 10.3390/s20226545
ER  -
TY  - EJOU
AU  - Shawon, Ashifur R.
AU  - Ko, Jonghan
AU  - Jeong, Seungtaek
AU  - Shin, Taehwan
AU  - Lee, Kyung D.
AU  - Shim, Sang I.
TI  - Two-Dimensional Simulation of Barley Growth and Yield Using a Model Integrated with Remote-Controlled Aerial Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - It is important to be able to predict the yield and monitor the growth conditions of crops in the field to increase productivity. One way to assess field-based geospatial crop productivity is by integrating a crop model with a remote-controlled aerial system (RAS). The objective of this study was to simulate spatiotemporal barley growth and yield based on the development of a crop-modeling system integrated with RAS-based remote sensing images. We performed field experiments to obtain ground truth data and RAS images of crop growth conditions and yields at Chonnam National University (CNU), Gwangju, South Korea in 2018, and at Gyeongsang National University (GNU), Jinju, South Gyeongsang, South Korea in 2018 and 2019. In model calibration, there was no significant difference (p = 0.12) between the simulated barley yields and measured yields, based on a two-sample t-test at CNU in 2018. In model validation, there was no significant difference between simulated yields and measured yields at p = 0.98 and 0.76, according to two-sample t-tests at GNU in 2018 and 2019, respectively. The remote sensing-integrated crop model accurately reproduced geospatial variations in barley yield and growth variables. The results demonstrate that the crop modeling approach is useful for monitoring at-field barley conditions.
KW  - barley
KW  - crop model
KW  - integration
KW  - RAS
KW  - remote sensing
KW  - yield
DO  - 10.3390/rs12223766
ER  -
TY  - EJOU
AU  - Pérez, Javier
AU  - Guardiola, Jose-Luis
AU  - Perez, Alberto J.
AU  - Perez-Cortes, Juan-Carlos
TI  - Probabilistic Evaluation of 3D Surfaces Using Statistical Shape Models (SSM)
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - Inspecting a 3D object which shape has elastic manufacturing tolerances in order to find defects is a challenging and time-consuming task. This task usually involves humans, either in the specification stage followed by some automatic measurements, or in other points along the process. Even when a detailed inspection is performed, the measurements are limited to a few dimensions instead of a complete examination of the object. In this work, a probabilistic method to evaluate 3D surfaces is presented. This algorithm relies on a training stage to learn the shape of the object building a statistical shape model. Making use of this model, any inspected object can be evaluated obtaining a probability that the whole object or any of its dimensions are compatible with the model, thus allowing to easily find defective objects. Results in simulated and real environments are presented and compared to two different alternatives.
KW  - 3D surface evaluation
KW  - 3D reconstruction
KW  - statistical shape model
KW  - quality assessment
KW  - 3D metrics
DO  - 10.3390/s20226554
ER  -
TY  - EJOU
AU  - Tassi, Andrea
AU  - Vizzari, Marco
TI  - Object-Oriented LULC Classification in Google Earth Engine Combining SNIC, GLCM, and Machine Learning Algorithms
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Google Earth Engine (GEE) is a versatile cloud platform in which pixel-based (PB) and object-oriented (OO) Land Use&ndash;Land Cover (LULC) classification approaches can be implemented, thanks to the availability of the many state-of-art functions comprising various Machine Learning (ML) algorithms. OO approaches, including both object segmentation and object textural analysis, are still not common in the GEE environment, probably due to the difficulties existing in concatenating the proper functions, and in tuning the various parameters to overcome the GEE computational limits. In this context, this work is aimed at developing and testing an OO classification approach combining the Simple Non-Iterative Clustering (SNIC) algorithm to identify spatial clusters, the Gray-Level Co-occurrence Matrix (GLCM) to calculate cluster textural indices, and two ML algorithms (Random Forest (RF) or Support Vector Machine (SVM)) to perform the final classification. A Principal Components Analysis (PCA) is applied to the main seven GLCM indices to synthesize in one band the textural information used for the OO classification. The proposed approach is implemented in a user-friendly, freely available GEE code useful to perform the OO classification, tuning various parameters (e.g., choose the input bands, select the classification algorithm, test various segmentation scales) and compare it with a PB approach. The accuracy of OO and PB classifications can be assessed both visually and through two confusion matrices that can be used to calculate the relevant statistics (producer&rsquo;s, user&rsquo;s, overall accuracy (OA)). The proposed methodology was broadly tested in a 154 km2 study area, located in the Lake Trasimeno area (central Italy), using Landsat 8 (L8), Sentinel 2 (S2), and PlanetScope (PS) data. The area was selected considering its complex LULC mosaic mainly composed of artificial surfaces, annual and permanent crops, small lakes, and wooded areas. In the study area, the various tests produced interesting results on the different datasets (OA: PB RF (L8 = 72.7%, S2 = 82%, PS = 74.2), PB SVM (L8 = 79.1%, S2 = 80.2%, PS = 74.8%), OO RF (L8 = 64%, S2 = 89.3%, PS = 77.9), OO SVM (L8 = 70.4, S2 = 86.9%, PS = 73.9)). The broad code application demonstrated very good reliability of the whole process, even though the OO classification process resulted, sometimes, too demanding on higher resolution data, considering the available computational GEE resources.
KW  - land use land cover
KW  - Google Earth Engine (GEE)
KW  - Sentinel 2
KW  - Landsat 8
KW  - PlanetScope
KW  - SNIC
KW  - GLCM
KW  - machine learning
KW  - Random Forest (RF)
KW  - Support Vector Machine (SVM)
KW  - accuracy assessment
DO  - 10.3390/rs12223776
ER  -
TY  - EJOU
AU  - Zhang, Zichen
AU  - Boubin, Jayson
AU  - Stewart, Christopher
AU  - Khanal, Sami
TI  - Whole-Field Reinforcement Learning: A Fully Autonomous Aerial Scouting Method for Precision Agriculture
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - Unmanned aerial systems (UAS) are increasingly used in precision agriculture to collect crop health related data. UAS can capture data more often and more cost-effectively than sending human scouts into the field. However, in large crop fields, flight time, and hence data collection, is limited by battery life. In a conventional UAS approach, human operators are required to exchange depleted batteries many times, which can be costly and time consuming. In this study, we developed a novel, fully autonomous aerial scouting approach that preserves battery life by sampling sections of a field for sensing and predicting crop health for the whole field. Our approach uses reinforcement learning (RL) and convolutional neural networks (CNN) to accurately and autonomously sample the field. To develop and test the approach, we ran flight simulations on an aerial image dataset collected from an 80-acre corn field. The excess green vegetation Index was used as a proxy for crop health condition. Compared to the conventional UAS scouting approach, the proposed scouting approach sampled 40% of the field, predicted crop health with 89.8% accuracy, reduced labor cost by 4.8&times; and increased agricultural profits by 1.36&times;.
KW  - convolutional neural networks
KW  - reinforcement learning
KW  - unmanned aerial systems
KW  - autonomous systems
KW  - precision agriculture
KW  - crop scouting
DO  - 10.3390/s20226585
ER  -
TY  - EJOU
AU  - Li, Bo
AU  - Gan, Zhigang
AU  - Chen, Daqing
AU  - Sergey Aleksandrovich, Dyachenko
TI  - UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.
KW  - UAV
KW  - maneuvering target tracking
KW  - deep reinforcement learning
KW  - meta-learning
KW  - multi-tasks
DO  - 10.3390/rs12223789
ER  -
TY  - EJOU
AU  - Khanal, Sami
AU  - KC, Kushal
AU  - Fulton, John P.
AU  - Shearer, Scott
AU  - Ozkan, Erdal
TI  - Remote Sensing in Agriculture—Accomplishments, Limitations, and Opportunities
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34%), followed by the United States (20%) and China (11%). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability.
KW  - remote sensing
KW  - satellite
KW  - UAS
KW  - precision agriculture
DO  - 10.3390/rs12223783
ER  -
TY  - EJOU
AU  - Lee, Sunmin
AU  - Baek, Won-Kyung
AU  - Jung, Hyung-Sup
AU  - Lee, Saro
TI  - Susceptibility Mapping on Urban Landslides Using Deep Learning Approaches in Mt. Umyeon
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 22
SN  - 2076-3417

AB  - In recent years, the incidence of localized heavy rainfall has increased as abnormal weather events occur more frequently. In densely populated urban areas, this type of heavy rain can cause extreme landslide damage, so that it is necessary to estimate and analyze the susceptibility of future landslides. In this regard, deep learning (DL) methodologies have been used to identify areas prone to landslides recently. Therefore, in this study, DL methodologies, including a deep neural network (DNN), kernel-based DNN, and convolutional neural network (CNN) were used to identify areas where landslides could occur. As a detailed step for this purpose, landslide occurrence was first determined as landslide inventory through aerial photographs with comparative analysis using field survey data; a training set was built for model training through oversampling based on the landslide inventory. A total of 17 landslide influencing variables that influence the frequency of landslides by topography and geomorphology, as well as soil and forest variables, were selected to establish a landslide inventory. Then models were built using DNN, kernel-based DNN, and CNN models, and the susceptibility of landslides in the study area was determined. Model performance was evaluated through the average precision (AP) score and root mean square error (RMSE) for each of the three models. Finally, DNN, kernel-based DNN, and CNN models showed performances of 99.45%, 99.44%, and 99.41%, and RMSE values of 0.1694, 0.1806, and 0.1747, respectively. As a result, all three models showed similar performance, indicating excellent predictive ability of the models developed in this study. The information of landslides occurring in urban areas, which cause a great damage even with a small number of occurrences, can provide a basis for reference to the government and local authorities for urban landslide management.
KW  - susceptibility mapping
KW  - urban landslides
KW  - debris flow
KW  - deep learning
DO  - 10.3390/app10228189
ER  -
TY  - EJOU
AU  - Radke, David
AU  - Radke, Daniel
AU  - Radke, John
TI  - Beyond Measurement: Extracting Vegetation Height from High Resolution Imagery with Deep Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Measuring and monitoring the height of vegetation provides important insights into forest age and habitat quality. These are essential for the accuracy of applications that are highly reliant on up-to-date and accurate vegetation data. Current vegetation sensing practices involve ground survey, photogrammetry, synthetic aperture radar (SAR), and airborne light detection and ranging sensors (LiDAR). While these methods provide high resolution and accuracy, their hardware and collection effort prohibits highly recurrent and widespread collection. In response to the limitations of current methods, we designed Y-NET, a novel deep learning model to generate high resolution models of vegetation from highly recurrent multispectral aerial imagery and elevation data. Y-NET&rsquo;s architecture uses convolutional layers to learn correlations between different input features and vegetation height, generating an accurate vegetation surface model (VSM) at 1&times;1 m resolution. We evaluated Y-NET on 235 km2 of the East San Francisco Bay Area and find that Y-NET achieves low error from LiDAR when tested on new locations. Y-NET also achieves an R2 of 0.83 and can effectively model complex vegetation through side-by-side visual comparisons. Furthermore, we show that Y-NET is able to identify instances of vegetation growth and mitigation by comparing aerial imagery and LiDAR collected at different times.
KW  - deep learning
KW  - artificial intelligence
KW  - vegetation surface modeling
DO  - 10.3390/rs12223797
ER  -
TY  - EJOU
AU  - Rashidi, Maria
AU  - Mohammadi, Masoud
AU  - Sadeghlou Kivi, Saba
AU  - Abdolvand, Mohammad M.
AU  - Truong-Hong, Linh
AU  - Samali, Bijan
TI  - A Decade of Modern Bridge Monitoring Using Terrestrial Laser Scanning: Review and Future Directions
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Over the last decade, particular interest in using state-of-the-art emerging technologies for inspection, assessment, and management of civil infrastructures has remarkably increased. Advanced technologies, such as laser scanners, have become a suitable alternative for labor intensive, expensive, and unsafe traditional inspection and maintenance methods, which encourage the increasing use of this technology in construction industry, especially in bridges. This paper aims to provide a thorough mixed scientometric and state-of-the-art review on the application of terrestrial laser scanners (TLS) in bridge engineering and explore investigations and recommendations of researchers in this area. Following the review, more than 1500 research publications were collected, investigated and analyzed through a two-fold literature search published within the last decade from 2010 to 2020. Research trends, consisting of dominated sub-fields, co-occurrence of keywords, network of researchers and their institutions, along with the interaction of research networks, were quantitatively analyzed. Moreover, based on the collected papers, application of TLS in bridge engineering and asset management was reviewed according to four categories including (1) generation of 3D model, (2) quality inspection, (3) structural assessment, and (4) bridge information modeling (BrIM). Finally, the paper identifies the current research gaps, future directions obtained from the quantitative analysis, and in-depth discussions of the collected papers in this area.
KW  - terrestrial laser scanner (TLS)
KW  - bridge
KW  - 3D model reconstruction
KW  - quality inspection
KW  - structural assessment
KW  - bridge information modeling (BrIM)
DO  - 10.3390/rs12223796
ER  -
TY  - EJOU
AU  - Su, Jinhua
AU  - Bai, Yanbing
AU  - Wang, Xingrui
AU  - Lu, Dong
AU  - Zhao, Bo
AU  - Yang, Hanfang
AU  - Mas, Erick
AU  - Koshimura, Shunichi
TI  - Technical Solution Discussion for Key Challenges of Operational Convolutional Neural Network-Based Building-Damage Assessment from Satellite Imagery: Perspective from Benchmark xBD Dataset
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Earth Observation satellite imaging helps building diagnosis during a disaster. Several models are put forward on the xBD dataset, which can be divided into two levels: the building level and the pixel level. Models from two levels evolve into several versions that will be reviewed in this paper. There are four key challenges hindering researchers from moving forward on this task, and this paper tries to give technical solutions. First, metrics on different levels could not be compared directly. We put forward a fairer metric and give a method to convert between metrics of two levels. Secondly, drone images may be another important source, but drone data may have only a post-disaster image. This paper shows and compares methods of directly detecting and generating. Thirdly, the class imbalance is a typical feature of the xBD dataset and leads to a bad F1 score for minor damage and major damage. This paper provides four specific data resampling strategies, which are Main-Label Over-Sampling (MLOS), Discrimination After Cropping (DAC), Dilation of Area with Minority (DAM) and Synthetic Minority Over-Sampling Technique (SMOTE), as well as cost-sensitive re-weighting schemes. Fourthly, faster prediction meets the need for a real-time situation. This paper recommends three specific methods, feature-map subtraction, parameter sharing, and knowledge distillation. Finally, we developed our AI-driven Damage Diagnose Platform (ADDP). This paper introduces the structure of ADDP and technical details. Customized settings, interface preview, and upload and download satellite images are major services our platform provides.
KW  - convolutional neural network
KW  - building-damage assessment
KW  - benchmark xBD dataset
KW  - disaster response online platform
DO  - 10.3390/rs12223808
ER  -
TY  - EJOU
AU  - Sayeed, Mohd A.
AU  - Kumar, Rajesh
AU  - Sharma, Vishal
AU  - Sayeed, Mohd A.
TI  - Efficient Deployment with Throughput Maximization for UAVs Communication Networks
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - The article presents a throughput maximization approach for UAV assisted ground networks. Throughput maximization involves minimizing delay and packet loss through UAV trajectory optimization, reinforcing the congested nodes and transmission channels. The aggressive reinforcement policy is achieved by characterizing nodes, links, and overall topology through delay, loss, throughput, and distance. A position-aware graph neural network (GNN) is used for characterization, prediction, and dynamic UAV trajectory enhancement. To establish correctness, the proposed approach is validated against optimized link state routing (OLSR) driven UAV assisted ground networks. The proposed approach considerably outperforms the classical approach by demonstrating significant gains in throughput and packet delivery ratio with notable decrements in delay and packet loss. The performance analysis of the proposed approach against software-defined UAVs (U-S) and UAVs as base stations (U-B) verifies the consistency and gains in average throughput while minimizing delay and packet loss. The scalability test of the proposed approach is performed by varying data rates and the number of UAVs.
KW  - UAV
KW  - throughput
KW  - delay
KW  - packet loss
KW  - GNN
KW  - collaborative network
KW  - trajectory
DO  - 10.3390/s20226680
ER  -
TY  - EJOU
AU  - Xia, Junshi
AU  - Yokoya, Naoto
AU  - Pham, Tien D.
TI  - Probabilistic Mangrove Species Mapping with Multiple-Source Remote-Sensing Datasets Using Label Distribution Learning in Xuan Thuy National Park, Vietnam
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Mangrove forests play an important role in maintaining water quality, mitigating climate change impacts, and providing a wide range of ecosystem services. Effective identification of mangrove species using remote-sensing images remains a challenge. The combinations of multi-source remote-sensing datasets (with different spectral/spatial resolution) are beneficial to the improvement of mangrove tree species discrimination. In this paper, various combinations of remote-sensing datasets including Sentinel-1 dual-polarimetric synthetic aperture radar (SAR), Sentinel-2 multispectral, and Gaofen-3 full-polarimetric SAR data were used to classify the mangrove communities in Xuan Thuy National Park, Vietnam. The mixture of mangrove communities consisting of small and shrub mangrove patches is generally difficult to separate using low/medium spatial resolution. To alleviate this problem, we propose to use label distribution learning (LDL) to provide the probabilistic mapping of tree species, including Sonneratia caseolaris (SC), Kandelia obovata (KO), Aegiceras corniculatum (AC), Rhizophora stylosa (RS), and Avicennia marina (AM). The experimental results show that the best classification performance was achieved by an integration of Sentinel-2 and Gaofen-3 datasets, demonstrating that full-polarimetric Gaofen-3 data is superior to the dual-polarimetric Sentinel-1 data for mapping mangrove tree species in the tropics.
KW  - mangrove species mapping
KW  - label distribution learning
KW  - Gaofen-3
KW  - Sentinel
KW  - Vietnam
DO  - 10.3390/rs12223834
ER  -
TY  - EJOU
AU  - Tian, Xiaomin
AU  - Chen, Long
AU  - Zhang, Xiaoli
AU  - Chen, Erxue
TI  - Improved Prototypical Network Model for Forest Species Classification in Complex Stand
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Deep learning has become an effective method for hyperspectral image classification. However, the high band correlation and data volume associated with airborne hyperspectral images, and the insufficiency of training samples, present challenges to the application of deep learning in airborne image classification. Prototypical networks are practical deep learning networks that have demonstrated effectiveness in handling small-sample classification. In this study, an improved prototypical network is proposed (by adding L2 regularization to the convolutional layer and dropout to the maximum pooling layer) to address the problem of overfitting in small-sample classification. The proposed network has an optimal sample window for classification, and the window size is related to the area and distribution of the study area. After performing dimensionality reduction using principal component analysis, the time required for training using hyperspectral images shortened significantly, and the test accuracy increased drastically. Furthermore, when the size of the sample window was 27 &times; 27 after dimensionality reduction, the overall accuracy of forest species classification was 98.53%, and the Kappa coefficient was 0.9838. Therefore, by using an improved prototypical network with a sample window of an appropriate size, the network yielded desirable classification results, thereby demonstrating its suitability for the fine classification and mapping of tree species.
KW  - hyperspectral images
KW  - prototypical network
KW  - tree species classification
KW  - small-sample
KW  - dimensionality reduction
DO  - 10.3390/rs12223839
ER  -
TY  - EJOU
AU  - Scharvogel, Daniel
AU  - Brandmeier, Melanie
AU  - Weis, Manuel
TI  - A Deep Learning Approach for Calamity Assessment Using Sentinel-2 Data
T2  - Forests

PY  - 2020
VL  - 11
IS  - 12
SN  - 1999-4907

AB  - The number of severe storm events has increased in recent decades due to climate change. These storms are one of the main causes for timber loss in European forests and damaged areas are prone to further degradation by, for example, bark beetle infestations. Usually, manual mapping of damaged areas based on aerial photographs is conducted by forest departments. This is very time-consuming and therefore automatic detection of windthrows based on active and passive remote sensing data is an ongoing research topic. In this study we evaluated state-of-the-art Convolutional Neural Networks (CNNs) in combination with Geographic Information Systems (GIS) for calamity assessment. The study area is in in the northern part of Hesse (Germany) and was covered by twelve Sentinel-2 scenes from 2018. Labels of damaged areas from the Friedericke storm (18 January 2018) were provided by HessenForst. We conducted several experiments based on a custom U-Net setup to derive the optimal architecture and input data as well as to assess the transferability of the model. Results highlight the possibility to detect damaged forest areas using Sentinel-2 data. Using a binary classification, accuracies of more than 92% were achieved with an Intersection over Union (IoU) score of 46.6%. The proposed workflow was integrated into ArcGIS and is suitable for fast detection of damaged areas directly after a storm and for disaster management but is limited by the deca-meter spatial resolution of the Sentinel-2 data.
KW  - CNNs
KW  - remote sensing
KW  - windthrow
KW  - forest
KW  - Deep Learning
KW  - GIS
DO  - 10.3390/f11121239
ER  -
TY  - EJOU
AU  - Kolosov, Kirill
AU  - Miller, Alexander
AU  - Miller, Boris
TI  - Robust Data Fusion of UAV Navigation Measurements with Application to the Landing System
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - To perform precise approach and landing concerning an aircraft in automatic mode, local airfield-based landing systems are used. For joint processing of measurements of the onboard inertial navigation systems (INS), altimeters and local landing systems, the Kalman filter is usually used. The application of the quadratic criterion in the Kalman filter entails the well-known problem of high sensitivity of the estimate to anomalous measurement errors. During the automatic approach phase, abnormal navigation errors can lead to disaster, so the data fusion algorithm must automatically identify and isolate abnormal measurements. This paper presents a recurrent filtering algorithm that is resistant to anomalous errors in measurements and considers its application in the data fusion problem for landing system measurements with onboard sensor measurements&mdash;INS and altimeters. The robustness of the estimate is achieved through the combined use of the least modulus method and the Kalman filter. To detect and isolate failures the chi-square criterion is used. It makes possible the customization of the algorithm in accordance with the requirements for false alarm probability and the alarm missing probability. Testing results of the robust filtering algorithm are given both for synthesized data and for real measurements.
KW  - automatic landing
KW  - data fusion
KW  - Kalman filter
KW  - least modulus method
KW  - L1 optimization
KW  - M estimate
KW  - adaptive filtering
KW  - robust filtering
KW  - navigation
KW  - fault tolerance
KW  - non-Gaussian noise
DO  - 10.3390/rs12233849
ER  -
TY  - EJOU
AU  - Qi, Haixia
AU  - Zhu, Bingyu
AU  - Wu, Zeyu
AU  - Liang, Yu
AU  - Li, Jianwen
AU  - Wang, Leidi
AU  - Chen, Tingting
AU  - Lan, Yubin
AU  - Zhang, Lei
TI  - Estimation of Peanut Leaf Area Index from Unmanned Aerial Vehicle Multispectral Images
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 23
SN  - 1424-8220

AB  - Leaf area index (LAI) is used to predict crop yield, and unmanned aerial vehicles (UAVs) provide new ways to monitor LAI. In this study, we used a fixed-wing UAV with multispectral cameras for remote sensing monitoring. We conducted field experiments with two peanut varieties at different planting densities to estimate LAI from multispectral images and establish a high-precision LAI prediction model. We used eight vegetation indices (VIs) and developed simple regression and artificial neural network (BPN) models for LAI and spectral VIs. The empirical model was calibrated to estimate peanut LAI, and the best model was selected from the coefficient of determination and root mean square error. The red (660 nm) and near-infrared (790 nm) bands effectively predicted peanut LAI, and LAI increased with planting density. The predictive accuracy of the multiple regression model was higher than that of the single linear regression models, and the correlations between Modified Red-Edge Simple Ratio Index (MSR), Ratio Vegetation Index (RVI), Normalized Difference Vegetation Index (NDVI), and LAI were higher than the other indices. The combined VI BPN model was more accurate than the single VI BPN model, and the BPN model accuracy was higher. Planting density affects peanut LAI, and reflectance-based vegetation indices can help predict LAI.
KW  - leaf area index
KW  - multispectral
KW  - remote sensing
KW  - density
KW  - vegetation index
DO  - 10.3390/s20236732
ER  -
TY  - EJOU
AU  - Roy, Raphaëlle N.
AU  - Drougard, Nicolas
AU  - Gateau, Thibault
AU  - Dehais, Frédéric
AU  - Chanel, Caroline P. C.
TI  - How Can Physiological Computing Benefit Human-Robot Interaction?
T2  - Robotics

PY  - 2020
VL  - 9
IS  - 4
SN  - 2218-6581

AB  - As systems grow more automatized, the human operator is all too often overlooked. Although human-robot interaction (HRI) can be quite demanding in terms of cognitive resources, the mental states (MS) of the operators are not yet taken into account by existing systems. As humans are no providential agents, this lack can lead to hazardous situations. The growing number of neurophysiology and machine learning tools now allows for efficient operators&rsquo; MS monitoring. Sending feedback on MS in a closed-loop solution is therefore at hand. Involving a consistent automated planning technique to handle such a process could be a significant asset. This perspective article was meant to provide the reader with a synthesis of the significant literature with a view to implementing systems that adapt to the operator&rsquo;s MS to improve human-robot operations&rsquo; safety and performance. First of all, the need for this approach is detailed regarding remote operation, an example of HRI. Then, several MS identified as crucial for this type of HRI are defined, along with relevant electrophysiological markers. A focus is made on prime degraded MS linked to time-on-task and task demands, as well as collateral MS linked to system outputs (i.e., feedback and alarms). Lastly, the principle of symbiotic HRI is detailed and one solution is proposed to include the operator state vector into the system using a mixed-initiative decisional framework to drive such an interaction.
KW  - human-robot interaction
KW  - telerobotics
KW  - teleoperation
KW  - physiological computing
KW  - mental state monitoring
KW  - passive BCI
KW  - mixed-initiative
KW  - automated planning
DO  - 10.3390/robotics9040100
ER  -
TY  - EJOU
AU  - Ayele, Yonas Z.
AU  - Aliyari, Mostafa
AU  - Griffiths, David
AU  - Droguett, Enrique L.
TI  - Automatic Crack Segmentation for UAV-Assisted Bridge Inspection
T2  - Energies

PY  - 2020
VL  - 13
IS  - 23
SN  - 1996-1073

AB  - Bridges are a critical piece of infrastructure in the network of road and rail transport system. Many of the bridges in Norway (in Europe) are at the end of their lifespan, therefore regular inspection and maintenance are critical to ensure the safety of their operations. However, the traditional inspection procedures and resources required are so time consuming and costly that there exists a significant maintenance backlog. The central thrust of this paper is to demonstrate the significant benefits of adapting a Unmanned Aerial Vehicle (UAV)-assisted inspection to reduce the time and costs of bridge inspection and established the research needs associated with the processing of the (big) data produced by such autonomous technologies. In this regard, a methodology is proposed for analysing the bridge damage that comprises three key stages, (i) data collection and model training, where one performs experiments and trials to perfect drone flights for inspection using case study bridges to inform and provide necessary (big) data for the second key stage, (ii) 3D construction, where one built 3D models that offer a permanent record of element geometry for each bridge asset, which could be used for navigation and control purposes, (iii) damage identification and analysis, where deep learning-based data analytics and modelling are applied for processing and analysing UAV image data and to perform bridge damage performance assessment. The proposed methodology is exemplified via UAV-assisted inspection of Skodsberg bridge, a 140 m prestressed concrete bridge, in the Viken county in eastern Norway.
KW  - drone-assisted bridge inspection
KW  - crack detection
KW  - crack segmentation
KW  - damage assessment
KW  - UAV
KW  - performance analysis
DO  - 10.3390/en13236250
ER  -
TY  - EJOU
AU  - Egli, Sebastian
AU  - Höpke, Martin
TI  - CNN-Based Tree Species Classification Using High Resolution RGB Image Data from Automated UAV Observations
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - Data on the distribution of tree species are often requested by forest managers, inventory agencies, foresters as well as private and municipal forest owners. However, the automated detection of tree species based on passive remote sensing data from aerial surveys is still not sufficiently developed to achieve reliable results independent of the phenological stage, time of day, season, tree vitality and prevailing atmospheric conditions. Here, we introduce a novel tree species classification approach based on high resolution RGB image data gathered during automated UAV flights that overcomes these insufficiencies. For the classification task, a computationally lightweight convolutional neural network (CNN) was designed. We show that with the chosen CNN model architecture, average classification accuracies of 92% can be reached independently of the illumination conditions and the phenological stages of four different tree species. We also show that a minimal ground sampling density of 1.6 cm/px is needed for the classification model to be able to make use of the spatial-structural information in the data. Finally, to demonstrate the applicability of the presented approach to derive spatially explicit tree species information, a gridded product is generated that yields an average classification accuracy of 88%.
KW  - tree species classification
KW  - CNN
KW  - UAV
KW  - RGB
DO  - 10.3390/rs12233892
ER  -
TY  - EJOU
AU  - Zhu, Juncai
AU  - Wang, Zhizhong
AU  - Wang, Songwei
AU  - Chen, Shuli
TI  - Moving Object Detection Based on Background Compensation and Deep Learning
T2  - Symmetry

PY  - 2020
VL  - 12
IS  - 12
SN  - 2073-8994

AB  - Detecting moving objects in a video sequence is an important problem in many vision-based applications. In particular, detecting moving objects when the camera is moving is a difficult problem. In this study, we propose a symmetric method for detecting moving objects in the presence of a dynamic background. First, a background compensation method is used to detect the proposed region of motion. Next, in order to accurately locate the moving objects, we propose a convolutional neural network-based method called YOLOv3-SOD for detecting all objects in the image, which is lightweight and specifically designed for small objects. Finally, the moving objects are determined by fusing the results obtained by motion detection and object detection. Missed detections are recalled according to the temporal and spatial information in adjacent frames. A dataset is not currently available specifically for moving object detection and recognition, and thus, we have released the MDR105 dataset comprising three classes with 105 videos. Our experiments demonstrated that the proposed algorithm can accurately detect moving objects in various scenarios with good overall performance.
KW  - convolutional neural network
KW  - dynamic background
KW  - motion compensation
KW  - moving object detection
KW  - small target detection
DO  - 10.3390/sym12121965
ER  -
