TY  - EJOU
AU  - Li, Jin
AU  - Yan, Daifu
AU  - Luan, Kuan
AU  - Li, Zeyu
AU  - Liang, Hong
TI  - Deep Learning-Based Bird’s Nest Detection on Transmission Lines Using UAV Imagery
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 18
SN  - 2076-3417

AB  - In order to ensure the safety of transmission lines, the use of unmanned aerial vehicle (UAV) images for automatic object detection has important application prospects, such as the detection of birds&rsquo; nests. The traditional bird&rsquo;s nest detection methods mainly include the study of morphological characteristics of the bird&rsquo;s nest. These methods have poor applicability and low accuracy. In this work, we propose a deep learning-based birds&rsquo; nests automatic detection framework&mdash;region of interest (ROI) mining faster region-based convolutional neural networks (RCNN). First, the prior dimensions of anchors are obtained by using k-means clustering to improve the accuracy of coordinate boxes generation. Second, in order to balance the number of foreground and background samples in the training process, the focal loss function is introduced in the region proposal network (RPN) classification stage. Finally, the ROI mining module is added to solve the class imbalance problem in the classification stage, combined with the characteristics of difficult-to-classify bird&rsquo;s nest samples in the UAV images. After parameter optimization and experimental verification, the deep learning-based bird&rsquo;s nest automatic detection framework proposed in this work achieves high detection accuracy. In addition, the mean average precision (mAP) and formula 1 (F1) score of the proposed method are higher than the original faster RCNN and cascade RCNN. Our comparative analysis verifies the effectiveness of the proposed method.
KW  - transmission line
KW  - bird’s nest detection
KW  - convolutional neural network
KW  - deep learning
DO  - 10.3390/app10186147
ER  -
TY  - EJOU
AU  - Farella, Elisa M.
AU  - Torresani, Alessandro
AU  - Remondino, Fabio
TI  - Refining the Joint 3D Processing of Terrestrial and UAV Images Using Quality Measures
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - The paper presents an efficient photogrammetric workflow to improve the 3D reconstruction of scenes surveyed by integrating terrestrial and Unmanned Aerial Vehicle (UAV) images. In the last years, the integration of this kind of images has shown clear advantages for the complete and detailed 3D representation of large and complex scenarios. Nevertheless, their photogrammetric integration often raises several issues in the image orientation and dense 3D reconstruction processes. Noisy and erroneous 3D reconstructions are the typical result of inaccurate orientation results. In this work, we propose an automatic filtering procedure which works at the sparse point cloud level and takes advantage of photogrammetric quality features. The filtering step removes low-quality 3D tie points before refining the image orientation in a new adjustment and generating the final dense point cloud. Our method generalizes to many datasets, as it employs statistical analyses of quality feature distributions to identify suitable filtering thresholds. Reported results show the effectiveness and reliability of the method verified using both internal and external quality checks, as well as visual qualitative comparisons. We made the filtering tool publicly available on GitHub.
KW  - data fusion
KW  - sparse point cloud
KW  - filtering
KW  - image orientation
KW  - dense point cloud generation
DO  - 10.3390/rs12182873
ER  -
TY  - EJOU
AU  - Guo, Yahui
AU  - Wang, Hanxi
AU  - Wu, Zhaofei
AU  - Wang, Shuxin
AU  - Sun, Hongyong
AU  - Senthilnath, J.
AU  - Wang, Jingzhe
AU  - Robin Bryant, Christopher
AU  - Fu, Yongshuo
TI  - Modified Red Blue Vegetation Index for Chlorophyll Estimation and Yield Prediction of Maize from Visible Images Captured by UAV
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 18
SN  - 1424-8220

AB  - The vegetation index (VI) has been successfully used to monitor the growth and to predict the yield of agricultural crops. In this paper, a long-term observation was conducted for the yield prediction of maize using an unmanned aerial vehicle (UAV) and estimations of chlorophyll contents using SPAD-502. A new vegetation index termed as modified red blue VI (MRBVI) was developed to monitor the growth and to predict the yields of maize by establishing relationships between MRBVI- and SPAD-502-based chlorophyll contents. The coefficients of determination (R2s) were 0.462 and 0.570 in chlorophyll contents&rsquo; estimations and yield predictions using MRBVI, and the results were relatively better than the results from the seven other commonly used VI approaches. All VIs during the different growth stages of maize were calculated and compared with the measured values of chlorophyll contents directly, and the relative error (RE) of MRBVI is the lowest at 0.355. Further, machine learning (ML) methods such as the backpropagation neural network model (BP), support vector machine (SVM), random forest (RF), and extreme learning machine (ELM) were adopted for predicting the yields of maize. All VIs calculated for each image captured during important phenological stages of maize were set as independent variables and the corresponding yields of each plot were defined as dependent variables. The ML models used the leave one out method (LOO), where the root mean square errors (RMSEs) were 2.157, 1.099, 1.146, and 1.698 (g/hundred grain weight) for BP, SVM, RF, and ELM. The mean absolute errors (MAEs) were 1.739, 0.886, 0.925, and 1.356 (g/hundred grain weight) for BP, SVM, RF, and ELM, respectively. Thus, the SVM method performed better in predicting the yields of maize than the other ML methods. Therefore, it is strongly suggested that the MRBVI calculated from images acquired at different growth stages integrated with advanced ML methods should be used for agricultural- and ecological-related chlorophyll estimation and yield predictions.
KW  - maize
KW  - UAV/UAS
KW  - chlorophyll contents
KW  - yield predictions
KW  - machine learning
DO  - 10.3390/s20185055
ER  -
TY  - EJOU
AU  - Kundid Vasić, Mirela
AU  - Papić, Vladan
TI  - Multimodel Deep Learning for Person Detection in Aerial Images
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 9
SN  - 2079-9292

AB  - In this paper, we propose a novel method for person detection in aerial images of nonurban terrain gathered by an Unmanned Aerial Vehicle (UAV), which plays an important role in Search And Rescue (SAR) missions. The UAV in SAR operations contributes significantly due to the ability to survey a larger geographical area from an aerial viewpoint. Because of the high altitude of recording, the object of interest (person) covers a small part of an image (around 0.1%), which makes this task quite challenging. To address this problem, a multimodel deep learning approach is proposed. The solution consists of two different convolutional neural networks in region proposal, as well as in the classification stage. Additionally, contextual information is used in the classification stage in order to improve the detection results. Experimental results tested on the HERIDAL dataset achieved precision of 68.89% and a recall of 94.65%, which is better than current state-of-the-art methods used for person detection in similar scenarios. Consequently, it may be concluded that this approach is suitable for usage as an auxiliary method in real SAR operations.
KW  - convolutional neural networks
KW  - aerial images
KW  - person detection
KW  - search and rescue
DO  - 10.3390/electronics9091459
ER  -
TY  - EJOU
AU  - Khan, Khalil
AU  - Albattah, Waleed
AU  - Khan, Rehan U.
AU  - Qamar, Ali M.
AU  - Nayab, Durre
TI  - Advances and Trends in Real Time Visual Crowd Analysis
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 18
SN  - 1424-8220

AB  - Real time crowd analysis represents an active area of research within the computer vision community in general and scene analysis in particular. Over the last 10 years, various methods for crowd management in real time scenario have received immense attention due to large scale applications in people counting, public events management, disaster management, safety monitoring an so on. Although many sophisticated algorithms have been developed to address the task; crowd management in real time conditions is still a challenging problem being completely solved, particularly in wild and unconstrained conditions. In the proposed paper, we present a detailed review of crowd analysis and management, focusing on state-of-the-art methods for both controlled and unconstrained conditions. The paper illustrates both the advantages and disadvantages of state-of-the-art methods. The methods presented comprise the seminal research works on crowd management, and monitoring and then culminating state-of-the-art methods of the newly introduced deep learning methods. Comparison of the previous methods is presented, with a detailed discussion of the direction for future research work. We believe this review article will contribute to various application domains and will also augment the knowledge of the crowd analysis within the research community.
KW  - crowd image analysis
KW  - crowd monitoring
KW  - crowd management
KW  - deep learning
KW  - crowd detection
DO  - 10.3390/s20185073
ER  -
TY  - EJOU
AU  - Zheng, Ruihao
AU  - Xiong, Chen
AU  - Deng, Xiangbin
AU  - Li, Qiangsheng
AU  - Li, Yi
TI  - Assessment of Earthquake Destructive Power to Structures Based on Machine Learning Methods
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 18
SN  - 2076-3417

AB  - This study presents a machine learning-based method for the destructive power assessment of earthquake to structures. First, the analysis procedure of the method is presented, and the backpropagation neural network (BPNN) and convolutional neural network (CNN) are used as the machine learning algorithms. Second, the optimized BPNN architecture is obtained by discussing the influence of a different number of hidden layers and nodes. Third, the CNN architecture is proposed based on several classical deep learning networks. To build the machine learning models, 50,570 time-history analysis results of a structural system subjected to different ground motions are used as training, validation, and test samples. The results of the BPNN indicate that the features extraction method based on the short-time Fourier transform (STFT) can well reflect the frequency-/time-domain characteristics of ground motions. The results of the CNN indicate that the CNN exhibits better accuracy (R2 = 0.8737) compared with that of the BPNN (R2 = 0.6784). Furthermore, the CNN model exhibits remarkable computational efficiency, the prediction of 1000 structures based on the CNN model takes 0.762 s, while 507.81 s are required for the conventional time-history analysis (THA)-based simulation. Feature visualization of different layers of the CNN reveals that the shallow to deep layers of the CNN can extract the high to low-frequency features of ground motions. The proposed method can assist in the fast prediction of engineering demand parameters of large-number structures, which facilitates the damage or loss assessments of regional structures for timely emergency response and disaster relief after earthquake.
KW  - machine learning
KW  - backpropagation neural network
KW  - convolutional neural network
KW  - seismic damage simulation
KW  - time-history analysis
KW  - earthquake destructive power
DO  - 10.3390/app10186210
ER  -
TY  - EJOU
AU  - Huang, Faming
AU  - Yang, Jianbo
AU  - Zhang, Biao
AU  - Li, Yijing
AU  - Huang, Jinsong
AU  - Chen, Na
TI  - Regional Terrain Complexity Assessment Based on Principal Component Analysis and Geographic Information System: A Case of Jiangxi Province, China
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 9
SN  - 2220-9964

AB  - Regional terrain complexity assessment (TCA) is an important theoretical foundation for geological feature identification, hydrological information extraction and land resources utilization. However, the previous TCA models have many disadvantages; for example, comprehensive consideration and redundancy information analysis of terrain factors is lacking, and the terrain complexity index is difficult to quantify. To overcome these drawbacks, a TCA model based on principal component analysis (PCA) and a geographic information system (GIS) is proposed. Taking Jiangxi province of China as an example, firstly, ten terrain factors are extracted using a digital elevation model (DEM) in GIS software. Secondly, PCA is used to analyze the information redundancy of these terrain factors and deal with data compression. Then, the comprehensive evaluation of the compressed terrain factors is conducted to obtain quantitative terrain complexity indexes and a terrain complexity map (TCM). Finally, the TCM produced by the PCA method is compared with those produced by the slope-only, the variation coefficient and K-means clustering models based on the topographic map drawn by the Bureau of Land and Resources of Jiangxi province. Meanwhile, the TCM is also verified by the actual three-dimensional aerial images. Results show that the correlation coefficients between the TCMs produced by the PCA, slope-only, variable coefficient and K-means clustering models and the local topographic map are 0.894, 0.763, 0.816 and 0.788, respectively. It is concluded that the TCM of the PCA method matches well with the actual field terrain features, and the PCA method can reflect the regional terrain complexity characteristics more comprehensively and accurately when compared to the other three methods.
KW  - terrain complexity assessment
KW  - digital elevation model
KW  - principal component analysis
KW  - variation coefficient method
KW  - K-means clustering
KW  - geographic information system
DO  - 10.3390/ijgi9090539
ER  -
TY  - EJOU
AU  - Guo, Yahui
AU  - Yin, Guodong
AU  - Sun, Hongyong
AU  - Wang, Hanxi
AU  - Chen, Shouzhi
AU  - Senthilnath, J.
AU  - Wang, Jingzhe
AU  - Fu, Yongshuo
TI  - Scaling Effects on Chlorophyll Content Estimations with RGB Camera Mounted on a UAV Platform Using Machine-Learning Methods
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 18
SN  - 1424-8220

AB  - Timely monitoring and precise estimation of the leaf chlorophyll contents of maize are crucial for agricultural practices. The scale effects are very important as the calculated vegetation index (VI) were crucial for the quantitative remote sensing. In this study, the scale effects were investigated by analyzing the linear relationships between VI calculated from red&ndash;green&ndash;blue (RGB) images from unmanned aerial vehicles (UAV) and ground leaf chlorophyll contents of maize measured using SPAD-502. The scale impacts were assessed by applying different flight altitudes and the highest coefficient of determination (R2) can reach 0.85. We found that the VI from images acquired from flight altitude of 50 m was better to estimate the leaf chlorophyll contents using the DJI UAV platform with this specific camera (5472 &times; 3648 pixels). Moreover, three machine-learning (ML) methods including backpropagation neural network (BP), support vector machine (SVM), and random forest (RF) were applied for the grid-based chlorophyll content estimation based on the common VI. The average values of the root mean square error (RMSE) of chlorophyll content estimations using ML methods were 3.85, 3.11, and 2.90 for BP, SVM, and RF, respectively. Similarly, the mean absolute error (MAE) were 2.947, 2.460, and 2.389, for BP, SVM, and RF, respectively. Thus, the ML methods had relative high precision in chlorophyll content estimations using VI; in particular, the RF performed better than BP and SVM. Our findings suggest that the integrated ML methods with RGB images of this camera acquired at a flight altitude of 50 m (spatial resolution 0.018 m) can be perfectly applied for estimations of leaf chlorophyll content in agriculture.
KW  - scale effects
KW  - maize
KW  - UAV/UAS
KW  - SPAD
KW  - chlorophyll contents
KW  - HSV
KW  - machine learning
DO  - 10.3390/s20185130
ER  -
TY  - EJOU
AU  - Tan, Ziyi
AU  - Yang, Xu
AU  - Pang, Mingzhi
AU  - Gao, Shouwan
AU  - Li, Ming
AU  - Chen, Pengpeng
TI  - UAV-Assisted Low-Consumption Time Synchronization Utilizing Cross-Technology Communication
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 18
SN  - 1424-8220

AB  - Wireless sensor networks (WSNs) have been used in many fields due to its wide applicability. In this kind of network, each node is independent of each other and has its own local clock and communicates wirelessly. Time synchronization plays a vital role in WSNs and it can ensure accuracy requirements for coordination and data reliability. However, two key challenges exist in large-scale WSNs that are severe resource constraints overhead and multihop time synchronization errors. To address these issues, this paper proposes a novel unmanned aerial vehicle (UAV)-assisted low-consumption time synchronization algorithm based on cross-technology communication (CTC) for a large-scale WSN. This algorithm uses a UAV to send time synchronization data packets for calibration. Moreover, to ensure coverage and a high success rate for UAV data transmission, we use CTC for time synchronization. Without any relays, a high-power time synchronization packet can be sent by a UAV to achieve the time synchronization of low-power sensors. This algorithm can achieve accurate time synchronization with almost zero energy consumption for the sensor nodes. Finally, we implemented our algorithm with 30 low-power RF-CC2430 ZigBee nodes and a Da Jiang Innovations (DJI) M100 UAV on a 1 km highway and an indoor site. The results show that time synchronization can be achieved accurately with almost zero energy consumption for the sensor nodes, and the time synchronization error is less than 30 &mu;s in 99% of cases.
KW  - time synchronization
KW  - cross-technology communication
KW  - energy consumption
KW  - UAV
DO  - 10.3390/s20185134
ER  -
TY  - EJOU
AU  - Jung, Soyi
AU  - Kim, Joongheon
AU  - Kim, Jae-Hyun
TI  - Joint Message-Passing and Convex Optimization Framework for Energy-Efficient Surveillance UAV Scheduling
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 9
SN  - 2079-9292

AB  - In modern surveillance systems, the use of unmanned aerial vehicles (UAVs) has been actively discussed in order to extend target monitoring areas, even for an extreme circumstances. This paper proposes an energy-efficient UAV-based surveillance system that operates from two different sequential methods. First, the proposed algorithm pursues energy-efficient operations by deactivating selected surveillance cameras on the UAVs located in overlapping areas. For this objective, a message-passing based algorithm is used because the overlapping situations can be formulated using a max-weight independent set. Next, the unscheduled UAVs based on the message-passing fly to the charging towers to be charged. This algorithm computes the optimal matching between the UAVs and charging towers and the amount of energy allocation for the scheduled UAV-tower pairs. This joint optimization is initially formulated as non-convex, and it is then reformulated to be convex, which can guarantee optimal solutions. The proposed framework achieves the desired performance, as presented in the performance evaluation.
KW  - UAVs
KW  - surveillance
KW  - energy efficiency
KW  - charging scheduling
KW  - optimization
DO  - 10.3390/electronics9091475
ER  -
TY  - EJOU
AU  - Noonan, John
AU  - Rivlin, Ehud
AU  - Rotstein, Hector
TI  - FloorVLoc: A Modular Approach to Floorplan Monocular Localization
T2  - Robotics

PY  - 2020
VL  - 9
IS  - 3
SN  - 2218-6581

AB  - Intelligent vehicles for search and rescue, whose mission is assisting emergency personnel by visually exploring an unfamiliar building, require accurate localization. With GPS not available, and approaches relying on new infrastructure installation, artificial landmarks, or pre-constructed dense 3D maps not feasible, the question is whether there is an approach which can combine ubiquitous prior map information with a monocular camera for accurate positioning. Enter FloorVLoc&mdash;Floorplan Vision Vehicle Localization. We provide a means to integrate a monocular camera with a floorplan in a unified and modular fashion so that any monocular visual Simultaneous Localization and Mapping (SLAM) system can be seamlessly incorporated for global positioning. Using a floorplan is especially beneficial since walls are geometrically stable, the memory footprint is low, and prior map information is kept at a minimum. Furthermore, our theoretical analysis of the visual features associated with the walls shows how drift is corrected. To see this approach in action, we developed two full global positioning systems based on the core methodology introduced, operating in both Monte Carlo Localization and linear optimization frameworks. Experimental evaluation of the systems in simulation and a challenging real-world environment demonstrates that FloorVLoc performs with an average error of 0.06 m across 80 m in real-time.
KW  - indoor positioning
KW  - mobile robotics
KW  - visual SLAM
KW  - search and rescue
DO  - 10.3390/robotics9030069
ER  -
TY  - EJOU
AU  - Sapkota, Bishwa
AU  - Singh, Vijay
AU  - Neely, Clark
AU  - Rajan, Nithya
AU  - Bagavathiannan, Muthukumar
TI  - Detection of Italian Ryegrass in Wheat and Prediction of Competitive Interactions Using Remote-Sensing and Machine-Learning Techniques
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - Italian ryegrass (Lolium perenne ssp. multiflorum (Lam) Husnot) is a troublesome weed species in wheat (Triticum aestivum) production in the United States, severely affecting grain yields. Spatial mapping of ryegrass infestation in wheat fields and early prediction of its impact on yield can assist management decision making. In this study, unmanned aerial systems (UAS)-based red, green and blue (RGB) imageries acquired at an early wheat growth stage in two different experimental sites were used for developing predictive models. Deep neural networks (DNNs) coupled with an extensive feature selection method were used to detect ryegrass in wheat and estimate ryegrass canopy coverage. Predictive models were developed by regressing early-season ryegrass canopy coverage (%) with end-of-season (at wheat maturity) biomass and seed yield of ryegrass, as well as biomass and grain yield reduction (%) of wheat. Italian ryegrass was detected with high accuracy (precision = 95.44 &plusmn; 4.27%, recall = 95.48 &plusmn; 5.05%, F-score = 95.56 &plusmn; 4.11%) using the best model which included four features: hue, saturation, excess green index, and visible atmospheric resistant index. End-of-season ryegrass biomass was predicted with high accuracy (R2 = 0.87), whereas the other variables had moderate to high accuracy levels (R2 values of 0.74 for ryegrass seed yield, 0.73 for wheat biomass reduction, and 0.69 for wheat grain yield reduction). The methodology demonstrated in the current study shows great potential for mapping and quantifying ryegrass infestation and predicting its competitive response in wheat, allowing for timely management decisions.
KW  - computer vision
KW  - deep neural networks
KW  - precision agriculture
KW  - site-specific management
KW  - unmanned aerial systems
KW  - UAVs
KW  - weed-crop interactions
DO  - 10.3390/rs12182977
ER  -
TY  - EJOU
AU  - Oh, Sungchan
AU  - Chang, Anjin
AU  - Ashapure, Akash
AU  - Jung, Jinha
AU  - Dube, Nothabo
AU  - Maeda, Murilo
AU  - Gonzalez, Daniel
AU  - Landivar, Juan
TI  - Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - Assessing plant population of cotton is important to make replanting decisions in low plant density areas, prone to yielding penalties. Since the measurement of plant population in the field is labor intensive and subject to error, in this study, a new approach of image-based plant counting is proposed, using unmanned aircraft systems (UAS; DJI Mavic 2 Pro, Shenzhen, China) data. The previously developed image-based techniques required a priori information of geometry or statistical characteristics of plant canopy features, while also limiting the versatility of the methods in variable field conditions. In this regard, a deep learning-based plant counting algorithm was proposed to reduce the number of input variables, and to remove requirements for acquiring geometric or statistical information. The object detection model named You Only Look Once version 3 (YOLOv3) and photogrammetry were utilized to separate, locate, and count cotton plants in the seedling stage. The proposed algorithm was tested with four different UAS datasets, containing variability in plant size, overall illumination, and background brightness. Root mean square error (RMSE) and R2 values of the optimal plant count results ranged from 0.50 to 0.60 plants per linear meter of row (number of plants within 1 m distance along the planting row direction) and 0.96 to 0.97, respectively. The object detection algorithm, trained with variable plant size, ground wetness, and lighting conditions generally resulted in a lower detection error, unless an observable difference of developmental stages of cotton existed. The proposed plant counting algorithm performed well with 0&ndash;14 plants per linear meter of row, when cotton plants are generally separable in the seedling stage. This study is expected to provide an automated methodology for in situ evaluation of plant emergence using UAS data.
KW  - unmanned aircraft systems
KW  - remote sensing
KW  - plant population assessment
KW  - plant count
KW  - agriculture
KW  - cotton
KW  - deep learning
KW  - YOLOv3
DO  - 10.3390/rs12182981
ER  -
TY  - EJOU
AU  - Almeshal, Abdullah M.
AU  - Alenezi, Mohammad R.
AU  - Alshatti, Abdullah K.
TI  - Accuracy Assessment of Small Unmanned Aerial Vehicle for Traffic Accident Photogrammetry in the Extreme Operating Conditions of Kuwait
T2  - Information

PY  - 2020
VL  - 11
IS  - 9
SN  - 2078-2489

AB  - This study presents the first accuracy assessment of a low cost small unmanned aerial vehicle (sUAV) in reconstructing three dimensional (3D) models of traffic accidents at extreme operating environments. To date, previous studies have focused on the feasibility of adopting sUAVs in traffic accidents photogrammetry applications as well as the accuracy at normal operating conditions. In this study, 3D models of simulated accident scenes were reconstructed using a low-cost sUAV and cloud-based photogrammetry platform. Several experiments were carried out to evaluate the measurements accuracy at different flight altitudes during high temperature, low light, scattered rain and dusty high wind environments. Quantitative analyses are presented to highlight the precision range of the reconstructed traffic accident 3D model. Reported results range from highly accurate to fairly accurate represented by the root mean squared error (RMSE) range between 0.97 and 4.66 and a mean percentage absolute error (MAPE) between 1.03% and 20.2% at normal and extreme operating conditions, respectively. The findings offer an insight into the robustness and generalizability of UAV-based photogrammetry method for traffic accidents at extreme environments.
KW  - UAV
KW  - aerial robotic vehicle
KW  - photogrammetry
KW  - robustness
DO  - 10.3390/info11090442
ER  -
TY  - EJOU
AU  - Gée, Christelle
AU  - Denimal, Emmanuel
TI  - RGB Image-Derived Indicators for Spatial Assessment of the Impact of Broadleaf Weeds on Wheat Biomass
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - In precision agriculture, the development of proximal imaging systems embedded in autonomous vehicles allows to explore new weed management strategies for site-specific plant application. Accurate monitoring of weeds while controlling wheat growth requires indirect measurements of leaf area index (LAI) and above-ground dry matter biomass (BM) at early growth stages. This article explores the potential of RGB images to assess crop-weed competition in a wheat (Triticum aestivum L.) crop by generating two new indicators, the weed pressure (WP) and the local wheat biomass production (&delta;BMc). The fractional vegetation cover (FVC) of the crop and the weeds was automatically determined from the images with a SVM-RBF classifier, using bag of visual word vectors as inputs. It is based on a new vegetation index called MetaIndex, defined as a vote of six indices widely used in the literature. Beyond a simple map of weed infestation, the map of WP describes the crop-weed competition. The map of &delta;BMc, meanwhile, evaluates the local wheat above-ground biomass production and informs us about a potential stress. It is generated from the wheat FVC because it is highly correlated with LAI (r2 = 0.99) and BM (r2 = 0.93) obtained by destructive methods. By combining these two indicators, we aim at determining whether the origin of the wheat stress is due to weeds or not. This approach opens up new perspectives for the monitoring of weeds and the monitoring of their competition during crop growth with non-destructive and proximal sensing technologies in the early stages of development.
KW  - weed pressure
KW  - crop-weed competition
KW  - machine learning
KW  - vegetation index
KW  - visible images
KW  - SVM-RBF classification
DO  - 10.3390/rs12182982
ER  -
TY  - EJOU
AU  - Liu, Bo
AU  - Du, Shihong
AU  - Du, Shouji
AU  - Zhang, Xiuyuan
TI  - Incorporating Deep Features into GEOBIA Paradigm for Remote Sensing Imagery Classification: A Patch-Based Approach
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - The fast and accurate creation of land use/land cover maps from very-high-resolution (VHR) remote sensing imagery is crucial for urban planning and environmental monitoring. Geographic object-based image analysis methods (GEOBIA) provide an effective solution using image objects instead of individual pixels in VHR remote sensing imagery analysis. Simultaneously, convolutional neural networks (CNN) have been widely used in the image processing field because of their powerful feature extraction capabilities. This study presents a patch-based strategy for integrating deep features into GEOBIA for VHR remote sensing imagery classification. To extract deep features from irregular image objects through CNN, a patch-based approach is proposed for representing image objects and learning patch-based deep features, and a deep features aggregation method is proposed for aggregating patch-based deep features into object-based deep features. Finally, both object and deep features are integrated into a GEOBIA paradigm for classifying image objects. We explored the influences of segmentation scales and patch sizes in our method and explored the effectiveness of deep and object features in classification. Moreover, we performed 5-fold stratified cross validations 50 times to explore the uncertainty of our method. Additionally, we explored the importance of deep feature aggregation, and we evaluated our method by comparing it with three state-of-the-art methods in a Beijing dataset and Zurich dataset. The results indicate that smaller segmentation scales were more conducive to VHR remote sensing imagery classification, and it was not appropriate to select too large or too small patches as the patch size should be determined by imagery and its resolution. Moreover, we found that deep features are more effective than object features, while object features still matter for image classification, and deep feature aggregation is a critical step in our method. Finally, our method can achieve the highest overall accuracies compared with the state-of-the-art methods, and the overall accuracies are 91.21% for the Beijing dataset and 99.05% for the Zurich dataset.
KW  - GEOBIA
KW  - convolutional neural networks
KW  - very-high-resolution remote sensing images
DO  - 10.3390/rs12183007
ER  -
TY  - EJOU
AU  - Machefer, Mélissande
AU  - Lemarchand, François
AU  - Bonnefond, Virginie
AU  - Hitchins, Alasdair
AU  - Sidiropoulos, Panagiotis
TI  - Mask R-CNN Refitting Strategy for Plant Counting and Sizing in UAV Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - This work introduces a method that combines remote sensing and deep learning into a framework that is tailored for accurate, reliable and efficient counting and sizing of plants in aerial images. The investigated task focuses on two low-density crops, potato and lettuce. This double objective of counting and sizing is achieved through the detection and segmentation of individual plants by fine-tuning an existing deep learning architecture called Mask R-CNN. This paper includes a thorough discussion on the optimal parametrisation to adapt the Mask R-CNN architecture to this novel task. As we examine the correlation of the Mask R-CNN performance to the annotation volume and granularity (coarse or refined) of remotely sensed images of plants, we conclude that transfer learning can be effectively used to reduce the required amount of labelled data. Indeed, a previously trained Mask R-CNN on a low-density crop can improve performances after training on new crops. Once trained for a given crop, the Mask R-CNN solution is shown to outperform a manually-tuned computer vision algorithm. Model performances are assessed using intuitive metrics such as Mean Average Precision (mAP) from Intersection over Union (IoU) of the masks for individual plant segmentation and Multiple Object Tracking Accuracy (MOTA) for detection. The presented model reaches an mAP of 0.418 for potato plants and 0.660 for lettuces for the individual plant segmentation task. In detection, we obtain a MOTA of 0.781 for potato plants and 0.918 for lettuces.
KW  - UAV
KW  - crop mapping
KW  - image analysis
KW  - precision agriculture
KW  - deep learning
KW  - individual plant segmentation
KW  - plant detection
KW  - transfer learning
DO  - 10.3390/rs12183015
ER  -
TY  - EJOU
AU  - Lai, Ying-Chih
AU  - Huang, Zong-Ying
TI  - Detection of a Moving UAV Based on Deep Learning-Based Distance Estimation
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - Distance information of an obstacle is important for obstacle avoidance in many applications, and could be used to determine the potential risk of object collision. In this study, the detection of a moving fixed-wing unmanned aerial vehicle (UAV) with deep learning-based distance estimation to conduct a feasibility study of sense and avoid (SAA) and mid-air collision avoidance of UAVs is proposed by using a monocular camera to detect and track an incoming UAV. A quadrotor is regarded as an owned UAV, and it is able to estimate the distance of an incoming fixed-wing intruder. The adopted object detection method is based on the you only look once (YOLO) object detector. Deep neural network (DNN) and convolutional neural network (CNN) methods are applied to exam their performance in the distance estimation of moving objects. The feature extraction of fixed-wing UAVs is based on the VGG-16 model, and then its result is applied to the distance network to estimate the object distance. The proposed model is trained by using synthetic images from animation software and validated by using both synthetic and real flight videos. The results show that the proposed active vision-based scheme is able to detect and track a moving UAV with high detection accuracy and low distance errors.
KW  - unmanned aerial vehicle (UAV)
KW  - you only look once (YOLO)
KW  - deep neural network (DNN)
KW  - convolutional neural network (CNN)
KW  - object detection
KW  - sense and avoid (SAA)
KW  - mid-air collision avoidance
DO  - 10.3390/rs12183035
ER  -
TY  - EJOU
AU  - Yang, Chin-Ying
AU  - Yang, Ming-Der
AU  - Tseng, Wei-Cheng
AU  - Hsu, Yu-Chun
AU  - Li, Guan-Sin
AU  - Lai, Ming-Hsin
AU  - Wu, Dong-Hong
AU  - Lu, Hsiu-Ying
TI  - Assessment of Rice Developmental Stage Using Time Series UAV Imagery for Variable Irrigation Management
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 18
SN  - 1424-8220

AB  - Rice is one of the three major crops in the world and is the major crop in Asia. Climate change and water resource shortages may result in decreases in rice yields and possible food shortage crises. In this study, water-saving farming management was tested, and IOT field water level monitoring was used to regulate water inflow automatically. Plant height (PH) is an important phenotype to be used to determine difference in rice growth periods and yields using water-saving irrigation. An unmanned aerial vehicle (UAV) with an RGB camera captured sequential images of rice fields to estimate rice PH compared with PH measured on site for estimating rice growth stages. The test results, with two crop harvests in 2019, revealed that with adequate image calibration, the correlation coefficient between UAV-PH and field-PH was higher than 0.98, indicating that UAV images can accurately determine rice PH in the field and rice growth phase. The study demonstrated that water-saving farming is effective, decreasing water usage for the first and second crops of 2019 by 53.5% and 21.7%, respectively, without influencing the growth period and final yield. Coupled with an automated irrigation system, rice farming can be adaptive to water shortage situations.
KW  - UAV
KW  - image processing
KW  - irrigation
KW  - plant height
DO  - 10.3390/s20185354
ER  -
TY  - EJOU
AU  - Contreras, Ruben
AU  - Ayala, Angel
AU  - Cruz, Francisco
TI  - Unmanned Aerial Vehicle Control through Domain-Based Automatic Speech Recognition
T2  - Computers

PY  - 2020
VL  - 9
IS  - 3
SN  - 2073-431X

AB  - Currently, unmanned aerial vehicles, such as drones, are becoming a part of our lives and extend to many areas of society, including the industrialized world. A common alternative for controlling the movements and actions of the drone is through unwired tactile interfaces, for which different remote control devices are used. However, control through such devices is not a natural, human-like communication interface, which sometimes is difficult to master for some users. In this research, we experimented with a domain-based speech recognition architecture to effectively control an unmanned aerial vehicle such as a drone. The drone control was performed in a more natural, human-like way to communicate the instructions. Moreover, we implemented an algorithm for command interpretation using both Spanish and English languages, as well as to control the movements of the drone in a simulated domestic environment. We conducted experiments involving participants giving voice commands to the drone in both languages in order to compare the effectiveness of each, considering the mother tongue of the participants in the experiment. Additionally, different levels of distortion were applied to the voice commands to test the proposed approach when it encountered noisy input signals. The results obtained showed that the unmanned aerial vehicle was capable of interpreting user voice instructions. Speech-to-action recognition improved for both languages with phoneme matching in comparison to only using the cloud-based algorithm without domain-based instructions. Using raw audio inputs, the cloud-based approach achieves 74.81% and 97.04% accuracy for English and Spanish instructions, respectively. However, with our phoneme matching approach the results are improved, yielding 93.33% accuracy for English and 100.00% accuracy for Spanish.
KW  - drone control
KW  - automatic speech recognition
KW  - robot simulator
DO  - 10.3390/computers9030075
ER  -
TY  - EJOU
AU  - Osadchiev, Alexander
AU  - Barymova, Alexandra
AU  - Sedakov, Roman
AU  - Zhiba, Roman
AU  - Dbar, Roman
TI  - Spatial Structure, Short-temporal Variability, and Dynamical Features of Small River Plumes as Observed by Aerial Drones: Case Study of the Kodor and Bzyp River Plumes
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - Quadcopters can continuously observe ocean surface with high spatial resolution from relatively low altitude, albeit with certain limitations of their usage. Remote sensing from quadcopters provides unprecedented ability to study small river plumes formed in the coastal sea. The main goal of the current work is to describe structure and temporal variability of small river plumes on small spatial and temporal scales, which are limitedly covered by previous studies. We analyze optical imagery and video records acquired by quadcopters and accompanied by synchronous in situ measurements and satellite observations within the Kodor and Bzyp plumes, which are located in the northeastern part of the Black Sea. We describe extremely rapid response of these river plume to energetic rotating coastal eddies. We reveal several types of internal waves within these river plumes, measure their spatial and dynamical characteristics, and identify mechanisms of their generation. We suggest a new mechanism of formation of undulate fronts between small river plumes and ambient sea, which induces energetic lateral mixing across these fronts. The results reported in this study are addressed for the first time as previous related works were mainly limited by low spatial and/or temporal resolution of in situ measurements and satellite imagery.
KW  - small river plume
KW  - aerial drone
KW  - coastal processes
KW  - frontal zones
KW  - internal waves
DO  - 10.3390/rs12183079
ER  -
TY  - EJOU
AU  - Abdellatif, Mohamed
AU  - Peel, Harriet
AU  - Cohn, Anthony G.
AU  - Fuentes, Raul
TI  - Pavement Crack Detection from Hyperspectral Images Using a Novel Asphalt Crack Index
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 18
SN  - 2072-4292

AB  - Detection of road pavement cracks is important and needed at an early stage to repair the road and extend its lifetime for maintaining city roads. Cracks are hard to detect from images taken with visible spectrum cameras due to noise and ambiguity with background textures besides the lack of distinct features in cracks. Hyperspectral images are sensitive to surface material changes and their potential for road crack detection is explored here. The key observation is that road cracks reveal the interior material that is different from the worn surface material. A novel asphalt crack index is introduced here as an additional clue that is sensitive to the spectra in the range 450&ndash;550 nm. The crack index is computed and found to be strongly correlated with the appearance of fresh asphalt cracks. The new index is then used to differentiate cracks from road surfaces. Several experiments have been made, which confirmed that the proposed index is effective for crack detection. The recall-precision analysis showed an increase in the associated F1-score by an average of 21.37% compared to the VIS2 metric in the literature (a metric used to classify pavement condition from hyperspectral data).
KW  - pavement crack detection
KW  - pavement defect inspection
KW  - asphalt crack index
KW  - hyper-spectral imaging
KW  - autonomous road inspection
DO  - 10.3390/rs12183084
ER  -
TY  - EJOU
AU  - Yang, Shuting
AU  - Gu, Lingjia
AU  - Li, Xiaofeng
AU  - Jiang, Tao
AU  - Ren, Ruizhi
TI  - Crop Classification Method Based on Optimal Feature Selection and Hybrid CNN-RF Networks for Multi-Temporal Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - Although efforts and progress have been made in crop classification using optical remote sensing images, it is still necessary to make full use of the high spatial, temporal, and spectral resolutions of remote sensing images. However, with the increasing volume of remote sensing data, a key emerging issue in the field of crop classification is how to find useful information from massive data to balance classification accuracy and processing time. To address this challenge, we developed a novel crop classification method, combining optimal feature selection (OFSM) with hybrid convolutional neural network-random forest (CNN-RF) networks for multi-temporal optical remote sensing images. This research used 234 features including spectral, segmentation, color, and texture features from three scenes of Sentinel-2 images to identify crop types in the Jilin province of northeast China. To effectively extract the effective features of remote sensing data with lower time requirements, the use of OFSM was proposed with the results compared with two traditional feature selection methods (TFSM): random forest feature importance selection (RF-FI) and random forest recursive feature elimination (RF-RFE). Although the time required for OFSM was 26.05 s, which was between RF-FI with 1.97 s and RF-RFE with 132.54 s, OFSM outperformed RF-FI and RF-RFE in terms of the overall accuracy (OA) of crop classification by 4% and 0.3%, respectively. On the basis of obtaining effective feature information, to further improve the accuracy of crop classification we designed two hybrid CNN-RF networks to leverage the advantages of one-dimensional convolution (Conv1D) and Visual Geometry Group (VGG) with random forest (RF), respectively. Based on the selected optimal features using OFSM, four networks were tested for comparison: Conv1D-RF, VGG-RF, Conv1D, and VGG. Conv1D-RF achieved the highest OA at 94.27% as compared with VGG-RF (93.23%), Conv1D (92.59%), and VGG (91.89%), indicating that the Conv1D-RF method with optimal feature input provides an effective and efficient method of time series representation for multi-temporal crop-type classification.
KW  - feature selection
KW  - convolutional neural network
KW  - crop classification
KW  - multi-temporal remote sensing images
KW  - data fusion
DO  - 10.3390/rs12193119
ER  -
TY  - EJOU
AU  - Hu, Jie
AU  - Wang, Tuan
AU  - Yang, Jiacheng
AU  - Lan, Yubin
AU  - Lv, Shilei
AU  - Zhang, Yali
TI  - WSN-Assisted UAV Trajectory Adjustment for Pesticide Drift Control
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 19
SN  - 1424-8220

AB  - Unmanned Aerial Vehicles (UAVs) have been widely applied for pesticide spraying as they have high efficiency and operational flexibility. However, the pesticide droplet drift caused by wind may decrease the pesticide spraying efficiency and pollute the environment. A precision spraying system based on an airborne meteorological monitoring platform on manned agricultural aircrafts is not adaptable for. So far, there is no better solution for controlling droplet drift outside the target area caused by wind, especially by wind gusts. In this regard, a UAV trajectory adjustment system based on Wireless Sensor Network (WSN) for pesticide drift control was proposed in this research. By collecting data from ground WSN, the UAV utilizes the wind speed and wind direction as inputs to autonomously adjust its trajectory for keeping droplet deposition in the target spraying area. Two optimized algorithms, namely deep reinforcement learning and particle swarm optimization, were applied to generate the newly modified flight route. At the same time, a simplified pesticide droplet drift model that includes wind speed and wind direction as parameters was developed and adopted to simulate and compute the drift distance of pesticide droplets. Moreover, an LSTM-based wind speed prediction model and a RNN-based wind direction prediction model were established, so as to address the problem of missing the latest wind data caused by communication latency or a lack of connection with the ground nodes. Finally, experiments were carried out to test the communication latency between UAV and ground WSN, and to evaluate the proposed scheme with embedded Raspberry Pi boards in UAV for feasibility verification. Results show that the WSN-assisted UAV trajectory adjustment system is capable of providing a better performance of on-target droplet deposition for real time pesticide spraying with UAV.
KW  - WSN
KW  - UAV
KW  - trajectory adjustment
KW  - drift control
KW  - DQN
KW  - PSO
DO  - 10.3390/s20195473
ER  -
TY  - EJOU
AU  - Guan, HaiXiang
AU  - Liu, HuanJun
AU  - Meng, XiangTian
AU  - Luo, Chong
AU  - Bao, YiLin
AU  - Ma, YuYang
AU  - Yu, ZiYang
AU  - Zhang, XinLe
TI  - A Quantitative Monitoring Method for Determining Maize Lodging in Different Growth Stages
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - Many studies have achieved efficient and accurate methods for identifying crop lodging under homogeneous field surroundings. However, under complex field conditions, such as diverse fertilization methods, different crop growth stages, and various sowing periods, the accuracy of lodging identification must be improved. Therefore, a maize plot featuring different growth stages was selected in this study to explore an applicable and accurate lodging extraction method. Based on the Akaike information criterion (AIC), we propose an effective and rapid feature screening method (AIC method) and compare its performance using indexed methods (i.e., variation coefficient and relative difference). Seven feature sets extracted from unmanned aerial vehicle (UAV) images of lodging and nonlodging maize were established using a canopy height model (CHM) and the multispectral imagery acquired from the UAV. In addition to accuracy parameters (i.e., Kappa coefficient and overall accuracy), the difference index (DI) was applied to search for the optimal window size of texture features. After screening all feature sets by applying the AIC method, binary logistic regression classification (BLRC), maximum likelihood classification (MLC), and random forest classification (RFC) were utilized to discriminate among lodging and nonlodging maize based on the selected features. The results revealed that the optimal window sizes of the gray-level cooccurrence matrix (GLCM) and the gray-level difference histogram statistical (GLDM) texture information were 17 &times; 17 and 21 &times; 21, respectively. The AIC method incorporating GLCM texture yielded satisfactory results, obtaining an average accuracy of 82.84% and an average Kappa value of 0.66 and outperforming the index screening method (59.64%, 0.19). Furthermore, the canopy structure feature (CSF) was more beneficial than other features for identifying maize lodging areas at the plot scale. Based on the AIC method, we achieved a positive maize lodging recognition result using the CSFs and BLRC. This study provides a highly robust and novel method for monitoring maize lodging in complicated plot environments.
KW  - lodging
KW  - unmanned aerial vehicle (UAV)
KW  - canopy structure feature
KW  - Akaike information criterion (AIC) method
KW  - difference index (DI)
KW  - texture
DO  - 10.3390/rs12193149
ER  -
TY  - EJOU
AU  - El Boudani, Brahim
AU  - Kanaris, Loizos
AU  - Kokkinis, Akis
AU  - Kyriacou, Michalis
AU  - Chrysoulas, Christos
AU  - Stavrou, Stavros
AU  - Dagiuklas, Tasos
TI  - Implementing Deep Learning Techniques in 5G IoT Networks for 3D Indoor Positioning: DELTA (DeEp Learning-Based Co-operaTive Architecture)
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 19
SN  - 1424-8220

AB  - In the near future, the fifth-generation wireless technology is expected to be rolled out, offering low latency, high bandwidth and multiple antennas deployed in a single access point. This ecosystem will help further enhance various location-based scenarios such as assets tracking in smart factories, precise smart management of hydroponic indoor vertical farms and indoor way-finding in smart hospitals. Such a system will also integrate existing technologies like the Internet of Things (IoT), WiFi and other network infrastructures. In this respect, 5G precise indoor localization using heterogeneous IoT technologies (Zigbee, Raspberry Pi, Arduino, BLE, etc.) is a challenging research area. In this work, an experimental 5G testbed has been designed integrating C-RAN and IoT networks. This testbed is used to improve both vertical and horizontal localization (3D Localization) in a 5G IoT environment. To achieve this, we propose the DEep Learning-based co-operaTive Architecture (DELTA) machine learning model implemented on a 3D multi-layered fingerprint radiomap. The DELTA begins by estimating the 2D location. Then, the output is recursively used to predict the 3D location of a mobile station. This approach is going to benefit use cases such as 3D indoor navigation in multi-floor smart factories or in large complex buildings. Finally, we have observed that the proposed model has outperformed traditional algorithms such as Support Vector Machine (SVM) and K-Nearest Neighbor (KNN).
KW  - 5G IoT
KW  - indoor positioning
KW  - deep learning
KW  - tracking
KW  - localization
KW  - navigation
KW  - positioning accuracy
KW  - single access point positioning
KW  - Internet of Things
DO  - 10.3390/s20195495
ER  -
TY  - EJOU
AU  - Courtrai, Luc
AU  - Pham, Minh-Tan
AU  - Lefèvre, Sébastien
TI  - Small Object Detection in Remote Sensing Images Based on Super-Resolution with Auxiliary Generative Adversarial Networks
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - This article tackles the problem of detecting small objects in satellite or aerial remote sensing images by relying on super-resolution to increase image spatial resolution, thus the size and details of objects to be detected. We show how to improve the super-resolution framework starting from the learning of a generative adversarial network (GAN) based on residual blocks and then its integration into a cycle model. Furthermore, by adding to the framework an auxiliary network tailored for object detection, we considerably improve the learning and the quality of our final super-resolution architecture, and more importantly increase the object detection performance. Besides the improvement dedicated to the network architecture, we also focus on the training of super-resolution on target objects, leading to an object-focused approach. Furthermore, the proposed strategies do not depend on the choice of a baseline super-resolution framework, hence could be adopted for current and future state-of-the-art models. Our experimental study on small vehicle detection in remote sensing data conducted on both aerial and satellite images (i.e., ISPRS Potsdam and xView datasets) confirms the effectiveness of the improved super-resolution methods to assist with the small object detection tasks.
KW  - small object detection
KW  - super-resolution
KW  - remote sensing
KW  - deep learning
KW  - generative adversarial network (GAN)
KW  - cycle GAN
KW  - Wasserstein GAN
KW  - auxiliary network
DO  - 10.3390/rs12193152
ER  -
TY  - EJOU
AU  - Niazian, Mohsen
AU  - Niedbała, Gniewko
TI  - Machine Learning for Plant Breeding and Biotechnology
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 10
SN  - 2077-0472

AB  - Classical univariate and multivariate statistics are the most common methods used for data analysis in plant breeding and biotechnology studies. Evaluation of genetic diversity, classification of plant genotypes, analysis of yield components, yield stability analysis, assessment of biotic and abiotic stresses, prediction of parental combinations in hybrid breeding programs, and analysis of in vitro-based biotechnological experiments are mainly performed by classical statistical methods. Despite successful applications, these classical statistical methods have low efficiency in analyzing data obtained from plant studies, as the genotype, environment, and their interaction (G &times; E) result in nondeterministic and nonlinear nature of plant characteristics. Large-scale data flow, including phenomics, metabolomics, genomics, and big data, must be analyzed for efficient interpretation of results affected by G &times; E. Nonlinear nonparametric machine learning techniques are more efficient than classical statistical models in handling large amounts of complex and nondeterministic information with &ldquo;multiple-independent variables versus multiple-dependent variables&rdquo; nature. Neural networks, partial least square regression, random forest, and support vector machines are some of the most fascinating machine learning models that have been widely applied to analyze nonlinear and complex data in both classical plant breeding and in vitro-based biotechnological studies. High interpretive power of machine learning algorithms has made them popular in the analysis of plant complex multifactorial characteristics. The classification of different plant genotypes with morphological and molecular markers, modeling and predicting important quantitative characteristics of plants, the interpretation of complex and nonlinear relationships of plant characteristics, and predicting and optimizing of in vitro breeding methods are the examples of applications of machine learning in conventional plant breeding and in vitro-based biotechnological studies. Precision agriculture is possible through accurate measurement of plant characteristics using imaging techniques and then efficient analysis of reliable extracted data using machine learning algorithms. Perfect interpretation of high-throughput phenotyping data is applicable through coupled machine learning-image processing. Some applied and potentially applicable capabilities of machine learning techniques in conventional and in vitro-based plant breeding studies have been discussed in this overview. Discussions are of great value for future studies and could inspire researchers to apply machine learning in new layers of plant breeding.
KW  - artificial neural networks
KW  - big data
KW  - classification
KW  - high-throughput phenotyping
KW  - modeling
KW  - predicting
DO  - 10.3390/agriculture10100436
ER  -
TY  - EJOU
AU  - Park, Jinseok
AU  - Jang, Seongju
AU  - Hong, Rokgi
AU  - Suh, Kyo
AU  - Song, Inhong
TI  - Development of Land Cover Classification Model Using AI Based FusionNet Network
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - Prompt updates of land cover maps are important, as spatial information of land cover is widely used in many areas. However, current manual digitizing methods are time consuming and labor intensive, hindering rapid updates of land cover maps. The objective of this study was to develop an artificial intelligence (AI) based land cover classification model that allows for rapid land cover classification from high-resolution remote sensing (HRRS) images. The model comprises of three modules: pre-processing, land cover classification, and post-processing modules. The pre-processing module separates the HRRS image into multiple aspects by overlapping 75% using the sliding window algorithm. The land cover classification module was developed using the convolutional neural network (CNN) concept, based the FusionNet network and used to assign a land cover type to the separated HRRS images. Post-processing module determines ultimate land cover types by summing up the separated land cover result from the land cover classification module. Model training and validation were conducted to evaluate the performance of the developed model. The land cover maps and orthographic images of 547.29 km2 in area from the Jeonnam province in Korea were used to train the model. For model validation, two spatial and temporal different sites, one from Subuk-myeon of Jeonnam province in 2018 and the other from Daseo-myeon of Chungbuk province in 2016, were randomly chosen. The model performed reasonably well, demonstrating overall accuracies of 0.81 and 0.71, and kappa coefficients of 0.75 and 0.64, for the respective validation sites. The model performance was better when only considering the agricultural area by showing overall accuracy of 0.83 and kappa coefficients of 0.73. It was concluded that the developed model may assist rapid land cover update especially for agricultural areas and incorporation field boundary lineation is suggested as future study to further improve the model accuracy.
KW  - land cover map
KW  - land cover classification
KW  - convolutional neural network
KW  - AI
KW  - agricultural area
DO  - 10.3390/rs12193171
ER  -
TY  - EJOU
AU  - Camarretta, Nicolò
AU  - A. Harrison, Peter
AU  - Lucieer, Arko
AU  - M. Potts, Brad
AU  - Davidson, Neil
AU  - Hunt, Mark
TI  - From Drones to Phenotype: Using UAV-LiDAR to Detect Species and Provenance Variation in Tree Productivity and Structure
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - The use of unmanned aerial vehicles (UAVs) for remote sensing of natural environments has increased over the last decade. However, applications of this technology for high-throughput individual tree phenotyping in a quantitative genetic framework are rare. We here demonstrate a two-phased analytical pipeline that rapidly phenotypes and filters for genetic signals in traditional and novel tree productivity and architectural traits derived from ultra-dense light detection and ranging (LiDAR) point clouds. The goal of this study was rapidly phenotype individual trees to understand the genetic basis of ecologically and economically significant traits important for guiding the management of natural resources. Individual tree point clouds were acquired using UAV-LiDAR captured over a multi-provenance common-garden restoration field trial located in Tasmania, Australia, established using two eucalypt species (Eucalyptus pauciflora and Eucalyptus tenuiramis). Twenty-five tree productivity and architectural traits were calculated for each individual tree point cloud. The first phase of the analytical pipeline found significant species differences in 13 of the 25 derived traits, revealing key structural differences in productivity and crown architecture between species. The second phase investigated the within species variation in the same 25 structural traits. Significant provenance variation was detected for 20 structural traits in E. pauciflora and 10 in E. tenuiramis, with signals of divergent selection found for 11 and 7 traits, respectively, putatively driven by the home-site environment shaping the observed variation. Our results highlight the genetic-based diversity within and between species for traits important for forest structure, such as crown density and structural complexity. As species and provenances are being increasingly translocated across the landscape to mitigate the effects of rapid climate change, our results that were achieved through rapid phenotyping using UAV-LiDAR, raise the need to understand the functional value of productivity and architectural traits reflecting species and provenance differences in crown structure and the interplay they have on the dependent biotic communities.
KW  - UAV
KW  - LiDAR
KW  - structural traits
KW  - genetic variation
KW  - individual tree
KW  - tree architecture
KW  - Eucalyptus
DO  - 10.3390/rs12193184
ER  -
TY  - EJOU
AU  - López-Calderón, Magali J.
AU  - Estrada-Ávalos, Juan
AU  - Rodríguez-Moreno, Víctor M.
AU  - Mauricio-Ruvalcaba, Jorge E.
AU  - Martínez-Sifuentes, Aldo R.
AU  - Delgado-Ramírez, Gerardo
AU  - Miguel-Valle, Enrique
TI  - Estimation of Total Nitrogen Content in Forage Maize (Zea mays L.) Using Spectral Indices: Analysis by Random Forest
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 10
SN  - 2077-0472

AB  - Knowing the total Nitrogen content (Nt) of forage maize (Zea mays) is important so that decisions can be made quickly and efficiently to adjust the timing and amount of both irrigation and fertilizer. In 2017 and 2018 during three growing cycles in two study plots, leaf samples were collected and the Dumas method was used to estimate Nt. During the same growing seasons and on the same sampling plots, a Parrot Sequoia camera mounted on an unmanned aerial vehicle (UAV) was used to collect high resolution images of forage maize study plots. Thirteen multispectral indices were generated and, from these, a Random Forest (RF) algorithm was used to estimate Nt. RF is a machine-learning technique and is designed to work with extremely large datasets. Overall analysis showed five of the 13 indices as the most important. One of these five, the Transformed Chlorophyll Absorption in Reflectance Index/Optimized Soil-Adjusted Vegetation Index, was found to be the most important for estimation of Nt in forage maize (R2 = 0.76). RF handled the complex dataset in a time-efficient manner and Nt did not differ significantly when compared between traditional methods of evaluating Nt at the canopy level and using UAVs and RF to estimate Nt in forage maize. This result is an opportunity to explore many new research options in precision farming and digital agriculture.
KW  - nitrogen content
KW  - remote sensing
KW  - spectral indices
KW  - random forest
DO  - 10.3390/agriculture10100451
ER  -
TY  - EJOU
AU  - Chen, Jian
AU  - Zhang, Zichao
AU  - Zhang, Kai
AU  - Wang, Shubo
AU  - Han, Yu
TI  - UAV-Borne LiDAR Crop Point Cloud Enhancement Using Grasshopper Optimization and Point Cloud Up-Sampling Network
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - Because of low accuracy and density of crop point clouds obtained by the Unmanned Aerial Vehicle (UAV)-borne Light Detection and Ranging (LiDAR) scanning system of UAV, an integrated navigation and positioning optimization method based on the grasshopper optimization algorithm (GOA) and a point cloud density enhancement method were proposed. Firstly, a global positioning system (GPS)/inertial navigation system (INS) integrated navigation and positioning information fusion method based on a Kalman filter was constructed. Then, the GOA was employed to find the optimal solution by iterating the system noise variance matrix Q and measurement noise variance matrix R of Kalman filter. By feeding the optimal solution into the Kalman filter, the error variances of longitude were reduced to 0.00046 from 0.0091, and the error variances of latitude were reduced to 0.00034 from 0.0047. Based on the integrated navigation, an UAV-borne LiDAR scanning system was built for obtaining the crop point. During offline processing, the crop point cloud was filtered and transformed into WGS-84, the density clustering algorithm improved by the particle swarm optimization (PSO) algorithm was employed to the clustering segment. After the clustering segment, the pre-trained Point Cloud Up-Sampling Network (PU-net) was used for density enhancement of point cloud data and to carry out three-dimensional reconstruction. The features of the crop point cloud were kept under the processing of reconstruction model; meanwhile, the density of the crop point cloud was quadrupled.
KW  - UAV-borne LiDAR scanning system
KW  - grasshopper optimization algorithm
KW  - GPS/INS integrated navigation
KW  - point cloud up-sampling network (PU-net)
KW  - clustering segmentation
KW  - 3-dimensional reconstruction
DO  - 10.3390/rs12193208
ER  -
TY  - EJOU
AU  - Xie, Jingyi
AU  - Peng, Xiaodong
AU  - Wang, Haijiao
AU  - Niu, Wenlong
AU  - Zheng, Xiao
TI  - UAV Autonomous Tracking and Landing Based on Deep Reinforcement Learning Strategy
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 19
SN  - 1424-8220

AB  - Unmanned aerial vehicle (UAV) autonomous tracking and landing is playing an increasingly important role in military and civil applications. In particular, machine learning has been successfully introduced to robotics-related tasks. A novel UAV autonomous tracking and landing approach based on a deep reinforcement learning strategy is presented in this paper, with the aim of dealing with the UAV motion control problem in an unpredictable and harsh environment. Instead of building a prior model and inferring the landing actions based on heuristic rules, a model-free method based on a partially observable Markov decision process (POMDP) is proposed. In the POMDP model, the UAV automatically learns the landing maneuver by an end-to-end neural network, which combines the Deep Deterministic Policy Gradients (DDPG) algorithm and heuristic rules. A Modular Open Robots Simulation Engine (MORSE)-based reinforcement learning framework is designed and validated with a continuous UAV tracking and landing task on a randomly moving platform in high sensor noise and intermittent measurements. The simulation results show that when the moving platform is moving in different trajectories, the average landing success rate of the proposed algorithm is about 10% higher than that of the Proportional-Integral-Derivative (PID) method. As an indirect result, a state-of-the-art deep reinforcement learning-based UAV control method is validated, where the UAV can learn the optimal strategy of a continuously autonomous landing and perform properly in a simulation environment.
KW  - quadrotor unmanned aerial vehicle
KW  - deep reinforcement learning
KW  - autonomous tracking and landing
DO  - 10.3390/s20195630
ER  -
TY  - EJOU
AU  - Maimaitiyiming, Matthew
AU  - Sagan, Vasit
AU  - Sidike, Paheding
AU  - Maimaitijiang, Maitiniyazi
AU  - Miller, Allison J.
AU  - Kwasniewski, Misha
TI  - Leveraging Very-High Spatial Resolution Hyperspectral and Thermal UAV Imageries for Characterizing Diurnal Indicators of Grapevine Physiology
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - Efficient and accurate methods to monitor crop physiological responses help growers better understand crop physiology and improve crop productivity. In recent years, developments in unmanned aerial vehicles (UAV) and sensor technology have enabled image acquisition at very-high spectral, spatial, and temporal resolutions. However, potential applications and limitations of very-high-resolution (VHR) hyperspectral and thermal UAV imaging for characterization of plant diurnal physiology remain largely unknown, due to issues related to shadow and canopy heterogeneity. In this study, we propose a canopy zone-weighting (CZW) method to leverage the potential of VHR (&le;9 cm) hyperspectral and thermal UAV imageries in estimating physiological indicators, such as stomatal conductance (Gs) and steady-state fluorescence (Fs). Diurnal flights and concurrent in-situ measurements were conducted during grapevine growing seasons in 2017 and 2018 in a vineyard in Missouri, USA. We used neural net classifier and the Canny edge detection method to extract pure vine canopy from the hyperspectral and thermal images, respectively. Then, the vine canopy was segmented into three canopy zones (sunlit, nadir, and shaded) using K-means clustering based on the canopy shadow fraction and canopy temperature. Common reflectance-based spectral indices, sun-induced chlorophyll fluorescence (SIF), and simplified canopy water stress index (siCWSI) were computed as image retrievals. Using the coefficient of determination (R2) established between the image retrievals from three canopy zones and the in-situ measurements as a weight factor, weighted image retrievals were calculated and their correlation with in-situ measurements was explored. The results showed that the most frequent and the highest correlations were found for Gs and Fs, with CZW-based Photochemical reflectance index (PRI), SIF, and siCWSI (PRICZW, SIFCZW, and siCWSICZW), respectively. When all flights combined for the given field campaign date, PRICZW, SIFCZW, and siCWSICZW significantly improved the relationship with Gs and Fs. The proposed approach takes full advantage of VHR hyperspectral and thermal UAV imageries, and suggests that the CZW method is simple yet effective in estimating Gs and Fs.
KW  - remote sensing
KW  - PRI
KW  - SIF
KW  - CWSI
KW  - stomatal conductance
KW  - fluorescence
KW  - canopy zone-weighing
DO  - 10.3390/rs12193216
ER  -
TY  - EJOU
AU  - Yow, Kin-Choong
AU  - Kim, Insu
TI  - General Moving Object Localization from a Single Flying Camera
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 19
SN  - 2076-3417

AB  - Object localization is an important task in the visual surveillance of scenes, and it has important applications in locating personnel and/or equipment in large open spaces such as a farm or a mine. Traditionally, object localization can be performed using the technique of stereo vision: using two fixed cameras for a moving object, or using a single moving camera for a stationary object. This research addresses the problem of determining the location of a moving object using only a single moving camera, and it does not make use of any prior information on the type of object nor the size of the object. Our technique makes use of a single camera mounted on a quadrotor drone, which flies in a specific pattern relative to the object in order to remove the depth ambiguity associated with their relative motion. In our previous work, we showed that with three images, we can recover the location of an object moving parallel to the direction of motion of the camera. In this research, we find that with four images, we can recover the location of an object moving linearly in an arbitrary direction. We evaluated our algorithm on over 70 image sequences of objects moving in various directions, and the results showed a much smaller depth error rate (less than 8.0% typically) than other state-of-the-art algorithms.
KW  - object localization
KW  - flying camera
KW  - ego-motion
KW  - depth ambiguity
KW  - stereo vision
KW  - depth recovery
DO  - 10.3390/app10196945
ER  -
TY  - EJOU
AU  - Osco, Lucas P.
AU  - Junior, José M.
AU  - Ramos, Ana P.
AU  - Furuya, Danielle E.
AU  - Santana, Dthenifer C.
AU  - Teodoro, Larissa P.
AU  - Gonçalves, Wesley N.
AU  - Baio, Fábio H.
AU  - Pistori, Hemerson
AU  - Junior, Carlos A.
AU  - Teodoro, Paulo E.
TI  - Leaf Nitrogen Concentration and Plant Height Prediction for Maize Using UAV-Based Multispectral Imagery and Machine Learning Techniques
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 19
SN  - 2072-4292

AB  - Under ideal conditions of nitrogen (N), maize (Zea mays L.) can grow to its full potential, reaching maximum plant height (PH). As a rapid and nondestructive approach, the analysis of unmanned aerial vehicles (UAV)-based imagery may be of assistance to estimate N and height. The main objective of this study is to present an approach to predict leaf nitrogen concentration (LNC, g kg&minus;1) and PH (m) with machine learning techniques and UAV-based multispectral imagery in maize plants. An experiment with 11 maize cultivars under two rates of N fertilization was carried during the 2017/2018 and 2018/2019 crop seasons. The spectral vegetation indices (VI) normalized difference vegetation index (NDVI), normalized difference red-edge index (NDRE), green normalized difference vegetation (GNDVI), and the soil adjusted vegetation index (SAVI) were extracted from the images and, in a computational system, used alongside the spectral bands as input parameters for different machine learning models. A randomized 10-fold cross-validation strategy, with a total of 100 replicates, was used to evaluate the performance of 9 supervised machine learning (ML) models using the Pearson&rsquo;s correlation coefficient (r), mean absolute error (MAE), coefficient of regression (R&sup2;), and root mean square error (RMSE) metrics. The results indicated that the random forest (RF) algorithm performed better, with r and RMSE, respectively, of 0.91 and 1.9 g.kg&minus;&sup1; for LNC, and 0.86 and 0.17 m for PH. It was also demonstrated that VIs contributed more to the algorithm&rsquo;s performances than individual spectral bands. This study concludes that the RF model is appropriate to predict both agronomic variables in maize and may help farmers to monitor their plants based upon their LNC and PH diagnosis and use this knowledge to improve their production rates in the subsequent seasons.
KW  - UAV
KW  - random forest
KW  - nitrogen
KW  - maize
DO  - 10.3390/rs12193237
ER  -
TY  - EJOU
AU  - Kerkech, Mohamed
AU  - Hafiane, Adel
AU  - Canals, Raphael
TI  - VddNet: Vine Disease Detection Network Based on Multispectral Images and Depth Map
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 20
SN  - 2072-4292

AB  - Vine pathologies generate several economic and environmental problems, causing serious difficulties for the viticultural activity. The early detection of vine disease can significantly improve the control of vine diseases and avoid spread of virus or fungi. Currently, remote sensing and artificial intelligence technologies are emerging in the field of precision agriculture. They offer interesting potential for crop disease management. However, despite the advances in these technologies, particularly deep learning technologies, many problems still present considerable challenges, such as semantic segmentation of images for disease mapping. In this paper, we present a new deep learning architecture called Vine Disease Detection Network (VddNet). It is based on three parallel auto-encoders integrating different information (i.e., visible, infrared and depth). Then, the decoder reconstructs and retrieves the features, and assigns a class to each output pixel. An orthophotos registration method is also proposed to align the three types of images and enable the processing by VddNet. The proposed architecture is assessed by comparing it with the most known architectures: SegNet, U-Net, DeepLabv3+ and PSPNet. The deep learning architectures were trained on multispectral data from an unmanned aerial vehicle (UAV) and depth map information extracted from 3D processing. The results of the proposed architecture show that the VddNet architecture achieves higher scores than the baseline methods. Moreover, this study demonstrates that the proposed method has many advantages compared to methods that directly use the UAV images.
KW  - plant disease detection
KW  - precision agriculture
KW  - UAV multispectral images
KW  - machine learning
KW  - orthophotos registration
KW  - 3D information
KW  - orthophotos segmentation
DO  - 10.3390/rs12203305
ER  -
TY  - EJOU
AU  - Santos, André A.
AU  - Rocha, Filipe A. S.
AU  - Reis, Agnaldo J. da R.
AU  - Guimarães, Frederico G.
TI  - Automatic System for Visual Detection of Dirt Buildup on Conveyor Belts Using Convolutional Neural Networks
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 20
SN  - 1424-8220

AB  - Conveyor belts are the most widespread means of transportation for large quantities of materials in the mining sector. Therefore, autonomous methods that can help human beings to perform the inspection of the belt conveyor system is a major concern for companies. In this context, we present in this work a novel and automatic visual detector that recognizes dirt buildup on the structures of conveyor belts, which is one of the tasks of the maintenance inspectors. This visual detector can be embedded as sensors in autonomous robots for the inspection activity. The proposed system involves training a convolutional neural network from RGB images. The use of the transfer learning technique, i.e., retraining consolidated networks for image classification with our collected images has shown very effective. Two different approaches for transfer learning have been analyzed. The best one presented an average accuracy of 0.8975 with an F-1 Score of 0.8773 for the dirt recognition. A field validation experiment served to evaluate the performance of the proposed system in a real time classification task.
KW  - convolutional neural network
KW  - conveyor belt
KW  - machine learning
DO  - 10.3390/s20205762
ER  -
TY  - EJOU
AU  - Song, Ahram
AU  - Kim, Yongil
TI  - Semantic Segmentation of Remote-Sensing Imagery Using Heterogeneous Big Data: International Society for Photogrammetry and Remote Sensing Potsdam and Cityscape Datasets
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 10
SN  - 2220-9964

AB  - Although semantic segmentation of remote-sensing (RS) images using deep-learning networks has demonstrated its effectiveness recently, compared with natural-image datasets, obtaining RS images under the same conditions to construct data labels is difficult. Indeed, small datasets limit the effective learning of deep-learning networks. To address this problem, we propose a combined U-net model that is trained using a combined weighted loss function and can handle heterogeneous datasets. The network consists of encoder and decoder blocks. The convolutional layers that form the encoder blocks are shared with the heterogeneous datasets, and the decoder blocks are assigned separate training weights. Herein, the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Cityscape datasets are used as the RS and natural-image datasets, respectively. When the layers are shared, only visible bands of the ISPRS Potsdam data are used. Experimental results show that when same-sized heterogeneous datasets are used, the semantic segmentation accuracy of the Potsdam data obtained using our proposed method is lower than that obtained using only the Potsdam data (four bands) with other methods, such as SegNet, DeepLab-V3+, and the simplified version of U-net. However, the segmentation accuracy of the Potsdam images is improved when the larger Cityscape dataset is used. The combined U-net model can effectively train heterogeneous datasets and overcome the insufficient training data problem in the context of RS-image datasets. Furthermore, it is expected that the proposed method can not only be applied to segmentation tasks of aerial images but also to tasks with various purposes of using big heterogeneous datasets.
KW  - semantic segmentation
KW  - deep learning
KW  - big dataset
KW  - ISPRS Potsdam dataset
KW  - Cityscape dataset
DO  - 10.3390/ijgi9100601
ER  -
TY  - EJOU
AU  - Na, Jiaming
AU  - Xue, Kaikai
AU  - Xiong, Liyang
AU  - Tang, Guoan
AU  - Ding, Hu
AU  - Strobl, Josef
AU  - Pfeifer, Norbert
TI  - UAV-Based Terrain Modeling under Vegetation in the Chinese Loess Plateau: A Deep Learning and Terrain Correction Ensemble Framework
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 20
SN  - 2072-4292

AB  - Accurate topographic mapping is a critical task for various environmental applications because elevation affects hydrodynamics and vegetation distributions. UAV photogrammetry is popular in terrain modelling because of its lower cost compared to laser scanning. However, this method is restricted in vegetation area with a complex terrain, due to reduced ground visibility and lack of robust and automatic filtering algorithms. To solve this problem, this work proposed an ensemble method of deep learning and terrain correction. First, image matching point cloud was generated by UAV photogrammetry. Second, vegetation points were identified based on U-net deep learning network. After that, ground elevation was corrected by estimating vegetation height to generate the digital terrain model (DTM). Two scenarios, namely, discrete and continuous vegetation areas were considered. The vegetation points in the discrete area were directly removed and then interpolated, and terrain correction was applied for the points in the continuous areas. Case studies were conducted in three different landforms in the loess plateau of China, and accuracy assessment indicated that the overall accuracy of vegetation detection was 95.0%, and the MSE (Mean Square Error) of final DTM (Digital Terrain Model) was 0.024 m.
KW  - UAV photogrammetry
KW  - terrain modeling
KW  - vegetation removal
KW  - deep learning
DO  - 10.3390/rs12203318
ER  -
TY  - EJOU
AU  - Riddell, Audrey P.
AU  - Fitzgerald, Stephen A.
AU  - Qi, Chu
AU  - Strimbu, Bogdan M.
TI  - Classification Strategies for Unbalanced Binary Maps: Finding Ponderosa Pine (Pinus ponderosa) in the Willamette Valley
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 20
SN  - 2072-4292

AB  - Forest species classifications are becoming increasingly automated as advances are made in machine learning. Complex algorithms can reach high accuracies, but are not always suitable for small-scale classifications, which may benefit from simpler conventional methods. The goal of this classification was to identify contiguous stands of ponderosa pine (Pinus ponderosa Douglas ex Lawson) against a mix of forest and non-forest background in the southern Willamette Valley, Oregon. The study area is approximately 816,600 ha, considerably larger than most study areas used for presenting techniques for tree species classification. To achieve the objective, we used two classification procedures, one parametric and one non-parametric. For the parametric method, we selected the maximum likelihood (ML) algorithm, whereas for the non-parametric method we chose the random forest (RF) algorithm. To identify ponderosa pine, we used 1 m spatial resolution red-green-blue-infrared (RGBI) aerial images supplied by the U.S. National Agriculture Imagery Program (NAIP) and 1 m spatial resolution canopy height models (CHMs) provided by the Oregon Department of Geology and Mineral Industries (DOGAMI). We tested four data variations for each method: Aerial imagery, CHM-masked aerial imagery, aerial imagery with an additional CHM band, and CHM-masked aerial imagery with a CHM band. The parametric classifications of aerial imagery alone reached an average kappa coefficient of 0.29, which increased to 0.51 when masked with CHM data. The incorporation of CHM data as a fifth band resulted in a similar improvement in kappa (0.47), but the most effective parametric method was the incorporation of CHM data as both a fifth band and a post-classification mask, resulting in a kappa coefficient of 0.89. The non-parametric classification of aerial imagery achieved a mean validation kappa coefficient of 0.85 collectively and 0.90 individually, which only increased by approximately 0.01 or less when the CHM masks were applied. The addition of the CHM band increased the kappa value to 0.91 for both individual and collective tile classifications. The highest kappa of all methods was achieved through five-band non-parametric classification with the addition of the CHM band (0.94) for both collective and individual classifications. Our results suggest that parametric methods, when enhanced with a CHM mask, could be suitable for large-area, small-scale classifications based on RGBI imagery, but a non-parametric classification of fused spectral and height data will generally achieve the highest accuracy for large, unbalanced datasets.
KW  - tree species classification
KW  - binary maps
KW  - maximum likelihood
KW  - random forest
KW  - canopy height model
DO  - 10.3390/rs12203325
ER  -
TY  - EJOU
AU  - Mohammed, Thaha
AU  - Albeshri, Aiiad
AU  - Katib, Iyad
AU  - Mehmood, Rashid
TI  - UbiPriSEQ—Deep Reinforcement Learning to Manage Privacy, Security, Energy, and QoS in 5G IoT HetNets
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 20
SN  - 2076-3417

AB  - 5G networks and Internet of Things (IoT) offer a powerful platform for ubiquitous environments with their ubiquitous sensing, high speeds and other benefits. The data, analytics, and other computations need to be optimally moved and placed in these environments, dynamically, such that energy-efficiency and QoS demands are best satisfied. A particular challenge in this context is to preserve privacy and security while delivering quality of service (QoS) and energy-efficiency. Many works have tried to address these challenges but without a focus on optimizing all of them and assuming fixed models of environments and security threats. This paper proposes the UbiPriSEQ framework that uses Deep Reinforcement Learning (DRL) to adaptively, dynamically, and holistically optimize QoS, energy-efficiency, security, and privacy. UbiPriSEQ is built on a three-layered model and comprises two modules. UbiPriSEQ devises policies and makes decisions related to important parameters including local processing and offloading rates for data and computations, radio channel states, transmit power, task priority, and selection of fog nodes for offloading, data migration, and so forth. UbiPriSEQ is implemented in Python over the TensorFlow platform and is evaluated using a real-life application in terms of SINR, privacy metric, latency, and utility function, manifesting great promise.
KW  - 5G networks
KW  - Internet of Things (IoT)
KW  - fog computing
KW  - edge computing
KW  - cloud computing
KW  - Deep Reinforcement Learning (DRL)
KW  - privacy
KW  - security
KW  - energy
KW  - quality of service (QoS)
DO  - 10.3390/app10207120
ER  -
TY  - EJOU
AU  - Ai, Tianfu
AU  - Xu, Bin
AU  - Xiang, Changle
AU  - Fan, Wei
AU  - Zhang, Yibo
TI  - Modeling of a Novel Coaxial Ducted Fan Aerial Robot Combined with Corner Environment by Using Artificial Neural Network
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 20
SN  - 1424-8220

AB  - A novel coaxial ducted fan aerial robot with a manipulator is proposed which can achieve some hover operation tasks in a corner environment, such as switching on and off a wall-attached button on the corner. In order to study the aerodynamic interference between the prototype and the environment when the aerial robot is hovering in the corner environment, a method for the comprehensive modeling of the prototype and corner environment based on the artificial neural network is presented. By using the CFD simulation software, the flow field of the prototype at different positions with the corner effect is analyzed. After determining the input, output and structure of the neural network model, the Adam and gradient descent algorithms are selected as the neural network training algorithms, respectively. In addition, to optimize the initial weights and biases of the neural network model, the genetic algorithm is precisely used. The three-dimensional prediction surfaces generated by the three methods of the neural network, kriging surface and the polynomial fitting are compared. The results show that the neural network has high prediction accuracy, and can be applied to the comprehensive modeling of the prototype and the corner environment.
KW  - corner effect
KW  - ducted fan
KW  - aerodynamic interference
KW  - artificial neural network
DO  - 10.3390/s20205805
ER  -
TY  - EJOU
AU  - Abraham, Emerson R.
AU  - Mendes dos Reis, João G.
AU  - Vendrametto, Oduvaldo
AU  - Oliveira Costa Neto, Pedro L.
AU  - Carlo Toloi, Rodrigo
AU  - Souza, Aguinaldo E.
AU  - Oliveira Morais, Marcos D.
TI  - Time Series Prediction with Artificial Neural Networks: An Analysis Using Brazilian Soybean Production
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 10
SN  - 2077-0472

AB  - Food production to meet human demand has been a challenge to society. Nowadays, one of the main sources of feeding is soybean. Considering agriculture food crops, soybean is sixth by production volume and the fourth by both production area and economic value. The grain can be used directly to human consumption, but it is highly used as a source of protein for animal production that corresponds 75% of the total, or as oil and derived food products. Brazil and the US are the most important players responsible for more than 70% of world production. Therefore, a reliable forecasting is essential for decision-makers to plan adequate policies to this important commodity and to establish the necessary logistical resources. In this sense, this study aims to predict soybean harvest area, yield, and production using Artificial Neural Networks (ANN) and compare with classical methods of Time Series Analysis. To this end, we collected data from a time series (1961&ndash;2016) regarding soybean production in Brazil. The results reveal that ANN is the best approach to predict soybean harvest area and production while classical linear function remains more effective to predict soybean yield. Moreover, ANN presents as a reliable model to predict time series and can help the stakeholders to anticipate the world soybean offer.
KW  - artificial neural networks
KW  - time series forecasting
KW  - soybean
KW  - food production
DO  - 10.3390/agriculture10100475
ER  -
TY  - EJOU
AU  - Bae, Ji Y.
AU  - Choi, Won
AU  - Hong, Suk-Ju
AU  - Kim, Sangyeon
AU  - Kim, Eungchan
AU  - Lee, Chang-Hyup
AU  - Han, Yun-hyeok
AU  - Hur, Hwan
AU  - Lee, Kye-Sung
AU  - Chang, Ki S.
AU  - Kim, Geon-Hee
AU  - Kim, Ghiseok
TI  - Design, Fabrication, and Performance Evaluation of Portable and Large-Area Blackbody System
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 20
SN  - 1424-8220

AB  - In this study, a portable and large-area blackbody system was developed following a series of processes including design, computational analysis, fabrication, and experimental analysis and evaluation. The blackbody system was designed to be lightweight (5 kg), and its temperature could exceed the ambient temperature by up to 15 &deg;C under operation. A carbon-fiber-based heat source was used to achieve a uniform temperature distribution. A heat shield fabricated from an insulation material was embedded at the opposite side of the heating element to minimize heat loss. A prototype of the blackbody system was fabricated based on the design and transient coupled electro-thermal simulation results. The operation performance of this system, such as the thermal response, signal transfer function, and noise equivalent temperature difference, was evaluated by employing an infrared imaging system. In addition, emissivity was measured during operation. The results of this study show that the developed portable and large-area blackbody system can be expected to serve as a reliable reference source for the calibration of aerial infrared images for the application of aerial infrared techniques to remote sensing.
KW  - portable and large-area blackbody system
KW  - infrared sensor
KW  - finite elements analysis
KW  - signal transfer function
KW  - noise equivalent temperature difference
DO  - 10.3390/s20205836
ER  -
TY  - EJOU
AU  - Abreu Maranhão, João P.
AU  - Carvalho Lustosa da Costa, João P.
AU  - Pignaton de Freitas, Edison
AU  - Javidi, Elnaz
AU  - Timóteo de Sousa Júnior, Rafael
TI  - Error-Robust Distributed Denial of Service Attack Detection Based on an Average Common Feature Extraction Technique
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 20
SN  - 1424-8220

AB  - In recent years, advanced threats against Cyber&ndash;Physical Systems (CPSs), such as Distributed Denial of Service (DDoS) attacks, are increasing. Furthermore, traditional machine learning-based intrusion detection systems (IDSs) often fail to efficiently detect such attacks when corrupted datasets are used for IDS training. To face these challenges, this paper proposes a novel error-robust multidimensional technique for DDoS attack detection. By applying the well-known Higher Order Singular Value Decomposition (HOSVD), initially, the average value of the common features among instances is filtered out from the dataset. Next, the filtered data are forwarded to machine learning classification algorithms in which traffic information is classified as a legitimate or a DDoS attack. In terms of results, the proposed scheme outperforms traditional low-rank approximation techniques, presenting an accuracy of 98.94%, detection rate of 97.70% and false alarm rate of 4.35% for a dataset corruption level of 30% with a random forest algorithm applied for classification. In addition, for error-free conditions, it is found that the proposed approach outperforms other related works, showing accuracy, detection rate and false alarm rate of 99.87%, 99.86% and 0.16%, respectively, for the gradient boosting classifier.
KW  - cyber–physical systems
KW  - machine learning
KW  - tensor decomposition
KW  - classification
KW  - error-robustness
DO  - 10.3390/s20205845
ER  -
TY  - EJOU
AU  - Cira, Calimanut-Ionut
AU  - Alcarria, Ramón
AU  - Manso-Callejo, Miguel-Ángel
AU  - Serradilla, Francisco
TI  - A Deep Learning-Based Solution for Large-Scale Extraction of the Secondary Road Network from High-Resolution Aerial Orthoimagery
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 20
SN  - 2076-3417

AB  - Secondary roads represent the largest part of the road network. However, due to the absence of clearly defined edges, presence of occlusions, and differences in widths, monitoring and mapping them represents a great effort for public administration. We believe that recent advancements in machine vision allow the extraction of these types of roads from high-resolution remotely sensed imagery and can enable the automation of the mapping operation. In this work, we leverage these advances and propose a deep learning-based solution capable of efficiently extracting the surface area of secondary roads at a large scale. The solution is based on hybrid segmentation models trained with high-resolution remote sensing imagery divided in tiles of 256 &times; 256 pixels and their correspondent segmentation masks, resulting in increases in performance metrics of 2.7&ndash;3.5% when compared to the original architectures. The best performing model achieved Intersection over Union and F1 scores of maximum 0.5790 and 0.7120, respectively, with a minimum loss of 0.4985 and was integrated on a web platform which handles the evaluation of large areas, the association of the semantic predictions with geographical coordinates, the conversion of the tiles&rsquo; format and the generation of geotiff results compatible with geospatial databases.
KW  - aerial orthoimagery
KW  - deep learning
KW  - remote sensing
KW  - road extraction
KW  - semantic segmentation
KW  - web-based segmentation solution
DO  - 10.3390/app10207272
ER  -
TY  - EJOU
AU  - Nguyen, Doan T.
AU  - Lee, Han-Gyeol
AU  - Jeong, Eui-Rim
AU  - Lee, Han L.
AU  - Joung, Jingon
TI  - Deep Learning-Based Localization for UWB Systems
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 10
SN  - 2079-9292

AB  - Localization has been extensively studied owing to its huge potential in various areas, such as Internet of Things, 5G, and unmanned aerial vehicle services. Its wide applications include home automation, advanced production automation, and unmanned vehicle control. In this study, we propose a novel localization method that utilizes convolutional neural network (CNN) and ultra-wideband (UWB) signals. A localization problem is converted to a regression problem with the proposed CNN, in which the ranging and positioning phases are integrated. By integrating the ranging and positioning phases, the proposed CNN estimates the location of UWB transmitter directly without any additional step. To integrate both phases of localization, a simple-yet efficient input image generation method is proposed. In the proposed input image generation method, three oversampled two-dimensional input images are generated from the three received UWB signals and they are provided to the designed CNN through the three channels, which are represented by red-, green-, and blue-color channels, respectively. The proposed CNN-based localization system then estimates the location of the UWB transmitter directly using the three-channel image as an input of the CNN. Simulation results verify that the proposed CNN-based localization method outperforms the traditional threshold-based and existing CNN-based methods. Also, it is observed that the proposed method performs well under an asymmetric environment, unlike the existing method.
KW  - localization
KW  - deep learning
KW  - convolutional neural network (CNN)
KW  - ultra-wideband (UWB) system
DO  - 10.3390/electronics9101712
ER  -
TY  - EJOU
AU  - Temitope Yekeen, Shamsudeen
AU  - Balogun, Abdul-Lateef
TI  - Advances in Remote Sensing Technology, Machine Learning and Deep Learning for Marine Oil Spill Detection, Prediction and Vulnerability Assessment
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 20
SN  - 2072-4292

AB  - Although advancements in remote sensing technology have facilitated quick capture and identification of the source and location of oil spills in water bodies, the presence of other biogenic elements (lookalikes) with similar visual attributes hinder rapid detection and prompt decision making for emergency response. To date, different methods have been applied to distinguish oil spills from lookalikes with limited success. In addition, accurately modeling the trajectory of oil spills remains a challenge. Thus, we aim to provide further insights on the multi-faceted problem by undertaking a holistic review of past and current approaches to marine oil spill disaster reduction as well as explore the potentials of emerging digital trends in minimizing oil spill hazards. The scope of previous reviews is extended by covering the inter-related dimensions of detection, discrimination, and trajectory prediction of oil spills for vulnerability assessment. Findings show that both optical and microwave airborne and satellite remote sensors are used for oil spill monitoring with microwave sensors being more widely used due to their ability to operate under any weather condition. However, the accuracy of both sensors is affected by the presence of biogenic elements, leading to false positive depiction of oil spills. Statistical image segmentation has been widely used to discriminate lookalikes from oil spills with varying levels of accuracy but the emergence of digitalization technologies in the fourth industrial revolution (IR 4.0) is enabling the use of Machine learning (ML) and deep learning (DL) models, which are more promising than the statistical methods. The Support Vector Machine (SVM) and Artificial Neural Network (ANN) are the most used machine learning algorithms for oil spill detection, although the restriction of ML models to feed forward image classification without support for the end-to-end trainable framework limits its accuracy. On the other hand, deep learning models&rsquo; strong feature extraction and autonomous learning capability enhance their detection accuracy. Also, mathematical models based on lagrangian method have improved oil spill trajectory prediction with higher real time accuracy than the conventional worst case, average and survey-based approaches. However, these newer models are unable to quantify oil droplets and uncertainty in vulnerability prediction. Considering that there is yet no single best remote sensing technique for unambiguous detection and discrimination of oil spills and lookalikes, it is imperative to advance research in the field in order to improve existing technology and develop specialized sensors for accurate oil spill detection and enhanced classification, leveraging emerging geospatial computer vision initiatives.
KW  - oil spill
KW  - remote sensing
KW  - review
KW  - machine learning
KW  - deep learning
KW  - trajectory modeling
KW  - vulnerability assessment
DO  - 10.3390/rs12203416
ER  -
TY  - EJOU
AU  - Digulescu, Angela
AU  - Despina-Stoian, Cristina
AU  - Stănescu, Denis
AU  - Popescu, Florin
AU  - Enache, Florin
AU  - Ioana, Cornel
AU  - Rădoi, Emanuel
AU  - Rîncu, Iulian
AU  - Șerbănescu, Alexandru
TI  - New Approach of UAV Movement Detection and Characterization Using Advanced Signal Processing Methods Based on UWB Sensing
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 20
SN  - 1424-8220

AB  - In the last years, the commercial drone/unmanned aerial vehicles market has grown due to their technological performances (provided by the multiple onboard available sensors), low price, and ease of use. Being very attractive for an increasing number of applications, their presence represents a major issue for public or classified areas with a special status, because of the rising number of incidents. Our paper proposes a new approach for the drone movement detection and characterization based on the ultra-wide band (UWB) sensing system and advanced signal processing methods. This approach characterizes the movement of the drone using classical methods such as correlation, envelope detection, time-scale analysis, but also a new method, the recurrence plot analysis. The obtained results are compared in terms of movement map accuracy and required computation time in order to offer a future starting point for the drone intrusion detection.
KW  - drone
KW  - LSS UAV
KW  - UWB sensing
KW  - signal processing
KW  - movement map
KW  - RPA
DO  - 10.3390/s20205904
ER  -
TY  - EJOU
AU  - Dado, Walter T.
AU  - Deines, Jillian M.
AU  - Patel, Rinkal
AU  - Liang, Sang-Zi
AU  - Lobell, David B.
TI  - High-Resolution Soybean Yield Mapping Across the US Midwest Using Subfield Harvester Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 21
SN  - 2072-4292

AB  - Cloud computing and freely available, high-resolution satellite data have enabled recent progress in crop yield mapping at fine scales. However, extensive validation data at a matching resolution remain uncommon or infeasible due to data availability. This has limited the ability to evaluate different yield estimation models and improve understanding of key features useful for yield estimation in both data-rich and data-poor contexts. Here, we assess machine learning models&rsquo; capacity for soybean yield prediction using a unique ground-truth dataset of high-resolution (5 m) yield maps generated from combine harvester yield monitor data for over a million field-year observations across the Midwestern United States from 2008 to 2018. First, we compare random forest (RF) implementations, testing a range of feature engineering approaches using Sentinel-2 and Landsat spectral data for 20- and 30-m scale yield prediction. We find that Sentinel-2-based models can explain up to 45% of out-of-sample yield variability from 2017 to 2018 (r2 = 0.45), while Landsat models explain up to 43% across the longer 2008&ndash;2018 period. Using discrete Fourier transforms, or harmonic regressions, to capture soybean phenology improved the Landsat-based model considerably. Second, we compare RF models trained using this ground-truth data to models trained on available county-level statistics. We find that county-level models rely more heavily on just a few predictors, namely August weather covariates (vapor pressure deficit, rainfall, temperature) and July and August near-infrared observations. As a result, county-scale models perform relatively poorly on field-scale validation (r2 = 0.32), especially for high-yielding fields, but perform similarly to field-scale models when evaluated at the county scale (r2 = 0.82). Finally, we test whether our findings on variable importance can inform a simple, generalizable framework for regions or time periods beyond ground data availability. To do so, we test improvements to a Scalable Crop Yield Mapper (SCYM) approach that uses crop simulations to train statistical models for yield estimation. Based on findings from our RF models, we employ harmonic regressions to estimate peak vegetation index (VI) and a VI observation 30 days later, with August rainfall as the sole weather covariate in our new SCYM model. Modifications improved SCYM&rsquo;s explained variance (r2 = 0.27 at the 30 m scale) and provide a new, parsimonious model.
KW  - crop yields
KW  - yield mapping
KW  - US Corn Belt
KW  - Landsat
KW  - Sentinel
KW  - agricultural monitoring
KW  - machine learning
DO  - 10.3390/rs12213471
ER  -
TY  - EJOU
AU  - Kanchanatripop, Phusit
AU  - Zhang, Dafang
TI  - Adaptive Image Edge Extraction Based on Discrete Algorithm and Classical Canny Operator
T2  - Symmetry

PY  - 2020
VL  - 12
IS  - 11
SN  - 2073-8994

AB  - In order to improve the accuracy of image edge detection, this paper studies the adaptive image edge detection technology based on discrete algorithm and classical Canny operator. First, the traditional sub-pixel edge detection method is illustrated based on the related literature research. Then, Canny operator is used for detection, the edge model of the quadric curve is established using discrete data, and the adaptive image edge parameters are obtained using one-dimensional gray moment. Experimental results show that the accuracy of feature detection is 99%, which can be applied to the practice of image edge detection to a certain extent.
KW  - classical Canny operator
KW  - discrete algorithm
KW  - gray moment
KW  - image edge
KW  - extract
DO  - 10.3390/sym12111749
ER  -
TY  - EJOU
AU  - Zhang, Xiangzhu
AU  - Zhang, Lijia
AU  - Lewis, Frank L.
AU  - Pei, Hailong
TI  - Non-Uniform Discretization-based Ordinal Regression for Monocular Depth Estimation of an Indoor Drone
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 11
SN  - 2079-9292

AB  - At present, the main methods of solving the monocular depth estimation for indoor drones are the simultaneous localization and mapping (SLAM) algorithm and the deep learning algorithm. SLAM requires the construction of a depth map of the unknown environment, which is slow to calculate and generally requires expensive sensors, whereas current deep learning algorithms are mostly based on binary classification or regression. The output of the binary classification model gives the decision algorithm relatively rough control over the unmanned aerial vehicle. The regression model solves the problem of the binary classification, but it carries out the same processing for long and short distances, resulting in a decline in short-range prediction performance. In order to solve the above problems, according to the characteristics of the strong order correlation of the distance value, we propose a non-uniform spacing-increasing discretization-based ordinal regression algorithm (NSIDORA) to solve the monocular depth estimation for indoor drone tasks. According to the security requirements of this task, the distance label of the data set is discretized into three major areas&mdash;the dangerous area, decision area, and safety area&mdash;and the decision area is discretized based on spacing-increasing discretization. Considering the inconsistency of ordinal regression, a new distance decoder is produced. Experimental evaluation shows that the root-mean-square error (RMSE) of NSIDORA in the decision area is 33.5% lower than that of non-uniform discretization (NUD)-based ordinal regression methods. Although it is higher overall than that of the state-of-the-art two-stream regression algorithm, the RMSE of the NSIDORA in the top 10 categories of the decision area is 21.8% lower than that of the two-stream regression algorithm. The inference speed of NSIDORA is 3.4 times faster than that of two-stream ordinal regression. Furthermore, the effectiveness of the decoder has been proved through ablation experiments.
KW  - deep learning
KW  - monocular
KW  - discretization
KW  - ordinal regression
KW  - depth estimation
DO  - 10.3390/electronics9111767
ER  -
TY  - EJOU
AU  - Garg, Piyush
AU  - Nasimi, Roya
AU  - Ozdagli, Ali
AU  - Zhang, Su
AU  - Mascarenas, David D.
AU  - Reda Taha, Mahmoud
AU  - Moreu, Fernando
TI  - Measuring Transverse Displacements Using Unmanned Aerial Systems Laser Doppler Vibrometer (UAS-LDV): Development and Field Validation
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 21
SN  - 1424-8220

AB  - Measurement of bridge displacements is important for ensuring the safe operation of railway bridges. Traditionally, contact sensors such as Linear Variable Displacement Transducers (LVDT) and accelerometers have been used to measure the displacement of the railway bridges. However, these sensors need significant effort in installation and maintenance. Therefore, railroad management agencies are interested in new means to measure bridge displacements. This research focuses on mounting Laser Doppler Vibrometer (LDV) on an Unmanned Aerial System (UAS) to enable contact-free transverse dynamic displacement of railroad bridges. Researchers conducted three field tests by flying the Unmanned Aerial Systems Laser Doppler Vibrometer (UAS-LDV) 1.5 m away from the ground and measured the displacement of a moving target at various distances. The accuracy of the UAS-LDV measurements was compared to the Linear Variable Differential Transducer (LVDT) measurements. The results of the three field tests showed that the proposed system could measure non-contact, reference-free dynamic displacement with an average peak and root mean square (RMS) error for the three experiments of 10% and 8% compared to LVDT, respectively. Such errors are acceptable for field measurements in railroads, as the interest prior to bridge monitoring implementation of a new approach is to demonstrate similar success for different flights, as reported in the three results. This study also identified barriers for industrial adoption of this technology and proposed operational development practices for both technical and cost-effective implementation.
KW  - unmanned aerial system
KW  - reference-free displacement
KW  - non-contact displacement
KW  - laser
KW  - railroad bridge
KW  - field implementation
KW  - dynamic displacement
DO  - 10.3390/s20216051
ER  -
TY  - EJOU
AU  - Xie, Chunli
AU  - Wang, Xia
AU  - Qian, Cheng
AU  - Wang, Mengqi
TI  - A Source Code Similarity Based on Siamese Neural Network
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 21
SN  - 2076-3417

AB  - Finding similar code snippets is a fundamental task in the field of software engineering. Several approaches have been proposed for this task by using statistical language model which focuses on syntax and structure of codes rather than deep semantic information underlying codes. In this paper, a Siamese Neural Network is proposed that maps codes into continuous space vectors and try to capture their semantic meaning. Firstly, an unsupervised pre-trained method that models code snippets as a weighted series of word vectors. The weights of the series are fitted by the Term Frequency-Inverse Document Frequency (TF-IDF). Then, a Siamese Neural Network trained model is constructed to learn semantic vector representation of code snippets. Finally, the cosine similarity is provided to measure the similarity score between pairs of code snippets. Moreover, we have implemented our approach on a dataset of functionally similar code. The experimental results show that our method improves some performance over single word embedding method.
KW  - code similarity
KW  - word embedding
KW  - siamese neural networks
DO  - 10.3390/app10217519
ER  -
TY  - EJOU
AU  - Eskandari, Roghieh
AU  - Mahdianpari, Masoud
AU  - Mohammadimanesh, Fariba
AU  - Salehi, Bahram
AU  - Brisco, Brian
AU  - Homayouni, Saeid
TI  - Meta-analysis of Unmanned Aerial Vehicle (UAV) Imagery for Agro-environmental Monitoring Using Machine Learning and Statistical Models
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 21
SN  - 2072-4292

AB  - Unmanned Aerial Vehicle (UAV) imaging systems have recently gained significant attention from researchers and practitioners as a cost-effective means for agro-environmental applications. In particular, machine learning algorithms have been applied to UAV-based remote sensing data for enhancing the UAV capabilities of various applications. This systematic review was performed on studies through a statistical meta-analysis of UAV applications along with machine learning algorithms in agro-environmental monitoring. For this purpose, a total number of 163 peer-reviewed articles published in 13 high-impact remote sensing journals over the past 20 years were reviewed focusing on several features, including study area, application, sensor type, platform type, and spatial resolution. The meta-analysis revealed that 62% and 38% of the studies applied regression and classification models, respectively. Visible sensor technology was the most frequently used sensor with the highest overall accuracy among classification articles. Regarding regression models, linear regression and random forest were the most frequently applied models in UAV remote sensing imagery processing. Finally, the results of this study confirm that applying machine learning approaches on UAV imagery produces fast and reliable results. Agriculture, forestry, and grassland mapping were found as the top three UAV applications in this review, in 42%, 22%, and 8% of the studies, respectively.
KW  - Unmanned Aerial Vehicle (UAV)
KW  - remote sensing
KW  - machine learning
KW  - agro-environmental monitoring
KW  - classification
KW  - regression
DO  - 10.3390/rs12213511
ER  -
TY  - EJOU
AU  - Just, Gilson E.
AU  - E. Pellenz, Marcelo
AU  - Lima, Luiz A. de Paula
AU  - S. Chang, Bruno
AU  - Demo Souza, Richard
AU  - Montejo-Sánchez, Samuel
TI  - UAV Path Optimization for Precision Agriculture Wireless Sensor Networks
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 21
SN  - 1424-8220

AB  - The use of monitoring sensors is increasingly present in the context of precision agriculture. Usually, these sensor nodes (SNs) alternate their states between periods of activation and hibernation to reduce battery usage. When employing unmanned aerial vehicles (UAVs) to collect data from SNs distributed over a large agricultural area, we must synchronize the UAV route with the activation period of each SN. In this article, we address the problem of optimizing the UAV path through all the SNs to reduce its flight time, while also maximizing the SNs&rsquo; lifetime. Using the concept of timeslots for time base management combined with the idea of flight prohibition list, we propose an efficient algorithm for discovering and reconfiguring the activation time of the SNs. Experimental results were obtained through the development of our own simulator&mdash;UAV Simulator. These results demonstrate a considerable reduction in the distance traveled by the UAV and also in its flight time. In addition, the model provides a reduction in transmission time by SNs after reconfiguration, thus ensuring a longer lifetime for the SNs in the monitoring environment, as well as improving the freshness and continuity of the gathered data, which support the decision-making process.
KW  - unmanned aerial vehicle
KW  - path planning
KW  - precision agriculture
KW  - wireless sensor networks
KW  - data gathering
DO  - 10.3390/s20216098
ER  -
TY  - EJOU
AU  - Wang, Ruimeng
AU  - Xia, Haoming
AU  - Qin, Yaochen
AU  - Niu, Wenhui
AU  - Pan, Li
AU  - Li, Rumeng
AU  - Zhao, Xiaoyang
AU  - Bian, Xiqing
AU  - Fu, Pinde
TI  - Dynamic Monitoring of Surface Water Area during 1989–2019 in the Hetao Plain Using Landsat Data in Google Earth Engine
T2  - Water

PY  - 2020
VL  - 12
IS  - 11
SN  - 2073-4441

AB  - The spatio-temporal change of the surface water is very important to agricultural, economic, and social development in the Hetao Plain, as well as the structure and function of the ecosystem. To understand the long-term changes of the surface water area in the Hetao Plain, we used all available Landsat images (7534 scenes) and adopted the modified Normalized Difference Water Index (mNDWI), Enhanced Vegetation Index (EVI), and Normalized Difference Vegetation Index (NDVI) to map the open-surface water from 1989 to 2019 in the Google Earth Engine (GEE) cloud platform. We further analyzed precipitation, temperature, and irrigated area, revealing the impact of climate change and human activities on long-term surface water changes. The results show the following. (1) In the last 31 years, the maximum, seasonal, and annual average water body area values in the Hetao Plain have exhibited a downward trend. Meanwhile, the number of maximum, seasonal, and permanent water bodies displayed a significant upward trend. (2) The variation of the surface water area in the Hetao Plain is mainly affected by the maximum water body area, while the variation of the water body number is mainly affected by the number of minimum water bodies. (3) Precipitation has statistically significant positive effects on the water body area and water body number, which has statistically significant negative effects with temperature and irrigation. The findings of this study can be used to help the policy-makers and farmers understand changing water resources and its driving mechanism and provide a reference for water resources management, agricultural irrigation, and ecological protection.
KW  - surface water
KW  - Yellow River Basin
KW  - Hetao Plain
KW  - Google Earth Engine
KW  - Landsat images
KW  - climate change
DO  - 10.3390/w12113010
ER  -
TY  - EJOU
AU  - Pedro, Dário
AU  - Matos-Carvalho, João P.
AU  - Azevedo, Fábio
AU  - Sacoto-Martins, Ricardo
AU  - Bernardo, Luís
AU  - Campos, Luís
AU  - Fonseca, José M.
AU  - Mora, André
TI  - FFAU—Framework for Fully Autonomous UAVs
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 21
SN  - 2072-4292

AB  - Unmanned Aerial Vehicles (UAVs), although hardly a new technology, have recently gained a prominent role in many industries being widely used not only among enthusiastic consumers, but also in high demanding professional situations, and will have a massive societal impact over the coming years. However, the operation of UAVs is fraught with serious safety risks, such as collisions with dynamic obstacles (birds, other UAVs, or randomly thrown objects). These collision scenarios are complex to analyze in real-time, sometimes being computationally impossible to solve with existing State of the Art (SoA) algorithms, making the use of UAVs an operational hazard and therefore significantly reducing their commercial applicability in urban environments. In this work, a conceptual framework for both stand-alone and swarm (networked) UAVs is introduced, with a focus on the architectural requirements of the collision avoidance subsystem to achieve acceptable levels of safety and reliability. The SoA principles for collision avoidance against stationary objects are reviewed and a novel approach is described, using deep learning techniques to solve the computational intensive problem of real-time collision avoidance with dynamic objects. The proposed framework includes a web-interface allowing the full control of UAVs as remote clients with a supervisor cloud-based platform. The feasibility of the proposed approach was demonstrated through experimental tests using a UAV, developed from scratch using the proposed framework. Test flight results are presented for an autonomous UAV monitored from multiple countries across the world.
KW  - UAV
KW  - framework
KW  - drones
KW  - collision avoidance
KW  - resilience
KW  - artificial intelligence
KW  - machine learning
KW  - neuronal network
KW  - deep learning
DO  - 10.3390/rs12213533
ER  -
TY  - EJOU
AU  - Vujasinović, Stéphane
AU  - Becker, Stefan
AU  - Breuer, Timo
AU  - Bullinger, Sebastian
AU  - Scherer-Negenborn, Norbert
AU  - Arens, Michael
TI  - Integration of the 3D Environment for UAV Onboard Visual Object Tracking
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 21
SN  - 2076-3417

AB  - Single visual object tracking from an unmanned aerial vehicle (UAV) poses fundamental challenges such as object occlusion, small-scale objects, background clutter, and abrupt camera motion. To tackle these difficulties, we propose to integrate the 3D structure of the observed scene into a detection-by-tracking algorithm. We introduce a pipeline that combines a model-free visual object tracker, a sparse 3D reconstruction, and a state estimator. The 3D reconstruction of the scene is computed with an image-based Structure-from-Motion (SfM) component that enables us to leverage a state estimator in the corresponding 3D scene during tracking. By representing the position of the target in 3D space rather than in image space, we stabilize the tracking during ego-motion and improve the handling of occlusions, background clutter, and small-scale objects. We evaluated our approach on prototypical image sequences, captured from a UAV with low-altitude oblique views. For this purpose, we adapted an existing dataset for visual object tracking and reconstructed the observed scene in 3D. The experimental results demonstrate that the proposed approach outperforms methods using plain visual cues as well as approaches leveraging image-space-based state estimations. We believe that our approach can be beneficial for trafficmonitoring, video surveillance, and navigation.
KW  - single object tracking
KW  - deep learning
KW  - unmanned aerial vehicle
KW  - structure-from-motion
DO  - 10.3390/app10217622
ER  -
TY  - EJOU
AU  - Wei, Wei
AU  - Zhang, Chen
AU  - Deng, Dexiang
TI  - Content Estimation of Foreign Fibers in Cotton Based on Deep Learning
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 11
SN  - 2079-9292

AB  - Cotton foreign fibers directly affect the quality of a textile product; the less foreign fibers in raw cotton, the higher the quality grade of the textile product. Based on the foreign fiber clean machine, this paper proposed an evaluation method of foreign fiber content using deep learning. First of all, a large number of images of foreign fibers were collected from different production lines and annotated to obtain the mask image dataset of foreign fibers. Secondly, by comparing the image segmentation algorithm based on deep learning, tests showed that U-Net has a better performance on different segment metrics evaluations, and U-Net is improved to realize the real-time segmentation of foreign fiber images. The actual size of the foreign fiber could be calculated through the combination of the segment result and the mechanical parameters of the machine. Finally, the test results showed that the relative error between the estimated size and the actual size was less than 4%. After the prototype test, the algorithm was deployed on the actual production line and, by comparing the algorithm data in a random time with the actual foreign fiber statistical data, the overall error was less than 2%. The test showed that the new evaluation method can fully reflect the content of foreign fiber in raw cotton.
KW  - deep learning
KW  - image segmentation
KW  - foreign fiber
KW  - content evaluation
KW  - size estimation
DO  - 10.3390/electronics9111795
ER  -
TY  - EJOU
AU  - Garzon-Lopez, Carol X.
AU  - Lasso, Eloisa
TI  - Species Classification in a Tropical Alpine Ecosystem Using UAV-Borne RGB and Hyperspectral Imagery
T2  - Drones

PY  - 2020
VL  - 4
IS  - 4
SN  - 2504-446X

AB  - P&aacute;ramos host more than 3500 vascular plant species and are crucial water providers for millions of people in the northern Andes. Monitoring species distribution at large scales is an urgent conservation priority in the face of ongoing climatic changes and increasing anthropogenic pressure on this ecosystem. For the first time in this ecosystem, we explored the potential of unoccupied aerial vehicles (UAV)-borne red, green, and blue wavelengths (RGB) and hyperspectral imagery for p&aacute;ramo species classification by collecting both types of images in a 10-ha area, and ground vegetation cover data from 10 plots within this area. Five plots were used for calibration and the other five for validation. With the hyperspectral data, we tested our capacity to detect five representative p&aacute;ramo species with different growth forms using support vector machine (SVM) and random forest (RF) classifiers in combination with three feature selection methods and two class groups. Using RGB images, we could classify 21 species with an accuracy greater than 97%. From hyperspectral imaging, the highest accuracy (89%) was found using models built with RF or SVM classifiers combined with a binary grouping method and the sequential floating forward selection feature. Our results demonstrate that p&aacute;ramo species can be accurately mapped using both RGB and hyperspectral imagery.
KW  - RGB imagery
KW  - UAV
KW  - hyperspectral
KW  - páramo
KW  - northern Andes
KW  - species classification
KW  - random forest
KW  - support vector machine
DO  - 10.3390/drones4040069
ER  -
TY  - EJOU
AU  - Calvario, Gabriela
AU  - Alarcón, Teresa E.
AU  - Dalmau, Oscar
AU  - Sierra, Basilio
AU  - Hernandez, Carmen
TI  - An Agave Counting Methodology Based on Mathematical Morphology and Images Acquired through Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 21
SN  - 1424-8220

AB  - Blue agave is an important commercial crop in Mexico, and it is the main source of the traditional mexican beverage known as tequila. The variety of blue agave crop known as Tequilana Weber is a crucial element for tequila agribusiness and the agricultural economy in Mexico. The number of agave plants in the field is one of the main parameters for estimating production of tequila. In this manuscript, we describe a mathematical morphology-based algorithm that addresses the agave automatic counting task. The proposed methodology was applied to a set of real images collected using an Unmanned Aerial Vehicle equipped with a digital Red-Green-Blue (RGB) camera. The number of plants automatically identified in the collected images was compared to the number of plants counted by hand. Accuracy of the proposed algorithm depended on the size heterogeneity of plants in the field and illumination. Accuracy ranged from 0.8309 to 0.9806, and performance of the proposed algorithm was satisfactory.
KW  - precision agriculture
KW  - UAV
KW  - data mining
KW  - computer vision
KW  - geomatics
KW  - crop monitoring
DO  - 10.3390/s20216247
ER  -
TY  - EJOU
AU  - Trevisan, Rodrigo
AU  - Pérez, Osvaldo
AU  - Schmitz, Nathan
AU  - Diers, Brian
AU  - Martin, Nicolas
TI  - High-Throughput Phenotyping of Soybean Maturity Using Time Series UAV Imagery and Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 21
SN  - 2072-4292

AB  - Soybean maturity is a trait of critical importance for the development of new soybean cultivars, nevertheless, its characterization based on visual ratings has many challenges. Unmanned aerial vehicles (UAVs) imagery-based high-throughput phenotyping methodologies have been proposed as an alternative to the traditional visual ratings of pod senescence. However, the lack of scalable and accurate methods to extract the desired information from the images remains a significant bottleneck in breeding programs. The objective of this study was to develop an image-based high-throughput phenotyping system for evaluating soybean maturity in breeding programs. Images were acquired twice a week, starting when the earlier lines began maturation until the latest ones were mature. Two complementary convolutional neural networks (CNN) were developed to predict the maturity date. The first using a single date and the second using the five best image dates identified by the first model. The proposed CNN architecture was validated using more than 15,000 ground truth observations from five trials, including data from three growing seasons and two countries. The trained model showed good generalization capability with a root mean squared error lower than two days in four out of five trials. Four methods of estimating prediction uncertainty showed potential at identifying different sources of errors in the maturity date predictions. The architecture developed solves limitations of previous research and can be used at scale in commercial breeding programs.
KW  - machine learning
KW  - physiological maturity
KW  - computer vision
KW  - plant breeding
KW  - soybean phenology
KW  - Glycine max (L.) Merr
DO  - 10.3390/rs12213617
ER  -
TY  - EJOU
AU  - Mandal, Vishal
AU  - Mussah, Abdul R.
AU  - Jin, Peng
AU  - Adu-Gyamfi, Yaw
TI  - Artificial Intelligence-Enabled Traffic Monitoring System
T2  - Sustainability

PY  - 2020
VL  - 12
IS  - 21
SN  - 2071-1050

AB  - Manual traffic surveillance can be a daunting task as Traffic Management Centers operate a myriad of cameras installed over a network. Injecting some level of automation could help lighten the workload of human operators performing manual surveillance and facilitate making proactive decisions which would reduce the impact of incidents and recurring congestion on roadways. This article presents a novel approach to automatically monitor real time traffic footage using deep convolutional neural networks and a stand-alone graphical user interface. The authors describe the results of research received in the process of developing models that serve as an integrated framework for an artificial intelligence enabled traffic monitoring system. The proposed system deploys several state-of-the-art deep learning algorithms to automate different traffic monitoring needs. Taking advantage of a large database of annotated video surveillance data, deep learning-based models are trained to detect queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level segmentation approach is applied to detect traffic queues and predict severity. Real-time object detection algorithms coupled with different tracking systems are deployed to automatically detect stranded vehicles as well as perform vehicular counts. At each stage of development, interesting experimental results are presented to demonstrate the effectiveness of the proposed system. Overall, the results demonstrate that the proposed framework performs satisfactorily under varied conditions without being immensely impacted by environmental hazards such as blurry camera views, low illumination, rain, or snow.
KW  - traffic monitoring
KW  - intelligent transportation systems
KW  - traffic queues
KW  - vehicle counts
KW  - artificial intelligence
KW  - deep learning
DO  - 10.3390/su12219177
ER  -
TY  - EJOU
AU  - Bhowmick, Sutanu
AU  - Nagarajaiah, Satish
AU  - Veeraraghavan, Ashok
TI  - Vision and Deep Learning-Based Algorithms to Detect and Quantify Cracks on Concrete Surfaces from UAV Videos
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 21
SN  - 1424-8220

AB  - Immediate assessment of structural integrity of important civil infrastructures, like bridges, hospitals, or dams, is of utmost importance after natural disasters. Currently, inspection is performed manually by engineers who look for local damages and their extent on significant locations of the structure to understand its implication on its global stability. However, the whole process is time-consuming and prone to human errors. Due to their size and extent, some regions of civil structures are hard to gain access for manual inspection. In such situations, a vision-based system of Unmanned Aerial Vehicles (UAVs) programmed with Artificial Intelligence algorithms may be an effective alternative to carry out a health assessment of civil infrastructures in a timely manner. This paper proposes a framework of achieving the above-mentioned goal using computer vision and deep learning algorithms for detection of cracks on the concrete surface from its image by carrying out image segmentation of pixels, i.e., classification of pixels in an image of the concrete surface and whether it belongs to cracks or not. The image segmentation or dense pixel level classification is carried out using a deep neural network architecture named U-Net. Further, morphological operations on the segmented images result in dense measurements of crack geometry, like length, width, area, and crack orientation for individual cracks present in the image. The efficacy and robustness of the proposed method as a viable real-life application was validated by carrying out a laboratory experiment of a four-point bending test on an 8-foot-long concrete beam of which the video is recorded using a camera mounted on a UAV-based, as well as a still ground-based, video camera. Detection, quantification, and localization of damage on a civil infrastructure using the proposed framework can directly be used in the prognosis of the structure&rsquo;s ability to withstand service loads.
KW  - computer vision
KW  - morphological operations
KW  - unmanned aerial vehicle
KW  - U-Net
DO  - 10.3390/s20216299
ER  -
TY  - EJOU
AU  - Fernandez-Carrillo, Angel
AU  - Patočka, Zdeněk
AU  - Dobrovolný, Lumír
AU  - Franco-Nieto, Antonio
AU  - Revilla-Romero, Beatriz
TI  - Monitoring Bark Beetle Forest Damage in Central Europe. A Remote Sensing Approach Validated with Field Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 21
SN  - 2072-4292

AB  - Over the last decades, climate change has triggered an increase in the frequency of spruce bark beetle (Ips typographus L.) in Central Europe. More than 50% of forests in the Czech Republic are seriously threatened by this pest, leading to high ecological and economic losses. The exponential increase of bark beetle infestation hinders the implementation of costly field campaigns to prevent and mitigate its effects. Remote sensing may help to overcome such limitations as it provides frequent and spatially continuous data on vegetation condition. Using Sentinel-2 images as main input, two models have been developed to test the ability of this data source to map bark beetle damage and severity. All models were based on a change detection approach, and required the generation of previous forest mask and dominant species maps. The first damage mapping model was developed for 2019 and 2020, and it was based on bi-temporal regressions in spruce areas to estimate forest vitality and bark beetle damage. A second model was developed for 2020 considering all forest area, but excluding clear-cuts and completely dead areas, in order to map only changes in stands dominated by alive trees. The three products were validated with in situ data. All the maps showed high accuracies (acc &gt; 0.80). Accuracy was higher than 0.95 and F1-score was higher than 0.88 for areas with high severity, with omission errors under 0.09 in all cases. This confirmed the ability of all the models to detect bark beetle attack at the last phases. Areas with no damage or low severity showed more complex results. The no damage category yielded greater commission errors and relative bias (CEs = 0.30&ndash;0.42, relB = 0.42&ndash;0.51). The similar results obtained for 2020 leaving out clear-cuts and dead trees proved that the proposed methods could be used to help forest managers fight bark beetle pests. These biotic damage products based on Sentinel-2 can be set up for any location to derive regular forest vitality maps and inform of early damage.
KW  - bark beetle
KW  - Ips typographus L.
KW  - pest
KW  - remote sensing
KW  - change detection
KW  - forest damage
KW  - spruce
KW  - Sentinel-2
KW  - damage mapping
KW  - multi-temporal regression
DO  - 10.3390/rs12213634
ER  -
TY  - EJOU
AU  - Sanenga, Abraham
AU  - Mapunda, Galefang A.
AU  - Jacob, Tshepiso M.
AU  - Marata, Leatile
AU  - Basutli, Bokamoso
AU  - Chuma, Joseph M.
TI  - An Overview of Key Technologies in Physical Layer Security
T2  - Entropy

PY  - 2020
VL  - 22
IS  - 11
SN  - 1099-4300

AB  - The open nature of radio propagation enables ubiquitous wireless communication. This allows for seamless data transmission. However, unauthorized users may pose a threat to the security of the data being transmitted to authorized users. This gives rise to network vulnerabilities such as hacking, eavesdropping, and jamming of the transmitted information. Physical layer security (PLS) has been identified as one of the promising security approaches to safeguard the transmission from eavesdroppers in a wireless network. It is an alternative to the computationally demanding and complex cryptographic algorithms and techniques. PLS has continually received exponential research interest owing to the possibility of exploiting the characteristics of the wireless channel. One of the main characteristics includes the random nature of the transmission channel. The aforesaid nature makes it possible for confidential and authentic signal transmission between the sender and the receiver in the physical layer. We start by introducing the basic theories of PLS, including the wiretap channel, information-theoretic security, and a brief discussion of the cryptography security technique. Furthermore, an overview of multiple-input multiple-output (MIMO) communication is provided. The main focus of our review is based on the existing key-less PLS optimization techniques, their limitations, and challenges. The paper also looks into the promising key research areas in addressing these shortfalls. Lastly, a comprehensive overview of some of the recent PLS research in 5G and 6G technologies of wireless communication networks is provided.
KW  - artificial noise
KW  - beamforming
KW  - intelligent reflective surface
KW  - MIMO
KW  - optimization
KW  - physical layer security
KW  - zero forcing
DO  - 10.3390/e22111261
ER  -
TY  - EJOU
AU  - Ghazal, Mohammed
AU  - Basmaji, Tasnim
AU  - Yaghi, Maha
AU  - Alkhedher, Mohammad
AU  - Mahmoud, Mohamed
AU  - El-Baz, Ayman S.
TI  - Cloud-Based Monitoring of Thermal Anomalies in Industrial Environments Using AI and the Internet of Robotic Things
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 21
SN  - 1424-8220

AB  - Recent advancements in cloud computing, artificial intelligence, and the internet of things (IoT) create new opportunities for autonomous industrial environments monitoring. Nevertheless, detecting anomalies in harsh industrial settings remains challenging. This paper proposes an edge-fog-cloud architecture with mobile IoT edge nodes carried on autonomous robots for thermal anomalies detection in aluminum factories. We use companion drones as fog nodes to deliver first response services and a cloud back-end for thermal anomalies analysis. We also propose a self-driving deep learning architecture and a thermal anomalies detection and visualization algorithm. Our results show our robot surveyors are low-cost, deliver reduced response time, and more accurately detect anomalies compared to human surveyors or fixed IoT nodes monitoring the same industrial area. Our self-driving architecture has a root mean square error of 0.19 comparable to VGG-19 with a significantly reduced complexity and three times the frame rate at 60 frames per second. Our thermal to visual registration algorithm maximizes mutual information in the image-gradient domain while adapting to different resolutions and camera frame rates.
KW  - edge-fog-cloud computing
KW  - Internet of Things
KW  - robotics
KW  - artificial intelligence
KW  - autonomous driving
KW  - image registration
DO  - 10.3390/s20216348
ER  -
TY  - EJOU
AU  - Soloy, Antoine
AU  - Turki, Imen
AU  - Fournier, Matthieu
AU  - Costa, Stéphane
AU  - Peuziat, Bastien
AU  - Lecoq, Nicolas
TI  - A Deep Learning-Based Method for Quantifying and Mapping the Grain Size on Pebble Beaches
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 21
SN  - 2072-4292

AB  - This article proposes a new methodological approach to measure and map the size of coarse clasts on a land surface from photographs. This method is based on the use of the Mask Regional Convolutional Neural Network (R-CNN) deep learning algorithm, which allows the instance segmentation of objects after an initial training on manually labeled data. The algorithm is capable of identifying and classifying objects present in an image at the pixel scale, without human intervention, in a matter of seconds. This work demonstrates that it is possible to train the model to detect non-overlapping coarse sediments on scaled images, in order to extract their individual size and morphological characteristics with high efficiency (R2 = 0.98; Root Mean Square Error (RMSE) = 3.9 mm). It is then possible to measure element size profiles over a sedimentary body, as it was done on the pebble beach of Etretat (Normandy, France) in order to monitor the granulometric spatial variability before and after a storm. Applied at a larger scale using Unmanned Aerial Vehicle (UAV) derived ortho-images, the method allows the accurate characterization and high-resolution mapping of the surface coarse sediment size, as it was performed on the two pebble beaches of Etretat (D50 = 5.99 cm) and Hautot-sur-Mer (D50 = 7.44 cm) (Normandy, France). Validation results show a very satisfying overall representativity (R2 = 0.45 and 0.75; RMSE = 6.8 mm and 9.3 mm at Etretat and Hautot-sur-Mer, respectively), while the method remains fast, easy to apply and low-cost, although the method remains limited by the image resolution (objects need to be longer than 4 cm), and could still be improved in several ways, for instance by adding more manually labeled data to the training dataset, and by considering more accurate methods than the ellipse fitting for measuring the particle sizes.
KW  - remote sensing
KW  - clast size mapping
KW  - pebbles
KW  - granulometry
KW  - image segmentation
KW  - mask R-CNN
KW  - deep learning
KW  - Normandy
KW  - pebble beach
DO  - 10.3390/rs12213659
ER  -
TY  - EJOU
AU  - Cantieri, Alvaro
AU  - Ferraz, Matheus
AU  - Szekir, Guido
AU  - Antônio Teixeira, Marco
AU  - Lima, José
AU  - Schneider Oliveira, André
AU  - Aurélio Wehrmeister, Marco
TI  - Cooperative UAV–UGV Autonomous Power Pylon Inspection: An Investigation of Cooperative Outdoor Vehicle Positioning Architecture
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 21
SN  - 1424-8220

AB  - Realizing autonomous inspection, such as that of power distribution lines, through unmanned aerial vehicle (UAV) systems is a key research domain in robotics. In particular, the use of autonomous and semi-autonomous vehicles to execute the tasks of an inspection process can enhance the efficacy and safety of the operation; however, many technical problems, such as those pertaining to the precise positioning and path following of the vehicles, robust obstacle detection, and intelligent control, must be addressed. In this study, an innovative architecture involving an unmanned aircraft vehicle (UAV) and an unmanned ground vehicle (UGV) was examined for detailed inspections of power lines. In the proposed strategy, each vehicle provides its position information to the other, which ensures a safe inspection process. The results of real-world experiments indicate a satisfactory performance, thereby demonstrating the feasibility of the proposed approach.
KW  - cooperative UAV–UGV energy pylon inspection
KW  - cooperative vehicle position sensing
KW  - autonomous power line inspection
DO  - 10.3390/s20216384
ER  -
TY  - EJOU
AU  - Ren, Yuan
AU  - Zhang, Xuewei
AU  - Lu, Guangyue
TI  - The Wireless Solution to Realize Green IoT: Cellular Networks with Energy Efficient and Energy Harvesting Schemes
T2  - Energies

PY  - 2020
VL  - 13
IS  - 22
SN  - 1996-1073

AB  - With the tremendous increase of heterogeneous Internet of Things (IoT) devices and the different service requirements of these IoT applications, machine-type communication (MTC) has attracted considerable attention from both industry and academia. Owing to the prominent advantages of supporting pervasive connectivity and wide area coverage, the cellular network is advocated as the potential wireless solution to realize IoT deployment for MTC, and this creative network paradigm is called the cellular IoT (C-IoT). In this paper, we propose the three-layer structured C-IoT architecture for MTC and review the challenges for deploying green C-IoT. Then, effective strategies for realizing green C-IoT are presented, including the energy efficient and energy harvesting schemes. We put forward several strategies to make the C-IoT run in an energy-saving manner, such as efficient random access and barring mechanisms, self-adapting machine learning predictions, scheduling optimization, resource allocation, fog computing, and group-oriented transmission. As for the energy harvesting schemes, the ambient and dedicated energy harvesting strategies are investigated. Afterwards, we give a detailed case study, which shows the effectiveness of reducing power consumption for the proposed layered C-IoT architecture. Additionally, for real-time and non-real-time applications, the power consumption of different on-off states for MTC devices is discussed.
KW  - Internet of Things (IoT)
KW  - energy efficient
KW  - energy harvesting
KW  - machine-type communication
KW  - cellular IoT
DO  - 10.3390/en13225875
ER  -
TY  - EJOU
AU  - Pastucha, Elżbieta
AU  - Puniach, Edyta
AU  - Ścisłowicz, Agnieszka
AU  - Ćwiąkała, Paweł
AU  - Niewiem, Witold
AU  - Wiącek, Paweł
TI  - 3D Reconstruction of Power Lines Using UAV Images to Monitor Corridor Clearance
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Regular power line inspections are essential to ensure the reliability of electricity supply. The inspections of overground power submission lines include corridor clearance monitoring and fault identification. The power lines corridor is a three-dimensional space around power cables defined by a set distance. Any obstacles breaching this space should be detected, as they potentially threaten the safety of the infrastructure. Corridor clearance monitoring is usually performed either by a labor-intensive total station survey (TS), terrestrial laser scanning (TLS), or expensive airborne laser scanning (ALS) from a plane or a helicopter. This paper proposes a method that uses unmanned aerial vehicle (UAV) images to monitor corridor clearance. To maintain the adequate accuracy of the relative position of wires in regard to surrounding obstacles, the same data were used both to reconstruct a point cloud representation of a digital surface model (DSM) and a 3D power line. The proposed algorithm detects power lines in a series of images using decorrelation stretch for initial image processing, the modified Prewitt filter for edge enhancement, random sample consensus (RANSAC) with additional parameters for line fitting, and epipolar geometry for 3D reconstruction. DSM points intruding into the corridor are then detected by calculating the spatial distance between a reconstructed power line and the DSM point cloud representation. Problematic objects are localized by segmenting points into voxels and then subsequent clusterization. The processing results were compared to the results of two verification methods&mdash;TS and TLS. The comparison results show that the proposed method can be used to survey power lines with an accuracy consistent with that of classical measurements.
KW  - unmanned aerial vehicles
KW  - power lines
KW  - image-based reconstruction
KW  - 3D reconstruction
DO  - 10.3390/rs12223698
ER  -
TY  - EJOU
AU  - Barmpoutis, Panagiotis
AU  - Papaioannou, Periklis
AU  - Dimitropoulos, Kosmas
AU  - Grammalidis, Nikos
TI  - A Review on Early Forest Fire Detection Systems Using Optical Remote Sensing
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - The environmental challenges the world faces nowadays have never been greater or more complex. Global areas covered by forests and urban woodlands are threatened by natural disasters that have increased dramatically during the last decades, in terms of both frequency and magnitude. Large-scale forest fires are one of the most harmful natural hazards affecting climate change and life around the world. Thus, to minimize their impacts on people and nature, the adoption of well-planned and closely coordinated effective prevention, early warning, and response approaches are necessary. This paper presents an overview of the optical remote sensing technologies used in early fire warning systems and provides an extensive survey on both flame and smoke detection algorithms employed by each technology. Three types of systems are identified, namely terrestrial, airborne, and spaceborne-based systems, while various models aiming to detect fire occurrences with high accuracy in challenging environments are studied. Finally, the strengths and weaknesses of fire detection systems based on optical remote sensing are discussed aiming to contribute to future research projects for the development of early warning fire systems.
KW  - early fire detection
KW  - multispectral imaging systems
KW  - terrestrial
KW  - aerial
KW  - satellite
KW  - artificial intelligence
DO  - 10.3390/s20226442
ER  -
TY  - EJOU
AU  - Kim, Byunghyun
AU  - Cho, Soojin
TI  - Automated Multiple Concrete Damage Detection Using Instance Segmentation Deep Learning Model
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 22
SN  - 2076-3417

AB  - In many developed countries with a long history of urbanization, there is an increasing need for automated computer vision (CV)-based inspection to replace conventional labor-intensive visual inspection. This paper proposes a technique for the automated detection of multiple concrete damage based on a state-of-the-art deep learning framework, Mask R-CNN, developed for instance segmentation. The structure of Mask R-CNN, which consists of three stages (region proposal, classification, and segmentation) is optimized for multiple concrete damage detection. The optimized Mask R-CNN is trained with 765 concrete images including cracks, efflorescence, rebar exposure, and spalling. The performance of the trained Mask R-CNN is evaluated with 25 actual test images containing damage as well as environmental objects. Two types of metrics are proposed to measure localization and segmentation performance. On average, 90.41% precision and 90.81% recall are achieved for localization and 87.24% precision and 87.58% recall for segmentation, which indicates the excellent field applicability of the trained Mask R-CNN. This paper also qualitatively discusses the test results by explaining that the architecture of Mask R-CNN that is optimized for general object detection purposes, can be modified to detect long and slender shapes of cracks, rebar exposure, and efflorescence in further research.
KW  - multiple damage
KW  - deep learning
KW  - mask r-cnn
KW  - concrete crack
KW  - efflorescence
KW  - rebar exposure
KW  - spalling
DO  - 10.3390/app10228008
ER  -
TY  - EJOU
AU  - Zhao, Biquan
AU  - Li, Jiating
AU  - Baenziger, P. S.
AU  - Belamkar, Vikas
AU  - Ge, Yufeng
AU  - Zhang, Jian
AU  - Shi, Yeyin
TI  - Automatic Wheat Lodging Detection and Mapping in Aerial Imagery to Support High-Throughput Phenotyping and In-Season Crop Management
T2  - Agronomy

PY  - 2020
VL  - 10
IS  - 11
SN  - 2073-4395

AB  - Latest advances in unmanned aerial vehicle (UAV) technology and convolutional neural networks (CNNs) allow us to detect crop lodging in a more precise and accurate way. However, the performance and generalization of a model capable of detecting lodging when the plants may show different spectral and morphological signatures have not been investigated much. This study investigated and compared the performance of models trained using aerial imagery collected at two growth stages of winter wheat with different canopy phenotypes. Specifically, three CNN-based models were trained with aerial imagery collected at early grain filling stage only, at physiological maturity only, and at both stages. Results show that the multi-stage model trained by images from both growth stages outperformed the models trained by images from individual growth stages on all testing data. The mean accuracy of the multi-stage model was 89.23% for both growth stages, while the mean of the other two models were 52.32% and 84.9%, respectively. This study demonstrates the importance of diversity of training data in big data analytics, and the feasibility of developing a universal decision support system for wheat lodging detection and mapping multi-growth stages with high-resolution remote sensing imagery.
KW  - spatial data analysis
KW  - digital agriculture
KW  - decision support
KW  - deep learning
KW  - UAV
KW  - remote sensing
DO  - 10.3390/agronomy10111762
ER  -
TY  - EJOU
AU  - Kim, Jung J.
AU  - Kim, Ah-Ram
AU  - Lee, Seong-Won
TI  - Artificial Neural Network-Based Automated Crack Detection and Analysis for the Inspection of Concrete Structures
T2  - Applied Sciences

PY  - 2020
VL  - 10
IS  - 22
SN  - 2076-3417

AB  - The damage investigation and inspection methods for infrastructures performed in small-scale (type III) facilities usually involve a visual examination by an inspector using surveying tools (e.g., cracking, crack microscope, etc.) in the field. These methods can interfere with the subjectivity of the inspector, which may reduce the objectivity and reliability of the record. Therefore, a new image analysis technique is needed to automatically detect cracks and analyze the characteristics of the cracks objectively. In this study, an image analysis technique using deep learning is developed to detect cracks and analyze characteristics (e.g., length, and width) in images for small-scale facilities. Three stages of image processing pipeline are proposed to obtain crack detection and its characteristics. In the first and second stages, two-dimensional convolutional neural networks are used for crack image detection (e.g., classification and segmentation). Based on convolution neural network for the detection, hierarchical feature learning architecture is applied into our deep learning network. After deep learning-based detection, in the third stage, thinning and tracking algorithms are applied to analyze length and width of crack in the image. The performance of the proposed method was tested using various crack images with label and the results showed good performance of crack detection and its measurement.
KW  - concrete crack
KW  - concrete structure
KW  - artificial neural network
KW  - convolution neural network
DO  - 10.3390/app10228105
ER  -
TY  - EJOU
AU  - Liu, Huan
AU  - Li, Shiyong
AU  - Sun, Wei
TI  - Resource Allocation for Edge Computing without Using Cloud Center in Smart Home Environment: A Pricing Approach
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - Recently, more and more smart homes have become one of important parts of home infrastructure. However, most of the smart home applications are not interconnected and remain isolated. They use the cloud center as the control platform, which increases the risk of link congestion and data security. Thus, in the future, smart homes based on edge computing without using cloud center become an important research area. In this paper, we assume that all applications in a smart home environment are composed of edge nodes and users. In order to maximize the utility of users, we assume that all users and edge nodes are placed in a market and formulate a pricing resource allocation model with utility maximization. We apply the Lagrangian method to analyze the model, so an edge node (provider in the market) allocates its resources to a user (customer in the market) based on the prices of resources and the utility related to the preference of users. To obtain the optimal resource allocation, we propose a pricing-based resource allocation algorithm by using low-pass filtering scheme and conform that the proposed algorithm can achieve an optimum within reasonable convergence times through some numerical examples.
KW  - edge computing
KW  - smart homes
KW  - resource pricing
KW  - resource allocation
KW  - utility optimization
DO  - 10.3390/s20226545
ER  -
TY  - EJOU
AU  - Shawon, Ashifur R.
AU  - Ko, Jonghan
AU  - Jeong, Seungtaek
AU  - Shin, Taehwan
AU  - Lee, Kyung D.
AU  - Shim, Sang I.
TI  - Two-Dimensional Simulation of Barley Growth and Yield Using a Model Integrated with Remote-Controlled Aerial Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - It is important to be able to predict the yield and monitor the growth conditions of crops in the field to increase productivity. One way to assess field-based geospatial crop productivity is by integrating a crop model with a remote-controlled aerial system (RAS). The objective of this study was to simulate spatiotemporal barley growth and yield based on the development of a crop-modeling system integrated with RAS-based remote sensing images. We performed field experiments to obtain ground truth data and RAS images of crop growth conditions and yields at Chonnam National University (CNU), Gwangju, South Korea in 2018, and at Gyeongsang National University (GNU), Jinju, South Gyeongsang, South Korea in 2018 and 2019. In model calibration, there was no significant difference (p = 0.12) between the simulated barley yields and measured yields, based on a two-sample t-test at CNU in 2018. In model validation, there was no significant difference between simulated yields and measured yields at p = 0.98 and 0.76, according to two-sample t-tests at GNU in 2018 and 2019, respectively. The remote sensing-integrated crop model accurately reproduced geospatial variations in barley yield and growth variables. The results demonstrate that the crop modeling approach is useful for monitoring at-field barley conditions.
KW  - barley
KW  - crop model
KW  - integration
KW  - RAS
KW  - remote sensing
KW  - yield
DO  - 10.3390/rs12223766
ER  -
TY  - EJOU
AU  - Ma, Jingjing
AU  - Pang, Lei
AU  - Yan, Lei
AU  - Xiao, Jiang
TI  - Detection of Black Spot of Rose Based on Hyperspectral Imaging and Convolutional Neural Network
T2  - AgriEngineering

PY  - 2020
VL  - 2
IS  - 4
SN  - 2624-7402

AB  - Black spot is one of the seriously damaging plant diseases in China, especially in rose production. Hyperspectral technology reflects both external features and internal structure information of measured samples, which can be used to identify the disease. In this research, both the spectral and image features of two infected roses with black spot were used to train a convolutional neural network (CNN) model. Multiple scattering correction (MSC) and standard normal variable (SNV) methods were applied to preprocess the spectral data. Cropping, median filtering and binarization were pretreatments used on the hyperspectral images. Three CNN models based on Alexnet, VGG16 and neural discriminative dimensionality reduction (NDDR) were evaluated by analyzing the classification accuracy and loss function. The results show that the CNN model based on the fusion of features has higher accuracy. The highest accuracies of detection of blackspot in different roses are 12&ndash;26 (100%) and 13&ndash;54 (99.95%), applying the NDDR-CNN model. Therefore, this research indicates that the spectral analysis based on CNN can detect black spot of roses, which provides a reference for the detection of other plant diseases, and has favorable research significance as well as prospect for development.
KW  - hyperspectral imaging
KW  - black spot
KW  - infected samples
KW  - non-destructive detection
KW  - preprocessing
KW  - CNN model
DO  - 10.3390/agriengineering2040037
ER  -
TY  - EJOU
AU  - Li, Bo
AU  - Gan, Zhigang
AU  - Chen, Daqing
AU  - Sergey Aleksandrovich, Dyachenko
TI  - UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.
KW  - UAV
KW  - maneuvering target tracking
KW  - deep reinforcement learning
KW  - meta-learning
KW  - multi-tasks
DO  - 10.3390/rs12223789
ER  -
TY  - EJOU
AU  - Lu, Yijie
AU  - Zhang, Zhen
AU  - Huang, Danni
TI  - Glacier Mapping Based on Random Forest Algorithm: A Case Study over the Eastern Pamir
T2  - Water

PY  - 2020
VL  - 12
IS  - 11
SN  - 2073-4441

AB  - Debris-covered glaciers are common features on the eastern Pamir and serve as important indicators of climate change promptly. However, mapping of debris-covered glaciers in alpine regions is still challenging due to many factors including the spectral similarity between debris and the adjacent bedrock, shadows cast from mountains and clouds, and seasonal snow cover. Considering that few studies have added movement velocity features when extracting glacier boundaries, we innovatively developed an automatic algorithm consisting of rule-based image segmentation and Random Forest to extract information about debris-covered glaciers with Landsat-8 OLI/TIRS data for spectral, texture and temperature features, multi-digital elevation models (DEMs) for elevation and topographic features, and the Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) for movement velocity features, and accuracy evaluation was performed to determine the optimal feature combination extraction of debris-covered glaciers. The study found that the overall accuracy of extracting debris-covered glaciers using combined movement velocity features is 97.60%, and the Kappa coefficient is 0.9624, which is better than the extraction results using other schemes. The high classification accuracy obtained using our method overcomes most of the above-mentioned challenges and can detect debris-covered glaciers, illustrating that this method can be executed efficiently, which will further help water resources management.
KW  - Random Forest
KW  - Landsat
KW  - ITS_LIVE
KW  - movement velocity
KW  - the eastern Pamir
KW  - glacier mapping
DO  - 10.3390/w12113231
ER  -
TY  - EJOU
AU  - Khanal, Sami
AU  - KC, Kushal
AU  - Fulton, John P.
AU  - Shearer, Scott
AU  - Ozkan, Erdal
TI  - Remote Sensing in Agriculture—Accomplishments, Limitations, and Opportunities
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34%), followed by the United States (20%) and China (11%). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability.
KW  - remote sensing
KW  - satellite
KW  - UAS
KW  - precision agriculture
DO  - 10.3390/rs12223783
ER  -
TY  - EJOU
AU  - Ma, Lei
AU  - Schmitt, Michael
AU  - Zhu, Xiaoxiang
TI  - Uncertainty Analysis of Object-Based Land-Cover Classification Using Sentinel-2 Time-Series Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Recently, time-series from optical satellite data have been frequently used in object-based land-cover classification. This poses a significant challenge to object-based image analysis (OBIA) owing to the presence of complex spatio-temporal information in the time-series data. This study evaluates object-based land-cover classification in the northern suburbs of Munich using time-series from optical Sentinel data. Using a random forest classifier as the backbone, experiments were designed to analyze the impact of the segmentation scale, features (including spectral and temporal features), categories, frequency, and acquisition timing of optical satellite images. Based on our analyses, the following findings are reported: (1) Optical Sentinel images acquired over four seasons can make a significant contribution to the classification of agricultural areas, even though this contribution varies between spectral bands for the same period. (2) The use of time-series data alleviates the issue of identifying the &ldquo;optimal&rdquo; segmentation scale. The finding of this study can provide a more comprehensive understanding of the effects of classification uncertainty on object-based dense multi-temporal image classification.
KW  - OBIA
KW  - multi-temporal
KW  - random forest
KW  - mapping
KW  - optical Sentinel data
DO  - 10.3390/rs12223798
ER  -
TY  - EJOU
AU  - Sayeed, Mohd A.
AU  - Kumar, Rajesh
AU  - Sharma, Vishal
AU  - Sayeed, Mohd A.
TI  - Efficient Deployment with Throughput Maximization for UAVs Communication Networks
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 22
SN  - 1424-8220

AB  - The article presents a throughput maximization approach for UAV assisted ground networks. Throughput maximization involves minimizing delay and packet loss through UAV trajectory optimization, reinforcing the congested nodes and transmission channels. The aggressive reinforcement policy is achieved by characterizing nodes, links, and overall topology through delay, loss, throughput, and distance. A position-aware graph neural network (GNN) is used for characterization, prediction, and dynamic UAV trajectory enhancement. To establish correctness, the proposed approach is validated against optimized link state routing (OLSR) driven UAV assisted ground networks. The proposed approach considerably outperforms the classical approach by demonstrating significant gains in throughput and packet delivery ratio with notable decrements in delay and packet loss. The performance analysis of the proposed approach against software-defined UAVs (U-S) and UAVs as base stations (U-B) verifies the consistency and gains in average throughput while minimizing delay and packet loss. The scalability test of the proposed approach is performed by varying data rates and the number of UAVs.
KW  - UAV
KW  - throughput
KW  - delay
KW  - packet loss
KW  - GNN
KW  - collaborative network
KW  - trajectory
DO  - 10.3390/s20226680
ER  -
TY  - EJOU
AU  - Tian, Xiaomin
AU  - Chen, Long
AU  - Zhang, Xiaoli
AU  - Chen, Erxue
TI  - Improved Prototypical Network Model for Forest Species Classification in Complex Stand
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 22
SN  - 2072-4292

AB  - Deep learning has become an effective method for hyperspectral image classification. However, the high band correlation and data volume associated with airborne hyperspectral images, and the insufficiency of training samples, present challenges to the application of deep learning in airborne image classification. Prototypical networks are practical deep learning networks that have demonstrated effectiveness in handling small-sample classification. In this study, an improved prototypical network is proposed (by adding L2 regularization to the convolutional layer and dropout to the maximum pooling layer) to address the problem of overfitting in small-sample classification. The proposed network has an optimal sample window for classification, and the window size is related to the area and distribution of the study area. After performing dimensionality reduction using principal component analysis, the time required for training using hyperspectral images shortened significantly, and the test accuracy increased drastically. Furthermore, when the size of the sample window was 27 &times; 27 after dimensionality reduction, the overall accuracy of forest species classification was 98.53%, and the Kappa coefficient was 0.9838. Therefore, by using an improved prototypical network with a sample window of an appropriate size, the network yielded desirable classification results, thereby demonstrating its suitability for the fine classification and mapping of tree species.
KW  - hyperspectral images
KW  - prototypical network
KW  - tree species classification
KW  - small-sample
KW  - dimensionality reduction
DO  - 10.3390/rs12223839
ER  -
TY  - EJOU
AU  - Scharvogel, Daniel
AU  - Brandmeier, Melanie
AU  - Weis, Manuel
TI  - A Deep Learning Approach for Calamity Assessment Using Sentinel-2 Data
T2  - Forests

PY  - 2020
VL  - 11
IS  - 12
SN  - 1999-4907

AB  - The number of severe storm events has increased in recent decades due to climate change. These storms are one of the main causes for timber loss in European forests and damaged areas are prone to further degradation by, for example, bark beetle infestations. Usually, manual mapping of damaged areas based on aerial photographs is conducted by forest departments. This is very time-consuming and therefore automatic detection of windthrows based on active and passive remote sensing data is an ongoing research topic. In this study we evaluated state-of-the-art Convolutional Neural Networks (CNNs) in combination with Geographic Information Systems (GIS) for calamity assessment. The study area is in in the northern part of Hesse (Germany) and was covered by twelve Sentinel-2 scenes from 2018. Labels of damaged areas from the Friedericke storm (18 January 2018) were provided by HessenForst. We conducted several experiments based on a custom U-Net setup to derive the optimal architecture and input data as well as to assess the transferability of the model. Results highlight the possibility to detect damaged forest areas using Sentinel-2 data. Using a binary classification, accuracies of more than 92% were achieved with an Intersection over Union (IoU) score of 46.6%. The proposed workflow was integrated into ArcGIS and is suitable for fast detection of damaged areas directly after a storm and for disaster management but is limited by the deca-meter spatial resolution of the Sentinel-2 data.
KW  - CNNs
KW  - remote sensing
KW  - windthrow
KW  - forest
KW  - Deep Learning
KW  - GIS
DO  - 10.3390/f11121239
ER  -
TY  - EJOU
AU  - Kolosov, Kirill
AU  - Miller, Alexander
AU  - Miller, Boris
TI  - Robust Data Fusion of UAV Navigation Measurements with Application to the Landing System
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - To perform precise approach and landing concerning an aircraft in automatic mode, local airfield-based landing systems are used. For joint processing of measurements of the onboard inertial navigation systems (INS), altimeters and local landing systems, the Kalman filter is usually used. The application of the quadratic criterion in the Kalman filter entails the well-known problem of high sensitivity of the estimate to anomalous measurement errors. During the automatic approach phase, abnormal navigation errors can lead to disaster, so the data fusion algorithm must automatically identify and isolate abnormal measurements. This paper presents a recurrent filtering algorithm that is resistant to anomalous errors in measurements and considers its application in the data fusion problem for landing system measurements with onboard sensor measurements&mdash;INS and altimeters. The robustness of the estimate is achieved through the combined use of the least modulus method and the Kalman filter. To detect and isolate failures the chi-square criterion is used. It makes possible the customization of the algorithm in accordance with the requirements for false alarm probability and the alarm missing probability. Testing results of the robust filtering algorithm are given both for synthesized data and for real measurements.
KW  - automatic landing
KW  - data fusion
KW  - Kalman filter
KW  - least modulus method
KW  - L1 optimization
KW  - M estimate
KW  - adaptive filtering
KW  - robust filtering
KW  - navigation
KW  - fault tolerance
KW  - non-Gaussian noise
DO  - 10.3390/rs12233849
ER  -
TY  - EJOU
AU  - Qi, Haixia
AU  - Zhu, Bingyu
AU  - Wu, Zeyu
AU  - Liang, Yu
AU  - Li, Jianwen
AU  - Wang, Leidi
AU  - Chen, Tingting
AU  - Lan, Yubin
AU  - Zhang, Lei
TI  - Estimation of Peanut Leaf Area Index from Unmanned Aerial Vehicle Multispectral Images
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 23
SN  - 1424-8220

AB  - Leaf area index (LAI) is used to predict crop yield, and unmanned aerial vehicles (UAVs) provide new ways to monitor LAI. In this study, we used a fixed-wing UAV with multispectral cameras for remote sensing monitoring. We conducted field experiments with two peanut varieties at different planting densities to estimate LAI from multispectral images and establish a high-precision LAI prediction model. We used eight vegetation indices (VIs) and developed simple regression and artificial neural network (BPN) models for LAI and spectral VIs. The empirical model was calibrated to estimate peanut LAI, and the best model was selected from the coefficient of determination and root mean square error. The red (660 nm) and near-infrared (790 nm) bands effectively predicted peanut LAI, and LAI increased with planting density. The predictive accuracy of the multiple regression model was higher than that of the single linear regression models, and the correlations between Modified Red-Edge Simple Ratio Index (MSR), Ratio Vegetation Index (RVI), Normalized Difference Vegetation Index (NDVI), and LAI were higher than the other indices. The combined VI BPN model was more accurate than the single VI BPN model, and the BPN model accuracy was higher. Planting density affects peanut LAI, and reflectance-based vegetation indices can help predict LAI.
KW  - leaf area index
KW  - multispectral
KW  - remote sensing
KW  - density
KW  - vegetation index
DO  - 10.3390/s20236732
ER  -
TY  - EJOU
AU  - Roy, Raphaëlle N.
AU  - Drougard, Nicolas
AU  - Gateau, Thibault
AU  - Dehais, Frédéric
AU  - Chanel, Caroline P. C.
TI  - How Can Physiological Computing Benefit Human-Robot Interaction?
T2  - Robotics

PY  - 2020
VL  - 9
IS  - 4
SN  - 2218-6581

AB  - As systems grow more automatized, the human operator is all too often overlooked. Although human-robot interaction (HRI) can be quite demanding in terms of cognitive resources, the mental states (MS) of the operators are not yet taken into account by existing systems. As humans are no providential agents, this lack can lead to hazardous situations. The growing number of neurophysiology and machine learning tools now allows for efficient operators&rsquo; MS monitoring. Sending feedback on MS in a closed-loop solution is therefore at hand. Involving a consistent automated planning technique to handle such a process could be a significant asset. This perspective article was meant to provide the reader with a synthesis of the significant literature with a view to implementing systems that adapt to the operator&rsquo;s MS to improve human-robot operations&rsquo; safety and performance. First of all, the need for this approach is detailed regarding remote operation, an example of HRI. Then, several MS identified as crucial for this type of HRI are defined, along with relevant electrophysiological markers. A focus is made on prime degraded MS linked to time-on-task and task demands, as well as collateral MS linked to system outputs (i.e., feedback and alarms). Lastly, the principle of symbiotic HRI is detailed and one solution is proposed to include the operator state vector into the system using a mixed-initiative decisional framework to drive such an interaction.
KW  - human-robot interaction
KW  - telerobotics
KW  - teleoperation
KW  - physiological computing
KW  - mental state monitoring
KW  - passive BCI
KW  - mixed-initiative
KW  - automated planning
DO  - 10.3390/robotics9040100
ER  -
TY  - EJOU
AU  - Ayele, Yonas Z.
AU  - Aliyari, Mostafa
AU  - Griffiths, David
AU  - Droguett, Enrique L.
TI  - Automatic Crack Segmentation for UAV-Assisted Bridge Inspection
T2  - Energies

PY  - 2020
VL  - 13
IS  - 23
SN  - 1996-1073

AB  - Bridges are a critical piece of infrastructure in the network of road and rail transport system. Many of the bridges in Norway (in Europe) are at the end of their lifespan, therefore regular inspection and maintenance are critical to ensure the safety of their operations. However, the traditional inspection procedures and resources required are so time consuming and costly that there exists a significant maintenance backlog. The central thrust of this paper is to demonstrate the significant benefits of adapting a Unmanned Aerial Vehicle (UAV)-assisted inspection to reduce the time and costs of bridge inspection and established the research needs associated with the processing of the (big) data produced by such autonomous technologies. In this regard, a methodology is proposed for analysing the bridge damage that comprises three key stages, (i) data collection and model training, where one performs experiments and trials to perfect drone flights for inspection using case study bridges to inform and provide necessary (big) data for the second key stage, (ii) 3D construction, where one built 3D models that offer a permanent record of element geometry for each bridge asset, which could be used for navigation and control purposes, (iii) damage identification and analysis, where deep learning-based data analytics and modelling are applied for processing and analysing UAV image data and to perform bridge damage performance assessment. The proposed methodology is exemplified via UAV-assisted inspection of Skodsberg bridge, a 140 m prestressed concrete bridge, in the Viken county in eastern Norway.
KW  - drone-assisted bridge inspection
KW  - crack detection
KW  - crack segmentation
KW  - damage assessment
KW  - UAV
KW  - performance analysis
DO  - 10.3390/en13236250
ER  -
TY  - EJOU
AU  - Egli, Sebastian
AU  - Höpke, Martin
TI  - CNN-Based Tree Species Classification Using High Resolution RGB Image Data from Automated UAV Observations
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - Data on the distribution of tree species are often requested by forest managers, inventory agencies, foresters as well as private and municipal forest owners. However, the automated detection of tree species based on passive remote sensing data from aerial surveys is still not sufficiently developed to achieve reliable results independent of the phenological stage, time of day, season, tree vitality and prevailing atmospheric conditions. Here, we introduce a novel tree species classification approach based on high resolution RGB image data gathered during automated UAV flights that overcomes these insufficiencies. For the classification task, a computationally lightweight convolutional neural network (CNN) was designed. We show that with the chosen CNN model architecture, average classification accuracies of 92% can be reached independently of the illumination conditions and the phenological stages of four different tree species. We also show that a minimal ground sampling density of 1.6 cm/px is needed for the classification model to be able to make use of the spatial-structural information in the data. Finally, to demonstrate the applicability of the presented approach to derive spatially explicit tree species information, a gridded product is generated that yields an average classification accuracy of 88%.
KW  - tree species classification
KW  - CNN
KW  - UAV
KW  - RGB
DO  - 10.3390/rs12233892
ER  -
TY  - EJOU
AU  - Graf, Lukas
AU  - Bach, Heike
AU  - Tiede, Dirk
TI  - Semantic Segmentation of Sentinel-2 Imagery for Mapping Irrigation Center Pivots
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - Estimating the number and size of irrigation center pivot systems (CPS) from remotely sensed data, using artificial intelligence (AI), is a potential information source for assessing agricultural water use. In this study, we identified two technical challenges in the neural-network-based classification: Firstly, an effective reduction of the feature space of the remote sensing data to shorten training times and increase classification accuracy is required. Secondly, the geographical transferability of the AI algorithms is a pressing issue if AI is to replace human mapping efforts one day. Therefore, we trained the semantic image segmentation algorithm U-NET on four spectral channels (U-NET SPECS) and the first three principal components (U-NET principal component analysis (PCA)) of ESA/Copernicus Sentinel-2 images on a study area in Texas, USA, and assessed the geographic transferability of the trained models to two other sites: the Duero basin, in Spain, and South Africa. U-NET SPECS outperformed U-NET PCA at all three study areas, with the highest f1-score at Texas (0.87, U-NET PCA: 0.83), and a value of 0.68 (U-NET PCA: 0.43) in South Africa. At the Duero, both models showed poor classification accuracy (f1-score U-NET PCA: 0.08; U-NET SPECS: 0.16) and segmentation quality, which was particularly evident in the incomplete representation of the center pivot geometries. In South Africa and at the Duero site, a high rate of false positive and false negative was observed, which made the model less useful, especially at the Duero test site. Thus, geographical invariance is not an inherent model property and seems to be mainly driven by the complexity of land-use pattern. We do not consider PCA a suited spectral dimensionality reduction measure in this. However, shorter training times and a more stable training process indicate promising prospects for reducing computational burdens. We therefore conclude that effective dimensionality reduction and geographic transferability are important prospects for further research towards the operational usage of deep learning algorithms, not only regarding the mapping of CPS.
KW  - center pivot systems
KW  - irrigation
KW  - semantic segmentation
KW  - U-NET
KW  - neural network
KW  - AI
KW  - Sentinel-2
DO  - 10.3390/rs12233937
ER  -
TY  - EJOU
AU  - Cipriani, Giovanni
AU  - D’Amico, Antonino
AU  - Guarino, Stefania
AU  - Manno, Donatella
AU  - Traverso, Marzia
AU  - Di Dio, Vincenzo
TI  - Convolutional Neural Network for Dust and Hotspot Classification in PV Modules
T2  - Energies

PY  - 2020
VL  - 13
IS  - 23
SN  - 1996-1073

AB  - This paper proposes an innovative approach to classify the losses related to photovoltaic (PV) systems, through the use of thermographic non-destructive tests (TNDTs) supported by artificial intelligence techniques. Low electricity production in PV systems can be caused by an efficiency decrease in PV modules due to abnormal operating conditions such as failures or malfunctions. The most common performance decreases are due to the presence of dirt on the surface of the module, the impact of which depends on many parameters and conditions, and can be identified through the use of the TNDTs. The proposed approach allows one to automatically classify the thermographic images from the convolutional neural network (CNN) of the system, achieving an accuracy of 98% in tests that last a couple of minutes. This approach, compared to approaches in literature, offers numerous advantages, including speed of execution, speed of diagnosis, reduced costs, reduction in electricity production losses.
KW  - infrared thermography
KW  - diagnostics
KW  - renewable energy
KW  - photovoltaic energy
KW  - energy efficient
KW  - artificial intelligence
KW  - convolutional neural network
KW  - dust
KW  - hot spot
DO  - 10.3390/en13236357
ER  -
TY  - EJOU
AU  - Li, Ping
AU  - Ni, Zhiwei
AU  - Zhu, Xuhui
AU  - Song, Juan
AU  - Wu, Wenying
TI  - Optimal Transport with Dimensionality Reduction for Domain Adaptation
T2  - Symmetry

PY  - 2020
VL  - 12
IS  - 12
SN  - 2073-8994

AB  - Domain adaptation manages to learn a robust classifier for target domain, using the source domain, but they often follow different distributions. To bridge distribution shift between the two domains, most of previous works aim to align their feature distributions through feature transformation, of which optimal transport for domain adaptation has attract researchers&rsquo; interest, as it can exploit the local information of the two domains in the process of mapping the source instances to the target ones by minimizing Wasserstein distance between their feature distributions. However, it may weaken the feature discriminability of source domain, thus degrade domain adaptation performance. To address this problem, this paper proposes a two-stage feature-based adaptation approach, referred to as optimal transport with dimensionality reduction (OTDR). In the first stage, we apply the dimensionality reduction with intradomain variant maximization but source intraclass compactness minimization, to separate data samples as much as possible and enhance the feature discriminability of the source domain. In the second stage, we leverage optimal transport-based technique to preserve the local information of the two domains. Notably, the desirable properties in the first stage can mitigate the degradation of feature discriminability of the source domain in the second stage. Extensive experiments on several cross-domain image datasets validate that OTDR is superior to its competitors in classification accuracy.
KW  - domain adaptation
KW  - dimensionality reduction
KW  - optimal transport
KW  - feature alignment
DO  - 10.3390/sym12121994
ER  -
TY  - EJOU
AU  - Li, Zhen
AU  - Zhao, Baojun
AU  - Wang, Wenzheng
TI  - An Efficient Spectral Feature Extraction Framework for Hyperspectral Images
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - Extracting diverse spectral features from hyperspectral images has become a hot topic in recent years. However, these models are time consuming for training and test and suffer from a poor discriminative ability, resulting in low classification accuracy. In this paper, we design an effective feature extracting framework for the spectra of hyperspectral data. We construct a structured dictionary to encode spectral information and apply learning machine to map coding coefficients. To reduce training and testing time, the sparsity constraint is replaced by a block-diagonal constraint to accelerate the iteration, and an efficient extreme learning machine is employed to fit the spectral characteristics. To optimize the discriminative ability of our model, we first add spectral convolution to extract abundant spectral information. Then, we design shared constraints for subdictionaries so that the common features of subdictionaries can be expressed more effectively, and the discriminative and reconstructive ability of dictionary will be improved. The experimental results on diverse databases show that the proposed feature extraction framework can not only greatly reduce the training and testing time, but also lead to very competitive accuracy performance compared with deep learning models.
KW  - hyperspectral images
KW  - efficient
KW  - feature extraction
KW  - dictionary learning
DO  - 10.3390/rs12233967
ER  -
TY  - EJOU
AU  - Zhu, Yongyan
AU  - Jeon, Seongwoo
AU  - Sung, Hyunchan
AU  - Kim, Yoonji
AU  - Park, Chiyoung
AU  - Cha, Sungeun
AU  - Jo, Hyun-woo
AU  - Lee, Woo-kyun
TI  - Developing UAV-Based Forest Spatial Information and Evaluation Technology for Efficient Forest Management
T2  - Sustainability

PY  - 2020
VL  - 12
IS  - 23
SN  - 2071-1050

AB  - Forest spatial information is regularly established and managed as basic data for national forest planning and forest policy establishment. Among them, the grade of vegetation conservation shall be investigated and evaluated according to the value of vegetation conservation. As the collection of field data over large or remote areas is difficult, unmanned aerial vehicles (UAVs) are increasingly being used for this purpose. Consequently, there is a need for research on UAV-monitoring and three-dimensional (3D) image generation techniques. In this study, a new method that can efficiently collect and analyze UAV spatial data to survey and assess forests was developed. Both UAV-based and LiDAR imaging methods were evaluated in conjunction with the ground control point measurement method for forest surveys. In addition, by fusing the field survey database of each target site and the UAV optical and LiDAR images, the Gongju, Samcheok, and Seogwipo regions were analyzed based on deep learning. The kappa value showed 0.59, 0.47, and 0.78 accuracy for each of the sites in terms of vegetation type (artificial or natural), and 0.68, 0.53, and 0.62 accuracy in terms of vegetation layer structure. The results of comparative analysis with ecological natural maps by establishing vegetation conservation levels show that about 83.9% of the areas are consistent. The findings verified the applicability of this UAV-based approach for the construction of geospatial information on forests. The proposed method can be useful for improving the efficiency of the Vegetation Conservation Classification system and for conducting high-resolution monitoring in forests worldwide.
KW  - unmanned aerial vehicles
KW  - LiDAR
KW  - CNNs
KW  - forest assessments
KW  - vegetation conservation classification
DO  - 10.3390/su122310150
ER  -
TY  - EJOU
AU  - Zhou, Dongbo
AU  - Liu, Shuangjian
AU  - Yu, Jie
AU  - Li, Hao
TI  - A High-Resolution Spatial and Time-Series Labeled Unmanned Aerial Vehicle Image Dataset for Middle-Season Rice
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 12
SN  - 2220-9964

AB  - The existing remote sensing image datasets target the identification of objects, features, or man-made targets but lack the ability to provide the date and spatial information for the same feature in the time-series images. The spatial and temporal information is important for machine learning methods so that networks can be trained to support precision classification, particularly for agricultural applications of specific crops with distinct phenological growth stages. In this paper, we built a high-resolution unmanned aerial vehicle (UAV) image dataset for middle-season rice. We scheduled the UAV data acquisition in five villages of Hubei Province for three years, including 11 or 13 growing stages in each year that were accompanied by the annual agricultural surveying business. We investigated the accuracy of the vector maps for each field block and the precise information regarding the crops in the field by surveying each village and periodically arranging the UAV flight tasks on a weekly basis during the phenological stages. Subsequently, we developed a method to generate the samples automatically. Finally, we built a high-resolution UAV image dataset, including over 500,000 samples with the location and phenological growth stage information, and employed the imagery dataset in several machine learning algorithms for classification. We performed two exams to test our dataset. First, we used four classical deep learning networks for the fine classification of spatial and temporal information. Second, we used typical models to test the land cover on our dataset and compared this with the UCMerced Land Use Dataset and RSSCN7 Dataset. The results showed that the proposed image dataset supported typical deep learning networks in the classification task to identify the location and time of middle-season rice and achieved high accuracy with the public image dataset.
KW  - remote sensing image dataset
KW  - spatial and time-series data
KW  - deep learning
KW  - middle-season rice
KW  - UAV
DO  - 10.3390/ijgi9120728
ER  -
TY  - EJOU
AU  - Haji, Mona
AU  - Kerbache, Laoucine
AU  - Muhammad, Mahaboob
AU  - Al-Ansari, Tareq
TI  - Roles of Technology in Improving Perishable Food Supply Chains
T2  - Logistics

PY  - 2020
VL  - 4
IS  - 4
SN  - 2305-6290

AB  - Food supply chains are considered to be more complex systems than other types of supply chains. This complexity is due to the continuous changes taking place, particularly in ensuring the quality of food products throughout the entire supply chain, from growing, procurement of resources, production, and management of stock, to distribution to the final consumers. For that, food supply chain markets have become more highly developed in the use of modern technologies, and have begun to implement them in their logistical systems to satisfy their customers&rsquo; needs. The main objectives of this review are to identify the different technological implementations in different phases of the food supply chain processes and point out the key factors for using technologies to improve the characteristics of the perishable food supply chain. A total number of 137 articles were analyzed in this research to achieve these review objectives. Some of the various technologies found in different phases of the food supply chain were radio frequency identification (RFID), the Internet of Things (IoT), blockchain, three-dimensional printing (3DP), autonomous vehicles, and unmanned aerial vehicles (UAVs). These technologies were found in different phases of the food supply chain and improved the efficiency of supplying perishable foods. The review identified different characteristics of the perishable food supply chain. The main finding indicated that technological implementation enhances the efficiency and sustainability of the food supply chains and helps to retain perishable food characteristics.
KW  - food supply chain
KW  - traceability technologies
KW  - perishable foods
KW  - supply chain innovation
DO  - 10.3390/logistics4040033
ER  -
TY  - EJOU
AU  - Nevavuori, Petteri
AU  - Narra, Nathaniel
AU  - Linna, Petri
AU  - Lipping, Tarmo
TI  - Crop Yield Prediction Using Multitemporal UAV Data and Spatio-Temporal Deep Learning Models
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 23
SN  - 2072-4292

AB  - Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51% mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17% MAPE with four weekly frames from the beginning of the season.
KW  - crop yield prediction
KW  - UAV
KW  - spatio-temporal modelling
KW  - time series
KW  - deep learning
KW  - cnn-lstm
KW  - convolutional lstm
KW  - 3d-cnn
DO  - 10.3390/rs12234000
ER  -
TY  - EJOU
AU  - Yang, Zhao
AU  - Tang, Rong
AU  - Bao, Jie
AU  - Lu, Jiahuan
AU  - Zhang, Zhijie
TI  - A Real-Time Trajectory Prediction Method of Small-Scale Quadrotors Based on GPS Data and Neural Network
T2  - Sensors

PY  - 2020
VL  - 20
IS  - 24
SN  - 1424-8220

AB  - This paper proposes a real-time trajectory prediction method for quadrotors based on a bidirectional gated recurrent unit model. Historical trajectory data of ten types of quadrotors were obtained. The bidirectional gated recurrent units were constructed and utilized to learn the historic data. The prediction results were compared with the traditional gated recurrent unit method to test its prediction performance. The efficiency of the proposed algorithm was investigated by comparing the training loss and training time. The results over the testing datasets showed that the proposed model produced better prediction results than the baseline models for all scenarios of the testing datasets. It was also found that the proposed model can converge to a stable state faster than the traditional gated recurrent unit model. Moreover, various types of training samples were applied and compared. With the same randomly selected test datasets, the performance of the prediction model can be improved by selecting the historical trajectory samples of the quadrotors close to the weight or volume of the target quadrotor for training. In addition, the performance of stable trajectory samples is significantly better than that with unstable trajectory segments with a frequent change of speed and direction with large angles.
KW  - trajectory prediction
KW  - quadrotor
KW  - gated recurrent unit
KW  - position
DO  - 10.3390/s20247061
ER  -
TY  - EJOU
AU  - Murtiyoso, Arnadi
AU  - Veriandi, Mirza
AU  - Suwardhi, Deni
AU  - Soeksmantono, Budhy
AU  - Harto, Agung B.
TI  - Automatic Workflow for Roof Extraction and Generation of 3D CityGML Models from Low-Cost UAV Image-Derived Point Clouds
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 12
SN  - 2220-9964

AB  - Developments in UAV sensors and platforms in recent decades have stimulated an upsurge in its application for 3D mapping. The relatively low-cost nature of UAVs combined with the use of revolutionary photogrammetric algorithms, such as dense image matching, has made it a strong competitor to aerial lidar mapping. However, in the context of 3D city mapping, further 3D modeling is required to generate 3D city models which is often performed manually using, e.g., photogrammetric stereoplotting. The aim of the paper was to try to implement an algorithmic approach to building point cloud segmentation, from which an automated workflow for the generation of roof planes will also be presented. 3D models of buildings are then created using the roofs’ planes as a base, therefore satisfying the requirements for a Level of Detail (LoD) 2 in the CityGML paradigm. Consequently, the paper attempts to create an automated workflow starting from UAV-derived point clouds to LoD 2-compatible 3D model. Results show that the rule-based segmentation approach presented in this paper works well with the additional advantage of instance segmentation and automatic semantic attribute annotation, while the 3D modeling algorithm performs well for low to medium complexity roofs. The proposed workflow can therefore be implemented for simple roofs with a relatively low number of planar surfaces. Furthermore, the automated approach to the 3D modeling process also helps to maintain the geometric requirements of CityGML such as 3D polygon coplanarity vis-à-vis manual stereoplotting.
KW  - UAV
KW  - photogrammetry
KW  - point cloud
KW  - roof
KW  - 3D model
KW  - CityGML
KW  - low-cost
KW  - automation
DO  - 10.3390/ijgi9120743
ER  -
TY  - EJOU
AU  - Ellsäßer, Florian
AU  - Röll, Alexander
AU  - Ahongshangbam, Joyson
AU  - Waite, Pierre-André
AU  - Hendrayanto
AU  - Schuldt, Bernhard
AU  - Hölscher, Dirk
TI  - Predicting Tree Sap Flux and Stomatal Conductance from Drone-Recorded Surface Temperatures in a Mixed Agroforestry System—A Machine Learning Approach
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 24
SN  - 2072-4292

AB  - Plant transpiration is a key element in the hydrological cycle. Widely used methods for its assessment comprise sap flux techniques for whole-plant transpiration and porometry for leaf stomatal conductance. Recently emerging approaches based on surface temperatures and a wide range of machine learning techniques offer new possibilities to quantify transpiration. The focus of this study was to predict sap flux and leaf stomatal conductance based on drone-recorded and meteorological data and compare these predictions with in-situ measured transpiration. To build the prediction models, we applied classical statistical approaches and machine learning algorithms. The field work was conducted in an oil palm agroforest in lowland Sumatra. Random forest predictions yielded the highest congruence with measured sap flux (r2 = 0.87 for trees and r2 = 0.58 for palms) and confidence intervals for intercept and slope of a Passing-Bablok regression suggest interchangeability of the methods. Differences in model performance are indicated when predicting different tree species. Predictions for stomatal conductance were less congruent for all prediction methods, likely due to spatial and temporal offsets of the measurements. Overall, the applied drone and modelling scheme predicts whole-plant transpiration with high accuracy. We conclude that there is large potential in machine learning approaches for ecological applications such as predicting transpiration.
KW  - transpiration
KW  - method comparison
KW  - UAV
KW  - oil palm
KW  - multiple linear regression
KW  - support vector machine
KW  - random forest
KW  - artificial neural network
DO  - 10.3390/rs12244070
ER  -
TY  - EJOU
AU  - Kavats, Olena
AU  - Khramov, Dmitriy
AU  - Sergieieva, Kateryna
AU  - Vasyliev, Volodymyr
TI  - Monitoring of Sugarcane Harvest in Brazil Based on Optical and SAR Data
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 24
SN  - 2072-4292

AB  - The algorithms for determining sugarcane harvest dates are proposed; the algorithms allow the ability to monitor large areas and are based on the publicly available Synthetic Aperture Radar (SAR) and optical satellite data. Algorithm 1 uses the NDVI (Normalized Difference Vegetation Index) time series derived from Sentinel-2 data. Sharp and continuous decrease in the NDVI values is the main sign of sugarcane harvest. The NDVI time series allows the ability to determine most harvest dates. The best estimates of the sugarcane areas harvested per month have been obtained from March to August 2018 when cloudy pixel percentage is less than 45% of the image area. Algorithm 2 of the harvest monitoring uses the coherence time series derived from Sentinel-1 Single Look Complex (SLC) images and optical satellite data. Low coherence, demonstrating sharp growth upon the harvest completion, corresponds to the harvest period. The NDVI time series trends were used to refine the algorithm. It is supposed that the descending NDVI trend corresponds to harvest. The algorithms were used to identify the harvest dates and calculate the harvested areas of the reference sample of 574 sugarcane parcels with a total area of 3745 ha in the state of S&atilde;o Paulo, Brazil. The harvested areas identified by visual interpretation coincide with the optical-data algorithm (algorithm 1) by 97%; the coincidence with the algorithm based on SAR and optical data (algorithm 2) is 90%. The main practical applications of the algorithms are harvest monitoring and identification of the harvested fields to estimate the harvested area.
KW  - Sentinel-1
KW  - Sentinel-2
KW  - NDVI
KW  - SLC
KW  - harvest
KW  - sugarcane
KW  - coherence
KW  - Brazil
KW  - monitoring
DO  - 10.3390/rs12244080
ER  -
TY  - EJOU
AU  - Zhang, Xuechen
AU  - Shen, Huanfeng
AU  - Li, Tongwen
AU  - Zhang, Liangpei
TI  - The Effects of Fireworks Discharge on Atmospheric PM2.5 Concentration in the Chinese Lunar New Year
T2  - International Journal of Environmental Research and Public Health

PY  - 2020
VL  - 17
IS  - 24
SN  - 1660-4601

AB  - Discharging fireworks during the Chinese Lunar New Year celebrations is a deep-rooted custom in China. In this paper, we analyze the effect of this cultural activity on PM2.5 concentration using both ground observations and satellite data. By combining remote sensing data, the problem of uneven spatial distribution of ground monitoring has been compensated, and the research time span has been expanded. The results show that the extensive firework displays on New Year&rsquo;s Eve lead to a remarkable increase in nationwide PM2.5 concentration, which were 159~223% of the average level, indicating the instantaneous effect far exceeds that of any other factor over the whole year. However, the averaged PM2.5 concentrations of the celebration period were 0.99~16.32 &mu;g/m3 lower compared to the average values of the corresponding pre-celebration period and post-celebration period, indicating the sustained effect is not very significant. The implementation of firework prohibition policies can greatly reduce the instantaneous PM2.5 increase, but no obvious air quality improvement is observed over the entire celebration period. Combining these findings and the cultural significance of this activity, we recommend that this custom is actively maintained, using new technologies and scientific governance programs to minimize the negative effects.
KW  - fireworks
KW  - PM2.5 concentration
KW  - Chinese Lunar New Year
KW  - remote sensing
KW  - firework prohibition policy
DO  - 10.3390/ijerph17249333
ER  -
TY  - EJOU
AU  - Hyun, Chang-Uk
AU  - Park, Mijin
AU  - Lee, Won Y.
TI  - Remotely Piloted Aircraft System (RPAS)-Based Wildlife Detection: A Review and Case Studies in Maritime Antarctica
T2  - Animals

PY  - 2020
VL  - 10
IS  - 12
SN  - 2076-2615

AB  - In wildlife biology, it is important to conduct efficient observations and quantitative monitoring of wild animals. Conventional wildlife monitoring mainly relies on direct field observations by the naked eyes or through binoculars, on-site image acquisition at fixed spots, and sampling or capturing under severe areal constraints. Recently, remotely piloted aircraft systems (RPAS), also called drones or unmanned aerial vehicles (UAV), were successfully applied to detect wildlife with imaging sensors, such as RGB and thermal-imaging sensors, with superior detection capabilities to those of human observation. Here, we review studies with RPAS which has been increasingly used in wildlife detection and explain how an RPAS-based high-resolution RGB image can be applied to wild animal studies from the perspective of individual detection and population surveys as well as behavioral studies. The applicability of thermal-imaging sensors was also assessed with further information extractable from image analyses. In addition, RPAS-based case studies of acquisition of high-resolution RGB images for the purpose of detecting southern elephant seals (Mirounga leonina) and shape property extraction using thermal-imaging sensor in King George Island, maritime Antarctica is presented as applications in an extreme environment. The case studies suggest that currently available cost-effective small-sized RPAS, which are capable of flexible operation and mounting miniaturized imaging sensors, and are easily maneuverable even from an inflatable boat, can be an effective and supportive technique for both the visual interpretation and quantitative analysis of wild animals in low-accessible extreme or maritime environments.
KW  - wildlife biology
KW  - remotely piloted aircraft system
KW  - UAV
KW  - drone
KW  - quantitative monitoring
KW  - polar region
KW  - thermal-imaging sensor
DO  - 10.3390/ani10122387
ER  -
TY  - EJOU
AU  - Fuentes, Jose E.
AU  - Moya, Francisco D.
AU  - Montoya, Oscar D.
TI  - Method for Estimating Solar Energy Potential Based on Photogrammetry from Unmanned Aerial Vehicles
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 12
SN  - 2079-9292

AB  - This study presents a method to estimate the solar energy potential based on 3D data taken from unmanned aerial devices. The solar energy potential on the roof of a building was estimated before the placement of solar panels using photogrammetric data analyzed in a geographic information system, and the predictions were compared with the data recorded after installation. The areas of the roofs were chosen using digital surface models and the hemispherical viewshed algorithm, considering how the solar radiation on the roof surface would be affected by the orientation of the surface with respect to the sun, the shade of trees, surrounding objects, topography, and the atmospheric conditions. The results show that the efficiency percentages of the panels and the data modeled by the proposed method from surface models are very similar to the theoretical efficiency of the panels. Radiation potential can be estimated from photogrammetric data and a 3D model in great detail and at low cost. This method allows the estimation of solar potential as well as the optimization of the location and orientation of solar panels.
KW  - unmanned aerial vehicle
KW  - solar irradiation
KW  - geographic information systems
KW  - photovoltaic systems
KW  - digital surface model
KW  - solar panel efficiency
DO  - 10.3390/electronics9122144
ER  -
TY  - EJOU
AU  - Chadwick, Andrew J.
AU  - Goodbody, Tristan R. H.
AU  - Coops, Nicholas C.
AU  - Hervieux, Anne
AU  - Bater, Christopher W.
AU  - Martens, Lee A.
AU  - White, Barry
AU  - Röeser, Dominik
TI  - Automatic Delineation and Height Measurement of Regenerating Conifer Crowns under Leaf-Off Conditions Using UAV Imagery
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 24
SN  - 2072-4292

AB  - The increasing use of unmanned aerial vehicles (UAV) and high spatial resolution imagery from associated sensors necessitates the continued advancement of efficient means of image processing to ensure these tools are utilized effectively. This is exemplified in the field of forest management, where the extraction of individual tree crown information stands to benefit operational budgets. We explored training a region-based convolutional neural network (Mask R-CNN) to automatically delineate individual tree crown (ITC) polygons in regenerating forests (14 years after harvest) using true colour red-green-blue (RGB) imagery with an average ground sampling distance (GSD) of 3 cm. We predicted ITC polygons to extract height information using canopy height models generated from digital aerial photogrammetric (DAP) point clouds. Our approach yielded an average precision of 0.98, an average recall of 0.85, and an average F1 score of 0.91 for the delineation of ITC. Remote height measurements were strongly correlated with field height measurements (r2 = 0.93, RMSE = 0.34 m). The mean difference between DAP-derived and field-collected height measurements was &minus;0.37 m and &minus;0.24 m for white spruce (Picea glauca) and lodgepole pine (Pinus contorta), respectively. Our results show that accurate ITC delineation in young, regenerating stands is possible with fine-spatial resolution RGB imagery and that predicted ITC can be used in combination with DAP to estimate tree height.
KW  - forest regeneration
KW  - individual tree crown
KW  - RGB
KW  - UAV
KW  - DAP
KW  - CNN
KW  - Mask R-CNN
DO  - 10.3390/rs12244104
ER  -
TY  - EJOU
AU  - Sun, Changqi
AU  - Zhang, Cong
AU  - Xiong, Naixue
TI  - Infrared and Visible Image Fusion Techniques Based on Deep Learning: A Review
T2  - Electronics

PY  - 2020
VL  - 9
IS  - 12
SN  - 2079-9292

AB  - Infrared and visible image fusion technologies make full use of different image features obtained by different sensors, retain complementary information of the source images during the fusion process, and use redundant information to improve the credibility of the fusion image. In recent years, many researchers have used deep learning methods (DL) to explore the field of image fusion and found that applying DL has improved the time-consuming efficiency of the model and the fusion effect. However, DL includes many branches, and there is currently no detailed investigation of deep learning methods in image fusion. In this work, this survey reports on the development of image fusion algorithms based on deep learning in recent years. Specifically, this paper first conducts a detailed investigation on the fusion method of infrared and visible images based on deep learning, compares the existing fusion algorithms qualitatively and quantitatively with the existing fusion quality indicators, and discusses various fusions. The main contribution, advantages, and disadvantages of the algorithm. Finally, the research status of infrared and visible image fusion is summarized, and future work has prospected. This research can help us realize many image fusion methods in recent years and lay the foundation for future research work.
KW  - image fusion
KW  - visible image
KW  - infrared image
KW  - evaluation metric
KW  - generative adversarial network
DO  - 10.3390/electronics9122162
ER  -
TY  - EJOU
AU  - Armenta-Medina, Dagoberto
AU  - Ramirez-delReal, Tania A.
AU  - Villanueva-Vásquez, Daniel
AU  - Mejia-Aguirre, Cristian
TI  - Trends on Advanced Information and Communication Technologies for Improving Agricultural Productivities: A Bibliometric Analysis
T2  - Agronomy

PY  - 2020
VL  - 10
IS  - 12
SN  - 2073-4395

AB  - In this work, an exhaustive revision is given of the literature associated with advanced information and communication technologies in agriculture within a window of 25 years using bibliometric tools enabled to detect of the main actors, structure, and dynamics in the scientific papers. The main findings are a trend of growth in the dynamics of publications associated with advanced information and communication technologies in agriculture productivity. Another assertion is that countries, like the USA, China, and Brazil, stand out in many publications due to allocating more resources to research, development, and agricultural productivity. In addition, the collaboration networks between countries are frequently in regions with closer cultural and idiomatic ties; additionally, terms&rsquo; occurrence are obtained with Louvain algorithm predominating four clusters: precision agriculture, smart agriculture, remote sensing, and climate smart agriculture. Finally, the thematic-map characterization with Callon&rsquo;s density and centrality is applied in three periods. The first period of thematic analysis shows a transition in detecting the variability of a nutrient, such as nitrogen, through the help of immature georeferenced techniques, towards greater remote sensing involvement. In the transition from the second to the third stage, the maturation of technologies, such as unmanned aerial vehicles, wireless sensor networks, and the machine learning area, is observed.
KW  - bibliometrics
KW  - precision agriculture
KW  - science mapping
KW  - smart farming
KW  - IoT
DO  - 10.3390/agronomy10121989
ER  -
TY  - EJOU
AU  - De Vita, Fabrizio
AU  - Bruneo, Dario
TI  - Leveraging Stack4Things for Federated Learning in Intelligent Cyber Physical Systems
T2  - Journal of Sensor and Actuator Networks

PY  - 2020
VL  - 9
IS  - 4
SN  - 2224-2708

AB  - During the last decade, the Internet of Things acted as catalyst for the big data phenomenon. As result, modern edge devices can access a huge amount of data that can be exploited to build useful services. In such a context, artificial intelligence has a key role to develop intelligent systems (e.g., intelligent cyber physical systems) that create a connecting bridge with the physical world. However, as time goes by, machine and deep learning applications are becoming more complex, requiring increasing amounts of data and training time, which makes the use of centralized approaches unsuitable. Federated learning is an emerging paradigm which enables the cooperation of edge devices to learn a shared model (while keeping private their training data), thereby abating the training time. Although federated learning is a promising technique, its implementation is difficult and brings a lot of challenges. In this paper, we present an extension of Stack4Things, a cloud platform developed in our department; leveraging its functionalities, we enabled the deployment of federated learning on edge devices without caring their heterogeneity. Experimental results show a comparison with a centralized approach and demonstrate the effectiveness of the proposed approach in terms of both training time and model accuracy.
KW  - federated learning
KW  - intelligent cyber physical systems
KW  - stack4things
DO  - 10.3390/jsan9040059
ER  -
TY  - EJOU
AU  - Samarin, Maxim
AU  - Zweifel, Lauren
AU  - Roth, Volker
AU  - Alewell, Christine
TI  - Identifying Soil Erosion Processes in Alpine Grasslands on Aerial Imagery with a U-Net Convolutional Neural Network
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 24
SN  - 2072-4292

AB  - Erosion in alpine grasslands is a major threat to ecosystem services of alpine soils. Natural causes for the occurrence of soil erosion are steep topography and prevailing climate conditions in combination with soil fragility. To increase our understanding of ongoing erosion processes and support sustainable land-use management, there is a need to acquire detailed information on spatial occurrence and temporal trends. Existing approaches to identify these trends are typically laborious, have lack of transferability to other regions, and are consequently only applicable to smaller regions. In order to overcome these limitations and create a sophisticated erosion monitoring tool capable of large-scale analysis, we developed a model based on U-Net, a fully convolutional neural network, to map different erosion processes on high-resolution aerial images (RGB, 0.25&ndash;0.5 m). U-Net was trained on a high-quality data set consisting of labeled erosion sites mapped with object-based image analysis (OBIA) for the Urseren Valley (Central Swiss Alps) for five aerial images (16 year period). We used the U-Net model to map the same study area and conduct quality assessments based on a held-out test region and a temporal transferability test on new images. Erosion classes are assigned according to their type (shallow landslide and sites with reduced vegetation affected by sheet erosion) or land-use impacts (livestock trails and larger management affected areas). We show that results obtained by OBIA and U-Net follow similar linear trends for the 16 year study period, exhibiting increases in total degraded area of 167% and 201%, respectively. Segmentations of eroded sites are generally in good agreement, but also display method-specific differences, which lead to an overall precision of 73%, a recall of 84%, and a F1-score of 78%. Our results show that U-Net is transferable to spatially (within our study area) and temporally unseen data (data from new years) and is therefore a method suitable to efficiently and successfully capture the temporal trends and spatial heterogeneity of degradation in alpine grasslands. Additionally, U-Net is a powerful and robust tool to map erosion sites in a predictive manner utilising large amounts of new aerial imagery.
KW  - deep learning
KW  - semantic segmentation
KW  - remote sensing
KW  - object-based image analysis
KW  - erosion mapping
KW  - landslides
KW  - livestock trails
KW  - sheet erosion
DO  - 10.3390/rs12244149
ER  -
TY  - EJOU
AU  - Hassanijalilian, Oveis
AU  - Igathinathane, C.
AU  - Bajwa, Sreekala
AU  - Nowatzki, John
TI  - Rating Iron Deficiency in Soybean Using Image Processing and Decision-Tree Based Models
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 24
SN  - 2072-4292

AB  - The most efficient way of soybean (Glycine max (L.) Merrill) iron deficiency chlorosis (IDC) management is to select a tolerant cultivar suitable for the specific growing condition. These cultivars are selected by field experts based on IDC visual ratings. However, this visual rating method is laborious, expensive, time-consuming, subjective, and impractical on larger scales. Therefore, a modern digital image-based method using tree-based machine learning classifier models for rating soybean IDC at plot-scale was developed. Data were collected from soybean IDC cultivar trial plots. Images were processed with MATLAB and corrected for light intensity by using a standard color board in the image. The three machine learning models used in this study were decision tree (DT), random forest (RF), and adaptive boosting (AdaBoost). Calculated indices from images, such as dark green color index (DGCI), canopy size, and pixel counts into DGCI ranges and IDC visual scoring were used as input and target variables to train these models. Metrics such as precision, recall, and f1-score were used to assess the performance of the classifier models. Among all three models, AdaBoost had the best performance (average f1-score = 0.75) followed by RF and DT the least. Therefore, a ready-to-use methodology of image processing with AdaBoost model for soybean IDC rating was recommended. The developed method can be easily adapted to smartphone applications or scaled-up using images from aerial platforms.
KW  - soybean
KW  - iron deficiency chlorosis
KW  - image processing
KW  - machine learning
KW  - AdaBoost
KW  - classification
DO  - 10.3390/rs12244143
ER  -
TY  - EJOU
AU  - Zang, Yufu
AU  - Li, Bijun
AU  - Xiao, Xiongwu
AU  - Zhu, Jianfeng
AU  - Meng, Fancong
TI  - An Efficient Probabilistic Registration Based on Shape Descriptor for Heritage Field Inspection
T2  - ISPRS International Journal of Geo-Information

PY  - 2020
VL  - 9
IS  - 12
SN  - 2220-9964

AB  - Heritage documentation is implemented by digitally recording historical artifacts for the conservation and protection of these cultural heritage objects. As efficient spatial data acquisition tools, laser scanners have been widely used to collect highly accurate three-dimensional (3D) point clouds without damaging the original structure and the environment. To ensure the integrity and quality of the collected data, field inspection (i.e., on-spot checking the data quality) should be carried out to determine the need for additional measurements (i.e., extra laser scanning for areas with quality issues such as data missing and quality degradation). To facilitate inspection of all collected point clouds, especially checking the quality issues in overlaps between adjacent scans, all scans should be registered together. Thus, a point cloud registration method that is able to register scans fast and robustly is required. To fulfill the aim, this study proposes an efficient probabilistic registration for free-form cultural heritage objects by integrating the proposed principal direction descriptor and curve constraints. We developed a novel shape descriptor based on a local frame of principal directions. Within the frame, its density and distance feature images were generated to describe the shape of the local surface. We then embedded the descriptor into a probabilistic framework to reject ambiguous matches. Spatial curves were integrated as constraints to delimit the solution space. Finally, a multi-view registration was used to refine the position and orientation of each scan for the field inspection. Comprehensive experiments show that the proposed method was able to perform well in terms of rotation error, translation error, robustness, and runtime and outperformed some commonly used approaches.
KW  - cultural heritage objects
KW  - shape descriptor
KW  - probabilistic registration
KW  - curve constraints
KW  - field inspection
DO  - 10.3390/ijgi9120759
ER  -
TY  - EJOU
AU  - Bolfe, Édson L.
AU  - Jorge, Lúcio A.
AU  - Sanches, Ieda D.
AU  - Luchiari Júnior, Ariovaldo
AU  - da Costa, Cinthia C.
AU  - Victoria, Daniel D.
AU  - Inamasu, Ricardo Y.
AU  - Grego, Célia R.
AU  - Ferreira, Victor R.
AU  - Ramirez, Andrea R.
TI  - Precision and Digital Agriculture: Adoption of Technologies and Perception of Brazilian Farmers
T2  - Agriculture

PY  - 2020
VL  - 10
IS  - 12
SN  - 2077-0472

AB  - The rapid population growth has driven the demand for more food, fiber, energy, and water, which is associated to an increase in the need to use natural resources in a more sustainable way. The use of precision agriculture machinery and equipment since the 1990s has provided important productive gains and maximized the use of agricultural inputs. The growing connectivity in the rural environment, in addition to its greater integration with data from sensor systems, remote sensors, equipment, and smartphones have paved the way for new concepts from the so-called Agriculture 4.0 or Digital Agriculture. This article presents the results of a survey carried out with 504 Brazilian farmers about the digital technologies in use, as well as current and future applications, perceived benefits, and challenges. The questionnaire was prepared, organized, and made available to the public through the online platform LimeSurvey and was available from 17 April to 2 June 2020. The primary data obtained for each question previously defined were consolidated and analyzed statistically. The results indicate that 84% of the interviewed farmers use at least one digital technology in their production system that differs according to technological complexity level. The main perceived benefit refers to the perception of increased productivity and the main challenges are the acquisition costs of machines, equipment, software, and connectivity. It is also noteworthy that 95% of farmers would like to learn more about new technologies to strengthen the agricultural development in their properties.
KW  - agriculture 4.0
KW  - smart farming
KW  - farmer’s attitudes
KW  - Brazil
DO  - 10.3390/agriculture10120653
ER  -
TY  - EJOU
AU  - Tilon, Sofia
AU  - Nex, Francesco
AU  - Kerle, Norman
AU  - Vosselman, George
TI  - Post-Disaster Building Damage Detection from Earth Observation Imagery Using Unsupervised and Transferable Anomaly Detecting Generative Adversarial Networks
T2  - Remote Sensing

PY  - 2020
VL  - 12
IS  - 24
SN  - 2072-4292

AB  - We present an unsupervised deep learning approach for post-disaster building damage detection that can transfer to different typologies of damage or geographical locations. Previous advances in this direction were limited by insufficient qualitative training data. We propose to use a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN) because it only requires pre-event imagery of buildings in their undamaged state. This approach aids the post-disaster response phase because the model can be developed in the pre-event phase and rapidly deployed in the post-event phase. We used the xBD dataset, containing pre- and post- event satellite imagery of several disaster-types, and a custom made Unmanned Aerial Vehicle (UAV) dataset, containing post-earthquake imagery. Results showed that models trained on UAV-imagery were capable of detecting earthquake-induced damage. The best performing model for European locations obtained a recall, precision and F1-score of 0.59, 0.97 and 0.74, respectively. Models trained on satellite imagery were capable of detecting damage on the condition that the training dataset was void of vegetation and shadows. In this manner, the best performing model for (wild)fire events yielded a recall, precision and F1-score of 0.78, 0.99 and 0.87, respectively. Compared to other supervised and/or multi-epoch approaches, our results are encouraging. Moreover, in addition to image classifications, we show how contextual information can be used to create detailed damage maps without the need of a dedicated multi-task deep learning framework. Finally, we formulate practical guidelines to apply this single-epoch and unsupervised method to real-world applications.
KW  - deep learning
KW  - Generative Adversarial Networks
KW  - post-disaster
KW  - building damage assessment
KW  - anomaly detection
KW  - Unmanned Aerial Vehicles (UAV)
KW  - satellite
KW  - xBD
DO  - 10.3390/rs12244193
ER  -
TY  - EJOU
AU  - Devaraja, Rahul R.
AU  - Maskeliūnas, Rytis
AU  - Damaševičius, Robertas
TI  - Design and Evaluation of Anthropomorphic Robotic Hand for Object Grasping and Shape Recognition
T2  - Computers

PY  - 2021
VL  - 10
IS  - 1
SN  - 2073-431X

AB  - We developed an anthropomorphic multi-finger artificial hand for a fine-scale object grasping task, sensing the grasped object&rsquo;s shape. The robotic hand was created using the 3D printer and has the servo bed for stand-alone finger movement. The data containing the robotic fingers&rsquo; angular position are acquired using the Leap Motion device, and a hybrid Support Vector Machine (SVM) classifier is used for object shape identification. We trained the designed robotic hand on a few monotonous convex-shaped items similar to everyday objects (ball, cylinder, and rectangular box) using supervised learning techniques. We achieve the mean accuracy of object shape recognition of 94.4%.
KW  - robot manipulator
KW  - shape recognition
KW  - supervised learning
KW  - object grasping
KW  - 3D printing
DO  - 10.3390/computers10010001
ER  -
TY  - EJOU
AU  - Zhao, Wei
AU  - Yamada, William
AU  - Li, Tianxin
AU  - Digman, Matthew
AU  - Runge, Troy
TI  - Augmenting Crop Detection for Precision Agriculture with Deep Visual Transfer Learning—A Case Study of Bale Detection
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - In recent years, precision agriculture has been researched to increase crop production with less inputs, as a promising means to meet the growing demand of agriculture products. Computer vision-based crop detection with unmanned aerial vehicle (UAV)-acquired images is a critical tool for precision agriculture. However, object detection using deep learning algorithms rely on a significant amount of manually prelabeled training datasets as ground truths. Field object detection, such as bales, is especially difficult because of (1) long-period image acquisitions under different illumination conditions and seasons; (2) limited existing prelabeled data; and (3) few pretrained models and research as references. This work increases the bale detection accuracy based on limited data collection and labeling, by building an innovative algorithms pipeline. First, an object detection model is trained using 243 images captured with good illimitation conditions in fall from the crop lands. In addition, domain adaptation (DA), a kind of transfer learning, is applied for synthesizing the training data under diverse environmental conditions with automatic labels. Finally, the object detection model is optimized with the synthesized datasets. The case study shows the proposed method improves the bale detecting performance, including the recall, mean average precision (mAP), and F measure (F1 score), from averages of 0.59, 0.7, and 0.7 (the object detection) to averages of 0.93, 0.94, and 0.89 (the object detection + DA), respectively. This approach could be easily scaled to many other crop field objects and will significantly contribute to precision agriculture.
KW  - domain adaptation
KW  - computer vision
KW  - illimitation change
KW  - adverse weather conditions
KW  - generative adversarial network (GAN)
DO  - 10.3390/rs13010023
ER  -
TY  - EJOU
AU  - Maung, Win S.
AU  - Sasaki, Jun
TI  - Assessing the Natural Recovery of Mangroves after Human Disturbance Using Neural Network Classification and Sentinel-2 Imagery in Wunbaik Mangrove Forest, Myanmar
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - In this study, we examined the natural recovery of mangroves in abandoned shrimp ponds located in the Wunbaik Mangrove Forest (WMF) in Myanmar using artificial neural network (ANN) classification and a change detection approach with Sentinel-2 satellite images. In 2020, we conducted various experiments related to mangrove classification by tuning input features and hyper-parameters. The selected ANN model was used with a transfer learning approach to predict the mangrove distribution in 2015. Changes were detected using classification results from 2015 and 2020. Naturally recovering mangroves were identified by extracting the change detection results of three abandoned shrimp ponds selected during field investigation. The proposed method yielded an overall accuracy of 95.98%, a kappa coefficient of 0.92, mangrove and non-mangrove precisions of 0.95 and 0.98, respectively, recalls of 0.96, and F1 scores of 0.96 for the 2020 classification. For the 2015 prediction, transfer learning improved model performance, resulting in an overall accuracy of 97.20%, a kappa coefficient of 0.94, mangrove and non-mangrove precisions of 0.98 and 0.96, respectively, recalls of 0.98 and 0.97, and F1 scores of 0.96. The change detection results showed that mangrove forests in the WMF slightly decreased between 2015 and 2020. Naturally recovering mangroves were detected at approximately 50% of each abandoned site within a short abandonment period. This study demonstrates that the ANN method using Sentinel-2 imagery and topographic and canopy height data can produce reliable results for mangrove classification. The natural recovery of mangroves presents a valuable opportunity for mangrove rehabilitation at human-disturbed sites in the WMF.
KW  - mangrove
KW  - natural recovery
KW  - artificial neural network
KW  - Sentinel-2
KW  - transfer learning
KW  - change detection
DO  - 10.3390/rs13010052
ER  -
TY  - EJOU
AU  - Biffi, Leonardo J.
AU  - Mitishita, Edson
AU  - Liesenberg, Veraldo
AU  - Santos, Anderson A.
AU  - Gonçalves, Diogo N.
AU  - Estrabis, Nayara V.
AU  - Silva, Jonathan D.
AU  - Osco, Lucas P.
AU  - Ramos, Ana P.
AU  - Centeno, Jorge A.
AU  - Schimalski, Marcos B.
AU  - Rufato, Leo
AU  - Neto, Sílvio L.
AU  - Marcato Junior, José
AU  - Gonçalves, Wesley N.
TI  - ATSS Deep Learning-Based Approach to Detect Apple Fruits
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. Specifically, in fruit detection problems, several recent works were developed using Deep Learning (DL) methods applied in images acquired in different acquisition levels. However, the increasing use of anti-hail plastic net cover in commercial orchards highlights the importance of terrestrial remote sensing systems. Apples are one of the most highly-challenging fruits to be detected in images, mainly because of the target occlusion problem occurrence. Additionally, the introduction of high-density apple tree orchards makes the identification of single fruits a real challenge. To support farmers to detect apple fruits efficiently, this paper presents an approach based on the Adaptive Training Sample Selection (ATSS) deep learning method applied to close-range and low-cost terrestrial RGB images. The correct identification supports apple production forecasting and gives local producers a better idea of forthcoming management practices. The main advantage of the ATSS method is that only the center point of the objects is labeled, which is much more practicable and realistic than bounding-box annotations in heavily dense fruit orchards. Additionally, we evaluated other object detection methods such as RetinaNet, Libra Regions with Convolutional Neural Network (R-CNN), Cascade R-CNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), and High-Resolution Network (HRNet). The study area is a highly-dense apple orchard consisting of Fuji Suprema apple fruits (Malus domestica Borkh) located in a smallholder farm in the state of Santa Catarina (southern Brazil). A total of 398 terrestrial images were taken nearly perpendicularly in front of the trees by a professional camera, assuring both a good vertical coverage of the apple trees in terms of heights and overlapping between picture frames. After, the high-resolution RGB images were divided into several patches for helping the detection of small and/or occluded apples. A total of 3119, 840, and 2010 patches were used for training, validation, and testing, respectively. Moreover, the proposed method&rsquo;s generalization capability was assessed by applying simulated image corruptions to the test set images with different severity levels, including noise, blurs, weather, and digital processing. Experiments were also conducted by varying the bounding box size (80, 100, 120, 140, 160, and 180 pixels) in the image original for the proposed approach. Our results showed that the ATSS-based method slightly outperformed all other deep learning methods, between 2.4% and 0.3%. Also, we verified that the best result was obtained with a bounding box size of 160 &times; 160 pixels. The proposed method was robust regarding most of the corruption, except for snow, frost, and fog weather conditions. Finally, a benchmark of the reported dataset is also generated and publicly available.
KW  - convolutional neural network
KW  - object detection
KW  - precision agriculture
DO  - 10.3390/rs13010054
ER  -
TY  - EJOU
AU  - Pineda, Mónica
AU  - Barón, Matilde
AU  - Pérez-Bueno, María-Luisa
TI  - Thermal Imaging for Plant Stress Detection and Phenotyping
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - In the last few years, large efforts have been made to develop new methods to optimize stress detection in crop fields. Thus, plant phenotyping based on imaging techniques has become an essential tool in agriculture. In particular, leaf temperature is a valuable indicator of the physiological status of plants, responding to both biotic and abiotic stressors. Often combined with other imaging sensors and data-mining techniques, thermography is crucial in the implementation of a more automatized, precise and sustainable agriculture. However, thermal data need some corrections related to the environmental and measuring conditions in order to achieve a correct interpretation of the data. This review focuses on the state of the art of thermography applied to the detection of biotic stress. The work will also revise the most important abiotic stress factors affecting the measurements as well as practical issues that need to be considered in order to implement this technique, particularly at the field scale.
KW  - Remote sensing
KW  - proximal sensing
KW  - thermography
KW  - plant phenotyping
DO  - 10.3390/rs13010068
ER  -
TY  - EJOU
AU  - Wittstruck, Lucas
AU  - Kühling, Insa
AU  - Trautz, Dieter
AU  - Kohlbrecher, Maik
AU  - Jarmer, Thomas
TI  - UAV-Based RGB Imagery for Hokkaido Pumpkin (Cucurbita max.) Detection and Yield Estimation
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 1
SN  - 1424-8220

AB  - Pumpkins are economically and nutritionally valuable vegetables with increasing popularity and acreage across Europe. Successful commercialization, however, require detailed pre-harvest information about number and weight of the fruits. To get a non-destructive and cost-effective yield estimation, we developed an image processing methodology for high-resolution RGB data from Unmanned aerial vehicle (UAV) and applied this on a Hokkaido pumpkin farmer&rsquo;s field in North-western Germany. The methodology was implemented in the programming language Python and comprised several steps, including image pre-processing, pixel-based image classification, classification post-processing for single fruit detection, and fruit size and weight quantification. To derive the weight from two-dimensional imagery, we calculated elliptical spheroids from lengths of diameters and heights. The performance of this processes was evaluated by comparison with manually harvested ground-truth samples and cross-checked for misclassification from randomly selected test objects. Errors in classification and fruit geometry could be successfully reduced based on the described processing steps. Additionally, different lighting conditions, as well as shadows, in the image data could be compensated by the proposed methodology. The results revealed a satisfactory detection of 95% (error rate of 5%) from the field sample, as well as a reliable volume and weight estimation with Pearson&rsquo;s correlation coefficients of 0.83 and 0.84, respectively, from the described ellipsoid approach. The yield was estimated with 1.51 kg m&minus;2 corresponding to an average individual fruit weight of 1100 g and an average number of 1.37 pumpkins per m2. Moreover, spatial distribution of aggregated fruit densities and weights were calculated to assess in-field optimization potential for agronomic management as demonstrated between a shaded edge compared to the rest of the field. The proposed approach provides the Hokkaido producer useful information for more targeted pre-harvest marketing strategies, since most food retailers request homogeneous lots within prescribed size or weight classes.
KW  - remote sensing
KW  - drones
KW  - random forest
KW  - low-cost sensor
KW  - winter squash
KW  - vegetables
KW  - fruit size
KW  - fruit weight
KW  - Europe
DO  - 10.3390/s21010118
ER  -
TY  - EJOU
AU  - Yamaguchi, Tomoaki
AU  - Tanaka, Yukie
AU  - Imachi, Yuto
AU  - Yamashita, Megumi
AU  - Katsura, Keisuke
TI  - Feasibility of Combining Deep Learning and RGB Images Obtained by Unmanned Aerial Vehicle for Leaf Area Index Estimation in Rice
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - Leaf area index (LAI) is a vital parameter for predicting rice yield. Unmanned aerial vehicle (UAV) surveillance with an RGB camera has been shown to have potential as a low-cost and efficient tool for monitoring crop growth. Simultaneously, deep learning (DL) algorithms have attracted attention as a promising tool for the task of image recognition. The principal aim of this research was to evaluate the feasibility of combining DL and RGB images obtained by a UAV for rice LAI estimation. In the present study, an LAI estimation model developed by DL with RGB images was compared to three other practical methods: a plant canopy analyzer (PCA); regression models based on color indices (CIs) obtained from an RGB camera; and vegetation indices (VIs) obtained from a multispectral camera. The results showed that the estimation accuracy of the model developed by DL with RGB images (R2 = 0.963 and RMSE = 0.334) was higher than those of the PCA (R2 = 0.934 and RMSE = 0.555) and the regression models based on CIs (R2 = 0.802-0.947 and RMSE = 0.401&ndash;1.13), and comparable to that of the regression models based on VIs (R2 = 0.917&ndash;0.976 and RMSE = 0.332&ndash;0.644). Therefore, our results demonstrated that the estimation model using DL with an RGB camera on a UAV could be an alternative to the methods using PCA and a multispectral camera for rice LAI estimation.
KW  - unmanned aerial vehicle
KW  - drone
KW  - deep learning
KW  - leaf area index
KW  - growth estimation
KW  - rice
KW  - RGB camera
DO  - 10.3390/rs13010084
ER  -
TY  - EJOU
AU  - Rahman, Mohammad F.
AU  - Fan, Shurui
AU  - Zhang, Yan
AU  - Chen, Lei
TI  - A Comparative Study on Application of Unmanned Aerial Vehicle Systems in Agriculture
T2  - Agriculture

PY  - 2021
VL  - 11
IS  - 1
SN  - 2077-0472

AB  - Presently in agriculture, there is much ample scope for drone and UAS (Unmanned Aircraft System) development. Because of their low cost and small size, these devices have the ability to help many developing countries with economic prosperity. The entire aggregation of financial investments in the agricultural area has increased appreciably in recent years. Sooth to say, agriculture remains a massive part of the world&rsquo;s commercial growth, and due to some complications, the agriculture fields withstand massive losses. Pets and destructive insects seem to be the primary reasons for certain degenerative diseases. It minimizes the potential productivity of the crops. For increasing the quality of the plants, fertilizers and pesticides are appropriately applied. Using UAVs (Unmanned Aerial Vehicles) for spraying pesticides and fertilizing materials is an exuberant contraption. It adequately reduces the rate of health dilemma and the number of workers, which is quite an impressive landmark. Willing producers are also adopting UAVs in agriculture to soil and field analysis, seed sowing, lessen the time and costs correlated with crop scouting, and field mapping. It is rapid, and it can sensibly diminish a farmer&rsquo;s workload, which is significantly a part of the agricultural revolution. This article aims to proportionally represent the concept of agricultural purposed UAV clear to the neophytes. First, this paper outlines the harmonic framework of the agricultural UAV, and then it abundantly illustrates the methods and materials. Finally, the article portrays the outcome.
KW  - UAV
KW  - unmanned aerial vehicle
KW  - agricultural UAV
KW  - NDVI (Normalized Difference Vegetation Index)
KW  - spraying system
KW  - livestock
KW  - agricultural monitoring
DO  - 10.3390/agriculture11010022
ER  -
TY  - EJOU
AU  - Guo, Anting
AU  - Huang, Wenjiang
AU  - Dong, Yingying
AU  - Ye, Huichun
AU  - Ma, Huiqin
AU  - Liu, Bo
AU  - Wu, Wenbin
AU  - Ren, Yu
AU  - Ruan, Chao
AU  - Geng, Yun
TI  - Wheat Yellow Rust Detection Using UAV-Based Hyperspectral Technology
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - Yellow rust is a worldwide disease that poses a serious threat to the safety of wheat production. Numerous studies on near-surface hyperspectral remote sensing at the leaf scale have achieved good results for disease monitoring. The next step is to monitor the disease at the field scale, which is of great significance for disease control. In our study, an unmanned aerial vehicle (UAV) equipped with a hyperspectral sensor was used to obtain hyperspectral images at the field scale. Vegetation indices (VIs) and texture features (TFs) extracted from the UAV-based hyperspectral images and their combination were used to establish partial least-squares regression (PLSR)-based disease monitoring models in different infection periods. In addition, we resampled the original images with 1.2 cm spatial resolution to images with different spatial resolutions (3 cm, 5 cm, 7 cm, 10 cm, 15 cm, and 20 cm) to evaluate the effect of spatial resolution on disease monitoring accuracy. The findings showed that the VI-based model had the highest monitoring accuracy (R2 = 0.75) in the mid-infection period. The TF-based model could be used to monitor yellow rust at the field scale and obtained the highest R2 in the mid- and late-infection periods (0.65 and 0.82, respectively). The VI-TF-based models had the highest accuracy in each infection period and outperformed the VI-based or TF-based models. The spatial resolution had a negligible influence on the VI-based monitoring accuracy, but significantly influenced the TF-based monitoring accuracy. Furthermore, the optimal spatial resolution for monitoring yellow rust using the VI-TF-based model in each infection period was 10 cm. The findings provide a reference for accurate disease monitoring using UAV hyperspectral images.
KW  - UAV hyperspectral
KW  - wheat yellow rust
KW  - disease monitoring
KW  - vegetation index
KW  - texture
KW  - spatial resolution
DO  - 10.3390/rs13010123
ER  -
TY  - EJOU
AU  - Papp, Levente
AU  - van Leeuwen, Boudewijn
AU  - Szilassi, Péter
AU  - Tobak, Zalán
AU  - Szatmári, József
AU  - Árvai, Mátyás
AU  - Mészáros, János
AU  - Pásztor, László
TI  - Monitoring Invasive Plant Species Using Hyperspectral Remote Sensing Data
T2  - Land

PY  - 2021
VL  - 10
IS  - 1
SN  - 2073-445X

AB  - The species richness and biodiversity of vegetation in Hungary are increasingly threatened by invasive plant species brought in from other continents and foreign ecosystems. These invasive plant species have spread aggressively in the natural and semi-natural habitats of Europe. Common milkweed (Asclepias syriaca) is one of the species that pose the greatest ecological menace. Therefore, the primary purpose of the present study is to map and monitor the spread of common milkweed, the most common invasive plant species in Europe. Furthermore, the possibilities to detect and validate this special invasive plant by analyzing hyperspectral remote sensing data were investigated. In combination with field reference data, high-resolution hyperspectral aerial images acquired by an unmanned aerial vehicle (UAV) platform in 138 spectral bands in areas infected by common milkweed were examined. Then, support vector machine (SVM) and artificial neural network (ANN) classification algorithms were applied to the highly accurate field reference data. As a result, common milkweed individuals were distinguished in hyperspectral images, achieving an overall accuracy of 92.95% in the case of supervised SVM classification. Using the ANN model, an overall accuracy of 99.61% was achieved. To evaluate the proposed approach, two experimental tests were conducted, and in both cases, we managed to distinguish the individual specimens within the large variety of spreading invasive species in a study area of 2 ha, based on centimeter spatial resolution hyperspectral UAV imagery.
KW  - invasive species
KW  - common milkweed
KW  - hyperspectral imaging
KW  - UAV
KW  - artificial neural networks
KW  - SVM classification
DO  - 10.3390/land10010029
ER  -
TY  - EJOU
AU  - Roldán-Gómez, Juan J.
AU  - González-Gironda, Eduardo
AU  - Barrientos, Antonio
TI  - A Survey on Robotic Technologies for Forest Firefighting: Applying Drone Swarms to Improve Firefighters’ Efficiency and Safety
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 1
SN  - 2076-3417

AB  - Forest firefighting missions encompass multiple tasks related to prevention, surveillance, and extinguishing. This work presents a complete survey of firefighters on the current problems in their work and the potential technological solutions. Additionally, it reviews the efforts performed by the academy and industry to apply different types of robots in the context of firefighting missions. Finally, all this information is used to propose a concept of operation for the comprehensive application of drone swarms in firefighting. The proposed system is a fleet of quadcopters that individually are only able to visit waypoints and use payloads, but collectively can perform tasks of surveillance, mapping, monitoring, etc. Three operator roles are defined, each one with different access to information and functions in the mission: mission commander, team leaders, and team members. These operators take advantage of virtual and augmented reality interfaces to intuitively get the information of the scenario and, in the case of the mission commander, control the drone swarm.
KW  - robotics
KW  - multi-robot systems
KW  - swarms
KW  - drones
KW  - firefighting
DO  - 10.3390/app11010363
ER  -
TY  - EJOU
AU  - Flores, Donovan
AU  - González-Hernández, Iván
AU  - Lozano, Rogelio
AU  - Vazquez-Nicolas, Jesus M.
AU  - Hernandez Toral, Jorge L.
TI  - Automated Agave Detection and Counting Using a Convolutional Neural Network and Unmanned Aerial Systems
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - We present an automatic agave detection method for counting plants based on aerial data from a UAV (Unmanned Aerial Vehicle). Our objective is to autonomously count the number of agave plants in an area to aid management of the yield. An orthomosaic is obtained from agave plantations, which is then used to create a database. This database is in turn used to train a Convolutional Neural Network (CNN). The proposed method is based on computer image processing, and the CNN increases the detection performance of the approach. The main contribution of the present paper is to propose a method for agave plant detection with a high level of precision. In order to test the proposed method in a real agave plantation, we develop a UAV platform, which is equipped with several sensors to reach accurate counting. Therefore, our prototype can safely track a desired path to detect and count agave plants. For comparison purposes, we perform the same application using a simpler algorithm. The result shows that our proposed algorithm has better performance reaching an F1 score of 0.96 as opposed to 0.57 for the Haar algorithm. The obtained experimental results suggest that the proposed algorithm is robust and has considerable potential to help farmers manage agave agroecosystems.
KW  - precision agriculture
KW  - plant detection
KW  - monitoring
KW  - deep learning
KW  - counting
DO  - 10.3390/drones5010004
ER  -
TY  - EJOU
AU  - Yeh, Chia-Cheng
AU  - Chang, Yang-Lang
AU  - Alkhaleefah, Mohammad
AU  - Hsu, Pai-Hui
AU  - Eng, Weiyong
AU  - Koo, Voon-Chet
AU  - Huang, Bormin
AU  - Chang, Lena
TI  - YOLOv3-Based Matching Approach for Roof Region Detection from Drone Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - Due to the large data volume, the UAV image stitching and matching suffers from high computational cost. The traditional feature extraction algorithms&mdash;such as Scale-Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), and Oriented FAST Rotated BRIEF (ORB)&mdash;require heavy computation to extract and describe features in high-resolution UAV images. To overcome this issue, You Only Look Once version 3 (YOLOv3) combined with the traditional feature point matching algorithms is utilized to extract descriptive features from the drone dataset of residential areas for roof detection. Unlike the traditional feature extraction algorithms, YOLOv3 performs the feature extraction solely on the proposed candidate regions instead of the entire image, thus the complexity of the image matching is reduced significantly. Then, all the extracted features are fed into Structural Similarity Index Measure (SSIM) to identify the corresponding roof region pair between consecutive image sequences. In addition, the candidate corresponding roof pair by our architecture serves as the coarse matching region pair and limits the search range of features matching to only the detected roof region. This further improves the feature matching consistency and reduces the chances of wrong feature matching. Analytical results show that the proposed method is 13&times; faster than the traditional image matching methods with comparable performance.
KW  - image matching
KW  - deep learning
KW  - YOLOv3
KW  - roof region detection
KW  - drone images
KW  - high-performance computing
DO  - 10.3390/rs13010127
ER  -
TY  - EJOU
AU  - Zhao, Xuehua
AU  - Lv, Hanfang
AU  - Wei, Yizhao
AU  - Lv, Shujin
AU  - Zhu, Xueping
TI  - Streamflow Forecasting via Two Types of Predictive Structure-Based Gated Recurrent Unit Models
T2  - Water

PY  - 2021
VL  - 13
IS  - 1
SN  - 2073-4441

AB  - Data-intelligent methods designed for forecasting the streamflow of the Fenhe River are crucial for enhancing water resource management. Herein, the gated recurrent unit (GRU) is coupled with the optimization algorithm improved grey wolf optimizer (IGWO) to design a hybrid model (IGWO-GRU) to carry out streamflow forecasting. Two types of predictive structure-based models (sequential IGWO-GRU and monthly IGWO-GRU) are compared with other models, such as the single least-squares support vector machine (LSSVM) and single extreme learning machine (ELM) models. These models incorporate the historical streamflow series as inputs of the model to forecast the future streamflow with data from January 1956 to December 2016 at the Shangjingyou station and from January 1958 to December 2016 at the Fenhe reservoir station. The IGWO-GRU model exhibited a strong ability for mapping in streamflow series when the parameters were carefully tuned. The monthly predictive structure can effectively extract the instinctive hydrological information that is more easily learned by the predictive model than the traditional sequential predictive structure. The monthly IGWO-GRU model was found to be a better forecasting tool, with an average qualification rate of 91.66% in two stations. It also showed good performance in absolute error and peak flow forecasting.
KW  - gated recurrent unit
KW  - improved grey wolf optimizer
KW  - monthly streamflow forecasting
KW  - data-driven modeling
DO  - 10.3390/w13010091
ER  -
TY  - EJOU
AU  - De Swaef, Tom
AU  - Maes, Wouter H.
AU  - Aper, Jonas
AU  - Baert, Joost
AU  - Cougnon, Mathias
AU  - Reheul, Dirk
AU  - Steppe, Kathy
AU  - Roldán-Ruiz, Isabel
AU  - Lootens, Peter
TI  - Applying RGB- and Thermal-Based Vegetation Indices from UAVs for High-Throughput Field Phenotyping of Drought Tolerance in Forage Grasses
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 1
SN  - 2072-4292

AB  - The persistence and productivity of forage grasses, important sources for feed production, are threatened by climate change-induced drought. Breeding programs are in search of new drought tolerant forage grass varieties, but those programs still rely on time-consuming and less consistent visual scoring by breeders. In this study, we evaluate whether Unmanned Aerial Vehicle (UAV) based remote sensing can complement or replace this visual breeder score. A field experiment was set up to test the drought tolerance of genotypes from three common forage types of two different species: Festuca arundinacea, diploid Lolium perenne and tetraploid Lolium perenne. Drought stress was imposed by using mobile rainout shelters. UAV flights with RGB and thermal sensors were conducted at five time points during the experiment. Visual-based indices from different colour spaces were selected that were closely correlated to the breeder score. Furthermore, several indices, in particular H and NDLab, from the HSV (Hue Saturation Value) and CIELab (Commission Internationale de l&rsquo;&eacute;clairage) colour space, respectively, displayed a broad-sense heritability that was as high or higher than the visual breeder score, making these indices highly suited for high-throughput field phenotyping applications that can complement or even replace the breeder score. The thermal-based Crop Water Stress Index CWSI provided complementary information to visual-based indices, enabling the analysis of differences in ecophysiological mechanisms for coping with reduced water availability between species and ploidy levels. All species/types displayed variation in drought stress tolerance, which confirms that there is sufficient variation for selection within these groups of grasses. Our results confirmed the better drought tolerance potential of Festuca arundinacea, but also showed which Lolium perenne genotypes are more tolerant.
KW  - UAV
KW  - RGB camera
KW  - thermal camera
KW  - drought tolerance
KW  - forage grass
KW  - HSV
KW  - CIELab
KW  - broad-sense heritability
KW  - phenotyping gap
KW  - high throughput field phenotyping
DO  - 10.3390/rs13010147
ER  -
TY  - EJOU
AU  - Qin, Jun
AU  - Wang, Biao
AU  - Wu, Yanlan
AU  - Lu, Qi
AU  - Zhu, Haochen
TI  - Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Pine nematode is a highly contagious disease that causes great damage to the world&rsquo;s pine forest resources. Timely and accurate identification of pine nematode disease can help to control it. At present, there are few research on pine nematode disease identification, and it is difficult to accurately identify and locate nematode disease in a single pine by existing methods. This paper proposes a new network, SCANet (spatial-context-attention network), to identify pine nematode disease based on unmanned aerial vehicle (UAV) multi-spectral remote sensing images. In this method, a spatial information retention module is designed to reduce the loss of spatial information; it preserves the shallow features of pine nematode disease and expands the receptive field to enhance the extraction of deep features through a context information module. SCANet reached an overall accuracy of 79% and a precision and recall of around 0.86, and 0.91, respectively. In addition, 55 disease points among 59 known disease points were identified, which is better than other methods (DeepLab V3+, DenseNet, and HRNet). This paper presents a fast, precise, and practical method for identifying nematode disease and provides reliable technical support for the surveillance and control of pine wood nematode disease.
KW  - UAV remote sensing
KW  - pine wood nematode disease
KW  - deep learning
KW  - intelligent identifying
DO  - 10.3390/rs13020162
ER  -
TY  - EJOU
AU  - Koteluk, Oliwia
AU  - Wartecki, Adrian
AU  - Mazurek, Sylwia
AU  - Kołodziejczak, Iga
AU  - Mackiewicz, Andrzej
TI  - How Do Machines Learn? Artificial Intelligence as a New Era in Medicine
T2  - Journal of Personalized Medicine

PY  - 2021
VL  - 11
IS  - 1
SN  - 2075-4426

AB  - With an increased number of medical data generated every day, there is a strong need for reliable, automated evaluation tools. With high hopes and expectations, machine learning has the potential to revolutionize many fields of medicine, helping to make faster and more correct decisions and improving current standards of treatment. Today, machines can analyze, learn, communicate, and understand processed data and are used in health care increasingly. This review explains different models and the general process of machine learning and training the algorithms. Furthermore, it summarizes the most useful machine learning applications and tools in different branches of medicine and health care (radiology, pathology, pharmacology, infectious diseases, personalized decision making, and many others). The review also addresses the futuristic prospects and threats of applying artificial intelligence as an advanced, automated medicine tool.
KW  - machine learning
KW  - artificial intelligence
KW  - bioinformatics
KW  - medicine
KW  - algorithm
KW  - decision making
KW  - personalized medicine
KW  - data processing
KW  - data mining
KW  - personalized treatment
DO  - 10.3390/jpm11010032
ER  -
TY  - EJOU
AU  - Zhao, Rongkun
AU  - Li, Yuechen
AU  - Ma, Mingguo
TI  - Mapping Paddy Rice with Satellite Remote Sensing: A Review
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 2
SN  - 2071-1050

AB  - Paddy rice is a staple food of three billion people in the world. Timely and accurate estimation of the paddy rice planting area and paddy rice yield can provide valuable information for the government, planners and decision makers to formulate policies. This article reviews the existing paddy rice mapping methods presented in the literature since 2010, classifies these methods, and analyzes and summarizes the basic principles, advantages and disadvantages of these methods. According to the data sources used, the methods are divided into three categories: (I) Optical mapping methods based on remote sensing; (II) Mapping methods based on microwave remote sensing; and (III) Mapping methods based on the integration of optical and microwave remote sensing. We found that the optical remote sensing data sources are mainly MODIS, Landsat, and Sentinel-2, and the emergence of Sentinel-1 data has promoted research on radar mapping methods for paddy rice. Multisource data integration further enhances the accuracy of paddy rice mapping. The best methods are phenology algorithms, paddy rice mapping combined with machine learning, and multisource data integration. Innovative methods include the time series similarity method, threshold method combined with mathematical models, and object-oriented image classification. With the development of computer technology and the establishment of cloud computing platforms, opportunities are provided for obtaining large-scale high-resolution rice maps. Multisource data integration, paddy rice mapping under different planting systems and the connection with global changes are the focus of future development priorities.
KW  - optical remote sensing
KW  - microwave remote sensing
KW  - phenology-based method
DO  - 10.3390/su13020503
ER  -
TY  - EJOU
AU  - Kulsinskas, Andrius
AU  - Durdevic, Petar
AU  - Ortiz-Arroyo, Daniel
TI  - Internal Wind Turbine Blade Inspections Using UAVs: Analysis and Design Issues
T2  - Energies

PY  - 2021
VL  - 14
IS  - 2
SN  - 1996-1073

AB  - Interior and exterior wind turbine blade inspections are necessary to extend the lifetime of wind turbine generators. The use of unmanned vehicles is an alternative to exterior wind turbine blade inspections performed by technicians that require the use of cranes and ropes. Interior wind turbine blade inspections are even more challenging due to the confined spaces, lack of illumination, and the presence of potentially harmful internal structural components. Additionally, the cost of manned interior wind turbine blade inspections is a major limiting factor. This paper analyses all aspects of the viability of using manually controlled or autonomous aerial vehicles for interior wind turbine blade inspections. We discuss why the size, weight, and flight time of a vehicle, in addition to the structure of the wind turbine blade, are the main limiting factors in performing internal blade inspections. We also describe the design issues that must be considered to provide autonomy to unmanned vehicles and the control system, the sensors that can be used, and introduce some of the algorithms for localization, obstacle avoidance and path planning that are best suited for the task. Lastly, we briefly describe which non-destructive test instrumentation can be used for the purpose.
KW  - UAV
KW  - wind turbine inspection
KW  - autonomy
KW  - wind turbine blade
KW  - indoors UAV
DO  - 10.3390/en14020294
ER  -
TY  - EJOU
AU  - Bigazzi, Luca
AU  - Gherardini, Stefano
AU  - Innocenti, Giacomo
AU  - Basso, Michele
TI  - Development of Non Expensive Technologies for Precise Maneuvering of Completely Autonomous Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - In this paper, solutions for precise maneuvering of an autonomous small (e.g., 350-class) Unmanned Aerial Vehicles (UAVs) are designed and implemented from smart modifications of non expensive mass market technologies. The considered class of vehicles suffers from light load, and, therefore, only a limited amount of sensors and computing devices can be installed on-board. Then, to make the prototype capable of moving autonomously along a fixed trajectory, a &ldquo;cyber-pilot&rdquo;, able on demand to replace the human operator, has been implemented on an embedded control board. This cyber-pilot overrides the commands thanks to a custom hardware signal mixer. The drone is able to localize itself in the environment without ground assistance by using a camera possibly mounted on a 3 Degrees Of Freedom (DOF) gimbal suspension. A computer vision system elaborates the video stream pointing out land markers with known absolute position and orientation. This information is fused with accelerations from a 6-DOF Inertial Measurement Unit (IMU) to generate a &ldquo;virtual sensor&rdquo; which provides refined estimates of the pose, the absolute position, the speed and the angular velocities of the drone. Due to the importance of this sensor, several fusion strategies have been investigated. The resulting data are, finally, fed to a control algorithm featuring a number of uncoupled digital PID controllers which work to bring to zero the displacement from the desired trajectory.
KW  - aircraft navigation
KW  - automatic control
KW  - computer vision
KW  - sensor fusion
KW  - unmanned aerial vehicles
DO  - 10.3390/s21020391
ER  -
TY  - EJOU
AU  - Galyaev, Andrey A.
AU  - Lysenko, Pavel V.
AU  - Yakhno, Victor P.
TI  - 2D Optimal Trajectory Planning Problem in Threat Environment for UUV with Non-Uniform Radiation Pattern
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Path planning is necessary in many applications using unmanned underwater vehicles (UUVs). The main class of tasks is the planning of safe routes with minimal energy costs and/or minimal levels of emitted physical and information signals. Since the action planner is on board the UUV, the main focus is on methods and algorithms that allow it to build reference trajectories while minimizing the number of calculations. The study is devoted to the problem of the optimal route planning for a UUV with a non-uniform radiation pattern. The problem is stated in the form of two point variational problem for which necessary and sufficient optimality conditions are proved. Particular attention is paid to cases where optimality conditions are not met. These cases are directly related to found specific forms of a radiation pattern. Sufficient optimality conditions are extended on the class of two-link and multi-link motion paths. Software tools have been developed and computer simulations have been performed for various types of radiation patterns.
KW  - UUV path/trajectory planning
KW  - non-detection probability
KW  - non-uniform radiation pattern
DO  - 10.3390/s21020396
ER  -
TY  - EJOU
AU  - Dirscherl, Mariel
AU  - Dietz, Andreas J.
AU  - Kneisel, Christof
AU  - Kuenzer, Claudia
TI  - A Novel Method for Automated Supraglacial Lake Mapping in Antarctica Using Sentinel-1 SAR Imagery and Deep Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Supraglacial meltwater accumulation on ice sheets can be a main driver for accelerated ice discharge, mass loss, and global sea-level-rise. With further increasing surface air temperatures, meltwater-induced hydrofracturing, basal sliding, or surface thinning will cumulate and most likely trigger unprecedented ice mass loss on the Greenland and Antarctic ice sheets. While the Greenland surface hydrological network as well as its impacts on ice dynamics and mass balance has been studied in much detail, Antarctic supraglacial lakes remain understudied with a circum-Antarctic record of their spatio-temporal development entirely lacking. This study provides the first automated supraglacial lake extent mapping method using Sentinel-1 synthetic aperture radar (SAR) imagery over Antarctica and complements the developed optical Sentinel-2 supraglacial lake detection algorithm presented in our companion paper. In detail, we propose the use of a modified U-Net for semantic segmentation of supraglacial lakes in single-polarized Sentinel-1 imagery. The convolutional neural network (CNN) is implemented with residual connections for optimized performance as well as an Atrous Spatial Pyramid Pooling (ASPP) module for multiscale feature extraction. The algorithm is trained on 21,200 Sentinel-1 image patches and evaluated in ten spatially or temporally independent test acquisitions. In addition, George VI Ice Shelf is analyzed for intra-annual lake dynamics throughout austral summer 2019/2020 and a decision-level fused Sentinel-1 and Sentinel-2 maximum lake extent mapping product is presented for January 2020 revealing a more complete supraglacial lake coverage (~770 km2) than the individual single-sensor products. Classification results confirm the reliability of the proposed workflow with an average Kappa coefficient of 0.925 and a F1-score of 93.0% for the supraglacial water class across all test regions. Furthermore, the algorithm is applied in an additional test region covering supraglacial lakes on the Greenland ice sheet which further highlights the potential for spatio-temporal transferability. Future work involves the integration of more training data as well as intra-annual analyses of supraglacial lake occurrence across the whole continent and with focus on supraglacial lake development throughout a summer melt season and into Antarctic winter.
KW  - Antarctica
KW  - Antarctic ice sheet
KW  - supraglacial lakes
KW  - ice sheet hydrology
KW  - Sentinel-1
KW  - remote sensing
KW  - machine learning
KW  - deep learning
KW  - semantic segmentation
KW  - convolutional neural network
DO  - 10.3390/rs13020197
ER  -
TY  - EJOU
AU  - Korznikov, Kirill A.
AU  - Kislov, Dmitry E.
AU  - Altman, Jan
AU  - Doležal, Jiří
AU  - Vozmishcheva, Anna S.
AU  - Krestov, Pavel V.
TI  - Using U-Net-Like Deep Convolutional Neural Networks for Precise Tree Recognition in Very High Resolution RGB (Red, Green, Blue) Satellite Images
T2  - Forests

PY  - 2021
VL  - 12
IS  - 1
SN  - 1999-4907

AB  - Very high resolution satellite imageries provide an excellent foundation for precise mapping of plant communities and even single plants. We aim to perform individual tree recognition on the basis of very high resolution RGB (red, green, blue) satellite images using deep learning approaches for northern temperate mixed forests in the Primorsky Region of the Russian Far East. We used a pansharpened satellite RGB image by GeoEye-1 with a spatial resolution of 0.46 m/pixel, obtained in late April 2019. We parametrized the standard U-Net convolutional neural network (CNN) and trained it in manually delineated satellite images to solve the satellite image segmentation problem. For comparison purposes, we also applied standard pixel-based classification algorithms, such as random forest, k-nearest neighbor classifier, naive Bayes classifier, and quadratic discrimination. Pattern-specific features based on grey level co-occurrence matrices (GLCM) were computed to improve the recognition ability of standard machine learning methods. The U-Net-like CNN allowed us to obtain precise recognition of Mongolian poplar (Populus suaveolens Fisch. ex Loudon s.l.) and evergreen coniferous trees (Abies holophylla Maxim., Pinus koraiensis Siebold &amp; Zucc.). We were able to distinguish species belonging to either poplar or coniferous groups but were unable to separate species within the same group (i.e. A. holophylla and P. koraiensis were not distinguishable). The accuracy of recognition was estimated by several metrics and exceeded values obtained for standard machine learning approaches. In contrast to pixel-based recognition algorithms, the U-Net-like CNN does not lead to an increase in false-positive decisions when facing green-colored objects that are similar to trees. By means of U-Net-like CNN, we obtained a mean accuracy score of up to 0.96 in our computational experiments. The U-Net-like CNN recognizes tree crowns not as a set of pixels with known RGB intensities but as spatial objects with a specific geometry and pattern. This CNN&rsquo;s specific feature excludes misclassifications related to objects of similar colors as objects of interest. We highlight that utilization of satellite images obtained within the suitable phenological season is of high importance for successful tree recognition. The suitability of the phenological season is conceptualized as a group of conditions providing highlighting objects of interest over other components of vegetation cover. In our case, the use of satellite images captured in mid-spring allowed us to recognize evergreen fir and pine trees as the first class of objects (&ldquo;conifers&rdquo;) and poplars as the second class, which were in a leafless state among other deciduous tree species.
KW  - tree recognition
KW  - machine learning
KW  - convolutional neural network
DO  - 10.3390/f12010066
ER  -
TY  - EJOU
AU  - Ince, Omer F.
AU  - Kim, Jun-Sik
TI  - TIMA SLAM: Tracking Independently and Mapping Altogether for an Uncalibrated Multi-Camera System
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - We present a novel simultaneous localization and mapping (SLAM) system that extends the state-of-the-art ORB-SLAM2 for multi-camera usage without precalibration. In this system, each camera is tracked independently on a shared map, and the extrinsic parameters of each camera in the fixed multi-camera system are estimated online up to a scalar ambiguity (for RGB cameras). Thus, the laborious precalibration of extrinsic parameters between cameras becomes needless. By optimizing the map, the keyframe poses, and the relative poses of the multi-camera system simultaneously, observations from the multiple cameras are utilized robustly, and the accuracy of the shared map is improved. The system is not only compatible with RGB sensors but also works on RGB-D cameras. For RGB cameras, the performance of the system tested on the well-known EuRoC/ASL and KITTI datasets that are in the stereo configuration for indoor and outdoor environments, respectively, as well as our dataset that consists of three cameras with small overlapping regions. For the RGB-D tests, we created a dataset that consists of two cameras for an indoor environment. The experimental results showed that the proposed method successfully provides an accurate multi-camera SLAM system without precalibration of the multi-cameras.
KW  - SLAM
KW  - multi-camera
KW  - self extrinsic camera calibration
KW  - multi-edge based optimization
DO  - 10.3390/s21020409
ER  -
TY  - EJOU
AU  - Wang, Yutang
AU  - Wang, Jia
AU  - Chang, Shuping
AU  - Sun, Lu
AU  - An, Likun
AU  - Chen, Yuhan
AU  - Xu, Jiangqi
TI  - Classification of Street Tree Species Using UAV Tilt Photogrammetry
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - As an important component of the urban ecosystem, street trees have made an outstanding contribution to alleviating urban environmental pollution. Accurately extracting tree characteristics and species information can facilitate the monitoring and management of street trees, as well as aiding landscaping and studies of urban ecology. In this study, we selected the suburban areas of Beijing and Zhangjiakou and investigated six representative street tree species using unmanned aerial vehicle (UAV) tilt photogrammetry. We extracted five tree attributes and four combined attribute parameters and used four types of commonly-used machine learning classification algorithms as classifiers for tree species classification. The results show that random forest (RF), support vector machine (SVM), and back propagation (BP) neural network provide better classification results when using combined parameters for tree species classification, compared with those using individual tree attributes alone; however, the K-nearest neighbor (KNN) algorithm produced the opposite results. The best combination for classification is the BP neural network using combined attributes, with a classification precision of 89.1% and F-measure of 0.872, and we conclude that this approach best meets the requirements of street tree surveys. The results also demonstrate that optical UAV tilt photogrammetry combined with a machine learning classification algorithm is a low-cost, high-efficiency, and high-precision method for tree species classification.
KW  - tree species classification
KW  - street trees
KW  - UAV
KW  - machine learning
KW  - tilt photogrammetry
DO  - 10.3390/rs13020216
ER  -
TY  - EJOU
AU  - Xu, Jin
AU  - Pan, Xinxiang
AU  - Jia, Baozhu
AU  - Wu, Xuerui
AU  - Liu, Peng
AU  - Li, Bo
TI  - Oil Spill Detection Using LBP Feature and K-Means Clustering in Shipborne Radar Image
T2  - Journal of Marine Science and Engineering

PY  - 2021
VL  - 9
IS  - 1
SN  - 2077-1312

AB  - Oil spill accidents have seriously harmed the marine environment. Effective oil spill monitoring can provide strong scientific and technological support for emergency response of law enforcement departments. Shipborne radar can be used to monitor oil spills immediately after the accident. In this paper, the original shipborne radar image collected by the teaching-practice ship Yukun of Dalian Maritime University during the oil spill accident of Dalian on 16 July 2010 was taken as the research data, and an oil spill detection method was proposed by using LBP texture feature and K-means algorithm. First, Laplacian operator, Otsu algorithm, and mean filter were used to suppress the co-frequency interference noises and high brightness pixels. Then the gray intensity correction matrix was used to reduce image nonuniformity. Next, using LBP texture feature and K-means clustering algorithm, the effective oil spill regions were extracted. Finally, the adaptive threshold was applied to identify the oil films. This method can automatically detect oil spills in shipborne radar image. It can provide a guarantee for real-time monitoring of oil spill accidents.
KW  - oil spill
KW  - LBP
KW  - K-means
KW  - shipborne radar
KW  - remote sensing
KW  - oil pollution
KW  - image analysis
KW  - machine learning
KW  - radar detection
DO  - 10.3390/jmse9010065
ER  -
TY  - EJOU
AU  - Yu, Tong
AU  - Wu, Wenjin
AU  - Gong, Chen
AU  - Li, Xinwu
TI  - Residual Multi-Attention Classification Network for A Forest Dominated Tropical Landscape Using High-Resolution Remote Sensing Imagery
T2  - ISPRS International Journal of Geo-Information

PY  - 2021
VL  - 10
IS  - 1
SN  - 2220-9964

AB  - Tropical forests are of vital importance for maintaining biodiversity, regulating climate and material cycles while facing deforestation, agricultural reclamation, and managing various pressures. Remote sensing (RS) can support effective monitoring and mapping approaches for tropical forests, and to facilitate this we propose a deep neural network with an encoder&ndash;decoder architecture here to classify tropical forests and their environment. To deal with the complexity of tropical landscapes, this method utilizes a multi-scale convolution neural network (CNN) to expand the receptive field and extract multi-scale features. The model refines the features with several attention modules and fuses them through an upsampling module. A two-stage training strategy is proposed to alleviate misclassifications caused by sample imbalances. A joint loss function based on cross-entropy loss and the generalized Dice loss is applied in the first stage, and the second stage used the focal loss to fine-tune the weights. As a case study, we use Hainan tropical reserves to test the performance of this model. Compared with four state-of-the-art (SOTA) semantic segmentation networks, our network achieves the best performance with two Hainan datasets (mean intersection over union (MIoU) percentages of 85.78% and 82.85%). We also apply the new model to classify a public true color dataset which has 17 semantic classes and obtain results with an 83.75% MIoU. This further demonstrates the applicability and potential of this model in complex classification tasks.
KW  - remote sensing
KW  - deep convolution network
KW  - image analysis
KW  - land use and land cover (LULC)
KW  - tropical forest
DO  - 10.3390/ijgi10010022
ER  -
TY  - EJOU
AU  - Wang, Le
AU  - Xiang, Lirong
AU  - Tang, Lie
AU  - Jiang, Huanyu
TI  - A Convolutional Neural Network-Based Method for Corn Stand Counting in the Field
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Accurate corn stand count in the field at early season is of great interest to corn breeders and plant geneticists. However, the commonly used manual counting method is time consuming, laborious, and prone to error. Nowadays, unmanned aerial vehicles (UAV) tend to be a popular base for plant-image-collecting platforms. However, detecting corn stands in the field is a challenging task, primarily because of camera motion, leaf fluttering caused by wind, shadows of plants caused by direct sunlight, and the complex soil background. As for the UAV system, there are mainly two limitations for early seedling detection and counting. First, flying height cannot ensure a high resolution for small objects. It is especially difficult to detect early corn seedlings at around one week after planting, because the plants are small and difficult to differentiate from the background. Second, the battery life and payload of UAV systems cannot support long-duration online counting work. In this research project, we developed an automated, robust, and high-throughput method for corn stand counting based on color images extracted from video clips. A pipeline developed based on the YoloV3 network and Kalman filter was used to count corn seedlings online. The results demonstrate that our method is accurate and reliable for stand counting, achieving an accuracy of over 98% at growth stages V2 and V3 (vegetative stages with two and three visible collars) with an average frame rate of 47 frames per second (FPS). This pipeline can also be mounted easily on manned cart, tractor, or field robotic systems for online corn counting.
KW  - deep learning
KW  - YoloV3
KW  - video tracking
KW  - corn stand counting
DO  - 10.3390/s21020507
ER  -
TY  - EJOU
AU  - Lemaire, Pierre
AU  - Crispim-Junior, Carlos F.
AU  - Robinault, Lionel
AU  - Tougne, Laure
TI  - Registering Unmanned Aerial Vehicle Videos in the Long Term
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) have become a very popular way of acquiring video within contexts such as remote data acquisition or surveillance. Unfortunately, their viewpoint is often unstable, which tends to impact the automatic processing of their video flux negatively. To counteract the effects of an inconsistent viewpoint, two video processing strategies are classically adopted, namely registration and stabilization, which tend to be affected by distinct issues, namely jitter and drifting. Following our prior work, we suggest that the motion estimators used in both situations can be modeled to take into account their inherent errors. By acknowledging that drifting and jittery errors are of a different nature, we propose a combination that is able to limit their influence and build a hybrid solution for jitter-free video registration. In this work, however, our modeling was restricted to 2D-rigid transforms, which are rather limited in the case of airborne videos. In the present paper, we extend and refine the theoretical ground of our previous work. This addition allows us to show how to practically adapt our previous work to perspective transforms, which our study shows to be much more accurate for this problem. A lightweight implementation enables us to automatically register stationary UAV videos in real time. Our evaluation includes traffic surveillance recordings of up to 2 h and shows the potential of the proposed approach when paired with background subtraction tasks.
KW  - registration
KW  - stabilization
KW  - unmanned aerial vehicle
KW  - drone
DO  - 10.3390/s21020513
ER  -
TY  - EJOU
AU  - Dundas, Shannon J.
AU  - Vardanega, Molly
AU  - O’Brien, Patrick
AU  - McLeod, Steven R.
TI  - Quantifying Waterfowl Numbers: Comparison of Drone and Ground-Based Survey Methods for Surveying Waterfowl on Artificial Waterbodies
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - Drones are becoming a common method for surveying wildlife as they offer an aerial perspective of the landscape. For waterbirds in particular, drones can overcome challenges associated with surveying locations not accessible on foot. With the rapid uptake of drone technology for bird surveys, there is a need to compare and calibrate new technologies with existing survey methods. We compared waterfowl counts derived from ground- and drone-based survey methods. We sought to determine if group size and waterbody size influenced the difference between counts of non-nesting waterfowl and if detection of species varied between survey methods. Surveys of waterfowl were carried out at constructed irrigation dams and wastewater treatment ponds throughout the Riverina region of New South Wales (NSW), Australia. Data were analyzed using Bayesian multilevel models (BMLM) with weakly informative priors. Overall, drone-derived counts of waterfowl were greater (+36%) than ground counts using a spotting scope (&beta;_ground= 0.64 [0.62&ndash;0.66], (R2 = 0.973)). Ground counts also tended to underestimate the size of groups. Waterbody size had an effect on comparative counts, with ground counts being proportionally less than drone counts (mean = 0.74). The number of species identified in each waterbody type was similar regardless of survey method. Drone-derived counts are more accurate compared to traditional ground counts, but drones do have some drawbacks including initial equipment costs and time-consuming image or photo processing. Future surveys should consider using drones for more accurately surveying waterbirds, especially when large groups of birds are present on larger waterbodies.
KW  - population estimates
KW  - UAV
KW  - Australian ducks
DO  - 10.3390/drones5010005
ER  -
TY  - EJOU
AU  - Nguyen, Ha T.
AU  - Lopez Caceres, Maximo L.
AU  - Moritake, Koma
AU  - Kentsch, Sarah
AU  - Shu, Hase
AU  - Diez, Yago
TI  - Individual Sick Fir Tree (Abies mariesii) Identification in Insect Infested Forests by Means of UAV Images and Deep Learning
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Insect outbreaks are a recurrent natural phenomenon in forest ecosystems expected to increase due to climate change. Recent advances in Unmanned Aerial Vehicles (UAV) and Deep Learning (DL) Networks provide us with tools to monitor them. In this study we used nine orthomosaics and normalized Digital Surface Models (nDSM) to detect and classify healthy and sick Maries fir trees as well as deciduous trees. This study aims at automatically classifying treetops by means of a novel computer vision treetops detection algorithm and the adaptation of existing DL architectures. Considering detection alone, the accuracy results showed 85.70% success. In terms of detection and classification, we were able to detect/classify correctly 78.59% of all tree classes (39.64% for sick fir). However, with data augmentation, detection/classification percentage of the sick fir class rose to 73.01% at the cost of the result accuracy of all tree classes that dropped 63.57%. The implementation of UAV, computer vision and DL techniques contribute to the development of a new approach to evaluate the impact of insect outbreaks in forest.
KW  - deep learning
KW  - computer vision
KW  - UAV
KW  - individual tree detection
KW  - tree classification
KW  - sick tree detection
DO  - 10.3390/rs13020260
ER  -
TY  - EJOU
AU  - Yao, Guobiao
AU  - Yilmaz, Alper
AU  - Zhang, Li
AU  - Meng, Fei
AU  - Ai, Haibin
AU  - Jin, Fengxiang
TI  - Matching Large Baseline Oblique Stereo Images Using an End-to-End Convolutional Neural Network
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - The available stereo matching algorithms produce large number of false positive matches or only produce a few true-positives across oblique stereo images with large baseline. This undesired result happens due to the complex perspective deformation and radiometric distortion across the images. To address this problem, we propose a novel affine invariant feature matching algorithm with subpixel accuracy based on an end-to-end convolutional neural network (CNN). In our method, we adopt and modify a Hessian affine network, which we refer to as IHesAffNet, to obtain affine invariant Hessian regions using deep learning framework. To improve the correlation between corresponding features, we introduce an empirical weighted loss function (EWLF) based on the negative samples using K nearest neighbors, and then generate deep learning-based descriptors with high discrimination that is realized with our multiple hard network structure (MTHardNets). Following this step, the conjugate features are produced by using the Euclidean distance ratio as the matching metric, and the accuracy of matches are optimized through the deep learning transform based least square matching (DLT-LSM). Finally, experiments on Large baseline oblique stereo images acquired by ground close-range and unmanned aerial vehicle (UAV) verify the effectiveness of the proposed approach, and comprehensive comparisons demonstrate that our matching algorithm outperforms the state-of-art methods in terms of accuracy, distribution and correct ratio. The main contributions of this article are: (i) our proposed MTHardNets can generate high quality descriptors; and (ii) the IHesAffNet can produce substantial affine invariant corresponding features with reliable transform parameters.
KW  - large baseline
KW  - oblique stereo images
KW  - affine invariant features
KW  - convolutional neural network
KW  - deep learning
KW  - least square matching
DO  - 10.3390/rs13020274
ER  -
TY  - EJOU
AU  - Wada, Daichi
AU  - Araujo-Estrada, Sergio A.
AU  - Windsor, Shane
TI  - Unmanned Aerial Vehicle Pitch Control Using Deep Reinforcement Learning with Discrete Actions in Wind Tunnel Test
T2  - Aerospace

PY  - 2021
VL  - 8
IS  - 1
SN  - 2226-4310

AB  - Deep reinforcement learning is a promising method for training a nonlinear attitude controller for fixed-wing unmanned aerial vehicles. Until now, proof-of-concept studies have demonstrated successful attitude control in simulation. However, detailed experimental investigations have not yet been conducted. This study applied deep reinforcement learning for one-degree-of-freedom pitch control in wind tunnel tests with the aim of gaining practical understandings of attitude control application. Three controllers with different discrete action choices, that is, elevator angles, were designed. The controllers with larger action rates exhibited better performance in terms of following angle-of-attack commands. The root mean square errors for tracking angle-of-attack commands decreased from 3.42&deg; to 1.99&deg; as the maximum action rate increased from 10&deg;/s to 50&deg;/s. The comparison between experimental and simulation results showed that the controller with a smaller action rate experienced the friction effect, and the controllers with larger action rates experienced fluctuating behaviors in elevator maneuvers owing to delay. The investigation of the effect of friction and delay on pitch control highlighted the importance of conducting experiments to understand actual control performances, specifically when the controllers were trained with a low-fidelity model.
KW  - attitude control
KW  - deep reinforcement learning
KW  - fixed-wing aircraft
KW  - unmanned aerial vehicle
KW  - wind tunnel test
DO  - 10.3390/aerospace8010018
ER  -
TY  - EJOU
AU  - Manish, Raja
AU  - Lin, Yi-Chun
AU  - Ravi, Radhika
AU  - Hasheminasab, Seyyed M.
AU  - Zhou, Tian
AU  - Habib, Ayman
TI  - Development of a Miniaturized Mobile Mapping System for In-Row, Under-Canopy Phenotyping
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - This paper focuses on the development of a miniaturized mobile mapping platform with advantages over current agricultural phenotyping systems in terms of acquiring data that facilitate under-canopy plant trait extraction. The system is based on an unmanned ground vehicle (UGV) for in-row, under-canopy data acquisition to deliver accurately georeferenced 2D and 3D products. The paper addresses three main aspects pertaining to the UGV development: (a) architecture of the UGV mobile mapping system (MMS), (b) quality assessment of acquired data in terms of georeferencing information as well as derived 3D point cloud, and (c) ability to derive phenotypic plant traits using data acquired by the UGV MMS. The experimental results from this study demonstrate the ability of the UGV MMS to acquire dense and accurate data over agricultural fields that would facilitate highly accurate plant phenotyping (better than above-canopy platforms such as unmanned aerial systems and high-clearance tractors). Plant centers and plant count with an accuracy in the 90% range have been achieved.
KW  - unmanned ground vehicle (UGV)
KW  - mobile mapping system (MMS)
KW  - direct georeferencing
KW  - LiDAR
KW  - RGB imagery
KW  - under-canopy mapping
KW  - field-based phenotyping
DO  - 10.3390/rs13020276
ER  -
TY  - EJOU
AU  - Zheng, Qiong
AU  - Ye, Huichun
AU  - Huang, Wenjiang
AU  - Dong, Yingying
AU  - Jiang, Hao
AU  - Wang, Chongyang
AU  - Li, Dan
AU  - Wang, Li
AU  - Chen, Shuisen
TI  - Integrating Spectral Information and Meteorological Data to Monitor Wheat Yellow Rust at a Regional Scale: A Case Study
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Wheat yellow rust has a severe impact on wheat production and threatens food security in China; as such, an effective monitoring method is necessary at the regional scale. We propose a model for yellow rust monitoring based on Sentinel-2 multispectral images and a series of two-stage vegetation indices and meteorological data. Sensitive spectral vegetation indices (single- and two-stage indices) and meteorological features for wheat yellow rust discrimination were selected using the random forest method. Wheat yellow rust monitoring models were established using three different classification methods: linear discriminant analysis (LDA), support vector machine (SVM), and artificial neural network (ANN). The results show that models based on two-stage indices (i.e., those calculated using images from two different days) significantly outperform single-stage index models (i.e., those calculated using an image from a single day), the overall accuracy improved from 63.2% to 78.9%. The classification accuracies of models combining a vegetation index with meteorological feature are higher than those of pure vegetation index models. Among them, the model based on two-stage vegetation indices and meteorological features performs best, with a classification accuracy exceeding 73.7%. The SVM algorithm performed best for wheat yellow rust monitoring among the three algorithms; its classification accuracy (84.2%) was ~10.5% and 5.3% greater than those of LDA and ANN, respectively. Combined with crop growth and environmental information, our model has great potential for monitoring wheat yellow rust at a regional scale. Future work will focus on regional-scale monitoring and forecasting of crop disease.
KW  - wheat yellow rust
KW  - vegetation indices
KW  - meteorological information
KW  - food security
KW  - regional remote sensing
DO  - 10.3390/rs13020278
ER  -
TY  - EJOU
AU  - Zhang, Xiaomin
AU  - Zhao, Zhiyao
AU  - Wang, Zhaoyang
AU  - Wang, Xiaoyi
TI  - Fault Detection and Identification Method for Quadcopter Based on Airframe Vibration Signals
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Quadcopters are widely used in a variety of military and civilian mission scenarios. Real-time online detection of the abnormal state of the quadcopter is vital to the safety of aircraft. Existing data-driven fault detection methods generally usually require numerous sensors to collect data. However, quadcopter airframe space is limited. A large number of sensors cannot be loaded, meaning that it is difficult to use additional sensors to capture fault signals for quadcopters. In this paper, without additional sensors, a Fault Detection and Identification (FDI) method for quadcopter blades based on airframe vibration signals is proposed using the airborne acceleration sensor. This method integrates multi-axis data information and effectively detects and identifies quadcopter blade faults through Long and Short-Term Memory (LSTM) network models. Through flight experiments, the quadcopter triaxial accelerometer data are collected for airframe vibration signals at first. Then, the wavelet packet decomposition method is employed to extract data features, and the standard deviations of the wavelet packet coefficients are employed to form the feature vector. Finally, the LSTM-based FDI model is constructed for quadcopter blade FDI. The results show that the method can effectively detect and identify quadcopter blade faults with a better FDI performance and a higher model accuracy compared with the Back Propagation (BP) neural network-based FDI model.
KW  - quadcopter
KW  - fault detection and identification
KW  - wavelet packet decomposition
KW  - LSTM network
KW  - airframe vibration signals
DO  - 10.3390/s21020581
ER  -
TY  - EJOU
AU  - Berger, Katja
AU  - Rivera Caicedo, Juan P.
AU  - Martino, Luca
AU  - Wocher, Matthias
AU  - Hank, Tobias
AU  - Verrelst, Jochem
TI  - A Survey of Active Learning for Quantifying Vegetation Traits from Terrestrial Earth Observation Data
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - The current exponential increase of spatiotemporally explicit data streams from satellite-based Earth observation missions offers promising opportunities for global vegetation monitoring. Intelligent sampling through active learning (AL) heuristics provides a pathway for fast inference of essential vegetation variables by means of hybrid retrieval approaches, i.e., machine learning regression algorithms trained by radiative transfer model (RTM) simulations. In this study we summarize AL theory and perform a brief systematic literature survey about AL heuristics used in the context of Earth observation regression problems over terrestrial targets. Across all relevant studies it appeared that: (i) retrieval accuracy of AL-optimized training data sets outperformed models trained over large randomly sampled data sets, and (ii) Euclidean distance-based (EBD) diversity method tends to be the most efficient AL technique in terms of accuracy and computational demand. Additionally, a case study is presented based on experimental data employing both uncertainty and diversity AL criteria. Hereby, a a simulated training data base by the PROSAIL-PRO canopy RTM is used to demonstrate the benefit of AL techniques for the estimation of total leaf carotenoid content (Cxc) and leaf water content (Cw). Gaussian process regression (GPR) was incorporated to minimize and optimize the training data set with AL. Training the GPR algorithm on optimally AL-based sampled data sets led to improved variable retrievals compared to training on full data pools, which is further demonstrated on a mapping example. From these findings we can recommend the use of AL-based sub-sampling procedures to select the most informative samples out of large training data pools. This will not only optimize regression accuracy due to exclusion of redundant information, but also speed up processing time and reduce final model size of kernel-based machine learning regression algorithms, such as GPR. With this study we want to encourage further testing and implementation of AL sampling methods for hybrid retrieval workflows. AL can contribute to the solution of regression problems within the framework of operational vegetation monitoring using satellite imaging spectroscopy data, and may strongly facilitate data processing for cloud-computing platforms.
KW  - Gaussian process regression
KW  - EnMAP
KW  - hyperspectral
KW  - query strategies
KW  - optimal experimental design
DO  - 10.3390/rs13020287
ER  -
TY  - EJOU
AU  - Debella-Gilo, Misganu
AU  - Gjertsen, Arnt K.
TI  - Mapping Seasonal Agricultural Land Use Types Using Deep Learning on Sentinel-2 Image Time Series
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - The size and location of agricultural fields that are in active use and the type of use during the growing season are among the vital information that is needed for the careful planning and forecasting of agricultural production at national and regional scales. In areas where such data are not readily available, an independent seasonal monitoring method is needed. Remote sensing is a widely used tool to map land use types, although there are some limitations that can partly be circumvented by using, among others, multiple observations, careful feature selection and appropriate analysis methods. Here, we used Sentinel-2 satellite image time series (SITS) over the land area of Norway to map three agricultural land use classes: cereal crops, fodder crops (grass) and unused areas. The Multilayer Perceptron (MLP) and two variants of the Convolutional Neural Network (CNN), are implemented on SITS data of four different temporal resolutions. These enabled us to compare twelve model-dataset combinations to identify the model-dataset combination that results in the most accurate predictions. The CNN is implemented in the spectral and temporal dimensions instead of the conventional spatial dimension. Rather than using existing deep learning architectures, an autotuning procedure is implemented so that the model hyperparameters are empirically optimized during the training. The results obtained on held-out test data show that up to 94% overall accuracy and 90% Cohen&rsquo;s Kappa can be obtained when the 2D CNN is applied on the SITS data with a temporal resolution of 7 days. This is closely followed by the 1D CNN on the same dataset. However, the latter performs better than the former in predicting data outside the training set. It is further observed that cereal is predicted with the highest accuracy, followed by grass. Predicting the unused areas has been found to be difficult as there is no distinct surface condition that is common for all unused areas.
KW  - multilayer perceptron
KW  - CNN
KW  - hyperparameter tuning
KW  - cereal
KW  - grass
DO  - 10.3390/rs13020289
ER  -
TY  - EJOU
AU  - Jin, Xing
AU  - Tang, Ping
AU  - Houet, Thomas
AU  - Corpetti, Thomas
AU  - Alvarez-Vanhard, Emilien G.
AU  - Zhang, Zheng
TI  - Sequence Image Interpolation via Separable Convolution Network
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Remote-sensing time-series data are significant for global environmental change research and a better understanding of the Earth. However, remote-sensing acquisitions often provide sparse time series due to sensor resolution limitations and environmental factors, such as cloud noise for optical data. Image interpolation is the method that is often used to deal with this issue. This paper considers the deep learning method to learn the complex mapping of an interpolated intermediate image from predecessor and successor images, called separable convolution network for sequence image interpolation. The separable convolution network uses a separable 1D convolution kernel instead of 2D kernels to capture the spatial characteristics of input sequence images and then is trained end-to-end using sequence images. Our experiments, which were performed with unmanned aerial vehicle (UAV) and Landsat-8 datasets, show that the method is effective to produce high-quality time-series interpolated images, and the data-driven deep model can better simulate complex and diverse nonlinear image data information.
KW  - sequence image interpolation
KW  - separable convolution network
KW  - separable convolution kernel
KW  - UAV dataset
KW  - Landsat-8 dataset
DO  - 10.3390/rs13020296
ER  -
TY  - EJOU
AU  - Teng, Shuai
AU  - Liu, Zongchao
AU  - Chen, Gongfa
AU  - Cheng, Li
TI  - Concrete Crack Detection Based on Well-Known Feature Extractor Model and the YOLO_v2 Network
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 2
SN  - 2076-3417

AB  - This paper compares the crack detection performance (in terms of precision and computational cost) of the YOLO_v2 using 11 feature extractors, which provides a base for realizing fast and accurate crack detection on concrete structures. Cracks on concrete structures are an important indicator for assessing their durability and safety, and real-time crack detection is an essential task in structural maintenance. The object detection algorithm, especially the YOLO series network, has significant potential in crack detection, while the feature extractor is the most important component of the YOLO_v2. Hence, this paper employs 11 well-known CNN models as the feature extractor of the YOLO_v2 for crack detection. The results confirm that a different feature extractor model of the YOLO_v2 network leads to a different detection result, among which the AP value is 0.89, 0, and 0 for &lsquo;resnet18&rsquo;, &lsquo;alexnet&rsquo;, and &lsquo;vgg16&rsquo;, respectively meanwhile, the &lsquo;googlenet&rsquo; (AP = 0.84) and &lsquo;mobilenetv2&rsquo; (AP = 0.87) also demonstrate comparable AP values. In terms of computing speed, the &lsquo;alexnet&rsquo; takes the least computational time, the &lsquo;squeezenet&rsquo; and &lsquo;resnet18&rsquo; are ranked second and third respectively; therefore, the &lsquo;resnet18&rsquo; is the best feature extractor model in terms of precision and computational cost. Additionally, through the parametric study (influence on detection results of the training epoch, feature extraction layer, and testing image size), the associated parameters indeed have an impact on the detection results. It is demonstrated that: excellent crack detection results can be achieved by the YOLO_v2 detector, in which an appropriate feature extractor model, training epoch, feature extraction layer, and testing image size play an important role.
KW  - crack detection
KW  - YOLO network
KW  - feature extractor
KW  - feature extraction layer
KW  - computational cost
KW  - detection precision
DO  - 10.3390/app11020813
ER  -
TY  - EJOU
AU  - Yang, Baohua
AU  - Ma, Jifeng
AU  - Yao, Xia
AU  - Cao, Weixing
AU  - Zhu, Yan
TI  - Estimation of Leaf Nitrogen Content in Wheat Based on Fusion of Spectral Features and Deep Features from Near Infrared Hyperspectral Imagery
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 2
SN  - 1424-8220

AB  - Nitrogen is an important indicator for monitoring wheat growth. The rapid development and wide application of non-destructive detection provide many approaches for estimating leaf nitrogen content (LNC) in wheat. Previous studies have shown that better results have been obtained in the estimation of LNC in wheat based on spectral features. However, the lack of automatically extracted features leads to poor universality of the estimation model. Therefore, a feature fusion method for estimating LNC in wheat by combining spectral features with deep features (spatial features) was proposed. The deep features were automatically obtained with a convolutional neural network model based on the PyTorch framework. The spectral features were obtained using spectral information including position features (PFs) and vegetation indices (VIs). Different models based on feature combination for evaluating LNC in wheat were constructed: partial least squares regression (PLS), gradient boosting decision tree (GBDT), and support vector regression (SVR). The results indicate that the model based on the fusion feature from near-ground hyperspectral imagery has good estimation effect. In particular, the estimation accuracy of the GBDT model is the best (R2 = 0.975 for calibration set, R2 = 0.861 for validation set). These findings demonstrate that the approach proposed in this study improved the estimation performance of LNC in wheat, which could provide technical support in wheat growth monitoring.
KW  - convolutional neural network
KW  - leaf nitrogen content
KW  - deep features
KW  - wheat
KW  - spectral features
DO  - 10.3390/s21020613
ER  -
TY  - EJOU
AU  - Li, Haolu
AU  - Wang, Guojie
AU  - Dong, Zhen
AU  - Wei, Xikun
AU  - Wu, Mengjuan
AU  - Song, Huihui
AU  - Amankwah, Solomon O.
TI  - Identifying Cotton Fields from Remote Sensing Images Using Multiple Deep Learning Networks
T2  - Agronomy

PY  - 2021
VL  - 11
IS  - 1
SN  - 2073-4395

AB  - Remote sensing imageries processed through empirical and deterministic approaches help predict multiple agronomic traits throughout the growing season. Accurate identification of cotton crop from remotely sensed imageries is a significant task in precision agriculture. This study aims to utilize a deep learning-based framework for cotton crop field identification with Gaofen-1 (GF-1) high-resolution (16 m) imageries in Wei-Ku region, China. An optimized model for the pixel-wise multidimensional densely connected convolutional neural network (DenseNet) was used. Four widely-used classic convolutional neural networks (CNNs), including ResNet, VGG, SegNet, and DeepLab v3+, were also used for accuracy assessment. The results infer that DenseNet can identify cotton crop features within a relatively shorter time about 5 h for training convergence. The model performance was examined by multiple indicators (P, F1, R, and mIou) produced through the confusion matrix, and the derived cotton fields were then visualized. The DenseNet model has illustrated considerable improvements in comparison with the preceding mainstream models. The results showed that the retrieval precision was 0.948, F1 score was 0.953, and mIou was 0.911. Furthermore, its performance is relatively better in discriminating cotton crop fields&rsquo; fine structures when clouds, mountain shadows, and urban built up.
KW  - cotton identification
KW  - deep learning
KW  - DenseNet
KW  - remote sensing images
DO  - 10.3390/agronomy11010174
ER  -
TY  - EJOU
AU  - Zou, Kunlin
AU  - Chen, Xin
AU  - Zhang, Fan
AU  - Zhou, Hang
AU  - Zhang, Chunlong
TI  - A Field Weed Density Evaluation Method Based on UAV Imaging and Modified U-Net
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 2
SN  - 2072-4292

AB  - Weeds are one of the main factors affecting the yield and quality of agricultural products. Accurate evaluation of weed density is of great significance for field management, especially precision weeding. In this paper, a weed density calculating and mapping method in the field is proposed. An unmanned aerial vehicle (UAV) was used to capture field images. The excess green minus excess red index, combined with the minimum error threshold segmentation method, was used to segment green plants and bare land. A modified U-net was used to segment crops from images. After removing the bare land and crops from the field, images of weeds were obtained. The weed density was evaluated by the ratio of weed area to total area on the segmented image. The accuracy of the green plant segmentation was 93.5%. In terms of crop segmentation, the intersection over union (IoU) was 93.40%, and the segmentation time of a single image was 35.90 ms. Finally, the determination coefficient of the UAV evaluated weed density and the manually observed weed density was 0.94, and the root mean square error was 0.03. With the proposed method, the weed density of a field can be effectively evaluated from UAV images, hence providing critical information for precision weeding.
KW  - semantic segmentation
KW  - U-net
KW  - UAV
KW  - weed density
DO  - 10.3390/rs13020310
ER  -
TY  - EJOU
AU  - Zhu, Kuanxing
AU  - Xu, Peihua
AU  - Cao, Chen
AU  - Zheng, Lianjing
AU  - Liu, Yue
AU  - Dong, Xiujun
TI  - Preliminary Identification of Geological Hazards from Songpinggou to Feihong in Mao County along the Minjiang River Using SBAS-InSAR Technique Integrated Multiple Spatial Analysis Methods
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 3
SN  - 2071-1050

AB  - Landslides and collapses are common geological hazards in mountainous areas, posing significant threats to the lives and property of residents. Therefore, early identification of disasters is of great significance for disaster prevention. In this study, we used Small Baseline Subset Interferometric Synthetic Aperture Radar (SBAS-InSAR) technology to process C-band Sentinel-1A images to monitor the surface deformation from Songpinggou to Feihong in Maoxian County, Sichuan Province. Visibility analysis was used to remove the influence of geometric distortion on the SAR images and retain deformation information in the visible area. Hot spot and kernel density analyses were performed on the deformation data, and 18 deformation clusters were obtained. Velocity and slope data were integrated, and 26 disaster areas were interpreted from the 18 deformation clusters, including 20 potential landslides and 6 potential collapses. A detailed field investigation indicated that potential landslides No. 6 and No. 8 had developed cracks and were severely damaged, with a high probability of occurrence. Potential collapse No. 22 had developed fissures, exposing a dangerous rock mass and posing significant threats to the lives and property of residents. This study shows that the proposed method that combines visibility analysis, InSAR deformation rates, and spatial analysis can quickly and accurately identify potential geological disasters and provide guidance for local disaster prevention and mitigation.
KW  - landslide and collapse identification
KW  - SBAS-InSAR
KW  - visibility analysis
KW  - kernel density analysis
KW  - field investigation
DO  - 10.3390/su13031017
ER  -
TY  - EJOU
AU  - Sahal, Radhya
AU  - Alsamhi, Saeed H.
AU  - Breslin, John G.
AU  - Ali, Muhammad I.
TI  - Industry 4.0 towards Forestry 4.0: Fire Detection Use Case
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - Forestry 4.0 is inspired by the Industry 4.0 concept, which plays a vital role in the next industrial generation revolution. It is ushering in a new era for efficient and sustainable forest management. Environmental sustainability and climate change are related challenges to promote sustainable forest management of natural resources. Internet of Forest Things (IoFT) is an emerging technology that helps manage forest sustainability and protect forest from hazards via distributing smart devices for gathering data stream during monitoring and detecting fire. Stream processing is a well-known research area, and recently, it has gained a further significance due to the emergence of IoFT devices. Distributed stream processing platforms have emerged, e.g., Apache Flink, Storm, and Spark, etc. Querying windowing is the heart of any stream-processing platform which splits infinite data stream into chunks of finite data to execute a query. Dynamic query window-based processing can reduce the reporting time in case of missing and delayed events caused by data drift.In this paper, we present a novel dynamic mechanism to recommend the optimal window size and type based on the dynamic context of IoFT application. In particular, we designed a dynamic window selector for stream queries considering input stream data characteristics, application workload and resource constraints to recommend the optimal stream query window configuration. A research gap on the likelihood of adopting smart IoFT devices in environmental sustainability indicates a lack of empirical studies to pursue forest sustainability, i.e., sustainable forestry applications. So, we focus on forest fire management and detection as a use case of Forestry 4.0, one of the dynamic environmental management challenges, i.e., climate change, to deliver sustainable forestry goals. According to the dynamic window selector&rsquo;s experimental results, end-to-end latency time for the reported fire alerts has been reduced by dynamical adaptation of window size with IoFT stream rate changes.
KW  - IoT
KW  - query
KW  - industry 4.0
KW  - stream processing
KW  - window size
KW  - forestry 4.0
KW  - internet of forestry things
KW  - forest fire detection
KW  - forest sustainability
DO  - 10.3390/s21030694
ER  -
TY  - EJOU
AU  - Hong, Jin
AU  - Kwon, Junseok
TI  - Visual Tracking of Small Unmanned Aerial Vehicles Based on Object Proposal Voting
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 3
SN  - 2076-3417

AB  - In this paper, we propose a novel visual tracking method for unmanned aerial vehicles (UAVs) in aerial scenery. To track the UAVs robustly, we present a new object proposal method that can accurately determine the object regions that are likely to exist. The proposed object proposal method is robust to small objects and severe background clutter. For this, we vote on candidate areas of the object and increase or decrease the weight of the area accordingly. Thus, the method can accurately propose the object areas that can be used to track small-sized UAVs with the assumption that their motion is smooth over time. Experimental results verify that UAVs are accurately tracked even when they are very small and the background is complex. The proposed method qualitatively and quantitatively delivers state-of-the-art performance in comparison with conventional object proposal-based methods.
KW  - unmanned aerial vehicles
KW  - object tracking
KW  - object proposal voting
DO  - 10.3390/app11030953
ER  -
TY  - EJOU
AU  - Butcher, Paul A.
AU  - Colefax, Andrew P.
AU  - Gorkin, Robert A.
AU  - Kajiura, Stephen M.
AU  - López, Naima A.
AU  - Mourier, Johann
AU  - Purcell, Cormac R.
AU  - Skomal, Gregory B.
AU  - Tucker, James P.
AU  - Walsh, Andrew J.
AU  - Williamson, Jane E.
AU  - Raoult, Vincent
TI  - The Drone Revolution of Shark Science: A Review
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - Over the past decade, drones have become a popular tool for wildlife management and research. Drones have shown significant value for animals that were often difficult or dangerous to study using traditional survey methods. In the past five years drone technology has become commonplace for shark research with their use above, and more recently, below the water helping to minimise knowledge gaps about these cryptic species. Drones have enhanced our understanding of shark behaviour and are critically important tools, not only due to the importance and conservation of the animals in the ecosystem, but to also help minimise dangerous encounters with humans. To provide some guidance for their future use in relation to sharks, this review provides an overview of how drones are currently used with critical context for shark monitoring. We show how drones have been used to fill knowledge gaps around fundamental shark behaviours or movements, social interactions, and predation across multiple species and scenarios. We further detail the advancement in technology across sensors, automation, and artificial intelligence that are improving our abilities in data collection and analysis and opening opportunities for shark-related beach safety. An investigation of the shark-based research potential for underwater drones (ROV/AUV) is also provided. Finally, this review provides baseline observations that have been pioneered for shark research and recommendations for how drones might be used to enhance our knowledge in the future.
KW  - artificial intelligence
KW  - AUV
KW  - drones
KW  - protocols
KW  - ROV
KW  - sharks
KW  - UAV
DO  - 10.3390/drones5010008
ER  -
TY  - EJOU
AU  - Nguyen, Canh
AU  - Sagan, Vasit
AU  - Maimaitiyiming, Matthew
AU  - Maimaitijiang, Maitiniyazi
AU  - Bhadra, Sourav
AU  - Kwasniewski, Misha T.
TI  - Early Detection of Plant Viral Disease Using Hyperspectral Imaging and Deep Learning
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - Early detection of grapevine viral diseases is critical for early interventions in order to prevent the disease from spreading to the entire vineyard. Hyperspectral remote sensing can potentially detect and quantify viral diseases in a nondestructive manner. This study utilized hyperspectral imagery at the plant level to identify and classify grapevines inoculated with the newly discovered DNA virus grapevine vein-clearing virus (GVCV) at the early asymptomatic stages. An experiment was set up at a test site at South Farm Research Center, Columbia, MO, USA (38.92 N, &minus;92.28 W), with two grapevine groups, namely healthy and GVCV-infected, while other conditions were controlled. Images of each vine were captured by a SPECIM IQ 400&ndash;1000 nm hyperspectral sensor (Oulu, Finland). Hyperspectral images were calibrated and preprocessed to retain only grapevine pixels. A statistical approach was employed to discriminate two reflectance spectra patterns between healthy and GVCV vines. Disease-centric vegetation indices (VIs) were established and explored in terms of their importance to the classification power. Pixel-wise (spectral features) classification was performed in parallel with image-wise (joint spatial&ndash;spectral features) classification within a framework involving deep learning architectures and traditional machine learning. The results showed that: (1) the discriminative wavelength regions included the 900&ndash;940 nm range in the near-infrared (NIR) region in vines 30 days after sowing (DAS) and the entire visual (VIS) region of 400&ndash;700 nm in vines 90 DAS; (2) the normalized pheophytization index (NPQI), fluorescence ratio index 1 (FRI1), plant senescence reflectance index (PSRI), anthocyanin index (AntGitelson), and water stress and canopy temperature (WSCT) measures were the most discriminative indices; (3) the support vector machine (SVM) was effective in VI-wise classification with smaller feature spaces, while the RF classifier performed better in pixel-wise and image-wise classification with larger feature spaces; and (4) the automated 3D convolutional neural network (3D-CNN) feature extractor provided promising results over the 2D convolutional neural network (2D-CNN) in learning features from hyperspectral data cubes with a limited number of samples.
KW  - plant disease
KW  - spectral statistics
KW  - machine learning
KW  - 2D-CNN
KW  - 3D-CNN
KW  - grapevine vein-clearing virus (GVCV)
DO  - 10.3390/s21030742
ER  -
TY  - EJOU
AU  - Chen, Xinxin
AU  - Jiang, Kang
AU  - Zhu, Yushi
AU  - Wang, Xiangjun
AU  - Yun, Ting
TI  - Individual Tree Crown Segmentation Directly from UAV-Borne LiDAR Data Using the PointNet of Deep Learning
T2  - Forests

PY  - 2021
VL  - 12
IS  - 2
SN  - 1999-4907

AB  - Accurate individual tree crown (ITC) segmentation from scanned point clouds is a fundamental task in forest biomass monitoring and forest ecology management. Light detection and ranging (LiDAR) as a mainstream tool for forest survey is advancing the pattern of forest data acquisition. In this study, we performed a novel deep learning framework directly processing the forest point clouds belonging to the four forest types (i.e., the nursery base, the monastery garden, the mixed forest, and the defoliated forest) to realize the ITC segmentation. The specific steps of our approach were as follows: first, a voxelization strategy was conducted to subdivide the collected point clouds with various tree species from various forest types into many voxels. These voxels containing point clouds were taken as training samples for the PointNet deep learning framework to identify the tree crowns at the voxel scale. Second, based on the initial segmentation results, we used the height-related gradient information to accurately depict the boundaries of each tree crown. Meanwhile, the retrieved tree crown breadths of individual trees were compared with field measurements to verify the effectiveness of our approach. Among the four forest types, our results revealed the best performance for the nursery base (tree crown detection rate r = 0.90; crown breadth estimation R2 &gt; 0.94 and root mean squared error (RMSE) &lt; 0.2m). A sound performance was also achieved for the monastery garden and mixed forest, which had complex forest structures, complicated intersections of branches and different building types, with r = 0.85, R2 &gt; 0.88 and RMSE &lt; 0.6 m for the monastery garden and r = 0.80, R2 &gt; 0.85 and RMSE &lt; 0.8 m for the mixed forest. For the fourth forest plot type with the distribution of crown defoliation across the woodland, we achieved the performance with r = 0.82, R2 &gt; 0.79 and RMSE &lt; 0.7 m. Our method presents a robust framework inspired by the deep learning technology and computer graphics theory that solves the ITC segmentation problem and retrieves forest parameters under various forest conditions.
KW  - deep learning
KW  - individual tree crown segmentation
KW  - Airborne LiDAR data
KW  - computer graphics
DO  - 10.3390/f12020131
ER  -
TY  - EJOU
AU  - Avtar, Ram
AU  - Kouser, Asma
AU  - Kumar, Ashwani
AU  - Singh, Deepak
AU  - Misra, Prakhar
AU  - Gupta, Ankita
AU  - Yunus, Ali P.
AU  - Kumar, Pankaj
AU  - Johnson, Brian A.
AU  - Dasgupta, Rajarshi
AU  - Sahu, Netrananda
AU  - Besse Rimba, Andi
TI  - Remote Sensing for International Peace and Security: Its Role and Implications
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - Remote sensing technology has seen a massive rise in popularity over the last two decades, becoming an integral part of our lives. Space-based satellite technologies facilitated access to the inaccessible terrains, helped humanitarian teams, support complex emergencies, and contributed to monitoring and verifying conflict zones. The scoping phase of this review investigated the utility of the role of remote sensing application to complement international peace and security activities owing to their ability to provide objective near real-time insights at the ground level. The first part of this review looks into the major research concepts and implementation of remote sensing-based techniques for international peace and security applications and presented a meta-analysis on how advanced sensor capabilities can support various aspects of peace and security. With key examples, we demonstrated how this technology assemblage enacts multiple versions of peace and security: for refugee relief operations, in armed conflicts monitoring, tracking acts of genocide, providing evidence in courts of law, and assessing contravention in human rights. The second part of this review anticipates future challenges that can hinder the applicative capabilities of remote sensing in peace and security. Varying types of sensors pose discrepancies in image classifications and issues like cost, resolution, and difficulty of ground-truth in conflict areas. With emerging technologies and sufficient secondary resources available, remote sensing plays a vital operational tool in conflict-affected areas by supporting an extensive diversity in public policy actions for peacekeeping processes.
KW  - conflict resources monitoring
KW  - disease control and prevention
KW  - human rights
KW  - genocide tracking
KW  - human rights violation
KW  - geopolitics
DO  - 10.3390/rs13030439
ER  -
TY  - EJOU
AU  - Zhou, Xixuan
AU  - Yang, Liao
AU  - Wang, Weisheng
AU  - Chen, Baili
TI  - UAV Data as an Alternative to Field Sampling to Monitor Vineyards Using Machine Learning Based on UAV/Sentinel-2 Data Fusion
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - Pests and diseases affect the yield and quality of grapes directly and engender noteworthy economic losses. Diagnosing &ldquo;lesions&rdquo; on vines as soon as possible and dynamically monitoring symptoms caused by pests and diseases at a larger scale are essential to pest control. This study has appraised the capabilities of high-resolution unmanned aerial vehicle (UAV) data as an alternative to manual field sampling to obtain sampling canopy sets and to supplement satellite-based monitoring using machine learning models including partial least squared regression (PLSR), support vector regression (SVR), random forest regression (RFR), and extreme learning regression (ELR) with a new activation function. UAV data were acquired from two flights in Turpan to determine disease severity (DS) and disease incidence (DI) and compared with field visual assessments. The UAV-derived canopy structure including canopy height (CH) and vegetation fraction cover (VFC), as well as satellite-based spectral features calculated from Sentinel-2A/B data were analyzed to evaluate the potential of UAV data to replace manual sampling data and predict DI. It was found that SVR slightly outperformed the other methods with a root mean square error (RMSE) of 1.89%. Moreover, the combination of canopy structure (CS) and vegetation index (VIs) improved prediction accuracy compared with single-type features (RMSEcs of 2.86% and RMSEVIs of 1.93%). This study tested the ability of UAV sampling to replace manual sampling on a large scale and introduced opportunities and challenges of fusing different features to monitor vineyards using machine learning. Within this framework, disease incidence can be estimated efficiently and accurately for larger area monitoring operation.
KW  - unmanned aircraft system (UAS)
KW  - vineyard monitoring
KW  - machine learning
KW  - pests and diseases
KW  - Sentinel-2 data
KW  - UAV data
DO  - 10.3390/rs13030457
ER  -
TY  - EJOU
AU  - Carramiñana, David
AU  - Campaña, Iván
AU  - Bergesio, Luca
AU  - Bernardos, Ana M.
AU  - Besada, Juan A.
TI  - Sensors and Communication Simulation for Unmanned Traffic Management
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - Unmanned traffic management (UTM) systems will become a key enabler to the future drone market ecosystem, enabling the safe concurrent operation of both manned and unmanned aircrafts. Currently, these systems are usually tested by performing real scenarios that are costly, limited, hardly scalable, and poorly repeatable. As a solution, in this paper we propose an agent-based simulation platform, implemented through a micro service architecture, which may simulate UTM information sources, such as flight plans, telemetry messages, or tracks from a surveillance network. The final objective of this simulator is to use these information streams to perform a system-level evaluation of UTM systems both in the pre-flight and in-flight stages. The proposed platform, with a focus on simulation of communications and sensors, allows to model UTM actors&rsquo; behaviors and their interactions. In addition, it also considers the manual definition of events to simulate unexpected behaviors/events (contingencies), such as communications failures or pilots&rsquo; actions. In order to validate our architecture, we implemented a simulator that considers the following actors: drones, pilots, ground control stations, surveillance networks, and communications networks. This platform enables the simulation of the drone trajectory and control, the C2 (command and control) link, drone detection by surveillance sensors, and the communication of all agents by means of a mobile communications network. Our results show that it is possible to truthfully recreate complex scenarios using this simulator, mitigating the disadvantages of real testbeds.
KW  - UAV
KW  - drone
KW  - UTM
KW  - U-Space
KW  - C2
KW  - surveillance networks
KW  - agent-based simulation
DO  - 10.3390/s21030927
ER  -
TY  - EJOU
AU  - Yang, Wanting
AU  - Zhang, Xianfeng
AU  - Luo, Peng
TI  - Transferability of Convolutional Neural Network Models for Identifying Damaged Buildings Due to Earthquake
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 3
SN  - 2072-4292

AB  - The collapse of buildings caused by earthquakes can lead to a large loss of life and property. Rapid assessment of building damage with remote sensing image data can support emergency rescues. However, current studies indicate that only a limited sample set can usually be obtained from remote sensing images immediately following an earthquake. Consequently, the difficulty in preparing sufficient training samples constrains the generalization of the model in the identification of earthquake-damaged buildings. To produce a deep learning network model with strong generalization, this study adjusted four Convolutional Neural Network (CNN) models for extracting damaged building information and compared their performance. A sample dataset of damaged buildings was constructed by using multiple disaster images retrieved from the xBD dataset. Using satellite and aerial remote sensing data obtained after the 2008 Wenchuan earthquake, we examined the geographic and data transferability of the deep network model pre-trained on the xBD dataset. The result shows that the network model pre-trained with samples generated from multiple disaster remote sensing images can extract accurately collapsed building information from satellite remote sensing data. Among the adjusted CNN models tested in the study, the adjusted DenseNet121 was the most robust. Transfer learning solved the problem of poor adaptability of the network model to remote sensing images acquired by different platforms and could identify disaster-damaged buildings properly. These results provide a solution to the rapid extraction of earthquake-damaged building information based on a deep learning network model.
KW  - earthquake
KW  - disaster-damaged buildings
KW  - transfer learning
KW  - CNN
KW  - VHR images
DO  - 10.3390/rs13030504
ER  -
TY  - EJOU
AU  - Rahman, Ehab U.
AU  - Zhang, Yihong
AU  - Ahmad, Sohail
AU  - Ahmad, Hafiz I.
AU  - Jobaer, Sayed
TI  - Autonomous Vision-Based Primary Distribution Systems Porcelain Insulators Inspection Using UAVs
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 3
SN  - 1424-8220

AB  - The early detection of damaged (partially broken) outdoor insulators in primary distribution systems is of paramount importance for continuous electricity supply and public safety. Unmanned aerial vehicles (UAVs) present a safer, autonomous, and efficient way to examine the power system components without closing the power distribution system. In this work, a novel dataset is designed by capturing real images using UAVs and manually generated images collected to overcome the data insufficiency problem. A deep Laplacian pyramid-based super-resolution network is implemented to reconstruct high-resolution training images. To improve the visibility of low-light images, a low-light image enhancement technique is used for the robust exposure correction of the training images. A different fine-tuning strategy is implemented for fine-tuning the object detection model to increase detection accuracy for the specific faulty insulators. Several flight path strategies are proposed to overcome the shuttering effect of insulators, along with providing a less complex and time- and energy-efficient approach for capturing a video stream of the power system components. The performance of different object detection models is presented for selecting the most suitable one for fine-tuning on the specific faulty insulator dataset. For the detection of damaged insulators, our proposed method achieved an F1-score of 0.81 and 0.77 on two different datasets and presents a simple and more efficient flight strategy. Our approach is based on real aerial inspection of in-service porcelain insulators by extensive evaluation of several video sequences showing robust fault recognition and diagnostic capabilities. Our approach is demonstrated on data acquired by a drone in Swat, Pakistan.
KW  - primary distribution systems
KW  - transfer learning
KW  - YoloV4
KW  - porcelain insulator detection
KW  - UAVs
KW  - BRISQUE
KW  - LIME
KW  - LapSRN
KW  - YoloV5
DO  - 10.3390/s21030974
ER  -
TY  - EJOU
AU  - Oleksyn, Semonn
AU  - Tosetto, Louise
AU  - Raoult, Vincent
AU  - Joyce, Karen E.
AU  - Williamson, Jane E.
TI  - Going Batty: The Challenges and Opportunities of Using Drones to Monitor the Behaviour and Habitat Use of Rays
T2  - Drones

PY  - 2021
VL  - 5
IS  - 1
SN  - 2504-446X

AB  - The way an animal behaves in its habitat provides insight into its ecological role. As such, collecting robust, accurate datasets in a time-efficient manner is an ever-present pressure for the field of behavioural ecology. Faced with the shortcomings and physical limitations of traditional ground-based data collection techniques, particularly in marine studies, drones offer a low-cost and efficient approach for collecting data in a range of coastal environments. Despite drones being widely used to monitor a range of marine animals, they currently remain underutilised in ray research. The innovative application of drones in environmental and ecological studies has presented novel opportunities in animal observation and habitat assessment, although this emerging field faces substantial challenges. As we consider the possibility to monitor rays using drones, we face challenges related to local aviation regulations, the weather and environment, as well as sensor and platform limitations. Promising solutions continue to be developed, however, growing the potential for drone-based monitoring of behaviour and habitat use of rays. While the barriers to enter this field may appear daunting for researchers with little experience with drones, the technology is becoming increasingly accessible, helping ray researchers obtain a wide range of highly useful data.
KW  - UAV
KW  - UAS
KW  - RPA
KW  - benthic habitat mapping
KW  - ray ecology
KW  - coastal environments
KW  - batoidea
DO  - 10.3390/drones5010012
ER  -
TY  - EJOU
AU  - Wen, Qiaodi
AU  - Luo, Ziqi
AU  - Chen, Ruitao
AU  - Yang, Yifan
AU  - Li, Guofa
TI  - Deep Learning Approaches on Defect Detection in High Resolution Aerial Images of Insulators
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - By detecting the defect location in high-resolution insulator images collected by unmanned aerial vehicle (UAV) in various environments, the occurrence of power failure can be timely detected and the caused economic loss can be reduced. However, the accuracies of existing detection methods are greatly limited by the complex background interference and small target detection. To solve this problem, two deep learning methods based on Faster R-CNN (faster region-based convolutional neural network) are proposed in this paper, namely Exact R-CNN (exact region-based convolutional neural network) and CME-CNN (cascade the mask extraction and exact region-based convolutional neural network). Firstly, we proposed an Exact R-CNN based on a series of advanced techniques including FPN (feature pyramid network), cascade regression, and GIoU (generalized intersection over union). RoI Align (region of interest align) is introduced to replace RoI pooling (region of interest pooling) to address the misalignment problem, and the depthwise separable convolution and linear bottleneck are introduced to reduce the computational burden. Secondly, a new pipeline is innovatively proposed to improve the performance of insulator defect detection, namely CME-CNN. In our proposed CME-CNN, an insulator mask image is firstly generated to eliminate the complex background by using an encoder-decoder mask extraction network, and then the Exact R-CNN is used to detect the insulator defects. The experimental results show that our proposed method can effectively detect insulator defects, and its accuracy is better than the examined mainstream target detection algorithms.
KW  - deep learning
KW  - defect detection
KW  - power inspection
KW  - insulator
DO  - 10.3390/s21041033
ER  -
TY  - EJOU
AU  - Kamarudin, Mohd H.
AU  - Ismail, Zool H.
AU  - Saidi, Noor B.
TI  - Deep Learning Sensor Fusion in Plant Water Stress Assessment: A Comprehensive Review
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 4
SN  - 2076-3417

AB  - Water stress is one of the major challenges to food security, causing a significant economic loss for the nation as well for growers. Accurate assessment of water stress will enhance agricultural productivity through optimization of plant water usage, maximizing plant breeding strategies, and preventing forest wildfire for better ecosystem management. Recent advancements in sensor technologies have enabled high-throughput, non-contact, and cost-efficient plant water stress assessment through intelligence system modeling. The advanced deep learning sensor fusion technique has been reported to improve the performance of the machine learning application for processing the collected sensory data. This paper extensively reviews the state-of-the-art methods for plant water stress assessment that utilized the deep learning sensor fusion approach in their application, together with future prospects and challenges of the application domain. Notably, 37 deep learning solutions fell under six main areas, namely soil moisture estimation, soil water modelling, evapotranspiration estimation, evapotranspiration forecasting, plant water status estimation and plant water stress identification. Basically, there are eight deep learning solutions compiled for the 3D-dimensional data and plant varieties challenge, including unbalanced data that occurred due to isohydric plants, and the effect of variations that occur within the same species but cultivated from different locations.
KW  - artificial intelligence
KW  - agriculture monitoring system
KW  - modelling
KW  - plant-based water stress
KW  - smart sensor
DO  - 10.3390/app11041403
ER  -
TY  - EJOU
AU  - Chamran, Mohammad K.
AU  - Yau, Kok-Lim A.
AU  - Noor, Rafidah Md.
AU  - Wu, Celimuge
TI  - An Experimental Study on D2D Route Selection Mechanism in 5G Scenarios
T2  - Electronics

PY  - 2021
VL  - 10
IS  - 4
SN  - 2079-9292

AB  - This paper demonstrates a route selection mechanism on a testbed with heterogeneous device-to-device (D2D) wireless communication for a 5G network scenario. The source node receives information about the primary users’ (PUs’) (or licensed users’) activities and available routes from the macrocell base station (or a central controller) and makes a decision to select a multihop route to the destination node. The source node from small cells can either choose: (a) a route with direct communication with the macrocell base station to improve the route performance; or (b) a route with D2D communication among nodes in the small cells to offload traffic from the macrocell to improve spectrum efficiency. The selected D2D route has the least PUs’ activities. The route selection mechanism is investigated on our testbed that helps to improve the accuracy of network performance measurement. In traditional testbeds, each node (e.g., Universal Software Radio Peripheral (USRP) that serves as the front-end communication block) is connected to a single processing unit (e.g., a personal computer) via a switch using cables. In our testbed, each USRP node is connected to a separate processing unit, i.e., raspberry Pi3 B+ (or RP3), which offers three main advantages: (a) control messages and data packets are exchanged via the wireless medium; (b) separate processing units make decisions in a distributed and heterogeneous manner; and (c) the nodes are placed further apart from one another. Therefore, in the investigation of our route selection scheme, the response delay of control message exchange and the packet loss caused by the operating environment (e.g., ambient noise) are implied in our end-to-end delay and packet delivery ratio measurement. Our results show an increase of end-to-end delay and a decrease of packet delivery ratio due to the transmission of control messages and data packets in the wireless medium in the presence of the dynamic PUs’ activities. Furthermore, D2D communication can offload 25% to 75% traffic from macrocell base station to small cells.
KW  - D2D communication
KW  - 5G
KW  - sensor network
KW  - sensor
KW  - end-to-end delay
KW  - USRP
KW  - Raspberry Pi
KW  - primary users
DO  - 10.3390/electronics10040387
ER  -
TY  - EJOU
AU  - Melo, Aurelio G.
AU  - Pinto, Milena F.
AU  - Marcato, Andre L. M.
AU  - Honório, Leonardo M.
AU  - Coelho, Fabrício O.
TI  - Dynamic Optimization and Heuristics Based Online Coverage Path Planning in 3D Environment for UAVs
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Path planning is one of the most important issues in the robotics field, being applied in many domains ranging from aerospace technology and military tasks to manufacturing and agriculture. Path planning is a branch of autonomous navigation. In autonomous navigation, dynamic decisions about the path have to be taken while the robot moves towards its goal. Among the navigation area, an important class of problems is Coverage Path Planning (CPP). The CPP technique is associated with determining a collision-free path that passes through all viewpoints in a specific area. This paper presents a method to perform CPP in 3D environment for Unmanned Aerial Vehicles (UAVs) applications, namely 3D dynamic for CPP applications (3DD-CPP). The proposed method can be deployed in an unknown environment through a combination of linear optimization and heuristics. A model to estimate cost matrices accounting for UAV power usage is proposed and evaluated for a few different flight speeds. As linear optimization methods can be computationally demanding to be used on-board a UAV, this work also proposes a distributed execution of the algorithm through fog-edge computing. Results showed that 3DD-CPP had a good performance in both local execution and fog-edge for different simulated scenarios. The proposed heuristic is capable of re-optimization, enabling execution in environments with local knowledge of the environments.
KW  - coverage path planning
KW  - 3D path planning
KW  - waypoint graph
KW  - mapping
KW  - navigation
KW  - UAVs
DO  - 10.3390/s21041108
ER  -
TY  - EJOU
AU  - Horla, Dariusz
AU  - Giernacki, Wojciech
AU  - Cieślak, Jacek
AU  - Campoy, Pascual
TI  - Altitude Measurement-Based Optimization of the Landing Process of UAVs
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - The paper addresses the loop shaping problem in the altitude control of an unmanned aerial vehicle to land the flying robot with a specific landing scenario adopted. The proposed solution is optimal, in the sense of the selected performance indices, namely minimum-time, minimum-energy, and velocity-penalized related functions, achieving their minimal values, with numerous experiments conducted throughout the development and preparation to the Mohamed Bin Zayed International Robotics Challenge (MBZIRC 2020). A novel approach to generation of a reference altitude trajectory is presented, which is then tracked in a standard, though optimized, control loop. Three landing scenarios are considered, namely: minimum-time, minimum-energy, and velocity-penalized landing scenarios. The experimental results obtained with the use of the Simulink Support Package for Parrot Minidrones, and the OptiTrack motion capture system proved the effectiveness of the proposed approach.
KW  - optimization
KW  - energy
KW  - UAV
KW  - landing
DO  - 10.3390/s21041151
ER  -
TY  - EJOU
AU  - Vangi, Elia
AU  - D’Amico, Giovanni
AU  - Francini, Saverio
AU  - Giannetti, Francesca
AU  - Lasserre, Bruno
AU  - Marchetti, Marco
AU  - Chirici, Gherardo
TI  - The New Hyperspectral Satellite PRISMA: Imagery for Forest Types Discrimination
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Different forest types based on different tree species composition may have similar spectral signatures if observed with traditional multispectral satellite sensors. Hyperspectral imagery, with a more continuous representation of their spectral behavior may instead be used for their classification. The new hyperspectral Precursore IperSpettrale della Missione Applicativa (PRISMA) sensor, developed by the Italian Space Agency, is able to capture images in a continuum of 240 spectral bands ranging between 400 and 2500 nm, with a spectral resolution smaller than 12 nm. The new sensor can be employed for a large number of remote sensing applications, including forest types discrimination. In this study, we compared the capabilities of the new PRISMA sensor against the well-known Sentinel-2 Multi-Spectral Instrument (MSI) in recognition of different forest types through a pairwise separability analysis carried out in two study areas in Italy, using two different nomenclature systems and four separability metrics. The PRISMA hyperspectral sensor, compared to Sentinel-2 MSI, allowed for a better discrimination in all forest types, increasing the performance when the complexity of the nomenclature system also increased. PRISMA achieved an average improvement of 40% for the discrimination between two forest categories (coniferous vs. broadleaves) and of 102% in the discrimination between five forest types based on main tree species groups.
KW  - PRISMA
KW  - hyperspectral sensor
KW  - hyperspectral imagery
KW  - forest types discrimination
KW  - separability analysis
DO  - 10.3390/s21041182
ER  -
TY  - EJOU
AU  - Zhang, Xiuwei
AU  - Zhou, Yang
AU  - Jin, Jiaojiao
AU  - Wang, Yafei
AU  - Fan, Minhao
AU  - Wang, Ning
AU  - Zhang, Yanning
TI  - ICENETv2: A Fine-Grained River Ice Semantic Segmentation Network Based on UAV Images
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Accurate ice segmentation is one of the most crucial techniques for intelligent ice monitoring. Compared with ice segmentation, it can provide more information for ice situation analysis, change trend prediction, and so on. Therefore, the study of ice segmentation has important practical significance. In this study, we focused on fine-grained river ice segmentation using unmanned aerial vehicle (UAV) images. This has the following difficulties: (1) The scale of river ice varies greatly in different images and even in the same image; (2) the same kind of river ice differs greatly in color, shape, texture, size, and so on; and (3) the appearances of different kinds of river ice sometimes appear similar due to the complex formation and change procedure. Therefore, to perform this study, the NWPU_YRCC2 dataset was built, in which all UAV images were collected in the Ningxia–Inner Mongolia reach of the Yellow River. Then, a novel semantic segmentation method based on deep convolution neural network, named ICENETv2, is proposed. To achieve multiscale accurate prediction, we design a multilevel features fusion framework, in which multi-scale high-level semantic features and lower-level finer features are effectively fused. Additionally, a dual attention module is adopted to highlight distinguishable characteristics, and a learnable up-sampling strategy is further used to improve the segmentation accuracy of the details. Experiments show that ICENETv2 achieves the state-of-the-art on the NWPU_YRCC2 dataset. Finally, our ICENETv2 is also applied to solve a realistic problem, calculating drift ice cover density, which is one of the most important factors to predict the freeze-up data of the river. The results demonstrate that the performance of ICENETv2 meets the actual application demand.
KW  - fine-grained river ice
KW  - position attention
KW  - channel attention
KW  - drift ice cover density
KW  - semantic segmentation
DO  - 10.3390/rs13040633
ER  -
TY  - EJOU
AU  - Kopačková-Strnadová, Veronika
AU  - Koucká, Lucie
AU  - Jelének, Jan
AU  - Lhotáková, Zuzana
AU  - Oulehle, Filip
TI  - Canopy Top, Height and Photosynthetic Pigment Estimation Using Parrot Sequoia Multispectral Imagery and the Unmanned Aerial Vehicle (UAV)
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Remote sensing is one of the modern methods that have significantly developed over the last two decades and, nowadays, it provides a new means for forest monitoring. High spatial and temporal resolutions are demanded for the accurate and timely monitoring of forests. In this study, multi-spectral Unmanned Aerial Vehicle (UAV) images were used to estimate canopy parameters (definition of crown extent, top, and height, as well as photosynthetic pigment contents). The UAV images in Green, Red, Red-Edge, and Near infrared (NIR) bands were acquired by Parrot Sequoia camera over selected sites in two small catchments (Czech Republic) covered dominantly by Norway spruce monocultures. Individual tree extents, together with tree tops and heights, were derived from the Canopy Height Model (CHM). In addition, the following were tested: (i) to what extent can the linear relationship be established between selected vegetation indexes (Normalized Difference Vegetation Index (NDVI) and NDVIred edge) derived for individual trees and the corresponding ground truth (e.g., biochemically assessed needle photosynthetic pigment contents) and (ii) whether needle age selection as a ground truth and crown light conditions affect the validity of linear models. The results of the conducted statistical analysis show that the two vegetation indexes (NDVI and NDVIred edge) tested here have the potential to assess photosynthetic pigments in Norway spruce forests at a semi-quantitative level; however, the needle-age selection as a ground truth was revealed to be a very important factor. The only usable results were obtained for linear models when using the second year needle pigment contents as a ground truth. On the other hand, the illumination conditions of the crown proved to have very little effect on the model’s validity. No study was found to directly compare these results conducted on coniferous forest stands. This shows that there is a further need for studies dealing with a quantitative estimation of the biochemical variables of nature coniferous forests when employing spectral data that were acquired by the UAV platform at a very high spatial resolution.
KW  - UAV
KW  - Parrot Sequoia multispectral camera
KW  - photosynthetic pigments
KW  - Norway spruce
KW  - forest
KW  - linear models
KW  - ground truth
KW  - needle age
KW  - crown detection
DO  - 10.3390/rs13040705
ER  -
TY  - EJOU
AU  - Bohlin, Jonas
AU  - Wallerman, Jörgen
AU  - Fransson, Johan E. S.
TI  - Extraction of Spectral Information from Airborne 3D Data for Assessment of Tree Species Proportions
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - With the rapid development of photogrammetric software and accessible camera technology, land surveys and other mapping organizations now provide various point cloud and digital surface model products from aerial images, often including spectral information. In this study, methods for colouring the point cloud and the importance of different metrics were compared for tree species-specific estimates at a coniferous hemi-boreal test site in southern Sweden. A total of three different data sets of aerial image-based products and one multi-spectral lidar data set were used to estimate tree species-specific proportion and stem volume using an area-based approach. Metrics were calculated for 156 field plots (10 m radius) from point cloud data and used in a Random Forest analysis. Plot level accuracy was evaluated using leave-one-out cross-validation. The results showed small differences in estimation accuracy of species-specific variables between the colouring methods. Simple averages of the spectral metrics had the highest importance and using spectral data from two seasons improved species prediction, especially deciduous proportion. Best tree species-specific proportion was estimated using multi-spectral lidar with 0.22 root mean square error (RMSE) for pine, 0.22 for spruce and 0.16 for deciduous. Corresponding RMSE for aerial images was 0.24, 0.23 and 0.20 for pine, spruce and deciduous, respectively. For the species-specific stem volume at plot level using image data, the RMSE in percent of surveyed mean was 129% for pine, 60% for spruce and 118% for deciduous.
KW  - aerial images
KW  - multi-spectral lidar
KW  - Optec Titan
KW  - photogrammetry
KW  - species-specific proportion
KW  - stem volume
KW  - UltraCam
DO  - 10.3390/rs13040720
ER  -
TY  - EJOU
AU  - Liu, Feng
AU  - Dai, Shuling
AU  - Zhao, Yongjia
TI  - Learning to Have a Civil Aircraft Take Off under Crosswind Conditions by Reinforcement Learning with Multimodal Data and Preprocessing Data
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Autopilot technology in the field of aviation has developed over many years. However, it is difficult for an autopilot system to autonomously operate a civil aircraft under bad weather conditions. In this paper, we present a reinforcement learning (RL) algorithm using multimodal data and preprocessing data to have a civil aircraft take off autonomously under crosswind conditions. The multimodal data include the common flight status and visual information. The preprocessing is a new design that maps some flight data by nonlinear functions based on the general flight dynamics before these data are fed into the RL model. Extensive experiments under different crosswind conditions with a professional flight simulator demonstrate that the proposed method can effectively control a civil aircraft to take off under various crosswind conditions and achieve better performance than trials without visual information or preprocessing data.
KW  - autopilot
KW  - civil aircraft
KW  - multimodal data
KW  - reinforcement learning
KW  - preprocessing
DO  - 10.3390/s21041386
ER  -
TY  - EJOU
AU  - Gao, Bowen
AU  - Chen, Ninghua
AU  - Blaschke, Thomas
AU  - Wu, Chase Q.
AU  - Chen, Jianyu
AU  - Xu, Yaochen
AU  - Yang, Xiaoping
AU  - Du, Zhenhong
TI  - Automated Characterization of Yardangs Using Deep Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - The morphological characteristics of yardangs are the direct evidence that reveals the wind and fluvial erosion for lacustrine sediments in arid areas. These features can be critical indicators in reconstructing local wind directions and environment conditions. Thus, the fast and accurate extraction of yardangs is key to studying their regional distribution and evolution process. However, the existing automated methods to characterize yardangs are of limited generalization that may only be feasible for specific types of yardangs in certain areas. Deep learning methods, which are superior in representation learning, provide potential solutions for mapping yardangs with complex and variable features. In this study, we apply Mask region-based convolutional neural networks (Mask R-CNN) to automatically delineate and classify yardangs using very high spatial resolution images from Google Earth. The yardang field in the Qaidam Basin, northwestern China is selected to conduct the experiments and the method yields mean average precisions of 0.869 and 0.671 for intersection of union (IoU) thresholds of 0.5 and 0.75, respectively. The manual validation results on images of additional study sites show an overall detection accuracy of 74%, while more than 90% of the detected yardangs can be correctly classified and delineated. We then conclude that Mask R-CNN is a robust model to characterize multi-scale yardangs of various types and allows for the research of the morphological and evolutionary aspects of aeolian landform.
KW  - aeolian landform
KW  - yardang
KW  - morphological characteristic
KW  - deep learning
KW  - Mask R-CNN
KW  - Google Earth imagery
DO  - 10.3390/rs13040733
ER  -
TY  - EJOU
AU  - Nieto-Julián, Juan E.
AU  - Lara, Lenin
AU  - Moyano, Juan
TI  - Implementation of a TeamWork-HBIM for the Management and Sustainability of Architectural Heritage
T2  - Sustainability

PY  - 2021
VL  - 13
IS  - 4
SN  - 2071-1050

AB  - The benefits of Building Information Modelling (BIM) accrue from the needs of the interoperability of applied technologies. This scope is strongly related to heritage buildings. Protection plans encompassing phases of heritage conservation, interpretation, intervention and dissemination could lead to a sustainable model through a TeamWork-HBIM project. This work develops a step by step semantically enriched 3D model, from accurate data acquisition to the creation of a container of artistic assets. TeamWork-HBIM acts as a database for movable assets, i.e., parametric objects (GDL) with graphical and semantic information, which are valid for recording, inventory and cataloguing processes. Thus, heritage properties were created and used to create recording and inventory sheets related to movable assets. Consequently, a parametric object was edited in the HBIM project, so a new category called “Heritage Furniture” was available. Data from the monitoring of the artistic asset were included in that category. In addition, the specialist technicians from the TeamWork-HBIM team catalogued a dataset related to artistic, historical and conservation properties. Another advantage of the system was the reliability of the structure of the HBIM project, which was based on the actual geometry of the building provided by the point clouds. The information was valid for both modelling works and specialists in virtual monitoring. Moreover, the reliability of metadata was collected in a common data environment (CDE), which was available for everyone. As a result, the Teamwork-HBIM-CDE project meets the needs of private institutions, such as the Foundation of the Church of the Company of Jesus in Quito, related to the sustainability of the historic site. This sustainability is shown by the implementation of a methodology that strengthens the interdisciplinary information flow by including all disciplines of historical heritage.
KW  - HBIM
KW  - sustainability
KW  - cataloguing
KW  - scanning laser
KW  - scan-to-BIM
KW  - intervention in the architectural heritage
KW  - TeamWork project
KW  - Quito
KW  - Church of the Company of Jesus
DO  - 10.3390/su13042161
ER  -
TY  - EJOU
AU  - Blekos, Kostas
AU  - Tsakas, Anastasios
AU  - Xouris, Christos
AU  - Evdokidis, Ioannis
AU  - Alexandropoulos, Dimitris
AU  - Alexakos, Christos
AU  - Katakis, Sofoklis
AU  - Makedonas, Andreas
AU  - Theoharatos, Christos
AU  - Lalos, Aris
TI  - Analysis, Modeling and Multi-Spectral Sensing for the Predictive Management of Verticillium Wilt in Olive Groves
T2  - Journal of Sensor and Actuator Networks

PY  - 2021
VL  - 10
IS  - 1
SN  - 2224-2708

AB  - The intensification and expansion in the cultivation of olives have contributed to the significant spread of Verticillium wilt, which is the most important fungal problem affecting olive trees. Recent studies confirm that practices such as the use of innovative natural minerals (Zeoshell ZF1) and the application of beneficial microorganisms (Micosat F BS WP) restore health in infected trees. However, for their efficient implementation the above methodologies require the marking of trees in the early stages of infestation—a task that is impractical with traditional means (manual labor) but also very difficult, as early stages are difficult to perceive with the naked eye. In this paper, we present the results of the My Olive Grove Coach (MyOGC) project, which used multispectral imaging from unmanned aerial vehicles to develop an olive grove monitoring system based on the autonomous and automatic processing of the multispectral images using computer vision and machine learning techniques. The goal of the system is to monitor and assess the health of olive groves, help in the prediction of Verticillium wilt spread and implement a decision support system that guides the farmer/agronomist.
KW  - precision agriculture
KW  - intelligent management
KW  - multi-spectral sensing
KW  - multi-spectral co-registration
KW  - multi-spectral fusion of multispectral spectroscopy data
DO  - 10.3390/jsan10010015
ER  -
TY  - EJOU
AU  - Liao, Kuo-Chien
AU  - Lu, Jau-Huai
TI  - Using UAV to Detect Solar Module Fault Conditions of a Solar Power Farm with IR and Visual Image Analysis
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 4
SN  - 2076-3417

AB  - In recent years, solar energy has been regarded as one of the most important sustainable energy sources. Under the rapid and large-scale construction of solar farms, the maintenance and inspection of the health conditions of solar modules in a large solar farm become an important issue. This article proposes a method for detecting solar cell faults with unmanned aerial vehicle (UAV) equipped with a thermal imager and a visible light camera, and providing a fast and reliable detection method. The detection process includes a new concept of real-time monitoring of the detected area and analysis of the health of solar panels. An image process is proposed that may quickly and accurately detect the abnormality of a solar module. The whole process includes grayscale conversion, filtering, 3-D temperature representation, probability density function, and cumulative density function analysis. Ten cases in real fields have been studied with this process, including large scale solar farms and small size solar modules installed on buildings. Results show that the cumulative density function is a convenient way to determine the health status of the solar panel and may provide maintenance personnel a basis for determining whether replacement of solar cells is necessary for improving the overall power generation efficiency and simplify the maintenance process. It is worth noting that image recognition can increase the clarity of IR images and the cumulative chart can judge the defect rate of the cell. These two methods were combined to provide an instant, fast and accurate defect judgment.
KW  - UAV
KW  - solar farms
KW  - IR images
KW  - probability density function
KW  - cumulative distribution function
DO  - 10.3390/app11041835
ER  -
TY  - EJOU
AU  - Ribeiro, Roberto
AU  - Ramos, João
AU  - Safadinho, David
AU  - Reis, Arsénio
AU  - Rabadão, Carlos
AU  - Barroso, João
AU  - Pereira, António
TI  - Web AR Solution for UAV Pilot Training and Usability Testing
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Data and services are available anywhere at any time thanks to the Internet and mobile devices. Nowadays, there are new ways of representing data through trendy technologies such as augmented reality (AR), which extends our perception of reality through the addition of a virtual layer on top of real-time images. The great potential of unmanned aerial vehicles (UAVs) for carrying out routine and professional tasks has encouraged their use in the creation of several services, such as package delivery or industrial maintenance. Unfortunately, drone piloting is difficult to learn and requires specific training. Since regular training is performed with virtual simulations, we decided to propose a multiplatform cloud-hosted solution based in Web AR for drone training and usability testing. This solution defines a configurable trajectory through virtual elements represented over barcode markers placed on a real environment. The main goal is to provide an inclusive and accessible training solution which could be used by anyone who wants to learn how to pilot or test research related to UAV control. For this paper, we reviewed drones, AR, and human–drone interaction (HDI) to propose an architecture and implement a prototype, which was built using a Raspberry Pi 3, a camera, and barcode markers. The validation was conducted using several test scenarios. The results show that a real-time AR experience for drone pilot training and usability testing is achievable through web technologies. Some of the advantages of this approach, compared to traditional methods, are its high availability by using the web and other ubiquitous devices; the minimization of technophobia related to crashes; and the development of cost-effective alternatives to train pilots and make the testing phase easier for drone researchers and developers through trendy technologies.
KW  - augmented reality
KW  - AR obstacle courses
KW  - human–drone interaction
KW  - marker-based AR
KW  - unmanned aerial vehicles
KW  - UAV control interfaces
KW  - UAV pilot training
KW  - Web AR
DO  - 10.3390/s21041456
ER  -
TY  - EJOU
AU  - Li, Guoming
AU  - Huang, Yanbo
AU  - Chen, Zhiqian
AU  - Chesser, Gary D.
AU  - Purswell, Joseph L.
AU  - Linhoss, John
AU  - Zhao, Yang
TI  - Practices and Applications of Convolutional Neural Network-Based Computer Vision Systems in Animal Farming: A Review
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 4
SN  - 1424-8220

AB  - Convolutional neural network (CNN)-based computer vision systems have been increasingly applied in animal farming to improve animal management, but current knowledge, practices, limitations, and solutions of the applications remain to be expanded and explored. The objective of this study is to systematically review applications of CNN-based computer vision systems on animal farming in terms of the five deep learning computer vision tasks: image classification, object detection, semantic/instance segmentation, pose estimation, and tracking. Cattle, sheep/goats, pigs, and poultry were the major farm animal species of concern. In this research, preparations for system development, including camera settings, inclusion of variations for data recordings, choices of graphics processing units, image preprocessing, and data labeling were summarized. CNN architectures were reviewed based on the computer vision tasks in animal farming. Strategies of algorithm development included distribution of development data, data augmentation, hyperparameter tuning, and selection of evaluation metrics. Judgment of model performance and performance based on architectures were discussed. Besides practices in optimizing CNN-based computer vision systems, system applications were also organized based on year, country, animal species, and purposes. Finally, recommendations on future research were provided to develop and improve CNN-based computer vision systems for improved welfare, environment, engineering, genetics, and management of farm animals.
KW  - deep learning
KW  - convolutional neural network
KW  - computer vision system
KW  - animal farming
DO  - 10.3390/s21041492
ER  -
TY  - EJOU
AU  - Neupane, Bipul
AU  - Horanont, Teerayut
AU  - Aryal, Jagannath
TI  - Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.
KW  - deep learning
KW  - remote sensing
KW  - review
KW  - semantic segmentation
KW  - urban image classification
DO  - 10.3390/rs13040808
ER  -
TY  - EJOU
AU  - Qi, Haixia
AU  - Liang, Yu
AU  - Ding, Quanchen
AU  - Zou, Jun
TI  - Automatic Identification of Peanut-Leaf Diseases Based on Stack Ensemble
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 4
SN  - 2076-3417

AB  - Peanut is an important food crop, and diseases of its leaves can directly reduce its yield and quality. In order to solve the problem of automatic identification of peanut-leaf diseases, this paper uses a traditional machine-learning method to ensemble the output of a deep learning model to identify diseases of peanut leaves. The identification of peanut-leaf diseases included healthy leaves, rust disease on a single leaf, leaf-spot disease on a single leaf, scorch disease on a single leaf, and both rust disease and scorch disease on a single leaf. Three types of data-augmentation methods were used: image flipping, rotation, and scaling. In this experiment, the deep-learning model had a higher accuracy than the traditional machine-learning methods. Moreover, the deep-learning model achieved better performance when using data augmentation and a stacking ensemble. After ensemble by logistic regression, the accuracy of residual network with 50 layers (ResNet50) was as high as 97.59%, and the F1 score of dense convolutional network with 121 layers (DenseNet121) was as high as 90.50. The deep-learning model used in this experiment had the greatest improvement in F1 score after the logistic regression ensemble. Deep-learning networks with deeper network layers like ResNet50 and DenseNet121 performed better in this experiment. This study can provide a reference for the identification of peanut-leaf diseases.
KW  - peanut-leaf diseases
KW  - deep learning
KW  - convolutional neural network
KW  - identification
DO  - 10.3390/app11041950
ER  -
TY  - EJOU
AU  - Megahed, Yasmine
AU  - Shaker, Ahmed
AU  - Yan, Wai Y.
TI  - Fusion of Airborne LiDAR Point Clouds and Aerial Images for Heterogeneous Land-Use Urban Mapping
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 4
SN  - 2072-4292

AB  - The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors.
KW  - urban land-use
KW  - LiDAR-aerial integration
KW  - LiDAR-aerial geo-registration
KW  - LiDAR classification
KW  - supervised machine learning
KW  - maximum likelihood
KW  - support vector machines
KW  - neural networks
KW  - bootstrap aggregation
KW  - k-fold cross-validation
DO  - 10.3390/rs13040814
ER  -
TY  - EJOU
AU  - Bonci, Andrea
AU  - Cen Cheng, Pangcheng  David
AU  - Indri, Marina
AU  - Nabissi, Giacomo
AU  - Sibona, Fiorella
TI  - Human-Robot Perception in Industrial Environments: A Survey
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.
KW  - human-robot perception
KW  - human-robot collaboration
KW  - collision detection
KW  - human action recognition
KW  - collision avoidance
KW  - machine vision
KW  - 3D sensors
KW  - robot guidance
DO  - 10.3390/s21051571
ER  -
TY  - EJOU
AU  - Bajić, Milan
AU  - Bajić, Milan
TI  - Modeling and Simulation of Very High Spatial Resolution UXOs and Landmines in a Hyperspectral Scene for UAV Survey
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - This paper presents methods for the modeling and simulation of explosive target placement in terrain spectral images (i.e., real hyperspectral 90-channel VNIR data), considering unexploded ordnances, landmines, and improvised explosive devices. The models used for landmine detection operate at sub-pixel levels. The presented research uses very fine spatial resolutions, 0.945 × 0.945 mm for targets and 1.868 × 1.868 cm for the scene, where the number of target pixels ranges from 52 to 116. While previous research has used the mean spectral value of the target, it is omitted in this paper. The model considers the probability of detection and its confidence intervals, which are derived and used in the analysis of the considered explosive targets. The detection results are better when decreased target endmembers are used to match the scene resolution, rather than using endmembers at the full resolution of the target. Unmanned aerial vehicles, as carriers of snapshot hyperspectral cameras, enable flexible target resolution selection and good area coverage.
KW  - explosive devices
KW  - hyperspectral data
KW  - simulation
KW  - Spectral Angle Mapping
KW  - UAV
DO  - 10.3390/rs13050837
ER  -
TY  - EJOU
AU  - Safonova, Anastasiia
AU  - Guirado, Emilio
AU  - Maglinets, Yuriy
AU  - Alcaraz-Segura, Domingo
AU  - Tabik, Siham
TI  - Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask R-CNN
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.
KW  - instance segmentation
KW  - machine learning
KW  - deep neural networks
KW  - olive trees
KW  - ultra-high resolution images
DO  - 10.3390/s21051617
ER  -
TY  - EJOU
AU  - Papić, Vladan
AU  - Šolić, Petar
AU  - Milan, Ante
AU  - Gotovac, Sven
AU  - Polić, Miljenko
TI  - High-Resolution Image Transmission from UAV to Ground Station for Search and Rescue Missions Planning
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 5
SN  - 2076-3417

AB  - Search and rescue (SAR) missions comprise search for, and provision of aid to people who are in distress or imminent danger. Providing the best possible input for the planners and search teams, up-to-date information about the terrain is of essential importance because every additional hour needed to search a person decreases probability of success. Therefore, availability of aerial images and updated terrain maps as a basis for planning and monitoring SAR missions in real-time is very important for rescuers. In this paper, we present a system for transmission of high-resolution images from an unmanned aerial vehicle (UAV) to the ground station (GS). We define and calculate data rate and transmission distance requirements between the UAV and GS in a mission scenario. Five tests were designed and carried out to confirm the viability of the proposed system architecture and modules. Test results present throughput measurements for various UAV and GS distances, antenna heights and UAV antenna yaw angles. Experimental results from the series of conducted outdoor tests show that the proposed solution using two pMDDL2450 datalinks at 2.4 GHz and a directional antenna on the receiving side can be used for a real-time transmission of high-resolution images acquired with a camera on a UAV. Achieved throughput at a UAV-GS distance of 5 km was 1.4 MB/s (11.2 Mbps). The limitations and possible improvements of the proposed system as well as future work are also discussed.
KW  - search and rescue
KW  - high-resolution images transmission
KW  - throughput
KW  - data link
DO  - 10.3390/app11052105
ER  -
TY  - EJOU
AU  - Sadeghi-Tehran, Pouria
AU  - Virlet, Nicolas
AU  - Hawkesford, Malcolm J.
TI  - A Neural Network Method for Classification of Sunlit and Shaded Components of Wheat Canopies in the Field Using High-Resolution Hyperspectral Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - (1) Background: Information rich hyperspectral sensing, together with robust image analysis, is providing new research pathways in plant phenotyping. This combination facilitates the acquisition of spectral signatures of individual plant organs as well as providing detailed information about the physiological status of plants. Despite the advances in hyperspectral technology in field-based plant phenotyping, little is known about the characteristic spectral signatures of shaded and sunlit components in wheat canopies. Non-imaging hyperspectral sensors cannot provide spatial information; thus, they are not able to distinguish the spectral reflectance differences between canopy components. On the other hand, the rapid development of high-resolution imaging spectroscopy sensors opens new opportunities to investigate the reflectance spectra of individual plant organs which lead to the understanding of canopy biophysical and chemical characteristics. (2) Method: This study reports the development of a computer vision pipeline to analyze ground-acquired imaging spectrometry with high spatial and spectral resolutions for plant phenotyping. The work focuses on the critical steps in the image analysis pipeline from pre-processing to the classification of hyperspectral images. In this paper, two convolutional neural networks (CNN) are employed to automatically map wheat canopy components in shaded and sunlit regions and to determine their specific spectral signatures. The first method uses pixel vectors of the full spectral features as inputs to the CNN model and the second method integrates the dimension reduction technique known as linear discriminate analysis (LDA) along with the CNN to increase the feature discrimination and improves computational efficiency. (3) Results: The proposed technique alleviates the limitations and lack of separability inherent in existing pre-defined hyperspectral classification methods. It optimizes the use of hyperspectral imaging and ensures that the data provide information about the spectral characteristics of the targeted plant organs, rather than the background. We demonstrated that high-resolution hyperspectral imagery along with the proposed CNN model can be powerful tools for characterizing sunlit and shaded components of wheat canopies in the field. The presented method will provide significant advances in the determination and relevance of spectral properties of shaded and sunlit canopy components under natural light conditions.
KW  - hyperspectral imaging
KW  - phenotyping
KW  - hyperspectral image classification (HSI)
KW  - wheat canopies
KW  - segmentation
KW  - near infrared
DO  - 10.3390/rs13050898
ER  -
TY  - EJOU
AU  - Munaye, Yirga Y.
AU  - Juang, Rong-Terng
AU  - Lin, Hsin-Piao
AU  - Tarekegn, Getaneh B.
AU  - Lin, Ding-Bing
TI  - Deep Reinforcement Learning Based Resource Management in UAV-Assisted IoT Networks
T2  - Applied Sciences

PY  - 2021
VL  - 11
IS  - 5
SN  - 2076-3417

AB  - The resource management in wireless networks with massive Internet of Things (IoT) users is one of the most crucial issues for the advancement of fifth-generation networks. The main objective of this study is to optimize the usage of resources for IoT networks. Firstly, the unmanned aerial vehicle is considered to be a base station for air-to-ground communications. Secondly, according to the distribution and fluctuation of signals; the IoT devices are categorized into urban and suburban clusters. This clustering helps to manage the environment easily. Thirdly, real data collection and preprocessing tasks are carried out. Fourthly, the deep reinforcement learning approach is proposed as a main system development scheme for resource management. Fifthly, K-means and round-robin scheduling algorithms are applied for clustering and managing the users’ resource requests, respectively. Then, the TensorFlow (python) programming tool is used to test the overall capability of the proposed method. Finally, this paper evaluates the proposed approach with related works based on different scenarios. According to the experimental findings, our proposed scheme shows promising outcomes. Moreover, on the evaluation tasks, the outcomes show rapid convergence, suitable for heterogeneous IoT networks, and low complexity.
KW  - wireless resource management
KW  - deep reinforcement learning
KW  - unmanned aerial vehicles
KW  - wireless networks
DO  - 10.3390/app11052163
ER  -
TY  - EJOU
AU  - Ali, Luqman
AU  - Alnajjar, Fady
AU  - Jassmi, Hamad A.
AU  - Gocho, Munkhjargal
AU  - Khan, Wasif
AU  - Serhani, M. A.
TI  - Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.
KW  - automatic inspection
KW  - convolutional neural networks
KW  - crack detection
KW  - deep learning
KW  - transfer learning
DO  - 10.3390/s21051688
ER  -
TY  - EJOU
AU  - Najafi, Payam
AU  - Feizizadeh, Bakhtiar
AU  - Navid, Hossein
TI  - A Comparative Approach of Fuzzy Object Based Image Analysis and Machine Learning Techniques Which Are Applied to Crop Residue Cover Mapping by Using Sentinel-2 Satellite and UAV Imagery
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - Conservation tillage methods through leaving the crop residue cover (CRC) on the soil surface protect it from water and wind erosions. Hence, the percentage of the CRC on the soil surface is very critical for the evaluation of tillage intensity. The objective of this study was to develop a new methodology based on the semiautomated fuzzy object based image analysis (fuzzy OBIA) and compare its efficiency with two machine learning algorithms which include: support vector machine (SVM) and artificial neural network (ANN) for the evaluation of the previous CRC and tillage intensity. We also considered the spectral images from two remotely sensed platforms of the unmanned aerial vehicle (UAV) and Sentinel-2 satellite, respectively. The results indicated that fuzzy OBIA for multispectral Sentinel-2 image based on Gaussian membership function with overall accuracy and Cohen’s kappa of 0.920 and 0.874, respectively, surpassed machine learning algorithms and represented the useful results for the classification of tillage intensity. The results also indicated that overall accuracy and Cohen’s kappa for the classification of RGB images from the UAV using fuzzy OBIA method were 0.860 and 0.779, respectively. The semiautomated fuzzy OBIA clearly outperformed machine learning approaches in estimating the CRC and the classification of the tillage methods and also it has the potential to substitute or complement field techniques.
KW  - fuzzy object based approach
KW  - neural network
KW  - support vector machine
KW  - tillage intensity
KW  - soil erosion
DO  - 10.3390/rs13050937
ER  -
TY  - EJOU
AU  - Xue, Yongan
AU  - Zhao, Jinling
AU  - Zhang, Mingmei
TI  - A Watershed-Segmentation-Based Improved Algorithm for Extracting Cultivated Land Boundaries
T2  - Remote Sensing

PY  - 2021
VL  - 13
IS  - 5
SN  - 2072-4292

AB  - To accurately extract cultivated land boundaries based on high-resolution remote sensing imagery, an improved watershed segmentation algorithm was proposed herein based on a combination of pre- and post-improvement procedures. Image contrast enhancement was used as the pre-improvement, while the color distance of the Commission Internationale de l´Eclairage (CIE) color space, including the Lab and Luv, was used as the regional similarity measure for region merging as the post-improvement. Furthermore, the area relative error criterion (δA), the pixel quantity error criterion (δP), and the consistency criterion (Khat) were used for evaluating the image segmentation accuracy. The region merging in Red–Green–Blue (RGB) color space was selected to compare the proposed algorithm by extracting cultivated land boundaries. The validation experiments were performed using a subset of Chinese Gaofen-2 (GF-2) remote sensing image with a coverage area of 0.12 km2. The results showed the following: (1) The contrast-enhanced image exhibited an obvious gain in terms of improving the image segmentation effect and time efficiency using the improved algorithm. The time efficiency increased by 10.31%, 60.00%, and 40.28%, respectively, in the RGB, Lab, and Luv color spaces. (2) The optimal segmentation and merging scale parameters in the RGB, Lab, and Luv color spaces were C for minimum areas of 2000, 1900, and 2000, and D for a color difference of 1000, 40, and 40. (3) The algorithm improved the time efficiency of cultivated land boundary extraction in the Lab and Luv color spaces by 35.16% and 29.58%, respectively, compared to the RGB color space. The extraction accuracy was compared to the RGB color space using the δA, δP, and Khat, that were improved by 76.92%, 62.01%, and 16.83%, respectively, in the Lab color space, while they were 55.79%, 49.67%, and 13.42% in the Luv color space. (4) Through the visual comparison, time efficiency, and segmentation accuracy, the comprehensive extraction effect using the proposed algorithm was obviously better than that of RGB color-based space algorithm. The established accuracy evaluation indicators were also proven to be consistent with the visual evaluation. (5) The proposed method has a satisfying transferability by a wider test area with a coverage area of 1 km2. In addition, the proposed method, based on the image contrast enhancement, was to perform the region merging in the CIE color space according to the simulated immersion watershed segmentation results. It is a useful attempt for the watershed segmentation algorithm to extract cultivated land boundaries, which provides a reference for enhancing the watershed algorithm.
KW  - cultivated land
KW  - watershed segmentation algorithm
KW  - image contrast enhancement
KW  - region merging
KW  - CIE color space
KW  - Lab
KW  - Luv
DO  - 10.3390/rs13050939
ER  -
TY  - EJOU
AU  - Müezzinoğlu, Taha
AU  - Karaköse, Mehmet
TI  - An Intelligent Human–Unmanned Aerial Vehicle Interaction Approach in Real Time Based on Machine Learning Using Wearable Gloves
T2  - Sensors

PY  - 2021
VL  - 21
IS  - 5
SN  - 1424-8220

AB  - The interactions between humans and unmanned aerial vehicles (UAVs), whose applications are increasing in the civilian field rather than for military purposes, are a popular future research area. Human–UAV interactions are a challenging problem because UAVs move in a three-dimensional space. In this paper, we present an intelligent human–UAV interaction approach in real time based on machine learning using wearable gloves. The proposed approach offers scientific contributions such as a multi-mode command structure, machine-learning-based recognition, task scheduling algorithms, real-time usage, robust and effective use, and high accuracy rates. For this purpose, two wearable smart gloves working in real time were designed. The signal data obtained from the gloves were processed with machine-learning-based methods and classified multi-mode commands were included in the human–UAV interaction process via the interface according to the task scheduling algorithm to facilitate sequential and fast operation. The performance of the proposed approach was verified on a data set created using 25 different hand gestures from 20 different people. In a test using the proposed approach on 49,000 datapoints, process time performance of a few milliseconds was achieved with approximately 98 percent accuracy.
KW  - human–UAV interaction
KW  - wearable technologies
KW  - Internet of Things (IoT)
KW  - human–computer interaction
KW  - smart systems
DO  - 10.3390/s21051766
ER  -
