TY  - EJOU
AU  - Ma, Lingfei
AU  - Li, Ying
AU  - Li, Jonathan
AU  - Wang, Cheng
AU  - Wang, Ruisheng
AU  - Chapman, Michael A.
TI  - Mobile Laser Scanned Point-Clouds for Road Object Detection and Extraction: A Review
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - The mobile laser scanning (MLS) technique has attracted considerable attention for providing high-density, high-accuracy, unstructured, three-dimensional (3D) geo-referenced point-cloud coverage of the road environment. Recently, there has been an increasing number of applications of MLS in the detection and extraction of urban objects. This paper presents a systematic review of existing MLS related literature. This paper consists of three parts. Part 1 presents a brief overview of the state-of-the-art commercial MLS systems. Part 2 provides a detailed analysis of on-road and off-road information inventory methods, including the detection and extraction of on-road objects (e.g., road surface, road markings, driving lines, and road crack) and off-road objects (e.g., pole-like objects and power lines). Part 3 presents a refined integrated analysis of challenges and future trends. Our review shows that MLS technology is well proven in urban object detection and extraction, since the improvement of hardware and software accelerate the efficiency and accuracy of data collection and processing. When compared to other review papers focusing on MLS applications, we review the state-of-the-art road object detection and extraction methods using MLS data and discuss their performance and applicability. The main contribution of this review demonstrates that the MLS systems are suitable for supporting road asset inventory, ITS-related applications, high-definition maps, and other highly accurate localization services.
KW  - mobile laser scanning (MLS)
KW  - point cloud
KW  - road surface
KW  - road marking
KW  - driving line
KW  - road crack
KW  - traffic sign
KW  - street light
KW  - tree
KW  - power line
KW  - deep learning
DO  - 10.3390/rs10101531
TY  - EJOU
AU  - Luo, Lei
AU  - Wang, Xinyuan
AU  - Guo, Huadong
AU  - Lasaponara, Rosa
AU  - Shi, Pilong
AU  - Bachagha, Nabil
AU  - Li, Li
AU  - Yao, Ya
AU  - Masini, Nicola
AU  - Chen, Fulong
AU  - Ji, Wei
AU  - Cao, Hui
AU  - Li, Chao
AU  - Hu, Ningke
TI  - Google Earth as a Powerful Tool for Archaeological and Cultural Heritage Applications: A Review
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - Google Earth (GE), a large Earth-observation data-based geographical information computer application, is an intuitive three-dimensional virtual globe. It enables archaeologists around the world to communicate and share their multisource data and research findings. Different from traditional geographical information systems (GIS), GE is free and easy to use in data collection, exploration, and visualization. In the past decade, many peer-reviewed articles on the use of GE in the archaeological cultural heritage (ACH) research field have been published. Most of these concern specific ACH investigations with a wide spatial coverage. GE can often be used to survey and document ACH so that both skilled archaeologists and the public can more easily and intuitively understand the results. Based on geographical tools and multi-temporal very high-resolution (VHR) satellite imagery, GE has been shown to provide spatio-temporal change information that has a bearing on the physical, environmental, and geographical character of ACH. In this review, in order to discuss the huge potential of GE, a comprehensive review of GE and its applications to ACH in the published scientific literature is first presented; case studies in five main research fields demonstrating how GE can be deployed as a key tool for studying ACH are then described. The selected case studies illustrate how GE can be used effectively to investigate ACH at multiple scales, discover new archaeological sites in remote regions, monitor historical sites, and assess damage in areas of conflict, and promote virtual tourism. These examples form the basis for highlighting current trends in remote sensing archaeology based on the GE platform, which could provide access to a low-cost and easy-to-use tool for communicating and sharing ACH geospatial data more effectively to the general public in the era of Digital Earth. Finally, a discussion of the merits and limitations of GE is presented along with conclusions and remaining challenges.
KW  - Google Earth (GE)
KW  - archaeological
KW  - cultural heritage
KW  - remote sensing
KW  - Keyhole Markup Language
KW  - very high-resolution (VHR)
KW  - virtual
DO  - 10.3390/rs10101558
TY  - EJOU
AU  - Huang, Huasheng
AU  - Deng, Jizhong
AU  - Lan, Yubin
AU  - Yang, Aqing
AU  - Deng, Xiaoling
AU  - Wen, Sheng
AU  - Zhang, Huihui
AU  - Zhang, Yali
TI  - Accurate Weed Mapping and Prescription Map Generation Based on Fully Convolutional Networks Using UAV Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - Chemical control is necessary in order to control weed infestation and to ensure a rice yield. However, excessive use of herbicides has caused serious agronomic and environmental problems. Site specific weed management (SSWM) recommends an appropriate dose of herbicides according to the weed coverage, which may reduce the use of herbicides while enhancing their chemical effects. In the context of SSWM, the weed cover map and prescription map must be generated in order to carry out the accurate spraying. In this paper, high resolution unmanned aerial vehicle (UAV) imagery were captured over a rice field. Different workflows were evaluated to generate the weed cover map for the whole field. Fully convolutional networks (FCN) was applied for a pixel-level classification. Theoretical analysis and practical evaluation were carried out to seek for an architecture improvement and performance boost. A chessboard segmentation process was used to build the grid framework of the prescription map. The experimental results showed that the overall accuracy and mean intersection over union (mean IU) for weed mapping using FCN-4s were 0.9196 and 0.8473, and the total time (including the data collection and data processing) required to generate the weed cover map for the entire field (50 &times; 60 m) was less than half an hour. Different weed thresholds (0.00&ndash;0.25, with an interval of 0.05) were used for the prescription map generation. High accuracies (above 0.94) were observed for all of the threshold values, and the relevant herbicide saving ranged from 58.3% to 70.8%. All of the experimental results demonstrated that the method used in this work has the potential to produce an accurate weed cover map and prescription map in SSWM applications.
KW  - UAV
KW  - semantic labeling
KW  - FCN
KW  - weed mapping
KW  - prescription map
DO  - 10.3390/s18103299
TY  - EJOU
AU  - Tayara, Hilal
AU  - Chong, Kil T.
TI  - Object Detection in Very High-Resolution Aerial Images Using One-Stage Densely Connected Feature Pyramid Network
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - Object detection in very high-resolution (VHR) aerial images is an essential step for a wide range of applications such as military applications, urban planning, and environmental management. Still, it is a challenging task due to the different scales and appearances of the objects. On the other hand, object detection task in VHR aerial images has improved remarkably in recent years due to the achieved advances in convolution neural networks (CNN). Most of the proposed methods depend on a two-stage approach, namely: a region proposal stage and a classification stage such as Faster R-CNN. Even though two-stage approaches outperform the traditional methods, their optimization is not easy and they are not suitable for real-time applications. In this paper, a uniform one-stage model for object detection in VHR aerial images has been proposed. In order to tackle the challenge of different scales, a densely connected feature pyramid network has been proposed by which high-level multi-scale semantic feature maps with high-quality information are prepared for object detection. This work has been evaluated on two publicly available datasets and outperformed the current state-of-the-art results on both in terms of mean average precision (mAP) and computation time.
KW  - Aerial images
KW  - convolution neural network (CNN)
KW  - deep learning
KW  - feature pyramid network
KW  - focal loss
KW  - object detection
DO  - 10.3390/s18103341
TY  - EJOU
AU  - Opromolla, Roberto
AU  - Fasano, Giancarmine
AU  - Accardo, Domenico
TI  - A Vision-Based Approach to UAV Detection and Tracking in Cooperative Applications
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - This paper presents a visual-based approach that allows an Unmanned Aerial Vehicle (UAV) to detect and track a cooperative flying vehicle autonomously using a monocular camera. The algorithms are based on template matching and morphological filtering, thus being able to operate within a wide range of relative distances (i.e., from a few meters up to several tens of meters), while ensuring robustness against variations of illumination conditions, target scale and background. Furthermore, the image processing chain takes full advantage of navigation hints (i.e., relative positioning and own-ship attitude estimates) to improve the computational efficiency and optimize the trade-off between correct detections, false alarms and missed detections. Clearly, the required exchange of information is enabled by the cooperative nature of the formation through a reliable inter-vehicle data-link. Performance assessment is carried out by exploiting flight data collected during an ad hoc experimental campaign. The proposed approach is a key building block of cooperative architectures designed to improve UAV navigation performance either under nominal GNSS coverage or in GNSS-challenging environments.
KW  - unmanned aerial vehicles
KW  - visual detection
KW  - visual tracking
KW  - template matching
KW  - morphological filtering
KW  - cooperative UAV applications
KW  - autonomous navigation
DO  - 10.3390/s18103391
TY  - EJOU
AU  - Chen, Hongyi
AU  - Zhang, Fan
AU  - Tang, Bo
AU  - Yin, Qiang
AU  - Sun, Xian
TI  - Slim and Efficient Neural Network Design for Resource-Constrained SAR Target Recognition
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - Deep convolutional neural networks (CNN) have been recently applied to synthetic aperture radar (SAR) for automatic target recognition (ATR) and have achieved state-of-the-art results with significantly improved recognition performance. However, the training period of deep CNN is long, and the size of the network is huge, sometimes reaching hundreds of megabytes. These two factors of deep CNN hinders its practical implementation and deployment in real-time SAR platforms that are typically resource-constrained. To address this challenge, this paper presents three strategies of network compression and acceleration to decrease computing and memory resource dependencies while maintaining a competitive accuracy. First, we introduce a new weight-based network pruning and adaptive architecture squeezing method to reduce the network storage and the time of inference and training process, meanwhile maintain a balance between compression ratio and classification accuracy. Then we employ weight quantization and coding to compress the network storage space. Due to the fact that the amount of calculation is mainly reflected in the convolution layer, a fast approach for pruned convolutional layers is proposed to reduce the number of multiplication by exploiting the sparsity in the activation inputs and weights. Experimental results show that the convolutional neural networks for SAR-ATR can be compressed by     40 &times;     without loss of accuracy, and the number of multiplication can be reduced by     15 &times;    . Combining these strategies, we can easily load the network in resource-constrained platforms, speed up the inference process to get the results in real-time or even retrain a more suitable network with new image data in a specific situation.
KW  - deep learning
KW  - synthetic aperture radar (SAR)
KW  - automatic target recognition (ATR)
KW  - model compression
KW  - fast algorithm
DO  - 10.3390/rs10101618
TY  - EJOU
AU  - Feng, Yi
AU  - Zhang, Cong
AU  - Baek, Stanley
AU  - Rawashdeh, Samir
AU  - Mohammadi, Alireza
TI  - Autonomous Landing of a UAV on a Moving Platform Using Model Predictive Control
T2  - Drones

PY  - 2018
VL  - 2
IS  - 4
SN  - 2504-446X

AB  - Developing methods for autonomous landing of an unmanned aerial vehicle (UAV) on a mobile platform has been an active area of research over the past decade, as it offers an attractive solution for cases where rapid deployment and recovery of a fleet of UAVs, continuous flight tasks, extended operational ranges, and mobile recharging stations are desired. In this work, we present a new autonomous landing method that can be implemented on micro UAVs that require high-bandwidth feedback control loops for safe landing under various uncertainties and wind disturbances. We present our system architecture, including dynamic modeling of the UAV with a gimbaled camera, implementation of a Kalman filter for optimal localization of the mobile platform, and development of model predictive control (MPC), for guidance of UAVs. We demonstrate autonomous landing with an error of less than 37 cm from the center of a mobile platform traveling at a speed of up to 12 m/s under the condition of noisy measurements and wind disturbances.
KW  - quadcopter
KW  - drone
KW  - Kalman filter
KW  - vision-based guidance system
KW  - autonomous vehicle
KW  - unmanned aerial vehicle
KW  - model predictive control
KW  - aerospace control
DO  - 10.3390/drones2040034
TY  - EJOU
AU  - Xu, Yifan
AU  - Ren, Guochun
AU  - Chen, Jin
AU  - Zhang, Xiaobo
AU  - Jia, Luliang
AU  - Kong, Lijun
TI  - Interference-Aware Cooperative Anti-Jamming Distributed Channel Selection in UAV Communication Networks
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 10
SN  - 2076-3417

AB  - This paper investigates the cooperative anti-jamming distributed channel selection problem in UAV communication networks. Considering the existence of malicious jamming and co-channel interference, we design an interference-aware cooperative anti-jamming scheme for the purpose of maximizing users&rsquo; utilities. Moreover, the channel switching cost and cooperation cost are introduced, which have a great impact on users&rsquo; utilities. Users in the UAV group sense the co-channel interference signal energy to judge whether they are influenced by co-channel interference. When the received co-channel interference signal energy is lower than the co-channel interference threshold, users conduct channel selection strategies independently. Otherwise, users cooperate with each other and take joint actions with a cooperative anti-jamming pattern under the impact of co-channel interference. Aiming at the independent anti-jamming channel selection problem under no co-channel interference, a Markov decision process framework is introduced, whereas for the cooperative anti-jamming channel selection case under the influence of co-channel mutual interference, a Markov game framework is employed. Furthermore, motivated by Q-learning with a &ldquo;cooperation-decision-feedback-adjustment&rdquo; idea, we design an interference-aware cooperative anti-jamming distributed channel selection algorithm (ICADCSA) to obtain the optimal anti-jamming channel strategies for users in a distributed way. In addition, a discussion on the quick decision for UAVs is conducted. Finally, simulation results show that the proposed algorithm converges to a stable solution with which the UAV group can avoid malicious jamming, as well as co-channel interference effectively and can realize a quick decision in high mobility UAV communication networks.
KW  - interference-aware
KW  - cooperative anti-jamming
KW  - Markov decision process
KW  - Markov game
KW  - Q-learning
DO  - 10.3390/app8101911
TY  - EJOU
AU  - Kim, Byunghyun
AU  - Cho, Soojin
TI  - Automated Vision-Based Detection of Cracks on Concrete Surfaces Using a Deep Learning Technique
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - At present, a number of computer vision-based crack detection techniques have been developed to efficiently inspect and manage a large number of structures. However, these techniques have not replaced visual inspection, as they have been developed under near-ideal conditions and not in an on-site environment. This article proposes an automated detection technique for crack morphology on concrete surface under an on-site environment based on convolutional neural networks (CNNs). A well-known CNN, AlexNet is trained for crack detection with images scraped from the Internet. The training set is divided into five classes involving cracks, intact surfaces, two types of similar patterns of cracks, and plants. A comparative study evaluates the successfulness of the detailed surface categorization. A probability map is developed using a softmax layer value to add robustness to sliding window detection and a parametric study was carried out to determine its threshold. The applicability of the proposed method is evaluated on images taken from the field and real-time video frames taken using an unmanned aerial vehicle. The evaluation results confirm the high adoptability of the proposed method for crack inspection in an on-site environment.
KW  - crack
KW  - deep learning
KW  - convolutional neural networks
KW  - AlexNet
KW  - unmanned aerial vehicle
DO  - 10.3390/s18103452
TY  - EJOU
AU  - Ruan, Lang
AU  - Chen, Jin
AU  - Guo, Qiuju
AU  - Zhang, Xiaobo
AU  - Zhang, Yuli
AU  - Liu, Dianxiong
TI  - Group Buying-Based Data Transmission in Flying Ad-Hoc Networks: A Coalition Game Approach
T2  - Information

PY  - 2018
VL  - 9
IS  - 10
SN  - 2078-2489

AB  - In scenarios such as natural disasters and military strikes, it is common for unmanned aerial vehicles (UAVs) to form groups to execute reconnaissance and surveillance. To ensure the effectiveness of UAV communications, repeated resource acquisition issues and transmission mechanism designs need to be addressed urgently. Since large-scale UAVs will generate high transmission overhead due to the overlapping resource requirements, in this paper, we propose a resource allocation optimization method based on distributed data content in a Flying Ad-hoc network (FANET). The resource allocation problem with the goal of throughput maximization is constructed as a coalition game framework. Firstly, a data transmission mechanism is designed for UAVs to execute information interaction within the coalitions. Secondly, a novel mechanism of coalition selection based on group-buying is investigated for UAV coalitions to acquire data from the central UAV. The data transmission and coalition selection problem are modeled as coalition graph game and coalition formation game, respectively. Through the design of the utility function, we prove that both games have stable solutions. We also prove the convergence of the proposed approach with coalition order and Pareto order. Based on simulation results, coalition order based coalition selection algorithm (CO-CSA) and Pareto order based coalition selection algorithm (PO-CSA) are proposed to explore the stable coalition partition of system model. CO-CSA and PO-CSA can achieve higher data throughput than the contrast onetime coalition selection algorithm (Onetime-CSA) (at least increased by 34.5% and 16.9%, respectively). Besides, although PO-CSA has relatively lower throughput gain, its convergence times is on average 50.9% less than that of CO-CSA, which means that the algorithm choice is scenario-dependent.
KW  - coalition formation game
KW  - coalition graph game
KW  - data transmission
KW  - Nash equilibrium
KW  - resource allocation
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/info9100253
TY  - EJOU
AU  - Zhan, Tianming
AU  - Sun, Le
AU  - Xu, Yang
AU  - Yang, Guowei
AU  - Zhang, Yan
AU  - Wu, Zebin
TI  - Hyperspectral Classification via Superpixel Kernel Learning-Based Low Rank Representation
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - High dimensional image classification is a fundamental technique for information retrieval from hyperspectral remote sensing data. However, data quality is readily affected by the atmosphere and noise in the imaging process, which makes it difficult to achieve good classification performance. In this paper, multiple kernel learning-based low rank representation at superpixel level (Sp_MKL_LRR) is proposed to improve the classification accuracy for hyperspectral images. Superpixels are generated first from the hyperspectral image to reduce noise effect and form homogeneous regions. An optimal superpixel kernel parameter is then selected by the kernel matrix using a multiple kernel learning framework. Finally, a kernel low rank representation is applied to classify the hyperspectral image. The proposed method offers two advantages. (1) The global correlation constraint is exploited by the low rank representation, while the local neighborhood information is extracted as the superpixel kernel adaptively learns the high-dimensional manifold features of the samples in each class; (2) It can meet the challenges of multiscale feature learning and adaptive parameter determination in the conventional kernel methods. Experimental results on several hyperspectral image datasets demonstrate that the proposed method outperforms several state-of-the-art classifiers tested in terms of overall accuracy, average accuracy, and kappa statistic.
KW  - hyperspectral image
KW  - classification
KW  - superpixel kernel
KW  - multiple kernel learning
KW  - low rank representation
DO  - 10.3390/rs10101639
TY  - EJOU
AU  - Mozgeris, Gintautas
AU  - Juodkienė, Vytautė
AU  - Jonikavičius, Donatas
AU  - Straigytė, Lina
AU  - Gadal, Sébastien
AU  - Ouerghemmi, Walid
TI  - Ultra-Light Aircraft-Based Hyperspectral and Colour-Infrared Imaging to Identify Deciduous Tree Species in an Urban Environment
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - One may consider the application of remote sensing as a trade-off between the imaging platforms, sensors, and data gathering and processing techniques. This study addresses the potential of hyperspectral imaging using ultra-light aircraft for vegetation species mapping in an urban environment, exploring both the engineering and scientific aspects related to imaging platform design and image classification methods. An imaging system based on simultaneous use of Rikola frame format hyperspectral and Nikon D800E adopted colour infrared cameras installed onboard a Bekas X32 manned ultra-light aircraft is introduced. Two test imaging flight missions were conducted in July of 2015 and September of 2016 over a 4000 ha area in Kaunas City, Lithuania. Sixteen and 64 spectral bands in 2015 and 2016, respectively, in a spectral range of 500&ndash;900 nm were recorded with colour infrared images. Three research questions were explored assessing the identification of six deciduous tree species: (1) Pre-treatment of spectral features for classification, (2) testing five conventional machine learning classifiers, and (3) fusion of hyperspectral and colour infrared images. Classification performance was assessed by applying leave-one-out cross-validation at the individual crown level and using as a reference at least 100 field inventoried trees for each species. The best-performing classification algorithm&mdash;multilayer perceptron, using all spectral properties extracted from the hyperspectral images&mdash;resulted in a moderate classification accuracy. The overall classification accuracy was 63%, Cohen&rsquo;s Kappa was 0.54, and the species-specific classification accuracies were in the range of 51&ndash;72%. Hyperspectral images resulted in significantly better tree species classification ability than the colour infrared images and simultaneous use of spectral properties extracted from hyperspectral and colour infrared images improved slightly the accuracy over the 2015 image. Even though classifications using hyperspectral data cubes of 64 bands resulted in relatively larger accuracies than with 16 bands, classification error matrices were not statistically different. Alternative imaging platforms (like an unmanned aerial vehicle and a Cessna 172 aircraft) and settings of the flights were discussed using simulated imaging projects assuming the same study area and field of application. Ultra-light aircraft-based hyperspectral and colour-infrared imaging was considered to be a technically and economically sound solution for urban green space inventories to facilitate tree mapping, characterization, and monitoring.
KW  - hyperspectral
KW  - colour infrared
KW  - ultra-light aircraft
KW  - urban trees
KW  - classification
DO  - 10.3390/rs10101668
TY  - EJOU
AU  - Tu, Yu-Hsuan
AU  - Phinn, Stuart
AU  - Johansen, Kasper
AU  - Robson, Andrew
TI  - Assessing Radiometric Correction Approaches for Multi-Spectral UAS Imagery for Horticultural Applications
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Multi-spectral imagery captured from unmanned aerial systems (UAS) is becoming increasingly popular for the improved monitoring and managing of various horticultural crops. However, for UAS-based data to be used as an industry standard for assessing tree structure and condition as well as production parameters, it is imperative that the appropriate data collection and pre-processing protocols are established to enable multi-temporal comparison. There are several UAS-based radiometric correction methods commonly used for precision agricultural purposes. However, their relative accuracies have not been assessed for data acquired in complex horticultural environments. This study assessed the variations in estimated surface reflectance values of different radiometric corrections applied to multi-spectral UAS imagery acquired in both avocado and banana orchards. We found that inaccurate calibration panel measurements, inaccurate signal-to-reflectance conversion, and high variation in geometry between illumination, surface, and sensor viewing produced significant radiometric variations in at-surface reflectance estimates. Potential solutions to address these limitations included appropriate panel deployment, site-specific sensor calibration, and appropriate bidirectional reflectance distribution function (BRDF) correction. Future UAS-based horticultural crop monitoring can benefit from the proposed solutions to radiometric corrections to ensure they are using comparable image-based maps of multi-temporal biophysical properties.
KW  - unmanned aerial system
KW  - multi-spectral imagery
KW  - radiometric correction
KW  - bidirectional reflectance distribution function
KW  - horticulture
DO  - 10.3390/rs10111684
TY  - EJOU
AU  - Bah, M D.
AU  - Hafiane, Adel
AU  - Canals, Raphael
TI  - Deep Learning with Unsupervised Data Labeling for Weed Detection in Line Crops in UAV Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - In recent years, weeds have been responsible for most agricultural yield losses. To deal with this threat, farmers resort to spraying the fields uniformly with herbicides. This method not only requires huge quantities of herbicides but impacts the environment and human health. One way to reduce the cost and environmental impact is to allocate the right doses of herbicide to the right place and at the right time (precision agriculture). Nowadays, unmanned aerial vehicles (UAVs) are becoming an interesting acquisition system for weed localization and management due to their ability to obtain images of the entire agricultural field with a very high spatial resolution and at a low cost. However, despite significant advances in UAV acquisition systems, the automatic detection of weeds remains a challenging problem because of their strong similarity to the crops. Recently, a deep learning approach has shown impressive results in different complex classification problems. However, this approach needs a certain amount of training data, and creating large agricultural datasets with pixel-level annotations by an expert is an extremely time-consuming task. In this paper, we propose a novel fully automatic learning method using convolutional neuronal networks (CNNs) with an unsupervised training dataset collection for weed detection from UAV images. The proposed method comprises three main phases. First, we automatically detect the crop rows and use them to identify the inter-row weeds. In the second phase, inter-row weeds are used to constitute the training dataset. Finally, we perform CNNs on this dataset to build a model able to detect the crop and the weeds in the images. The results obtained are comparable to those of traditional supervised training data labeling, with differences in accuracy of 1.5% in the spinach field and 6% in the bean field.
KW  - weed detection
KW  - deep learning
KW  - unmanned aerial vehicle
KW  - image processing
KW  - precision agriculture
KW  - crop line detection
DO  - 10.3390/rs10111690
TY  - EJOU
AU  - Zhang, Pengbin
AU  - Ke, Yinghai
AU  - Zhang, Zhenxin
AU  - Wang, Mingli
AU  - Li, Peng
AU  - Zhang, Shuangyue
TI  - Urban Land Use and Land Cover Classification Using Novel Deep Learning Models Based on High Spatial Resolution Satellite Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Urban land cover and land use mapping plays an important role in urban planning and management. In this paper, novel multi-scale deep learning models, namely ASPP-Unet and ResASPP-Unet are proposed for urban land cover classification based on very high resolution (VHR) satellite imagery. The proposed ASPP-Unet model consists of a contracting path which extracts the high-level features, and an expansive path, which up-samples the features to create a high-resolution output. The atrous spatial pyramid pooling (ASPP) technique is utilized in the bottom layer in order to incorporate multi-scale deep features into a discriminative feature. The ResASPP-Unet model further improves the architecture by replacing each layer with residual unit. The models were trained and tested based on WorldView-2 (WV2) and WorldView-3 (WV3) imageries over the city of Beijing. Model parameters including layer depth and the number of initial feature maps (IFMs) as well as the input image bands were evaluated in terms of their impact on the model performances. It is shown that the ResASPP-Unet model with 11 layers and 64 IFMs based on 8-band WV2 imagery produced the highest classification accuracy (87.1% for WV2 imagery and 84.0% for WV3 imagery). The ASPP-Unet model with the same parameter setting produced slightly lower accuracy, with overall accuracy of 85.2% for WV2 imagery and 83.2% for WV3 imagery. Overall, the proposed models outperformed the state-of-the-art models, e.g., U-Net, convolutional neural network (CNN) and Support Vector Machine (SVM) model over both WV2 and WV3 images, and yielded robust and efficient urban land cover classification results.
KW  - urban land cover classification
KW  - high spatial resolution satellite imagery
KW  - deep learning
KW  - U-Net
KW  - CNN
DO  - 10.3390/s18113717
TY  - EJOU
AU  - Kuffer, Monika
AU  - Wang, Jiong
AU  - Nagenborg, Michael
AU  - Pfeffer, Karin
AU  - Kohli, Divyani
AU  - Sliuzas, Richard
AU  - Persello, Claudio
TI  - The Scope of Earth-Observation to Improve the Consistency of the SDG Slum Indicator
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 11
SN  - 2220-9964

AB  - The continuous increase in deprived living conditions in many cities of the Global South contradicts efforts to make cities inclusive, safe, resilient, and sustainable places. Using examples of Asian, African, and Latin American cities, this study shows the scope and limits of earth observation (EO)-based mapping of deprived living conditions in support of providing consistent global information for the SDG indicator 11.1.1 “proportion of urban population living in slums, informal settlements or inadequate housing”. At the technical level, we compare several EO-based methods and imagery for mapping deprived living conditions, discussing their ability to map such areas including differences in terms of accuracy and performance at the city scale. At the operational level, we compare available municipal maps showing identified deprived areas with the spatial extent of morphological mapped areas of deprived living conditions (using EO) at the city scale, discussing the reasons for inconsistencies between municipal and EO-based maps. We provide an outlook on how EO-based mapping of deprived living conditions could contribute to a global spatial information base to support targeting of deprived living conditions in support of the SDG Goal 11.1.1 indicator, when uncertainties and ethical considerations on data provision are well addressed.
KW  - deprived living conditions
KW  - slum
KW  - informal settlement
KW  - inadequate housing
KW  - Sustainable Development Goals (SDGs)
KW  - remote sensing
KW  - global urban data
KW  - uncertainties
KW  - geo-ethics
DO  - 10.3390/ijgi7110428
TY  - EJOU
AU  - Valentino, Rico
AU  - Jung, Woo-Sung
AU  - Ko, Young-Bae
TI  - A Design and Simulation of the Opportunistic Computation Offloading with Learning-Based Prediction for Unmanned Aerial Vehicle (UAV) Clustering Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Drones have recently become extremely popular, especially in military and civilian applications. Examples of drone utilization include reconnaissance, surveillance, and packet delivery. As time has passed, drones’ tasks have become larger and more complex. As a result, swarms or clusters of drones are preferred, because they offer more coverage, flexibility, and reliability. However, drone systems have limited computing power and energy resources, which means that sometimes it is difficult for drones to finish their tasks on schedule. A solution to this is required so that drone clusters can complete their work faster. One possible solution is an offloading scheme between drone clusters. In this study, we propose an opportunistic computational offloading system, which allows for a drone cluster with a high intensity task to borrow computing resources opportunistically from other nearby drone clusters. We design an artificial neural network-based response time prediction module for deciding whether it is faster to finish tasks by offloading them to other drone clusters. The offloading scheme is conducted only if the predicted offloading response time is smaller than the local computing time. Through simulation results, we show that our proposed scheme can decrease the response time of drone clusters through an opportunistic offloading process.
KW  - drone cluster
KW  - computation offloading
KW  - neural network
KW  - wireless communication
DO  - 10.3390/s18113751
TY  - EJOU
AU  - Cui, Jun-hui
AU  - Wei, Rui-xuan
AU  - Liu, Zong-cheng
AU  - Zhou, Kai
TI  - UAV Motion Strategies in Uncertain Dynamic Environments: A Path Planning Method Based on Q-Learning Strategy
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 11
SN  - 2076-3417

AB  - A solution framework for UAV motion strategies in uncertain dynamic environments is constructed in this paper. Considering that the motion states of UAV might be influenced by some dynamic uncertainties, such as control strategies, flight environments, and any other bursting-out threats, we model the uncertain factors that might cause such influences to the path planning of the UAV, unified as an unobservable part of the system and take the acceleration together with the bank angle of the UAV as a control variable. Meanwhile, the cost function is chosen based on the tracking error, then the control instructions and flight path for UAV can be achieved. Then, the cost function can be optimized through Q-learning, and the best UAV action sequence for conflict avoidance under the moving threat environment can be obtained. According to Bellman&rsquo;s optimization principle, the optimal action strategies can be obtained from the current confidence level. The method in this paper is more in line with the actual UAV path planning, since the generation of the path planning strategy at each moment takes into account the influence of the UAV control strategy on its motion at the next moment. The simulation results show that all the planning paths that are created according to the solution framework proposed in this paper have a very high tracking accuracy, and this method has a much shorter processing time as well as a shorter path it can create.
KW  - unmanned aerial vehicle
KW  - path planning
KW  - Q-Learning strategy
KW  - observational error
DO  - 10.3390/app8112169
TY  - EJOU
AU  - Wang, Xiaohong
AU  - Guo, Hongzhou
AU  - Wang, Jingbin
AU  - Wang, Lizhi
TI  - Predicting the Health Status of an Unmanned Aerial Vehicles Data-Link System Based on a Bayesian Network
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) require data-link system to link ground data terminals to the real-time controls of each UAV. Consequently, the ability to predict the health status of a UAV data-link system is vital for safe and efficient operations. The performance of a UAV data-link system is affected by the health status of both the hardware and UAV data-links. This paper proposes a method for predicting the health state of a UAV data-link system based on a Bayesian network fusion of information about potential hardware device failures and link failures. Our model employs the Bayesian network to describe the information and uncertainty associated with a complex multi-level system. To predict the health status of the UAV data-link, we use the health status information about the root node equipment with various life characteristics along with the health status of the links as affected by the bit error rate. In order to test the validity of the model, we tested its prediction of the health of a multi-level solar-powered unmanned aerial vehicle data-link system and the result shows that the method can quantitatively predict the health status of the solar-powered UAV data-link system. The results can provide guidance for improving the reliability of UAV data-link system and lay a foundation for predicting the health status of a UAV data-link system accurately.
KW  - UAV data-link system
KW  - Bayesian networks
KW  - health status prediction
KW  - networking mode
KW  - bit error rate
DO  - 10.3390/s18113916
TY  - EJOU
AU  - Boonpook, Wuttichai
AU  - Tan, Yumin
AU  - Ye, Yinghua
AU  - Torteeka, Peerapong
AU  - Torsri, Kritanai
AU  - Dong, Shengxian
TI  - A Deep Learning Approach on Building Detection from Unmanned Aerial Vehicle-Based Images in Riverbank Monitoring
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Buildings along riverbanks are likely to be affected by rising water levels, therefore the acquisition of accurate building information has great importance not only for riverbank environmental protection but also for dealing with emergency cases like flooding. UAV-based photographs are flexible and cloud-free compared to satellite images and can provide very high-resolution images up to centimeter level, while there exist great challenges in quickly and accurately detecting and extracting building from UAV images because there are usually too many details and distortions on UAV images. In this paper, a deep learning (DL)-based approach is proposed for more accurately extracting building information, in which the network architecture, SegNet, is used in the semantic segmentation after the network training on a completely labeled UAV image dataset covering multi-dimension urban settlement appearances along a riverbank area in Chongqing. The experiment results show that an excellent performance has been obtained in the detection of buildings from untrained locations with an average overall accuracy more than 90%. To verify the generality and advantage of the proposed method, the procedure is further evaluated by training and testing with another two open standard datasets which have a variety of building patterns and styles, and the final overall accuracies of building extraction are more than 93% and 95%, respectively.
KW  - building extraction
KW  - UAV dataset
KW  - deep learning
KW  - river bank monitoring
DO  - 10.3390/s18113921
TY  - EJOU
AU  - Zhang, Yihong
AU  - Yang, Yijin
AU  - Zhou, Wuneng
AU  - Shi, Lifeng
AU  - Li, Demin
TI  - Motion-Aware Correlation Filters for Online Visual Tracking
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - The discriminative correlation filters-based methods struggle deal with the problem of fast motion and heavy occlusion, the problem can severely degrade the performance of trackers, ultimately leading to tracking failures. In this paper, a novel Motion-Aware Correlation Filters (MACF) framework is proposed for online visual object tracking, where a motion-aware strategy based on joint instantaneous motion estimation Kalman filters is integrated into the Discriminative Correlation Filters (DCFs). The proposed motion-aware strategy is used to predict the possible region and scale of the target in the current frame by utilizing the previous estimated 3D motion information. Obviously, this strategy can prevent model drift caused by fast motion. On the base of the predicted region and scale, the MACF detects the position and scale of the target by using the DCFs-based method in the current frame. Furthermore, an adaptive model updating strategy is proposed to address the problem of corrupted models caused by occlusions, where the learning rate is determined by the confidence of the response map. The extensive experiments on popular Object Tracking Benchmark OTB-100, OTB-50 and unmanned aerial vehicles (UAV) video have demonstrated that the proposed MACF tracker performs better than most of the state-of-the-art trackers and achieves a high real-time performance. In addition, the proposed approach can be integrated easily and flexibly into other visual tracking algorithms.
KW  - visual tracking
KW  - correlation filters
KW  - motion-aware
KW  - adaptive update strategy
KW  - confidence response map
DO  - 10.3390/s18113937
TY  - EJOU
AU  - Lagkas, Thomas
AU  - Argyriou, Vasileios
AU  - Bibi, Stamatia
AU  - Sarigiannidis, Panagiotis
TI  - UAV IoT Framework Views and Challenges: Towards Protecting Drones as “Things”
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) have enormous potential in enabling new applications in various areas, ranging from military, security, medicine, and surveillance to traffic-monitoring applications. Lately, there has been heavy investment in the development of UAVs and multi-UAVs systems that can collaborate and complete missions more efficiently and economically. Emerging technologies such as 4G/5G networks have significant potential on UAVs equipped with cameras, sensors, and GPS receivers in delivering Internet of Things (IoT) services from great heights, creating an airborne domain of the IoT. However, there are many issues to be resolved before the effective use of UAVs can be made, including security, privacy, and management. As such, in this paper we review new UAV application areas enabled by the IoT and 5G technologies, analyze the sensor requirements, and overview solutions for fleet management over aerial-networking, privacy, and security challenges. Finally, we propose a framework that supports and enables these technologies on UAVs. The introduced framework provisions a holistic IoT architecture that enables the protection of UAVs as “flying” things in a collaborative networked environment.
KW  - security
KW  - privacy
KW  - drones
KW  - IoT
KW  - UAV
DO  - 10.3390/s18114015
TY  - EJOU
AU  - Yang, Tao
AU  - Ren, Qiang
AU  - Zhang, Fangbing
AU  - Xie, Bolin
AU  - Ren, Hailei
AU  - Li, Jing
AU  - Zhang, Yanning
TI  - Hybrid Camera Array-Based UAV Auto-Landing on Moving UGV in GPS-Denied Environment
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - With the rapid development of Unmanned Aerial Vehicle (UAV) systems, the autonomous landing of a UAV on a moving Unmanned Ground Vehicle (UGV) has received extensive attention as a key technology. At present, this technology is confronted with such problems as operating in GPS-denied environments, a low accuracy of target location, the poor precision of the relative motion estimation, delayed control responses, slow processing speeds, and poor stability. To address these issues, we present a hybrid camera array-based autonomous landing UAV that can land on a moving UGV in a GPS-denied environment. We first built a UAV autonomous landing system with a hybrid camera array comprising a fisheye lens camera and a stereo camera. Then, we integrated a wide Field of View (FOV) and depth imaging for locating the UGV accurately. In addition, we employed a state estimation algorithm based on motion compensation for establishing the motion state of the ground moving UGV, including its actual motion direction and speed. Thereafter, according to the characteristics of the designed system, we derived a nonlinear controller based on the UGV motion state to ensure that the UGV and UAV maintain the same motion state, which allows autonomous landing. Finally, to evaluate the performance of the proposed system, we carried out a large number of simulations in AirSim and conducted real-world experiments. Through the qualitative and quantitative analyses of the experimental results, as well as the analysis of the time performance, we verified that the autonomous landing performance of the system in the GPS-denied environment is effective and robust.
KW  - UAV autonomous landing
KW  - moving UGV
KW  - GPS-denied environment
KW  - hybrid camera array
KW  - motion compensation
DO  - 10.3390/rs10111829
TY  - EJOU
AU  - Csillik, Ovidiu
AU  - Cherbini, John
AU  - Johnson, Robert
AU  - Lyons, Andy
AU  - Kelly, Maggi
TI  - Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks
T2  - Drones

PY  - 2018
VL  - 2
IS  - 4
SN  - 2504-446X

AB  - Remote sensing is important to precision agriculture and the spatial resolution provided by Unmanned Aerial Vehicles (UAVs) is revolutionizing precision agriculture workflows for measurement crop condition and yields over the growing season, for identifying and monitoring weeds and other applications. Monitoring of individual trees for growth, fruit production and pest and disease occurrence remains a high research priority and the delineation of each tree using automated means as an alternative to manual delineation would be useful for long-term farm management. In this paper, we detected citrus and other crop trees from UAV images using a simple convolutional neural network (CNN) algorithm, followed by a classification refinement using superpixels derived from a Simple Linear Iterative Clustering (SLIC) algorithm. The workflow performed well in a relatively complex agricultural environment (multiple targets, multiple size trees and ages, etc.) achieving high accuracy (overall accuracy = 96.24%, Precision (positive predictive value) = 94.59%, Recall (sensitivity) = 97.94%). To our knowledge, this is the first time a CNN has been used with UAV multi-spectral imagery to focus on citrus trees. More of these individual cases are needed to develop standard automated workflows to help agricultural managers better incorporate large volumes of high resolution UAV imagery into agricultural management operations.
KW  - CNN
KW  - deep learning
KW  - superpixels
KW  - precision agriculture
KW  - UAS
KW  - feature extraction
KW  - citrus
KW  - tree identification
DO  - 10.3390/drones2040039
TY  - EJOU
AU  - Rahman, Muhammad M.
AU  - Robson, Andrew
AU  - Bristow, Mila
TI  - Exploring the Potential of High Resolution WorldView-3 Imagery for Estimating Yield of Mango
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Pre-harvest yield estimation of mango fruit is important for the optimization of inputs and other resources on the farm. Current industry practice of visual counting the fruit on a small number of trees for yield forecasting can be highly inaccurate due to the spatial variability, especially if the trees selected do not represent the entire crop. Therefore, this study evaluated the potential of high resolution WorldView-3 (WV3) satellite imagery to estimate yield of mango by integrating both geometric (tree crown area) and optical (spectral vegetation indices) data using artificial neural network (ANN) model. WV3 images were acquired in 2016&ndash;2017 and 2017&ndash;2018 growing seasons at the early fruit stage from three orchards in Acacia Hills region, Northern Territory, Australia. Stratified sampling technique (SST) was applied to select 18 trees from each orchard and subsequently ground truthed for yield (kg&middot;tree&minus;1) and fruit number per tree. For each sampled tree, spectral reflectance data and tree crown area (TCA) was extracted from WV3 imagery. The TCA was identified as the most important predictor of both fruit yield (kg&middot;tree&minus;1) and fruit number, followed by NDVI red-edge band when all trees from three orchards in two growing seasons were combined. The results of all sampled trees from three orchards in two growing seasons using ANN model produced a strong correlation (R2 = 0.70 and 0.68 for total fruit yield (kg&middot;tree&minus;1) and fruit number respectively), which suggest that the model can be obtained to predict yield on a regional level. On orchard level also the ANN model produced a high correlation when both growing seasons were combined. However, the model developed in one season could not be applied in another season due to the influence of seasonal variation and canopy condition. Using the relationship derived from the measured yield parameters against combined VIs and TCA data, the total fruit yield (t&middot;ha&minus;1) and fruit number were estimated for each orchard, produced 7% under estimation to less than 1% over estimation. The accuracy of the findings showed the potential of WV3 imagery to better predict the yield parameters than the current practice across the mango industry as well as to quantify lost yield as a result of delayed harvest.
KW  - WorldView-3 (WV3)
KW  - Mango (Mangifera indica)
KW  - tree crown area
KW  - yield prediction
DO  - 10.3390/rs10121866
TY  - EJOU
AU  - Morales, Giorgio
AU  - Kemper, Guillermo
AU  - Sevillano, Grace
AU  - Arteaga, Daniel
AU  - Ortega, Ivan
AU  - Telles, Joel
TI  - Automatic Segmentation of Mauritia flexuosa in Unmanned Aerial Vehicle (UAV) Imagery Using Deep Learning
T2  - Forests

PY  - 2018
VL  - 9
IS  - 12
SN  - 1999-4907

AB  - One of the most important ecosystems in the Amazon rainforest is the Mauritia flexuosa swamp or “aguajal”. However, deforestation of its dominant species, the Mauritia flexuosa palm, also known as “aguaje”, is a common issue, and conservation is poorly monitored because of the difficult access to these swamps. The contribution of this paper is twofold: the presentation of a dataset called MauFlex, and the proposal of a segmentation and measurement method for areas covered in Mauritia flexuosa palms using high-resolution aerial images acquired by UAVs. The method performs a semantic segmentation of Mauritia flexuosa using an end-to-end trainable Convolutional Neural Network (CNN) based on the Deeplab v3+ architecture. Images were acquired under different environment and light conditions using three different RGB cameras. The MauFlex dataset was created from these images and it consists of 25,248 image patches of     512 × 512     pixels and their respective ground truth masks. The results over the test set achieved an accuracy of 98.143%, specificity of 96.599%, and sensitivity of 95.556%. It is shown that our method is able not only to detect full-grown isolated Mauritia flexuosa palms, but also young palms or palms partially covered by other types of vegetation.
KW  - Mauritia flexuosa
KW  - semantic segmentation
KW  - end-to-end learning
KW  - convolutional neural network
KW  - forest inventory
DO  - 10.3390/f9120736
TY  - EJOU
AU  - Zhang, Bin
AU  - Wang, Cunpeng
AU  - Shen, Yonglin
AU  - Liu, Yueyan
TI  - Fully Connected Conditional Random Fields for High-Resolution Remote Sensing Land Use/Land Cover Classification with Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - The interpretation of land use and land cover (LULC) is an important issue in the fields of high-resolution remote sensing (RS) image processing and land resource management. Fully training a new or existing convolutional neural network (CNN) architecture for LULC classification requires a large amount of remote sensing images. Thus, fine-tuning a pre-trained CNN for LULC detection is required. To improve the classification accuracy for high resolution remote sensing images, it is necessary to use another feature descriptor and to adopt a classifier for post-processing. A fully connected conditional random fields (FC-CRF), to use the fine-tuned CNN layers, spectral features, and fully connected pairwise potentials, is proposed for image classification of high-resolution remote sensing images. First, an existing CNN model is adopted, and the parameters of CNN are fine-tuned by training datasets. Then, the probabilities of image pixels belong to each class type are calculated. Second, we consider the spectral features and digital surface model (DSM) and combined with a support vector machine (SVM) classifier, the probabilities belong to each LULC class type are determined. Combined with the probabilities achieved by the fine-tuned CNN, new feature descriptors are built. Finally, FC-CRF are introduced to produce the classification results, whereas the unary potentials are achieved by the new feature descriptors and SVM classifier, and the pairwise potentials are achieved by the three-band RS imagery and DSM. Experimental results show that the proposed classification scheme achieves good performance when the total accuracy is about 85%.
KW  - remote sensing
KW  - image classification
KW  - fully connected conditional random fields (FC-CRF)
KW  - convolutional neural networks (CNN)
DO  - 10.3390/rs10121889
TY  - EJOU
AU  - Ruan, Lang
AU  - Chen, Jin
AU  - Guo, Qiuju
AU  - Jiang, Han
AU  - Zhang, Yuli
AU  - Liu, Dianxiong
TI  - A Coalition Formation Game Approach for Efficient Cooperative Multi-UAV Deployment
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - Unmanned aerial vehicle (UAV) cooperative control has been an important issue in UAV-assisted sensor networks, thanks to the considerable benefit obtained from the cooperative mechanism of UAVs being applied as a flying base station. In a coverage scenarios, the trade-off between coverage and transmission performance often makes deployment of UAVs fall into a dilemma, since both indexes are related to the distance between UAVs. To address this issue, UAV coverage and data transmission mechanism is analyzed in this paper; then, an efficient multi-UAV cooperative deployment model is proposed. The problem is modeled as a coalition formation game (CFG). The CFG with Pareto order is proved to have a stable partition. Then, an effective approach consisting of coverage deployment and coalition selection is designed, wherein UAVs can decide strategies cooperatively to achieve better coverage performance. Combining analysis of game approach, coalition selection and the position deployment algorithm based on Pareto order (CSPDA-PO) is designed to execute coverage deployment and coalition selection. Finally, simulation results are shown to validate the proposed approach based on an efficient multi-UAV cooperative deployment model.
KW  - UAV-assisted sensor network
KW  - UAV cooperative coverage
KW  - coalition formation game
KW  - stable coalition partition
KW  - Nash equilibrium
DO  - 10.3390/app8122427
TY  - EJOU
AU  - Fu, Kun
AU  - Li, Yang
AU  - Sun, Hao
AU  - Yang, Xue
AU  - Xu, Guangluan
AU  - Li, Yuting
AU  - Sun, Xian
TI  - A Ship Rotation Detection Model in Remote Sensing Images Based on Feature Fusion Pyramid Network and Deep Reinforcement Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Ship detection plays an important role in automatic remote sensing image interpretation. The scale difference, large aspect ratio of ship, complex remote sensing image background and ship dense parking scene make the detection task difficult. To handle the challenging problems above, we propose a ship rotation detection model based on a Feature Fusion Pyramid Network and deep reinforcement learning (FFPN-RL) in this paper. The detection network can efficiently generate the inclined rectangular box for ship. First, we propose the Feature Fusion Pyramid Network (FFPN) that strengthens the reuse of different scales features, and FFPN can extract the low level location and high level semantic information that has an important impact on multi-scale ship detection and precise location of dense parking ships. Second, in order to get accurate ship angle information, we apply deep reinforcement learning to the inclined ship detection task for the first time. In addition, we put forward prior policy guidance and a long-term training method to train an angle prediction agent constructed through a dueling structure Q network, which is able to iteratively and accurately obtain the ship angle. In addition, we design soft rotation non-maximum suppression to reduce the missed ship detection while suppressing the redundant detection boxes. We carry out detailed experiments on the remote sensing ship image dataset, and the experiments validate that our FFPN-RL ship detection model has efficient detection performance.
KW  - ship detection
KW  - deep reinforcement learning
KW  - convolution neural network
KW  - feature map fusion
DO  - 10.3390/rs10121922
TY  - EJOU
AU  - He, Fangning
AU  - Zhou, Tian
AU  - Xiong, Weifeng
AU  - Hasheminnasab, Seyyed M.
AU  - Habib, Ayman
TI  - Automated Aerial Triangulation for UAV-Based Mapping
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Accurate 3D reconstruction/modelling from unmanned aerial vehicle (UAV)-based imagery has become the key prerequisite in various applications. Although current commercial software has automated the process of image-based reconstruction, a transparent system, which can be incorporated with different user-defined constraints, is still preferred by the photogrammetric research community. In this regard, this paper presents a transparent framework for the automated aerial triangulation of UAV images. The proposed framework is conducted in three steps. In the first step, two approaches, which take advantage of prior information regarding the flight trajectory, are implemented for reliable relative orientation recovery. Then, initial recovery of image exterior orientation parameters (EOPs) is achieved through either an incremental or global approach. Finally, a global bundle adjustment involving Ground Control Points (GCPs) and check points is carried out to refine all estimated parameters in the defined mapping coordinate system. Four real image datasets, which are acquired by two different UAV platforms, have been utilized to evaluate the feasibility of the proposed framework. In addition, a comparative analysis between the proposed framework and the existing commercial software is performed. The derived experimental results demonstrate the superior performance of the proposed framework in providing an accurate 3D model, especially when dealing with acquired UAV images containing repetitive pattern and significant image distortions.
KW  - unmanned aerial vehicle
KW  - 3D reconstruction
KW  - structure from motion
KW  - relative orientation
KW  - exterior orientation parameters
KW  - bundle adjustment
DO  - 10.3390/rs10121952
TY  - EJOU
AU  - Levitan, Nathaniel
AU  - Gross, Barry
TI  - Utilizing Collocated Crop Growth Model Simulations to Train Agronomic Satellite Retrieval Algorithms
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Due to its worldwide coverage and high revisit time, satellite-based remote sensing provides the ability to monitor in-season crop state variables and yields globally. In this study, we presented a novel approach to training agronomic satellite retrieval algorithms by utilizing collocated crop growth model simulations and solar-reflective satellite measurements. Specifically, we showed that bidirectional long short-term memory networks (BLSTMs) can be trained to predict the in-season state variables and yields of Agricultural Production Systems sIMulator (APSIM) maize crop growth model simulations from collocated Moderate Resolution Imaging Spectroradiometer (MODIS) 500-m satellite measurements over the United States Corn Belt at a regional scale. We evaluated the performance of the BLSTMs through both k-fold cross validation and comparison to regional scale ground-truth yields and phenology. Using k-fold cross validation, we showed that three distinct in-season maize state variables (leaf area index, aboveground biomass, and specific leaf area) can be retrieved with cross-validated R2 values ranging from 0.4 to 0.8 for significant portions of the season. Several other plant, soil, and phenological in-season state variables were also evaluated in the study for their retrievability via k-fold cross validation. In addition, by comparing to survey-based United State Department of Agriculture (USDA) ground truth data, we showed that the BLSTMs are able to predict actual county-level yields with R2 values between 0.45 and 0.6 and actual state-level phenological dates (emergence, silking, and maturity) with R2 values between 0.75 and 0.85. We believe that a potential application of this methodology is to develop satellite products to monitor in-season field-scale crop growth on a global scale by reproducing the methodology with field-scale crop growth model simulations (utilizing farmer-recorded field-scale agromanagement data) and collocated high-resolution satellite data (fused with moderate-resolution satellite data).
KW  - crop growth models
KW  - MODIS
KW  - BLSTMs
DO  - 10.3390/rs10121968
TY  - EJOU
AU  - Kang, Man-Sung
AU  - Lee, Hanju
AU  - Yim, Hong J.
AU  - An, Yun-Kyu
AU  - Kim, Dong J.
TI  - Multi-Channel Electrical Impedance-Based Crack Localization of Fiber-Reinforced Cementitious Composites under Bending Conditions
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - This study proposes a multi-channel electrical impedance-based crack localization technique of fiber-reinforced cementitious composites (FRCCs) under bending conditions. FRCCs have a self-sensing capability by adding conductive steel fibers into nonconductive cementitious composites, making it possible to measure electrical impedance without sensor installation. Moreover, FRCCs materials can be used as a structural member thanks to its own enhanced structural ductility as well as stiffness. In a structural health monitoring point of view, these characteristics make FRCCs suitable for monitoring structural hot spots, particularly where the crack is most likely to be initiated. Since the electrical impedance obtained from FRCCs is typically sensitive to environmental and operational conditions, false alarms are often triggered. The proposed technique can minimize the false alarms by using currently measured multi-path data as well as localize a crack within the sensing range. To examine the feasibility of crack localization in FRCCs, an instantaneous multi-channel electrical impedance acquisition system and a crack localization algorithm are developed. Subsequently, three-point bending tests are carried out under various temperature conditions. The validation test results reveal that cracks are successfully identified and localized even under varying temperature conditions.
KW  - fiber-reinforced cementitious composite
KW  - nondestructive testing
KW  - electrical impedance
KW  - crack localization
KW  - self-sensing concrete
KW  - structural health monitoring
DO  - 10.3390/app8122582
TY  - EJOU
AU  - Li, Jiaojiao
AU  - Xi, Bobo
AU  - Du, Qian
AU  - Song, Rui
AU  - Li, Yunsong
AU  - Ren, Guangbo
TI  - Deep Kernel Extreme-Learning Machine for the Spectral–Spatial Classification of Hyperspectral Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Extreme-learning machines (ELM) have attracted significant attention in hyperspectral image classification due to their extremely fast and simple training structure. However, their shallow architecture may not be capable of further improving classification accuracy. Recently, deep-learning-based algorithms have focused on deep feature extraction. In this paper, a deep neural network-based kernel extreme-learning machine (KELM) is proposed. Furthermore, an excellent spatial guided filter with first-principal component (GFFPC) is also proposed for spatial feature enhancement. Consequently, a new classification framework derived from the deep KELM network and GFFPC is presented to generate deep spectral and spatial features. Experimental results demonstrate that the proposed framework outperforms some state-of-the-art algorithms with very low cost, which can be used for real-time processes.
KW  - hyperspectral classification
KW  - deep layer
KW  - kernel-based ELM
KW  - spectral and spatial features
DO  - 10.3390/rs10122036
TY  - EJOU
AU  - Gstaiger, Veronika
AU  - Tian, Jiaojiao
AU  - Kiefl, Ralph
AU  - Kurz, Franz
TI  - 2D vs. 3D Change Detection Using Aerial Imagery to Support Crisis Management of Large-Scale Events
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Large-scale events represent a special challenge for crisis management. To ensure that participants can enjoy an event safely and carefree, it must be comprehensively prepared and attentively monitored. Remote sensing can provide valuable information to identify potential risks and take appropriate measures in order to prevent a disaster, or initiate emergency aid measures as quickly as possible in the event of an emergency. Especially, three-dimensional (3D) information that is derived using photogrammetry can be used to analyze the terrain and map existing structures that are set up at short notice. Using aerial imagery acquired during a German music festival in 2016 and the celebration of the German Protestant Church Assembly of 2017, the authors compare two-dimensional (2D) and novel fusion-based 3D change detection methods, and discuss their suitability for supporting large-scale events during the relevant phases of crisis management. This study serves to find out what added value the use of 3D change information can provide for on-site crisis management. Based on the results, an operational, fully automatic processor for crisis management operations and corresponding products for end users can be developed.
KW  - crisis management support
KW  - aerial imagery
KW  - large-scale event
KW  - 2D change detection
KW  - 3D change detection
DO  - 10.3390/rs10122054
TY  - EJOU
AU  - Guo, Hao
AU  - Wei, Guo
AU  - An, Jubai
TI  - Dark Spot Detection in SAR Images of Oil Spill Using Segnet
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - Damping Bragg scattering from the ocean surface is the basic underlying principle of synthetic aperture radar (SAR) oil slick detection, and they produce dark spots on SAR images. Dark spot detection is the first step in oil spill detection, which affects the accuracy of oil spill detection. However, some natural phenomena (such as waves, ocean currents, and low wind belts, as well as human factors) may change the backscatter intensity on the surface of the sea, resulting in uneven intensity, high noise, and blurred boundaries of oil slicks or lookalikes. In this paper, Segnet is used as a semantic segmentation model to detect dark spots in oil spill areas. The proposed method is applied to a data set of 4200 from five original SAR images of an oil spill. The effectiveness of the method is demonstrated through the comparison with fully convolutional networks (FCN), an initiator of semantic segmentation models, and some other segmentation methods. It is here observed that the proposed method can not only accurately identify the dark spots in SAR images, but also show a higher robustness under high noise and fuzzy boundary conditions.
KW  - image segmentation
KW  - deep learning
KW  - synthetic aperture radar (SAR)
KW  - oil slicks
KW  - segnet
DO  - 10.3390/app8122670
TY  - EJOU
AU  - Huang, Lingcao
AU  - Liu, Lin
AU  - Jiang, Liming
AU  - Zhang, Tingjun
TI  - Automatic Mapping of Thermokarst Landforms from Remote Sensing Images Using Deep Learning: A Case Study in the Northeastern Tibetan Plateau
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Thawing of ice-rich permafrost causes thermokarst landforms on the ground surface. Obtaining the distribution of thermokarst landforms is a prerequisite for understanding permafrost degradation and carbon exchange at local and regional scales. However, because of their diverse types and characteristics, it is challenging to map thermokarst landforms from remote sensing images. We conducted a case study towards automatically mapping a type of thermokarst landforms (i.e., thermo-erosion gullies) in a local area in the northeastern Tibetan Plateau from high-resolution images by the use of deep learning. In particular, we applied the DeepLab algorithm (based on Convolutional Neural Networks) to a 0.15-m-resolution Digital Orthophoto Map (created using aerial photographs taken by an Unmanned Aerial Vehicle). Here, we document the detailed processing flow with key steps including preparing training data, fine-tuning, inference, and post-processing. Validating against the field measurements and manual digitizing results, we obtained an F1 score of 0.74 (precision is 0.59 and recall is 1.0), showing that the proposed method can effectively map small and irregular thermokarst landforms. It is potentially viable to apply the designed method to mapping diverse thermokarst landforms in a larger area where high-resolution images and training data are available.
KW  - DeepLab
KW  - permafrost degradation
KW  - semantic segmentation
KW  - thermokarst landforms
KW  - thermo-erosion gullies
KW  - Tibetan Plateau
KW  - Unmanned Aerial Vehicle Images
DO  - 10.3390/rs10122067
TY  - EJOU
AU  - Rançon, Florian
AU  - Bombrun, Lionel
AU  - Keresztes, Barna
AU  - Germain, Christian
TI  - Comparison of SIFT Encoded and Deep Learning Features for the Classification and Detection of Esca Disease in Bordeaux Vineyards
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 1
SN  - 2072-4292

AB  - Grapevine wood fungal diseases such as esca are among the biggest threats in vineyards nowadays. The lack of very efficient preventive (best results using commercial products report 20% efficiency) and curative means induces huge economic losses. The study presented in this paper is centered around the in-field detection of foliar esca symptoms during summer, exhibiting a typical &ldquo;striped&rdquo; pattern. Indeed, in-field disease detection has shown great potential for commercial applications and has been successfully used for other agricultural needs such as yield estimation. Differentiation with foliar symptoms caused by other diseases or abiotic stresses was also considered. Two vineyards from the Bordeaux region (France, Aquitaine) were chosen as the basis for the experiment. Pictures of diseased and healthy vine plants were acquired during summer 2017 and labeled at the leaf scale, resulting in a patch database of around 6000 images (224 &times; 224 pixels) divided into red cultivar and white cultivar samples. Then, we tackled the classification part of the problem comparing state-of-the-art SIFT encoding and pre-trained deep learning feature extractors for the classification of database patches. In the best case, 91% overall accuracy was obtained using deep features extracted from MobileNet network trained on ImageNet database, demonstrating the efficiency of simple transfer learning approaches without the need to design an ad-hoc specific feature extractor. The third part aimed at disease detection (using bounding boxes) within full plant images. For this purpose, we integrated the deep learning base network within a &ldquo;one-step&rdquo; detection network (RetinaNet), allowing us to perform detection queries in real time (approximately six frames per second on GPU). Recall/Precision (RP) and Average Precision (AP) metrics then allowed us to evaluate the performance of the network on a 91-image (plants) validation database. Overall, 90% precision for a 40% recall was obtained while best esca AP was about 70%. Good correlation between annotated and detected symptomatic surface per plant was also obtained, meaning slightly symptomatic plants can be efficiently separated from severely attacked plants.
KW  - proximal sensing
KW  - disease detection
KW  - grapevine trunk disease
KW  - esca
KW  - SIFT
KW  - deep learning
DO  - 10.3390/rs11010001
TY  - EJOU
AU  - Wang, Yuhao
AU  - Liang, Binxiu
AU  - Ding, Meng
AU  - Li, Jiangyun
TI  - Dense Semantic Labeling with Atrous Spatial Pyramid Pooling and Decoder for High-Resolution Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 1
SN  - 2072-4292

AB  - Dense semantic labeling is significant in high-resolution remote sensing imagery research and it has been widely used in land-use analysis and environment protection. With the recent success of fully convolutional networks (FCN), various types of network architectures have largely improved performance. Among them, atrous spatial pyramid pooling (ASPP) and encoder-decoder are two successful ones. The former structure is able to extract multi-scale contextual information and multiple effective field-of-view, while the latter structure can recover the spatial information to obtain sharper object boundaries. In this study, we propose a more efficient fully convolutional network by combining the advantages from both structures. Our model utilizes the deep residual network (ResNet) followed by ASPP as the encoder and combines two scales of high-level features with corresponding low-level features as the decoder at the upsampling stage. We further develop a multi-scale loss function to enhance the learning procedure. In the postprocessing, a novel superpixel-based dense conditional random field is employed to refine the predictions. We evaluate the proposed method on the Potsdam and Vaihingen datasets and the experimental results demonstrate that our method performs better than other machine learning or deep learning methods. Compared with the state-of-the-art DeepLab_v3+ our model gains 0.4% and 0.6% improvements in overall accuracy on these two datasets respectively.
KW  - remote sensing imagery
KW  - dense semantic labeling
KW  - fully convolutional networks
KW  - atrous spatial pyramid pooling
KW  - encoder-decoder
KW  - superpixel-based DenseCRF
DO  - 10.3390/rs11010020
TY  - EJOU
AU  - Bae, Dae H.
AU  - Kim, Jae W.
AU  - Heo, Jae-Pil
TI  - Content-Aware Focal Plane Selection and Proposals for Object Tracking on Plenoptic Image Sequences
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 1
SN  - 1424-8220

AB  - Object tracking is a fundamental problem in computer vision since it is required in many practical applications including video-based surveillance and autonomous vehicles. One of the most challenging scenarios in the problem is when the target object is partially or even fully occluded by other objects. In such cases, most of existing trackers can fail in their task while the object is invisible. Recently, a few techniques have been proposed to tackle the occlusion problem by performing the tracking on plenoptic image sequences. Although they have shown promising results based on the refocusing capability of plenoptic images, there is still room for improvement. In this paper, we propose a novel focus index selection algorithm to identify an optimal focal plane where the tracking should be performed. To determine an optimal focus index, we use a focus measure to find maximally focused plane and a visual similarity to capture the plane where the target object is visible, and its appearance is distinguishably clear. We further use the selected focus index to generate proposals. Since the optimal focus index allows us to estimate the distance between the camera and the target object, we can more accurately guess the scale changes of the object in the image plane. Our proposal algorithm also takes the trajectory of the target object into account. We extensively evaluate our proposed techniques on three plenoptic image sequences by comparing them against the prior tracking methods specialized to the plenoptic image sequences. In experiments, our method provides higher accuracy and robustness over the prior art, and those results confirm that the merits of our proposed algorithms.
KW  - plenoptic imaging technique
KW  - object tracking
KW  - bounding box proposal
KW  - content-based image matching
DO  - 10.3390/s19010048
TY  - EJOU
AU  - Mahdianpari, Masoud
AU  - Salehi, Bahram
AU  - Mohammadimanesh, Fariba
AU  - Homayouni, Saeid
AU  - Gill, Eric
TI  - The First Wetland Inventory Map of Newfoundland at a Spatial Resolution of 10 m Using Sentinel-1 and Sentinel-2 Data on the Google Earth Engine Cloud Computing Platform
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 1
SN  - 2072-4292

AB  - Wetlands are one of the most important ecosystems that provide a desirable habitat for a great variety of flora and fauna. Wetland mapping and modeling using Earth Observation (EO) data are essential for natural resource management at both regional and national levels. However, accurate wetland mapping is challenging, especially on a large scale, given their heterogeneous and fragmented landscape, as well as the spectral similarity of differing wetland classes. Currently, precise, consistent, and comprehensive wetland inventories on a national- or provincial-scale are lacking globally, with most studies focused on the generation of local-scale maps from limited remote sensing data. Leveraging the Google Earth Engine (GEE) computational power and the availability of high spatial resolution remote sensing data collected by Copernicus Sentinels, this study introduces the first detailed, provincial-scale wetland inventory map of one of the richest Canadian provinces in terms of wetland extent. In particular, multi-year summer Synthetic Aperture Radar (SAR) Sentinel-1 and optical Sentinel-2 data composites were used to identify the spatial distribution of five wetland and three non-wetland classes on the Island of Newfoundland, covering an approximate area of 106,000 km2. The classification results were evaluated using both pixel-based and object-based random forest (RF) classifications implemented on the GEE platform. The results revealed the superiority of the object-based approach relative to the pixel-based classification for wetland mapping. Although the classification using multi-year optical data was more accurate compared to that of SAR, the inclusion of both types of data significantly improved the classification accuracies of wetland classes. In particular, an overall accuracy of 88.37% and a Kappa coefficient of 0.85 were achieved with the multi-year summer SAR/optical composite using an object-based RF classification, wherein all wetland and non-wetland classes were correctly identified with accuracies beyond 70% and 90%, respectively. The results suggest a paradigm-shift from standard static products and approaches toward generating more dynamic, on-demand, large-scale wetland coverage maps through advanced cloud computing resources that simplify access to and processing of the “Geo Big Data.” In addition, the resulting ever-demanding inventory map of Newfoundland is of great interest to and can be used by many stakeholders, including federal and provincial governments, municipalities, NGOs, and environmental consultants to name a few.
KW  - wetland
KW  - Google Earth Engine
KW  - Sentinel-1
KW  - Sentinel-2
KW  - random forest
KW  - cloud computing
KW  - geo-big data
DO  - 10.3390/rs11010043
TY  - EJOU
AU  - Moskalenko, Viacheslav
AU  - Moskalenko, Alona
AU  - Korobov, Artem
AU  - Semashko, Viktor
TI  - The Model and Training Algorithm of Compact Drone Autonomous Visual Navigation System
T2  - Data

PY  - 2019
VL  - 4
IS  - 1
SN  - 2306-5729

AB  - Trainable visual navigation systems based on deep learning demonstrate potential for robustness of onboard camera parameters and challenging environment. However, a deep model requires substantial computational resources and large labelled training sets for successful training. Implementation of the autonomous navigation and training-based fast adaptation to the new environment for a compact drone is a complicated task. The article describes an original model and training algorithms adapted to the limited volume of labelled training set and constrained computational resource. This model consists of a convolutional neural network for visual feature extraction, extreme-learning machine for estimating the position displacement and boosted information-extreme classifier for obstacle prediction. To perform unsupervised training of the convolution filters with a growing sparse-coding neural gas algorithm, supervised learning algorithms to construct the decision rules with simulated annealing search algorithm used for finetuning are proposed. The use of complex criterion for parameter optimization of the feature extractor model is considered. The resulting approach performs better trajectory reconstruction than the well-known ORB-SLAM. In particular, for sequence 7 from the KITTI dataset, the translation error is reduced by nearly 65.6% under the frame rate 10 frame per second. Besides, testing on the independent TUM sequence shot outdoors produces a translation error not exceeding 6% and a rotation error not exceeding 3.68 degrees per 100 m. Testing was carried out on the Raspberry Pi 3+ single-board computer.
KW  - navigation
KW  - visual odometry
KW  - convolutional neural network
KW  - neural gas
KW  - information criterion
KW  - extreme learning
DO  - 10.3390/data4010004
TY  - EJOU
AU  - Yoo, Jisang
AU  - Lee, Gyu-cheol
TI  - Moving Object Detection Using an Object Motion Reflection Model of Motion Vectors
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 1
SN  - 2073-8994

AB  - Moving object detection task can be solved by the background subtraction algorithm if the camera is fixed. However, because the background moves, detecting moving objects in a moving car is a difficult problem. There were attempts to detect moving objects using LiDAR or stereo cameras, but when the car moved, the detection rate decreased. We propose a moving object detection algorithm using an object motion reflection model of motion vectors. The proposed method first obtains the disparity map by searching the corresponding region between stereo images. Then, we estimate road by applying v-disparity method to the disparity map. The optical flow is used to acquire the motion vectors of symmetric pixels between adjacent frames where the road has been removed. We designed a probability model of how much the local motion is reflected in the motion vector to determine if the object is moving. We have experimented with the proposed method on two datasets, and confirmed that the proposed method detects moving objects with higher accuracy than other methods.
KW  - object motion detection
KW  - ego-motion
KW  - optical flow
KW  - stereo matching
KW  - RANdom SAmple Consensus (RANSAC)
DO  - 10.3390/sym11010034
TY  - EJOU
AU  - Jiménez López, Jesús
AU  - Mulero-Pázmány, Margarita
TI  - Drones for Conservation in Protected Areas: Present and Future
T2  - Drones

PY  - 2019
VL  - 3
IS  - 1
SN  - 2504-446X

AB  - Park managers call for cost-effective and innovative solutions to handle a wide variety of environmental problems that threaten biodiversity in protected areas. Recently, drones have been called upon to revolutionize conservation and hold great potential to evolve and raise better-informed decisions to assist management. Despite great expectations, the benefits that drones could bring to foster effectiveness remain fundamentally unexplored. To address this gap, we performed a literature review about the use of drones in conservation. We selected a total of 256 studies, of which 99 were carried out in protected areas. We classified the studies in five distinct areas of applications: “wildlife monitoring and management”; “ecosystem monitoring”; “law enforcement”; “ecotourism”; and “environmental management and disaster response”. We also identified specific gaps and challenges that would allow for the expansion of critical research or monitoring. Our results support the evidence that drones hold merits to serve conservation actions and reinforce effective management, but multidisciplinary research must resolve the operational and analytical shortcomings that undermine the prospects for drones integration in protected areas.
KW  - protected areas
KW  - drones
KW  - RPAS
KW  - conservation
KW  - effective management
KW  - biodiversity threats
DO  - 10.3390/drones3010010
TY  - EJOU
AU  - Yang, Shengying
AU  - Qin, Huibin
AU  - Liang, Xiaolin
AU  - Gulliver, Thomas A.
TI  - An Improved Unauthorized Unmanned Aerial Vehicle Detection Algorithm Using Radiofrequency-Based Statistical Fingerprint Analysis
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) are now readily available worldwide and users can easily fly them remotely using smart controllers. This has created the problem of keeping unauthorized UAVs away from private or sensitive areas where they can be a personal or public threat. This paper proposes an improved radio frequency (RF)-based method to detect UAVs. The clutter (interference) is eliminated using a background filtering method. Then singular value decomposition (SVD) and average filtering are used to reduce the noise and improve the signal to noise ratio (SNR). Spectrum accumulation (SA) and statistical fingerprint analysis (SFA) are employed to provide two frequency estimates. These estimates are used to determine if a UAV is present in the detection environment. The data size is reduced using a region of interest (ROI), and this improves the system efficiency and improves azimuth estimation accuracy. Detection results are obtained using real UAV RF signals obtained experimentally which show that the proposed method is more effective than other well-known detection algorithms. The recognition rate with this method is close to 100% within a distance of 2.4 km and greater than 90% within a distance of 3 km. Further, multiple UAVs can be detected accurately using the proposed method.
KW  - spectrum sensing
KW  - radio frequency (RF)
KW  - singular value decomposition (SVD)
KW  - spectrum accumulation (SA)
KW  - statistical fingerprint analysis (SFA)
DO  - 10.3390/s19020274
TY  - EJOU
AU  - Zhuo, Xiangyu
AU  - Fraundorfer, Friedrich
AU  - Kurz, Franz
AU  - Reinartz, Peter
TI  - Automatic Annotation of Airborne Images by Label Propagation Based on a Bayesian-CRF Model
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 2
SN  - 2072-4292

AB  - The tremendous advances in deep neural networks have demonstrated the superiority of deep learning techniques for applications such as object recognition or image classification. Nevertheless, deep learning-based methods usually require a large amount of training data, which mainly comes from manual annotation and is quite labor-intensive. In order to reduce the amount of manual work required for generating enough training data, we hereby propose to leverage existing labeled data to generate image annotations automatically. Specifically, the pixel labels are firstly transferred from one image modality to another image modality via geometric transformation to create initial image annotations, and then additional information (e.g., height measurements) is incorporated for Bayesian inference to update the labeling beliefs. Finally, the updated label assignments are optimized with a fully connected conditional random field (CRF), yielding refined labeling for all pixels in the image. The proposed approach is tested on two different scenarios, i.e., (1) label propagation from annotated aerial imagery to unmanned aerial vehicle (UAV) imagery and (2) label propagation from map database to aerial imagery. In each scenario, the refined image labels are used as pseudo-ground truth data for training a convolutional neural network (CNN). Results demonstrate that our model is able to produce accurate label assignments even around complex object boundaries; besides, the generated image labels can be effectively leveraged for training CNNs and achieve comparable classification accuracy as manual image annotations, more specifically, the per-class classification accuracy of the networks trained by the manual image annotations and the generated image labels have a difference within     &plusmn; 5 %    .
KW  - automatic image annotation
KW  - label propagation
KW  - Conditional Random Field (CRF)
KW  - Convolutional Neural Network (CNN)
DO  - 10.3390/rs11020145
TY  - EJOU
AU  - Gao, Pengbo
AU  - Zhang, Yan
AU  - Zhang, Linhuan
AU  - Noguchi, Ryozo
AU  - Ahamed, Tofael
TI  - Development of a Recognition System for Spraying Areas from Unmanned Aerial Vehicles Using a Machine Learning Approach
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - Unmanned aerial vehicle (UAV)-based spraying systems have recently become important for the precision application of pesticides, using machine learning approaches. Therefore, the objective of this research was to develop a machine learning system that has the advantages of high computational speed and good accuracy for recognizing spray and non-spray areas for UAV-based sprayers. A machine learning system was developed by using the mutual subspace method (MSM) for images collected from a UAV. Two target lands: agricultural croplands and orchard areas, were considered in building two classifiers for distinguishing spray and non-spray areas. The field experiments were conducted in target areas to train and test the system by using a commercial UAV (DJI Phantom 3 Pro) with an onboard 4K camera. The images were collected from low (5 m) and high (15 m) altitudes for croplands and orchards, respectively. The recognition system was divided into offline and online systems. In the offline recognition system, 74.4% accuracy was obtained for the classifiers in recognizing spray and non-spray areas for croplands. In the case of orchards, the average classifier recognition accuracy of spray and non-spray areas was 77%. On the other hand, the online recognition system performance had an average accuracy of 65.1% for croplands, and 75.1% for orchards. The computational time for the online recognition system was minimal, with an average of 0.0031 s for classifier recognition. The developed machine learning system had an average recognition accuracy of 70%, which can be implemented in an autonomous UAV spray system for recognizing spray and non-spray areas for real-time applications.
KW  - precision agriculture
KW  - recognition system
KW  - image classifiers
KW  - machine learning system
KW  - mutual subspace method
DO  - 10.3390/s19020313
TY  - EJOU
AU  - Xie, Zhuli
AU  - Chen, Yaoliang
AU  - Lu, Dengsheng
AU  - Li, Guiying
AU  - Chen, Erxue
TI  - Classification of Land Cover, Forest, and Tree Species Classes with ZiYuan-3 Multispectral and Stereo Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 2
SN  - 2072-4292

AB  - The global availability of high spatial resolution images makes mapping tree species distribution possible for better management of forest resources. Previous research mainly focused on mapping single tree species, but information about the spatial distribution of all kinds of trees, especially plantations, is often required. This research aims to identify suitable variables and algorithms for classifying land cover, forest, and tree species. Bi-temporal ZiYuan-3 multispectral and stereo images were used. Spectral responses and textures from multispectral imagery, canopy height features from bi-temporal stereo imagery, and slope and elevation from the stereo-derived digital surface model data were examined through comparative analysis of six classification algorithms including maximum likelihood classifier (MLC), k-nearest neighbor (kNN), decision tree (DT), random forest (RF), artificial neural network (ANN), and support vector machine (SVM). The results showed that use of multiple source data&mdash;spectral bands, vegetation indices, textures, and topographic factors&mdash;considerably improved land-cover and forest classification accuracies compared to spectral bands alone, which the highest overall accuracy of 84.5% for land cover classes was from the SVM, and, of 89.2% for forest classes, was from the MLC. The combination of leaf-on and leaf-off seasonal images further improved classification accuracies by 7.8% to 15.0% for land cover classes and by 6.0% to 11.8% for forest classes compared to single season spectral image. The combination of multiple source data also improved land cover classification by 3.7% to 15.5% and forest classification by 1.0% to 12.7% compared to the spectral image alone. MLC provided better land-cover and forest classification accuracies than machine learning algorithms when spectral data alone were used. However, some machine learning approaches such as RF and SVM provided better performance than MLC when multiple data sources were used. Further addition of canopy height features into multiple source data had no or limited effects in improving land-cover or forest classification, but improved classification accuracies of some tree species such as birch and Mongolia scotch pine. Considering tree species classification, Chinese pine, Mongolia scotch pine, red pine, aspen and elm, and other broadleaf trees as having classification accuracies of over 92%, and larch and birch have relatively low accuracies of 87.3% and 84.5%. However, these high classification accuracies are from different data sources and classification algorithms, and no one classification algorithm provided the best accuracy for all tree species classes. This research implies the same data source and the classification algorithm cannot provide the best classification results for different land cover classes. It is necessary to develop a comprehensive classification procedure using an expert-based approach or hierarchical-based classification approach that can employ specific data variables and algorithm for each tree species class.
KW  - tree species
KW  - classification
KW  - ZiYuan-3
KW  - stereo image
KW  - machine learning
DO  - 10.3390/rs11020164
TY  - EJOU
AU  - Liu, Wei
AU  - Cheng, Dayu
AU  - Yin, Pengcheng
AU  - Yang, Mengyuan
AU  - Li, Erzhu
AU  - Xie, Meng
AU  - Zhang, Lianpeng
TI  - Small Manhole Cover Detection in Remote Sensing Imagery with Deep Convolutional Neural Networks
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 1
SN  - 2220-9964

AB  - With the development of remote sensing technology and the advent of high-resolution images, obtaining data has become increasingly convenient. However, the acquisition of small manhole cover information still has shortcomings including low efficiency of manual surveying and high leakage rate. Recently, deep learning models, especially deep convolutional neural networks (DCNNs), have proven to be effective at object detection. However, several challenges limit the applications of DCNN in manhole cover object detection using remote sensing imagery: (1) Manhole cover objects often appear at different scales in remotely sensed images and DCNNs&rsquo; fixed receptive field cannot match the scale variability of such objects; (2) Manhole cover objects in large-scale remotely-sensed images are relatively small in size and densely packed, while DCNNs have poor localization performance when applied to such objects. To address these problems, we propose an effective method for detecting manhole cover objects in remotely-sensed images. First, we redesign the feature extractor by adopting the visual geometry group (VGG), which can increase the variety of receptive field size. Then, detection is performed using two sub-networks: a multi-scale output network (MON) for manhole cover object-like edge generation from several intermediate layers whose receptive fields match different object scales and a multi-level convolution matching network (M-CMN) for object detection based on fused feature maps, which combines several feature maps that enable small and densely packed manhole cover objects to produce a stronger response. The results show that our method is more accurate than existing methods at detecting manhole covers in remotely-sensed images.
KW  - manhole cover
KW  - remote sensing images
KW  - object detection
KW  - deep convolutional neural networks
DO  - 10.3390/ijgi8010049
TY  - EJOU
AU  - Zhang, Lei
AU  - Zhai, Zhengjun
AU  - He, Lang
AU  - Wen, Pengcheng
AU  - Niu, Wensheng
TI  - Infrared-Inertial Navigation for Commercial Aircraft Precision Landing in Low Visibility and GPS-Denied Environments
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - This paper proposes a novel infrared-inertial navigation method for the precise landing of commercial aircraft in low visibility and Global Position System (GPS)-denied environments. Within a Square-root Unscented Kalman Filter (SR_UKF), inertial measurement unit (IMU) data, forward-looking infrared (FLIR) images and airport geo-information are integrated to estimate the position, velocity and attitude of the aircraft during landing. Homography between the synthetic image and the real image which implicates the camera pose deviations is created as vision measurement. To accurately extract real runway features, the current results of runway detection are used as the prior knowledge for the next frame detection. To avoid possible homography decomposition solutions, it is directly converted to a vector and fed to the SR_UKF. Moreover, the proposed navigation system is proven to be observable by nonlinear observability analysis. Last but not least, a general aircraft was elaborately equipped with vision and inertial sensors to collect flight data for algorithm verification. The experimental results have demonstrated that the proposed method could be used for the precise landing of commercial aircraft in low visibility and GPS-denied environments.
KW  - infrared-inertial navigation
KW  - homography
KW  - runway detection
KW  - observability analysis
KW  - precise landing
KW  - low visibility
KW  - GPS-denied
DO  - 10.3390/s19020408
TY  - EJOU
AU  - Ghorbanzadeh, Omid
AU  - Blaschke, Thomas
AU  - Gholamnia, Khalil
AU  - Meena, Sansar R.
AU  - Tiede, Dirk
AU  - Aryal, Jagannath
TI  - Evaluation of Different Machine Learning Methods and Deep-Learning Convolutional Neural Networks for Landslide Detection
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 2
SN  - 2072-4292

AB  - There is a growing demand for detailed and accurate landslide maps and inventories around the globe, but particularly in hazard-prone regions such as the Himalayas. Most standard mapping methods require expert knowledge, supervision and fieldwork. In this study, we use optical data from the Rapid Eye satellite and topographic factors to analyze the potential of machine learning methods, i.e., artificial neural network (ANN), support vector machines (SVM) and random forest (RF), and different deep-learning convolution neural networks (CNNs) for landslide detection. We use two training zones and one test zone to independently evaluate the performance of different methods in the highly landslide-prone Rasuwa district in Nepal. Twenty different maps are created using ANN, SVM and RF and different CNN instantiations and are compared against the results of extensive fieldwork through a mean intersection-over-union (mIOU) and other common metrics. This accuracy assessment yields the best result of 78.26% mIOU for a small window size CNN, which uses spectral information only. The additional information from a 5 m digital elevation model helps to discriminate between human settlements and landslides but does not improve the overall classification accuracy. CNNs do not automatically outperform ANN, SVM and RF, although this is sometimes claimed. Rather, the performance of CNNs strongly depends on their design, i.e., layer depth, input window sizes and training strategies. Here, we conclude that the CNN method is still in its infancy as most researchers will either use predefined parameters in solutions like Google TensorFlow or will apply different settings in a trial-and-error manner. Nevertheless, deep-learning can improve landslide mapping in the future if the effects of the different designs are better understood, enough training samples exist, and the effects of augmentation strategies to artificially increase the number of existing samples are better understood.
KW  - deep-learning
KW  - convolution neural networks (CNNs)
KW  - artificial neural network
KW  - RapidEye
KW  - landslide mapping
KW  - mean intersection-over-union (mIOU)
DO  - 10.3390/rs11020196
TY  - EJOU
AU  - Pham, Tien D.
AU  - Yokoya, Naoto
AU  - Bui, Dieu T.
AU  - Yoshino, Kunihiko
AU  - Friess, Daniel A.
TI  - Remote Sensing Approaches for Monitoring Mangrove Species, Structure, and Biomass: Opportunities and Challenges
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - The mangrove ecosystem plays a vital role in the global carbon cycle, by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, mangroves have been lost worldwide, resulting in substantial carbon stock losses. Additionally, some aspects of the mangrove ecosystem remain poorly characterized compared to other forest ecosystems due to practical difficulties in measuring and monitoring mangrove biomass and their carbon stocks. Without a quantitative method for effectively monitoring biophysical parameters and carbon stocks in mangroves, robust policies and actions for sustainably conserving mangroves in the context of climate change mitigation and adaptation are more difficult. In this context, remote sensing provides an important tool for monitoring mangroves and identifying attributes such as species, biomass, and carbon stocks. A wide range of studies is based on optical imagery (aerial photography, multispectral, and hyperspectral) and synthetic aperture radar (SAR) data. Remote sensing approaches have been proven effective for mapping mangrove species, estimating their biomass, and assessing changes in their extent. This review provides an overview of the techniques that are currently being used to map various attributes of mangroves, summarizes the studies that have been undertaken since 2010 on a variety of remote sensing applications for monitoring mangroves, and addresses the limitations of these studies. We see several key future directions for the potential use of remote sensing techniques combined with machine learning techniques for mapping mangrove areas and species, and evaluating their biomass and carbon stocks.
KW  - mangrove species
KW  - mapping
KW  - biomass
KW  - blue carbon
KW  - machine learning
KW  - REDD+
DO  - 10.3390/rs11030230
TY  - EJOU
AU  - Rasti, Pejman
AU  - Ahmad, Ali
AU  - Samiei, Salma
AU  - Belin, Etienne
AU  - Rousseau, David
TI  - Supervised Image Classification by Scattering Transform with Application to Weed Detection in Culture Crops of High Density
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - In this article, we assess the interest of the recently introduced multiscale scattering transform for texture classification applied for the first time in plant science. Scattering transform is shown to outperform monoscale approaches (gray-level co-occurrence matrix, local binary patterns) but also multiscale approaches (wavelet decomposition) which do not include combinatory steps. The regime in which scatter transform also outperforms a standard CNN architecture in terms of data-set size is evaluated (    10 4     instances). An approach on how to optimally design the scatter transform based on energy contrast is provided. This is illustrated on the hard and open problem of weed detection in culture crops of high density from the top view in intensity images. An annotated synthetic data-set available under the form of a data challenge and a simulator are proposed for reproducible science. Scatter transform only trained on synthetic data shows an accuracy of     85 %     when tested on real data.
KW  - weed detection
KW  - scatter transform
KW  - deep learning
KW  - machine-learning classification
KW  - annotation
KW  - synthetic data
KW  - local binary pattern
DO  - 10.3390/rs11030249
TY  - EJOU
AU  - Xu, Zheng
AU  - Luo, Haibo
AU  - Hui, Bin
AU  - Chang, Zheng
TI  - Siamese Tracking from Single Point Initialization
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Recently, we have been concerned with locating and tracking vehicles in aerial videos. Vehicles in aerial videos usually have small sizes due to use of cameras from a remote distance. However, most of the current methods use a fixed bounding box region as the input of tracking. For the purpose of target locating and tracking in our system, detecting the contour of the target is utilized and can help with improving the accuracy of target tracking, because a shape-adaptive template segmented by object contour contains the most useful information and the least background for object tracking. In this paper, we propose a new start-up of tracking by clicking on the target, and implement the whole tracking process by modifying and combining a contour detection network and a fully convolutional Siamese tracking network. The experimental results show that our algorithm has significantly improved tracking accuracy compared to the state-of-the-art regarding vehicle images in both OTB100 and DARPA datasets. We propose utilizing our method in real time tracking and guidance systems.
KW  - object tracking
KW  - contour detection
KW  - Siamese network
KW  - deep learning
DO  - 10.3390/s19030514
TY  - EJOU
AU  - Wang, Xiaohong
AU  - Fan, Wenhui
AU  - Li, Xinjun
AU  - Wang, Lizhi
TI  - Weak Degradation Characteristics Analysis of UAV Motors Based on Laplacian Eigenmaps and Variational Mode Decomposition
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Brushless direct current (BLDC) motors are the source of flight power during the operation of rotary-wing unmanned aerial vehicles (UAVs), and their working state directly affects the safety of the whole system. To predict and avoid motor faults, it is necessary to accurately understand the health degradation process of the motor before any fault occurs. However, in actual working conditions, due to the aerodynamic environmental conditions of the aircraft flight, the background noise components of the vibration signals characterizing the running state of the motor are complex and severely coupled, making it difficult for the weak degradation characteristics to be clearly reflected. To address these problems, a weak degradation characteristic extraction method based on variational mode decomposition (VMD) and Laplacian Eigenmaps (LE) was proposed in this study to precisely identify the degradation information in system health data, avoid the loss of critical information and the interference of redundant information, and to optimize the description of a motor&rsquo;s degradation process despite the presence of complex background noise. A validation experiment was conducted on a specific type of motor under operation with load, to obtain the degradation characteristics of multiple types of vibration signals, and to test the proposed method. The results proved that this method can improve the stability and accuracy of predicting motor health, thereby helping to predict the degradation state and to optimize the maintenance strategies.
KW  - variational mode decomposition
KW  - Laplacian eigenmaps
KW  - multi-rotor unmanned aerial vehicle
KW  - brushless direct current motor
KW  - weak degradation characteristics
DO  - 10.3390/s19030524
TY  - EJOU
AU  - Qing, Xinlin
AU  - Li, Wenzhuo
AU  - Wang, Yishou
AU  - Sun, Hu
TI  - Piezoelectric Transducer-Based Structural Health Monitoring for Aircraft Applications
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Structural health monitoring (SHM) is being widely evaluated by the aerospace industry as a method to improve the safety and reliability of aircraft structures and also reduce operational cost. Built-in sensor networks on an aircraft structure can provide crucial information regarding the condition, damage state and/or service environment of the structure. Among the various types of transducers used for SHM, piezoelectric materials are widely used because they can be employed as either actuators or sensors due to their piezoelectric effect and vice versa. This paper provides a brief overview of piezoelectric transducer-based SHM system technology developed for aircraft applications in the past two decades. The requirements for practical implementation and use of structural health monitoring systems in aircraft application are then introduced. State-of-the-art techniques for solving some practical issues, such as sensor network integration, scalability to large structures, reliability and effect of environmental conditions, robust damage detection and quantification are discussed. Development trend of SHM technology is also discussed.
KW  - structural health monitoring
KW  - piezoelectric transducer
KW  - sensor network
KW  - damage detection
KW  - aircraft
DO  - 10.3390/s19030545
TY  - EJOU
AU  - Xu, Ziyao
AU  - Lian, Jijian
AU  - Bin, Lingling
AU  - Hua, Kaixun
AU  - Xu, Kui
AU  - Chan, Hoi Y.
TI  - Water Price Prediction for Increasing Market Efficiency Using Random Forest Regression: A Case Study in the Western United States
T2  - Water

PY  - 2019
VL  - 11
IS  - 2
SN  - 2073-4441

AB  - The existence of water markets establishes water prices, promoting trading of water from low- to high-valued uses. However, market participants can face uncertainty when asking and offering prices because water rights are heterogeneous, resulting in inefficiency of the market. This paper proposes three random forest regression models (RFR) to predict water price in the western United States: a full variable set model and two reduced ones with optimal numbers of variables using a backward variable elimination (BVE) approach. Transactions of 12 semiarid states, from 1987 to 2009, and a dataset containing various predictors, were assembled. Multiple replications of k-fold cross-validation were applied to assess the model performance and their generalizability was tested on unused data. The importance of price influencing factors was then analyzed based on two plausible variable importance rankings. Results show that the RFR models have good predictive power for water price. They outperform a baseline model without leading to overfitting. Also, the higher degree of accuracy of the reduced models is insignificant, reflecting the robustness of RFR to including lower informative variables. This study suggests that, due to its ability to automatically learn from and make predictions on data, RFR-based models can aid water market participants in making more efficient decisions.
KW  - water market
KW  - water price prediction
KW  - market efficiency
KW  - random forest regression
KW  - machine learning
DO  - 10.3390/w11020228
TY  - EJOU
AU  - Wu, Ruidong
AU  - Liu, Bing
AU  - Fu, Ping
AU  - Li, Junbao
AU  - Feng, Shou
TI  - An Accelerator Architecture of Changeable-Dimension Matrix Computing Method for SVM
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 2
SN  - 2079-9292

AB  - Matrix multiplication is a critical time-consuming processing step in many machine learning applications. Due to the diversity of practical applications, the matrix dimensions are generally not fixed. However, most matrix calculation methods, based on field programmable gate array (FPGA) currently use fixed matrix dimensions, which limit the flexibility of machine learning algorithms in a FPGA. The bottleneck lies in the limited FPGA resources. Therefore, this paper proposes an accelerator architecture for matrix computing method with changeable dimensions. Multi-matrix synchronous calculation concept allows matrix data to be processed continuously, which improves the parallel computing characteristics of FPGA and optimizes the computational efficiency. This paper tests matrix multiplication using support vector machine (SVM) algorithm to verify the performance of proposed architecture on the ZYNQ platform. The experimental results show that, compared to the software processing method, the proposed architecture increases the performance by 21.18 times with 9947 dimensions. The dimension is changeable with a maximum value of 2,097,151, without changing hardware design. This method is also applicable to matrix multiplication processing with other machine learning algorithms.
KW  - changeable-dimension matrix computing
KW  - field programmable gate array (FPGA)
KW  - support vector machine (SVM)
KW  - ZYNQ
DO  - 10.3390/electronics8020143
TY  - EJOU
AU  - Fu, Yongyong
AU  - Liu, Kunkun
AU  - Shen, Zhangquan
AU  - Deng, Jinsong
AU  - Gan, Muye
AU  - Liu, Xinguo
AU  - Lu, Dongming
AU  - Wang, Ke
TI  - Mapping Impervious Surfaces in Town–Rural Transition Belts Using China’s GF-2 Imagery and Object-Based Deep CNNs
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Impervious surfaces play an important role in urban planning and sustainable environmental management. High-spatial-resolution (HSR) images containing pure pixels have significant potential for the detailed delineation of land surfaces. However, due to high intraclass variability and low interclass distance, the mapping and monitoring of impervious surfaces in complex town&ndash;rural areas using HSR images remains a challenge. The fully convolutional network (FCN) model, a variant of convolution neural networks (CNNs), recently achieved state-of-the-art performance in HSR image classification applications. However, due to the inherent nature of FCN processing, it is challenging for an FCN to precisely capture the detailed information of classification targets. To solve this problem, we propose an object-based deep CNN framework that integrates object-based image analysis (OBIA) with deep CNNs to accurately extract and estimate impervious surfaces. Specifically, we also adopted two widely used transfer learning technologies to expedite the training of deep CNNs. Finally, we compare our approach with conventional OBIA classification and state-of-the-art FCN-based methods, such as FCN-8s and the U-Net methods. Both of these FCN-based methods are well designed for pixel-wise classification applications and have achieved great success. Our results show that the proposed approach effectively identified impervious surfaces, with 93.9% overall accuracy. Compared with the existing methods, i.e., OBIA, FCN-8s and U-Net methods, it shows that our method achieves obviously improvement in accuracy. Our findings also suggest that the classification performance of our proposed method is related to training strategy, indicating that significantly higher accuracy can be achieved through transfer learning by fine-tuning rather than feature extraction. Our approach for the automatic extraction and mapping of impervious surfaces also lays a solid foundation for intelligent monitoring and the management of land use and land cover.
KW  - transfer learning
KW  - remote sensing
KW  - deep learning
KW  - object-based image analysis (OBIA)
DO  - 10.3390/rs11030280
TY  - EJOU
AU  - Gebremedhin, Alem
AU  - Badenhorst, Pieter E.
AU  - Wang, Junping
AU  - Spangenberg, German C.
AU  - Smith, Kevin F.
TI  - Prospects for Measurement of Dry Matter Yield in Forage Breeding Programs Using Sensor Technologies
T2  - Agronomy

PY  - 2019
VL  - 9
IS  - 2
SN  - 2073-4395

AB  - Increasing the yield of perennial forage crops remains a crucial factor underpinning the profitability of grazing industries, and therefore is a priority for breeding programs. Breeding for high dry matter yield (DMY) in forage crops is likely to be enhanced with the development of genomic selection (GS) strategies. However, realising the full potential of GS will require an increase in the amount of phenotypic data and the rate at which it is collected. Therefore, phenotyping remains a critical bottleneck in the implementation of GS in forage species. Assessments of DMY in forage crop breeding include visual scores, sample clipping and mowing of plots, which are often costly and time-consuming. New ground- and aerial-based platforms equipped with advanced sensors offer opportunities for fast, nondestructive and low-cost, high-throughput phenotyping (HTP) of plant growth, development and yield in a field environment. The workflow of image acquisition, processing and analysis are reviewed. The &ldquo;big data&rdquo; challenges, proposed storage and management techniques, development of advanced statistical tools and methods for incorporating the HTP into forage breeding systems are also reviewed. Initial results where these techniques have been applied to forages have been promising but further research and development is required to adapt them to forage breeding situations, particularly with respect to the management of large data sets and the integration of information from spaced plants to sward plots. However, realizing the potential of sensor technologies combined with GS leads to greater rates of genetic gain in forages.
KW  - forage dry matter yield
KW  - high-throughput phenotyping
KW  - automation
KW  - imaging and image analysis
DO  - 10.3390/agronomy9020065
TY  - EJOU
AU  - Salamí, Esther
AU  - Gallardo, Antonia
AU  - Skorobogatov, Georgy
AU  - Barrado, Cristina
TI  - On-the-Fly Olive Tree Counting Using a UAS and Cloud Services
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Unmanned aerial systems (UAS) are becoming a common tool for aerial sensing applications. Nevertheless, sensed data need further processing before becoming useful information. This processing requires large computing power and time before delivery. In this paper, we present a parallel architecture that includes an unmanned aerial vehicle (UAV), a small embedded computer on board, a communication link to the Internet, and a cloud service with the aim to provide useful real-time information directly to the end-users. The potential of parallelism as a solution in remote sensing has not been addressed for a distributed architecture that includes the UAV processors. The architecture is demonstrated for a specific problem: the counting of olive trees in a crop field where the trees are regularly spaced from each other. During the flight, the embedded computer is able to process individual images on board the UAV and provide the total count. The tree counting algorithm obtains an     F 1     score of     99.09 %     for a sequence of ten images with 332 olive trees. The detected trees are geolocated and can be visualized on the Internet seconds after the take-off of the flight, with no further processing required. This is a use case to demonstrate near real-time results obtained from UAS usage. Other more complex UAS applications, such as tree inventories, search and rescue, fire detection, or stock breeding, can potentially benefit from this architecture and obtain faster outcomes, accessible while the UAV is still on flight.
KW  - UAS
KW  - UAV
KW  - image segmentation
KW  - tree counting
KW  - distributed services
KW  - cloud computing
DO  - 10.3390/rs11030316
TY  - EJOU
AU  - Ba, Rui
AU  - Song, Weiguo
AU  - Li, Xiaolian
AU  - Xie, Zixi
AU  - Lo, Siuming
TI  - Integration of Multiple Spectral Indices and a Neural Network for Burned Area Mapping Based on MODIS Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Since wildfires have occurred frequently in recent years, accurate burned area mapping is required for wildfire severity assessment and burned land reconstruction. Satellite remote sensing is an effective technology that can provide valuable information for wildfire assessment. However, the common approaches based on using a single satellite image to promptly detect the burned areas have low accuracy and limited applicability. This paper develops a new burned area mapping method that surpasses the detection accuracy of previous methods, while still using a single Moderate Resolution Imaging Spectroradiometer (MODIS) sensor image. The key innovation is integrating optimal spectral indices and a neural network algorithm. We used the traditional empirical formula method, multi-threshold method and visual interpretation method to extract the sample sets of five typical types (burned area, vegetation, cloud, bare soil, and cloud shadow) from the MODIS data of several wildfires in the American states of Nevada, Washington and California in 2016. Afterward, the separability index M was adopted to assess the capacity of seven spectral bands and 13 spectral indices to distinguish the burned area from four unburned land cover types. Based on the separability analysis between the burned area and unburned areas, the spectral indices with an M value higher than 1.0 were employed to generate the training sample sets that were assessed to have an overall accuracy of 98.68% and Kappa coefficient of 97.46%. Finally, we utilized a back-propagation neural network (BPNN) to learn the spectral differences of different types from the training sample sets and obtain the output burned area map. The proposed method was applied to three wildfire cases in the American states of Idaho, Nevada and Oregon in 2017. A comparison of detection results between the new MODIS-based burned area map and the reference burned area map compiled from Landsat-8 Operational Land Imager (OLI) data indicates that the proposed method can effectively exploit the spectral characteristics of various land cover types. Also, this new method can achieve higher accuracy with the reduction of commission error (CE, &gt;10%) and omission error (OE, &gt;6%) compared to the traditional empirical formula method. The new burned area mapping method could help managers and the public perform more effective wildfire assessments and emergency management.
KW  - MODIS
KW  - burned area
KW  - spectral indices
KW  - neural network
DO  - 10.3390/rs11030326
TY  - EJOU
AU  - Huang, Huasheng
AU  - Deng, Jizhong
AU  - Lan, Yubin
AU  - Yang, Aqing
AU  - Zhang, Lei
AU  - Wen, Sheng
AU  - Zhang, Huihui
AU  - Zhang, Yali
AU  - Deng, Yusen
TI  - Detection of Helminthosporium Leaf Blotch Disease Based on UAV Imagery
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 3
SN  - 2076-3417

AB  - Helminthosporium leaf blotch (HLB) is a serious disease of wheat causing yield reduction globally. Usually, HLB disease is controlled by uniform chemical spraying, which is adopted by most farmers. However, increased use of chemical controls have caused agronomic and environmental problems. To solve these problems, an accurate spraying system must be applied. In this case, the disease detection over the whole field can provide decision support information for the spraying machines. The objective of this paper is to evaluate the potential of unmanned aerial vehicle (UAV) remote sensing for HLB detection. In this work, the UAV imagery acquisition and ground investigation were conducted in Central China on April 22th, 2017. Four disease categories (normal, light, medium, and heavy) were established based on different severity degrees. A convolutional neural network (CNN) was proposed for HLB disease classification. The experiments on data preprocessing, classification, and hyper-parameters tuning were conducted. The overall accuracy and standard error of the CNN method was 91.43% and 0.83%, which outperformed other methods in terms of accuracy and stabilization. Especially for the detection of the diseased samples, the CNN method significantly outperformed others. Experimental results showed that the HLB infected areas and healthy areas can be precisely discriminated based on UAV remote sensing data, indicating that UAV remote sensing can be proposed as an efficient tool for HLB disease detection.
KW  - UAV imagery
KW  - remote sensing
KW  - Helminthosporium leaf blotch
KW  - convolution neural network
KW  - SVM
DO  - 10.3390/app9030558
TY  - EJOU
AU  - Chen, Chaoyue
AU  - Gong, Weiguo
AU  - Chen, Yongliang
AU  - Li, Weihong
TI  - Object Detection in Remote Sensing Images Based on a Scene-Contextual Feature Pyramid Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Object detection has attracted increasing attention in the field of remote sensing image analysis. Complex backgrounds, vertical views, and variations in target kind and size in remote sensing images make object detection a challenging task. In this work, considering that the types of objects are often closely related to the scene in which they are located, we propose a convolutional neural network (CNN) by combining scene-contextual information for object detection. Specifically, we put forward the scene-contextual feature pyramid network (SCFPN), which aims to strengthen the relationship between the target and the scene and solve problems resulting from variations in target size. Additionally, to improve the capability of feature extraction, the network is constructed by repeating a building aggregated residual block. This block increases the receptive field, which can extract richer information for targets and achieve excellent performance with respect to small object detection. Moreover, to improve the proposed model performance, we use group normalization, which divides the channels into groups and computes the mean and variance for normalization within each group, to solve the limitation of the batch normalization. The proposed method is validated on a public and challenging dataset. The experimental results demonstrate that our proposed method outperforms other state-of-the-art object detection models.
KW  - convolutional neural network (CNN)
KW  - object detection
KW  - remote sensing images
KW  - scene-contextual feature pyramid network (SCFPN)
DO  - 10.3390/rs11030339
TY  - EJOU
AU  - Jorge, Vitor A. M.
AU  - Granada, Roger
AU  - Maidana, Renan G.
AU  - Jurak, Darlan A.
AU  - Heck, Guilherme
AU  - Negreiros, Alvaro P. F.
AU  - dos Santos, Davi H.
AU  - Gonçalves, Luiz M. G.
AU  - Amory, Alexandre M.
TI  - A Survey on Unmanned Surface Vehicles for Disaster Robotics: Main Challenges and Directions
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Disaster robotics has become a research area in its own right, with several reported cases of successful robot deployment in actual disaster scenarios. Most of these disaster deployments use aerial, ground, or underwater robotic platforms. However, the research involving autonomous boats or Unmanned Surface Vehicles (USVs) for Disaster Management (DM) is currently spread across several publications, with varying degrees of depth, and focusing on more than one unmanned vehicle&mdash;usually under the umbrella of Unmanned Marine Vessels (UMV). Therefore, the current importance of USVs for the DM process in its different phases is not clear. This paper presents the first comprehensive survey about the applications and roles of USVs for DM, as far as we know. This work demonstrates that there are few current deployments in disaster scenarios, with most of the research in the area focusing on the technological aspects of USV hardware and software, such as Guidance Navigation and Control, and not focusing on their actual importance for DM. Finally, to guide future research, this paper also summarizes our own contributions, the lessons learned, guidelines, and research gaps.
KW  - survey
KW  - disaster management
KW  - unmanned surface vehicle
KW  - USV
KW  - unmanned surface craft
KW  - USC
KW  - autonomous surface craft
KW  - ASC
KW  - autonomous boat
KW  - disaster robotics
KW  - floods
KW  - landslides
KW  - hurricanes
KW  - tsunamis
KW  - hazard
KW  - search and rescue
DO  - 10.3390/s19030702
TY  - EJOU
AU  - Guo, Kai
AU  - Liu, Liansheng
AU  - Shi, Shuhui
AU  - Liu, Datong
AU  - Peng, Xiyuan
TI  - UAV Sensor Fault Detection Using a Classifier without Negative Samples: A Local Density Regulated Optimization Algorithm
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - Fault detection for sensors of unmanned aerial vehicles is essential for ensuring flight security, in which the flight control system conducts real-time control for the vehicles relying on the sensing information from sensors, and erroneous sensor data will lead to false flight control commands, causing undesirable consequences. However, because of the scarcity of faulty instances, it still remains a challenging issue for flight sensor fault detection. The one-class support vector machine approach is a favorable classifier without negative samples, however, it is sensitive to outliers that deviate from the center and lacks a mechanism for coping with them. The compactness of its decision boundary is influenced, leading to the degradation of detection rate. To deal with this issue, an optimized one-class support vector machine approach regulated by local density is proposed in this paper, which regulates the tolerance extents of its decision boundary to the outliers according to their extent of abnormality indicated by their local densities. The application scope of the local density theory is narrowed to keep the internal instances unchanged and a rule for assigning the outliers continuous density coefficients is raised. Simulation results on a real flight control system model have proved its effectiveness and superiority.
KW  - fault detection
KW  - sensors
KW  - unmanned aerial vehicles
KW  - flight control system
KW  - one-class support vector machine
KW  - local density
DO  - 10.3390/s19040771
TY  - EJOU
AU  - Kwak, Geun-Ho
AU  - Park, No-Wook
TI  - Impact of Texture Information on Crop Classification with Machine Learning and UAV Images
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - Unmanned aerial vehicle (UAV) images that can provide thematic information at much higher spatial and temporal resolutions than satellite images have great potential in crop classification. Due to the ultra-high spatial resolution of UAV images, spatial contextual information such as texture is often used for crop classification. From a data availability viewpoint, it is not always possible to acquire time-series UAV images due to limited accessibility to the study area. Thus, it is necessary to improve classification performance for situations when a single or minimum number of UAV images are available for crop classification. In this study, we investigate the potential of gray-level co-occurrence matrix (GLCM)-based texture information for crop classification with time-series UAV images and machine learning classifiers including random forest and support vector machine. In particular, the impact of combining texture and spectral information on the classification performance is evaluated for cases that use only one UAV image or multi-temporal images as input. A case study of crop classification in Anbandegi of Korea was conducted for the above comparisons. The best classification accuracy was achieved when multi-temporal UAV images which can fully account for the growth cycles of crops were combined with GLCM-based texture features. However, the impact of the utilization of texture information was not significant. In contrast, when one August UAV image was used for crop classification, the utilization of texture information significantly affected the classification performance. Classification using texture features extracted from GLCM with larger kernel size significantly improved classification accuracy, an improvement of 7.72%p in overall accuracy for the support vector machine classifier, compared with classification based solely on spectral information. These results indicate the usefulness of texture information for classification of ultra-high-spatial-resolution UAV images, particularly when acquisition of time-series UAV images is difficult and only one UAV image is used for crop classification.
KW  - unmanned aerial vehicle
KW  - texture
KW  - gray-level co-occurrence matrix
KW  - machine learning
KW  - crop
DO  - 10.3390/app9040643
TY  - EJOU
AU  - Giernacki, Wojciech
TI  - Iterative Learning Method for In-Flight Auto-Tuning of UAV Controllers Based on Basic Sensory Information
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - With an increasing number of multirotor unmanned aerial vehicles (UAVs), solutions supporting the improvement in their precision of operation and safety of autonomous flights are gaining importance. They are particularly crucial in transportation tasks, where control systems are required to provide a stable and controllable flight in various environmental conditions, especially after changing the total mass of the UAV (by adding extra load). In the paper, the problem of using only available basic sensory information for fast, locally best, iterative real-time auto-tuning of parameters of fixed-gain altitude controllers is considered. The machine learning method proposed for this purpose is based on a modified zero-order optimization algorithm (golden-search algorithm) and bootstrapping technique. It has been validated in numerous simulations and real-world experiments in terms of its effectiveness in such aspects as: the impact of environmental disturbances (wind gusts); flight with change in mass; and change of sensory information sources in the auto-tuning procedure. The main advantage of the proposed method is that for the trajectory primitives repeatedly followed by an UAV (for programmed controller gains), the method effectively minimizes the selected performance index (cost function). Such a performance index might, e.g., express indirect requirements about tracking quality and energy expenditure. In the paper, a comprehensive description of the method, as well as a wide discussion of the results obtained from experiments conducted in the AeroLab for a low-cost UAV (Bebop 2), are included. The results have confirmed high efficiency of the method at the expected, low computational complexity.
KW  - UAV
KW  - auto-tuning
KW  - machine learning
KW  - iterative learning
KW  - extremum-seeking
KW  - altitude controller
DO  - 10.3390/app9040648
TY  - EJOU
AU  - Cheng, Qiao
AU  - Wang, Xiangke
AU  - Yang, Jian
AU  - Shen, Lincheng
TI  - Automated Enemy Avoidance of Unmanned Aerial Vehicles Based on Reinforcement Learning
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - This paper focuses on one of the collision avoidance scenarios for unmanned aerial vehicles (UAVs), where the UAV needs to avoid collision with the enemy UAV during its flying path to the goal point. Such a type of problem is defined as the enemy avoidance problem in this paper. To deal with this problem, a learning based framework is proposed. Under this framework, the enemy avoidance problem is formulated as a Markov Decision Process (MDP), and the maneuver policies for the UAV are learned based on a temporal-difference reinforcement learning method called Sarsa. To handle the enemy avoidance problem in continuous state space, the Cerebellar Model Arithmetic Computer (CMAC) function approximation technique is embodied in the proposed framework. Furthermore, a hardware-in-the-loop (HITL) simulation environment is established. Simulation results show that the UAV agent can learn a satisfying policy under the proposed framework. Comparing with the random policy and the fixed-rule policy, the learned policy can achieve a far higher possibility in reaching the goal point without colliding with the enemy UAV.
KW  - enemy avoidance
KW  - reinforcement learning
KW  - decision making
KW  - hardware-in-the-loop simulation
KW  - unmanned aerial vehicles
DO  - 10.3390/app9040669
TY  - EJOU
AU  - Ampatzidis, Yiannis
AU  - Partel, Victor
TI  - UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 4
SN  - 2072-4292

AB  - Traditional plant breeding evaluation methods are time-consuming, labor-intensive, and costly. Accurate and rapid phenotypic trait data acquisition and analysis can improve genomic selection and accelerate cultivar development. In this work, a technique for data acquisition and image processing was developed utilizing small unmanned aerial vehicles (UAVs), multispectral imaging, and deep learning convolutional neural networks to evaluate phenotypic characteristics on citrus crops. This low-cost and automated high-throughput phenotyping technique utilizes artificial intelligence (AI) and machine learning (ML) to: (i) detect, count, and geolocate trees and tree gaps; (ii) categorize trees based on their canopy size; (iii) develop individual tree health indices; and (iv) evaluate citrus varieties and rootstocks. The proposed remote sensing technique was able to detect and count citrus trees in a grove of 4,931 trees, with precision and recall of 99.9% and 99.7%, respectively, estimate their canopy size with overall accuracy of 85.5%, and detect, count, and geolocate tree gaps with a precision and recall of 100% and 94.6%, respectively. This UAV-based technique provides a consistent, more direct, cost-effective, and rapid method to evaluate phenotypic characteristics of citrus varieties and rootstocks.
KW  - UAV
KW  - artificial intelligence
KW  - machine learning
KW  - smart agriculture
KW  - precision agriculture
KW  - neural networks
KW  - deep learning
DO  - 10.3390/rs11040410
TY  - EJOU
AU  - Yan, Lei
AU  - Cao, Suzhi
AU  - Gong, Yongsheng
AU  - Han, Hao
AU  - Wei, Junyong
AU  - Zhao, Yi
AU  - Yang, Shuling
TI  - SatEC: A 5G Satellite Edge Computing Framework Based on Microservice Architecture
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - As outlined in the 3Gpp Release 16, 5G satellite access is important for 5G network development in the future. A terrestrial-satellite network integrated with 5G has the characteristics of low delay, high bandwidth, and ubiquitous coverage. A few researchers have proposed integrated schemes for such a network; however, these schemes do not consider the possibility of achieving optimization of the delay characteristic by changing the computing mode of the 5G satellite network. We propose a 5G satellite edge computing framework (5GsatEC), which aims to reduce delay and expand network coverage. This framework consists of embedded hardware platforms and edge computing microservices in satellites. To increase the flexibility of the framework in complex scenarios, we unify the resource management of the central processing unit (CPU), graphics processing unit (GPU), and field-programmable gate array (FPGA); we divide the services into three types: system services, basic services, and user services. In order to verify the performance of the framework, we carried out a series of experiments. The results show that 5GsatEC has a broader coverage than the ground 5G network. The results also show that 5GsatEC has lower delay, a lower packet loss rate, and lower bandwidth consumption than the 5G satellite network.
KW  - edge computing
KW  - on-board data processing
KW  - microservices
KW  - Integrated Terrestrial-Satellite Networks
DO  - 10.3390/s19040831
TY  - EJOU
AU  - Shihavuddin, ASM
AU  - Chen, Xiao
AU  - Fedorov, Vladimir
AU  - Nymark Christensen, Anders
AU  - Andre Brogaard Riis, Nicolai
AU  - Branner, Kim
AU  - Bjorholm Dahl, Anders
AU  - Reinhold Paulsen, Rasmus
TI  - Wind Turbine Surface Damage Detection by Deep Learning Aided Drone Inspection Analysis
T2  - Energies

PY  - 2019
VL  - 12
IS  - 4
SN  - 1996-1073

AB  - Timely detection of surface damages on wind turbine blades is imperative for minimizing downtime and avoiding possible catastrophic structural failures. With recent advances in drone technology, a large number of high-resolution images of wind turbines are routinely acquired and subsequently analyzed by experts to identify imminent damages. Automated analysis of these inspection images with the help of machine learning algorithms can reduce the inspection cost. In this work, we develop a deep learning-based automated damage suggestion system for subsequent analysis of drone inspection images. Experimental results demonstrate that the proposed approach can achieve almost human-level precision in terms of suggested damage location and types on wind turbine blades. We further demonstrate that for relatively small training sets, advanced data augmentation during deep learning training can better generalize the trained model, providing a significant gain in precision.
KW  - wind energy
KW  - rotor blade
KW  - wind turbine
KW  - drone inspection
KW  - damage detection
KW  - deep learning
KW  - Convolutional Neural Network (CNN)
DO  - 10.3390/en12040676
TY  - EJOU
AU  - Wang, Linhui
AU  - Yue, Xuejun
AU  - Liu, Yongxin
AU  - Wang, Jian
AU  - Wang, Huihui
TI  - An Intelligent Vision Based Sensing Approach for Spraying Droplets Deposition Detection
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - The rapid development of vision sensor based on artificial intelligence (AI) is reforming industries and making our world smarter. Among these trends, it is of great significance to adapt AI technologies into the intelligent agricultural management. In smart agricultural aviation spraying, the droplets&rsquo; distribution and deposition are important indexes for estimating effectiveness in plant protection process. However, conventional approaches are problematic, they lack adaptivity to environmental changes, and consumes non-reusable test materials. One example is that the machine vision algorithms they employ can&rsquo;t guarantee that the division of adhesive droplets thereby disabling the accurate measurement of critical parameters. To alleviate these problems, we put forward an intelligent visual droplet detection node which can adapt to the environment illumination change. Then, we propose a modified marker controllable watershed segmentation algorithm to segment those adhesive droplets, and calculate their characteristic parameters on the basis of the segmentation results, including number, coverage, coverage density, etc. Finally, we use the intelligent node to detect droplets, and then expound the situation that the droplet region is effectively segmented and marked. The intelligent node has better adaptability and robustness even under the condition of illumination changing. The large-scale distributed detection result indicates that our approach has good consistency with the non-recyclable water-sensitive paper approach. Our approach provides an intelligent and environmental friendly way of tests for spraying techniques, especially for plant protection with Unmanned Aerial Vehicles.
KW  - droplets
KW  - intelligent node
KW  - vision sensor
KW  - adaptability
KW  - Unmanned Aerial Vehicles
DO  - 10.3390/s19040933
TY  - EJOU
AU  - Liu, Zheng
AU  - Abbaszadeh, Shiva
TI  - Double Q-Learning for Radiation Source Detection
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - Anomalous radiation source detection in urban environments is challenging due to the complex nature of background radiation. When a suspicious area is determined, a radiation survey is usually carried out to search for anomalous radiation sources. To locate the source with high accuracy and in a short time, different survey approaches have been studied such as scanning the area with fixed survey paths and data-driven approaches that update the survey path on the fly with newly acquired measurements. In this work, we propose reinforcement learning as a data-driven approach to conduct radiation detection tasks with no human intervention. A simulated radiation environment is constructed, and a convolutional neural network-based double Q-learning algorithm is built and tested for radiation source detection tasks. Simulation results show that the double Q-learning algorithm can reliably navigate the detector and reduce the searching time by at least 44% compared with traditional uniform search methods and gradient search methods.
KW  - reinforcement learning
KW  - radiation detection
KW  - source searching
DO  - 10.3390/s19040960
TY  - EJOU
AU  - Madokoro, Hirokazu
AU  - Sato, Kazuhito
AU  - Shimoi, Nobuhiro
TI  - Vision-Based Indoor Scene Recognition from Time-Series Aerial Images Obtained Using a MAV Mounted Monocular Camera
T2  - Drones

PY  - 2019
VL  - 3
IS  - 1
SN  - 2504-446X

AB  - This paper presents a vision-based indoor scene recognition method from aerial time-series images obtained using a micro air vehicle (MAV). The proposed method comprises two procedures: a codebook feature description procedure, and a recognition procedure using category maps. For the former procedure, codebooks are created automatically as visual words using self-organizing maps (SOMs) after extracting part-based local features using a part-based descriptor from time-series scene images. For the latter procedure, category maps are created using counter propagation networks (CPNs) with the extraction of category boundaries using a unified distance matrix (U-Matrix). Using category maps, topologies of image features are mapped into a low-dimensional space based on competitive and neighborhood learning. We obtained aerial time-series image datasets of five sets for two flight routes: a round flight route and a zigzag flight route. The experimentally obtained results with leave-one-out cross-validation (LOOCV) revealed respective mean recognition accuracies for the round flight datasets (RFDs) and zigzag flight datasets (ZFDs) of 71.7% and 65.5% for 10 zones. The category maps addressed the complexity of scenes because of segmented categories. Although extraction results of category boundaries using U-Matrix were partially discontinuous, we obtained comprehensive category boundaries that segment scenes into several categories.
KW  - category maps
KW  - counter propagation networks
KW  - leave-one-out cross-validation
KW  - micro air vehicles
KW  - self-organizing maps
KW  - unified distance matrix
DO  - 10.3390/drones3010022
TY  - EJOU
AU  - Hrabia, Christopher-Eyk
AU  - Hessler, Axel
AU  - Xu, Yuan
AU  - Seibert, Jacob
AU  - Brehmer, Jan
AU  - Albayrak, Sahin
TI  - EffFeu Project: Towards Mission-Guided Application of Drones in Safety and Security Environments
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - The number of unmanned aerial system (UAS) applications for supporting rescue forces is growing in recent years. Nevertheless, the analysis of sensed information and control of unmanned aerial vehicle (UAV) creates an enormous psychological and emotional load for the involved humans especially in critical and hectic situations. The introduced research project EffFeu (Efficient Operation of Unmanned Aerial Vehicle for Industrial Firefighters) especially focuses on a holistic integration of UAS in the daily work of industrial firefighters. This is done by enabling autonomous mission-guided control on top of the presented overall system architecture, goal-oriented high-level task control, comprehensive localisation process combining several approaches to enable the transition from and to GNSS-supported and GNSS-denied environments, as well as a deep-learning based object recognition of relevant entities. This work describes the concepts, current stage, and first evaluation results of the research project.
KW  - decisional autonomy
KW  - decision-making
KW  - planning
KW  - object recognition
KW  - deep learning
KW  - GNSS-denied localisation
DO  - 10.3390/s19040973
TY  - EJOU
AU  - Wen, Sheng
AU  - Zhang, Quanyong
AU  - Yin, Xuanchun
AU  - Lan, Yubin
AU  - Zhang, Jiantao
AU  - Ge, Yufeng
TI  - Design of Plant Protection UAV Variable Spray System Based on Neural Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 5
SN  - 1424-8220

AB  - Recently, unmanned aerial vehicles (UAVs) have rapidly emerged as a new technology in the fields of plant protection and pest control in China. Based on existing variable spray research, a plant protection UAV variable spray system integrating neural network based decision making is designed. Using the existing data on plant protection UAV operations, combined with artificial neural network (ANN) technology, an error back propagation (BP) neural network model between the factors affecting droplet deposition is trained. The factors affecting droplet deposition include ambient temperature, ambient humidity, wind speed, flight speed, flight altitude, propeller pitch, nozzles pitch and prescription value. Subsequently, the BP neural network model is combined with variable rate spray control for plant protection UAVs, and real-time information is collected by multi-sensor. The deposition rate is determined by the neural network model, and the flow rate of the spray system is regulated according to the predicted deposition amount. The amount of droplet deposition can meet the prescription requirement. The results show that the training variance of the ANN is 0.003, and thus, the model is stable and reliable. The outdoor tests show that the error between the predicted droplet deposition and actual droplet deposition is less than 20%. The ratio of droplet deposition to prescription value in each unit is approximately equal, and a variable spray operation under different conditions is realized.
KW  - UAV
KW  - BP neural network
KW  - droplet deposition
KW  - variable spray
DO  - 10.3390/s19051112
TY  - EJOU
AU  - Li, Yang
AU  - Shi, Leyi
AU  - Feng, Haijie
TI  - A Game-Theoretic Analysis for Distributed Honeypots
T2  - Future Internet

PY  - 2019
VL  - 11
IS  - 3
SN  - 1999-5903

AB  - A honeypot is a decoy tool for luring an attacker and interacting with it, further consuming its resources. Due to its fake property, a honeypot can be recognized by the adversary and loses its value. Honeypots equipped with dynamic characteristics are capable of deceiving intruders. However, most of their dynamic properties are reflected in the system configuration, rather than the location. Dynamic honeypots are faced with the risk of being identified and avoided. In this paper, we focus on the dynamic locations of honeypots and propose a distributed honeypot scheme. By periodically changing the services, the attacker cannot distinguish the real services from honeypots, and the illegal attack flow can be recognized. We adopt game theory to illustrate the effectiveness of our system. Gambit simulations are conducted to validate our proposed scheme. The game-theoretic reasoning shows that our system comprises an innovative system defense. Further simulation results prove that the proposed scheme improves the server’s payoff and that the attacker tends to abandon launching attacks. Therefore, the proposed distributed honeypot scheme is effective for network security.
KW  - game theory
KW  - honeypot
KW  - network security
KW  - proactive defense
DO  - 10.3390/fi11030065
TY  - EJOU
AU  - Carl, Christin
AU  - Lehmann, Jan R. K.
AU  - Landgraf, Dirk
AU  - Pretzsch, Hans
TI  - Robinia pseudoacacia L. in Short Rotation Coppice: Seed and Stump Shoot Reproduction as well as UAS-based Spreading Analysis
T2  - Forests

PY  - 2019
VL  - 10
IS  - 3
SN  - 1999-4907

AB  - Varying reproduction strategies are an important trait that tree species need in order both to survive and to spread. Black locust is able to reproduce via seeds, stump shoots, and root suckers. However, little research has been conducted on the reproduction and spreading of black locust in short rotation coppices. This research study focused on seed germination, stump shoot resprout, and spreading by root suckering of black locust in ten short rotation coppices in Germany. Seed experiments and sample plots were analyzed for the study. Spreading was detected and measured with unmanned aerial system (UAS)-based images and classification technology&mdash;object-based image analysis (OBIA). Additionally, the classification of single UAS images was tested by applying a convolutional neural network (CNN), a deep learning model. The analyses showed that seed germination increases with increasing warm-cold variety and scarification. Moreover, it was found that the number of shoots per stump decreases as shoot age increases. Furthermore, spreading increases with greater light availability and decreasing tillage. The OBIA and CNN image analysis technologies achieved 97% and 99.5% accuracy for black locust classification in UAS images. All in all, the three reproduction strategies of black locust in short rotation coppices differ with regards to initialization, intensity, and growth performance, but all play a role in the survival and spreading of black locust.
KW  - Robinia pseudoacacia L.
KW  - reproduction
KW  - spreading
KW  - short rotation coppice
KW  - unmanned aerial system (UAS)
KW  - object-based image analysis (OBIA)
KW  - convolutional neural network (CNN)
DO  - 10.3390/f10030235
TY  - EJOU
AU  - Gao, Lin
AU  - Song, Weidong
AU  - Dai, Jiguang
AU  - Chen, Yang
TI  - Road Extraction from High-Resolution Remote Sensing Imagery Using Refined Deep Residual Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 5
SN  - 2072-4292

AB  - Road extraction is one of the most significant tasks for modern transportation systems. This task is normally difficult due to complex backgrounds such as rural roads that have heterogeneous appearances with large intraclass and low interclass variations and urban roads that are covered by vehicles, pedestrians and the shadows of surrounding trees or buildings. In this paper, we propose a novel method for extracting roads from optical satellite images using a refined deep residual convolutional neural network (RDRCNN) with a postprocessing stage. RDRCNN consists of a residual connected unit (RCU) and a dilated perception unit (DPU). The RDRCNN structure is symmetric to generate the outputs of the same size. A math morphology and a tensor voting algorithm are used to improve RDRCNN performance during postprocessing. Experiments are conducted on two datasets of high-resolution images to demonstrate the performance of the proposed network architectures, and the results of the proposed architectures are compared with those of other network architectures. The results demonstrate the effective performance of the proposed method for extracting roads from a complex scene.
KW  - refined deep residual convolutional neural network
KW  - road extraction
KW  - remote sensing
KW  - tensor voting
KW  - math morphology
KW  - high-resolution imagery
DO  - 10.3390/rs11050552
TY  - EJOU
AU  - Asadi, Khashayar
AU  - Chen, Pengyu
AU  - Han, Kevin
AU  - Wu, Tianfu
AU  - Lobaton, Edgar
TI  - LNSNet: Lightweight Navigable Space Segmentation for Autonomous Robots on Construction Sites
T2  - Data

PY  - 2019
VL  - 4
IS  - 1
SN  - 2306-5729

AB  - An autonomous robot that can monitor a construction site should be able to be can contextually detect its surrounding environment by recognizing objects and making decisions based on its observation. Pixel-wise semantic segmentation in real-time is vital to building an autonomous and mobile robot. However, the learning models&rsquo; size and high memory usage associated with real-time segmentation are the main challenges for mobile robotics systems that have limited computing resources. To overcome these challenges, this paper presents an efficient semantic segmentation method named LNSNet (lightweight navigable space segmentation network) that can run on embedded platforms to determine navigable space in real-time. The core of model architecture is a new block based on separable convolution which compresses the parameters of present residual block meanwhile maintaining the accuracy and performance. LNSNet is faster, has fewer parameters and less model size, while provides similar accuracy compared to existing models. A new pixel-level annotated dataset for real-time and mobile navigable space segmentation in construction environments has been constructed for the proposed method. The results demonstrate the effectiveness and efficiency that are necessary for the future development of the autonomous robotics systems.
KW  - efficient real-time segmentation
KW  - embedded platform
KW  - autonomous navigation in construction
KW  - autonomous data collection
DO  - 10.3390/data4010040
TY  - EJOU
AU  - Zhang, Chengming
AU  - Han, Yingjuan
AU  - Li, Feng
AU  - Gao, Shuai
AU  - Song, Dejuan
AU  - Zhao, Hui
AU  - Fan, Keqi
AU  - Zhang, Ya’nan
TI  - A New CNN-Bayesian Model for Extracting Improved Winter Wheat Spatial Distribution from GF-2 imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - When the spatial distribution of winter wheat is extracted from high-resolution remote sensing imagery using convolutional neural networks (CNN), field edge results are usually rough, resulting in lowered overall accuracy. This study proposed a new per-pixel classification model using CNN and Bayesian models (CNN-Bayesian model) for improved extraction accuracy. In this model, a feature extractor generates a feature vector for each pixel, an encoder transforms the feature vector of each pixel into a category-code vector, and a two-level classifier uses the difference between elements of category-probability vectors as the confidence value to perform per-pixel classifications. The first level is used to determine the category of a pixel with high confidence, and the second level is an improved Bayesian model used to determine the category of low-confidence pixels. The CNN-Bayesian model was trained and tested on Gaofen 2 satellite images. Compared to existing models, our approach produced an improvement in overall accuracy, the overall accuracy of SegNet, DeepLab, VGG-Ex, and CNN-Bayesian was 0.791, 0.852, 0.892, and 0.946, respectively. Thus, this approach can produce superior results when winter wheat spatial distribution is extracted from satellite imagery.
KW  - winter wheat
KW  - convolutional neural network
KW  - Visual Geometry Group Network
KW  - Bayesian model
KW  - per-pixel classification
KW  - high-resolution remote sensing imager
KW  - Gaofen 2 image
DO  - 10.3390/rs11060619
TY  - EJOU
AU  - Tang, Shixi
AU  - Gu, Jinan
AU  - Tang, Keming
AU  - Zou, Rong
AU  - Sun, Xiaohong
AU  - Uddin, Saad
TI  - A Fault-Signal-Based Generalizing Remaining Useful Life Prognostics Method for Wheel Hub Bearings
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 6
SN  - 2076-3417

AB  - The goal of this work is to improve the generalization of remaining useful life (RUL) prognostics for wheel hub bearings. The traditional life prognostics methods assume that the data used in RUL prognostics is composed of one specific fatigue damage type, the data used in RUL prognostics is accurate, and the RUL prognostics are conducted in the short term. Due to which, a generalizing RUL prognostics method is designed based on fault signal data. Firstly, the fault signal model is designed with the signal in a complex and mutative environment. Then, the generalizing RUL prognostics method is designed based on the fault signal model. Lastly, the simplified solution of the generalizing RUL prognostics method is deduced. The experimental results show that the proposed method gained good accuracies for RUL prognostics for all the amplitude, energy, and kurtosis features with fatigue damage types. The proposed method can process inaccurate fault signals with different kinds of noise in the actual working environment, and it can be conducted in the long term. Therefore, the RUL prognostics method has a good generalization.
KW  - data-driven method
KW  - remaining useful life prognostics
KW  - fault signal analysis
KW  - grey system
KW  - differential hydrological grey method
KW  - wheel hub bearings
DO  - 10.3390/app9061080
TY  - EJOU
AU  - Safonova, Anastasiia
AU  - Tabik, Siham
AU  - Alcaraz-Segura, Domingo
AU  - Rubtsov, Alexey
AU  - Maglinets, Yuriy
AU  - Herrera, Francisco
TI  - Detection of Fir Trees (Abies sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Invasion of the Polygraphus proximus Blandford bark beetle causes catastrophic damage to forests with firs (Abies sibirica Ledeb) in Russia, especially in Central Siberia. Determining tree damage stage based on the shape, texture and colour of tree crown in unmanned aerial vehicle (UAV) images could help to assess forest health in a faster and cheaper way. However, this task is challenging since (i) fir trees at different damage stages coexist and overlap in the canopy, (ii) the distribution of fir trees in nature is irregular and hence distinguishing between different crowns is hard, even for the human eye. Motivated by the latest advances in computer vision and machine learning, this work proposes a two-stage solution: In a first stage, we built a detection strategy that finds the regions of the input UAV image that are more likely to contain a crown, in the second stage, we developed a new convolutional neural network (CNN) architecture that predicts the fir tree damage stage in each candidate region. Our experiments show that the proposed approach shows satisfactory results on UAV Red, Green, Blue (RGB) images of forest areas in the state nature reserve “Stolby” (Krasnoyarsk, Russia).
KW  - multi-class classification
KW  - drone
KW  - aerial photography
KW  - Siberian fir
KW  - Siberia
KW  - deep-learning
KW  - convolutional neural networks
KW  - forest health
DO  - 10.3390/rs11060643
TY  - EJOU
AU  - Li, Yundong
AU  - Hu, Wei
AU  - Dong, Han
AU  - Zhang, Xueyan
TI  - Building Damage Detection from Post-Event Aerial Imagery Using Single Shot Multibox Detector
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 6
SN  - 2076-3417

AB  - Using aerial cameras, satellite remote sensing or unmanned aerial vehicles (UAV) equipped with cameras can facilitate search and rescue tasks after disasters. The traditional manual interpretation of huge aerial images is inefficient and could be replaced by machine learning-based methods combined with image processing techniques. Given the development of machine learning, researchers find that convolutional neural networks can effectively extract features from images. Some target detection methods based on deep learning, such as the single-shot multibox detector (SSD) algorithm, can achieve better results than traditional methods. However, the impressive performance of machine learning-based methods results from the numerous labeled samples. Given the complexity of post-disaster scenarios, obtaining many samples in the aftermath of disasters is difficult. To address this issue, a damaged building assessment method using SSD with pretraining and data augmentation is proposed in the current study and highlights the following aspects. (1) Objects can be detected and classified into undamaged buildings, damaged buildings, and ruins. (2) A convolution auto-encoder (CAE) that consists of VGG16 is constructed and trained using unlabeled post-disaster images. As a transfer learning strategy, the weights of the SSD model are initialized using the weights of the CAE counterpart. (3) Data augmentation strategies, such as image mirroring, rotation, Gaussian blur, and Gaussian noise processing, are utilized to augment the training data set. As a case study, aerial images of Hurricane Sandy in 2012 were maximized to validate the proposed method&rsquo;s effectiveness. Experiments show that the pretraining strategy can improve of 10% in terms of overall accuracy compared with the SSD trained from scratch. These experiments also demonstrate that using data augmentation strategies can improve mAP and mF1 by 72% and 20%, respectively. Finally, the experiment is further verified by another dataset of Hurricane Irma, and it is concluded that the paper method is feasible.
KW  - building damage assessment
KW  - post-event
KW  - deep learning
KW  - SSD
KW  - convolutional autoencoder
DO  - 10.3390/app9061128
TY  - EJOU
AU  - Liu, Shengjie
AU  - Qi, Zhixin
AU  - Li, Xia
AU  - Yeh, Anthony G.
TI  - Integration of Convolutional Neural Networks and Object-Based Post-Classification Refinement for Land Use and Land Cover Mapping with Optical and SAR Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Object-based image analysis (OBIA) has been widely used for land use and land cover (LULC) mapping using optical and synthetic aperture radar (SAR) images because it can utilize spatial information, reduce the effect of salt and pepper, and delineate LULC boundaries. With recent advances in machine learning, convolutional neural networks (CNNs) have become state-of-the-art algorithms. However, CNNs cannot be easily integrated with OBIA because the processing unit of CNNs is a rectangular image, whereas that of OBIA is an irregular image object. To obtain object-based thematic maps, this study developed a new method that integrates object-based post-classification refinement (OBPR) and CNNs for LULC mapping using Sentinel optical and SAR data. After producing the classification map by CNN, each image object was labeled with the most frequent land cover category of its pixels. The proposed method was tested on the optical-SAR Sentinel Guangzhou dataset with 10 m spatial resolution, the optical-SAR Zhuhai-Macau local climate zones (LCZ) dataset with 100 m spatial resolution, and a hyperspectral benchmark the University of Pavia with 1.3 m spatial resolution. It outperformed OBIA support vector machine (SVM) and random forest (RF). SVM and RF could benefit more from the combined use of optical and SAR data compared with CNN, whereas spatial information learned by CNN was very effective for classification. With the ability to extract spatial features and maintain object boundaries, the proposed method considerably improved the classification accuracy of urban ground targets. It achieved overall accuracy (OA) of 95.33% for the Sentinel Guangzhou dataset, OA of 77.64% for the Zhuhai-Macau LCZ dataset, and OA of 95.70% for the University of Pavia dataset with only 10 labeled samples per class.
KW  - object-based post-classification refinement (OBPR)
KW  - convolutional neural network (CNN)
KW  - synthetic aperture radar (SAR)
KW  - land use and land cover
KW  - object-based image analysis (OBIA)
DO  - 10.3390/rs11060690
TY  - EJOU
AU  - Wu, Jintao
AU  - Yang, Guijun
AU  - Yang, Xiaodong
AU  - Xu, Bo
AU  - Han, Liang
AU  - Zhu, Yaohui
TI  - Automatic Counting of in situ Rice Seedlings from UAV Images Based on a Deep Fully Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - The number of rice seedlings in the field is one of the main agronomic components for determining rice yield. This counting task, however, is still mainly performed using human vision rather than computer vision and is thus cumbersome and time-consuming. A fast and accurate alternative method of acquiring such data may contribute to monitoring the efficiency of crop management practices, to earlier estimations of rice yield, and as a phenotyping trait in breeding programs. In this paper, we propose an efficient method that uses computer vision to accurately count rice seedlings in a digital image. First, an unmanned aerial vehicle (UAV) equipped with red-green-blue (RGB) cameras was used to acquire field images at the seedling stage. Next, we use a regression network (Basic Network) inspired by a deep fully convolutional neural network to regress the density map and estimate the number of rice seedlings for a given UAV image. Finally, an improved version of the Basic Network, the Combined Network, is also proposed to further improve counting accuracy. To explore the efficacy of the proposed method, a novel rice seedling counting (RSC) dataset was built, which consisted of 40 images (where the number of seedlings varied between 3732 and 16,173) and corresponding manually-dotted annotations. The results demonstrated high average accuracy (higher than 93%) between counts according to the proposed method and manual (UAV image-based) rice seedling counts, and very good performance, with a high coefficient of determination (R2) (around 0.94). In conclusion, the results indicate that the proposed method is an efficient alternative for large-scale counting of rice seedlings, and offers a new opportunity for yield estimation. The RSC dataset and source code are available online.
KW  - rice seedlings
KW  - object counting
KW  - computer vision
KW  - deep learning
KW  - fully convolutional neural networks
DO  - 10.3390/rs11060691
TY  - EJOU
AU  - Pang, Shiyan
AU  - Hu, Xiangyun
AU  - Zhang, Mi
AU  - Cai, Zhongliang
AU  - Liu, Fengzhu
TI  - Co-Segmentation and Superpixel-Based Graph Cuts for Building Change Detection from Bi-Temporal Digital Surface Models and Aerial Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Thanks to the recent development of laser scanner hardware and the technology of dense image matching (DIM), the acquisition of three-dimensional (3D) point cloud data has become increasingly convenient. However, how to effectively combine 3D point cloud data and images to realize accurate building change detection is still a hotspot in the field of photogrammetry and remote sensing. Therefore, with the bi-temporal aerial images and point cloud data obtained by airborne laser scanner (ALS) or DIM as the data source, a novel building change detection method combining co-segmentation and superpixel-based graph cuts is proposed in this paper. In this method, the bi-temporal point cloud data are firstly combined to achieve a co-segmentation to obtain bi-temporal superpixels with the simple linear iterative clustering (SLIC) algorithm. Secondly, for each period of aerial images, semantic segmentation based on a deep convolutional neural network is used to extract building areas, and this is the basis for subsequent superpixel feature extraction. Again, with the bi-temporal superpixel as the processing unit, a graph-cuts-based building change detection algorithm is proposed to extract the changed buildings. In this step, the building change detection problem is modeled as two binary classifications, and acquisition of each period&rsquo;s changed buildings is a binary classification, in which the changed building is regarded as foreground and the other area as background. Then, the graph cuts algorithm is used to obtain the optimal solution. Next, by combining the bi-temporal changed buildings and digital surface models (DSMs), these changed buildings are further classified as &ldquo;newly built,&rdquo; &ldquo;taller,&rdquo; &ldquo;demolished&rdquo;, and &ldquo;lower&rdquo;. Finally, two typical datasets composed of bi-temporal aerial images and point cloud data obtained by ALS or DIM are used to validate the proposed method, and the experiments demonstrate the effectiveness and generality of the proposed algorithm.
KW  - building change detection
KW  - co-segmentation
KW  - graph cuts
KW  - digital surface models
KW  - aerial images
DO  - 10.3390/rs11060729
TY  - EJOU
AU  - Windrim, Lloyd
AU  - Bryson, Mitch
AU  - McLean, Michael
AU  - Randle, Jeremy
AU  - Stone, Christine
TI  - Automated Mapping of Woody Debris over Harvested Forest Plantations Using UAVs, High-Resolution Imagery, and Machine Learning
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Surveying of woody debris left over from harvesting operations on managed forests is an important step in monitoring site quality, managing the extraction of residues and reconciling differences in pre-harvest inventories and actual timber yields. Traditional methods for post-harvest survey involving manual assessment of debris on the ground over small sample plots are labor-intensive, time-consuming, and do not scale well to heterogeneous landscapes. In this paper, we propose and evaluate new automated methods for the collection and interpretation of high-resolution, Unmanned Aerial Vehicle (UAV)-borne imagery over post-harvested forests for estimating quantities of fine and coarse woody debris. Using high-resolution, geo-registered color mosaics generated from UAV-borne images, we develop manual and automated processing methods for detecting, segmenting and counting both fine and coarse woody debris, including tree stumps, exploiting state-of-the-art machine learning and image processing techniques. Results are presented using imagery over a post-harvested compartment in a Pinus radiata plantation and demonstrate the capacity for both manual image annotations and automated image processing to accurately detect and quantify coarse woody debris and stumps left over after harvest, providing a cost-effective and scalable survey method for forest managers.
KW  - Unmanned Aerial Vehicles (UAVs)
KW  - computer vision
KW  - forestry
KW  - Coarse Woody Debris (CWD)
KW  - Convolutional Neural Networks (CNNs)
DO  - 10.3390/rs11060733
TY  - EJOU
AU  - Maimaitiyiming, Matthew
AU  - Sagan, Vasit
AU  - Sidike, Paheding
AU  - Kwasniewski, Misha T.
TI  - Dual Activation Function-Based Extreme Learning Machine (ELM) for Estimating Grapevine Berry Yield and Quality
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Reliable assessment of grapevine productivity is a destructive and time-consuming process. In addition, the mixed effects of grapevine water status and scion-rootstock interactions on grapevine productivity are not always linear. Despite the potential opportunity of applying remote sensing and machine learning techniques to predict plant traits, there are still limitations to previously studied techniques for vine productivity due to the complexity of the system not being adequately modeled. During the 2014 and 2015 growing seasons, hyperspectral reflectance spectra were collected using a handheld spectroradiometer in a vineyard designed to investigate the effects of irrigation level (0%, 50%, and 100%) and rootstocks (1103 Paulsen, 3309 Couderc, SO4 and Chambourcin) on vine productivity. To assess vine productivity, it is necessary to measure factors related to fruit ripeness and not just yield, as an over cropped vine may produce high-yield but poor-quality fruit. Therefore, yield, Total Soluble Solids (TSS), Titratable Acidity (TA) and the ratio TSS/TA (maturation index, IMAD) were measured. A total of 20 vegetation indices were calculated from hyperspectral data and used as input for predictive model calibration. Prediction performance of linear/nonlinear multiple regression methods and Weighted Regularized Extreme Learning Machine (WRELM) were compared with our newly developed WRELM-TanhRe. The developed method is based on two activation functions: hyperbolic tangent (Tanh) and rectified linear unit (ReLU). The results revealed that WRELM and WRELM-TanhRe outperformed the widely used multiple regression methods when model performance was tested with an independent validation dataset. WRELM-TanhRe produced the highest prediction accuracy for all the berry yield and quality parameters (R2 of 0.522&ndash;0.682 and RMSE of 2&ndash;15%), except for TA, which was predicted best with WRELM (R2 of 0.545 and RMSE of 6%). The results demonstrate the value of combining hyperspectral remote sensing and machine learning methods for improving of berry yield and quality prediction.
KW  - grapevine productivity
KW  - hyperspectral reflectance
KW  - stress
KW  - rootstock
KW  - vegetation indices
KW  - WRELM-TanhRe
KW  - neural network
KW  - activation function
DO  - 10.3390/rs11070740
TY  - EJOU
AU  - Gebrehiwot, Asmamaw
AU  - Hashemi-Beni, Leila
AU  - Thompson, Gary
AU  - Kordjamshidi, Parisa
AU  - Langan, Thomas E.
TI  - Deep Convolutional Neural Network for Flood Extent Mapping Using Unmanned Aerial Vehicles Data
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Flooding is one of the leading threats of natural disasters to human life and property, especially in densely populated urban areas. Rapid and precise extraction of the flooded areas is key to supporting emergency-response planning and providing damage assessment in both spatial and temporal measurements. Unmanned Aerial Vehicles (UAV) technology has recently been recognized as an efficient photogrammetry data acquisition platform to quickly deliver high-resolution imagery because of its cost-effectiveness, ability to fly at lower altitudes, and ability to enter a hazardous area. Different image classification methods including SVM (Support Vector Machine) have been used for flood extent mapping. In recent years, there has been a significant improvement in remote sensing image classification using Convolutional Neural Networks (CNNs). CNNs have demonstrated excellent performance on various tasks including image classification, feature extraction, and segmentation. CNNs can learn features automatically from large datasets through the organization of multi-layers of neurons and have the ability to implement nonlinear decision functions. This study investigates the potential of CNN approaches to extract flooded areas from UAV imagery. A VGG-based fully convolutional network (FCN-16s) was used in this research. The model was fine-tuned and a k-fold cross-validation was applied to estimate the performance of the model on the new UAV imagery dataset. This approach allowed FCN-16s to be trained on the datasets that contained only one hundred training samples, and resulted in a highly accurate classification. Confusion matrix was calculated to estimate the accuracy of the proposed method. The image segmentation results obtained from FCN-16s were compared from the results obtained from FCN-8s, FCN-32s and SVMs. Experimental results showed that the FCNs could extract flooded areas precisely from UAV images compared to the traditional classifiers such as SVMs. The classification accuracy achieved by FCN-16s, FCN-8s, FCN-32s, and SVM for the water class was 97.52%, 97.8%, 94.20% and 89%, respectively.
KW  - remote sensing
KW  - convolutional neural networks
KW  - floodplain mapping
KW  - fully convolutional network
KW  - unmanned aerial vehicles
KW  - geospatial data processing
DO  - 10.3390/s19071486
TY  - EJOU
AU  - Fernandez-Gallego, Jose A.
AU  - Buchaillot, Ma. L.
AU  - Aparicio Gutiérrez, Nieves
AU  - Nieto-Taladriz, María T.
AU  - Araus, José L.
AU  - Kefauver, Shawn C.
TI  - Automatic Wheat Ear Counting Using Thermal Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Ear density is one of the most important agronomical yield components in wheat. Ear counting is time-consuming and tedious as it is most often conducted manually in field conditions. Moreover, different sampling techniques are often used resulting in a lack of standard protocol, which may eventually affect inter-comparability of results. Thermal sensors capture crop canopy features with more contrast than RGB sensors for image segmentation and classification tasks. An automatic thermal ear counting system is proposed to count the number of ears using zenithal/nadir thermal images acquired from a moderately high resolution handheld thermal camera. Three experimental sites under different growing conditions in Spain were used on a set of 24 varieties of durum wheat for this study. The automatic pipeline system developed uses contrast enhancement and filter techniques to segment image regions detected as ears. The approach is based on the temperature differential between the ears and the rest of the canopy, given that ears usually have higher temperatures due to their lower transpiration rates. Thermal images were acquired, together with RGB images and in situ (i.e., directly in the plot) visual ear counting from the same plot segment for validation purposes. The relationship between the thermal counting values and the in situ visual counting was fairly weak (R2 = 0.40), which highlights the difficulties in estimating ear density from one single image-perspective. However, the results show that the automatic thermal ear counting system performed quite well in counting the ears that do appear in the thermal images, exhibiting high correlations with the manual image-based counts from both thermal and RGB images in the sub-plot validation ring (R2 = 0.75&ndash;0.84). Automatic ear counting also exhibited high correlation with the manual counting from thermal images when considering the complete image (R2 = 0.80). The results also show a high correlation between the thermal and the RGB manual counting using the validation ring (R2 = 0.83). Methodological requirements and potential limitations of the technique are discussed.
KW  - thermal images
KW  - ear counting
KW  - digital image processing
KW  - wheat
DO  - 10.3390/rs11070751
TY  - EJOU
AU  - Zhao, Zhenbing
AU  - Zhen, Zhen
AU  - Zhang, Lei
AU  - Qi, Yincheng
AU  - Kong, Yinghui
AU  - Zhang, Ke
TI  - Insulator Detection Method in Inspection Image Based on Improved Faster R-CNN
T2  - Energies

PY  - 2019
VL  - 12
IS  - 7
SN  - 1996-1073

AB  - The detection of insulators in power transmission and transformation inspection images is the basis for insulator state detection and fault diagnosis in thereafter. Aiming at the detection of insulators with different aspect ratios and scales and ones with mutual occlusion, a method of insulator inspection image based on the improved faster region-convolutional neural network (R-CNN) is put forward in this paper. By constructing a power transmission and transformation insulation equipment detection dataset and fine-tuning the faster R-CNN model, the anchor generation method and non-maximum suppression (NMS) in the region proposal network (RPN) of the faster R-CNN model were improved, thus realizing a better detection of insulators. The experimental results show that the average precision (AP) value of the faster R-CNN model was increased to 0.818 with the improved anchor generation method under the VGG-16 Net. In addition, the detection effect of different aspect ratios and different scales of insulators in the inspection images was improved significantly, and the occlusion of insulators could be effectively distinguished and detected using the improved NMS.
KW  - insulator
KW  - Faster R-CNN
KW  - object detection
KW  - RPN
KW  - deep learning
DO  - 10.3390/en12071204
TY  - EJOU
AU  - Li, Yangyang
AU  - Wang, Ximing
AU  - Liu, Dianxiong
AU  - Guo, Qiuju
AU  - Liu, Xin
AU  - Zhang, Jie
AU  - Xu, Yitao
TI  - On the Performance of Deep Reinforcement Learning-Based Anti-Jamming Method Confronting Intelligent Jammer
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 7
SN  - 2076-3417

AB  - With the development of access technologies and artificial intelligence, a deep reinforcement learning (DRL) algorithm is proposed into channel accessing and anti-jamming. Assuming the jamming modes are sweeping, comb, dynamic and statistic, the DRL-based method through training can almost perfectly avoid jamming signal and communicate successfully. Instead, in this paper, from the perspective of jammers, we investigate the performance of a DRL-based anti-jamming method. First of all, we design an intelligent jamming method based on reinforcement learning to combat the DRL-based user. Then, we theoretically analyze the condition when the DRL-based anti-jamming algorithm cannot converge, and provide the proof. Finally, in order to investigate the performance of DRL-based method, various scenarios where users with different communicating modes combat jammers with different jamming modes are compared. As the simulation results show, the theoretical analysis is verified, and the proposed RL-based jamming can effectively restrict the performance of DRL-based anti-jamming method.
KW  - deep reinforcement learning
KW  - Q-learning
KW  - intelligent anti-jamming
KW  - intelligent jamming
DO  - 10.3390/app9071361
TY  - EJOU
AU  - Ostovar, Ahmad
AU  - Talbot, Bruce
AU  - Puliti, Stefano
AU  - Astrup, Rasmus
AU  - Ringdahl, Ola
TI  - Detection and Classification of Root and Butt-Rot (RBR) in Stumps of Norway Spruce Using RGB Images and Machine Learning
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Root and butt-rot (RBR) has a significant impact on both the material and economic outcome of timber harvesting, and therewith on the individual forest owner and collectively on the forest and wood processing industries. An accurate recording of the presence of RBR during timber harvesting would enable a mapping of the location and extent of the problem, providing a basis for evaluating spread in a climate anticipated to enhance pathogenic growth in the future. Therefore, a system to automatically identify and detect the presence of RBR would constitute an important contribution to addressing the problem without increasing workload complexity for the machine operator. In this study, we developed and evaluated an approach based on RGB images to automatically detect tree stumps and classify them as to the absence or presence of rot. Furthermore, since knowledge of the extent of RBR is valuable in categorizing logs, we also classify stumps into three classes of infestation; rot = 0%, 0% &lt; rot &lt; 50% and rot &ge; 50%. In this work we used deep-learning approaches and conventional machine-learning algorithms for detection and classification tasks. The results showed that tree stumps were detected with precision rate of 95% and recall of 80%. Using only the correct output (TP) of the stump detector, stumps without and with RBR were correctly classified with accuracy of 83.5% and 77.5%, respectively. Classifying rot into three classes resulted in 79.4%, 72.4%, and 74.1% accuracy for stumps with rot = 0%, 0% &lt; rot &lt; 50%, and rot &ge; 50%, respectively. With some modifications, the developed algorithm could be used either during the harvesting operation to detect RBR regions on the tree stumps or as an RBR detector for post-harvest assessment of tree stumps and logs.
KW  - deep learning
KW  - forest harvesting
KW  - tree stumps
KW  - automatic detection and classification
DO  - 10.3390/s19071579
TY  - EJOU
AU  - Azabi, Yousef
AU  - Savvaris, Al
AU  - Kipouros, Timoleon
TI  - Artificial Intelligence to Enhance Aerodynamic Shape Optimisation of the Aegis UAV
T2  - Machine Learning and Knowledge Extraction

PY  - 2019
VL  - 1
IS  - 2
SN  - 2504-4990

AB  - This article presents an optimisation framework that uses stochastic multi-objective optimisation, combined with an Artificial Neural Network (ANN), and describes its application to the aerodynamic design of aircraft shapes. The framework uses the Multi-Objective Particle Swarm Optimisation (MOPSO) algorithm and the obtained results confirm that the proposed technique provides highly optimal solutions in less computational time than other approaches to the same design problem. The main idea was to focus computational effort on worthwhile design solutions rather than exploring and evaluating all possible solutions in the design space. It is shown that the number of valid solutions obtained using ANN-MOPSO compared to MOPSO for 3000 evaluations grew from 529 to 1006 (90% improvement) with a penalty of only 8.3% (11 min) in computational time. It is demonstrated that including an ANN, the ANN-MOPSO with 3000 evaluations produced a larger number of valid solutions than the MOPSO with 5500 evaluations, and in 33% less computational time (64 min). This is taken as confirmation of the potential power of ANNs when applied to this type of design problem.
KW  - machine learning
KW  - data visualization
KW  - Multi-Objective Particle Swarm Optimisation
KW  - Multi-Objective Tabu Search
KW  - nimrod/tool
KW  - parallel coordinates
KW  - Athena Vortex Lattice
DO  - 10.3390/make1020033
TY  - EJOU
AU  - Hong, Suk-Ju
AU  - Han, Yunhyeok
AU  - Kim, Sang-Yeon
AU  - Lee, Ah-Yeong
AU  - Kim, Ghiseok
TI  - Application of Deep-Learning Methods to Bird Detection Using Unmanned Aerial Vehicle Imagery
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Wild birds are monitored with the important objectives of identifying their habitats and estimating the size of their populations. Especially in the case of migratory bird, they are significantly recorded during specific periods of time to forecast any possible spread of animal disease such as avian influenza. This study led to the construction of deep-learning-based object-detection models with the aid of aerial photographs collected by an unmanned aerial vehicle (UAV). The dataset containing the aerial photographs includes diverse images of birds in various bird habitats and in the vicinity of lakes and on farmland. In addition, aerial images of bird decoys are captured to achieve various bird patterns and more accurate bird information. Bird detection models such as Faster Region-based Convolutional Neural Network (R-CNN), Region-based Fully Convolutional Network (R-FCN), Single Shot MultiBox Detector (SSD), Retinanet, and You Only Look Once (YOLO) were created and the performance of all models was estimated by comparing their computing speed and average precision. The test results show Faster R-CNN to be the most accurate and YOLO to be the fastest among the models. The combined results demonstrate that the use of deep-learning-based detection methods in combination with UAV aerial imagery is fairly suitable for bird detection in various environments.
KW  - deep learning
KW  - convolutional neural networks
KW  - unmanned aerial vehicle
KW  - bird detection
DO  - 10.3390/s19071651
TY  - EJOU
AU  - Liao, Jianshang
AU  - Wang, Liguo
TI  - Hyperspectral Image Classification Based on Fusion of Curvature Filter and Domain Transform Recursive Filter
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - In recent decades, in order to enhance the performance of hyperspectral image classification, the spatial information of hyperspectral image obtained by various methods has become a research hotspot. For this work, it proposes a new classification method based on the fusion of two spatial information, which will be classified by a large margin distribution machine (LDM). First, the spatial texture information is extracted from the top of the principal component analysis for hyperspectral images by a curvature filter (CF). Second, the spatial correlation information of a hyperspectral image is completed by using domain transform recursive filter (DTRF). Last, the spatial texture information and correlation information are fused to be classified with LDM. The experimental results of hyperspectral images classification demonstrate that the proposed curvature filter and domain transform recursive filter with LDM(CFDTRF-LDM) method is superior to other classification methods.
KW  - hyperspectral image
KW  - classification
KW  - curvature filter
KW  - domain transform recursive filter
KW  - large margin distribution machine
DO  - 10.3390/rs11070833
TY  - EJOU
AU  - Mao, Huihui
AU  - Meng, Jihua
AU  - Ji, Fujiang
AU  - Zhang, Qiankun
AU  - Fang, Huiting
TI  - Comparison of Machine Learning Regression Algorithms for Cotton Leaf Area Index Retrieval Using Sentinel-2 Spectral Bands
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 7
SN  - 2076-3417

AB  - Leaf area index (LAI) is a crucial crop biophysical parameter that has been widely used in a variety of fields. Five state-of-the-art machine learning regression algorithms (MLRAs), namely, artificial neural network (ANN), support vector regression (SVR), Gaussian process regression (GPR), random forest (RF) and gradient boosting regression tree (GBRT), have been used in the retrieval of cotton LAI with Sentinel-2 spectral bands. The performances of the five machine learning models are compared for better applications of MLRAs in remote sensing, since challenging problems remain in the selection of MLRAs for crop LAI retrieval, as well as the decision as to the optimal number for the training sample size and spectral bands to different MLRAs. A comprehensive evaluation was employed with respect to model accuracy, computational efficiency, sensitivity to training sample size and sensitivity to spectral bands. We conducted the comparison of five MLRAs in an agricultural area of Northwest China over three cotton seasons with the corresponding field campaigns for modeling and validation. Results show that the GBRT model outperforms the other models with respect to model accuracy in average (       R 2   &macr;      = 0.854,       R M S E  &macr;      = 0.674 and       M A E  &macr;      = 0.456). SVR achieves the best performance in computational efficiency, which means it is fast to train, and to validate that it has great potentials to deliver near-real-time operational products for crop management. As for sensitivity to training sample size, GBRT behaves as the most robust model, and provides the best model accuracy on the average among the variations of training sample size, compared with other models (       R 2   &macr;      = 0.884,       R M S E  &macr;      = 0.615 and       M A E  &macr;      = 0.452). Spectral bands sensitivity analysis with dCor (distance correlation), combined with the backward elimination approach, indicates that SVR, GPR and RF provide relatively robust performance to the spectral bands, while ANN outperforms the other models in terms of model accuracy on the average among the reduction of spectral bands (       R 2   &macr;      = 0.881,       R M S E  &macr;      = 0.625 and       M A E  &macr;      = 0.480). A comprehensive evaluation indicates that GBRT is an appealing alternative for cotton LAI retrieval, except for its computational efficiency. Despite the different performance of the ML models, all models exhibited considerable potential for cotton LAI retrieval, which could offer accurate crop parameters information timely and accurately for crop fields management and agricultural production decisions.
KW  - leaf area index (LAI)
KW  - machine learning
KW  - Sentinel-2
KW  - sensitivity analysis
KW  - training sample size
KW  - spectral bands
DO  - 10.3390/app9071459
TY  - EJOU
AU  - Li, You
AU  - Zahran, Shady
AU  - Zhuang, Yuan
AU  - Gao, Zhouzheng
AU  - Luo, Yiran
AU  - He, Zhe
AU  - Pei, Ling
AU  - Chen, Ruizhi
AU  - El-Sheimy, Naser
TI  - IMU/Magnetometer/Barometer/Mass-Flow Sensor Integrated Indoor Quadrotor UAV Localization with Robust Velocity Updates
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Velocity updates have been proven to be important for constraining motion-sensor-based dead-reckoning (DR) solutions in indoor unmanned aerial vehicle (UAV) applications. The forward velocity from a mass flow sensor and the lateral and vertical non-holonomic constraints (NHC) can be utilized for three-dimensional (3D) velocity updates. However, it is observed that (a) the quadrotor UAV may have a vertical velocity trend when it is controlled to move horizontally; (b) the quadrotor may have a pitch angle when moving horizontally; and (c) the mass flow sensor may suffer from sensor errors, especially the scale factor error. Such phenomenons degrade the performance of velocity updates. Thus, this paper presents a multi-sensor integrated localization system that has more effective sensor interactions. Specifically, (a) the barometer data are utilized to detect height changes and thus determine the weight of vertical velocity update; (b) the pitch angle from the inertial measurement unit (IMU) and magnetometer data fusion is used to set the weight of forward velocity update; and (c) an extra mass flow sensor calibration module is introduced. Indoor flight tests have indicated the effectiveness of the proposed sensor interaction strategies in enhancing indoor quadrotor DR solutions, which can also be used for detecting outliers in external localization technologies such as ultrasonics.
KW  - indoor localization
KW  - quadrotor UAV
KW  - air flow
KW  - inertial sensor
KW  - magnetometer
KW  - barometer
KW  - ultrasonic
KW  - Kalman filter
DO  - 10.3390/rs11070838
TY  - EJOU
AU  - Liu, Yao
AU  - Shi, Jianmai
AU  - Liu, Zhong
AU  - Huang, Jincai
AU  - Zhou, Tianren
TI  - Two-Layer Routing for High-Voltage Powerline Inspection by Cooperated Ground Vehicle and Drone
T2  - Energies

PY  - 2019
VL  - 12
IS  - 7
SN  - 1996-1073

AB  - A novel high-voltage powerline inspection system was investigated, which consists of the cooperated ground vehicle and drone. The ground vehicle acts as a mobile platform that can launch and recycle the drone, while the drone can fly over the powerline for inspection within limited endurance. This inspection system enables the drone to inspect powerline networks in a very large area. Both vehicle&rsquo; route in the road network and drone&rsquo;s routes along the powerline network have to be optimized for improving the inspection efficiency, which generates a new Two-Layer Point-Arc Routing Problem (2L-PA-RP). Two constructive heuristics were designed based on &ldquo;Cluster First, Route Second&rdquo; and &ldquo;Route First, Split Second&rdquo;. Then, local search strategies were developed to further improve the quality of the solution. To test the performance of the proposed algorithms, different-scale practical cases were designed based on the road network and powerline network of Ji&rsquo;an, China. Sensitivity analysis on the parameters related to the drone&rsquo;s inspection speed and battery capacity was conducted. Computational results indicate that technical improvement on the inspection sensor is more important for the cooperated ground vehicle and drone system.
KW  - high-voltage powerline inspection
KW  - vehicle routing
KW  - arc routing
KW  - drone
KW  - heuristic
DO  - 10.3390/en12071385
TY  - EJOU
AU  - Khan, Nabeel
AU  - Martini, Maria G.
TI  - Bandwidth Modeling of Silicon Retinas for Next Generation Visual Sensor Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - Silicon retinas, also known as Dynamic Vision Sensors (DVS) or event-based visual sensors, have shown great advantages in terms of low power consumption, low bandwidth, wide dynamic range and very high temporal resolution. Owing to such advantages as compared to conventional vision sensors, DVS devices are gaining more and more attention in various applications such as drone surveillance, robotics, high-speed motion photography, etc. The output of such sensors is a sequence of events rather than a series of frames as for classical cameras. Estimating the data rate of the stream of events associated with such sensors is needed for the appropriate design of transmission systems involving such sensors. In this work, we propose to consider information about the scene content and sensor speed to support such estimation, and we identify suitable metrics to quantify the complexity of the scene for this purpose. According to the results of this study, the event rate shows an exponential relationship with the metric associated with the complexity of the scene and linear relationships with the speed of the sensor. Based on these results, we propose a two-parameter model for the dependency of the event rate on scene complexity and sensor speed. The model achieves a prediction accuracy of approximately 88.4% for the outdoor environment along with the overall prediction performance of approximately 84%.
KW  - neuromorphic engineering
KW  - dynamic and active-pixel vision sensor
KW  - scene complexity
KW  - neuromorphic event rate
KW  - gradient approximation
KW  - scene texture
KW  - Sobel
KW  - Roberts
KW  - Prewitt
DO  - 10.3390/s19081751
TY  - EJOU
AU  - Zhang, Hehu
AU  - Wang, Xiushan
AU  - Chen, Ying
AU  - Jiang, Guoqiang
AU  - Lin, Shifeng
TI  - Research on Vision-Based Navigation for Plant Protection UAV under the Near Color Background
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 4
SN  - 2073-8994

AB  - GPS (Global Positioning System) navigation in agriculture is facing many challenges, such as weak signals in orchards and the high cost for small plots of farmland. With the reduction of camera cost and the emergence of excellent visual algorithms, visual navigation can solve the above problems. Visual navigation is a navigation technology that uses cameras to sense environmental information as the basis of an aircraft flight. It is mainly divided into five parts: Image acquisition, landmark recognition, route planning, flight control, and obstacle avoidance. Here, landmarks are plant canopy, buildings, mountains, and rivers, with unique geographical characteristics in a place. During visual navigation, landmark location and route tracking are key links. When there are significant color-differences (for example, the differences among red, green, and blue) between a landmark and the background, the landmark can be recognized based on classical visual algorithms. However, in the case of non-significant color-differences (for example, the differences between dark green and vivid green) between a landmark and the background, there are no robust and high-precision methods for landmark identification. In view of the above problem, visual navigation in a maize field is studied. First, the block recognition method based on fine-tuned Inception-V3 is developed; then, the maize canopy landmark is recognized based on the above method; finally, local navigation lines are extracted from the landmarks based on the maize canopy grayscale gradient law. The results show that the accuracy is 0.9501. When the block number is 256, the block recognition method achieves the best segmentation. The average segmentation quality is 0.87, and time is 0.251 s. This study suggests that stable visual semantic navigation can be achieved under the near color background. It will be an important reference for the navigation of plant protection UAV (Unmanned Aerial Vehicle).
KW  - landmark location
KW  - route tracking
KW  - inception-V3
KW  - visual navigation
KW  - grayscale gradient law
DO  - 10.3390/sym11040533
TY  - EJOU
AU  - Yang, Lingyu
AU  - Feng, Xiaoke
AU  - Zhang, Jing
AU  - Shu, Xiangqian
TI  - Multi-Ray Modeling of Ultrasonic Sensors and Application for Micro-UAV Localization in Indoor Environments
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - Due to its payload, size and computational limits, localizing a micro air vehicle (MAV) using only its onboard sensors in an indoor environment is a challenging problem in practice. This paper introduces an indoor localization approach that relies on only the inertial measurement unit (IMU) and four ultrasonic sensors. Specifically, a novel multi-ray ultrasonic sensor model is proposed to provide a rapid and accurate approximation of the complex beam pattern of the ultrasonic sensors. A fast algorithm for calculating the Jacobian matrix of the measurement function is presented, and then an extended Kalman filter (EKF) is used to fuse the information from the ultrasonic sensors and the IMU. A test based on a MaxSonar MB1222 sensor demonstrates the accuracy of the model, and a simulation and experiment based on the     T h a l e s  I I     MAV platform are conducted. The results indicate good localization performance and robustness against measurement noises.
KW  - indoor location
KW  - multi-ray model of ultrasonic sensors
KW  - micro-UAV
KW  - extended Kalman filter
DO  - 10.3390/s19081770
TY  - EJOU
AU  - Guo, Qiang
AU  - Yu, Xin
AU  - Ruan, Guoqing
TI  - LPI Radar Waveform Recognition Based on Deep Convolutional Neural Network Transfer Learning
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 4
SN  - 2073-8994

AB  - Low Probability of Intercept (LPI) radar waveform recognition is not only an important branch of the electronic reconnaissance field, but also an important means to obtain non-cooperative radar information. To solve the problems of LPI radar waveform recognition rate, difficult feature extraction and large number of samples needed, an automatic classification and recognition system based on Choi-Williams distribution (CWD) and depth convolution neural network migration learning is proposed in this paper. First, the system performs CWD time-frequency transform on the LPI radar waveform to obtain a 2-D time-frequency image. Then the system preprocesses the original time-frequency image. In addition, then the system sends the pre-processed image to the pre-training model (Inception-v3 or ResNet-152) of the deep convolution network for feature extraction. Finally, the extracted features are sent to a Support Vector Machine (SVM) classifier to realize offline training and online recognition of radar waveforms. The simulation results show that the overall recognition rate of the eight LPI radar signals (LFM, BPSK, Costas, Frank, and T1&ndash;T4) of the ResNet-152-SVM system reaches 97.8%, and the overall recognition rate of the Inception-v3-SVM system reaches 96.2% when the SNR is &minus;2 dB.
KW  - Low Probability of Intercept
KW  - CWD time-frequency analysis
KW  - Inception-v3
KW  - ResNet-152
KW  - transfer learning
DO  - 10.3390/sym11040540
TY  - EJOU
AU  - Zhu, Jiasong
AU  - Chen, Siyuan
AU  - Tu, Wei
AU  - Sun, Ke
TI  - Tracking and Simulating Pedestrian Movements at Intersections Using Unmanned Aerial Vehicles
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 8
SN  - 2072-4292

AB  - For a city to be livable and walkable is the ultimate goal of future cities. However, conflicts among pedestrians, vehicles, and cyclists at traffic intersections are becoming severe in high-density urban transportation areas, especially in China. Correspondingly, the transit time at intersections is becoming prolonged, and pedestrian safety is becoming endangered. Simulating pedestrian movements at complex traffic intersections is necessary to optimize the traffic organization. We propose an unmanned aerial vehicle (UAV)-based method for tracking and simulating pedestrian movements at intersections. Specifically, high-resolution videos acquired by a UAV are used to recognize and position moving targets, including pedestrians, cyclists, and vehicles, using the convolutional neural network. An improved social force-based motion model is proposed, considering the conflicts among pedestrians, cyclists, and vehicles. In addition, maximum likelihood estimation is performed to calibrate an improved social force model. UAV videos of intersections in Shenzhen are analyzed to demonstrate the performance of the presented approach. The results demonstrate that the proposed social force-based motion model can effectively simulate the movement of pedestrians and cyclists at road intersections. The presented approach provides an alternative method to track and simulate pedestrian movements, thus benefitting the organization of pedestrian flow and traffic signals controlling the intersections.
KW  - pedestrian simulation
KW  - social force model
KW  - intersection
KW  - UAV
KW  - convolutional neural network
KW  - deep learning
DO  - 10.3390/rs11080925
TY  - EJOU
AU  - Tzitzilonis, Vasileios
AU  - Malandrakis, Konstantinos
AU  - Zanotti Fragonara, Luca
AU  - Gonzalez Domingo, Jose A.
AU  - Avdelidis, Nicolas P.
AU  - Tsourdos, Antonios
AU  - Forster, Kevin
TI  - Inspection of Aircraft Wing Panels Using Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - In large civil aircraft manufacturing, a time-consuming post-production process is the non-destructive inspection of wing panels. This work aims to address this challenge and improve the defects&rsquo; detection by performing automated aerial inspection using a small off-the-shelf multirotor. The UAV is equipped with a wide field-of-view camera and an ultraviolet torch for implementing non-invasive imaging inspection. In particular, the UAV is programmed to perform the complete mission and stream video, in real-time, to the ground control station where the defects&rsquo; detection algorithm is executed. The proposed platform was mathematically modelled in MATLAB/SIMULINK in order to assess the behaviour of the system using a path following method during the aircraft wing inspection. In addition, two defect detection algorithms were implemented and tested on a dataset containing images obtained during inspection at Airbus facilities. The results show that for the current dataset the proposed methods can identify all the images containing defects.
KW  - Non-Destructive Testing
KW  - ultraviolet light
KW  - automated inspection
KW  - defects detection
KW  - UAV
KW  - image processing
DO  - 10.3390/s19081824
TY  - EJOU
AU  - Hu, Zhongyang
AU  - Dietz, Andreas J.
AU  - Kuenzer, Claudia
TI  - Deriving Regional Snow Line Dynamics during the Ablation Seasons 1984–2018 in European Mountains
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 8
SN  - 2072-4292

AB  - Snowmelt in the mid-latitude European mountains is undergoing significant spatiotemporal changes. Regional snow line elevation (RSLE) is an appropriate indicator for assessing snow cover variations in mountain areas. To derive regional snow line dynamics during the ablation seasons 1984&ndash;2018, the present study unprecedentedly introduced a readily applicable framework. The framework constitutes four steps: atmospheric and topographic correction, snow classification, RSLE retrieval, and regional snow line retreat curve (RSLRC) derivation. The developed framework has been successfully applied to 8641 satellite images acquired by Landsat, ASTER, and Sentinel-2. The results of the intra-annual regional snow line variations show that: (1) regional snow lines in the Alpine catchments preserve the longest; (2) RSLEs are lower in the northern Pyrenees than in the southern part; (3) regional snow lines persist the shortest in the Carpathian catchments; and (4) during the end of the ablation season 2018, intermediate snowfall events in the catchments Adda, Tagliamento, and Uzh are observed. In terms of the long-term inter-annual variations, significantly accelerating snow line recession is detected in the northern Pyrenean catchment Ariege. In the Alpine catchment Alpenrhein and Drac, RSLRCs are shifting towards lower accumulated air-temperature (AT) significantly, with the magnitude of &minus;3.77 &deg;C&middot;a&minus;1 (Alpenrhein) and &minus;3.99 &deg;C&middot;a&minus;1 (Drac).
KW  - snow line dynamics
KW  - European mountains
KW  - ablation season
KW  - regional snow line elevation (RSLE)
KW  - regional snow line retreat curves (RSLRCs)
KW  - M-estimation
KW  - Landsat
KW  - ASTER
KW  - Sentinel-2
KW  - time-series
DO  - 10.3390/rs11080933
TY  - EJOU
AU  - Nilwong, Sivapong
AU  - Hossain, Delowar
AU  - Kaneko, Shin-ichiro
AU  - Capi, Genci
TI  - Deep Learning-Based Landmark Detection for Mobile Robot Outdoor Localization
T2  - Machines

PY  - 2019
VL  - 7
IS  - 2
SN  - 2075-1702

AB  - Outdoor mobile robot applications generally implement Global Positioning Systems (GPS) for localization tasks. However, GPS accuracy in outdoor localization has less accuracy in different environmental conditions. This paper presents two outdoor localization methods based on deep learning and landmark detection. The first localization method is based on the Faster Regional-Convolutional Neural Network (Faster R-CNN) landmark detection in the captured image. Then, a feedforward neural network (FFNN) is trained to determine robot location coordinates and compass orientation from detected landmarks. The second localization employs a single convolutional neural network (CNN) to determine location and compass orientation from the whole image. The dataset consists of images, geolocation data and labeled bounding boxes to train and test two proposed localization methods. Results are illustrated with absolute errors from the comparisons between localization results and reference geolocation data in the dataset. The experimental results pointed both presented localization methods to be promising alternatives to GPS for outdoor localization.
KW  - outdoor localization
KW  - deep learning
KW  - landmark detection
KW  - Faster R-CNN
KW  - CNN
DO  - 10.3390/machines7020025
TY  - EJOU
AU  - Barbedo, Jayme G.
TI  - A Review on the Use of Unmanned Aerial Vehicles and Imaging Sensors for Monitoring and Assessing Plant Stresses
T2  - Drones

PY  - 2019
VL  - 3
IS  - 2
SN  - 2504-446X

AB  - Unmanned aerial vehicles (UAVs) are becoming a valuable tool to collect data in a variety of contexts. Their use in agriculture is particularly suitable, as those areas are often vast, making ground scouting difficult, and sparsely populated, which means that injury and privacy risks are not as important as in urban settings. Indeed, the use of UAVs for monitoring and assessing crops, orchards, and forests has been growing steadily during the last decade, especially for the management of stresses such as water, diseases, nutrition deficiencies, and pests. This article presents a critical overview of the main advancements on the subject, focusing on the strategies that have been used to extract the information contained in the images captured during the flights. Based on the information found in more than 100 published articles and on our own research, a discussion is provided regarding the challenges that have already been overcome and the main research gaps that still remain, together with some suggestions for future research.
KW  - drone
KW  - UAV
KW  - UAS
KW  - precision agriculture
KW  - stress
KW  - crop
KW  - orchard
DO  - 10.3390/drones3020040
TY  - EJOU
AU  - Pham, Tien D.
AU  - Xia, Junshi
AU  - Ha, Nam T.
AU  - Bui, Dieu T.
AU  - Le, Nga N.
AU  - Tekeuchi, Wataru
TI  - A Review of Remote Sensing Approaches for Monitoring Blue Carbon Ecosystems: Mangroves, Seagrassesand Salt Marshes during 2010–2018
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - Blue carbon (BC) ecosystems are an important coastal resource, as they provide a range of goods and services to the environment. They play a vital role in the global carbon cycle by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, there has been a large reduction in the global BC ecosystems due to their conversion to agriculture and aquaculture, overexploitation, and removal for human settlements. Effectively monitoring BC ecosystems at large scales remains a challenge owing to practical difficulties in monitoring and the time-consuming field measurement approaches used. As a result, sensible policies and actions for the sustainability and conservation of BC ecosystems can be hard to implement. In this context, remote sensing provides a useful tool for mapping and monitoring BC ecosystems faster and at larger scales. Numerous studies have been carried out on various sensors based on optical imagery, synthetic aperture radar (SAR), light detection and ranging (LiDAR), aerial photographs (APs), and multispectral data. Remote sensing-based approaches have been proven effective for mapping and monitoring BC ecosystems by a large number of studies. However, to the best of our knowledge, this is the first comprehensive review on the applications of remote sensing techniques for mapping and monitoring BC ecosystems. The main goal of this review is to provide an overview and summary of the key studies undertaken from 2010 onwards on remote sensing applications for mapping and monitoring BC ecosystems. Our review showed that optical imagery, such as multispectral and hyper-spectral data, is the most common for mapping BC ecosystems, while the Landsat time-series are the most widely-used data for monitoring their changes on larger scales. We investigate the limitations of current studies and suggest several key aspects for future applications of remote sensing combined with state-of-the-art machine learning techniques for mapping coastal vegetation and monitoring their extents and changes.
KW  - coastal ecosystems
KW  - remote sensing
KW  - blue carbon
KW  - mangroves
KW  - seagrasses
KW  - salt marshes
DO  - 10.3390/s19081933
TY  - EJOU
AU  - Quirós Vargas, Juan J.
AU  - Zhang, Chongyuan
AU  - Smitchger, Jamin A.
AU  - McGee, Rebecca J.
AU  - Sankaran, Sindhuja
TI  - Phenotyping of Plant Biomass and Performance Traits Using Remote Sensing Techniques in Pea (Pisum sativum, L.)
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 9
SN  - 1424-8220

AB  - Field pea cultivars are constantly improved through breeding programs to enhance biotic and abiotic stress tolerance and increase seed yield potential. In pea breeding, the Above Ground Biomass (AGBM) is assessed due to its influence on seed yield, canopy closure, and weed suppression. It is also the primary yield component for peas used as a cover crop and/or grazing. Measuring AGBM is destructive and labor-intensive process. Sensor-based phenotyping of such traits can greatly enhance crop breeding efficiency. In this research, high resolution RGB and multispectral images acquired with unmanned aerial systems were used to assess phenotypes in spring and winter pea breeding plots. The Green Red Vegetation Index (GRVI), Normalized Difference Vegetation Index (NDVI), Normalized Difference Red Edge Index (NDRE), plot volume, canopy height, and canopy coverage were extracted from RGB and multispectral information at five imaging times (between 365 to 1948 accumulated degree days/ADD after 1 May) in four winter field pea experiments and at three imaging times (between 1231 to 1648 ADD) in one spring field pea experiment. The image features were compared to ground-truth data including AGBM, lodging, leaf type, days to 50% flowering, days to physiological maturity, number of the first reproductive node, and seed yield. In two of the winter pea experiments, a strong correlation between image features and seed yield was observed at 1268 ADD (flowering). An increase in correlation between image features with the phenological traits such as days to 50% flowering and days to physiological maturity was observed at about 1725 ADD in these winter pea experiments. In the spring pea experiment, the plot volume estimated from images was highly correlated with ground truth canopy height (r = 0.83) at 1231 ADD. In two other winter pea experiments and the spring pea experiment, the GRVI and NDVI features were significantly correlated with AGBM at flowering. When selected image features were used to develop a least absolute shrinkage and selection operator model for AGBM estimation, the correlation coefficient between the actual and predicted AGBM was 0.60 and 0.84 in the winter and spring pea experiments, respectively. A SPOT-6 satellite image (1.5 m resolution) was also evaluated for its applicability to assess biomass and seed yield. The image features extracted from satellite imagery showed significant correlation with seed yield in two winter field pea experiments, however, the trend was not consistent. In summary, the study supports the potential of using unmanned aerial system-based imaging techniques to estimate biomass and crop performance in pea breeding programs.
KW  - crop monitoring
KW  - prediction model
KW  - satellite imagery
KW  - vegetation indices
KW  - crop surface model
DO  - 10.3390/s19092031
TY  - EJOU
AU  - Li, Weijia
AU  - He, Conghui
AU  - Fu, Haohuan
AU  - Zheng, Juepeng
AU  - Dong, Runmin
AU  - Xia, Maocai
AU  - Yu, Le
AU  - Luk, Wayne
TI  - A Real-Time Tree Crown Detection Approach for Large-Scale Remote Sensing Images on FPGAs
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - The on-board real-time tree crown detection from high-resolution remote sensing images is beneficial for avoiding the delay between data acquisition and processing, reducing the quantity of data transmission from the satellite to the ground, monitoring the growing condition of individual trees, and discovering the damage of trees as early as possible, etc. Existing high performance platform based tree crown detection studies either focus on processing images in a small size or suffer from high power consumption or slow processing speed. In this paper, we propose the first FPGA-based real-time tree crown detection approach for large-scale satellite images. A pipelined-friendly and resource-economic tree crown detection algorithm (PF-TCD) is designed through reconstructing and modifying the workflow of the original algorithm into three computational kernels on FPGAs. Compared with the well-optimized software implementation of the original algorithm on an Intel 12-core CPU, our proposed PF-TCD obtains the speedup of 18.75 times for a satellite image with a size of 12,188 &times; 12,576 pixels without reducing the detection accuracy. The image processing time for the large-scale remote sensing image is only 0.33 s, which satisfies the requirements of the on-board real-time data processing on satellites.
KW  - tree crown detection
KW  - high-resolution satellite images
KW  - field-programmable gate array (FPGA)
KW  - real-time processing
DO  - 10.3390/rs11091025
TY  - EJOU
AU  - Tyralis, Hristos
AU  - Papacharalampous, Georgia
AU  - Langousis, Andreas
TI  - A Brief Review of Random Forests for Water Scientists and Practitioners and Their Recent History in Water Resources
T2  - Water

PY  - 2019
VL  - 11
IS  - 5
SN  - 2073-4441

AB  - Random forests (RF) is a supervised machine learning algorithm, which has recently started to gain prominence in water resources applications. However, existing applications are generally restricted to the implementation of Breiman&rsquo;s original algorithm for regression and classification problems, while numerous developments could be also useful in solving diverse practical problems in the water sector. Here we popularize RF and their variants for the practicing water scientist, and discuss related concepts and techniques, which have received less attention from the water science and hydrologic communities. In doing so, we review RF applications in water resources, highlight the potential of the original algorithm and its variants, and assess the degree of RF exploitation in a diverse range of applications. Relevant implementations of random forests, as well as related concepts and techniques in the R programming language, are also covered.
KW  - classification
KW  - data-driven
KW  - hydrological modeling
KW  - hydrology
KW  - machine learning
KW  - prediction
KW  - quantile regression forests
KW  - supervised learning
KW  - variable importance metrics
DO  - 10.3390/w11050910
TY  - EJOU
AU  - Dorafshan, Sattar
AU  - Thomas, Robert J.
AU  - Maguire, Marc
TI  - Benchmarking Image Processing Algorithms for Unmanned Aerial System-Assisted Crack Detection in Concrete Structures
T2  - Infrastructures

PY  - 2019
VL  - 4
IS  - 2
SN  - 2412-3811

AB  - This paper summarizes the results of traditional image processing algorithms for detection of defects in concrete using images taken by Unmanned Aerial Systems (UASs). Such algorithms are useful for improving the accuracy of crack detection during autonomous inspection of bridges and other structures, and they have yet to be compared and evaluated on a dataset of concrete images taken by UAS. The authors created a generic image processing algorithm for crack detection, which included the major steps of filter design, edge detection, image enhancement, and segmentation, designed to uniformly compare different edge detectors. Edge detection was carried out by six filters in the spatial (Roberts, Prewitt, Sobel, and Laplacian of Gaussian) and frequency (Butterworth and Gaussian) domains. These algorithms were applied to fifty images each of defected and sound concrete. Performances of the six filters were compared in terms of accuracy, precision, minimum detectable crack width, computational time, and noise-to-signal ratio. In general, frequency domain techniques were slower than spatial domain methods because of the computational intensity of the Fourier and inverse Fourier transformations used to move between spatial and frequency domains. Frequency domain methods also produced noisier images than spatial domain methods. Crack detection in the spatial domain using the Laplacian of Gaussian filter proved to be the fastest, most accurate, and most precise method, and it resulted in the finest detectable crack width. The Laplacian of Gaussian filter in spatial domain is recommended for future applications of real-time crack detection using UAS.
KW  - structural condition assessment
KW  - concrete structures
KW  - unmanned aerial systems
KW  - crack detection
KW  - image processing
KW  - noncontact methods
DO  - 10.3390/infrastructures4020019
TY  - EJOU
AU  - He, Haiqing
AU  - Zhou, Junchao
AU  - Chen, Min
AU  - Chen, Ting
AU  - Li, Dajun
AU  - Cheng, Penggen
TI  - Building Extraction from UAV Images Jointly Using 6D-SLIC and Multiscale Siamese Convolutional Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - Automatic building extraction using a single data type, either 2D remotely-sensed images or light detection and ranging 3D point clouds, remains insufficient to accurately delineate building outlines for automatic mapping, despite active research in this area and the significant progress which has been achieved in the past decade. This paper presents an effective approach to extracting buildings from Unmanned Aerial Vehicle (UAV) images through the incorporation of superpixel segmentation and semantic recognition. A framework for building extraction is constructed by jointly using an improved Simple Linear Iterative Clustering (SLIC) algorithm and Multiscale Siamese Convolutional Networks (MSCNs). The SLIC algorithm, improved by additionally imposing a digital surface model for superpixel segmentation, namely 6D-SLIC, is suited for building boundary detection under building and image backgrounds with similar radiometric signatures. The proposed MSCNs, including a feature learning network and a binary decision network, are used to automatically learn a multiscale hierarchical feature representation and detect building objects under various complex backgrounds. In addition, a gamma-transform green leaf index is proposed to truncate vegetation superpixels for further processing to improve the robustness and efficiency of building detection, the Douglas&ndash;Peucker algorithm and iterative optimization are used to eliminate jagged details generated from small structures as a result of superpixel segmentation. In the experiments, the UAV datasets, including many buildings in urban and rural areas with irregular shapes and different heights and that are obscured by trees, are collected to evaluate the proposed method. The experimental results based on the qualitative and quantitative measures confirm the effectiveness and high accuracy of the proposed framework relative to the digitized results. The proposed framework performs better than state-of-the-art building extraction methods, given its higher values of recall, precision, and intersection over Union (IoU).
KW  - building extraction
KW  - simple linear iterative clustering (SLIC)
KW  - multiscale Siamese convolutional networks (MSCNs)
KW  - binary decision network
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/rs11091040
TY  - EJOU
AU  - Li, Zhiwei
AU  - Lu, Yu
AU  - Shi, Yun
AU  - Wang, Zengguang
AU  - Qiao, Wenxin
AU  - Liu, Yicen
TI  - A Dyna-Q-Based Solution for UAV Networks Against Smart Jamming Attacks
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 5
SN  - 2073-8994

AB  - Unmanned aerial vehicle (UAV) networks have a wide range of applications, such as in the Internet of Things (IoT), 5G communications, and so forth. However, the communications between UAVs and UAVs to ground control stations mainly use radio channels, and therefore these communications are vulnerable to cyberattacks. With the advent of software-defined radio (SDR), smart attacks that can flexibly select attack strategies according to the defender&rsquo;s state information are gradually attracting the attention of researchers and potential attackers of UAV networks. The smart attack can even induce the defender to take a specific defense strategy, causing even greater damage. Inspired by symmetrical thinking, a solution using a software-defined network (SDN) to combat software-defined radio was proposed. We propose a network architecture which uses dual controllers, including a UAV flight controller and SDN controller, to achieve collaborative decision-making. Built on the top of the SDN, the state information of the whole network converges quickly and is fitted to an environment model used to develop an improved Dyna-Q-based reinforcement learning algorithm. The improved algorithm integrates the power allocation and track planning of UAVs into a unified action space. The simulation data showed that the proposed communication solution can effectively avoid smart jamming attacks and has faster learning efficiency and higher convergence performance than the compared algorithms.
KW  - UAV networks
KW  - SDN
KW  - reinforcement learning
KW  - Dyna-Q
KW  - IoT
KW  - cyberattacks
DO  - 10.3390/sym11050617
TY  - EJOU
AU  - Rahnemoonfar, Maryam
AU  - Dobbs, Dugan
AU  - Yari, Masoud
AU  - Starek, Michael J.
TI  - DisCountNet: Discriminating and Counting Network for Real-Time Counting and Localization of Sparse Objects in High-Resolution UAV Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - Recent deep-learning counting techniques revolve around two distinct features of data&mdash;sparse data, which favors detection networks, or dense data where density map networks are used. Both techniques fail to address a third scenario, where dense objects are sparsely located. Raw aerial images represent sparse distributions of data in most situations. To address this issue, we propose a novel and exceedingly portable end-to-end model, DisCountNet, and an example dataset to test it on. DisCountNet is a two-stage network that uses theories from both detection and heat-map networks to provide a simple yet powerful design. The first stage, DiscNet, operates on the theory of coarse detection, but does so by converting a rich and high-resolution image into a sparse representation where only important information is encoded. Following this, CountNet operates on the dense regions of the sparse matrix to generate a density map, which provides fine locations and count predictions on densities of objects. Comparing the proposed network to current state-of-the-art networks, we find that we can maintain competitive performance while using a fraction of the computational complexity, resulting in a real-time solution.
KW  - deep learning
KW  - automatic counting
KW  - UAV
KW  - real-time
DO  - 10.3390/rs11091128
TY  - EJOU
AU  - Petrellis, Nikos
TI  - Plant Disease Diagnosis for Smart Phone Applications with Extensible Set of Diseases
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 9
SN  - 2076-3417

AB  - A plant disease diagnosis method that can be implemented with the resources of a mobile phone application, that does not have to be connected to a remote server, is presented and evaluated on citrus diseases. It can be used both by amateur gardeners and by professional agriculturists for early detection of diseases. The features used are extracted from photographs of plant parts like leaves or fruits and include the color, the relative area and the number of the lesion spots. These classification features, along with additional information like weather metadata, form disease signatures that can be easily defined by the end user (e.g., an agronomist). These signatures are based on the statistical processing of a small number of representative training photographs. The extracted features of a test photograph are compared against the disease signatures in order to select the most likely disease. An important advantage of the proposed approach is that the diagnosis does not depend on the orientation, the scale or the resolution of the photograph. The experiments have been conducted under several light exposure conditions. The accuracy was experimentally measured between 70% and 99%. An acceptable accuracy higher than 90% can be achieved in most of the cases since the lesion spots can recognized interactively with high precision.
KW  - plant disease
KW  - smart phone application
KW  - image processing
KW  - classification
KW  - segmentation
KW  - citrus diseases
DO  - 10.3390/app9091952
TY  - EJOU
AU  - Bejiga, Mesay B.
AU  - Melgani, Farid
AU  - Beraldini, Pietro
TI  - Domain Adversarial Neural Networks for Large-Scale Land Cover Classification
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - Learning classification models require sufficiently labeled training samples, however, collecting labeled samples for every new problem is time-consuming and costly. An alternative approach is to transfer knowledge from one problem to another, which is called transfer learning. Domain adaptation (DA) is a type of transfer learning that aims to find a new latent space where the domain discrepancy between the source and the target domain is negligible. In this work, we propose an unsupervised DA technique called domain adversarial neural networks (DANNs), composed of a feature extractor, a class predictor, and domain classifier blocks, for large-scale land cover classification. Contrary to the traditional methods that perform representation and classifier learning in separate stages, DANNs combine them into a single stage, thereby learning a new representation of the input data that is both domain-invariant and discriminative. Once trained, the classifier of a DANN can be used to predict both source and target domain labels. Additionally, we also modify the domain classifier of a DANN to evaluate its suitability for multi-target domain adaptation problems. Experimental results obtained for both single and multiple target DA problems show that the proposed method provides a performance gain of up to 40%.
KW  - domain adaptation
KW  - domain adversarial neural networks
KW  - large-scale land cover classification
KW  - representation learning
DO  - 10.3390/rs11101153
TY  - EJOU
AU  - Fuentes-Pacheco, Jorge
AU  - Torres-Olivares, Juan
AU  - Roman-Rangel, Edgar
AU  - Cervantes, Salvador
AU  - Juarez-Lopez, Porfirio
AU  - Hermosillo-Valadez, Jorge
AU  - Rendón-Mancha, Juan Manuel
TI  - Fig Plant Segmentation from Aerial Images Using a Deep Convolutional Encoder-Decoder Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - Crop segmentation is an important task in Precision Agriculture, where the use of aerial robots with an on-board camera has contributed to the development of new solution alternatives. We address the problem of fig plant segmentation in top-view RGB (Red-Green-Blue) images of a crop grown under open-field difficult circumstances of complex lighting conditions and non-ideal crop maintenance practices defined by local farmers. We present a Convolutional Neural Network (CNN) with an encoder-decoder architecture that classifies each pixel as crop or non-crop using only raw colour images as input. Our approach achieves a mean accuracy of 93.85% despite the complexity of the background and a highly variable visual appearance of the leaves. We make available our CNN code to the research community, as well as the aerial image data set and a hand-made ground truth segmentation with pixel precision to facilitate the comparison among different algorithms.
KW  - convolutional neural network
KW  - crop segmentation
KW  - Ficus carica
KW  - unmanned aerial vehicles
DO  - 10.3390/rs11101157
TY  - EJOU
AU  - Shan, Zeyong
AU  - Li, Ruijian
AU  - Schwertfeger, Sören
TI  - RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 10
SN  - 1424-8220

AB  - Using camera sensors for ground robot Simultaneous Localization and Mapping (SLAM) has many benefits over laser-based approaches, such as the low cost and higher robustness. RGBD sensors promise the best of both worlds: dense data from cameras with depth information. This paper proposes to fuse RGBD and IMU data for a visual SLAM system, called VINS-RGBD, that is built upon the open source VINS-Mono software. The paper analyses the VINS approach and highlights the observability problems. Then, we extend the VINS-Mono system to make use of the depth data during the initialization process as well as during the VIO (Visual Inertial Odometry) phase. Furthermore, we integrate a mapping system based on subsampled depth data and octree filtering to achieve real-time mapping, including loop closing. We provide the software as well as datasets for evaluation. Our extensive experiments are performed with hand-held, wheeled and tracked robots in different environments. We show that ORB-SLAM2 fails for our application and see that our VINS-RGBD approach is superior to VINS-Mono.
KW  - visual-inertial systems
KW  - SLAM
KW  - inertial motion tracking
KW  - ground robots
KW  - rescue robots
KW  - sensor fusion
KW  - state estimation
KW  - RGBD sensor
DO  - 10.3390/s19102251
TY  - EJOU
AU  - Han, Jiaming
AU  - Yang, Zhong
AU  - Zhang, Qiuyan
AU  - Chen, Cong
AU  - Li, Hongchen
AU  - Lai, Shangxiang
AU  - Hu, Guoxiong
AU  - Xu, Changliang
AU  - Xu, Hao
AU  - Wang, Di
AU  - Chen, Rui
TI  - A Method of Insulator Faults Detection in Aerial Images for High-Voltage Transmission Lines Inspection
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 10
SN  - 2076-3417

AB  - Insulator faults detection is an important task for high-voltage transmission line inspection. However, current methods often suffer from the lack of accuracy and robustness. Moreover, these methods can only detect one fault in the insulator string, but cannot detect a multi-fault. In this paper, a novel method is proposed for insulator one fault and multi-fault detection in UAV-based aerial images, the backgrounds of which usually contain much complex interference. The shapes of the insulators also vary obviously due to the changes in filming angle and distance. To reduce the impact of complex interference on insulator faults detection, we make full use of the deep neural network to distinguish between insulators and background interference. First of all, plenty of insulator aerial images with manually labelled ground-truth are collected to construct a standard insulator detection dataset &lsquo;InST_detection&rsquo;. Secondly, a new convolutional network is proposed to obtain accurate insulator string positions in the aerial image. Finally, a novel fault detection method is proposed that can detect both insulator one fault and multi-fault in aerial images. Experimental results on a large number of aerial images show that our proposed method is more effective and efficient than the state-of-the-art insulator fault detection methods.
KW  - unmanned aerial vehicle
KW  - high-voltage transmission line inspection
KW  - aerial image
KW  - insulator fault detection
DO  - 10.3390/app9102009
TY  - EJOU
AU  - Buters, Todd M.
AU  - Bateman, Philip W.
AU  - Robinson, Todd
AU  - Belton, David
AU  - Dixon, Kingsley W.
AU  - Cross, Adam T.
TI  - Methodological Ambiguity and Inconsistency Constrain Unmanned Aerial Vehicles as A Silver Bullet for Monitoring Ecological Restoration
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - The last decade has seen an exponential increase in the application of unmanned aerial vehicles (UAVs) to ecological monitoring research, though with little standardisation or comparability in methodological approaches and research aims. We reviewed the international peer-reviewed literature in order to explore the potential limitations on the feasibility of UAV-use in the monitoring of ecological restoration, and examined how they might be mitigated to maximise the quality, reliability and comparability of UAV-generated data. We found little evidence of translational research applying UAV-based approaches to ecological restoration, with less than 7% of 2133 published UAV monitoring studies centred around ecological restoration. Of the 48 studies, &gt; 65% had been published in the three years preceding this study. Where studies utilised UAVs for rehabilitation or restoration applications, there was a strong propensity for single-sensor monitoring using commercially available RPAs fitted with the modest-resolution RGB sensors available. There was a strong positive correlation between the use of complex and expensive sensors (e.g., LiDAR, thermal cameras, hyperspectral sensors) and the complexity of chosen image classification techniques (e.g., machine learning), suggesting that cost remains a primary constraint to the wide application of multiple or complex sensors in UAV-based research. We propose that if UAV-acquired data are to represent the future of ecological monitoring, research requires a) consistency in the proven application of different platforms and sensors to the monitoring of target landforms, organisms and ecosystems, underpinned by clearly articulated monitoring goals and outcomes; b) optimization of data analysis techniques and the manner in which data are reported, undertaken in cross-disciplinary partnership with fields such as bioinformatics and machine learning; and c) the development of sound, reasonable and multi-laterally homogenous regulatory and policy framework supporting the application of UAVs to the large-scale and potentially trans-disciplinary ecological applications of the future.
KW  - ecological restoration
KW  - drone
KW  - UAS
KW  - rehabilitation
KW  - revegetation
DO  - 10.3390/rs11101180
TY  - EJOU
AU  - Chawade, Aakash
AU  - van Ham, Joost
AU  - Blomquist, Hanna
AU  - Bagge, Oscar
AU  - Alexandersson, Erik
AU  - Ortiz, Rodomiro
TI  - High-Throughput Field-Phenotyping Tools for Plant Breeding and Precision Agriculture
T2  - Agronomy

PY  - 2019
VL  - 9
IS  - 5
SN  - 2073-4395

AB  - High-throughput field phenotyping has garnered major attention in recent years leading to the development of several new protocols for recording various plant traits of interest. Phenotyping of plants for breeding and for precision agriculture have different requirements due to different sizes of the plots and fields, differing purposes and the urgency of the action required after phenotyping. While in plant breeding phenotyping is done on several thousand small plots mainly to evaluate them for various traits, in plant cultivation, phenotyping is done in large fields to detect the occurrence of plant stresses and weeds at an early stage. The aim of this review is to highlight how various high-throughput phenotyping methods are used for plant breeding and farming and the key differences in the applications of such methods. Thus, various techniques for plant phenotyping are presented together with applications of these techniques for breeding and cultivation. Several examples from the literature using these techniques are summarized and the key technical aspects are highlighted.
KW  - field phenotyping
KW  - precision breeding
KW  - precision agriculture
KW  - decision support systems
DO  - 10.3390/agronomy9050258
TY  - EJOU
AU  - Li, Jing
AU  - Chen, Shuo
AU  - Zhang, Fangbing
AU  - Li, Erkang
AU  - Yang, Tao
AU  - Lu, Zhaoyang
TI  - An Adaptive Framework for Multi-Vehicle Ground Speed Estimation in Airborne Videos
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - With the rapid development of unmanned aerial vehicles (UAVs), UAV-based intelligent airborne surveillance systems represented by real-time ground vehicle speed estimation have attracted wide attention from researchers. However, there are still many challenges in extracting speed information from UAV videos, including the dynamic moving background, small target size, complicated environment, and diverse scenes. In this paper, we propose a novel adaptive framework for multi-vehicle ground speed estimation in airborne videos. Firstly, we build a traffic dataset based on UAV. Then, we use the deep learning detection algorithm to detect the vehicle in the UAV field of view and obtain the trajectory in the image through the tracking-by-detection algorithm. Thereafter, we present a motion compensation method based on homography. This method obtains matching feature points by an optical flow method and eliminates the influence of the detected target to accurately calculate the homography matrix to determine the real motion trajectory in the current frame. Finally, vehicle speed is estimated based on the mapping relationship between the pixel distance and the actual distance. The method regards the actual size of the car as prior information and adaptively recovers the pixel scale by estimating the vehicle size in the image; it then calculates the vehicle speed. In order to evaluate the performance of the proposed system, we carry out a large number of experiments on the AirSim Simulation platform as well as real UAV aerial surveillance experiments. Through quantitative and qualitative analysis of the simulation results and real experiments, we verify that the proposed system has a unique ability to detect, track, and estimate the speed of ground vehicles simultaneously even with a single downward-looking camera. Additionally, the system can obtain effective and accurate speed estimation results, even in various complex scenes.
KW  - ground vehicle speed estimation
KW  - intelligent airborne video surveillance
KW  - unmanned aerial vehicle
KW  - object detection and tracking
KW  - motion compensation
DO  - 10.3390/rs11101241
TY  - EJOU
AU  - Tan, Jin Y.
AU  - Ker, Pin J.
AU  - Lau, K. Y.
AU  - Hannan, M. A.
AU  - Tang, Shirley G.
TI  - Applications of Photonics in Agriculture Sector: A Review
T2  - Molecules

PY  - 2019
VL  - 24
IS  - 10
SN  - 1420-3049

AB  - The agricultural industry has made a tremendous contribution to the foundations of civilization. Basic essentials such as food, beverages, clothes and domestic materials are enriched by the agricultural industry. However, the traditional method in agriculture cultivation is labor-intensive and inadequate to meet the accelerating nature of human demands. This scenario raises the need to explore state-of-the-art crop cultivation and harvesting technologies. In this regard, optics and photonics technologies have proven to be effective solutions. This paper aims to present a comprehensive review of three photonic techniques, namely imaging, spectroscopy and spectral imaging, in a comparative manner for agriculture applications. Essentially, the spectral imaging technique is a robust solution which combines the benefits of both imaging and spectroscopy but faces the risk of underutilization. This review also comprehends the practicality of all three techniques by presenting existing examples in agricultural applications. Furthermore, the potential of these techniques is reviewed and critiqued by looking into agricultural activities involving palm oil, rubber, and agro-food crops. All the possible issues and challenges in implementing the photonic techniques in agriculture are given prominence with a few selective recommendations. The highlighted insights in this review will hopefully lead to an increased effort in the development of photonics applications for the future agricultural industry.
KW  - agriculture
KW  - photonics
KW  - imaging
KW  - spectral imaging
KW  - spectroscopy
DO  - 10.3390/molecules24102025
TY  - EJOU
AU  - Zhou, Xisheng
AU  - Li, Long
AU  - Chen, Longqian
AU  - Liu, Yunqiang
AU  - Cui, Yifan
AU  - Zhang, Yu
AU  - Zhang, Ting
TI  - Discriminating Urban Forest Types from Sentinel-2A Image Data through Linear Spectral Mixture Analysis: A Case Study of Xuzhou, East China
T2  - Forests

PY  - 2019
VL  - 10
IS  - 6
SN  - 1999-4907

AB  - Urban forests are an important component of the urban ecosystem. Urban forest types are a key piece of information required for monitoring the condition of an urban ecosystem. In this study, we propose an urban forest type discrimination method based on linear spectral mixture analysis (LSMA) and a support vector machine (SVM) in the case study of Xuzhou, east China. From 10-m Sentinel-2A imagery data, three different vegetation endmembers, namely broadleaved forest, coniferous forest, and low vegetation, and their abundances were extracted through LSMA. Using a combination of image spectra, topography, texture, and vegetation abundances, four SVM classification models were performed and compared to investigate the impact of these features on classification accuracy. With a particular interest in the role that vegetation abundances play in classification, we also compared SVM and other classifiers, i.e., random forest (RF), artificial neural network (ANN), and quick unbiased efficient statistical tree (QUEST). Results indicate that (1) the LSMA method can derive accurate vegetation abundances from Sentinel-2A image data, and the root-mean-square error (RMSE) was 0.019; (2) the classification accuracies of the four SVM models were improved after adding topographic features, textural features, and vegetation abundances one after the other; (3) the SVM produced higher classification accuracies than the other three classifiers when identical classification features were used; and (4) vegetation endmember abundances improved classification accuracy regardless of which classifier was used. It is concluded that Sentinel-2A image data has a strong capability to discriminate urban forest types in spectrally heterogeneous urban areas, and that vegetation abundances derived from LSMA can enhance such discrimination.
KW  - urban forest
KW  - Sentinel-2A
KW  - LSMA
KW  - SVM
DO  - 10.3390/f10060478
TY  - EJOU
AU  - Wang, Dongliang
AU  - Shao, Quanqin
AU  - Yue, Huanyin
TI  - Surveying Wild Animals from Satellites, Manned Aircraft and Unmanned Aerial Systems (UASs): A Review
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 11
SN  - 2072-4292

AB  - This article reviews studies regarding wild animal surveys based on multiple platforms, including satellites, manned aircraft, and unmanned aircraft systems (UASs), and focuses on the data used, animal detection methods, and their accuracies. We also discuss the advantages and limitations of each type of remote sensing data and highlight some new research opportunities and challenges. Submeter very-high-resolution (VHR) spaceborne imagery has potential in modeling the population dynamics of large (&gt;0.6 m) wild animals at large spatial and temporal scales, but has difficulty discerning small (&lt;0.6 m) animals at the species level, although high-resolution commercial satellites, such as WorldView-3 and -4, have been able to collect images with a ground resolution of up to 0.31 m in panchromatic mode. This situation will not change unless the satellite image resolution is greatly improved in the future. Manned aerial surveys have long been employed to capture the centimeter-scale images required for animal censuses over large areas. However, such aerial surveys are costly to implement in small areas and can cause significant disturbances to wild animals because of their noise. In contrast, UAS surveys are seen as a safe, convenient and less expensive alternative to ground-based and conventional manned aerial surveys, but most UASs can cover only small areas. The proposed use of UAS imagery in combination with VHR satellite imagery would produce critical population data for large wild animal species and colonies over large areas. The development of software systems for automatically producing image mosaics and recognizing wild animals will further improve survey efficiency.
KW  - very-high-resolution satellites
KW  - unmanned aircraft systems
KW  - wild animal surveys
KW  - remote sensing
DO  - 10.3390/rs11111308
TY  - EJOU
AU  - Zhao, Rui
AU  - Shi, Zhenwei
AU  - Zou, Zhengxia
AU  - Zhang, Zhou
TI  - Ensemble-Based Cascaded Constrained Energy Minimization for Hyperspectral Target Detection
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 11
SN  - 2072-4292

AB  - Ensemble learning is an important group of machine learning techniques that aim to enhance the nonlinearity and generalization ability of a learning system by aggregating multiple learners. We found that ensemble techniques show great potential for improving the performance of traditional hyperspectral target detection algorithms, while at present, there are few previous works have been done on this topic. To this end, we propose an Ensemble based Constrained Energy Minimization (E-CEM) detector for hyperspectral image target detection. Classical hyperspectral image target detection algorithms like Constrained Energy Minimization (CEM), matched filter (MF) and adaptive coherence/cosine estimator (ACE) are usually designed based on constrained least square regression methods or hypothesis testing methods with Gaussian distribution assumption. However, remote sensing hyperspectral data captured in a real-world environment usually shows strong nonlinearity and non-Gaussianity, which will lead to performance degradation of these classical detection algorithms. Although some hierarchical detection models are able to learn strong nonlinear discrimination of spectral data, due to the spectrum changes, these models usually suffer from the instability in detection tasks. The proposed E-CEM is designed based on the classical CEM detection algorithm. To improve both of the detection nonlinearity and generalization ability, the strategies of &ldquo;cascaded detection&rdquo;, &ldquo;random averaging&rdquo; and &ldquo;multi-scale scanning&rdquo; are specifically designed. Experiments on one synthetic hyperspectral image and two real hyperspectral images demonstrate the effectiveness of our method. E-CEM outperforms the traditional CEM detector and other state-of-the-art detection algorithms. Our code will be made publicly available.
KW  - hyperspectral image
KW  - target detection
KW  - constrained energy minimization
KW  - cascaded detection
KW  - ensemble
KW  - multi-scale scanning
DO  - 10.3390/rs11111310
TY  - EJOU
AU  - Bote-Curiel, Luis
AU  - Muñoz-Romero, Sergio
AU  - Gerrero-Curieses, Alicia
AU  - Rojo-Álvarez, José L.
TI  - Deep Learning and Big Data in Healthcare: A Double Review for Critical Beginners
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 11
SN  - 2076-3417

AB  - In the last few years, there has been a growing expectation created about the analysis of large amounts of data often available in organizations, which has been both scrutinized by the academic world and successfully exploited by industry. Nowadays, two of the most common terms heard in scientific circles are Big Data and Deep Learning. In this double review, we aim to shed some light on the current state of these different, yet somehow related branches of Data Science, in order to understand the current state and future evolution within the healthcare area. We start by giving a simple description of the technical elements of Big Data technologies, as well as an overview of the elements of Deep Learning techniques, according to their usual description in scientific literature. Then, we pay attention to the application fields that can be said to have delivered relevant real-world success stories, with emphasis on examples from large technology companies and financial institutions, among others. The academic effort that has been put into bringing these technologies to the healthcare sector are then summarized and analyzed from a twofold view as follows: first, the landscape of application examples is globally scrutinized according to the varying nature of medical data, including the data forms in electronic health recordings, medical time signals, and medical images; second, a specific application field is given special attention, in particular the electrocardiographic signal analysis, where a number of works have been published in the last two years. A set of toy application examples are provided with the publicly-available MIMIC dataset, aiming to help the beginners start with some principled, basic, and structured material and available code. Critical discussion is provided for current and forthcoming challenges on the use of both sets of techniques in our future healthcare.
KW  - deep learning
KW  - big data
KW  - statistical learning
KW  - healthcare
KW  - electrocardiogram
KW  - databases
KW  - MIMIC
KW  - review
KW  - machine learning
DO  - 10.3390/app9112331
TY  - EJOU
AU  - Ahmed, Sarfraz
AU  - Huda, M. N.
AU  - Rajbhandari, Sujan
AU  - Saha, Chitta
AU  - Elshaw, Mark
AU  - Kanarachos, Stratis
TI  - Pedestrian and Cyclist Detection and Intent Estimation for Autonomous Vehicles: A Survey
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 11
SN  - 2076-3417

AB  - As autonomous vehicles become more common on the roads, their advancement draws on safety concerns for vulnerable road users, such as pedestrians and cyclists. This paper presents a review of recent developments in pedestrian and cyclist detection and intent estimation to increase the safety of autonomous vehicles, for both the driver and other road users. Understanding the intentions of the pedestrian/cyclist enables the self-driving vehicle to take actions to avoid incidents. To make this possible, development of methods/techniques, such as deep learning (DL), for the autonomous vehicle will be explored. For example, the development of pedestrian detection has been significantly advanced using DL approaches, such as; Fast Region-Convolutional Neural Network (R-CNN) , Faster R-CNN and Single Shot Detector (SSD). Although DL has been around for several decades, the hardware to realise the techniques have only recently become viable. Using these DL methods for pedestrian and cyclist detection and applying it for the tracking, motion modelling and pose estimation can allow for a successful and accurate method of intent estimation for the vulnerable road users. Although there has been a growth in research surrounding the study of pedestrian detection using vision-based approaches, further attention should include focus on cyclist detection. To further improve safety for these vulnerable road users (VRUs), approaches such as sensor fusion and intent estimation should be investigated.
KW  - pedestrian detection
KW  - cyclist detection
KW  - deep learning
KW  - CNN
KW  - Fast R-CNN
KW  - Faster R-CNN
KW  - pose estimation
KW  - motion modelling
KW  - tracking
KW  - intent estimation
DO  - 10.3390/app9112335
TY  - EJOU
AU  - Rostami, Mohammad
AU  - Kolouri, Soheil
AU  - Eaton, Eric
AU  - Kim, Kyungnam
TI  - Deep Transfer Learning for Few-Shot SAR Image Classification
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 11
SN  - 2072-4292

AB  - The reemergence of Deep Neural Networks (DNNs) has lead to high-performance supervised learning algorithms for the Electro-Optical (EO) domain classification and detection problems. This success is because generating huge labeled datasets has become possible using modern crowdsourcing labeling platforms such as Amazon&rsquo;s Mechanical Turk that recruit ordinary people to label data. Unlike the EO domain, labeling the Synthetic Aperture Radar (SAR) domain data can be much more challenging, and for various reasons, using crowdsourcing platforms is not feasible for labeling the SAR domain data. As a result, training deep networks using supervised learning is more challenging in the SAR domain. In the paper, we present a new framework to train a deep neural network for classifying Synthetic Aperture Radar (SAR) images by eliminating the need for a huge labeled dataset. Our idea is based on transferring knowledge from a related EO domain problem, where labeled data are easy to obtain. We transfer knowledge from the EO domain through learning a shared invariant cross-domain embedding space that is also discriminative for classification. To this end, we train two deep encoders that are coupled through their last year to map data points from the EO and the SAR domains to the shared embedding space such that the distance between the distributions of the two domains is minimized in the latent embedding space. We use the Sliced Wasserstein Distance (SWD) to measure and minimize the distance between these two distributions and use a limited number of SAR label data points to match the distributions class-conditionally. As a result of this training procedure, a classifier trained from the embedding space to the label space using mostly the EO data would generalize well on the SAR domain. We provide a theoretical analysis to demonstrate why our approach is effective and validate our algorithm on the problem of ship classification in the SAR domain by comparing against several other competing learning approaches.
KW  - transfer learning
KW  - convolutional neural network
KW  - electro-optical imaging
KW  - Synthetic Aperture Radar (SAR) imaging
KW  - optimal transport metric
DO  - 10.3390/rs11111374
TY  - EJOU
AU  - Zhou, Chengquan
AU  - Ye, Hongbao
AU  - Xu, Zhifu
AU  - Hu, Jun
AU  - Shi, Xiaoyan
AU  - Hua, Shan
AU  - Yue, Jibo
AU  - Yang, Guijun
TI  - Estimating Maize-Leaf Coverage in Field Conditions by Applying a Machine Learning Algorithm to UAV Remote Sensing Images
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 11
SN  - 2076-3417

AB  - Leaf coverage is an indicator of plant growth rate and predicted yield, and thus it is crucial to plant-breeding research. Robust image segmentation of leaf coverage from remote-sensing images acquired by unmanned aerial vehicles (UAVs) in varying environments can be directly used for large-scale coverage estimation, and is a key component of high-throughput field phenotyping. We thus propose an image-segmentation method based on machine learning to extract relatively accurate coverage information from the orthophoto generated after preprocessing. The image analysis pipeline, including dataset augmenting, removing background, classifier training and noise reduction, generates a set of binary masks to obtain leaf coverage from the image. We compare the proposed method with three conventional methods (Hue-Saturation-Value, edge-detection-based algorithm, random forest) and a frontier deep-learning method called DeepLabv3+. The proposed method improves indicators such as Qseg, Sr, Es and mIOU by 15% to 30%. The experimental results show that this approach is less limited by radiation conditions, and that the protocol can easily be implemented for extensive sampling at low cost. As a result, with the proposed method, we recommend using red-green-blue (RGB)-based technology in addition to conventional equipment for acquiring the leaf coverage of agricultural crops.
KW  - machine learning
KW  - maize-leaf coverage
KW  - image segmentation
KW  - UAV remoting images
DO  - 10.3390/app9112389
TY  - EJOU
AU  - Li, Wei
AU  - Jiang, Jiale
AU  - Guo, Tai
AU  - Zhou, Meng
AU  - Tang, Yining
AU  - Wang, Ying
AU  - Zhang, Yu
AU  - Cheng, Tao
AU  - Zhu, Yan
AU  - Cao, Weixing
AU  - Yao, Xia
TI  - Generating Red-Edge Images at 3 M Spatial Resolution by Fusing Sentinel-2 and Planet Satellite Products
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 12
SN  - 2072-4292

AB  - High-resolution satellite images can be used to some extent to mitigate the mixed-pixel problem caused by the lack of intensive production, farmland fragmentation, and the uneven growth of field crops in developing countries. Specifically, red-edge (RE) satellite images can be used in this context to reduce the influence of soil background at early stages as well as saturation due to crop leaf area index (LAI) at later stages. However, the availability of high-resolution RE satellite image products for research and application globally remains limited. This study uses the weight-and-unmixing algorithm as well as the SUPer-REsolution for multi-spectral Multi-resolution Estimation (Wu-SupReME) approach to combine the advantages of Sentinel-2 spectral and Planet spatial resolution and generate a high-resolution RE product. The resultant fused image is highly correlated (R2 &gt; 0.98) with Sentinel-2 image and clearly illustrates the persistent advantages of such products. This fused image was significantly more accurate than the originals when used to predict heterogeneous wheat LAI and therefore clearly illustrated the persistence of Sentinel-2 spectral and Planet spatial advantage, which indirectly proved that the fusion methodology of generating high-resolution red-edge products from Planet and Sentinel-2 images is possible. This study provided method reference for multi-source data fusion and image product for accurate parameter inversion in quantitative remote sensing of vegetation.
KW  - Sentinel-2
KW  - Planet
KW  - SupReME
KW  - weight-and-unmixing
KW  - fusion image
KW  - wheat LAI
DO  - 10.3390/rs11121422
TY  - EJOU
AU  - Agapiou, Athos
TI  - Enhancement of Archaeological Proxies at Non-Homogenous Environments in Remotely Sensed Imagery
T2  - Sustainability

PY  - 2019
VL  - 11
IS  - 12
SN  - 2071-1050

AB  - Optical remote sensing has been widely used for the identification of archaeological proxies. Such proxies, known as crop or soil marks, can be detected in multispectral images due to their spectral signatures and the distinct contrast that they provide in relation to the surrounding area. The current availability of high-resolution satellite datasets has enabled researchers to provide new methodologies and algorithms that can further enhance archaeological proxies supporting thus image-interpretation. However, a critical point that remains unsolved is the detection of crop and soil marks in non-homogenous environments. In these areas, interpretation is problematic even after the application of sophisticated image enhancement analysis techniques due to the mixed landscape and spectral confusion produced from the high-resolution datasets. To overcome this problem, we propose an image-based methodology in which the vegetation is suppressed following the &ldquo;forced invariance&rdquo; method and then we apply a linear orthogonal transformation to the suppressed spectral bands. The new Red&ndash;Green&ndash;Blue (RGB) image corresponds to a new three-band spectral space where the three axes are linked with the crop mark, vegetation, and soil components. The study evaluates the proposed approach in the archaeological site of &ldquo;Nea Paphos&rdquo; in Cyprus using a WorldView-2 multispectral image aiming to overcome the limitations of the mixed environments.
KW  - remote sensing archaeology
KW  - vegetation suppression
KW  - orthogonal equations
KW  - archaeological proxies
KW  - buried archaeological remains
KW  - soil marks
KW  - crop marks
KW  - Cyprus
DO  - 10.3390/su11123339
TY  - EJOU
AU  - Yao, Huang
AU  - Qin, Rongjun
AU  - Chen, Xiaoyu
TI  - Unmanned Aerial Vehicle for Remote Sensing Applications—A Review
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 12
SN  - 2072-4292

AB  - The unmanned aerial vehicle (UAV) sensors and platforms nowadays are being used in almost every application (e.g., agriculture, forestry, and mining) that needs observed information from the top or oblique views. While they intend to be a general remote sensing (RS) tool, the relevant RS data processing and analysis methods are still largely ad-hoc to applications. Although the obvious advantages of UAV data are their high spatial resolution and flexibility in acquisition and sensor integration, there is in general a lack of systematic analysis on how these characteristics alter solutions for typical RS tasks such as land-cover classification, change detection, and thematic mapping. For instance, the ultra-high-resolution data (less than 10 cm of Ground Sampling Distance (GSD)) bring more unwanted classes of objects (e.g., pedestrian and cars) in land-cover classification; the often available 3D data generated from photogrammetric images call for more advanced techniques for geometric and spectral analysis. In this paper, we perform a critical review on RS tasks that involve UAV data and their derived products as their main sources including raw perspective images, digital surface models, and orthophotos. In particular, we focus on solutions that address the &ldquo;new&rdquo; aspects of the UAV data including (1) ultra-high resolution; (2) availability of coherent geometric and spectral data; and (3) capability of simultaneously using multi-sensor data for fusion. Based on these solutions, we provide a brief summary of existing examples of UAV-based RS in agricultural, environmental, urban, and hazards assessment applications, etc., and by discussing their practical potentials, we share our views in their future research directions and draw conclusive remarks.
KW  - UAVs
KW  - remote sensing applications
KW  - data analysis
DO  - 10.3390/rs11121443
TY  - EJOU
AU  - Munaye, Yirga Y.
AU  - Lin, Hsin-Piao
AU  - Adege, Abebe B.
AU  - Tarekegn, Getaneh B.
TI  - UAV Positioning for Throughput Maximization Using Deep Learning Approaches
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 12
SN  - 1424-8220

AB  - The use of unmanned aerial vehicles (UAVs) as a communication platform has great practical importance for future wireless networks, especially for on-demand deployment for temporary and emergency conditions. The user throughput estimation in a wireless system depends on the data traffic load and the available capacity to support that load. In UAV-assisted communication, the position of the UAV is one major factor that affects the capacity available to the data flows being served. This study applies multi-layer perceptron (MLP) and long short term memory (LSTM) approaches to determine the position of a UAV that maximizes the overall system performance and user throughput. To analyze and evaluate the system performance, we apply the hybrid of MLP-LSTM for classification regression tasks and K-means algorithms for automatic clustering of classes. The implementation of our work is done through TensorFlow packages. The performance of our proposed system is compared with other approaches to give accurate and novel results for both classification and regression tasks of the user throughput maximization and UAV positioning. According to the results, 98% of the user throughput maximization accuracy is correctly classified. Moreover, the UAV positioning provides accuracy levels of 94.73%, 98.33%, and 99.53% for original datasets (scenario 1), reduced features on the estimated values of user throughput at each grid point (scenario 2), and reduced feature datasets collected on different days and grid points achieved maximum throughput (scenario 3), respectively.
KW  - user throughput
KW  - maximization
KW  - UAV
KW  - positioning
KW  - deep learning (DL)
DO  - 10.3390/s19122775
TY  - EJOU
AU  - Yang, Zhen
AU  - Yuan, Yongbo
AU  - Zhang, Mingyuan
AU  - Zhao, Xuefeng
AU  - Zhang, Yang
AU  - Tian, Boquan
TI  - Safety Distance Identification for Crane Drivers Based on Mask R-CNN
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 12
SN  - 1424-8220

AB  - Tower cranes are the most commonly used large-scale equipment on construction site. Because workers can&rsquo;t always pay attention to the environment at the top of the head, it is often difficult to avoid accidents when heavy objects fall. Therefore, safety construction accidents such as struck-by often occurs. In order to address crane issue, this research recorded video data by a tower crane camera, labeled the pictures, and operated image recognition with the MASK R-CNN method. Furthermore, The RGB color extraction was performed on the identified mask layer to obtain the pixel coordinates of workers and dangerous zone. At last, we used the pixel and actual distance conversion method to measure the safety distance. The contribution of this research to safety problem area is twofold: On one hand, without affecting the normal behavior of workers, an automatic collection, analysis, and early-warning system was established. On the other hand, the proposed automatic inspection system can help improve the safety operation of tower crane drivers.
KW  - construction management
KW  - construction safety
KW  - cranes
KW  - imaging techniques
KW  - safety distance
DO  - 10.3390/s19122789
TY  - EJOU
AU  - Parrott, Elizabeth
AU  - Panter, Heather
AU  - Morrissey, Joanne
AU  - Bezombes, Frederic
TI  - A Low Cost Approach to Disturbed Soil Detection Using Low Altitude Digital Imagery from an Unmanned Aerial Vehicle
T2  - Drones

PY  - 2019
VL  - 3
IS  - 2
SN  - 2504-446X

AB  - Until recently, clandestine burial investigations relied upon witness statements to determine target search areas of soil and vegetation disturbance. Due to this, remote sensing technologies are increasingly used to detect fresh clandestine graves. However, despite the increased capabilities of remote sensing, clandestine burial searches remain resourcefully intensive as the police have little access to the technology when it is required. In contrast to this, Unmanned Aerial Vehicle (UAV) technology is increasingly popular amongst law enforcement worldwide. As such, this paper explores the use of digital imagery collected from a low cost UAV for the aided detection of disturbed soil sites indicative of fresh clandestine graves. This is done by assessing the unaltered UAV video output using image processing tools to detect sites of disturbance, therefore highlighting previously unrecognised capabilities of police UAVs. This preliminary investigation provides a low cost rapid approach to detecting fresh clandestine graves, further supporting the use of UAV technology by UK police.
KW  - forensic science
KW  - unmanned aerial vehicle
KW  - clandestine burial
KW  - grave detection
KW  - airborne
KW  - image processing
KW  - policing
DO  - 10.3390/drones3020050
TY  - EJOU
AU  - Zhang, Heng
AU  - Eziz, Anwar
AU  - Xiao, Jian
AU  - Tao, Shengli
AU  - Wang, Shaopeng
AU  - Tang, Zhiyao
AU  - Zhu, Jiangling
AU  - Fang, Jingyun
TI  - High-Resolution Vegetation Mapping Using eXtreme Gradient Boosting Based on Extensive Features
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 12
SN  - 2072-4292

AB  - Accurate mapping of vegetation is a premise for conserving, managing, and sustainably using vegetation resources, especially in conditions of intensive human activities and accelerating global changes. However, it is still challenging to produce high-resolution multiclass vegetation map in high accuracy, due to the incapacity of traditional mapping techniques in distinguishing mosaic vegetation classes with subtle differences and the paucity of fieldwork data. This study created a workflow by adopting a promising classifier, extreme gradient boosting (XGBoost), to produce accurate vegetation maps of two strikingly different cases (the Dzungarian Basin in China and New Zealand) based on extensive features and abundant vegetation data. For the Dzungarian Basin, a vegetation map with seven vegetation types, 17 subtypes, and 43 associations was produced with an overall accuracy of 0.907, 0.801, and 0.748, respectively. For New Zealand, a map of 10 habitats and a map of 41 vegetation classes were produced with 0.946, and 0.703 overall accuracy, respectively. The workflow incorporating simplified field survey procedures outperformed conventional field survey and remote sensing based methods in terms of accuracy and efficiency. In addition, it opens a possibility of building large-scale, high-resolution, and timely vegetation monitoring platforms for most terrestrial ecosystems worldwide with the aid of Google Earth Engine and citizen science programs.
KW  - vegetation mapping
KW  - XGBoost
KW  - simplified field survey
KW  - Dzungarian Basin
KW  - New Zealand
DO  - 10.3390/rs11121505
TY  - EJOU
AU  - Cardenal, Javier
AU  - Fernández, Tomás
AU  - Pérez-García, José L.
AU  - Gómez-López, José M.
TI  - Measurement of Road Surface Deformation Using Images Captured from UAVs
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 12
SN  - 2072-4292

AB  - This paper presents a methodology for measuring road surface deformation due to terrain instability processes. The methodology is based on ultra-high resolution images acquired from unmanned aerial vehicles (UAVs). Flights are georeferenced by means of Structure from Motion (SfM) techniques. Dense point clouds, obtained using the multiple-view stereo (MVS) approach, are used to generate digital surface models (DSM) and high resolution orthophotographs (0.02 m GSD). The methodology has been applied to an unstable area located in La Guardia (Jaen, Southern Spain), where an active landslide was identified. This landslide affected some roads and accesses to a highway at the landslide foot. The detailed road deformation was monitored between 2012 and 2015 by means of eleven UAV flights of ultrahigh resolution covering an area of about 260 m × 90 m. The accuracy of the analysis has been established in 0.02 ± 0.01 m in XY and 0.04 ± 0.02 m in Z. Large deformations in the order of two meters were registered in the total period analyzed that resulted in maximum average rates of 0.62 m/month in the unstable area. Some boundary conditions were considered because of the low required flying height (&lt;50 m above ground level) in order to achieve a suitable image GSD, the fast landslide dynamic, continuous maintenance works on the affected roads and dramatic seasonal vegetation changes throughout the monitoring period. Finally, we have analyzed the relation of displacements to rainfalls in the area, finding a significant correlation between the two variables, as well as two different reactivation episodes.
KW  - road surface deformation
KW  - UAV images
KW  - SfM-MVS
KW  - monitoring points
DO  - 10.3390/rs11121507
TY  - EJOU
AU  - Fetai, Bujar
AU  - Oštir, Krištof
AU  - Kosmatin Fras, Mojca
AU  - Lisec, Anka
TI  - Extraction of Visible Boundaries for Cadastral Mapping Based on UAV Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 13
SN  - 2072-4292

AB  - In order to transcend the challenge of accelerating the establishment of cadastres and to efficiently maintain them once established, innovative, and automated cadastral mapping techniques are needed. The focus of the research is on the use of high-resolution optical sensors on unmanned aerial vehicle (UAV) platforms. More specifically, this study investigates the potential of UAV-based cadastral mapping, where the ENVI feature extraction (FX) module has been used for data processing. The paper describes the workflow, which encompasses image pre-processing, automatic extraction of visible boundaries on the UAV imagery, and data post-processing. It shows that this approach should be applied when the UAV orthoimage is resampled to a larger ground sample distance (GSD). In addition, the findings show that it is important to filter the extracted boundary maps to improve the results. The results of the accuracy assessment showed that almost 80% of the extracted visible boundaries were correct. Based on the automatic extraction method, the proposed workflow has the potential to accelerate and facilitate the creation of cadastral maps, especially for developing countries. In developed countries, the extracted visible boundaries might be used for the revision of existing cadastral maps. However, in both cases, the extracted visible boundaries must be validated by landowners and other beneficiaries.
KW  - land plot
KW  - land cadastre
KW  - cadastral boundaries
KW  - cadastral maps
KW  - UAV
KW  - image processing
KW  - image segmentation
KW  - feature extraction
DO  - 10.3390/rs11131510
TY  - EJOU
AU  - Tian, Jiarong
AU  - Dai, Tingting
AU  - Li, Haidong
AU  - Liao, Chengrui
AU  - Teng, Wenxiu
AU  - Hu, Qingwu
AU  - Ma, Weibo
AU  - Xu, Yannan
TI  - A Novel Tree Height Extraction Approach for Individual Trees by Combining TLS and UAV Image-Based Point Cloud Integration
T2  - Forests

PY  - 2019
VL  - 10
IS  - 7
SN  - 1999-4907

AB  - Research Highlights: This study carried out a feasibility analysis on the tree height extraction of a planted coniferous forest with high canopy density by combining terrestrial laser scanner (TLS) and unmanned aerial vehicle (UAV) image&ndash;based point cloud data at small and midsize tree farms. Background and Objectives: Tree height is an important factor for forest resource surveys. This information plays an important role in forest structure evaluation and forest stock estimation. The objectives of this study were to solve the problem of underestimating tree height and to guarantee the precision of tree height extraction in medium and high-density planted coniferous forests. Materials and Methods: This study developed a novel individual tree localization (ITL)-based tree height extraction method to obtain preliminary results in a planted coniferous forest plots with 107 trees (Metasequoia). Then, the final accurate results were achieved based on the canopy height model (CHM) and CHM seed points (CSP). Results: The registration accuracy of the TLS and UAV image-based point cloud data reached 6 cm. The authors optimized the precision of tree height extraction using the ITL-based method by improving CHM resolution from 0.2 m to 0.1 m. Due to the overlapping of forest canopies, the CSP method failed to delineate all individual tree crowns in medium to high-density forest stands with the matching rates of about 75%. However, the accuracy of CSP-based tree height extraction showed obvious advantages compared with the ITL-based method. Conclusion: The proposed method provided a solid foundation for dynamically monitoring forest resources in a high-accuracy and low-cost way, especially in planted tree farms.
KW  - planted coniferous forest
KW  - tree height
KW  - TLS
KW  - UAV
KW  - image-based
KW  - point cloud
DO  - 10.3390/f10070537
TY  - EJOU
AU  - Koch, Tobias
AU  - Körner, Marco
AU  - Fraundorfer, Friedrich
TI  - Automatic and Semantically-Aware 3D UAV Flight Planning for Image-Based 3D Reconstruction
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 13
SN  - 2072-4292

AB  - Small-scaled unmanned aerial vehicles (UAVs) emerge as ideal image acquisition platforms due to their high maneuverability even in complex and tightly built environments. The acquired images can be utilized to generate high-quality 3D models using current multi-view stereo approaches. However, the quality of the resulting 3D model highly depends on the preceding flight plan which still requires human expert knowledge, especially in complex urban and hazardous environments. In terms of safe flight plans, practical considerations often define prohibited and restricted airspaces to be accessed with the vehicle. We propose a 3D UAV path planning framework designed for detailed and complete small-scaled 3D reconstructions considering the semantic properties of the environment allowing for user-specified restrictions on the airspace. The generated trajectories account for the desired model resolution and the demands on a successful photogrammetric reconstruction. We exploit semantics from an initial flight to extract the target object and to define restricted and prohibited airspaces which have to be avoided during the path planning process to ensure a safe and short UAV path, while still aiming to maximize the object reconstruction quality. The path planning problem is formulated as an orienteering problem and solved via discrete optimization exploiting submodularity and photogrammetrical relevant heuristics. An evaluation of our method on a customized synthetic scene and on outdoor experiments suggests the real-world capability of our methodology by providing feasible, short and safe flight plans for the generation of detailed 3D reconstruction models.
KW  - UAV
KW  - trajectory optimization
KW  - path planning
KW  - discrete optimization
KW  - 3D reconstruction
KW  - semantics
KW  - urban mapping
DO  - 10.3390/rs11131550
TY  - EJOU
AU  - Zhang, Xin
AU  - Han, Liangxiu
AU  - Dong, Yingying
AU  - Shi, Yue
AU  - Huang, Wenjiang
AU  - Han, Lianghao
AU  - González-Moreno, Pablo
AU  - Ma, Huiqin
AU  - Ye, Huichun
AU  - Sobeih, Tam
TI  - A Deep Learning-Based Approach for Automated Yellow Rust Disease Detection from High-Resolution Hyperspectral UAV Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 13
SN  - 2072-4292

AB  - Yellow rust in winter wheat is a widespread and serious fungal disease, resulting in significant yield losses globally. Effective monitoring and accurate detection of yellow rust are crucial to ensure stable and reliable wheat production and food security. The existing standard methods often rely on manual inspection of disease symptoms in a small crop area by agronomists or trained surveyors. This is costly, time consuming and prone to error due to the subjectivity of surveyors. Recent advances in unmanned aerial vehicles (UAVs) mounted with hyperspectral image sensors have the potential to address these issues with low cost and high efficiency. This work proposed a new deep convolutional neural network (DCNN) based approach for automated crop disease detection using very high spatial resolution hyperspectral images captured with UAVs. The proposed model introduced multiple Inception-Resnet layers for feature extraction and was optimized to establish the most suitable depth and width of the network. Benefiting from the ability of convolution layers to handle three-dimensional data, the model used both spatial and spectral information for yellow rust detection. The model was calibrated with hyperspectral imagery collected by UAVs in five different dates across a whole crop cycle over a well-controlled field experiment with healthy and rust infected wheat plots. Its performance was compared across sampling dates and with random forest, a representative of traditional classification methods in which only spectral information was used. It was found that the method has high performance across all the growing cycle, particularly at late stages of the disease spread. The overall accuracy of the proposed model (0.85) was higher than that of the random forest classifier (0.77). These results showed that combining both spectral and spatial information is a suitable approach to improving the accuracy of crop disease detection with high resolution UAV hyperspectral images.
KW  - winter wheat
KW  - yellow rust
KW  - crop disease
KW  - unmanned aerial vehicle
KW  - hyperspectral
KW  - deep learning
KW  - classification
DO  - 10.3390/rs11131554
TY  - EJOU
AU  - Ryu, June-Woo
AU  - Pham, Quoc-Viet
AU  - Luan, Huynh N. T.
AU  - Hwang, Won-Joo
AU  - Kim, Jong-Deok
AU  - Lee, Jung-Tae
TI  - Multi-Access Edge Computing Empowered Heterogeneous Networks: A Novel Architecture and Potential Works
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 7
SN  - 2073-8994

AB  - One of the most promising approaches to address the mismatch between computation- intensive applications and computation-limited end devices is multi-access edge computing (MEC). To overcome the rapid increase in traffic volume and offload the traffic from macrocells, a massive number of small cells have been deployed, so-called heterogeneous networks (HetNets). Strongly motivated by the close integration of MEC and HetNets, in this paper, we propose an envisioned architecture of MEC-empowered HetNets, where both wireless and wired backhaul solutions are supported, flying base stations (BSs) can be equipped with MEC servers, and mobile users (MUs) need both communication and computation resources for their computationally heavy tasks. Subsequently, we provide the research progress summary of task offloading and resource allocation in the proposed MEC-empowered unmanned aerial vehicle (UAV)-assisted heterogeneous networks. We complete this article by spotlighting key challenges and open future directives for researches.
KW  - computation offloading
KW  - Internet of Things (IoT)
KW  - heterogeneous networks (HetNets)
KW  - multi-access edge computing (MEC)
KW  - non-orthogonal multiple access (NOMA)
KW  - resource allocation
KW  - unmanned aerial vehicles (UAV)
DO  - 10.3390/sym11070842
TY  - EJOU
AU  - Zhang, Jianming
AU  - Lu, Chaoquan
AU  - Wang, Jin
AU  - Wang, Lei
AU  - Yue, Xiao-Guang
TI  - Concrete Cracks Detection Based on FCN with Dilated Convolution
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 13
SN  - 2076-3417

AB  - In civil engineering, the stability of concrete is of great significance to safety of people&rsquo;s life and property, so it is necessary to detect concrete damage effectively. In this paper, we treat crack detection on concrete surface as a semantic segmentation task that distinguishes background from crack at the pixel level. Inspired by Fully Convolutional Networks (FCN), we propose a full convolution network based on dilated convolution for concrete crack detection, which consists of an encoder and a decoder. Specifically, we first used the residual network to extract the feature maps of the input image, designed the dilated convolutions with different dilation rates to extract the feature maps of different receptive fields, and fused the extracted features from multiple branches. Then, we exploited the stacked deconvolution to do up-sampling operator in the fused feature maps. Finally, we used the SoftMax function to classify the feature maps at the pixel level. In order to verify the validity of the model, we introduced the commonly used evaluation indicators of semantic segmentation: Pixel Accuracy (PA), Mean Pixel Accuracy (MPA), Mean Intersection over Union (MIoU), and Frequency Weighted Intersection over Union (FWIoU). The experimental results show that the proposed model converges faster and has better generalization performance on the test set by introducing dilated convolutions with different dilation rates and a multi-branch fusion strategy. Our model has a PA of 96.84%, MPA of 92.55%, MIoU of 86.05% and FWIoU of 94.22% on the test set, which is superior to other models.
KW  - FCN
KW  - crack detection
KW  - residual network
KW  - dilated convolution
KW  - semantic segmentation
DO  - 10.3390/app9132686
TY  - EJOU
AU  - Klouček, Tomáš
AU  - Komárek, Jan
AU  - Surový, Peter
AU  - Hrach, Karel
AU  - Janata, Přemysl
AU  - Vašíček, Bedřich
TI  - The Use of UAV Mounted Sensors for Precise Detection of Bark Beetle Infestation
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 13
SN  - 2072-4292

AB  - The bark beetle (Ips typographus) disturbance represents serious environmental and economic issue and presents a major challenge for forest management. A timely detection of bark beetle infestation is therefore necessary to reduce losses. Besides wood production, a bark beetle outbreak affects the forest ecosystem in many other ways including the water cycle, nutrient cycle, or carbon fixation. On that account, (not just) European temperate coniferous forests may become endangered ecosystems. Our study was performed in the unmanaged zone of the Krkono&scaron;e Mountains National Park in the northern part of the Czech Republic where the natural spreading of bark beetle is slow and, therefore, allow us to continuously monitor the infested trees that are, in contrast to managed forests, not being removed. The aim of this work is to evaluate possibilities of unmanned aerial vehicle (UAV)-mounted low-cost RGB and modified near-infrared sensors for detection of different stages of infested trees at the individual level, using a retrospective time series for recognition of still green but already infested trees (so-called green attack). A mosaic was created from the UAV imagery, radiometrically calibrated for surface reflectance, and five vegetation indices were calculated; the reference data about the stage of bark beetle infestation was obtained through a combination of field survey and visual interpretation of an orthomosaic. The differences of vegetation indices between infested and healthy trees over four time points were statistically evaluated and classified using the Maximum Likelihood classifier. Achieved results confirm our assumptions that it is possible to use a low-cost UAV-based sensor for detection of various stages of bark beetle infestation across seasons; with increasing time after infection, distinguishing infested trees from healthy ones grows easier. The best performance was achieved by the Greenness Index with overall accuracy of 78%&ndash;96% across the time periods. The performance of the indices based on near-infrared band was lower.
KW  - bark beetle detection
KW  - spectral change
KW  - UAVs
KW  - green attack
KW  - forest infestation
KW  - near infrared (NIR)
KW  - visible spectrum
DO  - 10.3390/rs11131561
TY  - EJOU
AU  - Lee, SangSik
AU  - Jeong, YiNa
AU  - Son, SuRak
AU  - Lee, ByungKwan
TI  - A Self-Predictable Crop Yield Platform (SCYP) Based On Crop Diseases Using Deep Learning
T2  - Sustainability

PY  - 2019
VL  - 11
IS  - 13
SN  - 2071-1050

AB  - This paper proposes a self-predictable crop yield platform (SCYP) based on crop diseases using deep learning that collects weather information (temperature, humidity, sunshine, precipitation, etc.) and farm status information (harvest date, disease information, crop status, ground temperature, etc.), diagnoses crop diseases by using convolutional neural network (CNN), and predicts crop yield based on factors such as climate change, crop diseases, and others by using artificial neural network (ANN). The SCYP consists of an image preprocessing module (IPM) to determine crop diseases through the Google Vision API and image resizing, a crop disease diagnosis module (CDDM) based on CNN to diagnose the types and extent of crop diseases through photographs, and a crop yield prediction module (CYPM) based on ANN by using information of crop diseases, remaining time until harvest (based on the date), current temperature, humidity and precipitation (amount of snowfall) in the area, sunshine amount, ground temperature, atmospheric pressure, moisture evaporation in the ground, etc. Four experiments were conducted to verify the efficiency of the SCYP. In the CDMM, the accuracy and operation time of each model were measured using three neural network models: CNN, region-CNN(R-CNN), and you only look once (YOLO). In the CYPM, rectified linear unit (ReLU), Sigmoid, and Step activation functions were compared to measure ANN accuracy. The accuracy of CNN was about 3.5% higher than that of R-CNN and about 5.4% higher than that of YOLO. The operation time of CNN was about 37 s less than that of R-CNN and about 72 s less than that of YOLO. The CDDM had slightly less operation time, but in this paper, we prefer accuracy over operation time to diagnose crop diseases efficiently and accurately. When the activation function of the ANN used in the CYPM was ReLU, the accuracy of the ANN was 2% higher than that of Sigmoid and 7% higher than that of Step. The CYPM prediction was about 34% more accurate when using multiple diseases than when not using them. Therefore, the SCYP can predict farm yields more accurately than traditional methods.
KW  - crop disease diagnosis
KW  - yield prediction
KW  - CNN
KW  - ANN
KW  - image preprocessing
DO  - 10.3390/su11133637
TY  - EJOU
AU  - Chen, Yang
AU  - Lee, Won S.
AU  - Gan, Hao
AU  - Peres, Natalia
AU  - Fraisse, Clyde
AU  - Zhang, Yanchao
AU  - He, Yong
TI  - Strawberry Yield Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoimages
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 13
SN  - 2072-4292

AB  - Strawberry growers in Florida suffer from a lack of efficient and accurate yield forecasts for strawberries, which would allow them to allocate optimal labor and equipment, as well as other resources for harvesting, transportation, and marketing. Accurate estimation of the number of strawberry flowers and their distribution in a strawberry field is, therefore, imperative for predicting the coming strawberry yield. Usually, the number of flowers and their distribution are estimated manually, which is time-consuming, labor-intensive, and subjective. In this paper, we develop an automatic strawberry flower detection system for yield prediction with minimal labor and time costs. The system used a small unmanned aerial vehicle (UAV) (DJI Technology Co., Ltd., Shenzhen, China) equipped with an RGB (red, green, blue) camera to capture near-ground images of two varieties (Sensation and Radiance) at two different heights (2 m and 3 m) and built orthoimages of a 402 m2 strawberry field. The orthoimages were automatically processed using the Pix4D software and split into sequential pieces for deep learning detection. A faster region-based convolutional neural network (R-CNN), a state-of-the-art deep neural network model, was chosen for the detection and counting of the number of flowers, mature strawberries, and immature strawberries. The mean average precision (mAP) was 0.83 for all detected objects at 2 m heights and 0.72 for all detected objects at 3 m heights. We adopted this model to count strawberry flowers in November and December from 2 m aerial images and compared the results with a manual count. The average deep learning counting accuracy was 84.1% with average occlusion of 13.5%. Using this system could provide accurate counts of strawberry flowers, which can be used to forecast future yields and build distribution maps to help farmers observe the growth cycle of strawberry fields.
KW  - strawberry yield prediction
KW  - unmanned aerial vehicle
KW  - orthoimages
KW  - deep neural network
KW  - distribution map
DO  - 10.3390/rs11131584
TY  - EJOU
AU  - Contreras-Cruz, Marco A.
AU  - Ramirez-Paredes, Juan P.
AU  - Hernandez-Belmonte, Uriel H.
AU  - Ayala-Ramirez, Victor
TI  - Vision-Based Novelty Detection Using Deep Features and Evolved Novelty Filters for Specific Robotic Exploration and Inspection Tasks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 13
SN  - 1424-8220

AB  - One of the essential abilities in animals is to detect novelties within their environment. From the computational point of view, novelty detection consists of finding data that are different in some aspect to the known data. In robotics, researchers have incorporated novelty modules in robots to develop automatic exploration and inspection tasks. The visual sensor is one of the preferred sensors to perform this task. However, there exist problems as illumination changes, occlusion, and scale, among others. Besides, novelty detectors vary their performance depending on the specific application scenario. In this work, we propose a visual novelty detection framework for specific exploration and inspection tasks based on evolved novelty detectors. The system uses deep features to represent the visual information captured by the robots and applies a global optimization technique to design novelty detectors for specific robotics applications. We verified the performance of the proposed system against well-established state-of-the-art methods in a challenging scenario. This scenario was an outdoor environment covering typical problems in computer vision such as illumination changes, occlusion, and geometric transformations. The proposed framework presented high-novelty detection accuracy with competitive or even better results than the baseline methods.
KW  - visual inspection
KW  - one-class classifier
KW  - grow-when-required neural network
KW  - evolving connectionist systems
KW  - automatic design
KW  - bio-inspired techniques
KW  - artificial bee colony
DO  - 10.3390/s19132965
TY  - EJOU
AU  - Jalil, Bushra
AU  - Leone, Giuseppe R.
AU  - Martinelli, Massimo
AU  - Moroni, Davide
AU  - Pascali, Maria A.
AU  - Berton, Andrea
TI  - Fault Detection in Power Equipment via an Unmanned Aerial System Using Multi Modal Data
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 13
SN  - 1424-8220

AB  - The power transmission lines are the link between power plants and the points of consumption, through substations. Most importantly, the assessment of damaged aerial power lines and rusted conductors is of extreme importance for public safety; hence, power lines and associated components must be periodically inspected to ensure a continuous supply and to identify any fault and defect. To achieve these objectives, recently, Unmanned Aerial Vehicles (UAVs) have been widely used; in fact, they provide a safe way to bring sensors close to the power transmission lines and their associated components without halting the equipment during the inspection, and reducing operational cost and risk. In this work, a drone, equipped with multi-modal sensors, captures images in the visible and infrared domain and transmits them to the ground station. We used state-of-the-art computer vision methods to highlight expected faults (i.e., hot spots) or damaged components of the electrical infrastructure (i.e., damaged insulators). Infrared imaging, which is invariant to large scale and illumination changes in the real operating environment, supported the identification of faults in power transmission lines; while a neural network is adapted and trained to detect and classify insulators from an optical video stream. We demonstrate our approach on data captured by a drone in Parma, Italy.
KW  - image analysis
KW  - RGB images
KW  - infrared images
KW  - wire detection
KW  - unmanned aerial vehicles
KW  - object detection
KW  - neural networks
DO  - 10.3390/s19133014
TY  - EJOU
AU  - Bui, Xuan-Nam
AU  - Lee, Chang W.
AU  - Nguyen, Hoang
AU  - Bui, Hoang-Bac
AU  - Long, Nguyen Q.
AU  - Le, Qui-Thao
AU  - Nguyen, Van-Duc
AU  - Nguyen, Ngoc-Bich
AU  - Moayedi, Hossein
TI  - Estimating PM10 Concentration from Drilling Operations in Open-Pit Mines Using an Assembly of SVR and PSO
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 14
SN  - 2076-3417

AB  - Dust is one of the components causing heavy environmental pollution in open-pit mines, especially PM10. Some pathologies related to the lung, respiratory system, and occupational diseases have been identified due to the effects of PM10 in open-pit mines. Therefore, the prediction and control of PM10 concentration in the production process are necessary for environmental and health protection. In this study, PM10 concentration from drilling operations in the Coc Sau open-pit coal mine (Vietnam) was investigated and considered through a database including 245 datasets collected. A novel hybrid artificial intelligence model was developed based on support vector regression (SVR) and a swarm optimization algorithm (i.e., particle swarm optimization (PSO)), namely PSO-SVR, for estimating PM10 concentration from drilling operations at the mine. Polynomial (P), radial basis function (RBF), and linear (L) kernel functions were considered and applied to the development of the PSO-SVR models in the present study, abbreviated as PSO-SVR-P, PSO-SVR-RBF, and PSO-SVR-L. Also, three benchmark artificial intelligence techniques, such as k-nearest neighbors (KNN), random forest (RF), and classification and regression trees (CART), were applied and developed for estimating PM10 concentration and then compared with the PSO-SVR models. Root-mean-squared error (RMSE) and determination coefficient (R2) were used as the statistical criteria for evaluating the performance of the developed models. The results exhibited that the PSO algorithm had an essential role in the optimization of the hyper-parameters of the SVR models. The PSO-SVR models (i.e., PSO-SVR-L, PSO-SVR-P, and PSO-SVR-RBF) had higher performance levels than the other models (i.e., RF, CART, and KNN) with an RMSE of 0.040, 0.042, and 0.043; and R2 of 0.954, 0.948, and 0.946; for the PSO-SVR-L, PSO-SVR-P, and PSO-SVR-RBF models, respectively. Of these PSO-SVR models, the PSO-SVR-L model was the most dominant model with an RMSE of 0.040 and R2 of 0.954. The remaining three benchmark models (i.e., RF, CART, and KNN) yielded a more unsatisfactory performance with an RMSE of 0.060, 0.052, and 0.067; and R2 of 0.894, 0.924, and 0.867, for the RF, CART, and KNN models, respectively. Furthermore, the findings of this study demonstrated that the density of rock mass, moisture content, and the penetration rate of the drill were essential parameters on the PM10 concentration caused by drilling operations in open-pit mines.
KW  - meta-heuristic algorithm
KW  - PM10 concentration
KW  - drilling operation
KW  - artificial intelligence
KW  - open-pit coal mine
DO  - 10.3390/app9142806
TY  - EJOU
AU  - Peng, Yahui
AU  - Liu, Xiaochen
AU  - Shen, Chong
AU  - Huang, Haoqian
AU  - Zhao, Donghua
AU  - Cao, Huiliang
AU  - Guo, Xiaoting
TI  - An Improved Optical Flow Algorithm Based on Mask-R-CNN and K-Means for Velocity Calculation
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 14
SN  - 2076-3417

AB  - Aiming at enhancing the accuracy and reliability of velocity calculation in vision navigation, an improved method is proposed in this paper. The method integrates Mask-R-CNN (Mask Region-based Convolutional Neural Network) and K-Means with the pyramid Lucas Kanade algorithm in order to reduce the harmful effect of moving objects on velocity calculation. Firstly, Mask-R-CNN is used to recognize the objects which have motions relative to the ground and covers them with masks to enhance the similarity between pixels and to reduce the impacts of the noisy moving pixels. Then, the pyramid Lucas Kanade algorithm is used to calculate the optical flow value. Finally, the value is clustered by the K-Means algorithm to abandon the outliers, and vehicle velocity is calculated by the processed optical flow. The prominent advantages of the proposed algorithm are (i) decreasing the bad impacts to velocity calculation, due to the objects which have relative motions; (ii) obtaining the correct optical flow sets and velocity calculation outputs with less fluctuation; and (iii) the applicability enhancement of the optical flow algorithm in complex navigation environment. The proposed algorithm is tested by actual experiments. Results with superior precision and reliability show the feasibility and effectiveness of the proposed method for vehicle velocity calculation in vision navigation system.
KW  - velocity calculation
KW  - optical flow
KW  - pyramid LK
KW  - Mask-R-CNN
KW  - clustering algorithm
DO  - 10.3390/app9142808
TY  - EJOU
AU  - Zhou, Chengquan
AU  - Ye, Hongbao
AU  - Hu, Jun
AU  - Shi, Xiaoyan
AU  - Hua, Shan
AU  - Yue, Jibo
AU  - Xu, Zhifu
AU  - Yang, Guijun
TI  - Automated Counting of Rice Panicle by Applying Deep Learning Model to Images from Unmanned Aerial Vehicle Platform
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 14
SN  - 1424-8220

AB  - The number of panicles per unit area is a common indicator of rice yield and is of great significance to yield estimation, breeding, and phenotype analysis. Traditional counting methods have various drawbacks, such as long delay times and high subjectivity, and they are easily perturbed by noise. To improve the accuracy of rice detection and counting in the field, we developed and implemented a panicle detection and counting system that is based on improved region-based fully convolutional networks, and we use the system to automate rice-phenotype measurements. The field experiments were conducted in target areas to train and test the system and used a rotor light unmanned aerial vehicle equipped with a high-definition RGB camera to collect images. The trained model achieved a precision of 0.868 on a held-out test set, which demonstrates the feasibility of this approach. The algorithm can deal with the irregular edge of the rice panicle, the significantly different appearance between the different varieties and growing periods, the interference due to color overlapping between panicle and leaves, and the variations in illumination intensity and shading effects in the field. The result is more accurate and efficient recognition of rice-panicles, which facilitates rice breeding. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a global scale.
KW  - rice panicle counting
KW  - UAV platform
KW  - deep learning
KW  - yield estimation
DO  - 10.3390/s19143106
TY  - EJOU
AU  - Fu, Yongyong
AU  - Ye, Ziran
AU  - Deng, Jinsong
AU  - Zheng, Xinyu
AU  - Huang, Yibo
AU  - Yang, Wu
AU  - Wang, Yaohua
AU  - Wang, Ke
TI  - Finer Resolution Mapping of Marine Aquaculture Areas Using WorldView-2 Imagery and a Hierarchical Cascade Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 14
SN  - 2072-4292

AB  - Marine aquaculture plays an important role in seafood supplement, economic development, and coastal ecosystem service provision. The precise delineation of marine aquaculture areas from high spatial resolution (HSR) imagery is vital for the sustainable development and management of coastal marine resources. However, various sizes and detailed structures of marine objects make it difficult for accurate mapping from HSR images by using conventional methods. Therefore, this study attempts to extract marine aquaculture areas by using an automatic labeling method based on the convolutional neural network (CNN), i.e., an end-to-end hierarchical cascade network (HCNet). Specifically, for marine objects of various sizes, we propose to improve the classification performance by utilizing multi-scale contextual information. Technically, based on the output of a CNN encoder, we employ atrous convolutions to capture multi-scale contextual information and aggregate them in a hierarchical cascade way. Meanwhile, for marine objects with detailed structures, we propose to refine the detailed information gradually by using a series of long-span connections with fine resolution features from the shallow layers. In addition, to decrease the semantic gaps between features in different levels, we propose to refine the feature space (i.e., channel and spatial dimensions) using an attention-based module. Experimental results show that our proposed HCNet can effectively identify and distinguish different kinds of marine aquaculture, with 98% of overall accuracy. It also achieves better classification performance compared with object-based support vector machine and state-of-the-art CNN-based methods, such as FCN-32s, U-Net, and DeeplabV2. Our developed method lays a solid foundation for the intelligent monitoring and management of coastal marine resources.
KW  - marine aquaculture areas
KW  - WorldView-2 imagery
KW  - fully convolutional network (FCN)
KW  - land-use and land-cover (LULC) mapping
DO  - 10.3390/rs11141678
TY  - EJOU
AU  - Guo, Jia
AU  - Gong, Xiangyang
AU  - Wang, Wendong
AU  - Que, Xirong
AU  - Liu, Jingyu
TI  - SASRT: Semantic-Aware Super-Resolution Transmission for Adaptive Video Streaming over Wireless Multimedia Sensor Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 14
SN  - 1424-8220

AB  - There are few network resources in wireless multimedia sensor networks (WMSNs). Compressing media data can reduce the reliance of user&rsquo;s Quality of Experience (QoE) on network resources. Existing video coding software, such as H.264 and H.265, focuses only on spatial and short-term information redundancy. However, video usually contains redundancy over a long period of time. Therefore, compressing video information redundancy with a long period of time without compromising the user experience and adaptive delivery is a challenge in WMSNs. In this paper, a semantic-aware super-resolution transmission for adaptive video streaming system (SASRT) for WMSNs is presented. In the SASRT, some deep learning algorithms are used to extract video semantic information and enrich the video quality. On the multimedia sensor, different bit-rate semantic information and video data are encoded and uploaded to user. Semantic information can also be identified on the user side, further reducing the amount of data that needs to be transferred. However, identifying semantic information on the user side may increase the computational cost of the user side. On the user side, video quality is enriched with super-resolution technologies. The major challenges faced by SASRT include where the semantic information is identified, how to choose the bit rates of semantic and video information, and how network resources should be allocated to video and semantic information. The optimization problem is formulated as a complexity-constrained nonlinear NP-hard problem. Three adaptive strategies and a heuristic algorithm are proposed to solve the optimization problem. Simulation results demonstrate that SASRT can compress video information redundancy with a long period of time effectively and enrich the user experience with limited network resources while simultaneously improving the utilization of these network resources.
KW  - video streaming optimization
KW  - semantic-aware
KW  - super-resolution
KW  - wireless multimedia sensor networks
DO  - 10.3390/s19143121
TY  - EJOU
AU  - Manzo, Mario
TI  - Graph-Based Image Matching for Indoor Localization
T2  - Machine Learning and Knowledge Extraction

PY  - 2019
VL  - 1
IS  - 3
SN  - 2504-4990

AB  - Graphs are a very useful framework for representing information. In general, these data structures are used in different application domains where data of interest are described in terms of local and spatial relations. In this context, the aim is to propose an alternative graph-based image representation. An image is encoded by a Region Adjacency Graph (RAG), based on Multicolored Neighborhood (MCN) clustering. This representation is integrated into a Content-Based Image Retrieval (CBIR) system, designed for the vision-based positioning task. The image matching phase, in the CBIR system, is managed with an approach of attributed graph matching, named the extended-VF algorithm. Evaluated in a context of indoor localization, the proposed system reports remarkable performance.
KW  - content-based image retrieval
KW  - clustering
KW  - attributed graph matching
KW  - image-based localization
DO  - 10.3390/make1030046
TY  - EJOU
AU  - Farooq, Adnan
AU  - Jia, Xiuping
AU  - Hu, Jiankun
AU  - Zhou, Jun
TI  - Multi-Resolution Weed Classification via Convolutional Neural Network and Superpixel Based Local Binary Pattern Using Remote Sensing Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 14
SN  - 2072-4292

AB  - Automatic weed detection and classification faces the challenges of large intraclass variation and high spectral similarity to other vegetation. With the availability of new high-resolution remote sensing data from various platforms and sensors, it is possible to capture both spectral and spatial characteristics of weed species at multiple scales. Effective multi-resolution feature learning is then desirable to extract distinctive intensity, texture and shape features of each category of weed to enhance the weed separability. We propose a feature extraction method using a Convolutional Neural Network (CNN) and superpixel based Local Binary Pattern (LBP). Both middle and high level spatial features are learned using the CNN. Local texture features from superpixel-based LBP are extracted, and are also used as input to Support Vector Machines (SVM) for weed classification. Experimental results on the hyperspectral and remote sensing datasets verify the effectiveness of the proposed method, and show that it outperforms several feature extraction approaches.
KW  - hyperspectral images
KW  - weed mapping
KW  - multi-resolution
KW  - local binary pattern (LBP)
KW  - convolutional neural network (CNN)
DO  - 10.3390/rs11141692
TY  - EJOU
AU  - Mekhalfi, Mohamed L.
AU  - Bejiga, Mesay B.
AU  - Soresina, Davide
AU  - Melgani, Farid
AU  - Demir, Begüm
TI  - Capsule Networks for Object Detection in UAV Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 14
SN  - 2072-4292

AB  - Recent advances in Convolutional Neural Networks (CNNs) have attracted great attention in remote sensing due to their high capability to model high-level semantic content of Remote Sensing (RS) images. However, CNNs do not explicitly retain the relative position of objects in an image and, thus, the effectiveness of the obtained features is limited in the framework of the complex object detection problems. To address this problem, in this paper we introduce Capsule Networks (CapsNets) for object detection in Unmanned Aerial Vehicle-acquired images. Unlike CNNs, CapsNets extract and exploit the information content about objects&rsquo; relative position across several layers, which enables parsing crowded scenes with overlapping objects. Experimental results obtained on two datasets for car and solar panel detection problems show that CapsNets provide similar object detection accuracies when compared to state-of-the-art deep models with significantly reduced computational time. This is due to the fact that CapsNets emphasize dynamic routine instead of the depth.
KW  - unmanned aerial vehicles
KW  - object detection
KW  - convolutional neural networks
KW  - capsule networks
KW  - dynamic routing
DO  - 10.3390/rs11141694
TY  - EJOU
AU  - Cao, Shuang
AU  - Yu, Yongtao
AU  - Guan, Haiyan
AU  - Peng, Daifeng
AU  - Yan, Wanqian
TI  - Affine-Function Transformation-Based Object Matching for Vehicle Detection from Unmanned Aerial Vehicle Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 14
SN  - 2072-4292

AB  - Vehicle detection from remote sensing images plays a significant role in transportation related applications. However, the scale variations, orientation variations, illumination variations, and partial occlusions of vehicles, as well as the image qualities, bring great challenges for accurate vehicle detection. In this paper, we present an affine-function transformation-based object matching framework for vehicle detection from unmanned aerial vehicle (UAV) images. First, meaningful and non-redundant patches are generated through a superpixel segmentation strategy. Then, the affine-function transformation-based object matching framework is applied to a vehicle template and each of the patches for vehicle existence estimation. Finally, vehicles are detected and located after matching cost thresholding, vehicle location estimation, and multiple response elimination. Quantitative evaluations on two UAV image datasets show that the proposed method achieves an average completeness, correctness, quality, and F1-measure of 0.909, 0.969, 0.883, and 0.938, respectively. Comparative studies also demonstrate that the proposed method achieves compatible performance with the Faster R-CNN and outperforms the other eight existing methods in accurately detecting vehicles of various conditions.
KW  - vehicle detection
KW  - object matching
KW  - superpixel segmentation
KW  - unmanned aerial vehicle
KW  - remote sensing imagery
DO  - 10.3390/rs11141708
TY  - EJOU
AU  - Jozdani, Shahab E.
AU  - Johnson, Brian A.
AU  - Chen, Dongmei
TI  - Comparing Deep Neural Networks, Ensemble Classifiers, and Support Vector Machine Algorithms for Object-Based Urban Land Use/Land Cover Classification
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 14
SN  - 2072-4292

AB  - With the advent of high-spatial resolution (HSR) satellite imagery, urban land use/land cover (LULC) mapping has become one of the most popular applications in remote sensing. Due to the importance of context information (e.g., size/shape/texture) for classifying urban LULC features, Geographic Object-Based Image Analysis (GEOBIA) techniques are commonly employed for mapping urban areas. Regardless of adopting a pixel- or object-based framework, the selection of a suitable classifier is of critical importance for urban mapping. The popularity of deep learning (DL) (or deep neural networks (DNNs)) for image classification has recently skyrocketed, but it is still arguable if, or to what extent, DL methods can outperform other state-of-the art ensemble and/or Support Vector Machines (SVM) algorithms in the context of urban LULC classification using GEOBIA. In this study, we carried out an experimental comparison among different architectures of DNNs (i.e., regular deep multilayer perceptron (MLP), regular autoencoder (RAE), sparse, autoencoder (SAE), variational autoencoder (AE), convolutional neural networks (CNN)), common ensemble algorithms (Random Forests (RF), Bagging Trees (BT), Gradient Boosting Trees (GB), and Extreme Gradient Boosting (XGB)), and SVM to investigate their potential for urban mapping using a GEOBIA approach. We tested the classifiers on two RS images (with spatial resolutions of 30 cm and 50 cm). Based on our experiments, we drew three main conclusions: First, we found that the MLP model was the most accurate classifier. Second, unsupervised pretraining with the use of autoencoders led to no improvement in the classification result. In addition, the small difference in the classification accuracies of MLP from those of other models like SVM, GB, and XGB classifiers demonstrated that other state-of-the-art machine learning classifiers are still versatile enough to handle mapping of complex landscapes. Finally, the experiments showed that the integration of CNN and GEOBIA could not lead to more accurate results than the other classifiers applied.
KW  - remote sensing
KW  - high-spatial resolution imagery
KW  - deep learning
KW  - GEOBIA
KW  - land use/cover classification
DO  - 10.3390/rs11141713
TY  - EJOU
AU  - Gong, Qingwu
AU  - Tan, Si
AU  - Wang, Yubo
AU  - Liu, Dong
AU  - Qiao, Hui
AU  - Wu, Liuchuang
TI  - Online Operation Risk Assessment of the Wind Power System of the Convolution Neural Network (CNN) Considering Multiple Random Factors
T2  - Processes

PY  - 2019
VL  - 7
IS  - 7
SN  - 2227-9717

AB  - In order to solve the problem of the inaccuracy of the traditional online operation risk assessment model based on a physical mechanism and the inability to adapt to the actual operation of massive online operation monitoring data, this paper proposes an online operation risk assessment of the wind power system of the convolution neural network (CNN) considering multiple random factors. This paper analyzes multiple random factors of the wind power system, including uncertain wind power output, load fluctuations, frequent changes in operation patterns, and the electrical equipment failure rate, and generates the sample data based on multi-random factors. It uses the CNN algorithm network, offline training to obtain the risk assessment model, and online application to obtain the real-time online operation risk state of the wind power system. Finally, the online operation risk assessment model is verified by simulation using the standard network of 39 nodes of 10 machines New England system. The results prove that the risk assessment model presented in this paper is more rapid and suitable for online application.
KW  - online operation risk assessment
KW  - uncertain wind power output
KW  - load fluctuations
KW  - operation pattern
KW  - equipment failure rate
KW  - CNN
DO  - 10.3390/pr7070464
TY  - EJOU
AU  - Kim, Whui
AU  - Jung, Woo-Sung
AU  - Choi, Hyun K.
TI  - Lightweight Driver Monitoring System Based on Multi-Task Mobilenets
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 14
SN  - 1424-8220

AB  - Research on driver status recognition has been actively conducted to reduce fatal crashes caused by the driver&rsquo;s distraction and drowsiness. As in many other research areas, deep-learning-based algorithms are showing excellent performance for driver status recognition. However, despite decades of research in the driver status recognition area, the visual image-based driver monitoring system has not been widely used in the automobile industry. This is because the system requires high-performance processors, as well as has a hierarchical structure in which each procedure is affected by an inaccuracy from the previous procedure. To avoid using a hierarchical structure, we propose a method using Mobilenets without the functions of face detection and tracking and show this method is enabled to recognize facial behaviors that indicate the driver&rsquo;s distraction. However, frames per second processed by Mobilenets with a Raspberry pi, one of the single-board computers, is not enough to recognize the driver status. To alleviate this problem, we propose a lightweight driver monitoring system using a resource sharing device in a vehicle (e.g., a driver&rsquo;s mobile phone). The proposed system is based on Multi-Task Mobilenets (MT-Mobilenets), which consists of the Mobilenets&rsquo; base and multi-task classifier. The three Softmax regressions of the multi-task classifier help one Mobilenets base recognize facial behaviors related to the driver status, such as distraction, fatigue, and drowsiness. The proposed system based on MT-Mobilenets improved the accuracy of the driver status recognition with Raspberry Pi by using one additional device.
KW  - lightweight
KW  - driver assistance
KW  - drowsiness
KW  - fatigue
KW  - distraction
KW  - PERCLOS
KW  - ECT
KW  - ECD
KW  - single-board computer
KW  - SBC
KW  - Raspberry pi
DO  - 10.3390/s19143200
TY  - EJOU
AU  - Tsolakis, Naoum
AU  - Bechtsis, Dimitrios
AU  - Bochtis, Dionysis
TI  - AgROS: A Robot Operating System Based Emulation Tool for Agricultural Robotics
T2  - Agronomy

PY  - 2019
VL  - 9
IS  - 7
SN  - 2073-4395

AB  - This research aims to develop a farm management emulation tool that enables agrifood producers to effectively introduce advanced digital technologies, like intelligent and autonomous unmanned ground vehicles (UGVs), in real-world field operations. To that end, we first provide a critical taxonomy of studies investigating agricultural robotic systems with regard to: (i) the analysis approach, i.e., simulation, emulation, real-world implementation; (ii) farming operations; and (iii) the farming type. Our analysis demonstrates that simulation and emulation modelling have been extensively applied to study advanced agricultural machinery while the majority of the extant research efforts focuses on harvesting/picking/mowing and fertilizing/spraying activities; most studies consider a generic agricultural layout. Thereafter, we developed AgROS, an emulation tool based on the Robot Operating System, which could be used for assessing the efficiency of real-world robot systems in customized fields. The AgROS allows farmers to select their actual field from a map layout, import the landscape of the field, add characteristics of the actual agricultural layout (e.g., trees, static objects), select an agricultural robot from a predefined list of commercial systems, import the selected UGV into the emulation environment, and test the robot&rsquo;s performance in a quasi-real-world environment. AgROS supports farmers in the ex-ante analysis and performance evaluation of robotized precision farming operations while lays the foundations for realizing &ldquo;digital twins&rdquo; in agriculture.
KW  - digital agriculture
KW  - agricultural robotics
KW  - unmanned ground vehicles
KW  - robot operating system
KW  - emulation
KW  - precision farming
DO  - 10.3390/agronomy9070403
TY  - EJOU
AU  - Tao, Jiadong
AU  - Yin, Zhong
AU  - Liu, Lei
AU  - Tian, Ying
AU  - Sun, Zhanquan
AU  - Zhang, Jianhua
TI  - Individual-Specific Classification of Mental Workload Levels Via an Ensemble Heterogeneous Extreme Learning Machine for EEG Modeling
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 7
SN  - 2073-8994

AB  - In a human&ndash;machine cooperation system, assessing the mental workload (MW) of the human operator is quite crucial to maintaining safe operation conditions. Among various MW indicators, electroencephalography (EEG) signals are particularly attractive because of their high temporal resolution and sensitivity to the occupation of working memory. However, the individual difference of the EEG feature distribution may impair the machine-learning based MW classifier. In this paper, we employed a fast-training neural network, extreme learning machine (ELM), as the basis to build an individual-specific classifier ensemble to recognize binary MW. To improve the diversity of the classification committee, heterogeneous member classifiers were adopted by fusing multiple ELMs and Bayesian models. Specifically, a deep network structure was applied in each weak model aiming at finding informative EEG feature representations. The structure of hyper-parameters of the proposed heterogeneous ensemble ELM (HE-ELM) was then identified and then its performance was compared against several competitive MW classifiers. We found that the HE-ELM model was superior for improving the individual-specific accuracy of MW assessments.
KW  - electroencephalography
KW  - mental workload
KW  - extreme learning machine
KW  - ensemble learning
DO  - 10.3390/sym11070944
TY  - EJOU
AU  - Iannace, Gino
AU  - Ciaburro, Giuseppe
AU  - Trematerra, Amelia
TI  - Fault Diagnosis for UAV Blades Using Artificial Neural Network
T2  - Robotics

PY  - 2019
VL  - 8
IS  - 3
SN  - 2218-6581

AB  - In recent years, unmanned aerial vehicles (UAVs) have been used in several fields including, for example, archaeology, cargo transport, conservation, healthcare, filmmaking, hobbies and recreational use. UAVs are aircraft characterized by the absence of a human pilot on board. The extensive use of these devices has highlighted maintenance problems with regard to the propellers, which represent the source of propulsion of the aircraft. A defect in the propellers of a drone can cause the aircraft to fall to the ground and its consequent destruction, and it also constitutes a safety problem for objects and people that are in the range of action of the aircraft. In this study, the measurements of the noise emitted by a UAV were used to build a classification model to detect unbalanced blades in a UAV propeller. To simulate the fault condition, two strips of paper tape were applied to the upper surface of a blade. The paper tape created a substantial modification of the aerodynamics of the blade, and this modification characterized the noise produced by the blade in its rotation. Then, a model based on artificial neural network algorithms was built to detect unbalanced blades in a UAV propeller. This model showed high accuracy (0.9763), indicating a high number of correct detections and suggests the adoption of this tool to verify the operating conditions of a UAV. The test must be performed indoors; from the measurements of the noise produced by the UAV it is possible to identify an imbalance in the propeller blade.
KW  - quadcopter UAV
KW  - artificial neural network
KW  - fault diagnosis
KW  - acoustic measurements
KW  - unbalanced propeller
DO  - 10.3390/robotics8030059
TY  - EJOU
AU  - Zhou, Sanzhang
AU  - Kang, Feng
AU  - Li, Wenbin
AU  - Kan, Jiangming
AU  - Zheng, Yongjun
AU  - He, Guojian
TI  - Extracting Diameter at Breast Height with a Handheld Mobile LiDAR System in an Outdoor Environment
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 14
SN  - 1424-8220

AB  - Mobile laser scanning (MLS) is widely used in the mapping of forest environments. It has become important for extracting the parameters of forest trees using the generated environmental map. In this study, a three-dimensional point cloud map of a forest area was generated by using the Velodyne VLP-16 LiDAR system, so as to extract the diameter at breast height (DBH) of individual trees. The Velodyne VLP-16 LiDAR system and inertial measurement units (IMU) were used to construct a mobile measurement platform for generating 3D point cloud maps for forest areas. The 3D point cloud map in the forest area was processed offline, and the ground point cloud was removed by the random sample consensus (RANSAC) algorithm. The trees in the experimental area were segmented by the European clustering algorithm, and the DBH component of the tree point cloud was extracted and projected onto a 2D plane, fitting the DBH of the trees using the RANSAC algorithm in the plane. A three-dimensional point cloud map of 71 trees was generated in the experimental area, and estimated the DBH. The mean and variance of the absolute error were 0.43 cm and 0.50, respectively. The relative error of the whole was 2.27%, the corresponding variance was 15.09, and the root mean square error (RMSE) was 0.70 cm. The experimental results were good and met the requirements of forestry mapping, and the application value and significance were presented.
KW  - mobile laser scanning
KW  - 3D point cloud map
KW  - diameter at breast height
DO  - 10.3390/s19143212
TY  - EJOU
AU  - Cho, Jaechan
AU  - Jung, Yongchul
AU  - Kim, Dong-Sun
AU  - Lee, Seongjoo
AU  - Jung, Yunho
TI  - Moving Object Detection Based on Optical Flow Estimation and a Gaussian Mixture Model for Advanced Driver Assistance Systems
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 14
SN  - 1424-8220

AB  - Most approaches for moving object detection (MOD) based on computer vision are limited to stationary camera environments. In advanced driver assistance systems (ADAS), however, ego-motion is added to image frames owing to the use of a moving camera. This results in mixed motion in the image frames and makes it difficult to classify target objects and background. In this paper, we propose an efficient MOD algorithm that can cope with moving camera environments. In addition, we present a hardware design and implementation results for the real-time processing of the proposed algorithm. The proposed moving object detector was designed using hardware description language (HDL) and its real-time performance was evaluated using an FPGA based test system. Experimental results demonstrate that our design achieves better detection performance than existing MOD systems. The proposed moving object detector was implemented with 13.2K logic slices, 104 DSP48s, and 163 BRAM and can support real-time processing of 30 fps at an operating frequency of 200 MHz.
KW  - ADAS
KW  - background subtraction
KW  - FPGA
KW  - moving object detection
KW  - optical flow estimation
DO  - 10.3390/s19143217
TY  - EJOU
AU  - Jakovljevic, Gordana
AU  - Govedarica, Miro
AU  - Alvarez-Taboada, Flor
AU  - Pajic, Vladimir
TI  - Accuracy Assessment of Deep Learning Based Classification of LiDAR and UAV Points Clouds for DTM Creation and Flood Risk Mapping
T2  - Geosciences

PY  - 2019
VL  - 9
IS  - 7
SN  - 2076-3263

AB  - Digital elevation model (DEM) has been frequently used for the reduction and management of flood risk. Various classification methods have been developed to extract DEM from point clouds. However, the accuracy and computational efficiency need to be improved. The objectives of this study were as follows: (1) to determine the suitability of a new method to produce DEM from unmanned aerial vehicle (UAV) and light detection and ranging (LiDAR) data, using a raw point cloud classification and ground point filtering based on deep learning and neural networks (NN); (2) to test the convenience of rebalancing datasets for point cloud classification; (3) to evaluate the effect of the land cover class on the algorithm performance and the elevation accuracy; and (4) to assess the usability of the LiDAR and UAV structure from motion (SfM) DEM in flood risk mapping. In this paper, a new method of raw point cloud classification and ground point filtering based on deep learning using NN is proposed and tested on LiDAR and UAV data. The NN was trained on approximately 6 million points from which local and global geometric features and intensity data were extracted. Pixel-by-pixel accuracy assessment and visual inspection confirmed that filtering point clouds based on deep learning using NN is an appropriate technique for ground classification and producing DEM, as for the test and validation areas, both ground and non-ground classes achieved high recall (&gt;0.70) and high precision values (&gt;0.85), which showed that the two classes were well handled by the model. The type of method used for balancing the original dataset did not have a significant influence in the algorithm accuracy, and it was suggested not to use any of them unless the distribution of the generated and real data set will remain the same. Furthermore, the comparisons between true data and LiDAR and a UAV structure from motion (UAV SfM) point clouds were analyzed, as well as the derived DEM. The root mean square error (RMSE) and the mean average error (MAE) of the DEM were 0.25 m and 0.05 m, respectively, for LiDAR data, and 0.59 m and &ndash;0.28 m, respectively, for UAV data. For all land cover classes, the UAV DEM overestimated the elevation, whereas the LIDAR DEM underestimated it. The accuracy was not significantly different in the LiDAR DEM for the different vegetation classes, while for the UAV DEM, the RMSE increased with the height of the vegetation class. The comparison of the inundation areas derived from true LiDAR and UAV data for different water levels showed that in all cases, the largest differences were obtained for the lowest water level tested, while they performed best for very high water levels. Overall, the approach presented in this work produced DEM from LiDAR and UAV data with the required accuracy for flood mapping according to European Flood Directive standards. Although LiDAR is the recommended technology for point cloud acquisition, a suitable alternative is also UAV SfM in hilly areas.
KW  - DEM
KW  - NN
KW  - deep learning
KW  - classification
KW  - LIDAR
KW  - UAV
KW  - SfM
KW  - point cloud
DO  - 10.3390/geosciences9070323
TY  - EJOU
AU  - Ali, Haibat
AU  - Choi, Jae-ho
TI  - A Review of Underground Pipeline Leakage and Sinkhole Monitoring Methods Based on Wireless Sensor Networking
T2  - Sustainability

PY  - 2019
VL  - 11
IS  - 15
SN  - 2071-1050

AB  - Major metropolitan cities worldwide have extensively invested to secure utilities and build state-of-the-art infrastructure related to underground fluid transportation. Sewer and water pipelines make our lives extremely convenient when they function appropriately. However, leakages in underground pipe mains causes sinkholes and drinking-water scarcity. Sinkholes are the complex problems stemming from the interaction of leaked water and ground. The aim of this work is to review the existing methods for monitoring leakage in underground pipelines, the sinkholes caused by these leakages, and the viability of wireless sensor networking (WSN) for monitoring leakages and sinkholes. Herein, the authors have discussed the methods based on different objectives and their applicability via various approaches&mdash;(1) patent analysis; (2) web-of-science analysis; (3) WSN-based pipeline leakage and sinkhole monitoring. The study shows that the research on sinkholes due to leakages in sewer and water pipelines by using WSN is still in a premature stage and needs extensive investigation and research contributions. Additionally, the authors have suggested prospects for future research by comparing, analyzing, and classifying the reviewed methods. This study advocates collocating WSN, Internet of things, and artificial intelligence with pipeline monitoring methods to resolve the issues of the sinkhole occurrence.
KW  - WSN
KW  - pipeline leakage
KW  - human-induced sinkhole
KW  - leakage detection
KW  - sewer pipeline
KW  - sensors
DO  - 10.3390/su11154007
TY  - EJOU
AU  - Cao, Mingwei
AU  - Jia, Wei
AU  - Lv, Zhihan
AU  - Zheng, Liping
AU  - Liu, Xiaoping
TI  - Superpixel-Based Feature Tracking for Structure from Motion
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 15
SN  - 2076-3417

AB  - Feature tracking in image collections significantly affects the efficiency and accuracy of Structure from Motion (SFM). Insufficient correspondences may result in disconnected structures and incomplete components, while the redundant correspondences containing incorrect ones may yield to folded and superimposed structures. In this paper, we present a Superpixel-based feature tracking method for structure from motion. In the proposed method, we first propose to use a joint approach to detect local keypoints and compute descriptors. Second, the superpixel-based approach is used to generate labels for the input image. Third, we combine the Speed Up Robust Feature and binary test in the generated label regions to produce a set of combined descriptors for the detected keypoints. Fourth, the locality-sensitive hash (LSH)-based k nearest neighboring matching (KNN) is utilized to produce feature correspondences, and then the ratio test approach is used to remove outliers from the previous matching collection. Finally, we conduct comprehensive experiments on several challenging benchmarking datasets including highly ambiguous and duplicated scenes. Experimental results show that the proposed method gets better performances with respect to the state of the art methods.
KW  - feature tracking
KW  - superpixel
KW  - structure from motion
KW  - three-dimensional reconstruction
KW  - local feature
KW  - multi-view stereo
DO  - 10.3390/app9152961
TY  - EJOU
AU  - Li, Songyang
AU  - Yuan, Fei
AU  - Ata-UI-Karim, Syed T.
AU  - Zheng, Hengbiao
AU  - Cheng, Tao
AU  - Liu, Xiaojun
AU  - Tian, Yongchao
AU  - Zhu, Yan
AU  - Cao, Weixing
AU  - Cao, Qiang
TI  - Combining Color Indices and Textures of UAV-Based Digital Imagery for Rice LAI Estimation
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 15
SN  - 2072-4292

AB  - Leaf area index (LAI) is a fundamental indicator of plant growth status in agronomic and environmental studies. Due to rapid advances in unmanned aerial vehicle (UAV) and sensor technologies, UAV-based remote sensing is emerging as a promising solution for monitoring crop LAI with great flexibility and applicability. This study aimed to determine the feasibility of combining color and texture information derived from UAV-based digital images for estimating LAI of rice (Oryza sativa L.). Rice field trials were conducted at two sites using different nitrogen application rates, varieties, and transplanting methods during 2016 to 2017. Digital images were collected using a consumer-grade UAV after sampling at key growth stages of tillering, stem elongation, panicle initiation and booting. Vegetation color indices (CIs) and grey level co-occurrence matrix-based textures were extracted from mosaicked UAV ortho-images for each plot. As a solution of using indices composed by two different textures, normalized difference texture indices (NDTIs) were calculated by two randomly selected textures. The relationships between rice LAIs and each calculated index were then compared using simple linear regression. Multivariate regression models with different input sets were further used to test the potential of combining CIs with various textures for rice LAI estimation. The results revealed that the visible atmospherically resistant index (VARI) based on three visible bands and the NDTI based on the mean textures derived from the red and green bands were the best for LAI retrieval in the CI and NDTI groups, respectively. Independent accuracy assessment showed that random forest (RF) exhibited the best predictive performance when combining CI and texture inputs (R2 = 0.84, RMSE = 0.87, MAE = 0.69). This study introduces a promising solution of combining color indices and textures from UAV-based digital imagery for rice LAI estimation. Future studies are needed on finding the best operation mode, suitable ground resolution, and optimal predictive methods for practical applications.
KW  - leaf area index (LAI)
KW  - UAV RGB imagery
KW  - color index
KW  - texture
KW  - rice
DO  - 10.3390/rs11151763
TY  - EJOU
AU  - Yun, Sungmin
AU  - Kim, Sungho
TI  - TIR-MS: Thermal Infrared Mean-Shift for Robust Pedestrian Head Tracking in Dynamic Target and Background Variations
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 15
SN  - 2076-3417

AB  - Thermal infrared (TIR) pedestrian tracking is one of the major issues in computer vision. Mean-shift is a powerful and versatile non-parametric iterative algorithm for finding local maxima in probability distributions. In existing infrared data, and mean-shift-based tracking is generally based on the brightness feature values. Unfortunately, the brightness is distorted by the target and background variations. This paper proposes a novel pedestrian tracking algorithm, thermal infrared mean-shift (TIR-MS), by introducing radiometric temperature data in mean-shift tracking. The thermal brightness image (eight-bits) was distorted by the automatic contrast enhancement of the scene such as hot objects in the background. On the other hand, the temperature data was unaffected directly by the background change, except for variations by the seasonal effect, which is more stable than the brightness. The experimental results showed that the TIR-MS outperformed the original mean-shift-based brightness when tracking a pedestrian head with successive background variations.
KW  - pedestrian tracking
KW  - infrared
KW  - temperature
KW  - brightness
KW  - background contrast
KW  - radiometry
KW  - mean shift
DO  - 10.3390/app9153015
TY  - EJOU
AU  - Salhaoui, Marouane
AU  - Guerrero-González, Antonio
AU  - Arioua, Mounir
AU  - Ortiz, Francisco J.
AU  - El Oualkadi, Ahmed
AU  - Torregrosa, Carlos L.
TI  - Smart Industrial IoT Monitoring and Control System Based on UAV and Cloud Computing Applied to a Concrete Plant
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 15
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) are now considered one of the best remote sensing techniques for gathering data over large areas. They are now being used in the industry sector as sensing tools for proactively solving or preventing many issues, besides quantifying production and helping to make decisions. UAVs are a highly consistent technological platform for efficient and cost-effective data collection and event monitoring. The industrial Internet of things (IIoT) sends data from systems that monitor and control the physical world to data processing systems that cloud computing has shown to be important tools for meeting processing requirements. In fog computing, the IoT gateway links different objects to the internet. It can operate as a joint interface for different networks and support different communication protocols. A great deal of effort has been put into developing UAVs and multi-UAV systems. This paper introduces a smart IIoT monitoring and control system based on an unmanned aerial vehicle that uses cloud computing services and exploits fog computing as the bridge between IIoT layers. Its novelty lies in the fact that the UAV is automatically integrated into an industrial control system through an IoT gateway platform, while UAV photos are systematically and instantly computed and analyzed in the cloud. Visual supervision of the plant by drones and cloud services is integrated in real-time into the control loop of the industrial control system. As a proof of concept, the platform was used in a case study in an industrial concrete plant. The results obtained clearly illustrate the feasibility of the proposed platform in providing a reliable and efficient system for UAV remote control to improve product quality and reduce waste. For this, we studied the communication latency between the different IIoT layers in different IoT gateways.
KW  - UAVs
KW  - drones
KW  - industry 4.0
KW  - concrete plant
KW  - IoT protocols
KW  - IoT gateway
KW  - image recognition
KW  - cloud computing
KW  - network latency
KW  - end-to-end delay
DO  - 10.3390/s19153316
TY  - EJOU
AU  - Fan, Guangpeng
AU  - Chen, Feixiang
AU  - Li, Yan
AU  - Liu, Binbin
AU  - Fan, Xu
TI  - Development and Testing of a New Ground Measurement Tool to Assist in Forest GIS Surveys
T2  - Forests

PY  - 2019
VL  - 10
IS  - 8
SN  - 1999-4907

AB  - In present forest surveys, some problems occur because of the cost and time required when using external tools to acquire tree measurement. Therefore, it is of great importance to develop a new cost-saving and time-saving ground measurement method implemented in a forest geographic information system (GIS) survey. To obtain a better solution, this paper presents the design and implementation of a new ground measurement tool in which mobile devices play a very important role. Based on terrestrial photogrammetry, location-based services (LBS), and computer vision, the tool assists forest GIS surveys in obtaining important forest structure factors such as tree position, diameter at breast height (DBH), tree height, and tree species. This paper selected two plots to verify the accuracy of the ground measurement tool. Experiments show that the root mean square error (RMSE) of the position coordinates of the trees was 0.222 m and 0.229 m, respectively, and the relative root mean square error (rRMSE) was close to 0. The rRMSE of the DBH measurement was 10.17% and 13.38%, and the relative Bias (rBias) of the DBH measurement was &minus;0.88% and &minus;2.41%. The rRMSE of tree height measurement was 6.74% and 6.69%, and the rBias of tree height measurement was &minus;1.69% and &minus;1.27%, which conforms to the forest investigation requirements. In addition, workers usually make visual observations of trees and then combine their personal knowledge or experience to identify tree species, which may lead to the situations when they cannot distinguish tree species due to insufficient knowledge or experience. Based on MobileNets, a lightweight convolutional neural network designed for mobile phone, a model was trained to assist workers in identifying tree species. The dataset was collected from some forest parks in Beijing. The accuracy of the tree species recognition model was 94.02% on a test dataset and 93.21% on a test dataset in the mobile phone. This provides an effective reference for workers to identify tree species and can assist in artificial identification of tree species. Experiments show that this solution using the ground measurement tool saves time and cost for forest resources GIS surveys.
KW  - forest GIS surveys
KW  - terrestrial photogrammetry
KW  - LBS
KW  - MobileNets
KW  - measurement tool
DO  - 10.3390/f10080643
TY  - EJOU
AU  - Fuentes, Sigfredo
AU  - Tongson, Eden J.
AU  - De Bei, Roberta
AU  - Gonzalez Viejo, Claudia
AU  - Ristic, Renata
AU  - Tyerman, Stephen
AU  - Wilkinson, Kerry
TI  - Non-Invasive Tools to Detect Smoke Contamination in Grapevine Canopies, Berries and Wine: A Remote Sensing and Machine Learning Modeling Approach
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 15
SN  - 1424-8220

AB  - Bushfires are becoming more frequent and intensive due to changing climate. Those that occur close to vineyards can cause smoke contamination of grapevines and grapes, which can affect wines, producing smoke-taint. At present, there are no available practical in-field tools available for detection of smoke contamination or taint in berries. This research proposes a non-invasive/in-field detection system for smoke contamination in grapevine canopies based on predictable changes in stomatal conductance patterns based on infrared thermal image analysis and machine learning modeling based on pattern recognition. A second model was also proposed to quantify levels of smoke-taint related compounds as targets in berries and wines using near-infrared spectroscopy (NIR) as inputs for machine learning fitting modeling. Results showed that the pattern recognition model to detect smoke contamination from canopies had 96% accuracy. The second model to predict smoke taint compounds in berries and wine fit the NIR data with a correlation coefficient (R) of 0.97 and with no indication of overfitting. These methods can offer grape growers quick, affordable, accurate, non-destructive in-field screening tools to assist in vineyard management practices to minimize smoke taint in wines with in-field applications using smartphones and unmanned aerial systems (UAS).
KW  - bushfires
KW  - infrared thermography
KW  - near-infrared spectroscopy
KW  - smoke taint
KW  - artificial intelligence
DO  - 10.3390/s19153335
TY  - EJOU
AU  - Böhler, Jonas E.
AU  - Schaepman, Michael E.
AU  - Kneubühler, Mathias
TI  - Optimal Timing Assessment for Crop Separation Using Multispectral Unmanned Aerial Vehicle (UAV) Data and Textural Features
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 15
SN  - 2072-4292

AB  - The separation of crop types is essential for many agricultural applications, particularly when within-season information is required. Generally, remote sensing may provide timely information with varying accuracy over the growing season, but in small structured agricultural areas, a very high spatial resolution may be needed that exceeds current satellite capabilities. This paper presents an experiment using spectral and textural features of NIR-red-green-blue (NIR-RGB) bands data sets acquired with an unmanned aerial vehicle (UAV). The study area is located in the Swiss Plateau, which has highly fragmented and small structured agricultural fields. The observations took place between May 5 and September 29, 2015 over 11 days. The analyses are based on a random forest (RF) approach, predicting crop separation metrics of all analyzed crops. Three temporal windows of observations based on accumulated growing degree days (AGDD) were identified: an early temporal window (515&ndash;1232 AGDD, 5 May&ndash;17 June 2015) with an average accuracy (AA) of 70&ndash;75%; a mid-season window (1362&ndash;2016 AGDD, 25 June&ndash;22 July 2015) with an AA of around 80%; and a late window (2626&ndash;3238 AGDD, 21 August&ndash;29 September 2015) with an AA of &lt;65%. Therefore, crop separation is most promising in the mid-season window, and an additional NIR band increases the accuracy significantly. However, discrimination of winter crops is most effective in the early window, adding further observational requirements to the first window.
KW  - crop type separation
KW  - temporal window
KW  - small structured agricultural area
KW  - uncalibrated consumer-grade camera
KW  - unmanned aerial vehicle (UAV)
KW  - very high resolution (VHR)
KW  - random forest (RF) classifier
KW  - spectral and textural features
DO  - 10.3390/rs11151780
TY  - EJOU
AU  - Dash, Jonathan P.
AU  - Watt, Michael S.
AU  - Paul, Thomas S. H.
AU  - Morgenroth, Justin
AU  - Pearse, Grant D.
TI  - Early Detection of Invasive Exotic Trees Using UAV and Manned Aircraft Multispectral and LiDAR Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 15
SN  - 2072-4292

AB  - Exotic conifers can provide significant ecosystem services, but in some environments, they have become invasive and threaten indigenous ecosystems. In New Zealand, this phenomenon is of considerable concern as the area occupied by invasive exotic trees is large and increasing rapidly. Remote sensing methods offer a potential means of identifying and monitoring land infested by these trees, enabling managers to efficiently allocate resources for their control. In this study, we sought to develop methods for remote detection of exotic invasive trees, namely Pinus sylvestris and P. ponderosa. Critically, the study aimed to detect these species prior to the onset of maturity and coning as this is important for preventing further spread. In the study environment in New Zealand&rsquo;s South Island, these species reach maturity and begin bearing cones at a young age. As such, detection of these smaller individuals requires specialist methods and very high-resolution remote sensing data. We examined the efficacy of classifiers developed using two machine learning algorithms with multispectral and laser scanning data collected from two platforms&mdash;manned aircraft and unmanned aerial vehicles (UAV). The study focused on a localized conifer invasion originating from a multi-species pine shelter belt in a grassland environment. This environment provided a useful means of defining the detection thresholds of the methods and technologies employed. An extensive field dataset including over 17,000 trees (height range = 1 cm to 476 cm) was used as an independent validation dataset for the detection methods developed. We found that data from both platforms and using both logistic regression and random forests for classification provided highly accurate (kappa     &lt; 0.996    ) detection of invasive conifers. Our analysis showed that the data from both UAV and manned aircraft was useful for detecting trees down to 1 m in height and therefore shorter than 99.3% of the coning individuals in the study dataset. We also explored the relative contribution of both multispectral and airborne laser scanning (ALS) data in the detection of invasive trees through fitting classification models with different combinations of predictors and found that the most useful models included data from both sensors. However, the combination of ALS and multispectral data did not significantly improve classification accuracy. We believe that this was due to the simplistic vegetation and terrain structure in the study site that resulted in uncomplicated separability of invasive conifers from other vegetation. This study provides valuable new knowledge of the efficacy of detecting invasive conifers prior to the onset of coning using high-resolution data from UAV and manned aircraft. This will be an important tool in managing the spread of these important invasive plants.
KW  - bio-security
KW  - LiDAR
KW  - invasive plants
KW  - random forest
KW  - logistic regression
KW  - drones
KW  - RPAS
KW  - invasion monitoring
KW  - invasive alien plants
KW  - multispectral
DO  - 10.3390/rs11151812
TY  - EJOU
AU  - Lin, Lishan
AU  - Yang, Yuji
AU  - Cheng, Hui
AU  - Chen, Xuechen
TI  - Autonomous Vision-Based Aerial Grasping for Rotorcraft Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 15
SN  - 1424-8220

AB  - Autonomous vision-based aerial grasping is an essential and challenging task for aerial manipulation missions. In this paper, we propose a vision-based aerial grasping system for a Rotorcraft Unmanned Aerial Vehicle (UAV) to grasp a target object. The UAV system is equipped with a monocular camera, a 3-DOF robotic arm with a gripper and a Jetson TK1 computer. Efficient and reliable visual detectors and control laws are crucial for autonomous aerial grasping using limited onboard sensing and computational capabilities. To detect and track the target object in real time, an efficient proposal algorithm is presented to reliably estimate the region of interest (ROI), then a correlation filter-based classifier is developed to track the detected object. Moreover, a support vector regression (SVR)-based grasping position detector is proposed to improve the grasp success rate with high computational efficiency. Using the estimated grasping position and the UAV?Äôs states, novel control laws of the UAV and the robotic arm are proposed to perform aerial grasping. Extensive simulations and outdoor flight experiments have been implemented. The experimental results illustrate that the proposed vision-based aerial grasping system can autonomously and reliably grasp the target object while working entirely onboard.
KW  - autonomous aerial grasping
KW  - unmanned aerial vehicle
KW  - visual perception
KW  - localization
DO  - 10.3390/s19153410
TY  - EJOU
AU  - Chen, Bo
AU  - Hua, Chunsheng
AU  - Li, Decai
AU  - He, Yuqing
AU  - Han, Jianda
TI  - Intelligent Human–UAV Interaction System with Joint Cross-Validation over Action–Gesture Recognition and Scene Understanding
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 16
SN  - 2076-3417

AB  - We propose an intelligent human&ndash;unmanned aerial vehicle (UAV) interaction system, in which, instead of using the conventional remote controller, the UAV flight actions are controlled by a deep learning-based action&ndash;gesture joint detection system. The Resnet-based scene-understanding algorithm is introduced into the proposed system to enable the UAV to adjust its flight strategy automatically, according to the flying conditions. Meanwhile, both the deep learning-based action detection and multi-feature cascade gesture recognition methods are employed by a cross-validation process to create the corresponding flight action. The effectiveness and efficiency of the proposed system are confirmed by its application to controlling the flight action of a real flying UAV for more than 3 h.
KW  - action detection
KW  - gesture recognition
KW  - scene understanding
KW  - joint cross validation
DO  - 10.3390/app9163277
TY  - EJOU
AU  - Lygouras, Eleftherios
AU  - Santavas, Nicholas
AU  - Taitzoglou, Anastasios
AU  - Tarchanidis, Konstantinos
AU  - Mitropoulos, Athanasios
AU  - Gasteratos, Antonios
TI  - Unsupervised Human Detection with an Embedded Vision System on a Fully Autonomous UAV for Search and Rescue Operations
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 16
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) play a primary role in a plethora of technical and scientific fields owing to their wide range of applications. In particular, the provision of emergency services during the occurrence of a crisis event is a vital application domain where such aerial robots can contribute, sending out valuable assistance to both distressed humans and rescue teams. Bearing in mind that time constraints constitute a crucial parameter in search and rescue (SAR) missions, the punctual and precise detection of humans in peril is of paramount importance. The paper in hand deals with real-time human detection onboard a fully autonomous rescue UAV. Using deep learning techniques, the implemented embedded system was capable of detecting open water swimmers. This allowed the UAV to provide assistance accurately in a fully unsupervised manner, thus enhancing first responder operational capabilities. The novelty of the proposed system is the combination of global navigation satellite system (GNSS) techniques and computer vision algorithms for both precise human detection and rescue apparatus release. Details about hardware configuration as well as the system&rsquo;s performance evaluation are fully discussed.
KW  - unmanned aerial vehicles (UAVs)
KW  - search and rescue (SAR) missions
KW  - human detection
KW  - deep learning
DO  - 10.3390/s19163542
TY  - EJOU
AU  - Yeom, Seokwon
AU  - Cho, In-Jun
TI  - Detection and Tracking of Moving Pedestrians with a Small Unmanned Aerial Vehicle
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 16
SN  - 2076-3417

AB  - Small unmanned aircraft vehicles (SUAVs) or drones are very useful for visual detection and tracking due to their efficiency in capturing scenes. This paper addresses the detection and tracking of moving pedestrians with an SUAV. The detection step consists of frame subtraction, followed by thresholding, morphological filter, and false alarm reduction, taking into consideration the true size of targets. The center of the detected area is input to the next tracking stage. Interacting multiple model (IMM) filtering estimates the state of vectors and covariance matrices, using multiple modes of Kalman filtering. In the experiments, a dozen people and one car are captured by a stationary drone above the road. The Kalman filter and the IMM filter with two or three modes are compared in the accuracy of the state estimation. The root-mean squared errors (RMSE) of position and velocity are obtained for each target and show the good accuracy in detecting and tracking the target position&mdash;the average detection rate is 96.5%. When the two-mode IMM filter is used, the minimum average position and velocity RMSE obtained are around 0.8 m and 0.59 m/s, respectively.
KW  - UAV/drone imaging
KW  - pedestrian detection
KW  - multiple target tracking
KW  - state estimator
DO  - 10.3390/app9163359
TY  - EJOU
AU  - Santos, Anderson A.
AU  - Marcato Junior, José
AU  - Araújo, Márcio S.
AU  - Di Martini, David R.
AU  - Tetila, Everton C.
AU  - Siqueira, Henrique L.
AU  - Aoki, Camila
AU  - Eltner, Anette
AU  - Matsubara, Edson T.
AU  - Pistori, Hemerson
AU  - Feitosa, Raul Q.
AU  - Liesenberg, Veraldo
AU  - Gonçalves, Wesley N.
TI  - Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 16
SN  - 1424-8220

AB  - Detection and classification of tree species from remote sensing data were performed using mainly multispectral and hyperspectral images and Light Detection And Ranging (LiDAR) data. Despite the comparatively lower cost and higher spatial resolution, few studies focused on images captured by Red-Green-Blue (RGB) sensors. Besides, the recent years have witnessed an impressive progress of deep learning methods for object detection. Motivated by this scenario, we proposed and evaluated the usage of Convolutional Neural Network (CNN)-based methods combined with Unmanned Aerial Vehicle (UAV) high spatial resolution RGB imagery for the detection of law protected tree species. Three state-of-the-art object detection methods were evaluated: Faster Region-based Convolutional Neural Network (Faster R-CNN), YOLOv3 and RetinaNet. A dataset was built to assess the selected methods, comprising 392 RBG images captured from August 2018 to February 2019, over a forested urban area in midwest Brazil. The target object is an important tree species threatened by extinction known as Dipteryx alata Vogel (Fabaceae). The experimental analysis delivered average precision around 92% with an associated processing times below 30 miliseconds.
KW  - object-detection
KW  - deep learning
KW  - remote sensing
DO  - 10.3390/s19163595
TY  - EJOU
AU  - Du, Jinyang
AU  - Watts, Jennifer D.
AU  - Jiang, Lingmei
AU  - Lu, Hui
AU  - Cheng, Xiao
AU  - Duguay, Claude
AU  - Farina, Mary
AU  - Qiu, Yubao
AU  - Kim, Youngwook
AU  - Kimball, John S.
AU  - Tarolli, Paolo
TI  - Remote Sensing of Environmental Changes in Cold Regions: Methods, Achievements and Challenges
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 16
SN  - 2072-4292

AB  - Cold regions, including high-latitude and high-altitude landscapes, are experiencing profound environmental changes driven by global warming. With the advance of earth observation technology, remote sensing has become increasingly important for detecting, monitoring, and understanding environmental changes over vast and remote regions. This paper provides an overview of recent achievements, challenges, and opportunities for land remote sensing of cold regions by (a) summarizing the physical principles and methods in remote sensing of selected key variables related to ice, snow, permafrost, water bodies, and vegetation; (b) highlighting recent environmental nonstationarity occurring in the Arctic, Tibetan Plateau, and Antarctica as detected from satellite observations; (c) discussing the limits of available remote sensing data and approaches for regional monitoring; and (d) exploring new opportunities from next-generation satellite missions and emerging methods for accurate, timely, and multi-scale mapping of cold regions.
KW  - remote sensing
KW  - cryosphere
KW  - climate change
KW  - northern high latitudes
KW  - Antarctica
KW  - Tibetan Plateau
DO  - 10.3390/rs11161952
TY  - EJOU
AU  - Joung, Jingon
AU  - Lee, Han L.
AU  - Zhao, Jian
AU  - Kang, Xin
TI  - Power Control Method for Energy Efficient Buffer-Aided Relay Systems
T2  - Energies

PY  - 2019
VL  - 12
IS  - 17
SN  - 1996-1073

AB  - In this paper, a power control method is proposed for a buffer-aided relay node (RN) to enhance the energy efficiency of the RN system. By virtue of a buffer, the RN can reserve the data at the buffer when the the channel gain between an RN and a destination node (DN) is weaker than that between SN and RN. The RN then opportunistically forward the reserved data in the buffer according to channel condition between the RN and the DN. By exploiting the buffer, RN reduces transmit power when it reduces the transmit data rate and reserve the data in the buffer. Therefore, without any total throughput reduction, the power consumption of RN can be reduced, resulting in the energy efficiency (EE) improvement of the RN system. Furthermore, for the power control, we devise a simple power control method based on a two-dimensional surface fitting model of an optimal transmit power of RN. The proposed RN power control method is readily and locally implementable at the RN, and it can significantly improve EE of the RN compared to the fixed power control method and the spectral efficiency based method as verified by the rigorous numerical results.
KW  - UAV
KW  - relay
KW  - cooperative communications
KW  - buffer
KW  - power control
KW  - energy efficiency
DO  - 10.3390/en12173234
TY  - EJOU
AU  - Khoufi, Ines
AU  - Laouiti, Anis
AU  - Adjih, Cedric
TI  - A Survey of Recent Extended Variants of the Traveling Salesman and Vehicle Routing Problems for Unmanned Aerial Vehicles
T2  - Drones

PY  - 2019
VL  - 3
IS  - 3
SN  - 2504-446X

AB  - The use of Unmanned Aerial Vehicles (UAVs) is rapidly growing in popularity. Initially introduced for military purposes, over the past few years, UAVs and related technologies have successfully transitioned to a whole new range of civilian applications such as delivery, logistics, surveillance, entertainment, and so forth. They have opened new possibilities such as allowing operation in otherwise difficult or hazardous areas, for instance. For all applications, one foremost concern is the selection of the paths and trajectories of UAVs, and at the same time, UAVs control comes with many challenges, as they have limited energy, limited load capacity and are vulnerable to difficult weather conditions. Generally, efficiently operating a drone can be mathematically formalized as a path optimization problem under some constraints. This shares some commonalities with similar problems that have been extensively studied in the context of urban vehicles and it is only natural that the recent literature has extended the latter to fit aerial vehicle constraints. The knowledge of such problems, their formulation, the resolution methods proposed—through the variants induced specifically by UAVs features—are of interest for practitioners for any UAV application. Hence, in this study, we propose a review of existing literature devoted to such UAV path optimization problems, focusing specifically on the sub-class of problems that consider the mobility on a macroscopic scale. These are related to the two existing general classic ones—the Traveling Salesman Problem and the Vehicle Routing Problem. We analyze the recent literature that adapted the problems to the UAV context, provide an extensive classification and taxonomy of their problems and their formulation and also give a synthetic overview of the resolution techniques, performance metrics and obtained numerical results.
KW  - UAVs
KW  - optimization problems
KW  - TSP
KW  - VRP
DO  - 10.3390/drones3030066
TY  - EJOU
AU  - Yang, Qinchen
AU  - Liu, Man
AU  - Zhang, Zhitao
AU  - Yang, Shuqin
AU  - Ning, Jifeng
AU  - Han, Wenting
TI  - Mapping Plastic Mulched Farmland for High Resolution Images of Unmanned Aerial Vehicle Using Deep Semantic Segmentation
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 17
SN  - 2072-4292

AB  - With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images.
KW  - plastic mulched farmland
KW  - fully convolutional networks
KW  - unmanned aerial vehicle remote sensing image
KW  - deep semantic segmentation
DO  - 10.3390/rs11172008
TY  - EJOU
AU  - Wei, Lifei
AU  - Yu, Ming
AU  - Liang, Yajing
AU  - Yuan, Ziran
AU  - Huang, Can
AU  - Li, Rong
AU  - Yu, Yiwei
TI  - Precise Crop Classification Using Spectral-Spatial-Location Fusion Based on Conditional Random Fields for UAV-Borne Hyperspectral Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 17
SN  - 2072-4292

AB  - The precise classification of crop types is an important basis of agricultural monitoring and crop protection. With the rapid development of unmanned aerial vehicle (UAV) technology, UAV-borne hyperspectral remote sensing imagery with high spatial resolution has become the ideal data source for the precise classification of crops. For precise classification of crops with a wide variety of classes and varied spectra, the traditional spectral-based classification method has difficulty in mining large-scale spatial information and maintaining the detailed features of the classes. Therefore, a precise crop classification method using spectral-spatial-location fusion based on conditional random fields (SSLF-CRF) for UAV-borne hyperspectral remote sensing imagery is proposed in this paper. The proposed method integrates the spectral information, the spatial context, the spatial features, and the spatial location information in the conditional random field model by the probabilistic potentials, providing complementary information for the crop discrimination from different perspectives. The experimental results obtained with two UAV-borne high spatial resolution hyperspectral images confirm that the proposed method can solve the problems of large-scale spatial information modeling and spectral variability, improving the classification accuracy for each crop type. This method has important significance for the precise classification of crops in hyperspectral remote sensing imagery.
KW  - hyperspectral remote sensing imagery
KW  - conditional random fields
KW  - spatial features
KW  - spatial location
KW  - precise crop classification
KW  - unmanned aerial vehicle
DO  - 10.3390/rs11172011
TY  - EJOU
AU  - Ghorbanzadeh, Omid
AU  - Meena, Sansar R.
AU  - Blaschke, Thomas
AU  - Aryal, Jagannath
TI  - UAV-Based Slope Failure Detection Using Deep-Learning Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 17
SN  - 2072-4292

AB  - Slope failures occur when parts of a slope collapse abruptly under the influence of gravity, often triggered by a rainfall event or earthquake. The resulting slope failures often cause problems in mountainous or hilly regions, and the detection of slope failure is therefore an important topic for research. Most of the methods currently used for mapping and modelling slope failures rely on classification algorithms or feature extraction, but the spatial complexity of slope failures, the uncertainties inherent in expert knowledge, and problems in transferability, all combine to inhibit slope failure detection. In an attempt to overcome some of these problems we have analyzed the potential of deep learning convolutional neural networks (CNNs) for slope failure detection, in an area along a road section in the northern Himalayas, India. We used optical data from unmanned aerial vehicles (UAVs) over two separate study areas. Different CNN designs were used to produce eight different slope failure distribution maps, which were then compared with manually extracted slope failure polygons using different accuracy assessment metrics such as the precision, F-score, and mean intersection-over-union (mIOU). A slope failure inventory data set was produced for each of the study areas using a frequency-area distribution (FAD). The CNN approach that was found to perform best (precision accuracy assessment of almost 90% precision, F-score 85%, mIOU 74%) was one that used a window size of 64 &times; 64 pixels for the sample patches, and included slope data as an additional input layer. The additional information from the slope data helped to discriminate between slope failure areas and roads, which had similar spectral characteristics in the optical imagery. We concluded that the effectiveness of CNNs for slope failure detection was strongly dependent on their design (i.e., the window size selected for the sample patch, the data used, and the training strategies), but that CNNs are currently only designed by trial and error. While CNNs can be powerful tools, such trial and error strategies make it difficult to explain why a particular pooling or layer numbering works better than any other.
KW  - landslide
KW  - unmanned aerial vehicle (UAV)
KW  - deep learning
KW  - frequency area distribution (FAD)
KW  - mean intersection-over-union (mIOU)
KW  - sample patches selection
DO  - 10.3390/rs11172046
TY  - EJOU
AU  - Li, Kexin
AU  - Wang, Jun
AU  - Qi, Dawei
TI  - An Intelligent Warning Method for Diagnosing Underwater Structural Damage
T2  - Algorithms

PY  - 2019
VL  - 12
IS  - 9
SN  - 1999-4893

AB  - A number of intelligent warning techniques have been implemented for detecting underwater infrastructure diagnosis to partially replace human-conducted on-site inspections. However, the extensively varying real-world situation (e.g., the adverse environmental conditions, the limited sample space, and the complex defect types) can lead to challenges to the wide adoption of intelligent warning techniques. To overcome these challenges, this paper proposed an intelligent algorithm combing gray level co-occurrence matrix (GLCM) with self-organization map (SOM) for accurate diagnosis of the underwater structural damage. In order to optimize the generative criterion for GLCM construction, a triangle algorithm was proposed based on orthogonal experiments. The constructed GLCM were utilized to evaluate the texture features of the regions of interest (ROI) of micro-injury images of underwater structures and extracted damage image texture characteristic parameters. The digital feature screening (DFS) method was used to obtain the most relevant features as the input for the SOM network. According to the unique topology information of the SOM network, the classification result, recognition efficiency, parameters, such as the network layer number, hidden layer node, and learning step, were optimized. The robustness and adaptability of the proposed approach were tested on underwater structure images through the DFS method. The results showed that the proposed method revealed quite better performances and can diagnose structure damage in underwater realistic situations.
KW  - structural health monitoring
KW  - digital image processing
KW  - damage
KW  - gray level co-occurrence matrix
KW  - self-organization map
DO  - 10.3390/a12090183
TY  - EJOU
AU  - González-deSantos, L. M.
AU  - Martínez-Sánchez, J.
AU  - González-Jorge, H.
AU  - Ribeiro, M.
AU  - de Sousa, J. B.
AU  - Arias, P.
TI  - Payload for Contact Inspection Tasks with UAV Systems
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 17
SN  - 1424-8220

AB  - This paper presents a payload designed to perform semi-autonomous contact inspection tasks without any type of positioning system external to the UAV, such as a global navigation satellite system (GNSS) or motion capture system, making possible inspection in challenging GNSS- denied sites. This payload includes two LiDAR sensors which measure the distance between the UAV and the target structure and their inner orientation angle. The system uses this information to control the approaching of the UAV to the structure and the contact between both, actuating over the pitch and yaw signals. This control is performed using a hybrid automaton with different states that represent all the possible UAV status during the inspection tasks. It uses different control strategies in each state. An ultrasonic gauge has been used as the inspection sensor of the payload to measure the thickness of a metallic sheet. The sensor requires a stable contact in order to collect reliable measurements. Several tests have been performed on the system, reaching accurate results which show it is able to maintain a stable contact with the target structure.
KW  - autonomous navigation
KW  - contact inspection
KW  - NDT
KW  - UAV
KW  - payload
DO  - 10.3390/s19173752
TY  - EJOU
AU  - Stodola, Petr
AU  - Drozd, Jan
AU  - Mazal, Jan
AU  - Hodický, Jan
AU  - Procházka, Dalibor
TI  - Cooperative Unmanned Aerial System Reconnaissance in a Complex Urban Environment and Uneven Terrain
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 17
SN  - 1424-8220

AB  - Using unmanned robotic systems in military operations such as reconnaissance or surveillance, as well as in many civil applications, is common practice. In this article, the problem of monitoring the specified area of interest by a fleet of unmanned aerial systems is examined. The monitoring is planned via the Cooperative Aerial Model, which deploys a number of waypoints in the area; these waypoints are visited successively by unmanned systems. The original model proposed in the past assumed that the area to be explored is perfectly flat. A new formulation of this model is introduced in this article so that the model can be used in a complex environment with uneven terrain and/or with many obstacles, which may occlude some parts of the area of interest. The optimization algorithm based on the simulated annealing principles is proposed for positioning of waypoints to cover as large an area as possible. A set of scenarios has been designed to verify and evaluate the proposed approach. The key experiments are aimed at finding the minimum number of waypoints needed to explore at least the minimum requested portion of the area. Furthermore, the results are compared to the algorithm based on the lawnmower pattern.
KW  - cooperative aerial reconnaissance
KW  - unmanned aerial systems
KW  - reconnaissance operation
KW  - simulated annealing
KW  - art gallery problem
KW  - waypoint optimization
KW  - occlusion effect
DO  - 10.3390/s19173754
TY  - EJOU
AU  - Moeini Rad, Amir
AU  - Abkar, Ali A.
AU  - Mojaradi, Barat
TI  - Supervised Distance-Based Feature Selection for Hyperspectral Target Detection
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 17
SN  - 2072-4292

AB  - Feature/band selection (FS/BS) for target detection (TD) attempts to select features/bands that increase the discrimination between the target and the image background. Moreover, TD usually suffers from background interference. Therefore, bands that help detectors to effectively suppress the background and magnify the target signal are considered to be more useful. In this regard, three supervised distance-based filter FS methods are proposed in this paper. The first method is based on the TD concept. It uses the image autocorrelation matrix and the target signature in the detection space (DS) for FS. Features that increase the first-norm distance between the target energy and the mean energy of the background in DS are selected as optimal. The other two methods use background modeling via image clustering. The cluster mean spectra, along with the target spectrum, are then transferred into DS. Orthogonal subspace projection distance (OSPD) and first-norm distance (FND) are used as two FS criteria to select optimal features. Two datasets, HyMap RIT and SIM.GA, are used for the experiments. Several measures, i.e., true positives (TPs), false alarms (FAs), target detection accuracy (TDA), total negative score (TNS), and the receiver operating characteristics (ROC) area under the curve (AUC) are employed to evaluate the proposed methods and to investigate the impact of FS on the TD performance. The experimental results show that our proposed FS methods, as compared with five existing FS methods, have improving impacts on common target detectors and help them to yield better results.
KW  - supervised feature selection
KW  - target detection
KW  - background suppression
KW  - hyperspectral data
DO  - 10.3390/rs11172049
TY  - EJOU
AU  - Tilly, Nora
AU  - Bareth, Georg
TI  - Estimating Nitrogen from Structural Crop Traits at Field Scale—A Novel Approach Versus Spectral Vegetation Indices
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 17
SN  - 2072-4292

AB  - A sufficient nitrogen (N) supply is mandatory for healthy crop growth, but negative consequences of N losses into the environment are known. Hence, deeply understanding and monitoring crop growth for an optimized N management is advisable. In this context, remote sensing facilitates the capturing of crop traits. While several studies on estimating biomass from spectral and structural data can be found, N is so far only estimated from spectral features. It is well known that N is negatively related to dry biomass, which, in turn, can be estimated from crop height. Based on this indirect link, the present study aims at estimating N concentration at field scale in a two-step model: first, using crop height to estimate biomass, and second, using the modeled biomass to estimate N concentration. For comparison, N concentration was estimated from spectral data. The data was captured on a spring barley field experiment in two growing seasons. Crop surface height was measured with a terrestrial laser scanner, seven vegetation indices were calculated from field spectrometer measurements, and dry biomass and N concentration were destructively sampled. In the validation, better results were obtained with the models based on structural data (R2 &lt; 0.85) than on spectral data (R2 &lt; 0.70). A brief look at the N concentration of different plant organs showed stronger dependencies on structural data (R2: 0.40&ndash;0.81) than on spectral data (R2: 0.18&ndash;0.68). Overall, this first study shows the potential of crop-specific across‑season two-step models based on structural data for estimating crop N concentration at field scale. The validity of the models for in-season estimations requires further research.
KW  - terrestrial laser scanning
KW  - spectrometer
KW  - plant height
KW  - vegetation indices
KW  - biomass
KW  - nitrogen concentration
KW  - precision agriculture
DO  - 10.3390/rs11172066
TY  - EJOU
AU  - Zhao, Xin
AU  - Yuan, Yitong
AU  - Song, Mengdie
AU  - Ding, Yang
AU  - Lin, Fenfang
AU  - Liang, Dong
AU  - Zhang, Dongyan
TI  - Use of Unmanned Aerial Vehicle Imagery and Deep Learning UNet to Extract Rice Lodging
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 18
SN  - 1424-8220

AB  - Rice lodging severely affects harvest yield. Traditional evaluation methods and manual on-site measurement are found to be time-consuming, labor-intensive, and cost-intensive. In this study, a new method for rice lodging assessment based on a deep learning UNet (U-shaped Network) architecture was proposed. The UAV (unmanned aerial vehicle) equipped with a high-resolution digital camera and a three-band multispectral camera synchronously was used to collect lodged and non-lodged rice images at an altitude of 100 m. After splicing and cropping the original images, the datasets with the lodged and non-lodged rice image samples were established by augmenting for building a UNet model. The research results showed that the dice coefficients in RGB (Red, Green and Blue) image and multispectral image test set were 0.9442 and 0.9284, respectively. The rice lodging recognition effect using the RGB images without feature extraction is better than that of multispectral images. The findings of this study are useful for rice lodging investigations by different optical sensors, which can provide an important method for large-area, high-efficiency, and low-cost rice lodging monitoring research.
KW  - rice lodging
KW  - UAV
KW  - UNet
KW  - semantic segmentation
KW  - assessment
KW  - Oryza sativa L.
DO  - 10.3390/s19183859
TY  - EJOU
AU  - Moon, Jiyoun
AU  - Lee, Beom-Hee
TI  - PDDL Planning with Natural Language-Based Scene Understanding for UAV-UGV Cooperation
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 18
SN  - 2076-3417

AB  - Natural-language-based scene understanding can enable heterogeneous robots to cooperate efficiently in large and unconstructed environments. However, studies on symbolic planning rarely consider the semantic knowledge acquisition problem associated with the surrounding environments. Further, recent developments in deep learning methods show outstanding performance for semantic scene understanding using natural language. In this paper, a cooperation framework that connects deep learning techniques and a symbolic planner for heterogeneous robots is proposed. The framework is largely composed of the scene understanding engine, planning agent, and knowledge engine. We employ neural networks for natural-language-based scene understanding to share environmental information among robots. We then generate a sequence of actions for each robot using a planning domain definition language planner. JENA-TDB is used for knowledge acquisition storage. The proposed method is validated using simulation results obtained from one unmanned aerial and three ground vehicles.
KW  - mission planning
KW  - language descriptions
KW  - semantic graphs
KW  - autonomous robots
KW  - artificial intelligence
DO  - 10.3390/app9183789
TY  - EJOU
AU  - Fan, Shurui
AU  - Li, Zirui
AU  - Xia, Kewen
AU  - Hao, Dongxia
TI  - Quantitative and Qualitative Analysis of Multicomponent Gas Using Sensor Array
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 18
SN  - 1424-8220

AB  - The gas sensor array has long been a major tool for measuring gas due to its high sensitivity, quick response, and low power consumption. This goal, however, faces a difficult challenge because of the cross-sensitivity of the gas sensor. This paper presents a novel gas mixture analysis method for gas sensor array applications. The features extracted from the raw data utilizing principal component analysis (PCA) were used to complete random forest (RF) modeling, which enabled qualitative identification. Support vector regression (SVR), optimized by the particle swarm optimization (PSO) algorithm, was used to select hyperparameters C and &gamma; to establish the optimal regression model for the purpose of quantitative analysis. Utilizing the dataset, we evaluated the effectiveness of our approach. Compared with logistic regression (LR) and support vector machine (SVM), the average recognition rate of PCA combined with RF was the highest (97%). The fitting effect of SVR optimized by PSO for gas concentration was better than that of SVR and solved the problem of hyperparameters selection.
KW  - gas sensor array
KW  - cross-sensitivity
KW  - PCA
KW  - random forest
KW  - particle swarm optimization
DO  - 10.3390/s19183917
TY  - EJOU
AU  - Liu, Xiaolei
AU  - Liu, Liansheng
AU  - Wang, Lulu
AU  - Guo, Qing
AU  - Peng, Xiyuan
TI  - Performance Sensing Data Prediction for an Aircraft Auxiliary Power Unit Using the Optimized Extreme Learning Machine
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 18
SN  - 1424-8220

AB  - The aircraft auxiliary power unit (APU) is responsible for environmental control in the cabin and the main engines starting the aircraft. The prediction of its performance sensing data is significant for condition-based maintenance. As a complex system, its performance sensing data have a typically nonlinear feature. In order to monitor this process, a model with strong nonlinear fitting ability needs to be formulated. A neural network has advantages of solving a nonlinear problem. Compared with the traditional back propagation neural network algorithm, an extreme learning machine (ELM) has features of a faster learning speed and better generalization performance. To enhance the training of the neural network with a back propagation algorithm, an ELM is employed to predict the performance sensing data of the APU in this study. However, the randomly generated weights and thresholds of the ELM often may result in unstable prediction results. To address this problem, a restricted Boltzmann machine (RBM) is utilized to optimize the ELM. In this way, a stable performance parameter prediction model of the APU can be obtained and better performance parameter prediction results can be achieved. The proposed method is evaluated by the real APU sensing data of China Southern Airlines Company Limited Shenyang Maintenance Base. Experimental results show that the optimized ELM with an RBM is more stable and can obtain more accurate prediction results.
KW  - auxiliary power unit
KW  - improved neural network
KW  - stable prediction
KW  - performance sensing data prediction
DO  - 10.3390/s19183935
TY  - EJOU
AU  - Tan, Yumin
AU  - Li, Yunxin
TI  - UAV Photogrammetry-Based 3D Road Distress Detection
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 9
SN  - 2220-9964

AB  - The timely and proper rehabilitation of damaged roads is essential for road maintenance, and an effective method to detect road surface distress with high efficiency and low cost is urgently needed. Meanwhile, unmanned aerial vehicles (UAVs), with the advantages of high flexibility, low cost, and easy maneuverability, are a new fascinating choice for road condition monitoring. In this paper, road images from UAV oblique photogrammetry are used to reconstruct road three-dimensional (3D) models, from which road pavement distress is automatically detected and the corresponding dimensions are extracted using the developed algorithm. Compared with a field survey, the detection result presents a high precision with an error of around 1 cm in the height dimension for most cases, demonstrating the potential of the proposed method for future engineering practice.
KW  - UAV
KW  - road distress detection
KW  - digital surface model
KW  - point cloud
KW  - region growing algorithm
DO  - 10.3390/ijgi8090409
