TY  - EJOU
AU  - Sun, Ying
AU  - Zhang, Xinchang
AU  - Zhao, Xiaoyang
AU  - Xin, Qinchuan
TI  - Extracting Building Boundaries from High Resolution Optical Images and LiDAR Data by Integrating the Convolutional Neural Network and the Active Contour Model
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - Identifying and extracting building boundaries from remote sensing data has been one of the hot topics in photogrammetry for decades. The active contour model (ACM) is a robust segmentation method that has been widely used in building boundary extraction, but which often results in biased building boundary extraction due to tree and background mixtures. Although the classification methods can improve this efficiently by separating buildings from other objects, there are often ineluctable salt and pepper artifacts. In this paper, we combine the robust classification convolutional neural networks (CNN) and ACM to overcome the current limitations in algorithms for building boundary extraction. We conduct two types of experiments: the first integrates ACM into the CNN construction progress, whereas the second starts building footprint detection with a CNN and then uses ACM for post processing. Three level assessments conducted demonstrate that the proposed methods could efficiently extract building boundaries in five test scenes from two datasets. The achieved mean accuracies in terms of the F1 score for the first type (and the second type) of the experiment are 96.43 &plusmn; 3.34% (95.68 &plusmn; 3.22%), 88.60 &plusmn; 3.99% (89.06 &plusmn; 3.96%), and 91.62 &plusmn;1.61% (91.47 &plusmn; 2.58%) at the scene, object, and pixel levels, respectively. The combined CNN and ACM solutions were shown to be effective at extracting building boundaries from high-resolution optical images and LiDAR data.
KW  - building boundary extraction
KW  - convolutional neural network
KW  - active contour model
KW  - high resolution optical images
KW  - LiDAR
DO  - 10.3390/rs10091459
ER  -
TY  - EJOU
AU  - Zhao, Pengfei
AU  - Liu, Kai
AU  - Zou, Hao
AU  - Zhen, Xiantong
TI  - Multi-Stream Convolutional Neural Network for SAR Automatic Target Recognition
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - Despite the fact that automatic target recognition (ATR) in Synthetic aperture radar (SAR) images has been extensively researched due to its practical use in both military and civil applications, it remains an unsolved problem. The major challenges of ATR in SAR stem from severe data scarcity and great variation of SAR images. Recent work started to adopt convolutional neural networks (CNNs), which, however, remain unable to handle the aforementioned challenges due to their high dependency on large quantities of data. In this paper, we propose a novel deep convolutional learning architecture, called Multi-Stream CNN (MS-CNN), for ATR in SAR by leveraging SAR images from multiple views. Specifically, we deploy a multi-input architecture that fuses information from multiple views of the same target in different aspects; therefore, the elaborated multi-view design of MS-CNN enables it to make full use of limited SAR image data to improve recognition performance. We design a Fourier feature fusion framework derived from kernel approximation based on random Fourier features which allows us to unravel the highly nonlinear relationship between images and classes. More importantly, MS-CNN is qualified with the desired characteristic of easy and quick manoeuvrability in real SAR ATR scenarios, because it only needs to acquire real-time GPS information from airborne SAR to calculate aspect differences used for constructing testing samples. The effectiveness and generalization ability of MS-CNN have been demonstrated by extensive experiments under both the Standard Operating Condition (SOC) and Extended Operating Condition (EOC) on the MSTAR dataset. Experimental results have shown that our proposed MS-CNN can achieve high recognition rates and outperform other state-of-the-art ATR methods.
KW  - CNN
KW  - deep learning
KW  - multi-view
KW  - ATR
KW  - SAR
KW  - MSTAR
DO  - 10.3390/rs10091473
ER  -
TY  - EJOU
AU  - Zhang, Weixing
AU  - Witharana, Chandi
AU  - Liljedahl, Anna K.
AU  - Kanevskiy, Mikhail
TI  - Deep Convolutional Neural Networks for Automated Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 9
SN  - 2072-4292

AB  - The microtopography associated with ice-wedge polygons governs many aspects of Arctic ecosystem, permafrost, and hydrologic dynamics from local to regional scales owing to the linkages between microtopography and the flow and storage of water, vegetation succession, and permafrost dynamics. Wide-spread ice-wedge degradation is transforming low-centered polygons into high-centered polygons at an alarming rate. Accurate data on spatial distribution of ice-wedge polygons at a pan-Arctic scale are not yet available, despite the availability of sub-meter-scale remote sensing imagery. This is because the necessary spatial detail quickly produces data volumes that hamper both manual and semi-automated mapping approaches across large geographical extents. Accordingly, transforming big imagery into &lsquo;science-ready&rsquo; insightful analytics demands novel image-to-assessment pipelines that are fueled by advanced machine learning techniques and high-performance computational resources. In this exploratory study, we tasked a deep-learning driven object instance segmentation method (i.e., the Mask R-CNN) with delineating and classifying ice-wedge polygons in very high spatial resolution aerial orthoimagery. We conducted a systematic experiment to gauge the performances and interoperability of the Mask R-CNN across spatial resolutions (0.15 m to 1 m) and image scene contents (a total of 134 km2) near Nuiqsut, Northern Alaska. The trained Mask R-CNN reported mean average precisions of 0.70 and 0.60 at thresholds of 0.50 and 0.75, respectively. Manual validations showed that approximately 95% of individual ice-wedge polygons were correctly delineated and classified, with an overall classification accuracy of 79%. Our findings show that the Mask R-CNN is a robust method to automatically identify ice-wedge polygons from fine-resolution optical imagery. Overall, this automated imagery-enabled intense mapping approach can provide a foundational framework that may propel future pan-Arctic studies of permafrost thaw, tundra landscape evolution, and the role of high latitudes in the global climate system.
KW  - deep learning
KW  - arctic
KW  - ice-wedge polygons
KW  - Mask-RCNN
KW  - VHSR imagery
DO  - 10.3390/rs10091487
ER  -
TY  - EJOU
AU  - Deng, Fei
AU  - Pu, Shengliang
AU  - Chen, Xuehong
AU  - Shi, Yusheng
AU  - Yuan, Ting
AU  - Pu, Shengyan
TI  - Hyperspectral Image Classification with Capsule Network Using Limited Training Samples
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 9
SN  - 1424-8220

AB  - Deep learning techniques have boosted the performance of hyperspectral image (HSI) classification. In particular, convolutional neural networks (CNNs) have shown superior performance to that of the conventional machine learning algorithms. Recently, a novel type of neural networks called capsule networks (CapsNets) was presented to improve the most advanced CNNs. In this paper, we present a modified two-layer CapsNet with limited training samples for HSI classification, which is inspired by the comparability and simplicity of the shallower deep learning models. The presented CapsNet is trained using two real HSI datasets, i.e., the PaviaU (PU) and SalinasA datasets, representing complex and simple datasets, respectively, and which are used to investigate the robustness or representation of every model or classifier. In addition, a comparable paradigm of network architecture design has been proposed for the comparison of CNN and CapsNet. Experiments demonstrate that CapsNet shows better accuracy and convergence behavior for the complex data than the state-of-the-art CNN. For CapsNet using the PU dataset, the Kappa coefficient, overall accuracy, and average accuracy are 0.9456, 95.90%, and 96.27%, respectively, compared to the corresponding values yielded by CNN of 0.9345, 95.11%, and 95.63%. Moreover, we observed that CapsNet has much higher confidence for the predicted probabilities. Subsequently, this finding was analyzed and discussed with probability maps and uncertainty analysis. In terms of the existing literature, CapsNet provides promising results and explicit merits in comparison with CNN and two baseline classifiers, i.e., random forests (RFs) and support vector machines (SVMs).
KW  - capsule network
KW  - hyperspectral
KW  - image classification
KW  - deep learning
KW  - possibility density
DO  - 10.3390/s18093153
ER  -
TY  - EJOU
AU  - Duarte-Carvajalino, Julio M.
AU  - Alzate, Diego F.
AU  - Ramirez, Andrés A.
AU  - Santa-Sepulveda, Juan D.
AU  - Fajardo-Rojas, Alexandra E.
AU  - Soto-Suárez, Mauricio
TI  - Evaluating Late Blight Severity in Potato Crops Using Unmanned Aerial Vehicles and Machine Learning Algorithms
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - This work presents quantitative prediction of severity of the disease caused by Phytophthora infestans in potato crops using machine learning algorithms such as multilayer perceptron, deep learning convolutional neural networks, support vector regression, and random forests. The machine learning algorithms are trained using datasets extracted from multispectral data captured at the canopy level with an unmanned aerial vehicle, carrying an inexpensive digital camera. The results indicate that deep learning convolutional neural networks, random forests and multilayer perceptron using band differences can predict the level of Phytophthora infestans affectation on potato crops with acceptable accuracy.
KW  - UAV
KW  - remote sensing
KW  - Phytophthora infestans
KW  - multispectral
KW  - neural networks
KW  - deep learning
DO  - 10.3390/rs10101513
ER  -
TY  - EJOU
AU  - Leonita, Gina
AU  - Kuffer, Monika
AU  - Sliuzas, Richard
AU  - Persello, Claudio
TI  - Machine Learning-Based Slum Mapping in Support of Slum Upgrading Programs: The Case of Bandung City, Indonesia
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - The survey-based slum mapping (SBSM) program conducted by the Indonesian government to reach the national target of &ldquo;cities without slums&rdquo; by 2019 shows mapping inconsistencies due to several reasons, e.g., the dependency on the surveyor&rsquo;s experiences and the complexity of the slum indicators set. By relying on such inconsistent maps, it will be difficult to monitor the national slum upgrading program&rsquo;s progress. Remote sensing imagery combined with machine learning algorithms could support the reduction of these inconsistencies. This study evaluates the performance of two machine learning algorithms, i.e., support vector machine (SVM) and random forest (RF), for slum mapping in support of the slum mapping campaign in Bandung, Indonesia. Recognizing the complexity in differentiating slum and formal areas in Indonesia, the study used a combination of spectral, contextual, and morphological features. In addition, sequential feature selection (SFS) combined with the Hilbert&ndash;Schmidt independence criterion (HSIC) was used to select significant features for classifying slums. Overall, the highest accuracy (88.5%) was achieved by the SVM with SFS using contextual, morphological, and spectral features, which is higher than the estimated accuracy of the SBSM. To evaluate the potential of machine learning-based slum mapping (MLBSM) in support of slum upgrading programs, interviews were conducted with several local and national stakeholders. Results show that local acceptance for a remote sensing-based slum mapping approach varies among stakeholder groups. Therefore, a locally adapted framework is required to combine ground surveys with robust and consistent machine learning methods, for being able to deal with big data, and to allow the rapid extraction of consistent information on the dynamics of slums at a large scale.
KW  - machine learning
KW  - slums
KW  - slum upgrading programs
KW  - Bandung
KW  - Indonesia
DO  - 10.3390/rs10101522
ER  -
TY  - EJOU
AU  - Ma, Lingfei
AU  - Li, Ying
AU  - Li, Jonathan
AU  - Wang, Cheng
AU  - Wang, Ruisheng
AU  - Chapman, Michael A.
TI  - Mobile Laser Scanned Point-Clouds for Road Object Detection and Extraction: A Review
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - The mobile laser scanning (MLS) technique has attracted considerable attention for providing high-density, high-accuracy, unstructured, three-dimensional (3D) geo-referenced point-cloud coverage of the road environment. Recently, there has been an increasing number of applications of MLS in the detection and extraction of urban objects. This paper presents a systematic review of existing MLS related literature. This paper consists of three parts. Part 1 presents a brief overview of the state-of-the-art commercial MLS systems. Part 2 provides a detailed analysis of on-road and off-road information inventory methods, including the detection and extraction of on-road objects (e.g., road surface, road markings, driving lines, and road crack) and off-road objects (e.g., pole-like objects and power lines). Part 3 presents a refined integrated analysis of challenges and future trends. Our review shows that MLS technology is well proven in urban object detection and extraction, since the improvement of hardware and software accelerate the efficiency and accuracy of data collection and processing. When compared to other review papers focusing on MLS applications, we review the state-of-the-art road object detection and extraction methods using MLS data and discuss their performance and applicability. The main contribution of this review demonstrates that the MLS systems are suitable for supporting road asset inventory, ITS-related applications, high-definition maps, and other highly accurate localization services.
KW  - mobile laser scanning (MLS)
KW  - point cloud
KW  - road surface
KW  - road marking
KW  - driving line
KW  - road crack
KW  - traffic sign
KW  - street light
KW  - tree
KW  - power line
KW  - deep learning
DO  - 10.3390/rs10101531
ER  -
TY  - EJOU
AU  - Liu, Yan
AU  - Ren, Qirui
AU  - Geng, Jiahui
AU  - Ding, Meng
AU  - Li, Jiangyun
TI  - Efficient Patch-Wise Semantic Segmentation for Large-Scale Remote Sensing Images
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - Efficient and accurate semantic segmentation is the key technique for automatic remote sensing image analysis. While there have been many segmentation methods based on traditional hand-craft feature extractors, it is still challenging to process high-resolution and large-scale remote sensing images. In this work, a novel patch-wise semantic segmentation method with a new training strategy based on fully convolutional networks is presented to segment common land resources. First, to handle the high-resolution image, the images are split as local patches and then a patch-wise network is built. Second, training data is preprocessed in several ways to meet the specific characteristics of remote sensing images, i.e., color imbalance, object rotation variations and lens distortion. Third, a multi-scale training strategy is developed to solve the severe scale variation problem. In addition, the impact of conditional random field (CRF) is studied to improve the precision. The proposed method was evaluated on a dataset collected from a capital city in West China with the Gaofen-2 satellite. The dataset contains ten common land resources (Grassland, Road, etc.). The experimental results show that the proposed algorithm achieves 54.96% in terms of mean intersection over union (MIoU) and outperforms other state-of-the-art methods in remote sensing image segmentation.
KW  - remote sensing
KW  - image segmentation
KW  - fully convolutional network
KW  - patch-wise
KW  - multi-scale
DO  - 10.3390/s18103232
ER  -
TY  - EJOU
AU  - Luo, Lei
AU  - Wang, Xinyuan
AU  - Guo, Huadong
AU  - Lasaponara, Rosa
AU  - Shi, Pilong
AU  - Bachagha, Nabil
AU  - Li, Li
AU  - Yao, Ya
AU  - Masini, Nicola
AU  - Chen, Fulong
AU  - Ji, Wei
AU  - Cao, Hui
AU  - Li, Chao
AU  - Hu, Ningke
TI  - Google Earth as a Powerful Tool for Archaeological and Cultural Heritage Applications: A Review
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - Google Earth (GE), a large Earth-observation data-based geographical information computer application, is an intuitive three-dimensional virtual globe. It enables archaeologists around the world to communicate and share their multisource data and research findings. Different from traditional geographical information systems (GIS), GE is free and easy to use in data collection, exploration, and visualization. In the past decade, many peer-reviewed articles on the use of GE in the archaeological cultural heritage (ACH) research field have been published. Most of these concern specific ACH investigations with a wide spatial coverage. GE can often be used to survey and document ACH so that both skilled archaeologists and the public can more easily and intuitively understand the results. Based on geographical tools and multi-temporal very high-resolution (VHR) satellite imagery, GE has been shown to provide spatio-temporal change information that has a bearing on the physical, environmental, and geographical character of ACH. In this review, in order to discuss the huge potential of GE, a comprehensive review of GE and its applications to ACH in the published scientific literature is first presented; case studies in five main research fields demonstrating how GE can be deployed as a key tool for studying ACH are then described. The selected case studies illustrate how GE can be used effectively to investigate ACH at multiple scales, discover new archaeological sites in remote regions, monitor historical sites, and assess damage in areas of conflict, and promote virtual tourism. These examples form the basis for highlighting current trends in remote sensing archaeology based on the GE platform, which could provide access to a low-cost and easy-to-use tool for communicating and sharing ACH geospatial data more effectively to the general public in the era of Digital Earth. Finally, a discussion of the merits and limitations of GE is presented along with conclusions and remaining challenges.
KW  - Google Earth (GE)
KW  - archaeological
KW  - cultural heritage
KW  - remote sensing
KW  - Keyhole Markup Language
KW  - very high-resolution (VHR)
KW  - virtual
DO  - 10.3390/rs10101558
ER  -
TY  - EJOU
AU  - Huang, Huasheng
AU  - Deng, Jizhong
AU  - Lan, Yubin
AU  - Yang, Aqing
AU  - Deng, Xiaoling
AU  - Wen, Sheng
AU  - Zhang, Huihui
AU  - Zhang, Yali
TI  - Accurate Weed Mapping and Prescription Map Generation Based on Fully Convolutional Networks Using UAV Imagery
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - Chemical control is necessary in order to control weed infestation and to ensure a rice yield. However, excessive use of herbicides has caused serious agronomic and environmental problems. Site specific weed management (SSWM) recommends an appropriate dose of herbicides according to the weed coverage, which may reduce the use of herbicides while enhancing their chemical effects. In the context of SSWM, the weed cover map and prescription map must be generated in order to carry out the accurate spraying. In this paper, high resolution unmanned aerial vehicle (UAV) imagery were captured over a rice field. Different workflows were evaluated to generate the weed cover map for the whole field. Fully convolutional networks (FCN) was applied for a pixel-level classification. Theoretical analysis and practical evaluation were carried out to seek for an architecture improvement and performance boost. A chessboard segmentation process was used to build the grid framework of the prescription map. The experimental results showed that the overall accuracy and mean intersection over union (mean IU) for weed mapping using FCN-4s were 0.9196 and 0.8473, and the total time (including the data collection and data processing) required to generate the weed cover map for the entire field (50 &times; 60 m) was less than half an hour. Different weed thresholds (0.00&ndash;0.25, with an interval of 0.05) were used for the prescription map generation. High accuracies (above 0.94) were observed for all of the threshold values, and the relevant herbicide saving ranged from 58.3% to 70.8%. All of the experimental results demonstrated that the method used in this work has the potential to produce an accurate weed cover map and prescription map in SSWM applications.
KW  - UAV
KW  - semantic labeling
KW  - FCN
KW  - weed mapping
KW  - prescription map
DO  - 10.3390/s18103299
ER  -
TY  - EJOU
AU  - Tayara, Hilal
AU  - Chong, Kil T.
TI  - Object Detection in Very High-Resolution Aerial Images Using One-Stage Densely Connected Feature Pyramid Network
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - Object detection in very high-resolution (VHR) aerial images is an essential step for a wide range of applications such as military applications, urban planning, and environmental management. Still, it is a challenging task due to the different scales and appearances of the objects. On the other hand, object detection task in VHR aerial images has improved remarkably in recent years due to the achieved advances in convolution neural networks (CNN). Most of the proposed methods depend on a two-stage approach, namely: a region proposal stage and a classification stage such as Faster R-CNN. Even though two-stage approaches outperform the traditional methods, their optimization is not easy and they are not suitable for real-time applications. In this paper, a uniform one-stage model for object detection in VHR aerial images has been proposed. In order to tackle the challenge of different scales, a densely connected feature pyramid network has been proposed by which high-level multi-scale semantic feature maps with high-quality information are prepared for object detection. This work has been evaluated on two publicly available datasets and outperformed the current state-of-the-art results on both in terms of mean average precision (mAP) and computation time.
KW  - Aerial images
KW  - convolution neural network (CNN)
KW  - deep learning
KW  - feature pyramid network
KW  - focal loss
KW  - object detection
DO  - 10.3390/s18103341
ER  -
TY  - EJOU
AU  - Opromolla, Roberto
AU  - Fasano, Giancarmine
AU  - Accardo, Domenico
TI  - A Vision-Based Approach to UAV Detection and Tracking in Cooperative Applications
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - This paper presents a visual-based approach that allows an Unmanned Aerial Vehicle (UAV) to detect and track a cooperative flying vehicle autonomously using a monocular camera. The algorithms are based on template matching and morphological filtering, thus being able to operate within a wide range of relative distances (i.e., from a few meters up to several tens of meters), while ensuring robustness against variations of illumination conditions, target scale and background. Furthermore, the image processing chain takes full advantage of navigation hints (i.e., relative positioning and own-ship attitude estimates) to improve the computational efficiency and optimize the trade-off between correct detections, false alarms and missed detections. Clearly, the required exchange of information is enabled by the cooperative nature of the formation through a reliable inter-vehicle data-link. Performance assessment is carried out by exploiting flight data collected during an ad hoc experimental campaign. The proposed approach is a key building block of cooperative architectures designed to improve UAV navigation performance either under nominal GNSS coverage or in GNSS-challenging environments.
KW  - unmanned aerial vehicles
KW  - visual detection
KW  - visual tracking
KW  - template matching
KW  - morphological filtering
KW  - cooperative UAV applications
KW  - autonomous navigation
DO  - 10.3390/s18103391
ER  -
TY  - EJOU
AU  - Adeyemi, Olutobi
AU  - Grove, Ivan
AU  - Peets, Sven
AU  - Domun, Yuvraj
AU  - Norton, Tomas
TI  - Dynamic Neural Network Modelling of Soil Moisture Content for Predictive Irrigation Scheduling
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - Sustainable freshwater management is underpinned by technologies which improve the efficiency of agricultural irrigation systems. Irrigation scheduling has the potential to incorporate real-time feedback from soil moisture and climatic sensors. However, for robust closed-loop decision support, models of the soil moisture dynamics are essential in order to predict crop water needs while adapting to external perturbation and disturbances. This paper presents a Dynamic Neural Network approach for modelling of the temporal soil moisture fluxes. The models are trained to generate a one-day-ahead prediction of the volumetric soil moisture content based on past soil moisture, precipitation, and climatic measurements. Using field data from three sites, a      R 2      value above 0.94 was obtained during model evaluation in all sites. The models were also able to generate robust soil moisture predictions for independent sites which were not used in training the models. The application of the Dynamic Neural Network models in a predictive irrigation scheduling system was demonstrated using AQUACROP simulations of the potato-growing season. The predictive irrigation scheduling system was evaluated against a rule-based system that applies irrigation based on predefined thresholds. Results indicate that the predictive system achieves a water saving ranging between 20 and 46% while realizing a yield and water use efficiency similar to that of the rule-based system.
KW  - irrigation scheduling
KW  - modeling
KW  - dynamic neural networks
KW  - soil moisture dynamics
KW  - sensors
DO  - 10.3390/s18103408
ER  -
TY  - EJOU
AU  - Chen, Hongyi
AU  - Zhang, Fan
AU  - Tang, Bo
AU  - Yin, Qiang
AU  - Sun, Xian
TI  - Slim and Efficient Neural Network Design for Resource-Constrained SAR Target Recognition
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - Deep convolutional neural networks (CNN) have been recently applied to synthetic aperture radar (SAR) for automatic target recognition (ATR) and have achieved state-of-the-art results with significantly improved recognition performance. However, the training period of deep CNN is long, and the size of the network is huge, sometimes reaching hundreds of megabytes. These two factors of deep CNN hinders its practical implementation and deployment in real-time SAR platforms that are typically resource-constrained. To address this challenge, this paper presents three strategies of network compression and acceleration to decrease computing and memory resource dependencies while maintaining a competitive accuracy. First, we introduce a new weight-based network pruning and adaptive architecture squeezing method to reduce the network storage and the time of inference and training process, meanwhile maintain a balance between compression ratio and classification accuracy. Then we employ weight quantization and coding to compress the network storage space. Due to the fact that the amount of calculation is mainly reflected in the convolution layer, a fast approach for pruned convolutional layers is proposed to reduce the number of multiplication by exploiting the sparsity in the activation inputs and weights. Experimental results show that the convolutional neural networks for SAR-ATR can be compressed by     40 &times;     without loss of accuracy, and the number of multiplication can be reduced by     15 &times;    . Combining these strategies, we can easily load the network in resource-constrained platforms, speed up the inference process to get the results in real-time or even retrain a more suitable network with new image data in a specific situation.
KW  - deep learning
KW  - synthetic aperture radar (SAR)
KW  - automatic target recognition (ATR)
KW  - model compression
KW  - fast algorithm
DO  - 10.3390/rs10101618
ER  -
TY  - EJOU
AU  - Feng, Yi
AU  - Zhang, Cong
AU  - Baek, Stanley
AU  - Rawashdeh, Samir
AU  - Mohammadi, Alireza
TI  - Autonomous Landing of a UAV on a Moving Platform Using Model Predictive Control
T2  - Drones

PY  - 2018
VL  - 2
IS  - 4
SN  - 2504-446X

AB  - Developing methods for autonomous landing of an unmanned aerial vehicle (UAV) on a mobile platform has been an active area of research over the past decade, as it offers an attractive solution for cases where rapid deployment and recovery of a fleet of UAVs, continuous flight tasks, extended operational ranges, and mobile recharging stations are desired. In this work, we present a new autonomous landing method that can be implemented on micro UAVs that require high-bandwidth feedback control loops for safe landing under various uncertainties and wind disturbances. We present our system architecture, including dynamic modeling of the UAV with a gimbaled camera, implementation of a Kalman filter for optimal localization of the mobile platform, and development of model predictive control (MPC), for guidance of UAVs. We demonstrate autonomous landing with an error of less than 37 cm from the center of a mobile platform traveling at a speed of up to 12 m/s under the condition of noisy measurements and wind disturbances.
KW  - quadcopter
KW  - drone
KW  - Kalman filter
KW  - vision-based guidance system
KW  - autonomous vehicle
KW  - unmanned aerial vehicle
KW  - model predictive control
KW  - aerospace control
DO  - 10.3390/drones2040034
ER  -
TY  - EJOU
AU  - Xu, Yifan
AU  - Ren, Guochun
AU  - Chen, Jin
AU  - Zhang, Xiaobo
AU  - Jia, Luliang
AU  - Kong, Lijun
TI  - Interference-Aware Cooperative Anti-Jamming Distributed Channel Selection in UAV Communication Networks
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 10
SN  - 2076-3417

AB  - This paper investigates the cooperative anti-jamming distributed channel selection problem in UAV communication networks. Considering the existence of malicious jamming and co-channel interference, we design an interference-aware cooperative anti-jamming scheme for the purpose of maximizing users&rsquo; utilities. Moreover, the channel switching cost and cooperation cost are introduced, which have a great impact on users&rsquo; utilities. Users in the UAV group sense the co-channel interference signal energy to judge whether they are influenced by co-channel interference. When the received co-channel interference signal energy is lower than the co-channel interference threshold, users conduct channel selection strategies independently. Otherwise, users cooperate with each other and take joint actions with a cooperative anti-jamming pattern under the impact of co-channel interference. Aiming at the independent anti-jamming channel selection problem under no co-channel interference, a Markov decision process framework is introduced, whereas for the cooperative anti-jamming channel selection case under the influence of co-channel mutual interference, a Markov game framework is employed. Furthermore, motivated by Q-learning with a &ldquo;cooperation-decision-feedback-adjustment&rdquo; idea, we design an interference-aware cooperative anti-jamming distributed channel selection algorithm (ICADCSA) to obtain the optimal anti-jamming channel strategies for users in a distributed way. In addition, a discussion on the quick decision for UAVs is conducted. Finally, simulation results show that the proposed algorithm converges to a stable solution with which the UAV group can avoid malicious jamming, as well as co-channel interference effectively and can realize a quick decision in high mobility UAV communication networks.
KW  - interference-aware
KW  - cooperative anti-jamming
KW  - Markov decision process
KW  - Markov game
KW  - Q-learning
DO  - 10.3390/app8101911
ER  -
TY  - EJOU
AU  - Kim, Byunghyun
AU  - Cho, Soojin
TI  - Automated Vision-Based Detection of Cracks on Concrete Surfaces Using a Deep Learning Technique
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 10
SN  - 1424-8220

AB  - At present, a number of computer vision-based crack detection techniques have been developed to efficiently inspect and manage a large number of structures. However, these techniques have not replaced visual inspection, as they have been developed under near-ideal conditions and not in an on-site environment. This article proposes an automated detection technique for crack morphology on concrete surface under an on-site environment based on convolutional neural networks (CNNs). A well-known CNN, AlexNet is trained for crack detection with images scraped from the Internet. The training set is divided into five classes involving cracks, intact surfaces, two types of similar patterns of cracks, and plants. A comparative study evaluates the successfulness of the detailed surface categorization. A probability map is developed using a softmax layer value to add robustness to sliding window detection and a parametric study was carried out to determine its threshold. The applicability of the proposed method is evaluated on images taken from the field and real-time video frames taken using an unmanned aerial vehicle. The evaluation results confirm the high adoptability of the proposed method for crack inspection in an on-site environment.
KW  - crack
KW  - deep learning
KW  - convolutional neural networks
KW  - AlexNet
KW  - unmanned aerial vehicle
DO  - 10.3390/s18103452
ER  -
TY  - EJOU
AU  - Duarte, Diogo
AU  - Nex, Francesco
AU  - Kerle, Norman
AU  - Vosselman, George
TI  - Multi-Resolution Feature Fusion for Image Classification of Building Damages with Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - Remote sensing images have long been preferred to perform building damage assessments. The recently proposed methods to extract damaged regions from remote sensing imagery rely on convolutional neural networks (CNN). The common approach is to train a CNN independently considering each of the different resolution levels (satellite, aerial, and terrestrial) in a binary classification approach. In this regard, an ever-growing amount of multi-resolution imagery are being collected, but the current approaches use one single resolution as their input. The use of up/down-sampled images for training has been reported as beneficial for the image classification accuracy both in the computer vision and remote sensing domains. However, it is still unclear if such multi-resolution information can also be captured from images with different spatial resolutions such as imagery of the satellite and airborne (from both manned and unmanned platforms) resolutions. In this paper, three multi-resolution CNN feature fusion approaches are proposed and tested against two baseline (mono-resolution) methods to perform the image classification of building damages. Overall, the results show better accuracy and localization capabilities when fusing multi-resolution feature maps, specifically when these feature maps are merged and consider feature information from the intermediate layers of each of the resolution level networks. Nonetheless, these multi-resolution feature fusion approaches behaved differently considering each level of resolution. In the satellite and aerial (unmanned) cases, the improvements in the accuracy reached 2% while the accuracy improvements for the airborne (manned) case was marginal. The results were further confirmed by testing the approach for geographical transferability, in which the improvements between the baseline and multi-resolution experiments were overall maintained.
KW  - earthquake
KW  - deep learning
KW  - UAV
KW  - satellite
KW  - aerial
KW  - dilated convolutions
KW  - residual connections
DO  - 10.3390/rs10101636
ER  -
TY  - EJOU
AU  - Ruan, Lang
AU  - Chen, Jin
AU  - Guo, Qiuju
AU  - Zhang, Xiaobo
AU  - Zhang, Yuli
AU  - Liu, Dianxiong
TI  - Group Buying-Based Data Transmission in Flying Ad-Hoc Networks: A Coalition Game Approach
T2  - Information

PY  - 2018
VL  - 9
IS  - 10
SN  - 2078-2489

AB  - In scenarios such as natural disasters and military strikes, it is common for unmanned aerial vehicles (UAVs) to form groups to execute reconnaissance and surveillance. To ensure the effectiveness of UAV communications, repeated resource acquisition issues and transmission mechanism designs need to be addressed urgently. Since large-scale UAVs will generate high transmission overhead due to the overlapping resource requirements, in this paper, we propose a resource allocation optimization method based on distributed data content in a Flying Ad-hoc network (FANET). The resource allocation problem with the goal of throughput maximization is constructed as a coalition game framework. Firstly, a data transmission mechanism is designed for UAVs to execute information interaction within the coalitions. Secondly, a novel mechanism of coalition selection based on group-buying is investigated for UAV coalitions to acquire data from the central UAV. The data transmission and coalition selection problem are modeled as coalition graph game and coalition formation game, respectively. Through the design of the utility function, we prove that both games have stable solutions. We also prove the convergence of the proposed approach with coalition order and Pareto order. Based on simulation results, coalition order based coalition selection algorithm (CO-CSA) and Pareto order based coalition selection algorithm (PO-CSA) are proposed to explore the stable coalition partition of system model. CO-CSA and PO-CSA can achieve higher data throughput than the contrast onetime coalition selection algorithm (Onetime-CSA) (at least increased by 34.5% and 16.9%, respectively). Besides, although PO-CSA has relatively lower throughput gain, its convergence times is on average 50.9% less than that of CO-CSA, which means that the algorithm choice is scenario-dependent.
KW  - coalition formation game
KW  - coalition graph game
KW  - data transmission
KW  - Nash equilibrium
KW  - resource allocation
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/info9100253
ER  -
TY  - EJOU
AU  - Zhan, Tianming
AU  - Sun, Le
AU  - Xu, Yang
AU  - Yang, Guowei
AU  - Zhang, Yan
AU  - Wu, Zebin
TI  - Hyperspectral Classification via Superpixel Kernel Learning-Based Low Rank Representation
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - High dimensional image classification is a fundamental technique for information retrieval from hyperspectral remote sensing data. However, data quality is readily affected by the atmosphere and noise in the imaging process, which makes it difficult to achieve good classification performance. In this paper, multiple kernel learning-based low rank representation at superpixel level (Sp_MKL_LRR) is proposed to improve the classification accuracy for hyperspectral images. Superpixels are generated first from the hyperspectral image to reduce noise effect and form homogeneous regions. An optimal superpixel kernel parameter is then selected by the kernel matrix using a multiple kernel learning framework. Finally, a kernel low rank representation is applied to classify the hyperspectral image. The proposed method offers two advantages. (1) The global correlation constraint is exploited by the low rank representation, while the local neighborhood information is extracted as the superpixel kernel adaptively learns the high-dimensional manifold features of the samples in each class; (2) It can meet the challenges of multiscale feature learning and adaptive parameter determination in the conventional kernel methods. Experimental results on several hyperspectral image datasets demonstrate that the proposed method outperforms several state-of-the-art classifiers tested in terms of overall accuracy, average accuracy, and kappa statistic.
KW  - hyperspectral image
KW  - classification
KW  - superpixel kernel
KW  - multiple kernel learning
KW  - low rank representation
DO  - 10.3390/rs10101639
ER  -
TY  - EJOU
AU  - Wang, Zifeng
AU  - Liu, Junguo
AU  - Li, Jinbao
AU  - Zhang, David D.
TI  - Multi-Spectral Water Index (MuWI): A Native 10-m Multi-Spectral Water Index for Accurate Water Mapping on Sentinel-2
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - Accurate water mapping depends largely on the water index. However, most previously widely-adopted water index methods are developed from 30-m resolution Landsat imagery, with low-albedo commission error (e.g., shadow misclassified as water) and threshold instability being identified as the primary issues. Besides, since the shortwave-infrared (SWIR) spectral band (band 11) on Sentinel-2 is 20 m spatial resolution, current SWIR-included water index methods usually produce water maps at 20 m resolution instead of the highest 10 m resolution of Sentinel-2 bands, which limits the ability of Sentinel-2 to detect surface water at finer scales. This study aims to develop a water index from Sentinel-2 that improves native resolution and accuracy of water mapping at the same time. Support Vector Machine (SVM) is used to exploit the 10-m spectral bands among Sentinel-2 bands of three resolutions (10-m; 20-m; 60-m). The new Multi-Spectral Water Index (MuWI), consisting of the complete version and the revised version (MuWI-C and MuWI-R), is designed as the combination of normalized differences for threshold stability. The proposed method is assessed on coincident Sentinel-2 and sub-meter images covering a variety of water types. When compared to previous water indexes, results show that both versions of MuWI enable to produce native 10-m resolution water maps with higher classification accuracies (p-value &lt; 0.01). Commission and omission errors are also significantly reduced particularly in terms of shadow and sunglint. Consistent accuracy over complex water mapping scenarios is obtained by MuWI due to high threshold stability. Overall, the proposed MuWI method is applicable to accurate water mapping with improved spatial resolution and accuracy, which possibly facilitates water mapping and its related studies and applications on growing Sentinel-2 images.
KW  - MuWI
KW  - Sentinel-2
KW  - multi-spectral water index
KW  - water index
KW  - NDWI
KW  - MNDWI
KW  - AWEI
KW  - machine learning
KW  - SVM
KW  - OSH
KW  - water mapping
KW  - water classification
KW  - Landsat
KW  - sunglint
KW  - shadow
DO  - 10.3390/rs10101643
ER  -
TY  - EJOU
AU  - Mozgeris, Gintautas
AU  - Juodkienė, Vytautė
AU  - Jonikavičius, Donatas
AU  - Straigytė, Lina
AU  - Gadal, Sébastien
AU  - Ouerghemmi, Walid
TI  - Ultra-Light Aircraft-Based Hyperspectral and Colour-Infrared Imaging to Identify Deciduous Tree Species in an Urban Environment
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 10
SN  - 2072-4292

AB  - One may consider the application of remote sensing as a trade-off between the imaging platforms, sensors, and data gathering and processing techniques. This study addresses the potential of hyperspectral imaging using ultra-light aircraft for vegetation species mapping in an urban environment, exploring both the engineering and scientific aspects related to imaging platform design and image classification methods. An imaging system based on simultaneous use of Rikola frame format hyperspectral and Nikon D800E adopted colour infrared cameras installed onboard a Bekas X32 manned ultra-light aircraft is introduced. Two test imaging flight missions were conducted in July of 2015 and September of 2016 over a 4000 ha area in Kaunas City, Lithuania. Sixteen and 64 spectral bands in 2015 and 2016, respectively, in a spectral range of 500&ndash;900 nm were recorded with colour infrared images. Three research questions were explored assessing the identification of six deciduous tree species: (1) Pre-treatment of spectral features for classification, (2) testing five conventional machine learning classifiers, and (3) fusion of hyperspectral and colour infrared images. Classification performance was assessed by applying leave-one-out cross-validation at the individual crown level and using as a reference at least 100 field inventoried trees for each species. The best-performing classification algorithm&mdash;multilayer perceptron, using all spectral properties extracted from the hyperspectral images&mdash;resulted in a moderate classification accuracy. The overall classification accuracy was 63%, Cohen&rsquo;s Kappa was 0.54, and the species-specific classification accuracies were in the range of 51&ndash;72%. Hyperspectral images resulted in significantly better tree species classification ability than the colour infrared images and simultaneous use of spectral properties extracted from hyperspectral and colour infrared images improved slightly the accuracy over the 2015 image. Even though classifications using hyperspectral data cubes of 64 bands resulted in relatively larger accuracies than with 16 bands, classification error matrices were not statistically different. Alternative imaging platforms (like an unmanned aerial vehicle and a Cessna 172 aircraft) and settings of the flights were discussed using simulated imaging projects assuming the same study area and field of application. Ultra-light aircraft-based hyperspectral and colour-infrared imaging was considered to be a technically and economically sound solution for urban green space inventories to facilitate tree mapping, characterization, and monitoring.
KW  - hyperspectral
KW  - colour infrared
KW  - ultra-light aircraft
KW  - urban trees
KW  - classification
DO  - 10.3390/rs10101668
ER  -
TY  - EJOU
AU  - Tu, Yu-Hsuan
AU  - Phinn, Stuart
AU  - Johansen, Kasper
AU  - Robson, Andrew
TI  - Assessing Radiometric Correction Approaches for Multi-Spectral UAS Imagery for Horticultural Applications
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Multi-spectral imagery captured from unmanned aerial systems (UAS) is becoming increasingly popular for the improved monitoring and managing of various horticultural crops. However, for UAS-based data to be used as an industry standard for assessing tree structure and condition as well as production parameters, it is imperative that the appropriate data collection and pre-processing protocols are established to enable multi-temporal comparison. There are several UAS-based radiometric correction methods commonly used for precision agricultural purposes. However, their relative accuracies have not been assessed for data acquired in complex horticultural environments. This study assessed the variations in estimated surface reflectance values of different radiometric corrections applied to multi-spectral UAS imagery acquired in both avocado and banana orchards. We found that inaccurate calibration panel measurements, inaccurate signal-to-reflectance conversion, and high variation in geometry between illumination, surface, and sensor viewing produced significant radiometric variations in at-surface reflectance estimates. Potential solutions to address these limitations included appropriate panel deployment, site-specific sensor calibration, and appropriate bidirectional reflectance distribution function (BRDF) correction. Future UAS-based horticultural crop monitoring can benefit from the proposed solutions to radiometric corrections to ensure they are using comparable image-based maps of multi-temporal biophysical properties.
KW  - unmanned aerial system
KW  - multi-spectral imagery
KW  - radiometric correction
KW  - bidirectional reflectance distribution function
KW  - horticulture
DO  - 10.3390/rs10111684
ER  -
TY  - EJOU
AU  - Bah, M D.
AU  - Hafiane, Adel
AU  - Canals, Raphael
TI  - Deep Learning with Unsupervised Data Labeling for Weed Detection in Line Crops in UAV Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - In recent years, weeds have been responsible for most agricultural yield losses. To deal with this threat, farmers resort to spraying the fields uniformly with herbicides. This method not only requires huge quantities of herbicides but impacts the environment and human health. One way to reduce the cost and environmental impact is to allocate the right doses of herbicide to the right place and at the right time (precision agriculture). Nowadays, unmanned aerial vehicles (UAVs) are becoming an interesting acquisition system for weed localization and management due to their ability to obtain images of the entire agricultural field with a very high spatial resolution and at a low cost. However, despite significant advances in UAV acquisition systems, the automatic detection of weeds remains a challenging problem because of their strong similarity to the crops. Recently, a deep learning approach has shown impressive results in different complex classification problems. However, this approach needs a certain amount of training data, and creating large agricultural datasets with pixel-level annotations by an expert is an extremely time-consuming task. In this paper, we propose a novel fully automatic learning method using convolutional neuronal networks (CNNs) with an unsupervised training dataset collection for weed detection from UAV images. The proposed method comprises three main phases. First, we automatically detect the crop rows and use them to identify the inter-row weeds. In the second phase, inter-row weeds are used to constitute the training dataset. Finally, we perform CNNs on this dataset to build a model able to detect the crop and the weeds in the images. The results obtained are comparable to those of traditional supervised training data labeling, with differences in accuracy of 1.5% in the spinach field and 6% in the bean field.
KW  - weed detection
KW  - deep learning
KW  - unmanned aerial vehicle
KW  - image processing
KW  - precision agriculture
KW  - crop line detection
DO  - 10.3390/rs10111690
ER  -
TY  - EJOU
AU  - Wu, Keyu
AU  - Abolfazli Esfahani, Mahdi
AU  - Yuan, Shenghai
AU  - Wang, Han
TI  - Learn to Steer through Deep Reinforcement Learning
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - It is crucial for robots to autonomously steer in complex environments safely without colliding with any obstacles. Compared to conventional methods, deep reinforcement learning-based methods are able to learn from past experiences automatically and enhance the generalization capability to cope with unseen circumstances. Therefore, we propose an end-to-end deep reinforcement learning algorithm in this paper to improve the performance of autonomous steering in complex environments. By embedding a branching noisy dueling architecture, the proposed model is capable of deriving steering commands directly from raw depth images with high efficiency. Specifically, our learning-based approach extracts the feature representation from depth inputs through convolutional neural networks and maps it to both linear and angular velocity commands simultaneously through different streams of the network. Moreover, the training framework is also meticulously designed to improve the learning efficiency and effectiveness. It is worth noting that the developed system is readily transferable from virtual training scenarios to real-world deployment without any fine-tuning by utilizing depth images. The proposed method is evaluated and compared with a series of baseline methods in various virtual environments. Experimental results demonstrate the superiority of the proposed model in terms of average reward, learning efficiency, success rate as well as computational time. Moreover, a variety of real-world experiments are also conducted which reveal the high adaptability of our model to both static and dynamic obstacle-cluttered environments.
KW  - deep reinforcement learning
KW  - autonomous steering
KW  - depth image
DO  - 10.3390/s18113650
ER  -
TY  - EJOU
AU  - Kuffer, Monika
AU  - Wang, Jiong
AU  - Nagenborg, Michael
AU  - Pfeffer, Karin
AU  - Kohli, Divyani
AU  - Sliuzas, Richard
AU  - Persello, Claudio
TI  - The Scope of Earth-Observation to Improve the Consistency of the SDG Slum Indicator
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 11
SN  - 2220-9964

AB  - The continuous increase in deprived living conditions in many cities of the Global South contradicts efforts to make cities inclusive, safe, resilient, and sustainable places. Using examples of Asian, African, and Latin American cities, this study shows the scope and limits of earth observation (EO)-based mapping of deprived living conditions in support of providing consistent global information for the SDG indicator 11.1.1 “proportion of urban population living in slums, informal settlements or inadequate housing”. At the technical level, we compare several EO-based methods and imagery for mapping deprived living conditions, discussing their ability to map such areas including differences in terms of accuracy and performance at the city scale. At the operational level, we compare available municipal maps showing identified deprived areas with the spatial extent of morphological mapped areas of deprived living conditions (using EO) at the city scale, discussing the reasons for inconsistencies between municipal and EO-based maps. We provide an outlook on how EO-based mapping of deprived living conditions could contribute to a global spatial information base to support targeting of deprived living conditions in support of the SDG Goal 11.1.1 indicator, when uncertainties and ethical considerations on data provision are well addressed.
KW  - deprived living conditions
KW  - slum
KW  - informal settlement
KW  - inadequate housing
KW  - Sustainable Development Goals (SDGs)
KW  - remote sensing
KW  - global urban data
KW  - uncertainties
KW  - geo-ethics
DO  - 10.3390/ijgi7110428
ER  -
TY  - EJOU
AU  - Valentino, Rico
AU  - Jung, Woo-Sung
AU  - Ko, Young-Bae
TI  - A Design and Simulation of the Opportunistic Computation Offloading with Learning-Based Prediction for Unmanned Aerial Vehicle (UAV) Clustering Networks
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Drones have recently become extremely popular, especially in military and civilian applications. Examples of drone utilization include reconnaissance, surveillance, and packet delivery. As time has passed, drones’ tasks have become larger and more complex. As a result, swarms or clusters of drones are preferred, because they offer more coverage, flexibility, and reliability. However, drone systems have limited computing power and energy resources, which means that sometimes it is difficult for drones to finish their tasks on schedule. A solution to this is required so that drone clusters can complete their work faster. One possible solution is an offloading scheme between drone clusters. In this study, we propose an opportunistic computational offloading system, which allows for a drone cluster with a high intensity task to borrow computing resources opportunistically from other nearby drone clusters. We design an artificial neural network-based response time prediction module for deciding whether it is faster to finish tasks by offloading them to other drone clusters. The offloading scheme is conducted only if the predicted offloading response time is smaller than the local computing time. Through simulation results, we show that our proposed scheme can decrease the response time of drone clusters through an opportunistic offloading process.
KW  - drone cluster
KW  - computation offloading
KW  - neural network
KW  - wireless communication
DO  - 10.3390/s18113751
ER  -
TY  - EJOU
AU  - Rehush, Nataliia
AU  - Abegg, Meinrad
AU  - Waser, Lars T.
AU  - Brändli, Urs-Beat
TI  - Identifying Tree-Related Microhabitats in TLS Point Clouds Using Machine Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Tree-related microhabitats (TreMs) play an important role in maintaining forest biodiversity and have recently received more attention in ecosystem conservation, forest management and research. However, TreMs have until now only been assessed by experts during field surveys, which are time-consuming and difficult to reproduce. In this study, we evaluate the potential of close-range terrestrial laser scanning (TLS) for semi-automated identification of different TreMs (bark, bark pockets, cavities, fungi, ivy and mosses) in dense TLS point clouds using machine learning algorithms, including deep learning. To classify the TreMs, we applied: (1) the Random Forest (RF) classifier, incorporating frequently used local geometric features and two additional self-developed orientation features, and (2) a deep Convolutional Neural Network (CNN) trained using rasterized multiview orthographic projections (MVOPs) containing top view, front view and side view of the point&rsquo;s local 3D neighborhood. The results confirmed that using local geometric features is beneficial for identifying the six groups of TreMs in dense tree-stem point clouds, but the rasterized MVOPs are even more suitable. Whereas the overall accuracy of the RF was 70%, that of the deep CNN was substantially higher (83%). This study reveals that close-range TLS is promising for the semi-automated identification of TreMs for forest monitoring purposes, in particular when applying deep learning techniques.
KW  - tree-related microhabitats
KW  - terrestrial laser scanning
KW  - forest monitoring
KW  - Convolutional Neural Network
KW  - Random Forest
DO  - 10.3390/rs10111735
ER  -
TY  - EJOU
AU  - Dominguez-Sanchez, Alex
AU  - Cazorla, Miguel
AU  - Orts-Escolano, Sergio
TI  - A New Dataset and Performance Evaluation of a Region-Based CNN for Urban Object Detection
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 11
SN  - 2079-9292

AB  - In recent years, we have seen a large growth in the number of applications which use deep learning-based object detectors. Autonomous driving assistance systems (ADAS) are one of the areas where they have the most impact. This work presents a novel study evaluating a state-of-the-art technique for urban object detection and localization. In particular, we investigated the performance of the Faster R-CNN method to detect and localize urban objects in a variety of outdoor urban videos involving pedestrians, cars, bicycles and other objects moving in the scene (urban driving). We propose a new dataset that is used for benchmarking the accuracy of a real-time object detector (Faster R-CNN). Part of the data was collected using an HD camera mounted on a vehicle. Furthermore, some of the data is weakly annotated so it can be used for testing weakly supervised learning techniques. There already exist urban object datasets, but none of them include all the essential urban objects. We carried out extensive experiments demonstrating the effectiveness of the baseline approach. Additionally, we propose an R-CNN plus tracking technique to accelerate the process of real-time urban object detection.
KW  - real-time object detection
KW  - autonomous driving assistance system
KW  - urban object detector
KW  - convolutional neural networks
DO  - 10.3390/electronics7110301
ER  -
TY  - EJOU
AU  - Cui, Jun-hui
AU  - Wei, Rui-xuan
AU  - Liu, Zong-cheng
AU  - Zhou, Kai
TI  - UAV Motion Strategies in Uncertain Dynamic Environments: A Path Planning Method Based on Q-Learning Strategy
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 11
SN  - 2076-3417

AB  - A solution framework for UAV motion strategies in uncertain dynamic environments is constructed in this paper. Considering that the motion states of UAV might be influenced by some dynamic uncertainties, such as control strategies, flight environments, and any other bursting-out threats, we model the uncertain factors that might cause such influences to the path planning of the UAV, unified as an unobservable part of the system and take the acceleration together with the bank angle of the UAV as a control variable. Meanwhile, the cost function is chosen based on the tracking error, then the control instructions and flight path for UAV can be achieved. Then, the cost function can be optimized through Q-learning, and the best UAV action sequence for conflict avoidance under the moving threat environment can be obtained. According to Bellman&rsquo;s optimization principle, the optimal action strategies can be obtained from the current confidence level. The method in this paper is more in line with the actual UAV path planning, since the generation of the path planning strategy at each moment takes into account the influence of the UAV control strategy on its motion at the next moment. The simulation results show that all the planning paths that are created according to the solution framework proposed in this paper have a very high tracking accuracy, and this method has a much shorter processing time as well as a shorter path it can create.
KW  - unmanned aerial vehicle
KW  - path planning
KW  - Q-Learning strategy
KW  - observational error
DO  - 10.3390/app8112169
ER  -
TY  - EJOU
AU  - Zhu, Qing
AU  - Wang, Feng
AU  - Hu, Han
AU  - Ding, Yulin
AU  - Xie, Jiali
AU  - Wang, Weixi
AU  - Zhong, Ruofei
TI  - Intact Planar Abstraction of Buildings via Global Normal Refinement from Noisy Oblique Photogrammetric Point Clouds
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 11
SN  - 2220-9964

AB  - Oblique photogrammetric point clouds are currently one of the major data sources for the three-dimensional level-of-detail reconstruction of buildings. However, they are severely noise-laden and pose serious problems for the effective and automatic surface extraction of buildings. In addition, conventional methods generally use normal vectors estimated in a local neighborhood, which are liable to be affected by noise, leading to inferior results in successive building reconstruction. In this paper, we propose an intact planar abstraction method for buildings, which explicitly handles noise by integrating information in a larger context through global optimization. The information propagates hierarchically from a local to global scale through the following steps: first, based on voxel cloud connectivity segmentation, single points are clustered into supervoxels that are enforced to not cross the surface boundary; second, each supervoxel is expanded to nearby supervoxels through the maximal support region, which strictly enforces planarity; third, the relationships established by the maximal support regions are injected into a global optimization, which reorients the local normal vectors to be more consistent in a larger context; finally, the intact planar surfaces are obtained by region growing using robust normal and point connectivity in the established spatial relations. Experiments on the photogrammetric point clouds obtained from oblique images showed that the proposed method is effective in reducing the influence of noise and retrieving almost all of the major planar structures of the examined buildings.
KW  - photogrammetric point cloud
KW  - normal estimation
KW  - region growing
KW  - global optimization
DO  - 10.3390/ijgi7110431
ER  -
TY  - EJOU
AU  - Ghaffarian, Saman
AU  - Kerle, Norman
AU  - Filatova, Tatiana
TI  - Remote Sensing-Based Proxies for Urban Disaster Risk Management and Resilience: A Review
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Rapid increase in population and growing concentration of capital in urban areas has escalated both the severity and longer-term impact of natural disasters. As a result, Disaster Risk Management (DRM) and reduction have been gaining increasing importance for urban areas. Remote sensing plays a key role in providing information for urban DRM analysis due to its agile data acquisition, synoptic perspective, growing range of data types, and instrument sophistication, as well as low cost. As a consequence numerous methods have been developed to extract information for various phases of DRM analysis. However, given the diverse information needs, only few of the parameters of interest are extracted directly, while the majority have to be elicited indirectly using proxies. This paper provides a comprehensive review of the proxies developed for two risk elements typically associated with pre-disaster situations (vulnerability and resilience), and two post-disaster elements (damage and recovery), while focusing on urban DRM. The proxies were reviewed in the context of four main environments and their corresponding sub-categories: built-up (buildings, transport, and others), economic (macro, regional and urban economics, and logistics), social (services and infrastructures, and socio-economic status), and natural. All environments and the corresponding proxies are discussed and analyzed in terms of their reliability and sufficiency in comprehensively addressing the selected DRM assessments. We highlight strength and identify gaps and limitations in current proxies, including inconsistencies in terminology for indirect measurements. We present a systematic overview for each group of the reviewed proxies that could simplify cross-fertilization across different DRM domains and may assist the further development of methods. While systemizing examples from the wider remote sensing domain and insights from social and economic sciences, we suggest a direction for developing new proxies, also potentially suitable for capturing functional recovery.
KW  - urban DRM
KW  - remote sensing
KW  - damage
KW  - recovery
KW  - vulnerability
KW  - resilience
KW  - economic
KW  - social
KW  - proxy
KW  - indirect measurement
DO  - 10.3390/rs10111760
ER  -
TY  - EJOU
AU  - Zhao, Yingyi
AU  - Hu, Qingwu
AU  - Li, Haidong
AU  - Wang, Shaohua
AU  - Ai, Mingyao
TI  - Evaluating Carbon Sequestration and PM2.5 Removal of Urban Street Trees Using Mobile Laser Scanning Data
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Street trees are an important part of urban facilities, and they can provide both aesthetic benefits and ecological benefits for urban environments. Ecological benefits of street trees now are attracting more attention because of environmental deterioration in cities. Conventional methods of evaluating ecological benefits require a lot of labor and time, and establishing an efficient and effective evaluating method is challenging. In this study, we investigated the feasibility to use mobile laser scanning (MLS) data to evaluate carbon sequestration and fine particulate matter (PM2.5) removal of street trees. We explored the approach to extract individual street trees from MLS data, and street trees of three streets in Nantong City were extracted. The correctness rates and completeness rates of extraction results were both over 92%. Morphological parameters, including tree height, crown width, and diameter at breast height (DBH), were measured for extracted street trees, and parameters derived from MLS data were in a good agreement with field-measured parameters. Necessary information about street trees, including tree height, DBH, and tree species, meteorological data and PM2.5 deposition velocities were imported into i-Tree Eco model to estimate carbon sequestration and PM2.5 removal. The estimation results indicated that ecological benefits generated by different tree species were considerably varied and the differences for trees of the same species were mainly caused by the differences in morphological parameters (tree height and DBH). This study succeeds in estimating the amount of carbon sequestration and PM2.5 removal of individual street trees with MLS data, and provides researchers with a novel and efficient way to investigate ecological benefits of urban street trees or urban forests.
KW  - ecological benefits
KW  - i-Tree Eco model
KW  - LiDAR
KW  - MLS
KW  - morphological parameters
KW  - street trees extraction
DO  - 10.3390/rs10111759
ER  -
TY  - EJOU
AU  - Zhu, Yadi
AU  - Chen, Feng
AU  - Li, Ming
AU  - Wang, Zijia
TI  - Inferring the Economic Attributes of Urban Rail Transit Passengers Based on Individual Mobility Using Multisource Data
T2  - Sustainability

PY  - 2018
VL  - 10
IS  - 11
SN  - 2071-1050

AB  - Socioeconomic attributes are essential characteristics of people, and many studies on economic attribute inference focus on data that contain user profile information. For data without user profiles, like smart card data, there is no validated method for inferring individual economic attributes. This study aims to bridge this gap by formulating a mobility to attribute framework to infer passengers&rsquo; economic attributes based on the relationship between individual mobility and personal attributes. This framework integrates shop consumer prices, house prices, and smart card data using three steps: individual mobility extraction, location feature identification, and economic attribute inference. Each passenger&rsquo;s individual mobility is extracted by smart card data. Economic features of stations are described using house price and shop consumer price data. Then, each passenger&rsquo;s comprehensive consumption indicator set is formulated by integrating these data. Finally, individual economic levels are classified. From the case study of Beijing, commuting distance and trip frequency using the metro have a negative correlation with passengers&rsquo; income and the results confirm that metro passengers are mainly in the low- and middle-income groups. This study improves on passenger information extracted from data without user profile information and provides a method to integrate multisource big data mining for more information.
KW  - transportation planning
KW  - individual economic attributes
KW  - individual mobility
KW  - smart card data (SCD)
KW  - multisource data
DO  - 10.3390/su10114178
ER  -
TY  - EJOU
AU  - Wang, Xiaohong
AU  - Guo, Hongzhou
AU  - Wang, Jingbin
AU  - Wang, Lizhi
TI  - Predicting the Health Status of an Unmanned Aerial Vehicles Data-Link System Based on a Bayesian Network
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) require data-link system to link ground data terminals to the real-time controls of each UAV. Consequently, the ability to predict the health status of a UAV data-link system is vital for safe and efficient operations. The performance of a UAV data-link system is affected by the health status of both the hardware and UAV data-links. This paper proposes a method for predicting the health state of a UAV data-link system based on a Bayesian network fusion of information about potential hardware device failures and link failures. Our model employs the Bayesian network to describe the information and uncertainty associated with a complex multi-level system. To predict the health status of the UAV data-link, we use the health status information about the root node equipment with various life characteristics along with the health status of the links as affected by the bit error rate. In order to test the validity of the model, we tested its prediction of the health of a multi-level solar-powered unmanned aerial vehicle data-link system and the result shows that the method can quantitatively predict the health status of the solar-powered UAV data-link system. The results can provide guidance for improving the reliability of UAV data-link system and lay a foundation for predicting the health status of a UAV data-link system accurately.
KW  - UAV data-link system
KW  - Bayesian networks
KW  - health status prediction
KW  - networking mode
KW  - bit error rate
DO  - 10.3390/s18113916
ER  -
TY  - EJOU
AU  - Boonpook, Wuttichai
AU  - Tan, Yumin
AU  - Ye, Yinghua
AU  - Torteeka, Peerapong
AU  - Torsri, Kritanai
AU  - Dong, Shengxian
TI  - A Deep Learning Approach on Building Detection from Unmanned Aerial Vehicle-Based Images in Riverbank Monitoring
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Buildings along riverbanks are likely to be affected by rising water levels, therefore the acquisition of accurate building information has great importance not only for riverbank environmental protection but also for dealing with emergency cases like flooding. UAV-based photographs are flexible and cloud-free compared to satellite images and can provide very high-resolution images up to centimeter level, while there exist great challenges in quickly and accurately detecting and extracting building from UAV images because there are usually too many details and distortions on UAV images. In this paper, a deep learning (DL)-based approach is proposed for more accurately extracting building information, in which the network architecture, SegNet, is used in the semantic segmentation after the network training on a completely labeled UAV image dataset covering multi-dimension urban settlement appearances along a riverbank area in Chongqing. The experiment results show that an excellent performance has been obtained in the detection of buildings from untrained locations with an average overall accuracy more than 90%. To verify the generality and advantage of the proposed method, the procedure is further evaluated by training and testing with another two open standard datasets which have a variety of building patterns and styles, and the final overall accuracies of building extraction are more than 93% and 95%, respectively.
KW  - building extraction
KW  - UAV dataset
KW  - deep learning
KW  - river bank monitoring
DO  - 10.3390/s18113921
ER  -
TY  - EJOU
AU  - Zhang, Yihong
AU  - Yang, Yijin
AU  - Zhou, Wuneng
AU  - Shi, Lifeng
AU  - Li, Demin
TI  - Motion-Aware Correlation Filters for Online Visual Tracking
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - The discriminative correlation filters-based methods struggle deal with the problem of fast motion and heavy occlusion, the problem can severely degrade the performance of trackers, ultimately leading to tracking failures. In this paper, a novel Motion-Aware Correlation Filters (MACF) framework is proposed for online visual object tracking, where a motion-aware strategy based on joint instantaneous motion estimation Kalman filters is integrated into the Discriminative Correlation Filters (DCFs). The proposed motion-aware strategy is used to predict the possible region and scale of the target in the current frame by utilizing the previous estimated 3D motion information. Obviously, this strategy can prevent model drift caused by fast motion. On the base of the predicted region and scale, the MACF detects the position and scale of the target by using the DCFs-based method in the current frame. Furthermore, an adaptive model updating strategy is proposed to address the problem of corrupted models caused by occlusions, where the learning rate is determined by the confidence of the response map. The extensive experiments on popular Object Tracking Benchmark OTB-100, OTB-50 and unmanned aerial vehicles (UAV) video have demonstrated that the proposed MACF tracker performs better than most of the state-of-the-art trackers and achieves a high real-time performance. In addition, the proposed approach can be integrated easily and flexibly into other visual tracking algorithms.
KW  - visual tracking
KW  - correlation filters
KW  - motion-aware
KW  - adaptive update strategy
KW  - confidence response map
DO  - 10.3390/s18113937
ER  -
TY  - EJOU
AU  - Sun, Jian
AU  - Huang, Guanhua
AU  - Sun, Gang
AU  - Yu, Hongfang
AU  - Sangaiah, Arun K.
AU  - Chang, Victor
TI  - A Q-Learning-Based Approach for Deploying Dynamic Service Function Chains
T2  - Symmetry

PY  - 2018
VL  - 10
IS  - 11
SN  - 2073-8994

AB  - As the size and service requirements of today&rsquo;s networks gradually increase, large numbers of proprietary devices are deployed, which leads to network complexity, information security crises and makes network service and service provider management increasingly difficult. Network function virtualization (NFV) technology is one solution to this problem. NFV separates network functions from hardware and deploys them as software on a common server. NFV can be used to improve service flexibility and isolate the services provided for each user, thus guaranteeing the security of user data. Therefore, the use of NFV technology includes many problems worth studying. For example, when there is a free choice of network path, one problem is how to choose a service function chain (SFC) that both meets the requirements and offers the service provider maximum profit. Most existing solutions are heuristic algorithms with high time efficiency, or integer linear programming (ILP) algorithms with high accuracy. It&rsquo;s necessary to design an algorithm that symmetrically considers both time efficiency and accuracy. In this paper, we propose the Q-learning Framework Hybrid Module algorithm (QLFHM), which includes reinforcement learning to solve this SFC deployment problem in dynamic networks. The reinforcement learning module in QLFHM is responsible for the output of alternative paths, while the load balancing module in QLFHM is responsible for picking the optimal solution from them. The results of a comparison simulation experiment on a dynamic network topology show that the proposed algorithm can output the approximate optimal solution in a relatively short time while also considering the network load balance. Thus, it achieves the goal of maximizing the benefit to the service provider.
KW  - network function virtualization
KW  - service function chain
KW  - reinforcement learning
KW  - load balancing
KW  - security
DO  - 10.3390/sym10110646
ER  -
TY  - EJOU
AU  - Lagkas, Thomas
AU  - Argyriou, Vasileios
AU  - Bibi, Stamatia
AU  - Sarigiannidis, Panagiotis
TI  - UAV IoT Framework Views and Challenges: Towards Protecting Drones as “Things”
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) have enormous potential in enabling new applications in various areas, ranging from military, security, medicine, and surveillance to traffic-monitoring applications. Lately, there has been heavy investment in the development of UAVs and multi-UAVs systems that can collaborate and complete missions more efficiently and economically. Emerging technologies such as 4G/5G networks have significant potential on UAVs equipped with cameras, sensors, and GPS receivers in delivering Internet of Things (IoT) services from great heights, creating an airborne domain of the IoT. However, there are many issues to be resolved before the effective use of UAVs can be made, including security, privacy, and management. As such, in this paper we review new UAV application areas enabled by the IoT and 5G technologies, analyze the sensor requirements, and overview solutions for fleet management over aerial-networking, privacy, and security challenges. Finally, we propose a framework that supports and enables these technologies on UAVs. The introduced framework provisions a holistic IoT architecture that enables the protection of UAVs as “flying” things in a collaborative networked environment.
KW  - security
KW  - privacy
KW  - drones
KW  - IoT
KW  - UAV
DO  - 10.3390/s18114015
ER  -
TY  - EJOU
AU  - Song, Ahram
AU  - Choi, Jaewan
AU  - Han, Youkyung
AU  - Kim, Yongil
TI  - Change Detection in Hyperspectral Images Using Recurrent 3D Fully Convolutional Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Hyperspectral change detection (CD) can be effectively performed using deep-learning networks. Although these approaches require qualified training samples, it is difficult to obtain ground-truth data in the real world. Preserving spatial information during training is difficult due to structural limitations. To solve such problems, our study proposed a novel CD method for hyperspectral images (HSIs), including sample generation and a deep-learning network, called the recurrent three-dimensional (3D) fully convolutional network (Re3FCN), which merged the advantages of a 3D fully convolutional network (FCN) and a convolutional long short-term memory (ConvLSTM). Principal component analysis (PCA) and the spectral correlation angle (SCA) were used to generate training samples with high probabilities of being changed or unchanged. The strategy assisted in training fewer samples of representative feature expression. The Re3FCN was mainly comprised of spectral&ndash;spatial and temporal modules. Particularly, a spectral&ndash;spatial module with a 3D convolutional layer extracts the spectral&ndash;spatial features from the HSIs simultaneously, whilst a temporal module with ConvLSTM records and analyzes the multi-temporal HSI change information. The study first proposed a simple and effective method to generate samples for network training. This method can be applied effectively to cases with no training samples. Re3FCN can perform end-to-end detection for binary and multiple changes. Moreover, Re3FCN can receive multi-temporal HSIs directly as input without learning the characteristics of multiple changes. Finally, the network could extract joint spectral&ndash;spatial&ndash;temporal features and it preserved the spatial structure during the learning process through the fully convolutional structure. This study was the first to use a 3D FCN and a ConvLSTM for the remote-sensing CD. To demonstrate the effectiveness of the proposed CD method, we performed binary and multi-class CD experiments. Results revealed that the Re3FCN outperformed the other conventional methods, such as change vector analysis, iteratively reweighted multivariate alteration detection, PCA-SCA, FCN, and the combination of 2D convolutional layers-fully connected LSTM.
KW  - change detection
KW  - fully convolutional network
KW  - 3D convolution
KW  - convolutional LSTM
KW  - hyperspectral image
DO  - 10.3390/rs10111827
ER  -
TY  - EJOU
AU  - Kim, Yunbin
AU  - Sa, Jaewon
AU  - Chung, Yongwha
AU  - Park, Daihee
AU  - Lee, Sungju
TI  - Resource-Efficient Pet Dog Sound Events Classification Using LSTM-FCN Based on Time-Series Data
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - The use of IoT (Internet of Things) technology for the management of pet dogs left alone at home is increasing. This includes tasks such as automatic feeding, operation of play equipment, and location detection. Classification of the vocalizations of pet dogs using information from a sound sensor is an important method to analyze the behavior or emotions of dogs that are left alone. These sounds should be acquired by attaching the IoT sound sensor to the dog, and then classifying the sound events (e.g., barking, growling, howling, and whining). However, sound sensors tend to transmit large amounts of data and consume considerable amounts of power, which presents issues in the case of resource-constrained IoT sensor devices. In this paper, we propose a way to classify pet dog sound events and improve resource efficiency without significant degradation of accuracy. To achieve this, we only acquire the intensity data of sounds by using a relatively resource-efficient noise sensor. This presents issues as well, since it is difficult to achieve sufficient classification accuracy using only intensity data due to the loss of information from the sound events. To address this problem and avoid significant degradation of classification accuracy, we apply long short-term memory-fully convolutional network (LSTM-FCN), which is a deep learning method, to analyze time-series data, and exploit bicubic interpolation. Based on experimental results, the proposed method based on noise sensors (i.e., Shapelet and LSTM-FCN for time-series) was found to improve energy efficiency by 10 times without significant degradation of accuracy compared to typical methods based on sound sensors (i.e., mel-frequency cepstrum coefficient (MFCC), spectrogram, and mel-spectrum for feature extraction, and support vector machine (SVM) and k-nearest neighbor (K-NN) for classification).
KW  - pet dogs
KW  - separation anxiety
KW  - IoT sensor
KW  - sound events processing
KW  - resource efficiency
KW  - LSTM-FCN
DO  - 10.3390/s18114019
ER  -
TY  - EJOU
AU  - Yang, Tao
AU  - Ren, Qiang
AU  - Zhang, Fangbing
AU  - Xie, Bolin
AU  - Ren, Hailei
AU  - Li, Jing
AU  - Zhang, Yanning
TI  - Hybrid Camera Array-Based UAV Auto-Landing on Moving UGV in GPS-Denied Environment
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - With the rapid development of Unmanned Aerial Vehicle (UAV) systems, the autonomous landing of a UAV on a moving Unmanned Ground Vehicle (UGV) has received extensive attention as a key technology. At present, this technology is confronted with such problems as operating in GPS-denied environments, a low accuracy of target location, the poor precision of the relative motion estimation, delayed control responses, slow processing speeds, and poor stability. To address these issues, we present a hybrid camera array-based autonomous landing UAV that can land on a moving UGV in a GPS-denied environment. We first built a UAV autonomous landing system with a hybrid camera array comprising a fisheye lens camera and a stereo camera. Then, we integrated a wide Field of View (FOV) and depth imaging for locating the UGV accurately. In addition, we employed a state estimation algorithm based on motion compensation for establishing the motion state of the ground moving UGV, including its actual motion direction and speed. Thereafter, according to the characteristics of the designed system, we derived a nonlinear controller based on the UGV motion state to ensure that the UGV and UAV maintain the same motion state, which allows autonomous landing. Finally, to evaluate the performance of the proposed system, we carried out a large number of simulations in AirSim and conducted real-world experiments. Through the qualitative and quantitative analyses of the experimental results, as well as the analysis of the time performance, we verified that the autonomous landing performance of the system in the GPS-denied environment is effective and robust.
KW  - UAV autonomous landing
KW  - moving UGV
KW  - GPS-denied environment
KW  - hybrid camera array
KW  - motion compensation
DO  - 10.3390/rs10111829
ER  -
TY  - EJOU
AU  - Csillik, Ovidiu
AU  - Cherbini, John
AU  - Johnson, Robert
AU  - Lyons, Andy
AU  - Kelly, Maggi
TI  - Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks
T2  - Drones

PY  - 2018
VL  - 2
IS  - 4
SN  - 2504-446X

AB  - Remote sensing is important to precision agriculture and the spatial resolution provided by Unmanned Aerial Vehicles (UAVs) is revolutionizing precision agriculture workflows for measurement crop condition and yields over the growing season, for identifying and monitoring weeds and other applications. Monitoring of individual trees for growth, fruit production and pest and disease occurrence remains a high research priority and the delineation of each tree using automated means as an alternative to manual delineation would be useful for long-term farm management. In this paper, we detected citrus and other crop trees from UAV images using a simple convolutional neural network (CNN) algorithm, followed by a classification refinement using superpixels derived from a Simple Linear Iterative Clustering (SLIC) algorithm. The workflow performed well in a relatively complex agricultural environment (multiple targets, multiple size trees and ages, etc.) achieving high accuracy (overall accuracy = 96.24%, Precision (positive predictive value) = 94.59%, Recall (sensitivity) = 97.94%). To our knowledge, this is the first time a CNN has been used with UAV multi-spectral imagery to focus on citrus trees. More of these individual cases are needed to develop standard automated workflows to help agricultural managers better incorporate large volumes of high resolution UAV imagery into agricultural management operations.
KW  - CNN
KW  - deep learning
KW  - superpixels
KW  - precision agriculture
KW  - UAS
KW  - feature extraction
KW  - citrus
KW  - tree identification
DO  - 10.3390/drones2040039
ER  -
TY  - EJOU
AU  - Rahman, Muhammad M.
AU  - Robson, Andrew
AU  - Bristow, Mila
TI  - Exploring the Potential of High Resolution WorldView-3 Imagery for Estimating Yield of Mango
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Pre-harvest yield estimation of mango fruit is important for the optimization of inputs and other resources on the farm. Current industry practice of visual counting the fruit on a small number of trees for yield forecasting can be highly inaccurate due to the spatial variability, especially if the trees selected do not represent the entire crop. Therefore, this study evaluated the potential of high resolution WorldView-3 (WV3) satellite imagery to estimate yield of mango by integrating both geometric (tree crown area) and optical (spectral vegetation indices) data using artificial neural network (ANN) model. WV3 images were acquired in 2016&ndash;2017 and 2017&ndash;2018 growing seasons at the early fruit stage from three orchards in Acacia Hills region, Northern Territory, Australia. Stratified sampling technique (SST) was applied to select 18 trees from each orchard and subsequently ground truthed for yield (kg&middot;tree&minus;1) and fruit number per tree. For each sampled tree, spectral reflectance data and tree crown area (TCA) was extracted from WV3 imagery. The TCA was identified as the most important predictor of both fruit yield (kg&middot;tree&minus;1) and fruit number, followed by NDVI red-edge band when all trees from three orchards in two growing seasons were combined. The results of all sampled trees from three orchards in two growing seasons using ANN model produced a strong correlation (R2 = 0.70 and 0.68 for total fruit yield (kg&middot;tree&minus;1) and fruit number respectively), which suggest that the model can be obtained to predict yield on a regional level. On orchard level also the ANN model produced a high correlation when both growing seasons were combined. However, the model developed in one season could not be applied in another season due to the influence of seasonal variation and canopy condition. Using the relationship derived from the measured yield parameters against combined VIs and TCA data, the total fruit yield (t&middot;ha&minus;1) and fruit number were estimated for each orchard, produced 7% under estimation to less than 1% over estimation. The accuracy of the findings showed the potential of WV3 imagery to better predict the yield parameters than the current practice across the mango industry as well as to quantify lost yield as a result of delayed harvest.
KW  - WorldView-3 (WV3)
KW  - Mango (Mangifera indica)
KW  - tree crown area
KW  - yield prediction
DO  - 10.3390/rs10121866
ER  -
TY  - EJOU
AU  - Morales, Giorgio
AU  - Kemper, Guillermo
AU  - Sevillano, Grace
AU  - Arteaga, Daniel
AU  - Ortega, Ivan
AU  - Telles, Joel
TI  - Automatic Segmentation of Mauritia flexuosa in Unmanned Aerial Vehicle (UAV) Imagery Using Deep Learning
T2  - Forests

PY  - 2018
VL  - 9
IS  - 12
SN  - 1999-4907

AB  - One of the most important ecosystems in the Amazon rainforest is the Mauritia flexuosa swamp or “aguajal”. However, deforestation of its dominant species, the Mauritia flexuosa palm, also known as “aguaje”, is a common issue, and conservation is poorly monitored because of the difficult access to these swamps. The contribution of this paper is twofold: the presentation of a dataset called MauFlex, and the proposal of a segmentation and measurement method for areas covered in Mauritia flexuosa palms using high-resolution aerial images acquired by UAVs. The method performs a semantic segmentation of Mauritia flexuosa using an end-to-end trainable Convolutional Neural Network (CNN) based on the Deeplab v3+ architecture. Images were acquired under different environment and light conditions using three different RGB cameras. The MauFlex dataset was created from these images and it consists of 25,248 image patches of     512 × 512     pixels and their respective ground truth masks. The results over the test set achieved an accuracy of 98.143%, specificity of 96.599%, and sensitivity of 95.556%. It is shown that our method is able not only to detect full-grown isolated Mauritia flexuosa palms, but also young palms or palms partially covered by other types of vegetation.
KW  - Mauritia flexuosa
KW  - semantic segmentation
KW  - end-to-end learning
KW  - convolutional neural network
KW  - forest inventory
DO  - 10.3390/f9120736
ER  -
TY  - EJOU
AU  - Zhang, Bin
AU  - Wang, Cunpeng
AU  - Shen, Yonglin
AU  - Liu, Yueyan
TI  - Fully Connected Conditional Random Fields for High-Resolution Remote Sensing Land Use/Land Cover Classification with Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - The interpretation of land use and land cover (LULC) is an important issue in the fields of high-resolution remote sensing (RS) image processing and land resource management. Fully training a new or existing convolutional neural network (CNN) architecture for LULC classification requires a large amount of remote sensing images. Thus, fine-tuning a pre-trained CNN for LULC detection is required. To improve the classification accuracy for high resolution remote sensing images, it is necessary to use another feature descriptor and to adopt a classifier for post-processing. A fully connected conditional random fields (FC-CRF), to use the fine-tuned CNN layers, spectral features, and fully connected pairwise potentials, is proposed for image classification of high-resolution remote sensing images. First, an existing CNN model is adopted, and the parameters of CNN are fine-tuned by training datasets. Then, the probabilities of image pixels belong to each class type are calculated. Second, we consider the spectral features and digital surface model (DSM) and combined with a support vector machine (SVM) classifier, the probabilities belong to each LULC class type are determined. Combined with the probabilities achieved by the fine-tuned CNN, new feature descriptors are built. Finally, FC-CRF are introduced to produce the classification results, whereas the unary potentials are achieved by the new feature descriptors and SVM classifier, and the pairwise potentials are achieved by the three-band RS imagery and DSM. Experimental results show that the proposed classification scheme achieves good performance when the total accuracy is about 85%.
KW  - remote sensing
KW  - image classification
KW  - fully connected conditional random fields (FC-CRF)
KW  - convolutional neural networks (CNN)
DO  - 10.3390/rs10121889
ER  -
TY  - EJOU
AU  - Al Rahhal, Mohamad M.
AU  - Bazi, Yakoub
AU  - Abdullah, Taghreed
AU  - Mekhalfi, Mohamed L.
AU  - AlHichri, Haikel
AU  - Zuair, Mansour
TI  - Learning a Multi-Branch Neural Network from Multiple Sources for Knowledge Adaptation in Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - In this paper we propose a multi-branch neural network, called MB-Net, for solving the problem of knowledge adaptation from multiple remote sensing scene datasets acquired with different sensors over diverse locations and manually labeled with different experts. Our aim is to learn invariant feature representations from multiple source domains with labeled images and one target domain with unlabeled images. To this end, we define for MB-Net an objective function that mitigates the multiple domain shifts at both feature representation and decision levels, while retaining the ability to discriminate between different land-cover classes. The complete architecture is trainable end-to-end via the backpropagation algorithm. In the experiments, we demonstrate the effectiveness of the proposed method on a new multiple domain dataset created from four heterogonous scene datasets well known to the remote sensing community, namely, the University of California (UC-Merced) dataset, the Aerial Image dataset (AID), the PatternNet dataset, and the Northwestern Polytechnical University (NWPU) dataset. In particular, this method boosts the average accuracy over all transfer scenarios up to 89.05% compared to standard architecture based only on cross-entropy loss, which yields an average accuracy of 78.53%.
KW  - scene classification
KW  - multiple sources
KW  - multiple domain shifts
KW  - multi-branch neural network
DO  - 10.3390/rs10121890
ER  -
TY  - EJOU
AU  - Ruan, Lang
AU  - Chen, Jin
AU  - Guo, Qiuju
AU  - Jiang, Han
AU  - Zhang, Yuli
AU  - Liu, Dianxiong
TI  - A Coalition Formation Game Approach for Efficient Cooperative Multi-UAV Deployment
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - Unmanned aerial vehicle (UAV) cooperative control has been an important issue in UAV-assisted sensor networks, thanks to the considerable benefit obtained from the cooperative mechanism of UAVs being applied as a flying base station. In a coverage scenarios, the trade-off between coverage and transmission performance often makes deployment of UAVs fall into a dilemma, since both indexes are related to the distance between UAVs. To address this issue, UAV coverage and data transmission mechanism is analyzed in this paper; then, an efficient multi-UAV cooperative deployment model is proposed. The problem is modeled as a coalition formation game (CFG). The CFG with Pareto order is proved to have a stable partition. Then, an effective approach consisting of coverage deployment and coalition selection is designed, wherein UAVs can decide strategies cooperatively to achieve better coverage performance. Combining analysis of game approach, coalition selection and the position deployment algorithm based on Pareto order (CSPDA-PO) is designed to execute coverage deployment and coalition selection. Finally, simulation results are shown to validate the proposed approach based on an efficient multi-UAV cooperative deployment model.
KW  - UAV-assisted sensor network
KW  - UAV cooperative coverage
KW  - coalition formation game
KW  - stable coalition partition
KW  - Nash equilibrium
DO  - 10.3390/app8122427
ER  -
TY  - EJOU
AU  - Fu, Kun
AU  - Li, Yang
AU  - Sun, Hao
AU  - Yang, Xue
AU  - Xu, Guangluan
AU  - Li, Yuting
AU  - Sun, Xian
TI  - A Ship Rotation Detection Model in Remote Sensing Images Based on Feature Fusion Pyramid Network and Deep Reinforcement Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Ship detection plays an important role in automatic remote sensing image interpretation. The scale difference, large aspect ratio of ship, complex remote sensing image background and ship dense parking scene make the detection task difficult. To handle the challenging problems above, we propose a ship rotation detection model based on a Feature Fusion Pyramid Network and deep reinforcement learning (FFPN-RL) in this paper. The detection network can efficiently generate the inclined rectangular box for ship. First, we propose the Feature Fusion Pyramid Network (FFPN) that strengthens the reuse of different scales features, and FFPN can extract the low level location and high level semantic information that has an important impact on multi-scale ship detection and precise location of dense parking ships. Second, in order to get accurate ship angle information, we apply deep reinforcement learning to the inclined ship detection task for the first time. In addition, we put forward prior policy guidance and a long-term training method to train an angle prediction agent constructed through a dueling structure Q network, which is able to iteratively and accurately obtain the ship angle. In addition, we design soft rotation non-maximum suppression to reduce the missed ship detection while suppressing the redundant detection boxes. We carry out detailed experiments on the remote sensing ship image dataset, and the experiments validate that our FFPN-RL ship detection model has efficient detection performance.
KW  - ship detection
KW  - deep reinforcement learning
KW  - convolution neural network
KW  - feature map fusion
DO  - 10.3390/rs10121922
ER  -
TY  - EJOU
AU  - He, Zhi
AU  - Liu, Lin
TI  - Hyperspectral Image Super-Resolution Inspired by Deep Laplacian Pyramid Network
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Existing hyperspectral sensors usually produce high-spectral-resolution but low-spatial-resolution images, and super-resolution has yielded impressive results in improving the resolution of the hyperspectral images (HSIs). However, most of the super-resolution methods require multiple observations of the same scene and improve the spatial resolution without fully considering the spectral information. In this paper, we propose an HSI super-resolution method inspired by the deep Laplacian pyramid network (LPN). First, the spatial resolution is enhanced by an LPN, which can exploit the knowledge from natural images without using any auxiliary observations. The LPN progressively reconstructs the high-spatial-resolution images in a coarse-to-fine fashion by using multiple pyramid levels. Second, spectral characteristics between the low- and high-resolution HSIs are studied by the non-negative dictionary learning (NDL), which is proposed to learn the common dictionary with non-negative constraints. The super-resolution results can finally be obtained by multiplying the learned dictionary and its corresponding sparse codes. Experimental results on three hyperspectral datasets demonstrate the feasibility of the proposed method in enhancing the spatial resolution of the HSI with preserving the spectral information simultaneously.
KW  - hyperspectral image (HSI)
KW  - super-resolution
KW  - deep Laplacian pyramid network (LPN)
KW  - dictionary learning
DO  - 10.3390/rs10121939
ER  -
TY  - EJOU
AU  - Wang, Chengyou
AU  - Zhang, Zhi
AU  - Zhou, Xiao
TI  - An Image Copy-Move Forgery Detection Scheme Based on A-KAZE and SURF Features
T2  - Symmetry

PY  - 2018
VL  - 10
IS  - 12
SN  - 2073-8994

AB  - The popularity of image editing software has made it increasingly easy to alter the content of images. These alterations threaten the authenticity and integrity of images, causing misjudgments and possibly even affecting social stability. The copy-move technique is one of the most commonly used approaches for manipulating images. As a defense, the image forensics technique has become popular for judging whether a picture has been tampered with via copy-move, splicing, or other forgery techniques. In this paper, a scheme based on accelerated-KAZE (A-KAZE) and speeded-up robust features (SURF) is proposed for image copy-move forgery detection (CMFD). It is difficult for most keypoint-based CMFD methods to obtain sufficient points in smooth regions. To remedy this defect, the response thresholds for the A-KAZE and SURF feature detection stages are set to small values in the proposed method. In addition, a new correlation coefficient map is presented, in which the duplicated regions are demarcated, combining filtering and mathematical morphology operations. Numerous experiments are conducted to demonstrate the effectiveness of the proposed method in searching for duplicated regions and its robustness against distortions and post-processing techniques, such as noise addition, rotation, scaling, image blurring, joint photographic expert group (JPEG) compression, and hybrid image manipulation. The experimental results demonstrate that the performance of the proposed scheme is superior to that of other tested CMFD methods.
KW  - image forensics
KW  - copy-move forgery detection (CMFD)
KW  - accelerated-KAZE (A-KAZE) feature
KW  - speeded-up robust features (SURF)
DO  - 10.3390/sym10120706
ER  -
TY  - EJOU
AU  - Zhang, Ye
AU  - Wang, Gang
AU  - Li, Mingchao
AU  - Han, Shuai
TI  - Automated Classification Analysis of Geological Structures Based on Images Data and Deep Learning Model
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - It is meaningful to study the geological structures exposed on the Earth&rsquo;s surface, which is paramount to engineering design and construction. In this research, we used 2206 images with 12 labels to identify geological structures based on the Inception-v3 model. Grayscale and color images were adopted in the model. A convolutional neural network (CNN) model was also built in this research. Meanwhile, K nearest neighbors (KNN), artificial neural network (ANN) and extreme gradient boosting (XGBoost) were applied in geological structures classification based on features extracted by the Open Source Computer Vision Library (OpenCV). Finally, the performances of the five methods were compared and the results indicated that KNN, ANN, and XGBoost had a poor performance, with the accuracy of less than 40.0%. CNN was overfitting. The model trained using transfer learning had a significant effect on a small dataset of geological structure images; and the top-1 and top-3 accuracy of the model reached 83.3% and 90.0%, respectively. This shows that texture is the key feature in this research. Transfer learning based on a deep learning model can extract features of small geological structure data effectively, and it is robust in geological structure image classification.
KW  - OpenCV
KW  - machine learning
KW  - transfer learning
KW  - Inception-v3
KW  - geological structure images
KW  - convolutional neural networks
DO  - 10.3390/app8122493
ER  -
TY  - EJOU
AU  - Sang, Jun
AU  - Wu, Zhongyuan
AU  - Guo, Pei
AU  - Hu, Haibo
AU  - Xiang, Hong
AU  - Zhang, Qian
AU  - Cai, Bin
TI  - An Improved YOLOv2 for Vehicle Detection
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - Vehicle detection is one of the important applications of object detection in intelligent transportation systems. It aims to extract specific vehicle-type information from pictures or videos containing vehicles. To solve the problems of existing vehicle detection, such as the lack of vehicle-type recognition, low detection accuracy, and slow speed, a new vehicle detection model YOLOv2_Vehicle based on YOLOv2 is proposed in this paper. The k-means++ clustering algorithm was used to cluster the vehicle bounding boxes on the training dataset, and six anchor boxes with different sizes were selected. Considering that the different scales of the vehicles may influence the vehicle detection model, normalization was applied to improve the loss calculation method for length and width of bounding boxes. To improve the feature extraction ability of the network, the multi-layer feature fusion strategy was adopted, and the repeated convolution layers in high layers were removed. The experimental results on the Beijing Institute of Technology (BIT)-Vehicle validation dataset demonstrated that the mean Average Precision (mAP) could reach 94.78%. The proposed model also showed excellent generalization ability on the CompCars test dataset, where the &ldquo;vehicle face&rdquo; is quite different from the training dataset. With the comparison experiments, it was proven that the proposed method is effective for vehicle detection. In addition, with network visualization, the proposed model showed excellent feature extraction ability.
KW  - vehicle detection
KW  - object detection
KW  - YOLOv2
KW  - convolutional neural network
DO  - 10.3390/s18124272
ER  -
TY  - EJOU
AU  - He, Fangning
AU  - Zhou, Tian
AU  - Xiong, Weifeng
AU  - Hasheminnasab, Seyyed M.
AU  - Habib, Ayman
TI  - Automated Aerial Triangulation for UAV-Based Mapping
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Accurate 3D reconstruction/modelling from unmanned aerial vehicle (UAV)-based imagery has become the key prerequisite in various applications. Although current commercial software has automated the process of image-based reconstruction, a transparent system, which can be incorporated with different user-defined constraints, is still preferred by the photogrammetric research community. In this regard, this paper presents a transparent framework for the automated aerial triangulation of UAV images. The proposed framework is conducted in three steps. In the first step, two approaches, which take advantage of prior information regarding the flight trajectory, are implemented for reliable relative orientation recovery. Then, initial recovery of image exterior orientation parameters (EOPs) is achieved through either an incremental or global approach. Finally, a global bundle adjustment involving Ground Control Points (GCPs) and check points is carried out to refine all estimated parameters in the defined mapping coordinate system. Four real image datasets, which are acquired by two different UAV platforms, have been utilized to evaluate the feasibility of the proposed framework. In addition, a comparative analysis between the proposed framework and the existing commercial software is performed. The derived experimental results demonstrate the superior performance of the proposed framework in providing an accurate 3D model, especially when dealing with acquired UAV images containing repetitive pattern and significant image distortions.
KW  - unmanned aerial vehicle
KW  - 3D reconstruction
KW  - structure from motion
KW  - relative orientation
KW  - exterior orientation parameters
KW  - bundle adjustment
DO  - 10.3390/rs10121952
ER  -
TY  - EJOU
AU  - Zhu, Dongjie
AU  - Du, Haiwen
AU  - Sun, Yundong
AU  - Cao, Ning
TI  - Research on Path Planning Model Based on Short-Term Traffic Flow Prediction in Intelligent Transportation System
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - Vehicle driving path planning is an important information service in intelligent transportation systems. As an important basis for path planning optimization, the travel time prediction method has attracted much attention. However, traffic flow has features of high nonlinearity, time-varying, and uncertainty, which makes it hard for prediction method with single feature to meet the accuracy demand of intelligent transportation system in big data environment. In this paper, the historical vehicle Global Positioning System (GPS) information data is used to establish the traffic prediction model. Firstly, the Clustering in QUEst (CLIQUE)-based clustering algorithm V-CLIQUE is proposed to analyze the historical vehicle GPS data. Secondly, an artificial neural network (ANN)-based prediction model is proposed. Finally, the ANN-based weighted shortest path algorithm, A-Dijkstra, is proposed. We used mean absolute percentage error (MAPE) to evaluate the predictive model and compare it with the predicted results of Average and support regression vector (SRV). Experiments show that the improved ANN path planning model we proposed can accurately predict real-time traffic status at the given location. It has less relative error and saves time for users&rsquo; travel while saving social resources.
KW  - path planning
KW  - short-term traffic flow prediction
KW  - intelligent transportation system
KW  - A-Dijkstra
DO  - 10.3390/s18124275
ER  -
TY  - EJOU
AU  - Colpaert, Achiel
AU  - Vinogradov, Evgenii
AU  - Pollin, Sofie
TI  - Aerial Coverage Analysis of Cellular Systems at LTE and mmWave Frequencies Using 3D City Models
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - Cellular connectivity for UAV systems is interesting because it promises coverage in beyond visual line of sight scenarios. Inter-cell interference has been shown to be the main limiting factor at high altitudes. Using a realistic 3D simulator model, with real base station locations, this study confirms that UAVs at high altitudes suffer from significant interference, resulting in a worse coverage compared to ground users. When replacing the existing base stations by mmWave cells, our results indicate that ground coverage is decreased to only 90%, while UAVs just above rooftop level have a coverage probability of 100%. However, UAVs at higher altitude still suffer from excessive interference. Beamforming has the potential to improve mmWave link budget and to decrease interference and is for this reason a promising technology for ensuring connectivity to aerial users.
KW  - UAV
KW  - LTE
KW  - mmWave
KW  - beamforming
KW  - cellular
KW  - drone
KW  - interference
KW  - SINR
DO  - 10.3390/s18124311
ER  -
TY  - EJOU
AU  - Vinayaraj, Poliyapram
AU  - Imamoglu, Nevrez
AU  - Nakamura, Ryosuke
AU  - Oda, Atsushi
TI  - Investigation on Perceptron Learning for Water Region Estimation Using Large-Scale Multispectral Images
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - Land cover classification and investigation of temporal changes are considered to be common applications of remote sensing. Water/non-water region estimation is one of the most fundamental classification tasks, analyzing the occurrence of water on the Earth&rsquo;s surface. However, common remote sensing practices such as thresholding, spectral analysis, and statistical approaches are not sufficient to produce a globally adaptable water classification. The aim of this study is to develop a formula with automatically derived tuning parameters using perceptron neural networks for water/non-water region estimation, which we call the Perceptron-Derived Water Formula (PDWF), using Landsat-8 images. Water/non-water region estimates derived from PDWF were compared with three different approaches&mdash;Modified Normalized Difference Water Index (MNDWI), Automatic Water Extraction Index (AWEI), and Deep Convolutional Neural Network&mdash;using various case studies. Our proposed method outperforms all three approaches, showing a significant improvement in water/non-water region estimation. PDWF performance is consistently better even in cases of challenging conditions such as low reflectance due to hill shadows, building-shadows, and dark soils. Moreover, our study implemented a sunglint correction to adapt water/non-water region estimation over sunglint-affected pixels.
KW  - AWEI
KW  - deep neural network
KW  - Landsat-8
KW  - MNDWI
KW  - PDWF
KW  - perceptron neural network
KW  - surface water bodies
DO  - 10.3390/s18124333
ER  -
TY  - EJOU
AU  - Torti, Emanuele
AU  - Fontanella, Alessandro
AU  - Plaza, Antonio
AU  - Plaza, Javier
AU  - Leporati, Francesco
TI  - Hyperspectral Image Classification Using Parallel Autoencoding Diabolo Networks on Multi-Core and Many-Core Architectures
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 12
SN  - 2079-9292

AB  - One of the most important tasks in hyperspectral imaging is the classification of the pixels in the scene in order to produce thematic maps. This problem can be typically solved through machine learning techniques. In particular, deep learning algorithms have emerged in recent years as a suitable methodology to classify hyperspectral data. Moreover, the high dimensionality of hyperspectral data, together with the increasing availability of unlabeled samples, makes deep learning an appealing approach to process and interpret those data. However, the limited number of labeled samples often complicates the exploitation of supervised techniques. Indeed, in order to guarantee a suitable precision, a large number of labeled samples is normally required. This hurdle can be overcome by resorting to unsupervised classification algorithms. In particular, autoencoders can be used to analyze a hyperspectral image using only unlabeled data. However, the high data dimensionality leads to prohibitive training times. In this regard, it is important to realize that the operations involved in autoencoders training are intrinsically parallel. Therefore, in this paper we present an approach that exploits multi-core and many-core devices in order to achieve efficient autoencoders training in hyperspectral imaging applications. Specifically, in this paper, we present new OpenMP and CUDA frameworks for autoencoder training. The obtained results show that the CUDA framework provides a speed-up of about two orders of magnitudes as compared to an optimized serial processing chain.
KW  - Graphics Processing Units (GPUs)
KW  - multi-core CPU
KW  - parallel processing
KW  - CUDA
KW  - OpenMP
KW  - hyperspectral imaging
DO  - 10.3390/electronics7120411
ER  -
TY  - EJOU
AU  - Piltan, Farzin
AU  - Kim, Jong-Myon
TI  - Bearing Fault Diagnosis Using an Extended Variable Structure Feedback Linearization Observer
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - The rolling element bearing is a significant component in rotating machinery. Suitable bearing fault detection and diagnosis (FDD) is vital to maintaining machine operations in a safe and healthy state. To address this issue, an extended observer-based FDD method is proposed, which uses a variable structure feedback linearization observer (FLO). The traditional feedback linearization observer is stable; however, this technique suffers from a lack of robustness. The proposed variable structure technique was used to improve the robustness of the fault estimation while reducing the uncertainties in the feedback linearization observer. The effectiveness of the proposed FLO procedure for the identification of outer, inner, and ball faults was tested using the Case Western University vibration dataset. The proposed model outperformed the variable structure observer (VSO), traditional feedback linearization observer (TFLO), and proportional-integral observer (PIO) by achieving average performance improvements of 5.5%, 8.5%, and 18.5%, respectively.
KW  - bearing fault detection
KW  - feedback linearization observer
KW  - model-reference fault diagnosis
KW  - variable structure observer
KW  - proportional integral observer
DO  - 10.3390/s18124359
ER  -
TY  - EJOU
AU  - Kang, Man-Sung
AU  - Lee, Hanju
AU  - Yim, Hong J.
AU  - An, Yun-Kyu
AU  - Kim, Dong J.
TI  - Multi-Channel Electrical Impedance-Based Crack Localization of Fiber-Reinforced Cementitious Composites under Bending Conditions
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - This study proposes a multi-channel electrical impedance-based crack localization technique of fiber-reinforced cementitious composites (FRCCs) under bending conditions. FRCCs have a self-sensing capability by adding conductive steel fibers into nonconductive cementitious composites, making it possible to measure electrical impedance without sensor installation. Moreover, FRCCs materials can be used as a structural member thanks to its own enhanced structural ductility as well as stiffness. In a structural health monitoring point of view, these characteristics make FRCCs suitable for monitoring structural hot spots, particularly where the crack is most likely to be initiated. Since the electrical impedance obtained from FRCCs is typically sensitive to environmental and operational conditions, false alarms are often triggered. The proposed technique can minimize the false alarms by using currently measured multi-path data as well as localize a crack within the sensing range. To examine the feasibility of crack localization in FRCCs, an instantaneous multi-channel electrical impedance acquisition system and a crack localization algorithm are developed. Subsequently, three-point bending tests are carried out under various temperature conditions. The validation test results reveal that cracks are successfully identified and localized even under varying temperature conditions.
KW  - fiber-reinforced cementitious composite
KW  - nondestructive testing
KW  - electrical impedance
KW  - crack localization
KW  - self-sensing concrete
KW  - structural health monitoring
DO  - 10.3390/app8122582
ER  -
TY  - EJOU
AU  - Liu, Ziquan
AU  - Wang, Huifang
TI  - Automatic Detection of Transformer Components in Inspection Images Based on Improved Faster R-CNN
T2  - Energies

PY  - 2018
VL  - 11
IS  - 12
SN  - 1996-1073

AB  - To detect the categories and positions of various transformer components in inspection images automatically, this paper proposes a transformer component detection model with high detection accuracy, based on the structure of Faster R-CNN. In consideration of the significant difference in component sizes, double feature maps are used to adapt to the size change, by adjusting two weights dynamically according to the object size. Moreover, different from the detection of ordinary objects, there is abundant useful information contained in the relative positions between components. Thus, the relative position features are defined and introduced to the refinement of the detection results. Then, the training process and detection process are proposed specifically for the improved model. Finally, an experiment is given to compare the accuracy and efficiency of the improved model and the original Faster R-CNN, along with other object detection models. Results show that the improved model has an obvious advantage in accuracy, and the efficiency is significantly higher than that of manual detection, which suggests that the model is suitable for practical engineering applications.
KW  - object detection
KW  - transformer component
KW  - Faster R-CNN
KW  - double feature maps
KW  - relative position features
KW  - random forests
KW  - image processing
KW  - computer vision
DO  - 10.3390/en11123496
ER  -
TY  - EJOU
AU  - Gstaiger, Veronika
AU  - Tian, Jiaojiao
AU  - Kiefl, Ralph
AU  - Kurz, Franz
TI  - 2D vs. 3D Change Detection Using Aerial Imagery to Support Crisis Management of Large-Scale Events
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Large-scale events represent a special challenge for crisis management. To ensure that participants can enjoy an event safely and carefree, it must be comprehensively prepared and attentively monitored. Remote sensing can provide valuable information to identify potential risks and take appropriate measures in order to prevent a disaster, or initiate emergency aid measures as quickly as possible in the event of an emergency. Especially, three-dimensional (3D) information that is derived using photogrammetry can be used to analyze the terrain and map existing structures that are set up at short notice. Using aerial imagery acquired during a German music festival in 2016 and the celebration of the German Protestant Church Assembly of 2017, the authors compare two-dimensional (2D) and novel fusion-based 3D change detection methods, and discuss their suitability for supporting large-scale events during the relevant phases of crisis management. This study serves to find out what added value the use of 3D change information can provide for on-site crisis management. Based on the results, an operational, fully automatic processor for crisis management operations and corresponding products for end users can be developed.
KW  - crisis management support
KW  - aerial imagery
KW  - large-scale event
KW  - 2D change detection
KW  - 3D change detection
DO  - 10.3390/rs10122054
ER  -
TY  - EJOU
AU  - Guo, Hao
AU  - Wei, Guo
AU  - An, Jubai
TI  - Dark Spot Detection in SAR Images of Oil Spill Using Segnet
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - Damping Bragg scattering from the ocean surface is the basic underlying principle of synthetic aperture radar (SAR) oil slick detection, and they produce dark spots on SAR images. Dark spot detection is the first step in oil spill detection, which affects the accuracy of oil spill detection. However, some natural phenomena (such as waves, ocean currents, and low wind belts, as well as human factors) may change the backscatter intensity on the surface of the sea, resulting in uneven intensity, high noise, and blurred boundaries of oil slicks or lookalikes. In this paper, Segnet is used as a semantic segmentation model to detect dark spots in oil spill areas. The proposed method is applied to a data set of 4200 from five original SAR images of an oil spill. The effectiveness of the method is demonstrated through the comparison with fully convolutional networks (FCN), an initiator of semantic segmentation models, and some other segmentation methods. It is here observed that the proposed method can not only accurately identify the dark spots in SAR images, but also show a higher robustness under high noise and fuzzy boundary conditions.
KW  - image segmentation
KW  - deep learning
KW  - synthetic aperture radar (SAR)
KW  - oil slicks
KW  - segnet
DO  - 10.3390/app8122670
ER  -
TY  - EJOU
AU  - Huang, Lingcao
AU  - Liu, Lin
AU  - Jiang, Liming
AU  - Zhang, Tingjun
TI  - Automatic Mapping of Thermokarst Landforms from Remote Sensing Images Using Deep Learning: A Case Study in the Northeastern Tibetan Plateau
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Thawing of ice-rich permafrost causes thermokarst landforms on the ground surface. Obtaining the distribution of thermokarst landforms is a prerequisite for understanding permafrost degradation and carbon exchange at local and regional scales. However, because of their diverse types and characteristics, it is challenging to map thermokarst landforms from remote sensing images. We conducted a case study towards automatically mapping a type of thermokarst landforms (i.e., thermo-erosion gullies) in a local area in the northeastern Tibetan Plateau from high-resolution images by the use of deep learning. In particular, we applied the DeepLab algorithm (based on Convolutional Neural Networks) to a 0.15-m-resolution Digital Orthophoto Map (created using aerial photographs taken by an Unmanned Aerial Vehicle). Here, we document the detailed processing flow with key steps including preparing training data, fine-tuning, inference, and post-processing. Validating against the field measurements and manual digitizing results, we obtained an F1 score of 0.74 (precision is 0.59 and recall is 1.0), showing that the proposed method can effectively map small and irregular thermokarst landforms. It is potentially viable to apply the designed method to mapping diverse thermokarst landforms in a larger area where high-resolution images and training data are available.
KW  - DeepLab
KW  - permafrost degradation
KW  - semantic segmentation
KW  - thermokarst landforms
KW  - thermo-erosion gullies
KW  - Tibetan Plateau
KW  - Unmanned Aerial Vehicle Images
DO  - 10.3390/rs10122067
ER  -
TY  - EJOU
AU  - Rançon, Florian
AU  - Bombrun, Lionel
AU  - Keresztes, Barna
AU  - Germain, Christian
TI  - Comparison of SIFT Encoded and Deep Learning Features for the Classification and Detection of Esca Disease in Bordeaux Vineyards
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 1
SN  - 2072-4292

AB  - Grapevine wood fungal diseases such as esca are among the biggest threats in vineyards nowadays. The lack of very efficient preventive (best results using commercial products report 20% efficiency) and curative means induces huge economic losses. The study presented in this paper is centered around the in-field detection of foliar esca symptoms during summer, exhibiting a typical &ldquo;striped&rdquo; pattern. Indeed, in-field disease detection has shown great potential for commercial applications and has been successfully used for other agricultural needs such as yield estimation. Differentiation with foliar symptoms caused by other diseases or abiotic stresses was also considered. Two vineyards from the Bordeaux region (France, Aquitaine) were chosen as the basis for the experiment. Pictures of diseased and healthy vine plants were acquired during summer 2017 and labeled at the leaf scale, resulting in a patch database of around 6000 images (224 &times; 224 pixels) divided into red cultivar and white cultivar samples. Then, we tackled the classification part of the problem comparing state-of-the-art SIFT encoding and pre-trained deep learning feature extractors for the classification of database patches. In the best case, 91% overall accuracy was obtained using deep features extracted from MobileNet network trained on ImageNet database, demonstrating the efficiency of simple transfer learning approaches without the need to design an ad-hoc specific feature extractor. The third part aimed at disease detection (using bounding boxes) within full plant images. For this purpose, we integrated the deep learning base network within a &ldquo;one-step&rdquo; detection network (RetinaNet), allowing us to perform detection queries in real time (approximately six frames per second on GPU). Recall/Precision (RP) and Average Precision (AP) metrics then allowed us to evaluate the performance of the network on a 91-image (plants) validation database. Overall, 90% precision for a 40% recall was obtained while best esca AP was about 70%. Good correlation between annotated and detected symptomatic surface per plant was also obtained, meaning slightly symptomatic plants can be efficiently separated from severely attacked plants.
KW  - proximal sensing
KW  - disease detection
KW  - grapevine trunk disease
KW  - esca
KW  - SIFT
KW  - deep learning
DO  - 10.3390/rs11010001
ER  -
TY  - EJOU
AU  - Li, Weijia
AU  - Dong, Runmin
AU  - Fu, Haohuan
AU  - Yu, Le
TI  - Large-Scale Oil Palm Tree Detection from High-Resolution Satellite Images Using Two-Stage Convolutional Neural Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 1
SN  - 2072-4292

AB  - Being an important economic crop that contributes 35% of the total consumption of vegetable oil, remote sensing-based quantitative detection of oil palm trees has long been a key research direction for both agriculture and environmental purposes. While existing methods already demonstrate satisfactory effectiveness for small regions, performing the detection for a large region with satisfactory accuracy is still challenging. In this study, we proposed a two-stage convolutional neural network (TS-CNN)-based oil palm detection method using high-resolution satellite images (i.e. Quickbird) in a large-scale study area of Malaysia. The TS-CNN consists of one CNN for land cover classification and one CNN for object classification. The two CNNs were trained and optimized independently based on 20,000 samples collected through human interpretation. For the large-scale oil palm detection for an area of 55 km2, we proposed an effective workflow that consists of an overlapping partitioning method for large-scale image division, a multi-scale sliding window method for oil palm coordinate prediction, and a minimum distance filter method for post-processing. Our proposed approach achieves a much higher average F1-score of 94.99% in our study area compared with existing oil palm detection methods (87.95%, 81.80%, 80.61%, and 78.35% for single-stage CNN, Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN), respectively), and much fewer confusions with other vegetation and buildings in the whole image detection results.
KW  - oil palm tree
KW  - object detection
KW  - convolutional neural networks
KW  - deep learning
KW  - high-resolution satellite imagery
DO  - 10.3390/rs11010011
ER  -
TY  - EJOU
AU  - Aamir, Muhammad
AU  - Pu, Yi-Fei
AU  - Rahman, Ziaur
AU  - Tahir, Muhammad
AU  - Naeem, Hamad
AU  - Dai, Qiang
TI  - A Framework for Automatic Building Detection from Low-Contrast Satellite Images
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 1
SN  - 2073-8994

AB  - Building detection in satellite images has been considered an essential field of research in remote sensing and computer vision. There are currently numerous techniques and algorithms used to achieve building detection performance. Different algorithms have been proposed to extract building objects from high-resolution satellite images with standard contrast. However, building detection from low-contrast satellite images to predict symmetrical findings as of past studies using normal contrast images is considered a challenging task and may play an integral role in a wide range of applications. Having received significant attention in recent years, this manuscript proposes a methodology to detect buildings from low-contrast satellite images. In an effort to enhance visualization of satellite images, in this study, first, the contrast of an image is optimized to represent all the information using singular value decomposition (SVD) based on the discrete wavelet transform (DWT). Second, a line-segment detection scheme is applied to accurately detect building line segments. Third, the detected line segments are hierarchically grouped to recognize the relationship of identified line segments, and the complete contours of the building are attained to obtain candidate rectangular buildings. In this paper, the results from the method above are compared with existing approaches based on high-resolution images with reasonable contrast. The proposed method achieves high performance thus yields more diversified and insightful results over conventional techniques.
KW  - low-contrast satellite image
KW  - high-resolution satellite imagery
KW  - image equalization
KW  - building extraction
KW  - DWT–SVD
KW  - perceptual grouping
DO  - 10.3390/sym11010003
ER  -
TY  - EJOU
AU  - Wang, Yuhao
AU  - Liang, Binxiu
AU  - Ding, Meng
AU  - Li, Jiangyun
TI  - Dense Semantic Labeling with Atrous Spatial Pyramid Pooling and Decoder for High-Resolution Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 1
SN  - 2072-4292

AB  - Dense semantic labeling is significant in high-resolution remote sensing imagery research and it has been widely used in land-use analysis and environment protection. With the recent success of fully convolutional networks (FCN), various types of network architectures have largely improved performance. Among them, atrous spatial pyramid pooling (ASPP) and encoder-decoder are two successful ones. The former structure is able to extract multi-scale contextual information and multiple effective field-of-view, while the latter structure can recover the spatial information to obtain sharper object boundaries. In this study, we propose a more efficient fully convolutional network by combining the advantages from both structures. Our model utilizes the deep residual network (ResNet) followed by ASPP as the encoder and combines two scales of high-level features with corresponding low-level features as the decoder at the upsampling stage. We further develop a multi-scale loss function to enhance the learning procedure. In the postprocessing, a novel superpixel-based dense conditional random field is employed to refine the predictions. We evaluate the proposed method on the Potsdam and Vaihingen datasets and the experimental results demonstrate that our method performs better than other machine learning or deep learning methods. Compared with the state-of-the-art DeepLab_v3+ our model gains 0.4% and 0.6% improvements in overall accuracy on these two datasets respectively.
KW  - remote sensing imagery
KW  - dense semantic labeling
KW  - fully convolutional networks
KW  - atrous spatial pyramid pooling
KW  - encoder-decoder
KW  - superpixel-based DenseCRF
DO  - 10.3390/rs11010020
ER  -
TY  - EJOU
AU  - Moskalenko, Viacheslav
AU  - Moskalenko, Alona
AU  - Korobov, Artem
AU  - Semashko, Viktor
TI  - The Model and Training Algorithm of Compact Drone Autonomous Visual Navigation System
T2  - Data

PY  - 2019
VL  - 4
IS  - 1
SN  - 2306-5729

AB  - Trainable visual navigation systems based on deep learning demonstrate potential for robustness of onboard camera parameters and challenging environment. However, a deep model requires substantial computational resources and large labelled training sets for successful training. Implementation of the autonomous navigation and training-based fast adaptation to the new environment for a compact drone is a complicated task. The article describes an original model and training algorithms adapted to the limited volume of labelled training set and constrained computational resource. This model consists of a convolutional neural network for visual feature extraction, extreme-learning machine for estimating the position displacement and boosted information-extreme classifier for obstacle prediction. To perform unsupervised training of the convolution filters with a growing sparse-coding neural gas algorithm, supervised learning algorithms to construct the decision rules with simulated annealing search algorithm used for finetuning are proposed. The use of complex criterion for parameter optimization of the feature extractor model is considered. The resulting approach performs better trajectory reconstruction than the well-known ORB-SLAM. In particular, for sequence 7 from the KITTI dataset, the translation error is reduced by nearly 65.6% under the frame rate 10 frame per second. Besides, testing on the independent TUM sequence shot outdoors produces a translation error not exceeding 6% and a rotation error not exceeding 3.68 degrees per 100 m. Testing was carried out on the Raspberry Pi 3+ single-board computer.
KW  - navigation
KW  - visual odometry
KW  - convolutional neural network
KW  - neural gas
KW  - information criterion
KW  - extreme learning
DO  - 10.3390/data4010004
ER  -
TY  - EJOU
AU  - Yoo, Jisang
AU  - Lee, Gyu-cheol
TI  - Moving Object Detection Using an Object Motion Reflection Model of Motion Vectors
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 1
SN  - 2073-8994

AB  - Moving object detection task can be solved by the background subtraction algorithm if the camera is fixed. However, because the background moves, detecting moving objects in a moving car is a difficult problem. There were attempts to detect moving objects using LiDAR or stereo cameras, but when the car moved, the detection rate decreased. We propose a moving object detection algorithm using an object motion reflection model of motion vectors. The proposed method first obtains the disparity map by searching the corresponding region between stereo images. Then, we estimate road by applying v-disparity method to the disparity map. The optical flow is used to acquire the motion vectors of symmetric pixels between adjacent frames where the road has been removed. We designed a probability model of how much the local motion is reflected in the motion vector to determine if the object is moving. We have experimented with the proposed method on two datasets, and confirmed that the proposed method detects moving objects with higher accuracy than other methods.
KW  - object motion detection
KW  - ego-motion
KW  - optical flow
KW  - stereo matching
KW  - RANdom SAmple Consensus (RANSAC)
DO  - 10.3390/sym11010034
ER  -
TY  - EJOU
AU  - Zhou, Xiaomao
AU  - Gao, Yanbin
AU  - Guan, Lianwu
TI  - Towards Goal-Directed Navigation Through Combining Learning Based Global and Local Planners
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 1
SN  - 1424-8220

AB  - Robot navigation is a fundamental problem in robotics and various approaches have been developed to cope with this problem. Despite the great success of previous approaches, learning-based methods are receiving growing interest in the research community. They have shown great efficiency in solving navigation tasks and offer considerable promise to build intelligent navigation systems. This paper presents a goal-directed robot navigation system that integrates global planning based on goal-directed end-to-end learning and local planning based on reinforcement learning (RL). The proposed system aims to navigate the robot to desired goal positions while also being adaptive to changes in the environment. The global planner is trained to imitate an expert&rsquo;s navigation between different positions by goal-directed end-to-end learning, where both the goal representations and local observations are incorporated to generate actions. However, it is trained in a supervised fashion and is weak in dealing with changes in the environment. To solve this problem, a local planner based on deep reinforcement learning (DRL) is designed. The local planner is first implemented in a simulator and then transferred to the real world. It works complementarily to deal with situations that have not been met during training the global planner and is able to generalize over different situations. The experimental results on a robot platform demonstrate the effectiveness of the proposed navigation system.
KW  - robot navigation
KW  - global planning
KW  - end-to-end learning
KW  - local planning
KW  - reinforcement learning
DO  - 10.3390/s19010176
ER  -
TY  - EJOU
AU  - Xu, Lu
AU  - Ming, Dongping
AU  - Zhou, Wen
AU  - Bao, Hanqing
AU  - Chen, Yangyang
AU  - Ling, Xiao
TI  - Farmland Extraction from High Spatial Resolution Remote Sensing Images Based on Stratified Scale Pre-Estimation
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 2
SN  - 2072-4292

AB  - Extracting farmland from high spatial resolution remote sensing images is a basic task for agricultural information management. According to Tobler&rsquo;s first law of geography, closer objects have a stronger relation. Meanwhile, due to the scale effect, there are differences on both spatial and attribute scales among different kinds of objects. Thus, it is not appropriate to segment images with unique or fixed parameters for different kinds of objects. In view of this, this paper presents a stratified object-based farmland extraction method, which includes two key processes: one is image region division on a rough scale and the other is scale parameter pre-estimation within local regions. Firstly, the image in RGB color space is converted into HSV color space, and then the texture features of the hue layer are calculated using the grey level co-occurrence matrix method. Thus, the whole image can be divided into different regions based on the texture features, such as the mean and homogeneity. Secondly, within local regions, the optimal spatial scale segmentation parameter was pre-estimated by average local variance and its first-order and second-order rate of change. The optimal attribute scale segmentation parameter can be estimated based on the histogram of local variance. Through stratified regionalization and local segmentation parameters estimation, fine farmland segmentation can be achieved. GF-2 and Quickbird images were used in this paper, and mean-shift and multi-resolution segmentation algorithms were applied as examples to verify the validity of the proposed method. The experimental results have shown that the stratified processing method can release under-segmentation and over-segmentation phenomena to a certain extent, which ultimately benefits the accurate farmland information extraction.
KW  - remote sensing images
KW  - region division
KW  - scale parameter pre-estimation
KW  - image segmentation
KW  - farmland extraction
DO  - 10.3390/rs11020108
ER  -
TY  - EJOU
AU  - Jiménez López, Jesús
AU  - Mulero-Pázmány, Margarita
TI  - Drones for Conservation in Protected Areas: Present and Future
T2  - Drones

PY  - 2019
VL  - 3
IS  - 1
SN  - 2504-446X

AB  - Park managers call for cost-effective and innovative solutions to handle a wide variety of environmental problems that threaten biodiversity in protected areas. Recently, drones have been called upon to revolutionize conservation and hold great potential to evolve and raise better-informed decisions to assist management. Despite great expectations, the benefits that drones could bring to foster effectiveness remain fundamentally unexplored. To address this gap, we performed a literature review about the use of drones in conservation. We selected a total of 256 studies, of which 99 were carried out in protected areas. We classified the studies in five distinct areas of applications: “wildlife monitoring and management”; “ecosystem monitoring”; “law enforcement”; “ecotourism”; and “environmental management and disaster response”. We also identified specific gaps and challenges that would allow for the expansion of critical research or monitoring. Our results support the evidence that drones hold merits to serve conservation actions and reinforce effective management, but multidisciplinary research must resolve the operational and analytical shortcomings that undermine the prospects for drones integration in protected areas.
KW  - protected areas
KW  - drones
KW  - RPAS
KW  - conservation
KW  - effective management
KW  - biodiversity threats
DO  - 10.3390/drones3010010
ER  -
TY  - EJOU
AU  - Glowacz, Adam
TI  - Fault Detection of Electric Impact Drills and Coffee Grinders Using Acoustic Signals
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - Increasing demand for higher safety of motors can be noticed in recent years. Developing of new fault detection techniques is related with higher safety of motors. This paper presents fault detection technique of an electric impact drill (EID), coffee grinder A (CG-A), and coffee grinder B (CG-B) using acoustic signals. The EID, CG-A, and CG-B use commutator motors. Measurement of acoustic signals of the EID, CG-A, and CG-B was carried out using a microphone. Five signals of the EID are analysed: healthy, with 15 broken rotor blades (faulty fan), with a bent spring, with a shifted brush (motor off), with a rear ball bearing fault. Four signals of the CG-A are analysed: healthy, with a heavily damaged rear sliding bearing, with a damaged shaft and heavily damaged rear sliding bearing, motor off. Three acoustic signals of the CG-B are analysed: healthy, with a light damaged rear sliding bearing, motor off. Methods such as: Root Mean Square (RMS), MSAF-17-MULTIEXPANDED-FILTER-14 are used for feature extraction. The MSAF-17-MULTIEXPANDED-FILTER-14 method is also developed and described in the paper. Classification is carried out using the Nearest Neighbour (NN) classifier. An acoustic based analysis is carried out. The results of the developed method MSAF-17-MULTIEXPANDED-FILTER-14 are very good (total efficiency of recognition of all classes—TED = 96%, TECG-A = 97%, TECG-B = 100%).
KW  - motor
KW  - mechanical fault
KW  - detection
KW  - RMS
KW  - sound
KW  - drill
KW  - safety
KW  - pattern
KW  - bearing
KW  - fan
KW  - shaft
DO  - 10.3390/s19020269
ER  -
TY  - EJOU
AU  - Yang, Shengying
AU  - Qin, Huibin
AU  - Liang, Xiaolin
AU  - Gulliver, Thomas A.
TI  - An Improved Unauthorized Unmanned Aerial Vehicle Detection Algorithm Using Radiofrequency-Based Statistical Fingerprint Analysis
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) are now readily available worldwide and users can easily fly them remotely using smart controllers. This has created the problem of keeping unauthorized UAVs away from private or sensitive areas where they can be a personal or public threat. This paper proposes an improved radio frequency (RF)-based method to detect UAVs. The clutter (interference) is eliminated using a background filtering method. Then singular value decomposition (SVD) and average filtering are used to reduce the noise and improve the signal to noise ratio (SNR). Spectrum accumulation (SA) and statistical fingerprint analysis (SFA) are employed to provide two frequency estimates. These estimates are used to determine if a UAV is present in the detection environment. The data size is reduced using a region of interest (ROI), and this improves the system efficiency and improves azimuth estimation accuracy. Detection results are obtained using real UAV RF signals obtained experimentally which show that the proposed method is more effective than other well-known detection algorithms. The recognition rate with this method is close to 100% within a distance of 2.4 km and greater than 90% within a distance of 3 km. Further, multiple UAVs can be detected accurately using the proposed method.
KW  - spectrum sensing
KW  - radio frequency (RF)
KW  - singular value decomposition (SVD)
KW  - spectrum accumulation (SA)
KW  - statistical fingerprint analysis (SFA)
DO  - 10.3390/s19020274
ER  -
TY  - EJOU
AU  - Chen, Xi
AU  - Kopsaftopoulos, Fotis
AU  - Wu, Qi
AU  - Ren, He
AU  - Chang, Fu-Kuo
TI  - A Self-Adaptive 1D Convolutional Neural Network for Flight-State Identification
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - The vibration of a wing structure in the air reflects coupled aerodynamic&ndash;mechanical responses under varying flight states that are defined by the angle of attack and airspeed. It is of great challenge to identify the flight state from the complex vibration signals. In this paper, a novel one-dimension convolutional neural network (CNN) is developed, which is able to automatically extract useful features from the structural vibration of a recently fabricated self-sensing wing through wind-tunnel experiments. The obtained signals are firstly decomposed into various subsignals with different frequency bands via dual-tree complex-wavelet packet transformation. Then, the reconstructed subsignals are selected to form the best combination for multichannel inputs of the CNN. A swarm-based evolutionary algorithm called grey-wolf optimizer is utilized to optimize a set of key parameters of the CNN, which saves considerable human efforts. Two case studies demonstrate the high identification accuracy and robustness of the proposed method over standard deep-learning methods in flight-state identification, thus providing new perspectives in self-awareness toward the next generation of intelligent air vehicles.
KW  - self-sensing wing
KW  - dual-tree complex-wavelet packet transformation
KW  - convolution neural network
KW  - grey-wolf optimizer
KW  - flight-state identification
DO  - 10.3390/s19020275
ER  -
TY  - EJOU
AU  - Shi, Xiaoran
AU  - Zhou, Feng
AU  - Yang, Shuang
AU  - Zhang, Zijing
AU  - Su, Tao
TI  - Automatic Target Recognition for Synthetic Aperture Radar Images Based on Super-Resolution Generative Adversarial Network and Deep Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 2
SN  - 2072-4292

AB  - Aiming at the problem of the difficulty of high-resolution synthetic aperture radar (SAR) image acquisition and poor feature characterization ability of low-resolution SAR image, this paper proposes a method of an automatic target recognition method for SAR images based on a super-resolution generative adversarial network (SRGAN) and deep convolutional neural network (DCNN). First, the threshold segmentation is utilized to eliminate the SAR image background clutter and speckle noise and accurately extract target area of interest. Second, the low-resolution SAR image is enhanced through SRGAN to improve the visual resolution and the feature characterization ability of target in the SAR image. Third, the automatic classification and recognition for SAR image is realized by using DCNN with good generalization performance. Finally, the open data set, moving and stationary target acquisition and recognition, is utilized and good recognition results are obtained under standard operating condition and extended operating conditions, which verify the effectiveness, robustness, and good generalization performance of the proposed method.
KW  - synthetic aperture radar (SAR)
KW  - automatic target recognition (ATR)
KW  - image segmentation
KW  - super-resolution generative adversarial network (SRGAN)
KW  - deep convolutional neural network (DCNN)
DO  - 10.3390/rs11020135
ER  -
TY  - EJOU
AU  - Zhuo, Xiangyu
AU  - Fraundorfer, Friedrich
AU  - Kurz, Franz
AU  - Reinartz, Peter
TI  - Automatic Annotation of Airborne Images by Label Propagation Based on a Bayesian-CRF Model
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 2
SN  - 2072-4292

AB  - The tremendous advances in deep neural networks have demonstrated the superiority of deep learning techniques for applications such as object recognition or image classification. Nevertheless, deep learning-based methods usually require a large amount of training data, which mainly comes from manual annotation and is quite labor-intensive. In order to reduce the amount of manual work required for generating enough training data, we hereby propose to leverage existing labeled data to generate image annotations automatically. Specifically, the pixel labels are firstly transferred from one image modality to another image modality via geometric transformation to create initial image annotations, and then additional information (e.g., height measurements) is incorporated for Bayesian inference to update the labeling beliefs. Finally, the updated label assignments are optimized with a fully connected conditional random field (CRF), yielding refined labeling for all pixels in the image. The proposed approach is tested on two different scenarios, i.e., (1) label propagation from annotated aerial imagery to unmanned aerial vehicle (UAV) imagery and (2) label propagation from map database to aerial imagery. In each scenario, the refined image labels are used as pseudo-ground truth data for training a convolutional neural network (CNN). Results demonstrate that our model is able to produce accurate label assignments even around complex object boundaries; besides, the generated image labels can be effectively leveraged for training CNNs and achieve comparable classification accuracy as manual image annotations, more specifically, the per-class classification accuracy of the networks trained by the manual image annotations and the generated image labels have a difference within     &plusmn; 5 %    .
KW  - automatic image annotation
KW  - label propagation
KW  - Conditional Random Field (CRF)
KW  - Convolutional Neural Network (CNN)
DO  - 10.3390/rs11020145
ER  -
TY  - EJOU
AU  - Gao, Pengbo
AU  - Zhang, Yan
AU  - Zhang, Linhuan
AU  - Noguchi, Ryozo
AU  - Ahamed, Tofael
TI  - Development of a Recognition System for Spraying Areas from Unmanned Aerial Vehicles Using a Machine Learning Approach
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - Unmanned aerial vehicle (UAV)-based spraying systems have recently become important for the precision application of pesticides, using machine learning approaches. Therefore, the objective of this research was to develop a machine learning system that has the advantages of high computational speed and good accuracy for recognizing spray and non-spray areas for UAV-based sprayers. A machine learning system was developed by using the mutual subspace method (MSM) for images collected from a UAV. Two target lands: agricultural croplands and orchard areas, were considered in building two classifiers for distinguishing spray and non-spray areas. The field experiments were conducted in target areas to train and test the system by using a commercial UAV (DJI Phantom 3 Pro) with an onboard 4K camera. The images were collected from low (5 m) and high (15 m) altitudes for croplands and orchards, respectively. The recognition system was divided into offline and online systems. In the offline recognition system, 74.4% accuracy was obtained for the classifiers in recognizing spray and non-spray areas for croplands. In the case of orchards, the average classifier recognition accuracy of spray and non-spray areas was 77%. On the other hand, the online recognition system performance had an average accuracy of 65.1% for croplands, and 75.1% for orchards. The computational time for the online recognition system was minimal, with an average of 0.0031 s for classifier recognition. The developed machine learning system had an average recognition accuracy of 70%, which can be implemented in an autonomous UAV spray system for recognizing spray and non-spray areas for real-time applications.
KW  - precision agriculture
KW  - recognition system
KW  - image classifiers
KW  - machine learning system
KW  - mutual subspace method
DO  - 10.3390/s19020313
ER  -
TY  - EJOU
AU  - Feng, Quanlong
AU  - Zhu, Dehai
AU  - Yang, Jianyu
AU  - Li, Baoguo
TI  - Multisource Hyperspectral and LiDAR Data Fusion for Urban Land-Use Mapping based on a Modified Two-Branch Convolutional Neural Network
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 1
SN  - 2220-9964

AB  - Accurate urban land-use mapping is a challenging task in the remote-sensing field. With the availability of diverse remote sensors, synthetic use and integration of multisource data provides an opportunity for improving urban land-use classification accuracy. Neural networks for Deep Learning have achieved very promising results in computer-vision tasks, such as image classification and object detection. However, the problem of designing an effective deep-learning model for the fusion of multisource remote-sensing data still remains. To tackle this issue, this paper proposes a modified two-branch convolutional neural network for the adaptive fusion of hyperspectral imagery (HSI) and Light Detection and Ranging (LiDAR) data. Specifically, the proposed model consists of a HSI branch and a LiDAR branch, sharing the same network structure to reduce the time cost of network design. A residual block is utilized in each branch to extract hierarchical, parallel, and multiscale features. An adaptive-feature fusion module is proposed to integrate HSI and LiDAR features in a more reasonable and natural way (based on &ldquo;Squeeze-and-Excitation Networks&rdquo;). Experiments indicate that the proposed two-branch network shows good performance, with an overall accuracy of almost 92%. Compared with single-source data, the introduction of multisource data improves accuracy by at least 8%. The adaptive fusion model can also increase classification accuracy by more than 3% when compared with the feature-stacking method (simple concatenation). The results demonstrate that the proposed network can effectively extract and fuse features for a better urban land-use mapping accuracy.
KW  - convolutional neural networks
KW  - multisource data
KW  - feature fusion
KW  - urban land-use mapping
DO  - 10.3390/ijgi8010028
ER  -
TY  - EJOU
AU  - Du, Ming
AU  - Ding, Yan
AU  - Meng, Xiuyun
AU  - Wei, Hua-Liang
AU  - Zhao, Yifan
TI  - Distractor-Aware Deep Regression for Visual Tracking
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - In recent years, regression trackers have drawn increasing attention in the visual-object tracking community due to their favorable performance and easy implementation. The tracker algorithms directly learn mapping from dense samples around the target object to Gaussian-like soft labels. However, in many real applications, when applied to test data, the extreme imbalanced distribution of training samples usually hinders the robustness and accuracy of regression trackers. In this paper, we propose a novel effective distractor-aware loss function to balance this issue by highlighting the significant domain and by severely penalizing the pure background. In addition, we introduce a full differentiable hierarchy-normalized concatenation connection to exploit abstractions across multiple convolutional layers. Extensive experiments were conducted on five challenging benchmark-tracking datasets, that is, OTB-13, OTB-15, TC-128, UAV-123, and VOT17. The experimental results are promising and show that the proposed tracker performs much better than nearly all the compared state-of-the-art approaches.
KW  - object tracking
KW  - deep-regression networks
KW  - data imbalance
KW  - distractor aware
DO  - 10.3390/s19020387
ER  -
TY  - EJOU
AU  - Liu, Wei
AU  - Cheng, Dayu
AU  - Yin, Pengcheng
AU  - Yang, Mengyuan
AU  - Li, Erzhu
AU  - Xie, Meng
AU  - Zhang, Lianpeng
TI  - Small Manhole Cover Detection in Remote Sensing Imagery with Deep Convolutional Neural Networks
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 1
SN  - 2220-9964

AB  - With the development of remote sensing technology and the advent of high-resolution images, obtaining data has become increasingly convenient. However, the acquisition of small manhole cover information still has shortcomings including low efficiency of manual surveying and high leakage rate. Recently, deep learning models, especially deep convolutional neural networks (DCNNs), have proven to be effective at object detection. However, several challenges limit the applications of DCNN in manhole cover object detection using remote sensing imagery: (1) Manhole cover objects often appear at different scales in remotely sensed images and DCNNs&rsquo; fixed receptive field cannot match the scale variability of such objects; (2) Manhole cover objects in large-scale remotely-sensed images are relatively small in size and densely packed, while DCNNs have poor localization performance when applied to such objects. To address these problems, we propose an effective method for detecting manhole cover objects in remotely-sensed images. First, we redesign the feature extractor by adopting the visual geometry group (VGG), which can increase the variety of receptive field size. Then, detection is performed using two sub-networks: a multi-scale output network (MON) for manhole cover object-like edge generation from several intermediate layers whose receptive fields match different object scales and a multi-level convolution matching network (M-CMN) for object detection based on fused feature maps, which combines several feature maps that enable small and densely packed manhole cover objects to produce a stronger response. The results show that our method is more accurate than existing methods at detecting manhole covers in remotely-sensed images.
KW  - manhole cover
KW  - remote sensing images
KW  - object detection
KW  - deep convolutional neural networks
DO  - 10.3390/ijgi8010049
ER  -
TY  - EJOU
AU  - Zhang, Lei
AU  - Zhai, Zhengjun
AU  - He, Lang
AU  - Wen, Pengcheng
AU  - Niu, Wensheng
TI  - Infrared-Inertial Navigation for Commercial Aircraft Precision Landing in Low Visibility and GPS-Denied Environments
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 2
SN  - 1424-8220

AB  - This paper proposes a novel infrared-inertial navigation method for the precise landing of commercial aircraft in low visibility and Global Position System (GPS)-denied environments. Within a Square-root Unscented Kalman Filter (SR_UKF), inertial measurement unit (IMU) data, forward-looking infrared (FLIR) images and airport geo-information are integrated to estimate the position, velocity and attitude of the aircraft during landing. Homography between the synthetic image and the real image which implicates the camera pose deviations is created as vision measurement. To accurately extract real runway features, the current results of runway detection are used as the prior knowledge for the next frame detection. To avoid possible homography decomposition solutions, it is directly converted to a vector and fed to the SR_UKF. Moreover, the proposed navigation system is proven to be observable by nonlinear observability analysis. Last but not least, a general aircraft was elaborately equipped with vision and inertial sensors to collect flight data for algorithm verification. The experimental results have demonstrated that the proposed method could be used for the precise landing of commercial aircraft in low visibility and GPS-denied environments.
KW  - infrared-inertial navigation
KW  - homography
KW  - runway detection
KW  - observability analysis
KW  - precise landing
KW  - low visibility
KW  - GPS-denied
DO  - 10.3390/s19020408
ER  -
TY  - EJOU
AU  - Oki, Takuro
AU  - Miyamoto, Ryusuke
AU  - Yomo, Hiroyuki
AU  - Hara, Shinsuke
TI  - Detection Accuracy of Soccer Players in Aerial Images Captured from Several Viewpoints
T2  - Journal of Functional Morphology and Kinesiology

PY  - 2019
VL  - 4
IS  - 1
SN  - 2411-5142

AB  - In the fields of professional and amateur sports, players&rsquo; health, physical and physiological conditions during exercise should be properly monitored and managed. The authors of this paper previously proposed a real-time vital-sign monitoring system for players using a wireless multi-hop sensor network that transmits their vital data. However, existing routing schemes based on the received signal strength indicator or global positioning system do not work well, because of the high speeds and the density of sensor nodes attached to players. To solve this problem, we proposed a novel scheme, image-assisted routing (IAR), which estimates the locations of sensor nodes using images captured from cameras mounted on unmanned aerial vehicles. However, it is not clear where the best viewpoints are for aerial player detection. In this study, the authors investigated detection accuracy from several viewpoints using an aerial-image dataset generated with computer graphics. Experimental results show that the detection accuracy was best when the viewpoints were slightly distant from just above the center of the field. In the best case, the detection accuracy was very good: 0.005524 miss rate at 0.01 false positive-per-image. These results are informative for player detection using aerial images and can facilitate to realize IAR.
KW  - player detection
KW  - aerial images
KW  - informed-filters
DO  - 10.3390/jfmk4010009
ER  -
TY  - EJOU
AU  - Lei, Songze
AU  - Zhang, Boxing
AU  - Wang, Yanhong
AU  - Dong, Baihua
AU  - Li, Xiaoping
AU  - Xiao, Feng
TI  - Object Recognition Using Non-Negative Matrix Factorization with Sparseness Constraint and Neural Network
T2  - Information

PY  - 2019
VL  - 10
IS  - 2
SN  - 2078-2489

AB  - UAVs (unmanned aerial vehicles) have been widely used in many fields, where they need to be detected and controlled. Small-sample UAV recognition requires an effective detecting and recognition method. When identifying a UAV target using the backward propagation (BP) neural network, fully connected neurons of BP neural network and the high-dimensional input features will generate too many weights for training, induce complex network structure, and poor recognition performance. In this paper, a novel recognition method based on non-negative matrix factorization (NMF) with sparseness constraint feature dimension reduction and BP neural network is proposed for the above difficulties. The Edgeboxes are used for candidate regions and Log-Gabor features are extracted in candidate target regions. In order to avoid the complexity of the matrix operation with the high-dimensional Log-Gabor features, preprocessing for feature reduction by downsampling is adopted, which makes the NMF fast and the feature discriminative. The classifier is trained by neural network with the feature of dimension reduction. The experimental results show that the method is better than the traditional methods of dimension reduction, such as PCA (principal component analysis), FLD (Fisher linear discrimination), LPP (locality preserving projection), and KLPP (kernel locality preserving projection), and can identify the UAV target quickly and accurately.
KW  - non-negative matrix factor (NMF)
KW  - sparseness constraint
KW  - back propagation (BP) neural network
KW  - feature dimension reduction
DO  - 10.3390/info10020037
ER  -
TY  - EJOU
AU  - Pham, Tien D.
AU  - Yokoya, Naoto
AU  - Bui, Dieu T.
AU  - Yoshino, Kunihiko
AU  - Friess, Daniel A.
TI  - Remote Sensing Approaches for Monitoring Mangrove Species, Structure, and Biomass: Opportunities and Challenges
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - The mangrove ecosystem plays a vital role in the global carbon cycle, by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, mangroves have been lost worldwide, resulting in substantial carbon stock losses. Additionally, some aspects of the mangrove ecosystem remain poorly characterized compared to other forest ecosystems due to practical difficulties in measuring and monitoring mangrove biomass and their carbon stocks. Without a quantitative method for effectively monitoring biophysical parameters and carbon stocks in mangroves, robust policies and actions for sustainably conserving mangroves in the context of climate change mitigation and adaptation are more difficult. In this context, remote sensing provides an important tool for monitoring mangroves and identifying attributes such as species, biomass, and carbon stocks. A wide range of studies is based on optical imagery (aerial photography, multispectral, and hyperspectral) and synthetic aperture radar (SAR) data. Remote sensing approaches have been proven effective for mapping mangrove species, estimating their biomass, and assessing changes in their extent. This review provides an overview of the techniques that are currently being used to map various attributes of mangroves, summarizes the studies that have been undertaken since 2010 on a variety of remote sensing applications for monitoring mangroves, and addresses the limitations of these studies. We see several key future directions for the potential use of remote sensing techniques combined with machine learning techniques for mapping mangrove areas and species, and evaluating their biomass and carbon stocks.
KW  - mangrove species
KW  - mapping
KW  - biomass
KW  - blue carbon
KW  - machine learning
KW  - REDD+
DO  - 10.3390/rs11030230
ER  -
TY  - EJOU
AU  - Memon, Mudasar L.
AU  - Saxena, Navrati
AU  - Roy, Abhishek
AU  - Shin, Dong R.
TI  - Backscatter Communications: Inception of the Battery-Free Era—A Comprehensive Survey
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 2
SN  - 2079-9292

AB  - The ever increasing proliferation of wireless objects and consistent connectivity demands are creating significant challenges for battery-constrained wireless devices. The vision of massive IoT, involving billions of smart objects to be connected to the cellular network, needs to address the problem of uninterrupted power consumption while taking advantage of emerging high-frequency 5G communications. The problem of limited battery power motivates us to utilize radio frequency (RF) signals as the energy source for battery-free communications in next-generation wireless networks. Backscatter communication (BackCom) makes it possible to harvest energy from incident RF signals and reflect back parts of the same signals while modulating the data. Ambient BackCom (Amb-BackCom) is a type of BackCom that can harvest energy from nearby WiFi, TV, and cellular RF signals to modulate information. The objective of this article is to review BackCom as a solution to the limited battery life problem and enable future battery-free communications for combating the energy issues for devices in emerging wireless networks. We first highlight the energy constraint in existing wireless communications. We then investigate BackCom as a practical solution to the limited battery life problem. Subsequently, in order to take the advantages of omnipresent radio waves, we elaborate BackCom tag architecture and various types of BackCom. To understand encoding and data extraction, we demonstrate signal processing aspects that cover channel coding, interference, decoding, and signal detection schemes. Moreover, we also describe BackCom communication modes, modulation schemes, and multiple access techniques to accommodate maximum users with high throughput. Similarly, to mitigate the increased network energy, adequate data and power transfer schemes for BackCom are elaborated, in addition to reliability, security, and range extension. Finally, we highlight BackCom applications with existing research challenges and future directions for next-generation 5G wireless networks.
KW  - backscatter communications
KW  - ambient backscatter
KW  - battery-free communications
KW  - wireless communications
KW  - energy harvesting
KW  - mm-waves
KW  - IoT
KW  - 5G
DO  - 10.3390/electronics8020129
ER  -
TY  - EJOU
AU  - Rasti, Pejman
AU  - Ahmad, Ali
AU  - Samiei, Salma
AU  - Belin, Etienne
AU  - Rousseau, David
TI  - Supervised Image Classification by Scattering Transform with Application to Weed Detection in Culture Crops of High Density
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - In this article, we assess the interest of the recently introduced multiscale scattering transform for texture classification applied for the first time in plant science. Scattering transform is shown to outperform monoscale approaches (gray-level co-occurrence matrix, local binary patterns) but also multiscale approaches (wavelet decomposition) which do not include combinatory steps. The regime in which scatter transform also outperforms a standard CNN architecture in terms of data-set size is evaluated (    10 4     instances). An approach on how to optimally design the scatter transform based on energy contrast is provided. This is illustrated on the hard and open problem of weed detection in culture crops of high density from the top view in intensity images. An annotated synthetic data-set available under the form of a data challenge and a simulator are proposed for reproducible science. Scatter transform only trained on synthetic data shows an accuracy of     85 %     when tested on real data.
KW  - weed detection
KW  - scatter transform
KW  - deep learning
KW  - machine-learning classification
KW  - annotation
KW  - synthetic data
KW  - local binary pattern
DO  - 10.3390/rs11030249
ER  -
TY  - EJOU
AU  - Wang, Xiaohong
AU  - Fan, Wenhui
AU  - Li, Xinjun
AU  - Wang, Lizhi
TI  - Weak Degradation Characteristics Analysis of UAV Motors Based on Laplacian Eigenmaps and Variational Mode Decomposition
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Brushless direct current (BLDC) motors are the source of flight power during the operation of rotary-wing unmanned aerial vehicles (UAVs), and their working state directly affects the safety of the whole system. To predict and avoid motor faults, it is necessary to accurately understand the health degradation process of the motor before any fault occurs. However, in actual working conditions, due to the aerodynamic environmental conditions of the aircraft flight, the background noise components of the vibration signals characterizing the running state of the motor are complex and severely coupled, making it difficult for the weak degradation characteristics to be clearly reflected. To address these problems, a weak degradation characteristic extraction method based on variational mode decomposition (VMD) and Laplacian Eigenmaps (LE) was proposed in this study to precisely identify the degradation information in system health data, avoid the loss of critical information and the interference of redundant information, and to optimize the description of a motor&rsquo;s degradation process despite the presence of complex background noise. A validation experiment was conducted on a specific type of motor under operation with load, to obtain the degradation characteristics of multiple types of vibration signals, and to test the proposed method. The results proved that this method can improve the stability and accuracy of predicting motor health, thereby helping to predict the degradation state and to optimize the maintenance strategies.
KW  - variational mode decomposition
KW  - Laplacian eigenmaps
KW  - multi-rotor unmanned aerial vehicle
KW  - brushless direct current motor
KW  - weak degradation characteristics
DO  - 10.3390/s19030524
ER  -
TY  - EJOU
AU  - Li, Xuelong
AU  - Yuan, Zhenghang
AU  - Wang, Qi
TI  - Unsupervised Deep Noise Modeling for Hyperspectral Image Change Detection
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Hyperspectral image (HSI) change detection plays an important role in remote sensing applications, and considerable research has been done focused on improving change detection performance. However, the high dimension of hyperspectral data makes it hard to extract discriminative features for hyperspectral processing tasks. Though deep convolutional neural networks (CNN) have superior capability in high-level semantic feature learning, it is difficult to employ CNN for change detection tasks. As a ground truth map is usually used for the evaluation of change detection algorithms, it cannot be directly used for supervised learning. In order to better extract discriminative CNN features, a novel noise modeling-based unsupervised fully convolutional network (FCN) framework is presented for HSI change detection in this paper. Specifically, the proposed method utilizes the change detection maps of existing unsupervised change detection methods to train the deep CNN, and then removes the noise during the end-to-end training process. The main contributions of this paper are threefold: (1) A new end-to-end FCN-based deep network architecture for HSI change detection is presented with powerful learning features; (2) An unsupervised noise modeling method is introduced for the robust training of the proposed deep network; (3) Experimental results on three datasets confirm the effectiveness of the proposed method.
KW  - hyperspectral images (HSI)
KW  - change detection
KW  - deep learning
KW  - convolutional neural networks (CNN)
KW  - noise modeling
DO  - 10.3390/rs11030258
ER  -
TY  - EJOU
AU  - Mo, Nan
AU  - Yan, Li
AU  - Zhu, Ruixi
AU  - Xie, Hong
TI  - Class-Specific Anchor Based and Context-Guided Multi-Class Object Detection in High Resolution Remote Sensing Imagery with a Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - In this paper, the problem of multi-scale geospatial object detection in High Resolution Remote Sensing Images (HRRSI) is tackled. The different flight heights, shooting angles and sizes of geographic objects in the HRRSI lead to large scale variance in geographic objects. The inappropriate anchor size to propose the objects and the indiscriminative ability of features for describing the objects are the main causes of missing detection and false detection in multi-scale geographic object detection. To address these challenges, we propose a class-specific anchor based and context-guided multi-class object detection method with a convolutional neural network (CNN), which can be divided into two parts: a class-specific anchor based region proposal network (RPN) and a discriminative feature with a context information classification network. A class-specific anchor block providing better initial values for RPN is proposed to generate the anchor of the most suitable scale for each category in order to increase the recall ratio. Meanwhile, we proposed to incorporate the context information into the original convolutional feature to improve the discriminative ability of the features and increase classification accuracy. Considering the quality of samples for classification, the soft filter is proposed to select effective boxes to improve the diversity of the samples for the classifier and avoid missing or false detection to some extent. We also introduced the focal loss in order to improve the classifier in classifying the hard samples. The proposed method is tested on a benchmark dataset of ten classes to prove the superiority. The proposed method outperforms some state-of-the-art methods with a mean average precision (mAP) of 90.4% and better detects the multi-scale objects, especially when objects show a minor shape change.
KW  - multi-scale geospatial object detection
KW  - class-specific anchor
KW  - discriminative feature with context information
KW  - focal loss
KW  - soft filter
DO  - 10.3390/rs11030272
ER  -
TY  - EJOU
AU  - Gao, Jianwei
AU  - Sun, Yi
AU  - Zhang, Bing
AU  - Chen, Zhengchao
AU  - Gao, Lianru
AU  - Zhang, Wenjuan
TI  - Multi-GPU Based Parallel Design of the Ant Colony Optimization Algorithm for Endmember Extraction from Hyperspectral Images
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Spectral unmixing is a vital procedure in hyperspectral remote sensing image exploitation. The linear mixture model has been widely utilized to unmix hyperspectral images by extracting a set of pure spectral signatures, called endmembers in hyperspectral jargon, and estimating their respective fractional abundances in each pixel of the scene. Many algorithms have been proposed to extract endmembers automatically, which is a critical step in the spectral unmixing chain. In recent years, the ant colony optimization (ACO) algorithm has been developed for endmember extraction from hyperspectral data, which was regarded as a combinatorial optimization problem. Although the ACO for endmember extraction (ACOEE) can acquire accurate endmember results, its high computational complexity has limited its application in the hyperspectral data analysis. The GPUs parallel computing technique can be utilized to improve the computational performance of ACOEE, but the architecture of GPUs determines that the ACOEE should be redesigned to take full advantage of computing resources on GPUs. In this paper, a multiple sub-ant-colony-based parallel design of ACOEE was proposed, in which an innovative mechanism of local pheromone for sub-ant-colonies is utilized to enable ACOEE to be preferably executed on the multi-GPU system. The proposed method can avoid much synchronization among different GPUs to affect the computational performance improvement. The experiments on two real hyperspectral datasets demonstrated that the computational performance of ACOEE significantly benefited from the proposed methods.
KW  - hyperspectral images
KW  - endmember extraction
KW  - multi-GPU
KW  - ant colony optimization (ACO)
KW  - parallel computing
DO  - 10.3390/s19030598
ER  -
TY  - EJOU
AU  - Fu, Yongyong
AU  - Liu, Kunkun
AU  - Shen, Zhangquan
AU  - Deng, Jinsong
AU  - Gan, Muye
AU  - Liu, Xinguo
AU  - Lu, Dongming
AU  - Wang, Ke
TI  - Mapping Impervious Surfaces in Town–Rural Transition Belts Using China’s GF-2 Imagery and Object-Based Deep CNNs
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Impervious surfaces play an important role in urban planning and sustainable environmental management. High-spatial-resolution (HSR) images containing pure pixels have significant potential for the detailed delineation of land surfaces. However, due to high intraclass variability and low interclass distance, the mapping and monitoring of impervious surfaces in complex town&ndash;rural areas using HSR images remains a challenge. The fully convolutional network (FCN) model, a variant of convolution neural networks (CNNs), recently achieved state-of-the-art performance in HSR image classification applications. However, due to the inherent nature of FCN processing, it is challenging for an FCN to precisely capture the detailed information of classification targets. To solve this problem, we propose an object-based deep CNN framework that integrates object-based image analysis (OBIA) with deep CNNs to accurately extract and estimate impervious surfaces. Specifically, we also adopted two widely used transfer learning technologies to expedite the training of deep CNNs. Finally, we compare our approach with conventional OBIA classification and state-of-the-art FCN-based methods, such as FCN-8s and the U-Net methods. Both of these FCN-based methods are well designed for pixel-wise classification applications and have achieved great success. Our results show that the proposed approach effectively identified impervious surfaces, with 93.9% overall accuracy. Compared with the existing methods, i.e., OBIA, FCN-8s and U-Net methods, it shows that our method achieves obviously improvement in accuracy. Our findings also suggest that the classification performance of our proposed method is related to training strategy, indicating that significantly higher accuracy can be achieved through transfer learning by fine-tuning rather than feature extraction. Our approach for the automatic extraction and mapping of impervious surfaces also lays a solid foundation for intelligent monitoring and the management of land use and land cover.
KW  - transfer learning
KW  - remote sensing
KW  - deep learning
KW  - object-based image analysis (OBIA)
DO  - 10.3390/rs11030280
ER  -
TY  - EJOU
AU  - Gebremedhin, Alem
AU  - Badenhorst, Pieter E.
AU  - Wang, Junping
AU  - Spangenberg, German C.
AU  - Smith, Kevin F.
TI  - Prospects for Measurement of Dry Matter Yield in Forage Breeding Programs Using Sensor Technologies
T2  - Agronomy

PY  - 2019
VL  - 9
IS  - 2
SN  - 2073-4395

AB  - Increasing the yield of perennial forage crops remains a crucial factor underpinning the profitability of grazing industries, and therefore is a priority for breeding programs. Breeding for high dry matter yield (DMY) in forage crops is likely to be enhanced with the development of genomic selection (GS) strategies. However, realising the full potential of GS will require an increase in the amount of phenotypic data and the rate at which it is collected. Therefore, phenotyping remains a critical bottleneck in the implementation of GS in forage species. Assessments of DMY in forage crop breeding include visual scores, sample clipping and mowing of plots, which are often costly and time-consuming. New ground- and aerial-based platforms equipped with advanced sensors offer opportunities for fast, nondestructive and low-cost, high-throughput phenotyping (HTP) of plant growth, development and yield in a field environment. The workflow of image acquisition, processing and analysis are reviewed. The &ldquo;big data&rdquo; challenges, proposed storage and management techniques, development of advanced statistical tools and methods for incorporating the HTP into forage breeding systems are also reviewed. Initial results where these techniques have been applied to forages have been promising but further research and development is required to adapt them to forage breeding situations, particularly with respect to the management of large data sets and the integration of information from spaced plants to sward plots. However, realizing the potential of sensor technologies combined with GS leads to greater rates of genetic gain in forages.
KW  - forage dry matter yield
KW  - high-throughput phenotyping
KW  - automation
KW  - imaging and image analysis
DO  - 10.3390/agronomy9020065
ER  -
TY  - EJOU
AU  - Marques, Gonçalo
AU  - Pitarma, Rui
TI  - A Cost-Effective Air Quality Supervision Solution for Enhanced Living Environments through the Internet of Things
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 2
SN  - 2079-9292

AB  - We spend about 90% of our lives in indoor living environments. Thus, it is essential to provide indoor air quality monitoring for enhanced living environments. Advances in networking, sensors, and embedded devices have made monitoring and supply of assistance possible to people in their homes. Technological advancements have made possible the building of smart devices with significant capabilities for sensing and connecting, but also provide several improvements in ambient assisted living system architectures. Indoor air quality assumes an important role in building productive and healthy indoor environments. In this paper, the authors present an Internet of Things system for real-time indoor air quality monitoring named iAir. This system is composed by an ESP8266 as the communication and processing unit and a MICS-6814 sensor as the sensing unit. The MICS-6814 is a metal oxide semiconductor sensor capable of detecting several gases such as carbon monoxide, nitrogen dioxide, ethanol, methane, and propane. The iAir system also provides a smartphone application for data consulting and real-time notifications. Compared to other solutions, the iAir system is based on open-source technologies and operates as a totally Wi-Fi system, with several advantages such as its modularity, scalability, low cost, and easy installation. The results obtained are very promising, representing a meaningful contribution for enhanced living environments as iAir provides real-time monitoring for enhanced ambient assisted living and occupational health.
KW  - ambient assisted living (AAL)
KW  - enhanced living environments
KW  - health monitoring
KW  - indoor air quality (IAQ)
KW  - IoT (Internet of Things)
KW  - occupational health
KW  - smart cities
DO  - 10.3390/electronics8020170
ER  -
TY  - EJOU
AU  - Jiang, Changhui
AU  - Chen, Yuwei
AU  - Chen, Shuai
AU  - Bo, Yuming
AU  - Li, Wei
AU  - Tian, Wenxin
AU  - Guo, Jun
TI  - A Mixed Deep Recurrent Neural Network for MEMS Gyroscope Noise Suppressing
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 2
SN  - 2079-9292

AB  - Currently, positioning, navigation, and timing information is becoming more and more vital for both civil and military applications. Integration of the global navigation satellite system and /inertial navigation system is the most popular solution for various carriers or vehicle positioning. As is well-known, the global navigation satellite system positioning accuracy will degrade in signal challenging environments. Under this condition, the integration system will fade to a standalone inertial navigation system outputting navigation solutions. However, without outer aiding, positioning errors of the inertial navigation system diverge quickly due to the noise contained in the raw data of the inertial measurement unit. In particular, the micromechanics system inertial measurement unit experiences more complex errors due to the manufacturing technology. To improve the navigation accuracy of inertial navigation systems, one effective approach is to model the raw signal noise and suppress it. Commonly, an inertial measurement unit is composed of three gyroscopes and three accelerometers, among them, the gyroscopes play an important role in the accuracy of the inertial navigation system&rsquo;s navigation solutions. Motivated by this problem, in this paper, an advanced deep recurrent neural network was employed and evaluated in noise modeling of a micromechanics system gyroscope. Specifically, a deep long short term memory recurrent neural network and a deep gated recurrent unit&ndash;recurrent neural network were combined together to construct a two-layer recurrent neural network for noise modeling. In this method, the gyroscope data were treated as a time series, and a real dataset from a micromechanics system inertial measurement unit was employed in the experiments. The results showed that, compared to the two-layer long short term memory, the three-axis attitude errors of the mixed long short term memory&ndash;gated recurrent unit decreased by 7.8%, 20.0%, and 5.1%. When compared with the two-layer gated recurrent unit, the proposed method showed 15.9%, 14.3%, and 10.5% improvement. These results supported a positive conclusion on the performance of designed method, specifically, the mixed deep recurrent neural networks outperformed than the two-layer gated recurrent unit and the two-layer long short term memory recurrent neural networks.
KW  - global navigation satellite system (GNSS)
KW  - inertial navigation system (INS)
KW  - long short term memory (LSTM)
KW  - gated recurrent unit (GRU)
KW  - microelectronics system (MEMS)
DO  - 10.3390/electronics8020181
ER  -
TY  - EJOU
AU  - Freudenberg, Maximilian
AU  - Nölke, Nils
AU  - Agostini, Alejandro
AU  - Urban, Kira
AU  - Wörgötter, Florentin
AU  - Kleinn, Christoph
TI  - Large Scale Palm Tree Detection in High Resolution Satellite Images Using U-Net
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Oil and coconut palm trees are important crops in many tropical countries, which are either planted as plantations or scattered in the landscape. Monitoring in terms of counting provides useful information for various stakeholders. Most of the existing monitoring methods are based on spectral profiles or simple neural networks and either fall short in terms of accuracy or speed. We use a neural network of the U-Net type in order to detect oil and coconut palms on very high resolution satellite images. The method is applied to two different study areas: (1) large monoculture oil palm plantations in Jambi, Indonesia, and (2) coconut palms in the Bengaluru Metropolitan Region in India. The results show that the proposed method reaches a performance comparable to state of the art approaches, while being about one order of magnitude faster. We reach a maximum throughput of 235 ha/s with a spatial image resolution of 40 cm. The proposed method proves to be reliable even under difficult conditions, such as shadows or urban areas, and can easily be transferred from one region to another. The method detected palms with accuracies between 89% and 92%.
KW  - U-Net
KW  - WorldView
KW  - CNN
KW  - segmentation
KW  - palm tree
KW  - deep learning
DO  - 10.3390/rs11030312
ER  -
TY  - EJOU
AU  - Salamí, Esther
AU  - Gallardo, Antonia
AU  - Skorobogatov, Georgy
AU  - Barrado, Cristina
TI  - On-the-Fly Olive Tree Counting Using a UAS and Cloud Services
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Unmanned aerial systems (UAS) are becoming a common tool for aerial sensing applications. Nevertheless, sensed data need further processing before becoming useful information. This processing requires large computing power and time before delivery. In this paper, we present a parallel architecture that includes an unmanned aerial vehicle (UAV), a small embedded computer on board, a communication link to the Internet, and a cloud service with the aim to provide useful real-time information directly to the end-users. The potential of parallelism as a solution in remote sensing has not been addressed for a distributed architecture that includes the UAV processors. The architecture is demonstrated for a specific problem: the counting of olive trees in a crop field where the trees are regularly spaced from each other. During the flight, the embedded computer is able to process individual images on board the UAV and provide the total count. The tree counting algorithm obtains an     F 1     score of     99.09 %     for a sequence of ten images with 332 olive trees. The detected trees are geolocated and can be visualized on the Internet seconds after the take-off of the flight, with no further processing required. This is a use case to demonstrate near real-time results obtained from UAS usage. Other more complex UAS applications, such as tree inventories, search and rescue, fire detection, or stock breeding, can potentially benefit from this architecture and obtain faster outcomes, accessible while the UAV is still on flight.
KW  - UAS
KW  - UAV
KW  - image segmentation
KW  - tree counting
KW  - distributed services
KW  - cloud computing
DO  - 10.3390/rs11030316
ER  -
TY  - EJOU
AU  - Liu, Chang
AU  - Shirowzhan, Sara
AU  - Sepasgozar, Samad M. E.
AU  - Kaboli, Ali
TI  - Evaluation of Classical Operators and Fuzzy Logic Algorithms for Edge Detection of Panels at Exterior Cladding of Buildings
T2  - Buildings

PY  - 2019
VL  - 9
IS  - 2
SN  - 2075-5309

AB  - The automated process of construction defect detection using non-contact methods provides vital information for quality control and updating building information modelling. The external cladding in modular construction should be regularly controlled in terms of the quality of panels and proper installation because its appearance is very important for clients. However, there are limited computational methods for examining the installation issues of external cladding remotely in an automated manner. These issues could be the incorrect sitting of a panel, unequal joints in an elevation, scratches or cracks on the face of a panel or dimensions of different elements of external cladding. This paper aims to present seven algorithms to detect panel edges and statistically compare their performance through application on two scenarios of buildings in construction sites. Two different scenarios are selected, where the building fa&ccedil;ades are available to the public, and a sample of 100 images is taken using a state-of-the-art 3D camera for edge detection analysis. The experimentation results are validated by using a series of computational error and accuracy analyses and statistical methods including Mean Square Error, Peak Signal to Noise Ratio and Structural Similarity Index. The performance of an image processing algorithm depends on the quality of images and the algorithm utilised. The results show better performance of the fuzzy logic algorithm because it detects clear edges for installed panels. The applications of classical operators including Sobel, Canny, LoG, Prewitt and Roberts algorithms give similar results and show similarities in terms of the average of errors and accuracy. In addition, the results show that the minor difference of the average of the error and accuracy indices for Sobel, Canny, LoG, Prewitt and Roberts methods between both scenarios are not statistically significant, while the difference in the average of the error and accuracy indices for RGB-Sobel and Fuzzy methods between both scenarios are statistically significant. The accuracy of the algorithms can be improved by removing unwanted items such as vegetation and clouds in the sky. The evaluated algorithms assist practitioners to analyse their images collected day to day from construction sites, and to update building information modelling and the project digital drawings. Future work may need to focus on the combination of the evaluated algorithms using new data sets including colour edge detection for automatic defect identification using RGB and 360-degree images.
KW  - edge detection
KW  - object
KW  - change detection
KW  - defect
KW  - modular construction
KW  - fabricated methods
KW  - cladding
KW  - panel installation
KW  - automation in construction
KW  - quality control
KW  - 3D camera
KW  - visualisation
DO  - 10.3390/buildings9020040
ER  -
TY  - EJOU
AU  - Ba, Rui
AU  - Song, Weiguo
AU  - Li, Xiaolian
AU  - Xie, Zixi
AU  - Lo, Siuming
TI  - Integration of Multiple Spectral Indices and a Neural Network for Burned Area Mapping Based on MODIS Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Since wildfires have occurred frequently in recent years, accurate burned area mapping is required for wildfire severity assessment and burned land reconstruction. Satellite remote sensing is an effective technology that can provide valuable information for wildfire assessment. However, the common approaches based on using a single satellite image to promptly detect the burned areas have low accuracy and limited applicability. This paper develops a new burned area mapping method that surpasses the detection accuracy of previous methods, while still using a single Moderate Resolution Imaging Spectroradiometer (MODIS) sensor image. The key innovation is integrating optimal spectral indices and a neural network algorithm. We used the traditional empirical formula method, multi-threshold method and visual interpretation method to extract the sample sets of five typical types (burned area, vegetation, cloud, bare soil, and cloud shadow) from the MODIS data of several wildfires in the American states of Nevada, Washington and California in 2016. Afterward, the separability index M was adopted to assess the capacity of seven spectral bands and 13 spectral indices to distinguish the burned area from four unburned land cover types. Based on the separability analysis between the burned area and unburned areas, the spectral indices with an M value higher than 1.0 were employed to generate the training sample sets that were assessed to have an overall accuracy of 98.68% and Kappa coefficient of 97.46%. Finally, we utilized a back-propagation neural network (BPNN) to learn the spectral differences of different types from the training sample sets and obtain the output burned area map. The proposed method was applied to three wildfire cases in the American states of Idaho, Nevada and Oregon in 2017. A comparison of detection results between the new MODIS-based burned area map and the reference burned area map compiled from Landsat-8 Operational Land Imager (OLI) data indicates that the proposed method can effectively exploit the spectral characteristics of various land cover types. Also, this new method can achieve higher accuracy with the reduction of commission error (CE, &gt;10%) and omission error (OE, &gt;6%) compared to the traditional empirical formula method. The new burned area mapping method could help managers and the public perform more effective wildfire assessments and emergency management.
KW  - MODIS
KW  - burned area
KW  - spectral indices
KW  - neural network
DO  - 10.3390/rs11030326
ER  -
TY  - EJOU
AU  - Huang, Huasheng
AU  - Deng, Jizhong
AU  - Lan, Yubin
AU  - Yang, Aqing
AU  - Zhang, Lei
AU  - Wen, Sheng
AU  - Zhang, Huihui
AU  - Zhang, Yali
AU  - Deng, Yusen
TI  - Detection of Helminthosporium Leaf Blotch Disease Based on UAV Imagery
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 3
SN  - 2076-3417

AB  - Helminthosporium leaf blotch (HLB) is a serious disease of wheat causing yield reduction globally. Usually, HLB disease is controlled by uniform chemical spraying, which is adopted by most farmers. However, increased use of chemical controls have caused agronomic and environmental problems. To solve these problems, an accurate spraying system must be applied. In this case, the disease detection over the whole field can provide decision support information for the spraying machines. The objective of this paper is to evaluate the potential of unmanned aerial vehicle (UAV) remote sensing for HLB detection. In this work, the UAV imagery acquisition and ground investigation were conducted in Central China on April 22th, 2017. Four disease categories (normal, light, medium, and heavy) were established based on different severity degrees. A convolutional neural network (CNN) was proposed for HLB disease classification. The experiments on data preprocessing, classification, and hyper-parameters tuning were conducted. The overall accuracy and standard error of the CNN method was 91.43% and 0.83%, which outperformed other methods in terms of accuracy and stabilization. Especially for the detection of the diseased samples, the CNN method significantly outperformed others. Experimental results showed that the HLB infected areas and healthy areas can be precisely discriminated based on UAV remote sensing data, indicating that UAV remote sensing can be proposed as an efficient tool for HLB disease detection.
KW  - UAV imagery
KW  - remote sensing
KW  - Helminthosporium leaf blotch
KW  - convolution neural network
KW  - SVM
DO  - 10.3390/app9030558
ER  -
TY  - EJOU
AU  - Chen, Chaoyue
AU  - Gong, Weiguo
AU  - Chen, Yongliang
AU  - Li, Weihong
TI  - Object Detection in Remote Sensing Images Based on a Scene-Contextual Feature Pyramid Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 3
SN  - 2072-4292

AB  - Object detection has attracted increasing attention in the field of remote sensing image analysis. Complex backgrounds, vertical views, and variations in target kind and size in remote sensing images make object detection a challenging task. In this work, considering that the types of objects are often closely related to the scene in which they are located, we propose a convolutional neural network (CNN) by combining scene-contextual information for object detection. Specifically, we put forward the scene-contextual feature pyramid network (SCFPN), which aims to strengthen the relationship between the target and the scene and solve problems resulting from variations in target size. Additionally, to improve the capability of feature extraction, the network is constructed by repeating a building aggregated residual block. This block increases the receptive field, which can extract richer information for targets and achieve excellent performance with respect to small object detection. Moreover, to improve the proposed model performance, we use group normalization, which divides the channels into groups and computes the mean and variance for normalization within each group, to solve the limitation of the batch normalization. The proposed method is validated on a public and challenging dataset. The experimental results demonstrate that our proposed method outperforms other state-of-the-art object detection models.
KW  - convolutional neural network (CNN)
KW  - object detection
KW  - remote sensing images
KW  - scene-contextual feature pyramid network (SCFPN)
DO  - 10.3390/rs11030339
ER  -
TY  - EJOU
AU  - Jorge, Vitor A. M.
AU  - Granada, Roger
AU  - Maidana, Renan G.
AU  - Jurak, Darlan A.
AU  - Heck, Guilherme
AU  - Negreiros, Alvaro P. F.
AU  - dos Santos, Davi H.
AU  - Gonçalves, Luiz M. G.
AU  - Amory, Alexandre M.
TI  - A Survey on Unmanned Surface Vehicles for Disaster Robotics: Main Challenges and Directions
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 3
SN  - 1424-8220

AB  - Disaster robotics has become a research area in its own right, with several reported cases of successful robot deployment in actual disaster scenarios. Most of these disaster deployments use aerial, ground, or underwater robotic platforms. However, the research involving autonomous boats or Unmanned Surface Vehicles (USVs) for Disaster Management (DM) is currently spread across several publications, with varying degrees of depth, and focusing on more than one unmanned vehicle&mdash;usually under the umbrella of Unmanned Marine Vessels (UMV). Therefore, the current importance of USVs for the DM process in its different phases is not clear. This paper presents the first comprehensive survey about the applications and roles of USVs for DM, as far as we know. This work demonstrates that there are few current deployments in disaster scenarios, with most of the research in the area focusing on the technological aspects of USV hardware and software, such as Guidance Navigation and Control, and not focusing on their actual importance for DM. Finally, to guide future research, this paper also summarizes our own contributions, the lessons learned, guidelines, and research gaps.
KW  - survey
KW  - disaster management
KW  - unmanned surface vehicle
KW  - USV
KW  - unmanned surface craft
KW  - USC
KW  - autonomous surface craft
KW  - ASC
KW  - autonomous boat
KW  - disaster robotics
KW  - floods
KW  - landslides
KW  - hurricanes
KW  - tsunamis
KW  - hazard
KW  - search and rescue
DO  - 10.3390/s19030702
ER  -
TY  - EJOU
AU  - Li, Xuanpeng
AU  - Wang, Dong
AU  - Ao, Huanxuan
AU  - Belaroussi, Rachid
AU  - Gruyer, Dominique
TI  - Fast 3D Semantic Mapping in Road Scenes
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - Fast 3D reconstruction with semantic information in road scenes is of great requirements for autonomous navigation. It involves issues of geometry and appearance in the field of computer vision. In this work, we propose a fast 3D semantic mapping system based on the monocular vision by fusion of localization, mapping, and scene parsing. From visual sequences, it can estimate the camera pose, calculate the depth, predict the semantic segmentation, and finally realize the 3D semantic mapping. Our system consists of three modules: a parallel visual Simultaneous Localization And Mapping (SLAM) and semantic segmentation module, an incrementally semantic transfer from 2D image to 3D point cloud, and a global optimization based on Conditional Random Field (CRF). It is a heuristic approach that improves the accuracy of the 3D semantic labeling in light of the spatial consistency on each step of 3D reconstruction. In our framework, there is no need to make semantic inference on each frame of sequence, since the 3D point cloud data with semantic information is corresponding to sparse reference frames. It saves on the computational cost and allows our mapping system to perform online. We evaluate the system on road scenes, e.g., KITTI, and observe a significant speed-up in the inference stage by labeling on the 3D point cloud.
KW  - 3D semantic mapping
KW  - incrementally probabilistic fusion
KW  - CRF regularization
KW  - road scenes
DO  - 10.3390/app9040631
ER  -
TY  - EJOU
AU  - Guo, Kai
AU  - Liu, Liansheng
AU  - Shi, Shuhui
AU  - Liu, Datong
AU  - Peng, Xiyuan
TI  - UAV Sensor Fault Detection Using a Classifier without Negative Samples: A Local Density Regulated Optimization Algorithm
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - Fault detection for sensors of unmanned aerial vehicles is essential for ensuring flight security, in which the flight control system conducts real-time control for the vehicles relying on the sensing information from sensors, and erroneous sensor data will lead to false flight control commands, causing undesirable consequences. However, because of the scarcity of faulty instances, it still remains a challenging issue for flight sensor fault detection. The one-class support vector machine approach is a favorable classifier without negative samples, however, it is sensitive to outliers that deviate from the center and lacks a mechanism for coping with them. The compactness of its decision boundary is influenced, leading to the degradation of detection rate. To deal with this issue, an optimized one-class support vector machine approach regulated by local density is proposed in this paper, which regulates the tolerance extents of its decision boundary to the outliers according to their extent of abnormality indicated by their local densities. The application scope of the local density theory is narrowed to keep the internal instances unchanged and a rule for assigning the outliers continuous density coefficients is raised. Simulation results on a real flight control system model have proved its effectiveness and superiority.
KW  - fault detection
KW  - sensors
KW  - unmanned aerial vehicles
KW  - flight control system
KW  - one-class support vector machine
KW  - local density
DO  - 10.3390/s19040771
ER  -
TY  - EJOU
AU  - Kwak, Geun-Ho
AU  - Park, No-Wook
TI  - Impact of Texture Information on Crop Classification with Machine Learning and UAV Images
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - Unmanned aerial vehicle (UAV) images that can provide thematic information at much higher spatial and temporal resolutions than satellite images have great potential in crop classification. Due to the ultra-high spatial resolution of UAV images, spatial contextual information such as texture is often used for crop classification. From a data availability viewpoint, it is not always possible to acquire time-series UAV images due to limited accessibility to the study area. Thus, it is necessary to improve classification performance for situations when a single or minimum number of UAV images are available for crop classification. In this study, we investigate the potential of gray-level co-occurrence matrix (GLCM)-based texture information for crop classification with time-series UAV images and machine learning classifiers including random forest and support vector machine. In particular, the impact of combining texture and spectral information on the classification performance is evaluated for cases that use only one UAV image or multi-temporal images as input. A case study of crop classification in Anbandegi of Korea was conducted for the above comparisons. The best classification accuracy was achieved when multi-temporal UAV images which can fully account for the growth cycles of crops were combined with GLCM-based texture features. However, the impact of the utilization of texture information was not significant. In contrast, when one August UAV image was used for crop classification, the utilization of texture information significantly affected the classification performance. Classification using texture features extracted from GLCM with larger kernel size significantly improved classification accuracy, an improvement of 7.72%p in overall accuracy for the support vector machine classifier, compared with classification based solely on spectral information. These results indicate the usefulness of texture information for classification of ultra-high-spatial-resolution UAV images, particularly when acquisition of time-series UAV images is difficult and only one UAV image is used for crop classification.
KW  - unmanned aerial vehicle
KW  - texture
KW  - gray-level co-occurrence matrix
KW  - machine learning
KW  - crop
DO  - 10.3390/app9040643
ER  -
TY  - EJOU
AU  - Giernacki, Wojciech
TI  - Iterative Learning Method for In-Flight Auto-Tuning of UAV Controllers Based on Basic Sensory Information
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - With an increasing number of multirotor unmanned aerial vehicles (UAVs), solutions supporting the improvement in their precision of operation and safety of autonomous flights are gaining importance. They are particularly crucial in transportation tasks, where control systems are required to provide a stable and controllable flight in various environmental conditions, especially after changing the total mass of the UAV (by adding extra load). In the paper, the problem of using only available basic sensory information for fast, locally best, iterative real-time auto-tuning of parameters of fixed-gain altitude controllers is considered. The machine learning method proposed for this purpose is based on a modified zero-order optimization algorithm (golden-search algorithm) and bootstrapping technique. It has been validated in numerous simulations and real-world experiments in terms of its effectiveness in such aspects as: the impact of environmental disturbances (wind gusts); flight with change in mass; and change of sensory information sources in the auto-tuning procedure. The main advantage of the proposed method is that for the trajectory primitives repeatedly followed by an UAV (for programmed controller gains), the method effectively minimizes the selected performance index (cost function). Such a performance index might, e.g., express indirect requirements about tracking quality and energy expenditure. In the paper, a comprehensive description of the method, as well as a wide discussion of the results obtained from experiments conducted in the AeroLab for a low-cost UAV (Bebop 2), are included. The results have confirmed high efficiency of the method at the expected, low computational complexity.
KW  - UAV
KW  - auto-tuning
KW  - machine learning
KW  - iterative learning
KW  - extremum-seeking
KW  - altitude controller
DO  - 10.3390/app9040648
ER  -
TY  - EJOU
AU  - Cheng, Qiao
AU  - Wang, Xiangke
AU  - Yang, Jian
AU  - Shen, Lincheng
TI  - Automated Enemy Avoidance of Unmanned Aerial Vehicles Based on Reinforcement Learning
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 4
SN  - 2076-3417

AB  - This paper focuses on one of the collision avoidance scenarios for unmanned aerial vehicles (UAVs), where the UAV needs to avoid collision with the enemy UAV during its flying path to the goal point. Such a type of problem is defined as the enemy avoidance problem in this paper. To deal with this problem, a learning based framework is proposed. Under this framework, the enemy avoidance problem is formulated as a Markov Decision Process (MDP), and the maneuver policies for the UAV are learned based on a temporal-difference reinforcement learning method called Sarsa. To handle the enemy avoidance problem in continuous state space, the Cerebellar Model Arithmetic Computer (CMAC) function approximation technique is embodied in the proposed framework. Furthermore, a hardware-in-the-loop (HITL) simulation environment is established. Simulation results show that the UAV agent can learn a satisfying policy under the proposed framework. Comparing with the random policy and the fixed-rule policy, the learned policy can achieve a far higher possibility in reaching the goal point without colliding with the enemy UAV.
KW  - enemy avoidance
KW  - reinforcement learning
KW  - decision making
KW  - hardware-in-the-loop simulation
KW  - unmanned aerial vehicles
DO  - 10.3390/app9040669
ER  -
TY  - EJOU
AU  - Ampatzidis, Yiannis
AU  - Partel, Victor
TI  - UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 4
SN  - 2072-4292

AB  - Traditional plant breeding evaluation methods are time-consuming, labor-intensive, and costly. Accurate and rapid phenotypic trait data acquisition and analysis can improve genomic selection and accelerate cultivar development. In this work, a technique for data acquisition and image processing was developed utilizing small unmanned aerial vehicles (UAVs), multispectral imaging, and deep learning convolutional neural networks to evaluate phenotypic characteristics on citrus crops. This low-cost and automated high-throughput phenotyping technique utilizes artificial intelligence (AI) and machine learning (ML) to: (i) detect, count, and geolocate trees and tree gaps; (ii) categorize trees based on their canopy size; (iii) develop individual tree health indices; and (iv) evaluate citrus varieties and rootstocks. The proposed remote sensing technique was able to detect and count citrus trees in a grove of 4,931 trees, with precision and recall of 99.9% and 99.7%, respectively, estimate their canopy size with overall accuracy of 85.5%, and detect, count, and geolocate tree gaps with a precision and recall of 100% and 94.6%, respectively. This UAV-based technique provides a consistent, more direct, cost-effective, and rapid method to evaluate phenotypic characteristics of citrus varieties and rootstocks.
KW  - UAV
KW  - artificial intelligence
KW  - machine learning
KW  - smart agriculture
KW  - precision agriculture
KW  - neural networks
KW  - deep learning
DO  - 10.3390/rs11040410
ER  -
TY  - EJOU
AU  - Yan, Lei
AU  - Cao, Suzhi
AU  - Gong, Yongsheng
AU  - Han, Hao
AU  - Wei, Junyong
AU  - Zhao, Yi
AU  - Yang, Shuling
TI  - SatEC: A 5G Satellite Edge Computing Framework Based on Microservice Architecture
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - As outlined in the 3Gpp Release 16, 5G satellite access is important for 5G network development in the future. A terrestrial-satellite network integrated with 5G has the characteristics of low delay, high bandwidth, and ubiquitous coverage. A few researchers have proposed integrated schemes for such a network; however, these schemes do not consider the possibility of achieving optimization of the delay characteristic by changing the computing mode of the 5G satellite network. We propose a 5G satellite edge computing framework (5GsatEC), which aims to reduce delay and expand network coverage. This framework consists of embedded hardware platforms and edge computing microservices in satellites. To increase the flexibility of the framework in complex scenarios, we unify the resource management of the central processing unit (CPU), graphics processing unit (GPU), and field-programmable gate array (FPGA); we divide the services into three types: system services, basic services, and user services. In order to verify the performance of the framework, we carried out a series of experiments. The results show that 5GsatEC has a broader coverage than the ground 5G network. The results also show that 5GsatEC has lower delay, a lower packet loss rate, and lower bandwidth consumption than the 5G satellite network.
KW  - edge computing
KW  - on-board data processing
KW  - microservices
KW  - Integrated Terrestrial-Satellite Networks
DO  - 10.3390/s19040831
ER  -
TY  - EJOU
AU  - Erickson, Adam
AU  - Strigul, Nikolay
TI  - A Forest Model Intercomparison Framework and Application at Two Temperate Forests Along the East Coast of the United States
T2  - Forests

PY  - 2019
VL  - 10
IS  - 2
SN  - 1999-4907

AB  - State-of-the-art forest models are often complex, analytically intractable, and computationally expensive, due to the explicit representation of detailed biogeochemical and ecological processes. Different models often produce distinct results while predictions from the same model vary with parameter values. In this project, we developed a rigorous quantitative approach for conducting model intercomparisons and assessing model performance. We have applied our original methodology to compare two forest biogeochemistry models, the Perfect Plasticity Approximation with Simple Biogeochemistry (PPA-SiBGC) and Landscape Disturbance and Succession with Net Ecosystem Carbon and Nitrogen (LANDIS-II NECN). We simulated past-decade conditions at flux tower sites located within Harvard Forest, MA, USA (HF-EMS) and Jones Ecological Research Center, GA, USA (JERC-RD). We mined field data available from both sites to perform model parameterization, validation, and intercomparison. We assessed model performance using the following time-series metrics: Net ecosystem exchange, aboveground net primary production, aboveground biomass, C, and N, belowground biomass, C, and N, soil respiration, and species total biomass and relative abundance. We also assessed static observations of soil organic C and N, and concluded with an assessment of general model usability, performance, and transferability. Despite substantial differences in design, both models achieved good accuracy across the range of pool metrics. While LANDIS-II NECN showed better fidelity to interannual NEE fluxes, PPA-SiBGC indicated better overall performance for both sites across the 11 temporal and two static metrics tested (HF-EMS       R 2  &macr;  = 0.73 , + 0.07    ,       R M S E  &macr;  = 4.68 , &minus; 9.96    ; JERC-RD       R 2  &macr;  = 0.73 , + 0.01    ,       R M S E  &macr;  = 2.18 , &minus; 1.64    ). To facilitate further testing of forest models at the two sites, we provide pre-processed datasets and original software written in the R language of statistical computing. In addition to model intercomparisons, our approach may be employed to test modifications to forest models and their sensitivity to different parameterizations.
KW  - Perfect Plasticity Approximation
KW  - SORTIE-PPA
KW  - LANDIS-II
KW  - forest ecosystem simulation
KW  - forest biogeochemistry model
KW  - forest landscape model
KW  - model intercomparison
KW  - Harvard Forest
KW  - Jones Ecological Research Center
DO  - 10.3390/f10020180
ER  -
TY  - EJOU
AU  - Shihavuddin, ASM
AU  - Chen, Xiao
AU  - Fedorov, Vladimir
AU  - Nymark Christensen, Anders
AU  - Andre Brogaard Riis, Nicolai
AU  - Branner, Kim
AU  - Bjorholm Dahl, Anders
AU  - Reinhold Paulsen, Rasmus
TI  - Wind Turbine Surface Damage Detection by Deep Learning Aided Drone Inspection Analysis
T2  - Energies

PY  - 2019
VL  - 12
IS  - 4
SN  - 1996-1073

AB  - Timely detection of surface damages on wind turbine blades is imperative for minimizing downtime and avoiding possible catastrophic structural failures. With recent advances in drone technology, a large number of high-resolution images of wind turbines are routinely acquired and subsequently analyzed by experts to identify imminent damages. Automated analysis of these inspection images with the help of machine learning algorithms can reduce the inspection cost. In this work, we develop a deep learning-based automated damage suggestion system for subsequent analysis of drone inspection images. Experimental results demonstrate that the proposed approach can achieve almost human-level precision in terms of suggested damage location and types on wind turbine blades. We further demonstrate that for relatively small training sets, advanced data augmentation during deep learning training can better generalize the trained model, providing a significant gain in precision.
KW  - wind energy
KW  - rotor blade
KW  - wind turbine
KW  - drone inspection
KW  - damage detection
KW  - deep learning
KW  - Convolutional Neural Network (CNN)
DO  - 10.3390/en12040676
ER  -
TY  - EJOU
AU  - Wang, Linhui
AU  - Yue, Xuejun
AU  - Liu, Yongxin
AU  - Wang, Jian
AU  - Wang, Huihui
TI  - An Intelligent Vision Based Sensing Approach for Spraying Droplets Deposition Detection
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - The rapid development of vision sensor based on artificial intelligence (AI) is reforming industries and making our world smarter. Among these trends, it is of great significance to adapt AI technologies into the intelligent agricultural management. In smart agricultural aviation spraying, the droplets&rsquo; distribution and deposition are important indexes for estimating effectiveness in plant protection process. However, conventional approaches are problematic, they lack adaptivity to environmental changes, and consumes non-reusable test materials. One example is that the machine vision algorithms they employ can&rsquo;t guarantee that the division of adhesive droplets thereby disabling the accurate measurement of critical parameters. To alleviate these problems, we put forward an intelligent visual droplet detection node which can adapt to the environment illumination change. Then, we propose a modified marker controllable watershed segmentation algorithm to segment those adhesive droplets, and calculate their characteristic parameters on the basis of the segmentation results, including number, coverage, coverage density, etc. Finally, we use the intelligent node to detect droplets, and then expound the situation that the droplet region is effectively segmented and marked. The intelligent node has better adaptability and robustness even under the condition of illumination changing. The large-scale distributed detection result indicates that our approach has good consistency with the non-recyclable water-sensitive paper approach. Our approach provides an intelligent and environmental friendly way of tests for spraying techniques, especially for plant protection with Unmanned Aerial Vehicles.
KW  - droplets
KW  - intelligent node
KW  - vision sensor
KW  - adaptability
KW  - Unmanned Aerial Vehicles
DO  - 10.3390/s19040933
ER  -
TY  - EJOU
AU  - Madokoro, Hirokazu
AU  - Sato, Kazuhito
AU  - Shimoi, Nobuhiro
TI  - Vision-Based Indoor Scene Recognition from Time-Series Aerial Images Obtained Using a MAV Mounted Monocular Camera
T2  - Drones

PY  - 2019
VL  - 3
IS  - 1
SN  - 2504-446X

AB  - This paper presents a vision-based indoor scene recognition method from aerial time-series images obtained using a micro air vehicle (MAV). The proposed method comprises two procedures: a codebook feature description procedure, and a recognition procedure using category maps. For the former procedure, codebooks are created automatically as visual words using self-organizing maps (SOMs) after extracting part-based local features using a part-based descriptor from time-series scene images. For the latter procedure, category maps are created using counter propagation networks (CPNs) with the extraction of category boundaries using a unified distance matrix (U-Matrix). Using category maps, topologies of image features are mapped into a low-dimensional space based on competitive and neighborhood learning. We obtained aerial time-series image datasets of five sets for two flight routes: a round flight route and a zigzag flight route. The experimentally obtained results with leave-one-out cross-validation (LOOCV) revealed respective mean recognition accuracies for the round flight datasets (RFDs) and zigzag flight datasets (ZFDs) of 71.7% and 65.5% for 10 zones. The category maps addressed the complexity of scenes because of segmented categories. Although extraction results of category boundaries using U-Matrix were partially discontinuous, we obtained comprehensive category boundaries that segment scenes into several categories.
KW  - category maps
KW  - counter propagation networks
KW  - leave-one-out cross-validation
KW  - micro air vehicles
KW  - self-organizing maps
KW  - unified distance matrix
DO  - 10.3390/drones3010022
ER  -
TY  - EJOU
AU  - Hrabia, Christopher-Eyk
AU  - Hessler, Axel
AU  - Xu, Yuan
AU  - Seibert, Jacob
AU  - Brehmer, Jan
AU  - Albayrak, Sahin
TI  - EffFeu Project: Towards Mission-Guided Application of Drones in Safety and Security Environments
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 4
SN  - 1424-8220

AB  - The number of unmanned aerial system (UAS) applications for supporting rescue forces is growing in recent years. Nevertheless, the analysis of sensed information and control of unmanned aerial vehicle (UAV) creates an enormous psychological and emotional load for the involved humans especially in critical and hectic situations. The introduced research project EffFeu (Efficient Operation of Unmanned Aerial Vehicle for Industrial Firefighters) especially focuses on a holistic integration of UAS in the daily work of industrial firefighters. This is done by enabling autonomous mission-guided control on top of the presented overall system architecture, goal-oriented high-level task control, comprehensive localisation process combining several approaches to enable the transition from and to GNSS-supported and GNSS-denied environments, as well as a deep-learning based object recognition of relevant entities. This work describes the concepts, current stage, and first evaluation results of the research project.
KW  - decisional autonomy
KW  - decision-making
KW  - planning
KW  - object recognition
KW  - deep learning
KW  - GNSS-denied localisation
DO  - 10.3390/s19040973
ER  -
TY  - EJOU
AU  - Feng, Jie
AU  - Wang, Lin
AU  - Yu, Haipeng
AU  - Jiao, Licheng
AU  - Zhang, Xiangrong
TI  - Divide-and-Conquer Dual-Architecture Convolutional Neural Network for Classification of Hyperspectral Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 5
SN  - 2072-4292

AB  - Convolutional neural network (CNN) is well-known for its powerful capability on image classification. In hyperspectral images (HSIs), fixed-size spatial window is generally used as the input of CNN for pixel-wise classification. However, single fixed-size spatial architecture hinders the excellent performance of CNN due to the neglect of various land-cover distributions in HSIs. Moreover, insufficient samples in HSIs may cause the overfitting problem. To address these problems, a novel divide-and-conquer dual-architecture CNN (DDCNN) method is proposed for HSI classification. In DDCNN, a novel regional division strategy based on local and non-local decisions is devised to distinguish homogeneous and heterogeneous regions. Then, for homogeneous regions, a multi-scale CNN architecture with larger spatial window inputs is constructed to learn joint spectral-spatial features. For heterogeneous regions, a fine-grained CNN architecture with smaller spatial window inputs is constructed to learn hierarchical spectral features. Moreover, to alleviate the problem of insufficient training samples, unlabeled samples with high confidences are pre-labeled under adaptively spatial constraint. Experimental results on HSIs demonstrate that the proposed method provides encouraging classification performance, especially region uniformity and edge preservation with limited training samples.
KW  - Hyperspectral image classification
KW  - divide-and-conquer
KW  - dual-architecture convolutional neural network
KW  - homogeneous and heterogeneous regions
KW  - superpixel segmentation
DO  - 10.3390/rs11050484
ER  -
TY  - EJOU
AU  - Pu, Can
AU  - Song, Runzi
AU  - Tylecek, Radim
AU  - Li, Nanbo
AU  - Fisher, Robert B.
TI  - SDF-MAN: Semi-Supervised Disparity Fusion with Multi-Scale Adversarial Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 5
SN  - 2072-4292

AB  - Refining raw disparity maps from different algorithms to exploit their complementary advantages is still challenging. Uncertainty estimation and complex disparity relationships among pixels limit the accuracy and robustness of existing methods and there is no standard method for fusion of different kinds of depth data. In this paper, we introduce a new method to fuse disparity maps from different sources, while incorporating supplementary information (intensity, gradient, etc.) into a refiner network to better refine raw disparity inputs. A discriminator network classifies disparities at different receptive fields and scales. Assuming a Markov Random Field for the refined disparity map produces better estimates of the true disparity distribution. Both fully supervised and semi-supervised versions of the algorithm are proposed. The approach includes a more robust loss function to inpaint invalid disparity values and requires much less labeled data to train in the semi-supervised learning mode. The algorithm can be generalized to fuse depths from different kinds of depth sources. Experiments explored different fusion opportunities: stereo-monocular fusion, stereo-ToF fusion and stereo-stereo fusion. The experiments show the superiority of the proposed algorithm compared with the most recent algorithms on public synthetic datasets (Scene Flow, SYNTH3, our synthetic garden dataset) and real datasets (Kitti2015 dataset and Trimbot2020 Garden dataset).
KW  - depth fusion
KW  - disparity fusion
KW  - stereo vision, monocular vision
KW  - time of flight
DO  - 10.3390/rs11050487
ER  -
TY  - EJOU
AU  - Zhang, Wei
AU  - Tang, Ping
AU  - Zhao, Lijun
TI  - Remote Sensing Image Scene Classification Using CNN-CapsNet
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 5
SN  - 2072-4292

AB  - Remote sensing image scene classification is one of the most challenging problems in understanding high-resolution remote sensing images. Deep learning techniques, especially the convolutional neural network (CNN), have improved the performance of remote sensing image scene classification due to the powerful perspective of feature learning and reasoning. However, several fully connected layers are always added to the end of CNN models, which is not efficient in capturing the hierarchical structure of the entities in the images and does not fully consider the spatial information that is important to classification. Fortunately, capsule network (CapsNet), which is a novel network architecture that uses a group of neurons as a capsule or vector to replace the neuron in the traditional neural network and can encode the properties and spatial information of features in an image to achieve equivariance, has become an active area in the classification field in the past two years. Motivated by this idea, this paper proposes an effective remote sensing image scene classification architecture named CNN-CapsNet to make full use of the merits of these two models: CNN and CapsNet. First, a CNN without fully connected layers is used as an initial feature maps extractor. In detail, a pretrained deep CNN model that was fully trained on the ImageNet dataset is selected as a feature extractor in this paper. Then, the initial feature maps are fed into a newly designed CapsNet to obtain the final classification result. The proposed architecture is extensively evaluated on three public challenging benchmark remote sensing image datasets: the UC Merced Land-Use dataset with 21 scene categories, AID dataset with 30 scene categories, and the NWPU-RESISC45 dataset with 45 challenging scene categories. The experimental results demonstrate that the proposed method can lead to a competitive classification performance compared with the state-of-the-art methods.
KW  - remote sensing
KW  - scene classification
KW  - CNN
KW  - capsule
KW  - PrimaryCaps
KW  - CapsNet
DO  - 10.3390/rs11050494
ER  -
TY  - EJOU
AU  - Ma, Fei
AU  - Gao, Fei
AU  - Sun, Jinping
AU  - Zhou, Huiyu
AU  - Hussain, Amir
TI  - Weakly Supervised Segmentation of SAR Imagery Using Superpixel and Hierarchically Adversarial CRF
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 5
SN  - 2072-4292

AB  - Synthetic aperture radar (SAR) image segmentation aims at generating homogeneous regions from a pixel-based image and is the basis of image interpretation. However, most of the existing segmentation methods usually neglect the appearance and spatial consistency during feature extraction and also require a large number of training data. In addition, pixel-based processing cannot meet the real time requirement. We hereby present a weakly supervised algorithm to perform the task of segmentation for high-resolution SAR images. For effective segmentation, the input image is first over-segmented into a set of primitive superpixels. This algorithm combines hierarchical conditional generative adversarial nets (CGAN) and conditional random fields (CRF). The CGAN-based networks can leverage abundant unlabeled data learning parameters, reducing their reliance on the labeled samples. In order to preserve neighborhood consistency in the feature extraction stage, the hierarchical CGAN is composed of two sub-networks, which are employed to extract the information of the central superpixels and the corresponding background superpixels, respectively. Afterwards, CRF is utilized to perform label optimization using the concatenated features. Quantified experiments on an airborne SAR image dataset prove that the proposed method can effectively learn feature representations and achieve competitive accuracy to the state-of-the-art segmentation approaches. More specifically, our algorithm has a higher Cohen&rsquo;s kappa coefficient and overall accuracy. Its computation time is less than the current mainstream pixel-level semantic segmentation networks.
KW  - synthetic aperture radar (SAR)
KW  - segmentation
KW  - conditional random fields (CRF)
KW  - conditional generative adversarial nets (CGAN)
KW  - neighborhood consistency
DO  - 10.3390/rs11050512
ER  -
TY  - EJOU
AU  - Wen, Sheng
AU  - Zhang, Quanyong
AU  - Yin, Xuanchun
AU  - Lan, Yubin
AU  - Zhang, Jiantao
AU  - Ge, Yufeng
TI  - Design of Plant Protection UAV Variable Spray System Based on Neural Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 5
SN  - 1424-8220

AB  - Recently, unmanned aerial vehicles (UAVs) have rapidly emerged as a new technology in the fields of plant protection and pest control in China. Based on existing variable spray research, a plant protection UAV variable spray system integrating neural network based decision making is designed. Using the existing data on plant protection UAV operations, combined with artificial neural network (ANN) technology, an error back propagation (BP) neural network model between the factors affecting droplet deposition is trained. The factors affecting droplet deposition include ambient temperature, ambient humidity, wind speed, flight speed, flight altitude, propeller pitch, nozzles pitch and prescription value. Subsequently, the BP neural network model is combined with variable rate spray control for plant protection UAVs, and real-time information is collected by multi-sensor. The deposition rate is determined by the neural network model, and the flow rate of the spray system is regulated according to the predicted deposition amount. The amount of droplet deposition can meet the prescription requirement. The results show that the training variance of the ANN is 0.003, and thus, the model is stable and reliable. The outdoor tests show that the error between the predicted droplet deposition and actual droplet deposition is less than 20%. The ratio of droplet deposition to prescription value in each unit is approximately equal, and a variable spray operation under different conditions is realized.
KW  - UAV
KW  - BP neural network
KW  - droplet deposition
KW  - variable spray
DO  - 10.3390/s19051112
ER  -
TY  - EJOU
AU  - Li, Yang
AU  - Shi, Leyi
AU  - Feng, Haijie
TI  - A Game-Theoretic Analysis for Distributed Honeypots
T2  - Future Internet

PY  - 2019
VL  - 11
IS  - 3
SN  - 1999-5903

AB  - A honeypot is a decoy tool for luring an attacker and interacting with it, further consuming its resources. Due to its fake property, a honeypot can be recognized by the adversary and loses its value. Honeypots equipped with dynamic characteristics are capable of deceiving intruders. However, most of their dynamic properties are reflected in the system configuration, rather than the location. Dynamic honeypots are faced with the risk of being identified and avoided. In this paper, we focus on the dynamic locations of honeypots and propose a distributed honeypot scheme. By periodically changing the services, the attacker cannot distinguish the real services from honeypots, and the illegal attack flow can be recognized. We adopt game theory to illustrate the effectiveness of our system. Gambit simulations are conducted to validate our proposed scheme. The game-theoretic reasoning shows that our system comprises an innovative system defense. Further simulation results prove that the proposed scheme improves the server’s payoff and that the attacker tends to abandon launching attacks. Therefore, the proposed distributed honeypot scheme is effective for network security.
KW  - game theory
KW  - honeypot
KW  - network security
KW  - proactive defense
DO  - 10.3390/fi11030065
ER  -
TY  - EJOU
AU  - Fu, Kun
AU  - Dai, Wei
AU  - Zhang, Yue
AU  - Wang, Zhirui
AU  - Yan, Menglong
AU  - Sun, Xian
TI  - MultiCAM: Multiple Class Activation Mapping for Aircraft Recognition in Remote Sensing Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 5
SN  - 2072-4292

AB  - Aircraft recognition in remote sensing images has long been a meaningful topic. Most related methods treat entire images as a whole and do not concentrate on the features of parts. In fact, a variety of aircraft types have small interclass variance, and the main evidence for classifying subcategories is related to some discriminative object parts. In this paper, we introduce the idea of fine-grained visual classification (FGVC) and attempt to make full use of the features from discriminative object parts. First, multiple class activation mapping (MultiCAM) is proposed to extract the discriminative parts of aircrafts of different categories. Second, we present a mask filter (MF) strategy to enhance the discriminative object parts and filter the interference of the background from original images. Third, a selective connected feature fusion method is proposed to fuse the features extracted from both networks, focusing on the original images and the results of MF, respectively. Compared with the single prediction category in class activation mapping (CAM), MultiCAM makes full use of the predictions of all categories to overcome the wrong discriminative parts produced by a wrong single prediction category. Additionally, the designed MF preserves the object scale information and helps the network to concentrate on the object itself rather than the interfering background. Experiments on a challenging dataset prove that our method can achieve state-of-the-art performance.
KW  - aircraft recognition in remote sensing images
KW  - fine-grained visual classification
KW  - multiple class activation mapping
KW  - mask filter
KW  - selective connected feature fusion
DO  - 10.3390/rs11050544
ER  -
TY  - EJOU
AU  - Carl, Christin
AU  - Lehmann, Jan R. K.
AU  - Landgraf, Dirk
AU  - Pretzsch, Hans
TI  - Robinia pseudoacacia L. in Short Rotation Coppice: Seed and Stump Shoot Reproduction as well as UAS-based Spreading Analysis
T2  - Forests

PY  - 2019
VL  - 10
IS  - 3
SN  - 1999-4907

AB  - Varying reproduction strategies are an important trait that tree species need in order both to survive and to spread. Black locust is able to reproduce via seeds, stump shoots, and root suckers. However, little research has been conducted on the reproduction and spreading of black locust in short rotation coppices. This research study focused on seed germination, stump shoot resprout, and spreading by root suckering of black locust in ten short rotation coppices in Germany. Seed experiments and sample plots were analyzed for the study. Spreading was detected and measured with unmanned aerial system (UAS)-based images and classification technology&mdash;object-based image analysis (OBIA). Additionally, the classification of single UAS images was tested by applying a convolutional neural network (CNN), a deep learning model. The analyses showed that seed germination increases with increasing warm-cold variety and scarification. Moreover, it was found that the number of shoots per stump decreases as shoot age increases. Furthermore, spreading increases with greater light availability and decreasing tillage. The OBIA and CNN image analysis technologies achieved 97% and 99.5% accuracy for black locust classification in UAS images. All in all, the three reproduction strategies of black locust in short rotation coppices differ with regards to initialization, intensity, and growth performance, but all play a role in the survival and spreading of black locust.
KW  - Robinia pseudoacacia L.
KW  - reproduction
KW  - spreading
KW  - short rotation coppice
KW  - unmanned aerial system (UAS)
KW  - object-based image analysis (OBIA)
KW  - convolutional neural network (CNN)
DO  - 10.3390/f10030235
ER  -
TY  - EJOU
AU  - Asadi, Khashayar
AU  - Chen, Pengyu
AU  - Han, Kevin
AU  - Wu, Tianfu
AU  - Lobaton, Edgar
TI  - LNSNet: Lightweight Navigable Space Segmentation for Autonomous Robots on Construction Sites
T2  - Data

PY  - 2019
VL  - 4
IS  - 1
SN  - 2306-5729

AB  - An autonomous robot that can monitor a construction site should be able to be can contextually detect its surrounding environment by recognizing objects and making decisions based on its observation. Pixel-wise semantic segmentation in real-time is vital to building an autonomous and mobile robot. However, the learning models&rsquo; size and high memory usage associated with real-time segmentation are the main challenges for mobile robotics systems that have limited computing resources. To overcome these challenges, this paper presents an efficient semantic segmentation method named LNSNet (lightweight navigable space segmentation network) that can run on embedded platforms to determine navigable space in real-time. The core of model architecture is a new block based on separable convolution which compresses the parameters of present residual block meanwhile maintaining the accuracy and performance. LNSNet is faster, has fewer parameters and less model size, while provides similar accuracy compared to existing models. A new pixel-level annotated dataset for real-time and mobile navigable space segmentation in construction environments has been constructed for the proposed method. The results demonstrate the effectiveness and efficiency that are necessary for the future development of the autonomous robotics systems.
KW  - efficient real-time segmentation
KW  - embedded platform
KW  - autonomous navigation in construction
KW  - autonomous data collection
DO  - 10.3390/data4010040
ER  -
TY  - EJOU
AU  - Popa, Alexandru
AU  - Hnatiuc, Mihaela
AU  - Paun, Mirel
AU  - Geman, Oana
AU  - Hemanth, D. J.
AU  - Dorcea, Daniel
AU  - Son, Le H.
AU  - Ghita, Simona
TI  - An Intelligent IoT-Based Food Quality Monitoring Approach Using Low-Cost Sensors
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 3
SN  - 2073-8994

AB  - The evolution of multipurpose sensors over the last decades has been investigated with the aim of developing innovative devices with applications in several fields of technology, including in the food industry. The integration of such sensors in food packaging technology has paved the way for intelligent food packaging. These integrated systems are capable of providing reliable information about the quality of the packed products during their storage period. To accomplish this goal, intelligent packs use a variety of sensors suited for monitoring the quality and safety of food products by recording the evolution of parameters like the quantity of pathogen agents, gases, temperature, humidity and storage period. This technology, when combined with IoT, is able to provide a lot more information than conventional food inspection technologies, which are limited to weight, volume, color and aspect inspection. The original system described in this work relies on a simple but effective method of integrated food monitoring, right at the client home, suitable for user prepared vacuum-packed foods. It builds upon the IoT concept and is able to create a network of interconnected devices. By using this approach, we are able to combine actuators and sensing devices also providing a common operating picture (COP) by sharing information over the platforms. More precisely, our system consists of gas, temperature and humidity sensors, which provide the essential information needed for evaluating the quality of the packed product. This information is transmitted wirelessly to a computer system providing an interface where the user can observe the evolution of the product quality over time.
KW  - intelligent food packaging
KW  - sensor
KW  - IoT
KW  - sensor calibration
KW  - food quality
DO  - 10.3390/sym11030374
ER  -
TY  - EJOU
AU  - Zhang, Chengming
AU  - Han, Yingjuan
AU  - Li, Feng
AU  - Gao, Shuai
AU  - Song, Dejuan
AU  - Zhao, Hui
AU  - Fan, Keqi
AU  - Zhang, Ya’nan
TI  - A New CNN-Bayesian Model for Extracting Improved Winter Wheat Spatial Distribution from GF-2 imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - When the spatial distribution of winter wheat is extracted from high-resolution remote sensing imagery using convolutional neural networks (CNN), field edge results are usually rough, resulting in lowered overall accuracy. This study proposed a new per-pixel classification model using CNN and Bayesian models (CNN-Bayesian model) for improved extraction accuracy. In this model, a feature extractor generates a feature vector for each pixel, an encoder transforms the feature vector of each pixel into a category-code vector, and a two-level classifier uses the difference between elements of category-probability vectors as the confidence value to perform per-pixel classifications. The first level is used to determine the category of a pixel with high confidence, and the second level is an improved Bayesian model used to determine the category of low-confidence pixels. The CNN-Bayesian model was trained and tested on Gaofen 2 satellite images. Compared to existing models, our approach produced an improvement in overall accuracy, the overall accuracy of SegNet, DeepLab, VGG-Ex, and CNN-Bayesian was 0.791, 0.852, 0.892, and 0.946, respectively. Thus, this approach can produce superior results when winter wheat spatial distribution is extracted from satellite imagery.
KW  - winter wheat
KW  - convolutional neural network
KW  - Visual Geometry Group Network
KW  - Bayesian model
KW  - per-pixel classification
KW  - high-resolution remote sensing imager
KW  - Gaofen 2 image
DO  - 10.3390/rs11060619
ER  -
TY  - EJOU
AU  - Hartling, Sean
AU  - Sagan, Vasit
AU  - Sidike, Paheding
AU  - Maimaitijiang, Maitiniyazi
AU  - Carron, Joshua
TI  - Urban Tree Species Classification Using a WorldView-2/3 and LiDAR Data Fusion Approach and Deep Learning
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 6
SN  - 1424-8220

AB  - Urban areas feature complex and heterogeneous land covers which create challenging issues for tree species classification. The increased availability of high spatial resolution multispectral satellite imagery and LiDAR datasets combined with the recent evolution of deep learning within remote sensing for object detection and scene classification, provide promising opportunities to map individual tree species with greater accuracy and resolution. However, there are knowledge gaps that are related to the contribution of Worldview-3 SWIR bands, very high resolution PAN band and LiDAR data in detailed tree species mapping. Additionally, contemporary deep learning methods are hampered by lack of training samples and difficulties of preparing training data. The objective of this study was to examine the potential of a novel deep learning method, Dense Convolutional Network (DenseNet), to identify dominant individual tree species in a complex urban environment within a fused image of WorldView-2 VNIR, Worldview-3 SWIR and LiDAR datasets. DenseNet results were compared against two popular machine classifiers in remote sensing image analysis, Random Forest (RF) and Support Vector Machine (SVM). Our results demonstrated that: (1) utilizing a data fusion approach beginning with VNIR and adding SWIR, LiDAR, and panchromatic (PAN) bands increased the overall accuracy of the DenseNet classifier from 75.9% to 76.8%, 81.1% and 82.6%, respectively. (2) DenseNet significantly outperformed RF and SVM for the classification of eight dominant tree species with an overall accuracy of 82.6%, compared to 51.8% and 52% for SVM and RF classifiers, respectively. (3) DenseNet maintained superior performance over RF and SVM classifiers under restricted training sample quantities which is a major limiting factor for deep learning techniques. Overall, the study reveals that DenseNet is more effective for urban tree species classification as it outperforms the popular RF and SVM techniques when working with highly complex image scenes regardless of training sample size.
KW  - deep learning
KW  - dense convolutional network (DenseNet)
KW  - convolutional neural network (CNN)
KW  - support vector machine (SVM)
KW  - random forest (RF)
KW  - tree species classification
KW  - data fusion
DO  - 10.3390/s19061284
ER  -
TY  - EJOU
AU  - Safonova, Anastasiia
AU  - Tabik, Siham
AU  - Alcaraz-Segura, Domingo
AU  - Rubtsov, Alexey
AU  - Maglinets, Yuriy
AU  - Herrera, Francisco
TI  - Detection of Fir Trees (Abies sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Invasion of the Polygraphus proximus Blandford bark beetle causes catastrophic damage to forests with firs (Abies sibirica Ledeb) in Russia, especially in Central Siberia. Determining tree damage stage based on the shape, texture and colour of tree crown in unmanned aerial vehicle (UAV) images could help to assess forest health in a faster and cheaper way. However, this task is challenging since (i) fir trees at different damage stages coexist and overlap in the canopy, (ii) the distribution of fir trees in nature is irregular and hence distinguishing between different crowns is hard, even for the human eye. Motivated by the latest advances in computer vision and machine learning, this work proposes a two-stage solution: In a first stage, we built a detection strategy that finds the regions of the input UAV image that are more likely to contain a crown, in the second stage, we developed a new convolutional neural network (CNN) architecture that predicts the fir tree damage stage in each candidate region. Our experiments show that the proposed approach shows satisfactory results on UAV Red, Green, Blue (RGB) images of forest areas in the state nature reserve “Stolby” (Krasnoyarsk, Russia).
KW  - multi-class classification
KW  - drone
KW  - aerial photography
KW  - Siberian fir
KW  - Siberia
KW  - deep-learning
KW  - convolutional neural networks
KW  - forest health
DO  - 10.3390/rs11060643
ER  -
TY  - EJOU
AU  - Huang, Hong
AU  - Li, Zhengying
AU  - Pan, Yinsong
TI  - Multi-Feature Manifold Discriminant Analysis for Hyperspectral Image Classification
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Hyperspectral image (HSI) provides both spatial structure and spectral information for classification, but many traditional methods simply concatenate spatial features and spectral features together that usually lead to the curse-of-dimensionality and unbalanced representation of different features. To address this issue, a new dimensionality reduction (DR) method, termed multi-feature manifold discriminant analysis (MFMDA), was proposed in this paper. At first, MFMDA explores local binary patterns (LBP) operator to extract textural features for encoding the spatial information in HSI. Then, under graph embedding framework, the intrinsic and penalty graphs of LBP and spectral features are constructed to explore the discriminant manifold structure in both spatial and spectral domains, respectively. After that, a new spatial-spectral DR model for multi-feature fusion is built to extract discriminant spatial-spectral combined features, and it not only preserves the similarity relationship between spectral features and LBP features but also possesses strong discriminating ability in the low-dimensional embedding space. Experiments on Indian Pines, Heihe and Pavia University (PaviaU) hyperspectral data sets demonstrate that the proposed MFMDA method performs significantly better than some state-of-the-art methods using only single feature or simply stacking spectral features and spatial features together, and the classification accuracies of it can reach 95.43%, 97.19% and 96.60%, respectively.
KW  - hyperspectral image
KW  - multi-feature classification
KW  - dimensionality reduction
KW  - graph embedding
KW  - spatial-spectral features
DO  - 10.3390/rs11060651
ER  -
TY  - EJOU
AU  - Li, Yong
AU  - Tong, Guofeng
AU  - Gao, Huashuai
AU  - Wang, Yuebin
AU  - Zhang, Liqiang
AU  - Chen, Huairong
TI  - Pano-RSOD: A Dataset and Benchmark for Panoramic Road Scene Object Detection
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 3
SN  - 2079-9292

AB  - Panoramic images have a wide range of applications in many fields with their ability to perceive all-round information. Object detection based on panoramic images has certain advantages in terms of environment perception due to the characteristics of panoramic images, e.g., lager perspective. In recent years, deep learning methods have achieved remarkable results in image classification and object detection. Their performance depends on the large amount of training data. Therefore, a good training dataset is a prerequisite for the methods to achieve better recognition results. Then, we construct a benchmark named Pano-RSOD for panoramic road scene object detection. Pano-RSOD contains vehicles, pedestrians, traffic signs and guiding arrows. The objects of Pano-RSOD are labelled by bounding boxes in the images. Different from traditional object detection datasets, Pano-RSOD contains more objects in a panoramic image, and the high-resolution images have 360-degree environmental perception, more annotations, more small objects and diverse road scenes. The state-of-the-art deep learning algorithms are trained on Pano-RSOD for object detection, which demonstrates that Pano-RSOD is a useful benchmark, and it provides a better panoramic image training dataset for object detection tasks, especially for small and deformed objects.
KW  - panoramic image dataset
KW  - road scene
KW  - object detection
KW  - deep learning
KW  - convolutional neural network
DO  - 10.3390/electronics8030329
ER  -
TY  - EJOU
AU  - Li, Yundong
AU  - Hu, Wei
AU  - Dong, Han
AU  - Zhang, Xueyan
TI  - Building Damage Detection from Post-Event Aerial Imagery Using Single Shot Multibox Detector
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 6
SN  - 2076-3417

AB  - Using aerial cameras, satellite remote sensing or unmanned aerial vehicles (UAV) equipped with cameras can facilitate search and rescue tasks after disasters. The traditional manual interpretation of huge aerial images is inefficient and could be replaced by machine learning-based methods combined with image processing techniques. Given the development of machine learning, researchers find that convolutional neural networks can effectively extract features from images. Some target detection methods based on deep learning, such as the single-shot multibox detector (SSD) algorithm, can achieve better results than traditional methods. However, the impressive performance of machine learning-based methods results from the numerous labeled samples. Given the complexity of post-disaster scenarios, obtaining many samples in the aftermath of disasters is difficult. To address this issue, a damaged building assessment method using SSD with pretraining and data augmentation is proposed in the current study and highlights the following aspects. (1) Objects can be detected and classified into undamaged buildings, damaged buildings, and ruins. (2) A convolution auto-encoder (CAE) that consists of VGG16 is constructed and trained using unlabeled post-disaster images. As a transfer learning strategy, the weights of the SSD model are initialized using the weights of the CAE counterpart. (3) Data augmentation strategies, such as image mirroring, rotation, Gaussian blur, and Gaussian noise processing, are utilized to augment the training data set. As a case study, aerial images of Hurricane Sandy in 2012 were maximized to validate the proposed method&rsquo;s effectiveness. Experiments show that the pretraining strategy can improve of 10% in terms of overall accuracy compared with the SSD trained from scratch. These experiments also demonstrate that using data augmentation strategies can improve mAP and mF1 by 72% and 20%, respectively. Finally, the experiment is further verified by another dataset of Hurricane Irma, and it is concluded that the paper method is feasible.
KW  - building damage assessment
KW  - post-event
KW  - deep learning
KW  - SSD
KW  - convolutional autoencoder
DO  - 10.3390/app9061128
ER  -
TY  - EJOU
AU  - Leung, Carson K.
AU  - Braun, Peter
AU  - Cuzzocrea, Alfredo
TI  - AI-Based Sensor Information Fusion for Supporting Deep Supervised Learning
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 6
SN  - 1424-8220

AB  - In recent years, artificial intelligence (AI) and its subarea of deep learning have drawn the attention of many researchers. At the same time, advances in technologies enable the generation or collection of large amounts of valuable data (e.g., sensor data) from various sources in different applications, such as those for the Internet of Things (IoT), which in turn aims towards the development of smart cities. With the availability of sensor data from various sources, sensor information fusion is in demand for effective integration of big data. In this article, we present an AI-based sensor-information fusion system for supporting deep supervised learning of transportation data generated and collected from various types of sensors, including remote sensed imagery for the geographic information system (GIS), accelerometers, as well as sensors for the global navigation satellite system (GNSS) and global positioning system (GPS). The discovered knowledge and information returned from our system provides analysts with a clearer understanding of trajectories or mobility of citizens, which in turn helps to develop better transportation models to achieve the ultimate goal of smarter cities. Evaluation results show the effectiveness and practicality of our AI-based sensor information fusion system for supporting deep supervised learning of big transportation data.
KW  - sensor
KW  - information fusion
KW  - sensor fusion
KW  - artificial intelligence (AI)
KW  - deep learning
KW  - supervised learning
KW  - data mining
KW  - transportation
KW  - geographic information system (GIS)
KW  - global navigation satellite system (GNSS)
KW  - global positioning system (GPS)
DO  - 10.3390/s19061345
ER  -
TY  - EJOU
AU  - Zabalza, Jaime
AU  - Fei, Zixiang
AU  - Wong, Cuebong
AU  - Yan, Yijun
AU  - Mineo, Carmelo
AU  - Yang, Erfu
AU  - Rodden, Tony
AU  - Mehnen, Jorn
AU  - Pham, Quang-Cuong
AU  - Ren, Jinchang
TI  - Smart Sensing and Adaptive Reasoning for Enabling Industrial Robots with Interactive Human-Robot Capabilities in Dynamic Environments—A Case Study
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 6
SN  - 1424-8220

AB  - Traditional industry is seeing an increasing demand for more autonomous and flexible manufacturing in unstructured settings, a shift away from the fixed, isolated workspaces where robots perform predefined actions repetitively. This work presents a case study in which a robotic manipulator, namely a KUKA KR90 R3100, is provided with smart sensing capabilities such as vision and adaptive reasoning for real-time collision avoidance and online path planning in dynamically-changing environments. A machine vision module based on low-cost cameras and color detection in the hue, saturation, value (HSV) space is developed to make the robot aware of its changing environment. Therefore, this vision allows the detection and localization of a randomly moving obstacle. Path correction to avoid collision avoidance for such obstacles with robotic manipulator is achieved by exploiting an adaptive path planning module along with a dedicated robot control module, where the three modules run simultaneously. These sensing/smart capabilities allow the smooth interactions between the robot and its dynamic environment, where the robot needs to react to dynamic changes through autonomous thinking and reasoning with the reaction times below the average human reaction time. The experimental results demonstrate that effective human-robot and robot-robot interactions can be realized through the innovative integration of emerging sensing techniques, efficient planning algorithms and systematic designs.
KW  - adaptive reasoning
KW  - dynamic environments
KW  - human-robot interaction
KW  - path planning
KW  - robot control
KW  - smart sensing
DO  - 10.3390/s19061354
ER  -
TY  - EJOU
AU  - Hu, Jingwen
AU  - Xia, Gui-Song
AU  - Sun, Hong
TI  - An SRTM-Aided Epipolar Resampling Method for Multi-Source High-Resolution Satellite Stereo Observation
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Binocular stereo observation with multi-source satellite images used to be challenging and impractical, but is now a valuable research issue with the introduction of powerful deep-learning-based stereo matching approaches. However, epipolar resampling, which is critical for binocular stereo observation, has rarely been studied with multi-source satellite images. The main problem is that, under the multi-source stereo mode, the epipolar-line-direction (ELD) at an image location may vary when computed with different elevations. Thus, a novel SRTM (Shuttle Radar Topography Mission)-aided approach is proposed, where a point is transformed from the original image-space to the epipolar image-space through a global rotation, followed by a block-wise homography transformation. The global rotation transfers the ELDs at the center of the overlapping area to the x-axis, and then block-wise transformation shifts the ELDs of all grid-points to the x-axis and eliminates the y-disparities between the virtual corresponding points. Experiments with both single-source and multi-source stereo images showed that the proposed method is obviously more accurate than the previous methods that do not use SRTM. Moreover, with some of the multi-source image pairs, only the proposed method ensured the y-disparities remained within &plusmn;1 pixel.
KW  - epipolar resampling
KW  - high-resolution satellite image
KW  - stereo observation
KW  - SRTM
KW  - dense image-matching
KW  - RFM
KW  - geometric transformation
DO  - 10.3390/rs11060678
ER  -
TY  - EJOU
AU  - Kulich, Miroslav
AU  - Kubalík, Jiří
AU  - Přeučil, Libor
TI  - An Integrated Approach to Goal Selection in Mobile Robot Exploration
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 6
SN  - 1424-8220

AB  - This paper deals with the problem of autonomous navigation of a mobile robot in an unknown 2D environment to fully explore the environment as efficiently as possible. We assume a terrestrial mobile robot equipped with a ranging sensor with a limited range and     360 &deg;     field of view. The key part of the exploration process is formulated as the d-Watchman Route Problem which consists of two coupled tasks&mdash;candidate goals generation and finding an optimal path through a subset of goals&mdash;which are solved in each exploration step. The latter has been defined as a constrained variant of the Generalized Traveling Salesman Problem and solved using an evolutionary algorithm. An evolutionary algorithm that uses an indirect representation and the nearest neighbor based constructive procedure was proposed to solve this problem. Individuals evolved in this evolutionary algorithm do not directly code the solutions to the problem. Instead, they represent sequences of instructions to construct a feasible solution. The problems with efficiently generating feasible solutions typically arising when applying traditional evolutionary algorithms to constrained optimization problems are eliminated this way. The proposed exploration framework was evaluated in a simulated environment on three maps and the time needed to explore the whole environment was compared to state-of-the-art exploration methods. Experimental results show that our method outperforms the compared ones in environments with a low density of obstacles by up to     12.5 %    , while it is slightly worse in office-like environments by     4.5 %     at maximum. The framework has also been deployed on a real robot to demonstrate the applicability of the proposed solution with real hardware.
KW  - path planning
KW  - routing
KW  - autonomous navigation
KW  - generalized traveling salesman problem
KW  - evolutionary algorithm
DO  - 10.3390/s19061400
ER  -
TY  - EJOU
AU  - Liu, Shengjie
AU  - Qi, Zhixin
AU  - Li, Xia
AU  - Yeh, Anthony G.
TI  - Integration of Convolutional Neural Networks and Object-Based Post-Classification Refinement for Land Use and Land Cover Mapping with Optical and SAR Data
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Object-based image analysis (OBIA) has been widely used for land use and land cover (LULC) mapping using optical and synthetic aperture radar (SAR) images because it can utilize spatial information, reduce the effect of salt and pepper, and delineate LULC boundaries. With recent advances in machine learning, convolutional neural networks (CNNs) have become state-of-the-art algorithms. However, CNNs cannot be easily integrated with OBIA because the processing unit of CNNs is a rectangular image, whereas that of OBIA is an irregular image object. To obtain object-based thematic maps, this study developed a new method that integrates object-based post-classification refinement (OBPR) and CNNs for LULC mapping using Sentinel optical and SAR data. After producing the classification map by CNN, each image object was labeled with the most frequent land cover category of its pixels. The proposed method was tested on the optical-SAR Sentinel Guangzhou dataset with 10 m spatial resolution, the optical-SAR Zhuhai-Macau local climate zones (LCZ) dataset with 100 m spatial resolution, and a hyperspectral benchmark the University of Pavia with 1.3 m spatial resolution. It outperformed OBIA support vector machine (SVM) and random forest (RF). SVM and RF could benefit more from the combined use of optical and SAR data compared with CNN, whereas spatial information learned by CNN was very effective for classification. With the ability to extract spatial features and maintain object boundaries, the proposed method considerably improved the classification accuracy of urban ground targets. It achieved overall accuracy (OA) of 95.33% for the Sentinel Guangzhou dataset, OA of 77.64% for the Zhuhai-Macau LCZ dataset, and OA of 95.70% for the University of Pavia dataset with only 10 labeled samples per class.
KW  - object-based post-classification refinement (OBPR)
KW  - convolutional neural network (CNN)
KW  - synthetic aperture radar (SAR)
KW  - land use and land cover
KW  - object-based image analysis (OBIA)
DO  - 10.3390/rs11060690
ER  -
TY  - EJOU
AU  - Wu, Jintao
AU  - Yang, Guijun
AU  - Yang, Xiaodong
AU  - Xu, Bo
AU  - Han, Liang
AU  - Zhu, Yaohui
TI  - Automatic Counting of in situ Rice Seedlings from UAV Images Based on a Deep Fully Convolutional Neural Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - The number of rice seedlings in the field is one of the main agronomic components for determining rice yield. This counting task, however, is still mainly performed using human vision rather than computer vision and is thus cumbersome and time-consuming. A fast and accurate alternative method of acquiring such data may contribute to monitoring the efficiency of crop management practices, to earlier estimations of rice yield, and as a phenotyping trait in breeding programs. In this paper, we propose an efficient method that uses computer vision to accurately count rice seedlings in a digital image. First, an unmanned aerial vehicle (UAV) equipped with red-green-blue (RGB) cameras was used to acquire field images at the seedling stage. Next, we use a regression network (Basic Network) inspired by a deep fully convolutional neural network to regress the density map and estimate the number of rice seedlings for a given UAV image. Finally, an improved version of the Basic Network, the Combined Network, is also proposed to further improve counting accuracy. To explore the efficacy of the proposed method, a novel rice seedling counting (RSC) dataset was built, which consisted of 40 images (where the number of seedlings varied between 3732 and 16,173) and corresponding manually-dotted annotations. The results demonstrated high average accuracy (higher than 93%) between counts according to the proposed method and manual (UAV image-based) rice seedling counts, and very good performance, with a high coefficient of determination (R2) (around 0.94). In conclusion, the results indicate that the proposed method is an efficient alternative for large-scale counting of rice seedlings, and offers a new opportunity for yield estimation. The RSC dataset and source code are available online.
KW  - rice seedlings
KW  - object counting
KW  - computer vision
KW  - deep learning
KW  - fully convolutional neural networks
DO  - 10.3390/rs11060691
ER  -
TY  - EJOU
AU  - Salgadoe, Arachchige S.
AU  - Robson, Andrew J.
AU  - Lamb, David W.
AU  - Schneider, Derek
TI  - A Non-Reference Temperature Histogram Method for Determining Tc from Ground-Based Thermal Imagery of Orchard Tree Canopies
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Obtaining average canopy temperature (Tc) by thresholding canopy pixels from on-ground thermal imagery has historically been undertaken using &lsquo;wet&rsquo; and &lsquo;dry&rsquo; reference surfaces in the field (reference temperature thresholding). However, this method is extremely time inefficient and can suffer inaccuracies if the surfaces are non-standardised or unable to stabilise with the environment. The research presented in this paper evaluates non-reference techniques to obtain average canopy temperature (Tc) from thermal imagery of avocado trees, both for the shaded side and sunlit side, without the need of reference temperature values. A sample of 510 thermal images (from 130 avocado trees) were acquired with a FLIR B250 handheld thermal imaging camera. Two methods based on temperature histograms were evaluated for removing non-canopy-related pixel information from the analysis, enabling Tc to be determined. These approaches included: 1) Histogram gradient thresholding based on temperature intensity changes (HG); and 2) histogram thresholding at one or more standard deviation (SD) above and below the mean. The HG method was found to be more accurate (R2 &gt; 0.95) than the SD method in defining canopy pixels and calculating Tc from each thermal image (shaded and sunlit) when compared to the standard reference temperature thresholding method. The results from this study present an alternative non-reference method for determining Tc from ground-based thermal imagery without the need of calibration surfaces. As such, it offers a more efficient and computationally autonomous method that will ultimately support the greater adoption of non-invasive thermal technologies within a precision agricultural system.
KW  - ground canopy thermal imagery
KW  - canopy pixels thresholding
KW  - handheld thermal camera
KW  - ‘dry’ and ‘wet’ reference surfaces
KW  - histogram gradient thresholding
KW  - average canopy temperature
KW  - avocado trees
DO  - 10.3390/rs11060714
ER  -
TY  - EJOU
AU  - Pang, Shiyan
AU  - Hu, Xiangyun
AU  - Zhang, Mi
AU  - Cai, Zhongliang
AU  - Liu, Fengzhu
TI  - Co-Segmentation and Superpixel-Based Graph Cuts for Building Change Detection from Bi-Temporal Digital Surface Models and Aerial Images
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Thanks to the recent development of laser scanner hardware and the technology of dense image matching (DIM), the acquisition of three-dimensional (3D) point cloud data has become increasingly convenient. However, how to effectively combine 3D point cloud data and images to realize accurate building change detection is still a hotspot in the field of photogrammetry and remote sensing. Therefore, with the bi-temporal aerial images and point cloud data obtained by airborne laser scanner (ALS) or DIM as the data source, a novel building change detection method combining co-segmentation and superpixel-based graph cuts is proposed in this paper. In this method, the bi-temporal point cloud data are firstly combined to achieve a co-segmentation to obtain bi-temporal superpixels with the simple linear iterative clustering (SLIC) algorithm. Secondly, for each period of aerial images, semantic segmentation based on a deep convolutional neural network is used to extract building areas, and this is the basis for subsequent superpixel feature extraction. Again, with the bi-temporal superpixel as the processing unit, a graph-cuts-based building change detection algorithm is proposed to extract the changed buildings. In this step, the building change detection problem is modeled as two binary classifications, and acquisition of each period&rsquo;s changed buildings is a binary classification, in which the changed building is regarded as foreground and the other area as background. Then, the graph cuts algorithm is used to obtain the optimal solution. Next, by combining the bi-temporal changed buildings and digital surface models (DSMs), these changed buildings are further classified as &ldquo;newly built,&rdquo; &ldquo;taller,&rdquo; &ldquo;demolished&rdquo;, and &ldquo;lower&rdquo;. Finally, two typical datasets composed of bi-temporal aerial images and point cloud data obtained by ALS or DIM are used to validate the proposed method, and the experiments demonstrate the effectiveness and generality of the proposed algorithm.
KW  - building change detection
KW  - co-segmentation
KW  - graph cuts
KW  - digital surface models
KW  - aerial images
DO  - 10.3390/rs11060729
ER  -
TY  - EJOU
AU  - Jin, Ren
AU  - Jiang, Jiaqi
AU  - Qi, Yuhua
AU  - Lin, Defu
AU  - Song, Tao
TI  - Drone Detection and Pose Estimation Using Relational Graph Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 6
SN  - 1424-8220

AB  - With the upsurge in use of Unmanned Aerial Vehicles (UAVs), drone detection and pose estimation by using optical sensors becomes an important research subject in cooperative flight and low-altitude security. The existing technology only obtains the position of the target UAV based on object detection methods. To achieve better adaptability and enhanced cooperative performance, the attitude information of the target drone becomes a key message to understand its state and intention, e.g., the acceleration of quadrotors. At present, most of the object 6D pose estimation algorithms depend on accurate pose annotation or a 3D target model, which costs a lot of human resource and is difficult to apply to non-cooperative targets. To overcome these problems, a quadrotor 6D pose estimation algorithm was proposed in this paper. It was based on keypoints detection (only need keypoints annotation), relational graph network and perspective-n-point (PnP) algorithm, which achieves state-of-the-art performance both in simulation and real scenario. In addition, the inference ability of our relational graph network to the keypoints of four motors was also evaluated. The accuracy and speed were improved significantly compared with the state-of-the-art keypoints detection algorithm.
KW  - drone detection
KW  - pose estimation
KW  - acceleration estimation
KW  - relational graph
DO  - 10.3390/s19061479
ER  -
TY  - EJOU
AU  - Windrim, Lloyd
AU  - Bryson, Mitch
AU  - McLean, Michael
AU  - Randle, Jeremy
AU  - Stone, Christine
TI  - Automated Mapping of Woody Debris over Harvested Forest Plantations Using UAVs, High-Resolution Imagery, and Machine Learning
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-4292

AB  - Surveying of woody debris left over from harvesting operations on managed forests is an important step in monitoring site quality, managing the extraction of residues and reconciling differences in pre-harvest inventories and actual timber yields. Traditional methods for post-harvest survey involving manual assessment of debris on the ground over small sample plots are labor-intensive, time-consuming, and do not scale well to heterogeneous landscapes. In this paper, we propose and evaluate new automated methods for the collection and interpretation of high-resolution, Unmanned Aerial Vehicle (UAV)-borne imagery over post-harvested forests for estimating quantities of fine and coarse woody debris. Using high-resolution, geo-registered color mosaics generated from UAV-borne images, we develop manual and automated processing methods for detecting, segmenting and counting both fine and coarse woody debris, including tree stumps, exploiting state-of-the-art machine learning and image processing techniques. Results are presented using imagery over a post-harvested compartment in a Pinus radiata plantation and demonstrate the capacity for both manual image annotations and automated image processing to accurately detect and quantify coarse woody debris and stumps left over after harvest, providing a cost-effective and scalable survey method for forest managers.
KW  - Unmanned Aerial Vehicles (UAVs)
KW  - computer vision
KW  - forestry
KW  - Coarse Woody Debris (CWD)
KW  - Convolutional Neural Networks (CNNs)
DO  - 10.3390/rs11060733
ER  -
TY  - EJOU
AU  - Liu, Bingxin
AU  - Li, Ying
AU  - Li, Guannan
AU  - Liu, Anling
TI  - A Spectral Feature Based Convolutional Neural Network for Classification of Sea Surface Oil Spill
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 4
SN  - 2220-9964

AB  - Spectral characteristics play an important role in the classification of oil film, but the presence of too many bands can lead to information redundancy and reduced classification accuracy. In this study, a classification model that combines spectral indices-based band selection (SIs) and one-dimensional convolutional neural networks was proposed to realize automatic oil films classification using hyperspectral remote sensing images. Additionally, for comparison, the minimum Redundancy Maximum Relevance (mRMR) was tested for reducing the number of bands. The support vector machine (SVM), random forest (RF), and Hu&rsquo;s convolutional neural networks (CNN) were trained and tested. The results show that the accuracy of classifications through the one dimensional convolutional neural network (1D CNN) models surpassed the accuracy of other machine learning algorithms such as SVM and RF. The model of SIs+1D CNN could produce a relatively higher accuracy oil film distribution map within less time than other models.
KW  - Convolutional Neural networks (CNN)
KW  - band selection
KW  - oil film
KW  - classification
DO  - 10.3390/ijgi8040160
ER  -
TY  - EJOU
AU  - Maimaitiyiming, Matthew
AU  - Sagan, Vasit
AU  - Sidike, Paheding
AU  - Kwasniewski, Misha T.
TI  - Dual Activation Function-Based Extreme Learning Machine (ELM) for Estimating Grapevine Berry Yield and Quality
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Reliable assessment of grapevine productivity is a destructive and time-consuming process. In addition, the mixed effects of grapevine water status and scion-rootstock interactions on grapevine productivity are not always linear. Despite the potential opportunity of applying remote sensing and machine learning techniques to predict plant traits, there are still limitations to previously studied techniques for vine productivity due to the complexity of the system not being adequately modeled. During the 2014 and 2015 growing seasons, hyperspectral reflectance spectra were collected using a handheld spectroradiometer in a vineyard designed to investigate the effects of irrigation level (0%, 50%, and 100%) and rootstocks (1103 Paulsen, 3309 Couderc, SO4 and Chambourcin) on vine productivity. To assess vine productivity, it is necessary to measure factors related to fruit ripeness and not just yield, as an over cropped vine may produce high-yield but poor-quality fruit. Therefore, yield, Total Soluble Solids (TSS), Titratable Acidity (TA) and the ratio TSS/TA (maturation index, IMAD) were measured. A total of 20 vegetation indices were calculated from hyperspectral data and used as input for predictive model calibration. Prediction performance of linear/nonlinear multiple regression methods and Weighted Regularized Extreme Learning Machine (WRELM) were compared with our newly developed WRELM-TanhRe. The developed method is based on two activation functions: hyperbolic tangent (Tanh) and rectified linear unit (ReLU). The results revealed that WRELM and WRELM-TanhRe outperformed the widely used multiple regression methods when model performance was tested with an independent validation dataset. WRELM-TanhRe produced the highest prediction accuracy for all the berry yield and quality parameters (R2 of 0.522&ndash;0.682 and RMSE of 2&ndash;15%), except for TA, which was predicted best with WRELM (R2 of 0.545 and RMSE of 6%). The results demonstrate the value of combining hyperspectral remote sensing and machine learning methods for improving of berry yield and quality prediction.
KW  - grapevine productivity
KW  - hyperspectral reflectance
KW  - stress
KW  - rootstock
KW  - vegetation indices
KW  - WRELM-TanhRe
KW  - neural network
KW  - activation function
DO  - 10.3390/rs11070740
ER  -
TY  - EJOU
AU  - Gebrehiwot, Asmamaw
AU  - Hashemi-Beni, Leila
AU  - Thompson, Gary
AU  - Kordjamshidi, Parisa
AU  - Langan, Thomas E.
TI  - Deep Convolutional Neural Network for Flood Extent Mapping Using Unmanned Aerial Vehicles Data
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Flooding is one of the leading threats of natural disasters to human life and property, especially in densely populated urban areas. Rapid and precise extraction of the flooded areas is key to supporting emergency-response planning and providing damage assessment in both spatial and temporal measurements. Unmanned Aerial Vehicles (UAV) technology has recently been recognized as an efficient photogrammetry data acquisition platform to quickly deliver high-resolution imagery because of its cost-effectiveness, ability to fly at lower altitudes, and ability to enter a hazardous area. Different image classification methods including SVM (Support Vector Machine) have been used for flood extent mapping. In recent years, there has been a significant improvement in remote sensing image classification using Convolutional Neural Networks (CNNs). CNNs have demonstrated excellent performance on various tasks including image classification, feature extraction, and segmentation. CNNs can learn features automatically from large datasets through the organization of multi-layers of neurons and have the ability to implement nonlinear decision functions. This study investigates the potential of CNN approaches to extract flooded areas from UAV imagery. A VGG-based fully convolutional network (FCN-16s) was used in this research. The model was fine-tuned and a k-fold cross-validation was applied to estimate the performance of the model on the new UAV imagery dataset. This approach allowed FCN-16s to be trained on the datasets that contained only one hundred training samples, and resulted in a highly accurate classification. Confusion matrix was calculated to estimate the accuracy of the proposed method. The image segmentation results obtained from FCN-16s were compared from the results obtained from FCN-8s, FCN-32s and SVMs. Experimental results showed that the FCNs could extract flooded areas precisely from UAV images compared to the traditional classifiers such as SVMs. The classification accuracy achieved by FCN-16s, FCN-8s, FCN-32s, and SVM for the water class was 97.52%, 97.8%, 94.20% and 89%, respectively.
KW  - remote sensing
KW  - convolutional neural networks
KW  - floodplain mapping
KW  - fully convolutional network
KW  - unmanned aerial vehicles
KW  - geospatial data processing
DO  - 10.3390/s19071486
ER  -
TY  - EJOU
AU  - Fernandez-Gallego, Jose A.
AU  - Buchaillot, Ma. L.
AU  - Aparicio Gutiérrez, Nieves
AU  - Nieto-Taladriz, María T.
AU  - Araus, José L.
AU  - Kefauver, Shawn C.
TI  - Automatic Wheat Ear Counting Using Thermal Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Ear density is one of the most important agronomical yield components in wheat. Ear counting is time-consuming and tedious as it is most often conducted manually in field conditions. Moreover, different sampling techniques are often used resulting in a lack of standard protocol, which may eventually affect inter-comparability of results. Thermal sensors capture crop canopy features with more contrast than RGB sensors for image segmentation and classification tasks. An automatic thermal ear counting system is proposed to count the number of ears using zenithal/nadir thermal images acquired from a moderately high resolution handheld thermal camera. Three experimental sites under different growing conditions in Spain were used on a set of 24 varieties of durum wheat for this study. The automatic pipeline system developed uses contrast enhancement and filter techniques to segment image regions detected as ears. The approach is based on the temperature differential between the ears and the rest of the canopy, given that ears usually have higher temperatures due to their lower transpiration rates. Thermal images were acquired, together with RGB images and in situ (i.e., directly in the plot) visual ear counting from the same plot segment for validation purposes. The relationship between the thermal counting values and the in situ visual counting was fairly weak (R2 = 0.40), which highlights the difficulties in estimating ear density from one single image-perspective. However, the results show that the automatic thermal ear counting system performed quite well in counting the ears that do appear in the thermal images, exhibiting high correlations with the manual image-based counts from both thermal and RGB images in the sub-plot validation ring (R2 = 0.75&ndash;0.84). Automatic ear counting also exhibited high correlation with the manual counting from thermal images when considering the complete image (R2 = 0.80). The results also show a high correlation between the thermal and the RGB manual counting using the validation ring (R2 = 0.83). Methodological requirements and potential limitations of the technique are discussed.
KW  - thermal images
KW  - ear counting
KW  - digital image processing
KW  - wheat
DO  - 10.3390/rs11070751
ER  -
TY  - EJOU
AU  - Zhao, Zhenbing
AU  - Zhen, Zhen
AU  - Zhang, Lei
AU  - Qi, Yincheng
AU  - Kong, Yinghui
AU  - Zhang, Ke
TI  - Insulator Detection Method in Inspection Image Based on Improved Faster R-CNN
T2  - Energies

PY  - 2019
VL  - 12
IS  - 7
SN  - 1996-1073

AB  - The detection of insulators in power transmission and transformation inspection images is the basis for insulator state detection and fault diagnosis in thereafter. Aiming at the detection of insulators with different aspect ratios and scales and ones with mutual occlusion, a method of insulator inspection image based on the improved faster region-convolutional neural network (R-CNN) is put forward in this paper. By constructing a power transmission and transformation insulation equipment detection dataset and fine-tuning the faster R-CNN model, the anchor generation method and non-maximum suppression (NMS) in the region proposal network (RPN) of the faster R-CNN model were improved, thus realizing a better detection of insulators. The experimental results show that the average precision (AP) value of the faster R-CNN model was increased to 0.818 with the improved anchor generation method under the VGG-16 Net. In addition, the detection effect of different aspect ratios and different scales of insulators in the inspection images was improved significantly, and the occlusion of insulators could be effectively distinguished and detected using the improved NMS.
KW  - insulator
KW  - Faster R-CNN
KW  - object detection
KW  - RPN
KW  - deep learning
DO  - 10.3390/en12071204
ER  -
TY  - EJOU
AU  - Zhang, Xiaodong
AU  - Zhu, Kun
AU  - Chen, Guanzhou
AU  - Tan, Xiaoliang
AU  - Zhang, Lifei
AU  - Dai, Fan
AU  - Liao, Puyun
AU  - Gong, Yuanfu
TI  - Geospatial Object Detection on High Resolution Remote Sensing Imagery Based on Double Multi-Scale Feature Pyramid Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Object detection on very-high-resolution (VHR) remote sensing imagery has attracted a lot of attention in the field of image automatic interpretation. Region-based convolutional neural networks (CNNs) have been vastly promoted in this domain, which first generate candidate regions and then accurately classify and locate the objects existing in these regions. However, the overlarge images, the complex image backgrounds and the uneven size and quantity distribution of training samples make the detection tasks more challenging, especially for small and dense objects. To solve these problems, an effective region-based VHR remote sensing imagery object detection framework named Double Multi-scale Feature Pyramid Network (DM-FPN) was proposed in this paper, which utilizes inherent multi-scale pyramidal features and combines the strong-semantic, low-resolution features and the weak-semantic, high-resolution features simultaneously. DM-FPN consists of a multi-scale region proposal network and a multi-scale object detection network, these two modules share convolutional layers and can be trained end-to-end. We proposed several multi-scale training strategies to increase the diversity of training data and overcome the size restrictions of the input images. We also proposed multi-scale inference and adaptive categorical non-maximum suppression (ACNMS) strategies to promote detection performance, especially for small and dense objects. Extensive experiments and comprehensive evaluations on large-scale DOTA dataset demonstrate the effectiveness of the proposed framework, which achieves mean average precision (mAP) value of 0.7927 on validation dataset and the best mAP value of 0.793 on testing dataset.
KW  - very-high-resolution (VHR) remote sensing imagery
KW  - object detection
KW  - multi-scale pyramidal features
KW  - multi-scale strategies
DO  - 10.3390/rs11070755
ER  -
TY  - EJOU
AU  - Gao, Jianlei
AU  - Chai, Senchun
AU  - Zhang, Baihai
AU  - Xia, Yuanqing
TI  - Research on Network Intrusion Detection Based on Incremental Extreme Learning Machine and Adaptive Principal Component Analysis
T2  - Energies

PY  - 2019
VL  - 12
IS  - 7
SN  - 1996-1073

AB  - Recently, network attacks launched by malicious attackers have seriously affected modern life and enterprise production, and these network attack samples have the characteristic of type imbalance, which undoubtedly increases the difficulty of intrusion detection. In response to this problem, it would naturally be very meaningful to design an intrusion detection system (IDS) to effectively and quickly identify and detect malicious behaviors. In our work, we have proposed a method for an IDS-combined incremental extreme learning machine (I-ELM) with an adaptive principal component (A-PCA). In this method, the relevant features of network traffic are adaptively selected, where the best detection accuracy can then be obtained by I-ELM. We have used the NSL-KDD standard dataset and UNSW-NB15 standard dataset to evaluate the performance of our proposed method. Through analysis of the experimental results, we can see that our proposed method has better computation capacity, stronger generalization ability, and higher accuracy.
KW  - network intrusion detection (IDS)
KW  - incremented extreme learning machine (I-ELM)
KW  - adaptive-principal component analysis (A-PCA)
KW  - NSL-KDD
KW  - UNSW-NB15
DO  - 10.3390/en12071223
ER  -
TY  - EJOU
AU  - Li, Yangyang
AU  - Wang, Ximing
AU  - Liu, Dianxiong
AU  - Guo, Qiuju
AU  - Liu, Xin
AU  - Zhang, Jie
AU  - Xu, Yitao
TI  - On the Performance of Deep Reinforcement Learning-Based Anti-Jamming Method Confronting Intelligent Jammer
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 7
SN  - 2076-3417

AB  - With the development of access technologies and artificial intelligence, a deep reinforcement learning (DRL) algorithm is proposed into channel accessing and anti-jamming. Assuming the jamming modes are sweeping, comb, dynamic and statistic, the DRL-based method through training can almost perfectly avoid jamming signal and communicate successfully. Instead, in this paper, from the perspective of jammers, we investigate the performance of a DRL-based anti-jamming method. First of all, we design an intelligent jamming method based on reinforcement learning to combat the DRL-based user. Then, we theoretically analyze the condition when the DRL-based anti-jamming algorithm cannot converge, and provide the proof. Finally, in order to investigate the performance of DRL-based method, various scenarios where users with different communicating modes combat jammers with different jamming modes are compared. As the simulation results show, the theoretical analysis is verified, and the proposed RL-based jamming can effectively restrict the performance of DRL-based anti-jamming method.
KW  - deep reinforcement learning
KW  - Q-learning
KW  - intelligent anti-jamming
KW  - intelligent jamming
DO  - 10.3390/app9071361
ER  -
TY  - EJOU
AU  - Ostovar, Ahmad
AU  - Talbot, Bruce
AU  - Puliti, Stefano
AU  - Astrup, Rasmus
AU  - Ringdahl, Ola
TI  - Detection and Classification of Root and Butt-Rot (RBR) in Stumps of Norway Spruce Using RGB Images and Machine Learning
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Root and butt-rot (RBR) has a significant impact on both the material and economic outcome of timber harvesting, and therewith on the individual forest owner and collectively on the forest and wood processing industries. An accurate recording of the presence of RBR during timber harvesting would enable a mapping of the location and extent of the problem, providing a basis for evaluating spread in a climate anticipated to enhance pathogenic growth in the future. Therefore, a system to automatically identify and detect the presence of RBR would constitute an important contribution to addressing the problem without increasing workload complexity for the machine operator. In this study, we developed and evaluated an approach based on RGB images to automatically detect tree stumps and classify them as to the absence or presence of rot. Furthermore, since knowledge of the extent of RBR is valuable in categorizing logs, we also classify stumps into three classes of infestation; rot = 0%, 0% &lt; rot &lt; 50% and rot &ge; 50%. In this work we used deep-learning approaches and conventional machine-learning algorithms for detection and classification tasks. The results showed that tree stumps were detected with precision rate of 95% and recall of 80%. Using only the correct output (TP) of the stump detector, stumps without and with RBR were correctly classified with accuracy of 83.5% and 77.5%, respectively. Classifying rot into three classes resulted in 79.4%, 72.4%, and 74.1% accuracy for stumps with rot = 0%, 0% &lt; rot &lt; 50%, and rot &ge; 50%, respectively. With some modifications, the developed algorithm could be used either during the harvesting operation to detect RBR regions on the tree stumps or as an RBR detector for post-harvest assessment of tree stumps and logs.
KW  - deep learning
KW  - forest harvesting
KW  - tree stumps
KW  - automatic detection and classification
DO  - 10.3390/s19071579
ER  -
TY  - EJOU
AU  - Azabi, Yousef
AU  - Savvaris, Al
AU  - Kipouros, Timoleon
TI  - Artificial Intelligence to Enhance Aerodynamic Shape Optimisation of the Aegis UAV
T2  - Machine Learning and Knowledge Extraction

PY  - 2019
VL  - 1
IS  - 2
SN  - 2504-4990

AB  - This article presents an optimisation framework that uses stochastic multi-objective optimisation, combined with an Artificial Neural Network (ANN), and describes its application to the aerodynamic design of aircraft shapes. The framework uses the Multi-Objective Particle Swarm Optimisation (MOPSO) algorithm and the obtained results confirm that the proposed technique provides highly optimal solutions in less computational time than other approaches to the same design problem. The main idea was to focus computational effort on worthwhile design solutions rather than exploring and evaluating all possible solutions in the design space. It is shown that the number of valid solutions obtained using ANN-MOPSO compared to MOPSO for 3000 evaluations grew from 529 to 1006 (90% improvement) with a penalty of only 8.3% (11 min) in computational time. It is demonstrated that including an ANN, the ANN-MOPSO with 3000 evaluations produced a larger number of valid solutions than the MOPSO with 5500 evaluations, and in 33% less computational time (64 min). This is taken as confirmation of the potential power of ANNs when applied to this type of design problem.
KW  - machine learning
KW  - data visualization
KW  - Multi-Objective Particle Swarm Optimisation
KW  - Multi-Objective Tabu Search
KW  - nimrod/tool
KW  - parallel coordinates
KW  - Athena Vortex Lattice
DO  - 10.3390/make1020033
ER  -
TY  - EJOU
AU  - Hong, Suk-Ju
AU  - Han, Yunhyeok
AU  - Kim, Sang-Yeon
AU  - Lee, Ah-Yeong
AU  - Kim, Ghiseok
TI  - Application of Deep-Learning Methods to Bird Detection Using Unmanned Aerial Vehicle Imagery
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Wild birds are monitored with the important objectives of identifying their habitats and estimating the size of their populations. Especially in the case of migratory bird, they are significantly recorded during specific periods of time to forecast any possible spread of animal disease such as avian influenza. This study led to the construction of deep-learning-based object-detection models with the aid of aerial photographs collected by an unmanned aerial vehicle (UAV). The dataset containing the aerial photographs includes diverse images of birds in various bird habitats and in the vicinity of lakes and on farmland. In addition, aerial images of bird decoys are captured to achieve various bird patterns and more accurate bird information. Bird detection models such as Faster Region-based Convolutional Neural Network (R-CNN), Region-based Fully Convolutional Network (R-FCN), Single Shot MultiBox Detector (SSD), Retinanet, and You Only Look Once (YOLO) were created and the performance of all models was estimated by comparing their computing speed and average precision. The test results show Faster R-CNN to be the most accurate and YOLO to be the fastest among the models. The combined results demonstrate that the use of deep-learning-based detection methods in combination with UAV aerial imagery is fairly suitable for bird detection in various environments.
KW  - deep learning
KW  - convolutional neural networks
KW  - unmanned aerial vehicle
KW  - bird detection
DO  - 10.3390/s19071651
ER  -
TY  - EJOU
AU  - Mao, Huihui
AU  - Meng, Jihua
AU  - Ji, Fujiang
AU  - Zhang, Qiankun
AU  - Fang, Huiting
TI  - Comparison of Machine Learning Regression Algorithms for Cotton Leaf Area Index Retrieval Using Sentinel-2 Spectral Bands
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 7
SN  - 2076-3417

AB  - Leaf area index (LAI) is a crucial crop biophysical parameter that has been widely used in a variety of fields. Five state-of-the-art machine learning regression algorithms (MLRAs), namely, artificial neural network (ANN), support vector regression (SVR), Gaussian process regression (GPR), random forest (RF) and gradient boosting regression tree (GBRT), have been used in the retrieval of cotton LAI with Sentinel-2 spectral bands. The performances of the five machine learning models are compared for better applications of MLRAs in remote sensing, since challenging problems remain in the selection of MLRAs for crop LAI retrieval, as well as the decision as to the optimal number for the training sample size and spectral bands to different MLRAs. A comprehensive evaluation was employed with respect to model accuracy, computational efficiency, sensitivity to training sample size and sensitivity to spectral bands. We conducted the comparison of five MLRAs in an agricultural area of Northwest China over three cotton seasons with the corresponding field campaigns for modeling and validation. Results show that the GBRT model outperforms the other models with respect to model accuracy in average (       R 2   &macr;      = 0.854,       R M S E  &macr;      = 0.674 and       M A E  &macr;      = 0.456). SVR achieves the best performance in computational efficiency, which means it is fast to train, and to validate that it has great potentials to deliver near-real-time operational products for crop management. As for sensitivity to training sample size, GBRT behaves as the most robust model, and provides the best model accuracy on the average among the variations of training sample size, compared with other models (       R 2   &macr;      = 0.884,       R M S E  &macr;      = 0.615 and       M A E  &macr;      = 0.452). Spectral bands sensitivity analysis with dCor (distance correlation), combined with the backward elimination approach, indicates that SVR, GPR and RF provide relatively robust performance to the spectral bands, while ANN outperforms the other models in terms of model accuracy on the average among the reduction of spectral bands (       R 2   &macr;      = 0.881,       R M S E  &macr;      = 0.625 and       M A E  &macr;      = 0.480). A comprehensive evaluation indicates that GBRT is an appealing alternative for cotton LAI retrieval, except for its computational efficiency. Despite the different performance of the ML models, all models exhibited considerable potential for cotton LAI retrieval, which could offer accurate crop parameters information timely and accurately for crop fields management and agricultural production decisions.
KW  - leaf area index (LAI)
KW  - machine learning
KW  - Sentinel-2
KW  - sensitivity analysis
KW  - training sample size
KW  - spectral bands
DO  - 10.3390/app9071459
ER  -
TY  - EJOU
AU  - Li, You
AU  - Zahran, Shady
AU  - Zhuang, Yuan
AU  - Gao, Zhouzheng
AU  - Luo, Yiran
AU  - He, Zhe
AU  - Pei, Ling
AU  - Chen, Ruizhi
AU  - El-Sheimy, Naser
TI  - IMU/Magnetometer/Barometer/Mass-Flow Sensor Integrated Indoor Quadrotor UAV Localization with Robust Velocity Updates
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - Velocity updates have been proven to be important for constraining motion-sensor-based dead-reckoning (DR) solutions in indoor unmanned aerial vehicle (UAV) applications. The forward velocity from a mass flow sensor and the lateral and vertical non-holonomic constraints (NHC) can be utilized for three-dimensional (3D) velocity updates. However, it is observed that (a) the quadrotor UAV may have a vertical velocity trend when it is controlled to move horizontally; (b) the quadrotor may have a pitch angle when moving horizontally; and (c) the mass flow sensor may suffer from sensor errors, especially the scale factor error. Such phenomenons degrade the performance of velocity updates. Thus, this paper presents a multi-sensor integrated localization system that has more effective sensor interactions. Specifically, (a) the barometer data are utilized to detect height changes and thus determine the weight of vertical velocity update; (b) the pitch angle from the inertial measurement unit (IMU) and magnetometer data fusion is used to set the weight of forward velocity update; and (c) an extra mass flow sensor calibration module is introduced. Indoor flight tests have indicated the effectiveness of the proposed sensor interaction strategies in enhancing indoor quadrotor DR solutions, which can also be used for detecting outliers in external localization technologies such as ultrasonics.
KW  - indoor localization
KW  - quadrotor UAV
KW  - air flow
KW  - inertial sensor
KW  - magnetometer
KW  - barometer
KW  - ultrasonic
KW  - Kalman filter
DO  - 10.3390/rs11070838
ER  -
TY  - EJOU
AU  - Liu, Yao
AU  - Shi, Jianmai
AU  - Liu, Zhong
AU  - Huang, Jincai
AU  - Zhou, Tianren
TI  - Two-Layer Routing for High-Voltage Powerline Inspection by Cooperated Ground Vehicle and Drone
T2  - Energies

PY  - 2019
VL  - 12
IS  - 7
SN  - 1996-1073

AB  - A novel high-voltage powerline inspection system was investigated, which consists of the cooperated ground vehicle and drone. The ground vehicle acts as a mobile platform that can launch and recycle the drone, while the drone can fly over the powerline for inspection within limited endurance. This inspection system enables the drone to inspect powerline networks in a very large area. Both vehicle&rsquo; route in the road network and drone&rsquo;s routes along the powerline network have to be optimized for improving the inspection efficiency, which generates a new Two-Layer Point-Arc Routing Problem (2L-PA-RP). Two constructive heuristics were designed based on &ldquo;Cluster First, Route Second&rdquo; and &ldquo;Route First, Split Second&rdquo;. Then, local search strategies were developed to further improve the quality of the solution. To test the performance of the proposed algorithms, different-scale practical cases were designed based on the road network and powerline network of Ji&rsquo;an, China. Sensitivity analysis on the parameters related to the drone&rsquo;s inspection speed and battery capacity was conducted. Computational results indicate that technical improvement on the inspection sensor is more important for the cooperated ground vehicle and drone system.
KW  - high-voltage powerline inspection
KW  - vehicle routing
KW  - arc routing
KW  - drone
KW  - heuristic
DO  - 10.3390/en12071385
ER  -
TY  - EJOU
AU  - Yang, Bo
AU  - Zhang, Sheng
AU  - Tian, Yan
AU  - Li, Bijun
TI  - Front-Vehicle Detection in Video Images Based on Temporal and Spatial Characteristics
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 7
SN  - 1424-8220

AB  - Assisted driving and unmanned driving have been areas of focus for both industry and academia. Front-vehicle detection technology, a key component of both types of driving, has also attracted great interest from researchers. In this paper, to achieve front-vehicle detection in unmanned or assisted driving, a vision-based, efficient, and fast front-vehicle detection method based on the spatial and temporal characteristics of the front vehicle is proposed. First, a method to extract the motion vector of the front vehicle is put forward based on Oriented FAST and Rotated BRIEF (ORB) and the spatial position constraint. Then, by analyzing the differences between the motion vectors of the vehicle and those of the background, feature points of the vehicle are extracted. Finally, a feature-point clustering method based on a combination of temporal and spatial characteristics are applied to realize front-vehicle detection. The effectiveness of the proposed algorithm is verified using a large number of videos.
KW  - motion vector
KW  - vanishing point
KW  - clustering
KW  - front-vehicle detection
DO  - 10.3390/s19071728
ER  -
TY  - EJOU
AU  - Du, Zhenrong
AU  - Yang, Jianyu
AU  - Ou, Cong
AU  - Zhang, Tingting
TI  - Smallholder Crop Area Mapped with a Semantic Segmentation Deep Learning Method
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 7
SN  - 2072-4292

AB  - The growing population in China has led to an increasing importance of crop area (CA) protection. A powerful tool for acquiring accurate and up-to-date CA maps is automatic mapping using information extracted from high spatial resolution remote sensing (RS) images. RS image information extraction includes feature classification, which is a long-standing research issue in the RS community. Emerging deep learning techniques, such as the deep semantic segmentation network technique, are effective methods to automatically discover relevant contextual features and get better image classification results. In this study, we exploited deep semantic segmentation networks to classify and extract CA from high-resolution RS images. WorldView-2 (WV-2) images with only Red-Green-Blue (RGB) bands were used to confirm the effectiveness of the proposed semantic classification framework for information extraction and the CA mapping task. Specifically, we used the deep learning framework TensorFlow to construct a platform for sampling, training, testing, and classifying to extract and map CA on the basis of DeepLabv3+. By leveraging per-pixel and random sample point accuracy evaluation methods, we conclude that the proposed approach can efficiently obtain acceptable accuracy (Overall Accuracy = 95%, Kappa = 0.90) of CA classification in the study area, and the approach performs better than other deep semantic segmentation networks (U-Net/PspNet/SegNet/DeepLabv2) and traditional machine learning methods, such as Maximum Likelihood (ML), Support Vector Machine (SVM), and RF (Random Forest). Furthermore, the proposed approach is highly scalable for the variety of crop types in a crop area. Overall, the proposed approach can train a precise and effective model that is capable of adequately describing the small, irregular fields of smallholder agriculture and handling the great level of details in RGB high spatial resolution images.
KW  - agriculture
KW  - high spatial resolution images
KW  - semantic labeling
DO  - 10.3390/rs11070888
ER  -
TY  - EJOU
AU  - Khan, Nabeel
AU  - Martini, Maria G.
TI  - Bandwidth Modeling of Silicon Retinas for Next Generation Visual Sensor Networks
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - Silicon retinas, also known as Dynamic Vision Sensors (DVS) or event-based visual sensors, have shown great advantages in terms of low power consumption, low bandwidth, wide dynamic range and very high temporal resolution. Owing to such advantages as compared to conventional vision sensors, DVS devices are gaining more and more attention in various applications such as drone surveillance, robotics, high-speed motion photography, etc. The output of such sensors is a sequence of events rather than a series of frames as for classical cameras. Estimating the data rate of the stream of events associated with such sensors is needed for the appropriate design of transmission systems involving such sensors. In this work, we propose to consider information about the scene content and sensor speed to support such estimation, and we identify suitable metrics to quantify the complexity of the scene for this purpose. According to the results of this study, the event rate shows an exponential relationship with the metric associated with the complexity of the scene and linear relationships with the speed of the sensor. Based on these results, we propose a two-parameter model for the dependency of the event rate on scene complexity and sensor speed. The model achieves a prediction accuracy of approximately 88.4% for the outdoor environment along with the overall prediction performance of approximately 84%.
KW  - neuromorphic engineering
KW  - dynamic and active-pixel vision sensor
KW  - scene complexity
KW  - neuromorphic event rate
KW  - gradient approximation
KW  - scene texture
KW  - Sobel
KW  - Roberts
KW  - Prewitt
DO  - 10.3390/s19081751
ER  -
TY  - EJOU
AU  - Zhang, Hehu
AU  - Wang, Xiushan
AU  - Chen, Ying
AU  - Jiang, Guoqiang
AU  - Lin, Shifeng
TI  - Research on Vision-Based Navigation for Plant Protection UAV under the Near Color Background
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 4
SN  - 2073-8994

AB  - GPS (Global Positioning System) navigation in agriculture is facing many challenges, such as weak signals in orchards and the high cost for small plots of farmland. With the reduction of camera cost and the emergence of excellent visual algorithms, visual navigation can solve the above problems. Visual navigation is a navigation technology that uses cameras to sense environmental information as the basis of an aircraft flight. It is mainly divided into five parts: Image acquisition, landmark recognition, route planning, flight control, and obstacle avoidance. Here, landmarks are plant canopy, buildings, mountains, and rivers, with unique geographical characteristics in a place. During visual navigation, landmark location and route tracking are key links. When there are significant color-differences (for example, the differences among red, green, and blue) between a landmark and the background, the landmark can be recognized based on classical visual algorithms. However, in the case of non-significant color-differences (for example, the differences between dark green and vivid green) between a landmark and the background, there are no robust and high-precision methods for landmark identification. In view of the above problem, visual navigation in a maize field is studied. First, the block recognition method based on fine-tuned Inception-V3 is developed; then, the maize canopy landmark is recognized based on the above method; finally, local navigation lines are extracted from the landmarks based on the maize canopy grayscale gradient law. The results show that the accuracy is 0.9501. When the block number is 256, the block recognition method achieves the best segmentation. The average segmentation quality is 0.87, and time is 0.251 s. This study suggests that stable visual semantic navigation can be achieved under the near color background. It will be an important reference for the navigation of plant protection UAV (Unmanned Aerial Vehicle).
KW  - landmark location
KW  - route tracking
KW  - inception-V3
KW  - visual navigation
KW  - grayscale gradient law
DO  - 10.3390/sym11040533
ER  -
TY  - EJOU
AU  - Yang, Lingyu
AU  - Feng, Xiaoke
AU  - Zhang, Jing
AU  - Shu, Xiangqian
TI  - Multi-Ray Modeling of Ultrasonic Sensors and Application for Micro-UAV Localization in Indoor Environments
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - Due to its payload, size and computational limits, localizing a micro air vehicle (MAV) using only its onboard sensors in an indoor environment is a challenging problem in practice. This paper introduces an indoor localization approach that relies on only the inertial measurement unit (IMU) and four ultrasonic sensors. Specifically, a novel multi-ray ultrasonic sensor model is proposed to provide a rapid and accurate approximation of the complex beam pattern of the ultrasonic sensors. A fast algorithm for calculating the Jacobian matrix of the measurement function is presented, and then an extended Kalman filter (EKF) is used to fuse the information from the ultrasonic sensors and the IMU. A test based on a MaxSonar MB1222 sensor demonstrates the accuracy of the model, and a simulation and experiment based on the     T h a l e s  I I     MAV platform are conducted. The results indicate good localization performance and robustness against measurement noises.
KW  - indoor location
KW  - multi-ray model of ultrasonic sensors
KW  - micro-UAV
KW  - extended Kalman filter
DO  - 10.3390/s19081770
ER  -
TY  - EJOU
AU  - Girolamo-Neto, Cesare D.
AU  - Sanches, Ieda D.
AU  - Neves, Alana K.
AU  - Prudente, Victor H.
AU  - Körting, Thales S.
AU  - Picoli, Michelle C.
AU  - Aragão, Luiz E.
TI  - Assessment of Texture Features for Bermudagrass (Cynodon dactylon) Detection in Sugarcane Plantations
T2  - Drones

PY  - 2019
VL  - 3
IS  - 2
SN  - 2504-446X

AB  - Sugarcane products contribute significantly to the Brazilian economy, generating U.S. $12.2 billion in revenue in 2018. Identifying and monitoring factors that induce yield reduction, such as weed occurrence, is thus imperative. The detection of Bermudagrass in sugarcane crops using remote sensing data, however, is a challenge considering their spectral similarity. To overcome this limitation, this paper aims to explore the potential of texture features derived from images acquired by an optical sensor onboard anunmanned aerial vehicle (UAV) to detect Bermudagrass in sugarcane. Aerial images with a spatial resolution of 2 cm were acquired from a sugarcane field in Brazil. The Green-Red Vegetation Index and several texture metrics derived from the gray-level co-occurrence matrix were calculated to perform an automatic classification using arandom forest algorithm. Adding texture metrics to the classification process improved the overall accuracy from 83.00% to 92.54%, and this improvement was greater considering larger window sizes, since they representeda texture transition between two targets. Production losses induced by Bermudagrass presence reached 12.1 tons &times; ha&minus;1 in the study site. This study not only demonstrated the capacity of UAV images to overcome the well-known limitation of detecting Bermudagrass in sugarcane crops, but also highlighted the importance of texture for high-accuracy quantification of weed invasion in sugarcane crops.
KW  - remote sensing
KW  - classification
KW  - agriculture
KW  - weed detection
KW  - UAV images
DO  - 10.3390/drones3020036
ER  -
TY  - EJOU
AU  - Zhu, Jiasong
AU  - Chen, Siyuan
AU  - Tu, Wei
AU  - Sun, Ke
TI  - Tracking and Simulating Pedestrian Movements at Intersections Using Unmanned Aerial Vehicles
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 8
SN  - 2072-4292

AB  - For a city to be livable and walkable is the ultimate goal of future cities. However, conflicts among pedestrians, vehicles, and cyclists at traffic intersections are becoming severe in high-density urban transportation areas, especially in China. Correspondingly, the transit time at intersections is becoming prolonged, and pedestrian safety is becoming endangered. Simulating pedestrian movements at complex traffic intersections is necessary to optimize the traffic organization. We propose an unmanned aerial vehicle (UAV)-based method for tracking and simulating pedestrian movements at intersections. Specifically, high-resolution videos acquired by a UAV are used to recognize and position moving targets, including pedestrians, cyclists, and vehicles, using the convolutional neural network. An improved social force-based motion model is proposed, considering the conflicts among pedestrians, cyclists, and vehicles. In addition, maximum likelihood estimation is performed to calibrate an improved social force model. UAV videos of intersections in Shenzhen are analyzed to demonstrate the performance of the presented approach. The results demonstrate that the proposed social force-based motion model can effectively simulate the movement of pedestrians and cyclists at road intersections. The presented approach provides an alternative method to track and simulate pedestrian movements, thus benefitting the organization of pedestrian flow and traffic signals controlling the intersections.
KW  - pedestrian simulation
KW  - social force model
KW  - intersection
KW  - UAV
KW  - convolutional neural network
KW  - deep learning
DO  - 10.3390/rs11080925
ER  -
TY  - EJOU
AU  - Tzitzilonis, Vasileios
AU  - Malandrakis, Konstantinos
AU  - Zanotti Fragonara, Luca
AU  - Gonzalez Domingo, Jose A.
AU  - Avdelidis, Nicolas P.
AU  - Tsourdos, Antonios
AU  - Forster, Kevin
TI  - Inspection of Aircraft Wing Panels Using Unmanned Aerial Vehicles
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - In large civil aircraft manufacturing, a time-consuming post-production process is the non-destructive inspection of wing panels. This work aims to address this challenge and improve the defects&rsquo; detection by performing automated aerial inspection using a small off-the-shelf multirotor. The UAV is equipped with a wide field-of-view camera and an ultraviolet torch for implementing non-invasive imaging inspection. In particular, the UAV is programmed to perform the complete mission and stream video, in real-time, to the ground control station where the defects&rsquo; detection algorithm is executed. The proposed platform was mathematically modelled in MATLAB/SIMULINK in order to assess the behaviour of the system using a path following method during the aircraft wing inspection. In addition, two defect detection algorithms were implemented and tested on a dataset containing images obtained during inspection at Airbus facilities. The results show that for the current dataset the proposed methods can identify all the images containing defects.
KW  - Non-Destructive Testing
KW  - ultraviolet light
KW  - automated inspection
KW  - defects detection
KW  - UAV
KW  - image processing
DO  - 10.3390/s19081824
ER  -
TY  - EJOU
AU  - Hu, Zhongyang
AU  - Dietz, Andreas J.
AU  - Kuenzer, Claudia
TI  - Deriving Regional Snow Line Dynamics during the Ablation Seasons 1984–2018 in European Mountains
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 8
SN  - 2072-4292

AB  - Snowmelt in the mid-latitude European mountains is undergoing significant spatiotemporal changes. Regional snow line elevation (RSLE) is an appropriate indicator for assessing snow cover variations in mountain areas. To derive regional snow line dynamics during the ablation seasons 1984&ndash;2018, the present study unprecedentedly introduced a readily applicable framework. The framework constitutes four steps: atmospheric and topographic correction, snow classification, RSLE retrieval, and regional snow line retreat curve (RSLRC) derivation. The developed framework has been successfully applied to 8641 satellite images acquired by Landsat, ASTER, and Sentinel-2. The results of the intra-annual regional snow line variations show that: (1) regional snow lines in the Alpine catchments preserve the longest; (2) RSLEs are lower in the northern Pyrenees than in the southern part; (3) regional snow lines persist the shortest in the Carpathian catchments; and (4) during the end of the ablation season 2018, intermediate snowfall events in the catchments Adda, Tagliamento, and Uzh are observed. In terms of the long-term inter-annual variations, significantly accelerating snow line recession is detected in the northern Pyrenean catchment Ariege. In the Alpine catchment Alpenrhein and Drac, RSLRCs are shifting towards lower accumulated air-temperature (AT) significantly, with the magnitude of &minus;3.77 &deg;C&middot;a&minus;1 (Alpenrhein) and &minus;3.99 &deg;C&middot;a&minus;1 (Drac).
KW  - snow line dynamics
KW  - European mountains
KW  - ablation season
KW  - regional snow line elevation (RSLE)
KW  - regional snow line retreat curves (RSLRCs)
KW  - M-estimation
KW  - Landsat
KW  - ASTER
KW  - Sentinel-2
KW  - time-series
DO  - 10.3390/rs11080933
ER  -
TY  - EJOU
AU  - Barbedo, Jayme G.
TI  - A Review on the Use of Unmanned Aerial Vehicles and Imaging Sensors for Monitoring and Assessing Plant Stresses
T2  - Drones

PY  - 2019
VL  - 3
IS  - 2
SN  - 2504-446X

AB  - Unmanned aerial vehicles (UAVs) are becoming a valuable tool to collect data in a variety of contexts. Their use in agriculture is particularly suitable, as those areas are often vast, making ground scouting difficult, and sparsely populated, which means that injury and privacy risks are not as important as in urban settings. Indeed, the use of UAVs for monitoring and assessing crops, orchards, and forests has been growing steadily during the last decade, especially for the management of stresses such as water, diseases, nutrition deficiencies, and pests. This article presents a critical overview of the main advancements on the subject, focusing on the strategies that have been used to extract the information contained in the images captured during the flights. Based on the information found in more than 100 published articles and on our own research, a discussion is provided regarding the challenges that have already been overcome and the main research gaps that still remain, together with some suggestions for future research.
KW  - drone
KW  - UAV
KW  - UAS
KW  - precision agriculture
KW  - stress
KW  - crop
KW  - orchard
DO  - 10.3390/drones3020040
ER  -
TY  - EJOU
AU  - Li, Shuman
AU  - Li, Chao
AU  - Xu, Liyang
AU  - Yang, Wenjing
AU  - Chen, Xucan
TI  - Numerical Simulation and Analysis of Fish-Like Robots Swarm
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 8
SN  - 2076-3417

AB  - Artificial fish-like robot is an important branch of underwater robot research. At present, most of fish-like robot research focuses on single robot mechanism behavior, some research pays attention to the influence of the hydro-environment on robot crowds but does not reach a unified conclusion on the efficiency of fish-like robots swarm. In this work, the fish-like robots swarm is studied by numerical simulation. Four different formations, including the tandem, the phalanx, the diamond, and the rectangle are conducted by changing the spacing between fishes. The results show that at close spacing, the fish in the back can obtain a large wake from the front fish, but suffers large lateral power loss from the lateral fish. On the contrary, when the spacing is large, both the wake and pressure caused by the front and side fishes become small. In terms of the average swimming efficiency of fish swarms, we find that when the fish spacing is less than     1.25 L     (L is the length of the fish body), the tandem swarm is the best choice. When the spacing is     1.25 L    , the tandem, diamond and rectangle swarms have similar efficiency. When the spacing is larger than     1.25 L    , the rectangle swarm is more efficient than other formations. The findings will provide significant guidance for the control of fish-like robots swarm.
KW  - fish schooling
KW  - hydrodynamics
KW  - swimming efficiency
KW  - robots swarm
KW  - robot controlling
DO  - 10.3390/app9081652
ER  -
TY  - EJOU
AU  - Pham, Tien D.
AU  - Xia, Junshi
AU  - Ha, Nam T.
AU  - Bui, Dieu T.
AU  - Le, Nga N.
AU  - Tekeuchi, Wataru
TI  - A Review of Remote Sensing Approaches for Monitoring Blue Carbon Ecosystems: Mangroves, Seagrassesand Salt Marshes during 2010–2018
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 8
SN  - 1424-8220

AB  - Blue carbon (BC) ecosystems are an important coastal resource, as they provide a range of goods and services to the environment. They play a vital role in the global carbon cycle by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, there has been a large reduction in the global BC ecosystems due to their conversion to agriculture and aquaculture, overexploitation, and removal for human settlements. Effectively monitoring BC ecosystems at large scales remains a challenge owing to practical difficulties in monitoring and the time-consuming field measurement approaches used. As a result, sensible policies and actions for the sustainability and conservation of BC ecosystems can be hard to implement. In this context, remote sensing provides a useful tool for mapping and monitoring BC ecosystems faster and at larger scales. Numerous studies have been carried out on various sensors based on optical imagery, synthetic aperture radar (SAR), light detection and ranging (LiDAR), aerial photographs (APs), and multispectral data. Remote sensing-based approaches have been proven effective for mapping and monitoring BC ecosystems by a large number of studies. However, to the best of our knowledge, this is the first comprehensive review on the applications of remote sensing techniques for mapping and monitoring BC ecosystems. The main goal of this review is to provide an overview and summary of the key studies undertaken from 2010 onwards on remote sensing applications for mapping and monitoring BC ecosystems. Our review showed that optical imagery, such as multispectral and hyper-spectral data, is the most common for mapping BC ecosystems, while the Landsat time-series are the most widely-used data for monitoring their changes on larger scales. We investigate the limitations of current studies and suggest several key aspects for future applications of remote sensing combined with state-of-the-art machine learning techniques for mapping coastal vegetation and monitoring their extents and changes.
KW  - coastal ecosystems
KW  - remote sensing
KW  - blue carbon
KW  - mangroves
KW  - seagrasses
KW  - salt marshes
DO  - 10.3390/s19081933
ER  -
TY  - EJOU
AU  - Jiang, Liubing
AU  - Zhou, Xiaolong
AU  - Che, Li
AU  - Rong, Shuwei
AU  - Wen, Hexin
TI  - Feature Extraction and Reconstruction by Using 2D-VMD Based on Carrier-Free UWB Radar Application in Human Motion Recognition
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 9
SN  - 1424-8220

AB  - As the size of the radar hardware platform becomes smaller and smaller, the cost becomes lower and lower. The application of indoor radar-based human motion recognition has become a reality, which can be realized in a low-cost device with simple architecture. Compared with narrow-band radar (such as continuous wave radar, etc.), the human motion echo signal of the carrier-free ultra-wideband (UWB) radar contains more abundant characteristic information of human motion, which is helpful for identifying different types of human motion. In this paper, a novel feature extraction method by two-dimensional variational mode decomposition (2D-VMD) algorithm is proposed. And it is used for extracting the primary features of human motion. The 2D-VMD algorithm is an adaptive non-recursive multiscale decomposition method for nonlinear and nonstationary signals. Firstly, the original 2D radar echo signals are decomposed by the 2D-VMD algorithm to capture several 2D intrinsic mode function (BIMFs) which represent different groups of central frequency components of a certain type of human motion. Secondly, original echo signals are reconstructed according to the several BIMFs, which not only have a certain inhibitory effect on the clutter in the echo signal, but can also further demonstrate that the BIMFs obtained by the 2D-VMD algorithm can represent the original 2D echo signal well. Finally, based on the measured ten different types of UWB radar human motion 2D echo analysis signals, the characteristics of these different types of human motion are extracted and the original echo signal are reconstructed. Then, the three indicators of the PCC, UQI, and PSNR between the original echo signals and extraction/reconstruction 2D signals are analyzed, which illustrate the effectiveness of 2D-VMD algorithm to extract feature of human motion 2D echo signals of the carrier-free UWB radar. Experimental results show that BIMFs by 2D-VMD algorithm can well represent the echo signal characteristics of this type of human motion, which is a very effective tool for human motion radar echo signal feature extraction.
KW  - carrier-free UWB Radar
KW  - human motion recognition
KW  - 2D-VMD algorithm
KW  - feature extraction
KW  - reconstruction
DO  - 10.3390/s19091962
ER  -
TY  - EJOU
AU  - Zhang, Yang
AU  - Xiong, Zhangyue
AU  - Zang, Yu
AU  - Wang, Cheng
AU  - Li, Jonathan
AU  - Li, Xiang
TI  - Topology-Aware Road Network Extraction via Multi-Supervised Generative Adversarial Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - Road network extraction from remote sensing images has played an important role in various areas. However, due to complex imaging conditions and terrain factors, such as occlusion and shades, it is very challenging to extract road networks with complete topology structures. In this paper, we propose a learning-based road network extraction framework via a Multi-supervised Generative Adversarial Network (MsGAN), which is jointly trained by the spectral and topology features of the road network. Such a design makes the network capable of learning how to &ldquo;guess&rdquo; the aberrant road cases, which is caused by occlusion and shadow, based on the relationship between the road region and centerline; thus, it is able to provide a road network with integrated topology. Additionally, we also present a sample quality measurement to efficiently generate a large number of training samples with a little human interaction. Through the experiments on images from various satellites and the comprehensive comparisons to state-of-the-art approaches on the public datasets, it is demonstrated that the proposed method is able to provide high-quality results, especially for the completeness of the road network.
KW  - multi-supervised generative adversarial network
KW  - road topology reconstruction
KW  - road centerline extraction
DO  - 10.3390/rs11091017
ER  -
TY  - EJOU
AU  - Quirós Vargas, Juan J.
AU  - Zhang, Chongyuan
AU  - Smitchger, Jamin A.
AU  - McGee, Rebecca J.
AU  - Sankaran, Sindhuja
TI  - Phenotyping of Plant Biomass and Performance Traits Using Remote Sensing Techniques in Pea (Pisum sativum, L.)
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 9
SN  - 1424-8220

AB  - Field pea cultivars are constantly improved through breeding programs to enhance biotic and abiotic stress tolerance and increase seed yield potential. In pea breeding, the Above Ground Biomass (AGBM) is assessed due to its influence on seed yield, canopy closure, and weed suppression. It is also the primary yield component for peas used as a cover crop and/or grazing. Measuring AGBM is destructive and labor-intensive process. Sensor-based phenotyping of such traits can greatly enhance crop breeding efficiency. In this research, high resolution RGB and multispectral images acquired with unmanned aerial systems were used to assess phenotypes in spring and winter pea breeding plots. The Green Red Vegetation Index (GRVI), Normalized Difference Vegetation Index (NDVI), Normalized Difference Red Edge Index (NDRE), plot volume, canopy height, and canopy coverage were extracted from RGB and multispectral information at five imaging times (between 365 to 1948 accumulated degree days/ADD after 1 May) in four winter field pea experiments and at three imaging times (between 1231 to 1648 ADD) in one spring field pea experiment. The image features were compared to ground-truth data including AGBM, lodging, leaf type, days to 50% flowering, days to physiological maturity, number of the first reproductive node, and seed yield. In two of the winter pea experiments, a strong correlation between image features and seed yield was observed at 1268 ADD (flowering). An increase in correlation between image features with the phenological traits such as days to 50% flowering and days to physiological maturity was observed at about 1725 ADD in these winter pea experiments. In the spring pea experiment, the plot volume estimated from images was highly correlated with ground truth canopy height (r = 0.83) at 1231 ADD. In two other winter pea experiments and the spring pea experiment, the GRVI and NDVI features were significantly correlated with AGBM at flowering. When selected image features were used to develop a least absolute shrinkage and selection operator model for AGBM estimation, the correlation coefficient between the actual and predicted AGBM was 0.60 and 0.84 in the winter and spring pea experiments, respectively. A SPOT-6 satellite image (1.5 m resolution) was also evaluated for its applicability to assess biomass and seed yield. The image features extracted from satellite imagery showed significant correlation with seed yield in two winter field pea experiments, however, the trend was not consistent. In summary, the study supports the potential of using unmanned aerial system-based imaging techniques to estimate biomass and crop performance in pea breeding programs.
KW  - crop monitoring
KW  - prediction model
KW  - satellite imagery
KW  - vegetation indices
KW  - crop surface model
DO  - 10.3390/s19092031
ER  -
TY  - EJOU
AU  - Li, Weijia
AU  - He, Conghui
AU  - Fu, Haohuan
AU  - Zheng, Juepeng
AU  - Dong, Runmin
AU  - Xia, Maocai
AU  - Yu, Le
AU  - Luk, Wayne
TI  - A Real-Time Tree Crown Detection Approach for Large-Scale Remote Sensing Images on FPGAs
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - The on-board real-time tree crown detection from high-resolution remote sensing images is beneficial for avoiding the delay between data acquisition and processing, reducing the quantity of data transmission from the satellite to the ground, monitoring the growing condition of individual trees, and discovering the damage of trees as early as possible, etc. Existing high performance platform based tree crown detection studies either focus on processing images in a small size or suffer from high power consumption or slow processing speed. In this paper, we propose the first FPGA-based real-time tree crown detection approach for large-scale satellite images. A pipelined-friendly and resource-economic tree crown detection algorithm (PF-TCD) is designed through reconstructing and modifying the workflow of the original algorithm into three computational kernels on FPGAs. Compared with the well-optimized software implementation of the original algorithm on an Intel 12-core CPU, our proposed PF-TCD obtains the speedup of 18.75 times for a satellite image with a size of 12,188 &times; 12,576 pixels without reducing the detection accuracy. The image processing time for the large-scale remote sensing image is only 0.33 s, which satisfies the requirements of the on-board real-time data processing on satellites.
KW  - tree crown detection
KW  - high-resolution satellite images
KW  - field-programmable gate array (FPGA)
KW  - real-time processing
DO  - 10.3390/rs11091025
ER  -
TY  - EJOU
AU  - Gonzalez Viejo, Claudia
AU  - Torrico, Damir D.
AU  - Dunshea, Frank R.
AU  - Fuentes, Sigfredo
TI  - Development of Artificial Neural Network Models to Assess Beer Acceptability Based on Sensory Properties Using a Robotic Pourer: A Comparative Model Approach to Achieve an Artificial Intelligence System
T2  - Beverages

PY  - 2019
VL  - 5
IS  - 2
SN  - 2306-5710

AB  - Artificial neural networks (ANN) have become popular for optimization and prediction of parameters in foods, beverages, agriculture and medicine. For brewing, they have been explored to develop rapid methods to assess product quality and acceptability. Different beers (N = 17) were analyzed in triplicates using a robotic pourer, RoboBEER (University of Melbourne, Melbourne, Australia), to assess 15 color and foam-related parameters using computer-vision. Those samples were tested using sensory analysis for acceptability of carbonation mouthfeel, bitterness, flavor and overall liking with 30 consumers using a 9-point hedonic scale. ANN models were developed using 17 different training algorithms with 15 color and foam-related parameters as inputs and liking of four descriptors obtained from consumers as targets. Each algorithm was tested using five, seven and ten neurons and compared to select the best model based on correlation coefficients, slope and performance (mean squared error (MSE). Bayesian Regularization algorithm with seven neurons presented the best correlation (R = 0.98) and highest performance (MSE = 0.03) with no overfitting. These models may be used as a cost-effective method for fast-screening of beers during processing to assess acceptability more efficiently. The use of RoboBEER, computer-vision algorithms and ANN will allow the implementation of an artificial intelligence system for the brewing industry to assess its effectiveness.
KW  - beer acceptability
KW  - machine learning
KW  - robotics
KW  - fast-screening
KW  - automation
DO  - 10.3390/beverages5020033
ER  -
TY  - EJOU
AU  - He, Haiqing
AU  - Zhou, Junchao
AU  - Chen, Min
AU  - Chen, Ting
AU  - Li, Dajun
AU  - Cheng, Penggen
TI  - Building Extraction from UAV Images Jointly Using 6D-SLIC and Multiscale Siamese Convolutional Networks
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - Automatic building extraction using a single data type, either 2D remotely-sensed images or light detection and ranging 3D point clouds, remains insufficient to accurately delineate building outlines for automatic mapping, despite active research in this area and the significant progress which has been achieved in the past decade. This paper presents an effective approach to extracting buildings from Unmanned Aerial Vehicle (UAV) images through the incorporation of superpixel segmentation and semantic recognition. A framework for building extraction is constructed by jointly using an improved Simple Linear Iterative Clustering (SLIC) algorithm and Multiscale Siamese Convolutional Networks (MSCNs). The SLIC algorithm, improved by additionally imposing a digital surface model for superpixel segmentation, namely 6D-SLIC, is suited for building boundary detection under building and image backgrounds with similar radiometric signatures. The proposed MSCNs, including a feature learning network and a binary decision network, are used to automatically learn a multiscale hierarchical feature representation and detect building objects under various complex backgrounds. In addition, a gamma-transform green leaf index is proposed to truncate vegetation superpixels for further processing to improve the robustness and efficiency of building detection, the Douglas&ndash;Peucker algorithm and iterative optimization are used to eliminate jagged details generated from small structures as a result of superpixel segmentation. In the experiments, the UAV datasets, including many buildings in urban and rural areas with irregular shapes and different heights and that are obscured by trees, are collected to evaluate the proposed method. The experimental results based on the qualitative and quantitative measures confirm the effectiveness and high accuracy of the proposed framework relative to the digitized results. The proposed framework performs better than state-of-the-art building extraction methods, given its higher values of recall, precision, and intersection over Union (IoU).
KW  - building extraction
KW  - simple linear iterative clustering (SLIC)
KW  - multiscale Siamese convolutional networks (MSCNs)
KW  - binary decision network
KW  - unmanned aerial vehicle (UAV)
DO  - 10.3390/rs11091040
ER  -
TY  - EJOU
AU  - Li, Zhiwei
AU  - Lu, Yu
AU  - Shi, Yun
AU  - Wang, Zengguang
AU  - Qiao, Wenxin
AU  - Liu, Yicen
TI  - A Dyna-Q-Based Solution for UAV Networks Against Smart Jamming Attacks
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 5
SN  - 2073-8994

AB  - Unmanned aerial vehicle (UAV) networks have a wide range of applications, such as in the Internet of Things (IoT), 5G communications, and so forth. However, the communications between UAVs and UAVs to ground control stations mainly use radio channels, and therefore these communications are vulnerable to cyberattacks. With the advent of software-defined radio (SDR), smart attacks that can flexibly select attack strategies according to the defender&rsquo;s state information are gradually attracting the attention of researchers and potential attackers of UAV networks. The smart attack can even induce the defender to take a specific defense strategy, causing even greater damage. Inspired by symmetrical thinking, a solution using a software-defined network (SDN) to combat software-defined radio was proposed. We propose a network architecture which uses dual controllers, including a UAV flight controller and SDN controller, to achieve collaborative decision-making. Built on the top of the SDN, the state information of the whole network converges quickly and is fitted to an environment model used to develop an improved Dyna-Q-based reinforcement learning algorithm. The improved algorithm integrates the power allocation and track planning of UAVs into a unified action space. The simulation data showed that the proposed communication solution can effectively avoid smart jamming attacks and has faster learning efficiency and higher convergence performance than the compared algorithms.
KW  - UAV networks
KW  - SDN
KW  - reinforcement learning
KW  - Dyna-Q
KW  - IoT
KW  - cyberattacks
DO  - 10.3390/sym11050617
ER  -
TY  - EJOU
AU  - Zemmour, Elie
AU  - Kurtser, Polina
AU  - Edan, Yael
TI  - Automatic Parameter Tuning for Adaptive Thresholding in Fruit Detection
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 9
SN  - 1424-8220

AB  - This paper presents an automatic parameter tuning procedure specially developed for a dynamic adaptive thresholding algorithm for fruit detection. One of the major algorithm strengths is its high detection performances using a small set of training images. The algorithm enables robust detection in highly-variable lighting conditions. The image is dynamically split into variably-sized regions, where each region has approximately homogeneous lighting conditions. Nine thresholds were selected to accommodate three different illumination levels for three different dimensions in four color spaces: RGB, HSI, LAB, and NDI. Each color space uses a different method to represent a pixel in an image: RGB (Red, Green, Blue), HSI (Hue, Saturation, Intensity), LAB (Lightness, Green to Red and Blue to Yellow) and NDI (Normalized Difference Index, which represents the normal difference between the RGB color dimensions). The thresholds were selected by quantifying the required relation between the true positive rate and false positive rate. A tuning process was developed to determine the best fit values of the algorithm parameters to enable easy adaption to different kinds of fruits (shapes, colors) and environments (illumination conditions). Extensive analyses were conducted on three different databases acquired in natural growing conditions: red apples (nine images with 113 apples), green grape clusters (129 images with 1078 grape clusters), and yellow peppers (30 images with 73 peppers). These databases are provided as part of this paper for future developments. The algorithm was evaluated using cross-validation with 70% images for training and 30% images for testing. The algorithm successfully detected apples and peppers in variable lighting conditions resulting with an F-score of 93.17% and 99.31% respectively. Results show the importance of the tuning process for the generalization of the algorithm to different kinds of fruits and environments. In addition, this research revealed the importance of evaluating different color spaces since for each kind of fruit, a different color space might be superior over the others. The LAB color space is most robust to noise. The algorithm is robust to changes in the threshold learned by the training process and to noise effects in images.
KW  - adaptive thresholding
KW  - fruit detection
KW  - parameter tuning
DO  - 10.3390/s19092130
ER  -
TY  - EJOU
AU  - Rahnemoonfar, Maryam
AU  - Dobbs, Dugan
AU  - Yari, Masoud
AU  - Starek, Michael J.
TI  - DisCountNet: Discriminating and Counting Network for Real-Time Counting and Localization of Sparse Objects in High-Resolution UAV Imagery
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 9
SN  - 2072-4292

AB  - Recent deep-learning counting techniques revolve around two distinct features of data&mdash;sparse data, which favors detection networks, or dense data where density map networks are used. Both techniques fail to address a third scenario, where dense objects are sparsely located. Raw aerial images represent sparse distributions of data in most situations. To address this issue, we propose a novel and exceedingly portable end-to-end model, DisCountNet, and an example dataset to test it on. DisCountNet is a two-stage network that uses theories from both detection and heat-map networks to provide a simple yet powerful design. The first stage, DiscNet, operates on the theory of coarse detection, but does so by converting a rich and high-resolution image into a sparse representation where only important information is encoded. Following this, CountNet operates on the dense regions of the sparse matrix to generate a density map, which provides fine locations and count predictions on densities of objects. Comparing the proposed network to current state-of-the-art networks, we find that we can maintain competitive performance while using a fraction of the computational complexity, resulting in a real-time solution.
KW  - deep learning
KW  - automatic counting
KW  - UAV
KW  - real-time
DO  - 10.3390/rs11091128
ER  -
TY  - EJOU
AU  - Petrellis, Nikos
TI  - Plant Disease Diagnosis for Smart Phone Applications with Extensible Set of Diseases
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 9
SN  - 2076-3417

AB  - A plant disease diagnosis method that can be implemented with the resources of a mobile phone application, that does not have to be connected to a remote server, is presented and evaluated on citrus diseases. It can be used both by amateur gardeners and by professional agriculturists for early detection of diseases. The features used are extracted from photographs of plant parts like leaves or fruits and include the color, the relative area and the number of the lesion spots. These classification features, along with additional information like weather metadata, form disease signatures that can be easily defined by the end user (e.g., an agronomist). These signatures are based on the statistical processing of a small number of representative training photographs. The extracted features of a test photograph are compared against the disease signatures in order to select the most likely disease. An important advantage of the proposed approach is that the diagnosis does not depend on the orientation, the scale or the resolution of the photograph. The experiments have been conducted under several light exposure conditions. The accuracy was experimentally measured between 70% and 99%. An acceptable accuracy higher than 90% can be achieved in most of the cases since the lesion spots can recognized interactively with high precision.
KW  - plant disease
KW  - smart phone application
KW  - image processing
KW  - classification
KW  - segmentation
KW  - citrus diseases
DO  - 10.3390/app9091952
ER  -
TY  - EJOU
AU  - Bejiga, Mesay B.
AU  - Melgani, Farid
AU  - Beraldini, Pietro
TI  - Domain Adversarial Neural Networks for Large-Scale Land Cover Classification
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - Learning classification models require sufficiently labeled training samples, however, collecting labeled samples for every new problem is time-consuming and costly. An alternative approach is to transfer knowledge from one problem to another, which is called transfer learning. Domain adaptation (DA) is a type of transfer learning that aims to find a new latent space where the domain discrepancy between the source and the target domain is negligible. In this work, we propose an unsupervised DA technique called domain adversarial neural networks (DANNs), composed of a feature extractor, a class predictor, and domain classifier blocks, for large-scale land cover classification. Contrary to the traditional methods that perform representation and classifier learning in separate stages, DANNs combine them into a single stage, thereby learning a new representation of the input data that is both domain-invariant and discriminative. Once trained, the classifier of a DANN can be used to predict both source and target domain labels. Additionally, we also modify the domain classifier of a DANN to evaluate its suitability for multi-target domain adaptation problems. Experimental results obtained for both single and multiple target DA problems show that the proposed method provides a performance gain of up to 40%.
KW  - domain adaptation
KW  - domain adversarial neural networks
KW  - large-scale land cover classification
KW  - representation learning
DO  - 10.3390/rs11101153
ER  -
TY  - EJOU
AU  - Fuentes-Pacheco, Jorge
AU  - Torres-Olivares, Juan
AU  - Roman-Rangel, Edgar
AU  - Cervantes, Salvador
AU  - Juarez-Lopez, Porfirio
AU  - Hermosillo-Valadez, Jorge
AU  - Rendón-Mancha, Juan Manuel
TI  - Fig Plant Segmentation from Aerial Images Using a Deep Convolutional Encoder-Decoder Network
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - Crop segmentation is an important task in Precision Agriculture, where the use of aerial robots with an on-board camera has contributed to the development of new solution alternatives. We address the problem of fig plant segmentation in top-view RGB (Red-Green-Blue) images of a crop grown under open-field difficult circumstances of complex lighting conditions and non-ideal crop maintenance practices defined by local farmers. We present a Convolutional Neural Network (CNN) with an encoder-decoder architecture that classifies each pixel as crop or non-crop using only raw colour images as input. Our approach achieves a mean accuracy of 93.85% despite the complexity of the background and a highly variable visual appearance of the leaves. We make available our CNN code to the research community, as well as the aerial image data set and a hand-made ground truth segmentation with pixel precision to facilitate the comparison among different algorithms.
KW  - convolutional neural network
KW  - crop segmentation
KW  - Ficus carica
KW  - unmanned aerial vehicles
DO  - 10.3390/rs11101157
ER  -
TY  - EJOU
AU  - Shan, Zeyong
AU  - Li, Ruijian
AU  - Schwertfeger, Sören
TI  - RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 10
SN  - 1424-8220

AB  - Using camera sensors for ground robot Simultaneous Localization and Mapping (SLAM) has many benefits over laser-based approaches, such as the low cost and higher robustness. RGBD sensors promise the best of both worlds: dense data from cameras with depth information. This paper proposes to fuse RGBD and IMU data for a visual SLAM system, called VINS-RGBD, that is built upon the open source VINS-Mono software. The paper analyses the VINS approach and highlights the observability problems. Then, we extend the VINS-Mono system to make use of the depth data during the initialization process as well as during the VIO (Visual Inertial Odometry) phase. Furthermore, we integrate a mapping system based on subsampled depth data and octree filtering to achieve real-time mapping, including loop closing. We provide the software as well as datasets for evaluation. Our extensive experiments are performed with hand-held, wheeled and tracked robots in different environments. We show that ORB-SLAM2 fails for our application and see that our VINS-RGBD approach is superior to VINS-Mono.
KW  - visual-inertial systems
KW  - SLAM
KW  - inertial motion tracking
KW  - ground robots
KW  - rescue robots
KW  - sensor fusion
KW  - state estimation
KW  - RGBD sensor
DO  - 10.3390/s19102251
ER  -
TY  - EJOU
AU  - Muñoz–Bañón, Miguel Á.
AU  - del Pino, Iván
AU  - Candelas, Francisco A.
AU  - Torres, Fernando
TI  - Framework for Fast Experimental Testing of Autonomous Navigation Algorithms
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 10
SN  - 2076-3417

AB  - Research in mobile robotics requires fully operative autonomous systems to test and compare algorithms in real-world conditions. However, the implementation of such systems remains to be a highly time-consuming process. In this work, we present an robot operating system (ROS)-based navigation framework that allows the generation of new autonomous navigation applications in a fast and simple way. Our framework provides a powerful basic structure based on abstraction levels that ease the implementation of minimal solutions with all the functionalities required to implement a whole autonomous system. This approach helps to keep the focus in any sub-problem of interest (i.g. localization or control) while permitting to carry out experimental tests in the context of a complete application. To show the validity of the proposed framework we implement an autonomous navigation system for a ground robot using a localization module that fuses global navigation satellite system (GNSS) positioning and Monte Carlo localization by means of a Kalman filter. Experimental tests are performed in two different outdoor environments, over more than twenty kilometers. All the developed software is available in a GitHub repository.
KW  - autonomous navigation
KW  - mobile robots
KW  - Monte Carlo localization
KW  - SLAM
KW  - GNSS
KW  - planning
KW  - control
KW  - Kalman filter
DO  - 10.3390/app9101997
ER  -
TY  - EJOU
AU  - Han, Jiaming
AU  - Yang, Zhong
AU  - Zhang, Qiuyan
AU  - Chen, Cong
AU  - Li, Hongchen
AU  - Lai, Shangxiang
AU  - Hu, Guoxiong
AU  - Xu, Changliang
AU  - Xu, Hao
AU  - Wang, Di
AU  - Chen, Rui
TI  - A Method of Insulator Faults Detection in Aerial Images for High-Voltage Transmission Lines Inspection
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 10
SN  - 2076-3417

AB  - Insulator faults detection is an important task for high-voltage transmission line inspection. However, current methods often suffer from the lack of accuracy and robustness. Moreover, these methods can only detect one fault in the insulator string, but cannot detect a multi-fault. In this paper, a novel method is proposed for insulator one fault and multi-fault detection in UAV-based aerial images, the backgrounds of which usually contain much complex interference. The shapes of the insulators also vary obviously due to the changes in filming angle and distance. To reduce the impact of complex interference on insulator faults detection, we make full use of the deep neural network to distinguish between insulators and background interference. First of all, plenty of insulator aerial images with manually labelled ground-truth are collected to construct a standard insulator detection dataset &lsquo;InST_detection&rsquo;. Secondly, a new convolutional network is proposed to obtain accurate insulator string positions in the aerial image. Finally, a novel fault detection method is proposed that can detect both insulator one fault and multi-fault in aerial images. Experimental results on a large number of aerial images show that our proposed method is more effective and efficient than the state-of-the-art insulator fault detection methods.
KW  - unmanned aerial vehicle
KW  - high-voltage transmission line inspection
KW  - aerial image
KW  - insulator fault detection
DO  - 10.3390/app9102009
ER  -
TY  - EJOU
AU  - Bi, Fukun
AU  - Hou, Jinyuan
AU  - Chen, Liang
AU  - Yang, Zhihua
AU  - Wang, Yanping
TI  - Ship Detection for Optical Remote Sensing Images Based on Visual Attention Enhanced Network
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 10
SN  - 1424-8220

AB  - Ship detection plays a significant role in military and civil fields. Although some state-of-the-art detection methods, based on convolutional neural networks (CNN) have certain advantages, they still cannot solve the challenge well, including the large size of images, complex scene structure, a large amount of false alarm interference, and inshore ships. This paper proposes a ship detection method from optical remote sensing images, based on visual attention enhanced network. To effectively reduce false alarm in non-ship area and improve the detection efficiency from remote sensing images, we developed a light-weight local candidate scene network(     L 2     CSN) to extract the local candidate scenes with ships. Then, for the selected local candidate scenes, we propose a ship detection method, based on the visual attention DSOD(VA-DSOD). Here, to enhance the detection performance and positioning accuracy of inshore ships, we both extract semantic features, based on DSOD and embed a visual attention enhanced network in DSOD to extract the visual features. We test the detection method on a large number of typical remote sensing datasets, which consist of Google Earth images and GaoFen-2 images. We regard the state-of-the-art method [sliding window DSOD (SW+DSOD)] as a baseline, which achieves the average precision (AP) of 82.33%. The AP of the proposed method increases by 7.53%. The detection and location performance of our proposed method outperforms the baseline in complex remote sensing scenes.
KW  - scene classification
KW  - ship detection
KW  - visual attention enhanced network
KW  - DSOD
DO  - 10.3390/s19102271
ER  -
TY  - EJOU
AU  - Wang, Bodi
AU  - Liu, Guixiong
AU  - Wu, Junfang
TI  - Blind Deblurring of Saturated Images Based on Optimization and Deep Learning for Dynamic Visual Inspection on the Assembly Line
T2  - Symmetry

PY  - 2019
VL  - 11
IS  - 5
SN  - 2073-8994

AB  - Image deblurring can improve visual quality and mitigates motion blur for dynamic visual inspection. We propose a method to deblur saturated images for dynamic visual inspection by applying blur kernel estimation and deconvolution modeling. The blur kernel is estimated in a transform domain, whereas the deconvolution model is decoupled into deblurring and denoising stages via variable splitting. Deblurring predicts the mask specifying saturated pixels, which are then discarded, and denoising is learned via the fast and flexible denoising network (FFDNet) convolutional neural network (CNN) at a wide range of noise levels. Hence, the proposed deconvolution model provides the benefits of both model optimization and deep learning. Experiments demonstrate that the proposed method suitably restores visual quality and outperforms existing approaches with good score improvements.
KW  - visual inspection
KW  - image deblurring
KW  - blur kernel
KW  - deconvolution
KW  - deep learning
DO  - 10.3390/sym11050678
ER  -
TY  - EJOU
AU  - Sheykhmousa, Mohammadreza
AU  - Kerle, Norman
AU  - Kuffer, Monika
AU  - Ghaffarian, Saman
TI  - Post-Disaster Recovery Assessment with Machine Learning-Derived Land Cover and Land Use Information
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - Post-disaster recovery (PDR) is a complex, long-lasting, resource intensive, and poorly understood process. PDR goes beyond physical reconstruction (physical recovery) and includes relevant processes such as economic and social (functional recovery) processes. Knowing the size and location of the places that positively or negatively recovered is important to effectively support policymakers to help readjust planning and resource allocation to rebuild better. Disasters and the subsequent recovery are mainly expressed through unique land cover and land use changes (LCLUCs). Although LCLUCs have been widely studied in remote sensing, their value for recovery assessment has not yet been explored, which is the focus of this paper. An RS-based methodology was created for PDR assessment based on multi-temporal, very high-resolution satellite images. Different trajectories of change were analyzed and evaluated, i.e., transition patterns (TPs) that signal positive or negative recovery. Experimental analysis was carried out on three WorldView-2 images acquired over Tacloban city, Philippines, which was heavily affected by Typhoon Haiyan in 2013. Support vector machine, a robust machine learning algorithm, was employed with texture features extracted from the grey level co-occurrence matrix and local binary patterns. Although classification results for the images before and four years after the typhoon show high accuracy, substantial uncertainties mark the results for the immediate post-event image. All land cover (LC) and land use (LU) classified maps were stacked, and only changes related to TPs were extracted. The final products are LC and LU recovery maps that quantify the PDR process at the pixel level. It was found that physical and functional recovery can be mainly explained through the LCLUC information. In addition, LC and LU-based recovery maps support a general and a detailed recovery understanding, respectively. It is therefore suggested to use the LC and LU-based recovery maps to monitor and support the short and the long-term recovery, respectively.
KW  - post-disaster recovery assessment
KW  - land cover and land use based recovery maps
KW  - machine Learning
KW  - multi-temporal worldview-2 imagery
KW  - SVM
KW  - super typhoon haiyan
KW  - the Philippines
DO  - 10.3390/rs11101174
ER  -
TY  - EJOU
AU  - Buters, Todd M.
AU  - Bateman, Philip W.
AU  - Robinson, Todd
AU  - Belton, David
AU  - Dixon, Kingsley W.
AU  - Cross, Adam T.
TI  - Methodological Ambiguity and Inconsistency Constrain Unmanned Aerial Vehicles as A Silver Bullet for Monitoring Ecological Restoration
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - The last decade has seen an exponential increase in the application of unmanned aerial vehicles (UAVs) to ecological monitoring research, though with little standardisation or comparability in methodological approaches and research aims. We reviewed the international peer-reviewed literature in order to explore the potential limitations on the feasibility of UAV-use in the monitoring of ecological restoration, and examined how they might be mitigated to maximise the quality, reliability and comparability of UAV-generated data. We found little evidence of translational research applying UAV-based approaches to ecological restoration, with less than 7% of 2133 published UAV monitoring studies centred around ecological restoration. Of the 48 studies, &gt; 65% had been published in the three years preceding this study. Where studies utilised UAVs for rehabilitation or restoration applications, there was a strong propensity for single-sensor monitoring using commercially available RPAs fitted with the modest-resolution RGB sensors available. There was a strong positive correlation between the use of complex and expensive sensors (e.g., LiDAR, thermal cameras, hyperspectral sensors) and the complexity of chosen image classification techniques (e.g., machine learning), suggesting that cost remains a primary constraint to the wide application of multiple or complex sensors in UAV-based research. We propose that if UAV-acquired data are to represent the future of ecological monitoring, research requires a) consistency in the proven application of different platforms and sensors to the monitoring of target landforms, organisms and ecosystems, underpinned by clearly articulated monitoring goals and outcomes; b) optimization of data analysis techniques and the manner in which data are reported, undertaken in cross-disciplinary partnership with fields such as bioinformatics and machine learning; and c) the development of sound, reasonable and multi-laterally homogenous regulatory and policy framework supporting the application of UAVs to the large-scale and potentially trans-disciplinary ecological applications of the future.
KW  - ecological restoration
KW  - drone
KW  - UAS
KW  - rehabilitation
KW  - revegetation
DO  - 10.3390/rs11101180
ER  -
TY  - EJOU
AU  - Abioye, Ayodeji O.
AU  - Prior, Stephen D.
AU  - Saddington, Peter
AU  - Ramchurn, Sarvapali D.
TI  - Effects of Varying Noise Levels and Lighting Levels on Multimodal Speech and Visual Gesture Interaction with Aerobots
T2  - Applied Sciences

PY  - 2019
VL  - 9
IS  - 10
SN  - 2076-3417

AB  - This paper investigated the effects of varying noise levels and varying lighting levels on speech and gesture control command interfaces for aerobots. The aim was to determine the practical suitability of the multimodal combination of speech and visual gesture in human aerobotic interaction, by investigating the limits and feasibility of use of the individual components. In order to determine this, a custom multimodal speech and visual gesture interface was developed using CMU (Carnegie Mellon University) sphinx and OpenCV (Open source Computer Vision) libraries, respectively. An experiment study was designed to measure the individual effects of each of the two main components of speech and gesture, and 37 participants were recruited to participate in the experiment. The ambient noise level was varied from 55 dB to 85 dB. The ambient lighting level was varied from 10 Lux to 1400 Lux, under different lighting colour temperature mixtures of yellow (3500 K) and white (5500 K), and different background for capturing the finger gestures. The results of the experiment, which consisted of around 3108 speech utterance and 999 gesture quality observations, were presented and discussed. It was observed that speech recognition accuracy/success rate falls as noise levels rise, with 75 dB noise level being the aerobot&rsquo;s practical application limit, as the speech control interaction becomes very unreliable due to poor recognition beyond this. It was concluded that multi-word speech commands were considered more reliable and effective than single-word speech commands. In addition, some speech command words (e.g., land) were more noise resistant than others (e.g., hover) at higher noise levels, due to their articulation. From the results of the gesture-lighting experiment, the effects of both lighting conditions and the environment background on the quality of gesture recognition, was almost insignificant, less than 0.5%. The implication of this is that other factors such as the gesture capture system design and technology (camera and computer hardware), type of gesture being captured (upper body, whole body, hand, fingers, or facial gestures), and the image processing technique (gesture classification algorithms), are more important in developing a successful gesture recognition system. Some further works were suggested based on the conclusions drawn from this findings which included using alternative ASR (Automatic Speech Recognition) speech models and developing more robust gesture recognition algorithm.
KW  - speech
KW  - visual gesture
KW  - unmanned aerial vehicle (UAV)
KW  - multimodal speech and visual gesture (mSVG)
KW  - aerobot
DO  - 10.3390/app9102066
ER  -
TY  - EJOU
AU  - Kim, Nari
AU  - Ha, Kyung-Ja
AU  - Park, No-Wook
AU  - Cho, Jaeil
AU  - Hong, Sungwook
AU  - Lee, Yang-Won
TI  - A Comparison Between Major Artificial Intelligence Models for Crop Yield Prediction: Case Study of the Midwestern United States, 2006–2015
T2  - ISPRS International Journal of Geo-Information

PY  - 2019
VL  - 8
IS  - 5
SN  - 2220-9964

AB  - This paper compares different artificial intelligence (AI) models in order to develop the best crop yield prediction model for the Midwestern United States (US). Through experiments to examine the effects of phenology using three different periods, we selected the July&ndash;August (JA) database as the best months to predict corn and soybean yields. Six different AI models for crop yield prediction are tested in this research. Then, a comprehensive and objective comparison is conducted between the AI models. Particularly for the deep neural network (DNN) model, we performed an optimization process to ensure the best configurations for the layer structure, cost function, optimizer, activation function, and drop-out ratio. In terms of mean absolute error (MAE), our DNN model with the JA database was approximately 21&ndash;33% and 17&ndash;22% more accurate for corn and soybean yields, respectively, than the other five AI models. This indicates that corn and soybean yields for a given year can be forecasted in advance, at the beginning of September, approximately a month or more ahead of harvesting time. A combination of the optimized DNN model and spatial statistical methods should be investigated in future work, to mitigate partly clustered errors in some regions.
KW  - crop yield
KW  - artificial intelligence
KW  - satellite product
KW  - meteorological dataset
DO  - 10.3390/ijgi8050240
ER  -
TY  - EJOU
AU  - Chawade, Aakash
AU  - van Ham, Joost
AU  - Blomquist, Hanna
AU  - Bagge, Oscar
AU  - Alexandersson, Erik
AU  - Ortiz, Rodomiro
TI  - High-Throughput Field-Phenotyping Tools for Plant Breeding and Precision Agriculture
T2  - Agronomy

PY  - 2019
VL  - 9
IS  - 5
SN  - 2073-4395

AB  - High-throughput field phenotyping has garnered major attention in recent years leading to the development of several new protocols for recording various plant traits of interest. Phenotyping of plants for breeding and for precision agriculture have different requirements due to different sizes of the plots and fields, differing purposes and the urgency of the action required after phenotyping. While in plant breeding phenotyping is done on several thousand small plots mainly to evaluate them for various traits, in plant cultivation, phenotyping is done in large fields to detect the occurrence of plant stresses and weeds at an early stage. The aim of this review is to highlight how various high-throughput phenotyping methods are used for plant breeding and farming and the key differences in the applications of such methods. Thus, various techniques for plant phenotyping are presented together with applications of these techniques for breeding and cultivation. Several examples from the literature using these techniques are summarized and the key technical aspects are highlighted.
KW  - field phenotyping
KW  - precision breeding
KW  - precision agriculture
KW  - decision support systems
DO  - 10.3390/agronomy9050258
ER  -
TY  - EJOU
AU  - Li, Jing
AU  - Chen, Shuo
AU  - Zhang, Fangbing
AU  - Li, Erkang
AU  - Yang, Tao
AU  - Lu, Zhaoyang
TI  - An Adaptive Framework for Multi-Vehicle Ground Speed Estimation in Airborne Videos
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - With the rapid development of unmanned aerial vehicles (UAVs), UAV-based intelligent airborne surveillance systems represented by real-time ground vehicle speed estimation have attracted wide attention from researchers. However, there are still many challenges in extracting speed information from UAV videos, including the dynamic moving background, small target size, complicated environment, and diverse scenes. In this paper, we propose a novel adaptive framework for multi-vehicle ground speed estimation in airborne videos. Firstly, we build a traffic dataset based on UAV. Then, we use the deep learning detection algorithm to detect the vehicle in the UAV field of view and obtain the trajectory in the image through the tracking-by-detection algorithm. Thereafter, we present a motion compensation method based on homography. This method obtains matching feature points by an optical flow method and eliminates the influence of the detected target to accurately calculate the homography matrix to determine the real motion trajectory in the current frame. Finally, vehicle speed is estimated based on the mapping relationship between the pixel distance and the actual distance. The method regards the actual size of the car as prior information and adaptively recovers the pixel scale by estimating the vehicle size in the image; it then calculates the vehicle speed. In order to evaluate the performance of the proposed system, we carry out a large number of experiments on the AirSim Simulation platform as well as real UAV aerial surveillance experiments. Through quantitative and qualitative analysis of the simulation results and real experiments, we verify that the proposed system has a unique ability to detect, track, and estimate the speed of ground vehicles simultaneously even with a single downward-looking camera. Additionally, the system can obtain effective and accurate speed estimation results, even in various complex scenes.
KW  - ground vehicle speed estimation
KW  - intelligent airborne video surveillance
KW  - unmanned aerial vehicle
KW  - object detection and tracking
KW  - motion compensation
DO  - 10.3390/rs11101241
ER  -
TY  - EJOU
AU  - Ge, Xuming
AU  - Wu, Bo
AU  - Li, Yuan
AU  - Hu, Han
TI  - A Multi-Primitive-Based Hierarchical Optimal Approach for Semantic Labeling of ALS Point Clouds
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 10
SN  - 2072-4292

AB  - There are normally three main steps to carrying out the labeling of airborne laser scanning (ALS) point clouds. The first step is to use appropriate primitives to represent the scanning scenes, the second is to calculate the discriminative features of each primitive, and the third is to introduce a classifier to label the point clouds. This paper investigates multiple primitives to effectively represent scenes and exploit their geometric relationships. Relationships are graded according to the properties of related primitives. Then, based on initial labeling results, a novel, hierarchical, and optimal strategy is developed to optimize semantic labeling results. The proposed approach was tested using two sets of representative ALS point clouds, namely the Vaihingen datasets and Hong Kong&rsquo;s Central District dataset. The results were compared with those generated by other typical methods in previous work. Quantitative assessments for the two experimental datasets showed that the performance of the proposed approach was superior to reference methods in both datasets. The scores for correctness attained over 98% in all cases of the Vaihingen datasets and up to 96% in the Hong Kong dataset. The results reveal that our approach of labeling different classes in terms of ALS point clouds is robust and bears significance for future applications, such as 3D modeling and change detection from point clouds.
KW  - labeling
KW  - classification
KW  - multiple primitives
KW  - ALS point clouds
DO  - 10.3390/rs11101243
ER  -
TY  - EJOU
AU  - You, Shixun
AU  - Diao, Ming
AU  - Gao, Lipeng
TI  - Completing Explorer Games with a Deep Reinforcement Learning Framework Based on Behavior Angle Navigation
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 5
SN  - 2079-9292

AB  - In cognitive electronic warfare, when a typical combat vehicle, such as an unmanned combat air vehicle (UCAV), uses radar sensors to explore an unknown space, the target-searching fails due to an inefficient servoing/tracking system. Thus, to solve this problem, we developed an autonomous reasoning search method that can generate efficient decision-making actions and guide the UCAV as early as possible to the target area. For high-dimensional continuous action space, the UCAV&rsquo;s maneuvering strategies are subject to certain physical constraints. We first record the path histories of the UCAV as a sample set of supervised experiments and then construct a grid cell network using long short-term memory (LSTM) to generate a new displacement prediction to replace the target location estimation. Finally, we enable a variety of continuous-control-based deep reinforcement learning algorithms to output optimal/sub-optimal decision-making actions. All these tasks are performed in a three-dimensional target-searching simulator, i.e., the Explorer game. Please note that we use the behavior angle (BHA) for the first time as the main factor of the reward-shaping of the deep reinforcement learning framework and successfully make the trained UCAV achieve a 99.96% target destruction rate, i.e., the game win rate, in a 0.1 s operating cycle.
KW  - target-searching
KW  - cognitive electronic warfare
KW  - deep reinforcement learning
KW  - continuous control-based navigation optimization
KW  - behavior angle
DO  - 10.3390/electronics8050576
ER  -
TY  - EJOU
AU  - Lin, Shijie
AU  - Wang, Jinwang
AU  - Peng, Rui
AU  - Yang, Wen
TI  - Development of an Autonomous Unmanned Aerial Manipulator Based on a Real-Time Oriented-Object Detection Method
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 10
SN  - 1424-8220

AB  - Autonomous Unmanned Aerial Manipulators (UAMs) have shown promising potential in mobile 3-dimensional grasping applications, but they still suffer from some difficulties impeding their board applications, such as target detection and indoor positioning. For the autonomous grasping mission, the UAMs need ability to recognize the objects and grasp them. Considering the efficiency and precision, we present a novel oriented-object detection method called Rotation-SqueezeDet. This method can run on embedded-platforms in near real-time. Besides, this method can give the oriented bounding box of an object in images to enable a rotation-aware grasping. Based on this method, a UAM platform was designed and built. We have given the formulation, positioning, control, and planning of the whole UAM system. All the mechanical designs are fully provided as open-source hardware for reuse by the community. Finally, the effectiveness of the proposed scheme was validated in multiple experimental trials, highlighting its applicability of autonomous aerial rotational grasping in Global Positioning System (GPS) denied environments. We believe this system can be deployed to many potential workplaces which need UAM to accomplish difficult manipulation tasks.
KW  - aerial manipulation
KW  - aerial system
KW  - deep learning
DO  - 10.3390/s19102396
ER  -
TY  - EJOU
AU  - Tan, Jin Y.
AU  - Ker, Pin J.
AU  - Lau, K. Y.
AU  - Hannan, M. A.
AU  - Tang, Shirley G.
TI  - Applications of Photonics in Agriculture Sector: A Review
T2  - Molecules

PY  - 2019
VL  - 24
IS  - 10
SN  - 1420-3049

AB  - The agricultural industry has made a tremendous contribution to the foundations of civilization. Basic essentials such as food, beverages, clothes and domestic materials are enriched by the agricultural industry. However, the traditional method in agriculture cultivation is labor-intensive and inadequate to meet the accelerating nature of human demands. This scenario raises the need to explore state-of-the-art crop cultivation and harvesting technologies. In this regard, optics and photonics technologies have proven to be effective solutions. This paper aims to present a comprehensive review of three photonic techniques, namely imaging, spectroscopy and spectral imaging, in a comparative manner for agriculture applications. Essentially, the spectral imaging technique is a robust solution which combines the benefits of both imaging and spectroscopy but faces the risk of underutilization. This review also comprehends the practicality of all three techniques by presenting existing examples in agricultural applications. Furthermore, the potential of these techniques is reviewed and critiqued by looking into agricultural activities involving palm oil, rubber, and agro-food crops. All the possible issues and challenges in implementing the photonic techniques in agriculture are given prominence with a few selective recommendations. The highlighted insights in this review will hopefully lead to an increased effort in the development of photonics applications for the future agricultural industry.
KW  - agriculture
KW  - photonics
KW  - imaging
KW  - spectral imaging
KW  - spectroscopy
DO  - 10.3390/molecules24102025
ER  -
TY  - EJOU
AU  - Brabant, Charlotte
AU  - Alvarez-Vanhard, Emilien
AU  - Laribi, Achour
AU  - Morin, Gwénaël
AU  - Thanh Nguyen, Kim
AU  - Thomas, Alban
AU  - Houet, Thomas
TI  - Comparison of Hyperspectral Techniques for Urban Tree Diversity Classification
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 11
SN  - 2072-4292

AB  - This research aims to assess the capabilities of Very High Spatial Resolution (VHSR) hyperspectral satellite data in order to discriminate urban tree diversity. Four dimension reduction methods and two classifiers are tested, using two learning methods and applied with four in situ sample datasets. An airborne HySpex image (408 bands/2 m) was acquired in July 2015 from which prototypal spaceborne hyperspectral images (named HYPXIM) at 4 m and 8 m and a multispectral Sentinel2 image at 10 m have been simulated for the purpose of this study. A comparison is made using these methods and datasets. The influence of dimension reduction methods is assessed on hyperspectral (HySpex and HYPXIM) and Sentinel2 datasets. The influence of conventional classifiers (Support Vector Machine &ndash;SVM&ndash; and Random Forest &ndash;RF&ndash;) and learning methods is evaluated on all image datasets (reduced and non-reduced hyperspectral and Sentinel2 datasets). Results show that HYPXIM 4 m and HySpex 2 m reduced by Minimum Noise Fraction (MNF) provide the greatest classification of 14 species using the SVM with an overall accuracy of 78.4% (&plusmn;1.5) and a kappa index of agreement of 0.7. More generally, the learning methods have a stronger influence than classifiers, or even than dimensional reduction methods, on urban tree diversity classification. Prototypal HYPXIM images appear to present a great compromise (192 spectral bands/4 m resolution) for urban vegetation applications compared to HySpex or Sentinel2 images.
KW  - very high spatial resolution
KW  - tree species
KW  - training methods
KW  - dimension reduction methods
KW  - classification methods
DO  - 10.3390/rs11111269
ER  -
TY  - EJOU
AU  - Xiong, Xin
AU  - Zhang, Jingjin
AU  - Guo, Doudou
AU  - Chang, Liying
AU  - Huang, Danfeng
TI  - Non-Invasive Sensing of Nitrogen in Plant Using Digital Images and Machine Learning for Brassica Campestris ssp. Chinensis L.
T2  - Sensors

PY  - 2019
VL  - 19
IS  - 11
SN  - 1424-8220

AB  - Monitoring plant nitrogen (N) in a timely way and accurately is critical for precision fertilization. The imaging technology based on visible light is relatively inexpensive and ubiquitous, and open-source analysis tools have proliferated. In this study, texture- and geometry-related phenotyping combined with color properties were investigated for their potential use in evaluating N in pakchoi (Brassica campestris ssp. chinensis L.). Potted pakchoi treated with four levels of N were cultivated in a greenhouse. Their top-view images were acquired using a camera at six growth stages. The corresponding plant N concentration was determined destructively. The quantitative relationships between the nitrogen nutrition index (NNI) and the image-based phenotyping features were established using the following algorithms: random forest (RF), support vector regression (SVR), and neural network (NN). The results showed the full model based on the color, texture, and geometry-related features outperforms the model based on only the color-related feature in predicting the NNI. The RF full model exhibited the most robust performance in both the seedling and harvest stages, reaching prediction accuracies of 0.823 and 0.943, respectively. The high prediction accuracy of the model allows for a low-cost, non-destructive monitoring of N in the field of precision crop management.
KW  - visible light imaging
KW  - phenotyping
KW  - machine learning
KW  - nitrogen nutrition index
KW  - leafy vegetable
KW  - precision fertilization
DO  - 10.3390/s19112448
ER  -
TY  - EJOU
AU  - Halicek, Martin
AU  - Fabelo, Himar
AU  - Ortega, Samuel
AU  - Callico, Gustavo M.
AU  - Fei, Baowei
TI  - In-Vivo and Ex-Vivo Tissue Analysis through Hyperspectral Imaging Techniques: Revealing the Invisible Features of Cancer
T2  - Cancers

PY  - 2019
VL  - 11
IS  - 6
SN  - 2072-6694

AB  - In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology.
KW  - hyperspectral imaging
KW  - clinical diagnosis
KW  - biomedical optical imaging
KW  - cancer
KW  - medical diagnostic imaging
KW  - artificial intelligence
KW  - machine learning
DO  - 10.3390/cancers11060756
ER  -
TY  - EJOU
AU  - Wong, Ching-Chang
AU  - Liu, Chih-Cheng
AU  - Xiao, Sheng-Ru
AU  - Yang, Hao-Yu
AU  - Lau, Meng-Cheng
TI  - Q-Learning of Straightforward Gait Pattern for Humanoid Robot Based on Automatic Training Platform
T2  - Electronics

PY  - 2019
VL  - 8
IS  - 6
SN  - 2079-9292

AB  - In this paper, an oscillator-based gait pattern with sinusoidal functions is designed and implemented on a field-programmable gate array (FPGA) chip to generate a trajectory plan and achieve bipedal locomotion for a small-sized humanoid robot. In order to let the robot can walk straight, the turning direction is viewed as a parameter of the gait pattern and Q-learning is used to obtain a straightforward gait pattern. Moreover, an automatic training platform is designed so that the learning process is automated. In this way, the turning direction can be adjusted flexibly and efficiently under the supervision of the automatic training platform. The experimental results show that the proposed learning framework allows the humanoid robot to gradually walk straight in the automated learning process.
KW  - humanoid robot
KW  - gait pattern
KW  - trajectory planning
KW  - bipedal locomotion
KW  - Q-learning
DO  - 10.3390/electronics8060615
ER  -
TY  - EJOU
AU  - Zhou, Xisheng
AU  - Li, Long
AU  - Chen, Longqian
AU  - Liu, Yunqiang
AU  - Cui, Yifan
AU  - Zhang, Yu
AU  - Zhang, Ting
TI  - Discriminating Urban Forest Types from Sentinel-2A Image Data through Linear Spectral Mixture Analysis: A Case Study of Xuzhou, East China
T2  - Forests

PY  - 2019
VL  - 10
IS  - 6
SN  - 1999-4907

AB  - Urban forests are an important component of the urban ecosystem. Urban forest types are a key piece of information required for monitoring the condition of an urban ecosystem. In this study, we propose an urban forest type discrimination method based on linear spectral mixture analysis (LSMA) and a support vector machine (SVM) in the case study of Xuzhou, east China. From 10-m Sentinel-2A imagery data, three different vegetation endmembers, namely broadleaved forest, coniferous forest, and low vegetation, and their abundances were extracted through LSMA. Using a combination of image spectra, topography, texture, and vegetation abundances, four SVM classification models were performed and compared to investigate the impact of these features on classification accuracy. With a particular interest in the role that vegetation abundances play in classification, we also compared SVM and other classifiers, i.e., random forest (RF), artificial neural network (ANN), and quick unbiased efficient statistical tree (QUEST). Results indicate that (1) the LSMA method can derive accurate vegetation abundances from Sentinel-2A image data, and the root-mean-square error (RMSE) was 0.019; (2) the classification accuracies of the four SVM models were improved after adding topographic features, textural features, and vegetation abundances one after the other; (3) the SVM produced higher classification accuracies than the other three classifiers when identical classification features were used; and (4) vegetation endmember abundances improved classification accuracy regardless of which classifier was used. It is concluded that Sentinel-2A image data has a strong capability to discriminate urban forest types in spectrally heterogeneous urban areas, and that vegetation abundances derived from LSMA can enhance such discrimination.
KW  - urban forest
KW  - Sentinel-2A
KW  - LSMA
KW  - SVM
DO  - 10.3390/f10060478
ER  -
TY  - EJOU
AU  - Wang, Dongliang
AU  - Shao, Quanqin
AU  - Yue, Huanyin
TI  - Surveying Wild Animals from Satellites, Manned Aircraft and Unmanned Aerial Systems (UASs): A Review
T2  - Remote Sensing

PY  - 2019
VL  - 11
IS  - 11
SN  - 2072-4292

AB  - This article reviews studies regarding wild animal surveys based on multiple platforms, including satellites, manned aircraft, and unmanned aircraft systems (UASs), and focuses on the data used, animal detection methods, and their accuracies. We also discuss the advantages and limitations of each type of remote sensing data and highlight some new research opportunities and challenges. Submeter very-high-resolution (VHR) spaceborne imagery has potential in modeling the population dynamics of large (&gt;0.6 m) wild animals at large spatial and temporal scales, but has difficulty discerning small (&lt;0.6 m) animals at the species level, although high-resolution commercial satellites, such as WorldView-3 and -4, have been able to collect images with a ground resolution of up to 0.31 m in panchromatic mode. This situation will not change unless the satellite image resolution is greatly improved in the future. Manned aerial surveys have long been employed to capture the centimeter-scale images required for animal censuses over large areas. However, such aerial surveys are costly to implement in small areas and can cause significant disturbances to wild animals because of their noise. In contrast, UAS surveys are seen as a safe, convenient and less expensive alternative to ground-based and conventional manned aerial surveys, but most UASs can cover only small areas. The proposed use of UAS imagery in combination with VHR satellite imagery would produce critical population data for large wild animal species and colonies over large areas. The development of software systems for automatically producing image mosaics and recognizing wild animals will further improve survey efficiency.
KW  - very-high-resolution satellites
KW  - unmanned aircraft systems
KW  - wild animal surveys
KW  - remote sensing
DO  - 10.3390/rs11111308
ER  -
