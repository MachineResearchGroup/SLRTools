TY  - EJOU
AU  - Michez, Adrien
AU  - Bauwens, Sébastien
AU  - Brostaux, Yves
AU  - Hiel, Marie-Pierre
AU  - Garré, Sarah
AU  - Lejeune, Philippe
AU  - Dumont, Benjamin
TI  - How Far Can Consumer-Grade UAV RGB Imagery Describe Crop Production? A 3D and Multitemporal Modeling Approach Applied to Zea mays
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - In recent decades, remote sensing has increasingly been used to estimate the spatio-temporal evolution of crop biophysical parameters such as the above-ground biomass (AGB). On a local scale, the advent of unmanned aerial vehicles (UAVs) seems to be a promising trade-off between satellite/airborne and terrestrial remote sensing. This study aims to evaluate the potential of a low-cost UAV RGB solution to predict the final AGB of Zea mays. Besides evaluating the interest of 3D data and multitemporality, our study aims to answer operational questions such as when one should plan a combination of two UAV flights for AGB modeling. In this case, study, final AGB prediction model performance reached 0.55 (R-square) using only UAV information and 0.8 (R-square) when combining UAV information from a single flight with a single-field AGB measurement. The adding of UAV height information to the model improves the quality of the AGB prediction. Performing two flights provides almost systematically an improvement in AGB prediction ability in comparison to most single flights. Our study provides clear insight about how we can counter the low spectral resolution of consumer-grade RGB cameras using height information and multitemporality. Our results highlight the importance of the height information which can be derived from UAV data on one hand, and on the other hand, the lower relative importance of RGB spectral information.
KW  - unmanned aerial vehicles
KW  - unmanned aerial systems
KW  - drone
KW  - above-ground biomass
KW  - RGB imagery
KW  - photogrammetry
KW  - Zea mays
DO  - 10.3390/rs10111798
TY  - EJOU
AU  - Liu, Yanli
AU  - Zhang, Heng
AU  - Guo, Hanlei
AU  - Xiong, Neal N.
TI  - A FAST-BRISK Feature Detector with Depth Information
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - RGB-D cameras offer both color and depth images of the surrounding environment, making them an attractive option for robotic and vision applications. This work introduces the BRISK_D algorithm, which efficiently combines Features from Accelerated Segment Test (FAST) and Binary Robust Invariant Scalable Keypoints (BRISK) methods. In the BRISK_D algorithm, the keypoints are detected by the FAST algorithm and the location of the keypoint is refined in the scale and the space. The scale factor of the keypoint is directly computed with the depth information of the image. In the experiment, we have made a detailed comparative analysis of the three algorithms SURF, BRISK and BRISK_D from the aspects of scaling, rotation, perspective and blur. The BRISK_D algorithm combines depth information and has good algorithm performance.
KW  - BRISK (Binary Robust Invariant Scalable Keypoints)
KW  - depth information
KW  - scale factor
KW  - scale invariance
KW  - rotation invariance
DO  - 10.3390/s18113908
TY  - EJOU
AU  - Zhu, Yadi
AU  - Chen, Feng
AU  - Li, Ming
AU  - Wang, Zijia
TI  - Inferring the Economic Attributes of Urban Rail Transit Passengers Based on Individual Mobility Using Multisource Data
T2  - Sustainability

PY  - 2018
VL  - 10
IS  - 11
SN  - 2071-1050

AB  - Socioeconomic attributes are essential characteristics of people, and many studies on economic attribute inference focus on data that contain user profile information. For data without user profiles, like smart card data, there is no validated method for inferring individual economic attributes. This study aims to bridge this gap by formulating a mobility to attribute framework to infer passengers&rsquo; economic attributes based on the relationship between individual mobility and personal attributes. This framework integrates shop consumer prices, house prices, and smart card data using three steps: individual mobility extraction, location feature identification, and economic attribute inference. Each passenger&rsquo;s individual mobility is extracted by smart card data. Economic features of stations are described using house price and shop consumer price data. Then, each passenger&rsquo;s comprehensive consumption indicator set is formulated by integrating these data. Finally, individual economic levels are classified. From the case study of Beijing, commuting distance and trip frequency using the metro have a negative correlation with passengers&rsquo; income and the results confirm that metro passengers are mainly in the low- and middle-income groups. This study improves on passenger information extracted from data without user profile information and provides a method to integrate multisource big data mining for more information.
KW  - transportation planning
KW  - individual economic attributes
KW  - individual mobility
KW  - smart card data (SCD)
KW  - multisource data
DO  - 10.3390/su10114178
TY  - EJOU
AU  - Boonpook, Wuttichai
AU  - Tan, Yumin
AU  - Ye, Yinghua
AU  - Torteeka, Peerapong
AU  - Torsri, Kritanai
AU  - Dong, Shengxian
TI  - A Deep Learning Approach on Building Detection from Unmanned Aerial Vehicle-Based Images in Riverbank Monitoring
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Buildings along riverbanks are likely to be affected by rising water levels, therefore the acquisition of accurate building information has great importance not only for riverbank environmental protection but also for dealing with emergency cases like flooding. UAV-based photographs are flexible and cloud-free compared to satellite images and can provide very high-resolution images up to centimeter level, while there exist great challenges in quickly and accurately detecting and extracting building from UAV images because there are usually too many details and distortions on UAV images. In this paper, a deep learning (DL)-based approach is proposed for more accurately extracting building information, in which the network architecture, SegNet, is used in the semantic segmentation after the network training on a completely labeled UAV image dataset covering multi-dimension urban settlement appearances along a riverbank area in Chongqing. The experiment results show that an excellent performance has been obtained in the detection of buildings from untrained locations with an average overall accuracy more than 90%. To verify the generality and advantage of the proposed method, the procedure is further evaluated by training and testing with another two open standard datasets which have a variety of building patterns and styles, and the final overall accuracies of building extraction are more than 93% and 95%, respectively.
KW  - building extraction
KW  - UAV dataset
KW  - deep learning
KW  - river bank monitoring
DO  - 10.3390/s18113921
TY  - EJOU
AU  - Mishra, Niti B.
AU  - Mainali, Kumar P.
AU  - Shrestha, Bharat B.
AU  - Radenz, Jackson
AU  - Karki, Debendra
TI  - Species-Level Vegetation Mapping in a Himalayan Treeline Ecotone Using Unmanned Aerial System (UAS) Imagery
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 11
SN  - 2220-9964

AB  - Understanding ecological patterns and response to climate change requires unbiased data on species distribution. This can be challenging, especially in biodiverse but extreme environments like the Himalaya. This study presents the results of the first ever application of Unmanned Aerial Systems (UAS) imagery for species-level mapping of vegetation in the Himalaya following a hierarchical Geographic Object Based Image Analysis (GEOBIA) method. The first level of classification separated green vegetated objects from the rest with overall accuracy of 95%. At the second level, seven cover types were identified (including four woody vegetation species). For this, the suitability of various spectral, shape and textural features were tested for classifying them using an ensemble decision tree algorithm. Spectral features alone yielded ~70% accuracy (kappa 0.66) whereas adding textural and shape features marginally improved the accuracy (73%) but at the cost of a substantial increase in processing time. Contrast in plant morphological traits was the key to distinguishing nearby stands as different species. Hence, broad-leaved versus fine needle leaved vegetation were mapped more accurately than structurally similar classes such as Rhododendron anthopogon versus non-photosynthetic vegetation. Results highlight the potential and limitations of the suggested UAS-GEOBIA approach for detailed mapping of plant communities and suggests future research directions.
KW  - species mapping
KW  - Unmanned Aerial Systems
KW  - hierarchical GEOBIA
KW  - Himalaya
KW  - treeline ecotone
KW  - random forest
KW  - Langtang National Park
DO  - 10.3390/ijgi7110445
TY  - EJOU
AU  - Zhang, Yihong
AU  - Yang, Yijin
AU  - Zhou, Wuneng
AU  - Shi, Lifeng
AU  - Li, Demin
TI  - Motion-Aware Correlation Filters for Online Visual Tracking
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - The discriminative correlation filters-based methods struggle deal with the problem of fast motion and heavy occlusion, the problem can severely degrade the performance of trackers, ultimately leading to tracking failures. In this paper, a novel Motion-Aware Correlation Filters (MACF) framework is proposed for online visual object tracking, where a motion-aware strategy based on joint instantaneous motion estimation Kalman filters is integrated into the Discriminative Correlation Filters (DCFs). The proposed motion-aware strategy is used to predict the possible region and scale of the target in the current frame by utilizing the previous estimated 3D motion information. Obviously, this strategy can prevent model drift caused by fast motion. On the base of the predicted region and scale, the MACF detects the position and scale of the target by using the DCFs-based method in the current frame. Furthermore, an adaptive model updating strategy is proposed to address the problem of corrupted models caused by occlusions, where the learning rate is determined by the confidence of the response map. The extensive experiments on popular Object Tracking Benchmark OTB-100, OTB-50 and unmanned aerial vehicles (UAV) video have demonstrated that the proposed MACF tracker performs better than most of the state-of-the-art trackers and achieves a high real-time performance. In addition, the proposed approach can be integrated easily and flexibly into other visual tracking algorithms.
KW  - visual tracking
KW  - correlation filters
KW  - motion-aware
KW  - adaptive update strategy
KW  - confidence response map
DO  - 10.3390/s18113937
TY  - EJOU
AU  - Sun, Jian
AU  - Huang, Guanhua
AU  - Sun, Gang
AU  - Yu, Hongfang
AU  - Sangaiah, Arun K.
AU  - Chang, Victor
TI  - A Q-Learning-Based Approach for Deploying Dynamic Service Function Chains
T2  - Symmetry

PY  - 2018
VL  - 10
IS  - 11
SN  - 2073-8994

AB  - As the size and service requirements of today&rsquo;s networks gradually increase, large numbers of proprietary devices are deployed, which leads to network complexity, information security crises and makes network service and service provider management increasingly difficult. Network function virtualization (NFV) technology is one solution to this problem. NFV separates network functions from hardware and deploys them as software on a common server. NFV can be used to improve service flexibility and isolate the services provided for each user, thus guaranteeing the security of user data. Therefore, the use of NFV technology includes many problems worth studying. For example, when there is a free choice of network path, one problem is how to choose a service function chain (SFC) that both meets the requirements and offers the service provider maximum profit. Most existing solutions are heuristic algorithms with high time efficiency, or integer linear programming (ILP) algorithms with high accuracy. It&rsquo;s necessary to design an algorithm that symmetrically considers both time efficiency and accuracy. In this paper, we propose the Q-learning Framework Hybrid Module algorithm (QLFHM), which includes reinforcement learning to solve this SFC deployment problem in dynamic networks. The reinforcement learning module in QLFHM is responsible for the output of alternative paths, while the load balancing module in QLFHM is responsible for picking the optimal solution from them. The results of a comparison simulation experiment on a dynamic network topology show that the proposed algorithm can output the approximate optimal solution in a relatively short time while also considering the network load balance. Thus, it achieves the goal of maximizing the benefit to the service provider.
KW  - network function virtualization
KW  - service function chain
KW  - reinforcement learning
KW  - load balancing
KW  - security
DO  - 10.3390/sym10110646
TY  - EJOU
AU  - Lagkas, Thomas
AU  - Argyriou, Vasileios
AU  - Bibi, Stamatia
AU  - Sarigiannidis, Panagiotis
TI  - UAV IoT Framework Views and Challenges: Towards Protecting Drones as “Things”
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - Unmanned aerial vehicles (UAVs) have enormous potential in enabling new applications in various areas, ranging from military, security, medicine, and surveillance to traffic-monitoring applications. Lately, there has been heavy investment in the development of UAVs and multi-UAVs systems that can collaborate and complete missions more efficiently and economically. Emerging technologies such as 4G/5G networks have significant potential on UAVs equipped with cameras, sensors, and GPS receivers in delivering Internet of Things (IoT) services from great heights, creating an airborne domain of the IoT. However, there are many issues to be resolved before the effective use of UAVs can be made, including security, privacy, and management. As such, in this paper we review new UAV application areas enabled by the IoT and 5G technologies, analyze the sensor requirements, and overview solutions for fleet management over aerial-networking, privacy, and security challenges. Finally, we propose a framework that supports and enables these technologies on UAVs. The introduced framework provisions a holistic IoT architecture that enables the protection of UAVs as “flying” things in a collaborative networked environment.
KW  - security
KW  - privacy
KW  - drones
KW  - IoT
KW  - UAV
DO  - 10.3390/s18114015
TY  - EJOU
AU  - Song, Ahram
AU  - Choi, Jaewan
AU  - Han, Youkyung
AU  - Kim, Yongil
TI  - Change Detection in Hyperspectral Images Using Recurrent 3D Fully Convolutional Networks
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 11
SN  - 2072-4292

AB  - Hyperspectral change detection (CD) can be effectively performed using deep-learning networks. Although these approaches require qualified training samples, it is difficult to obtain ground-truth data in the real world. Preserving spatial information during training is difficult due to structural limitations. To solve such problems, our study proposed a novel CD method for hyperspectral images (HSIs), including sample generation and a deep-learning network, called the recurrent three-dimensional (3D) fully convolutional network (Re3FCN), which merged the advantages of a 3D fully convolutional network (FCN) and a convolutional long short-term memory (ConvLSTM). Principal component analysis (PCA) and the spectral correlation angle (SCA) were used to generate training samples with high probabilities of being changed or unchanged. The strategy assisted in training fewer samples of representative feature expression. The Re3FCN was mainly comprised of spectral&ndash;spatial and temporal modules. Particularly, a spectral&ndash;spatial module with a 3D convolutional layer extracts the spectral&ndash;spatial features from the HSIs simultaneously, whilst a temporal module with ConvLSTM records and analyzes the multi-temporal HSI change information. The study first proposed a simple and effective method to generate samples for network training. This method can be applied effectively to cases with no training samples. Re3FCN can perform end-to-end detection for binary and multiple changes. Moreover, Re3FCN can receive multi-temporal HSIs directly as input without learning the characteristics of multiple changes. Finally, the network could extract joint spectral&ndash;spatial&ndash;temporal features and it preserved the spatial structure during the learning process through the fully convolutional structure. This study was the first to use a 3D FCN and a ConvLSTM for the remote-sensing CD. To demonstrate the effectiveness of the proposed CD method, we performed binary and multi-class CD experiments. Results revealed that the Re3FCN outperformed the other conventional methods, such as change vector analysis, iteratively reweighted multivariate alteration detection, PCA-SCA, FCN, and the combination of 2D convolutional layers-fully connected LSTM.
KW  - change detection
KW  - fully convolutional network
KW  - 3D convolution
KW  - convolutional LSTM
KW  - hyperspectral image
DO  - 10.3390/rs10111827
TY  - EJOU
AU  - Kim, Yunbin
AU  - Sa, Jaewon
AU  - Chung, Yongwha
AU  - Park, Daihee
AU  - Lee, Sungju
TI  - Resource-Efficient Pet Dog Sound Events Classification Using LSTM-FCN Based on Time-Series Data
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - The use of IoT (Internet of Things) technology for the management of pet dogs left alone at home is increasing. This includes tasks such as automatic feeding, operation of play equipment, and location detection. Classification of the vocalizations of pet dogs using information from a sound sensor is an important method to analyze the behavior or emotions of dogs that are left alone. These sounds should be acquired by attaching the IoT sound sensor to the dog, and then classifying the sound events (e.g., barking, growling, howling, and whining). However, sound sensors tend to transmit large amounts of data and consume considerable amounts of power, which presents issues in the case of resource-constrained IoT sensor devices. In this paper, we propose a way to classify pet dog sound events and improve resource efficiency without significant degradation of accuracy. To achieve this, we only acquire the intensity data of sounds by using a relatively resource-efficient noise sensor. This presents issues as well, since it is difficult to achieve sufficient classification accuracy using only intensity data due to the loss of information from the sound events. To address this problem and avoid significant degradation of classification accuracy, we apply long short-term memory-fully convolutional network (LSTM-FCN), which is a deep learning method, to analyze time-series data, and exploit bicubic interpolation. Based on experimental results, the proposed method based on noise sensors (i.e., Shapelet and LSTM-FCN for time-series) was found to improve energy efficiency by 10 times without significant degradation of accuracy compared to typical methods based on sound sensors (i.e., mel-frequency cepstrum coefficient (MFCC), spectrogram, and mel-spectrum for feature extraction, and support vector machine (SVM) and k-nearest neighbor (K-NN) for classification).
KW  - pet dogs
KW  - separation anxiety
KW  - IoT sensor
KW  - sound events processing
KW  - resource efficiency
KW  - LSTM-FCN
DO  - 10.3390/s18114019
TY  - EJOU
AU  - Csillik, Ovidiu
AU  - Cherbini, John
AU  - Johnson, Robert
AU  - Lyons, Andy
AU  - Kelly, Maggi
TI  - Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks
T2  - Drones

PY  - 2018
VL  - 2
IS  - 4
SN  - 2504-446X

AB  - Remote sensing is important to precision agriculture and the spatial resolution provided by Unmanned Aerial Vehicles (UAVs) is revolutionizing precision agriculture workflows for measurement crop condition and yields over the growing season, for identifying and monitoring weeds and other applications. Monitoring of individual trees for growth, fruit production and pest and disease occurrence remains a high research priority and the delineation of each tree using automated means as an alternative to manual delineation would be useful for long-term farm management. In this paper, we detected citrus and other crop trees from UAV images using a simple convolutional neural network (CNN) algorithm, followed by a classification refinement using superpixels derived from a Simple Linear Iterative Clustering (SLIC) algorithm. The workflow performed well in a relatively complex agricultural environment (multiple targets, multiple size trees and ages, etc.) achieving high accuracy (overall accuracy = 96.24%, Precision (positive predictive value) = 94.59%, Recall (sensitivity) = 97.94%). To our knowledge, this is the first time a CNN has been used with UAV multi-spectral imagery to focus on citrus trees. More of these individual cases are needed to develop standard automated workflows to help agricultural managers better incorporate large volumes of high resolution UAV imagery into agricultural management operations.
KW  - CNN
KW  - deep learning
KW  - superpixels
KW  - precision agriculture
KW  - UAS
KW  - feature extraction
KW  - citrus
KW  - tree identification
DO  - 10.3390/drones2040039
TY  - EJOU
AU  - N. de Sousa, Marcelo
AU  - S. Thomä, Reiner
TI  - Enhancement of Localization Systems in NLOS Urban Scenario with Multipath Ray Tracing Fingerprints and Machine Learning
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 11
SN  - 1424-8220

AB  - A hybrid technique is proposed to enhance the localization performance of a time difference of arrival (TDOA) deployed in non-line-of-sight (NLOS) suburban scenario. The idea was to use Machine Learning framework on the dataset, produced by the ray tracing simulation, and the Channel Impulse Response estimation from the real signal received by each sensor. Conventional localization techniques mitigate errors trying to avoid NLOS measurements in processing emitter position, while the proposed method uses the multipath fingerprint information produced by ray tracing (RT) simulation together with calibration emitters to refine a Machine Learning engine, which gives an extra layer of information to improve the emitter position estimation. The ray-tracing fingerprints perform the target localization embedding all the reflection and diffraction in the propagation scenario. A validation campaign was performed and showed the feasibility of the proposed method, provided that the buildings can be appropriately included in the scenario description.
KW  - wireless positioning
KW  - cooperative positioning
KW  - machine learning
KW  - hybrid positioning
KW  - multipath exploitation
KW  - time difference of arrival localization
KW  - ray tracing fingerprints
DO  - 10.3390/s18114073
TY  - EJOU
AU  - White, Raechel A.
AU  - Bomber, Michael
AU  - Hupy, Joseph P.
AU  - Shortridge, Ashton
TI  - UAS-GEOBIA Approach to Sapling Identification in Jack Pine Barrens after Fire
T2  - Drones

PY  - 2018
VL  - 2
IS  - 4
SN  - 2504-446X

AB  - Jack pine (pinus banksiana) forests are unique ecosystems controlled by wildfire. Understanding the traits of revegetation after wildfire is important for sustainable forest management, as these forests not only provide economic resources, but also are home to specialized species, like the Kirtland Warbler (Setophaga kirtlandii). Individual tree detection of jack pine saplings after fire events can provide information about an environment&rsquo;s recovery. Traditional satellite and manned aerial sensors lack the flexibility and spatial resolution required for identifying saplings in early post-fire analysis. Here we evaluated the use of unmanned aerial systems and geographic object-based image analysis for jack pine sapling identification in a region burned during the 2012 Duck Lake Fire in the Upper Peninsula of Michigan. Results of this study indicate that sapling identification accuracies can top 90%, and that accuracy improves with the inclusion of red and near infrared spectral bands. Results also indicated that late season imagery performed best when discriminating between young (&lt;5 years) jack pines and herbaceous ground cover in these environments.
KW  - UAV
KW  - jack pine
KW  - succession
KW  - forest disturbance
KW  - fire
DO  - 10.3390/drones2040040
TY  - EJOU
AU  - Garcia Millan, Virginia
AU  - Sanchez-Azofeifa, Arturo
TI  - Quantifying Changes on Forest Succession in a Dry Tropical Forest Using Angular-Hyperspectral Remote Sensing
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - The tropical dry forest (TDF) is one the most threatened ecosystems in South America, existing on a landscape with different levels of ecological succession. Among satellites dedicated to Earth observation and monitoring ecosystem succession, CHRIS/PROBA is the only satellite that presents quasi-simultaneous multi-angular pointing and hyperspectral imaging. These two characteristics permit the study of structural and compositional differences of TDFs with different levels of succession. In this paper, we use 2008 and 2014 CHRIS/PROBA images from a TDF in Minas Gerais, Brazil to study ecosystem succession after abandonment. Using a &minus;55&deg; angle of observation; several classifiers including spectral angle mapper (SAM), support vector machine (SVM), and decision trees (DT) were used to test how well they discriminate between different successional stages. Our findings suggest that the SAM is the most appropriate method to classify TDFs as a function of succession (accuracies ~80 for % for late stage, ~85% for the intermediate stage, ~70% for early stage, and ~50% for other classes). Although CHRIS/PROBA cannot be used for large-scale/long-term monitoring of tropical forests because of its experimental nature; our results support the potential of using multi-angle hyperspectral data to characterize the structure and composition of TDFs in the near future.
KW  - dry forests
KW  - ecological succession
KW  - multi-angle remote sensing
DO  - 10.3390/rs10121865
TY  - EJOU
AU  - Rahman, Muhammad M.
AU  - Robson, Andrew
AU  - Bristow, Mila
TI  - Exploring the Potential of High Resolution WorldView-3 Imagery for Estimating Yield of Mango
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Pre-harvest yield estimation of mango fruit is important for the optimization of inputs and other resources on the farm. Current industry practice of visual counting the fruit on a small number of trees for yield forecasting can be highly inaccurate due to the spatial variability, especially if the trees selected do not represent the entire crop. Therefore, this study evaluated the potential of high resolution WorldView-3 (WV3) satellite imagery to estimate yield of mango by integrating both geometric (tree crown area) and optical (spectral vegetation indices) data using artificial neural network (ANN) model. WV3 images were acquired in 2016&ndash;2017 and 2017&ndash;2018 growing seasons at the early fruit stage from three orchards in Acacia Hills region, Northern Territory, Australia. Stratified sampling technique (SST) was applied to select 18 trees from each orchard and subsequently ground truthed for yield (kg&middot;tree&minus;1) and fruit number per tree. For each sampled tree, spectral reflectance data and tree crown area (TCA) was extracted from WV3 imagery. The TCA was identified as the most important predictor of both fruit yield (kg&middot;tree&minus;1) and fruit number, followed by NDVI red-edge band when all trees from three orchards in two growing seasons were combined. The results of all sampled trees from three orchards in two growing seasons using ANN model produced a strong correlation (R2 = 0.70 and 0.68 for total fruit yield (kg&middot;tree&minus;1) and fruit number respectively), which suggest that the model can be obtained to predict yield on a regional level. On orchard level also the ANN model produced a high correlation when both growing seasons were combined. However, the model developed in one season could not be applied in another season due to the influence of seasonal variation and canopy condition. Using the relationship derived from the measured yield parameters against combined VIs and TCA data, the total fruit yield (t&middot;ha&minus;1) and fruit number were estimated for each orchard, produced 7% under estimation to less than 1% over estimation. The accuracy of the findings showed the potential of WV3 imagery to better predict the yield parameters than the current practice across the mango industry as well as to quantify lost yield as a result of delayed harvest.
KW  - WorldView-3 (WV3)
KW  - Mango (Mangifera indica)
KW  - tree crown area
KW  - yield prediction
DO  - 10.3390/rs10121866
TY  - EJOU
AU  - Han, Xiongzhe
AU  - Thomasson, J. Alex
AU  - Bagnall, G. Cody
AU  - Pugh, N. Ace
AU  - Horne, David W.
AU  - Rooney, William L.
AU  - Jung, Jinha
AU  - Chang, Anjin
AU  - Malambo, Lonesome
AU  - Popescu, Sorin C.
AU  - Gates, Ian T.
AU  - Cope, Dale A.
TI  - Measurement and Calibration of Plant-Height from Fixed-Wing UAV Images
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - Continuing population growth will result in increasing global demand for food and fiber for the foreseeable future. During the growing season, variability in the height of crops provides important information on plant health, growth, and response to environmental effects. This paper indicates the feasibility of using structure from motion (SfM) on images collected from 120 m above ground level (AGL) with a fixed-wing unmanned aerial vehicle (UAV) to estimate sorghum plant height with reasonable accuracy on a relatively large farm field. Correlations between UAV-based estimates and ground truth were strong on all dates (R2 &gt; 0.80) but are clearly better on some dates than others. Furthermore, a new method for improving UAV-based plant height estimates with multi-level ground control points (GCPs) was found to lower the root mean square error (RMSE) by about 20%. These results indicate that GCP-based height calibration has a potential for future application where accuracy is particularly important. Lastly, the image blur appeared to have a significant impact on the accuracy of plant height estimation. A strong correlation (R2 = 0.85) was observed between image quality and plant height RMSE and the influence of wind was a challenge in obtaining high-quality plant height data. A strong relationship (R2 = 0.99) existed between wind speed and image blurriness.
KW  - fixed-wing UAV
KW  - sorghum plant height
KW  - structure from motion
KW  - multi-level GCPs
KW  - GCP-based height calibration
KW  - image blurriness
KW  - wind speed
DO  - 10.3390/s18124092
TY  - EJOU
AU  - Aragon, Bruno
AU  - Houborg, Rasmus
AU  - Tu, Kevin
AU  - Fisher, Joshua B.
AU  - McCabe, Matthew
TI  - CubeSats Enable High Spatiotemporal Retrievals of Crop-Water Use for Precision Agriculture
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Remote sensing based estimation of evapotranspiration (ET) provides a direct accounting of the crop water use. However, the use of satellite data has generally required that a compromise between spatial and temporal resolution is made, i.e., one could obtain low spatial resolution data regularly, or high spatial resolution occasionally. As a consequence, this spatiotemporal trade-off has tended to limit the impact of remote sensing for precision agricultural applications. With the recent emergence of constellations of small CubeSat-based satellite systems, these constraints are rapidly being removed, such that daily 3 m resolution optical data are now a reality for earth observation. Such advances provide an opportunity to develop new earth system monitoring and assessment tools. In this manuscript we evaluate the capacity of CubeSats to advance the estimation of ET via application of the Priestley-Taylor Jet Propulsion Laboratory (PT-JPL) retrieval model. To take advantage of the high-spatiotemporal resolution afforded by these systems, we have integrated a CubeSat derived leaf area index as a forcing variable into PT-JPL, as well as modified key biophysical model parameters. We evaluate model performance over an irrigated farmland in Saudi Arabia using observations from an eddy covariance tower. Crop water use retrievals were also compared against measured irrigation from an in-line flow meter installed within a center-pivot system. To leverage the high spatial resolution of the CubeSat imagery, PT-JPL retrievals were integrated over the source area of the eddy covariance footprint, to allow an equivalent intercomparison. Apart from offering new precision agricultural insights into farm operations and management, the 3 m resolution ET retrievals were shown to explain 86% of the observed variability and provide a relative RMSE of 32.9% for irrigated maize, comparable to previously reported satellite-based retrievals. An observed underestimation was diagnosed as a possible misrepresentation of the local surface moisture status, highlighting the challenge of high-resolution modeling applications for precision agriculture and informing future research directions.
KW  - CubeSats
KW  - evapotranspiration
KW  - PT-JPL
KW  - remote sensing
KW  - Saudi Arabia
KW  - high-resolution
KW  - precision agriculture
DO  - 10.3390/rs10121867
TY  - EJOU
AU  - Morales, Giorgio
AU  - Kemper, Guillermo
AU  - Sevillano, Grace
AU  - Arteaga, Daniel
AU  - Ortega, Ivan
AU  - Telles, Joel
TI  - Automatic Segmentation of Mauritia flexuosa in Unmanned Aerial Vehicle (UAV) Imagery Using Deep Learning
T2  - Forests

PY  - 2018
VL  - 9
IS  - 12
SN  - 1999-4907

AB  - One of the most important ecosystems in the Amazon rainforest is the Mauritia flexuosa swamp or “aguajal”. However, deforestation of its dominant species, the Mauritia flexuosa palm, also known as “aguaje”, is a common issue, and conservation is poorly monitored because of the difficult access to these swamps. The contribution of this paper is twofold: the presentation of a dataset called MauFlex, and the proposal of a segmentation and measurement method for areas covered in Mauritia flexuosa palms using high-resolution aerial images acquired by UAVs. The method performs a semantic segmentation of Mauritia flexuosa using an end-to-end trainable Convolutional Neural Network (CNN) based on the Deeplab v3+ architecture. Images were acquired under different environment and light conditions using three different RGB cameras. The MauFlex dataset was created from these images and it consists of 25,248 image patches of     512 × 512     pixels and their respective ground truth masks. The results over the test set achieved an accuracy of 98.143%, specificity of 96.599%, and sensitivity of 95.556%. It is shown that our method is able not only to detect full-grown isolated Mauritia flexuosa palms, but also young palms or palms partially covered by other types of vegetation.
KW  - Mauritia flexuosa
KW  - semantic segmentation
KW  - end-to-end learning
KW  - convolutional neural network
KW  - forest inventory
DO  - 10.3390/f9120736
TY  - EJOU
AU  - Al Rahhal, Mohamad M.
AU  - Bazi, Yakoub
AU  - Abdullah, Taghreed
AU  - Mekhalfi, Mohamed L.
AU  - AlHichri, Haikel
AU  - Zuair, Mansour
TI  - Learning a Multi-Branch Neural Network from Multiple Sources for Knowledge Adaptation in Remote Sensing Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - In this paper we propose a multi-branch neural network, called MB-Net, for solving the problem of knowledge adaptation from multiple remote sensing scene datasets acquired with different sensors over diverse locations and manually labeled with different experts. Our aim is to learn invariant feature representations from multiple source domains with labeled images and one target domain with unlabeled images. To this end, we define for MB-Net an objective function that mitigates the multiple domain shifts at both feature representation and decision levels, while retaining the ability to discriminate between different land-cover classes. The complete architecture is trainable end-to-end via the backpropagation algorithm. In the experiments, we demonstrate the effectiveness of the proposed method on a new multiple domain dataset created from four heterogonous scene datasets well known to the remote sensing community, namely, the University of California (UC-Merced) dataset, the Aerial Image dataset (AID), the PatternNet dataset, and the Northwestern Polytechnical University (NWPU) dataset. In particular, this method boosts the average accuracy over all transfer scenarios up to 89.05% compared to standard architecture based only on cross-entropy loss, which yields an average accuracy of 78.53%.
KW  - scene classification
KW  - multiple sources
KW  - multiple domain shifts
KW  - multi-branch neural network
DO  - 10.3390/rs10121890
TY  - EJOU
AU  - Sarron, Julien
AU  - Malézieux, Éric
AU  - Sané, Cheikh A.
AU  - Faye, Émile
TI  - Mango Yield Mapping at the Orchard Scale Based on Tree Structure and Land Cover Assessed by UAV
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - In the value chain, yields are key information for both growers and other stakeholders in market supply and exports. However, orchard yields are often still based on an extrapolation of tree production which is visually assessed on a limited number of trees; a tedious and inaccurate task that gives no yield information at a finer scale than the orchard plot. In this work, we propose a method to accurately map individual tree production at the orchard scale by developing a trade-off methodology between mechanistic yield modelling and extensive fruit counting using machine vision systems. A methodological toolbox was developed and tested to estimate and map tree species, structure, and yields in mango orchards of various cropping systems (from monocultivar to plurispecific orchards) in the Niayes region, West Senegal. Tree structure parameters (height, crown area and volume), species, and mango cultivars were measured using unmanned aerial vehicle (UAV) photogrammetry and geographic, object-based image analysis. This procedure reached an average overall accuracy of 0.89 for classifying tree species and mango cultivars. Tree structure parameters combined with a fruit load index, which takes into account year and management effects, were implemented in predictive production models of three mango cultivars. Models reached satisfying accuracies with R2 greater than 0.77 and RMSE% ranging from 20% to 29% when evaluated with the measured production of 60 validation trees. In 2017, this methodology was applied to 15 orchards overflown by UAV, and estimated yields were compared to those measured by the growers for six of them, showing the proper efficiency of our technology. The proposed method achieved the breakthrough of rapidly and precisely mapping mango yields without detecting fruits from ground imagery, but rather, by linking yields with tree structural parameters. Such a tool will provide growers with accurate yield estimations at the orchard scale, and will permit them to study the parameters that drive yield heterogeneity within and between orchards.
KW  - unmanned aerial vehicle
KW  - mango orchard
KW  - yield estimation
KW  - fruit detection
KW  - tree architecture
KW  - random forest
KW  - GEOBIA
KW  - structure-from-motion
DO  - 10.3390/rs10121900
TY  - EJOU
AU  - Pádua, Luís
AU  - Marques, Pedro
AU  - Hruška, Jonáš
AU  - Adão, Telmo
AU  - Peres, Emanuel
AU  - Morais, Raul
AU  - Sousa, Joaquim J.
TI  - Multi-Temporal Vineyard Monitoring through UAV-Based RGB Imagery
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - This study aimed to characterize vineyard vegetation thorough multi-temporal monitoring using a commercial low-cost rotary-wing unmanned aerial vehicle (UAV) equipped with a consumer-grade red/green/blue (RGB) sensor. Ground-truth data and UAV-based imagery were acquired on nine distinct dates, covering the most significant vegetative growing cycle until harvesting season, over two selected vineyard plots. The acquired UAV-based imagery underwent photogrammetric processing resulting, per flight, in an orthophoto mosaic, used for vegetation estimation. Digital elevation models were used to compute crop surface models. By filtering vegetation within a given height-range, it was possible to separate grapevine vegetation from other vegetation present in a specific vineyard plot, enabling the estimation of grapevine area and volume. The results showed high accuracy in grapevine detection (94.40%) and low error in grapevine volume estimation (root mean square error of 0.13 m and correlation coefficient of 0.78 for height estimation). The accuracy assessment showed that the proposed method based on UAV-based RGB imagery is effective and has potential to become an operational technique. The proposed method also allows the estimation of grapevine areas that can potentially benefit from canopy management operations.
KW  - unmanned aerial vehicles
KW  - precision viticulture
KW  - multi-temporal analysis
KW  - crop surface models
DO  - 10.3390/rs10121907
TY  - EJOU
AU  - Griffith, David C.
AU  - Hay, Geoffrey J.
TI  - Integrating GEOBIA, Machine Learning, and Volunteered Geographic Information to Map Vegetation over Rooftops
T2  - ISPRS International Journal of Geo-Information

PY  - 2018
VL  - 7
IS  - 12
SN  - 2220-9964

AB  - The objective of this study is to evaluate operational methods for creating a particular type of urban vegetation map—one focused on vegetation over rooftops (VOR), specifically trees that extend over urban residential buildings. A key constraint was the use of passive remote sensing data only. To achieve this, we (1) conduct a review of the urban remote sensing vegetation classification literature, and we then (2) discuss methods to derive a detailed map of VOR for a study area in Calgary, Alberta, Canada from a late season, high-resolution airborne orthomosaic based on an integration of Geographic Object-Based Image Analysis (GEOBIA), pre-classification filtering of image-objects using Volunteered Geographic Information (VGI), and a machine learning classifier. Pre-classification filtering lowered the computational burden of classification by reducing the number of input objects by 14%. Accuracy assessment results show that, despite the presence of senescing vegetation with low vegetation index values and deep shadows, classification using a small number of image-object spectral attributes as classification features (n = 9) had similar overall accuracy (88.5%) to a much more complex classification (91.8%) comprising a comprehensive set of spectral, texture, and spatial attributes as classification features (n = 86). This research provides an example of the very specific questions answerable about precise urban locations using a combination of high-resolution passive imagery and freely available VGI data. It highlights the benefits of pre-classification filtering and the judicious selection of features from image-object attributes to reduce processing load without sacrificing classification accuracy.
KW  - GEOBIA
KW  - vegetation over rooftops
KW  - machine learning
DO  - 10.3390/ijgi7120462
TY  - EJOU
AU  - Fu, Kun
AU  - Li, Yang
AU  - Sun, Hao
AU  - Yang, Xue
AU  - Xu, Guangluan
AU  - Li, Yuting
AU  - Sun, Xian
TI  - A Ship Rotation Detection Model in Remote Sensing Images Based on Feature Fusion Pyramid Network and Deep Reinforcement Learning
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Ship detection plays an important role in automatic remote sensing image interpretation. The scale difference, large aspect ratio of ship, complex remote sensing image background and ship dense parking scene make the detection task difficult. To handle the challenging problems above, we propose a ship rotation detection model based on a Feature Fusion Pyramid Network and deep reinforcement learning (FFPN-RL) in this paper. The detection network can efficiently generate the inclined rectangular box for ship. First, we propose the Feature Fusion Pyramid Network (FFPN) that strengthens the reuse of different scales features, and FFPN can extract the low level location and high level semantic information that has an important impact on multi-scale ship detection and precise location of dense parking ships. Second, in order to get accurate ship angle information, we apply deep reinforcement learning to the inclined ship detection task for the first time. In addition, we put forward prior policy guidance and a long-term training method to train an angle prediction agent constructed through a dueling structure Q network, which is able to iteratively and accurately obtain the ship angle. In addition, we design soft rotation non-maximum suppression to reduce the missed ship detection while suppressing the redundant detection boxes. We carry out detailed experiments on the remote sensing ship image dataset, and the experiments validate that our FFPN-RL ship detection model has efficient detection performance.
KW  - ship detection
KW  - deep reinforcement learning
KW  - convolution neural network
KW  - feature map fusion
DO  - 10.3390/rs10121922
TY  - EJOU
AU  - Wang, Bing
AU  - Jia, Kun
AU  - Liang, Shunlin
AU  - Xie, Xianhong
AU  - Wei, Xiangqin
AU  - Zhao, Xiang
AU  - Yao, Yunjun
AU  - Zhang, Xiaotong
TI  - Assessment of Sentinel-2 MSI Spectral Band Reflectances for Estimating Fractional Vegetation Cover
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Fractional vegetation cover (FVC) is an essential parameter for characterizing the land surface vegetation conditions and plays an important role in earth surface process simulations and global change studies. The Sentinel-2 missions carrying multi-spectral instrument (MSI) sensors with 13 multispectral bands are potentially useful for estimating FVC. However, the performance of these bands for FVC estimation is unclear. Therefore, the objective of this study was to assess the performance of Sentinel-2 MSI spectral band reflectances on FVC estimation. The samples, including the Sentinel-2 MSI canopy reflectances and corresponding FVC values, were simulated using the PROSPECT + SAIL radiative transfer model under different conditions, and random forest regression (RFR) method was then used to develop FVC estimation models and assess the performance of various band reflectances for FVC estimation. These models were finally evaluated using field survey data. The results indicate that the three most important bands of Sentinel-2 MSI data for FVC estimation are band 4 (Red), band 12 (SWIR2) and band 8a (NIR2). FVC estimation using these bands has a comparable accuracy (root mean square error (RMSE) = 0.085) with that using all bands (RMSE = 0.090). The results also demonstrate that band 12 had a better performance for FVC estimation than the green band (RMSE = 0.097). However, the newly added red-edge bands, with low scores in the RFR model, have little significance for improving FVC estimation accuracy compared with the Red, NIR2 and SWIR2 bands.
KW  - Sentinel-2 satellites
KW  - fractional vegetation cover
KW  - variable selection
KW  - random forest regression
DO  - 10.3390/rs10121927
TY  - EJOU
AU  - Wu, Hao
AU  - Pang, Bo
AU  - Dai, Dahai
AU  - Wu, Jiani
AU  - Wang, Xuesong
TI  - Unmanned Aerial Vehicle Recognition Based on Clustering by Fast Search and Find of Density Peaks (CFSFDP) with Polarimetric Decomposition
T2  - Electronics

PY  - 2018
VL  - 7
IS  - 12
SN  - 2079-9292

AB  - Unmanned aerial vehicles (UAV) have become vital targets in civilian and military fields. However, the polarization characteristics are rarely studied. This paper studies the polarization property of UAVs via the fusion of three polarimetric decomposition methods. A novel algorithm is presented to classify and recognize UAVs automatically which includes a clustering method proposed in &ldquo;Science&rdquo;, one of the top journals in academia. Firstly, the selection of the imaging algorithm ensures the quality of the radar images. Secondly, local geometrical structures of UAVs can be extracted based on Pauli, Krogager, and Cameron polarimetric decomposition. Finally, the proposed algorithm with clustering by fast search and find of density peaks (CFSFDP) has been demonstrated to be better than the original methods under the various noise conditions with the fusion of three polarimetric decomposition methods.
KW  - unmanned aerial vehicle
KW  - clustering methods
KW  - man-made targets
KW  - synthetic aperture radar (SAR)
KW  - inverse synthetic aperture radar (ISAR)
KW  - polarimetric decomposition
DO  - 10.3390/electronics7120364
TY  - EJOU
AU  - Liu, Mingyue
AU  - Mao, Dehua
AU  - Wang, Zongming
AU  - Li, Lin
AU  - Man, Weidong
AU  - Jia, Mingming
AU  - Ren, Chunying
AU  - Zhang, Yuanzhi
TI  - Rapid Invasion of Spartina alterniflora in the Coastal Zone of Mainland China: New Observations from Landsat OLI Images
T2  - Remote Sensing

PY  - 2018
VL  - 10
IS  - 12
SN  - 2072-4292

AB  - Plant invasion imposes significant threats to biodiversity and ecosystem function. Thus, monitoring the spatial pattern of invasive plants is vital for effective ecosystem management. Spartina alterniflora (S. alterniflora) has been one of the most prevalent invasive plants along the China coast, and its spread has had severe ecological consequences. Here, we provide new observation from Landsat operational land imager (OLI) images. Specifically, 43 Landsat-8 OLI images from 2014 to 2016, a combination of object-based image analysis (OBIA) and support vector machine (SVM) methods, and field surveys covering the whole coast were used to construct an up-to-date dataset for 2015 and investigate the spatial variability of S. alterniflora in the coastal zone of mainland China. The classification results achieved good estimation, with a kappa coefficient of 0.86 and 96% overall accuracy. Our results revealed that there was approximately 545.80 km2 of S. alterniflora distributed in the coastal zone of mainland China in 2015, from Hebei to Guangxi provinces. Nearly 92% of the total area of S. alterniflora was distributed within four provinces: Jiangsu, Shanghai, Zhejiang, and Fujian. Seven national nature reserves invaded by S. alterniflora encompassed approximately one-third (174.35 km2) of the total area of S. alterniflora over mainland China. The Yancheng National Nature Reserve exhibited the largest area of S. alterniflora (115.62 km2) among the reserves. Given the rapid and extensive expansion of S. alterniflora in the 40 years since its introduction and its various ecological effects, geospatially varied responding decisions are needed to promote sustainable coastal ecosystems.
KW  - invasive plants
KW  - Spartina alterniflora
KW  - CAS S. alterniflora
KW  - object-based image analysis
KW  - Landsat OLI
DO  - 10.3390/rs10121933
TY  - EJOU
AU  - Castillejo-González, Isabel L.
TI  - Mapping of Olive Trees Using Pansharpened QuickBird Images: An Evaluation of Pixel- and Object-Based Analyses
T2  - Agronomy

PY  - 2018
VL  - 8
IS  - 12
SN  - 2073-4395

AB  - This study sought to verify whether remote sensing offers the ability to efficiently delineate olive tree canopies using QuickBird (QB) satellite imagery. This paper compares four classification algorithms performed in pixel- and object-based analyses. To increase the spectral and spatial resolution of the standard QB image, three different pansharpened images were obtained based on variations in the weight of the red and near infrared bands. The results showed slight differences between classifiers. Maximum Likelihood algorithm yielded the highest results in pixel-based classifications with an average overall accuracy (OA) of 94.2%. In object-based analyses, Maximum Likelihood and Decision Tree classifiers offered the highest precisions with average OA of 95.3% and 96.6%, respectively. Between pixel- and object-based analyses no clear difference was observed, showing an increase of average OA values of approximately 1% for all classifiers except Decision Tree, which improved up to 4.5%. The alteration of the weight of different bands in the pansharpen process exhibited satisfactory results with a general performance improvement of up to 9% and 11% in pixel- and object-based analyses, respectively. Thus, object-based analyses with the DT algorithm and the pansharpened imagery with the near-infrared band altered would be highly recommended to obtain accurate maps for site-specific management.
KW  - Á Trous algorithm
KW  - conservation agriculture
KW  - crop inventory
KW  - remote sensing
KW  - spectral-weight variations in fused images
DO  - 10.3390/agronomy8120288
TY  - EJOU
AU  - Xu, Yanlei
AU  - Gao, Zongmei
AU  - Khot, Lav
AU  - Meng, Xiaotian
AU  - Zhang, Qin
TI  - A Real-Time Weed Mapping and Precision Herbicide Spraying System for Row Crops
T2  - Sensors

PY  - 2018
VL  - 18
IS  - 12
SN  - 1424-8220

AB  - This study developed and field tested an automated weed mapping and variable-rate herbicide spraying (VRHS) system for row crops. Weed detection was performed through a machine vision sub-system that used a custom threshold segmentation method, an improved particle swarm optimum (IPSO) algorithm, capable of segmenting the field images. The VRHS system also used a lateral histogram-based algorithm for fast extraction of weed maps. This was the basis for determining real-time herbicide application rates. The central processor of the VRHS system had high logic operation capacity, compared to the conventional controller-based systems. Custom developed monitoring system allowed real-time visualization of the spraying system functionalities. Integrated system performance was then evaluated through field experiments. The IPSO successfully segmented weeds within corn crop at seedling growth stage and reduced segmentation error rates to 0.1% from 7.1% of traditional particle swarm optimization algorithm. IPSO processing speed was 0.026 s/frame. The weed detection to chemical actuation response time of integrated system was 1.562 s. Overall, VRHS system met the real-time data processing and actuation requirements for its use in practical weed management applications.
KW  - variable-rate herbicide spraying
KW  - weed map
KW  - particle swarm optimum algorithm
KW  - smart controller
DO  - 10.3390/s18124245
TY  - EJOU
AU  - Behmann, Jan
AU  - Bohnenkamp, David
AU  - Paulus, Stefan
AU  - Mahlein, Anne-Katrin
TI  - Spatial Referencing of Hyperspectral Images for Tracing of Plant Disease Symptoms
T2  - Journal of Imaging

PY  - 2018
VL  - 4
IS  - 12
SN  - 2313-433X

AB  - The characterization of plant disease symptoms by hyperspectral imaging is often limited by the missing ability to investigate early, still invisible states. Automatically tracing the symptom position on the leaf back in time could be a promising approach to overcome this limitation. Therefore we present a method to spatially reference time series of close range hyperspectral images. Based on reference points, a robust method is presented to derive a suitable transformation model for each observation within a time series experiment. A non-linear 2D polynomial transformation model has been selected to cope with the specific structure and growth processes of wheat leaves. The potential of the method is outlined by an improved labeling procedure for very early symptoms and by extracting spectral characteristics of single symptoms represented by Vegetation Indices over time. The characteristics are extracted for brown rust and septoria tritici blotch on wheat, based on time series observations using a VISNIR (400&ndash;1000 nm) hyperspectral camera.
KW  - hyperspectral imaging
KW  - plant phenotyping
KW  - disease detection
KW  - spectral tracking
KW  - time series
DO  - 10.3390/jimaging4120143
TY  - EJOU
AU  - Zhang, Ye
AU  - Wang, Gang
AU  - Li, Mingchao
AU  - Han, Shuai
TI  - Automated Classification Analysis of Geological Structures Based on Images Data and Deep Learning Model
T2  - Applied Sciences

PY  - 2018
VL  - 8
IS  - 12
SN  - 2076-3417

AB  - It is meaningful to study the geological structures exposed on the Earth&rsquo;s surface, which is paramount to engineering design and construction. In this research, we used 2206 images with 12 labels to identify geological structures based on the Inception-v3 model. Grayscale and color images were adopted in the model. A convolutional neural network (CNN) model was also built in this research. Meanwhile, K nearest neighbors (KNN), artificial neural network (ANN) and extreme gradient boosting (XGBoost) were applied in geological structures classification based on features extracted by the Open Source Computer Vision Library (OpenCV). Finally, the performances of the five methods were compared and the results indicated that KNN, ANN, and XGBoost had a poor performance, with the accuracy of less than 40.0%. CNN was overfitting. The model trained using transfer learning had a significant effect on a small dataset of geological structure images; and the top-1 and top-3 accuracy of the model reached 83.3% and 90.0%, respectively. This shows that texture is the key feature in this research. Transfer learning based on a deep learning model can extract features of small geological structure data effectively, and it is robust in geological structure image classification.
KW  - OpenCV
KW  - machine learning
KW  - transfer learning
KW  - Inception-v3
KW  - geological structure images
KW  - convolutional neural networks
DO  - 10.3390/app8122493
