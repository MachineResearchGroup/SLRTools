
@Article{rs12142327,
AUTHOR = {Yang, Ming-Der and Huang, Kai-Hsiang and Tsai, Hui-Ping},
TITLE = {Integrating MNF and HHT Transformations into Artificial Neural Networks for Hyperspectral Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2327},
URL = {https://www.mdpi.com/2072-4292/12/14/2327},
ISSN = {2072-4292},
ABSTRACT = {The critical issue facing hyperspectral image (HSI) classification is the imbalance between dimensionality and the number of available training samples. This study attempted to solve the issue by proposing an integrating method using minimum noise fractions (MNF) and Hilbert&ndash;Huang transform (HHT) transformations into artificial neural networks (ANNs) for HSI classification tasks. MNF and HHT function as a feature extractor and image decomposer, respectively, to minimize influences of noises and dimensionality and to maximize training sample efficiency. Experimental results using two benchmark datasets, Indian Pine (IP) and Pavia University (PaviaU) hyperspectral images, are presented. With the intention of optimizing the number of essential neurons and training samples in the ANN, 1 to 1000 neurons and four proportions of training sample were tested, and the associated classification accuracies were evaluated. For the IP dataset, the results showed a remarkable classification accuracy of 99.81% with a 30% training sample from the MNF1&ndash;14+HHT-transformed image set using 500 neurons. Additionally, a high accuracy of 97.62% using only a 5% training sample was achieved for the MNF1&ndash;14+HHT-transformed images. For the PaviaU dataset, the highest classification accuracy was 98.70% with a 30% training sample from the MNF1&ndash;14+HHT-transformed image using 800 neurons. In general, the accuracy increased as the neurons increased, and as the training samples increased. However, the accuracy improvement curve became relatively flat when more than 200 neurons were used, which revealed that using more discriminative information from transformed images can reduce the number of neurons needed to adequately describe the data as well as reducing the complexity of the ANN model. Overall, the proposed method opens new avenues in the use of MNF and HHT transformations for HSI classification with outstanding accuracy performance using an ANN.},
DOI = {10.3390/rs12142327}
}



@Article{s20144042,
AUTHOR = {Vidal, Vinicius F. and Honório, Leonardo M. and Dias, Felipe M. and Pinto, Milena F. and Carvalho, Alexandre L. and Marcato, Andre L. M.},
TITLE = {Sensors Fusion and Multidimensional Point Cloud Analysis for Electrical Power System Inspection},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4042},
URL = {https://www.mdpi.com/1424-8220/20/14/4042},
ISSN = {1424-8220},
ABSTRACT = {Thermal inspection is a powerful tool that enables the diagnosis of several components at its early stages. One critical aspect that influences thermal inspection outputs is the infrared reflection from external sources. This situation may change the readings, demanding that an expert correctly define the camera position, which is a time consuming and expensive operation. To mitigate this problem, this work proposes an autonomous system capable of identifying infrared reflections by filtering and fusing data obtained from both stereo and thermal cameras. The process starts by acquiring readings from multiples Observation Points (OPs) where, at each OP, the system processes the 3D point cloud and thermal image by fusing them together. The result is a dense point cloud where each point has its spatial position and temperature. Considering that each point&rsquo;s information is acquired from multiple poses, it is possible to generate a temperature profile of each spatial point and filter undesirable readings caused by interference and other phenomena. To deploy and test this approach, a Directional Robotic System (DRS) is mounted over a traditional human-operated service vehicle. In that way, the DRS autonomously tracks and inspects any desirable equipment as the service vehicle passes them by. To demonstrate the results, this work presents the algorithm workflow, a proof of concept, and a real application result, showing improved performance in real-life conditions.},
DOI = {10.3390/s20144042}
}



@Article{s20154082,
AUTHOR = {Qiu, Zhengjun and Zhao, Nan and Zhou, Lei and Wang, Mengcen and Yang, Liangliang and Fang, Hui and He, Yong and Liu, Yufei},
TITLE = {Vision-Based Moving Obstacle Detection and Tracking in Paddy Field Using Improved Yolov3 and Deep SORT},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4082},
URL = {https://www.mdpi.com/1424-8220/20/15/4082},
ISSN = {1424-8220},
ABSTRACT = {Using intelligent agricultural machines in paddy fields has received great attention. An obstacle avoidance system is required with the development of agricultural machines. In order to make the machines more intelligent, detecting and tracking obstacles, especially the moving obstacles in paddy fields, is the basis of obstacle avoidance. To achieve this goal, a red, green and blue (RGB) camera and a computer were used to build a machine vision system, mounted on a transplanter. A method that combined the improved You Only Look Once version 3 (Yolov3) and deep Simple Online and Realtime Tracking (deep SORT) was used to detect and track typical moving obstacles, and figure out the center point positions of the obstacles in paddy fields. The improved Yolov3 has 23 residual blocks and upsamples only once, and has new loss calculation functions. Results showed that the improved Yolov3 obtained mean intersection over union (mIoU) score of 0.779 and was 27.3% faster in processing speed than standard Yolov3 on a self-created test dataset of moving obstacles (human and water buffalo) in paddy fields. An acceptable performance for detecting and tracking could be obtained in a real paddy field test with an average processing speed of 5&ndash;7 frames per second (FPS), which satisfies actual work demands. In future research, the proposed system could support the intelligent agriculture machines more flexible in autonomous navigation.},
DOI = {10.3390/s20154082}
}



@Article{rs12152379,
AUTHOR = {Pulido, Dagoberto and Salas, Joaquín and Rös, Matthias and Puettmann, Klaus and Karaman, Sertac},
TITLE = {Assessment of Tree Detection Methods in Multispectral Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2379},
URL = {https://www.mdpi.com/2072-4292/12/15/2379},
ISSN = {2072-4292},
ABSTRACT = {Detecting individual trees and quantifying their biomass is crucial for carbon accounting procedures at the stand, landscape, and national levels. A significant challenge for many organizations is the amount of effort necessary to document carbon storage levels, especially in terms of human labor. To advance towards the goal of efficiently assessing the carbon content of forest, we evaluate methods to detect trees from high-resolution images taken from unoccupied aerial systems (UAS). In the process, we introduce the Digital Elevated Vegetation Model (DEVM), a representation that combines multispectral images, digital surface models, and digital terrain models. We show that the DEVM facilitates the development of refined synthetic data to detect individual trees using deep learning-based approaches. We carried out experiments in two tree fields located in different countries. Simultaneously, we perform comparisons among an array of classical and deep learning-based methods highlighting the precision and reliability of the DEVM.},
DOI = {10.3390/rs12152379}
}



@Article{f11080808,
AUTHOR = {Prosekov, Alexander and Kuznetsov, Alexander and Rada, Artem and Ivanova, Svetlana},
TITLE = {Methods for Monitoring Large Terrestrial Animals in the Wild},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {808},
URL = {https://www.mdpi.com/1999-4907/11/8/808},
ISSN = {1999-4907},
ABSTRACT = {Reliable information about wildlife is absolutely important for making informed management decisions. The issues with the effectiveness of the control and monitoring of both large and small wild animals are relevant to assess and protect the world&rsquo;s biodiversity. Monitoring becomes part of the methods in wildlife ecology for observation, assessment, and forecasting of the human environment. World practice reveals the potential of the joint application of both proven traditional and modern technologies using specialized equipment to organize environmental control and management processes. Monitoring large terrestrial animals require an individual approach due to their low density and larger habitat. Elk/moose are such animals. This work aims to evaluate the methods for monitoring large wild animals, suitable for controlling the number of elk/moose in the framework of nature conservation activities. Using different models allows determining the population size without affecting the animals and without significant financial costs. Although, the accuracy of each model is determined by its postulates implementation and initial conditions that need statistical data. Depending on the geographical, climatic, and economic conditions in each territory, it is possible to use different tools and equipment (e.g., cameras, GPS sensors, and unmanned aerial vehicles), a flexible variation of which will allow reaching the golden mean between the desires and capabilities of researchers.},
DOI = {10.3390/f11080808}
}



@Article{rs12152397,
AUTHOR = {Schlosser, Aletta Dóra and Szabó, Gergely and Bertalan, László and Varga, Zsolt and Enyedi, Péter and Szabó, Szilárd},
TITLE = {Building Extraction Using Orthophotos and Dense Point Cloud Derived from Visual Band Aerial Imagery Based on Machine Learning and Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2397},
URL = {https://www.mdpi.com/2072-4292/12/15/2397},
ISSN = {2072-4292},
ABSTRACT = {Urban sprawl related increase of built-in areas requires reliable monitoring methods and remote sensing can be an efficient technique. Aerial surveys, with high spatial resolution, provide detailed data for building monitoring, but archive images usually have only visible bands. We aimed to reveal the efficiency of visible orthophotographs and photogrammetric dense point clouds in building detection with segmentation-based machine learning (with five algorithms) using visible bands, texture information, and spectral and morphometric indices in different variable sets. Usually random forest (RF) had the best (99.8%) and partial least squares the worst overall accuracy (~60%). We found that &gt;95% accuracy can be gained even in class level. Recursive feature elimination (RFE) was an efficient variable selection tool, its result with six variables was like when we applied all the available 31 variables. Morphometric indices had 82% producer&rsquo;s and 85% user&rsquo;s Accuracy (PA and UA, respectively) and combining them with spectral and texture indices, it had the largest contribution in the improvement. However, morphometric indices are not always available but by adding texture and spectral indices to red-green-blue (RGB) bands the PA improved with 12% and the UA with 6%. Building extraction from visual aerial surveys can be accurate, and archive images can be involved in the time series of a monitoring.},
DOI = {10.3390/rs12152397}
}



@Article{su12156080,
AUTHOR = {Zwęgliński, Tomasz},
TITLE = {The Use of Drones in Disaster Aerial Needs Reconnaissance and Damage Assessment – Three-Dimensional Modeling and Orthophoto Map Study},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {6080},
URL = {https://www.mdpi.com/2071-1050/12/15/6080},
ISSN = {2071-1050},
ABSTRACT = {The aim of this research is to provide disaster managers with the results of testing three-dimensional modeling and orthophoto mapping, so as to add value to aerial assessments of flood-related needs and damages. The relevant testing of solutions concerning the real needs of disaster managers is an essential part of the pre-disaster phase. As such, providing evidence-based results of the solutions&rsquo; performance is critical with regard to purchasing them and their successful implementation for disaster management purposes. Since disaster response is mostly realized in complex and dynamic, rather than repetitive, environments, it requires pertinent testing methods. A quasi-experimental approach, applied in a form of a full-scale trial meets disaster manager&rsquo;s requirements as well as addressing limitations resulting from the disaster environment&rsquo;s characteristics. Three-dimensional modeling and orthophoto mapping have already proven their potential in many professional fields; however, they have not yet been broadly tested for disaster response purposes. Therefore, the objective here is to verify the technologies regarding their applicability in aerial reconnaissance in sudden-onset disasters. The hypothesis assumes that they will improve the efficiency (e.g., time) and effectiveness (e.g., accuracy of revealed data) of this process. The research verifies that the technologies have a potential to facilitate disaster managers with more precise damage assessment; however, their effectivity was less than expected in terms of needs reconnaissance. Secondly, the overall assessment process is heavily burdened by data processing time, however, the technologies allow a reduction of analytical work.},
DOI = {10.3390/su12156080}
}



@Article{rs12152426,
AUTHOR = {Pleșoianu, Alin-Ionuț and Stupariu, Mihai-Sorin and Șandric, Ionuț and Pătru-Stupariu, Ileana and Drăguț, Lucian},
TITLE = {Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2426},
URL = {https://www.mdpi.com/2072-4292/12/15/2426},
ISSN = {2072-4292},
ABSTRACT = {Traditional methods for individual tree-crown (ITC) detection (image classification, segmentation, template matching, etc.) applied to very high-resolution remote sensing imagery have been shown to struggle in disparate landscape types or image resolutions due to scale problems and information complexity. Deep learning promised to overcome these shortcomings due to its superior performance and versatility, proven with reported detection rates of ~90%. However, such models still find their limits in transferability across study areas, because of different tree conditions (e.g., isolated trees vs. compact forests) and/or resolutions of the input data. This study introduces a highly replicable deep learning ensemble design for ITC detection and species classification based on the established single shot detector (SSD) model. The ensemble model design is based on varying the input data for the SSD models, coupled with a voting strategy for the output predictions. Very high-resolution unmanned aerial vehicles (UAV), aerial remote sensing imagery and elevation data are used in different combinations to test the performance of the ensemble models in three study sites with highly contrasting spatial patterns. The results show that ensemble models perform better than any single SSD model, regardless of the local tree conditions or image resolution. The detection performance and the accuracy rates improved by 3&ndash;18% with only as few as two participant single models, regardless of the study site. However, when more than two models were included, the performance of the ensemble models only improved slightly and even dropped.},
DOI = {10.3390/rs12152426}
}



@Article{rs12152427,
AUTHOR = {Cai, Yiming and Ding, Yalin and Zhang, Hongwen and Xiu, Jihong and Liu, Zhiming},
TITLE = {Geo-Location Algorithm for Building Targets in Oblique Remote Sensing Images Based on Deep Learning and Height Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2427},
URL = {https://www.mdpi.com/2072-4292/12/15/2427},
ISSN = {2072-4292},
ABSTRACT = {To improve the accuracy of the geographic positioning of a single aerial remote sensing image, the height information of a building in the image must be considered. Oblique remote sensing images are essentially two-dimensional images and produce a large positioning error if a traditional positioning algorithm is used to locate the building directly. To address this problem, this study uses a convolutional neural network to automatically detect the location of buildings in remote sensing images. Moreover, it optimizes an automatic building recognition algorithm for oblique aerial remote sensing images based on You Only Look Once V4 (YOLO V4). This study also proposes a positioning algorithm for the building target, which uses the imaging angle to estimate the height of a building, and combines the spatial coordinate transformation matrix to calculate high-accuracy geo-location of target buildings. Simulation analysis shows that the traditional positioning algorithm inevitably leads to large errors in the positioning of building targets. When the target height is 50 m and the imaging angle is 70&deg;, the positioning error is 114.89 m. Flight tests show that the algorithm established in this study can improve the positioning accuracy of building targets by approximately 20%&ndash;50% depending on the difference in target height.},
DOI = {10.3390/rs12152427}
}



@Article{s20154214,
AUTHOR = {Mekhalfi, Mohamed Lamine and Nicolò, Carlo and Ianniello, Ivan and Calamita, Federico and Goller, Rino and Barazzuol, Maurizio and Melgani, Farid},
TITLE = {Vision System for Automatic On-Tree Kiwifruit Counting and Yield Estimation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4214},
URL = {https://www.mdpi.com/1424-8220/20/15/4214},
ISSN = {1424-8220},
ABSTRACT = {Yield estimation is an essential preharvest practice among most large-scale farming companies, since it enables the predetermination of essential logistics to be allocated (i.e., transportation means, supplies, labor force, among others). An overestimation may thus incur further costs, whereas an underestimation entails potential crop waste. More interestingly, an accurate yield estimation enables stakeholders to better place themselves in the market. Yet, computer-aided precision farming is set to play a pivotal role in this respect. Kiwifruit represents a major produce in several countries (e.g., Italy, China, New and Zealand). However, up to date, the relevant literature remains short of a complete as well as automatic system for kiwifruit yield estimation. In this paper, we present a fully automatic and noninvasive computer vision system for kiwifruit yield estimation across a given orchard. It consists mainly of an optical sensor mounted on a minitractor that surveys the orchard of interest at a low pace. Afterwards, the acquired images are fed to a pipeline that incorporates image preprocessing, stitching, and fruit counting stages and outputs an estimated fruit count and yield estimation. Experimental results conducted on two large kiwifruit orchards confirm a high plausibility (i.e., errors of 6% and 15%) of the proposed system. The proposed yield estimation solution has been in commercial use for about 2 years. With respect to the traditional manual yield estimation carried out by kiwifruit companies, it was demonstrated to save a significant amount of time and cut down on estimation errors, especially when speaking of large-scale farming.},
DOI = {10.3390/s20154214}
}



@Article{iot1010003,
AUTHOR = {Michailidis, Emmanouel T. and Potirakis, Stelios M. and Kanatas, Athanasios G.},
TITLE = {AI-Inspired Non-Terrestrial Networks for IIoT: Review on Enabling Technologies and Applications},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {1},
PAGES = {21--48},
URL = {https://www.mdpi.com/2624-831X/1/1/3},
ISSN = {2624-831X},
ABSTRACT = {During the last few years, various Industrial Internet of Things (IIoT) applications have emerged with numerous network elements interconnected using wired and wireless communication technologies and equipped with strategically placed sensors and actuators. This paper justifies why non-terrestrial networks (NTNs) will bring the IIoT vision closer to reality by providing improved data acquisition and massive connectivity to sensor fields in large and remote areas. NTNs are engineered to utilize satellites, airships, and aircrafts, which can be employed to extend the radio coverage and provide remote monitoring and sensing services. Additionally, this paper describes indicative delay-tolerant massive IIoT and delay-sensitive mission-critical IIoT applications spanning a large number of vertical markets with diverse and stringent requirements. As the heterogeneous nature of NTNs and the complex and dynamic communications scenarios lead to uncertainty and a high degree of variability, conventional wireless communication technologies cannot sufficiently support ultra-reliable and low-latency communications (URLLC) and offer ubiquitous and uninterrupted interconnectivity. In this regard, this paper sheds light on the potential role of artificial intelligence (AI) techniques, including machine learning (ML) and deep learning (DL), in the provision of challenging NTN-based IIoT services and provides a thorough review of the relevant research works. By adding intelligence and facilitating the decision-making and prediction procedures, the NTNs can effectively adapt to their surrounding environment, thus enhancing the performance of various metrics with significantly lower complexity compared to typical optimization methods.},
DOI = {10.3390/iot1010003}
}



@Article{en13153910,
AUTHOR = {Li, Hongchen and Yang, Zhong and Han, Jiaming and Lai, Shangxiang and Zhang, Qiuyan and Zhang, Chi and Fang, Qianhui and Hu, Guoxiong},
TITLE = {TL-Net: A Novel Network for Transmission Line Scenes Classification},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {3910},
URL = {https://www.mdpi.com/1996-1073/13/15/3910},
ISSN = {1996-1073},
ABSTRACT = {With the development of unmanned aerial vehicle (UAV) control technology, one of the recent trends in this research domain is to utilize UAVs to perform non-contact transmission line inspection. The RGB camera mounted on UAVs collects large numbers of images during the transmission line inspection, but most of them contain no critical components of transmission lines. Hence, it is a momentous task to adopt image classification algorithms to distinguish key images from all aerial images. In this work, we propose a novel classification method to remove redundant data and retain informative images. A novel transmission line scene dataset, namely TLS_dataset, is built to evaluate the classification performance of networks. Then, we propose a novel convolutional neural network (CNN), namely TL-Net, to classify transmission line scenes. In comparison to other typical deep learning networks, TL-Nets gain better classification accuracy and less memory consumption. The experimental results show that TL-Net101 gains 99.68% test accuracy on the TLS_dataset.},
DOI = {10.3390/en13153910}
}



@Article{rs12152460,
AUTHOR = {You, Yanan and Cao, Jingyi and Zhou, Wenli},
TITLE = {A Survey of Change Detection Methods Based on Remote Sensing Images for Multi-Source and Multi-Objective Scenarios},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2460},
URL = {https://www.mdpi.com/2072-4292/12/15/2460},
ISSN = {2072-4292},
ABSTRACT = {Quantities of multi-temporal remote sensing (RS) images create favorable conditions for exploring the urban change in the long term. However, diverse multi-source features and change patterns bring challenges to the change detection in urban cases. In order to sort out the development venation of urban change detection, we make an observation of the literatures on change detection in the last five years, which focuses on the disparate multi-source RS images and multi-objective scenarios determined according to scene category. Based on the survey, a general change detection framework, including change information extraction, data fusion, and analysis of multi-objective scenarios modules, is summarized. Owing to the attributes of input RS images affect the technical selection of each module, data characteristics and application domains across different categories of RS images are discussed firstly. On this basis, not only the evolution process and relationship of the representative solutions are elaborated in the module description, through emphasizing the feasibility of fusing diverse data and the manifold application scenarios, we also advocate a complete change detection pipeline. At the end of the paper, we conclude the current development situation and put forward possible research direction of urban change detection, in the hope of providing insights to the following research.},
DOI = {10.3390/rs12152460}
}



@Article{s20154282,
AUTHOR = {Jia, Guifeng and Li, Wei and Meng, Junyu and Tan, Hequn and Feng, Yaoze},
TITLE = {Non-Contact Evaluation of Pigs’ Body Temperature Incorporating Environmental Factors},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4282},
URL = {https://www.mdpi.com/1424-8220/20/15/4282},
ISSN = {1424-8220},
ABSTRACT = {Internal body temperature is the gold standard for the fever of pigs, however non-contact infrared imaging technology (IRT) can only measure the skin temperature of regions of interest (ROI). Therefore, using IRT to detect the internal body temperature should be based on a correlation model between the ROI temperature and the internal temperature. When heat exchange between the ROI and the surroundings makes the ROI temperature more correlated with the environment, merely depending on the ROI to predict the internal temperature is unreliable. To ensure a high prediction accuracy, this paper investigated the influence of air temperature and humidity on ROI temperature, then built a prediction model incorporating them. The animal test includes 18 swine. IRT was employed to collect the temperatures of the backside, eye, vulva, and ear root ROIs; meanwhile, the air temperature and humidity were recorded. Body temperature prediction models incorporating environmental factors and the ROI temperature were constructed based on Back Propagate Neural Net (BPNN), Random Forest (RF), and Support Vector Regression (SVR). All three models yielded better results regarding the maximum error, minimum error, and mean square error (MSE) when the environmental factors were considered. When environmental factors were incorporated, SVR produced the best outcome, with the maximum error at 0.478 &deg;C, the minimum error at 0.124 &deg;C, and the MSE at 0.159 &deg;C. The result demonstrated the accuracy and applicability of SVR as a prediction model of pigs&prime; internal body temperature.},
DOI = {10.3390/s20154282}
}



@Article{en13153916,
AUTHOR = {Nielsen, Mikkel Schou and Nikolov, Ivan and Kruse, Emil Krog and Garnæs, Jørgen and Madsen, Claus Brøndgaard},
TITLE = {High-Resolution Structure-from-Motion for Quantitative Measurement of Leading-Edge Roughness},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {3916},
URL = {https://www.mdpi.com/1996-1073/13/15/3916},
ISSN = {1996-1073},
ABSTRACT = {Over time, erosion of the leading edge of wind turbine blades increases the leading-edge roughness (LER). This may reduce the aerodynamic performance of the blade and hence the annual energy production of the wind turbine. As early detection is key for cost-effective maintenance, inspection methods are needed to quantify the LER of the blade. The aim of this proof-of-principle study is to determine whether high-resolution Structure-from-Motion (SfM) has the sufficient resolution and accuracy for quantitative inspection of LER. SfM provides 3D reconstruction of an object geometry using overlapping images of the object acquired with an RGB camera. Using information of the camera positions and orientations, absolute scale of the reconstruction can be achieved. Combined with a UAV platform, SfM has the potential for remote blade inspections with a reduced downtime. The tip of a decommissioned blade with an artificially enhanced erosion was used for the measurements. For validation, replica molding was used to transfer areas-of-interest to the lab for reference measurements using confocal microscopy. The SfM reconstruction resulted in a spatial resolution of 1 mm as well as a sub-mm accuracy in both the RMS surface roughness and the size of topographic features. In conclusion, high-resolution SfM demonstrated a successful quantitative reconstruction of LER.},
DOI = {10.3390/en13153916}
}



@Article{smartcities3030039,
AUTHOR = {Su, Wen-Hao},
TITLE = {Advanced Machine Learning in Point Spectroscopy, RGB- and Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review},
JOURNAL = {Smart Cities},
VOLUME = {3},
YEAR = {2020},
NUMBER = {3},
PAGES = {767--792},
URL = {https://www.mdpi.com/2624-6511/3/3/39},
ISSN = {2624-6511},
ABSTRACT = {Crop productivity is readily reduced by competition from weeds. It is particularly important to control weeds early to prevent yield losses. Limited herbicide choices and increasing costs of weed management are threatening the profitability of crops. Smart agriculture can use intelligent technology to accurately measure the distribution of weeds in the field and perform weed control tasks in selected areas, which cannot only improve the effectiveness of pesticides, but also increase the economic benefits of agricultural products. The most important thing for an automatic system to remove weeds within crop rows is to utilize reliable sensing technology to achieve accurate differentiation of weeds and crops at specific locations in the field. In recent years, there have been many significant achievements involving the differentiation of crops and weeds. These studies are related to the development of rapid and non-destructive sensors, as well as the analysis methods for the data obtained. This paper presents a review of the use of three sensing methods including spectroscopy, color imaging, and hyperspectral imaging in the discrimination of crops and weeds. Several algorithms of machine learning have been employed for data analysis such as convolutional neural network (CNN), artificial neural network (ANN), and support vector machine (SVM). Successful applications include the weed detection in grain crops (such as maize, wheat, and soybean), vegetable crops (such as tomato, lettuce, and radish), and fiber crops (such as cotton) with unsupervised or supervised learning. This review gives a brief introduction into proposed sensing and machine learning methods, then provides an overview of instructive examples of these techniques for weed/crop discrimination. The discussion describes the recent progress made in the development of automated technology for accurate plant identification as well as the challenges and future prospects. It is believed that this review is of great significance to those who study automatic plant care in crops using intelligent technology.},
DOI = {10.3390/smartcities3030039}
}



@Article{app10165436,
AUTHOR = {Kim, Dong-Hyun and Go, Yong-Guk and Choi, Soo-Mi},
TITLE = {An Aerial Mixed-Reality Environment for First-Person-View Drone Flying},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/2076-3417/10/16/5436},
ISSN = {2076-3417},
ABSTRACT = {A drone be able to fly without colliding to preserve the surroundings and its own safety. In addition, it must also incorporate numerous features of interest for drone users. In this paper, an aerial mixed-reality environment for first-person-view drone flying is proposed to provide an immersive experience and a safe environment for drone users by creating additional virtual obstacles when flying a drone in an open area. The proposed system is effective in perceiving the depth of obstacles, and enables bidirectional interaction between real and virtual worlds using a drone equipped with a stereo camera based on human binocular vision. In addition, it synchronizes the parameters of the real and virtual cameras to effectively and naturally create virtual objects in a real space. Based on user studies that included both general and expert users, we confirm that the proposed system successfully creates a mixed-reality environment using a flying drone by quickly recognizing real objects and stably combining them with virtual objects.},
DOI = {10.3390/app10165436}
}



@Article{app10165461,
AUTHOR = {Blaya-Ros, Pedro José and Blanco, Víctor and Domingo, Rafael and Soto-Valles, Fulgencio and Torres-Sánchez, Roque},
TITLE = {Feasibility of Low-Cost Thermal Imaging for Monitoring Water Stress in Young and Mature Sweet Cherry Trees},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5461},
URL = {https://www.mdpi.com/2076-3417/10/16/5461},
ISSN = {2076-3417},
ABSTRACT = {Infrared thermography has been introduced as an affordable tool for plant water status monitoring, especially in regions where water availability is the main limiting factor in agricultural production. This paper outlines the potential applications of low-cost thermal imaging devices to evaluate the water status of young and mature sweet cherry trees (Prunus avium L.) submitted to water stress. Two treatments per plot were assayed: (i) a control treatment irrigated to ensure non-limiting soil water conditions; and (ii) a water-stress treatment. The seasonal evolution of the temperature of the canopy (Tc) and the difference between Tc and air temperature (&Delta;T) were compared and three thermal indices were calculated: crop water stress index (CWSI), degrees above control treatment (DAC) and degrees above non-water-stressed baseline (DANS). Midday stem water potential (&Psi;stem) was used as the reference indicator of water stress and linear relationships of Tc, &Delta;T, CWSI, DAC and DANS with &Psi;stem were discussed in order to assess their sensitivity to quantify water stress. CWSI and DANS exhibited strong relationships with &Psi;stem and two regression lines to young and mature trees were found. The promising results obtained highlight that using low-cost infrared thermal devices can be used to determine the plant water status in sweet cherry trees.},
DOI = {10.3390/app10165461}
}



@Article{s20164410,
AUTHOR = {Jamil, Faisal and Iqbal, Naeem and Ahmad, Shabir and Kim, Do-Hyeun},
TITLE = {Toward Accurate Position Estimation Using Learning to Prediction Algorithm in Indoor Navigation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4410},
URL = {https://www.mdpi.com/1424-8220/20/16/4410},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things is advancing, and the augmented role of smart navigation in automating processes is at its vanguard. Smart navigation and location tracking systems are finding increasing use in the area of the mission-critical indoor scenario, logistics, medicine, and security. A demanding emerging area is an Indoor Localization due to the increased fascination towards location-based services. Numerous inertial assessments unit-based indoor localization mechanisms have been suggested in this regard. However, these methods have many shortcomings pertaining to accuracy and consistency. In this study, we propose a novel position estimation system based on learning to the prediction model to address the above challenges. The designed system consists of two modules; learning to prediction module and position estimation using sensor fusion in an indoor environment. The prediction algorithm is attached to the learning module. Moreover, the learning module continuously controls, observes, and enhances the efficiency of the prediction algorithm by evaluating the output and taking into account the exogenous factors that may have an impact on its outcome. On top of that, we reckon a situation where the prediction algorithm can be applied to anticipate the accurate gyroscope and accelerometer reading from the noisy sensor readings. In the designed system, we consider a scenario where the learning module, based on Artificial Neural Network, and Kalman filter are used as a prediction algorithm to predict the actual accelerometer and gyroscope reading from the noisy sensor reading. Moreover, to acquire data, we use the next-generation inertial measurement unit, which contains a 3-axis accelerometer and gyroscope data. Finally, for the performance and accuracy of the proposed system, we carried out numbers of experiments, and we observed that the proposed Kalman filter with learning module performed better than the traditional Kalman filter algorithm in terms of root mean square error metric.},
DOI = {10.3390/s20164410}
}



@Article{rs12162564,
AUTHOR = {Alebele, Yeshanbele and Zhang, Xue and Wang, Wenhui and Yang, Gaoxiang and Yao, Xia and Zheng, Hengbiao and Zhu, Yan and Cao, Weixing and Cheng, Tao},
TITLE = {Estimation of Canopy Biomass Components in Paddy Rice from Combined Optical and SAR Data Using Multi-Target Gaussian Regressor Stacking},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2564},
URL = {https://www.mdpi.com/2072-4292/12/16/2564},
ISSN = {2072-4292},
ABSTRACT = {Crop biomass is a critical variable to make sound decisions about field crop monitoring activities (fertilizers and irrigation) and crop productivity forecasts. More importantly, crop biomass estimations by components are essential for crop growth monitoring as the yield formation of crops results from the accumulation and transportation of substances between different organs. Retrieval of crop biomass from synthetic aperture radar SAR or optical imagery is of paramount importance for in-season monitoring of crop growth. A combination of optical and SAR imagery can compensate for their limitations and has exhibited comparative advantages in biomass estimation. Notably, the joint estimations of biophysical parameters might be more accurate than that of an individual parameter. Previous studies have attempted to use satellite imagery to estimate aboveground biomass, but the estimation of biomass for individual organs remains a challenge. Multi-target Gaussian process regressor stacking (MGPRS), as a new machine learning method, can be suitably utilized to estimate biomass components jointly from satellite imagery data, as the model does not require a large amount of data for training and can be adjusted to the required degrees of relationship exhibited by the given data. Thus, the aim of this study was to estimate the biomass of individual organs by using MGPRS in conjunction with optical (Sentinel-2A) and SAR (Sentinel-1A) imagery. Two hybrid indices, SAR and optical multiplication vegetation index (SOMVI) and SAR and optical difference vegetation index (SODVI), have been constructed to examine their estimation performance. The hybrid vegetation indices were used as input for the MGPRS and single-target Gaussian process regression (SGPR). The accuracy of the estimation methods was analyzed by in situ measurements of aboveground biomass (AGB) and organ biomass conducted in 2018 and 2019 over the paddy rice fields of Xinghua in Jiangsu Province, China. The results showed that the combined indices (SOMVI and SODVI) performed better than those derived from either the optical or SAR data only. The best predictive accuracy was achieved by the MGPRS using SODVI as input (r2 = 0.84, RMSE = 0.4 kg/m2 for stem biomass; r2 = 0.87, RMSE = 0.16 kg/m2 for AGB). This was higher than using SOMVI as input for the MGPRS (r2 = 0.71, RMSE = 1.12 kg/m2 for stem biomass; r2 = 0.71, RMSE = 0.56 kg/m2 for AGB) or SGPR (r2 = 0.63, RMSE = 1.08 kg/m2 for stem biomass; r2 = 0.67, RMSE = 1.08 kg/m2 for AGB). Relatively, higher accuracy for leaf biomass was achieved using SOMVI (r2 = 0.83) than using SODVI (r2 = 0.73) as input for MGPRS. Our results demonstrate that the combined indices are effective by integrating SAR and optical imagery and MGPRS outperformed SGPR with the same input variable for estimating rice crop biomass. The presented workflow will improve the estimation of crops biomass components from satellite data for effective crop growth monitoring.},
DOI = {10.3390/rs12162564}
}



@Article{plants9081008,
AUTHOR = {Billet, Kévin and Malinowska, Magdalena Anna and Munsch, Thibaut and Unlubayir, Marianne and Adler, Sophie and Delanoue, Guillaume and Lanoue, Arnaud},
TITLE = {Semi-Targeted Metabolomics to Validate Biomarkers of Grape Downy Mildew Infection Under Field Conditions},
JOURNAL = {Plants},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1008},
URL = {https://www.mdpi.com/2223-7747/9/8/1008},
PubMedID = {32784974},
ISSN = {2223-7747},
ABSTRACT = {Grape downy mildew is a devastating disease worldwide and new molecular phenotyping tools are required to detect metabolic changes associated to plant disease symptoms. In this purpose, we used UPLC-DAD-MS-based semi-targeted metabolomics to screen downy mildew symptomatic leaves that expressed oil spots (6 dpi, days post-infection) and necrotic lesions (15 dpi) under natural infections in the field. Leaf extract analyses enabled the identification of 47 metabolites belonging to the primary metabolism including 6 amino acids and 1 organic acid, as well as an important diversity of specialized metabolites including 9 flavonols, 11 flavan-3-ols, 3 phenolic acids, and stilbenoids with various degree of polymerization (DP) including 4 stilbenoids DP1, 8 stilbenoids DP2, and 4 stilbenoids DP3. Principal component analysis (PCA) was applied as unsupervised multivariate statistical analysis method to reveal metabolic variables that were affected by the infection status. Univariate and multivariate statistics revealed 33 and 27 metabolites as relevant infection biomarkers at 6 and 15 dpi, respectively. Correlation-based networks highlighted a general decrease of flavonoid-related metabolites, whereas stilbenoid DP1 and DP2 concentrations increased upon downy mildew infection. Stilbenoids DP3 were identified only in necrotic lesions representing late biomarkers of downy mildew infection.},
DOI = {10.3390/plants9081008}
}



@Article{rs12162576,
AUTHOR = {de Bem, Pablo Pozzobon and de Carvalho Júnior, Osmar Abílio and de Carvalho, Osmar Luiz Ferreira and Gomes, Roberto Arnaldo Trancoso and Fontes Guimarães, Renato},
TITLE = {Performance Analysis of Deep Convolutional Autoencoders with Different Patch Sizes for Change Detection from Burnt Areas},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2576},
URL = {https://www.mdpi.com/2072-4292/12/16/2576},
ISSN = {2072-4292},
ABSTRACT = {Fire is one of the primary sources of damages to natural environments globally. Estimates show that approximately 4 million km2 of land burns yearly. Studies have shown that such estimates often underestimate the real extent of burnt land, which highlights the need to find better, state-of-the-art methods to detect and classify these areas. This study aimed to analyze the use of deep convolutional Autoencoders in the classification of burnt areas, considering different sample patch sizes. A simple Autoencoder and the U-Net and ResUnet architectures were evaluated. We collected Landsat 8 OLI+ data from three scenes in four consecutive dates to detect the changes specifically in the form of burnt land. The data were sampled according to four different sampling strategies to evaluate possible performance changes related to sampling window sizes. The training stage used two scenes, while the validation stage used the remaining scene. The ground truth change mask was created using the Normalized Burn Ratio (NBR) spectral index through a thresholding approach. The classifications were evaluated according to the F1 index, Kappa index, and mean Intersection over Union (mIoU) value. Results have shown that the U-Net and ResUnet architectures offered the best classifications with average F1, Kappa, and mIoU values of approximately 0.96, representing excellent classification results. We have also verified that a sampling window size of 256 by 256 pixels offered the best results.},
DOI = {10.3390/rs12162576}
}



@Article{agriculture10080348,
AUTHOR = {Wei, Marcelo Chan Fu and Molin, José Paulo},
TITLE = {Soybean Yield Estimation and Its Components: A Linear Regression Approach},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {348},
URL = {https://www.mdpi.com/2077-0472/10/8/348},
ISSN = {2077-0472},
ABSTRACT = {Soybean yield estimation is either based on yield monitors or agro-meteorological and satellite imagery data, but they present several limiting factors regarding on-farm decision level. Aware that machine learning approaches have been largely applied to estimate soybean yield and the availability of data regarding soybean yield and its components (number of grains (NG) and thousand grains weight (TGW)), there is an opportunity to study their relationships. The objective was to explore the relationships between soybean yield and its components, generate equations to estimate yield and evaluate its prediction accuracy. The training dataset was composed of soybean yield and its components&rsquo; data from 2010 to 2019. Linear regression models based on NG, TGW and yield were fitted on the training dataset and applied to a validation dataset composed of 58 on-field collected samples. It was found that globally TGW and NG presented weak (r = 0.50) and strong (r = 0.92) linear relationships with yield, respectively. In addition to that, applying the fitted models to the validation dataset, model based on NG presented the highest accuracy, coefficient of determination (R2) of 0.70, mean absolute error (MAE) of 639.99 kg ha&minus;1 and root mean squared error (RMSE) of 726.67 kg ha&minus;1.},
DOI = {10.3390/agriculture10080348}
}



@Article{rs12162578,
AUTHOR = {Li, Daoliang and Zhang, Pan and Chen, Tao and Qin, Wei},
TITLE = {Recent Development and Challenges in Spectroscopy and Machine Vision Technologies for Crop Nitrogen Diagnosis: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2578},
URL = {https://www.mdpi.com/2072-4292/12/16/2578},
ISSN = {2072-4292},
ABSTRACT = {Recent development of non-destructive optical techniques, such as spectroscopy and machine vision technologies, have laid a good foundation for real-time monitoring and precise management of crop N status. However, their advantages and disadvantages have not been systematically summarized and evaluated. Here, we reviewed the state-of-the-art of non-destructive optical methods for monitoring the N status of crops, and summarized their advantages and disadvantages. We mainly focused on the contribution of spectral and machine vision technology to the accurate diagnosis of crop N status from three aspects: system selection, data processing, and estimation methods. Finally, we discussed the opportunities and challenges of the application of these technologies, followed by recommendations for future work to address the challenges.},
DOI = {10.3390/rs12162578}
}



@Article{ijgi9080485,
AUTHOR = {Ding, Kaimeng and Liu, Yueming and Xu, Qin and Lu, Fuqiang},
TITLE = {A Subject-Sensitive Perceptual Hash Based on MUM-Net for the Integrity Authentication of High Resolution Remote Sensing Images},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {485},
URL = {https://www.mdpi.com/2220-9964/9/8/485},
ISSN = {2220-9964},
ABSTRACT = {Data security technology is of great significance to the application of high resolution remote sensing image (HRRS) images. As an important data security technology, perceptual hash overcomes the shortcomings of cryptographic hashing that is not robust and can achieve integrity authentication of HRRS images based on perceptual content. However, the existing perceptual hash does not take into account whether the user focuses on certain types of information of the HRRS image. In this paper, we introduce the concept of subject-sensitive perceptual hash, which can be seen as a special case of conventional perceptual hash, for the integrity authentication of HRRS image. To achieve subject-sensitive perceptual hash, we propose a new deep convolutional neural network architecture, named MUM-Net, for extracting robust features of HRRS images. MUM-Net is the core of perceptual hash algorithm, and it uses focal loss as the loss function to overcome the imbalance between the positive and negative samples in the training samples. The robust features extracted by MUM-Net are further compressed and encoded to obtain the perceptual hash sequence of HRRS image. Experiments show that our algorithm has higher tamper sensitivity to subject-related malicious tampering, and the robustness is improved by about 10% compared to the existing U-net-based algorithm; compared to other deep learning-based algorithms, this algorithm achieves a better balance between robustness and tampering sensitivity, and has better overall performance.},
DOI = {10.3390/ijgi9080485}
}



@Article{rs12162599,
AUTHOR = {Gonçalves, Gil and Andriolo, Umberto and Gonçalves, Luísa and Sobral, Paula and Bessa, Filipa},
TITLE = {Quantifying Marine Macro Litter Abundance on a Sandy Beach Using Unmanned Aerial Systems and Object-Oriented Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2599},
URL = {https://www.mdpi.com/2072-4292/12/16/2599},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial systems (UASs) have recently been proven to be valuable remote sensing tools for detecting marine macro litter (MML), with the potential of supporting pollution monitoring programs on coasts. Very low altitude images, acquired with a low-cost RGB camera onboard a UAS on a sandy beach, were used to characterize the abundance of stranded macro litter. We developed an object-oriented classification strategy for automatically identifying the marine macro litter items on a UAS-based orthomosaic. A comparison is presented among three automated object-oriented machine learning (OOML) techniques, namely random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN). Overall, the detection was satisfactory for the three techniques, with mean F-scores of 65% for KNN, 68% for SVM, and 72% for RF. A comparison with manual detection showed that the RF technique was the most accurate OOML macro litter detector, as it returned the best overall detection quality (F-score) with the lowest number of false positives. Because the number of tuning parameters varied among the three automated machine learning techniques and considering that the three generated abundance maps correlated similarly with the abundance map produced manually, the simplest KNN classifier was preferred to the more complex RF. This work contributes to advances in remote sensing marine litter surveys on coasts, optimizing the automated detection on UAS-derived orthomosaics. MML abundance maps, produced by UAS surveys, assist coastal managers and authorities through environmental pollution monitoring programs. In addition, they contribute to search and evaluation of the mitigation measures and improve clean-up operations on coastal environments.},
DOI = {10.3390/rs12162599}
}



@Article{s20164524,
AUTHOR = {Rojas-Perez, Leticia Oyuki and Martinez-Carranza, Jose},
TITLE = {DeepPilot: A CNN for Autonomous Drone Racing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4524},
URL = {https://www.mdpi.com/1424-8220/20/16/4524},
ISSN = {1424-8220},
ABSTRACT = {Autonomous Drone Racing (ADR) was first proposed in IROS 2016. It called for the development of an autonomous drone capable of beating a human in a drone race. After almost five years, several teams have proposed different solutions with a common pipeline: gate detection; drone localization; and stable flight control. Recently, Deep Learning (DL) has been used for gate detection and localization of the drone regarding the gate. However, recent competitions such as the Game of Drones, held at NeurIPS 2019, called for solutions where DL played a more significant role. Motivated by the latter, in this work, we propose a CNN approach called DeepPilot that takes camera images as input and predicts flight commands as output. These flight commands represent: the angular position of the drone&rsquo;s body frame in the roll and pitch angles, thus producing translation motion in those angles; rotational speed in the yaw angle; and vertical speed referred as altitude h. Values for these 4 flight commands, predicted by DeepPilot, are passed to the drone&rsquo;s inner controller, thus enabling the drone to navigate autonomously through the gates in the racetrack. For this, we assume that the next gate becomes visible immediately after the current gate has been crossed. We present evaluations in simulated racetrack environments where DeepPilot is run several times successfully to prove repeatability. In average, DeepPilot runs at 25 frames per second (fps). We also present a thorough evaluation of what we called a temporal approach, which consists of creating a mosaic image, with consecutive camera frames, that is passed as input to the DeepPilot. We argue that this helps to learn the drone&rsquo;s motion trend regarding the gate, thus acting as a local memory that leverages the prediction of the flight commands. Our results indicate that this purely DL-based artificial pilot is feasible to be used for the ADR challenge.},
DOI = {10.3390/s20164524}
}



@Article{rs12162610,
AUTHOR = {Viinikka, Arto and Hurskainen, Pekka and Keski-Saari, Sarita and Kivinen, Sonja and Tanhuanpää, Topi and Mäyrä, Janne and Poikolainen, Laura and Vihervaara, Petteri and Kumpula, Timo},
TITLE = {Detecting European Aspen (Populus tremula L.) in Boreal Forests Using Airborne Hyperspectral and Airborne Laser Scanning Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2610},
URL = {https://www.mdpi.com/2072-4292/12/16/2610},
ISSN = {2072-4292},
ABSTRACT = {Sustainable forest management increasingly highlights the maintenance of biological diversity and requires up-to-date information on the occurrence and distribution of key ecological features in forest environments. European aspen (Populus tremula L.) is one key feature in boreal forests contributing significantly to the biological diversity of boreal forest landscapes. However, due to their sparse and scattered occurrence in northern Europe, the explicit spatial data on aspen remain scarce and incomprehensive, which hampers biodiversity management and conservation efforts. Our objective was to study tree-level discrimination of aspen from other common species in northern boreal forests using airborne high-resolution hyperspectral and airborne laser scanning (ALS) data. The study contained multiple spatial analyses: First, we assessed the role of different spectral wavelengths (455&ndash;2500 nm), principal component analysis, and vegetation indices (VI) in tree species classification using two machine learning classifiers&mdash;support vector machine (SVM) and random forest (RF). Second, we tested the effect of feature selection for best classification accuracy achievable and third, we identified the most important spectral features to discriminate aspen from the other common tree species. SVM outperformed the RF model, resulting in the highest overall accuracy (OA) of 84% and Kappa value (0.74). The used feature set affected SVM performance little, but for RF, principal component analysis was the best. The most important common VI for deciduous trees contained Conifer Index (CI), Cellulose Absorption Index (CAI), Plant Stress Index 3 (PSI3), and Vogelmann Index 1 (VOG1), whereas Green Ratio (GR), Red Edge Inflection Point (REIP), and Red Well Position (RWP) were specific for aspen. Normalized Difference Red Edge Index (NDRE) and Modified Normalized Difference Index (MND705) were important for coniferous trees. The most important wavelengths for discriminating aspen from other species included reflectance bands of red edge range (724&ndash;727 nm) and shortwave infrared (1520&ndash;1564 nm and 1684&ndash;1706 nm). The highest classification accuracy of 92% (F1-score) for aspen was achieved using the SVM model with mean reflectance values combined with VI, which provides a possibility to produce a spatially explicit map of aspen occurrence that can contribute to biodiversity management and conservation efforts in boreal forests.},
DOI = {10.3390/rs12162610}
}



@Article{app10165608,
AUTHOR = {Yaghoubi, Ehsan and Khezeli, Farhad and Borza, Diana and Kumar, SV Aruna and Neves, João and Proença, Hugo},
TITLE = {Human Attribute Recognition— A Comprehensive Survey},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5608},
URL = {https://www.mdpi.com/2076-3417/10/16/5608},
ISSN = {2076-3417},
ABSTRACT = {Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR&rsquo;s main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.},
DOI = {10.3390/app10165608}
}



@Article{s20164546,
AUTHOR = {Zhao, Weiwei and Chu, Hairong and Miao, Xikui and Guo, Lihong and Shen, Honghai and Zhu, Chenhao and Zhang, Feng and Liang, Dongxin},
TITLE = {Research on the Multiagent Joint Proximal Policy Optimization Algorithm Controlling Cooperative Fixed-Wing UAV Obstacle Avoidance},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4546},
URL = {https://www.mdpi.com/1424-8220/20/16/4546},
ISSN = {1424-8220},
ABSTRACT = {Multiple unmanned aerial vehicle (UAV) collaboration has great potential. To increase the intelligence and environmental adaptability of multi-UAV control, we study the application of deep reinforcement learning algorithms in the field of multi-UAV cooperative control. Aiming at the problem of a non-stationary environment caused by the change of learning agent strategy in reinforcement learning in a multi-agent environment, the paper presents an improved multiagent reinforcement learning algorithm&mdash;the multiagent joint proximal policy optimization (MAJPPO) algorithm with the centralized learning and decentralized execution. This algorithm uses the moving window averaging method to make each agent obtain a centralized state value function, so that the agents can achieve better collaboration. The improved algorithm enhances the collaboration and increases the sum of reward values obtained by the multiagent system. To evaluate the performance of the algorithm, we use the MAJPPO algorithm to complete the task of multi-UAV formation and the crossing of multiple-obstacle environments. To simplify the control complexity of the UAV, we use the six-degree of freedom and 12-state equations of the dynamics model of the UAV with an attitude control loop. The experimental results show that the MAJPPO algorithm has better performance and better environmental adaptability.},
DOI = {10.3390/s20164546}
}



@Article{electronics9081306,
AUTHOR = {Ijiga, Owoicho E. and Malekian, Reza and Chude-Okonkwo, Uche A. K.},
TITLE = {Enabling Emergent Configurations in the Industrial Internet of Things for Oil and Gas Explorations: A Survey},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1306},
URL = {https://www.mdpi.com/2079-9292/9/8/1306},
ISSN = {2079-9292},
ABSTRACT = {Several heterogeneous, intelligent, and distributed devices can be connected to interact with one another over the Internet in what is termed internet of things (IoT). Also, the concept of IoT can be exploited in the industrial environment for enhancing the production of goods and services and for mitigating the risk of disaster occurrences. This application of IoT for enhancing industrial production is known as industrial IoT (IIoT). Emergent configuration (EC) is a technology that can be adopted to enhance the operation and collaboration of IoT connected devices in order to improve the efficiency of the connected IoT systems for maximum user satisfaction. To meet user goals, the connected devices are required to cooperate with one another in an adaptive, interoperable, and homogeneous manner. In this paper, a survey of the concept of IoT is presented in addition to a review of IIoT systems. The application of ubiquitous computing-aided software define networking (SDN)-based EC architecture is propounded for enhancing the throughput of oil and gas production in the maritime ecosystems by managing the exploration process especially in emergency situations that involve anthropogenic oil and gas spillages.},
DOI = {10.3390/electronics9081306}
}



@Article{rs12162624,
AUTHOR = {Ingman, Matias and Virtanen, Juho-Pekka and Vaaja, Matti T. and Hyyppä, Hannu},
TITLE = {A Comparison of Low-Cost Sensor Systems in Automatic Cloud-Based Indoor 3D Modeling},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2624},
URL = {https://www.mdpi.com/2072-4292/12/16/2624},
ISSN = {2072-4292},
ABSTRACT = {The automated 3D modeling of indoor spaces is a rapidly advancing field, in which recent developments have made the modeling process more accessible to consumers by lowering the cost of instruments and offering a highly automated service for 3D model creation. We compared the performance of three low-cost sensor systems; one RGB-D camera, one low-end terrestrial laser scanner (TLS), and one panoramic camera, using a cloud-based processing service to automatically create mesh models and point clouds, evaluating the accuracy of the results against a reference point cloud from a higher-end TLS. While adequately accurate results could be obtained with all three sensor systems, the TLS performed the best both in terms of reconstructing the overall room geometry and smaller details, with the panoramic camera clearly trailing the other systems and the RGB-D offering a middle ground in terms of both cost and quality. The results demonstrate the attractiveness of fully automatic cloud-based indoor 3D modeling for low-cost sensor systems, with the latter providing better model accuracy and completeness, and with all systems offering a rapid rate of data acquisition through an easy-to-use interface.},
DOI = {10.3390/rs12162624}
}



@Article{rs12162640,
AUTHOR = {Vlachopoulos, Odysseas and Leblon, Brigitte and Wang, Jinfei and Haddadi, Ataollah and LaRocque, Armand and Patterson, Greg},
TITLE = {Delineation of Crop Field Areas and Boundaries from UAS Imagery Using PBIA and GEOBIA with Random Forest Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2640},
URL = {https://www.mdpi.com/2072-4292/12/16/2640},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aircraft systems (UAS) have been proven cost- and time-effective remote-sensing platforms for precision agriculture applications. This study presents a method for automatic delineation of field areas and boundaries that uses UAS multispectral orthomosaics acquired over 7 vegetated fields having a variety of crops in Prince Edward Island (PEI). This information is needed by crop insurance agencies and growers for an accurate determination of crop insurance premiums. The field areas and boundaries were delineated by applying both a pixel-based and an object-based supervised random forest (RF) classifier applied to reflectance and vegetation index images, followed by a vectorization pipeline. Both methodologies performed exceptionally well, resulting in a mean area goodness of fit (AGoF) for the field areas greater than 98% and a mean boundary mean positional error (BMPE) lower than 0.8 m for the seven surveyed fields.},
DOI = {10.3390/rs12162640}
}



@Article{rs12162646,
AUTHOR = {Zhang, Shiyu and Zhuo, Li and Zhang, Hui and Li, Jiafeng},
TITLE = {Object Tracking in Unmanned Aerial Vehicle Videos via Multifeature Discrimination and Instance-Aware Attention Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2646},
URL = {https://www.mdpi.com/2072-4292/12/16/2646},
ISSN = {2072-4292},
ABSTRACT = {Visual object tracking in unmanned aerial vehicle (UAV) videos plays an important role in a variety of fields, such as traffic data collection, traffic monitoring, as well as film and television shooting. However, it is still challenging to track the target robustly in UAV vision task due to several factors such as appearance variation, background clutter, and severe occlusion. In this paper, we propose a novel two-stage UAV tracking framework, which includes a target detection stage based on multifeature discrimination and a bounding-box estimation stage based on the instance-aware attention network. In the target detection stage, we explore a feature representation scheme for a small target that integrates handcrafted features, low-level deep features, and high-level deep features. Then, the correlation filter is used to roughly predict target location. In the bounding-box estimation stage, an instance-aware intersection over union (IoU)-Net is integrated together with an instance-aware attention network to estimate the target size based on the bounding-box proposals generated in the target detection stage. Extensive experimental results on the UAV123 and UAVDT datasets show that our tracker, running at over 25 frames per second (FPS), has superior performance as compared with state-of-the-art UAV visual tracking approaches.},
DOI = {10.3390/rs12162646}
}



@Article{bdcc4030020,
AUTHOR = {Ciaburro, Giuseppe},
TITLE = {Sound Event Detection in Underground Parking Garage Using Convolutional Neural Network},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {4},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {20},
URL = {https://www.mdpi.com/2504-2289/4/3/20},
ISSN = {2504-2289},
ABSTRACT = {Parking is a crucial element in urban mobility management. The availability of parking areas makes it easier to use a service, determining its success. Proper parking management allows economic operators located nearby to increase their business revenue. Underground parking areas during off-peak hours are uncrowded places, where user safety is guaranteed by company overseers. Due to the large size, ensuring adequate surveillance would require many operators to increase the costs of parking fees. To reduce costs, video surveillance systems are used, in which an operator monitors many areas. However, some activities are beyond the control of this technology. In this work, a procedure to identify sound events in an underground garage is developed. The aim of the work is to detect sounds identifying dangerous situations and to activate an automatic alert that draws the attention of surveillance in that area. To do this, the sounds of a parking sector were detected with the use of sound sensors. These sounds were analyzed by a sound detector based on convolutional neural networks. The procedure returned high accuracy in identifying a car crash in an underground parking area.},
DOI = {10.3390/bdcc4030020}
}



@Article{s20164655,
AUTHOR = {Sun, Meiwei and Deng, Yingbin and Li, Miao and Jiang, Hao and Huang, Haoling and Liao, Wenyue and Liu, Yangxiaoyue and Yang, Ji and Li, Yong},
TITLE = {Extraction and Analysis of Blue Steel Roofs Information Based on CNN Using Gaofen-2 Imageries},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4655},
URL = {https://www.mdpi.com/1424-8220/20/16/4655},
ISSN = {1424-8220},
ABSTRACT = {Blue steel roof is advantageous for its low cost, durability, and ease of installation. It is generally used by industrial areas. The accurate and rapid mapping of blue steel roof is important for the preliminary assessment of inefficient industrial areas and is one of the key elements for quantifying environmental issues like urban heat islands. Here, the DeeplabV3+ semantic segmentation neural network based on GaoFen-2 images was used to analyze the quantity and spatial distribution of blue steel roofs in the Nanhai district, Foshan (including the towns of Shishan, Guicheng, Dali, and Lishui), which is the important manufacturing industry base of China. We found that: (1) the DeeplabV3+ performs well with an overall accuracy of 92%, higher than the maximum likelihood classification; (2) the distribution of blue steel roofs was not even across the whole study area, but they were evenly distributed within the town scale; and (3) strong positive correlation was observed between blue steel roofs area and industrial gross output. These results not only can be used to detect the inefficient industrial areas for regional planning but also provide fundamental data for studies of urban environmental issues.},
DOI = {10.3390/s20164655}
}



@Article{drones4030046,
AUTHOR = {Adamopoulos, Efstathios and Rinaudo, Fulvio},
TITLE = {UAS-Based Archaeological Remote Sensing: Review, Meta-Analysis and State-of-the-Art},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {46},
URL = {https://www.mdpi.com/2504-446X/4/3/46},
ISSN = {2504-446X},
ABSTRACT = {Over the last decade, we have witnessed momentous technological developments in unmanned aircraft systems (UAS) and in lightweight sensors operating at various wavelengths, at and beyond the visible spectrum, which can be integrated with unmanned aerial platforms. These innovations have made feasible close-range and high-resolution remote sensing for numerous archaeological applications, including documentation, prospection, and monitoring bridging the gap between satellite, high-altitude airborne, and terrestrial sensing of historical sites and landscapes. In this article, we track the progress made so far, by systematically reviewing the literature relevant to the combined use of UAS platforms with visible, infrared, multi-spectral, hyper-spectral, laser, and radar sensors to reveal archaeological features otherwise invisible to archaeologists with applied non-destructive techniques. We review, specific applications and their global distribution, as well as commonly used platforms, sensors, and data-processing workflows. Furthermore, we identify the contemporary state-of-the-art and discuss the challenges that have already been overcome, and those that have not, to propose suggestions for future research.},
DOI = {10.3390/drones4030046}
}



@Article{rs12172683,
AUTHOR = {Jimenez-Sierra, David Alejandro and Benítez-Restrepo, Hernán Darío and Vargas-Cardona, Hernán Darío and Chanussot, Jocelyn},
TITLE = {Graph-Based Data Fusion Applied to: Change Detection and Biomass Estimation in Rice Crops},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2683},
URL = {https://www.mdpi.com/2072-4292/12/17/2683},
ISSN = {2072-4292},
ABSTRACT = {The complementary nature of different modalities and multiple bands used in remote sensing data is helpful for tasks such as change detection and the prediction of agricultural variables. Nonetheless, correctly processing a multi-modal dataset is not a simple task, owing to the presence of different data resolutions and formats. In the past few years, graph-based methods have proven to be a useful tool in capturing inherent data similarity, in spite of different data formats, and preserving relevant topological and geometric information. In this paper, we propose a graph-based data fusion algorithm for remotely sensed images applied to (i) data-driven semi-unsupervised change detection and (ii) biomass estimation in rice crops. In order to detect the change, we evaluated the performance of four competing algorithms on fourteen datasets. To estimate biomass in rice crops, we compared our proposal in terms of root mean squared error (RMSE) concerning a recent approach based on vegetation indices as features. The results confirm that the proposed graph-based data fusion algorithm outperforms state-of-the-art methods for change detection and biomass estimation in rice crops.},
DOI = {10.3390/rs12172683}
}



@Article{s20174691,
AUTHOR = {Bisagno, Niccolò and Xamin, Alberto and De Natale, Francesco and Conci, Nicola and Rinner, Bernhard},
TITLE = {Dynamic Camera Reconfiguration with Reinforcement Learning and Stochastic Methods for Crowd Surveillance},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4691},
URL = {https://www.mdpi.com/1424-8220/20/17/4691},
ISSN = {1424-8220},
ABSTRACT = {Crowd surveillance plays a key role to ensure safety and security in public areas. Surveillance systems traditionally rely on fixed camera networks, which suffer from limitations, as coverage of the monitored area, video resolution and analytic performance. On the other hand, a smart camera network provides the ability to reconfigure the sensing infrastructure by incorporating active devices such as pan-tilt-zoom (PTZ) cameras and UAV-based cameras, thus enabling the network to adapt over time to changes in the scene. We propose a new decentralised approach for network reconfiguration, where each camera dynamically adapts its parameters and position to optimise scene coverage. Two policies for decentralised camera reconfiguration are presented: a greedy approach and a reinforcement learning approach. In both cases, cameras are able to locally control the state of their neighbourhood and dynamically adjust their position and PTZ parameters. When crowds are present, the network balances between global coverage of the entire scene and high resolution for the crowded areas. We evaluate our approach in a simulated environment monitored with fixed, PTZ and UAV-based cameras.},
DOI = {10.3390/s20174691}
}



@Article{app10175773,
AUTHOR = {Alparslan, Onur and Arakawa, Shin’ichi and Murata, Masayuki},
TITLE = {SDN-Based Control of IoT Network by Brain-Inspired Bayesian Attractor Model and Network Slicing},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {5773},
URL = {https://www.mdpi.com/2076-3417/10/17/5773},
ISSN = {2076-3417},
ABSTRACT = {One of the models in the literature for modeling the behavior of the brain is the Bayesian attractor model, which is a kind of machine-learning algorithm. According to this model, the brain assigns stochastic variables to possible decisions (attractors) and chooses one of them when enough evidence is collected from sensory systems to achieve a confidence level high enough to make a decision. In this paper, we introduce a software defined networking (SDN) application based on a brain-inspired Bayesian attractor model for identification of the current traffic pattern for the supervision and automation of Internet of things (IoT) networks that exhibit a limited number of traffic patterns. In a real SDN testbed, we demonstrate that our SDN application can identify the traffic patterns using a limited set of fluctuating network statistics of edge link utilization. Moreover, we show that our application can improve core link utilization and the power efficiency of IoT networks by immediately applying a pre-calculated network configuration optimized by traffic engineering with network slicing for the identified pattern.},
DOI = {10.3390/app10175773}
}



@Article{s20174709,
AUTHOR = {Wang, Bin and Gu, Yinjuan},
TITLE = {An Improved FBPN-Based Detection Network for Vehicles in Aerial Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4709},
URL = {https://www.mdpi.com/1424-8220/20/17/4709},
ISSN = {1424-8220},
ABSTRACT = {With the development of artificial intelligence and big data analytics, an increasing number of researchers have tried to use deep-learning technology to train neural networks and achieved great success in the field of vehicle detection. However, as a special domain of object detection, vehicle detection in aerial images still has made limited progress because of low resolution, complex backgrounds and rotating objects. In this paper, an improved feature-balanced pyramid network (FBPN) has been proposed to enhance the network&rsquo;s ability to detect small objects. By combining FBPN with modified faster region convolutional neural network (faster-RCNN), a vehicle detection framework for aerial images is proposed. The focal loss function is adopted in the proposed framework to reduce the imbalance between easy and hard samples. The experimental results based on the VEDIA, USCAS-AOD, and DOTA datasets show that the proposed framework outperforms other state-of-the-art vehicle detection algorithms for aerial images.},
DOI = {10.3390/s20174709}
}



@Article{s20174739,
AUTHOR = {Walker, Ory and Vanegas, Fernando and Gonzalez, Felipe},
TITLE = {A Framework for Multi-Agent UAV Exploration and Target-Finding in GPS-Denied and Partially Observable Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4739},
URL = {https://www.mdpi.com/1424-8220/20/17/4739},
ISSN = {1424-8220},
ABSTRACT = {The problem of multi-agent remote sensing for the purposes of finding survivors or surveying points of interest in GPS-denied and partially observable environments remains a challenge. This paper presents a framework for multi-agent target-finding using a combination of online POMDP based planning and Deep Reinforcement Learning based control. The framework is implemented considering planning and control as two separate problems. The planning problem is defined as a decentralised multi-agent graph search problem and is solved using a modern online POMDP solver. The control problem is defined as a local continuous-environment exploration problem and is solved using modern Deep Reinforcement Learning techniques. The proposed framework combines the solution to both of these problems and testing shows that it enables multiple agents to find a target within large, simulated test environments in the presence of unknown obstacles and obstructions. The proposed approach could also be extended or adapted to a number of time sensitive remote-sensing problems, from searching for multiple survivors during a disaster to surveying points of interest in a hazardous environment by adjusting the individual model definitions.},
DOI = {10.3390/s20174739}
}



@Article{rs12172729,
AUTHOR = {Yang, Jianxiu and Xie, Xuemei and Shi, Guangming and Yang, Wenzhe},
TITLE = {A Feature-Enhanced Anchor-Free Network for UAV Vehicle Detection},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2729},
URL = {https://www.mdpi.com/2072-4292/12/17/2729},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection based on unmanned aerial vehicle (UAV) images is a challenging task. One reason is that the objects are small size, low-resolution, and large scale variations, resulting in weak feature representation. Another reason is the imbalance between positive and negative examples. In this paper, we propose a novel architecture for UAV vehicle detection to solve above problems. In detail, we use anchor-free mechanism to eliminate predefined anchors, which can reduce complicated computation and relieve the imbalance between positive and negative samples. Meanwhile, to enhance the features for vehicles, we design a multi-scale semantic enhancement block (MSEB) and an effective 49-layer backbone which is based on the DetNet59. The proposed network offers appropriate receptive fields that match the small-sized vehicles, and involves precise localization information provided by the contexts with high resolution. The MSEB strengthens discriminative feature representation at various scales, without reducing the spatial resolution of prediction layers. Experiments show that the proposed method achieves the state-of-the-art performance. Particularly, the main part of vehicles, much smaller ones, the accuracy is about 2% higher than other existing methods.},
DOI = {10.3390/rs12172729}
}



@Article{rs12172732,
AUTHOR = {Abdulridha, Jaafar and Ampatzidis, Yiannis and Qureshi, Jawwad and Roberts, Pamela},
TITLE = {Laboratory and UAV-Based Identification and Classification of Tomato Yellow Leaf Curl, Bacterial Spot, and Target Spot Diseases in Tomato Utilizing Hyperspectral Imaging and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2732},
URL = {https://www.mdpi.com/2072-4292/12/17/2732},
ISSN = {2072-4292},
ABSTRACT = {Tomato crops are susceptible to multiple diseases, several of which may be present during the same season. Therefore, rapid disease identification could enhance crop management consequently increasing the yield. In this study, nondestructive methods were developed to detect diseases that affect tomato crops, such as bacterial spot (BS), target spot (TS), and tomato yellow leaf curl (TYLC) for two varieties of tomato (susceptible and tolerant to TYLC only) by using hyperspectral sensing in two conditions: a) laboratory (benchtop scanning), and b) in field using an unmanned aerial vehicle (UAV-based). The stepwise discriminant analysis (STDA) and the radial basis function were applied to classify the infected plants and distinguish them from noninfected or healthy (H) plants. Multiple vegetation indices (VIs) and the M statistic method were utilized to distinguish and classify the diseased plants. In general, the classification results between healthy and diseased plants were highly accurate for all diseases; for instance, when comparing H vs. BS, TS, and TYLC in the asymptomatic stage and laboratory conditions, the classification rates were 94%, 95%, and 100%, respectively. Similarly, in the symptomatic stage, the classification rates between healthy and infected plants were 98% for BS, and 99&ndash;100% for TS and TYLC diseases. The classification results in the field conditions also showed high values of 98%, 96%, and 100%, for BS, TS, and TYLC, respectively. The VIs that could best identify these diseases were the renormalized difference vegetation index (RDVI), and the modified triangular vegetation index 1 (MTVI 1) in both laboratory and field. The results were promising and suggest the possibility to identify these diseases using remote sensing.},
DOI = {10.3390/rs12172732}
}



@Article{insects11090565,
AUTHOR = {Zhang, Zhiliang and Zhan, Wei and He, Zhangzhang and Zou, Yafeng},
TITLE = {Application of Spatio-Temporal Context and Convolution Neural Network (CNN) in Grooming Behavior of Bactrocera minax (Diptera: Trypetidae) Detection and Statistics},
JOURNAL = {Insects},
VOLUME = {11},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {565},
URL = {https://www.mdpi.com/2075-4450/11/9/565},
PubMedID = {32846918},
ISSN = {2075-4450},
ABSTRACT = {Statistical analysis and research on insect grooming behavior can find more effective methods for pest control. Traditional manual insect grooming behavior statistical methods are time-consuming, labor-intensive, and error-prone. Based on computer vision technology, this paper uses spatio-temporal context to extract video features, uses self-built Convolution Neural Network (CNN) to train the detection model, and proposes a simple and effective Bactrocera minax grooming behavior detection method, which automatically detects the grooming behaviors of the flies and analysis results by a computer program. Applying the method training detection model proposed in this paper, the videos of 22 adult flies with a total of 1320 min of grooming behavior were detected and analyzed, and the total detection accuracy was over 95%, the standard error of the accuracy of the behavior detection of each adult flies was less than 3%, and the difference was less than 15% when compared with the results of manual observation. The experimental results show that the method in this paper greatly reduces the time of manual observation and at the same time ensures the accuracy of insect behavior detection and analysis, which proposes a new informatization analysis method for the behavior statistics of Bactrocera minax and also provides a new idea for related insect behavior identification research.},
DOI = {10.3390/insects11090565}
}



@Article{rs12172733,
AUTHOR = {Devadoss, Jashvina and Falco, Nicola and Dafflon, Baptiste and Wu, Yuxin and Franklin, Maya and Hermes, Anna and Hinckley, Eve-Lyn S. and Wainwright, Haruko},
TITLE = {Remote Sensing-Informed Zonation for Understanding Snow, Plant and Soil Moisture Dynamics within a Mountain Ecosystem},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2733},
URL = {https://www.mdpi.com/2072-4292/12/17/2733},
ISSN = {2072-4292},
ABSTRACT = {In the headwater catchments of the Rocky Mountains, plant productivity and its dynamics are largely dependent upon water availability, which is influenced by changing snowmelt dynamics associated with climate change. Understanding and quantifying the interactions between snow, plants and soil moisture is challenging, since these interactions are highly heterogeneous in mountainous terrain, particularly as they are influenced by microtopography within a hillslope. Recent advances in satellite remote sensing have created an opportunity for monitoring snow and plant dynamics at high spatiotemporal resolutions that can capture microtopographic effects. In this study, we investigate the relationships among topography, snowmelt, soil moisture and plant dynamics in the East River watershed, Crested Butte, Colorado, based on a time series of 3-meter resolution PlanetScope normalized difference vegetation index (NDVI) images. To make use of a large volume of high-resolution time-lapse images (17 images total), we use unsupervised machine learning methods to reduce the dimensionality of the time lapse images by identifying spatial zones that have characteristic NDVI time series. We hypothesize that each zone represents a set of similar snowmelt and plant dynamics that differ from other identified zones and that these zones are associated with key topographic features, plant species and soil moisture. We compare different distance measures (Ward and complete linkage) to understand the effects of their influence on the zonation map. Results show that the identified zones are associated with particular microtopographic features; highly productive zones are associated with low slopes and high topographic wetness index, in contrast with zones of low productivity, which are associated with high slopes and low topographic wetness index. The zones also correspond to particular plant species distributions; higher forb coverage is associated with zones characterized by higher peak productivity combined with rapid senescence in low moisture conditions, while higher sagebrush coverage is associated with low productivity and similar senescence patterns between high and low moisture conditions. In addition, soil moisture probe and sensor data confirm that each zone has a unique soil moisture distribution. This cluster-based analysis can tractably analyze high-resolution time-lapse images to examine plant-soil-snow interactions, guide sampling and sensor placements and identify areas likely vulnerable to ecological change in the future.},
DOI = {10.3390/rs12172733}
}



@Article{s20174807,
AUTHOR = {Zhang, Dawei and Zheng, Zhonglong and Wang, Tianxiang and He, Yiran},
TITLE = {HROM: Learning High-Resolution Representation and Object-Aware Masks for Visual Object Tracking},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4807},
URL = {https://www.mdpi.com/1424-8220/20/17/4807},
ISSN = {1424-8220},
ABSTRACT = {Siamese network-based trackers consider tracking as features cross-correlation between the target template and the search region. Therefore, feature representation plays an important role for constructing a high-performance tracker. However, all existing Siamese networks extract the deep but low-resolution features of the entire patch, which is not robust enough to estimate the target bounding box accurately. In this work, to address this issue, we propose a novel high-resolution Siamese network, which connects the high-to-low resolution convolution streams in parallel as well as repeatedly exchanges the information across resolutions to maintain high-resolution representations. The resulting representation is semantically richer and spatially more precise by a simple yet effective multi-scale feature fusion strategy. Moreover, we exploit attention mechanisms to learn object-aware masks for adaptive feature refinement, and use deformable convolution to handle complex geometric transformations. This makes the target more discriminative against distractors and background. Without bells and whistles, extensive experiments on popular tracking benchmarks containing OTB100, UAV123, VOT2018 and LaSOT demonstrate that the proposed tracker achieves state-of-the-art performance and runs in real time, confirming its efficiency and effectiveness.},
DOI = {10.3390/s20174807}
}



@Article{rs12172767,
AUTHOR = {Chen, Yu and Wei, Yongming and Wang, Qinjun and Chen, Fang and Lu, Chunyan and Lei, Shaohua},
TITLE = {Mapping Post-Earthquake Landslide Susceptibility: A U-Net Like Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2767},
URL = {https://www.mdpi.com/2072-4292/12/17/2767},
ISSN = {2072-4292},
ABSTRACT = {A serious earthquake could trigger thousands of landslides and produce some slopes more sensitive to slide in future. Landslides could threaten human&rsquo;s lives and properties, and thus mapping the post-earthquake landslide susceptibility is very valuable for a rapid response to landslide disasters in terms of relief resource allocation and posterior earthquake reconstruction. Previous researchers have proposed many methods to map landslide susceptibility but seldom considered the spatial structure information of the factors that influence a slide. In this study, we first developed a U-net like model suitable for mapping post-earthquake landslide susceptibility. The post-earthquake high spatial airborne images were used for producing a landslide inventory. Pre-earthquake Landsat TM (Thematic Mapper) images and the influencing factors such as digital elevation model (DEM), slope, aspect, multi-scale topographic position index (mTPI), lithology, fault, road network, streams network, and macroseismic intensity (MI) were prepared as the input layers of the model. Application of the model to the heavy-hit area of the destructive 2008 Wenchuan earthquake resulted in a high validation accuracy (precision 0.77, recall 0.90, F1 score 0.83, and AUC 0.90). The performance of this U-net like model was also compared with those of traditional logistic regression (LR) and support vector machine (SVM) models on both the model area and independent testing area with the former being stronger than the two traditional models. The U-net like model introduced in this paper provides us the inspiration that balancing the environmental influence of a pixel itself and its surrounding pixels to perform a better landslide susceptibility mapping (LSM) task is useful and feasible when using remote sensing and GIS technology.},
DOI = {10.3390/rs12172767}
}



@Article{sym12091424,
AUTHOR = {Alabdulwahab, Saleh and Moon, BongKyo},
TITLE = {Feature Selection Methods Simultaneously Improve the Detection Accuracy and Model Building Time of Machine Learning Classifiers},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1424},
URL = {https://www.mdpi.com/2073-8994/12/9/1424},
ISSN = {2073-8994},
ABSTRACT = {The detection accuracy and model building time of machine learning (ML) classifiers are vital aspects for an intrusion detection system (IDS) to predict attacks in real life. Recently, researchers have introduced feature selection methods to increase the detection accuracy and minimize the model building time of a limited number of ML classifiers. Therefore, identifying more ML classifiers with very high detection accuracy and the lowest possible model building time is necessary. In this study, the authors tested six supervised classifiers on a full NSL-KDD training dataset (a benchmark record for Internet traffic) using 10-fold cross-validation in the Weka tool with and without feature selection/reduction methods. The authors aimed to identify more options to outperform and secure classifiers with the highest detection accuracy and lowest model building time. The results show that the feature selection/reduction methods, including the wrapper method in combination with the discretize filter, the filter method in combination with the discretize filter, and the discretize filter, can significantly decrease model building time without compromising detection accuracy. The suggested ML algorithms and feature selection/reduction methods are automated pattern recognition approaches to detect network attacks, which are within the scope of the Symmetry journal.},
DOI = {10.3390/sym12091424}
}



@Article{math8091456,
AUTHOR = {Martinez-Soltero, Gabriel and Alanis, Alma Y. and Arana-Daniel, Nancy and Lopez-Franco, Carlos},
TITLE = {Semantic Segmentation for Aerial Mapping},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1456},
URL = {https://www.mdpi.com/2227-7390/8/9/1456},
ISSN = {2227-7390},
ABSTRACT = {Mobile robots commonly have to traverse rough terrains. One way to find the easiest traversable path is by determining the types of terrains in the environment. The result of this process can be used by the path planning algorithms to find the best traversable path. In this work, we present an approach for terrain classification from aerial images while using a Convolutional Neural Networks at the pixel level. The segmented images can be used in robot mapping and navigation tasks. The performance of two different Convolutional Neural Networks is analyzed in order to choose the best architecture.},
DOI = {10.3390/math8091456}
}



@Article{rs12172833,
AUTHOR = {Arabameri, Alireza and Asadi Nalivan, Omid and Chandra Pal, Subodh and Chakrabortty, Rabin and Saha, Asish and Lee, Saro and Pradhan, Biswajeet and Tien Bui, Dieu},
TITLE = {Novel Machine Learning Approaches for Modelling the Gully Erosion Susceptibility},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2833},
URL = {https://www.mdpi.com/2072-4292/12/17/2833},
ISSN = {2072-4292},
ABSTRACT = {The extreme form of land degradation caused by the formation of gullies is a major challenge for the sustainability of land resources. This problem is more vulnerable in the arid and semi-arid environment and associated damage to agriculture and allied economic activities. Appropriate modeling of such erosion is therefore needed with optimum accuracy for estimating vulnerable regions and taking appropriate initiatives. The Golestan Dam has faced an acute problem of gully erosion over the last decade and has adversely affected society. Here, the artificial neural network (ANN), general linear model (GLM), maximum entropy (MaxEnt), and support vector machine (SVM) machine learning algorithm with 90/10, 80/20, 70/30, 60/40, and 50/50 random partitioning of training and validation samples was selected purposively for estimating the gully erosion susceptibility. The main objective of this work was to predict the susceptible zone with the maximum possible accuracy. For this purpose, random partitioning approaches were implemented. For this purpose, 20 gully erosion conditioning factors were considered for predicting the susceptible areas by considering the multi-collinearity test. The variance inflation factor (VIF) and tolerance (TOL) limit were considered for multi-collinearity assessment for reducing the error of the models and increase the efficiency of the outcome. The ANN with 50/50 random partitioning of the sample is the most optimal model in this analysis. The area under curve (AUC) values of receiver operating characteristics (ROC) in ANN (50/50) for the training and validation data are 0.918 and 0.868, respectively. The importance of the causative factors was estimated with the help of the Jackknife test, which reveals that the most important factor is the topography position index (TPI). Apart from this, the prioritization of all predicted models was estimated taking into account the training and validation data set, which should help future researchers to select models from this perspective. This type of outcome should help planners and local stakeholders to implement appropriate land and water conservation measures.},
DOI = {10.3390/rs12172833}
}



@Article{electronics9091416,
AUTHOR = {Qamar, Faizan and Siddiqui, Maraj Uddin Ahmed and Hindia, MHD Nour and Hassan, Rosilah and Nguyen, Quang Ngoc},
TITLE = {Issues, Challenges, and Research Trends in Spectrum Management: A Comprehensive Overview and New Vision for Designing 6G Networks},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1416},
URL = {https://www.mdpi.com/2079-9292/9/9/1416},
ISSN = {2079-9292},
ABSTRACT = {With an extensive growth in user demand for high throughput, large capacity, and low latency, the ongoing deployment of Fifth-Generation (5G) systems is continuously exposing the inherent limitations of the system, as compared with its original premises. Such limitations are encouraging researchers worldwide to focus on next-generation 6G wireless systems, which are expected to address the constraints. To meet the above demands, future radio network architecture should be effectively designed to utilize its maximum radio spectrum capacity. It must simultaneously utilize various new techniques and technologies, such as Carrier Aggregation (CA), Cognitive Radio (CR), and small cell-based Heterogeneous Networks (HetNet), high-spectrum access (mmWave), and Massive Multiple-Input-Multiple-Output (M-MIMO), to achieve the desired results. However, the concurrent operations of these techniques in current 5G cellular networks create several spectrum management issues; thus, a comprehensive overview of these emerging technologies is presented in detail in this study. Then, the problems involved in the concurrent operations of various technologies for the spectrum management of the current 5G network are highlighted. The study aims to provide a detailed review of cooperative communication among all the techniques and potential problems associated with the spectrum management that has been addressed with the possible solutions proposed by the latest researches. Future research challenges are also discussed to highlight the necessary steps that can help achieve the desired objectives for designing 6G wireless networks.},
DOI = {10.3390/electronics9091416}
}



@Article{ijgi9090527,
AUTHOR = {Liu, Jiantao and Feng, Quanlong and Wang, Ying and Batsaikhan, Bayartungalag and Gong, Jianhua and Li, Yi and Liu, Chunting and Ma, Yin},
TITLE = {Urban Green Plastic Cover Mapping Based on VHR Remote Sensing Images and a Deep Semi-Supervised Learning Framework},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {527},
URL = {https://www.mdpi.com/2220-9964/9/9/527},
ISSN = {2220-9964},
ABSTRACT = {With the rapid process of both urban sprawl and urban renewal, large numbers of old buildings have been demolished in China, leading to wide spread construction sites, which could cause severe dust contamination. To alleviate the accompanied dust pollution, green plastic mulch has been widely used by local governments of China. Therefore, timely and accurate mapping of urban green plastic covered regions is of great significance to both urban environmental management and the understanding of urban growth status. However, the complex spatial patterns of the urban landscape make it challenging to accurately identify these areas of green plastic cover. To tackle this issue, we propose a deep semi-supervised learning framework for green plastic cover mapping using very high resolution (VHR) remote sensing imagery. Specifically, a multi-scale deformable convolution neural network (CNN) was exploited to learn representative and discriminative features under complex urban landscapes. Afterwards, a semi-supervised learning strategy was proposed to integrate the limited labeled data and massive unlabeled data for model co-training. Experimental results indicate that the proposed method could accurately identify green plastic-covered regions in Jinan with an overall accuracy (OA) of 91.63%. An ablation study indicated that, compared with supervised learning, the semi-supervised learning strategy in this study could increase the OA by 6.38%. Moreover, the multi-scale deformable CNN outperforms several classic CNN models in the computer vision field. The proposed method is the first attempt to map urban green plastic-covered regions based on deep learning, which could serve as a baseline and useful reference for future research.},
DOI = {10.3390/ijgi9090527}
}



@Article{s20174976,
AUTHOR = {Meng , Kaitao and Li , Deshi and He , Xiaofan and Liu , Mingliu and Song , Weitao},
TITLE = {Real-Time Compact Environment Representation for UAV Navigation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4976},
URL = {https://www.mdpi.com/1424-8220/20/17/4976},
ISSN = {1424-8220},
ABSTRACT = {Recently, unmanned aerial vehicles (UAVs) have attracted much attention due to their on-demand deployment, high mobility, and low cost. For UAVs navigating in an unknown environment, efficient environment representation is needed due to the storage limitation of the UAVs. Nonetheless, building an accurate and compact environment representation model is highly non-trivial because of the unknown shape of the obstacles and the time-consuming operations such as finding and eliminating the environmental details. To overcome these challenges, a novel vertical strip extraction algorithm is proposed to analyze the probability density function characteristics of the normalized disparity value and segment the obstacles through an adaptive size sliding window. In addition, a plane adjustment algorithm is proposed to represent the obstacle surfaces as polygonal prism profiles while minimizing the redundant obstacle information. By combining these two proposed algorithms, the depth sensor data can be converted into the multi-layer polygonal prism models in real time. Besides, a drone platform equipped with a depth sensor is developed to build the compact environment representation models in the real world. Experimental results demonstrate that the proposed scheme achieves better performance in terms of precision and storage as compared to the baseline.},
DOI = {10.3390/s20174976}
}



@Article{rs12172863,
AUTHOR = {Dang, L. Minh and Wang, Hanxiang and Li, Yanfen and Min, Kyungbok and Kwak, Jin Tae and Lee, O. New and Park, Hanyong and Moon, Hyeonjoon},
TITLE = {Fusarium Wilt of Radish Detection Using RGB and Near Infrared Images from Unmanned Aerial Vehicles},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2863},
URL = {https://www.mdpi.com/2072-4292/12/17/2863},
ISSN = {2072-4292},
ABSTRACT = {The radish is a delicious, healthy vegetable and an important ingredient to many side dishes and main recipes. However, climate change, pollinator decline, and especially Fusarium wilt cause a significant reduction in the cultivation area and the quality of the radish yield. Previous studies on plant disease identification have relied heavily on extracting features manually from images, which is time-consuming and inefficient. In addition to Red-Green-Blue (RGB) images, the development of near-infrared (NIR) sensors has enabled a more effective way to monitor the diseases and evaluate plant health based on multispectral imagery. Thus, this study compares two distinct approaches in detecting radish wilt using RGB images and NIR images taken by unmanned aerial vehicles (UAV). The main research contributions include (1) a high-resolution RGB and NIR radish field dataset captured by drone from low to high altitudes, which can serve several research purposes; (2) implementation of a superpixel segmentation method to segment captured radish field images into separated segments; (3) a customized deep learning-based radish identification framework for the extracted segmented images, which achieved remarkable performance in terms of accuracy and robustness with the highest accuracy of 96%; (4) the proposal for a disease severity analysis that can detect different stages of the wilt disease; (5) showing that the approach based on NIR images is more straightforward and effective in detecting wilt disease than the learning approach based on the RGB dataset.},
DOI = {10.3390/rs12172863}
}



@Article{rs12182866,
AUTHOR = {Ren, Yongfeng and Yu, Yongtao and Guan, Haiyan},
TITLE = {DA-CapsUNet: A Dual-Attention Capsule U-Net for Road Extraction from Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2866},
URL = {https://www.mdpi.com/2072-4292/12/18/2866},
ISSN = {2072-4292},
ABSTRACT = {The up-to-date and information-accurate road database plays a significant role in many applications. Recently, with the improvement in image resolutions and quality, remote sensing images have provided an important data source for road extraction tasks. However, due to the topology variations, spectral diversities, and complex scenarios, it is still challenging to realize fully automated and highly accurate road extractions from remote sensing images. This paper proposes a novel dual-attention capsule U-Net (DA-CapsUNet) for road region extraction by combining the advantageous properties of capsule representations and the powerful features of attention mechanisms. By constructing a capsule U-Net architecture, the DA-CapsUNet can extract and fuse multiscale capsule features to recover a high-resolution and semantically strong feature representation. By designing the multiscale context-augmentation and two types of feature attention modules, the DA-CapsUNet can exploit multiscale contextual properties at a high-resolution perspective and generate an informative and class-specific feature encoding. Quantitative evaluations on a large dataset showed that the DA-CapsUNet provides a competitive road extraction performance with a precision of 0.9523, a recall of 0.9486, and an F-score of 0.9504, respectively. Comparative studies with eight recently developed deep learning methods also confirmed the applicability and superiority or compatibility of the DA-CapsUNet in road extraction tasks.},
DOI = {10.3390/rs12182866}
}



@Article{app10186147,
AUTHOR = {Li, Jin and Yan, Daifu and Luan, Kuan and Li, Zeyu and Liang, Hong},
TITLE = {Deep Learning-Based Bird’s Nest Detection on Transmission Lines Using UAV Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6147},
URL = {https://www.mdpi.com/2076-3417/10/18/6147},
ISSN = {2076-3417},
ABSTRACT = {In order to ensure the safety of transmission lines, the use of unmanned aerial vehicle (UAV) images for automatic object detection has important application prospects, such as the detection of birds&rsquo; nests. The traditional bird&rsquo;s nest detection methods mainly include the study of morphological characteristics of the bird&rsquo;s nest. These methods have poor applicability and low accuracy. In this work, we propose a deep learning-based birds&rsquo; nests automatic detection framework&mdash;region of interest (ROI) mining faster region-based convolutional neural networks (RCNN). First, the prior dimensions of anchors are obtained by using k-means clustering to improve the accuracy of coordinate boxes generation. Second, in order to balance the number of foreground and background samples in the training process, the focal loss function is introduced in the region proposal network (RPN) classification stage. Finally, the ROI mining module is added to solve the class imbalance problem in the classification stage, combined with the characteristics of difficult-to-classify bird&rsquo;s nest samples in the UAV images. After parameter optimization and experimental verification, the deep learning-based bird&rsquo;s nest automatic detection framework proposed in this work achieves high detection accuracy. In addition, the mean average precision (mAP) and formula 1 (F1) score of the proposed method are higher than the original faster RCNN and cascade RCNN. Our comparative analysis verifies the effectiveness of the proposed method.},
DOI = {10.3390/app10186147}
}



@Article{app10186151,
AUTHOR = {Hung, Sheng-Chieh and Wu, Hui-Ching and Tseng, Ming-Hseng},
TITLE = {Remote Sensing Scene Classification and Explanation Using RSSCNet and LIME},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6151},
URL = {https://www.mdpi.com/2076-3417/10/18/6151},
ISSN = {2076-3417},
ABSTRACT = {Classification is needed in disaster investigation, traffic control, and land-use resource management. How to quickly and accurately classify such remote sensing imagery has become a popular research topic. However, the application of large, deep neural network models for the training of classifiers in the hope of obtaining good classification results is often very time-consuming. In this study, a new CNN (convolutional neutral networks) architecture, i.e., RSSCNet (remote sensing scene classification network), with high generalization capability was designed. Moreover, a two-stage cyclical learning rate policy and the no-freezing transfer learning method were developed to speed up model training and enhance accuracy. In addition, the manifold learning t-SNE (t-distributed stochastic neighbor embedding) algorithm was used to verify the effectiveness of the proposed model, and the LIME (local interpretable model, agnostic explanation) algorithm was applied to improve the results in cases where the model made wrong predictions. Comparing the results of three publicly available datasets in this study with those obtained in previous studies, the experimental results show that the model and method proposed in this paper can achieve better scene classification more quickly and more efficiently.},
DOI = {10.3390/app10186151}
}



@Article{rs12182873,
AUTHOR = {Farella, Elisa Mariarosaria and Torresani, Alessandro and Remondino, Fabio},
TITLE = {Refining the Joint 3D Processing of Terrestrial and UAV Images Using Quality Measures},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2873},
URL = {https://www.mdpi.com/2072-4292/12/18/2873},
ISSN = {2072-4292},
ABSTRACT = {The paper presents an efficient photogrammetric workflow to improve the 3D reconstruction of scenes surveyed by integrating terrestrial and Unmanned Aerial Vehicle (UAV) images. In the last years, the integration of this kind of images has shown clear advantages for the complete and detailed 3D representation of large and complex scenarios. Nevertheless, their photogrammetric integration often raises several issues in the image orientation and dense 3D reconstruction processes. Noisy and erroneous 3D reconstructions are the typical result of inaccurate orientation results. In this work, we propose an automatic filtering procedure which works at the sparse point cloud level and takes advantage of photogrammetric quality features. The filtering step removes low-quality 3D tie points before refining the image orientation in a new adjustment and generating the final dense point cloud. Our method generalizes to many datasets, as it employs statistical analyses of quality feature distributions to identify suitable filtering thresholds. Reported results show the effectiveness and reliability of the method verified using both internal and external quality checks, as well as visual qualitative comparisons. We made the filtering tool publicly available on GitHub.},
DOI = {10.3390/rs12182873}
}



@Article{s20185047,
AUTHOR = {Wei, Jiaqi and Shao, Shuai and Ma, Hui and Wang, Penghui and Zhang, Lei and Liu, Hongwei},
TITLE = {High-Resolution ISAR Imaging with Modified Joint Range Spatial-Variant Autofocus and Azimuth Scaling},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5047},
URL = {https://www.mdpi.com/1424-8220/20/18/5047},
ISSN = {1424-8220},
ABSTRACT = {Well-focused and accurately scaled high-resolution inverse synthetic aperture radar (ISAR) images provide a sound basis for feature extraction and target recognition. This paper proposes a novel high-resolution ISAR imaging algorithm, namely modified joint range spatial-variant autofocus and azimuth scaling algorithm (MJAAS). After motion compensation, the shift of the equivalent rotational center (ERC) of the target destroys the linear relationship between the azimuth chirp rates (ACR) of echo signals and the range coordinates of scattering points, thereby leading to the failure of azimuth scaling. Accordingly, a new joint equivalent rotational center position and effective rotational velocity (JERCP-ERV) signal model is established, serving as the basis of MJAAS. By recourse to the Davidon-Fletcher-Powell (DFP) algorithm, MJAAS can jointly estimate the ERCP and ERV by solving a minimum entropy optimization problem, so as to simultaneously achieve accurate azimuth scaling and range spatial-variant autofocus, which further improves the image focusing performance. MJAAS is not restricted by the modes of motion errors (coherent or non-coherent) and the motion compensation methods, so it can be widely applied to real data with the advantages of strong practicality and high accuracy. Extensive experimental results based on both simulated and real data are provided to corroborate the effectiveness of the proposed algorithm.},
DOI = {10.3390/s20185047}
}



@Article{s20185055,
AUTHOR = {Guo, Yahui and Wang, Hanxi and Wu, Zhaofei and Wang, Shuxin and Sun, Hongyong and Senthilnath, J. and Wang, Jingzhe and Robin Bryant, Christopher and Fu, Yongshuo},
TITLE = {Modified Red Blue Vegetation Index for Chlorophyll Estimation and Yield Prediction of Maize from Visible Images Captured by UAV},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5055},
URL = {https://www.mdpi.com/1424-8220/20/18/5055},
ISSN = {1424-8220},
ABSTRACT = {The vegetation index (VI) has been successfully used to monitor the growth and to predict the yield of agricultural crops. In this paper, a long-term observation was conducted for the yield prediction of maize using an unmanned aerial vehicle (UAV) and estimations of chlorophyll contents using SPAD-502. A new vegetation index termed as modified red blue VI (MRBVI) was developed to monitor the growth and to predict the yields of maize by establishing relationships between MRBVI- and SPAD-502-based chlorophyll contents. The coefficients of determination (R2s) were 0.462 and 0.570 in chlorophyll contents&rsquo; estimations and yield predictions using MRBVI, and the results were relatively better than the results from the seven other commonly used VI approaches. All VIs during the different growth stages of maize were calculated and compared with the measured values of chlorophyll contents directly, and the relative error (RE) of MRBVI is the lowest at 0.355. Further, machine learning (ML) methods such as the backpropagation neural network model (BP), support vector machine (SVM), random forest (RF), and extreme learning machine (ELM) were adopted for predicting the yields of maize. All VIs calculated for each image captured during important phenological stages of maize were set as independent variables and the corresponding yields of each plot were defined as dependent variables. The ML models used the leave one out method (LOO), where the root mean square errors (RMSEs) were 2.157, 1.099, 1.146, and 1.698 (g/hundred grain weight) for BP, SVM, RF, and ELM. The mean absolute errors (MAEs) were 1.739, 0.886, 0.925, and 1.356 (g/hundred grain weight) for BP, SVM, RF, and ELM, respectively. Thus, the SVM method performed better in predicting the yields of maize than the other ML methods. Therefore, it is strongly suggested that the MRBVI calculated from images acquired at different growth stages integrated with advanced ML methods should be used for agricultural- and ecological-related chlorophyll estimation and yield predictions.},
DOI = {10.3390/s20185055}
}



@Article{electronics9091459,
AUTHOR = {Kundid Vasić, Mirela and Papić, Vladan},
TITLE = {Multimodel Deep Learning for Person Detection in Aerial Images},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1459},
URL = {https://www.mdpi.com/2079-9292/9/9/1459},
ISSN = {2079-9292},
ABSTRACT = {In this paper, we propose a novel method for person detection in aerial images of nonurban terrain gathered by an Unmanned Aerial Vehicle (UAV), which plays an important role in Search And Rescue (SAR) missions. The UAV in SAR operations contributes significantly due to the ability to survey a larger geographical area from an aerial viewpoint. Because of the high altitude of recording, the object of interest (person) covers a small part of an image (around 0.1%), which makes this task quite challenging. To address this problem, a multimodel deep learning approach is proposed. The solution consists of two different convolutional neural networks in region proposal, as well as in the classification stage. Additionally, contextual information is used in the classification stage in order to improve the detection results. Experimental results tested on the HERIDAL dataset achieved precision of 68.89% and a recall of 94.65%, which is better than current state-of-the-art methods used for person detection in similar scenarios. Consequently, it may be concluded that this approach is suitable for usage as an auxiliary method in real SAR operations.},
DOI = {10.3390/electronics9091459}
}



@Article{s20185073,
AUTHOR = {Khan, Khalil and Albattah, Waleed and Khan, Rehan Ullah and Qamar, Ali Mustafa and Nayab, Durre},
TITLE = {Advances and Trends in Real Time Visual Crowd Analysis},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5073},
URL = {https://www.mdpi.com/1424-8220/20/18/5073},
ISSN = {1424-8220},
ABSTRACT = {Real time crowd analysis represents an active area of research within the computer vision community in general and scene analysis in particular. Over the last 10 years, various methods for crowd management in real time scenario have received immense attention due to large scale applications in people counting, public events management, disaster management, safety monitoring an so on. Although many sophisticated algorithms have been developed to address the task; crowd management in real time conditions is still a challenging problem being completely solved, particularly in wild and unconstrained conditions. In the proposed paper, we present a detailed review of crowd analysis and management, focusing on state-of-the-art methods for both controlled and unconstrained conditions. The paper illustrates both the advantages and disadvantages of state-of-the-art methods. The methods presented comprise the seminal research works on crowd management, and monitoring and then culminating state-of-the-art methods of the newly introduced deep learning methods. Comparison of the previous methods is presented, with a detailed discussion of the direction for future research work. We believe this review article will contribute to various application domains and will also augment the knowledge of the crowd analysis within the research community.},
DOI = {10.3390/s20185073}
}



@Article{app10186210,
AUTHOR = {Zheng, Ruihao and Xiong, Chen and Deng, Xiangbin and Li, Qiangsheng and Li, Yi},
TITLE = {Assessment of Earthquake Destructive Power to Structures Based on Machine Learning Methods},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6210},
URL = {https://www.mdpi.com/2076-3417/10/18/6210},
ISSN = {2076-3417},
ABSTRACT = {This study presents a machine learning-based method for the destructive power assessment of earthquake to structures. First, the analysis procedure of the method is presented, and the backpropagation neural network (BPNN) and convolutional neural network (CNN) are used as the machine learning algorithms. Second, the optimized BPNN architecture is obtained by discussing the influence of a different number of hidden layers and nodes. Third, the CNN architecture is proposed based on several classical deep learning networks. To build the machine learning models, 50,570 time-history analysis results of a structural system subjected to different ground motions are used as training, validation, and test samples. The results of the BPNN indicate that the features extraction method based on the short-time Fourier transform (STFT) can well reflect the frequency-/time-domain characteristics of ground motions. The results of the CNN indicate that the CNN exhibits better accuracy (R2 = 0.8737) compared with that of the BPNN (R2 = 0.6784). Furthermore, the CNN model exhibits remarkable computational efficiency, the prediction of 1000 structures based on the CNN model takes 0.762 s, while 507.81 s are required for the conventional time-history analysis (THA)-based simulation. Feature visualization of different layers of the CNN reveals that the shallow to deep layers of the CNN can extract the high to low-frequency features of ground motions. The proposed method can assist in the fast prediction of engineering demand parameters of large-number structures, which facilitates the damage or loss assessments of regional structures for timely emergency response and disaster relief after earthquake.},
DOI = {10.3390/app10186210}
}



@Article{rs12182908,
AUTHOR = {Hugon, Florèn and Liquet, Benoit and D’Amico, Frank},
TITLE = {Multi-Site and Multi-Year Remote Records of Operative Temperatures with Biomimetic Loggers Reveal Spatio-Temporal Variability in Mountain Lizard Activity and Persistence Proxy Estimates},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2908},
URL = {https://www.mdpi.com/2072-4292/12/18/2908},
ISSN = {2072-4292},
ABSTRACT = {Commonly, when studies deal with the effects of climate change on biodiversity, mean value is used more than other parameters. However, climate change also leads to greater temperature variability, and many papers have demonstrated its importance in the implementation of biodiversity response strategies. We studied the spatio-temporal variability of activity time and persistence index, calculated from operative temperatures measured at three sites over three years, for a mountain endemic species. Temperatures were recorded with biomimetic loggers, an original remote sensing technology, which has the same advantages as these tools but is suitable for recording biological organisms data. Among the 42 tests conducted, 71% were significant for spatial variability and 28% for temporal variability. The differences in daily activity times and in persistence indices demonstrated the effects of the micro-habitat, habitat, slope, altitude, hydrography, and year. These observations have highlighted the great variability existence in the environmental temperatures experienced by lizard populations. Thus, our study underlines the importance to implement multi-year and multi-site studies to quantify the variability and produce more representative results. These studies can be facilitated by the use of biomimetic loggers, for which a user guide is provided in the last part of this paper.},
DOI = {10.3390/rs12182908}
}



@Article{s20185130,
AUTHOR = {Guo, Yahui and Yin, Guodong and Sun, Hongyong and Wang, Hanxi and Chen, Shouzhi and Senthilnath, J. and Wang, Jingzhe and Fu, Yongshuo},
TITLE = {Scaling Effects on Chlorophyll Content Estimations with RGB Camera Mounted on a UAV Platform Using Machine-Learning Methods},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5130},
URL = {https://www.mdpi.com/1424-8220/20/18/5130},
ISSN = {1424-8220},
ABSTRACT = {Timely monitoring and precise estimation of the leaf chlorophyll contents of maize are crucial for agricultural practices. The scale effects are very important as the calculated vegetation index (VI) were crucial for the quantitative remote sensing. In this study, the scale effects were investigated by analyzing the linear relationships between VI calculated from red&ndash;green&ndash;blue (RGB) images from unmanned aerial vehicles (UAV) and ground leaf chlorophyll contents of maize measured using SPAD-502. The scale impacts were assessed by applying different flight altitudes and the highest coefficient of determination (R2) can reach 0.85. We found that the VI from images acquired from flight altitude of 50 m was better to estimate the leaf chlorophyll contents using the DJI UAV platform with this specific camera (5472 &times; 3648 pixels). Moreover, three machine-learning (ML) methods including backpropagation neural network (BP), support vector machine (SVM), and random forest (RF) were applied for the grid-based chlorophyll content estimation based on the common VI. The average values of the root mean square error (RMSE) of chlorophyll content estimations using ML methods were 3.85, 3.11, and 2.90 for BP, SVM, and RF, respectively. Similarly, the mean absolute error (MAE) were 2.947, 2.460, and 2.389, for BP, SVM, and RF, respectively. Thus, the ML methods had relative high precision in chlorophyll content estimations using VI; in particular, the RF performed better than BP and SVM. Our findings suggest that the integrated ML methods with RGB images of this camera acquired at a flight altitude of 50 m (spatial resolution 0.018 m) can be perfectly applied for estimations of leaf chlorophyll content in agriculture.},
DOI = {10.3390/s20185130}
}



@Article{s20185134,
AUTHOR = {Tan, Ziyi and Yang, Xu and Pang, Mingzhi and Gao, Shouwan and Li, Ming and Chen, Pengpeng},
TITLE = {UAV-Assisted Low-Consumption Time Synchronization Utilizing Cross-Technology Communication},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5134},
URL = {https://www.mdpi.com/1424-8220/20/18/5134},
ISSN = {1424-8220},
ABSTRACT = {Wireless sensor networks (WSNs) have been used in many fields due to its wide applicability. In this kind of network, each node is independent of each other and has its own local clock and communicates wirelessly. Time synchronization plays a vital role in WSNs and it can ensure accuracy requirements for coordination and data reliability. However, two key challenges exist in large-scale WSNs that are severe resource constraints overhead and multihop time synchronization errors. To address these issues, this paper proposes a novel unmanned aerial vehicle (UAV)-assisted low-consumption time synchronization algorithm based on cross-technology communication (CTC) for a large-scale WSN. This algorithm uses a UAV to send time synchronization data packets for calibration. Moreover, to ensure coverage and a high success rate for UAV data transmission, we use CTC for time synchronization. Without any relays, a high-power time synchronization packet can be sent by a UAV to achieve the time synchronization of low-power sensors. This algorithm can achieve accurate time synchronization with almost zero energy consumption for the sensor nodes. Finally, we implemented our algorithm with 30 low-power RF-CC2430 ZigBee nodes and a Da Jiang Innovations (DJI) M100 UAV on a 1 km highway and an indoor site. The results show that time synchronization can be achieved accurately with almost zero energy consumption for the sensor nodes, and the time synchronization error is less than 30 &mu;s in 99% of cases.},
DOI = {10.3390/s20185134}
}



@Article{pr8091123,
AUTHOR = {Park, You-Jin and Fan, Shu-Kai S. and Hsu, Chia-Yu},
TITLE = {A Review on Fault Detection and Process Diagnostics in Industrial Processes},
JOURNAL = {Processes},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1123},
URL = {https://www.mdpi.com/2227-9717/8/9/1123},
ISSN = {2227-9717},
ABSTRACT = {The main roles of fault detection and diagnosis (FDD) for industrial processes are to make an effective indicator which can identify faulty status of a process and then to take a proper action against a future failure or unfavorable accidents. In order to enhance many process performances (e.g., quality and throughput), FDD has attracted great attention from various industrial sectors. Many traditional FDD techniques have been developed for checking the existence of a trend or pattern in the process or whether a certain process variable behaves normally or not. However, they might fail to produce several hidden characteristics of the process or fail to discover the faults in processes due to underlying process dynamics. In this paper, we present current research and developments of FDD approaches for process monitoring as well as a broad literature review of many useful FDD approaches.},
DOI = {10.3390/pr8091123}
}



@Article{electronics9091475,
AUTHOR = {Jung, Soyi and Kim, Joongheon and Kim, Jae-Hyun},
TITLE = {Joint Message-Passing and Convex Optimization Framework for Energy-Efficient Surveillance UAV Scheduling},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1475},
URL = {https://www.mdpi.com/2079-9292/9/9/1475},
ISSN = {2079-9292},
ABSTRACT = {In modern surveillance systems, the use of unmanned aerial vehicles (UAVs) has been actively discussed in order to extend target monitoring areas, even for an extreme circumstances. This paper proposes an energy-efficient UAV-based surveillance system that operates from two different sequential methods. First, the proposed algorithm pursues energy-efficient operations by deactivating selected surveillance cameras on the UAVs located in overlapping areas. For this objective, a message-passing based algorithm is used because the overlapping situations can be formulated using a max-weight independent set. Next, the unscheduled UAVs based on the message-passing fly to the charging towers to be charged. This algorithm computes the optimal matching between the UAVs and charging towers and the amount of energy allocation for the scheduled UAV-tower pairs. This joint optimization is initially formulated as non-convex, and it is then reformulated to be convex, which can guarantee optimal solutions. The proposed framework achieves the desired performance, as presented in the performance evaluation.},
DOI = {10.3390/electronics9091475}
}



@Article{jimaging6090094,
AUTHOR = {Trujillo-Jiménez, Magda Alexandra and Navarro, Pablo and Pazos, Bruno and Morales, Leonardo and Ramallo, Virginia and Paschetta, Carolina and De Azevedo, Soledad and Ruderman, Anahí and Pérez, Orlando and Delrieux, Claudio and Gonzalez-José, Rolando},
TITLE = {body2vec: 3D Point Cloud Reconstruction for Precise Anthropometry with Handheld Devices},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {94},
URL = {https://www.mdpi.com/2313-433X/6/9/94},
ISSN = {2313-433X},
ABSTRACT = {Current point cloud extraction methods based on photogrammetry generate large amounts of spurious detections that hamper useful 3D mesh reconstructions or, even worse, the possibility of adequate measurements. Moreover, noise removal methods for point clouds are complex, slow and incapable to cope with semantic noise. In this work, we present body2vec, a model-based body segmentation tool that uses a specifically trained Neural Network architecture. Body2vec is capable to perform human body point cloud reconstruction from videos taken on hand-held devices (smartphones or tablets), achieving high quality anthropometric measurements. The main contribution of the proposed workflow is to perform a background removal step, thus avoiding the spurious points generation that is usual in photogrammetric reconstruction. A group of 60 persons were taped with a smartphone, and the corresponding point clouds were obtained automatically with standard photogrammetric methods. We used as a 3D silver standard the clean meshes obtained at the same time with LiDAR sensors post-processed and noise-filtered by expert anthropological biologists. Finally, we used as gold standard anthropometric measurements of the waist and hip of the same people, taken by expert anthropometrists. Applying our method to the raw videos significantly enhanced the quality of the results of the point cloud as compared with the LiDAR-based mesh, and of the anthropometric measurements as compared with the actual hip and waist perimeter measured by the anthropometrists. In both contexts, the resulting quality of body2vec is equivalent to the LiDAR reconstruction.},
DOI = {10.3390/jimaging6090094}
}



@Article{rs12182977,
AUTHOR = {Sapkota, Bishwa and Singh, Vijay and Neely, Clark and Rajan, Nithya and Bagavathiannan, Muthukumar},
TITLE = {Detection of Italian Ryegrass in Wheat and Prediction of Competitive Interactions Using Remote-Sensing and Machine-Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2977},
URL = {https://www.mdpi.com/2072-4292/12/18/2977},
ISSN = {2072-4292},
ABSTRACT = {Italian ryegrass (Lolium perenne ssp. multiflorum (Lam) Husnot) is a troublesome weed species in wheat (Triticum aestivum) production in the United States, severely affecting grain yields. Spatial mapping of ryegrass infestation in wheat fields and early prediction of its impact on yield can assist management decision making. In this study, unmanned aerial systems (UAS)-based red, green and blue (RGB) imageries acquired at an early wheat growth stage in two different experimental sites were used for developing predictive models. Deep neural networks (DNNs) coupled with an extensive feature selection method were used to detect ryegrass in wheat and estimate ryegrass canopy coverage. Predictive models were developed by regressing early-season ryegrass canopy coverage (%) with end-of-season (at wheat maturity) biomass and seed yield of ryegrass, as well as biomass and grain yield reduction (%) of wheat. Italian ryegrass was detected with high accuracy (precision = 95.44 &plusmn; 4.27%, recall = 95.48 &plusmn; 5.05%, F-score = 95.56 &plusmn; 4.11%) using the best model which included four features: hue, saturation, excess green index, and visible atmospheric resistant index. End-of-season ryegrass biomass was predicted with high accuracy (R2 = 0.87), whereas the other variables had moderate to high accuracy levels (R2 values of 0.74 for ryegrass seed yield, 0.73 for wheat biomass reduction, and 0.69 for wheat grain yield reduction). The methodology demonstrated in the current study shows great potential for mapping and quantifying ryegrass infestation and predicting its competitive response in wheat, allowing for timely management decisions.},
DOI = {10.3390/rs12182977}
}



@Article{rs12182981,
AUTHOR = {Oh, Sungchan and Chang, Anjin and Ashapure, Akash and Jung, Jinha and Dube, Nothabo and Maeda, Murilo and Gonzalez, Daniel and Landivar, Juan},
TITLE = {Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2981},
URL = {https://www.mdpi.com/2072-4292/12/18/2981},
ISSN = {2072-4292},
ABSTRACT = {Assessing plant population of cotton is important to make replanting decisions in low plant density areas, prone to yielding penalties. Since the measurement of plant population in the field is labor intensive and subject to error, in this study, a new approach of image-based plant counting is proposed, using unmanned aircraft systems (UAS; DJI Mavic 2 Pro, Shenzhen, China) data. The previously developed image-based techniques required a priori information of geometry or statistical characteristics of plant canopy features, while also limiting the versatility of the methods in variable field conditions. In this regard, a deep learning-based plant counting algorithm was proposed to reduce the number of input variables, and to remove requirements for acquiring geometric or statistical information. The object detection model named You Only Look Once version 3 (YOLOv3) and photogrammetry were utilized to separate, locate, and count cotton plants in the seedling stage. The proposed algorithm was tested with four different UAS datasets, containing variability in plant size, overall illumination, and background brightness. Root mean square error (RMSE) and R2 values of the optimal plant count results ranged from 0.50 to 0.60 plants per linear meter of row (number of plants within 1 m distance along the planting row direction) and 0.96 to 0.97, respectively. The object detection algorithm, trained with variable plant size, ground wetness, and lighting conditions generally resulted in a lower detection error, unless an observable difference of developmental stages of cotton existed. The proposed plant counting algorithm performed well with 0&ndash;14 plants per linear meter of row, when cotton plants are generally separable in the seedling stage. This study is expected to provide an automated methodology for in situ evaluation of plant emergence using UAS data.},
DOI = {10.3390/rs12182981}
}



@Article{info11090442,
AUTHOR = {Almeshal, Abdullah M. and Alenezi, Mohammad R. and Alshatti, Abdullah K.},
TITLE = {Accuracy Assessment of Small Unmanned Aerial Vehicle for Traffic Accident Photogrammetry in the Extreme Operating Conditions of Kuwait},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {442},
URL = {https://www.mdpi.com/2078-2489/11/9/442},
ISSN = {2078-2489},
ABSTRACT = {This study presents the first accuracy assessment of a low cost small unmanned aerial vehicle (sUAV) in reconstructing three dimensional (3D) models of traffic accidents at extreme operating environments. To date, previous studies have focused on the feasibility of adopting sUAVs in traffic accidents photogrammetry applications as well as the accuracy at normal operating conditions. In this study, 3D models of simulated accident scenes were reconstructed using a low-cost sUAV and cloud-based photogrammetry platform. Several experiments were carried out to evaluate the measurements accuracy at different flight altitudes during high temperature, low light, scattered rain and dusty high wind environments. Quantitative analyses are presented to highlight the precision range of the reconstructed traffic accident 3D model. Reported results range from highly accurate to fairly accurate represented by the root mean squared error (RMSE) range between 0.97 and 4.66 and a mean percentage absolute error (MAPE) between 1.03% and 20.2% at normal and extreme operating conditions, respectively. The findings offer an insight into the robustness and generalizability of UAV-based photogrammetry method for traffic accidents at extreme environments.},
DOI = {10.3390/info11090442}
}



@Article{rs12182982,
AUTHOR = {Gée, Christelle and Denimal, Emmanuel},
TITLE = {RGB Image-Derived Indicators for Spatial Assessment of the Impact of Broadleaf Weeds on Wheat Biomass},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2982},
URL = {https://www.mdpi.com/2072-4292/12/18/2982},
ISSN = {2072-4292},
ABSTRACT = {In precision agriculture, the development of proximal imaging systems embedded in autonomous vehicles allows to explore new weed management strategies for site-specific plant application. Accurate monitoring of weeds while controlling wheat growth requires indirect measurements of leaf area index (LAI) and above-ground dry matter biomass (BM) at early growth stages. This article explores the potential of RGB images to assess crop-weed competition in a wheat (Triticum aestivum L.) crop by generating two new indicators, the weed pressure (WP) and the local wheat biomass production (&delta;BMc). The fractional vegetation cover (FVC) of the crop and the weeds was automatically determined from the images with a SVM-RBF classifier, using bag of visual word vectors as inputs. It is based on a new vegetation index called MetaIndex, defined as a vote of six indices widely used in the literature. Beyond a simple map of weed infestation, the map of WP describes the crop-weed competition. The map of &delta;BMc, meanwhile, evaluates the local wheat above-ground biomass production and informs us about a potential stress. It is generated from the wheat FVC because it is highly correlated with LAI (r2 = 0.99) and BM (r2 = 0.93) obtained by destructive methods. By combining these two indicators, we aim at determining whether the origin of the wheat stress is due to weeds or not. This approach opens up new perspectives for the monitoring of weeds and the monitoring of their competition during crop growth with non-destructive and proximal sensing technologies in the early stages of development.},
DOI = {10.3390/rs12182982}
}



@Article{s20185262,
AUTHOR = {Li, Meizhu and Huang, Shaoguang and De Bock, Jasper and de Cooman, Gert and Pižurica, Aleksandra},
TITLE = {A Robust Dynamic Classifier Selection Approach for Hyperspectral Images with Imprecise Label Information},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5262},
URL = {https://www.mdpi.com/1424-8220/20/18/5262},
ISSN = {1424-8220},
ABSTRACT = {Supervised hyperspectral image (HSI) classification relies on accurate label information. However, it is not always possible to collect perfectly accurate labels for training samples. This motivates the development of classifiers that are sufficiently robust to some reasonable amounts of errors in data labels. Despite the growing importance of this aspect, it has not been sufficiently studied in the literature yet. In this paper, we analyze the effect of erroneous sample labels on probability distributions of the principal components of HSIs, and provide in this way a statistical analysis of the resulting uncertainty in classifiers. Building on the theory of imprecise probabilities, we develop a novel robust dynamic classifier selection (R-DCS) model for data classification with erroneous labels. Particularly, spectral and spatial features are extracted from HSIs to construct two individual classifiers for the dynamic selection, respectively. The proposed R-DCS model is based on the robustness of the classifiers&rsquo; predictions: the extent to which a classifier can be altered without changing its prediction. We provide three possible selection strategies for the proposed model with different computational complexities and apply them on three benchmark data sets. Experimental results demonstrate that the proposed model outperforms the individual classifiers it selects from and is more robust to errors in labels compared to widely adopted approaches.},
DOI = {10.3390/s20185262}
}



@Article{rs12183007,
AUTHOR = {Liu, Bo and Du, Shihong and Du, Shouji and Zhang, Xiuyuan},
TITLE = {Incorporating Deep Features into GEOBIA Paradigm for Remote Sensing Imagery Classification: A Patch-Based Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3007},
URL = {https://www.mdpi.com/2072-4292/12/18/3007},
ISSN = {2072-4292},
ABSTRACT = {The fast and accurate creation of land use/land cover maps from very-high-resolution (VHR) remote sensing imagery is crucial for urban planning and environmental monitoring. Geographic object-based image analysis methods (GEOBIA) provide an effective solution using image objects instead of individual pixels in VHR remote sensing imagery analysis. Simultaneously, convolutional neural networks (CNN) have been widely used in the image processing field because of their powerful feature extraction capabilities. This study presents a patch-based strategy for integrating deep features into GEOBIA for VHR remote sensing imagery classification. To extract deep features from irregular image objects through CNN, a patch-based approach is proposed for representing image objects and learning patch-based deep features, and a deep features aggregation method is proposed for aggregating patch-based deep features into object-based deep features. Finally, both object and deep features are integrated into a GEOBIA paradigm for classifying image objects. We explored the influences of segmentation scales and patch sizes in our method and explored the effectiveness of deep and object features in classification. Moreover, we performed 5-fold stratified cross validations 50 times to explore the uncertainty of our method. Additionally, we explored the importance of deep feature aggregation, and we evaluated our method by comparing it with three state-of-the-art methods in a Beijing dataset and Zurich dataset. The results indicate that smaller segmentation scales were more conducive to VHR remote sensing imagery classification, and it was not appropriate to select too large or too small patches as the patch size should be determined by imagery and its resolution. Moreover, we found that deep features are more effective than object features, while object features still matter for image classification, and deep feature aggregation is a critical step in our method. Finally, our method can achieve the highest overall accuracies compared with the state-of-the-art methods, and the overall accuracies are 91.21% for the Beijing dataset and 99.05% for the Zurich dataset.},
DOI = {10.3390/rs12183007}
}



@Article{rs12183015,
AUTHOR = {Machefer, Mélissande and Lemarchand, François and Bonnefond, Virginie and Hitchins, Alasdair and Sidiropoulos, Panagiotis},
TITLE = {Mask R-CNN Refitting Strategy for Plant Counting and Sizing in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3015},
URL = {https://www.mdpi.com/2072-4292/12/18/3015},
ISSN = {2072-4292},
ABSTRACT = {This work introduces a method that combines remote sensing and deep learning into a framework that is tailored for accurate, reliable and efficient counting and sizing of plants in aerial images. The investigated task focuses on two low-density crops, potato and lettuce. This double objective of counting and sizing is achieved through the detection and segmentation of individual plants by fine-tuning an existing deep learning architecture called Mask R-CNN. This paper includes a thorough discussion on the optimal parametrisation to adapt the Mask R-CNN architecture to this novel task. As we examine the correlation of the Mask R-CNN performance to the annotation volume and granularity (coarse or refined) of remotely sensed images of plants, we conclude that transfer learning can be effectively used to reduce the required amount of labelled data. Indeed, a previously trained Mask R-CNN on a low-density crop can improve performances after training on new crops. Once trained for a given crop, the Mask R-CNN solution is shown to outperform a manually-tuned computer vision algorithm. Model performances are assessed using intuitive metrics such as Mean Average Precision (mAP) from Intersection over Union (IoU) of the masks for individual plant segmentation and Multiple Object Tracking Accuracy (MOTA) for detection. The presented model reaches an mAP of 0.418 for potato plants and 0.660 for lettuces for the individual plant segmentation task. In detection, we obtain a MOTA of 0.781 for potato plants and 0.918 for lettuces.},
DOI = {10.3390/rs12183015}
}



@Article{jimaging6090097,
AUTHOR = {Bhuiyan, Md Abul Ehsan and Witharana, Chandi and Liljedahl, Anna K. and Jones, Benjamin M. and Daanen, Ronald and Epstein, Howard E. and Kent, Kelcy and Griffin, Claire G. and Agnew, Amber},
TITLE = {Understanding the Effects of Optimal Combination of Spectral Bands on Deep Learning Model Predictions: A Case Study Based on Permafrost Tundra Landform Mapping Using High Resolution Multispectral Satellite Imagery},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {97},
URL = {https://www.mdpi.com/2313-433X/6/9/97},
ISSN = {2313-433X},
ABSTRACT = {Deep learning (DL) convolutional neural networks (CNNs) have been rapidly adapted in very high spatial resolution (VHSR) satellite image analysis. DLCNN-based computer visions (CV) applications primarily aim for everyday object detection from standard red, green, blue (RGB) imagery, while earth science remote sensing applications focus on geo object detection and classification from multispectral (MS) imagery. MS imagery includes RGB and narrow spectral channels from near- and/or middle-infrared regions of reflectance spectra. The central objective of this exploratory study is to understand to what degree MS band statistics govern DLCNN model predictions. We scaffold our analysis on a case study that uses Arctic tundra permafrost landform features called ice-wedge polygons (IWPs) as candidate geo objects. We choose Mask RCNN as the DLCNN architecture to detect IWPs from eight-band Worldview-02 VHSR satellite imagery. A systematic experiment was designed to understand the impact on choosing the optimal three-band combination in model prediction. We tasked five cohorts of three-band combinations coupled with statistical measures to gauge the spectral variability of input MS bands. The candidate scenes produced high model detection accuracies for the F1 score, ranging between 0.89 to 0.95, for two different band combinations (coastal blue, blue, green (1,2,3) and green, yellow, red (3,4,5)). The mapping workflow discerned the IWPs by exhibiting low random and systematic error in the order of 0.17&ndash;0.19 and 0.20&ndash;0.21, respectively, for band combinations (1,2,3). Results suggest that the prediction accuracy of the Mask-RCNN model is significantly influenced by the input MS bands. Overall, our findings accentuate the importance of considering the image statistics of input MS bands and careful selection of optimal bands for DLCNN predictions when DLCNN architectures are restricted to three spectral channels.},
DOI = {10.3390/jimaging6090097}
}



@Article{rs12183035,
AUTHOR = {Lai, Ying-Chih and Huang, Zong-Ying},
TITLE = {Detection of a Moving UAV Based on Deep Learning-Based Distance Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3035},
URL = {https://www.mdpi.com/2072-4292/12/18/3035},
ISSN = {2072-4292},
ABSTRACT = {Distance information of an obstacle is important for obstacle avoidance in many applications, and could be used to determine the potential risk of object collision. In this study, the detection of a moving fixed-wing unmanned aerial vehicle (UAV) with deep learning-based distance estimation to conduct a feasibility study of sense and avoid (SAA) and mid-air collision avoidance of UAVs is proposed by using a monocular camera to detect and track an incoming UAV. A quadrotor is regarded as an owned UAV, and it is able to estimate the distance of an incoming fixed-wing intruder. The adopted object detection method is based on the you only look once (YOLO) object detector. Deep neural network (DNN) and convolutional neural network (CNN) methods are applied to exam their performance in the distance estimation of moving objects. The feature extraction of fixed-wing UAVs is based on the VGG-16 model, and then its result is applied to the distance network to estimate the object distance. The proposed model is trained by using synthetic images from animation software and validated by using both synthetic and real flight videos. The results show that the proposed active vision-based scheme is able to detect and track a moving UAV with high detection accuracy and low distance errors.},
DOI = {10.3390/rs12183035}
}



@Article{rs12183046,
AUTHOR = {Xiao, Yingxin and Dong, Yingying and Huang, Wenjiang and Liu, Linyi and Ma, Huiqin and Ye, Huichun and Wang, Kun},
TITLE = {Dynamic Remote Sensing Prediction for Wheat Fusarium Head Blight by Combining Host and Habitat Conditions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3046},
URL = {https://www.mdpi.com/2072-4292/12/18/3046},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing technology provides a feasible option for early prediction for wheat Fusarium head blight (FHB). This study presents a methodology for the dynamic prediction of this classic meteorological crop disease. Host and habitat conditions were comprehensively considered as inputs of the FHB prediction model, and the advantages, accuracy, and generalization ability of the model were evaluated. Firstly, multi-source satellite images were used to predict growth stages and to obtain remote sensing features, then weather features around the predicted stages were extracted. Then, with changes in the inputting features, the severity of FHB was dynamically predicted on February 18, March 6, April 23, and May 9, 2017. Compared to the results obtained by the Logistic model, the prediction with the Relevance Vector Machine performed better, with the overall accuracy on these four dates as 0.71, 0.78, 0.85, and 0.93, and with the area under the receiver operating characteristic curve as 0.66, 0.67, 0.72, and 0.75. Additionally, compared with the prediction with only one factor, the integration of multiple factors was more accurate. The results showed that when the date of the remote sensing features was closer to the heading or flowering stage, the prediction was more accurate, especially in severe areas. Though the habitat conditions were suitable for FHB, the infection can be inhibited when the host&rsquo;s growth meets certain requirements.},
DOI = {10.3390/rs12183046}
}



@Article{s20185343,
AUTHOR = {Opiela, Miroslav and Galčík, František},
TITLE = {Grid-Based Bayesian Filtering Methods for Pedestrian Dead Reckoning Indoor Positioning Using Smartphones},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5343},
URL = {https://www.mdpi.com/1424-8220/20/18/5343},
ISSN = {1424-8220},
ABSTRACT = {Indoor positioning systems for smartphones are often based on Pedestrian Dead Reckoning, which computes the current position from the previously estimated location. Noisy sensor measurements, inaccurate step length estimations, faulty direction detections, and a demand on the real-time calculation introduce the error which is suppressed using a map model and a Bayesian filtering. The main focus of this paper is on grid-based implementations of Bayes filters as an alternative to commonly used Kalman and particle filters. Our previous work regarding grid-based filters is elaborated and enriched with convolution mask calculations. More advanced implementations, the centroid grid filter, and the advanced point-mass filter are introduced. These implementations are analyzed and compared using different configurations on the same raw sensor recordings. The evaluation is performed on three sets of experiments: a custom simple path in faculty building in Slovakia, and on datasets from IPIN competitions from a shopping mall in France, 2018 and a research institute in Italy, 2019. Evaluation results suggests that proposed methods are qualified alternatives to the particle filter. Advantages, drawbacks and proper configurations of these filters are discussed in this paper.},
DOI = {10.3390/s20185343}
}



@Article{en13184898,
AUTHOR = {Zeng, Han and Zuo, Pengqi and Deng, Fangming and Zhang, Pei},
TITLE = {Monitoring System of Transmission Line in Mountainous Area Based on LPWAN},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {4898},
URL = {https://www.mdpi.com/1996-1073/13/18/4898},
ISSN = {1996-1073},
ABSTRACT = {In light of the difficulty of the inspection and maintenance of a transmission line condition monitoring system in remote mountainous areas, this paper proposes a long-term online monitoring scheme based on a low power wide area network (LPWAN). Considering different failure rates, three monitoring periods of transmission lines in mountainous areas are proposed. An online monitoring framework of transmission lines in mountainous areas was designed based on long range radio (LoRa) and a cellular mobile network, and a dynamic group network model of LoRa was established. The multi-objective particle swarm optimization algorithm can be used to optimize the energy and delay of the system, and then the suitable working mode for the three monitoring periods can be obtained. The simulation results showed that the minimum packet loss rate of the system could be less than 1%, the energy consumption of the system was 80% lower than the existing monitoring system, and the service life of the system can reach 15.13 years under the normal failure rate. Compared with the existing schemes, the proposed work shows the advantages of high reliability transmission, low cost and long-term monitoring, which is especially for transmission line monitoring in mountainous areas.},
DOI = {10.3390/en13184898}
}



@Article{s20185354,
AUTHOR = {Yang, Chin-Ying and Yang, Ming-Der and Tseng, Wei-Cheng and Hsu, Yu-Chun and Li, Guan-Sin and Lai, Ming-Hsin and Wu, Dong-Hong and Lu, Hsiu-Ying},
TITLE = {Assessment of Rice Developmental Stage Using Time Series UAV Imagery for Variable Irrigation Management},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5354},
URL = {https://www.mdpi.com/1424-8220/20/18/5354},
ISSN = {1424-8220},
ABSTRACT = {Rice is one of the three major crops in the world and is the major crop in Asia. Climate change and water resource shortages may result in decreases in rice yields and possible food shortage crises. In this study, water-saving farming management was tested, and IOT field water level monitoring was used to regulate water inflow automatically. Plant height (PH) is an important phenotype to be used to determine difference in rice growth periods and yields using water-saving irrigation. An unmanned aerial vehicle (UAV) with an RGB camera captured sequential images of rice fields to estimate rice PH compared with PH measured on site for estimating rice growth stages. The test results, with two crop harvests in 2019, revealed that with adequate image calibration, the correlation coefficient between UAV-PH and field-PH was higher than 0.98, indicating that UAV images can accurately determine rice PH in the field and rice growth phase. The study demonstrated that water-saving farming is effective, decreasing water usage for the first and second crops of 2019 by 53.5% and 21.7%, respectively, without influencing the growth period and final yield. Coupled with an automated irrigation system, rice farming can be adaptive to water shortage situations.},
DOI = {10.3390/s20185354}
}



@Article{computers9030075,
AUTHOR = {Contreras, Ruben and Ayala, Angel and Cruz, Francisco},
TITLE = {Unmanned Aerial Vehicle Control through Domain-Based Automatic Speech Recognition},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {75},
URL = {https://www.mdpi.com/2073-431X/9/3/75},
ISSN = {2073-431X},
ABSTRACT = {Currently, unmanned aerial vehicles, such as drones, are becoming a part of our lives and extend to many areas of society, including the industrialized world. A common alternative for controlling the movements and actions of the drone is through unwired tactile interfaces, for which different remote control devices are used. However, control through such devices is not a natural, human-like communication interface, which sometimes is difficult to master for some users. In this research, we experimented with a domain-based speech recognition architecture to effectively control an unmanned aerial vehicle such as a drone. The drone control was performed in a more natural, human-like way to communicate the instructions. Moreover, we implemented an algorithm for command interpretation using both Spanish and English languages, as well as to control the movements of the drone in a simulated domestic environment. We conducted experiments involving participants giving voice commands to the drone in both languages in order to compare the effectiveness of each, considering the mother tongue of the participants in the experiment. Additionally, different levels of distortion were applied to the voice commands to test the proposed approach when it encountered noisy input signals. The results obtained showed that the unmanned aerial vehicle was capable of interpreting user voice instructions. Speech-to-action recognition improved for both languages with phoneme matching in comparison to only using the cloud-based algorithm without domain-based instructions. Using raw audio inputs, the cloud-based approach achieves 74.81% and 97.04% accuracy for English and Spanish instructions, respectively. However, with our phoneme matching approach the results are improved, yielding 93.33% accuracy for English and 100.00% accuracy for Spanish.},
DOI = {10.3390/computers9030075}
}



@Article{s20185374,
AUTHOR = {Zhao, Long and Ishag Mahmoud, Mubarak Adam and Ren, Honge and Zhu, Meng},
TITLE = {A Visual Tracker Offering More Solutions},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5374},
URL = {https://www.mdpi.com/1424-8220/20/18/5374},
ISSN = {1424-8220},
ABSTRACT = {Most trackers focus solely on robustness and accuracy. Visual tracking, however, is a long-term problem with a high time limitation. A tracker that is robust, accurate, with long-term sustainability and real-time processing, is of high research value and practical significance. In this paper, we comprehensively consider these requirements in order to propose a new, state-of-the-art tracker with an excellent performance. EfficientNet-B0 is adopted for the first time via neural architecture search technology as the backbone network for the tracking task. This improves the network feature extraction ability and significantly reduces the number of parameters required for the tracker backbone network. In addition, maximal Distance Intersection-over-Union is set as the target estimation method, enhancing network stability and increasing the offline training convergence rate. Channel and spatial dual attention mechanisms are employed in the target classification module to improve the discrimination of the trackers. Furthermore, the conjugate gradient optimization strategy increases the speed of the online learning target classification module. A two-stage search method combined with a screening module is proposed to enable the tracker to cope with sudden target movement and reappearance following a brief disappearance. Our proposed method has an obvious speed advantage compared with pure global searching and achieves an optimal performance on OTB2015, VOT2016, VOT2018-LT, UAV-123 and LaSOT while running at over 50 FPS.},
DOI = {10.3390/s20185374}
}



@Article{rs12183075,
AUTHOR = {Raja Abdullah, Raja Syamsul Azmir and Alhaji Musa, Surajo and Abdul Rashid, Nur Emileen and Sali, Aduwati and Salah, Asem Ahmad and Ismail, Alyani},
TITLE = {Passive Forward-Scattering Radar Using Digital Video Broadcasting Satellite Signal for Drone Detection},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3075},
URL = {https://www.mdpi.com/2072-4292/12/18/3075},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a passive radar system using a signal of opportunity from Digital Video Broadcasting Satellite (DVB-S). The ultimate purpose of the system is to be used as an air traffic monitoring and surveillance system. However, the work focuses on drone detection as a proof of the concept. Detecting a drone by using satellite-based passive radar possess inherent challenges, such as the small radar cross section and low speed. Therefore, this paper proposes a unique method by leveraging the advantage of forward-scattering radar (FSR) topology and characteristics to detect a drone; in other words, the system is known as a passive FSR (p-FSR) system. In the signal-processing algorithm, the empirical mode decomposition (EMD) is applied to the received signal to extract the unique feature vector of the micro-Doppler frequency from the drone&rsquo;s rotating blades. The paper highlights the p-FSR experimental setup and experiment campaign to detect drones. The experimental results show the feasibility of the p-FSR using a signal transmitted from a satellite to detect flying drone crossing the forward-scatter baseline between the satellite and ground station.},
DOI = {10.3390/rs12183075}
}



@Article{rs12183079,
AUTHOR = {Osadchiev, Alexander and Barymova, Alexandra and Sedakov, Roman and Zhiba, Roman and Dbar, Roman},
TITLE = {Spatial Structure, Short-temporal Variability, and Dynamical Features of Small River Plumes as Observed by Aerial Drones: Case Study of the Kodor and Bzyp River Plumes},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3079},
URL = {https://www.mdpi.com/2072-4292/12/18/3079},
ISSN = {2072-4292},
ABSTRACT = {Quadcopters can continuously observe ocean surface with high spatial resolution from relatively low altitude, albeit with certain limitations of their usage. Remote sensing from quadcopters provides unprecedented ability to study small river plumes formed in the coastal sea. The main goal of the current work is to describe structure and temporal variability of small river plumes on small spatial and temporal scales, which are limitedly covered by previous studies. We analyze optical imagery and video records acquired by quadcopters and accompanied by synchronous in situ measurements and satellite observations within the Kodor and Bzyp plumes, which are located in the northeastern part of the Black Sea. We describe extremely rapid response of these river plume to energetic rotating coastal eddies. We reveal several types of internal waves within these river plumes, measure their spatial and dynamical characteristics, and identify mechanisms of their generation. We suggest a new mechanism of formation of undulate fronts between small river plumes and ambient sea, which induces energetic lateral mixing across these fronts. The results reported in this study are addressed for the first time as previous related works were mainly limited by low spatial and/or temporal resolution of in situ measurements and satellite imagery.},
DOI = {10.3390/rs12183079}
}



@Article{rs12183084,
AUTHOR = {Abdellatif, Mohamed and Peel, Harriet and Cohn, Anthony G. and Fuentes, Raul},
TITLE = {Pavement Crack Detection from Hyperspectral Images Using a Novel Asphalt Crack Index},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3084},
URL = {https://www.mdpi.com/2072-4292/12/18/3084},
ISSN = {2072-4292},
ABSTRACT = {Detection of road pavement cracks is important and needed at an early stage to repair the road and extend its lifetime for maintaining city roads. Cracks are hard to detect from images taken with visible spectrum cameras due to noise and ambiguity with background textures besides the lack of distinct features in cracks. Hyperspectral images are sensitive to surface material changes and their potential for road crack detection is explored here. The key observation is that road cracks reveal the interior material that is different from the worn surface material. A novel asphalt crack index is introduced here as an additional clue that is sensitive to the spectra in the range 450&ndash;550 nm. The crack index is computed and found to be strongly correlated with the appearance of fresh asphalt cracks. The new index is then used to differentiate cracks from road surfaces. Several experiments have been made, which confirmed that the proposed index is effective for crack detection. The recall-precision analysis showed an increase in the associated F1-score by an average of 21.37% compared to the VIS2 metric in the literature (a metric used to classify pavement condition from hyperspectral data).},
DOI = {10.3390/rs12183084}
}



@Article{electronics9101563,
AUTHOR = {Noh, Sanguk},
TITLE = {Intelligent Data Fusion and Multi-Agent Coordination for Target Allocation},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1563},
URL = {https://www.mdpi.com/2079-9292/9/10/1563},
ISSN = {2079-9292},
ABSTRACT = {This paper addresses the fusion processing techniques for multi-sensor data perceived through the infrared sensors of military surveillance robots, and proposes their decision-theoretic coordination to effectively monitor multiple targets. To combine the multi-sensor data from the distributed battlefield robots, a set of fusion rules are used to formulate a combined prediction from the multi-source data. The possible type of a target is estimated through the fusion rules. For the identification of targets, agents need to keep track of targets for continuous situation awareness. The coordination of the agents with limited range of surveillance is indispensable for their successful monitoring of multiple targets. For dynamic and flexible coordination, our agents follow the decision-theoretic approach. We implement a military simulator to compare the capabilities of fusion processing and those of coordination, and conduct experiments with our framework in distributed and uncertain battlefield environments. The experimental results show that the fusion process of multi-sensor data from military robots can improve the performance of estimation of the type of a target, and our coordinated agents outperform agents using random strategy for their target selection in various military scenarios.},
DOI = {10.3390/electronics9101563}
}



@Article{rs12193134,
AUTHOR = {Hu, Guanghui and Dai, Wen and Li, Sijin and Xiong, Liyang and Tang, Guoan},
TITLE = {A Vector Operation to Extract Second-Order Terrain Derivatives from Digital Elevation Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3134},
URL = {https://www.mdpi.com/2072-4292/12/19/3134},
ISSN = {2072-4292},
ABSTRACT = {Terrain derivatives exhibit surface morphology in various aspects. However, existing spatial change calculation methods for terrain derivatives are based on a mathematical scalar operating system, which may disregard the directional property of the original data to a certain extent. This situation is particularly true in second-order terrain derivatives, in which original data can be terrain derivatives with clear directional properties, such as slope or aspect. Thus, this study proposes a mathematical vector operation method for the calculation of second-order terrain derivatives. Given the examples of the first-order terrain derivatives of slope and aspect, their second-order terrain derivatives are calculated using the proposed vector method. Directional properties are considered and vectorized using the following steps: rotation-type judgment, standardization of initial direction, and vector representation. The proposed vector method is applied to one mathematical Gaussian surface and three different ground landform areas using digital elevation models (DEMs) with 5 and 1 m resolutions. Comparison analysis results between the vector and scalar methods show that the former achieves more reasonable and accurate second-order terrain derivatives than the latter. Moreover, the vector method avoids overexpression or even exaggeration errors. This vector operation concept and its expanded methods can be applied in calculating other terrain derivatives in geomorphometry.},
DOI = {10.3390/rs12193134}
}



@Article{s20195473,
AUTHOR = {Hu, Jie and Wang, Tuan and Yang, Jiacheng and Lan, Yubin and Lv, Shilei and Zhang, Yali},
TITLE = {WSN-Assisted UAV Trajectory Adjustment for Pesticide Drift Control},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5473},
URL = {https://www.mdpi.com/1424-8220/20/19/5473},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) have been widely applied for pesticide spraying as they have high efficiency and operational flexibility. However, the pesticide droplet drift caused by wind may decrease the pesticide spraying efficiency and pollute the environment. A precision spraying system based on an airborne meteorological monitoring platform on manned agricultural aircrafts is not adaptable for. So far, there is no better solution for controlling droplet drift outside the target area caused by wind, especially by wind gusts. In this regard, a UAV trajectory adjustment system based on Wireless Sensor Network (WSN) for pesticide drift control was proposed in this research. By collecting data from ground WSN, the UAV utilizes the wind speed and wind direction as inputs to autonomously adjust its trajectory for keeping droplet deposition in the target spraying area. Two optimized algorithms, namely deep reinforcement learning and particle swarm optimization, were applied to generate the newly modified flight route. At the same time, a simplified pesticide droplet drift model that includes wind speed and wind direction as parameters was developed and adopted to simulate and compute the drift distance of pesticide droplets. Moreover, an LSTM-based wind speed prediction model and a RNN-based wind direction prediction model were established, so as to address the problem of missing the latest wind data caused by communication latency or a lack of connection with the ground nodes. Finally, experiments were carried out to test the communication latency between UAV and ground WSN, and to evaluate the proposed scheme with embedded Raspberry Pi boards in UAV for feasibility verification. Results show that the WSN-assisted UAV trajectory adjustment system is capable of providing a better performance of on-target droplet deposition for real time pesticide spraying with UAV.},
DOI = {10.3390/s20195473}
}



@Article{rs12193149,
AUTHOR = {Guan, HaiXiang and Liu, HuanJun and Meng, XiangTian and Luo, Chong and Bao, YiLin and Ma, YuYang and Yu, ZiYang and Zhang, XinLe},
TITLE = {A Quantitative Monitoring Method for Determining Maize Lodging in Different Growth Stages},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3149},
URL = {https://www.mdpi.com/2072-4292/12/19/3149},
ISSN = {2072-4292},
ABSTRACT = {Many studies have achieved efficient and accurate methods for identifying crop lodging under homogeneous field surroundings. However, under complex field conditions, such as diverse fertilization methods, different crop growth stages, and various sowing periods, the accuracy of lodging identification must be improved. Therefore, a maize plot featuring different growth stages was selected in this study to explore an applicable and accurate lodging extraction method. Based on the Akaike information criterion (AIC), we propose an effective and rapid feature screening method (AIC method) and compare its performance using indexed methods (i.e., variation coefficient and relative difference). Seven feature sets extracted from unmanned aerial vehicle (UAV) images of lodging and nonlodging maize were established using a canopy height model (CHM) and the multispectral imagery acquired from the UAV. In addition to accuracy parameters (i.e., Kappa coefficient and overall accuracy), the difference index (DI) was applied to search for the optimal window size of texture features. After screening all feature sets by applying the AIC method, binary logistic regression classification (BLRC), maximum likelihood classification (MLC), and random forest classification (RFC) were utilized to discriminate among lodging and nonlodging maize based on the selected features. The results revealed that the optimal window sizes of the gray-level cooccurrence matrix (GLCM) and the gray-level difference histogram statistical (GLDM) texture information were 17 &times; 17 and 21 &times; 21, respectively. The AIC method incorporating GLCM texture yielded satisfactory results, obtaining an average accuracy of 82.84% and an average Kappa value of 0.66 and outperforming the index screening method (59.64%, 0.19). Furthermore, the canopy structure feature (CSF) was more beneficial than other features for identifying maize lodging areas at the plot scale. Based on the AIC method, we achieved a positive maize lodging recognition result using the CSFs and BLRC. This study provides a highly robust and novel method for monitoring maize lodging in complicated plot environments.},
DOI = {10.3390/rs12193149}
}



@Article{drones4040063,
AUTHOR = {Steup, Christoph and Parlow, Simon and Mai, Sebastian and Mostaghim, Sanaz},
TITLE = {Generic Component-Based Mission-Centric Energy Model for Micro-Scale Unmanned Aerial Vehicles},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {63},
URL = {https://www.mdpi.com/2504-446X/4/4/63},
ISSN = {2504-446X},
ABSTRACT = {The trend towards the usage of battery-electric unmanned aerial vehicles needs new strategies in mission planning and in the design of the systems themselves. To create an optimal mission plan and take appropriate decisions during the mission, a reliable, accurate and adaptive energy model is of utmost importance. However, most existing approaches either use very generic models or ones that are especially tailored towards a specific UAV. We present a generic energy model that is based on decomposing a robotic system into multiple observable components. The generic model is applied to a swarm of quadcopters and evaluated in multiple flights with different manoeuvres. We additionally use the data from practical experiments to learn and generate a mission-agnostic energy model which can match the typical behaviour of our quadcopters such as hovering; movement in x, y and z directions; landing; communication; and illumination. The learned energy model concurs with the overall energy consumption with an accuracy over 95% compared to the training flights for the indoor use case. An extended model reduces the error to less than 1.4%. Consequently, the proposed model enables an estimation of the energy used in flight and on the ground, which can be easily incorporated in autonomous systems and enhance decision-making with reliable input. The used learning mechanism allows to deploy the approach with minimal effort to new platforms needing only some representative test missions, which was shown using additional outdoor validation flights with a different quadcopter of the same build and the originally trained models. This set-up increased the prediction error of our model to 4.46%.},
DOI = {10.3390/drones4040063}
}



@Article{s20195495,
AUTHOR = {El Boudani, Brahim and Kanaris, Loizos and Kokkinis, Akis and Kyriacou, Michalis and Chrysoulas, Christos and Stavrou, Stavros and Dagiuklas, Tasos},
TITLE = {Implementing Deep Learning Techniques in 5G IoT Networks for 3D Indoor Positioning: DELTA (DeEp Learning-Based Co-operaTive Architecture)},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5495},
URL = {https://www.mdpi.com/1424-8220/20/19/5495},
ISSN = {1424-8220},
ABSTRACT = {In the near future, the fifth-generation wireless technology is expected to be rolled out, offering low latency, high bandwidth and multiple antennas deployed in a single access point. This ecosystem will help further enhance various location-based scenarios such as assets tracking in smart factories, precise smart management of hydroponic indoor vertical farms and indoor way-finding in smart hospitals. Such a system will also integrate existing technologies like the Internet of Things (IoT), WiFi and other network infrastructures. In this respect, 5G precise indoor localization using heterogeneous IoT technologies (Zigbee, Raspberry Pi, Arduino, BLE, etc.) is a challenging research area. In this work, an experimental 5G testbed has been designed integrating C-RAN and IoT networks. This testbed is used to improve both vertical and horizontal localization (3D Localization) in a 5G IoT environment. To achieve this, we propose the DEep Learning-based co-operaTive Architecture (DELTA) machine learning model implemented on a 3D multi-layered fingerprint radiomap. The DELTA begins by estimating the 2D location. Then, the output is recursively used to predict the 3D location of a mobile station. This approach is going to benefit use cases such as 3D indoor navigation in multi-floor smart factories or in large complex buildings. Finally, we have observed that the proposed model has outperformed traditional algorithms such as Support Vector Machine (SVM) and K-Nearest Neighbor (KNN).},
DOI = {10.3390/s20195495}
}



@Article{rs12193152,
AUTHOR = {Courtrai, Luc and Pham, Minh-Tan and Lefèvre, Sébastien},
TITLE = {Small Object Detection in Remote Sensing Images Based on Super-Resolution with Auxiliary Generative Adversarial Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3152},
URL = {https://www.mdpi.com/2072-4292/12/19/3152},
ISSN = {2072-4292},
ABSTRACT = {This article tackles the problem of detecting small objects in satellite or aerial remote sensing images by relying on super-resolution to increase image spatial resolution, thus the size and details of objects to be detected. We show how to improve the super-resolution framework starting from the learning of a generative adversarial network (GAN) based on residual blocks and then its integration into a cycle model. Furthermore, by adding to the framework an auxiliary network tailored for object detection, we considerably improve the learning and the quality of our final super-resolution architecture, and more importantly increase the object detection performance. Besides the improvement dedicated to the network architecture, we also focus on the training of super-resolution on target objects, leading to an object-focused approach. Furthermore, the proposed strategies do not depend on the choice of a baseline super-resolution framework, hence could be adopted for current and future state-of-the-art models. Our experimental study on small vehicle detection in remote sensing data conducted on both aerial and satellite images (i.e., ISPRS Potsdam and xView datasets) confirms the effectiveness of the improved super-resolution methods to assist with the small object detection tasks.},
DOI = {10.3390/rs12193152}
}



@Article{agriculture10100434,
AUTHOR = {Filip, Martin and Zoubek, Tomas and Bumbalek, Roman and Cerny, Pavel and Batista, Carlos E. and Olsan, Pavel and Bartos, Petr and Kriz, Pavel and Xiao, Maohua and Dolan, Antonin and Findura, Pavol},
TITLE = {Advanced Computational Methods for Agriculture Machinery Movement Optimization with Applications in Sugarcane Production},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {434},
URL = {https://www.mdpi.com/2077-0472/10/10/434},
ISSN = {2077-0472},
ABSTRACT = {This paper considers the evolution of processes applied in agriculture for field operations developed from non-organized handmade activities into very specialized and organized production processes. A set of new approaches based on the application of metaheuristic optimization methods and smart automatization known as Agriculture 4.0 has enabled a rapid increase in in-field operations&rsquo; productivity and offered unprecedented economic benefits. The aim of this paper is to review modern approaches to agriculture machinery movement optimization with applications in sugarcane production. Approaches based on algorithms for the division of spatial configuration, route planning or path planning, as well as approaches using cost parameters, e.g., energy, fuel and time consumption, are presented. The combination of algorithmic and economic methodologies including evaluation of the savings and investments and their cost/benefit relation is discussed.},
DOI = {10.3390/agriculture10100434}
}



@Article{agriculture10100436,
AUTHOR = {Niazian, Mohsen and Niedbała, Gniewko},
TITLE = {Machine Learning for Plant Breeding and Biotechnology},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {436},
URL = {https://www.mdpi.com/2077-0472/10/10/436},
ISSN = {2077-0472},
ABSTRACT = {Classical univariate and multivariate statistics are the most common methods used for data analysis in plant breeding and biotechnology studies. Evaluation of genetic diversity, classification of plant genotypes, analysis of yield components, yield stability analysis, assessment of biotic and abiotic stresses, prediction of parental combinations in hybrid breeding programs, and analysis of in vitro-based biotechnological experiments are mainly performed by classical statistical methods. Despite successful applications, these classical statistical methods have low efficiency in analyzing data obtained from plant studies, as the genotype, environment, and their interaction (G &times; E) result in nondeterministic and nonlinear nature of plant characteristics. Large-scale data flow, including phenomics, metabolomics, genomics, and big data, must be analyzed for efficient interpretation of results affected by G &times; E. Nonlinear nonparametric machine learning techniques are more efficient than classical statistical models in handling large amounts of complex and nondeterministic information with &ldquo;multiple-independent variables versus multiple-dependent variables&rdquo; nature. Neural networks, partial least square regression, random forest, and support vector machines are some of the most fascinating machine learning models that have been widely applied to analyze nonlinear and complex data in both classical plant breeding and in vitro-based biotechnological studies. High interpretive power of machine learning algorithms has made them popular in the analysis of plant complex multifactorial characteristics. The classification of different plant genotypes with morphological and molecular markers, modeling and predicting important quantitative characteristics of plants, the interpretation of complex and nonlinear relationships of plant characteristics, and predicting and optimizing of in vitro breeding methods are the examples of applications of machine learning in conventional plant breeding and in vitro-based biotechnological studies. Precision agriculture is possible through accurate measurement of plant characteristics using imaging techniques and then efficient analysis of reliable extracted data using machine learning algorithms. Perfect interpretation of high-throughput phenotyping data is applicable through coupled machine learning-image processing. Some applied and potentially applicable capabilities of machine learning techniques in conventional and in vitro-based plant breeding studies have been discussed in this overview. Discussions are of great value for future studies and could inspire researchers to apply machine learning in new layers of plant breeding.},
DOI = {10.3390/agriculture10100436}
}



@Article{s20195538,
AUTHOR = {Zhang, Yunsheng and Zhu, Yaochen and Li, Haifeng and Chen, Siyang and Peng, Jian and Zhao, Ling},
TITLE = {Automatic Changes Detection between Outdated Building Maps and New VHR Images Based on Pre-Trained Fully Convolutional Feature Maps},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5538},
URL = {https://www.mdpi.com/1424-8220/20/19/5538},
ISSN = {1424-8220},
ABSTRACT = {Detecting changes between the existing building basemaps and newly acquired high spatial resolution remotely sensed (HRS) images is a time-consuming task. This is mainly because of the data labeling and poor performance of hand-crafted features. In this paper, for efficient feature extraction, we propose a fully convolutional feature extractor that is reconstructed from the deep convolutional neural network (DCNN) and pre-trained on the Pascal VOC dataset. Our proposed method extract pixel-wise features, and choose salient features based on a random forest (RF) algorithm using the existing basemaps. A data cleaning method through cross-validation and label-uncertainty estimation is also proposed to select potential correct labels and use them for training an RF classifier to extract the building from new HRS images. The pixel-wise initial classification results are refined based on a superpixel-based graph cuts algorithm and compared to the existing building basemaps to obtain the change map. Experiments with two simulated and three real datasets confirm the effectiveness of our proposed method and indicate high accuracy and low false alarm rate.},
DOI = {10.3390/s20195538}
}



@Article{fi12100164,
AUTHOR = {Sun, Wei and Su, Hui and Xie, Huacheng},
TITLE = {Policy-Engineering Optimization with Visual Representation and Separation-of-Duty Constraints in Attribute-Based Access Control},
JOURNAL = {Future Internet},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {164},
URL = {https://www.mdpi.com/1999-5903/12/10/164},
ISSN = {1999-5903},
ABSTRACT = {Recently, attribute-based access control (ABAC) has received increasingly more attention and has emerged as the desired access control mechanism for many organizations because of its flexibility and scalability for authorization management, as well as its security policies, such as separation-of-duty constraints and mutually exclusive constraints. Policy-engineering technology is an effective approach for the construction of ABAC systems. However, most conventional methods lack interpretability, and their constructing processes are complex. Furthermore, they do not consider the separation-of-duty constraints. To address these issues in ABAC, this paper proposes a novel method called policy engineering optimization with visual representation and separation of duty constraints (PEO_VR&amp;SOD). First, to enhance interpretability while mining a minimal set of rules, we use the visual technique with Hamming distance to reduce the policy mining scale and present a policy mining algorithm. Second, to verify whether the separation of duty constraints can be satisfied in a constructed policy engineering system, we use the method of SAT-based model counting to reduce the constraints and construct mutually exclusive constraints to implicitly enforce the given separation of duty constraints. The experiments demonstrate the efficiency and effectiveness of the proposed method and show encouraging results.},
DOI = {10.3390/fi12100164}
}



@Article{rs12193171,
AUTHOR = {Park, Jinseok and Jang, Seongju and Hong, Rokgi and Suh, Kyo and Song, Inhong},
TITLE = {Development of Land Cover Classification Model Using AI Based FusionNet Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3171},
URL = {https://www.mdpi.com/2072-4292/12/19/3171},
ISSN = {2072-4292},
ABSTRACT = {Prompt updates of land cover maps are important, as spatial information of land cover is widely used in many areas. However, current manual digitizing methods are time consuming and labor intensive, hindering rapid updates of land cover maps. The objective of this study was to develop an artificial intelligence (AI) based land cover classification model that allows for rapid land cover classification from high-resolution remote sensing (HRRS) images. The model comprises of three modules: pre-processing, land cover classification, and post-processing modules. The pre-processing module separates the HRRS image into multiple aspects by overlapping 75% using the sliding window algorithm. The land cover classification module was developed using the convolutional neural network (CNN) concept, based the FusionNet network and used to assign a land cover type to the separated HRRS images. Post-processing module determines ultimate land cover types by summing up the separated land cover result from the land cover classification module. Model training and validation were conducted to evaluate the performance of the developed model. The land cover maps and orthographic images of 547.29 km2 in area from the Jeonnam province in Korea were used to train the model. For model validation, two spatial and temporal different sites, one from Subuk-myeon of Jeonnam province in 2018 and the other from Daseo-myeon of Chungbuk province in 2016, were randomly chosen. The model performed reasonably well, demonstrating overall accuracies of 0.81 and 0.71, and kappa coefficients of 0.75 and 0.64, for the respective validation sites. The model performance was better when only considering the agricultural area by showing overall accuracy of 0.83 and kappa coefficients of 0.73. It was concluded that the developed model may assist rapid land cover update especially for agricultural areas and incorporation field boundary lineation is suggested as future study to further improve the model accuracy.},
DOI = {10.3390/rs12193171}
}



@Article{rs12193175,
AUTHOR = {Geng, Kai and Sun, Xian and Yan, Zhiyuan and Diao, Wenhui and Gao, Xin},
TITLE = {Topological Space Knowledge Distillation for Compact Road Extraction in Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3175},
URL = {https://www.mdpi.com/2072-4292/12/19/3175},
ISSN = {2072-4292},
ABSTRACT = {Road extraction from optical remote sensing images has drawn much attention in recent decades and has a wide range of applications. Most of the previous studies rarely take into account the unique topological characteristics of the road. It is the most apparent feature of linear structure that describes the variety of connection relationships of the road. However, designing a particular topological feature extraction network usually results in a model that is too heavy and impractical. To address the problems mentioned above, in this paper, we propose a lightweight topological space network for road extraction based on knowledge distillation (TSKD-Road). Specifically, (1) narrow and short roads easily influence topological features extracted directly in optical remote sensing images. Therefore, we propose a denser teacher network for extracting road structures; (2) to enhance the weight of topological features, we propose a topological space loss calculation model with multiple widths and depths; (3) based on the above innovations, a topological space knowledge distillation framework is proposed, which aims to transfer different kinds of knowledge acquired in a heavy net to a lightweight net, while significantly improving the lightweight net’s accuracy. Experiments were conducted on two publicly available benchmark datasets, which show the obvious superiority and effectiveness of our network.},
DOI = {10.3390/rs12193175}
}



@Article{rs12193184,
AUTHOR = {Camarretta, Nicolò and A. Harrison, Peter and Lucieer, Arko and M. Potts, Brad and Davidson, Neil and Hunt, Mark},
TITLE = {From Drones to Phenotype: Using UAV-LiDAR to Detect Species and Provenance Variation in Tree Productivity and Structure},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3184},
URL = {https://www.mdpi.com/2072-4292/12/19/3184},
ISSN = {2072-4292},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) for remote sensing of natural environments has increased over the last decade. However, applications of this technology for high-throughput individual tree phenotyping in a quantitative genetic framework are rare. We here demonstrate a two-phased analytical pipeline that rapidly phenotypes and filters for genetic signals in traditional and novel tree productivity and architectural traits derived from ultra-dense light detection and ranging (LiDAR) point clouds. The goal of this study was rapidly phenotype individual trees to understand the genetic basis of ecologically and economically significant traits important for guiding the management of natural resources. Individual tree point clouds were acquired using UAV-LiDAR captured over a multi-provenance common-garden restoration field trial located in Tasmania, Australia, established using two eucalypt species (Eucalyptus pauciflora and Eucalyptus tenuiramis). Twenty-five tree productivity and architectural traits were calculated for each individual tree point cloud. The first phase of the analytical pipeline found significant species differences in 13 of the 25 derived traits, revealing key structural differences in productivity and crown architecture between species. The second phase investigated the within species variation in the same 25 structural traits. Significant provenance variation was detected for 20 structural traits in E. pauciflora and 10 in E. tenuiramis, with signals of divergent selection found for 11 and 7 traits, respectively, putatively driven by the home-site environment shaping the observed variation. Our results highlight the genetic-based diversity within and between species for traits important for forest structure, such as crown density and structural complexity. As species and provenances are being increasingly translocated across the landscape to mitigate the effects of rapid climate change, our results that were achieved through rapid phenotyping using UAV-LiDAR, raise the need to understand the functional value of productivity and architectural traits reflecting species and provenance differences in crown structure and the interplay they have on the dependent biotic communities.},
DOI = {10.3390/rs12193184}
}



@Article{app10196881,
AUTHOR = {Ciaburro, Giuseppe and Iannace, Gino and Puyana-Romero, Virginia and Trematerra, Amelia},
TITLE = {A Comparison between Numerical Simulation Models for the Prediction of Acoustic Behavior of Giant Reeds Shredded},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {6881},
URL = {https://www.mdpi.com/2076-3417/10/19/6881},
ISSN = {2076-3417},
ABSTRACT = {Giant reeds represent a natural fiber widely available in some areas of the world. Its use can be particularly useful as the uncontrolled growth of giant reeds can be a problem because large areas are invaded by them and the crops are damaged. In this study, two models of numerical simulation of the acoustic behavior of giant reeds were put in comparison: the Hamet model and a model based on artificial neural networks. First, the characteristics of the reeds were examined and the procedures for the preparation of the samples to be analyzed were described. Then air flow resistance, porosity and sound absorption coefficient were measured and analyzed in detail. Finally, the results of the numerical modeling of the acoustic coefficient were compared. The neural network-based model showed high Pearson correlation coefficient value, indicating a large number of correct predictions.},
DOI = {10.3390/app10196881}
}



@Article{agriculture10100451,
AUTHOR = {López-Calderón, Magali J. and Estrada-Ávalos, Juan and Rodríguez-Moreno, Víctor M. and Mauricio-Ruvalcaba, Jorge E. and Martínez-Sifuentes, Aldo R. and Delgado-Ramírez, Gerardo and Miguel-Valle, Enrique},
TITLE = {Estimation of Total Nitrogen Content in Forage Maize (Zea mays L.) Using Spectral Indices: Analysis by Random Forest},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {451},
URL = {https://www.mdpi.com/2077-0472/10/10/451},
ISSN = {2077-0472},
ABSTRACT = {Knowing the total Nitrogen content (Nt) of forage maize (Zea mays) is important so that decisions can be made quickly and efficiently to adjust the timing and amount of both irrigation and fertilizer. In 2017 and 2018 during three growing cycles in two study plots, leaf samples were collected and the Dumas method was used to estimate Nt. During the same growing seasons and on the same sampling plots, a Parrot Sequoia camera mounted on an unmanned aerial vehicle (UAV) was used to collect high resolution images of forage maize study plots. Thirteen multispectral indices were generated and, from these, a Random Forest (RF) algorithm was used to estimate Nt. RF is a machine-learning technique and is designed to work with extremely large datasets. Overall analysis showed five of the 13 indices as the most important. One of these five, the Transformed Chlorophyll Absorption in Reflectance Index/Optimized Soil-Adjusted Vegetation Index, was found to be the most important for estimation of Nt in forage maize (R2 = 0.76). RF handled the complex dataset in a time-efficient manner and Nt did not differ significantly when compared between traditional methods of evaluating Nt at the canopy level and using UAVs and RF to estimate Nt in forage maize. This result is an opportunity to explore many new research options in precision farming and digital agriculture.},
DOI = {10.3390/agriculture10100451}
}



@Article{rs12193208,
AUTHOR = {Chen, Jian and Zhang, Zichao and Zhang, Kai and Wang, Shubo and Han, Yu},
TITLE = {UAV-Borne LiDAR Crop Point Cloud Enhancement Using Grasshopper Optimization and Point Cloud Up-Sampling Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3208},
URL = {https://www.mdpi.com/2072-4292/12/19/3208},
ISSN = {2072-4292},
ABSTRACT = {Because of low accuracy and density of crop point clouds obtained by the Unmanned Aerial Vehicle (UAV)-borne Light Detection and Ranging (LiDAR) scanning system of UAV, an integrated navigation and positioning optimization method based on the grasshopper optimization algorithm (GOA) and a point cloud density enhancement method were proposed. Firstly, a global positioning system (GPS)/inertial navigation system (INS) integrated navigation and positioning information fusion method based on a Kalman filter was constructed. Then, the GOA was employed to find the optimal solution by iterating the system noise variance matrix Q and measurement noise variance matrix R of Kalman filter. By feeding the optimal solution into the Kalman filter, the error variances of longitude were reduced to 0.00046 from 0.0091, and the error variances of latitude were reduced to 0.00034 from 0.0047. Based on the integrated navigation, an UAV-borne LiDAR scanning system was built for obtaining the crop point. During offline processing, the crop point cloud was filtered and transformed into WGS-84, the density clustering algorithm improved by the particle swarm optimization (PSO) algorithm was employed to the clustering segment. After the clustering segment, the pre-trained Point Cloud Up-Sampling Network (PU-net) was used for density enhancement of point cloud data and to carry out three-dimensional reconstruction. The features of the crop point cloud were kept under the processing of reconstruction model; meanwhile, the density of the crop point cloud was quadrupled.},
DOI = {10.3390/rs12193208}
}



@Article{s20195630,
AUTHOR = {Xie, Jingyi and Peng, Xiaodong and Wang, Haijiao and Niu, Wenlong and Zheng, Xiao},
TITLE = {UAV Autonomous Tracking and Landing Based on Deep Reinforcement Learning Strategy},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5630},
URL = {https://www.mdpi.com/1424-8220/20/19/5630},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicle (UAV) autonomous tracking and landing is playing an increasingly important role in military and civil applications. In particular, machine learning has been successfully introduced to robotics-related tasks. A novel UAV autonomous tracking and landing approach based on a deep reinforcement learning strategy is presented in this paper, with the aim of dealing with the UAV motion control problem in an unpredictable and harsh environment. Instead of building a prior model and inferring the landing actions based on heuristic rules, a model-free method based on a partially observable Markov decision process (POMDP) is proposed. In the POMDP model, the UAV automatically learns the landing maneuver by an end-to-end neural network, which combines the Deep Deterministic Policy Gradients (DDPG) algorithm and heuristic rules. A Modular Open Robots Simulation Engine (MORSE)-based reinforcement learning framework is designed and validated with a continuous UAV tracking and landing task on a randomly moving platform in high sensor noise and intermittent measurements. The simulation results show that when the moving platform is moving in different trajectories, the average landing success rate of the proposed algorithm is about 10% higher than that of the Proportional-Integral-Derivative (PID) method. As an indirect result, a state-of-the-art deep reinforcement learning-based UAV control method is validated, where the UAV can learn the optimal strategy of a continuously autonomous landing and perform properly in a simulation environment.},
DOI = {10.3390/s20195630}
}



@Article{land9100368,
AUTHOR = {Esmali Ouri, Abazar and Golshan, Mohammad and Janizadeh, Saeid and Cerdà, Artemi and Melesse, Assefa M.},
TITLE = {Soil Erosion Susceptibility Mapping in Kozetopraghi Catchment, Iran: A Mixed Approach Using Rainfall Simulator and Data Mining Techniques},
JOURNAL = {Land},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {368},
URL = {https://www.mdpi.com/2073-445X/9/10/368},
ISSN = {2073-445X},
ABSTRACT = {Soil erosion determines landforms, soil formation and distribution, soil fertility, and land degradation processes. In arid and semiarid ecosystems, soil erosion is a key process to understand, foresee, and prevent desertification. Addressing soil erosion throughout watersheds scales requires basic information to develop soil erosion control strategies and to reduce land degradation. To assess and remediate the non-sustainable soil erosion rates, restoration programs benefit from the knowledge of the spatial distribution of the soil losses to develop maps of soil erosion. This study presents Support Vector Machine (SVM), Random Forest (RF), and adaptive boosting (AdaBoost) data mining models to map soil erosion susceptibility in Kozetopraghi watershed, Iran. A soil erosion inventory map was prepared from field rainfall simulation experiments on 174 randomly selected points along the Kozetopraghi watershed. In previous studies, this map has been prepared using indirect methods such as the Universal Soil Loss Equation to assess soil erosion. Direct field measurements for mapping soil erosion susceptibility have so far not been carried out in our study site in the past. The soil erosion rate data generated by simulated rainfall in 1 m2 plots at rainfall rate of 40 mmh&minus;1 was used to develop the soil erosion map. Of the available data, 70% and 30% were randomly classified to calibrate and validate the models, respectively. As a result, the RF model with the highest area under the curve (AUC) value in a receiver operating characteristics (ROC) curve (0.91), and the lowest mean square error (MSE) value (0.09), has the most concordance and spatial differentiation. Sensitivity analysis by Jackknife and IncNodePurity methods indicates that the slope angle is the most important factor within the soil erosion susceptibility map. The RF susceptibility map showed that the areas located in the center and near the watershed outlet have the most susceptibility to soil erosion. This information can be used to support the development of sustainable restoration plans with more accuracy. Our methodology has been evaluated and can be also applied in other regions.},
DOI = {10.3390/land9100368}
}



@Article{rs12193216,
AUTHOR = {Maimaitiyiming, Matthew and Sagan, Vasit and Sidike, Paheding and Maimaitijiang, Maitiniyazi and Miller, Allison J. and Kwasniewski, Misha},
TITLE = {Leveraging Very-High Spatial Resolution Hyperspectral and Thermal UAV Imageries for Characterizing Diurnal Indicators of Grapevine Physiology},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3216},
URL = {https://www.mdpi.com/2072-4292/12/19/3216},
ISSN = {2072-4292},
ABSTRACT = {Efficient and accurate methods to monitor crop physiological responses help growers better understand crop physiology and improve crop productivity. In recent years, developments in unmanned aerial vehicles (UAV) and sensor technology have enabled image acquisition at very-high spectral, spatial, and temporal resolutions. However, potential applications and limitations of very-high-resolution (VHR) hyperspectral and thermal UAV imaging for characterization of plant diurnal physiology remain largely unknown, due to issues related to shadow and canopy heterogeneity. In this study, we propose a canopy zone-weighting (CZW) method to leverage the potential of VHR (&le;9 cm) hyperspectral and thermal UAV imageries in estimating physiological indicators, such as stomatal conductance (Gs) and steady-state fluorescence (Fs). Diurnal flights and concurrent in-situ measurements were conducted during grapevine growing seasons in 2017 and 2018 in a vineyard in Missouri, USA. We used neural net classifier and the Canny edge detection method to extract pure vine canopy from the hyperspectral and thermal images, respectively. Then, the vine canopy was segmented into three canopy zones (sunlit, nadir, and shaded) using K-means clustering based on the canopy shadow fraction and canopy temperature. Common reflectance-based spectral indices, sun-induced chlorophyll fluorescence (SIF), and simplified canopy water stress index (siCWSI) were computed as image retrievals. Using the coefficient of determination (R2) established between the image retrievals from three canopy zones and the in-situ measurements as a weight factor, weighted image retrievals were calculated and their correlation with in-situ measurements was explored. The results showed that the most frequent and the highest correlations were found for Gs and Fs, with CZW-based Photochemical reflectance index (PRI), SIF, and siCWSI (PRICZW, SIFCZW, and siCWSICZW), respectively. When all flights combined for the given field campaign date, PRICZW, SIFCZW, and siCWSICZW significantly improved the relationship with Gs and Fs. The proposed approach takes full advantage of VHR hyperspectral and thermal UAV imageries, and suggests that the CZW method is simple yet effective in estimating Gs and Fs.},
DOI = {10.3390/rs12193216}
}



@Article{app10196945,
AUTHOR = {Yow, Kin-Choong and Kim, Insu},
TITLE = {General Moving Object Localization from a Single Flying Camera},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {6945},
URL = {https://www.mdpi.com/2076-3417/10/19/6945},
ISSN = {2076-3417},
ABSTRACT = {Object localization is an important task in the visual surveillance of scenes, and it has important applications in locating personnel and/or equipment in large open spaces such as a farm or a mine. Traditionally, object localization can be performed using the technique of stereo vision: using two fixed cameras for a moving object, or using a single moving camera for a stationary object. This research addresses the problem of determining the location of a moving object using only a single moving camera, and it does not make use of any prior information on the type of object nor the size of the object. Our technique makes use of a single camera mounted on a quadrotor drone, which flies in a specific pattern relative to the object in order to remove the depth ambiguity associated with their relative motion. In our previous work, we showed that with three images, we can recover the location of an object moving parallel to the direction of motion of the camera. In this research, we find that with four images, we can recover the location of an object moving linearly in an arbitrary direction. We evaluated our algorithm on over 70 image sequences of objects moving in various directions, and the results showed a much smaller depth error rate (less than 8.0% typically) than other state-of-the-art algorithms.},
DOI = {10.3390/app10196945}
}



@Article{rs12193233,
AUTHOR = {Meng, Ran and Lv, Zhengang and Yan, Jianbing and Chen, Gengshen and Zhao, Feng and Zeng, Linglin and Xu, Binyuan},
TITLE = {Development of Spectral Disease Indices for Southern Corn Rust Detection and Severity Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3233},
URL = {https://www.mdpi.com/2072-4292/12/19/3233},
ISSN = {2072-4292},
ABSTRACT = {Southern Corn Rust (SCR) is one of the most destructive diseases in corn production, significantly affecting corn quality and yields globally. Field-based fast, nondestructive diagnosis of SCR is critical for smart agriculture applications to reduce pesticide use and ensure food safety. The development of spectral disease indices (SDIs), based on in situ leaf reflectance spectra, has proven to be an effective method in detecting plant diseases in the field. However, little is known about leaf spectral signatures that can assist in the accurate diagnosis of SCR, and no SDIs-based model has been reported for the field-based SCR monitoring. Here, to address those issues, we developed SDIs-based monitoring models to detect SCR-infected leaves and classify SCR damage severity. In detail, we first collected in situ leaf reflectance spectra (350&ndash;2500 nm) of healthy and infected corn plants with three severity levels (light, medium, and severe) using a portable spectrometer. Then, the RELIEF-F algorithm was performed to select the most discriminative features (wavelengths) and two band normalized differences for developing SDIs (i.e., health index and severity index) in SCR detection and severity classification, respectively. The leaf reflectance spectra, most sensitive to SCR detection and severity classification, were found in the 572 nm, 766 nm, and 1445 nm wavelength and 575 nm, 640 nm, and 1670 nm wavelength, respectively. These spectral features were associated with leaf pigment and leaf water content. Finally, by employing a support vector machine (SVM), the performances of developed SCR-SDIs were assessed and compared with 38 stress-related vegetation indices (VIs) identified in the literature. The SDIs-based models developed in this study achieved an overall accuracy of 87% and 70% in SCR detection and severity classification, 1.1% and 8.3% higher than the other best VIs-based model under study, respectively. Our results thus suggest that the SCR-SDIs is a promising tool for fast, nondestructive diagnosis of SCR in the field over large areas. To our knowledge, this study represents one of the first few efforts to provide a theoretical basis for remote sensing of SCR at field and larger scales. With the increasing use of unmanned aerial vehicles (UAVs) with hyperspectral measurement capability, more studies should be conducted to expand our developed SCR-SDIs for SCR monitoring at different study sites and growing stages in the future.},
DOI = {10.3390/rs12193233}
}



@Article{electronics9101640,
AUTHOR = {Khan, Usman Ali and Lee, Sang Sun},
TITLE = {Distance-Based Resource Allocation for Vehicle-to-Pedestrian Safety Communication},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1640},
URL = {https://www.mdpi.com/2079-9292/9/10/1640},
ISSN = {2079-9292},
ABSTRACT = {Cellular Vehicle to Everything (V2X) has redefined the vehicular communication architecture as something that needs an ultra-reliable link, high capacity, and fast message delivery in vehicular networks. The V2X scenarios are broadly categorized as Vehicle to Vehicle (V2V), Vehicle to Infrastructure (V2I), Vehicle to Pedestrians (V2P), and Vehicle to Network (V2N). Vulnerable pedestrians belong to the V2P category and hence require an ultra-reliable link and a fast message delivery in case the moving vehicle is in the close proximity of the pedestrian. However, congestion in the network calls for an optimized resource allocation that would allow a fast and secure connection between a vehicle and the pedestrian. In this paper, we have proposed a distance-based resource allocation that classifies the pedestrians in different categories, performs a one-to-many weighted bipartite matching, and finally a reinforcement learning based power allocation.},
DOI = {10.3390/electronics9101640}
}



@Article{rs12193237,
AUTHOR = {Osco, Lucas Prado and Junior, José Marcato and Ramos, Ana Paula Marques and Furuya, Danielle Elis Garcia and Santana, Dthenifer Cordeiro and Teodoro, Larissa Pereira Ribeiro and Gonçalves, Wesley Nunes and Baio, Fábio Henrique Rojo and Pistori, Hemerson and Junior, Carlos Antonio da Silva and Teodoro, Paulo Eduardo},
TITLE = {Leaf Nitrogen Concentration and Plant Height Prediction for Maize Using UAV-Based Multispectral Imagery and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3237},
URL = {https://www.mdpi.com/2072-4292/12/19/3237},
ISSN = {2072-4292},
ABSTRACT = {Under ideal conditions of nitrogen (N), maize (Zea mays L.) can grow to its full potential, reaching maximum plant height (PH). As a rapid and nondestructive approach, the analysis of unmanned aerial vehicles (UAV)-based imagery may be of assistance to estimate N and height. The main objective of this study is to present an approach to predict leaf nitrogen concentration (LNC, g kg&minus;1) and PH (m) with machine learning techniques and UAV-based multispectral imagery in maize plants. An experiment with 11 maize cultivars under two rates of N fertilization was carried during the 2017/2018 and 2018/2019 crop seasons. The spectral vegetation indices (VI) normalized difference vegetation index (NDVI), normalized difference red-edge index (NDRE), green normalized difference vegetation (GNDVI), and the soil adjusted vegetation index (SAVI) were extracted from the images and, in a computational system, used alongside the spectral bands as input parameters for different machine learning models. A randomized 10-fold cross-validation strategy, with a total of 100 replicates, was used to evaluate the performance of 9 supervised machine learning (ML) models using the Pearson&rsquo;s correlation coefficient (r), mean absolute error (MAE), coefficient of regression (R&sup2;), and root mean square error (RMSE) metrics. The results indicated that the random forest (RF) algorithm performed better, with r and RMSE, respectively, of 0.91 and 1.9 g.kg&minus;&sup1; for LNC, and 0.86 and 0.17 m for PH. It was also demonstrated that VIs contributed more to the algorithm&rsquo;s performances than individual spectral bands. This study concludes that the RF model is appropriate to predict both agronomic variables in maize and may help farmers to monitor their plants based upon their LNC and PH diagnosis and use this knowledge to improve their production rates in the subsequent seasons.},
DOI = {10.3390/rs12193237}
}



@Article{rs12193265,
AUTHOR = {Sonobe, Rei and Yamashita, Hiroto and Mihara, Harumi and Morita, Akio and Ikka, Takashi},
TITLE = {Estimation of Leaf Chlorophyll a, b and Carotenoid Contents and Their Ratios Using Hyperspectral Reflectance},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3265},
URL = {https://www.mdpi.com/2072-4292/12/19/3265},
ISSN = {2072-4292},
ABSTRACT = {Japanese horseradish (wasabi) grows in very specific conditions, and recent environmental climate changes have damaged wasabi production. In addition, the optimal culture methods are not well known, and it is becoming increasingly difficult for incipient farmers to cultivate it. Chlorophyll a, b and carotenoid contents, as well as their allocation, could be an adequate indicator in evaluating its production and environmental stress; thus, developing an in situ method to monitor photosynthetic pigments based on reflectance could be useful for agricultural management. Besides original reflectance (OR), five pre-processing techniques, namely, first derivative reflectance (FDR), continuum-removed (CR), de-trending (DT), multiplicative scatter correction (MSC), and standard normal variate transformation (SNV), were compared to assess the accuracy of the estimation. Furthermore, five machine learning algorithms&mdash;random forest (RF), support vector machine (SVM), kernel-based extreme learning machine (KELM), Cubist, and Stochastic Gradient Boosting (SGB)&mdash;were considered. To classify the samples under different pH or sulphur ion concentration conditions, the end of the red edge bands was effective for OR, FDR, DT, MSC, and SNV, while a green-peak band was effective for CR. Overall, KELM and Cubist showed high performance and incorporating pre-processing techniques was effective for obtaining estimated values with high accuracy. The best combinations were found to be DT&ndash;KELM for chl a (RPD = 1.511&ndash;5.17, RMSE = 1.23&ndash;3.62 &mu;g cm&minus;2) and chl a:b (RPD = 0.73&ndash;3.17, RMSE = 0.13&ndash;0.60); CR&ndash;KELM for chl b (RPD = 1.92&ndash;5.06, RMSE = 0.41&ndash;1.03 &mu;g cm&minus;2) and chl a:car (RPD = 1.31&ndash;3.23, RMSE = 0.26&ndash;0.50); SNV&ndash;Cubist for car (RPD = 1.63&ndash;3.32, RMSE = 0.31&ndash;1.89 &mu;g cm&minus;2); and DT&ndash;Cubist for chl:car (RPD = 1.53&ndash;3.96, RMSE = 0.27&ndash;0.74).},
DOI = {10.3390/rs12193265}
}



@Article{ijgi9100595,
AUTHOR = {Wang, Yongjun and Jiang, Tengping and Liu, Jing and Li, Xiaorui and Liang, Chong},
TITLE = {Hierarchical Instance Recognition of Individual Roadside Trees in Environmentally Complex Urban Areas from UAV Laser Scanning Point Clouds},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {595},
URL = {https://www.mdpi.com/2220-9964/9/10/595},
ISSN = {2220-9964},
ABSTRACT = {Individual tree segmentation is essential for many applications in city management and urban ecology. Light Detection and Ranging (LiDAR) system acquires accurate point clouds in a fast and environmentally-friendly manner, which enables single tree detection. However, the large number of object categories and occlusion from nearby objects in complex environment pose great challenges in urban tree inventory, resulting in omission or commission errors. Therefore, this paper addresses these challenges and increases the accuracy of individual tree segmentation by proposing an automated method for instance recognition urban roadside trees. The proposed algorithm was implemented of unmanned aerial vehicles laser scanning (UAV-LS) data. First, an improved filtering algorithm was developed to identify ground and non-ground points. Second, we extracted tree-like objects via labeling on non-ground points using a deep learning model with a few smaller modifications. Unlike only concentrating on the global features in previous method, the proposed method revises a pointwise semantic learning network to capture both the global and local information at multiple scales, significantly avoiding the information loss in local neighborhoods and reducing useless convolutional computations. Afterwards, the semantic representation is fed into a graph-structured optimization model, which obtains globally optimal classification results by constructing a weighted indirect graph and solving the optimization problem with graph-cuts. The segmented tree points were extracted and consolidated through a series of operations, and they were finally recognized by combining graph embedding learning with a structure-aware loss function and a supervoxel-based normalized cut segmentation method. Experimental results on two public datasets demonstrated that our framework achieved better performance in terms of classification accuracy and recognition ratio of tree.},
DOI = {10.3390/ijgi9100595}
}



@Article{rs12203305,
AUTHOR = {Kerkech, Mohamed and Hafiane, Adel and Canals, Raphael},
TITLE = {VddNet: Vine Disease Detection Network Based on Multispectral Images and Depth Map},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3305},
URL = {https://www.mdpi.com/2072-4292/12/20/3305},
ISSN = {2072-4292},
ABSTRACT = {Vine pathologies generate several economic and environmental problems, causing serious difficulties for the viticultural activity. The early detection of vine disease can significantly improve the control of vine diseases and avoid spread of virus or fungi. Currently, remote sensing and artificial intelligence technologies are emerging in the field of precision agriculture. They offer interesting potential for crop disease management. However, despite the advances in these technologies, particularly deep learning technologies, many problems still present considerable challenges, such as semantic segmentation of images for disease mapping. In this paper, we present a new deep learning architecture called Vine Disease Detection Network (VddNet). It is based on three parallel auto-encoders integrating different information (i.e., visible, infrared and depth). Then, the decoder reconstructs and retrieves the features, and assigns a class to each output pixel. An orthophotos registration method is also proposed to align the three types of images and enable the processing by VddNet. The proposed architecture is assessed by comparing it with the most known architectures: SegNet, U-Net, DeepLabv3+ and PSPNet. The deep learning architectures were trained on multispectral data from an unmanned aerial vehicle (UAV) and depth map information extracted from 3D processing. The results of the proposed architecture show that the VddNet architecture achieves higher scores than the baseline methods. Moreover, this study demonstrates that the proposed method has many advantages compared to methods that directly use the UAV images.},
DOI = {10.3390/rs12203305}
}



@Article{s20205762,
AUTHOR = {Santos, André A. and Rocha, Filipe A. S. and Reis, Agnaldo J. da R. and Guimarães, Frederico G.},
TITLE = {Automatic System for Visual Detection of Dirt Buildup on Conveyor Belts Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5762},
URL = {https://www.mdpi.com/1424-8220/20/20/5762},
ISSN = {1424-8220},
ABSTRACT = {Conveyor belts are the most widespread means of transportation for large quantities of materials in the mining sector. Therefore, autonomous methods that can help human beings to perform the inspection of the belt conveyor system is a major concern for companies. In this context, we present in this work a novel and automatic visual detector that recognizes dirt buildup on the structures of conveyor belts, which is one of the tasks of the maintenance inspectors. This visual detector can be embedded as sensors in autonomous robots for the inspection activity. The proposed system involves training a convolutional neural network from RGB images. The use of the transfer learning technique, i.e., retraining consolidated networks for image classification with our collected images has shown very effective. Two different approaches for transfer learning have been analyzed. The best one presented an average accuracy of 0.8975 with an F-1 Score of 0.8773 for the dirt recognition. A field validation experiment served to evaluate the performance of the proposed system in a real time classification task.},
DOI = {10.3390/s20205762}
}



@Article{rs12203318,
AUTHOR = {Na, Jiaming and Xue, Kaikai and Xiong, Liyang and Tang, Guoan and Ding, Hu and Strobl, Josef and Pfeifer, Norbert},
TITLE = {UAV-Based Terrain Modeling under Vegetation in the Chinese Loess Plateau: A Deep Learning and Terrain Correction Ensemble Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3318},
URL = {https://www.mdpi.com/2072-4292/12/20/3318},
ISSN = {2072-4292},
ABSTRACT = {Accurate topographic mapping is a critical task for various environmental applications because elevation affects hydrodynamics and vegetation distributions. UAV photogrammetry is popular in terrain modelling because of its lower cost compared to laser scanning. However, this method is restricted in vegetation area with a complex terrain, due to reduced ground visibility and lack of robust and automatic filtering algorithms. To solve this problem, this work proposed an ensemble method of deep learning and terrain correction. First, image matching point cloud was generated by UAV photogrammetry. Second, vegetation points were identified based on U-net deep learning network. After that, ground elevation was corrected by estimating vegetation height to generate the digital terrain model (DTM). Two scenarios, namely, discrete and continuous vegetation areas were considered. The vegetation points in the discrete area were directly removed and then interpolated, and terrain correction was applied for the points in the continuous areas. Case studies were conducted in three different landforms in the loess plateau of China, and accuracy assessment indicated that the overall accuracy of vegetation detection was 95.0%, and the MSE (Mean Square Error) of final DTM (Digital Terrain Model) was 0.024 m.},
DOI = {10.3390/rs12203318}
}



@Article{app10207120,
AUTHOR = {Mohammed, Thaha and Albeshri, Aiiad and Katib, Iyad and Mehmood, Rashid},
TITLE = {UbiPriSEQ—Deep Reinforcement Learning to Manage Privacy, Security, Energy, and QoS in 5G IoT HetNets},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7120},
URL = {https://www.mdpi.com/2076-3417/10/20/7120},
ISSN = {2076-3417},
ABSTRACT = {5G networks and Internet of Things (IoT) offer a powerful platform for ubiquitous environments with their ubiquitous sensing, high speeds and other benefits. The data, analytics, and other computations need to be optimally moved and placed in these environments, dynamically, such that energy-efficiency and QoS demands are best satisfied. A particular challenge in this context is to preserve privacy and security while delivering quality of service (QoS) and energy-efficiency. Many works have tried to address these challenges but without a focus on optimizing all of them and assuming fixed models of environments and security threats. This paper proposes the UbiPriSEQ framework that uses Deep Reinforcement Learning (DRL) to adaptively, dynamically, and holistically optimize QoS, energy-efficiency, security, and privacy. UbiPriSEQ is built on a three-layered model and comprises two modules. UbiPriSEQ devises policies and makes decisions related to important parameters including local processing and offloading rates for data and computations, radio channel states, transmit power, task priority, and selection of fog nodes for offloading, data migration, and so forth. UbiPriSEQ is implemented in Python over the TensorFlow platform and is evaluated using a real-life application in terms of SINR, privacy metric, latency, and utility function, manifesting great promise.},
DOI = {10.3390/app10207120}
}



@Article{s20205805,
AUTHOR = {Ai, Tianfu and Xu, Bin and Xiang, Changle and Fan, Wei and Zhang, Yibo},
TITLE = {Modeling of a Novel Coaxial Ducted Fan Aerial Robot Combined with Corner Environment by Using Artificial Neural Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5805},
URL = {https://www.mdpi.com/1424-8220/20/20/5805},
ISSN = {1424-8220},
ABSTRACT = {A novel coaxial ducted fan aerial robot with a manipulator is proposed which can achieve some hover operation tasks in a corner environment, such as switching on and off a wall-attached button on the corner. In order to study the aerodynamic interference between the prototype and the environment when the aerial robot is hovering in the corner environment, a method for the comprehensive modeling of the prototype and corner environment based on the artificial neural network is presented. By using the CFD simulation software, the flow field of the prototype at different positions with the corner effect is analyzed. After determining the input, output and structure of the neural network model, the Adam and gradient descent algorithms are selected as the neural network training algorithms, respectively. In addition, to optimize the initial weights and biases of the neural network model, the genetic algorithm is precisely used. The three-dimensional prediction surfaces generated by the three methods of the neural network, kriging surface and the polynomial fitting are compared. The results show that the neural network has high prediction accuracy, and can be applied to the comprehensive modeling of the prototype and the corner environment.},
DOI = {10.3390/s20205805}
}



@Article{agriculture10100475,
AUTHOR = {Abraham, Emerson Rodolfo and Mendes dos Reis, João Gilberto and Vendrametto, Oduvaldo and Oliveira Costa Neto, Pedro Luiz de and Carlo Toloi, Rodrigo and Souza, Aguinaldo Eduardo de and Oliveira Morais, Marcos de},
TITLE = {Time Series Prediction with Artificial Neural Networks: An Analysis Using Brazilian Soybean Production},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {475},
URL = {https://www.mdpi.com/2077-0472/10/10/475},
ISSN = {2077-0472},
ABSTRACT = {Food production to meet human demand has been a challenge to society. Nowadays, one of the main sources of feeding is soybean. Considering agriculture food crops, soybean is sixth by production volume and the fourth by both production area and economic value. The grain can be used directly to human consumption, but it is highly used as a source of protein for animal production that corresponds 75% of the total, or as oil and derived food products. Brazil and the US are the most important players responsible for more than 70% of world production. Therefore, a reliable forecasting is essential for decision-makers to plan adequate policies to this important commodity and to establish the necessary logistical resources. In this sense, this study aims to predict soybean harvest area, yield, and production using Artificial Neural Networks (ANN) and compare with classical methods of Time Series Analysis. To this end, we collected data from a time series (1961&ndash;2016) regarding soybean production in Brazil. The results reveal that ANN is the best approach to predict soybean harvest area and production while classical linear function remains more effective to predict soybean yield. Moreover, ANN presents as a reliable model to predict time series and can help the stakeholders to anticipate the world soybean offer.},
DOI = {10.3390/agriculture10100475}
}



@Article{rs12203364,
AUTHOR = {Collins, Adam M. and Brodie, Katherine L. and Bak, Andrew Spicer and Hesser, Tyler J. and Farthing, Matthew W. and Lee, Jonghyun and Long, Joseph W.},
TITLE = {Bathymetric Inversion and Uncertainty Estimation from Synthetic Surf-Zone Imagery with Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3364},
URL = {https://www.mdpi.com/2072-4292/12/20/3364},
ISSN = {2072-4292},
ABSTRACT = {Resolving surf-zone bathymetry from high-resolution imagery typically involves measuring wave speeds and performing a physics-based inversion process using linear wave theory, or data assimilation techniques which combine multiple remotely sensed parameters with numerical models. In this work, we explored what types of coastal imagery can be best utilized in a 2-dimensional fully convolutional neural network to directly estimate nearshore bathymetry from optical expressions of wave kinematics. Specifically, we explored utilizing time-averaged images (timex) of the surf-zone, which can be used as a proxy for wave dissipation, as well as including a single-frame image input, which has visible patterns of wave refraction and instantaneous expressions of wave breaking. Our results show both types of imagery can be used to estimate nearshore bathymetry. However, the single-frame imagery provides more complete information across the domain, decreasing the error over the test set by approximately 10% relative to using timex imagery alone. A network incorporating both inputs had the best performance, with an overall root-mean-squared-error of 0.39 m. Activation maps demonstrate the additional information provided by the single-frame imagery in non-breaking wave areas which aid in prediction. Uncertainty in model predictions is explored through three techniques (Monte Carlo (MC) dropout, infer-transformation, and infer-noise) to provide additional actionable information about the spatial reliability of each bathymetric prediction.},
DOI = {10.3390/rs12203364}
}



@Article{s20205836,
AUTHOR = {Bae, Ji Yong and Choi, Won and Hong, Suk-Ju and Kim, Sangyeon and Kim, Eungchan and Lee, Chang-Hyup and Han, Yun-hyeok and Hur, Hwan and Lee, Kye-Sung and Chang, Ki Soo and Kim, Geon-Hee and Kim, Ghiseok},
TITLE = {Design, Fabrication, and Performance Evaluation of Portable and Large-Area Blackbody System},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5836},
URL = {https://www.mdpi.com/1424-8220/20/20/5836},
ISSN = {1424-8220},
ABSTRACT = {In this study, a portable and large-area blackbody system was developed following a series of processes including design, computational analysis, fabrication, and experimental analysis and evaluation. The blackbody system was designed to be lightweight (5 kg), and its temperature could exceed the ambient temperature by up to 15 &deg;C under operation. A carbon-fiber-based heat source was used to achieve a uniform temperature distribution. A heat shield fabricated from an insulation material was embedded at the opposite side of the heating element to minimize heat loss. A prototype of the blackbody system was fabricated based on the design and transient coupled electro-thermal simulation results. The operation performance of this system, such as the thermal response, signal transfer function, and noise equivalent temperature difference, was evaluated by employing an infrared imaging system. In addition, emissivity was measured during operation. The results of this study show that the developed portable and large-area blackbody system can be expected to serve as a reliable reference source for the calibration of aerial infrared images for the application of aerial infrared techniques to remote sensing.},
DOI = {10.3390/s20205836}
}



@Article{computers9040084,
AUTHOR = {Doumbia, Mamadou and Cheng, Xu},
TITLE = {State Estimation and Localization Based on Sensor Fusion for Autonomous Robots in Indoor Environment},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {84},
URL = {https://www.mdpi.com/2073-431X/9/4/84},
ISSN = {2073-431X},
ABSTRACT = {Currently, almost all robot state estimation and localization systems are based on the Kalman filter (KF) and its derived methods, in particular the unscented Kalman filter (UKF). When applying the UKF alone, the estimate of the state is not sufficiently precise. In this paper, a new hierarchical infrared navigational algorithm hybridization (HIRNAH) system is developed to provide better state estimation and localization for mobile robots. Two navigation subsystems (inertial navigation system (INS) and, using a novel infrared navigation algorithm (NIRNA), Odom-NIRNA) and an RPLIDAR-A3 scanner cooperation to build HIRNAH. The robot pose (position and orientation) errors are estimated by a system filtering module (SFM) and used to smooth the robot&rsquo;s final poses. A prototype (two rotary encoders, one smartphone-based robot sensing model and one RPLIDAR-A3 scanner) has been built and mounted on a four-wheeled mobile robot (4-WMR). Simulation results have motivated real-life experiments, and obtained results are compared to some existent research (hardware and control technology navigation (HCTNav), rapid exploring random tree (RRT) and in stand-alone mode (INS)) for performance measurements. The experimental results confirm that HIRNAH presents a more accurate estimation and a lower mean square error (MSE) of the robot&rsquo;s state than those calculated by the previously cited HCTNav, RRT and INS.},
DOI = {10.3390/computers9040084}
}



@Article{app10207263,
AUTHOR = {Lee, Yong-Hyeok and Jang, Dong-Won and Kim, Jae-Bin and Park, Rae-Hong and Park, Hyung-Min},
TITLE = {Audio–Visual Speech Recognition Based on Dual Cross-Modality Attentions with the Transformer Model},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7263},
URL = {https://www.mdpi.com/2076-3417/10/20/7263},
ISSN = {2076-3417},
ABSTRACT = {Since attention mechanism was introduced in neural machine translation, attention has been combined with the long short-term memory (LSTM) or replaced the LSTM in a transformer model to overcome the sequence-to-sequence (seq2seq) problems with the LSTM. In contrast to the neural machine translation, audio&ndash;visual speech recognition (AVSR) may provide improved performance by learning the correlation between audio and visual modalities. As a result that the audio has richer information than the video related to lips, AVSR is hard to train attentions with balanced modalities. In order to increase the role of visual modality to a level of audio modality by fully exploiting input information in learning attentions, we propose a dual cross-modality (DCM) attention scheme that utilizes both an audio context vector using video query and a video context vector using audio query. Furthermore, we introduce a connectionist-temporal-classification (CTC) loss in combination with our attention-based model to force monotonic alignments required in AVSR. Recognition experiments on LRS2-BBC and LRS3-TED datasets showed that the proposed model with the DCM attention scheme and the hybrid CTC/attention architecture achieved at least a relative improvement of 7.3% on average in the word error rate (WER) compared to competing methods based on the transformer model.},
DOI = {10.3390/app10207263}
}



@Article{rs12203416,
AUTHOR = {Temitope Yekeen, Shamsudeen and Balogun, Abdul-Lateef},
TITLE = {Advances in Remote Sensing Technology, Machine Learning and Deep Learning for Marine Oil Spill Detection, Prediction and Vulnerability Assessment},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3416},
URL = {https://www.mdpi.com/2072-4292/12/20/3416},
ISSN = {2072-4292},
ABSTRACT = {Although advancements in remote sensing technology have facilitated quick capture and identification of the source and location of oil spills in water bodies, the presence of other biogenic elements (lookalikes) with similar visual attributes hinder rapid detection and prompt decision making for emergency response. To date, different methods have been applied to distinguish oil spills from lookalikes with limited success. In addition, accurately modeling the trajectory of oil spills remains a challenge. Thus, we aim to provide further insights on the multi-faceted problem by undertaking a holistic review of past and current approaches to marine oil spill disaster reduction as well as explore the potentials of emerging digital trends in minimizing oil spill hazards. The scope of previous reviews is extended by covering the inter-related dimensions of detection, discrimination, and trajectory prediction of oil spills for vulnerability assessment. Findings show that both optical and microwave airborne and satellite remote sensors are used for oil spill monitoring with microwave sensors being more widely used due to their ability to operate under any weather condition. However, the accuracy of both sensors is affected by the presence of biogenic elements, leading to false positive depiction of oil spills. Statistical image segmentation has been widely used to discriminate lookalikes from oil spills with varying levels of accuracy but the emergence of digitalization technologies in the fourth industrial revolution (IR 4.0) is enabling the use of Machine learning (ML) and deep learning (DL) models, which are more promising than the statistical methods. The Support Vector Machine (SVM) and Artificial Neural Network (ANN) are the most used machine learning algorithms for oil spill detection, although the restriction of ML models to feed forward image classification without support for the end-to-end trainable framework limits its accuracy. On the other hand, deep learning models&rsquo; strong feature extraction and autonomous learning capability enhance their detection accuracy. Also, mathematical models based on lagrangian method have improved oil spill trajectory prediction with higher real time accuracy than the conventional worst case, average and survey-based approaches. However, these newer models are unable to quantify oil droplets and uncertainty in vulnerability prediction. Considering that there is yet no single best remote sensing technique for unambiguous detection and discrimination of oil spills and lookalikes, it is imperative to advance research in the field in order to improve existing technology and develop specialized sensors for accurate oil spill detection and enhanced classification, leveraging emerging geospatial computer vision initiatives.},
DOI = {10.3390/rs12203416}
}



@Article{electronics9101714,
AUTHOR = {Park, JiWoong and Nam, SungChan and Choi, HongBeom and Ko, YoungEun and Ko, Young-Bae},
TITLE = {Improving Deep Learning-Based UWB LOS/NLOS Identification with Transfer Learning: An Empirical Approach},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1714},
URL = {https://www.mdpi.com/2079-9292/9/10/1714},
ISSN = {2079-9292},
ABSTRACT = {This paper presents an improved ultra-wideband (UWB) line of sight (LOS)/non-line of sight (NLOS) identification scheme based on a hybrid method of deep learning and transfer learning. Previous studies have limitations, in that the classification accuracy significantly decreases in an unknown place. To solve this problem, we propose a transfer learning-based NLOS identification method for classifying the NLOS conditions of the UWB signal in an unmeasured environment. Both the multilayer perceptron and convolutional neural network (CNN) are introduced as classifiers for NLOS conditions. We evaluate the proposed scheme by conducting experiments in both measured and unmeasured environments. Channel data were measured using a Decawave EVK1000 in two similar indoor office environments. In the unmeasured environment, the existing CNN method showed an accuracy of approximately 44%, but when the proposed scheme was applied to the CNN, it showed an accuracy of up to 98%. The training time of the proposed scheme was measured to be approximately 48 times faster than that of the existing CNN. When comparing the proposed scheme with learning a new CNN in an unmeasured environment, the proposed scheme demonstrated an approximately 10% higher accuracy and approximately five times faster training time.},
DOI = {10.3390/electronics9101714}
}



@Article{s20205904,
AUTHOR = {Digulescu, Angela and Despina-Stoian, Cristina and Stănescu, Denis and Popescu, Florin and Enache, Florin and Ioana, Cornel and Rădoi, Emanuel and Rîncu, Iulian and Șerbănescu, Alexandru},
TITLE = {New Approach of UAV Movement Detection and Characterization Using Advanced Signal Processing Methods Based on UWB Sensing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5904},
URL = {https://www.mdpi.com/1424-8220/20/20/5904},
ISSN = {1424-8220},
ABSTRACT = {In the last years, the commercial drone/unmanned aerial vehicles market has grown due to their technological performances (provided by the multiple onboard available sensors), low price, and ease of use. Being very attractive for an increasing number of applications, their presence represents a major issue for public or classified areas with a special status, because of the rising number of incidents. Our paper proposes a new approach for the drone movement detection and characterization based on the ultra-wide band (UWB) sensing system and advanced signal processing methods. This approach characterizes the movement of the drone using classical methods such as correlation, envelope detection, time-scale analysis, but also a new method, the recurrence plot analysis. The obtained results are compared in terms of movement map accuracy and required computation time in order to offer a future starting point for the drone intrusion detection.},
DOI = {10.3390/s20205904}
}



@Article{e22101182,
AUTHOR = {Xiao, Yu and Deng, Zhenghong and Wu, Tao},
TITLE = {Information–Theoretic Radar Waveform Design under the SINR Constraint},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1182},
URL = {https://www.mdpi.com/1099-4300/22/10/1182},
ISSN = {1099-4300},
ABSTRACT = {This study investigates the information&ndash;theoretic waveform design problem to improve radar performance in the presence of signal-dependent clutter environments. The goal was to study the waveform energy allocation strategies and provide guidance for radar waveform design through the trade-off relationship between the information theory criterion and the signal-to-interference-plus-noise ratio (SINR) criterion. To this end, a model of the constraint relationship among the mutual information (MI), the Kullback&ndash;Leibler divergence (KLD), and the SINR is established in the frequency domain. The effects of the SINR value range on maximizing the MI and KLD under the energy constraint are derived. Under the constraints of energy and the SINR, the optimal radar waveform method based on maximizing the MI is proposed for radar estimation, with another method based on maximizing the KLD proposed for radar detection. The maximum MI value range is bounded by SINR and the maximum KLD value range is between 0 and the Jenson&ndash;Shannon divergence (J-divergence) value. Simulation results show that under the SINR constraint, the MI-based optimal signal waveform can make full use of the transmitted energy to target information extraction and put the signal energy in the frequency bin where the target spectrum is larger than the clutter spectrum. The KLD-based optimal signal waveform can therefore make full use of the transmitted energy to detect the target and put the signal energy in the frequency bin with the maximum target spectrum.},
DOI = {10.3390/e22101182}
}



@Article{s20205940,
AUTHOR = {Klaer, Peter and Huang, Andi and Sévigny, Pascale and Rajan, Sreeraman and Pant, Shashank and Patnaik, Prakash and Balaji, Bhashyam},
TITLE = {An Investigation of Rotary Drone HERM Line Spectrum under Manoeuvering Conditions},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5940},
URL = {https://www.mdpi.com/1424-8220/20/20/5940},
ISSN = {1424-8220},
ABSTRACT = {Detecting and identifying drones is of great interest due to the proliferation of highly manoeuverable drones with on-board sensors of increasing sensing capabilities. In this paper, we investigate the use of radars for tackling this problem. In particular, we focus on the problem of detecting rotary drones and distinguishing between single-propeller and multi-propeller drones using a micro-Doppler analysis. Two different radars were used, an ultra wideband (UWB) continuous wave (CW) C-band radar and an automotive frequency modulated continuous wave (FMCW) W-band radar, to collect micro-Doppler signatures of the drones. By taking a closer look at HElicopter Rotor Modulation (HERM) lines, the spool and chopping lines are identified for the first time in the context of drones to determine the number of propeller blades. Furthermore, a new multi-frequency analysis method using HERM lines is developed, which allows the detection of propeller rotation rates (spool and chopping frequencies) of single and multi-propeller drones. Therefore, the presented method is a promising technique to aid in the classification of drones.},
DOI = {10.3390/s20205940}
}



@Article{rs12213471,
AUTHOR = {Dado, Walter T. and Deines, Jillian M. and Patel, Rinkal and Liang, Sang-Zi and Lobell, David B.},
TITLE = {High-Resolution Soybean Yield Mapping Across the US Midwest Using Subfield Harvester Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3471},
URL = {https://www.mdpi.com/2072-4292/12/21/3471},
ISSN = {2072-4292},
ABSTRACT = {Cloud computing and freely available, high-resolution satellite data have enabled recent progress in crop yield mapping at fine scales. However, extensive validation data at a matching resolution remain uncommon or infeasible due to data availability. This has limited the ability to evaluate different yield estimation models and improve understanding of key features useful for yield estimation in both data-rich and data-poor contexts. Here, we assess machine learning models&rsquo; capacity for soybean yield prediction using a unique ground-truth dataset of high-resolution (5 m) yield maps generated from combine harvester yield monitor data for over a million field-year observations across the Midwestern United States from 2008 to 2018. First, we compare random forest (RF) implementations, testing a range of feature engineering approaches using Sentinel-2 and Landsat spectral data for 20- and 30-m scale yield prediction. We find that Sentinel-2-based models can explain up to 45% of out-of-sample yield variability from 2017 to 2018 (r2 = 0.45), while Landsat models explain up to 43% across the longer 2008&ndash;2018 period. Using discrete Fourier transforms, or harmonic regressions, to capture soybean phenology improved the Landsat-based model considerably. Second, we compare RF models trained using this ground-truth data to models trained on available county-level statistics. We find that county-level models rely more heavily on just a few predictors, namely August weather covariates (vapor pressure deficit, rainfall, temperature) and July and August near-infrared observations. As a result, county-scale models perform relatively poorly on field-scale validation (r2 = 0.32), especially for high-yielding fields, but perform similarly to field-scale models when evaluated at the county scale (r2 = 0.82). Finally, we test whether our findings on variable importance can inform a simple, generalizable framework for regions or time periods beyond ground data availability. To do so, we test improvements to a Scalable Crop Yield Mapper (SCYM) approach that uses crop simulations to train statistical models for yield estimation. Based on findings from our RF models, we employ harmonic regressions to estimate peak vegetation index (VI) and a VI observation 30 days later, with August rainfall as the sole weather covariate in our new SCYM model. Modifications improved SCYM&rsquo;s explained variance (r2 = 0.27 at the 30 m scale) and provide a new, parsimonious model.},
DOI = {10.3390/rs12213471}
}



@Article{rs12213474,
AUTHOR = {Wang, Chunsheng and Chang, Lili and Zhao, Lingran and Niu, Ruiqing},
TITLE = {Automatic Identification and Dynamic Monitoring of Open-Pit Mines Based on Improved Mask R-CNN and Transfer Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3474},
URL = {https://www.mdpi.com/2072-4292/12/21/3474},
ISSN = {2072-4292},
ABSTRACT = {As the ecological problems caused by mine development become increasingly prominent, the conflict between mining activity and environmental protection is gradually intensifying. There is an urgent problem regarding how to effectively monitor mineral exploitation activities. In order to automatic identify and dynamically monitor open-pit mines of Hubei Province, an open-pit mine extraction model based on Improved Mask R-CNN (Region Convolutional Neural Network) and Transfer learning (IMRT) is proposed, a set of multi-source open-pit mine sample databases consisting of Gaofen-1, Gaofen-2 and Google Earth satellite images with a resolution of two meters is constructed, and an automatic batch production process of open-pit mine targets is designed. In this paper, pixel-based evaluation indexes and object-based evaluation indexes are used to compare the recognition effect of IMRT, faster R-CNN, Maximum Likelihood (MLE) and Support Vector Machine (SVM). The IMRT model has the best performance in Pixel Accuracy (PA), Kappa and MissingAlarm, with values of 0.9718, 0.8251 and 0.0862, respectively, which shows that the IMRT model has a better effect on open-pit mine automatic identification, and the results are also used as evaluation units of the environmental damages of the mines. The evaluation results show that level Ⅰ (serious) land occupation and destruction of key mining areas account for 34.62%, and 36.2% of topographical landscape damage approached level I. This study has great practical significance in terms of realizing the coordinated development of mines and ecological environments.},
DOI = {10.3390/rs12213474}
}



@Article{s20216051,
AUTHOR = {Garg, Piyush and Nasimi, Roya and Ozdagli, Ali and Zhang, Su and Mascarenas, David Dennis Lee and Reda Taha, Mahmoud and Moreu, Fernando},
TITLE = {Measuring Transverse Displacements Using Unmanned Aerial Systems Laser Doppler Vibrometer (UAS-LDV): Development and Field Validation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6051},
URL = {https://www.mdpi.com/1424-8220/20/21/6051},
ISSN = {1424-8220},
ABSTRACT = {Measurement of bridge displacements is important for ensuring the safe operation of railway bridges. Traditionally, contact sensors such as Linear Variable Displacement Transducers (LVDT) and accelerometers have been used to measure the displacement of the railway bridges. However, these sensors need significant effort in installation and maintenance. Therefore, railroad management agencies are interested in new means to measure bridge displacements. This research focuses on mounting Laser Doppler Vibrometer (LDV) on an Unmanned Aerial System (UAS) to enable contact-free transverse dynamic displacement of railroad bridges. Researchers conducted three field tests by flying the Unmanned Aerial Systems Laser Doppler Vibrometer (UAS-LDV) 1.5 m away from the ground and measured the displacement of a moving target at various distances. The accuracy of the UAS-LDV measurements was compared to the Linear Variable Differential Transducer (LVDT) measurements. The results of the three field tests showed that the proposed system could measure non-contact, reference-free dynamic displacement with an average peak and root mean square (RMS) error for the three experiments of 10% and 8% compared to LVDT, respectively. Such errors are acceptable for field measurements in railroads, as the interest prior to bridge monitoring implementation of a new approach is to demonstrate similar success for different flights, as reported in the three results. This study also identified barriers for industrial adoption of this technology and proposed operational development practices for both technical and cost-effective implementation.},
DOI = {10.3390/s20216051}
}



@Article{rs12213504,
AUTHOR = {Li, Xingrong and Yang, Chenghai and Huang, Wenjiang and Tang, Jia and Tian, Yanqin and Zhang, Qing},
TITLE = {Identification of Cotton Root Rot by Multifeature Selection from Sentinel-2 Images Using Random Forest},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3504},
URL = {https://www.mdpi.com/2072-4292/12/21/3504},
ISSN = {2072-4292},
ABSTRACT = {Cotton root rot is a destructive cotton disease and significantly affects cotton quality and yield, and accurate identification of its distribution within fields is critical for cotton growers to control the disease effectively. In this study, Sentinel-2 images were used to explore the feasibility of creating classification maps and prescription maps for site-specific fungicide application. Eight cotton fields with different levels of root rot were selected and random forest (RF) was used to identify the optimal spectral indices and texture features of the Sentinel-2 images. Five optimal spectral indices (plant senescence reflectance index (PSRI), normalized difference vegetation index (NDVI), normalized difference water index (NDWI1), moisture stressed index (MSI), and renormalized difference vegetation index (RDVI)) and seven optimal texture features (Contrast 1, Dissimilarity 1, Entory 2, Mean 1, Variance 1, Homogeneity 1, and Second moment 2) were identified. Three binary logistic regression (BLR) models, including a spectral model, a texture model, and a spectral-texture model, were constructed for cotton root rot classification and prescription map creation. The results were compared with classification maps and prescription maps based on airborne imagery. Accuracy assessment showed that the accuracies of the classification maps for the spectral, texture, and spectral-texture models were 92.95%, 84.81%, and 91.87%, respectively, and the accuracies of the prescription maps for the three respective models were 90.83%, 87.14%, and 91.40%. These results confirmed that it was feasible to identify cotton root rot and create prescription maps using different features of Sentinel-2 imagery. The addition of texture features had little effect on the overall accuracy, but it could improve the ability to identify root rot areas. The producer&rsquo;s accuracy (PA) for infested cotton in the classification maps for the texture model and the spectral-texture model was 2.82% and 1.07% higher, respectively, than that of the spectral model, and the PA for treatment zones in the prescription maps for the two respective models was 8.6% and 8.22% higher than that of the spectral model. Results based on the eight cotton fields showed that the spectral model was appropriate for the cotton fields with relatively severe infestation and the spectral-texture model was more appropriate for the cotton fields with low or moderate infestation.},
DOI = {10.3390/rs12213504}
}



@Article{agronomy10111648,
AUTHOR = {Demestichas, Konstantinos and Daskalakis, Emmanouil},
TITLE = {Data Lifecycle Management in Precision Agriculture Supported by Information and Communication Technology},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1648},
URL = {https://www.mdpi.com/2073-4395/10/11/1648},
ISSN = {2073-4395},
ABSTRACT = {The role of agriculture in environmental degradation and climate change has been at the center of a long-lasting and controversial debate. This situation combined with the expected growth in crop demand and the increasing prices of fertilizers and pesticides has made the need for a more resource-efficient and environmentally sustainable agriculture more evident than ever. Precision agriculture (PA), as a relatively new farming management concept, aims to improve crop performance as well as to reduce the environmental footprint by utilizing information about the temporal and the spatial variability of crops. Information and communication technology (ICT) systems have influenced and shaped every part of modern life, and PA is no exception. The current paper conducts a literature review of prominent ICT solutions, focusing on their role in supporting different phases of the lifecycle of PA-related data. In addition to this, a data lifecycle model was developed as part of a novel categorization approach for the analyzed solutions.},
DOI = {10.3390/agronomy10111648}
}



@Article{rs12213511,
AUTHOR = {Eskandari, Roghieh and Mahdianpari, Masoud and Mohammadimanesh, Fariba and Salehi, Bahram and Brisco, Brian and Homayouni, Saeid},
TITLE = {Meta-analysis of Unmanned Aerial Vehicle (UAV) Imagery for Agro-environmental Monitoring Using Machine Learning and Statistical Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3511},
URL = {https://www.mdpi.com/2072-4292/12/21/3511},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) imaging systems have recently gained significant attention from researchers and practitioners as a cost-effective means for agro-environmental applications. In particular, machine learning algorithms have been applied to UAV-based remote sensing data for enhancing the UAV capabilities of various applications. This systematic review was performed on studies through a statistical meta-analysis of UAV applications along with machine learning algorithms in agro-environmental monitoring. For this purpose, a total number of 163 peer-reviewed articles published in 13 high-impact remote sensing journals over the past 20 years were reviewed focusing on several features, including study area, application, sensor type, platform type, and spatial resolution. The meta-analysis revealed that 62% and 38% of the studies applied regression and classification models, respectively. Visible sensor technology was the most frequently used sensor with the highest overall accuracy among classification articles. Regarding regression models, linear regression and random forest were the most frequently applied models in UAV remote sensing imagery processing. Finally, the results of this study confirm that applying machine learning approaches on UAV imagery produces fast and reliable results. Agriculture, forestry, and grassland mapping were found as the top three UAV applications in this review, in 42%, 22%, and 8% of the studies, respectively.},
DOI = {10.3390/rs12213511}
}



@Article{s20216098,
AUTHOR = {Just, Gilson E. and E. Pellenz, Marcelo and Lima, Luiz A. de Paula and S. Chang, Bruno and Demo Souza, Richard and Montejo-Sánchez, Samuel},
TITLE = {UAV Path Optimization for Precision Agriculture Wireless Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6098},
URL = {https://www.mdpi.com/1424-8220/20/21/6098},
ISSN = {1424-8220},
ABSTRACT = {The use of monitoring sensors is increasingly present in the context of precision agriculture. Usually, these sensor nodes (SNs) alternate their states between periods of activation and hibernation to reduce battery usage. When employing unmanned aerial vehicles (UAVs) to collect data from SNs distributed over a large agricultural area, we must synchronize the UAV route with the activation period of each SN. In this article, we address the problem of optimizing the UAV path through all the SNs to reduce its flight time, while also maximizing the SNs&rsquo; lifetime. Using the concept of timeslots for time base management combined with the idea of flight prohibition list, we propose an efficient algorithm for discovering and reconfiguring the activation time of the SNs. Experimental results were obtained through the development of our own simulator&mdash;UAV Simulator. These results demonstrate a considerable reduction in the distance traveled by the UAV and also in its flight time. In addition, the model provides a reduction in transmission time by SNs after reconfiguration, thus ensuring a longer lifetime for the SNs in the monitoring environment, as well as improving the freshness and continuity of the gathered data, which support the decision-making process.},
DOI = {10.3390/s20216098}
}



@Article{s20216097,
AUTHOR = {Gardner, Marcus and Mancero Castillo, C. Sebastian and Wilson, Samuel and Farina, Dario and Burdet, Etienne and Khoo, Boo Cheong and Atashzar, S. Farokh and Vaidyanathan, Ravi},
TITLE = {A Multimodal Intention Detection Sensor Suite for Shared Autonomy of Upper-Limb Robotic Prostheses},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6097},
URL = {https://www.mdpi.com/1424-8220/20/21/6097},
ISSN = {1424-8220},
ABSTRACT = {Neurorobotic augmentation (e.g., robotic assist) is now in regular use to support individuals suffering from impaired motor functions. A major unresolved challenge, however, is the excessive cognitive load necessary for the human&ndash;machine interface (HMI). Grasp control remains one of the most challenging HMI tasks, demanding simultaneous, agile, and precise control of multiple degrees-of-freedom (DoFs) while following a specific timing pattern in the joint and human&ndash;robot task spaces. Most commercially available systems use either an indirect mode-switching configuration or a limited sequential control strategy, limiting activation to one DoF at a time. To address this challenge, we introduce a shared autonomy framework centred around a low-cost multi-modal sensor suite fusing: (a) mechanomyography (MMG) to estimate the intended muscle activation, (b) camera-based visual information for integrated autonomous object recognition, and (c) inertial measurement to enhance intention prediction based on the grasping trajectory. The complete system predicts user intent for grasp based on measured dynamical features during natural motions. A total of 84 motion features were extracted from the sensor suite, and tests were conducted on 10 able-bodied and 1 amputee participants for grasping common household objects with a robotic hand. Real-time grasp classification accuracy using visual and motion features obtained 100%, 82.5%, and 88.9% across all participants for detecting and executing grasping actions for a bottle, lid, and box, respectively. The proposed multimodal sensor suite is a novel approach for predicting different grasp strategies and automating task performance using a commercial upper-limb prosthetic device. The system also shows potential to improve the usability of modern neurorobotic systems due to the intuitive control design.},
DOI = {10.3390/s20216097}
}



@Article{w12113010,
AUTHOR = {Wang, Ruimeng and Xia, Haoming and Qin, Yaochen and Niu, Wenhui and Pan, Li and Li, Rumeng and Zhao, Xiaoyang and Bian, Xiqing and Fu, Pinde},
TITLE = {Dynamic Monitoring of Surface Water Area during 1989–2019 in the Hetao Plain Using Landsat Data in Google Earth Engine},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3010},
URL = {https://www.mdpi.com/2073-4441/12/11/3010},
ISSN = {2073-4441},
ABSTRACT = {The spatio-temporal change of the surface water is very important to agricultural, economic, and social development in the Hetao Plain, as well as the structure and function of the ecosystem. To understand the long-term changes of the surface water area in the Hetao Plain, we used all available Landsat images (7534 scenes) and adopted the modified Normalized Difference Water Index (mNDWI), Enhanced Vegetation Index (EVI), and Normalized Difference Vegetation Index (NDVI) to map the open-surface water from 1989 to 2019 in the Google Earth Engine (GEE) cloud platform. We further analyzed precipitation, temperature, and irrigated area, revealing the impact of climate change and human activities on long-term surface water changes. The results show the following. (1) In the last 31 years, the maximum, seasonal, and annual average water body area values in the Hetao Plain have exhibited a downward trend. Meanwhile, the number of maximum, seasonal, and permanent water bodies displayed a significant upward trend. (2) The variation of the surface water area in the Hetao Plain is mainly affected by the maximum water body area, while the variation of the water body number is mainly affected by the number of minimum water bodies. (3) Precipitation has statistically significant positive effects on the water body area and water body number, which has statistically significant negative effects with temperature and irrigation. The findings of this study can be used to help the policy-makers and farmers understand changing water resources and its driving mechanism and provide a reference for water resources management, agricultural irrigation, and ecological protection.},
DOI = {10.3390/w12113010}
}



@Article{rs12213533,
AUTHOR = {Pedro, Dário and Matos-Carvalho, João P. and Azevedo, Fábio and Sacoto-Martins, Ricardo and Bernardo, Luís and Campos, Luís and Fonseca, José M. and Mora, André},
TITLE = {FFAU—Framework for Fully Autonomous UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3533},
URL = {https://www.mdpi.com/2072-4292/12/21/3533},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), although hardly a new technology, have recently gained a prominent role in many industries being widely used not only among enthusiastic consumers, but also in high demanding professional situations, and will have a massive societal impact over the coming years. However, the operation of UAVs is fraught with serious safety risks, such as collisions with dynamic obstacles (birds, other UAVs, or randomly thrown objects). These collision scenarios are complex to analyze in real-time, sometimes being computationally impossible to solve with existing State of the Art (SoA) algorithms, making the use of UAVs an operational hazard and therefore significantly reducing their commercial applicability in urban environments. In this work, a conceptual framework for both stand-alone and swarm (networked) UAVs is introduced, with a focus on the architectural requirements of the collision avoidance subsystem to achieve acceptable levels of safety and reliability. The SoA principles for collision avoidance against stationary objects are reviewed and a novel approach is described, using deep learning techniques to solve the computational intensive problem of real-time collision avoidance with dynamic objects. The proposed framework includes a web-interface allowing the full control of UAVs as remote clients with a supervisor cloud-based platform. The feasibility of the proposed approach was demonstrated through experimental tests using a UAV, developed from scratch using the proposed framework. Test flight results are presented for an autonomous UAV monitored from multiple countries across the world.},
DOI = {10.3390/rs12213533}
}



@Article{app10217622,
AUTHOR = {Vujasinović, Stéphane and Becker, Stefan and Breuer, Timo and Bullinger, Sebastian and Scherer-Negenborn, Norbert and Arens, Michael},
TITLE = {Integration of the 3D Environment for UAV Onboard Visual Object Tracking},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7622},
URL = {https://www.mdpi.com/2076-3417/10/21/7622},
ISSN = {2076-3417},
ABSTRACT = {Single visual object tracking from an unmanned aerial vehicle (UAV) poses fundamental challenges such as object occlusion, small-scale objects, background clutter, and abrupt camera motion. To tackle these difficulties, we propose to integrate the 3D structure of the observed scene into a detection-by-tracking algorithm. We introduce a pipeline that combines a model-free visual object tracker, a sparse 3D reconstruction, and a state estimator. The 3D reconstruction of the scene is computed with an image-based Structure-from-Motion (SfM) component that enables us to leverage a state estimator in the corresponding 3D scene during tracking. By representing the position of the target in 3D space rather than in image space, we stabilize the tracking during ego-motion and improve the handling of occlusions, background clutter, and small-scale objects. We evaluated our approach on prototypical image sequences, captured from a UAV with low-altitude oblique views. For this purpose, we adapted an existing dataset for visual object tracking and reconstructed the observed scene in 3D. The experimental results demonstrate that the proposed approach outperforms methods using plain visual cues as well as approaches leveraging image-space-based state estimations. We believe that our approach can be beneficial for trafficmonitoring, video surveillance, and navigation.},
DOI = {10.3390/app10217622}
}



@Article{electronics9111794,
AUTHOR = {Kim, Wooseong},
TITLE = {Evolutionary Game for Content Cache in a mm-Wave-Based Vehicular Fog},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1794},
URL = {https://www.mdpi.com/2079-9292/9/11/1794},
ISSN = {2079-9292},
ABSTRACT = {Vehicular fog computing is attractive for sharing computing resources and data for safety and infortainment of self-driving cars. Recently, the V2X communication technology using mm-Wave frequency spectrum accelerates such future mobile computing with large bandwidth and beam-forming using a directional antenna. Although the beam-forming technique requires a complicate procedure for beam alignment, it can reduce mutual interference by spatial diversity. From the beam-forming scheduling, the vehicular fog can improve network performance, which is limited by data locations. Beams toward a vehicle for the same content should be scheduled in the time domain. Instead, we propose to replicate the content to multiple vehicles nearby to diversify beam directions. However, it is a challenge for vehicles to cache the content because the content caching costs not only limited local storage, but data transmission for other vehicles. For this, we adopt evolutionary game theory in which vehicles learn an evolutionarily stable strategy (ESS) from repeated games and maximize social utility. In this paper, we contribute to modeling a road segmentation for the mm-Wave V2X communication in order to derive connectivity probability with distributed content caches for the vehicular fog, and centralized and distributed algorithms for the evolutionary content cache game. From experiments, we confirm that content cache can improve V2X connectivity and the proposed evolution algorithm leads vehicles to choose the ESS for the content cache in the vehicular fog.},
DOI = {10.3390/electronics9111794}
}



@Article{s20216187,
AUTHOR = {F. Pinto, Milena and G. Melo, Aurelio and M. Honório, Leonardo and L. M. Marcato, André and G. S. Conceição, André and O. Timotheo, Amanda},
TITLE = {Deep Learning Applied to Vegetation Identification and Removal Using Multidimensional Aerial Data},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6187},
URL = {https://www.mdpi.com/1424-8220/20/21/6187},
ISSN = {1424-8220},
ABSTRACT = {When performing structural inspection, the generation of three-dimensional (3D) point clouds is a common resource. Those are usually generated from photogrammetry or through laser scan techniques. However, a significant drawback for complete inspection is the presence of covering vegetation, hiding possible structural problems, and making difficult the acquisition of proper object surfaces in order to provide a reliable diagnostic. Therefore, this research&rsquo;s main contribution is developing an effective vegetation removal methodology through the use of a deep learning structure that is capable of identifying and extracting covering vegetation in 3D point clouds. The proposed approach uses pre and post-processing filtering stages that take advantage of colored point clouds, if they are available, or operate independently. The results showed high classification accuracy and good effectiveness when compared with similar methods in the literature. After this step, if color is available, then a color filter is applied, enhancing the results obtained. Besides, the results are analyzed in light of real Structure From Motion (SFM) reconstruction data, which further validates the proposed method. This research also presented a colored point cloud library of bushes built for the work used by other studies in the field.},
DOI = {10.3390/s20216187}
}



@Article{rs12213552,
AUTHOR = {Yoo, Cheolhee and Lee, Yeonsu and Cho, Dongjin and Im, Jungho and Han, Daehyeon},
TITLE = {Improving Local Climate Zone Classification Using Incomplete Building Data and Sentinel 2 Images Based on Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3552},
URL = {https://www.mdpi.com/2072-4292/12/21/3552},
ISSN = {2072-4292},
ABSTRACT = {Recent studies have enhanced the mapping performance of the local climate zone (LCZ), a standard framework for evaluating urban form and function for urban heat island research, through remote sensing (RS) images and deep learning classifiers such as convolutional neural networks (CNNs). The accuracy in the urban-type LCZ (LCZ1-10), however, remains relatively low because RS data cannot provide vertical or horizontal building components in detail. Geographic information system (GIS)-based building datasets can be used as primary sources in LCZ classification, but there is a limit to using them as input data for CNN due to their incompleteness. This study proposes novel methods to classify LCZ using Sentinel 2 images and incomplete building data based on a CNN classifier. We designed three schemes (S1, S2, and a scheme fusion; SF) for mapping 50 m LCZs in two megacities: Berlin and Seoul. S1 used only RS images, and S2 used RS and building components such as area and height (or the number of stories). SF combined two schemes (S1 and S2) based on three conditions, mainly focusing on the confidence level of the CNN classifier. When compared to S1, the overall accuracies for all LCZ classes (OA) and the urban-type LCZ (OAurb) of SF increased by about 4% and 7&ndash;9%, respectively, for the two study areas. This study shows that SF can compensate for the imperfections in the building data, which causes misclassifications in S2. The suggested approach can be excellent guidance to produce a high accuracy LCZ map for cities where building databases can be obtained, even if they are incomplete.},
DOI = {10.3390/rs12213552}
}



@Article{s20216205,
AUTHOR = {Silva, Luís Augusto and Sanchez San Blas, Héctor and Peral García, David and Sales Mendes, André and Villarubia González, Gabriel},
TITLE = {An Architectural Multi-Agent System for a Pavement Monitoring System with Pothole Recognition in UAV Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6205},
URL = {https://www.mdpi.com/1424-8220/20/21/6205},
ISSN = {1424-8220},
ABSTRACT = {In recent years, maintenance work on public transport routes has drastically decreased in many countries due to difficult economic situations. The various studies that have been conducted by groups of drivers and groups related to road safety concluded that accidents are increasing due to the poor conditions of road surfaces, even affecting the condition of vehicles through costly breakdowns. Currently, the processes of detecting any type of damage to a road are carried out manually or are based on the use of a road vehicle, which incurs a high labor cost. To solve this problem, many research centers are investigating image processing techniques to identify poor-condition road areas using deep learning algorithms. The main objective of this work is to design of a distributed platform that allows the detection of damage to transport routes using drones and to provide the results of the most important classifiers. A case study is presented using a multi-agent system based on PANGEA that coordinates the different parts of the architecture using techniques based on ubiquitous computing. The results obtained by means of the customization of the You Only Look Once (YOLO) v4 classifier are promising, reaching an accuracy of more than 95%. The images used have been published in a dataset for use by the scientific community.},
DOI = {10.3390/s20216205}
}



@Article{s20216219,
AUTHOR = {Vega Díaz, Jhon Jairo and Vlaminck, Michiel and Lefkaditis, Dionysios and Orjuela Vargas, Sergio Alejandro and Luong, Hiep},
TITLE = {Solar Panel Detection within Complex Backgrounds Using Thermal Images Acquired by UAVs},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6219},
URL = {https://www.mdpi.com/1424-8220/20/21/6219},
ISSN = {1424-8220},
ABSTRACT = {The installation of solar plants everywhere in the world increases year by year. Automated diagnostic methods are needed to inspect the solar plants and to identify anomalies within these photovoltaic panels. The inspection is usually carried out by unmanned aerial vehicles (UAVs) using thermal imaging sensors. The first step in the whole process is to detect the solar panels in those images. However, standard image processing techniques fail in case of low-contrast images or images with complex backgrounds. Moreover, the shades of power lines or structures similar to solar panels impede the automated detection process. In this research, two self-developed methods are compared for the detection of panels in this context, one based on classical techniques and another one based on deep learning, both with a common post-processing step. The first method is based on edge detection and classification, in contrast to the second method is based on training a region based convolutional neural networks to identify a panel. The first method corrects for the low contrast of the thermal image using several preprocessing techniques. Subsequently, edge detection, segmentation and segment classification are applied. The latter is done using a support vector machine trained with an optimized texture descriptor vector. The second method is based on deep learning trained with images that have been subjected to three different pre-processing operations. The postprocessing use the detected panels to infer the location of panels that were not detected. This step selects contours from detected panels based on the panel area and the angle of rotation. Then new panels are determined by the extrapolation of these contours. The panels in 100 random images taken from eleven UAV flights over three solar plants are labeled and used to evaluate the detection methods. The metrics for the new method based on classical techniques reaches a precision of 0.997, a recall of 0.970 and a F1 score of 0.983. The metrics for the method of deep learning reaches a precision of 0.996, a recall of 0.981 and a F1 score of 0.989. The two panel detection methods are highly effective in the presence of complex backgrounds.},
DOI = {10.3390/s20216219}
}



@Article{drones4040069,
AUTHOR = {Garzon-Lopez, Carol X. and Lasso, Eloisa},
TITLE = {Species Classification in a Tropical Alpine Ecosystem Using UAV-Borne RGB and Hyperspectral Imagery},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/2504-446X/4/4/69},
ISSN = {2504-446X},
ABSTRACT = {P&aacute;ramos host more than 3500 vascular plant species and are crucial water providers for millions of people in the northern Andes. Monitoring species distribution at large scales is an urgent conservation priority in the face of ongoing climatic changes and increasing anthropogenic pressure on this ecosystem. For the first time in this ecosystem, we explored the potential of unoccupied aerial vehicles (UAV)-borne red, green, and blue wavelengths (RGB) and hyperspectral imagery for p&aacute;ramo species classification by collecting both types of images in a 10-ha area, and ground vegetation cover data from 10 plots within this area. Five plots were used for calibration and the other five for validation. With the hyperspectral data, we tested our capacity to detect five representative p&aacute;ramo species with different growth forms using support vector machine (SVM) and random forest (RF) classifiers in combination with three feature selection methods and two class groups. Using RGB images, we could classify 21 species with an accuracy greater than 97%. From hyperspectral imaging, the highest accuracy (89%) was found using models built with RF or SVM classifiers combined with a binary grouping method and the sequential floating forward selection feature. Our results demonstrate that p&aacute;ramo species can be accurately mapped using both RGB and hyperspectral imagery.},
DOI = {10.3390/drones4040069}
}



@Article{en13215712,
AUTHOR = {Bemposta Rosende, Sergio and Sánchez-Soriano, Javier and Gómez Muñoz, Carlos Quiterio and Fernández Andrés, Javier},
TITLE = {Remote Management Architecture of UAV Fleets for Maintenance, Surveillance, and Security Tasks in Solar Power Plants},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {5712},
URL = {https://www.mdpi.com/1996-1073/13/21/5712},
ISSN = {1996-1073},
ABSTRACT = {This article presents a remote management architecture of an unmanned aerial vehicles (UAVs) fleet to aid in the management of solar power plants and object tracking. The proposed system is a competitive advantage for sola r energy production plants, due to the reduction in costs for maintenance, surveillance, and security tasks, especially in large solar farms. This new approach consists of creating a hardware and software architecture that allows for performing different tasks automatically, as well as remotely using fleets of UAVs. The entire system, composed of the aircraft, the servers, communication networks, and the processing center, as well as the interfaces for accessing the services via the web, has been designed for this specific purpose. Image processing and automated remote control of the UAV allow generating autonomous missions for the inspection of defects in solar panels, saving costs compared to traditional manual inspection. Another application of this architecture related to security is the detection and tracking of pedestrians and vehicles, both for road safety and for surveillance and security issues of solar plants. The novelty of this system with respect to current systems is summarized in that all the software and hardware elements that allow the inspection of solar panels, surveillance, and people counting, as well as traffic management tasks, have been defined and detailed. The modular system presented allows the exchange of different specific vision modules for each task to be carried out. Finally, unlike other systems, calibrated fixed cameras are used in addition to the cameras embedded in the drones of the fleet, which complement the system with vision algorithms based on deep learning for identification, surveillance, and inspection.},
DOI = {10.3390/en13215712}
}



@Article{s20216247,
AUTHOR = {Calvario, Gabriela and Alarcón, Teresa E. and Dalmau, Oscar and Sierra, Basilio and Hernandez, Carmen},
TITLE = {An Agave Counting Methodology Based on Mathematical Morphology and Images Acquired through Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6247},
URL = {https://www.mdpi.com/1424-8220/20/21/6247},
ISSN = {1424-8220},
ABSTRACT = {Blue agave is an important commercial crop in Mexico, and it is the main source of the traditional mexican beverage known as tequila. The variety of blue agave crop known as Tequilana Weber is a crucial element for tequila agribusiness and the agricultural economy in Mexico. The number of agave plants in the field is one of the main parameters for estimating production of tequila. In this manuscript, we describe a mathematical morphology-based algorithm that addresses the agave automatic counting task. The proposed methodology was applied to a set of real images collected using an Unmanned Aerial Vehicle equipped with a digital Red-Green-Blue (RGB) camera. The number of plants automatically identified in the collected images was compared to the number of plants counted by hand. Accuracy of the proposed algorithm depended on the size heterogeneity of plants in the field and illumination. Accuracy ranged from 0.8309 to 0.9806, and performance of the proposed algorithm was satisfactory.},
DOI = {10.3390/s20216247}
}



@Article{informatics7040050,
AUTHOR = {Kazllarof, Vangjel and Karlos, Stamatis and Kotsiantis, Sotiris},
TITLE = {Investigation of Combining Logitboost(M5P) under Active Learning Classification Tasks},
JOURNAL = {Informatics},
VOLUME = {7},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2227-9709/7/4/50},
ISSN = {2227-9709},
ABSTRACT = {Active learning is the category of partially supervised algorithms that is differentiated by its strategy to combine both the predictive ability of a base learner and the human knowledge so as to exploit adequately the existence of unlabeled data. Its ambition is to compose powerful learning algorithms which otherwise would be based only on insufficient labelled samples. Since the latter kind of information could raise important monetization costs and time obstacles, the human contribution should be seriously restricted compared with the former. For this reason, we investigate the use of the Logitboost wrapper classifier, a popular variant of ensemble algorithms which adopts the technique of boosting along with a regression base learner based on Model trees into 3 different active learning query strategies. We study its efficiency against 10 separate learners under a well-described active learning framework over 91 datasets which have been split to binary and multi-class problems. We also included one typical Logitboost variant with a separate internal regressor for discriminating the benefits of adopting a more accurate regression tree than one-node trees, while we examined the efficacy of one hyperparameter of the proposed algorithm. Since the application of the boosting technique may provide overall less biased predictions, we assume that the proposed algorithm, named as Logitboost(M5P), could provide both accurate and robust decisions under active learning scenarios that would be beneficial on real-life weakly supervised classification tasks. Its smoother weighting stage over the misclassified cases during training as well as the accurate behavior of M5P are the main factors that lead towards this performance. Proper statistical comparisons over the metric of classification accuracy verify our assumptions, while adoption of M5P instead of weak decision trees was proven to be more competitive for the majority of the examined problems. We present our results through appropriate summarization approaches and explanatory visualizations, commenting our results per case.},
DOI = {10.3390/informatics7040050}
}



@Article{iot1020020,
AUTHOR = {Kontogiannis, Sotirios and Asiminidis, Christodoulos},
TITLE = {A Proposed Low-Cost Viticulture Stress Framework for Table Grape Varieties},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {337--359},
URL = {https://www.mdpi.com/2624-831X/1/2/20},
ISSN = {2624-831X},
ABSTRACT = {Climate change significantly affects viticulture by reducing the production yield and the quality characteristics of its final products. In some observed cases, the consequences of climate outages such as droughts, hail and floods are absolutely devastating for the farmers and the sustained local economies. Hence, it is essential to develop new in implementation monitoring solutions that offer remote real-time surveillance, alert triggering, minimum maintenance and automated generation of incident alerts with precision responses. This paper presents a new framework and a system for vine stress monitoring called Vity-stress. The Vity-stress framework combines field measurements with precise viticulture suggestions and stress avoidance planning. The key points of the proposed framework&rsquo;s system are that it is easy to develop, easy to maintain and cheap to implement applicability. Focusing on the Mediterranean cultivated table grape varieties that are strongly affected by climate change, we propose a new stress conditions monitoring system to support our framework. The proposition includes distributed field located sensors and a novel camera module implementing deep neural network algorithms to detect stress indicators. Additionally, a new wireless sensor network supported by the iBeacon protocol has been developed. The results of the sensory measurements&rsquo; data logging and imposed image detection process&rsquo;s evaluation shows that the proposed system can successfully detect different stress levels in vineyards, which in turn can allow producers to identify specific areas for irrigation, thereby saving water, energy and time.},
DOI = {10.3390/iot1020020}
}



@Article{rs12213617,
AUTHOR = {Trevisan, Rodrigo and Pérez, Osvaldo and Schmitz, Nathan and Diers, Brian and Martin, Nicolas},
TITLE = {High-Throughput Phenotyping of Soybean Maturity Using Time Series UAV Imagery and Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3617},
URL = {https://www.mdpi.com/2072-4292/12/21/3617},
ISSN = {2072-4292},
ABSTRACT = {Soybean maturity is a trait of critical importance for the development of new soybean cultivars, nevertheless, its characterization based on visual ratings has many challenges. Unmanned aerial vehicles (UAVs) imagery-based high-throughput phenotyping methodologies have been proposed as an alternative to the traditional visual ratings of pod senescence. However, the lack of scalable and accurate methods to extract the desired information from the images remains a significant bottleneck in breeding programs. The objective of this study was to develop an image-based high-throughput phenotyping system for evaluating soybean maturity in breeding programs. Images were acquired twice a week, starting when the earlier lines began maturation until the latest ones were mature. Two complementary convolutional neural networks (CNN) were developed to predict the maturity date. The first using a single date and the second using the five best image dates identified by the first model. The proposed CNN architecture was validated using more than 15,000 ground truth observations from five trials, including data from three growing seasons and two countries. The trained model showed good generalization capability with a root mean squared error lower than two days in four out of five trials. Four methods of estimating prediction uncertainty showed potential at identifying different sources of errors in the maturity date predictions. The architecture developed solves limitations of previous research and can be used at scale in commercial breeding programs.},
DOI = {10.3390/rs12213617}
}



@Article{rs12213621,
AUTHOR = {Bi, Luning and Hu, Guiping and Raza, Muhammad Mohsin and Kandel, Yuba and Leandro, Leonor and Mueller, Daren},
TITLE = {A Gated Recurrent Units (GRU)-Based Model for Early Detection of Soybean Sudden Death Syndrome through Time-Series Satellite Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3621},
URL = {https://www.mdpi.com/2072-4292/12/21/3621},
ISSN = {2072-4292},
ABSTRACT = {In general, early detection and timely management of plant diseases are essential for reducing yield loss. Traditional manual inspection of fields is often time-consuming and laborious. Automated imaging techniques have recently been successfully applied to detect plant diseases. However, these methods mostly focus on the current state of the crop. This paper proposes a gated recurrent unit (GRU)-based model to predict soybean sudden death syndrome (SDS) disease development. To detect SDS at a quadrat level, the proposed method uses satellite images collected from PlanetScope as the training set. The pixel image data include the spectral bands of red, green, blue and near-infrared (NIR). Data collected during the 2016 and 2017 soybean-growing seasons were analyzed. Instead of using individual static imagery, the GRU-based model converts the original imagery into time-series data. SDS predictions were made on different data scenarios and the results were compared with fully connected deep neural network (FCDNN) and XGBoost methods. The overall test accuracy of classifying healthy and diseased quadrates in all methods was above 76%. The test accuracy of the FCDNN and XGBoost were 76.3&ndash;85.5% and 80.6&ndash;89.2%, respectively, while the test accuracy of the GRU-based model was 82.5&ndash;90.4%. The calculation results show that the proposed method can improve the detection accuracy by up to 7% with time-series imagery. Thus, the proposed method has the potential to predict SDS at a future time.},
DOI = {10.3390/rs12213621}
}



@Article{s20216299,
AUTHOR = {Bhowmick, Sutanu and Nagarajaiah, Satish and Veeraraghavan, Ashok},
TITLE = {Vision and Deep Learning-Based Algorithms to Detect and Quantify Cracks on Concrete Surfaces from UAV Videos},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6299},
URL = {https://www.mdpi.com/1424-8220/20/21/6299},
ISSN = {1424-8220},
ABSTRACT = {Immediate assessment of structural integrity of important civil infrastructures, like bridges, hospitals, or dams, is of utmost importance after natural disasters. Currently, inspection is performed manually by engineers who look for local damages and their extent on significant locations of the structure to understand its implication on its global stability. However, the whole process is time-consuming and prone to human errors. Due to their size and extent, some regions of civil structures are hard to gain access for manual inspection. In such situations, a vision-based system of Unmanned Aerial Vehicles (UAVs) programmed with Artificial Intelligence algorithms may be an effective alternative to carry out a health assessment of civil infrastructures in a timely manner. This paper proposes a framework of achieving the above-mentioned goal using computer vision and deep learning algorithms for detection of cracks on the concrete surface from its image by carrying out image segmentation of pixels, i.e., classification of pixels in an image of the concrete surface and whether it belongs to cracks or not. The image segmentation or dense pixel level classification is carried out using a deep neural network architecture named U-Net. Further, morphological operations on the segmented images result in dense measurements of crack geometry, like length, width, area, and crack orientation for individual cracks present in the image. The efficacy and robustness of the proposed method as a viable real-life application was validated by carrying out a laboratory experiment of a four-point bending test on an 8-foot-long concrete beam of which the video is recorded using a camera mounted on a UAV-based, as well as a still ground-based, video camera. Detection, quantification, and localization of damage on a civil infrastructure using the proposed framework can directly be used in the prognosis of the structure&rsquo;s ability to withstand service loads.},
DOI = {10.3390/s20216299}
}



@Article{rs12213634,
AUTHOR = {Fernandez-Carrillo, Angel and Patočka, Zdeněk and Dobrovolný, Lumír and Franco-Nieto, Antonio and Revilla-Romero, Beatriz},
TITLE = {Monitoring Bark Beetle Forest Damage in Central Europe. A Remote Sensing Approach Validated with Field Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3634},
URL = {https://www.mdpi.com/2072-4292/12/21/3634},
ISSN = {2072-4292},
ABSTRACT = {Over the last decades, climate change has triggered an increase in the frequency of spruce bark beetle (Ips typographus L.) in Central Europe. More than 50% of forests in the Czech Republic are seriously threatened by this pest, leading to high ecological and economic losses. The exponential increase of bark beetle infestation hinders the implementation of costly field campaigns to prevent and mitigate its effects. Remote sensing may help to overcome such limitations as it provides frequent and spatially continuous data on vegetation condition. Using Sentinel-2 images as main input, two models have been developed to test the ability of this data source to map bark beetle damage and severity. All models were based on a change detection approach, and required the generation of previous forest mask and dominant species maps. The first damage mapping model was developed for 2019 and 2020, and it was based on bi-temporal regressions in spruce areas to estimate forest vitality and bark beetle damage. A second model was developed for 2020 considering all forest area, but excluding clear-cuts and completely dead areas, in order to map only changes in stands dominated by alive trees. The three products were validated with in situ data. All the maps showed high accuracies (acc &gt; 0.80). Accuracy was higher than 0.95 and F1-score was higher than 0.88 for areas with high severity, with omission errors under 0.09 in all cases. This confirmed the ability of all the models to detect bark beetle attack at the last phases. Areas with no damage or low severity showed more complex results. The no damage category yielded greater commission errors and relative bias (CEs = 0.30&ndash;0.42, relB = 0.42&ndash;0.51). The similar results obtained for 2020 leaving out clear-cuts and dead trees proved that the proposed methods could be used to help forest managers fight bark beetle pests. These biotic damage products based on Sentinel-2 can be set up for any location to derive regular forest vitality maps and inform of early damage.},
DOI = {10.3390/rs12213634}
}



@Article{e22111261,
AUTHOR = {Sanenga, Abraham and Mapunda, Galefang Allycan and Jacob, Tshepiso Merapelo Ludo and Marata, Leatile and Basutli, Bokamoso and Chuma, Joseph Monamati},
TITLE = {An Overview of Key Technologies in Physical Layer Security},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1261},
URL = {https://www.mdpi.com/1099-4300/22/11/1261},
ISSN = {1099-4300},
ABSTRACT = {The open nature of radio propagation enables ubiquitous wireless communication. This allows for seamless data transmission. However, unauthorized users may pose a threat to the security of the data being transmitted to authorized users. This gives rise to network vulnerabilities such as hacking, eavesdropping, and jamming of the transmitted information. Physical layer security (PLS) has been identified as one of the promising security approaches to safeguard the transmission from eavesdroppers in a wireless network. It is an alternative to the computationally demanding and complex cryptographic algorithms and techniques. PLS has continually received exponential research interest owing to the possibility of exploiting the characteristics of the wireless channel. One of the main characteristics includes the random nature of the transmission channel. The aforesaid nature makes it possible for confidential and authentic signal transmission between the sender and the receiver in the physical layer. We start by introducing the basic theories of PLS, including the wiretap channel, information-theoretic security, and a brief discussion of the cryptography security technique. Furthermore, an overview of multiple-input multiple-output (MIMO) communication is provided. The main focus of our review is based on the existing key-less PLS optimization techniques, their limitations, and challenges. The paper also looks into the promising key research areas in addressing these shortfalls. Lastly, a comprehensive overview of some of the recent PLS research in 5G and 6G technologies of wireless communication networks is provided.},
DOI = {10.3390/e22111261}
}



@Article{info11110520,
AUTHOR = {Xue, Rui and Liu, Jing and Tang, Huaiyu},
TITLE = {Two-Dimensional Jamming Recognition Algorithm Based on the Sevcik Fractal Dimension and Energy Concentration Property for UAV Frequency Hopping Systems},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {520},
URL = {https://www.mdpi.com/2078-2489/11/11/520},
ISSN = {2078-2489},
ABSTRACT = {Unmanned aircraft vehicle frequency hopping (UAV-FH) systems face multiple types of jamming, and one anti-jamming method cannot cope with all types of jamming. Therefore, the jamming signals of the environment where the UAV-FH system is located must be identified and classified; moreover, anti-jamming measures must be selected in accordance with different jamming types. First, the algorithm extracts the Sevcik fractal dimension from the frequency domain (SFDF) and the degree of energy concentration from the fractional Fourier domain of various types of jamming. Then, these parameters are combined into a two-dimensional feature vector and used as a feature parameter for classification and recognition. Lastly, a binary tree-based support vector machine (BT-SVM) multi-classifier is used to classify the jamming signal. Simulation results show that the feature parameters extracted by the proposed method have good separation and strong stability. Compared with the existing box-dimensional recognition algorithm, the new algorithm not only can quickly and accurately identify the type of jamming signal but also has more advantages when the jamming-to-noise ratio (JNR) is low.},
DOI = {10.3390/info11110520}
}



@Article{electronics9111864,
AUTHOR = {Sobb, Theresa and Turnbull, Benjamin and Moustafa, Nour},
TITLE = {Supply Chain 4.0: A Survey of Cyber Security Challenges, Solutions and Future Directions},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1864},
URL = {https://www.mdpi.com/2079-9292/9/11/1864},
ISSN = {2079-9292},
ABSTRACT = {Supply chain 4.0 denotes the fourth revolution of supply chain management systems, integrating manufacturing operations with telecommunication and Information Technology processes. Although the overarching aim of supply chain 4.0 is the enhancement of production systems within supply chains, making use of global reach, increasing agility and emerging technology, with the ultimate goal of increasing efficiency, timeliness and profitability, Supply chain 4.0 suffers from unique and emerging operational and cyber risks. Supply chain 4.0 has a lack of semantic standards, poor interoperability, and a dearth of security in the operation of its manufacturing and Information Technology processes. The technologies that underpin supply chain 4.0 include blockchain, smart contracts, applications of Artificial Intelligence, cyber-physical systems, Internet of Things and Industrial Internet of Things. Each of these technologies, individually and combined, create cyber security issues that should be addressed. This paper explains the nature of the military supply chains 4.0 and how it uniquely differs from the commercial supply chain, revealing their strengths, weaknesses, dependencies and the fundamental technologies upon which they are built. This encompasses an assessment of the cyber risks and opportunities for research in the field, including consideration of connectivity, sensing and convergence of systems. Current and emerging semantic models related to the standardization, development and safety assurance considerations for implementing new technologies into military supply chains 4.0 are also discussed. This is examined from a holistic standpoint and through technology-specific lenses to determine current states and implications for future research directions.},
DOI = {10.3390/electronics9111864}
}



@Article{rs12213659,
AUTHOR = {Soloy, Antoine and Turki, Imen and Fournier, Matthieu and Costa, Stéphane and Peuziat, Bastien and Lecoq, Nicolas},
TITLE = {A Deep Learning-Based Method for Quantifying and Mapping the Grain Size on Pebble Beaches},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3659},
URL = {https://www.mdpi.com/2072-4292/12/21/3659},
ISSN = {2072-4292},
ABSTRACT = {This article proposes a new methodological approach to measure and map the size of coarse clasts on a land surface from photographs. This method is based on the use of the Mask Regional Convolutional Neural Network (R-CNN) deep learning algorithm, which allows the instance segmentation of objects after an initial training on manually labeled data. The algorithm is capable of identifying and classifying objects present in an image at the pixel scale, without human intervention, in a matter of seconds. This work demonstrates that it is possible to train the model to detect non-overlapping coarse sediments on scaled images, in order to extract their individual size and morphological characteristics with high efficiency (R2 = 0.98; Root Mean Square Error (RMSE) = 3.9 mm). It is then possible to measure element size profiles over a sedimentary body, as it was done on the pebble beach of Etretat (Normandy, France) in order to monitor the granulometric spatial variability before and after a storm. Applied at a larger scale using Unmanned Aerial Vehicle (UAV) derived ortho-images, the method allows the accurate characterization and high-resolution mapping of the surface coarse sediment size, as it was performed on the two pebble beaches of Etretat (D50 = 5.99 cm) and Hautot-sur-Mer (D50 = 7.44 cm) (Normandy, France). Validation results show a very satisfying overall representativity (R2 = 0.45 and 0.75; RMSE = 6.8 mm and 9.3 mm at Etretat and Hautot-sur-Mer, respectively), while the method remains fast, easy to apply and low-cost, although the method remains limited by the image resolution (objects need to be longer than 4 cm), and could still be improved in several ways, for instance by adding more manually labeled data to the training dataset, and by considering more accurate methods than the ellipse fitting for measuring the particle sizes.},
DOI = {10.3390/rs12213659}
}



@Article{app10217930,
AUTHOR = {Vintimilla-Tapia, Paúl and Bravo-Torres, Jack and López-Nores, Martín and Gallegos-Segovia, Pablo and Ordóñez-Morales, Esteban and Ramos-Cabrer, Manuel},
TITLE = {VaNetChain: A Framework for Trustworthy Exchanges of Information in VANETs Based on Blockchain and a Virtualization Layer},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7930},
URL = {https://www.mdpi.com/2076-3417/10/21/7930},
ISSN = {2076-3417},
ABSTRACT = {Vehicular ad hoc networks (VANETs) face challenges related to the reliability of the data exchanged and the unstability of the communication links. These shortcomings have hampered the development of the long-awaited applications that would turn roads into a smart environment. We present a framework to deploy such services, in which a virtualization layer ensures means to efficiently deliver messages between vehicles and roadside units (RSUs) and, on top of that, blockchain technology is used to enable features of data integrity, traceability, and reliability that cannot be furnished by existing consensus and reputation mechanisms. A simulation experiment is included to determine the optimal number of RSUs to be installed as supporting infrastructure in a city.},
DOI = {10.3390/app10217930}
}



@Article{s20216384,
AUTHOR = {Cantieri, Alvaro and Ferraz, Matheus and Szekir, Guido and Antônio Teixeira, Marco and Lima, José and Schneider Oliveira, André and Aurélio Wehrmeister, Marco},
TITLE = {Cooperative UAV–UGV Autonomous Power Pylon Inspection: An Investigation of Cooperative Outdoor Vehicle Positioning Architecture},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6384},
URL = {https://www.mdpi.com/1424-8220/20/21/6384},
ISSN = {1424-8220},
ABSTRACT = {Realizing autonomous inspection, such as that of power distribution lines, through unmanned aerial vehicle (UAV) systems is a key research domain in robotics. In particular, the use of autonomous and semi-autonomous vehicles to execute the tasks of an inspection process can enhance the efficacy and safety of the operation; however, many technical problems, such as those pertaining to the precise positioning and path following of the vehicles, robust obstacle detection, and intelligent control, must be addressed. In this study, an innovative architecture involving an unmanned aircraft vehicle (UAV) and an unmanned ground vehicle (UGV) was examined for detailed inspections of power lines. In the proposed strategy, each vehicle provides its position information to the other, which ensures a safe inspection process. The results of real-world experiments indicate a satisfactory performance, thereby demonstrating the feasibility of the proposed approach.},
DOI = {10.3390/s20216384}
}



@Article{w12113135,
AUTHOR = {Liang, Zhongwei and Liu, Xiaochu and Xiong, Jianbin and Xiao, Jinrui},
TITLE = {Water Allocation and Integrative Management of Precision Irrigation: A Systematic Review},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3135},
URL = {https://www.mdpi.com/2073-4441/12/11/3135},
ISSN = {2073-4441},
ABSTRACT = {Precision irrigation, defined as an efficient water allocation technique characterized by the optimal management and best collaboration of various factors of the irrigation process, attracts considerable attention in agricultural production and crop cultivation. This paper reviews the latest research developments in water allocation mechanism and integrative management effectiveness of precision irrigation, and highlights how irrigation water allocation and integrative management contribute to the high-efficiency performance of precision irrigation techniques; the irrigation models, irrigation infrastructure, and management strategies currently being used are emphasized. Thereafter, the future development prospects in water allocation and integrative management could be systematically analyzed and subsequently explored. Some frontier techniques such as data-oriented irrigation management, performance-proven water allocation, and cloud-based irrigation control are among the critical technologies capable of building a sustainable, integrative, and evolutionary irrigation system while providing the higher quality and efficiency needed for a full application of precision irrigation. This review could be used as an effective reference to study the complicated correlations between precision irrigation and its constructive influences in different environmental conditions, and to facilitate the practical promotion of irrigation productivity with higher accuracy and increased reliability of returns.},
DOI = {10.3390/w12113135}
}



@Article{en13225875,
AUTHOR = {Ren, Yuan and Zhang, Xuewei and Lu, Guangyue},
TITLE = {The Wireless Solution to Realize Green IoT: Cellular Networks with Energy Efficient and Energy Harvesting Schemes},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {5875},
URL = {https://www.mdpi.com/1996-1073/13/22/5875},
ISSN = {1996-1073},
ABSTRACT = {With the tremendous increase of heterogeneous Internet of Things (IoT) devices and the different service requirements of these IoT applications, machine-type communication (MTC) has attracted considerable attention from both industry and academia. Owing to the prominent advantages of supporting pervasive connectivity and wide area coverage, the cellular network is advocated as the potential wireless solution to realize IoT deployment for MTC, and this creative network paradigm is called the cellular IoT (C-IoT). In this paper, we propose the three-layer structured C-IoT architecture for MTC and review the challenges for deploying green C-IoT. Then, effective strategies for realizing green C-IoT are presented, including the energy efficient and energy harvesting schemes. We put forward several strategies to make the C-IoT run in an energy-saving manner, such as efficient random access and barring mechanisms, self-adapting machine learning predictions, scheduling optimization, resource allocation, fog computing, and group-oriented transmission. As for the energy harvesting schemes, the ambient and dedicated energy harvesting strategies are investigated. Afterwards, we give a detailed case study, which shows the effectiveness of reducing power consumption for the proposed layered C-IoT architecture. Additionally, for real-time and non-real-time applications, the power consumption of different on-off states for MTC devices is discussed.},
DOI = {10.3390/en13225875}
}



@Article{rs12223698,
AUTHOR = {Pastucha, Elżbieta and Puniach, Edyta and Ścisłowicz, Agnieszka and Ćwiąkała, Paweł and Niewiem, Witold and Wiącek, Paweł},
TITLE = {3D Reconstruction of Power Lines Using UAV Images to Monitor Corridor Clearance},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3698},
URL = {https://www.mdpi.com/2072-4292/12/22/3698},
ISSN = {2072-4292},
ABSTRACT = {Regular power line inspections are essential to ensure the reliability of electricity supply. The inspections of overground power submission lines include corridor clearance monitoring and fault identification. The power lines corridor is a three-dimensional space around power cables defined by a set distance. Any obstacles breaching this space should be detected, as they potentially threaten the safety of the infrastructure. Corridor clearance monitoring is usually performed either by a labor-intensive total station survey (TS), terrestrial laser scanning (TLS), or expensive airborne laser scanning (ALS) from a plane or a helicopter. This paper proposes a method that uses unmanned aerial vehicle (UAV) images to monitor corridor clearance. To maintain the adequate accuracy of the relative position of wires in regard to surrounding obstacles, the same data were used both to reconstruct a point cloud representation of a digital surface model (DSM) and a 3D power line. The proposed algorithm detects power lines in a series of images using decorrelation stretch for initial image processing, the modified Prewitt filter for edge enhancement, random sample consensus (RANSAC) with additional parameters for line fitting, and epipolar geometry for 3D reconstruction. DSM points intruding into the corridor are then detected by calculating the spatial distance between a reconstructed power line and the DSM point cloud representation. Problematic objects are localized by segmenting points into voxels and then subsequent clusterization. The processing results were compared to the results of two verification methods&mdash;TS and TLS. The comparison results show that the proposed method can be used to survey power lines with an accuracy consistent with that of classical measurements.},
DOI = {10.3390/rs12223698}
}



@Article{s20226439,
AUTHOR = {Xu, Wei and Bao, Xiangyu and Chen, Genglin and Neumann, Ingo},
TITLE = {Intelligent Calibration of Static FEA Computations Based on Terrestrial Laser Scanning Reference},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6439},
URL = {https://www.mdpi.com/1424-8220/20/22/6439},
ISSN = {1424-8220},
ABSTRACT = {The demand for efficient and accurate finite element analysis (FEA) is becoming more prevalent with the increase in advanced calibration technologies and sensor-based monitoring methods. The current research explores a deep learning-based methodology to calibrate FEA results. The utilization of monitoring reference results from measurements, e.g., terrestrial laser scanning, can help to capture the actual features in the static loading process. We learn the deviation sequence results between the standard FEA computations with the simplified geometry and refined reference values by the long short-term memory method. The complex changing principles in different deviations are trained and captured effectively in the training process of deep learning. Hence, we generate the FEA sequence results corresponding to next adjacent loading steps. The final FEA computations are calibrated by the threshold control. The calibration reduces the mean square errors of the FEA future sequence results significantly. This strengthens the calibration depth. Consequently, the calibration of FEA computations with deep learning can play a helpful role in the prediction and monitoring problems regarding the future structural behaviors.},
DOI = {10.3390/s20226439}
}



@Article{s20226442,
AUTHOR = {Barmpoutis, Panagiotis and Papaioannou, Periklis and Dimitropoulos, Kosmas and Grammalidis, Nikos},
TITLE = {A Review on Early Forest Fire Detection Systems Using Optical Remote Sensing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6442},
URL = {https://www.mdpi.com/1424-8220/20/22/6442},
ISSN = {1424-8220},
ABSTRACT = {The environmental challenges the world faces nowadays have never been greater or more complex. Global areas covered by forests and urban woodlands are threatened by natural disasters that have increased dramatically during the last decades, in terms of both frequency and magnitude. Large-scale forest fires are one of the most harmful natural hazards affecting climate change and life around the world. Thus, to minimize their impacts on people and nature, the adoption of well-planned and closely coordinated effective prevention, early warning, and response approaches are necessary. This paper presents an overview of the optical remote sensing technologies used in early fire warning systems and provides an extensive survey on both flame and smoke detection algorithms employed by each technology. Three types of systems are identified, namely terrestrial, airborne, and spaceborne-based systems, while various models aiming to detect fire occurrences with high accuracy in challenging environments are studied. Finally, the strengths and weaknesses of fire detection systems based on optical remote sensing are discussed aiming to contribute to future research projects for the development of early warning fire systems.},
DOI = {10.3390/s20226442}
}



@Article{app10228005,
AUTHOR = {Giebas, Damian and Wojszczyk, Rafał},
TITLE = {Atomicity Violation in Multithreaded Applications and Its Detection in Static Code Analysis Process},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8005},
URL = {https://www.mdpi.com/2076-3417/10/22/8005},
ISSN = {2076-3417},
ABSTRACT = {This paper is a contribution to the field of research dealing with the parallel computing, which is used in multithreaded applications. The paper discusses the characteristics of atomicity violation in multithreaded applications and develops a new definition of atomicity violation based on previously defined relationships between operations, that can be used to atomicity violation detection. A method of detection of conflicts causing atomicity violation was also developed using the source code model of multithreaded applications that predicts errors in the software.},
DOI = {10.3390/app10228005}
}



@Article{app10228008,
AUTHOR = {Kim, Byunghyun and Cho, Soojin},
TITLE = {Automated Multiple Concrete Damage Detection Using Instance Segmentation Deep Learning Model},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8008},
URL = {https://www.mdpi.com/2076-3417/10/22/8008},
ISSN = {2076-3417},
ABSTRACT = {In many developed countries with a long history of urbanization, there is an increasing need for automated computer vision (CV)-based inspection to replace conventional labor-intensive visual inspection. This paper proposes a technique for the automated detection of multiple concrete damage based on a state-of-the-art deep learning framework, Mask R-CNN, developed for instance segmentation. The structure of Mask R-CNN, which consists of three stages (region proposal, classification, and segmentation) is optimized for multiple concrete damage detection. The optimized Mask R-CNN is trained with 765 concrete images including cracks, efflorescence, rebar exposure, and spalling. The performance of the trained Mask R-CNN is evaluated with 25 actual test images containing damage as well as environmental objects. Two types of metrics are proposed to measure localization and segmentation performance. On average, 90.41% precision and 90.81% recall are achieved for localization and 87.24% precision and 87.58% recall for segmentation, which indicates the excellent field applicability of the trained Mask R-CNN. This paper also qualitatively discusses the test results by explaining that the architecture of Mask R-CNN that is optimized for general object detection purposes, can be modified to detect long and slender shapes of cracks, rebar exposure, and efflorescence in further research.},
DOI = {10.3390/app10228008}
}



@Article{rs12223708,
AUTHOR = {Feng, Ziyi and Huang, Guanhua and Chi, Daocai},
TITLE = {Classification of the Complex Agricultural Planting Structure with a Semi-Supervised Extreme Learning Machine Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3708},
URL = {https://www.mdpi.com/2072-4292/12/22/3708},
ISSN = {2072-4292},
ABSTRACT = {Many approaches have been developed to analyze remote sensing images. However, for the classification of large-scale problems, most algorithms showed low computational efficiency and low accuracy. In this paper, the newly developed semi-supervised extreme learning machine (SS-ELM) framework with k-means clustering algorithm for image segmentation and co-training algorithm to enlarge the sample sets was used to classify the agricultural planting structure at large-scale areas. Data sets collected from a small-scale area within the Hetao Irrigation District (HID) at the upper reaches of the Yellow River basin were used to evaluate the SS-ELM framework. The results of the SS-ELM algorithm were compared with those of the random forest (RF), ELM, support vector machine (SVM) and semi-supervised support vector machine (S-SVM) algorithms. Then the SS-ELM algorithm was applied to analyze the complex planting structure of HID in 1986&ndash;2010 by comparing the remote sensing estimated results with the statistical data. In the small-scale case, the SS-ELM algorithm performed better than the RF, ELM, SVM, and S-SVM algorithms. For the SS-ELM algorithm, the average overall accuracy (OA) was in a range of 83.00&ndash;92.17%. On the contrary, for the other four algorithms, their average OA values ranged from 56.97% to 92.84%. Whereas, in the classification of planting structure in HID, the SS-ELM algorithm had an excellent performance in classification accuracy and computational efficiency for three major planting crops including maize, wheat, and sunflowers. The estimated areas by using the SS-ELM algorithm based on the remote sensing images were consistent with the statistical data, and their difference was within a range of 3&ndash;25%. This implied that the SS-ELM framework could be served as an effective method for the classification of complex planting structures with relatively fast training, good generalization, universal approximation capability, and reasonable learning accuracy.},
DOI = {10.3390/rs12223708}
}



@Article{agronomy10111762,
AUTHOR = {Zhao, Biquan and Li, Jiating and Baenziger, P. Stephen and Belamkar, Vikas and Ge, Yufeng and Zhang, Jian and Shi, Yeyin},
TITLE = {Automatic Wheat Lodging Detection and Mapping in Aerial Imagery to Support High-Throughput Phenotyping and In-Season Crop Management},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1762},
URL = {https://www.mdpi.com/2073-4395/10/11/1762},
ISSN = {2073-4395},
ABSTRACT = {Latest advances in unmanned aerial vehicle (UAV) technology and convolutional neural networks (CNNs) allow us to detect crop lodging in a more precise and accurate way. However, the performance and generalization of a model capable of detecting lodging when the plants may show different spectral and morphological signatures have not been investigated much. This study investigated and compared the performance of models trained using aerial imagery collected at two growth stages of winter wheat with different canopy phenotypes. Specifically, three CNN-based models were trained with aerial imagery collected at early grain filling stage only, at physiological maturity only, and at both stages. Results show that the multi-stage model trained by images from both growth stages outperformed the models trained by images from individual growth stages on all testing data. The mean accuracy of the multi-stage model was 89.23% for both growth stages, while the mean of the other two models were 52.32% and 84.9%, respectively. This study demonstrates the importance of diversity of training data in big data analytics, and the feasibility of developing a universal decision support system for wheat lodging detection and mapping multi-growth stages with high-resolution remote sensing imagery.},
DOI = {10.3390/agronomy10111762}
}



@Article{rs12223714,
AUTHOR = {Zeng, Qingjie and Qin, Hanlin and Yan, Xiang and Yang, Tingwu},
TITLE = {Fourier Domain Anomaly Detection and Spectral Fusion for Stripe Noise Removal of TIR Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3714},
URL = {https://www.mdpi.com/2072-4292/12/22/3714},
ISSN = {2072-4292},
ABSTRACT = {Stripe noise is a common and unwelcome noise pattern in various thermal infrared (TIR) image data including conventional TIR images and remote sensing TIR spectral images. Most existing stripe noise removal (destriping) methods are often difficult to keep a good and robust efficacy in dealing with the real-life complex noise cases. In this paper, based on the intrinsic spectral properties of TIR images and stripe noise, we propose a novel two-stage transform domain destriping method called Fourier domain anomaly detection and spectral fusion (ADSF). Considering the principal frequencies polluted by stripe noise as outliers in the statistical spectrum of TIR images, our naive idea is first to detect the potential anomalies and then correct them effectively in the Fourier domain to reconstruct a desired destriping result. More specifically, anomaly detection for stripe frequencies is achieved through a regional comparison between the original spectrum and the expected spectrum that statistically follows a generalized Laplacian regression model, and then an anomaly weight map is generated accordingly. In the correction stage, we propose a guidance-image-based spectrum fusion strategy, which integrates the original spectrum and the spectrum of a guidance image via the anomaly weight map. The final reconstruction result not only has no stripe noise but also maintains image structures and details well. Extensive real experiments are performed on conventional TIR images and remote sensing spectral images, respectively. The qualitative and quantitative assessment results demonstrate the superior effectiveness and strong robustness of the proposed method.},
DOI = {10.3390/rs12223714}
}



@Article{rs12223715,
AUTHOR = {Park, Minsoo and Tran, Dai Quoc and Jung, Daekyo and Park, Seunghee},
TITLE = {Wildfire-Detection Method Using DenseNet and CycleGAN Data Augmentation-Based Remote Camera Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3715},
URL = {https://www.mdpi.com/2072-4292/12/22/3715},
ISSN = {2072-4292},
ABSTRACT = {To minimize the damage caused by wildfires, a deep learning-based wildfire-detection technology that extracts features and patterns from surveillance camera images was developed. However, many studies related to wildfire-image classification based on deep learning have highlighted the problem of data imbalance between wildfire-image data and forest-image data. This data imbalance causes model performance degradation. In this study, wildfire images were generated using a cycle-consistent generative adversarial network (CycleGAN) to eliminate data imbalances. In addition, a densely-connected-convolutional-networks-based (DenseNet-based) framework was proposed and its performance was compared with pre-trained models. While training with a train set containing an image generated by a GAN in the proposed DenseNet-based model, the best performance result value was realized among the models with an accuracy of 98.27% and an F1 score of 98.16, obtained using the test dataset. Finally, this trained model was applied to high-quality drone images of wildfires. The experimental results showed that the proposed framework demonstrated high wildfire-detection accuracy.},
DOI = {10.3390/rs12223715}
}



@Article{rs12223726,
AUTHOR = {Sánchez-Aparicio, María and Del Pozo, Susana and Martín-Jiménez, Jose Antonio and González-González, Enrique and Andrés-Anaya, Paula and Lagüela, Susana},
TITLE = {Influence of LiDAR Point Cloud Density in the Geometric Characterization of Rooftops for Solar Photovoltaic Studies in Cities},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3726},
URL = {https://www.mdpi.com/2072-4292/12/22/3726},
ISSN = {2072-4292},
ABSTRACT = {The use of LiDAR (Light Detection and Ranging) data for the definition of the 3D geometry of roofs has been widely exploited in recent years for its posterior application in the field of solar energy. Point density in LiDAR data is an essential characteristic to be taken into account for the accurate estimation of roof geometry: area, orientation and slope. This paper presents a comparative study between LiDAR data of different point densities: 0.5, 1, 2 and 14 points/m2 for the measurement of the area of roofs of residential and industrial buildings. The data used for the study are the LiDAR data freely available by the Spanish Institute of Geography (IGN), which is offered according to the INSPIRE Directive. The results obtained show different behaviors for roofs with an area below and over 200 m2. While the use of low-density point clouds (0.5 point/m2) presents significant errors in the estimation of the area, the use of point clouds with higher density (1 or 2 points/m2) implies a great improvement in the area results, with no significant difference among them. The use of high-density point clouds (14 points/m2) also implies an improvement of the results, although the accuracy does not increase in the same ratio as the increase in density regarding 1 or 2 points/m2. Thus, the conclusion reached is that the geometrical characterization of roofs requires data acquisition with point density of 1 or 2 points/m2, and that higher point densities do not improve the results with the same intensity as they increase computation time.},
DOI = {10.3390/rs12223726}
}



@Article{s20226485,
AUTHOR = {Stuparu, Delia-Georgiana and Ciobanu, Radu-Ioan and Dobre, Ciprian},
TITLE = {Vehicle Detection in Overhead Satellite Images Using a One-Stage Object Detection Model},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6485},
URL = {https://www.mdpi.com/1424-8220/20/22/6485},
ISSN = {1424-8220},
ABSTRACT = {In order to improve the traffic in large cities and to avoid congestion, advanced methods of detecting and predicting vehicle behaviour are needed. Such methods require complex information regarding the number of vehicles on the roads, their positions, directions, etc. One way to obtain this information is by analyzing overhead images collected by satellites or drones, and extracting information from them through intelligent machine learning models. Thus, in this paper we propose and present a one-stage object detection model for finding vehicles in satellite images using the RetinaNet architecture and the Cars Overhead With Context dataset. By analyzing the results obtained by the proposed model, we show that it has a very good vehicle detection accuracy and a very low detection time, which shows that it can be employed to successfully extract data from real-time satellite or drone data.},
DOI = {10.3390/s20226485}
}



@Article{make2040030,
AUTHOR = {Combey, Théo and Loison, António and Faucher, Maxime and Hajri, Hatem},
TITLE = {Probabilistic Jacobian-Based Saliency Maps Attacks},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {2},
YEAR = {2020},
NUMBER = {4},
PAGES = {558--578},
URL = {https://www.mdpi.com/2504-4990/2/4/30},
ISSN = {2504-4990},
ABSTRACT = {Neural network classifiers (NNCs) are known to be vulnerable to malicious adversarial perturbations of inputs including those modifying a small fraction of the input features named sparse or L0 attacks. Effective and fast L0 attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are practical to fool NNCs but also to improve their robustness. In this paper, we show that penalising saliency maps of JSMA by the output probabilities and the input features of the NNC leads to more powerful attack algorithms that better take into account each input&rsquo;s characteristics. This leads us to introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), and demonstrate through a variety of white-box and black-box experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they are both significantly faster and more efficient than the original targeted and non-targeted versions of JSMA. Experiments also demonstrate, in some cases, very competitive results of our attacks in comparison with the Carlini-Wagner (CW) L0 attack, while remaining, like JSMA, significantly faster (WJSMA and TJSMA are more than 50 times faster than CW L0 on CIFAR-10). Therefore, our new attacks provide good trade-offs between JSMA and CW for L0 real-time adversarial testing on datasets such as the ones previously cited.},
DOI = {10.3390/make2040030}
}



@Article{smartcities3040065,
AUTHOR = {Thakker, Dhavalkumar and Mishra, Bhupesh Kumar and Abdullatif, Amr and Mazumdar, Suvodeep and Simpson, Sydney},
TITLE = {Explainable Artificial Intelligence for Developing Smart Cities Solutions},
JOURNAL = {Smart Cities},
VOLUME = {3},
YEAR = {2020},
NUMBER = {4},
PAGES = {1353--1382},
URL = {https://www.mdpi.com/2624-6511/3/4/65},
ISSN = {2624-6511},
ABSTRACT = {Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions, Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative training datasets and features engineering and less on the available domain expertise. We argue that such an approach to solution development makes the outcome of solutions less explainable, i.e., it is often not possible to explain the results of the model. There is a growing concern among policymakers in cities with this lack of explainability of AI solutions, and this is considered a major hindrance in the wider acceptability and trust in such AI-based solutions. In this work, we survey the concept of &lsquo;explainable deep learning&rsquo; as a subset of the &lsquo;explainable AI&rsquo; problem and propose a new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring application in the context of a European Commission-funded project. Monitoring of gullies and drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc., and building a DL-based classifier to detect such objects and classify blockages based on the presence and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier, the DL component detects object presence and coverage level and semantic rules designed with close consultation with experts carry out the classification. By using the expert knowledge in the flooding context, our hybrid classifier provides the flexibility on categorising the image using objects and their coverage relationships. The experimental results demonstrated with a real-world use case showed that this hybrid approach of image classification has on average 11% improvement (F-Measure) in image classification performance compared to DL-only classifier. It also has the distinct advantage of integrating experts&rsquo; knowledge on defining the decision-making rules to represent the complex circumstances and using such knowledge to explain the results.},
DOI = {10.3390/smartcities3040065}
}



@Article{s20226507,
AUTHOR = {Lu, Liang and Redondo, Carlos and Campoy, Pascual},
TITLE = {Optimal Frontier-Based Autonomous Exploration in Unconstructed Environment Using RGB-D Sensor},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6507},
URL = {https://www.mdpi.com/1424-8220/20/22/6507},
ISSN = {1424-8220},
ABSTRACT = {Aerial robots are widely used in search and rescue applications because of their small size and high maneuvering. However, designing an autonomous exploration algorithm is still a challenging and open task, because of the limited payload and computing resources on board UAVs. This paper presents an autonomous exploration algorithm for the aerial robots that shows several improvements for being used in the search and rescue tasks. First of all, an RGB-D sensor is used to receive information from the environment and the OctoMap divides the environment into obstacles, free and unknown spaces. Then, a clustering algorithm is used to filter the frontiers extracted from the OctoMap, and an information gain based cost function is applied to choose the optimal frontier. At last, the feasible path is given by A* path planner and a safe corridor generation algorithm. The proposed algorithm has been tested and compared with baseline algorithms in three different environments with the map resolutions of 0.2 m, and 0.3 m. The experimental results show that the proposed algorithm has a shorter exploration path and can save more exploration time when compared with the state of the art. The algorithm has also been validated in the real flight experiments.},
DOI = {10.3390/s20226507}
}



@Article{w12113195,
AUTHOR = {Park, Jungsu and Park, Jae-Hyeoung and Choi, June-Seok and Joo, Jin Chul and Park, Kihak and Yoon, Hyeon Cheol and Park, Cheol Young and Lee, Woo Hyoung and Heo, Tae-Young},
TITLE = {Ensemble Model Development for the Prediction of a Disaster Index in Water Treatment Systems},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3195},
URL = {https://www.mdpi.com/2073-4441/12/11/3195},
ISSN = {2073-4441},
ABSTRACT = {The quantitative analysis of the disaster effect on water supply systems can provide useful information for water supply system management. In this study, a total disaster index (TDI) was developed using open-source public data in 419 water treatment plants in Korea with 23 input variables. The TDI quantifies the possible effects or damage caused by three major disasters (typhoons, heavy rain, and earthquakes) on water supply systems. The four components (regional factor, risk factor, urgency factor, and response and recovery factor) were calculated using input variables to determine the disaster index (DI) of each disaster. The weight of the input variables was determined using principal component analysis (PCA), and the weights of the DI of three natural disasters and four components used to calculate the TDI were determined by the analytical hierarchy process (AHP). Specifically, two ensemble machine learning models, random forest (RF) and XGBoost (XGB), were used to develop models to predict the TDI. Both models predicted the TDI with the coefficient of determination and root-mean-square error-observations standard deviation ratio of 0.8435 and 0.3957 for the RF model and 0.8629 and 0.3703 for the XGB model, respectively. The relative importance analysis suggests that the number of input variables can be minimized, which improves the models&rsquo; practical applicability.},
DOI = {10.3390/w12113195}
}



@Article{app10228105,
AUTHOR = {Kim, Jung Jin and Kim, Ah-Ram and Lee, Seong-Won},
TITLE = {Artificial Neural Network-Based Automated Crack Detection and Analysis for the Inspection of Concrete Structures},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8105},
URL = {https://www.mdpi.com/2076-3417/10/22/8105},
ISSN = {2076-3417},
ABSTRACT = {The damage investigation and inspection methods for infrastructures performed in small-scale (type III) facilities usually involve a visual examination by an inspector using surveying tools (e.g., cracking, crack microscope, etc.) in the field. These methods can interfere with the subjectivity of the inspector, which may reduce the objectivity and reliability of the record. Therefore, a new image analysis technique is needed to automatically detect cracks and analyze the characteristics of the cracks objectively. In this study, an image analysis technique using deep learning is developed to detect cracks and analyze characteristics (e.g., length, and width) in images for small-scale facilities. Three stages of image processing pipeline are proposed to obtain crack detection and its characteristics. In the first and second stages, two-dimensional convolutional neural networks are used for crack image detection (e.g., classification and segmentation). Based on convolution neural network for the detection, hierarchical feature learning architecture is applied into our deep learning network. After deep learning-based detection, in the third stage, thinning and tracking algorithms are applied to analyze length and width of crack in the image. The performance of the proposed method was tested using various crack images with label and the results showed good performance of crack detection and its measurement.},
DOI = {10.3390/app10228105}
}



@Article{rs12223764,
AUTHOR = {Zhang, Peng and Du, Peijun and Lin, Cong and Wang, Xin and Li, Erzhu and Xue, Zhaohui and Bai, Xuyu},
TITLE = {A Hybrid Attention-Aware Fusion Network (HAFNet) for Building Extraction from High-Resolution Imagery and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3764},
URL = {https://www.mdpi.com/2072-4292/12/22/3764},
ISSN = {2072-4292},
ABSTRACT = {Automated extraction of buildings from earth observation (EO) data has long been a fundamental but challenging research topic. Combining data from different modalities (e.g., high-resolution imagery (HRI) and light detection and ranging (LiDAR) data) has shown great potential in building extraction. Recent studies have examined the role that deep learning (DL) could play in both multimodal data fusion and urban object extraction. However, DL-based multimodal fusion networks may encounter the following limitations: (1) the individual modal and cross-modal features, which we consider both useful and important for final prediction, cannot be sufficiently learned and utilized and (2) the multimodal features are fused by a simple summation or concatenation, which appears ambiguous in selecting cross-modal complementary information. In this paper, we address these two limitations by proposing a hybrid attention-aware fusion network (HAFNet) for building extraction. It consists of RGB-specific, digital surface model (DSM)-specific, and cross-modal streams to sufficiently learn and utilize both individual modal and cross-modal features. Furthermore, an attention-aware multimodal fusion block (Att-MFBlock) was introduced to overcome the fusion problem by adaptively selecting and combining complementary features from each modality. Extensive experiments conducted on two publicly available datasets demonstrated the effectiveness of the proposed HAFNet for building extraction.},
DOI = {10.3390/rs12223764}
}



@Article{s20226545,
AUTHOR = {Liu, Huan and Li, Shiyong and Sun, Wei},
TITLE = {Resource Allocation for Edge Computing without Using Cloud Center in Smart Home Environment: A Pricing Approach},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6545},
URL = {https://www.mdpi.com/1424-8220/20/22/6545},
ISSN = {1424-8220},
ABSTRACT = {Recently, more and more smart homes have become one of important parts of home infrastructure. However, most of the smart home applications are not interconnected and remain isolated. They use the cloud center as the control platform, which increases the risk of link congestion and data security. Thus, in the future, smart homes based on edge computing without using cloud center become an important research area. In this paper, we assume that all applications in a smart home environment are composed of edge nodes and users. In order to maximize the utility of users, we assume that all users and edge nodes are placed in a market and formulate a pricing resource allocation model with utility maximization. We apply the Lagrangian method to analyze the model, so an edge node (provider in the market) allocates its resources to a user (customer in the market) based on the prices of resources and the utility related to the preference of users. To obtain the optimal resource allocation, we propose a pricing-based resource allocation algorithm by using low-pass filtering scheme and conform that the proposed algorithm can achieve an optimum within reasonable convergence times through some numerical examples.},
DOI = {10.3390/s20226545}
}



@Article{rs12223766,
AUTHOR = {Shawon, Ashifur Rahman and Ko, Jonghan and Jeong, Seungtaek and Shin, Taehwan and Lee, Kyung Do and Shim, Sang In},
TITLE = {Two-Dimensional Simulation of Barley Growth and Yield Using a Model Integrated with Remote-Controlled Aerial Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3766},
URL = {https://www.mdpi.com/2072-4292/12/22/3766},
ISSN = {2072-4292},
ABSTRACT = {It is important to be able to predict the yield and monitor the growth conditions of crops in the field to increase productivity. One way to assess field-based geospatial crop productivity is by integrating a crop model with a remote-controlled aerial system (RAS). The objective of this study was to simulate spatiotemporal barley growth and yield based on the development of a crop-modeling system integrated with RAS-based remote sensing images. We performed field experiments to obtain ground truth data and RAS images of crop growth conditions and yields at Chonnam National University (CNU), Gwangju, South Korea in 2018, and at Gyeongsang National University (GNU), Jinju, South Gyeongsang, South Korea in 2018 and 2019. In model calibration, there was no significant difference (p = 0.12) between the simulated barley yields and measured yields, based on a two-sample t-test at CNU in 2018. In model validation, there was no significant difference between simulated yields and measured yields at p = 0.98 and 0.76, according to two-sample t-tests at GNU in 2018 and 2019, respectively. The remote sensing-integrated crop model accurately reproduced geospatial variations in barley yield and growth variables. The results demonstrate that the crop modeling approach is useful for monitoring at-field barley conditions.},
DOI = {10.3390/rs12223766}
}



@Article{s20226554,
AUTHOR = {Pérez, Javier and Guardiola, Jose-Luis and Perez, Alberto J. and Perez-Cortes, Juan-Carlos},
TITLE = {Probabilistic Evaluation of 3D Surfaces Using Statistical Shape Models (SSM)},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6554},
URL = {https://www.mdpi.com/1424-8220/20/22/6554},
ISSN = {1424-8220},
ABSTRACT = {Inspecting a 3D object which shape has elastic manufacturing tolerances in order to find defects is a challenging and time-consuming task. This task usually involves humans, either in the specification stage followed by some automatic measurements, or in other points along the process. Even when a detailed inspection is performed, the measurements are limited to a few dimensions instead of a complete examination of the object. In this work, a probabilistic method to evaluate 3D surfaces is presented. This algorithm relies on a training stage to learn the shape of the object building a statistical shape model. Making use of this model, any inspected object can be evaluated obtaining a probability that the whole object or any of its dimensions are compatible with the model, thus allowing to easily find defective objects. Results in simulated and real environments are presented and compared to two different alternatives.},
DOI = {10.3390/s20226554}
}



@Article{rs12223776,
AUTHOR = {Tassi, Andrea and Vizzari, Marco},
TITLE = {Object-Oriented LULC Classification in Google Earth Engine Combining SNIC, GLCM, and Machine Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3776},
URL = {https://www.mdpi.com/2072-4292/12/22/3776},
ISSN = {2072-4292},
ABSTRACT = {Google Earth Engine (GEE) is a versatile cloud platform in which pixel-based (PB) and object-oriented (OO) Land Use&ndash;Land Cover (LULC) classification approaches can be implemented, thanks to the availability of the many state-of-art functions comprising various Machine Learning (ML) algorithms. OO approaches, including both object segmentation and object textural analysis, are still not common in the GEE environment, probably due to the difficulties existing in concatenating the proper functions, and in tuning the various parameters to overcome the GEE computational limits. In this context, this work is aimed at developing and testing an OO classification approach combining the Simple Non-Iterative Clustering (SNIC) algorithm to identify spatial clusters, the Gray-Level Co-occurrence Matrix (GLCM) to calculate cluster textural indices, and two ML algorithms (Random Forest (RF) or Support Vector Machine (SVM)) to perform the final classification. A Principal Components Analysis (PCA) is applied to the main seven GLCM indices to synthesize in one band the textural information used for the OO classification. The proposed approach is implemented in a user-friendly, freely available GEE code useful to perform the OO classification, tuning various parameters (e.g., choose the input bands, select the classification algorithm, test various segmentation scales) and compare it with a PB approach. The accuracy of OO and PB classifications can be assessed both visually and through two confusion matrices that can be used to calculate the relevant statistics (producer&rsquo;s, user&rsquo;s, overall accuracy (OA)). The proposed methodology was broadly tested in a 154 km2 study area, located in the Lake Trasimeno area (central Italy), using Landsat 8 (L8), Sentinel 2 (S2), and PlanetScope (PS) data. The area was selected considering its complex LULC mosaic mainly composed of artificial surfaces, annual and permanent crops, small lakes, and wooded areas. In the study area, the various tests produced interesting results on the different datasets (OA: PB RF (L8 = 72.7%, S2 = 82%, PS = 74.2), PB SVM (L8 = 79.1%, S2 = 80.2%, PS = 74.8%), OO RF (L8 = 64%, S2 = 89.3%, PS = 77.9), OO SVM (L8 = 70.4, S2 = 86.9%, PS = 73.9)). The broad code application demonstrated very good reliability of the whole process, even though the OO classification process resulted, sometimes, too demanding on higher resolution data, considering the available computational GEE resources.},
DOI = {10.3390/rs12223776}
}



@Article{s20226585,
AUTHOR = {Zhang, Zichen and Boubin, Jayson and Stewart, Christopher and Khanal, Sami},
TITLE = {Whole-Field Reinforcement Learning: A Fully Autonomous Aerial Scouting Method for Precision Agriculture},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6585},
URL = {https://www.mdpi.com/1424-8220/20/22/6585},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial systems (UAS) are increasingly used in precision agriculture to collect crop health related data. UAS can capture data more often and more cost-effectively than sending human scouts into the field. However, in large crop fields, flight time, and hence data collection, is limited by battery life. In a conventional UAS approach, human operators are required to exchange depleted batteries many times, which can be costly and time consuming. In this study, we developed a novel, fully autonomous aerial scouting approach that preserves battery life by sampling sections of a field for sensing and predicting crop health for the whole field. Our approach uses reinforcement learning (RL) and convolutional neural networks (CNN) to accurately and autonomously sample the field. To develop and test the approach, we ran flight simulations on an aerial image dataset collected from an 80-acre corn field. The excess green vegetation Index was used as a proxy for crop health condition. Compared to the conventional UAS scouting approach, the proposed scouting approach sampled 40% of the field, predicted crop health with 89.8% accuracy, reduced labor cost by 4.8&times; and increased agricultural profits by 1.36&times;.},
DOI = {10.3390/s20226585}
}



@Article{rs12223789,
AUTHOR = {Li, Bo and Gan, Zhigang and Chen, Daqing and Sergey Aleksandrovich, Dyachenko},
TITLE = {UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2072-4292/12/22/3789},
ISSN = {2072-4292},
ABSTRACT = {This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.},
DOI = {10.3390/rs12223789}
}



@Article{rs12223783,
AUTHOR = {Khanal, Sami and KC, Kushal and Fulton, John P. and Shearer, Scott and Ozkan, Erdal},
TITLE = {Remote Sensing in Agriculture—Accomplishments, Limitations, and Opportunities},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3783},
URL = {https://www.mdpi.com/2072-4292/12/22/3783},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34%), followed by the United States (20%) and China (11%). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability.},
DOI = {10.3390/rs12223783}
}



@Article{app10228189,
AUTHOR = {Lee, Sunmin and Baek, Won-Kyung and Jung, Hyung-Sup and Lee, Saro},
TITLE = {Susceptibility Mapping on Urban Landslides Using Deep Learning Approaches in Mt. Umyeon},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8189},
URL = {https://www.mdpi.com/2076-3417/10/22/8189},
ISSN = {2076-3417},
ABSTRACT = {In recent years, the incidence of localized heavy rainfall has increased as abnormal weather events occur more frequently. In densely populated urban areas, this type of heavy rain can cause extreme landslide damage, so that it is necessary to estimate and analyze the susceptibility of future landslides. In this regard, deep learning (DL) methodologies have been used to identify areas prone to landslides recently. Therefore, in this study, DL methodologies, including a deep neural network (DNN), kernel-based DNN, and convolutional neural network (CNN) were used to identify areas where landslides could occur. As a detailed step for this purpose, landslide occurrence was first determined as landslide inventory through aerial photographs with comparative analysis using field survey data; a training set was built for model training through oversampling based on the landslide inventory. A total of 17 landslide influencing variables that influence the frequency of landslides by topography and geomorphology, as well as soil and forest variables, were selected to establish a landslide inventory. Then models were built using DNN, kernel-based DNN, and CNN models, and the susceptibility of landslides in the study area was determined. Model performance was evaluated through the average precision (AP) score and root mean square error (RMSE) for each of the three models. Finally, DNN, kernel-based DNN, and CNN models showed performances of 99.45%, 99.44%, and 99.41%, and RMSE values of 0.1694, 0.1806, and 0.1747, respectively. As a result, all three models showed similar performance, indicating excellent predictive ability of the models developed in this study. The information of landslides occurring in urban areas, which cause a great damage even with a small number of occurrences, can provide a basis for reference to the government and local authorities for urban landslide management.},
DOI = {10.3390/app10228189}
}



@Article{rs12223797,
AUTHOR = {Radke, David and Radke, Daniel and Radke, John},
TITLE = {Beyond Measurement: Extracting Vegetation Height from High Resolution Imagery with Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3797},
URL = {https://www.mdpi.com/2072-4292/12/22/3797},
ISSN = {2072-4292},
ABSTRACT = {Measuring and monitoring the height of vegetation provides important insights into forest age and habitat quality. These are essential for the accuracy of applications that are highly reliant on up-to-date and accurate vegetation data. Current vegetation sensing practices involve ground survey, photogrammetry, synthetic aperture radar (SAR), and airborne light detection and ranging sensors (LiDAR). While these methods provide high resolution and accuracy, their hardware and collection effort prohibits highly recurrent and widespread collection. In response to the limitations of current methods, we designed Y-NET, a novel deep learning model to generate high resolution models of vegetation from highly recurrent multispectral aerial imagery and elevation data. Y-NET&rsquo;s architecture uses convolutional layers to learn correlations between different input features and vegetation height, generating an accurate vegetation surface model (VSM) at 1&times;1 m resolution. We evaluated Y-NET on 235 km2 of the East San Francisco Bay Area and find that Y-NET achieves low error from LiDAR when tested on new locations. Y-NET also achieves an R2 of 0.83 and can effectively model complex vegetation through side-by-side visual comparisons. Furthermore, we show that Y-NET is able to identify instances of vegetation growth and mitigation by comparing aerial imagery and LiDAR collected at different times.},
DOI = {10.3390/rs12223797}
}



@Article{rs12223796,
AUTHOR = {Rashidi, Maria and Mohammadi, Masoud and Sadeghlou Kivi, Saba and Abdolvand, Mohammad Mehdi and Truong-Hong, Linh and Samali, Bijan},
TITLE = {A Decade of Modern Bridge Monitoring Using Terrestrial Laser Scanning: Review and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3796},
URL = {https://www.mdpi.com/2072-4292/12/22/3796},
ISSN = {2072-4292},
ABSTRACT = {Over the last decade, particular interest in using state-of-the-art emerging technologies for inspection, assessment, and management of civil infrastructures has remarkably increased. Advanced technologies, such as laser scanners, have become a suitable alternative for labor intensive, expensive, and unsafe traditional inspection and maintenance methods, which encourage the increasing use of this technology in construction industry, especially in bridges. This paper aims to provide a thorough mixed scientometric and state-of-the-art review on the application of terrestrial laser scanners (TLS) in bridge engineering and explore investigations and recommendations of researchers in this area. Following the review, more than 1500 research publications were collected, investigated and analyzed through a two-fold literature search published within the last decade from 2010 to 2020. Research trends, consisting of dominated sub-fields, co-occurrence of keywords, network of researchers and their institutions, along with the interaction of research networks, were quantitatively analyzed. Moreover, based on the collected papers, application of TLS in bridge engineering and asset management was reviewed according to four categories including (1) generation of 3D model, (2) quality inspection, (3) structural assessment, and (4) bridge information modeling (BrIM). Finally, the paper identifies the current research gaps, future directions obtained from the quantitative analysis, and in-depth discussions of the collected papers in this area.},
DOI = {10.3390/rs12223796}
}



@Article{rs12223808,
AUTHOR = {Su, Jinhua and Bai, Yanbing and Wang, Xingrui and Lu, Dong and Zhao, Bo and Yang, Hanfang and Mas, Erick and Koshimura, Shunichi},
TITLE = {Technical Solution Discussion for Key Challenges of Operational Convolutional Neural Network-Based Building-Damage Assessment from Satellite Imagery: Perspective from Benchmark xBD Dataset},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3808},
URL = {https://www.mdpi.com/2072-4292/12/22/3808},
ISSN = {2072-4292},
ABSTRACT = {Earth Observation satellite imaging helps building diagnosis during a disaster. Several models are put forward on the xBD dataset, which can be divided into two levels: the building level and the pixel level. Models from two levels evolve into several versions that will be reviewed in this paper. There are four key challenges hindering researchers from moving forward on this task, and this paper tries to give technical solutions. First, metrics on different levels could not be compared directly. We put forward a fairer metric and give a method to convert between metrics of two levels. Secondly, drone images may be another important source, but drone data may have only a post-disaster image. This paper shows and compares methods of directly detecting and generating. Thirdly, the class imbalance is a typical feature of the xBD dataset and leads to a bad F1 score for minor damage and major damage. This paper provides four specific data resampling strategies, which are Main-Label Over-Sampling (MLOS), Discrimination After Cropping (DAC), Dilation of Area with Minority (DAM) and Synthetic Minority Over-Sampling Technique (SMOTE), as well as cost-sensitive re-weighting schemes. Fourthly, faster prediction meets the need for a real-time situation. This paper recommends three specific methods, feature-map subtraction, parameter sharing, and knowledge distillation. Finally, we developed our AI-driven Damage Diagnose Platform (ADDP). This paper introduces the structure of ADDP and technical details. Customized settings, interface preview, and upload and download satellite images are major services our platform provides.},
DOI = {10.3390/rs12223808}
}



@Article{s20226680,
AUTHOR = {Sayeed, Mohd Abuzar and Kumar, Rajesh and Sharma, Vishal and Sayeed, Mohd Asim},
TITLE = {Efficient Deployment with Throughput Maximization for UAVs Communication Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6680},
URL = {https://www.mdpi.com/1424-8220/20/22/6680},
ISSN = {1424-8220},
ABSTRACT = {The article presents a throughput maximization approach for UAV assisted ground networks. Throughput maximization involves minimizing delay and packet loss through UAV trajectory optimization, reinforcing the congested nodes and transmission channels. The aggressive reinforcement policy is achieved by characterizing nodes, links, and overall topology through delay, loss, throughput, and distance. A position-aware graph neural network (GNN) is used for characterization, prediction, and dynamic UAV trajectory enhancement. To establish correctness, the proposed approach is validated against optimized link state routing (OLSR) driven UAV assisted ground networks. The proposed approach considerably outperforms the classical approach by demonstrating significant gains in throughput and packet delivery ratio with notable decrements in delay and packet loss. The performance analysis of the proposed approach against software-defined UAVs (U-S) and UAVs as base stations (U-B) verifies the consistency and gains in average throughput while minimizing delay and packet loss. The scalability test of the proposed approach is performed by varying data rates and the number of UAVs.},
DOI = {10.3390/s20226680}
}



@Article{rs12223834,
AUTHOR = {Xia, Junshi and Yokoya, Naoto and Pham, Tien Dat},
TITLE = {Probabilistic Mangrove Species Mapping with Multiple-Source Remote-Sensing Datasets Using Label Distribution Learning in Xuan Thuy National Park, Vietnam},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3834},
URL = {https://www.mdpi.com/2072-4292/12/22/3834},
ISSN = {2072-4292},
ABSTRACT = {Mangrove forests play an important role in maintaining water quality, mitigating climate change impacts, and providing a wide range of ecosystem services. Effective identification of mangrove species using remote-sensing images remains a challenge. The combinations of multi-source remote-sensing datasets (with different spectral/spatial resolution) are beneficial to the improvement of mangrove tree species discrimination. In this paper, various combinations of remote-sensing datasets including Sentinel-1 dual-polarimetric synthetic aperture radar (SAR), Sentinel-2 multispectral, and Gaofen-3 full-polarimetric SAR data were used to classify the mangrove communities in Xuan Thuy National Park, Vietnam. The mixture of mangrove communities consisting of small and shrub mangrove patches is generally difficult to separate using low/medium spatial resolution. To alleviate this problem, we propose to use label distribution learning (LDL) to provide the probabilistic mapping of tree species, including Sonneratia caseolaris (SC), Kandelia obovata (KO), Aegiceras corniculatum (AC), Rhizophora stylosa (RS), and Avicennia marina (AM). The experimental results show that the best classification performance was achieved by an integration of Sentinel-2 and Gaofen-3 datasets, demonstrating that full-polarimetric Gaofen-3 data is superior to the dual-polarimetric Sentinel-1 data for mapping mangrove tree species in the tropics.},
DOI = {10.3390/rs12223834}
}



@Article{rs12223839,
AUTHOR = {Tian, Xiaomin and Chen, Long and Zhang, Xiaoli and Chen, Erxue},
TITLE = {Improved Prototypical Network Model for Forest Species Classification in Complex Stand},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3839},
URL = {https://www.mdpi.com/2072-4292/12/22/3839},
ISSN = {2072-4292},
ABSTRACT = {Deep learning has become an effective method for hyperspectral image classification. However, the high band correlation and data volume associated with airborne hyperspectral images, and the insufficiency of training samples, present challenges to the application of deep learning in airborne image classification. Prototypical networks are practical deep learning networks that have demonstrated effectiveness in handling small-sample classification. In this study, an improved prototypical network is proposed (by adding L2 regularization to the convolutional layer and dropout to the maximum pooling layer) to address the problem of overfitting in small-sample classification. The proposed network has an optimal sample window for classification, and the window size is related to the area and distribution of the study area. After performing dimensionality reduction using principal component analysis, the time required for training using hyperspectral images shortened significantly, and the test accuracy increased drastically. Furthermore, when the size of the sample window was 27 &times; 27 after dimensionality reduction, the overall accuracy of forest species classification was 98.53%, and the Kappa coefficient was 0.9838. Therefore, by using an improved prototypical network with a sample window of an appropriate size, the network yielded desirable classification results, thereby demonstrating its suitability for the fine classification and mapping of tree species.},
DOI = {10.3390/rs12223839}
}



@Article{f11121239,
AUTHOR = {Scharvogel, Daniel and Brandmeier, Melanie and Weis, Manuel},
TITLE = {A Deep Learning Approach for Calamity Assessment Using Sentinel-2 Data},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1239},
URL = {https://www.mdpi.com/1999-4907/11/12/1239},
ISSN = {1999-4907},
ABSTRACT = {The number of severe storm events has increased in recent decades due to climate change. These storms are one of the main causes for timber loss in European forests and damaged areas are prone to further degradation by, for example, bark beetle infestations. Usually, manual mapping of damaged areas based on aerial photographs is conducted by forest departments. This is very time-consuming and therefore automatic detection of windthrows based on active and passive remote sensing data is an ongoing research topic. In this study we evaluated state-of-the-art Convolutional Neural Networks (CNNs) in combination with Geographic Information Systems (GIS) for calamity assessment. The study area is in in the northern part of Hesse (Germany) and was covered by twelve Sentinel-2 scenes from 2018. Labels of damaged areas from the Friedericke storm (18 January 2018) were provided by HessenForst. We conducted several experiments based on a custom U-Net setup to derive the optimal architecture and input data as well as to assess the transferability of the model. Results highlight the possibility to detect damaged forest areas using Sentinel-2 data. Using a binary classification, accuracies of more than 92% were achieved with an Intersection over Union (IoU) score of 46.6%. The proposed workflow was integrated into ArcGIS and is suitable for fast detection of damaged areas directly after a storm and for disaster management but is limited by the deca-meter spatial resolution of the Sentinel-2 data.},
DOI = {10.3390/f11121239}
}



@Article{rs12233849,
AUTHOR = {Kolosov, Kirill and Miller, Alexander and Miller, Boris},
TITLE = {Robust Data Fusion of UAV Navigation Measurements with Application to the Landing System},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3849},
URL = {https://www.mdpi.com/2072-4292/12/23/3849},
ISSN = {2072-4292},
ABSTRACT = {To perform precise approach and landing concerning an aircraft in automatic mode, local airfield-based landing systems are used. For joint processing of measurements of the onboard inertial navigation systems (INS), altimeters and local landing systems, the Kalman filter is usually used. The application of the quadratic criterion in the Kalman filter entails the well-known problem of high sensitivity of the estimate to anomalous measurement errors. During the automatic approach phase, abnormal navigation errors can lead to disaster, so the data fusion algorithm must automatically identify and isolate abnormal measurements. This paper presents a recurrent filtering algorithm that is resistant to anomalous errors in measurements and considers its application in the data fusion problem for landing system measurements with onboard sensor measurements&mdash;INS and altimeters. The robustness of the estimate is achieved through the combined use of the least modulus method and the Kalman filter. To detect and isolate failures the chi-square criterion is used. It makes possible the customization of the algorithm in accordance with the requirements for false alarm probability and the alarm missing probability. Testing results of the robust filtering algorithm are given both for synthesized data and for real measurements.},
DOI = {10.3390/rs12233849}
}



@Article{s20236732,
AUTHOR = {Qi, Haixia and Zhu, Bingyu and Wu, Zeyu and Liang, Yu and Li, Jianwen and Wang, Leidi and Chen, Tingting and Lan, Yubin and Zhang, Lei},
TITLE = {Estimation of Peanut Leaf Area Index from Unmanned Aerial Vehicle Multispectral Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6732},
URL = {https://www.mdpi.com/1424-8220/20/23/6732},
ISSN = {1424-8220},
ABSTRACT = {Leaf area index (LAI) is used to predict crop yield, and unmanned aerial vehicles (UAVs) provide new ways to monitor LAI. In this study, we used a fixed-wing UAV with multispectral cameras for remote sensing monitoring. We conducted field experiments with two peanut varieties at different planting densities to estimate LAI from multispectral images and establish a high-precision LAI prediction model. We used eight vegetation indices (VIs) and developed simple regression and artificial neural network (BPN) models for LAI and spectral VIs. The empirical model was calibrated to estimate peanut LAI, and the best model was selected from the coefficient of determination and root mean square error. The red (660 nm) and near-infrared (790 nm) bands effectively predicted peanut LAI, and LAI increased with planting density. The predictive accuracy of the multiple regression model was higher than that of the single linear regression models, and the correlations between Modified Red-Edge Simple Ratio Index (MSR), Ratio Vegetation Index (RVI), Normalized Difference Vegetation Index (NDVI), and LAI were higher than the other indices. The combined VI BPN model was more accurate than the single VI BPN model, and the BPN model accuracy was higher. Planting density affects peanut LAI, and reflectance-based vegetation indices can help predict LAI.},
DOI = {10.3390/s20236732}
}



@Article{robotics9040100,
AUTHOR = {Roy, Raphaëlle N. and Drougard, Nicolas and Gateau, Thibault and Dehais, Frédéric and Chanel, Caroline P. C.},
TITLE = {How Can Physiological Computing Benefit Human-Robot Interaction?},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {100},
URL = {https://www.mdpi.com/2218-6581/9/4/100},
ISSN = {2218-6581},
ABSTRACT = {As systems grow more automatized, the human operator is all too often overlooked. Although human-robot interaction (HRI) can be quite demanding in terms of cognitive resources, the mental states (MS) of the operators are not yet taken into account by existing systems. As humans are no providential agents, this lack can lead to hazardous situations. The growing number of neurophysiology and machine learning tools now allows for efficient operators&rsquo; MS monitoring. Sending feedback on MS in a closed-loop solution is therefore at hand. Involving a consistent automated planning technique to handle such a process could be a significant asset. This perspective article was meant to provide the reader with a synthesis of the significant literature with a view to implementing systems that adapt to the operator&rsquo;s MS to improve human-robot operations&rsquo; safety and performance. First of all, the need for this approach is detailed regarding remote operation, an example of HRI. Then, several MS identified as crucial for this type of HRI are defined, along with relevant electrophysiological markers. A focus is made on prime degraded MS linked to time-on-task and task demands, as well as collateral MS linked to system outputs (i.e., feedback and alarms). Lastly, the principle of symbiotic HRI is detailed and one solution is proposed to include the operator state vector into the system using a mixed-initiative decisional framework to drive such an interaction.},
DOI = {10.3390/robotics9040100}
}



@Article{en13236250,
AUTHOR = {Ayele, Yonas Zewdu and Aliyari, Mostafa and Griffiths, David and Droguett, Enrique Lopez},
TITLE = {Automatic Crack Segmentation for UAV-Assisted Bridge Inspection},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6250},
URL = {https://www.mdpi.com/1996-1073/13/23/6250},
ISSN = {1996-1073},
ABSTRACT = {Bridges are a critical piece of infrastructure in the network of road and rail transport system. Many of the bridges in Norway (in Europe) are at the end of their lifespan, therefore regular inspection and maintenance are critical to ensure the safety of their operations. However, the traditional inspection procedures and resources required are so time consuming and costly that there exists a significant maintenance backlog. The central thrust of this paper is to demonstrate the significant benefits of adapting a Unmanned Aerial Vehicle (UAV)-assisted inspection to reduce the time and costs of bridge inspection and established the research needs associated with the processing of the (big) data produced by such autonomous technologies. In this regard, a methodology is proposed for analysing the bridge damage that comprises three key stages, (i) data collection and model training, where one performs experiments and trials to perfect drone flights for inspection using case study bridges to inform and provide necessary (big) data for the second key stage, (ii) 3D construction, where one built 3D models that offer a permanent record of element geometry for each bridge asset, which could be used for navigation and control purposes, (iii) damage identification and analysis, where deep learning-based data analytics and modelling are applied for processing and analysing UAV image data and to perform bridge damage performance assessment. The proposed methodology is exemplified via UAV-assisted inspection of Skodsberg bridge, a 140 m prestressed concrete bridge, in the Viken county in eastern Norway.},
DOI = {10.3390/en13236250}
}



@Article{rs12233892,
AUTHOR = {Egli, Sebastian and Höpke, Martin},
TITLE = {CNN-Based Tree Species Classification Using High Resolution RGB Image Data from Automated UAV Observations},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3892},
URL = {https://www.mdpi.com/2072-4292/12/23/3892},
ISSN = {2072-4292},
ABSTRACT = {Data on the distribution of tree species are often requested by forest managers, inventory agencies, foresters as well as private and municipal forest owners. However, the automated detection of tree species based on passive remote sensing data from aerial surveys is still not sufficiently developed to achieve reliable results independent of the phenological stage, time of day, season, tree vitality and prevailing atmospheric conditions. Here, we introduce a novel tree species classification approach based on high resolution RGB image data gathered during automated UAV flights that overcomes these insufficiencies. For the classification task, a computationally lightweight convolutional neural network (CNN) was designed. We show that with the chosen CNN model architecture, average classification accuracies of 92% can be reached independently of the illumination conditions and the phenological stages of four different tree species. We also show that a minimal ground sampling density of 1.6 cm/px is needed for the classification model to be able to make use of the spatial-structural information in the data. Finally, to demonstrate the applicability of the presented approach to derive spatially explicit tree species information, a gridded product is generated that yields an average classification accuracy of 88%.},
DOI = {10.3390/rs12233892}
}



@Article{sym12121965,
AUTHOR = {Zhu, Juncai and Wang, Zhizhong and Wang, Songwei and Chen, Shuli},
TITLE = {Moving Object Detection Based on Background Compensation and Deep Learning},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1965},
URL = {https://www.mdpi.com/2073-8994/12/12/1965},
ISSN = {2073-8994},
ABSTRACT = {Detecting moving objects in a video sequence is an important problem in many vision-based applications. In particular, detecting moving objects when the camera is moving is a difficult problem. In this study, we propose a symmetric method for detecting moving objects in the presence of a dynamic background. First, a background compensation method is used to detect the proposed region of motion. Next, in order to accurately locate the moving objects, we propose a convolutional neural network-based method called YOLOv3-SOD for detecting all objects in the image, which is lightweight and specifically designed for small objects. Finally, the moving objects are determined by fusing the results obtained by motion detection and object detection. Missed detections are recalled according to the temporal and spatial information in adjacent frames. A dataset is not currently available specifically for moving object detection and recognition, and thus, we have released the MDR105 dataset comprising three classes with 105 videos. Our experiments demonstrated that the proposed algorithm can accurately detect moving objects in various scenarios with good overall performance.},
DOI = {10.3390/sym12121965}
}



