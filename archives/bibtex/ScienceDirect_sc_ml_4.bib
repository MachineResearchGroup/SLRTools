@article{PUNDIR2021103084,
title = {A Systematic Review of Quality of Service in Wireless Sensor Networks using Machine Learning: Recent Trend and Future Vision},
journal = {Journal of Network and Computer Applications},
volume = {188},
pages = {103084},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103084},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001065},
author = {Meena Pundir and Jasminder Kaur Sandhu},
keywords = {Wireless Sensor Network, Quality of Service, Machine Learning, Layered architecture, Network-specific parameters, Application-specific parameters},
abstract = {Wireless Sensor Network (WSN) is used in different research areas such as military, industry, healthcare, agriculture, Internet of Things (IoT), transportation, and smart cities. The reason behind this increased usage is the rapid development of smart sensors. There is a challenging need to satisfy the Quality of Service (QoS) requirements in different applications due to the dynamic network condition, heterogeneous traffic flows and resource-constrained behaviour of sensor nodes. Optimizing the QoS in terms of performance, privacy and security levels is an open issue in the WSN. It has limited resources and is deployed in hostile environment where achieving high performance is difficult. This performance level is categorized into four subcategories: deployment phase, layered architecture, measurability, network and application specific parameter. Privacy and security levels are divided into four parameters: security, confidentiality, integrity and safety. A systematic review is presented in this paper based on QoS parameters in the light of Machine Learning (ML) techniques. It also provides a methodological framework for the performance parameters. This study presents a statistical analysis of the past ten years ranging from 2011 to 2021 on various ML techniques used for the QoS parameters. Finally, the author's vision is highlighted with some discussion on the open issues which forms the baseline for the future research directions.}
}
@article{LU202034,
title = {LSTM variants meet graph neural networks for road speed prediction},
journal = {Neurocomputing},
volume = {400},
pages = {34-45},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303775},
author = {Zhilong Lu and Weifeng Lv and Yabin Cao and Zhipu Xie and Hao Peng and Bowen Du},
keywords = {Neural network, LSTM, LSTM Variant, GNN, Road speed prediction},
abstract = {Traffic flow prediction is a fundamental issue in smart cities and plays an important role in urban traffic planning and management. An accurate predictive model can help individuals make reliable travel plans and choose optimal routes while efficiently helping administrators maintain traffic order. Road speed prediction, which is a sub-task of traffic flow forecasting, is challenging due to the complicated spatial dependencies characterizing road networks and dynamic temporal traffic patterns. Given the power of recurrent neural networks (RNNs) in learning temporal relations and graph neural networks (GNNs) in integrating graph-structured and node-attributed features, in this paper, we design a novel graph LSTM (GLSTM) framework to capture spatial-temporal representations in road speed forecasting. More specifically, we first present a temporal directed attributed graph to model complex traffic flow. Then, to take advantage of the structure properties and graph features, we employ a message-passing mechanism for feature aggregation and updating. Finally, we further implement several variants of LSTMs with a GN block under the encoder-decoder framework to model spatial-temporal dependencies. The experiments show that our proposed model is able to fully utilize both the road latent graph structure and traffic speed to forecast the road state during future periods. The results on two real-world datasets show that our GLSTM can outperform state-of-the-art baseline methods by up to 32.8% in terms of MAE, 43.2% in terms of MAPE and 23.1% in terms of RMSE.}
}
@article{GUPTA2022108439,
title = {Cybersecurity of multi-cloud healthcare systems: A hierarchical deep learning approach},
journal = {Applied Soft Computing},
pages = {108439},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108439},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622000175},
author = {Lav Gupta and Tara Salman and Ali Ghubaish and Devrim Unal and Abdulla Khalid Al-Ali and Raj Jain},
keywords = {Cloud networks, Edge clouds, Network function virtualization, Critical healthcare, Deep neural networks, Stacked autoencoders, Hierarchical neural networks, Multi-cloud systems},
abstract = {With the increase in sophistication and connectedness of the healthcare networks, their attack surfaces and vulnerabilities increase significantly. Malicious agents threaten patients’ health and life by stealing or altering data as it flows among the multiple domains of healthcare networks. The problem is likely to exacerbate with the increasing use of IoT devices, edge, and core clouds in the next generation healthcare networks. Presented in this paper is MUSE, a system of deep hierarchical stacked neural networks for timely and accurate detection of malicious activity that leads to alteration of meta-information or payload of the dataflow between the IoT gateway, edge and core clouds. Smaller models at the edge clouds take substantially less time to train as compared to the large models in the core cloud. To improve the speed of training and accuracy of detection of large core cloud models, the MUSE system uses a novel method of merging and aggregating layers of trained edge cloud models to construct a partly pre-trained core cloud model. As a result, the model in the core cloud takes substantially smaller number of epochs (6 to 8) and, consequently, less time, compared to edge clouds, training of which take 35 to 40 epochs to converge. With the help of extensive evaluations, it is shown that with the MUSE system, large, merged models can be trained in significantly less time than the unmerged models that are created independently in the core cloud. Through several runs it is seen that the merged models give on an average 26.2% reduction in training times. From the experimental evaluation we demonstrate that along with fast training speeds the merged MUSE model gives high training and test accuracies, ranging from 95% to 100%, in detection of unknown attacks on dataflows. The merged model thus generalizes very well on the test data. This is a marked improvement when compared with the accuracy given by un-merged model as well as accuracy reported by other researchers with newer datasets.}
}
@article{NAEEM2020102154,
title = {Malware detection in industrial internet of things based on hybrid image visualization and deep learning model},
journal = {Ad Hoc Networks},
volume = {105},
pages = {102154},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102154},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519306948},
author = {Hamad Naeem and Farhan Ullah and Muhammad Rashid Naeem and Shehzad Khalid and Danish Vasan and Sohail Jabbar and Saqib Saeed},
keywords = {Image Visualization, Deep Learning, Industrial Internet of Things, Malware Analysis},
abstract = {Now the Industrial Internet of Things (IIoT) devices can be deployed to monitor the flow of data, the source of collection and supervision on a large scale of complex networks. It implements large networks for sending and receiving data connected by smart devices. Malware threats, which are primarily targeted at conventional computers linked to the Internet, can also be targeted at IoT machines. Therefore, a smart protection approach is needed to protect millions of IIoT users against malicious attacks. On the other hand, existing state-of - the-art malware identification methods are not better in terms of computational complexity. In this paper, we design architecture to detect malware attacks on the Industrial Internet of Things (MD-IIOT). For an in-depth analysis of malware, a methodology is proposed based on color image visualization and deep convolution neural network. The findings of the proposed method are compared to former approaches to malware detection. The experimental results indicate that the proposed method's predictive time and detection accuracy are higher than that of previous machine learning and deep learning methods.}
}
@article{D2021,
title = {A study on artificial intelligence for monitoring smart environments},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321043911},
author = {Karthika D.},
keywords = {Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies},
abstract = {Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.}
}
@article{KUYUK2018298,
title = {Real-Time Classification of Earthquake using Deep Learning},
journal = {Procedia Computer Science},
volume = {140},
pages = {298-305},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.316},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319896},
author = {H. Serdar Kuyuk and Ohno Susumu},
keywords = {Earthquake Early Warning System, Deep Learning, Convulational Neural Network, Long Short-Term Memory},
abstract = {Existing Earthquake Early Warning Systems (EEWSs) calculates the location and magnitude of an earthquake using real-time waveforms from seismic stations within a few seconds. Typically, three to six stations are necessary to estimate earthquake parameters. Waiting for primary (P-) wave information from closest stations results in a blind-zone area where the arrival of secondary (S-) wave cannot be provided around the epicenter of an earthquake. If an earthquake occurred under a city center, EEWSs would not work even though each building has a seismic sensor in a smart city in future. Here, we present a methodology to classify earthquake vibrations into near-source or far-source within one second after P-wave detection. This will allow warnings to citizens who are the residence of earthquake epicenter in case of an earthquake very close by. We trained a deep learning Long Short-Term Memory (LSTM) network for sequence-to-label classification. 305 three component accelerations recorded between 2000 and 2018 in Japan are used to train the artificial network by extracting thirteen features of one second of P-wave. The accuracy of the methodology is 98.2%. 54 out of 55 near-source waveforms classified correctly and only 2 of 80 waveforms were misclassified. We tested the LSTM network with 2018 Northern Osaka (M 6.1.) earthquakes in Japan where closest stations are correctly identified with 83.3% accuracy. Therefore, smart cities donated with smart automated shut-on/off machines and sensors will be more resilient against earthquake disaster even EEWSs are not available in the blind zone area in future.}
}
@article{BUNYAKITANON2020107433,
title = {Auto-3P: An autonomous VNF performance prediction & placement framework based on machine learning},
journal = {Computer Networks},
volume = {181},
pages = {107433},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107433},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620311221},
author = {Monchai Bunyakitanon and Aloizio Pereira {da Silva} and Xenofon Vasilakos and Reza Nejabati and Dimitra Simeonidou},
keywords = {Machine learning, Network function virtualization, End-to-End communication, Zero-Touch management, Cloud and edge computing},
abstract = {We propose Auto-3P, an Autonomous module for Virtual Network Functions Performance Prediction and Placement at network cloud and edge facilities based on Machine Learning (ML). Auto-3P augments the autonomous placement capabilities of MANagement and Orchestration frameworks (MANOs) by considering both resource availability at hosting nodes and the implied impact of a VNF node placement decisions on the whole service level end-to-end performance. Unlike that, most existing placement methods take a rather myopic approach after manual rule-based decisions, and/or based exclusively on a host-centric view that focuses merely on node-local resource availability and network metrics. We evaluate and validate Auto-3P with real-field trials in the context of a well-defined Smart City Safety use case using a real end-to-end application over a real city-based testbed. We meticulously conduct repeated tests to assess (i) the accuracy of our adopted prediction models; and (ii) their placement performance against three other existing MANO approaches, namely, a “Traditional”, a “Latency-aware” and a “Random” one, as well as against a collection of well-known Time Series Forecasting (TSF) methods. Our results show that the accuracy of our ML models outperforms the one by TSF models, with the most prominent accuracy performances being exhibited by models such as K-Nearest Neighbors Regression (K-NNR), Decision Tree (DT), and Support Vector Regression (SVR). What is more, the resulted end-to-end service level performance of our approach outperforms “Traditional”, “Latency-aware”, and Random MANO placement. Last, Auto-3P achieves load balancing at selected VNF hosts without degrading end-to-end service level delay, and without a need for a (fixed) overload threshold check, unlike what is suggested by other works in the literature for coping with heavy system-wide load conditions.}
}
@article{BOLHASANI2021100550,
title = {Deep learning applications for IoT in health care: A systematic review},
journal = {Informatics in Medicine Unlocked},
volume = {23},
pages = {100550},
year = {2021},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100550},
url = {https://www.sciencedirect.com/science/article/pii/S235291482100040X},
author = {Hamidreza Bolhasani and Maryam Mohseni and Amir Masoud Rahmani},
keywords = {Deep learning, Internet of things, Healthcare, Medical imaging, Wearable device, Systematic literature review},
abstract = {In machine learning, deep learning is the most popular topic having a wide range of applications such as computer vision, natural language processing, speech recognition, visual object detection, disease prediction, drug discovery, bioinformatics, biomedicine, etc. Of these applications, health care and medical science-related applications are dramatically on the rise. The tremendous big data growth, the Internet of Things (IoT), connected devices, and high-performance computers utilizing GPUs and TPUs are the main reasons why deep learning is so popular. Based on their specific tasks, medical IoT, digital images, electronic health record (EHR) data, genomic data, and central medical databases are the primary data sources for deep learning systems. Several potential issues such as privacy, QoS optimization, and deployment indicate the pivotal part of deep learning. In this paper, deep learning for IoT applications in health care systems is reviewed based on the Systematic Literature Review (SLR). This paper investigates the related researches, selected from among 44 published research papers, conducted within a period of ten years – 2010 to 2020. Firstly, theoretical concepts and ideas of deep learning and technical taxonomy are proposed. Afterwards, major deep learning applications for IoT in health care and medical sciences are presented through analyzing the related works. Later, the main idea, advantages, disadvantages, and limitations of each study are discussed, preceding suggestions for further research.}
}
@article{BIAN2021104490,
title = {A deep learning model for detection and tracking in high-throughput images of organoid},
journal = {Computers in Biology and Medicine},
volume = {134},
pages = {104490},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104490},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521002845},
author = {Xuesheng Bian and Gang Li and Cheng Wang and Weiquan Liu and Xiuhong Lin and Zexin Chen and Mancheung Cheung and Xiongbiao Luo},
keywords = {Organoids, High-throughput image, Tracking, Deep learning, Artificial intelligence},
abstract = {Organoid, an in vitro 3D culture, has extremely high similarity with its source organ or tissue, which creates a model in vitro that simulates the in vivo environment. Organoids have been extensively studied in cell biology, precision medicine, drug toxicity, efficacy tests, etc., which have been proven to have high research value. Periodic observation of organoids in microscopic images to obtain morphological or growth characteristics is essential for organoid research. It is difficult and time-consuming to perform manual screens for organoids, but there is no better solution in the prior art. In this paper, we established the first high-throughput organoid image dataset for organoids detection and tracking, which experienced experts annotate in detail. Moreover, we propose a novel deep neural network (DNN) that effectively detects organoids and dynamically tracks them throughout the entire culture. We divided our solution into two steps: First, the high-throughput sequential images are processed frame by frame to detect all organoids; Second, the similarities of the organoids in the adjacent frames are computed, and the organoids on the adjacent frames are matched in pairs. With the help of our proposed dataset, our model achieves organoids detection and tracking with fast speed and high accuracy, effectively reducing the burden on researchers. To our knowledge, this is the first exploration of applying deep learning to organoid tracking tasks. Experiments have demonstrated that our proposed method achieved satisfactory results on organoid detection and tracking, verifying the great potential of deep learning technology in this field.}
}
@article{LI2021103059,
title = {Network-wide traffic signal control optimization using a multi-agent deep reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {125},
pages = {103059},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103059},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21000851},
author = {Zhenning Li and Hao Yu and Guohui Zhang and Shangjia Dong and Cheng-Zhong Xu},
keywords = {Multi-agent reinforcement learning, Knowledge sharing, Adaptive traffic signal control, Deep learning, Transportation network},
abstract = {Inefficient traffic control may cause numerous problems such as traffic congestion and energy waste. This paper proposes a novel multi-agent reinforcement learning method, named KS-DDPG (Knowledge Sharing Deep Deterministic Policy Gradient) to achieve optimal control by enhancing the cooperation between traffic signals. By introducing the knowledge-sharing enabled communication protocol, each agent can access to the collective representation of the traffic environment collected by all agents. The proposed method is evaluated through two experiments respectively using synthetic and real-world datasets. The comparison with state-of-the-art reinforcement learning-based and conventional transportation methods demonstrate the proposed KS-DDPG has significant efficiency in controlling large-scale transportation networks and coping with fluctuations in traffic flow. In addition, the introduced communication mechanism has also been proven to speed up the convergence of the model without significantly increasing the computational burden.}
}
@article{AHMEDOUAMEUR2020100300,
title = {Machine learning enabled tools and methods for indoor localization using low power wireless network},
journal = {Internet of Things},
volume = {12},
pages = {100300},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100300},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520301323},
author = {Messaoud {Ahmed Ouameur} and Manouane Caza-Szoka and Daniel Massicotte},
keywords = {Low power wide area (LPWA), LoRaWAN, Indoor localization, Deep learning (DL), Machine learning (ML), WiFi, Receiver signal strength (RSS)},
abstract = {In this paper, we propose a framework for the indoor localization in non-line-of-sight (NLoS) conditions using partial knowledge of the channel state information (CSI) obtained from low power wide area (LPWA) radios. The framework is based on NLoS CSI classification using machine learning (ML) and deep learning (DL) models that leverage measurements using end-to-end LoRaWAN network. The measurement set-up provides access to not only the sensor data but also to the physical layer metrics such as the receiver signal strength (RSS), spreading factor (SF), and the frequency hoping signature to name a few. Since LoRa is based on narrow band spread spectrum modulation techniques derived from chirp spread spectrum technology, the CSI are partial in nature. We demonstrate that the partial CSI with frequency hopping signature can be efficiently exploited to predict indoor location with accuracy of more than 98% using a multilayer neural network (MNN).}
}
@article{CONLEY2022101752,
title = {Using a deep learning model to quantify trash accumulation for cleaner urban stormwater},
journal = {Computers, Environment and Urban Systems},
volume = {93},
pages = {101752},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101752},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001599},
author = {Gary Conley and Stephanie Castle Zinn and Taylor Hanson and Krista McDonald and Nicole Beck and Howard Wen},
keywords = {Urban trash, Litter, Stormwater, Machine learning, Mask R-CNN},
abstract = {With growing understanding of trash impacts on aquatic habitats throughout the world, cities increasingly face regulatory requirements to reduce trash inputs to local waterways and the ocean, but they often rely upon insufficient monitoring data to prioritize and measure trash reduction effectiveness. We present an approach designed to make urban trash monitoring more cost-efficient and align the data collected with critical information needs of cities. We quantified urban trash accumulation along roadsides using vehicle mounted cameras and a deep convolutional neural network model to identify trash in the imagery captured. We compared the trash detection performance of three different models, with the best performing model (Mask R-CNN) achieving 91% recall, 83% precision, and 77% accuracy using data collected along 84 road segments in two California Cities. Trash detection model outputs were interpreted via a statistical model to relate the proportion of image pixels identified as trash to measured trash volumes. The resulting model estimates explained 67% of the variance in measured trash volumes collected on roadsides, which is more than double the variance explained by walking visual assessments. With vastly more efficient data collection compared to the visual assessments, deep learning-based monitoring approaches can provide a stronger basis for understanding urban trash sources, changes over time, and cost-effective compliance with stormwater regulatory requirements.}
}
@article{DUAN2022,
title = {Privacy-preserving and verifiable deep learning inference based on secret sharing},
journal = {Neurocomputing},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.061},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222000807},
author = {Jia Duan and Jiantao Zhou and Yuanman Li and Caishi Huang},
keywords = {Deep neural network inference, deep learning prediction, secure multi-party computation, privacy-preserving, verifiable computation},
abstract = {Deep learning inference, providing the model utilization of deep learning, is usually deployed as a cloud-based framework for the resource-constrained client. However, the existing cloud-based frameworks suffer from severe information leakage or lead to significant increase of communication cost. In this work, we address the problem of privacy-preserving deep learning inference in a way that both the privacy of the input data and the model parameters can be protected with low communication and computational costs. Additionally, the user can verify the correctness of results with small overhead, which is very important for critical application. Specifically, by designing secure sub-protocols, we introduce a new layer to collaboratively perform the secure computations involved in the inference. With the cooperation of the secret sharing, we inject the verifiable data into the input, enabling us to check the correctness of the returned inference results. Theoretical analyses and extensive experimental results over MNIST and CIFAR10 datasets are provided to validate the superiority of our proposed privacy-preserving and verifiable deep learning inference (PVDLI) framework.}
}
@article{CHEN2022103484,
title = {MultiCycleNet: Multiple Cycles Self-Boosted Neural Network for Short-term Electric Household Load Forecasting},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103484},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103484},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007502},
author = {Rinan Chen and Chun Sing Lai and Cankun Zhong and Keda Pan and Wing W.Y. Ng and Zhanlian Li and Loi Lei Lai},
keywords = {Load forecasting, recurrent neural network, time-series forecasting, multiple historical cycles},
abstract = {Household load forecasting plays an important role in future grid planning and operation. However, compared with aggregated load forecasting, household load forecasting faces the challenge of the uncertainty of prolific load profiles. This paper presents a novel multiple cycles self-boosted neural network (MultiCycleNet) framework for household load forecasting, which aims to solve the uncertainty problem of household load profiles through the correlation analysis of electricity consumption patterns in multiple cycles. The basic idea of the proposed framework is that the predictor can learn customers’ power consumption patterns better by learning the features and contextual information of relevant load profiles in multiple historical cycles. We use two real-life datasets: 1. the household load consumption dataset from Low Carbon London project led by United Kingdom (UK) Power Networks and 2. the UK Domestic Appliance-Level Electricity (UK-DALE) dataset to evaluate the effectiveness of the proposed framework. Compared with the state-of-the-art methods, experimental results show that the proposed framework is effective and outperforms the state-of-the-art methods by 19.83%, 10.46%, 11.14% and 9.02% in terms of mean squared error, root mean squared error, mean absolute error and mean absolute percent error, respectively.}
}
@article{QU2021290,
title = {Features injected recurrent neural networks for short-term traffic speed prediction},
journal = {Neurocomputing},
volume = {451},
pages = {290-304},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.03.054},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221004380},
author = {Licheng Qu and Jiao Lyu and Wei Li and Dongfang Ma and Haiwei Fan},
keywords = {Traffic flow prediction, Deep learning, Recurrent neural networks, Autoencoder, Features fusion},
abstract = {Accurate traffic speed forecasting is critical in advanced transportation management and traveler route planing. Considering the important influences of spatial–temporal factors and excellent performance of recurrent neural networks (RNNs) in the field of time series analyzing, in this paper, the features injected recurrent neural networks (FI-RNNs) were proposed, which combines sequential time data with contextual factors to mine the potential relationship between traffic state and its context. In this model, a stacked RNN was used to learn the sequence features of traffic data. Meanwhile, a sparse Autoencoder was trained to expand the contextual features, which are high-level coding and abstract representations of contextual factors. Then an merging mechanism which injects contextual features into sequence features was explored to generate fusion features. Finally, the new fused features were fed to the predictor to learn the traffic patterns and predict future speed. Case studies based on two real-world data sets show that the injection of contextual features can greatly improve the accuracy of time series prediction. Comparison with ten frequently used models, including k-nearest neighbor (k-NN), support vector machine (SVM), decision tree (DT), gradient booting decision tree (GBDT), random forest (RF), stacked autoencoder (SAE), and four classic RNNs, also shows the proposed models outperform these state-of-the-art traffic prediction methods in terms of accuracy and stability.}
}
@article{SHAKARAMI2020107496,
title = {A survey on the computation offloading approaches in mobile edge computing: A machine learning-based perspective},
journal = {Computer Networks},
volume = {182},
pages = {107496},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107496},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620311634},
author = {Ali Shakarami and Mostafa Ghobaei-Arani and Ali Shahidinejad},
keywords = {Computation offloading, Mobile edge computing, Machine learning, Reinforcement learning, Supervised learning, Unsupervised learning},
abstract = {With the rapid developments in emerging mobile technologies, utilizing resource-hungry mobile applications such as media processing, online Gaming, Augmented Reality (AR), and Virtual Reality (VR) play an essential role in both businesses and entertainments. To soften the burden of such complexities incurred by fast developments of such serving technologies, distributed Mobile Edge Computing (MEC) has been developed, aimed at bringing the computation environments near the end-users, usually in one hop, to reach predefined requirements. In the literature, offloading approaches are developed to connect the computation environments to mobile devices by transferring resource-hungry tasks to the near servers. Because of some rising problems such as inherent software and hardware heterogeneity, restrictions, dynamism, and stochastic behavior of the ecosystem, the computation offloading issues consider as the essential challenging problems in the MEC environment. However, to the best of the author's knowledge, in spite of its significance, in machine learning-based (ML-based) computation offloading mechanisms, there is not any systematic, comprehensive, and detailed survey in the MEC environment. In this paper, we provide a review on the ML-based computation offloading mechanisms in the MEC environment in the form of a classical taxonomy to identify the contemporary mechanisms on this crucial topic and to offer open issues as well. The proposed taxonomy is classified into three main fields: Reinforcement learning-based mechanisms, supervised learning-based mechanisms, and unsupervised learning-based mechanisms. Next, these classes are compared with each other based on the essential features such as performance metrics, case studies, utilized techniques, and evaluation tools, and their advantages and weaknesses are discussed, as well. Finally, open issues and uncovered or inadequately covered future research challenges are argued, and the survey is concluded.}
}
@article{MASTALERZ20203780,
title = {Passenger BIBO detection with IoT support and machine learning techniques for intelligent transport systems},
journal = {Procedia Computer Science},
volume = {176},
pages = {3780-3793},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319001},
author = {Marcin W. Mastalerz and Aleksander Malinowski and Sławomir Kwiatkowski and Anna Śniegula and Bartosz Wieczorek},
keywords = {Machine learning, Electronic Toll Collection System, Internet of Things, Beacon, Smartphone, Prediction Analytics, Smart city},
abstract = {The present article discusses the issue of automation of the CICO (Check-In/Check-Out) process for public transport fare collection systems, using modern tools forming part of the Internet of Things, such as Beacon and Smartphone. It describes the concept of an integrated passenger identification model applying machine learning technology in order to reduce or eliminate the risks associated with the incorrect classification of a smartphone user as a vehicle passenger. This will allow for the construction of an intelligent fare collection system, operating in the BIBO (Be-In/Be-Out) model, implementing the "hands-free" and "pay-as-you-go" approach. The article describes the architecture of the research environment, and the implementation of the elaborated model in the Bad.App4 proprietary solution. We also presented the complete process of concept verification under real-life conditions. Research results were described and supplemented with commentary.}
}
@article{MANNION2015956,
title = {Parallel Reinforcement Learning for Traffic Signal Control},
journal = {Procedia Computer Science},
volume = {52},
pages = {956-961},
year = {2015},
note = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915009722},
author = {Patrick Mannion and Jim Duggan and Enda Howley},
keywords = {Reinforcement Learning, Parallel Learning, Multi Agent Systems, Intelligent Transportation Systems, Adaptive Traffic Signal Control, Smart Cities},
abstract = {Developing Adaptive Traffic Signal Control strategies for efficient urban traffic management is a challenging problem, which is not easily solved. Reinforcement Learning (RL) has been shown to be a promising approach when applied to traffic signal control (TSC) problems. When using RL agents for TSC, difficulties may arise with respect to convergence times and performance. This is especially pronounced on complex intersections with many different phases, due to the increased size of the state action space. Parallel Learning is an emerging technique in RL literature, which allows several learning agents to pool their experiences while learning concurrently on the same problem. Here we present an extension to a leading published work on RL for TSC, which leverages the benefits of Parallel Learning to increase exploration and reduce delay times and queue lengths.}
}
@article{LIU2022102727,
title = {A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks},
journal = {Ad Hoc Networks},
volume = {124},
pages = {102727},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102727},
url = {https://www.sciencedirect.com/science/article/pii/S157087052100216X},
author = {Qin Liu and Liqiong Chen and Hongbo Jiang and Jie Wu and Tian Wang and Tao Peng and Guojun Wang},
keywords = {Industrial Internet of Things, Secure microservices, Deep neural networks, Backdoor attacks, Backdoor defenses},
abstract = {Deep Learning shows a broad prospect in providing intelligence microservices to Industrial Internet of Things (IIoT). However, the existence of potential secure vulnerabilities limits the application of deep learning in IIoT. Therefore, how to provide secure deep learning services in IIoT applications becomes an important research topic. Among various attacks on deep neural networks (DNNs), backdoor attacks are generally recognized as the most imperceptible type, where an attacker can upload a poisoned DNN model that misbehaves only when inputs contain specific triggers. Existing defense solutions assume a defender has prior knowledge of backdoor triggers or DNN models, remaining far away from practical and flexible. To this end, this paper proposes a collaborative deep learning microservice, CoDefend, which employs strong intentional perturbation (STRIP) and cycle generative adversarial network (CycleGAN) to defend against backdoored neural networks. Compared with previous work, CoDefend has the advantages of flexibility and practicality. Empirical evaluations validate the high efficacy of CoDefend in providing secure deep learning microservices to IIoT.}
}
@article{YOON2021103321,
title = {Transferable traffic signal control: Reinforcement learning with graph centric state representation},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {130},
pages = {103321},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103321},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003272},
author = {Jinwon Yoon and Kyuree Ahn and Jinkyoo Park and Hwasoo Yeo},
keywords = {Traffic signal control, Reinforcement learning, Restricted exploration, Graph neural network, Transferable policy},
abstract = {Reinforcement learning (RL) has emerged as an alternative approach for optimizing the traffic signal control system. However, there is a restricted exploration problem encountered when a signal control model is trained with a predefined demand scenario in the traffic simulation. With the restricted exploration, the model learns a policy based only on partial experiences in the search space, which yields a partially-trained policy. Partially-trained policy fails to adapt to some unexperienced (‘unexplored’, ‘never-before-seen’) dataset that have different distributions from the training dataset. Although this issue has critical effects on training a signal control model, it has not been considered in the literature. Therefore, this research aims to obtain a transferable policy to enhance the model’s applicability on unexperienced traffic states. The key idea is to represent the state as graph-structured data, and train it using a graph neural network (GNN). Since this approach enables to learn the relationship between the features resulting from the spatial structure of the intersection, it is able to transfer the already-learned knowledge of the relationship to the unexperienced data. In order to investigate the transferability, an experiment is conducted on five unexperienced test demand scenarios. For the evaluation, the performance of the proposed GNN model is compared with the conventional DQN model that is based on vector-valued state. At first, the models are trained with only a single dataset (training demand scenario). Then, they are tested with different unexperienced dataset (test demand scenarios) without additional trainings. The results show that the proposed GNN model obtains a transferable policy so that it adapts better to the unexperienced traffic states, while the conventional DQN model fails.}
}
@article{ALQAHTANI2022224,
title = {A proactive caching and offloading technique using machine learning for mobile edge computing users},
journal = {Computer Communications},
volume = {181},
pages = {224-235},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003959},
author = {Fayez Alqahtani and Mohammed Al-Maitah and Osama Elshakankiry},
keywords = {Data caching, Deep learning, Mobile edge computing, Service offloading},
abstract = {The mobile edge computing (MEC) paradigm provides cloud and application services at the “edge” of user networks for providing ubiquitous access to resources. The heterogeneous services cause varying network traffic that sometimes increases delay. In edge-based services, concurrency in data distribution requires caching and offloading features. This article introduces a proactive caching technique with offloading (PCTO) ability by considering the need for parallel user services. The proposed method performs demand-aware offloading to meet the concurrent service dissemination requirements. Network-level caching and its forecast in concurrent service distribution are performed to reduce the response time. The offloading and caching processes are streamlined using deep recurrent learning for the failing service distribution intervals. In the learning process, the machine is trained for prior failures and for pursuing offloading instances. Based on the learning output, the caching level and offloading rate are determined for the queuing services. The performance of the proposed method is verified using the metrics service ratio, response failures, latency, offloading rate, and caching ratio.}
}
@article{MIHOUB2022107716,
title = {Denial of service attack detection and mitigation for internet of things using looking-back-enabled machine learning techniques},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107716},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107716},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000337},
author = {Alaeddine Mihoub and Ouissem Ben Fredj and Omar Cheikhrouhou and Abdelouahid Derhab and Moez Krichen},
keywords = {Cybersecurity, DoS, DDoS, IoT, Machine Learning, Looking-Back method},
abstract = {IoT (Internet of Things) systems are still facing a great number of attacks due to their integration in several areas of life. The most-reported attacks against IoT systems are "Denial of Service" (DoS) and "Distributed Denial of Service" (DDoS) attacks. In this paper, we investigate DoS/DDoS attacks detection for IoT using machine learning techniques. We propose a new architecture composed of two components: DoS/DDoS detection and DoS/DDoS mitigation. The detection component provides fine-granularity detection, as it identifies the specific type of attack, and the packet type used in the attack. In this way, it is possible to apply the corresponding mitigation countermeasure on specific packet types. The proposed DoS/DDoS detection component is a multi-class classifier that adopts the "Looking-Back" concept, and is evaluated on the Bot-IoT dataset. Evaluation results show promising results as a Looking-Back-enabled Random Forest classifier achieves an accuracy of 99.81%.}
}
@article{LORK2020115426,
title = {An uncertainty-aware deep reinforcement learning framework for residential air conditioning energy management},
journal = {Applied Energy},
volume = {276},
pages = {115426},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115426},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920309387},
author = {Clement Lork and Wen-Tai Li and Yan Qin and Yuren Zhou and Chau Yuen and Wayes Tushar and Tapan K. Saha},
keywords = {Bayesian neural networks, Air conditioning, Energy saving},
abstract = {Most existing methods for controlling the energy consumption of air conditioning (AC), focus on either scheduling the switching (on/off) of compressors or optimizing the overall energy consumption of AC system of an entire building. Unlike commercial buildings, residential apartments typically house separate ACs in individual rooms occupied by people with different thermal comfort preferences. Fortunately, the advancement of Internet-of-Things (IoT) technology has enabled the exploitation of sensory data to intelligently control the set-point temperature of ACs in individual rooms based on environmental conditions and occupant’s preferences, improving the energy efficiency of residential buildings. Indeed, control decisions based on sensory data may suffer from uncertainties due to error in data measurement and contribute to model uncertainty. This work proposes a data-driven uncertainty-aware approach to control split-type inverter ACs of residential buildings. First, information from similar AC and residential units are aggregated to reduce data imbalances, and Bayesian-Convolutional-Neural-Networks (BCNNs) are utilized to model the performance and uncertainty of the ACs from the aggregated data. Second, a Q-learning based reinforcement learning algorithm for set-point decision making is designed for setpoint optimization with transitions sampled from the BCNN models. Third, a case study is simulated based on such a framework to show that the control actions taken by the uncertainty-aware agent perform better in terms of discomfort management and energy savings compared to the uncertainty unaware agent. Further, the agent could also be adjusted to capture the trade-off between energy savings and comfort levels for varying degrees of energy and discomfort savings.}
}
@article{SALEHI2019136,
title = {Data interpretation framework integrating machine learning and pattern recognition for self-powered data-driven damage identification with harvested energy variations},
journal = {Engineering Applications of Artificial Intelligence},
volume = {86},
pages = {136-153},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619301897},
author = {Hadi Salehi and Subir Biswas and Rigoberto Burgueño},
keywords = {Structural health monitoring, Machine learning, Low-rank matrix completion, Pattern recognition, Self-powered sensors, Plate-like structures, Incomplete signals, Energy harvesting},
abstract = {Data mining methods have been widely used for structural health monitoring (SHM) and damage identification for analysis of continuous signals. Nonetheless, the applicability and effectiveness of these techniques cannot be guaranteed when dealing with discrete binary and incomplete/missing signals (i.e., not continuous in time). In this paper a novel data interpretation framework for SHM with noisy and incomplete signals, using a through-substrate self-powered sensing technology, is presented within the context of artificial intelligence (AI). AI methods, namely, machine learning and pattern recognition, were integrated within the data interpretation framework developed for use in a practical engineering problem: data-driven SHM of plate-like structures. Finite element simulations on an aircraft stabilizer wing and experimental vibration tests on a dynamically loaded plate were conducted to validate the proposed framework. Machine learning algorithms, including support vector machine, k-nearest neighbor, and artificial neural networks, were integrated within the developed learning framework for performance assessment of the monitored structures. Different levels of harvested energy were considered to evaluate the robustness of the SHM system with respect to such variations. Results demonstrate that the SHM methodology employing the proposed machine learning-based data interpretation framework is efficient and robust for damage detection with incomplete and sparse/missing binary signals, overcoming the notable issue of energy availability for smart damage identification platforms being used in structural/infrastructure and aerospace health monitoring. The present study aims to advance data mining and interpretation techniques in the SHM domain, promoting the practical application of machine learning and pattern recognition with incomplete and missing/sparse signals in smart cities and smart infrastructure monitoring.}
}
@article{NAYAK2021104078,
title = {A comprehensive review on deep learning-based methods for video anomaly detection},
journal = {Image and Vision Computing},
volume = {106},
pages = {104078},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2020.104078},
url = {https://www.sciencedirect.com/science/article/pii/S0262885620302109},
author = {Rashmiranjan Nayak and Umesh Chandra Pati and Santos Kumar Das},
keywords = {Deep learning, Deep regenerative models, Deep one-class models, Hybrid models, Spatiotemporal models, Video anomaly detection},
abstract = {Video surveillance systems are popular and used in public places such as market places, shopping malls, hospitals, banks, streets, education institutions, city administrative offices, and smart cities to enhance the safety of public lives and assets. Most of the time, the timely and accurate detection of video anomalies is the main objective of security applications. The video anomalies such as anomalous activities and anomalous entities are defined as the abnormal or irregular patterns present in the video that do not conform to the normal trained patterns. Anomalous activities such as fighting, riots, traffic rule violations, and stampede as well as anomalous entities such as weapons at the sensitive place and abandoned luggage should be detected automatically in time. However, the detection of video anomalies is challenging due to the ambiguous nature of the anomaly, various environmental conditions, the complex nature of human behaviors, and the lack of proper datasets. There are only a few dedicated surveys related to deep learning-based video anomaly detection as the research domain is in its early stages. However, state of the art lacks a review that provides a comprehensive study covering all the aspects such as definitions, classifications, modelings, performance evaluation methodologies, open and trending research challenges of video anomaly detection. Hence, in this survey, we present a comprehensive study of the deep learning-based methods reported in state of the art to detect the video anomalies. Further, we discuss the comparative analysis of the state of the art methods in terms of datasets, computational infrastructure, and performance metrics for both quantitative and qualitative analyses. Finally, we outline the challenges and promising directions for further research.}
}
@article{KINAWY2018286,
title = {Customizing information delivery to project stakeholders in the smart city},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {286-300},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717309800},
author = {S.N. Kinawy and T.E. El-Diraby and H. Konomi},
keywords = {Project communication system, Co-creation, Community engagement, Crowdsourcing, Ontology, Recommender systems},
abstract = {In the smart city, citizens are integral participants in the decision making process. They possess equally important knowledge to that of professionals. Effectively informing them about new project features is the first step in engaging them in the decision making and harnessing their knowledge. However, given the complexity and diversity of project information, citizens could face an information overload. Our objective is to support the delivery of the right information to the right person; and doing so in an adaptive manner that recognizes the needs of local context. We have developed a system that allows users to profile their information needs based on an ontology of user communications. Recommender algorithms are then used to match user profile to the most relevant knowledge items, such as documents, web pages, and tagged videos or images. Users who wish to rate or tag documents can do so through using concepts from the ontology or free text. If free text is used, semantic analysis is conducted to extract relevant tags. Tags are then fed-back into the recommender system to enhance its accuracy. Capturing community tags provides a good opportunity to use crowd input to contextualise the matching algorithms.}
}
@article{ZHOU201630,
title = {Green cell planning and deployment for small cell networks in smart cities},
journal = {Ad Hoc Networks},
volume = {43},
pages = {30-42},
year = {2016},
note = {Smart Wireless Access Networks and Systems for Smart Cities},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2016.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1570870516300312},
author = {Li Zhou and Zhengguo Sheng and Lei Wei and Xiping Hu and Haitao Zhao and Jibo Wei and Victor C.M. Leung},
keywords = {Small cell, Cell planning, Cell deployment, Energy efficiency},
abstract = {In smart cities, cellular network plays a crucial role to support wireless access for numerous devices anywhere and anytime. The future 5G network aims to build the infrastructure from mobile internet to connected world. Small Cell is one of the most promising technologies of 5G to provide more connections and high data rate. In order to make the best use of small cell technology, smart cell planning should be implemented to guarantee connectivity and performance for all end nodes. It is particularly a challenging task to deploy dense small cells in the presence of dynamic traffic demands and severe co-channel interference. In this paper, we model various traffic patterns using stochastic geometry approach and propose an energy-efficient scheme to deploy and plan small cells according to the prevailing traffic pattern. The simulation results indicate that our scheme can meet dynamic traffic demands with optimized deployment of small cells and enhance the energy efficiency of the system without compromising on quality-of-service (QoS) requirements. In addition, our scheme can achieve very close performance compared with the leading optimization solver CPLEX and find solutions in much less computational times than CPLEX.}
}
@article{ABDELDAYEM2022149834,
title = {Viral outbreaks detection and surveillance using wastewater-based epidemiology, viral air sampling, and machine learning techniques: A comprehensive review and outlook},
journal = {Science of The Total Environment},
volume = {803},
pages = {149834},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.149834},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721049093},
author = {Omar M. Abdeldayem and Areeg M. Dabbish and Mahmoud M. Habashy and Mohamed K. Mostafa and Mohamed Elhefnawy and Lobna Amin and Eslam G. Al-Sakkari and Ahmed Ragab and Eldon R. Rene},
keywords = {SARS-CoV-2, COVID-19, Wastewater based-epidemiology, Viral air surveillance, Artificial intelligence, Artificial neural networks, Machine learning, Deep learning, Reinforcement Learning},
abstract = {A viral outbreak is a global challenge that affects public health and safety. The coronavirus disease 2019 (COVID-19) has been spreading globally, affecting millions of people worldwide, and led to significant loss of lives and deterioration of the global economy. The current adverse effects caused by the COVID-19 pandemic demands finding new detection methods for future viral outbreaks. The environment's transmission pathways include and are not limited to air, surface water, and wastewater environments. The wastewater surveillance, known as wastewater-based epidemiology (WBE), can potentially monitor viral outbreaks and provide a complementary clinical testing method. Another investigated outbreak surveillance technique that has not been yet implemented in a sufficient number of studies is the surveillance of Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) in the air. Artificial intelligence (AI) and its related machine learning (ML) and deep learning (DL) technologies are currently emerging techniques for detecting viral outbreaks using global data. To date, there are no reports that illustrate the potential of using WBE with AI to detect viral outbreaks. This study investigates the transmission pathways of SARS-CoV-2 in the environment and provides current updates on the surveillance of viral outbreaks using WBE, viral air sampling, and AI. It also proposes a novel framework based on an ensemble of ML and DL algorithms to provide a beneficial supportive tool for decision-makers. The framework exploits available data from reliable sources to discover meaningful insights and knowledge that allows researchers and practitioners to build efficient methods and protocols that accurately monitor and detect viral outbreaks. The proposed framework could provide early detection of viruses, forecast risk maps and vulnerable areas, and estimate the number of infected citizens.}
}
@article{LIANG2021,
title = {Generating self-attention activation maps for visual interpretations of convolutional neural networks},
journal = {Neurocomputing},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.084},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017628},
author = {Yu Liang and Maozhen Li and Changjun Jiang},
keywords = {Interpretable machine learning, Black-box models, Transparent models, Deep learning, Explainable artificial intelligence},
abstract = {In recent years, many interpretable methods based on class activation maps (CAMs) have served as an important judging basis for the predictions of convolutional neural networks (CNNs). However, these methods still suffer from the problems of gradient noise, weight distortion, and perturbation deviation. In this work, we present self-attention class activation map (SA-CAM) and shed light on how it uses the self-attention mechanism to refine the existing CAM methods. In addition to generating basic activation feature maps, SA-CAM adds an attention skip connection as a regularization item for each feature map which further refines the focus area of an underlying CNN model. By introducing an attention branch and constructing a new attention operator, SA-CAM greatly alleviates the limitations of the CAM methods. The experimental results on the ImageNet dataset show that SA-CAM can not only generate highly accurate and intuitive interpretation but also have robust stability in adversarial comparison with the state-of-the-art CAM methods.}
}
@article{LI2021100057,
title = {Understanding rooftop PV panel semantic segmentation of satellite and aerial images for better using machine learning},
journal = {Advances in Applied Energy},
volume = {4},
pages = {100057},
year = {2021},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2021.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2666792421000494},
author = {Peiran Li and Haoran Zhang and Zhiling Guo and Suxing Lyu and Jinyu Chen and Wenjing Li and Xuan Song and Ryosuke Shibasaki and Jinyue Yan},
keywords = {PV, Computer vision, Deep learning, Satellite and aerial image, Semantic segmentation},
abstract = {The photovoltaic (PV) industry boom and increased PV applications call for better planning based on accurate and updated data on the installed capacity. Compared with the manual statistical approach, which is often time-consuming and labor-intensive, using satellite/aerial images to estimate the existing PV installed capacity offers a new method with cost-effective and data-consistent features. Previous studies investigated the feasibility of segmenting PV panels from images involving machine learning technologies. However, due to the particular characteristics of PV panel semantic-segmentation, the machine learning tools need to be designed and applied with careful considerations of the issue formulation, data quality, and model explainability. This paper investigated the characteristics of PV panel semantic-segmentation from the perspective of computer vision. The results reveal that the PV panel image data has several specific characteristics: highly class-imbalance and non-concentrated distribution; homogeneous texture and heterogenous color features; and the notable resolution threshold for effective semantic-segmentation. Moreover, this paper provided recommendations for data obtaining and model design, aiming at each observed character from the viewpoints of recent solutions in computer vision, which can be helpful for future improvement of the PV panel semantic-segmentation.}
}
@article{IMTIAZ2021844,
title = {DeepAMD: Detection and identification of Android malware using high-efficient Deep Artificial Neural Network},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {844-856},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2032985X},
author = {Syed Ibrahim Imtiaz and Saif ur Rehman and Abdul Rehman Javed and Zunera Jalil and Xuan Liu and Waleed S. Alnumay},
keywords = {Android malware, Malware family, Malware category, API calls, Deep learning, Machine learning, Cyberattack, Security},
abstract = {Android smartphones are being utilized by a vast majority of users for everyday planning, data exchanges, correspondences, social interaction, business execution, bank transactions, and almost in each walk of everyday lives. With the expansion of human reliance on smartphone technology, cyberattacks against these devices have surged exponentially. Smartphone applications use permissions to utilize various functionalities of the smartphone that can be maneuvered to launch an attack or inject malware by hackers. Existing studies present various approaches to detect Android malware but lack early detection and identification. Accordingly, there is a dire need to craft an efficient mechanism for malicious applications’ detection before they exploit the data. In this paper, a novel approach DeepAMD to defend against real-world Android malware using deep Artificial Neural Network (ANN) has been adopted including an efficiency comparison of DeepAMD with conventional machine learning classifiers and state-of-the-art studies based on performance measures such as accuracy, recall, f-score, and precision. As per the experimental analysis, DeepAMD outperforms other approaches in detecting and identifying malware attacks on both Static as well as Dynamic layers. On the Static layer, DeepAMD achieves the highest accuracy of 93.4% for malware classification, 92.5% for malware category classification, and 90% for malware family classification. On the Dynamic layer, DeepAMD achieves the highest accuracy of 80.3% for malware category classification and 59% for malware family classification in comparison with the state-of-the-art techniques.}
}
@article{BUONGIORNO2021549,
title = {Deep learning for processing electromyographic signals: A taxonomy-based survey},
journal = {Neurocomputing},
volume = {452},
pages = {549-565},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.139},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220319020},
author = {Domenico Buongiorno and Giacomo Donato Cascarano and Irio {De Feudis} and Antonio Brunetti and Leonarda Carnimeo and Giovanni Dimauro and Vitoantonio Bevilacqua},
keywords = {Deep learning, Convolutional neural network, Autoencoder, Deep belief network, Recurrent neural network, Electromyographic Signal – EMG},
abstract = {Deep Learning (DL) has been recently employed to build smart systems that perform incredibly well in a wide range of tasks, such as image recognition, machine translation, and self-driving cars. In several fields the considerable improvement in the computing hardware and the increasing need for big data analytics has boosted DL work. In recent years physiological signal processing has strongly benefited from deep learning. In general, there is an exponential increase in the number of studies concerning the processing of electromyographic (EMG) signals using DL methods. This phenomenon is mostly explained by the current limitation of myoelectric controlled prostheses as well as the recent release of large EMG recording datasets, e.g. Ninapro. Such a growing trend has inspired us to seek and review recent papers focusing on processing EMG signals using DL methods. Referring to the Scopus database, a systematic literature search of papers published between January 2014 and March 2019 was carried out, and sixty-five papers were chosen for review after a full text analysis. The bibliometric research revealed that the reviewed papers can be grouped in four main categories according to the final application of the EMG signal analysis: Hand Gesture Classification, Speech and Emotion Classification, Sleep Stage Classification and Other Applications. The review process also confirmed the increasing trend in terms of published papers, the number of papers published in 2018 is indeed four times the amount of papers published the year before. As expected, most of the analyzed papers (≈60 %) concern the identification of hand gestures, thus supporting our hypothesis. Finally, it is worth reporting that the convolutional neural network (CNN) is the most used topology among the several involved DL architectures, in fact, the sixty percent approximately of the reviewed articles consider a CNN.}
}
@article{IBRAHIM2020115237,
title = {Machine learning driven smart electric power systems: Current trends and new perspectives},
journal = {Applied Energy},
volume = {272},
pages = {115237},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115237},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920307492},
author = {Muhammad Sohail Ibrahim and Wei Dong and Qiang Yang},
keywords = {Smart grid, Smart energy systems, Machine learning, Deep learning, Neural networks},
abstract = {The current power systems are undergoing a rapid transition towards their more active, flexible, and intelligent counterpart smart grid, which brings about tremendous challenges in many domains, e.g., integration of various distributed renewable energy sources, cyberspace security, demand-side management, and decision-making of system planning and operation. The fulfillment of advanced functionalities in the smart grid firmly relies on the underlying information and communication infrastructure, and the efficient handling of a massive amount of data generated from various sources, e.g., smart meters, phasor measurement units, and various forms of sensors. In this paper, a comprehensive survey of over 200 recent publications is conducted to review the state-of-the-art practices and proposals of machine learning techniques and discuss the trend in a wide range of smart grid application domains. This study demonstrates the increasing interest and rapid expansion in the use of machine learning techniques to successfully address the technical challenges of the smart grid from various aspects. It is also revealed that some issues still remain open and worth further research efforts, such as the high-performance data processing and analysis for intelligent decision-making in large-scale complex multi-energy systems, lightweight machine learning-based solutions, and so forth. Moreover, the future perspectives of utilizing advanced computing and communication technologies, e.g., edge computing, ubiquitous internet of things and 5G wireless networks, in the smart grid are also highlighted. To the best of our knowledge, this is the first review of machine learning-driven solutions covering almost all the smart grid application domains. Machine learning will be one of the major drivers of future smart electric power systems, and this study can provide a preliminary foundation for further exploration and development of related knowledge and insights.}
}
@article{ZOU201867,
title = {A novel network security algorithm based on improved support vector machine from smart city perspective},
journal = {Computers & Electrical Engineering},
volume = {65},
pages = {67-78},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617331063},
author = {Xiang Zou and Jinghua Cao and Quan Guo and Tao Wen},
keywords = {Data mining, Intrusion detection system/intrusion prevention system, One class-support vector machine, Decision tree learning-iterative dichotomise 3},
abstract = {Computer generated security concerns have become more modern and complex. Intrusion detection(ID) is a practical issue in the field of computer security whose primary objective is to detect rare attack or assaults and to ensure the security of interior systems. This paper also proposes a semi-class intrusion detection method that combines multiple classifiers to arrange exceptions and typical exercises in a computer system. The abuse detection model is constructed in the light of the decision tree learning-iterative dichotomise 3(DTL-ID3) and is assembled by utilizing the gathered data based on anomaly detection model executed by one class-support vector machine(OC-SVM). In recent years, people have paid more attention to ID/intrusion prevention system (IDS / IPS), which is closely related to the protection and utilization of system management. A few machine-learning standards including neural system, direct hereditary programming, and advanced support vector machines(ASVMs), Bayesian system, multivariate versatile relapse splines, fluffy derivation systems(FIS) and other analogical systems have been researched for the outline of intrusion detection system. In this paper, we build up an amalgam method based on DTL-ID3 and OC-SVM(A-DT and SVM) and evaluate the performance of the projected methodology by using a specific dataset and a crossover method in order to enhance the accuracy of IDS/IPS when contrasted with a singular support vector machine.}
}
@article{MILOJEVICDUPONT2021102526,
title = {Machine learning for geographically differentiated climate change mitigation in urban areas},
journal = {Sustainable Cities and Society},
volume = {64},
pages = {102526},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102526},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720307423},
author = {Nikola Milojevic-Dupont and Felix Creutzig},
keywords = {Machine learning, Cities, Climate change mitigation, Urban governance},
abstract = {Artificial intelligence and machine learning are transforming scientific disciplines, but their full potential for climate change mitigation remains elusive. Here, we conduct a systematic review of applied machine learning studies that are of relevance for climate change mitigation, focusing specifically on the fields of remote sensing, urban transportation, and buildings. The relevant body of literature spans twenty years and is growing exponentially. We show that the emergence of big data and machine learning methods enables climate solution research to overcome generic recommendations and provide policy solutions at urban, street, building and household scale, adapted to specific contexts, but scalable to global mitigation potentials. We suggest a meta-algorithmic architecture and framework for using machine learning to optimize urban planning for accelerating, improving and transforming urban infrastructure provision.}
}
@article{ALAOUI2022102475,
title = {Towards to intelligent routing for DTN protocols using machine learning techniques},
journal = {Simulation Modelling Practice and Theory},
pages = {102475},
year = {2022},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102475},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X2100160X},
author = {El Arbi Abdellaoui Alaoui and Stephane Cedric Koumetio Tekouabou and Yassine Maleh and Anand Nayyar},
keywords = {Delay Tolerant Networks (DTN), DTN protocols, Internet of things (IoT), Machine learning, Interpretable machine learning, Shap values, Game Theory},
abstract = {The communication protocols of wireless networks have experienced great advances in recent years, specifically with the evolution of new technologies such as the Internet of Things (IoT). However, certain problems remain unsolved, in particular for wireless networks, and more specifically for DTN networks, which represent a major challenge in terms of DTN routing. This paper aims to design an intelligent routing system based on machine learning techniques, the use of which represents another possibility to classify bundles that have arrived at the destination successfully or not. These networks occasionally carry out an evaluation which makes it possible to choose the type of routing corresponding to a given situation. It then minimizes the unnecessary information of the entries and performs the classification of the data. Despite the problems cited, our challenge is to design an intelligent routing mechanism that is able to classify bundles that have arrived and those that have not arrived at their destination. The smart routing system uses machine learning as a main tool to design our system. Indeed,various Machine Learning techniques, such as Bagging and Boosting, have been used to classify whether bundles have arrived at their destination successfully or not. Machine Learning now enables us to learn directly from data rather than human expertise, resulting in higher accuracy. We utilized the SMOTE technique to balance the two groups of data, which allows us to collect the equal amount of samples for each class. We also included techniques for interpreting complicated Machine Learning Models to understand the reasoning for model decisions, such as SHAP values. Results show an overall accuracy of 80% for the Random Forest (RF) and ExtraTrees Classifier (ET).}
}
@article{LI2020109,
title = {Machine learning based code dissemination by selection of reliability mobile vehicles in 5G networks},
journal = {Computer Communications},
volume = {152},
pages = {109-118},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.034},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315609},
author = {Ting Li and Ming Zhao and Kelvin Kian Loong Wong},
keywords = {Machine learning, Code disseminations, Safety degree, Coverage ratio, Reliability, 5G networks},
abstract = {Recently, the evolving of 5G networks is foreseen as a major driver of future mobile vehicular social networks (VSNs), which can provide a novel method of code disseminations. Based on this concept, vehicles can be used as code disseminators. That is, infrastructures of a smart city can be upgraded by receiving updated program codes that are disseminated by vehicles in the VSNs. Specifically, vehicles in the 5G network are hard to be managed. Under this domain, safety of program codes is a key challenge. Meanwhile, improving coverage of program codes is also challenging. However, arranging plenty of vehicles as code disseminators will incur large costs of the ground control station (GCS). Therefore, by utilizing machine learning methods, this paper proposes a “Machine Learning based Code Dissemination by Selecting Reliability Mobile Vehicles in 5G Networks” (MLCD) scheme to choose vehicles with higher reliable degree and coverage ratio as code disseminators to deliver code with lower costs. Firstly, reliable degrees of vehicles are calculated and selected to improve safety degree of code disseminations. Secondly, vehicles with higher coverage ratio are preferred to promise code coverage. Thirdly, machine learning methods are utilized to select vehicles with both higher coverage ratios and reliable degrees as code disseminators with limited costs. Compared to random-selection and coverage-only scheme respectively, the MLCD scheme can improve safety degree of code dissemination process by 83.6% and 18.86% in 5G networks, and can improve coverage ratio of updated information by 23.16%. Comprehensive performances of the proposed scheme can be improved by 80.56% and 17.25% respectively. Future works focus on improving code security in 5G networks by more advanced and suitable machine learning methods.}
}
@article{MABINA2021111085,
title = {Sustainability matchmaking: Linking renewable sources to electric water heating through machine learning},
journal = {Energy and Buildings},
volume = {246},
pages = {111085},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111085},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821003698},
author = {P. Mabina and P. Mukoma and M.J. Booysen},
keywords = {Electric water heaters, Machine learning, Supervised learning, Unsupervised learning, Reinforcement learning, Renewable energy},
abstract = {A high penetration of renewable energy sources such as wind power generation and photovoltaic generation causes some problems in power systems such as the duck curve and unreliability due to environmental variability. An effective solution to this problem is Demand Response (DR). Electric Water Heaters (EWHs) are considered ideal candidates for DR due to their energy storage capability. Due to the benefits, control strategies or techniques for EWHs have received considerable academic attention. The energy sector has recently tapped into the disruptive artificial intelligence world to learn, among other related priorities, how to enhance operations, maintain energy resilience and improve consumer service. Consequently, this paper reviews the use of machine learning (ML) for optimization and scheduling of EWHs. The main contributions of this review paper are, firstly, to identify state of the art of energy optimization and scheduling of EWHs. Secondly, to review the current ML models for energy optimization and scheduling of EWHs in smart grids and smart building environment. While classical control strategies may deliver substantial improvements, optimum efficiency may not be reached. ML has demonstrated clear advantages over classical control. Based on these conclusions, recommendations for further research topics are drawn.}
}
@article{TIWARI2021110180,
title = {An enhanced intelligent model: To protect marine IoT sensor environment using ensemble machine learning approach},
journal = {Ocean Engineering},
volume = {242},
pages = {110180},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.110180},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821014980},
author = {Dimple Tiwari and Bhoopesh Singh Bhati and Bharti Nagpal and Shweta Sankhwar and Fadi Al-Turjman},
keywords = {Internet of things (IoT), Marine data safety, Sensor-based infrastructure, Machine learning, Ensemble-learning, Light-GBM},
abstract = {The research in marine sensors and the Internet of Things (IoT) has grown exponentially with the ample warehouse of natural materials in the sea. The growing activities in the marine sensor environment increased the threat of anomalies and cyber-attacks. Many Intrusion Detection Systems (IDS) and classical machine learning-based models have been proposed to secure the sensor-based IoT infrastructure. Still, these mechanisms have failed to achieve significant results for securing the marine sensor environment due to the discriminant requirements of the IoT appliances in deep oceans, such as distribution, information complexity, scalability, higher network bandwidth requirements, and low computational capacity. Hence, we propose a lightweight and robust ensemble model to secure the marine IoT environment from cyber-attacks and malicious activities. This paper established an optimized Light Gradient Boosting Machine (Light-GBM) algorithm for ocean IoT attack detection. The experiments were conducted on Distributed Smart Space Orchestration System (DS2OS) dataset. The proposed methodology includes a label encoding technique for best feature selection, hyper-parameter tuning, ensemble function, and a novel algorithm to develop an ocean IoT attack detection model. As an extension of traditional methods, the optimized Light-GBM model can handle the distributed IoT attacks in the deeper marine environments with low computational cost and with 98.52% detection accuracy. The comparative analysis confirms the effectiveness of the proposed model for marine sensor safety. Conclusively, the proposed model mitigates the threat of cyber-attacks in the marine sensor environment and presenting a promising future in real-time ocean-based IoT applications.}
}
@article{LIU2020107212,
title = {Anomaly detection based on machine learning in IoT-based vertical plant wall for indoor climate control},
journal = {Building and Environment},
volume = {183},
pages = {107212},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.107212},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320305837},
author = {Yu Liu and Zhibo Pang and Magnus Karlsson and Shaofang Gong},
keywords = {Vertical plant wall, Indoor climate control, Anomaly detection, Internet of Things, Machine learning, Neural networks},
abstract = {Indoor climate is closely related to human health, comfort and productivity. Vertical plant wall systems, embedded with sensors and actuators, have become a promising application for indoor climate control. In this study, we explore the possibility of applying machine learning based anomaly detection methods to vertical plant wall systems so as to enhance the automation and improve the intelligence to realize predictive maintenance of the indoor climate. Two categories of anomalies, namely point anomalies and contextual anomalies are researched. Prediction-based and pattern recognition-based methods are investigated and applied to indoor climate anomaly detection. The results show that neural network-based models, specifically the autoencoder (AE) and the long short-term memory encoder decoder (LSTM-ED) model surpass the others in terms of detecting point anomalies and contextual anomalies, respectively, therefore can be deployed into vertical plant walls systems in industrial practice. Based on the results, a new data cleaning method is proposed and a prediction-based method is deployed to the cloud in practice as a proof-of-concept. This study showcases the advancements in machine learning and Internet of things can be fully utilized by researches on building environment to accelerate the solution development.}
}
@article{CHANG2021102938,
title = {A deep learning based secured energy management framework within a smart island},
journal = {Sustainable Cities and Society},
volume = {70},
pages = {102938},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102938},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721002237},
author = {Qianqian Chang and Xiaolin Ma and Ming Chen and Xinwei Gao and Moslem Dehghani},
keywords = {Cyber security, Fast fourier transform, Deep learning, DC smart island, Fog networks},
abstract = {This study proposes a novel secured management method for renewable microgrids considering the policies required for diagnosing cyber-attacks happening in the communication networks, usually applied in the secondary control layer of microgrids (MGs). Due to the so long stochastic and bad information entering the systems in order to make malicious attacks, their location and time data links have the ability of straying of those acting in normal conditions that attempt to have an effect on the precise voltage regulation and current dividing via influencing sensors of current and voltage. The ability to extract high-level features due to the usage of fast fourier transform (FFT) and deep learning (DL) for attack detection in cyberspace has made them to be considered as a strong technique in the face of small mutations or new attacks. These self-educated and compaction abilities of DL architectures have been considered as basic techniques for hidden scheme detection from the training datum for this reason attacks have been distinguished from benign traffic. A novel method, deep learning and FFT, for cyber-security has been used in the following paper with the aim of enabling the attacks detection in DC smart MG. The deep model and traditional machine learning way are evaluated in terms of performance, and distributed attack detection has been compared to the centralized diagnosing procedure. The tests proved that the distributed attack detection system studied can be more advanced in comparison with centralized detection systems applying FFT in the role of the input index of the DL model. This suggested distributed method enables for scalable monitoring of a MG and has the ability of detecting the existence of cyber-attacks in the communications between distributed generation agents (DGAs) controlled via a control on the basis of consensus and isolating the communication link over that the attack has been injected. Any local attack detection needs restricted information about its neighbor’s dynamics. The most important factor of the proposed detection plan can be that has the ability of detecting cyber-attacks with great precision and distinguishing cyber-attack from load changes.in addition, this has been shown that the suggested model can be further useful in the detection of the attack.}
}
@article{ZHU201834,
title = {IoT applications in the ecological industry chain from information security and smart city perspectives},
journal = {Computers & Electrical Engineering},
volume = {65},
pages = {34-43},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.05.036},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315434},
author = {Nana Zhu and Hongyan Zhao},
keywords = {Internet of Things, Biological ambient intelligence, Wireless sensor and actor networks, Decision tree, Artificial neural controller, Organization under surveillance and control},
abstract = {Innovation holds the guarantee of opening new doors for enhancing the nature of our lives. Likewise, a worldwide data and correspondence system superstructure is developing as the Internet transforms into the Internet of Things. The Internet of Things will address an impressive number of current difficulties in all application spaces, and will add to economic development. The most critical application spaces of the pervasiveness of Internet of Things-based innovations are exhibited, including the case of the shrewd urban communities presently being studied, which will improve the standard of ordinary lives. As a prologue to this issue of general economic systems research, which is themed around mechanical biology, the paper intends to discuss the foundation of modern nature, while highlighting the role of, and commitments from, financial matters from the point of view of information security. The performance is verified by experiment.}
}
@article{MUKHINA2019176,
title = {Urban events prediction via convolutional neural networks and Instagram data},
journal = {Procedia Computer Science},
volume = {156},
pages = {176-184},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311123},
author = {Ksenia D. Mukhina and Alexander A. Visheratin and Denis Nasonov},
keywords = {convolution neural network, deep learning, Instagram, social network, event prediction},
abstract = {In today’s world, it is crucial to be proactive and be prepared for events which are not happening yet. Thus, there is no surprise that in the field of social media analysis the research agenda has moved from the development of event detection methods to a brand new area - event prediction models. This research field is extremely important for all sorts of applications, from natural disasters preparation and criminal activity prevention to urban management and development of smart cities. However, even the leading models have an important disadvantage: they are based on prior knowledge about events being expected. So forecasting systems based on such models are heavily limited by a list of events that can be predicted and all events of other types will be out of systems’ scope. In this work, we try to address this issue and propose a deep learning model, which is able to predict an area of the future event in the urban environment. This model is able to predict the future state of the city - a level of users activity in the location-based social network Instagram - with the average deviation from the ground truth of 1%, and achieves 69% recall when solving the events prediction problem.}
}
@article{HE2017370,
title = {Distributed proxy cache technology based on autonomic computing in smart cities},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {370-383},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16300590},
author = {Hui He and Lijie Cui and Fenglan Zhou and Dong Wang},
keywords = {HTTP proxy, Distributed proxy cache, Cache management, Autonomous perception, Autonomous decision},
abstract = {With the rapid growth of Internet services in smart cities, large network flow has caused network congestion among users, thereby deteriorating network service quality and user experience. In this study, a distributed proxy cache management platform based on autonomous decision is proposed. The platform realizes highly efficient automatic cache management by taking the advantages of autonomous perception and decision. First, the platform establishes the autonomous management framework of proxy cache on the basis of the idea of feedback loop. Next, the URL is distributed evenly among cache nodes through the hash ring storage mechanism with virtual nodes. Cache status adjustment strategies are made based on historical status sequences and the gray prediction model. A method that is used to predict content hot-rank based on domain name set is proposed to reduce loads of cache nodes. Second, the platform designs a distributed proxy cache management system by depending on the autonomous management framework of proxy cache and the cache management mechanism of autonomous decision, and the Cache Hot Spots Migrate algorithm is suggested for the dynamic migration and integration of virtual nodes on the hash ring. A system knowledge base is established through status sequence and corresponding adjustment strategies. Finally, experimental results on the hash ring, content hot-rank prediction, and adjustment of autonomous decision of cache status are analyzed, thereby confirming that the distributed proxy cache management mechanism based on autonomous decision could manage cache nodes effectively and automatically, and could improve the overall performance of the system.}
}
@article{EMBARAK2021445,
title = {A New Paradigm Through Machine Learning: A Learning Maximization Approach for Sustainable Education},
journal = {Procedia Computer Science},
volume = {191},
pages = {445-450},
year = {2021},
note = {The 18th International Conference on Mobile Systems and Pervasive Computing (MobiSPC), The 16th International Conference on Future Networks and Communications (FNC), The 11th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.055},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014551},
author = {Ossama Embarak},
keywords = {Theory of maximization model, machine learning in education, sustainable education system, education forms, sustainable education applications},
abstract = {Machine learning algorithms have been used to predict student performance in academic institutions over the last decade. The developed prediction models classified students as either those who were likely to receive a distinctive or those who were likely to be at-risk or withdraw from the class. Although, the early prediction aid in the targeting of educational interventions and achieved a more efficient allocation of educational resources. All proposed solutions fell within the scope of predictions that result in active or proactive actions to support universities and learners. On the other hand, they fail to comprehend the various forms of education systems and whether it appropriate for the twenty-first century and future generations. The paper classifies education into five types based on the design mode, the scope of production, and the interaction between learners and educational systems (Push, Pull, coupling, Integrated, and Sustainable). The paper proposes a sustainable education paradigm that maximizes the knowledge and skill matrix accumulated for the desired program. The proposed theory implementation phases are modelled and demonstrated using 21st-century technologies, such as personalized and coaching education based on the learner’s learning style and remediation actions for strong learners with innovative competencies. The study emphasized various aspects of sustainable education systems that are required for smart city transition. The limitations and proposed solutions for dealing with anticipated issues were demonstrated, and the benefits of sustainable education are based on the proposed maximization theory rather than the current block-based learning of outcomes.}
}
@article{MOHANDAS2019101499,
title = {Artificial Neural Network based Smart and Energy Efficient Street Lighting System: A Case Study for Residential area in Hosur},
journal = {Sustainable Cities and Society},
volume = {48},
pages = {101499},
year = {2019},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101499},
url = {https://www.sciencedirect.com/science/article/pii/S2210670718317530},
author = {Prabu Mohandas and Jerline Sheebha Anni Dhanaraj and Xiao-Zhi Gao},
keywords = {Street lighting, Artificial neural network, Fuzzy logic controller, Sensors},
abstract = {Smart city is the term described to integrate all facilities to the people in a frequently accessible manner. Street lighting system is one of the part of the facility provided in smart cities. The unwanted utilisation of the street lighting affects the economic status of the country indirectly. Power consumption through street lighting is major problem, hence action plan is taken to promote the reduction policies of the power consumption. Reducing the unnecessary power consumption is not a simple task, but with soft computing approaches power consumption can be reduced. The objective of this article is to present an ANN based energy efficient smart street lighting systems. The proposed design were implemented and executed in a residential area, Hosur and the results are carried out at different scenarios and various seasons. The decision making module exploits the analysis factors obtained via lighting sensor, motion sensor, PIR sensor, etc. artificial neural network and fuzzy logic controller makes an efficient decision making process for demand based utilisation and to avoid the unnecessary utilisation of street lights. The five levels of scenarios are tested and implemented in a real time. Through this work, the smart and energy efficiency street lighting system reduced the unwanted utilisation by 34% and reduced the power consumption rate of 13.5%.}
}
@article{PAN2022117271,
title = {Data-centric Engineering: integrating simulation, machine learning and statistics. Challenges and opportunities},
journal = {Chemical Engineering Science},
volume = {249},
pages = {117271},
year = {2022},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2021.117271},
url = {https://www.sciencedirect.com/science/article/pii/S0009250921008368},
author = {Indranil Pan and Lachlan R. Mason and Omar K. Matar},
keywords = {Digital twins, Artificial Intelligence, CFD, FEM, Data-centric Engineering, SimOps},
abstract = {Recent advances in machine learning, coupled with low-cost computation, availability of cheap streaming sensors, data storage and cloud technologies, has led to widespread multi-disciplinary research activity with significant interest and investment from commercial stakeholders. Mechanistic models, based on physical equations, and purely data-driven statistical approaches represent two ends of the modelling spectrum. New hybrid, data-centric engineering approaches, leveraging the best of both worlds and integrating both simulations and data, are emerging as a powerful tool with a transformative impact on the physical disciplines. We review the key research trends and application scenarios in the emerging field of integrating simulations, machine learning, and statistics. We highlight the opportunities that such an integrated vision can unlock and outline the key challenges holding back its realisation. We also discuss the bottlenecks in the translational aspects of the field and the long-term upskilling requirements for the existing workforce and future university graduates.}
}
@article{CHAUDHURI2020113234,
title = {Exploring the role of deep neural networks for post-disaster decision support},
journal = {Decision Support Systems},
volume = {130},
pages = {113234},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113234},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619302635},
author = {Neha Chaudhuri and Indranil Bose},
keywords = {Convolutional neural networks, Decision support, Deep learning, Disaster management, Image analytics},
abstract = {Disaster management operations are information intensive activities due to high uncertainty and complex information needs. Emergency response planners need to effectively plan response activities with limited resources and assign rescue teams to specific disaster sites with high probability of survivors swiftly. Decision making becomes tougher since the limited information available is heterogenous, untimely and often fragmented. We address the problem of lack of insightful information of the disaster sites by utilizing image data obtained from smart infrastructures. We collect geo-tagged images from earthquake-hit regions and apply deep learning method for classification of these images to identify survivors in debris. We find that deep learning method is able to classify the images with significantly higher accuracy than the conventionally used machine learning methods for image classification and utilizes significantly lesser time and computational resources. The novel application of image analytics and the resultant findings from our models have valuable implications for effective disaster response operations, especially in smart urban settlements.}
}
@article{JIANG2021216,
title = {Deep convolutional neural networks for data delivery in vehicular networks},
journal = {Neurocomputing},
volume = {432},
pages = {216-226},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S092523122031924X},
author = {Hejun Jiang and Xiaolan Tang and Kai Jin and Wenlong Chen and Juhua Pu},
keywords = {Vehicular networks, Data delivery, Maximum flow, Deep convolutional neural networks, Deep learning},
abstract = {In vehicular networks, most content delivery schemes only utilize vehicle cooperation or powerful infrastructure to satisfy data requests. How to fully utilize vehicle-to-vehicle and vehicle-to-infrastructure communications to improve data acquisition still requires further analysis. In this paper, the content delivery problem is formulated as a maximum flow of a directed network, which implies the encounters and the requests. Despite of a high delivery ratio, the proposed Content delivery scheme using mAximum Flow (CAF) is infeasible in large-scale real-time applications due to high computational complexity. To solve this problem, we transform the GPS trajectory data into two-dimensional coverage grid maps which indicate the communication opportunities between vehicles and infrastructures in CAF. The map set, which consists of coverage grid maps in a storage cycle, and the number of satisfied requests obtained from CAF compose the training set that can be trained by the deep convolutional neural networks. This solution combining CAF with deep neural networks is called CAF-Net. In the experiments, we evaluate the performances of four popular architectures of deep convolutional neural networks when outputting the targets. The results show that ResNet 50 has the smallest error and the computation time of a delivery ratio is only 82.84 ms, which is a lot shorter than 4531.53 s using CAF. The results also demonstrate the feasibility of applying the deep learning framework to vehicular networks.}
}
@article{VAISH2021104504,
title = {Machine learning applications in power system fault diagnosis: Research advancements and perspectives},
journal = {Engineering Applications of Artificial Intelligence},
volume = {106},
pages = {104504},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104504},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621003523},
author = {Rachna Vaish and U.D. Dwivedi and Saurabh Tewari and S.M. Tripathi},
keywords = {Machine learning (ML), Reinforcement learning, Supervised learning, Transfer learning, Unsupervised learning},
abstract = {Newer generation sources and loads are posing new challenges to the conventional power system protection schemes. Adaptive and intelligent protection methodology, based on advanced measurement techniques and intelligent fault diagnosis such as machine learning (ML), is found to be useful to meet these challenges. A large number of research works are reported on ML-based power system fault diagnosis. However, ML techniques are evolving at a very fast pace, and an inclusive, as well as state-of-the-art review on ML-based power system fault diagnosis, is not available in the literature. Given this need and growing trend towards ML, the study presented in this paper aims to provide a comprehensive review of ML-based power system fault diagnosis. At first, efforts have been made to enlist the issues present in conventional fault diagnosis which led to the popularity of ML techniques. Also, a baseline framework and workflow for ML-based fault diagnosis are presented. Next, various unsupervised and supervised learning techniques have been discussed separately which have been used by several researchers for fault diagnosis. The discussion throughout is supported with tabulated facts for fault detection, classification and localization works with techniques used, different simulation tools used, and their application system. The advantages and disadvantages of all the techniques of fault diagnosis have also been discussed which will help the readers in the selection of techniques for their research. A brief review of reinforcement learning and transfer learning is also given as they are gaining popularity in power system-related studies and have the potential to be used for fault diagnosis. Finally, the research trends, some key issues, and directions for future research have been highlighted.}
}
@article{VAZQUEZCANTELI20191072,
title = {Reinforcement learning for demand response: A review of algorithms and modeling techniques},
journal = {Applied Energy},
volume = {235},
pages = {1072-1089},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918317082},
author = {José R. Vázquez-Canteli and Zoltán Nagy},
keywords = {Machine learning, Deep learning, HVAC control, Building energy, Electric vehicles, Smart grid},
abstract = {Buildings account for about 40% of the global energy consumption. Renewable energy resources are one possibility to mitigate the dependence of residential buildings on the electrical grid. However, their integration into the existing grid infrastructure must be done carefully to avoid instability, and guarantee availability and security of supply. Demand response, or demand-side management, improves grid stability by increasing demand flexibility, and shifts peak demand towards periods of peak renewable energy generation by providing consumers with economic incentives. This paper reviews the use of reinforcement learning, a machine learning algorithm, for demand response applications in the smart grid. Reinforcement learning has been utilized to control diverse energy systems such as electric vehicles, heating ventilation and air conditioning (HVAC) systems, smart appliances, or batteries. The future of demand response greatly depends on its ability to prevent consumer discomfort and integrate human feedback into the control loop. Reinforcement learning is a potentially model-free algorithm that can adapt to its environment, as well as to human preferences by directly integrating user feedback into its control logic. Our review shows that, although many papers consider human comfort and satisfaction, most of them focus on single-agent systems with demand-independent electricity prices and a stationary environment. However, when electricity prices are modelled as demand-dependent variables, there is a risk of shifting the peak demand rather than shaving it. We identify a need to further explore reinforcement learning to coordinate multi-agent systems that can participate in demand response programs under demand-dependent electricity prices. Finally, we discuss directions for future research, e.g., quantifying how RL could adapt to changing urban conditions such as building refurbishment and urban or population growth.}
}
@article{SENGAN2021107211,
title = {Detection of false data cyber-attacks for the assessment of security in smart grid using deep learning},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107211},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107211},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002068},
author = {Sudhakar Sengan and Subramaniyaswamy V and Indragandhi V and Priya Velayutham and Logesh Ravi},
keywords = {Cybersecurity, Deep learning, Integrity attack, Neural networks, Smart grids},
abstract = {Smart Grid uses electricity and information flows to set up a highly developed, fully automated, and distributed electricity grid system. To identify the reliability of work and availability, cyber attacks detection in the smart grids play a significant role. This paper highlights the integrity of false data cyber-attacks in the physical layers of smart grids. As the first contribution, the Proposed True Data Integrity provides an attack exposure metric through an Agent-Based Model. Next, the research focuses on the decentralization of Data Integrity Security in the system with an Agent-based approach. Finally, the productivity and efficiency of the developed modeling techniques are experimentally evaluated and compared with the existing state-of-the-art supervised deep-learning models. The obtained results of the studies have shown the improved false data detection accuracy of 98.19% through replay cyber-attacks using the Artificial Feed-forward Network. Based on the research findings, deep neural network can be used to assess cyber data in smart grids to detect malware incidents and attacks.}
}
@article{ZHANG2020104600,
title = {Constructing a PM2.5 concentration prediction model by combining auto-encoder with Bi-LSTM neural networks},
journal = {Environmental Modelling & Software},
volume = {124},
pages = {104600},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.104600},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219300192},
author = {Bo Zhang and Hanwen Zhang and Gengming Zhao and Jie Lian},
keywords = {Deep learning, Auto-encoder, Bi-LSTM, Data preprocessing, PM concentration prediction, Air pollution},
abstract = {Air pollution problems have a severe effect on the natural environment and public health. The application of machine learning to air pollutant data can result in a better understanding of environmental quality. Of these methods, the deep learning method has proven to be a very efficient and accurate method to forecast complex air quality data. This paper proposes a deep learning model based on an auto-encoder and bidirectional long short-term memory (Bi-LSTM) to forecast PM2.5 concentrations to reveal the correlation between PM2.5 and multiple climate variables. The model comprises several aspects, including data preprocessing, auto-encoder layer, and Bi-LSTM layer. The performance of the proposed model was verified based on a real-world air pollution dataset, and the results indicated this model can improve the prediction accuracy in an experimental scenario.}
}
@article{ZABIN2022101474,
title = {Applications of machine learning to BIM: A systematic literature review},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101474},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101474},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100224X},
author = {Asem Zabin and Vicente A. González and Yang Zou and Robert Amor},
keywords = {Building information modeling, Artificial intelligence, Machine learning, Data mining, Systematic literature review},
abstract = {As Building Information Modeling (BIM) workflows are becoming very relevant for the different stages of the project’s lifecycle, more data is produced and managed across it. The information and data accumulated in BIM-based projects present an opportunity for analysis and extraction of project knowledge from the inception to the operation phase. In other industries, Machine Learning (ML) has been demonstrated to be an effective approach to automate processes and extract useful insights from different types and sources of data. The rapid development of ML applications, the growing generation of BIM-related data in projects, and the different needs for use of this data present serious challenges to adopt and effectively apply ML techniques to BIM-based projects in the Architecture, Engineering, Construction and Operations (AECO) industry. While research on the use of BIM data through ML has increased in the past decade, it is still in a nascent stage. In order to asses where the industry stands today, this paper carries out a systematic literature review (SLR) identifying and summarizing common emerging areas of application and utilization of ML within the context of BIM-generated data. Moreover, the paper identifies research gaps and trends. Based on the observed limitations, prominent future research directions are suggested, focusing on information architecture and data, applications scalability, and human information interactions.}
}
@article{MUTHANNA202233,
title = {Deep reinforcement learning based transmission policy enforcement and multi-hop routing in QoS aware LoRa IoT networks},
journal = {Computer Communications},
volume = {183},
pages = {33-50},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421004394},
author = {Mohammed Saleh Ali Muthanna and Ammar Muthanna and Ahsan Rafiq and Mohammad Hammoudeh and Reem Alkanhel and Stephen Lynch and Ahmed A. {Abd El-Latif}},
keywords = {Long Range (loRa) communication, Internet of Things (IoT), Transmission policy parameters optimization, Deep reinforcement learning, Quality of Service (QoS), Provisioning multi-hop routing},
abstract = {The LoRa wireless connectivity has become a de facto technology for intelligent critical infrastructures such as transport systems. Achieving high Quality of Service (QoS) in cooperative systems remains a challenging task in LoRa. However, high QoS can be achieved via optimizing the transmission policy parameters such as spreading factor, bandwidth, code rate and carrier frequency. Yet existing approaches have not optimized the complete LoRa parameters. Furthermore, the star of stars topology used by LoRa causes more energy consumption and a low packet reception ratio. Motivated by this, this paper presents transmission policy enforcement and multi-hop routing for QoS-aware LoRa networks (MQ-LoRa). A hybrid cluster root rotated tree topology is constructed in which gateways follow a tree topology and Internet of Things (IoT) nodes follow a cluster topology. A ‘membrane’ inspired form the cell tissues which form clusters to sharing the correct information. The membrane inspired clustering algorithm is developed to form clusters and an optimal header node is selected using the influence score. Data QoS ranking is implemented for IoT nodes where priority and non-priority information is identified by the new field of LoRa frame structure (QRank). The optimal transmission policy enforcement uses fast deep reinforcement learning called Soft Actor Critic (SAC) that utilizes the environmental parameters including QRank, signal quality and signal-to-interference-plus-noise-ratio. The transmission policy is optimized with respect to the spreading factor, code rate, bandwidth and carrier frequency. Then, a concurrent optimization multi-hop routing algorithm that uses mayfly and shuffled shepherd optimization to rank routes based on the fitness criteria. Finally, a weighted duty cycle is implemented using a multi-weighted sum model to reduce resource wastage and information loss in LoRa IoT networks. Performance evaluation is implemented using a NS3.26 LoRaWAN module. The performance is examined for various metrics such as packet reception ratio, packet rejection ratio, energy consumption, delay and throughput. Experimental results prove that the proposed MQ-LoRa outperforms the well-known LoRa methods.}
}
@article{NAYAK2021108974,
title = {Routing in wireless sensor networks using machine learning techniques: Challenges and opportunities},
journal = {Measurement},
volume = {178},
pages = {108974},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.108974},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121000117},
author = {Padmalaya Nayak and G.K. Swetha and Surbhi Gupta and K. Madhavi},
keywords = {WSNs, Artificial Intelligence, Machine Learning Techniques, Routing},
abstract = {Energy conservation is the primary task in Wireless Sensor Networks (WSNs) as these tiny sensor nodes are the backbone of today’s Internet of Things (IoT) applications. These nodes rely exclusively on battery power to maneuver in hazardous environments. So, there is a requirement to study and design efficient, robust communication protocols to handle the challenges of the WSNs to make the network operational for a long time. Although traditional technologies solve many issues in WSNs, it may not derive an accurate mathematical model for predicting system behavior. So, some challenging tasks like routing, data fusion, localization, and object tracking are handled by low complexity mathematical models to define system behavior. In this paper, an effort has been made to provide a big outlook to the current “researchers” on machine learning techniques that have been employed to handle various issues in WSNs, and special attention has been given to routing problems.}
}
@article{QIANG2021460,
title = {Defending CNN against privacy leakage in edge computing via binary neural networks},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {460-470},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002326},
author = {Weizhong Qiang and Renwan Liu and Hai Jin},
keywords = {Privacy-preserving machine learning, Binary neural network, Edge computing},
abstract = {As the IoT has developed, edge computing has played an increasingly important role in the IoT ecosystem. The edge computing paradigm offers low latency and high computing performance, which is conducive to machine learning tasks such as object detection in autonomous driving. However, data privacy risks in edge computing still exist and the existing privacy-preserving methods are not satisfactory due to the large computational overhead and unbearable accuracy loss. We have designed a privacy-preserving machine learning framework for both user and cloud data. Users and the cloud provide data for inference and training respectively, and the privacy protection of these two aspects is both considered in this paper. Users provide test data and want to access the data-processing models in cloud for inference, and the cloud provides the training data used for training an eligible model. For user data, in order to maintain the overall performance of the machine learning framework while using homomorphic encryption, instead of providing encrypted data to all machine learning tasks, we divide the neural network into two parts, with one part kept on the trusted edge and provided with plaintext, and the other deployed on the untrusted cloud and provided with encrypted input. For cloud data, we apply the binary neural network, a network with the binarized value of weights. This method is practical for narrowing the confidence score gap (between the training and test sets) predicted by the model, which accounts most for a successful exploratory attack on training data. Experiments demonstrate that the results of the adversary’s membership inference attack are close to random guessing, and the accuracy is only slightly affected. Compared with the unencrypted network on VGG19, when the network is split from conv4_1 to fc8, the efficiency of using HE is only 100 to 30 times slower.}
}
@article{KANG201616,
title = {Cooperative mobile video transmission for traffic surveillance in smart cities},
journal = {Computers & Electrical Engineering},
volume = {54},
pages = {16-25},
year = {2016},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616301689},
author = {Shaojie Kang and Wen Ji and Seungmin Rho and Varshinee {Anu Padigala} and Yiqiang Chen},
keywords = {Cooperative transmission, Quality bottleneck, Data scheduling, Opnet},
abstract = {Cooperation transmission among mobile devices is critical for video streaming systems. Conventional systems usually require huge storage space and bandwidth, subsequently causing a bottleneck for the development of streaming systems. To resolve such problems, this study proposes a cooperative mechanism for mobiles that can make full use of local storage space and uploading bandwidth to relieve the computational load for streaming servers. Firstly, a statistical distribution of the life cycle of devices is dynamically established based on the analysis of random arrival and departure of mobile users. Secondly, an adaptive sliding-window technology based on Scalable Video Coding (SVC) is proposed in this study to change the size of windows in real time. Thirdly, an adaptive streaming solution is introduced in the OPNET environment, which cooperatively integrates the strategies of overlay formation, quality adaptation, and data scheduling. Simulation results showed that the proposed mechanism saved power consumption of devices and ensured the video quality for networks with limited resources.}
}
@article{JUNEJO202145,
title = {Lightweight Trust Model with Machine Learning scheme for secure privacy in VANET},
journal = {Procedia Computer Science},
volume = {194},
pages = {45-59},
year = {2021},
note = {18th International Learning & Technology Conference 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020998},
author = {Muhammad Haleem Junejo and Ab Al-Hadi {Ab Rahman} and Riaz Ahmed Shaikh and Kamaludin Mohamad Yusof and Dileep Kumar and Imran Memon},
keywords = {VANET, Trust Model, Machine Learning},
abstract = {A vehicular ad hoc network (VANETs) is transforming public transport into a safer wireless network, increasing its safety and efficiency. The VANET consists of several nodes which include RSU (Roadside Units), vehicles, traffic signals, and other wireless communication devices that are communicating sensitive information in a network. Nevertheless, security threats are increasing day by day because of dependency on network infrastructure, dynamic nature, and control technologies used in VANET. The security threats could be addressed widely by using machine learning and artificial intelligence on the road transport nodes. In this paper, a comparison of trust and cryptography was presented based on applications and security requirements of VANET.}
}
@article{REN202256,
title = {A privacy-protected intelligent crowdsourcing application of IoT based on the reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {127},
pages = {56-69},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003411},
author = {Yingying Ren and Wei Liu and Anfeng Liu and Tian Wang and Ang Li},
keywords = {Crowdsourcing, Reinforcement learning, Q-learning, Trust evaluation, Privacy-protection},
abstract = {The crowdsourcing scheme emerges as a promising solution for data-based application in the Internet of Things (IoT) network by dividing the large-scale complex sensing tasks into simple micro sensing tasks. AI technologies have been widely applied in crowdsourcing applications for IoT security. However, there are still several issues to be stressed. There exist malicious participants aiming at gaining unjust payment. A trust evaluation mechanism can effectively filter the attackers. Nevertheless, the existing trust evaluation mechanisms cannot preclude co-cheating and overlook the conflicts with the privacy exposure of participants. A novel Privacy-protected Intelligent Crowdsourcing scheme based on Reinforcement Learning (PICRL) is proposed. PICRL optimizes the utility of the system considering the data amount, data quality, and costs at the same time. The main innovations of the PICRL are as follows. First, the quality is guaranteed by an effective trust evaluation mechanism. The proposed trust evaluation consists of three parts: privacy trust, crowd trust, and hybrid active trust. Second, the trust evaluation can effectively prevent co-cheating and provide personal privacy exposure choice for the participants. Third, PICRL maximizes the utility based on evaluated trust utilizing the reinforcement method Q-learning without knowing the specific sensing model, which aims at maximizing the cumulated reward by the selection of state–action pair. The effectiveness of the proposed PICRL is verified by the extensive simulation experiments.}
}
@article{CHANG2020104129,
title = {Using deep learning and visual analytics to explore hotel reviews and responses},
journal = {Tourism Management},
volume = {80},
pages = {104129},
year = {2020},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2020.104129},
url = {https://www.sciencedirect.com/science/article/pii/S0261517720300558},
author = {Yung-Chun Chang and Chih-Hao Ku and Chien-Hung Chen},
keywords = {Deep learning, Convolutional neural network, Natural language processing, Visual analytics, Hospitality, Tourism},
abstract = {This study aims to use computational linguistics, visual analytics, and deep learning techniques to analyze hotel reviews and responses collected on TripAdvisor and to identify response strategies. To this end, we collected and analyzed 113,685 hotel reviews and responses and their semantic and syntactic relations. We are among the first to use visual analytics and deep learning-based natural language processing to empirically identify managerial responses. The empirical results indicate that our proposed multi-feature fusion, convolutional neural network model can make different types of data complement each other, thereby outperforming the comparisons. The visualization results can also be used to improve the performance of the proposed model and provide insights into response strategies, which further shows the theoretical and technical contributions of this study.}
}
@article{CHEN20211,
title = {Deep reinforcement learning for computation offloading in mobile edge computing environment},
journal = {Computer Communications},
volume = {175},
pages = {1-12},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001729},
author = {Miaojiang Chen and Tian Wang and Shaobo Zhang and Anfeng Liu},
keywords = {Internet of things (IoT), Reinforcement learning, Markov decision process, Computation offloading, Deep learning, Mobile edge computing},
abstract = {Recently, in order to distribute computing, networking resources, services, near terminals, mobile fog is gradually becoming the mobile edge computing (MEC) paradigm. In a mobile fog environment, the quality of service affected by offloading speeds and the fog processing, however the traditional fog method to solve the problem of computation resources allocation is difficult because of the complex network states distribution environment (that is, F-AP states, AP states, mobile device states and code block states). In this paper, to improve the fog resource provisioning performance of mobile devices, the learning-based mobile fog scheme with deep deterministic policy gradient (DDPG) algorithm is proposed. An offloading block pulsating discrete event system is modeled as a Markov Decision Processes (MDPs), which can realize the offloading computing without knowing the transition probabilities among different network states. Furthermore, the DDPG algorithm is used to solve the issue of state spaces explosion and learn an optimal offloading policy on distributed mobile fog computing. The simulation results show that our proposed scheme achieves 20%, 37%, 46% improvement on related performance compared with the policy gradient (PG), deterministic policy gradient (DPG) and actor–critic (AC) methods. Besides, compared with the traditional fog provisioning scheme, our scheme shows better cost performance of fog resource provisioning under different locations number and different task arrival rates.}
}
@article{MAJUMDAR2021102500,
title = {Congestion prediction for smart sustainable cities using IoT and machine learning approaches},
journal = {Sustainable Cities and Society},
volume = {64},
pages = {102500},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102500},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720307198},
author = {Sharmila Majumdar and Moeez M. Subhani and Benjamin Roullier and Ashiq Anjum and Rongbo Zhu},
keywords = {Smart and sustainable cities, Internet of Things (IOT), Traffic congestion, LSTM, Neural networks, Congestion propagation},
abstract = {Congestion on road networks has a negative impact on sustainability in many cities through the exacerbation of air pollution. Smart congestion management allows road users to avoid congested areas, decreasing pollutant concentration. Accurately predicting congestion propagation is difficult however, due to the dynamic non-linear behavior of traffic flow. With the rise of Internet of Things devices, there are now data sets available that can be used to provide smart, sustainable transport solutions within cities. In this work, we introduce long short-term memory networks for the prediction of congestion propagation across a road network. Based on vehicle speed data from traffic sensors at two sites, our model predicts the propagation of congestion across a 5-min period within a busy town. Analysis of both univariate and multivariate predictive models show an accuracy of 84–95% depending on the road layout. This accuracy shows that long short-term memory networks are suitable for predicting congestion propagation on road networks and may form a key component of future traffic modelling approaches for smart and sustainable cities around the world.}
}
@article{ABOSULIMAN2021107555,
title = {Computer vision assisted human computer interaction for logistics management using deep learning},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107555},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107555},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004997},
author = {Shougi Suliman Abosuliman and Alaa Omran Almagrabi},
keywords = {Deep learning, Human-computer interaction, Long short term memory, Logistics management, Decision making},
abstract = {Human-Computer Interaction is the secret to technological advancement in the area of logistics and supply chain. The key challenges are the degree of energy transferred to devices, like automated vehicles and robotic equipment, and lack of belief in intelligent decision-making, which may overrule the system in the event of misperceptions of automated decisions. This paper presents an efficient Logistics Management Framework Using Deep Learning (eLMF-DL) to implement the computer vision-assisted Human-Computer Interaction (HCI) in the logistic management sector. With a hybrid CNN-LSTM network, eLMF-DL implements a single-stage or one-step convergence optimum decision-support design model that intelligently combines production maximization and demand forecasting. The architecture with the integration of convolutional neural network and long short-term memory network models the machine dynamics and relationships in assorted diverse logistics services demand. To determine uncertainties through dynamic delivery and optimal decisions on allocating logistical service power, the eLMF-DL results in the highest performance.}
}
@article{LIANG2019102,
title = {Deep Learning-Based Power Usage Forecast Modeling and Evaluation},
journal = {Procedia Computer Science},
volume = {154},
pages = {102-108},
year = {2019},
note = {Proceedings of the 9th International Conference of Information and Communication Technology [ICICT-2019] Nanning, Guangxi, China January 11-13, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307859},
author = {Fan Liang and Austin Yu and William G. Hatcher and Wei Yu and Chao Lu},
keywords = {Deep Learning, Smart Grid, Internet of Things, Cyber-Physical Systems},
abstract = {The growing Internet of Things (IoT) provides significant resources to be integrated with critical infrastructures to enable cyber-physical systems. More specifically, the deployment of smart meters for electricity usage monitoring in the smart grid can provide granular and detailed information from which power load forecasting can be carried out. However, the accurate prediction of long-term power usage remains a challenging issue. In light of many recent advances, deep learning has the potential to significantly improve the ability to assess data and make predictions, and is already rapidly changing the world we live in. As such, in this paper, we consider the use of deep learning, via Recursive Neural Network (RNN) and Long Short-Term Memory layers, for the long-term prediction of localized power consumption. In particular, we consider the optimization of both data feature sets and neural network models, developing three model-feature combinations to maximize prediction accuracy and minimize error. Through detailed experimental evaluation, our results demonstrate the ability to achieve highly accurate predictions over periods as large as 21 days through the integration of correlated features.}
}
@article{YANG2020145,
title = {Reinforcement learning in sustainable energy and electric systems: a survey},
journal = {Annual Reviews in Control},
volume = {49},
pages = {145-163},
year = {2020},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2020.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578820300079},
author = {Ting Yang and Liyuan Zhao and Wei Li and Albert Y. Zomaya},
keywords = {Reinforcement learning, Sustainable energy and electric systems, Deep reinforcement learning, Power system, Integrated energy system},
abstract = {The dynamic nature of sustainable energy and electric systems can vary significantly along with the environment and load change, and they represent the features of multivariate, high complexity and uncertainty of the nonlinear system. Moreover, the integration of intermittent renewable energy sources and energy consumption behaviours of households introduce more uncertainty into sustainable energy and electric systems. The operation, control and decision-making in such an environment definitely require increasing intelligence and flexibility in the control and optimization to ensure the quality of service of sustainable energy and electric systems. Reinforcement learning is a wide class of optimal control strategies that uses estimating value functions from experience, simulation, or search to learn in highly dynamic, stochastic environment. The interactive context enables reinforcement learning to develop strong learning ability and high adaptability. Reinforcement learning does not require the use of the model of system dynamics, which makes it suitable for sustainable energy and electric systems with complex nonlinearity and uncertainty. The use of reinforcement learning in sustainable energy and electric systems will certainly change the traditional energy utilization mode and bring more intelligence into the system. In this survey, an overview of reinforcement learning, the demand for reinforcement learning in sustainable energy and electric systems, reinforcement learning applications in sustainable energy and electric systems, and future challenges and opportunities will be explicitly addressed.}
}
@article{LEE2021103111,
title = {Towards secure intrusion detection systems using deep learning techniques: Comprehensive analysis and review},
journal = {Journal of Network and Computer Applications},
volume = {187},
pages = {103111},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103111},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001314},
author = {Sang-Woong Lee and Haval {Mohammed sidqi} and Mokhtar Mohammadi and Shima Rashidi and Amir Masoud Rahmani and Mohammad Masdari and Mehdi Hosseinzadeh},
keywords = {Intrusion detection, Deep learning, Auto-encoder, CNN, DNN, GAN, LSTM},
abstract = {Providing a high-performance Intrusion Detection System (IDS) can be very effective in controlling malicious behaviors and cyber-attacks. Regarding the ever-growing negative impacts of the security attacks on computer systems and networks, various Artificial Intelligence (AI)-based techniques have been used to introduce versatile IDS approaches. Deep learning is a branch of AI techniques, mainly based on multi-layer artificial neural networks. Recently, deep learning techniques have gained momentum in the intrusion detection domain and several IDS approaches are provided in the literature using various deep neural networks to deal with privacy concerns and security threats. For this purpose, this article focuses on the deep IDS approaches and investigates how deep learning networks are employed by different approaches in different steps of the intrusion detection process to achieve better results. It classifies the studied IDS schemes regarding the deep learning networks utilized in them and describes their main contributions and capabilities. Besides, in each category, their main features such as evaluated metrics, datasets, simulators, and environments are compared. Also, a comparison of the deep IDS approaches main properties are provided to illuminate the main techniques applied in them as well as the area less focused in the literature. Finally, the concluding remarks in the deep IDS context are provided and possible directions at the subsequent studies are listed.}
}
@article{THORAT2021100048,
title = {TaxoDaCML: Taxonomy based Divide and Conquer using machine learning approach for DDoS attack classification},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {2},
pages = {100048},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000410},
author = {Onkar Thorat and Nirali Parekh and Ramchandra Mangrulkar},
keywords = {Information management security, Machine learning, Distributed denial of service, Reflection attack, Exploitation attack},
abstract = {Distributed Denial of Service (DDoS) attack is one of the most dangerous attacks that result in bringing down the server(s) and it is essential to classify the exact attack to implement robust security measures. In this work, we present an approach for detecting the prominent DDoS attacks that can be carried over Transport Layer protocols. Four different levels are taken into consideration which helps to classify one of the 11 different attacks. A bigger problem is divided into smaller ones and then conquered. This approach, called TaxoDaCML - Taxonomy-based Divide and Conquer approach using ML minimizes computational cost and at the same time maintains the required accuracy. Results prove that our approach achieves 99.9% accuracy for DDoS attack detection and more than 85% for DDoS attack classification. Comparison of TaxoDaCML is done with the previous works and is found to perform better for DDoS attacks classification.}
}
@article{ANAGNOSTOPOULOS2015178,
title = {Assessing dynamic models for high priority waste collection in smart cities},
journal = {Journal of Systems and Software},
volume = {110},
pages = {178-192},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.08.049},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001922},
author = {Theodoros Anagnostopoulos and Kostas Kolomvatsos and Christos Anagnostopoulos and Arkady Zaslavsky and Stathes Hadjiefthymiades},
keywords = {Internet of things, Smart cities, Dynamic routing models, Waste collection},
abstract = {Waste Management (WM) represents an important part of Smart Cities (SCs) with significant impact on modern societies. WM involves a set of processes ranging from waste collection to the recycling of the collected materials. The proliferation of sensors and actuators enable the new era of Internet of Things (IoT) that can be adopted in SCs and help in WM. Novel approaches that involve dynamic routing models combined with the IoT capabilities could provide solutions that outperform existing models. In this paper, we focus on a SC where a number of collection bins are located in different areas with sensors attached to them. We study a dynamic waste collection architecture, which is based on data retrieved by sensors. We pay special attention to the possibility of immediate WM service in high priority areas, e.g., schools or hospitals where, possibly, the presence of dangerous waste or the negative effects on human quality of living impose the need for immediate collection. This is very crucial when we focus on sensitive groups of citizens like pupils, elderly or people living close to areas where dangerous waste is rejected. We propose novel algorithms aiming at providing efficient and scalable solutions to the dynamic waste collection problem through the management of the trade-off between the immediate collection and its cost. We describe how the proposed system effectively responds to the demand as realized by sensor observations and alerts originated in high priority areas. Our aim is to minimize the time required for serving high priority areas while keeping the average expected performance at high level. Comprehensive simulations on top of the data retrieved by a SC validate the proposed algorithms on both quantitative and qualitative criteria which are adopted to analyze their strengths and weaknesses. We claim that, local authorities could choose the model that best matches their needs and resources of each city.}
}
@article{QAVIDELFARD2022111771,
title = {Application of machine learning in thermal comfort studies: A review of methods, performance and challenges},
journal = {Energy and Buildings},
volume = {256},
pages = {111771},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111771},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821010550},
author = {Zahra {Qavidel Fard} and Zahra Sadat Zomorodian and Sepideh Sadat Korsavi},
keywords = {Thermal comfort, Machine learning, Group-based models, Personal comfort models, Performance, Prediction accuracy},
abstract = {This paper provides a systematic review on the application of Machine Learning (ML) in thermal comfort studies to highlight the latest methods and findings and provide an agenda for future studies. Reviewed studies were investigated to highlight ML applications, parameters, methods, performance and challenges. The results show that 62% of reviewed studies focused on developing group-based comfort models, while 35% focused on personal comfort models (PCMs) which account for individual differences and present high prediction accuracy. ML models could outperform PMV and adaptive models with up to 35.9% and 31% higher accuracy and PCMs could outperform PMV models with up to 74% higher accuracy. Applying ML-based control schemas reduced thermal comfort-related energy consumption in buildings up to 58.5%, while improving indoor quality up to 90% and reducing CO2 levels up to 24%. Using physiological parameters improved the prediction accuracy of PCMs up to 97%. Future studies are recommended to further investigate PCMs, determine the optimum sample size and consider both fitting and error metrics for model evaluation. This study introduces data collection, thermal comfort indices, time scale, sample size, feature selection, model selection, and real world application as the remaining challenges in the application of ML in thermal comfort studies.}
}
@article{DAWOUD201882,
title = {Deep learning and software-defined networks: Towards secure IoT architecture},
journal = {Internet of Things},
volume = {3-4},
pages = {82-89},
year = {2018},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S2542660518300593},
author = {Ahmed Dawoud and Seyed Shahristani and Chun Raun},
keywords = {Internet of Things, Software-defined networks, Anomalies detection, Deep learning, Restricted-Boltzmann machines},
abstract = {Internet of Things (IoT) introduces new challenges to conventional communication model. IoT networks characteristics, such as objects heterogeneity and scalability, require revolutionary solutions. Currently, there is no universal architecture for IoT. However, several architectures were proposed based on Software Defined Networks (SDN). SDN introduces network programmability, and centralisation, these features facilitate network abstractions, simplifying network management and eases evolution. In this paper, we investigate SDN as a novel communication architecture for IoT networking to enhance the security and resilience of IoT. SDN enhances network resilience and scalability which are essential in large-scale IoT deployments, e.g., smart cities. However, security is a significant concern for IoT while SDN deepens these concerns. SDN itself presents new security threats; specifically, threats related to the controller. We propose a secure, framework for IoT based on SDN. The framework is generalization for the integration of SDN and IoT. We focus on massive IoT deployment, for instance, smart cities applications, where, security is critical, and network traffic is enormous. The study investigates the SDN architecture from a security perspective. Improving SDN security will boost the deployment of SDN-based IoT architecture. We deploy an Intrusion detection system based on Deep Learning (DL). The detection module uses Restricted Boltzmann Machines (RBM). The precision rate shows significant improvements over standard ML, e.g. SVM and PCA.}
}
@article{TSAI2020106068,
title = {Optimizing hyperparameters of deep learning in predicting bus passengers based on simulated annealing},
journal = {Applied Soft Computing},
volume = {88},
pages = {106068},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106068},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620300089},
author = {Chun-Wei Tsai and Chien-Hui Hsia and Shuang-Jie Yang and Shih-Jui Liu and Zhi-Yan Fang},
keywords = {Bus transportation system, Simulated annealing, Deep learning, Hyperparameter optimization},
abstract = {Bus is certainly one of the most widely used public transportation systems in a modern city because it provides an inexpensive solution to public transportation users, such as commuters and tourists. Most people would like to avoid taking a crowded bus on the way. That is why forecasting the number of bus passengers has been a critical problem for years. The proposed method is inspired by the fact that there is no easy way to know the suitable parameters for most of the deep learning methods in solving the optimization problem of forecasting the number of passengers on a bus. To address this issue, the proposed algorithm uses a simulated annealing (SA) to find out a suitable number of neurons for each layer of a fully connected deep neural network (DNN) to enhance the accuracy rate in solving this particular optimization problem. The proposed method is compared with support vector machine, random forest, eXtreme gradient boosting, deep neural network, and deep neural network with dropout for the data provided by the Taichung city smart transportation big data research center, Taiwan (TSTBDRC). Our simulation results indicate that the proposed method outperforms all the other forecasting methods for forecasting the number of bus passengers in terms of the accuracy rate and the prediction time.}
}
@article{CHEN2021119,
title = {Delay-aware model-based reinforcement learning for continuous control},
journal = {Neurocomputing},
volume = {450},
pages = {119-128},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005427},
author = {Baiming Chen and Mengdi Xu and Liang Li and Ding Zhao},
keywords = {Model-based reinforcement learning, Markov decision process, Continuous control, Delayed system},
abstract = {Action delays degrade the performance of reinforcement learning in many real-world systems. This paper proposes a formal definition of delay-aware Markov Decision Process and proves it can be transformed into standard MDP with augmented states using the Markov reward process. We develop a delay-aware model-based reinforcement learning framework that can incorporate the multi-step delay into the learned system models without learning effort. Experiments with the Gym and MuJoCo platforms show that the proposed delay-aware model-based algorithm is more efficient in training and transferable between systems with various durations of delay compared with state-of-the-art model-free reinforcement learning methods.}
}
@article{BELHADI2021102541,
title = {Privacy reinforcement learning for faults detection in the smart grid},
journal = {Ad Hoc Networks},
volume = {119},
pages = {102541},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102541},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521000913},
author = {Asma Belhadi and Youcef Djenouri and Gautam Srivastava and Alireza Jolfaei and Jerry Chun-Wei Lin},
keywords = {Energy systems, Privacy learning, Reinforcement learning, Anomaly detection, Smart grid},
abstract = {Recent anticipated advancements in ad hoc Wireless Mesh Networks (WMN) have made them strong natural candidates for Smart Grid’s Neighborhood Area Network (NAN) and the ongoing work on Advanced Metering Infrastructure (AMI). Fault detection in these types of energy systems has recently shown lots of interest in the data science community, where anomalous behavior from energy platforms is identified. This paper develops a new framework based on privacy reinforcement learning to accurately identify anomalous patterns in a distributed and heterogeneous energy environment. The local outlier factor is first performed to derive the local simple anomalous patterns in each site of the distributed energy platform. A reinforcement privacy learning is then established using blockchain technology to merge the local anomalous patterns into global complex anomalous patterns. Besides, different optimization strategies are suggested to improve the whole outlier detection process. To demonstrate the applicability of the proposed framework, intensive experiments have been carried out on well-known CASAS (Center of Advanced Studies in Adaptive Systems) platform. Our results show that our proposed framework outperforms the baseline fault detection solutions.}
}
@article{MOTTA2021102154,
title = {A mixed approach for urban flood prediction using Machine Learning and GIS},
journal = {International Journal of Disaster Risk Reduction},
volume = {56},
pages = {102154},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102154},
url = {https://www.sciencedirect.com/science/article/pii/S2212420921001205},
author = {Marcel Motta and Miguel {de Castro Neto} and Pedro Sarmento},
keywords = {Flood prediction, Resilience planning, Smart cities, Machine learning, GIS},
abstract = {Extreme weather conditions, as one of many effects of climate change, is expected to increase the magnitude and frequency of environmental disasters. In parallel, urban centres are also expected to grow significantly in the next years, making necessary to implement the adequate mechanisms to tackle such threats, more specifically flooding. This project aims to develop a flood prediction system using a combination of Machine Learning classifiers along with GIS techniques to be used as an effective tool for urban management and resilience planning. This approach can establish sensible factors and risk indices for the occurrence of floods at the city level, which could be instrumental for outlining a long-term strategy for Smart Cities. The most performant Machine Learning model was a Random Forest, with a Matthew's Correlation Coefficient of 0.77 and an Accuracy of 0.96. To support and extend the capabilities of the Machine Learning model, a GIS model was developed to find areas with higher likelihood of being flooded under critical weather conditions. Therefore, hot spots were defined for the entire city given the observed flood history. The scores obtained from the Random Forest model and the Hot Spot analysis were then combined to create a flood risk index.}
}
@article{HAO2021101470,
title = {URLLC resource slicing and scheduling for trustworthy 6G vehicular services: A federated reinforcement learning approach},
journal = {Physical Communication},
volume = {49},
pages = {101470},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2021.101470},
url = {https://www.sciencedirect.com/science/article/pii/S187449072100207X},
author = {Min Hao and Dongdong Ye and Siming Wang and Beihai Tan and Rong Yu},
keywords = {6G, Vehicular edge computing, Resource slicing, Federated learning, Zero trust architecture},
abstract = {In the upcoming 6G era, vehicles will massively connect to the wireless network through edge access points such as roadside units (RSUs). The increasing number of connected vehicles and vehicular services will take 6G vehicular network to a new challenging security boundary, i.e., the so-called zero trust network. The traditional resource slicing and scheduling solution has to evolve to deal with the zero trust security problems. In this paper, we consider the trustworthy 6G vehicular services and focus on the typical scenario of vehicular task offloading with resource slicing and scheduling. In order to prevent vehicles from malicious attacks by untrusted edge access points, we exploit a subjective logic model to score the reputation of edge nodes. The vehicles will select the edge nodes with high reputation for task offloading. After that, we develop an federated asynchronous reinforcement learning algorithm to optimize the offloading problem. The simulation results show that our approach can efficiently schedule the slice resources and effectively protect the information security of vehicles.}
}
@article{CHAUHAN2021103726,
title = {An effective face recognition system based on Cloud based IoT with a deep learning model},
journal = {Microprocessors and Microsystems},
volume = {81},
pages = {103726},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103726},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120308711},
author = {Deepika Chauhan and Ashok Kumar and Pradeep Bedi and Vijay Anant Athavale and D. Veeraiah and Boppuru Rudra Pratap},
keywords = {Deep learning, IoT, Cloud, Edge computing, Deep neural network},
abstract = {As of late, the Internet of Things (IoT) innovation has been utilized in applications, for example, transportation, medical care, video observation, and so on. The quick appropriation and development of IoT in these segments are producing an enormous measure of information. For instance, IoT gadgets, for example, cameras produce various pictures when utilized in medical clinic reconnaissance sees. Here, face acknowledgement is one of the most significant instruments that can be utilized for clinic affirmations, enthusiastic discovery, and identification of patients, location of fake gadgets. patient, and test clinic models. Programmed and shrewd face acknowledgement frameworks are profoundly precise in an overseen climate; notwithstanding, they are less exact in an unmanaged climate. Additionally, frameworks must keep on running on numerous occasions in different applications, for example, insightful wellbeing. This work presents a tree-based profound framework for programmed face acknowledgement in a cloud climate. The inside and out pattern have been proposed to cost less for the PC without focusing on unwavering quality. In the model, the additional size is isolated into a few sections, and a stick is made for each part. The tree is characterized by its branch area and stature. The branches are spoken to by a leftover capacity, which comprises of a twofold layer, a stack game plan, and a non-direct capacity. The proposed technique is assessed in an assortment of generally accessible information bases. An examination of the method is likewise finished with top to bottom craftsmanship models for the eye to eye connection. The aftereffects of the tests indicated that the example was considered to have accomplished a precision of 98.65%, 99.19%, and 95.84%.}
}
@article{WANG2022101897,
title = {Deep learning for assessment of environmental satisfaction using BIM big data in energy efficient building digital twins},
journal = {Sustainable Energy Technologies and Assessments},
volume = {50},
pages = {101897},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101897},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821009115},
author = {Weixi Wang and Han Guo and Xiaoming Li and Shengjun Tang and Jizhe Xia and Zhihan Lv},
keywords = {Building digital twins, BIM big data, Deep learning, Assessment of environmental satisfaction, Energy efficient building},
abstract = {Energy efficient Building Digital Twins (BDTs) are researched using Building Information Model (BIM) to explore the key techniques of Digital Twins (DTs). DTs in buildings can be regarded as an expression of “BIM+,” born to digital descriptions. Comprehensive perception of physical systems is the preconditions for DTs implementation. BIM’s energy-saving design includes the selection of building orientation and building shape. BIM energy consumption analysis can compare different materials, examine the performance of various materials, and select the most suitable and most energy-efficient materials for building structure maintenance. Data Fusion Algorithm (DFA) in Wireless Sensor Networks (WSNs) is improved. A novel DFA is constructed by combining Backpropagation Neural Network (BPNN) with Dynamic Host Configuration Protocol (DCHP), recorded as BP-DCHP. Simulation experiment proves that BP-DCHP can prolong sensor nodes’ survival time and provide the highest data fusion quality. BP-DCHP runs for about 310 s, 500 s, and 705 s in WSNs consisting of 20, 50, and 100 WSNs, respectively. Moreover, BP-DCHP can provide higher quality given insufficient data fusion degree. Once the WSNs consume 50% of the total initial energy, BP-DCHP presents a shorter network delay, only 0.6 s on average in the 100-sensor-node-WSN. To validate BDTs’ effectiveness, the environmental satisfaction of residents from two Beijing intelligent communities is assessed using Deep Learning (DL) approach. Taking the data as the clue, the study establishes DTs serving the application of urban scene, which plays a certain role in promoting the technological innovation of BDTs, better optimizing the city and managing the city.}
}
@article{JAVED2021102572,
title = {Automated cognitive health assessment in smart homes using machine learning},
journal = {Sustainable Cities and Society},
volume = {65},
pages = {102572},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102572},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720307903},
author = {Abdul Rehman Javed and Labiba Gillani Fahad and Asma Ahmad Farhan and Sidra Abbas and Gautam Srivastava and Reza M. Parizi and Mohammad S. Khan},
keywords = {Cognitive assessment, Healthcare, Internet of Things, Remote Monitoring, Smart cities, Smart homes, Sustainability, MCI, Dementia, Machine learning},
abstract = {The Internet of Things (IoT) provides smart solutions for future urban communities to address key benefits with the least human intercession. A smart home offers the necessary capabilities to promote efficiency and sustainability to a resident with their healthcare-related, social, and emotional needs. In particular, it provides an opportunity to assess the functional health ability of the elderly or individuals with cognitive impairment in performing daily life activities. This work proposes an approach named Cognitive Assessment of Smart Home Resident (CA-SHR) to measure the ability of smart home residents in executing simple to complex activities of daily living using pre-defined scores assigned by a neuropsychologist. CA-SHR also measures the quality of tasks performed by the participants using supervised classification. Furthermore, CA-SHR provides a temporal feature analysis to estimate if the temporal features help to detect impaired individuals effectively. The goal of this study is to detect cognitively impaired individuals in their early stages. CA-SHR assess the health condition of individuals through significant features and improving the representation of dementia patients. For the classification of individuals into healthy, Mild Cognitive Impaired (MCI), and dementia categories, we use ensemble AdaBoost. This results in improving the reliability of the CA-SHR through the correct assignment of labels to the smart home resident compared with existing techniques.}
}
@article{TIEN2021603,
title = {A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand},
journal = {Renewable Energy},
volume = {177},
pages = {603-625},
year = {2021},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.05.155},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121008442},
author = {Paige Wenbin Tien and Shuangyu Wei and Tianshu Liu and John Calautit and Jo Darkwa and Christopher Wood},
keywords = {Deep learning, Building energy management, Building ventilation, Window opening, Window detection, HVAC systems},
abstract = {Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model.}
}
@article{JAURO2020106582,
title = {Deep learning architectures in emerging cloud computing architectures: Recent development, challenges and next research trend},
journal = {Applied Soft Computing},
volume = {96},
pages = {106582},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106582},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620305202},
author = {Fatsuma Jauro and Haruna Chiroma and Abdulsalam Y. Gital and Mubarak Almutairi and Shafi’i M. Abdulhamid and Jemal H. Abawajy},
keywords = {Convolutional neural network, Deep learning, Deep reinforcement learning, Edge computing, Fog computing, Emerging cloud computing, Serverless computing},
abstract = {The challenges of the conventional cloud computing paradigms motivated the emergence of the next generation cloud computing architectures. The emerging cloud computing architectures generate voluminous amount of data that are beyond the capability of the shallow intelligent algorithms to process. Deep learning algorithms, with their ability to process large-scale datasets, have recently started gaining tremendous attentions from researchers to solve problem in the emerging cloud computing architectures. However, no comprehensive literature review exists on the applications of deep learning architectures to solve complex problems in emerging cloud computing architectures. To fill this gap, we conducted a comprehensive literature survey on the applications of deep learning architectures in emerging cloud computing architectures. The survey shows that the adoption of deep learning architectures in emerging cloud computing architectures are increasingly becoming an interesting research area. We introduce a new taxonomy of deep learning architectures for emerging cloud computing architectures and provide deep insights into the current state-of-the-art active research works on deep learning to solve complex problems in emerging cloud computing architectures. The synthesis and analysis of the articles as well as their limitation are presented. A lot of challenges were identified in the literature and new future research directions to solve the identified challenges are presented. We believed that this article can serve as a reference guide to new researchers and an update for expert researchers to explore and develop more deep learning applications in the emerging cloud computing architectures.}
}
@article{WANG2022126736,
title = {Traffic prediction based on auto spatiotemporal Multi-graph Adversarial Neural Network},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {590},
pages = {126736},
year = {2022},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2021.126736},
url = {https://www.sciencedirect.com/science/article/pii/S0378437121009407},
author = {Jun Wang and Wenjun Wang and Xueli Liu and Wei Yu and Xiaoming Li and Peiliang Sun},
keywords = {Traffic prediction, Spatiotemporal state, CNN, GCN},
abstract = {Traffic prediction plays an essential role in the intelligent transportation systems and has broad applications in transportation management and planning. And the key to this field is to explore the spatiotemporal information of traffic data, synchronously. Recently, various deep learning methods, such as Convolution Neural Network (CNN) and Graph Convolutional Network (GCN), have shown promising performance in traffic prediction. However, these methods cannot automatically model spatial dependencies and dynamic spatiotemporal States, and there are no constraints on the distribution of outputs. To solve the above problems, in this paper, a method of automatically obtaining spatiotemporal dependence in data, which can automatically obtain the spatiotemporal state and spatiotemporal dependency using Multi-graph Adversarial Neural Network (GAN) and is named AST-MAGCN, is proposed. The new method AST-MAGCN combines GAN and GCN, extracts spatiotemporal state of the data in real-time, and outputs the traffic forecast by GAN constraint. Lastly, the proposed method is evaluated on two real-world traffic datasets, and the experimental results show that the proposed method outperforms baseline traffic prediction methods.}
}
@article{ALSHAMRANI2021,
title = {IoT and artificial intelligence implementations for remote healthcare monitoring systems: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001385},
author = {Mazin Alshamrani},
keywords = {RHM, H-IoT, Healthcare, Machine learning, Monitoring systems},
abstract = {The Internet of Things (IoT) and artificial intelligence (AI) are two of the fastest-growing technologies in the world. With more people moving to cities, the concept of a smart city is not foreign. The idea of a smart city is based on transforming the healthcare sector by increasing its efficiency, lowering costs, and putting the focus back on a better patient care system. Implementing IoT and AI for remote healthcare monitoring (RHM) systems requires a deep understanding of different frameworks in smart cities. These frameworks occur in the form of underlying technologies, devices, systems, models, designs, use cases, and applications. The IoT-based RHM system mainly employs both AI and machine learning (ML) by gathering different records and datasets. On the other hand, ML methods are broadly used to create analytic representations and are incorporated into clinical decision support systems and diverse healthcare service forms. After carefully examining each factor in clinical decision support systems, a unique treatment, lifestyle advice, and care strategy are proposed to patients. The technology used helps to support healthcare applications and analyze activities, body temperature, heart rate, blood glucose, etcetera. Keeping this in mind, this paper provides a survey that focuses on the identification of the most relevant health Internet of things (H-IoT) applications supported by smart city infrastructure. This study also evaluates related technologies and systems for RHM services by understanding the most pertinent monitoring applications based on several models with different corresponding IoT-based sensors. Finally, this research contributes to scientific knowledge by highlighting the main limitations of the topic and recommending possible opportunities in this research area.}
}
@article{WANG2022102542,
title = {Machine learning for encrypted malicious traffic detection: Approaches, datasets and comparative study},
journal = {Computers & Security},
volume = {113},
pages = {102542},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102542},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003667},
author = {Zihao Wang and Kar Wai Fok and Vrizlynn L.L. Thing},
keywords = {encrypted malicious traffic detection, traffic classification, machine learning, deep learning},
abstract = {As people’s demand for personal privacy and data security becomes a priority, encrypted traffic has become mainstream in the cyber world. However, traffic encryption is also shielding malicious and illegal traffic introduced by adversaries, from being detected. This is especially so in the post-COVID-19 environment where malicious traffic encryption is growing rapidly. Common security solutions that rely on plain payload content analysis such as deep packet inspection are rendered useless. Thus, machine learning based approaches have become an important direction for encrypted malicious traffic detection. In this paper, we formulate a universal framework of machine learning based encrypted malicious traffic detection techniques and provided a systematic review. Furthermore, current research adopts different datasets to train their models due to the lack of well-recognized datasets and feature sets. As a result, their model performance cannot be compared and analyzed reliably. Therefore, in this paper, we analyse, process and combine datasets from 5 different sources to generate a comprehensive and fair dataset to aid future research in this field. On this basis, we also implement and compare 10 encrypted malicious traffic detection algorithms. We then discuss challenges and propose future directions of research.}
}
@article{YANG2022105522,
title = {Predicting multiple types of traffic accident severity with explanations: A multi-task deep learning framework},
journal = {Safety Science},
volume = {146},
pages = {105522},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105522},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521003659},
author = {Zekun Yang and Wenping Zhang and Juan Feng},
keywords = {Deep learning, Traffic accident severity prediction, Explainable artificial intelligence, Machine learning},
abstract = {Predicting traffic accident severity is essential for traffic accident prevention and vulnerable road user safety. Furthermore, the explainability of the prediction is crucial for practitioners to extract relevant risk factors and implement corresponding countermeasures. Most extant research ignores the property loss severity of traffic accidents and fails to predict different levels of death and property loss severity. Moreover, while the explainability of traditional models is easy to achieve, an explainable design of deep neural network (DNN) is extremely deficient in existing research. Few attempts that incorporate neural networks suffer from the lack of multiple hidden layers and the negligence of structural information when explaining predictions. In this study, we propose a multi-task DNN framework for predicting different levels of injury, death, and property loss severity. The multi-task and deep learning design enables a comprehensive and precise analysis of traffic accident severity. Unlike many black-box DNN algorithms, our framework could identify key factors that cause the three types of traffic accident severity via layer-wise relevance propagation, which generates explanations based on the structure and weights of DNN. Based on the experiments conducted using Chinese traffic accident data, our proposed model predicts traffic accident severity risks with good accuracy and outperforms state-of-the-art methods. Furthermore, the case studies show that the key factors provided by our framework are more reasonable and informative than the explanations provided by baseline methods. Our model is the first multi-task learning model and the first DNN-based model for traffic accident severity prediction to the best of our knowledge.}
}
@article{AKHTER2021,
title = {Precision agriculture using IoT data analytics and machine learning},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001282},
author = {Ravesa Akhter and Shabir Ahmad Sofi},
keywords = {Internet of things (IoT), Data analytics (DA), Machine learning (ML)},
abstract = {In spite of the insight commonality may have concerning agrarian practice, fact is that nowadays agricultural science diligence is accurate, precise, data-driven, and vigorous than ever. The emanation of the technologies based on Internet of Things (IoT) has reformed nearly each industry like smart city, smart health, smart grid, smart home, including “smart agriculture or precision agriculture”. Applying machine learning using the IoT data analytics in agricultural sector will rise new benefits to increase the quantity and quality of production from the crop fields to meet the increasing food demand. Such world-shattering advancements are rocking the current agrarian approaches and generating novel and best chances besides a number of limitations. This paper climaxes the power and capability of computing techniques including internet of things, wireless sensor networks, data analytics and machine learning in agriculture. The paper proposed the prediction model of Apple disease in the apple orchards of Kashmir valley using data analytics and Machine learning in IoT system. Furthermore, a local survey was conducted to know from the farmers about the trending technologies and their effect in precision agriculture. Finally, the paper discusses the challenges faced when incorporating these technologies in the traditional farming approaches.}
}
@article{CHOWANDA2021821,
title = {Exploring Text-based Emotions Recognition Machine Learning Techniques on Social Media Conversation},
journal = {Procedia Computer Science},
volume = {179},
pages = {821-828},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001320},
author = {Andry Chowanda and Rhio Sutoyo and  Meiliana and Sansiri Tanachutiwat},
keywords = {Machine Learning, Emotions Recognition, Social Media, Text-based Emotions},
abstract = {Emotions hold a paramount role in the conversation, as it expresses context to the conversation. Text/word in conversation consists of lexical and contextual meanings. Extracting emotions from text has been an interesting work recent thees years. With the advancement of machine learning techniques and hardware to support the machine learning process, recognising emotions from a text with machine learning provides promising and significant results. This research aims to explore several popular machine learning to recognise emotions from a conversation in social media. The algorithms proposed in this research are ranged from traditional machine learning to deep learning techniques. The dataset used in this paper is provided by AffectiveTweets, with a baseline of F1Score of 0.71 with word N-grams and SentiStrength. The research contributes extensive explorations in a number of machine learning algorithms, resulting in a total of 2302 features sets were explored, where each features sets has 100-1000 features extracted from the text. The results demonstrate Generalised Linear Model provides the best Accuracy score (0.92), Recall (0.902), Precision (0.902), F1 score (0.901) with standard deviation of accuracy of ±1,2%.}
}
@article{WANG202068,
title = {A deep neural network of multi-form alliances for personalized recommendations},
journal = {Information Sciences},
volume = {531},
pages = {68-86},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.062},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302425},
author = {Xuna Wang and Qingmei Tan and Lifan Zhang},
keywords = {Neural network, Deep learning, Alliances, Recommendation},
abstract = {The collaborative filtering adopted by traditional recommendation system has data sparsity problem, and the matrix decomposition method simply decomposes users and items into linear models for potential factors. These limitations have led to limited effectiveness for traditional recommendation algorithms. In this case, the recommendation system based on deep learning has emerged. Most of the current deep learning recommendations use deep neural networks to model some basic information, and in the modeling process, according to the input data categories, multiple mapping paths are used to map the original data to the potential vector space. However, these recommendations ignore that the alliance between different categories may have a potential impact on the recommendation effect. Aiming at this problem, this paper proposes a feedforward deep neural network of multi-form category features combination for recommendation, which is deep alliance neural network. According to the different alliance modes, it can be divided into deep series network (DSN), deep parallel network (DPN) and deep random network (DRN), which are used to solve the recommendation problem of implicit feedback. At the same time, a fusion model SMLP based on deep alliance neural network and traditional Multi-Layer Perceptron (MLP) is proposed to try to explore the performance of the fusion model. Finally, experiments on public datasets show that our proposed method significantly improves the existing methods. Empirical evidence indicates that deep series network and deep parallel network can provide better recommendation performance, while the recommended performance of deep random network and fusion model SMLP is not ideal. This indicates that the deep alliance network needs to pay special attention to the order of the category features in the process of category features association.}
}
@article{ZOU2021100269,
title = {FDN-learning: Urban PM2.5-concentration Spatial Correlation Prediction Model Based on Fusion Deep Neural Network},
journal = {Big Data Research},
volume = {26},
pages = {100269},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100269},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000861},
author = {Guojian Zou and Bo Zhang and Ruihan Yong and Dongming Qin and Qin Zhao},
keywords = {Fusion deep neural network, Gaussian function, LSTM, PM-concentration prediction, Stacked anti-autoencoder},
abstract = {The problem of increasing air pollution poses a challenge to smart city development, as spatial air pollution correlation exists among adjacent cities. However, it is difficult to predict the degree of air pollution of a location by exploiting massive air pollution datasets incorporating data on spatially related locations. Construction of a spatial correlation prediction model for air pollution is therefore required for air pollution big-data mining. In this paper, we propose an air pollution-concentration spatial correlation prediction model based on a fusion deep neural network called FDN-learning. Three models are combined: a stacked anti-autoencoder network, Gaussian function model, and long short-term memory network (LSTM). The FDN-learning model is composed of three layers for feature expansion, intermediate processing, and data prediction. In the first layer, we employ a stacked anti-autoencoder model to learn the source-data spatial features through a feature expansion hidden layer; this can enrich the feature vector and mine more information for further prediction. In the second layer, the Gaussian function evaluates effective weights for the outputs of the stacked anti-autoencoder models in the preceding layer; the spatial correction effects are therefore incorporated in this layer. Finally, the LSTM model in the data prediction layer learns the air pollution-concentration temporal features. A fine-tuning method based on stochastic gradient descent is applied to the FDN-learning model for improved performance. Empirical results are used to verify the feasibility and effectiveness of our proposed model based on a real-world air pollution dataset.}
}
@article{KOKSAL202147,
title = {Performance characterization of reinforcement learning-enabled evolutionary algorithms for integrated school bus routing and scheduling problem},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {2},
pages = {47-56},
year = {2021},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2021.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666307421000061},
author = {Eda Koksal and Abhishek R. Hegde and Haresh P. Pandiarajan and Bharadwaj Veeravalli},
keywords = {Reinforcement learning, Ant colony optimization, Genetic algorithm, Particle swarm optimization, School bus routing and scheduling, Combinatorial optimization},
abstract = {Bi-objective school bus scheduling optimization problem that is a subset of vehicle fleet scheduling problem is focused in this paper. In the literature, school bus routing and scheduling problem is proven to be an NP-Hard problem. The processed data supplied by our framework is utilized to search a near-optimum schedule with the aid of reinforcement learning by evolutionary algorithms. They are named as reinforcement learning-enabled genetic algorithm (RL-enabled GA), reinforcement learning-enabled particle swarm optimization algorithm (RL-enabled PSO), and reinforcement learning-enabled ant colony optimization algorithm (RL-enabled ACO). In this paper, the performance characterization of reinforcement learning-enabled evolutionary algorithms for integrated school bus routing and scheduling problem is investigated. The efficiency of the conventional algorithms is improved, and the near-optimal schedule is achieved significantly in a shorter duration with the active guidance of the reinforcement learning algorithm. We attempt to carry out extensive performance evaluation and conducted experiments on a geospatial dataset comprising road networks, trip trajectories of buses, and the address of students. The conventional and reinforcement learning integrated algorithms are improving the travel time of buses and the students. More than 50% saving by the conventional and the reinforcement learning-enabled ant colony optimization algorithm compared to the constructive heuristic algorithm is achieved from 92nd and 54th iterations, respectively. Similarly, the saving by the conventional and the reinforcement learning-enabled genetic algorithm is 41.34% at 500th iterations and more than 50% improvement from 281st iterations, respectively. Lastly, more than 10% saving by the conventional and the reinforcement learning-enabled particle swarm algorithm is achieved from 432nd and 28th iterations, respectively.}
}
@article{MA2020115350,
title = {Soft detection of 5-day BOD with sparse matrix in city harbor water using deep learning techniques},
journal = {Water Research},
volume = {170},
pages = {115350},
year = {2020},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2019.115350},
url = {https://www.sciencedirect.com/science/article/pii/S0043135419311248},
author = {Jun Ma and Yuexiong Ding and Jack C.P. Cheng and Feifeng Jiang and Zherui Xu},
keywords = {Biochemical oxygen demand, Deep matrix factorization, Deep neural network, Sparse matrix, Harbor water},
abstract = {To better control and manage harbor water quality is an important mission for coastal cities such as New York City (NYC). To achieve this, managers and governors need keep track of key quality indicators, such as temperature, pH, and dissolved oxygen. Among these, the Biochemical Oxygen Demand (BOD) over five days is a critical indicator that requires much time and effort to detect, causing great inconvenience in both academia and industry. Existing experimental and statistical methods cannot effectively solve the detection time problem or provide limited accuracy. Also, due to various human-made mistakes or facility issues, the data used for BOD detection and prediction contain many missing values, resulting in a sparse matrix. Few studies have addressed the sparse matrix problem while developing statistical detection methods. To address these gaps, we propose a deep learning based model that combines Deep Matrix Factorization (DMF) and Deep Neural Network (DNN). The model was able to solve the sparse matrix problem more intelligently and predict the BOD value more accurately. To test its effectiveness, we conducted a case study on the NYC harbor water, based on 32,323 water samples. The results showed that the proposed method achieved 11.54%–17.23% lower RMSE than conventional matrix completion methods, and 19.20%–25.16% lower RMSE than traditional machine learning algorithms.}
}
@article{AMANULLAH2020495,
title = {Deep learning and big data technologies for IoT security},
journal = {Computer Communications},
volume = {151},
pages = {495-517},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315361},
author = {Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran},
keywords = {Deep learning, Big data, IoT security},
abstract = {Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.}
}
@article{HAO2021243,
title = {Asymmetric cryptographic functions based on generative adversarial neural networks for Internet of Things},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {243-253},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001801},
author = {Xiaohan Hao and Wei Ren and Ruoting Xiong and Tianqing Zhu and Kim-Kwang Raymond Choo},
keywords = {Generative adversarial network, Digital signature, Asymmetric encryption, Neural networks, Internet of Things (IoT)},
abstract = {Increasingly, one should assume that the (digital) environment, e.g., Internet-of-Things (IoT) systems, we operate in is untrusted. In other words, this is a zero trust environment, in the sense that all devices and systems can be compromised and hence, untrusted. However, information sharing in a zero trust environment is more challenging, in comparison to an environment where we can rely on some trusted third-party. To address this challenge, we propose a blockchain-enabled zero trust information sharing protocol that is able to support the filtering of fabricated information and protect participant privacy during information sharing. We then prove the security of our protocol in the universally composable secure framework, and also evaluate its performance using a series of experiments. The evaluation results show that the average execution times of the three key steps in our protocol are 0.059 s, 0.060 s and 0.032 s, which demonstrates its potential for deployment in a real-world setting.}
}
@article{BALOGUN2021100989,
title = {A review of the inter-correlation of climate change, air pollution and urban sustainability using novel machine learning algorithms and spatial information science},
journal = {Urban Climate},
volume = {40},
pages = {100989},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100989},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521002194},
author = {Abdul-Lateef Balogun and Abdulwaheed Tella and Lavania Baloo and Naheem Adebisi},
keywords = {Air pollution, Climate change, Digitalization, Sustainable cities, Machine learning, Spatial data science},
abstract = {Air pollution is a global geo-hazard with significant implications, including deterioration of health and premature death. Climatic variables such as temperature, rainfall, wind, and humidity impact air pollution by affecting the strength, transportation, and dispersion of pollutants in the atmosphere. Emerging data science tools, particularly Machine Learning (ML) big data analytics, are being utilized to predict air pollution intensity and frequency under varying climatic conditions for effective mitigation plans. However, comprehensive documentation of these digitalization approaches and outcomes in terms of correlating future air pollution with climate change remains scant. This study addresses this gap by systematically reviewing pertinent literature on climate change and air pollution studies. We also investigated the potentials of integrated spatial data science for spatial modelling and identifying cities vulnerable to air pollution hazards. Our findings show that climatic factors and seasonal variations are critical predictors of air quality in urban areas. A strong correlation exists between climate change and air quality, and air quality in urbanized regions is projected to deteriorate with climate change in the future. Therefore, climatic variables remain essential factors for the prediction of air quality. Also, air pollutants tend to have higher concentration in the warm season, making the consideration of seasonal changes crucial in air quality management. The study also revealed that machine learning algorithms such as random forest, gradient boosting machine, and classification and regression trees (CART) accurately predict air pollution hazard when integrated with spatial models. The detailed review of literature undertaken in this study provides a strong basis for the conclusion that the integration of spatial techniques and machine learning has the potential to improve air pollution prediction outcome and aid appropriate intervention initiatives by the stakeholders. Thus, emerging geospatial intelligence technologies and digital innovations particularly Artificial intelligence, machine learning and big data analytics that underpin the fourth industrial revolution (IR 4.0) can enhance existing early warning mechanisms and support a prompt and effective response to climate-change-induced air pollution, thereby fostering sustainable cities and societies.}
}
@article{AFAQ2021102667,
title = {Machine learning for 5G security: Architecture, recent advances, and challenges},
journal = {Ad Hoc Networks},
volume = {123},
pages = {102667},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102667},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521001785},
author = {Amir Afaq and Noman Haider and Muhammad Zeeshan Baig and Komal S. Khan and Muhammad Imran and Imran Razzak},
keywords = {5G network security, Threat landscape, Vulnerabilities, Threat intelligence, Machine learning, Federated learning, Threat classification},
abstract = {The granularization of crucial network functions implementation using software-centric, and virtualized approaches in 5G networks have brought forth unprecedented security challenges in general and privacy concerns. Moreover, these software components’ premature deployment and compromised supply chain put the individual network components at risk and have a ripple effect for the rest of the network. Some of the novel threats to 5G assets include tampering in identity and access management, supply-chain poisoning, masquerade and bot attacks, loop-holes in source codes. Machine learning (ML) in this context can help to provide heavily dynamic and robust security mechanisms for the software-centric architecture of 5G Networks. ML models’ development and implementation also rely on programmable environments; hence, they can play a vital role in designing, modelling, and automating efficient security protocols. This article presents the threat landscape across 5G networks and discusses the feasibility and architecture of different ML-based models to counter these threats. Also, we present the architecture for automated threat intelligence using cooperative and coordinated ML to secure 5G assets and infrastructure. We also present the summary of closely related existing works along with future research challenges.}
}
@article{HOU2021114116,
title = {Physical security of deep learning on edge devices: Comprehensive evaluation of fault injection attack vectors},
journal = {Microelectronics Reliability},
volume = {120},
pages = {114116},
year = {2021},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2021.114116},
url = {https://www.sciencedirect.com/science/article/pii/S0026271421000822},
author = {Xiaolu Hou and Jakub Breier and Dirmanto Jap and Lei Ma and Shivam Bhasin and Yang Liu},
keywords = {Fault attack, Neural network, AI security},
abstract = {Decision making tasks carried out by the usage of deep neural networks are successfully taking over in many areas, including those that are security critical, such as healthcare, transportation, smart grids, where intentional and unintentional failures can be disastrous. Edge computing systems are becoming ubiquitous nowadays, often serving deep learning tasks that do not need to be sent over to servers. Therefore, there is a necessity to evaluate the potential attacks that can target deep learning in the edge. In this work, we present evaluation of deep neural networks (DNNs) reliability against fault injection attacks. We first experimentally evaluate DNNs implemented in an embedded device by using laser fault injection to get the insight on possible attack vectors. We show practical results on four activation functions, ReLu, softmax, sigmoid, and tanh. We then perform a deep study on DNNs based on derived fault models by using several different attack strategies based on random faults. We also investigate a powerful attacker who can find effective fault location based on genetic algorithm, to show the most efficient attacks in terms of misclassification success rates. Finally, we show how a state of the art countermeasure against model extraction attack can be bypassed with a fault attack. Our results can serve as a basis to outline the susceptibility of DNNs to physical attacks which can be considered a viable attack vector whenever a device is deployed in hostile environment.}
}
@article{ZHOU2021104007,
title = {Intelligent diagnosis of Alzheimer's disease based on internet of things monitoring system and deep learning classification method},
journal = {Microprocessors and Microsystems},
volume = {83},
pages = {104007},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104007},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121001800},
author = {Yuxin Zhou and Yinan Lu and Zhili Pei},
keywords = {Alzheimer's disease (ad), Internet of things system, Deep learning, Human-Computer Interaction, Diagnosis Methods},
abstract = {Alzheimer's disease is a syndrome with a decreased ability to a different classification. Alzheimer's disease (AD), memory, and learning affect the classification domain as cognitive-motor or executive function, which is the most common dementia. Death and expensive detection is high treatment and between the result of the count rate of patient care. Early detection of AD is considered very important to improve patients' quality of life and their families. The purpose of this article is to reduce the cost of accelerating related to the diagnosis. The o allow complete access is possible to introduce a new non-invasive early diagnosis method. The new AD screening tests' interaction is based on introducing new technology in the business system, and an immersive and advanced human-computer has been introduced to the Internet system. Four experiments, based on the large-scale screening, damage to the mechanism of classification domain, shows that can be used in the network design of the thing system. In the proposed test, mainly common goal, focusing on evaluating the recent related memory loss in the conversation and events, human and between the virtual world and the real machine; abnormal the ability to recognize, express, and understand diagnostic differences in language problems terms. IOT (IOT) has integrated hundreds of millions of smart devices that can communicate with a minimum of human intervention. It should be strengthened to secure methods to ensure the ecosystem of useful IOT. Machine Learning (ML) is proceeding significantly over the past few years; in some critical applications, machine intelligence is actually of the machine to the laboratory curiosity conversion. The ability to monitor the IOT devices and intelligence will provide an essential solution for new or zero-day attacks. Components and apparatus of the ML Hano IOT are, based on whether learning the IOT in the environment is an effective way to explore the "normal," "abnormal" behavior data.}
}
@article{JATAIN2021,
title = {A contemplative perspective on federated machine learning: Taxonomy, threats & vulnerability assessment and challenges},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001312},
author = {Divya Jatain and Vikram Singh and Naveen Dahiya},
keywords = {Federated Learning, Security Concerns, Language Modelling, Fog Computing, Healthcare Informatics, Vulnerability Assessment},
abstract = {Today, the rapid growth of the internet and advancements in mobile technology and increased internet connectivity have brought us to a data-driven economy where an enormous amount of data is being used to train machine learning models to make strategic decisions. However, in the aftermath of a data breach by Facebook in 2018, there are some serious concerns over user data privacy and security being used to train the Machine Learning models. In this context, a new approach, Federated Machine Learning is now one of the most talked-about and recent approaches. Current research primarily focuses on Federated Learning's advantages over the traditional methods and/or its classification. However, being in a nascent stage of development as a method, certain challenges need to be addressed. This paper intends to address the totality of federated learning with a complete vulnerability assessment. During the study of the literature, it is found that security being promised as one of the key advantages of federated learning can still not be guaranteed because of some issues inherently present, and this can lead to poisoning, inference attacks and insertion of backdoors, etc. This paper intends to provide a complete picture by giving an in-depth and comprehensive analysis of Federated Learning and its taxonomy. It also provides a detailed vulnerability assessment and highlights the challenges faced in the current setting and future research directions to make federated learning a more functional, robust and secure method to train machine learning models.}
}
@article{DUBEY20201950,
title = {Household Waste Management System Using IoT and Machine Learning},
journal = {Procedia Computer Science},
volume = {167},
pages = {1950-1959},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.222},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306876},
author = {Sonali Dubey and Pushpa Singh and Piyush Yadav and Krishna Kant Singh},
keywords = {IoT, smart city, waste management, biodegradable, sensor, machine learning},
abstract = {IOT and machine learning based household waste management system for Green smart society are aimed to make management of waste from the every apartment of the society more efficient using the most upcoming technology IOT. This paper discusses the collection and decomposition of waste in the smart way so that benefit from the waste is maximized and the actual waste is minimized efficiently. This paper focus on the segregation of the waste at two levels: the first level of segregation is on the individual house of the society and the second level of segregation is at the society. Author, discuss the recycling of the biodegradable waste for making compost. The machine learning technique such as KNN is used to generate an alert message for various combinations of three sensor values like level of bio and non biodegradable waste,concentration of poisonous gas. The overall impact of this research is in the upliftment of the green technologies by reducing pollutants, conserving, resourcing and reusing the energy through the use of technology.}
}
@article{GE2021107784,
title = {Towards a deep learning-driven intrusion detection approach for Internet of Things},
journal = {Computer Networks},
volume = {186},
pages = {107784},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107784},
url = {https://www.sciencedirect.com/science/article/pii/S138912862031358X},
author = {Mengmeng Ge and Naeem Firdous Syed and Xiping Fu and Zubair Baig and Antonio Robles-Kelly},
keywords = {Intrusion detection, Internet of Things, Deep learning},
abstract = {Internet of Things (IoT) as a paradigm comes with a range of benefits to humanity. Domains of research for the IoT range from healthcare automation to energy and transport. However, due to their limited resources, IoT devices are vulnerable to various types of cyber attacks as carried out by the adversary. In this paper, we propose a novel intrusion detection approach for the IoT, through the adoption of a customised deep learning technique. We utilise a cutting-edge IoT dataset comprising IoT traces and realistic attack traffic, including denial of service, distributed denial of service, data gathering and data theft attacks. A feed-forward neural networks model with embedding layers (to encode high-dimensional categorical features) for multi-class classification, is developed. The concept of transfer learning is subsequently applied to encode high-dimensional categorical features to build a binary classifier based on a second feed-forward neural networks model. We obtain results through the evaluation of the proposed approach which demonstrate a high classification accuracy for both classifiers, namely, binary and multi-class.}
}