
@Article{rs13112060,
AUTHOR = {Matongera, Trylee Nyasha and Mutanga, Onisimo and Sibanda, Mbulisi and Odindi, John},
TITLE = {Estimating and Monitoring Land Surface Phenology in Rangelands: A Review of Progress and Challenges},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2060},
URL = {https://www.mdpi.com/2072-4292/13/11/2060},
ISSN = {2072-4292},
ABSTRACT = {Land surface phenology (LSP) has been extensively explored from global archives of satellite observations to track and monitor the seasonality of rangeland ecosystems in response to climate change. Long term monitoring of LSP provides large potential for the evaluation of interactions and feedbacks between climate and vegetation. With a special focus on the rangeland ecosystems, the paper reviews the progress, challenges and emerging opportunities in LSP while identifying possible gaps that could be explored in future. Specifically, the paper traces the evolution of satellite sensors and interrogates their properties as well as the associated indices and algorithms in estimating and monitoring LSP in productive rangelands. Findings from the literature revealed that the spectral characteristics of the early satellite sensors such as Landsat, AVHRR and MODIS played a critical role in the development of spectral vegetation indices that have been widely used in LSP applications. The normalized difference vegetation index (NDVI) pioneered LSP investigations, and most other spectral vegetation indices were primarily developed to address the weaknesses and shortcomings of the NDVI. New indices continue to be developed based on recent sensors such as Sentinel-2 that are characterized by unique spectral signatures and fine spatial resolutions, and their successful usage is catalyzed with the development of cutting-edge algorithms for modeling the LSP profiles. In this regard, the paper has documented several LSP algorithms that are designed to provide data smoothing, gap filling and LSP metrics retrieval methods in a single environment. In the future, the development of machine learning algorithms that can effectively model and characterize the phenological cycles of vegetation would help to unlock the value of LSP information in the rangeland monitoring and management process. Precisely, deep learning presents an opportunity to further develop robust software packages such as the decomposition and analysis of time series (DATimeS) with the abundance of data processing tools and techniques that can be used to better characterize the phenological cycles of vegetation in rangeland ecosystems.},
DOI = {10.3390/rs13112060}
}



@Article{s21113647,
AUTHOR = {Wu, Zhangnan and Chen, Yajun and Zhao, Bo and Kang, Xiaobing and Ding, Yuanyuan},
TITLE = {Review of Weed Detection Methods Based on Computer Vision},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3647},
URL = {https://www.mdpi.com/1424-8220/21/11/3647},
PubMedID = {34073867},
ISSN = {1424-8220},
ABSTRACT = {Weeds are one of the most important factors affecting agricultural production. The waste and pollution of farmland ecological environment caused by full-coverage chemical herbicide spraying are becoming increasingly evident. With the continuous improvement in the agricultural production level, accurately distinguishing crops from weeds and achieving precise spraying only for weeds are important. However, precise spraying depends on accurately identifying and locating weeds and crops. In recent years, some scholars have used various computer vision methods to achieve this purpose. This review elaborates the two aspects of using traditional image-processing methods and deep learning-based methods to solve weed detection problems. It provides an overview of various methods for weed detection in recent years, analyzes the advantages and disadvantages of existing methods, and introduces several related plant leaves, weed datasets, and weeding machinery. Lastly, the problems and difficulties of the existing weed detection methods are analyzed, and the development trend of future research is prospected.},
DOI = {10.3390/s21113647}
}



@Article{drones5020043,
AUTHOR = {Moreira, Bruno Miguez and Goyanes, Gabriel and Pina, Pedro and Vassilev, Oleg and Heleno, Sandra},
TITLE = {Assessment of the Influence of Survey Design and Processing Choices on the Accuracy of Tree Diameter at Breast Height (DBH) Measurements Using UAV-Based Photogrammetry},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {43},
URL = {https://www.mdpi.com/2504-446X/5/2/43},
ISSN = {2504-446X},
ABSTRACT = {This work provides a systematic evaluation of how survey design and computer processing choices (such as the software used or the workflow/parameters chosen) influence unmanned aerial vehicle (UAV)-based photogrammetry retrieval of tree diameter at breast height (DBH), an important 3D structural parameter in forest inventory and biomass estimation. The study areas were an agricultural field located in the province of Málaga, Spain, where a small group of olive trees was chosen for the UAV surveys, and an open woodland area in the outskirts of Sofia, the capital of Bulgaria, where a 10 ha area grove, composed mainly of birch trees, was overflown. A DJI Phantom 4 Pro quadcopter UAV was used for the image acquisition. We applied structure from motion (SfM) to generate 3D point clouds of individual trees, using Agisoft and Pix4D software packages. The estimation of DBH in the point clouds was made using a RANSAC-based circle fitting tool from the TreeLS R package. All trees modeled had their DBH tape-measured on the ground for accuracy assessment. In the first study site, we executed many diversely designed flights, to identify which parameters (flying altitude, camera tilt, and processing method) gave us the most accurate DBH estimations; then, the resulting best settings configuration was used to assess the replicability of the method in the forested area in Bulgaria. The best configuration tested (flight altitudes of about 25 m above tree canopies, camera tilt 60°, forward and side overlaps of 90%, Agisoft ultrahigh processing) resulted in root mean square errors (RMSEs; %) of below 5% of the tree diameters in the first site and below 12.5% in the forested area. We demonstrate that, when carefully designed methodologies are used, SfM can measure the DBH of single trees with very good accuracy, and to our knowledge, the results presented here are the best achieved so far using (above-canopy) UAV-based photogrammetry.},
DOI = {10.3390/drones5020043}
}



@Article{su13115941,
AUTHOR = {Khahro, Shabir Hussain and Memon, Zubair Ahmed and Gungat, Lillian and Yazid, Muhamad Razuhanafi Mat and Rahim, Abdur and Mubaraki, Muhammad and Md. Yusoff, Nur Izzi},
TITLE = {Low-Cost Pavement Management System for Developing Countries},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5941},
URL = {https://www.mdpi.com/2071-1050/13/11/5941},
ISSN = {2071-1050},
ABSTRACT = {Governments face numerous challenges in sustaining road network conditions. This is attributed to road authorities’ shortages of financial and physical infrastructure. As a result, low-cost automated solutions are being pursued to solve these problems and provide people with appropriate road conditions. Several attempts have been made to improve these technologies and incorporate them into a Pavement Management System (PMS) but limited attempts are made for developing countries. This study aimed to design a low-cost pavement management system for flexible pavement maintenance. A detailed literature review has been carried out, followed by a qualitative assessment of the various indicators considered for PMS. The priority ranks of the PMS indicators were made using an Analytical Network Process (ANP) and each rank was validated by a sensitivity assessment test using the Super Decision-Making tool. This paper also provides the conceptual framework for the low-cost PMS, followed by a fishbone diagram of the indicators and sub-indicators. It is concluded that an emergency maintenance plan with an ANP weight of (0.41) is one of the most significant plans for a low-cost PMS, followed by a routine with an ANP weight of (0.39) and periodic maintenance plans with a (0.20) ANP weight. Moreover, the functional indicators with an ANP weight of (0.32) are the most significant indicators for a low-cost PMS, followed by structural (0.26), safety (0.24), and serviceability(0.18) indicators. This model will assist the road planners in making better decisions on pavement maintenance management plans. The model will suggest the pavement sections on a higher priority to be added in the maintenance plans, especially where the maintenance budget is limited.},
DOI = {10.3390/su13115941}
}



@Article{rs13112077,
AUTHOR = {Fetai, Bujar and Račič, Matej and Lisec, Anka},
TITLE = {Deep Learning for Detection of Visible Land Boundaries from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2077},
URL = {https://www.mdpi.com/2072-4292/13/11/2077},
ISSN = {2072-4292},
ABSTRACT = {Current efforts aim to accelerate cadastral mapping through innovative and automated approaches and can be used to both create and update cadastral maps. This research aims to automate the detection of visible land boundaries from unmanned aerial vehicle (UAV) imagery using deep learning. In addition, we wanted to evaluate the advantages and disadvantages of programming-based deep learning compared to commercial software-based deep learning. For the first case, we used the convolutional neural network U-Net, implemented in Keras, written in Python using the TensorFlow library. For commercial software-based deep learning, we used ENVINet5. UAV imageries from different areas were used to train the U-Net model, which was performed in Google Collaboratory and tested in the study area in Odranci, Slovenia. The results were compared with the results of ENVINet5 using the same datasets. The results showed that both models achieved an overall accuracy of over 95%. The high accuracy is due to the problem of unbalanced classes, which is usually present in boundary detection tasks. U-Net provided a recall of 0.35 and a precision of 0.68 when the threshold was set to 0.5. A threshold can be viewed as a tool for filtering predicted boundary maps and balancing recall and precision. For equitable comparison with ENVINet5, the threshold was increased. U-Net provided more balanced results, a recall of 0.65 and a precision of 0.41, compared to ENVINet5 recall of 0.84 and a precision of 0.35. Programming-based deep learning provides a more flexible yet complex approach to boundary mapping than software-based, which is rigid and does not require programming. The predicted visible land boundaries can be used both to speed up the creation of cadastral maps and to automate the revision of existing cadastral maps and define areas where updates are needed. The predicted boundaries cannot be considered final at this stage but can be used as preliminary cadastral boundaries.},
DOI = {10.3390/rs13112077}
}



@Article{drones5020045,
AUTHOR = {Dronova, Iryna and Kislik, Chippie and Dinh, Zack and Kelly, Maggi},
TITLE = {A Review of Unoccupied Aerial Vehicle Use in Wetland Applications: Emerging Opportunities in Approach, Technology, and Data},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {45},
URL = {https://www.mdpi.com/2504-446X/5/2/45},
ISSN = {2504-446X},
ABSTRACT = {Recent developments in technology and data processing for Unoccupied Aerial Vehicles (UAVs) have revolutionized the scope of ecosystem monitoring, providing novel pathways to fill the critical gap between limited-scope field surveys and limited-customization satellite and piloted aerial platforms. These advances are especially ground-breaking for supporting management, restoration, and conservation of landscapes with limited field access and vulnerable ecological systems, particularly wetlands. This study presents a scoping review of the current status and emerging opportunities in wetland UAV applications, with particular emphasis on ecosystem management goals and remaining research, technology, and data needs to even better support these goals in the future. Using 122 case studies from 29 countries, we discuss which wetland monitoring and management objectives are most served by this rapidly developing technology, and what workflows were employed to analyze these data. This review showcases many ways in which UAVs may help reduce or replace logistically demanding field surveys and can help improve the efficiency of UAV-based workflows to support longer-term monitoring in the face of wetland environmental challenges and management constraints. We also highlight several emerging trends in applications, technology, and data and offer insights into future needs.},
DOI = {10.3390/drones5020045}
}



@Article{land10060557,
AUTHOR = {Koeva, Mila and Humayun, Mohammed Imaduddin and Timm, Christian and Stöcker, Claudia and Crommelinck, Sophie and Chipofya, Malumbo and Bennett, Rohan and Zevenbergen, Jaap},
TITLE = {Geospatial Tool and Geocloud Platform Innovations: A Fit-for-Purpose Land Administration Assessment},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {557},
URL = {https://www.mdpi.com/2073-445X/10/6/557},
ISSN = {2073-445X},
ABSTRACT = {The well-recognized and extensive task of mapping unrecorded land rights across sub-Saharan Africa demands innovative solutions. In response, the consortia of “its4land”, a European Commission Horizon 2020 project, developed, adapted, and tested innovative geospatial tools including (1) software underpinned by the smart Sketch maps concept, called SmartSkeMa; (2) a workflow for applying unmanned aerial vehicles (UAV); and (3) a boundary delineator tool based on the UAV images. Additionally, the consortium developed (4) a platform called Publish and Share (PaS), enabling integration of all the outputs of tool sharing and publishing of land information through geocloud web services. The individual tools were developed, tested, and demonstrated based on requirements from Rwanda, Kenya, Ethiopia, and Zanzibar. The platform was further tested by key informants and experts in a workshop in Rwanda after the AfricaGIS conference in 2019. With the project concluding in 2020, this paper seeks to undertake an assessment of the tools and the PaS platform against the elements of fit-for-purpose land administration. The results show that while the tools can function and deliver outputs independently and reliably, PaS enables interoperability by allowing them to be combined and integrated into land administration workflows. This feature is useful for tailoring approaches for specific country contexts. In this regard, developers of technical approaches tackling land administration issues are further encouraged to include interoperability and the use of recognized standards in designs.},
DOI = {10.3390/land10060557}
}



@Article{agronomy11061069,
AUTHOR = {Ahmed, Shibbir and Qiu, Baijing and Ahmad, Fiaz and Kong, Chun-Wei and Xin, Huang},
TITLE = {A State-of-the-Art Analysis of Obstacle Avoidance Methods from the Perspective of an Agricultural Sprayer UAV’s Operation Scenario},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1069},
URL = {https://www.mdpi.com/2073-4395/11/6/1069},
ISSN = {2073-4395},
ABSTRACT = {Over the last decade, Unmanned Aerial Vehicles (UAVs), also known as drones, have been broadly utilized in various agricultural fields, such as crop management, crop monitoring, seed sowing, and pesticide spraying. Nonetheless, autonomy is still a crucial limitation faced by the Internet of Things (IoT) UAV systems, especially when used as sprayer UAVs, where data needs to be captured and preprocessed for robust real-time obstacle detection and collision avoidance. Moreover, because of the objective and operational difference between general UAVs and sprayer UAVs, not every obstacle detection and collision avoidance method will be sufficient for sprayer UAVs. In this regard, this article seeks to review the most relevant developments on all correlated branches of the obstacle avoidance scenarios for agricultural sprayer UAVs, including a UAV sprayer’s structural details. Furthermore, the most relevant open challenges for current UAV sprayer solutions are enumerated, thus paving the way for future researchers to define a roadmap for devising new-generation, affordable autonomous sprayer UAV solutions. Agricultural UAV sprayers require data-intensive algorithms for the processing of the images acquired, and expertise in the field of autonomous flight is usually needed. The present study concludes that UAV sprayers are still facing obstacle detection challenges due to their dynamic operating and loading conditions.},
DOI = {10.3390/agronomy11061069}
}



@Article{s21113698,
AUTHOR = {Kawai, Takaaki},
TITLE = {Video Slice: Image Compression and Transmission for Agricultural Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3698},
URL = {https://www.mdpi.com/1424-8220/21/11/3698},
PubMedID = {34073404},
ISSN = {1424-8220},
ABSTRACT = {When agricultural automation systems are required to send cultivation field images to the cloud for field monitoring, pay-as-you-go mobile communication leads to high operation costs. To minimize cost, one can exploit a characteristic of cultivation field images wherein the landscape does not change considerably besides the appearance of the plants. Therefore, this paper presents a method that transmits only the difference data between the past and current images to minimize the amount of transmitted data. This method is easy to implement because the difference data are generated using an existing video encoder. Further, the difference data are generated based on an image at a specific time instead of the images at adjacent times, and thus the subsequent images can be reproduced even if the previous difference data are lost because of unstable mobile communication. A prototype of the proposed method was implemented with a MPEG-4 Visual video encoder. The amount of transmitted and received data on the medium access control layer was decreased to approximately 1/4 of that when using the secure copy protocol. The transmission time for one image was 5.6 s; thus, the proposed method achieved a reasonable processing time and a reduction of transmitted data.},
DOI = {10.3390/s21113698}
}



@Article{rs13112088,
AUTHOR = {Quemada, Carlos and Pérez-Escudero, José M. and Gonzalo, Ramón and Ederra, Iñigo and Santesteban, Luis G. and Torres, Nazareth and Iriarte, Juan Carlos},
TITLE = {Remote Sensing for Plant Water Content Monitoring: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2088},
URL = {https://www.mdpi.com/2072-4292/13/11/2088},
ISSN = {2072-4292},
ABSTRACT = {This paper reviews the different remote sensing techniques found in the literature to monitor plant water status, allowing farmers to control the irrigation management and to avoid unnecessary periods of water shortage and a needless waste of valuable water. The scope of this paper covers a broad range of 77 references published between the years 1981 and 2021 and collected from different search web sites, especially Scopus. Among them, 74 references are research papers and the remaining three are review papers. The different collected approaches have been categorized according to the part of the plant subjected to measurement, that is, soil (12.2%), canopy (33.8%), leaves (35.1%) or trunk (18.9%). In addition to a brief summary of each study, the main monitoring technologies have been analyzed in this review. Concerning the presentation of the data, different results have been obtained. According to the year of publication, the number of published papers has increased exponentially over time, mainly due to the technological development over the last decades. The most common sensor is the radiometer, which is employed in 15 papers (20.3%), followed by continuous-wave (CW) spectroscopy (12.2%), camera (10.8%) and THz time-domain spectroscopy (TDS) (10.8%). Excluding two studies, the minimum coefficient of determination (R2) obtained in the references of this review is 0.64. This indicates the high degree of correlation between the estimated and measured data for the different technologies and monitoring methods. The five most frequent water indicators of this study are: normalized difference vegetation index (NDVI) (12.2%), backscattering coefficients (10.8%), spectral reflectance (8.1%), reflection coefficient (8.1%) and dielectric constant (8.1%).},
DOI = {10.3390/rs13112088}
}



@Article{s21113703,
AUTHOR = {Cheng, Dongyang and Zhao, Dangjun and Zhang, Junchao and Wei, Caisheng and Tian, Di},
TITLE = {PCA-Based Denoising Algorithm for Outdoor Lidar Point Cloud Data},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3703},
URL = {https://www.mdpi.com/1424-8220/21/11/3703},
PubMedID = {34073498},
ISSN = {1424-8220},
ABSTRACT = {Due to the complexity of surrounding environments, lidar point cloud data (PCD) are often degraded by plane noise. In order to eliminate noise, this paper proposes a filtering scheme based on the grid principal component analysis (PCA) technique and the ground splicing method. The 3D PCD is first projected onto a desired 2D plane, within which the ground and wall data are well separated from the PCD via a prescribed index based on the statistics of points in all 2D mesh grids. Then, a KD-tree is constructed for the ground data, and rough segmentation in an unsupervised method is conducted to obtain the true ground data by using the normal vector as a distinctive feature. To improve the performance of noise removal, we propose an elaborate K nearest neighbor (KNN)-based segmentation method via an optimization strategy. Finally, the denoised data of the wall and ground are spliced for further 3D reconstruction. The experimental results show that the proposed method is efficient at noise removal and is superior to several traditional methods in terms of both denoising performance and run speed.},
DOI = {10.3390/s21113703}
}



@Article{app11114920,
AUTHOR = {Reis, João and Cohen, Yuval and Melão, Nuno and Costa, Joana and Jorge, Diana},
TITLE = {High-Tech Defense Industries: Developing Autonomous Intelligent Systems},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {4920},
URL = {https://www.mdpi.com/2076-3417/11/11/4920},
ISSN = {2076-3417},
ABSTRACT = {After the Cold War, the defense industries found themselves at a crossroads. However, it seems that they are gaining new momentum, as new technologies such as robotics and artificial intelligence are enabling the development of autonomous, highly innovative and disruptive intelligent systems. Despite this new impetus, there are still doubts about where to invest limited financial resources to boost high-tech defense industries. In order to shed some light on the topic, we decided to conduct a systematic literature review by using the PRISMA protocol and content analysis. The results indicate that autonomous intelligent systems are being developed by the defense industry and categorized into three different modes—fully autonomous operations, partially autonomous operations, and smart autonomous decision-making. In addition, it is also important to note that, at a strategic level of war, there is limited room for automation given the need for human intervention. However, at the tactical level of war, there is a high probability of growth in industrial defense, since, at this level, structured decisions and complex analytical-cognitive tasks are carried out. In the light of carrying out those decisions and tasks, robotics and artificial intelligence can make a contribution far superior to that of human beings.},
DOI = {10.3390/app11114920}
}



@Article{f12060692,
AUTHOR = {Choudhury, MD Abdul Mueed and Marcheggiani, Ernesto and Galli, Andrea and Modica, Giuseppe and Somers, Ben},
TITLE = {Mapping the Urban Atmospheric Carbon Stock by LiDAR and WorldView-3 Data},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {692},
URL = {https://www.mdpi.com/1999-4907/12/6/692},
ISSN = {1999-4907},
ABSTRACT = {Currently, the worsening impacts of urbanizations have been impelled to the importance of monitoring and management of existing urban trees, securing sustainable use of the available green spaces. Urban tree species identification and evaluation of their roles in atmospheric Carbon Stock (CS) are still among the prime concerns for city planners regarding initiating a convenient and easily adaptive urban green planning and management system. A detailed methodology on the urban tree carbon stock calibration and mapping was conducted in the urban area of Brussels, Belgium. A comparative analysis of the mapping outcomes was assessed to define the convenience and efficiency of two different remote sensing data sources, Light Detection and Ranging (LiDAR) and WorldView-3 (WV-3), in a unique urban area. The mapping results were validated against field estimated carbon stocks. At the initial stage, dominant tree species were identified and classified using the high-resolution WorldView3 image, leading to the final carbon stock mapping based on the dominant species. An object-based image analysis approach was employed to attain an overall accuracy (OA) of 71% during the classification of the dominant species. The field estimations of carbon stock for each plot were done utilizing an allometric model based on the field tree dendrometric data. Later based on the correlation among the field data and the variables (i.e., Normalized Difference Vegetation Index, NDVI and Crown Height Model, CHM) extracted from the available remote sensing data, the carbon stock mapping and validation had been done in a GIS environment. The calibrated NDVI and CHM had been used to compute possible carbon stock in either case of the WV-3 image and LiDAR data, respectively. A comparative discussion has been introduced to bring out the issues, especially for the developing countries, where WV-3 data could be a better solution over the hardly available LiDAR data. This study could assist city planners in understanding and deciding the applicability of remote sensing data sources based on their availability and the level of expediency, ensuring a sustainable urban green management system.},
DOI = {10.3390/f12060692}
}



@Article{rs13112123,
AUTHOR = {Aeberli, Aaron and Johansen, Kasper and Robson, Andrew and Lamb, David W. and Phinn, Stuart},
TITLE = {Detection of Banana Plants Using Multi-Temporal Multispectral UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2123},
URL = {https://www.mdpi.com/2072-4292/13/11/2123},
ISSN = {2072-4292},
ABSTRACT = {Unoccupied aerial vehicles (UAVs) have become increasingly commonplace in aiding planning and management decisions in agricultural and horticultural crop production. The ability of UAV-based sensing technologies to provide high spatial (&lt;1 m) and temporal (on-demand) resolution data facilitates monitoring of individual plants over time and can provide essential information about health, yield, and growth in a timely and quantifiable manner. Such applications would be beneficial for cropped banana plants due to their distinctive growth characteristics. Limited studies have employed UAV data for mapping banana crops and to our knowledge only one other investigation features multi-temporal detection of banana crowns. The purpose of this study was to determine the suitability of multiple-date UAV-captured multi-spectral data for the automated detection of individual plants using convolutional neural network (CNN), template matching (TM), and local maximum filter (LMF) methods in a geographic object-based image analysis (GEOBIA) software framework coupled with basic classification refinement. The results indicate that CNN returns the highest plant detection accuracies, with the developed rule set and model providing greater transferability between dates (F-score ranging between 0.93 and 0.85) than TM (0.86–0.74) and LMF (0.86–0.73) approaches. The findings provide a foundation for UAV-based individual banana plant counting and crop monitoring, which may be used for precision agricultural applications to monitor health, estimate yield, and to inform on fertilizer, pesticide, and other input requirements for optimized farm management.},
DOI = {10.3390/rs13112123}
}



@Article{rs13112126,
AUTHOR = {Wang, Yuliang and Li, Mingshi},
TITLE = {Annually Urban Fractional Vegetation Cover Dynamic Mapping in Hefei, China (1999–2018)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2126},
URL = {https://www.mdpi.com/2072-4292/13/11/2126},
ISSN = {2072-4292},
ABSTRACT = {Vegetation measures are crucial for assessing changes in the ecological environment. Fractional vegetation cover (FVC) provides information on the growth status, distribution characteristics, and structural changes of vegetation. An in-depth understanding of the dynamic changes in urban FVC contributes to the sustainable development of ecological civilization in the urbanization process. However, dynamic change detection of urban FVC using multi-temporal remote sensing images is a complex process and challenge. This paper proposed an improved FVC estimation model by fusing the optimized dynamic range vegetation index (ODRVI) model. The ODRVI model improved sensitivity to the water content, roughness degree, and soil type by minimizing the influence of bare soil in areas of sparse vegetation cover. The ODRVI model enhanced the stability of FVC estimation in the near-infrared (NIR) band in areas of dense and sparse vegetation cover through introducing the vegetation canopy vertical porosity (VCVP) model. The verification results confirmed that the proposed model had better performance than typical vegetation index (VI) models for multi-temporal Landsat images. The coefficient of determination (R2) between the ODRVI model and the FVC was 0.9572, which was 7.4% higher than the average R2 of other typical VI models. Moreover, the annual urban FVC dynamics were mapped using the proposed improved FVC estimation model in Hefei, China (1999–2018). The total area of all grades FVC decreased by 33.08% during the past 20 years in Hefei, China. The areas of the extremely low, low, and medium grades FVC exhibited apparent inter-annual fluctuations. The maximum standard deviation of the area change of the medium grade FVC was 13.35%. For other grades of FVC, the order of standard deviation of the change ratio was extremely low FVC &gt; low FVC &gt; medium-high FVC &gt; high FVC. The dynamic mapping of FVC revealed the influence intensity and direction of the urban sprawl on vegetation coverage, which contributes to the strategic development of sustainable urban management plans.},
DOI = {10.3390/rs13112126}
}



@Article{rs13112139,
AUTHOR = {de Castro, Ana I. and Shi, Yeyin and Maja, Joe Mari and Peña, Jose M.},
TITLE = {UAVs for Vegetation Monitoring: Overview and Recent Scientific Contributions},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2139},
URL = {https://www.mdpi.com/2072-4292/13/11/2139},
ISSN = {2072-4292},
ABSTRACT = {This paper reviewed a set of twenty-one original and innovative papers included in a special issue on UAVs for vegetation monitoring, which proposed new methods and techniques applied to diverse agricultural and forestry scenarios. Three general categories were considered: (1) sensors and vegetation indices used, (2) technological goals pursued, and (3) agroforestry applications. Some investigations focused on issues related to UAV flight operations, spatial resolution requirements, and computation and data analytics, while others studied the ability of UAVs for characterizing relevant vegetation features (mainly canopy cover and crop height) or for detecting different plant/crop stressors, such as nutrient content/deficiencies, water needs, weeds, and diseases. The general goal was proposing UAV-based technological solutions for a better use of agricultural and forestry resources and more efficient production with relevant economic and environmental benefits.},
DOI = {10.3390/rs13112139}
}



@Article{s21113777,
AUTHOR = {Zhang, Yani and Zhao, Huailin and Duan, Zuodong and Huang, Liangjun and Deng, Jiahao and Zhang, Qing},
TITLE = {Congested Crowd Counting via Adaptive Multi-Scale Context Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3777},
URL = {https://www.mdpi.com/1424-8220/21/11/3777},
PubMedID = {34072408},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we propose a novel congested crowd counting network for crowd density estimation, i.e., the Adaptive Multi-scale Context Aggregation Network (MSCANet). MSCANet efficiently leverages the spatial context information to accomplish crowd density estimation in a complicated crowd scene. To achieve this, a multi-scale context learning block, called the Multi-scale Context Aggregation module (MSCA), is proposed to first extract different scale information and then adaptively aggregate it to capture the full scale of the crowd. Employing multiple MSCAs in a cascaded manner, the MSCANet can deeply utilize the spatial context information and modulate preliminary features into more distinguishing and scale-sensitive features, which are finally applied to a 1 × 1 convolution operation to obtain the crowd density results. Extensive experiments on three challenging crowd counting benchmarks showed that our model yielded compelling performance against the other state-of-the-art methods. To thoroughly prove the generality of MSCANet, we extend our method to two relevant tasks: crowd localization and remote sensing object counting. The extension experiment results also confirmed the effectiveness of MSCANet.},
DOI = {10.3390/s21113777}
}



@Article{plants10061099,
AUTHOR = {Gong, Bin and Shu, Cheng and Han, Song and Cheng, Sheng-Gao},
TITLE = {Mine Vegetation Identification via Ecological Monitoring and Deep Belief Network},
JOURNAL = {Plants},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1099},
URL = {https://www.mdpi.com/2223-7747/10/6/1099},
PubMedID = {34070739},
ISSN = {2223-7747},
ABSTRACT = {Based on the characteristics of remote sensing images of mine vegetation, this research studied the application of deep belief network model in mine vegetation identification. Through vegetation identification and classification, the ecological environment index of mining area was determined according to the analysis of vegetation and coverage. Deep learning algorithm is adopted to improve the depth study, the vegetation coverage in the analysis was studied. Parameters and parameter values were selected for identification by establishing the optimal experimental design. The experimental results were compared with remote sensing images to determine the accuracy of deep learning identification and the effectiveness of the algorithm. When the sample size is 2,000,000 pixels, through repeated tests and classification effect comparison, the optimal parameter setting suitable for mine vegetation identification is obtained. Parameter setting: the number of network layers is 3 layers; the number of hidden layer neurons is 60. The learning rate is 0.01 and the number of iterations is 2. The average recognition rate of vegetation coverage was 95.95%, outperforming some other models, and the accuracy rate of kappa coefficient was 0.95, which can accurately reflect the vegetation coverage. The clearer the satellite image is, the more accurate the recognition result is, and the accuracy is closer to 100%. The identification of vegetation coverage has important guiding significance for determining the area and area of ecological restoration.},
DOI = {10.3390/plants10061099}
}



@Article{s21113830,
AUTHOR = {Almadhor, Ahmad and Rauf, Hafiz Tayyab and Lali, Muhammad Ikram Ullah and Damaševičius, Robertas and Alouffi, Bader and Alharbi, Abdullah},
TITLE = {AI-Driven Framework for Recognition of Guava Plant Diseases through Machine Learning from DSLR Camera Sensor Based High Resolution Imagery},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3830},
URL = {https://www.mdpi.com/1424-8220/21/11/3830},
PubMedID = {34205885},
ISSN = {1424-8220},
ABSTRACT = {Plant diseases can cause a considerable reduction in the quality and number of agricultural products. Guava, well known to be the tropics’ apple, is one significant fruit cultivated in tropical regions. It is attacked by 177 pathogens, including 167 fungal and others such as bacterial, algal, and nematodes. In addition, postharvest diseases may cause crucial production loss. Due to minor variations in various guava disease symptoms, an expert opinion is required for disease analysis. Improper diagnosis may cause economic losses to farmers’ improper use of pesticides. Automatic detection of diseases in plants once they emerge on the plants’ leaves and fruit is required to maintain high crop fields. In this paper, an artificial intelligence (AI) driven framework is presented to detect and classify the most common guava plant diseases. The proposed framework employs the ΔE color difference image segmentation to segregate the areas infected by the disease. Furthermore, color (RGB, HSV) histogram and textural (LBP) features are applied to extract rich, informative feature vectors. The combination of color and textural features are used to identify and attain similar outcomes compared to individual channels, while disease recognition is performed by employing advanced machine-learning classifiers (Fine KNN, Complex Tree, Boosted Tree, Bagged Tree, Cubic SVM). The proposed framework is evaluated on a high-resolution (18 MP) image dataset of guava leaves and fruit. The best recognition results were obtained by Bagged Tree classifier on a set of RGB, HSV, and LBP features (99% accuracy in recognizing four guava fruit diseases (Canker, Mummification, Dot, and Rust) against healthy fruit). The proposed framework may help the farmers to avoid possible production loss by taking early precautions.},
DOI = {10.3390/s21113830}
}



@Article{rs13112169,
AUTHOR = {Lee, Seunghyeon and Song, Youngkeun and Kil, Sung-Ho},
TITLE = {Feasibility Analyses of Real-Time Detection of Wildlife Using UAV-Derived Thermal and RGB Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2169},
URL = {https://www.mdpi.com/2072-4292/13/11/2169},
ISSN = {2072-4292},
ABSTRACT = {Wildlife monitoring is carried out for diverse reasons, and monitoring methods have gradually advanced through technological development. Direct field investigations have been replaced by remote monitoring methods, and unmanned aerial vehicles (UAVs) have recently become the most important tool for wildlife monitoring. Many previous studies on detecting wild animals have used RGB images acquired from UAVs, with most of the analyses depending on machine learning–deep learning (ML–DL) methods. These methods provide relatively accurate results, and when thermal sensors are used as a supplement, even more accurate detection results can be obtained through complementation with RGB images. However, because most previous analyses were based on ML–DL methods, a lot of time was required to generate training data and train detection models. This drawback makes ML–DL methods unsuitable for real-time detection in the field. To compensate for the disadvantages of the previous methods, this paper proposes a real-time animal detection method that generates a total of six applicable input images depending on the context and uses them for detection. The proposed method is based on the Sobel edge algorithm, which is simple but can detect edges quickly based on change values. The method can detect animals in a single image without training data. The fastest detection time per image was 0.033 s, and all frames of a thermal video could be analyzed. Furthermore, because of the synchronization of the properties of the thermal and RGB images, the performance of the method was above average in comparison with previous studies. With target images acquired at heights below 100 m, the maximum detection precision and detection recall of the most accurate input image were 0.804 and 0.699, respectively. However, the low resolution of the thermal sensor and its shooting height limitation were hindrances to wildlife detection. The aim of future research will be to develop a detection method that can improve these shortcomings.},
DOI = {10.3390/rs13112169}
}



@Article{app11115182,
AUTHOR = {Zhang, Shao and Yang, Guoqing and Sun, Tao and Du, Kunyang and Guo, Jin},
TITLE = {UAV Detection with Transfer Learning from Simulated Data of Laser Active Imaging},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5182},
URL = {https://www.mdpi.com/2076-3417/11/11/5182},
ISSN = {2076-3417},
ABSTRACT = {With the development of our society, unmanned aerial vehicles (UAVs) appear more frequently in people’s daily lives, which could become a threat to public security and privacy, especially at night. At the same time, laser active imaging is an important detection method for night vision. In this paper, we implement a UAV detection model for our laser active imaging system based on deep learning and a simulated dataset that we constructed. Firstly, the model is pre-trained on the largest available dataset. Then, it is transferred to a simulated dataset to learn about the UAV features. Finally, the trained model is tested on real laser active imaging data. The experimental results show that the performance of the proposed method is greatly improved compared to the model not trained on the simulated dataset, which verifies the transferability of features learned from the simulated data, the effectiveness of the proposed simulation method, and the feasibility of our solution for UAV detection in the laser active imaging domain. Furthermore, a comparative experiment with the previous method is carried out. The results show that our model can achieve high-precision, real-time detection at 104.1 frames per second (FPS).},
DOI = {10.3390/app11115182}
}



@Article{rs13112181,
AUTHOR = {Illarionova , Svetlana and Nesteruk , Sergey and Shadrin, Dmitrii and Ignatiev , Vladimir and Pukalchik , Maria and Oseledets, Ivan},
TITLE = {MixChannel: Advanced Augmentation for Multispectral Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2181},
URL = {https://www.mdpi.com/2072-4292/13/11/2181},
ISSN = {2072-4292},
ABSTRACT = {Usage of multispectral satellite imaging data opens vast possibilities for monitoring and quantitatively assessing properties or objects of interest on a global scale. Machine learning and computer vision (CV) approaches show themselves as promising tools for automatizing satellite image analysis. However, there are limitations in using CV for satellite data. Mainly, the crucial one is the amount of data available for model training. This paper presents a novel image augmentation approach called MixChannel that helps to address this limitation and improve the accuracy of solving segmentation and classification tasks with multispectral satellite images. The core idea is to utilize the fact that there is usually more than one image for each location in remote sensing tasks, and this extra data can be mixed to achieve the more robust performance of the trained models. The proposed approach substitutes some channels of the original training image with channels from other images of the exact location to mix auxiliary data. This augmentation technique preserves the spatial features of the original image and adds natural color variability with some probability. We also show an efficient algorithm to tune channel substitution probabilities. We report that the MixChannel image augmentation method provides a noticeable increase in performance of all the considered models in the studied forest types classification problem.},
DOI = {10.3390/rs13112181}
}



@Article{rs13112190,
AUTHOR = {Yuan, Ningge and Gong, Yan and Fang, Shenghui and Liu, Yating and Duan, Bo and Yang, Kaili and Wu, Xianting and Zhu, Renshan},
TITLE = {UAV Remote Sensing Estimation of Rice Yield Based on Adaptive Spectral Endmembers and Bilinear Mixing Model},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2190},
URL = {https://www.mdpi.com/2072-4292/13/11/2190},
ISSN = {2072-4292},
ABSTRACT = {The accurate estimation of rice yield using remote sensing (RS) technology is crucially important for agricultural decision-making. The rice yield estimation model based on the vegetation index (VI) is commonly used when working with RS methods, however, it is affected by irrelevant organs and background especially at heading stage. The spectral mixture analysis (SMA) can quantitatively obtain the abundance information and mitigate the impacts. Furthermore, according to the spectral variability and information complexity caused by the rice cropping system and canopy characteristics of reflection and scattering, in this study, the multi-endmember extraction by the pure pixel index (PPI) and the nonlinear unmixing method based on the bandwise generalized bilinear mixing model (NU-BGBM) were applied for SMA, and the VIE (VIs recalculated from endmember spectra) was integrated with abundance data to establish the yield estimation model at heading stage. In two paddy fields of different cultivation settings, multispectral images were collected by an unmanned aerial vehicle (UAV) at booting and heading stage. The correlation of several widely-used VIs and rice yield was tested and weaker at heading stage. In order to improve the yield estimation accuracy of rice at heading stage, the VIE and foreground abundances from SMA were combined to develop a linear yield estimation model. The results showed that VIE incorporated with abundances exhibited a better estimation ability than VI alone or the product of VI and abundances. In addition, when the structural difference of plants was obvious, the addition of the product of VIF (VIs recalculated from bilinear endmember spectra) and the corresponding bilinear abundances to the original product of VIE and abundances, enhanced model reliability. VIs using the near-infrared bands improved more significantly with the estimation error below 8.1%. This study verified the validation of the targeted SMA strategy while estimating crop yield by remotely sensed VI, especially for objects with obvious different spectra and complex structures.},
DOI = {10.3390/rs13112190}
}



@Article{ijgi10060382,
AUTHOR = {Al-Fugara, A’kif and Mabdeh, Ali Nouh and Ahmadlou, Mohammad and Pourghasemi, Hamid Reza and Al-Adamat, Rida and Pradhan, Biswajeet and Al-Shabeeb, Abdel Rahman},
TITLE = {Wildland Fire Susceptibility Mapping Using Support Vector Regression and Adaptive Neuro-Fuzzy Inference System-Based Whale Optimization Algorithm and Simulated Annealing},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {382},
URL = {https://www.mdpi.com/2220-9964/10/6/382},
ISSN = {2220-9964},
ABSTRACT = {Fires are one of the most destructive forces in natural ecosystems. This study aims to develop and compare four hybrid models using two well-known machine learning models, support vector regression (SVR) and the adaptive neuro-fuzzy inference system (ANFIS), as well as two meta-heuristic models, the whale optimization algorithm (WOA) and simulated annealing (SA) to map wildland fires in Jerash Province, Jordan. For modeling, 109 fire locations were used along with 14 relevant factors, including elevation, slope, aspect, land use, normalized difference vegetation index (NDVI), rainfall, temperature, wind speed, solar radiation, soil texture, topographic wetness index (TWI), distance to drainage, and population density, as the variables affecting the fire occurrence. The area under the receiver operating characteristic (AUROC) was used to evaluate the accuracy of the models. The findings indicated that SVR-based hybrid models yielded a higher AUROC value (0.965 and 0.949) than the ANFIS-based hybrid models (0.904 and 0.894, respectively). Wildland fire susceptibility maps can play a major role in shaping firefighting tactics.},
DOI = {10.3390/ijgi10060382}
}



@Article{rs13112194,
AUTHOR = {Khan, Asim and Asim, Warda and Ulhaq, Anwaar and Ghazi, Bilal and Robinson, Randall W.},
TITLE = {Health Assessment of Eucalyptus Trees Using Siamese Network from Google Street and Ground Truth Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2194},
URL = {https://www.mdpi.com/2072-4292/13/11/2194},
ISSN = {2072-4292},
ABSTRACT = {Urban greenery is an essential characteristic of the urban ecosystem, which offers various advantages, such as improved air quality, human health facilities, storm-water run-off control, carbon reduction, and an increase in property values. Therefore, identification and continuous monitoring of the vegetation (trees) is of vital importance for our urban lifestyle. This paper proposes a deep learning-based network, Siamese convolutional neural network (SCNN), combined with a modified brute-force-based line-of-bearing (LOB) algorithm that evaluates the health of Eucalyptus trees as healthy or unhealthy and identifies their geolocation in real time from Google Street View (GSV) and ground truth images. Our dataset represents Eucalyptus trees’ various details from multiple viewpoints, scales and different shapes to texture. The experiments were carried out in the Wyndham city council area in the state of Victoria, Australia. Our approach obtained an average accuracy of 93.2% in identifying healthy and unhealthy trees after training on around 4500 images and testing on 500 images. This study helps in identifying the Eucalyptus tree with health issues or dead trees in an automated way that can facilitate urban green management and assist the local council to make decisions about plantation and improvements in looking after trees. Overall, this study shows that even in a complex background, most healthy and unhealthy Eucalyptus trees can be detected by our deep learning algorithm in real time.},
DOI = {10.3390/rs13112194}
}



@Article{rs13112197,
AUTHOR = {Waldner, François and Diakogiannis, Foivos I. and Batchelor, Kathryn and Ciccotosto-Camp, Michael and Cooper-Williams, Elizabeth and Herrmann, Chris and Mata, Gonzalo and Toovey, Andrew},
TITLE = {Detect, Consolidate, Delineate: Scalable Mapping of Field Boundaries Using Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2197},
URL = {https://www.mdpi.com/2072-4292/13/11/2197},
ISSN = {2072-4292},
ABSTRACT = {Digital agriculture services can greatly assist growers to monitor their fields and optimize their use throughout the growing season. Thus, knowing the exact location of fields and their boundaries is a prerequisite. Unlike property boundaries, which are recorded in local council or title records, field boundaries are not historically recorded. As a result, digital services currently ask their users to manually draw their field, which is time-consuming and creates disincentives. Here, we present a generalized method, hereafter referred to as DECODE (DEtect, COnsolidate, and DElinetate), that automatically extracts accurate field boundary data from satellite imagery using deep learning based on spatial, spectral, and temporal cues. We introduce a new convolutional neural network (FracTAL ResUNet) as well as two uncertainty metrics to characterize the confidence of the field detection and field delineation processes. We finally propose a new methodology to compare and summarize field-based accuracy metrics. To demonstrate the performance and scalability of our method, we extracted fields across the Australian grains zone with a pixel-based accuracy of 0.87 and a field-based accuracy of up to 0.88 depending on the metric. We also trained a model on data from South Africa instead of Australia and found it transferred well to unseen Australian landscapes. We conclude that the accuracy, scalability and transferability of DECODE shows that large-scale field boundary extraction based on deep learning has reached operational maturity. This opens the door to new agricultural services that provide routine, near-real time field-based analytics.},
DOI = {10.3390/rs13112197}
}



@Article{rs13112220,
AUTHOR = {Bai, Yanbing and Wu, Wenqi and Yang, Zhengxin and Yu, Jinze and Zhao, Bo and Liu, Xing and Yang, Hanfang and Mas, Erick and Koshimura, Shunichi},
TITLE = {Enhancement of Detecting Permanent Water and Temporary Water in Flood Disasters by Fusing Sentinel-1 and Sentinel-2 Imagery Using Deep Learning Algorithms: Demonstration of Sen1Floods11 Benchmark Datasets},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2220},
URL = {https://www.mdpi.com/2072-4292/13/11/2220},
ISSN = {2072-4292},
ABSTRACT = {Identifying permanent water and temporary water in flood disasters efficiently has mainly relied on change detection method from multi-temporal remote sensing imageries, but estimating the water type in flood disaster events from only post-flood remote sensing imageries still remains challenging. Research progress in recent years has demonstrated the excellent potential of multi-source data fusion and deep learning algorithms in improving flood detection, while this field has only been studied initially due to the lack of large-scale labelled remote sensing images of flood events. Here, we present new deep learning algorithms and a multi-source data fusion driven flood inundation mapping approach by leveraging a large-scale publicly available Sen1Flood11 dataset consisting of roughly 4831 labelled Sentinel-1 SAR and Sentinel-2 optical imagery gathered from flood events worldwide in recent years. Specifically, we proposed an automatic segmentation method for surface water, permanent water, and temporary water identification, and all tasks share the same convolutional neural network architecture. We utilize focal loss to deal with the class (water/non-water) imbalance problem. Thorough ablation experiments and analysis confirmed the effectiveness of various proposed designs. In comparison experiments, the method proposed in this paper is superior to other classical models. Our model achieves a mean Intersection over Union (mIoU) of 52.99%, Intersection over Union (IoU) of 52.30%, and Overall Accuracy (OA) of 92.81% on the Sen1Flood11 test set. On the Sen1Flood11 Bolivia test set, our model also achieves very high mIoU (47.88%), IoU (76.74%), and OA (95.59%) and shows good generalization ability.},
DOI = {10.3390/rs13112220}
}



@Article{app11115303,
AUTHOR = {Huh, Eui-Nam and Hossain, Md Imtiaz},
TITLE = {Brainware Computing: Concepts, Scopes and Challenges},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5303},
URL = {https://www.mdpi.com/2076-3417/11/11/5303},
ISSN = {2076-3417},
ABSTRACT = {Over the decades, robotics technology has acquired sufficient advancement through the progression of 5G Internet, Artificial Intelligence (AI), Internet of Things (IoT), Cloud, and Edge Computing. Though nowadays, Cobot and Service Oriented Architecture (SOA) supported robots with edge computing paradigms have achieved remarkable performances in diverse applications, the existing SOA robotics technology fails to develop a multi-domain expert with high performing robots and demands improvement to Service-Oriented Brain, SOB (including AI model, driving service application and metadata) enabling robot for deploying brain and a new computing model with more scalability and flexibility. In this paper, instead of focusing on SOA and Robot as a Service (RaaS) model, we propose a novel computing architecture, addressed as Brainware Computing, for driving multiple domain-specific brains one-at-a-time in a single hardware robot according to the service, addressed as Brain as a Service (BaaS). In Brainware Computing, each robot can install and remove the virtual machine, which contains SOB and operating applications from the nearest edge cloud. Secondly, we provide an extensive explanation of the scope and possibilities of Brainware Computing. Finally, we demonstrate several challenges and opportunities and then concluded with future research directions in the field of Brainware Computing.},
DOI = {10.3390/app11115303}
}



@Article{rs13112232,
AUTHOR = {Paturkar, Abhipray and Sen Gupta, Gourab and Bailey, Donald},
TITLE = {Making Use of 3D Models for Plant Physiognomic Analysis: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2232},
URL = {https://www.mdpi.com/2072-4292/13/11/2232},
ISSN = {2072-4292},
ABSTRACT = {Use of 3D sensors in plant phenotyping has increased in the last few years. Various image acquisition, 3D representations, 3D model processing and analysis techniques exist to help the researchers. However, a review of approaches, algorithms, and techniques used for 3D plant physiognomic analysis is lacking. In this paper, we investigate the techniques and algorithms used at various stages of processing and analysing 3D models of plants, and identify their current limiting factors. This review will serve potential users as well as new researchers in this field. The focus is on exploring studies monitoring the plant growth of single plants or small scale canopies as opposed to large scale monitoring in the field.},
DOI = {10.3390/rs13112232}
}



@Article{land10060609,
AUTHOR = {Hara, Patryk and Piekutowska, Magdalena and Niedbała, Gniewko},
TITLE = {Selection of Independent Variables for Crop Yield Prediction Using Artificial Neural Network Models with Remote Sensing Data},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {609},
URL = {https://www.mdpi.com/2073-445X/10/6/609},
ISSN = {2073-445X},
ABSTRACT = {Knowing the expected crop yield in the current growing season provides valuable information for farmers, policy makers, and food processing plants. One of the main benefits of using reliable forecasting tools is generating more income from grown crops. Information on the amount of crop yielding before harvesting helps to guide the adoption of an appropriate strategy for managing agricultural products. The difficulty in creating forecasting models is related to the appropriate selection of independent variables. Their proper selection requires a perfect knowledge of the research object. The following article presents and discusses the most commonly used independent variables in agricultural crop yield prediction modeling based on artificial neural networks (ANNs). Particular attention is paid to environmental variables, such as climatic data, air temperature, total precipitation, insolation, and soil parameters. The possibility of using plant productivity indices and vegetation indices, which are valuable predictors obtained due to the application of remote sensing techniques, are analyzed in detail. The paper emphasizes that the increasingly common use of remote sensing and photogrammetric tools enables the development of precision agriculture. In addition, some limitations in the application of certain input variables are specified, as well as further possibilities for the development of non-linear modeling, using artificial neural networks as a tool supporting the practical use of and improvement in precision farming techniques.},
DOI = {10.3390/land10060609}
}



@Article{s21113936,
AUTHOR = {Spyridis, Yannis and Lagkas, Thomas and Sarigiannidis, Panagiotis and Argyriou, Vasileios and Sarigiannidis, Antonios and Eleftherakis, George and Zhang, Jie},
TITLE = {Towards 6G IoT: Tracing Mobile Sensor Nodes with Deep Learning Clustering in UAV Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3936},
URL = {https://www.mdpi.com/1424-8220/21/11/3936},
PubMedID = {34200449},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) in the role of flying anchor nodes have been proposed to assist the localisation of terrestrial Internet of Things (IoT) sensors and provide relay services in the context of the upcoming 6G networks. This paper considered the objective of tracing a mobile IoT device of unknown location, using a group of UAVs that were equipped with received signal strength indicator (RSSI) sensors. The UAVs employed measurements of the target’s radio frequency (RF) signal power to approach the target as quickly as possible. A deep learning model performed clustering in the UAV network at regular intervals, based on a graph convolutional network (GCN) architecture, which utilised information about the RSSI and the UAV positions. The number of clusters was determined dynamically at each instant using a heuristic method, and the partitions were determined by optimising an RSSI loss function. The proposed algorithm retained the clusters that approached the RF source more effectively, removing the rest of the UAVs, which returned to the base. Simulation experiments demonstrated the improvement of this method compared to a previous deterministic approach, in terms of the time required to reach the target and the total distance covered by the UAVs.},
DOI = {10.3390/s21113936}
}



@Article{app11125320,
AUTHOR = {Al-amri, Redhwan and Murugesan, Raja Kumar and Man, Mustafa and Abdulateef, Alaa Fareed and Al-Sharafi, Mohammed A. and Alkahtani, Ammar Ahmed},
TITLE = {A Review of Machine Learning and Deep Learning Techniques for Anomaly Detection in IoT Data},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5320},
URL = {https://www.mdpi.com/2076-3417/11/12/5320},
ISSN = {2076-3417},
ABSTRACT = {Anomaly detection has gained considerable attention in the past couple of years. Emerging technologies, such as the Internet of Things (IoT), are known to be among the most critical sources of data streams that produce massive amounts of data continuously from numerous applications. Examining these collected data to detect suspicious events can reduce functional threats and avoid unseen issues that cause downtime in the applications. Due to the dynamic nature of the data stream characteristics, many unresolved problems persist. In the existing literature, methods have been designed and developed to evaluate certain anomalous behaviors in IoT data stream sources. However, there is a lack of comprehensive studies that discuss all the aspects of IoT data processing. Thus, this paper attempts to fill this gap by providing a complete image of various state-of-the-art techniques on the major problems and core challenges in IoT data. The nature of data, anomaly types, learning mode, window model, datasets, and evaluation criteria are also presented. Research challenges related to data evolving, feature-evolving, windowing, ensemble approaches, nature of input data, data complexity and noise, parameters selection, data visualizations, heterogeneity of data, accuracy, and large-scale and high-dimensional data are investigated. Finally, the challenges that require substantial research efforts and future directions are summarized.},
DOI = {10.3390/app11125320}
}



@Article{rs13122237,
AUTHOR = {Alonso, Laura and Picos, Juan and Armesto, Julia},
TITLE = {Forest Land Cover Mapping at a Regional Scale Using Multi-Temporal Sentinel-2 Imagery and RF Models},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2237},
URL = {https://www.mdpi.com/2072-4292/13/12/2237},
ISSN = {2072-4292},
ABSTRACT = {Over the last several decades, thanks to improvements in and the diversification of open-access satellite imagery, land cover mapping techniques have evolved significantly. Notable changes in these techniques involve the automation of different steps, yielding promising results in terms of accuracy, class detection and efficiency. The most successful methodologies that have arisen rely on the use of multi-temporal data. Several different approaches have proven successful. In this study, one of the most recently developed methodologies is tested in the region of Galicia (in Northwestern Spain), with the aim of filling gaps in the mapping needs of the Galician forestry sector. The methodology mainly consists of performing a supervised classification of individual images from a selected time series and then combining them through aggregation using decision criteria. Several of the steps of the methodology can be addressed in multiple ways: pixel resolution selection, classification model building and aggregation methods. The effectiveness of these three tasks as well as some others are tested and evaluated and the most accurate and efficient parameters for the case study area are highlighted. The final land cover map that is obtained for Galicia has high accuracy metrics (an overall accuracy of 91.6%), which is in line with previous studies that have followed this methodology in other regions. This study has led to the development of an efficient open-access solution to support the mapping needs of the forestry sector.},
DOI = {10.3390/rs13122237}
}



@Article{data6060060,
AUTHOR = {Becerra, Miguel A. and Tobón, Catalina and Castro-Ospina, Andrés Eduardo and Peluffo-Ordóñez, Diego H.},
TITLE = {Information Quality Assessment for Data Fusion Systems},
JOURNAL = {Data},
VOLUME = {6},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {60},
URL = {https://www.mdpi.com/2306-5729/6/6/60},
ISSN = {2306-5729},
ABSTRACT = {This paper provides a comprehensive description of the current literature on data fusion, with an emphasis on Information Quality (IQ) and performance evaluation. This literature review highlights recent studies that reveal existing gaps, the need to find a synergy between data fusion and IQ, several research issues, and the challenges and pitfalls in this field. First, the main models, frameworks, architectures, algorithms, solutions, problems, and requirements are analyzed. Second, a general data fusion engineering process is presented to show how complex it is to design a framework for a specific application. Third, an IQ approach, as well as the different methodologies and frameworks used to assess IQ in information systems are addressed; in addition, data fusion systems are presented along with their related criteria. Furthermore, information on the context in data fusion systems and its IQ assessment are discussed. Subsequently, the issue of data fusion systems’ performance is reviewed. Finally, some key aspects and concluding remarks are outlined, and some future lines of work are gathered.},
DOI = {10.3390/data6060060}
}



@Article{s21123971,
AUTHOR = {de Oliveira, Gabriel Silva and Marcato Junior, José and Polidoro, Caio and Osco, Lucas Prado and Siqueira, Henrique and Rodrigues, Lucas and Jank, Liana and Barrios, Sanzio and Valle, Cacilda and Simeão, Rosângela and Carromeu, Camilo and Silveira, Eloise and  André de Castro Jorge, Lúcio and Gonçalves, Wesley and Santos, Mateus and Matsubara, Edson},
TITLE = {Convolutional Neural Networks to Estimate Dry Matter Yield in a Guineagrass Breeding Program Using UAV Remote Sensing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {3971},
URL = {https://www.mdpi.com/1424-8220/21/12/3971},
PubMedID = {34207543},
ISSN = {1424-8220},
ABSTRACT = {Forage dry matter is the main source of nutrients in the diet of ruminant animals. Thus, this trait is evaluated in most forage breeding programs with the objective of increasing the yield. Novel solutions combining unmanned aerial vehicles (UAVs) and computer vision are crucial to increase the efficiency of forage breeding programs, to support high-throughput phenotyping (HTP), aiming to estimate parameters correlated to important traits. The main goal of this study was to propose a convolutional neural network (CNN) approach using UAV-RGB imagery to estimate dry matter yield traits in a guineagrass breeding program. For this, an experiment composed of 330 plots of full-sib families and checks conducted at Embrapa Beef Cattle, Brazil, was used. The image dataset was composed of images obtained with an RGB sensor embedded in a Phantom 4 PRO. The traits leaf dry matter yield (LDMY) and total dry matter yield (TDMY) were obtained by conventional agronomic methodology and considered as the ground-truth data. Different CNN architectures were analyzed, such as AlexNet, ResNeXt50, DarkNet53, and two networks proposed recently for related tasks named MaCNN and LF-CNN. Pretrained AlexNet and ResNeXt50 architectures were also studied. Ten-fold cross-validation was used for training and testing the model. Estimates of DMY traits by each CNN architecture were considered as new HTP traits to compare with real traits. Pearson correlation coefficient r between real and HTP traits ranged from 0.62 to 0.79 for LDMY and from 0.60 to 0.76 for TDMY; root square mean error (RSME) ranged from 286.24 to 366.93 kg·ha−1 for LDMY and from 413.07 to 506.56 kg·ha−1 for TDMY. All the CNNs generated heritable HTP traits, except LF-CNN for LDMY and AlexNet for TDMY. Genetic correlations between real and HTP traits were high but varied according to the CNN architecture. HTP trait from ResNeXt50 pretrained achieved the best results for indirect selection regardless of the dry matter trait. This demonstrates that CNNs with remote sensing data are highly promising for HTP for dry matter yield traits in forage breeding programs.},
DOI = {10.3390/s21123971}
}



@Article{rs13122261,
AUTHOR = {Vayssade, Jehan-Antoine and Paoli, Jean-Noël and Gée, Christelle and Jones, Gawain},
TITLE = {DeepIndices: Remote Sensing Indices Based on Approximation of Functions through Deep-Learning, Application to Uncalibrated Vegetation Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2261},
URL = {https://www.mdpi.com/2072-4292/13/12/2261},
ISSN = {2072-4292},
ABSTRACT = {The form of a remote sensing index is generally empirically defined, whether by choosing specific reflectance bands, equation forms or its coefficients. These spectral indices are used as preprocessing stage before object detection/classification. But no study seems to search for the best form through function approximation in order to optimize the classification and/or segmentation. The objective of this study is to develop a method to find the optimal index, using a statistical approach by gradient descent on different forms of generic equations. From six wavebands images, five equations have been tested, namely: linear, linear ratio, polynomial, universal function approximator and dense morphological. Few techniques in signal processing and image analysis are also deployed within a deep-learning framework. Performances of standard indices and DeepIndices were evaluated using two metrics, the dice (similar to f1-score) and the mean intersection over union (mIoU) scores. The study focuses on a specific multispectral camera used in near-field acquisition of soil and vegetation surfaces. These DeepIndices are built and compared to 89 common vegetation indices using the same vegetation dataset and metrics. As an illustration the most used index for vegetation, NDVI (Normalized Difference Vegetation Indices) offers a mIoU score of 63.98% whereas our best models gives an analytic solution to reconstruct an index with a mIoU of 82.19%. This difference is significant enough to improve the segmentation and robustness of the index from various external factors, as well as the shape of detected elements.},
DOI = {10.3390/rs13122261}
}



@Article{inventions6020042,
AUTHOR = {Sangjan, Worasit and Carter, Arron H. and Pumphrey, Michael O. and Jitkov, Vadim and Sankaran, Sindhuja},
TITLE = {Development of a Raspberry Pi-Based Sensor System for Automated In-Field Monitoring to Support Crop Breeding Programs},
JOURNAL = {Inventions},
VOLUME = {6},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {42},
URL = {https://www.mdpi.com/2411-5134/6/2/42},
ISSN = {2411-5134},
ABSTRACT = {Sensor applications for plant phenotyping can advance and strengthen crop breeding programs. One of the powerful sensing options is the automated sensor system, which can be customized and applied for plant science research. The system can provide high spatial and temporal resolution data to delineate crop interaction with weather changes in a diverse environment. Such a system can be integrated with the internet to enable the internet of things (IoT)-based sensor system development for real-time crop monitoring and management. In this study, the Raspberry Pi-based sensor (imaging) system was fabricated and integrated with a microclimate sensor to evaluate crop growth in a spring wheat breeding trial for automated phenotyping applications. Such an in-field sensor system will increase the reproducibility of measurements and improve the selection efficiency by investigating dynamic crop responses as well as identifying key growth stages (e.g., heading), assisting in the development of high-performing crop varieties. In the low-cost system developed here-in, a Raspberry Pi computer and multiple cameras (RGB and multispectral) were the main components. The system was programmed to automatically capture and manage the crop image data at user-defined time points throughout the season. The acquired images were suitable for extracting quantifiable plant traits, and the images were automatically processed through a Python script (an open-source programming language) to extract vegetation indices, representing crop growth and overall health. Ongoing efforts are conducted towards integrating the sensor system for real-time data monitoring via the internet that will allow plant breeders to monitor multiple trials for timely crop management and decision making.},
DOI = {10.3390/inventions6020042}
}



@Article{rs13122272,
AUTHOR = {Xiong, Jinghua and Guo, Shenglian and Yin, Jiabo},
TITLE = {Discharge Estimation Using Integrated Satellite Data and Hybrid Model in the Midstream Yangtze River},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2272},
URL = {https://www.mdpi.com/2072-4292/13/12/2272},
ISSN = {2072-4292},
ABSTRACT = {Remotely sensing data have advantages in filling spatiotemporal gaps of in situ observation networks, showing potential application for monitoring floods in data-sparse regions. By using the water level retrievals of Jason-2/3 altimetry satellites, this study estimates discharge at a 10-day timescale for the virtual station (VS) 012 and 077 across the midstream Yangtze River Basin during 2009–2016 based on the developed Manning formula. Moreover, we calibrate a hybrid model combined with Gravity Recovery and Climate Experiment (GRACE) data, by coupling the GR6J hydrological model with a machine learning model to simulate discharge. To physically capture the flood processes, the random forest (RF) model is employed to downscale the 10-day discharge into a daily scale. The results show that: (1) discharge estimates from the developed Manning formula show good accuracy for the VS012 and VS077 based on the improved Multi-subwaveform Multi-weight Threshold Retracker; (2) the combination of the GR6J and the LSTM models substantially improves the performance of the discharge estimates solely from either the GR6J or LSTM models; (3) RF-downscaled daily discharge demonstrates a general consistency with in situ data, where NSE/KGE between them are as high as 0.69/0.83. Our approach, based on multi-source remotely sensing data and machine learning techniques, may benefit flood monitoring in poorly gauged areas.},
DOI = {10.3390/rs13122272}
}



@Article{rs13122279,
AUTHOR = {Dorado-Roda, Iván and Pascual, Adrián and Godinho, Sergio and Silva, Carlos A. and Botequim, Brigite and Rodríguez-Gonzálvez, Pablo and González-Ferreiro, Eduardo and Guerra-Hernández, Juan},
TITLE = {Assessing the Accuracy of GEDI Data for Canopy Height and Aboveground Biomass Estimates in Mediterranean Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2279},
URL = {https://www.mdpi.com/2072-4292/13/12/2279},
ISSN = {2072-4292},
ABSTRACT = {Global Ecosystem Dynamics Investigation (GEDI) satellite mission is expanding the spatial bounds and temporal resolution of large-scale mapping applications. Integrating the recent GEDI data into Airborne Laser Scanning (ALS)-derived estimations represents a global opportunity to update and extend forest models based on area based approaches (ABA) considering temporal and spatial dynamics. This study evaluates the effect of combining ALS-based aboveground biomass (AGB) estimates with GEDI-derived models by using temporally coincident datasets. A gradient of forest ecosystems, distributed through 21,766 km2 in the province of Badajoz (Spain), with different species and structural complexity, was used to: (i) assess the accuracy of GEDI canopy height in five Mediterranean Ecosystems and (ii) develop GEDI-based AGB models when using ALS-derived AGB estimates at GEDI footprint level. In terms of Pearson’s correlation (r) and rRMSE, the agreement between ALS and GEDI statistics on canopy height was stronger in the denser and homogeneous coniferous forest of P. pinaster and P. pinea than in sparse Quercus-dominated forests. The GEDI-derived AGB models using relative height and vertical canopy metrics yielded a model efficiency (Mef) ranging from 0.31 to 0.46, with a RMSE ranging from 14.13 to 32.16 Mg/ha and rRMSE from 38.17 to 84.74%, at GEDI footprint level by forest type. The impact of forest structure confirmed previous studies achievements, since GEDI data showed higher uncertainty in highly multilayered forests. In general, GEDI-derived models (GEDI-like Level4A) underestimated AGB over lower and higher ALS-derived AGB intervals. The proposed models could also be used to monitor biomass stocks at large-scale by using GEDI footprint level in Mediterranean areas, especially in remote and hard-to-reach areas for forest inventory. The findings from this study serve to provide an initial evaluation of GEDI data for estimating AGB in Mediterranean forest.},
DOI = {10.3390/rs13122279}
}



@Article{jmse9060645,
AUTHOR = {Chen, Hualong and Wen, Yuanqiao and Zhu, Man and Huang, Yamin and Xiao, Changshi and Wei, Tao and Hahn, Axel},
TITLE = {From Automation System to Autonomous System: An Architecture Perspective},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {9},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {645},
URL = {https://www.mdpi.com/2077-1312/9/6/645},
ISSN = {2077-1312},
ABSTRACT = {Autonomy is the core capability of future systems, and architecture design is one of the critical issues in system development and implementation. To discuss the architecture of autonomous systems in the future, this paper reviews the developing progress of architectures from automation systems to autonomous systems. Firstly, the autonomy and autonomous systems in different fields are summarized. The article classifies and summarizes the architecture of typical automated systems and infer three suggestions for building an autonomous system architecture: extensibility, evolvability, and collaborability. Accordingly, this paper builds an autonomous waterborne transportation system, and the architecture is composed of the object layer, cyberspace layer, cognition layer, and application layer, the proposed suggestions made in the construction of the architecture are reflected in the inter-relationships at all layers. Through the cooperation of four layers, the autonomous waterborne transportation system can autonomously complete the system functions, such as system control and transportation service. In the end, the characteristics of autonomous systems are concluded, from which the future primary research directions and the challenges of autonomous systems are provided.},
DOI = {10.3390/jmse9060645}
}



@Article{app11125410,
AUTHOR = {Zheng, Ke and Jia, Guozhu and Yang, Linchao and Wang, Jiaqing},
TITLE = {A Compound Fault Labeling and Diagnosis Method Based on Flight Data and BIT Record of UAV},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5410},
URL = {https://www.mdpi.com/2076-3417/11/12/5410},
ISSN = {2076-3417},
ABSTRACT = {In the process of Unmanned Aerial Vehicle (UAV) flight testing, plenty of compound faults exist, which could be composed of concurrent single faults or over-limit states alarmed by Built-In-Test (BIT) equipment. At present, there still lacks a suitable automatic labeling approach for UAV flight data, effectively utilizing the information of the BIT record. The performance of the originally employed flight data-driven fault diagnosis models based on machine learning needs to be improved as well. A compound fault labeling and diagnosis method based on actual flight data and the BIT record of the UAV during flight test phase is proposed, through labeling the flight data with compound fault modes corresponding to concurrent single faults recorded by the BIT system, and upgrading the original diagnosis model based on Gradient Boosting Decision Tree (GBDT) and Fully Convolutional Network (FCNN), to eXtreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and modified Convolutional Neural Network (CNN). The experimental results based on actual test flight data show that the proposed method could effectively label the flight data and obtain a significant improvement in diagnostic performance, appearing to be practical in the UAV test flight process.},
DOI = {10.3390/app11125410}
}



@Article{s21124023,
AUTHOR = {Honório, Leonardo M. and Pinto, Milena F. and Hillesheim, Maicon J. and de Araújo, Francisco C. and Santos, Alexandre B. and Soares, Delfim},
TITLE = {Photogrammetric Process to Monitor Stress Fields Inside Structural Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4023},
URL = {https://www.mdpi.com/1424-8220/21/12/4023},
PubMedID = {34200918},
ISSN = {1424-8220},
ABSTRACT = {This research employs displacement fields photogrammetrically captured on the surface of a solid or structure to estimate real-time stress distributions it undergoes during a given loading period. The displacement fields are determined based on a series of images taken from the solid surface while it experiences deformation. Image displacements are used to estimate the deformations in the plane of the beam surface, and Poisson’s Method is subsequently applied to reconstruct these surfaces, at a given time, by extracting triangular meshes from the corresponding points clouds. With the aid of the measured displacement fields, the Boundary Element Method (BEM) is considered to evaluate stress values throughout the solid. Herein, the unknown boundary forces must be additionally calculated. As the photogrammetrically reconstructed deformed surfaces may be defined by several million points, the boundary displacement values of boundary-element models having a convenient number of nodes are determined based on an optimized displacement surface that best fits the real measured data. The results showed the effectiveness and potential application of the proposed methodology in several tasks to determine real-time stress distributions in structures.},
DOI = {10.3390/s21124023}
}



@Article{e23060737,
AUTHOR = {Sun, Fengjie and Wang, Xianchang and Zhang, Rui},
TITLE = {Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {737},
URL = {https://www.mdpi.com/1099-4300/23/6/737},
PubMedID = {34207944},
ISSN = {1099-4300},
ABSTRACT = {An Unmanned Aerial Vehicle (UAV) can greatly reduce manpower in the agricultural plant protection such as watering, sowing, and pesticide spraying. It is essential to develop a Decision-making Support System (DSS) for UAVs to help them choose the correct action in states according to the policy. In an unknown environment, the method of formulating rules for UAVs to help them choose actions is not applicable, and it is a feasible solution to obtain the optimal policy through reinforcement learning. However, experiments show that the existing reinforcement learning algorithms cannot get the optimal policy for a UAV in the agricultural plant protection environment. In this work we propose an improved Q-learning algorithm based on similar state matching, and we prove theoretically that there has a greater probability for UAV choosing the optimal action according to the policy learned by the algorithm we proposed than the classic Q-learning algorithm in the agricultural plant protection environment. This proposed algorithm is implemented and tested on datasets that are evenly distributed based on real UAV parameters and real farm information. The performance evaluation of the algorithm is discussed in detail. Experimental results show that the algorithm we proposed can efficiently learn the optimal policy for UAVs in the agricultural plant protection environment.},
DOI = {10.3390/e23060737}
}



@Article{s21124026,
AUTHOR = {Brandoli, Bruno and de Geus, André R. and Souza, Jefferson R. and Spadon, Gabriel and Soares, Amilcar and Rodrigues, Jose F. and Komorowski, Jerzy and Matwin, Stan},
TITLE = {Aircraft Fuselage Corrosion Detection Using Artificial Intelligence},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4026},
URL = {https://www.mdpi.com/1424-8220/21/12/4026},
PubMedID = {34207959},
ISSN = {1424-8220},
ABSTRACT = {Corrosion identification and repair is a vital task in aircraft maintenance to ensure continued structural integrity. Regarding fuselage lap joints, typically, visual inspections are followed by non-destructive methodologies, which are time-consuming. The visual inspection of large areas suffers not only from subjectivity but also from the variable probability of corrosion detection, which is aggravated by the multiple layers used in fuselage construction. In this paper, we propose a methodology for automatic image-based corrosion detection of aircraft structures using deep neural networks. For machine learning, we use a dataset that consists of D-Sight Aircraft Inspection System (DAIS) images from different lap joints of Boeing and Airbus aircrafts. We also employ transfer learning to overcome the shortage of aircraft corrosion images. With precision of over 93%, we demonstrate that our approach detects corrosion with a precision comparable to that of trained operators, aiding to reduce the uncertainties related to operator fatigue or inadequate training. Our results indicate that our methodology can support specialists and engineers in corrosion monitoring in the aerospace industry, potentially contributing to the automation of condition-based maintenance protocols.},
DOI = {10.3390/s21124026}
}



@Article{en14123457,
AUTHOR = {Burdziakowski, Pawel},
TITLE = {Polymodal Method of Improving the Quality of Photogrammetric Images and Models},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {3457},
URL = {https://www.mdpi.com/1996-1073/14/12/3457},
ISSN = {1996-1073},
ABSTRACT = {Photogrammetry using unmanned aerial vehicles has become very popular and is already commonly used. The most frequent photogrammetry products are an orthoimage, digital terrain model and a 3D object model. When executing measurement flights, it may happen that there are unsuitable lighting conditions, and the flight itself is fast and not very stable. As a result, noise and blur appear on the images, and the images themselves can have too low of a resolution to satisfy the quality requirements for a photogrammetric product. In such cases, the obtained images are useless or will significantly reduce the quality of the end-product of low-level photogrammetry. A new polymodal method of improving measurement image quality has been proposed to avoid such issues. The method discussed in this article removes degrading factors from the images and, as a consequence, improves the geometric and interpretative quality of a photogrammetric product. The author analyzed 17 various image degradation cases, developed 34 models based on degraded and recovered images, and conducted an objective analysis of the quality of the recovered images and models. As evidenced, the result was a significant improvement in the interpretative quality of the images themselves and a better geometry model.},
DOI = {10.3390/en14123457}
}



@Article{rs13122288,
AUTHOR = {Quan, Longzhe and Li, Hengda and Li, Hailong and Jiang, Wei and Lou, Zhaoxia and Chen, Liqing},
TITLE = {Two-Stream Dense Feature Fusion Network Based on RGB-D Data for the Real-Time Prediction of Weed Aboveground Fresh Weight in a Field Environment},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2288},
URL = {https://www.mdpi.com/2072-4292/13/12/2288},
ISSN = {2072-4292},
ABSTRACT = {The aboveground fresh weight of weeds is an important indicator that reflects their biomass and physiological activity and directly affects the criteria for determining the amount of herbicides to apply. In precision agriculture, the development of models that can accurately locate weeds and predict their fresh weight can provide visual support for accurate, variable herbicide application in real time. In this work, we develop a two-stream dense feature fusion convolutional network model based on RGB-D data for the real-time prediction of the fresh weight of weeds. A data collection method is developed for the compilation and production of RGB-D data sets. The acquired images undergo data enhancement, and a depth transformation data enhancement method suitable for depth data is proposed. The main idea behind the approach in this study is to use the YOLO-V4 model to locate weeds and use the two-stream dense feature fusion network to predict their aboveground fresh weight. In the two-stream dense feature fusion network, DenseNet and NiN methods are used to construct a Dense-NiN-Block structure for deep feature extraction and fusion. The Dense-NiN-Block module was embedded in five convolutional neural networks for comparison, and the best results were achieved with DenseNet201. The test results show that the predictive ability of the convolutional network using RGB-D as the input is better than that of the network using RGB as the input without the Dense-NiN-Block module. The mAP of the proposed network is 75.34% (IoU value of 0.5), the IoU is 86.36%, the detection speed of the fastest model with a RTX2080Ti NVIDIA graphics card is 17.8 fps, and the average relative error is approximately 4%. The model proposed in this paper can provide visual technical support for precise, variable herbicide application. The model can also provide a reference method for the non-destructive prediction of crop fresh weight in the field and can contribute to crop breeding and genetic improvement.},
DOI = {10.3390/rs13122288}
}



@Article{rs13122290,
AUTHOR = {Zhang, Tao and Tang, Hong and Ding, Yi and Li, Penglong and Ji, Chao and Xu, Penglei},
TITLE = {FSRSS-Net: High-Resolution Mapping of Buildings from Middle-Resolution Satellite Images Using a Super-Resolution Semantic Segmentation Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2290},
URL = {https://www.mdpi.com/2072-4292/13/12/2290},
ISSN = {2072-4292},
ABSTRACT = {Satellite mapping of buildings and built-up areas used to be delineated from high spatial resolution (e.g., meters or sub-meters) and middle spatial resolution (e.g., tens of meters or hundreds of meters) satellite images, respectively. To the best of our knowledge, it is important to explore a deep-learning approach to delineate high-resolution semantic maps of buildings from middle-resolution satellite images. The approach is termed as super-resolution semantic segmentation in this paper. Specifically, we design a neural network with integrated low-level image features of super-resolution and high-level semantic features of super-resolution, which is trained with Sentinel-2A images (i.e., 10 m) and higher-resolution semantic maps (i.e., 2.5 m). The network, based on super-resolution semantic segmentation features is called FSRSS-Net. In China, the 35 cities are partitioned into three groups, i.e., 19 cities for model training, four cities for quantitative testing and the other 12 cities for qualitative generalization ability analysis of the learned networks. A large-scale sample dataset is created and utilized to train and validate the performance of the FSRSS-Net, which includes 8597 training samples and 766 quantitative accuracy evaluation samples. Quantitative evaluation results show that: (1) based on the 10 m Sentinel-2A image, the FSRSS-Net can achieve super-resolution semantic segmentation and produce 2.5 m building recognition results, and there is little difference between the accuracy of 2.5 m results by FSRSS-Net and 10 m results by U-Net. More importantly, the 2.5 m building recognition results by FSRSS-Net have higher accuracy than the 2.5 m results by U-Net 10 m building recognition results interpolation up-sampling; (2) from the spatial visualization of the results, the building recognition results of 2.5 m are more precise than those of 10 m, and the outline of the building is better depicted. Qualitative analysis shows that: (1) the learned FSRSS-Net can be also well generalized to other cities that are far from training regions; (2) the FSRSS-Net can still achieve comparable results to the U-Net 2 m building recognition results, even when the U-Net is directly trained using both 2-meter resolution GF2 satellite images and corresponding semantic labels.},
DOI = {10.3390/rs13122290}
}



@Article{s21124043,
AUTHOR = {Zhang, Wentao and Liu, Yucheng and Zhang, Shaohui and Long, Tuzhi and Liang, Jinglun},
TITLE = {Error Fusion of Hybrid Neural Networks for Mechanical Condition Dynamic Prediction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4043},
URL = {https://www.mdpi.com/1424-8220/21/12/4043},
PubMedID = {34208262},
ISSN = {1424-8220},
ABSTRACT = {It is important for equipment to operate safely and reliably so that the working state of mechanical parts pushes forward an immense influence. Therefore, in order to enhance the dependability and security of mechanical equipment, to accurately predict the changing trend of mechanical components in advance plays a significant role. This paper introduces a novel condition prediction method, named error fusion of hybrid neural networks (EFHNN), by combining the error fusion of multiple sparse auto-encoders with convolutional neural networks for predicting the mechanical condition. First, to improve prediction accuracy, we can use the error fusion of multiple sparse auto-encoders to collect multi-feature information, and obtain a trend curve representing machine condition as well as a threshold line that can indicate the beginning of mechanical failure by computing the square prediction error (SPE). Then, convolutional neural networks predict the state of the machine according to the original data when the SPE value exceeds the threshold line. It can be seen from this result that the EFHNN method in the prediction of mechanical fault time series is available and superior.},
DOI = {10.3390/s21124043}
}



@Article{rs13122308,
AUTHOR = {Aslahishahri, Masoomeh and Stanley, Kevin G. and Duddu, Hema and Shirtliffe, Steve and Vail, Sally and Stavness, Ian},
TITLE = {Spatial Super Resolution of Real-World Aerial Images for Image-Based Plant Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2308},
URL = {https://www.mdpi.com/2072-4292/13/12/2308},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) imaging is a promising data acquisition technique for image-based plant phenotyping. However, UAV images have a lower spatial resolution than similarly equipped in field ground-based vehicle systems, such as carts, because of their distance from the crop canopy, which can be particularly problematic for measuring small-sized plant features. In this study, the performance of three deep learning-based super resolution models, employed as a pre-processing tool to enhance the spatial resolution of low resolution images of three different kinds of crops were evaluated. To train a super resolution model, aerial images employing two separate sensors co-mounted on a UAV flown over lentil, wheat and canola breeding trials were collected. A software workflow to pre-process and align real-world low resolution and high-resolution images and use them as inputs and targets for training super resolution models was created. To demonstrate the effectiveness of real-world images, three different experiments employing synthetic images, manually downsampled high resolution images, or real-world low resolution images as input to the models were conducted. The performance of the super resolution models demonstrates that the models trained with synthetic images cannot generalize to real-world images and fail to reproduce comparable images with the targets. However, the same models trained with real-world datasets can reconstruct higher-fidelity outputs, which are better suited for measuring plant phenotypes.},
DOI = {10.3390/rs13122308}
}



@Article{app11125468,
AUTHOR = {Shmalko, Elizaveta and Diveev, Askhat},
TITLE = {Control Synthesis as Machine Learning Control by Symbolic Regression Methods},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5468},
URL = {https://www.mdpi.com/2076-3417/11/12/5468},
ISSN = {2076-3417},
ABSTRACT = {The problem of control synthesis is considered as machine learning control. The paper proposes a mathematical formulation of machine learning control, discusses approaches of supervised and unsupervised learning by symbolic regression methods. The principle of small variation of the basic solution is presented to set up the neighbourhood of the search and to increase search efficiency of symbolic regression methods. Different symbolic regression methods such as genetic programming, network operator, Cartesian and binary genetic programming are presented in details. It is shown on the computational example the possibilities of symbolic regression methods as unsupervised machine learning control technique to the solution of MLC problem of control synthesis for obtaining the stabilization system for a mobile robot.},
DOI = {10.3390/app11125468}
}



@Article{agronomy11061202,
AUTHOR = {Yang, Baohua and Gao, Zhiwei and Gao, Yuan and Zhu, Yue},
TITLE = {Rapid Detection and Counting of Wheat Ears in the Field Using YOLOv4 with Attention Module},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1202},
URL = {https://www.mdpi.com/2073-4395/11/6/1202},
ISSN = {2073-4395},
ABSTRACT = {The detection and counting of wheat ears are very important for crop field management, yield estimation, and phenotypic analysis. Previous studies have shown that most methods for detecting wheat ears were based on shallow features such as color and texture extracted by machine learning methods, which have obtained good results. However, due to the lack of robustness of these features, it was difficult for the above-mentioned methods to meet the detection and counting of wheat ears in natural scenes. Other studies have shown that convolutional neural network (CNN) methods could be used to achieve wheat ear detection and counting. However, the adhesion and occlusion of wheat ears limit the accuracy of detection. Therefore, to improve the accuracy of wheat ear detection and counting in the field, an improved YOLOv4 (you only look once v4) with CBAM (convolutional block attention module) including spatial and channel attention model was proposed that could enhance the feature extraction capabilities of the network by adding receptive field modules. In addition, to improve the generalization ability of the model, not only local wheat data (WD), but also two public data sets (WEDD and GWHDD) were used to construct the training set, the validation set, and the test set. The results showed that the model could effectively overcome the noise in the field environment and realize accurate detection and counting of wheat ears with different density distributions. The average accuracy of wheat ear detection was 94%, 96.04%, and 93.11%. Moreover, the wheat ears were counted on 60 wheat images. The results showed that R2 = 0.8968 for WD, 0.955 for WEDD, and 0.9884 for GWHDD. In short, the CBAM-YOLOv4 model could meet the actual requirements of wheat ear detection and counting, which provided technical support for other high-throughput parameters of the extraction of crops.},
DOI = {10.3390/agronomy11061202}
}



@Article{rs13122315,
AUTHOR = {Mokhtari, Ali and Ahmadi, Arman and Daccache, Andre and Drechsler, Kelley},
TITLE = {Actual Evapotranspiration from UAV Images: A Multi-Sensor Data Fusion Approach},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2315},
URL = {https://www.mdpi.com/2072-4292/13/12/2315},
ISSN = {2072-4292},
ABSTRACT = {Multispectral imaging using Unmanned Aerial Vehicles (UAVs) has changed the pace of precision agriculture. Actual evapotranspiration (ETa) from the very high spatial resolution of UAV images over agricultural fields can help farmers increase their production at the lowest possible cost. ETa estimation using UAVs requires a full package of sensors capturing the visible/infrared and thermal portions of the spectrum. Therefore, this study focused on a multi-sensor data fusion approach for ETa estimation (MSDF-ET) independent of thermal sensors. The method was based on sharpening the Landsat 8 pixels to UAV spatial resolution by considering the relationship between reference ETa fraction (ETrf) and a Vegetation Index (VI). Four Landsat 8 images were processed to calculate ETa of three UAV images over three almond fields. Two flights coincided with the overpasses and one was in between two consecutive Landsat 8 images. ETrf was chosen instead of ETa to interpolate the Landsat 8-derived ETrf images to obtain an ETrf image on the UAV flight. ETrf was defined as the ratio of ETa to grass reference evapotranspiration (ETr), and the VIs tested in this study included the Normalized Difference Vegetation Index (NDVI), Soil Adjusted Vegetation Index (SAVI), Enhanced Vegetation Index (EVI), Normalized Difference Water Index (NDWI), and Land Surface Water Index (LSWI). NDVI performed better under the study conditions. The MSDF-ET-derived ETa showed strong correlations against measured ETa, UAV- and Landsat 8-based METRIC ETa. Also, visual comparison of the MSDF-ET ETa maps was indicative of a promising performance of the method. In sum, the resulting ETa had a higher spatial resolution compared with thermal-based ETa without the need for the Albedo and hot/cold pixels selection procedure. However, wet soils were poorly detected, and in cases of continuous cloudy Landsat pixels the long interval between the images may cause biases in ETa estimation from the MSDF-ET method. Generally, the MSDF-ET method reduces the need for very high resolution thermal information from the ground, and the calculations can be conducted on a moderate-performance computer system because the main image processing is applied on Landsat images with coarser spatial resolutions.},
DOI = {10.3390/rs13122315}
}



@Article{rs13122318,
AUTHOR = {Lema, Darío G. and Pedrayes, Oscar D. and Usamentiaga, Rubén and García, Daniel F. and Alonso, Ángela},
TITLE = {Cost-Performance Evaluation of a Recognition Service of Livestock Activity Using Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2318},
URL = {https://www.mdpi.com/2072-4292/13/12/2318},
ISSN = {2072-4292},
ABSTRACT = {The recognition of livestock activity is essential to be eligible for subsides, to automatically supervise critical activities and to locate stray animals. In recent decades, research has been carried out into animal detection, but this paper also analyzes the detection of other key elements that can be used to verify the presence of livestock activity in a given terrain: manure piles, feeders, silage balls, silage storage areas, and slurry pits. In recent years, the trend is to apply Convolutional Neuronal Networks (CNN) as they offer significantly better results than those obtained by traditional techniques. To implement a livestock activity detection service, the following object detection algorithms have been evaluated: YOLOv2, YOLOv4, YOLOv5, SSD, and Azure Custom Vision. Since YOLOv5 offers the best results, producing a mean average precision (mAP) of 0.94, this detector is selected for the creation of a livestock activity recognition service. In order to deploy the service in the best infrastructure, the performance/cost ratio of various Azure cloud infrastructures are analyzed and compared with a local solution. The result is an efficient and accurate service that can help to identify the presence of livestock activity in a specified terrain.},
DOI = {10.3390/rs13122318}
}



@Article{electronics10121422,
AUTHOR = {Saad, Mohamad Hanif Md and Hamdan, Nurul Maisarah and Sarker, Mahidur R.},
TITLE = {State of the Art of Urban Smart Vertical Farming Automation System: Advanced Topologies, Issues and Recommendations},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {1422},
URL = {https://www.mdpi.com/2079-9292/10/12/1422},
ISSN = {2079-9292},
ABSTRACT = {The global economy is now under threat due to the ongoing domestic and international lockdown for COVID-19. Many have already lost their jobs, and businesses have been unstable in the Corona era. Apart from educational institutions, banks, privately owned institutions, and agriculture, there are signs of economic recession in almost all sectors. The roles of modern technology, the Internet of things, and artificial intelligence are undeniable in helping the world achieve economic prosperity in the post-COVID-19 economic downturn. Food production must increase by 60% by 2050 to meet global food security demands in the face of uncertainty such as the COVID-19 pandemic and a growing population. Given COVID 19’s intensity and isolation, improving food production and distribution systems is critical to combating hunger and addressing the double burden of malnutrition. As the world’s population is growing day by day, according to an estimation world’s population reaches 9.6 billion by 2050, so there is a growing need to modify the agriculture methods, technologies so that maximum crops can be attained and human effort can be reduced. The urban smart vertical farming (USVF) is a solution to secure food production, which can be introduced at any adaptive reuse, retrofit, or new buildings in vertical manners. This paper aims to provide a comprehensive review of the concept of USVF using various techniques to enhance productivity as well as its types, topologies, technologies, control systems, social acceptance, and benefits. This review has focused on numerous issues, challenges, and recommendations in the development of the system, vertical farming management, and modern technologies approach.},
DOI = {10.3390/electronics10121422}
}



@Article{rs13122328,
AUTHOR = {Hong, Yameng and Leng, Chengcai and Zhang, Xinyue and Pei, Zhao and Cheng, Irene and Basu, Anup},
TITLE = {HOLBP: Remote Sensing Image Registration Based on Histogram of Oriented Local Binary Pattern Descriptor},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2328},
URL = {https://www.mdpi.com/2072-4292/13/12/2328},
ISSN = {2072-4292},
ABSTRACT = {Image registration has always been an important research topic. This paper proposes a novel method of constructing descriptors called the histogram of oriented local binary pattern descriptor (HOLBP) for fast and robust matching. There are three new components in our algorithm. First, we redefined the gradient and angle calculation template to make it more sensitive to edge information. Second, we proposed a new construction method of the HOLBP descriptor and improved the traditional local binary pattern (LBP) computation template. Third, the principle of uniform rotation-invariant LBP was applied to add 10-dimensional gradient direction information to form a 138-dimension HOLBP descriptor vector. The experimental results showed that our method is very stable in terms of accuracy and computational time for different test images.},
DOI = {10.3390/rs13122328}
}



@Article{s21124089,
AUTHOR = {Kim, Jingyeom and Lee, Joohyung and Kim, Taeyeon},
TITLE = {AdaMM: Adaptive Object Movement and Motion Tracking in Hierarchical Edge Computing System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4089},
URL = {https://www.mdpi.com/1424-8220/21/12/4089},
PubMedID = {34198526},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a novel adaptive object movement and motion tracking (AdaMM) framework in a hierarchical edge computing system for achieving GPU memory footprint reduction of deep learning (DL)-based video surveillance services. DL-based object movement and motion tracking requires a significant amount of resources, such as (1) GPU processing power for the inference phase and (2) GPU memory for model loading. Despite the absence of an object in the video, if the DL model is loaded, the GPU memory must be kept allocated for the loaded model. Moreover, in several cases, video surveillance tries to capture events that rarely occur (e.g., abnormal object behaviors); therefore, such standby GPU memory might be easily wasted. To alleviate this problem, the proposed AdaMM framework categorizes the tasks used for the object movement and motion tracking procedure in an increasing order of the required processing and memory resources as task (1) frame difference calculation, task (2) object detection, and task (3) object motion and movement tracking. The proposed framework aims to adaptively release the unnecessary standby object motion and movement tracking model to save GPU memory by utilizing light tasks, such as frame difference calculation and object detection in a hierarchical manner. Consequently, object movement and motion tracking are adaptively triggered if the object is detected within the specified threshold time; otherwise, the GPU memory for the model of task (3) can be released. Moreover, object detection is also adaptively performed if the frame difference over time is greater than the specified threshold. We implemented the proposed AdaMM framework using commercial edge devices by considering a three-tier system, such as the 1st edge node for both tasks (1) and (2), the 2nd edge node for task (3), and the cloud for sending a push alarm. A measurement-based experiment reveals that the proposed framework achieves a maximum GPU memory reduction of 76.8% compared to the baseline system, while requiring a 2680 ms delay for loading the model for object movement and motion tracking.},
DOI = {10.3390/s21124089}
}



@Article{rs13122335,
AUTHOR = {Tasseron, Paolo and van Emmerik, Tim and Peller, Joseph and Schreyers, Louise and Biermann, Lauren},
TITLE = {Advancing Floating Macroplastic Detection from Space Using Experimental Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2335},
URL = {https://www.mdpi.com/2072-4292/13/12/2335},
ISSN = {2072-4292},
ABSTRACT = {Airborne and spaceborne remote sensing (RS) collecting hyperspectral imagery provides unprecedented opportunities for the detection and monitoring of floating riverine and marine plastic debris. However, a major challenge in the application of RS techniques is the lack of a fundamental understanding of spectral signatures of water-borne plastic debris. Recent work has emphasised the case for open-access hyperspectral reflectance reference libraries of commonly used polymer items. In this paper, we present and analyse a high-resolution hyperspectral image database of a unique mix of 40 virgin macroplastic items and vegetation. Our double camera setup covered the visible to shortwave infrared (VIS-SWIR) range from 400 to 1700 nm in a darkroom experiment with controlled illumination. The cameras scanned the samples floating in water and captured high-resolution images in 336 spectral bands. Using the resulting reflectance spectra of 1.89 million pixels in linear discriminant analyses (LDA), we determined the importance of each spectral band for discriminating between water and mixed floating debris, and vegetation and plastics. The absorption peaks of plastics (1215 nm, 1410 nm) and vegetation (710 nm, 1450 nm) are associated with high LDA weights. We then compared Sentinel-2 and Worldview-3 satellite bands with these outcomes and identified 12 satellite bands to overlap with important wavelengths for discrimination between the classes. Lastly, the Normalised Vegetation Difference Index (NDVI) and Floating Debris Index (FDI) were calculated to determine why they work, and how they could potentially be improved. These findings could be used to enhance existing efforts in monitoring macroplastic pollution, as well as form a baseline for the design of future multispectral RS systems.},
DOI = {10.3390/rs13122335}
}



@Article{electronics10121444,
AUTHOR = {Hwang, Kyunghun and Park, Joonghoo and Kim, Heejung and Kuc, Tea-Yong and Lim, Sejoon},
TITLE = {Development of a Simple Robotic Driver System (SimRoDS) to Test Fuel Economy of Hybrid Electric and Plug-In Hybrid Electric Vehicles Using Fuzzy-PI Control},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {1444},
URL = {https://www.mdpi.com/2079-9292/10/12/1444},
ISSN = {2079-9292},
ABSTRACT = {Over the past decade, new models of hybrid electric vehicles have been released worldwide, and the fuel efficiency of said vehicles has increased by more than 5%. To further improve fuel efficiency, vehicle manufacturers have made efforts to design modules (e.g., engines, motors, transmissions, and batteries) with the highest efficiency possible. To do so, the fuel economy test process, which is conducted primarily using a chassis dynamometer, must produce reliable and accurate results. To accurately analyze the fuel efficiency improvement rate of each module, it is necessary to reduce the test deviation. When the test conducted by human drivers, the test deviation is somewhat large. When the test is conducted by a physical robot driver, the test deviation is improved; however, these robots are expensive and time-consuming to install and take up considerable amount of space in the driver’s seat. To compensate for these shortcomings, we propose a simple, structured robot system that manipulates electrical signals without using mechanical link structures. The controller of this robot driver uses the widely used PI controller. Although PI controllers are simple and perform well, since the dynamics of each test vehicle is different (e.g., acceleration response), the PI controller has a disadvantage in that it cannot determine the optimal PI gain value for each vehicles. In this work, the fuzzy control theorem is applied to overcome this disadvantage. By using fuzzy control to deduce the optimal value of the PI gain, we confirmed that our proposed system is available to conduct tests on vehicles with different dynamics.},
DOI = {10.3390/electronics10121444}
}



@Article{rs13122351,
AUTHOR = {Torresani, Alessandro and Menna, Fabio and Battisti, Roberto and Remondino, Fabio},
TITLE = {A V-SLAM Guided and Portable System for Photogrammetric Applications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2351},
URL = {https://www.mdpi.com/2072-4292/13/12/2351},
ISSN = {2072-4292},
ABSTRACT = {Mobile and handheld mapping systems are becoming widely used nowadays as fast and cost-effective data acquisition systems for 3D reconstruction purposes. While most of the research and commercial systems are based on active sensors, solutions employing only cameras and photogrammetry are attracting more and more interest due to their significantly minor costs, size and power consumption. In this work we propose an ARM-based, low-cost and lightweight stereo vision mobile mapping system based on a Visual Simultaneous Localization And Mapping (V-SLAM) algorithm. The prototype system, named GuPho (Guided Photogrammetric System), also integrates an in-house guidance system which enables optimized image acquisitions, robust management of the cameras and feedback on positioning and acquisition speed. The presented results show the effectiveness of the developed prototype in mapping large scenarios, enabling motion blur prevention, robust camera exposure control and achieving accurate 3D results.},
DOI = {10.3390/rs13122351}
}



@Article{rs13122352,
AUTHOR = {Geng, Liying and Che, Tao and Ma, Mingguo and Tan, Junlei and Wang, Haibo},
TITLE = {Corn Biomass Estimation by Integrating Remote Sensing and Long-Term Observation Data Based on Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2352},
URL = {https://www.mdpi.com/2072-4292/13/12/2352},
ISSN = {2072-4292},
ABSTRACT = {The accurate and timely estimation of regional crop biomass at different growth stages is of great importance in guiding crop management decision making. The recent availability of long time series of remote sensing data offers opportunities for crop monitoring. In this paper, four machine learning models, namely random forest (RF), support vector machine (SVM), artificial neural network (ANN), and extreme gradient boosting (XGBoost) were adopted to estimate the seasonal corn biomass based on field observation data and moderate resolution imaging spectroradiometer (MODIS) reflectance data from 2012 to 2019 in the middle reaches of the Heihe River basin, China. Nine variables were selected with the forward feature selection approach from among twenty-seven variables potentially influencing corn biomass: soil-adjusted total vegetation index (SATVI), green ratio vegetation index (GRVI), Nadir_B7 (2105–2155 nm), Nadir_B6 (1628–1652 nm), land surface water index (LSWI), normalized difference vegetation index (NDVI), Nadir_B4 (545–565 nm), and Nadir_B3 (459–479 nm). The results indicated that the corn biomass was suitably estimated (the coefficient of determination (R2) was between 0.72 and 0.78) with the four machine learning models. The XGBoost model performed better than the other three models (R2 = 0.78, root mean squared error (RMSE) = 2.86 t/ha and mean absolute error (MAE) = 1.86 t/ha). Moreover, the RF model was an effective method (R2 = 0.77, RMSE = 2.91 t/ha and MAE = 1.91 t/ha), with a performance comparable to that of the XGBoost model. This study provides a reference for estimating crop biomass from MOD43A4 datasets. In addition, the research demonstrates the potential of machine learning techniques to achieve a relatively accurate estimation of daily corn biomass at a large scale.},
DOI = {10.3390/rs13122352}
}



@Article{computers10060081,
AUTHOR = {Planke, Lars J. and Gardi, Alessandro and Sabatini, Roberto and Kistan, Trevor and Ezer, Neta},
TITLE = {Online Multimodal Inference of Mental Workload for Cognitive Human Machine Systems},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {81},
URL = {https://www.mdpi.com/2073-431X/10/6/81},
ISSN = {2073-431X},
ABSTRACT = {With increasingly higher levels of automation in aerospace decision support systems, it is imperative that the human operator maintains the required level of situational awareness in different operational conditions and a central role in the decision-making process. While current aerospace systems and interfaces are limited in their adaptability, a Cognitive Human Machine System (CHMS) aims to perform dynamic, real-time system adaptation by estimating the cognitive states of the human operator. Nevertheless, to reliably drive system adaptation of current and emerging aerospace systems, there is a need to accurately and repeatably estimate cognitive states, particularly for Mental Workload (MWL), in real-time. As part of this study, two sessions were performed during a Multi-Attribute Task Battery (MATB) scenario, including a session for offline calibration and validation and a session for online validation of eleven multimodal inference models of MWL. The multimodal inference model implemented included an Adaptive Neuro Fuzzy Inference System (ANFIS), which was used in different configurations to fuse data from an Electroencephalogram (EEG) model’s output, four eye activity features and a control input feature. The online validation of the ANFIS models produced good results, while the best performing model (containing all four eye activity features and the control input feature) showed an average Mean Absolute Error (MAE) = 0.67 ± 0.18 and Correlation Coefficient (CC) = 0.71 ± 0.15. The remaining six ANFIS models included data from the EEG model’s output, which had an offset discrepancy. This resulted in an equivalent offset for the online multimodal fusion. Nonetheless, the efficacy of these ANFIS models could be confirmed by the pairwise correlation with the task level, where one model demonstrated a CC = 0.77 ± 0.06, which was the highest among all of the ANFIS models tested. Hence, this study demonstrates the suitability for online multimodal fusion of features extracted from EEG signals, eye activity and control inputs to produce an accurate and repeatable inference of MWL.},
DOI = {10.3390/computers10060081}
}



@Article{agronomy11061227,
AUTHOR = {Linaza, Maria Teresa and Posada, Jorge and Bund, Jürgen and Eisert, Peter and Quartulli, Marco and Döllner, Jürgen and Pagani, Alain and G. Olaizola, Igor and Barriguinha, Andre and Moysiadis, Theocharis and Lucat, Laurent},
TITLE = {Data-Driven Artificial Intelligence Applications for Sustainable Precision Agriculture},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1227},
URL = {https://www.mdpi.com/2073-4395/11/6/1227},
ISSN = {2073-4395},
ABSTRACT = {One of the main challenges for the implementation of artificial intelligence (AI) in agriculture includes the low replicability and the corresponding difficulty in systematic data gathering, as no two fields are exactly alike. Therefore, the comparison of several pilot experiments in different fields, weather conditions and farming techniques enhances the collective knowledge. Thus, this work provides a summary of the most recent research activities in the form of research projects implemented and validated by the authors in several European countries, with the objective of presenting the already achieved results, the current investigations and the still open technical challenges. As an overall conclusion, it can be mentioned that even though in their primary stages in some cases, AI technologies improve decision support at farm level, monitoring conditions and optimizing production to allow farmers to apply the optimal number of inputs for each crop, thereby boosting yields and reducing water use and greenhouse gas emissions. Future extensions of this work will include new concepts based on autonomous and intelligent robots for plant and soil sample retrieval, and effective livestock management.},
DOI = {10.3390/agronomy11061227}
}



@Article{s21124150,
AUTHOR = {Siemiatkowska, Barbara and Stecz, Wojciech},
TITLE = {A Framework for Planning and Execution of Drone Swarm Missions in a Hostile Environment},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4150},
URL = {https://www.mdpi.com/1424-8220/21/12/4150},
PubMedID = {34204272},
ISSN = {1424-8220},
ABSTRACT = {This article presents a framework for planning a drone swarm mission in a hostile environment. Elements of the planning framework are discussed in detail, including methods of planning routes for drone swarms using mixed integer linear programming (MILP) and methods of detecting potentially dangerous objects using EO/IR camera images and synthetic aperture radar (SAR). Methods of detecting objects in the field are used in the mission planning process to re-plan the swarm’s flight paths. The route planning model is discussed using the example of drone formations managed by one UAV that communicates through another UAV with the ground control station (GCS). This article presents practical examples of using algorithms for detecting dangerous objects for re-planning of swarm routes. A novelty in the work is the development of these algorithms in such a way that they can be implemented on mobile computers used by UAVs and integrated with MILP tasks. The methods of detection and classification of objects in real time by UAVs equipped with SAR and EO/IR are presented. Different sensors require different methods to detect objects. In the case of infrared or optoelectronic sensors, a convolutional neural network is used. For SAR images, a rule-based system is applied. The experimental results confirm that the stream of images can be analyzed in real-time.},
DOI = {10.3390/s21124150}
}



@Article{drones5020052,
AUTHOR = {Lee, Thomas and Mckeever, Susan and Courtney, Jane},
TITLE = {Flying Free: A Research Overview of Deep Learning in Drone Navigation Autonomy},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2504-446X/5/2/52},
ISSN = {2504-446X},
ABSTRACT = {With the rise of Deep Learning approaches in computer vision applications, significant strides have been made towards vehicular autonomy. Research activity in autonomous drone navigation has increased rapidly in the past five years, and drones are moving fast towards the ultimate goal of near-complete autonomy. However, while much work in the area focuses on specific tasks in drone navigation, the contribution to the overall goal of autonomy is often not assessed, and a comprehensive overview is needed. In this work, a taxonomy of drone navigation autonomy is established by mapping the definitions of vehicular autonomy levels, as defined by the Society of Automotive Engineers, to specific drone tasks in order to create a clear definition of autonomy when applied to drones. A top–down examination of research work in the area is conducted, focusing on drone navigation tasks, in order to understand the extent of research activity in each area. Autonomy levels are cross-checked against the drone navigation tasks addressed in each work to provide a framework for understanding the trajectory of current research. This work serves as a guide to research in drone autonomy with a particular focus on Deep Learning-based solutions, indicating key works and areas of opportunity for development of this area in the future.},
DOI = {10.3390/drones5020052}
}



@Article{app11125621,
AUTHOR = {An, Kang and Chen, Yixin and Wang, Suhong and Xiao, Zhifeng},
TITLE = {RCBi-CenterNet: An Absolute Pose Policy for 3D Object Detection in Autonomous Driving},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5621},
URL = {https://www.mdpi.com/2076-3417/11/12/5621},
ISSN = {2076-3417},
ABSTRACT = {3D Object detection is a critical mission of the perception system of a self-driving vehicle. Existing bounding box-based methods are hard to train due to the need to remove duplicated detections in the post-processing stage. In this paper, we propose a center point-based deep neural network (DNN) architecture named RCBi-CenterNet that predicts the absolute pose for each detected object in the 3D world space. RCBi-CenterNet is composed of a recursive composite network with a dual-backbone feature extractor and a bi-directional feature pyramid network (BiFPN) for cross-scale feature fusion. In the detection head, we predict a confidence heatmap that is used to determine the position of detected objects. The other pose information, including depth and orientation, is regressed. We conducted extensive experiments on the Peking University/Baidu-Autonomous Driving dataset, which contains more than 60,000 labeled 3D vehicle instances from 5277 real-world images, and each vehicle object is annotated with the absolute pose described by the six degrees of freedom (6DOF). We validated the design choices of various data augmentation methods and the backbone options. Through an ablation study and an overall comparison with the state-of-the-art (SOTA), namely CenterNet, we showed that the proposed RCBi-CenterNet presents performance gains of 2.16%, 2.76%, and 5.24% in Top 1, Top 3, and Top 10 mean average precision (mAP). The model and the result could serve as a credible benchmark for future research in center point-based object detection.},
DOI = {10.3390/app11125621}
}



@Article{rs13122377,
AUTHOR = {Huang, Yixin and Mu, Zhongcheng and Wu, Shufan and Cui, Benjie and Duan, Yuxiao},
TITLE = {Revising the Observation Satellite Scheduling Problem Based on Deep Reinforcement Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2377},
URL = {https://www.mdpi.com/2072-4292/13/12/2377},
ISSN = {2072-4292},
ABSTRACT = {Earth observation satellite task scheduling research plays a key role in space-based remote sensing services. An effective task scheduling strategy can maximize the utilization of satellite resources and obtain larger objective observation profits. In this paper, inspired by the success of deep reinforcement learning in optimization domains, the deep deterministic policy gradient algorithm is adopted to solve a time-continuous satellite task scheduling problem. Moreover, an improved graph-based minimum clique partition algorithm is proposed for preprocessing in the task clustering phase by considering the maximum task priority and the minimum observation slewing angle under constraint conditions. Experimental simulation results demonstrate that the deep reinforcement learning-based task scheduling method is feasible and performs much better than traditional metaheuristic optimization algorithms, especially in large-scale problems.},
DOI = {10.3390/rs13122377}
}



@Article{rs13122379,
AUTHOR = {Arumugam, Ponraj and Chemura, Abel and Schauberger, Bernhard and Gornott, Christoph},
TITLE = {Remote Sensing Based Yield Estimation of Rice (Oryza Sativa L.) Using Gradient Boosted Regression in India},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2379},
URL = {https://www.mdpi.com/2072-4292/13/12/2379},
ISSN = {2072-4292},
ABSTRACT = {Accurate and spatially explicit yield information is required to ensure farmers’ income and food security at local and national levels. Current approaches based on crop cutting experiments are expensive and usually too late for timely income stabilization measures like crop insurances. We, therefore, utilized a Gradient Boosted Regression (GBR), a machine learning technique, to estimate rice yields at ~500 m spatial resolution for rice-producing areas in India with potential application for near real-time estimates. We used resampled intermediate resolution (~5 km) images of the Moderate Resolution Imaging Spectroradiometer (MODIS) Leaf Area Index (LAI) and observed yields at the district level in India for calibrating GBR models. These GBRs were then used to downscale district yields to 500 m resolution. Downscaled yields were re-aggregated for validation against out-of-sample district yields not used for model training and an additional independent data set of block-level (below district-level) yields. Our downscaled and re-aggregated yields agree well with reported district-level observations from 2003 to 2015 (r = 0.85 &amp; MAE = 0.15 t/ha). The model performance improved further when estimating separate models for different rice cropping densities (up to r = 0.93). An additional out-of-sample validation for the years 2016 and 2017, proved successful with r = 0.84 and r = 0.77, respectively. Simulated yield accuracy was higher in water-limited, rainfed agricultural systems. We conclude that this downscaling approach of rice yield estimation using GBR is feasible across India and may complement current approaches for timely rice yield estimation required by insurance companies and government agencies.},
DOI = {10.3390/rs13122379}
}



@Article{infrastructures6060091,
AUTHOR = {Chacon, Pedro J. and Park, Jong-Yoon and Aly, Aly M. and Voyiadjis, George Z. and Choi, Jin-Woo},
TITLE = {A Moving Vehicle Height Monitoring Sensor System for Overheight Impact Avoidance},
JOURNAL = {Infrastructures},
VOLUME = {6},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {91},
URL = {https://www.mdpi.com/2412-3811/6/6/91},
ISSN = {2412-3811},
ABSTRACT = {Bridges, overpasses, and road construction sites with reduced vertical clearance lead to collision threats from crossing vehicles that exceed their clearance due to their inherent height or improper loading. These accidents can pose slight or severe physical damage to property and, primarily, damage to involved individuals in these vehicles or their affected components (i.e., collateral damage around and above an overpass or bridge). Furthermore, the resulting consequences may also incur fatalities, injuries, structural damages, and monetary damages. It has severely impacted the repair and/or replacement costs of the affected structures. Such accidents and consequences have been observed at a national level and could be reduced with a proper implementation of an overheight sensing system to prevent them from happening as often. This paper introduces the design, prototype, and implementation of a low power sensor network to monitor and characterize vehicle height and other characteristics in real time, thus alerting overheight vehicles well in advance of a possible collision.},
DOI = {10.3390/infrastructures6060091}
}



@Article{s21124195,
AUTHOR = {Pilipović, Ratko and Risojević, Vladimir and Božič, Janko and Bulić, Patricio and Lotrič, Uroš},
TITLE = {An Approximate GEMM Unit for Energy-Efficient Object Detection},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4195},
URL = {https://www.mdpi.com/1424-8220/21/12/4195},
PubMedID = {34207295},
ISSN = {1424-8220},
ABSTRACT = {Edge computing brings artificial intelligence algorithms and graphics processing units closer to data sources, making autonomy and energy-efficient processing vital for their design. Approximate computing has emerged as a popular strategy for energy-efficient circuit design, where the challenge is to achieve the best tradeoff between design efficiency and accuracy. The essential operation in artificial intelligence algorithms is the general matrix multiplication (GEMM) operation comprised of matrix multiplication and accumulation. This paper presents an approximate general matrix multiplication (AGEMM) unit that employs approximate multipliers to perform matrix–matrix operations on four-by-four matrices given in sixteen-bit signed fixed-point format. The synthesis of the proposed AGEMM unit to the 45 nm Nangate Open Cell Library revealed that it consumed only up to 36% of the area and 25% of the energy required by the exact general matrix multiplication unit. The AGEMM unit is ideally suited to convolutional neural networks, which can adapt to the error induced in the computation. We evaluated the AGEMM units’ usability for honeybee detection with the YOLOv4-tiny convolutional neural network. The results implied that we can deploy the AGEMM units in convolutional neural networks without noticeable performance degradation. Moreover, the AGEMM unit’s employment can lead to more area- and energy-efficient convolutional neural network processing, which in turn could prolong sensors’ and edge nodes’ autonomy.},
DOI = {10.3390/s21124195}
}



@Article{rs13122388,
AUTHOR = {Peprah, Clement Oppong and Yamashita, Megumi and Yamaguchi, Tomoaki and Sekino, Ryo and Takano, Kyohei and Katsura, Keisuke},
TITLE = {Spatio-Temporal Estimation of Biomass Growth in Rice Using Canopy Surface Model from Unmanned Aerial Vehicle Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2388},
URL = {https://www.mdpi.com/2072-4292/13/12/2388},
ISSN = {2072-4292},
ABSTRACT = {The awareness of spatial and temporal variations in site-specific crop parameters, such as aboveground biomass (total dry weight: (TDW), plant length (PL) and leaf area index (LAI), help in formulating appropriate management decisions. However, conventional monitoring methods rely on time-consuming manual field operations. In this study, the feasibility of using an unmanned aerial vehicle (UAV)-based remote sensing approach for monitoring growth in rice was evaluated using a digital surface model (DSM). Approximately 160 images of paddy fields were captured during each UAV survey campaign over two vegetation seasons. The canopy surface model (CSM) was developed based on the differences observed between each DSM and the first DSM after transplanting. Mean canopy height (CH) was used as a variable for the estimation models of LAI and TDW. The mean CSM of the mesh covering several hills was sufficient to explain the PL (R2 = 0.947). TDW and LAI prediction accuracy of the model were high (relative RMSE of 20.8% and 28.7%, and RMSE of 0.76 m2 m−2 and 141.4 g m−2, respectively) in the rice varieties studied (R2 = 0.937 (Basmati370), 0.837 (Nipponbare and IR64) for TDW, and 0.894 (Basmati370), 0.866 (Nipponbare and IR64) for LAI). The results of this study support the assertion of the benefits of DSM-derived CH for predicting biomass development. In addition, LAI and TDW could be estimated temporally and spatially using the UAV-based CSM, which is not easily affected by weather conditions.},
DOI = {10.3390/rs13122388}
}



@Article{rs13122401,
AUTHOR = {Albuquerque, Rafael Walter and Ferreira, Manuel Eduardo and Olsen, Søren Ingvor and Tymus, Julio Ricardo Caetano and Balieiro, Cintia Palheta and Mansur, Hendrik and Moura, Ciro José Ribeiro and Costa, João Vitor Silva and Branco, Maurício Ruiz Castello and Grohmann, Carlos Henrique},
TITLE = {Forest Restoration Monitoring Protocol with a Low-Cost Remotely Piloted Aircraft: Lessons Learned from a Case Study in the Brazilian Atlantic Forest},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2401},
URL = {https://www.mdpi.com/2072-4292/13/12/2401},
ISSN = {2072-4292},
ABSTRACT = {Traditional forest restoration (FR) monitoring methods employ spreadsheets and photos taken at the ground level. Since remotely piloted aircraft (RPA) generate a panoramic high resolution and georeferenced view of the entire area of interest, this technology has high potential to improve the traditional FR monitoring methods. This study evaluates how low-cost RPA data may contribute to FR monitoring of the Brazilian Atlantic Forest by the automatic remote measurement of Tree Density, Tree Height, Vegetation Cover (area covered by trees), and Grass Infestation. The point cloud data was processed to map the Tree Density, Tree Height, and Vegetation Cover parameters. The orthomosaic was used for a Random Forest classification that considered trees and grasses as a single land cover class. The Grass Infestation parameter was mapped by the difference between this land cover class (which considered trees and grasses) and the Vegetation Cover results (obtained by the point cloud data processing). Tree Density, Vegetation Cover, and Grass Infestation parameters presented F_scores of 0.92, 0.85, and 0.64, respectively. Tree Height accuracy was indicated by the Error Percentage considering the traditional fieldwork and the RPA results. The Error Percentage was equal to 0.13 and was considered accurate because it estimated a 13% shorter height for trees that averaged 1.93 m tall. Thus, this study showed that the FR structural parameters were accurately measured by the low-cost RPA, a technology that contributes to FR monitoring. Despite accurately measuring the structural parameters, this study reinforced the challenge of measuring the Biodiversity parameter via remote sensing because the classification of tree species was not possible. After all, the Brazilian Atlantic Forest is a biodiversity hotspot, and thus different species have similar spectral responses in the visible spectrum and similar geometric forms. Therefore, until improved automatic classification methods become available for tree species, traditional fieldwork remains necessary for a complete FR monitoring diagnostic.},
DOI = {10.3390/rs13122401}
}



@Article{rs13122404,
AUTHOR = {Perich, Gregor and Aasen, Helge and Verrelst, Jochem and Argento, Francesco and Walter, Achim and Liebisch, Frank},
TITLE = {Crop Nitrogen Retrieval Methods for Simulated Sentinel-2 Data Using In-Field Spectrometer Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2404},
URL = {https://www.mdpi.com/2072-4292/13/12/2404},
ISSN = {2072-4292},
ABSTRACT = {Nitrogen (N) is one of the key nutrients supplied in agricultural production worldwide. Over-fertilization can have negative influences on the field and the regional level (e.g., agro-ecosystems). Remote sensing of the plant N of field crops presents a valuable tool for the monitoring of N flows in agro-ecosystems. Available data for validation of satellite-based remote sensing of N is scarce. Therefore, in this study, field spectrometer measurements were used to simulate data of the Sentinel-2 (S2) satellites developed for vegetation monitoring by the ESA. The prediction performance of normalized ratio indices (NRIs), random forest regression (RFR) and Gaussian processes regression (GPR) for plant-N-related traits was assessed on a diverse real-world dataset including multiple crops, field sites and years. The plant N traits included the mass-based N measure, N concentration in the biomass (Nconc), and an area-based N measure approximating the plant N uptake (NUP). Spectral indices such as normalized ratio indices (NRIs) performed well, but the RFR and GPR methods outperformed the NRIs. Key spectral bands for each trait were identified using the RFR variable importance measure and the Gaussian processes regression band analysis tool (GPR-BAT), highlighting the importance of the short-wave infrared (SWIR) region for estimation of plant Nconc—and to a lesser extent the NUP. The red edge (RE) region was also important. The GPR-BAT showed that five bands were sufficient for plant N trait and leaf area index (LAI) estimation and that a surplus of bands effectively reduced prediction performance. A global sensitivity analysis (GSA) was performed on all traits simultaneously, showing the dominance of the LAI in the mixed remote sensing signal. To delineate the plant-N-related traits from this signal, regional and/or national data collection campaigns producing large crop spectral libraries (CSL) are needed. An improved database will likely enable the mapping of N at the agro-ecosystem level or for use in precision farming by farmers in the future.},
DOI = {10.3390/rs13122404}
}



@Article{agriculture11060563,
AUTHOR = {Li, Minhui and Shamshiri, Redmond R. and Schirrmann, Michael and Weltzien, Cornelia},
TITLE = {Impact of Camera Viewing Angle for Estimating Leaf Parameters of Wheat Plants from 3D Point Clouds},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {563},
URL = {https://www.mdpi.com/2077-0472/11/6/563},
ISSN = {2077-0472},
ABSTRACT = {Estimation of plant canopy using low-altitude imagery can help monitor the normal growth status of crops and is highly beneficial for various digital farming applications such as precision crop protection. However, extracting 3D canopy information from raw images requires studying the effect of sensor viewing angle by taking into accounts the limitations of the mobile platform routes inside the field. The main objective of this research was to estimate wheat (Triticum aestivum L.) leaf parameters, including leaf length and width, from the 3D model representation of the plants. For this purpose, experiments with different camera viewing angles were conducted to find the optimum setup of a mono-camera system that would result in the best 3D point clouds. The angle-control analytical study was conducted on a four-row wheat plot with a row spacing of 0.17 m and with two seeding densities and growth stages as factors. Nadir and six oblique view image datasets were acquired from the plot with 88% overlapping and were then reconstructed to point clouds using Structure from Motion (SfM) and Multi-View Stereo (MVS) methods. Point clouds were first categorized into three classes as wheat canopy, soil background, and experimental plot. The wheat canopy class was then used to extract leaf parameters, which were then compared with those values from manual measurements. The comparison between results showed that (i) multiple-view dataset provided the best estimation for leaf length and leaf width, (ii) among the single-view dataset, canopy, and leaf parameters were best modeled with angles vertically at −45° and horizontally at 0° (VA −45, HA 0), while (iii) in nadir view, fewer underlying 3D points were obtained with a missing leaf rate of 70%. It was concluded that oblique imagery is a promising approach to effectively estimate wheat canopy 3D representation with SfM-MVS using a single camera platform for crop monitoring. This study contributes to the improvement of the proximal sensing platform for crop health assessment.},
DOI = {10.3390/agriculture11060563}
}



@Article{s21124226,
AUTHOR = {Baek, Seongmin and Jung, Yunho and Lee, Seongjoo},
TITLE = {Signal Expansion Method in Indoor FMCW Radar Systems for Improving Range Resolution},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4226},
URL = {https://www.mdpi.com/1424-8220/21/12/4226},
PubMedID = {34203035},
ISSN = {1424-8220},
ABSTRACT = {As various unmanned autonomous driving technologies such as autonomous vehicles and autonomous driving drones are being developed, research on FMCW radar, a sensor related to these technologies, is actively being conducted. The range resolution, which is a parameter for accurately detecting an object in the FMCW radar system, depends on the modulation bandwidth. Expensive radars have a large modulation bandwidth, use the band above 77 GHz, and are mainly used as in-vehicle radar sensors. However, these high-performance radars have the disadvantage of being expensive and burdensome for use in areas that require precise sensors, such as indoor environment motion detection and autonomous drones. In this paper, the range resolution is improved beyond the limited modulation bandwidth by extending the beat frequency signal in the time domain through the proposed Adaptive Mirror Padding and Phase Correction Padding. The proposed algorithm has similar performance in the existing Zero Padding, Mirror Padding, and Range RMSE, but improved results were confirmed through the ρs indicating the size of the side lobe compared to the main lobe and the accurate detection rate of the OS CFAR. In the case of ρs, it was confirmed that with single targets, Adaptive Mirror Padding was improved by about 3 times and Phase Correct Padding was improved by about 6 times compared to the existing algorithm. The results of the OS CFAR were divided into single targets and multiple targets to confirm the performance. In single targets, Adaptive Mirror Padding improved by about 10% and Phase Correct Padding by about 20% compared to the existing algorithm. In multiple targets, Phase Correct Padding improved by about 20% compared to the existing algorithm. The proposed algorithm was verified through the MATLAB Tool and the actual FMCW radar. As the results were similar in the two experimental environments, it was verified that the algorithm works in real radar as well.},
DOI = {10.3390/s21124226}
}



@Article{ijerph18136705,
AUTHOR = {Pishgar, Maryam and Issa, Salah Fuad and Sietsema, Margaret and Pratap, Preethi and Darabi, Houshang},
TITLE = {REDECA: A Novel Framework to Review Artificial Intelligence and Its Applications in Occupational Safety and Health},
JOURNAL = {International Journal of Environmental Research and Public Health},
VOLUME = {18},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6705},
URL = {https://www.mdpi.com/1660-4601/18/13/6705},
PubMedID = {34206378},
ISSN = {1660-4601},
ABSTRACT = {Introduction: The field of artificial intelligence (AI) is rapidly expanding, with many applications seen routinely in health care, industry, and education, and increasingly in workplaces. Although there is growing evidence of applications of AI in workplaces across all industries to simplify and/or automate tasks there is a limited understanding of the role that AI contributes in addressing occupational safety and health (OSH) concerns. Methods: This paper introduces a new framework called Risk Evolution, Detection, Evaluation, and Control of Accidents (REDECA) that highlights the role that AI plays in the anticipation and control of exposure risks in a worker’s immediate environment. Two hundred and sixty AI papers across five sectors (oil and gas, mining, transportation, construction, and agriculture) were reviewed using the REDECA framework to highlight current applications and gaps in OSH and AI fields. Results: The REDECA framework highlighted the unique attributes and research focus of each of the five industrial sectors. The majority of evidence of AI in OSH research within the oil/gas and transportation sectors focused on the development of sensors to detect hazardous situations. In construction the focus was on the use of sensors to detect incidents. The research in the agriculture sector focused on sensors and actuators that removed workers from hazardous conditions. Application of the REDECA framework highlighted AI/OSH strengths and opportunities in various industries and potential areas for collaboration. Conclusions: As AI applications across industries continue to increase, further exploration of the benefits and challenges of AI applications in OSH is needed to optimally protect worker health, safety and well-being.},
DOI = {10.3390/ijerph18136705}
}



@Article{rs13132436,
AUTHOR = {Calamita, Federico and Imran, Hafiz Ali and Vescovo, Loris and Mekhalfi, Mohamed Lamine and La Porta, Nicola},
TITLE = {Early Identification of Root Rot Disease by Using Hyperspectral Reflectance: The Case of Pathosystem Grapevine/Armillaria},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2436},
URL = {https://www.mdpi.com/2072-4292/13/13/2436},
ISSN = {2072-4292},
ABSTRACT = {Armillaria genus represents one of the most common causes of chronic root rot disease in woody plants. Prompt recognition of diseased plants is crucial to control the pathogen. However, the current disease detection methods are limited at a field scale. Therefore, an alternative approach is needed. In this study, we investigated the potential of hyperspectral techniques to identify fungi-infected vs. healthy plants of Vitis vinifera. We used the hyperspectral imaging sensor Specim-IQ to acquire leaves’ reflectance data of the Teroldego Rotaliano grapevine cultivar. We analyzed three different groups of plants: healthy, asymptomatic, and diseased. Highly significant differences were found in the near-infrared (NIR) spectral region with a decreasing pattern from healthy to diseased plants attributable to the leaf mesophyll changes. Asymptomatic plants emerged from the other groups due to a lower reflectance in the red edge spectrum (around 705 nm), ascribable to an accumulation of secondary metabolites involved in plant defense strategies. Further significant differences were observed in the wavelengths close to 550 nm in diseased vs. asymptomatic plants. We evaluated several machine learning paradigms to differentiate the plant groups. The Naïve Bayes (NB) algorithm, combined with the most discriminant variables among vegetation indices and spectral narrow bands, provided the best results with an overall accuracy of 90% and 75% in healthy vs. diseased and healthy vs. asymptomatic plants, respectively. To our knowledge, this study represents the first report on the possibility of using hyperspectral data for root rot disease diagnosis in woody plants. Although further validation studies are required, it appears that the spectral reflectance technique, possibly implemented on unmanned aerial vehicles (UAVs), could be a promising tool for a cost-effective, non-invasive method of Armillaria disease diagnosis and mapping in-field, contributing to a significant step forward in precision viticulture.},
DOI = {10.3390/rs13132436}
}



@Article{s21134281,
AUTHOR = {López-Ardao, J. Carlos and Rodríguez-Rubio, Raúl F. and Suárez-González, Andrés and Rodríguez-Pérez, Miguel and Sousa-Vieira, M. Estrella},
TITLE = {Current Trends on Green Wireless Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4281},
URL = {https://www.mdpi.com/1424-8220/21/13/4281},
PubMedID = {34201485},
ISSN = {1424-8220},
ABSTRACT = {The issue of energy balancing in Wireless Sensor Networks is a pivotal one, crucial in their deployment. This problem can be subdivided in three areas: (i) energy conservation techniques, usually implying minimizing the cost of communication at the nodes since it is known that the radio is the biggest consumer of the available energy; (ii) energy-harvesting techniques, converting energy from not full-time available environmental sources and usually storing it; and (iii) energy transfer techniques, sharing energy resources from one node (either specialized or not) to another one. In this article, we survey the main contributions in these three areas and identify the main trending topics in recent research. A discussion and some future directions are also included.},
DOI = {10.3390/s21134281}
}



@Article{ijgi10070426,
AUTHOR = {Lan, Tingting and Qin, Danyang and Sun, Guanyu},
TITLE = {Joint Optimization on Trajectory, Cache Placement, and Transmission Power for Minimum Mission Time in UAV-Aided Wireless Networks},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {426},
URL = {https://www.mdpi.com/2220-9964/10/7/426},
ISSN = {2220-9964},
ABSTRACT = {In recent years, due to the strong mobility, easy deployment, and low cost of unmanned aerial vehicles (UAV), great interest has arisen in utilizing UAVs to assist in wireless communication, especially for on-demand deployment in emergency situations and temporary events. However, UAVs can only provide users with data transmission services through wireless backhaul links established with a ground base station, and the limited capacity of the wireless backhaul link would limit the transmission speed of UAVs. Therefore, this paper designed a UAV-assisted wireless communication system that used cache technology and realized the transmission of multi-user data by using the mobility of UAVs and wireless cache technology. Considering the limited storage space and energy of UAVs, the joint optimization problem of the UAV’s trajectory, cache placement, and transmission power was established to minimize the mission time of the UAV. Since this problem was a non-convex problem, it was decomposed into three sub-problems: trajectory optimization, cache placement optimization, and power allocation optimization. An iterative algorithm based on the successive convex approximation and alternate optimization techniques was proposed to solve these three optimization problems. Finally, in the power allocation optimization, the proposed algorithm was improved by changing the optimization objective function. Numerical results showed that the algorithm had good performance and could effectively reduce the task completion time of the UAV.},
DOI = {10.3390/ijgi10070426}
}



@Article{rs13132450,
AUTHOR = {Maxwell, Aaron E. and Warner, Timothy A. and Guillén, Luis Andrés},
TITLE = {Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies—Part 1: Literature Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2450},
URL = {https://www.mdpi.com/2072-4292/13/13/2450},
ISSN = {2072-4292},
ABSTRACT = {Convolutional neural network (CNN)-based deep learning (DL) is a powerful, recently developed image classification approach. With origins in the computer vision and image processing communities, the accuracy assessment methods developed for CNN-based DL use a wide range of metrics that may be unfamiliar to the remote sensing (RS) community. To explore the differences between traditional RS and DL RS methods, we surveyed a random selection of 100 papers from the RS DL literature. The results show that RS DL studies have largely abandoned traditional RS accuracy assessment terminology, though some of the accuracy measures typically used in DL papers, most notably precision and recall, have direct equivalents in traditional RS terminology. Some of the DL accuracy terms have multiple names, or are equivalent to another measure. In our sample, DL studies only rarely reported a complete confusion matrix, and when they did so, it was even more rare that the confusion matrix estimated population properties. On the other hand, some DL studies are increasingly paying attention to the role of class prevalence in designing accuracy assessment approaches. DL studies that evaluate the decision boundary threshold over a range of values tend to use the precision-recall (P-R) curve, the associated area under the curve (AUC) measures of average precision (AP) and mean average precision (mAP), rather than the traditional receiver operating characteristic (ROC) curve and its AUC. DL studies are also notable for testing the generalization of their models on entirely new datasets, including data from new areas, new acquisition times, or even new sensors.},
DOI = {10.3390/rs13132450}
}



@Article{diagnostics11071155,
AUTHOR = {El-Rashidy, Nora and Abdelrazik, Samir and Abuhmed, Tamer and Amer, Eslam and Ali, Farman and Hu, Jong-Wan and El-Sappagh, Shaker},
TITLE = {Comprehensive Survey of Using Machine Learning in the COVID-19 Pandemic},
JOURNAL = {Diagnostics},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1155},
URL = {https://www.mdpi.com/2075-4418/11/7/1155},
PubMedID = {34202587},
ISSN = {2075-4418},
ABSTRACT = {Since December 2019, the global health population has faced the rapid spreading of coronavirus disease (COVID-19). With the incremental acceleration of the number of infected cases, the World Health Organization (WHO) has reported COVID-19 as an epidemic that puts a heavy burden on healthcare sectors in almost every country. The potential of artificial intelligence (AI) in this context is difficult to ignore. AI companies have been racing to develop innovative tools that contribute to arm the world against this pandemic and minimize the disruption that it may cause. The main objective of this study is to survey the decisive role of AI as a technology used to fight against the COVID-19 pandemic. Five significant applications of AI for COVID-19 were found, including (1) COVID-19 diagnosis using various data types (e.g., images, sound, and text); (2) estimation of the possible future spread of the disease based on the current confirmed cases; (3) association between COVID-19 infection and patient characteristics; (4) vaccine development and drug interaction; and (5) development of supporting applications. This study also introduces a comparison between current COVID-19 datasets. Based on the limitations of the current literature, this review highlights the open research challenges that could inspire the future application of AI in COVID-19.},
DOI = {10.3390/diagnostics11071155}
}



@Article{rs13132465,
AUTHOR = {Koz, Alper and Efe, Ufuk},
TITLE = {Geometric- and Optimization-Based Registration Methods for Long-Wave Infrared Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2465},
URL = {https://www.mdpi.com/2072-4292/13/13/2465},
ISSN = {2072-4292},
ABSTRACT = {Registration of long-wave infrared (LWIR) hyperspectral images with their thermal and emissivity components has until now received comparatively less attention with respect to the visible near and short wave infrared hyperspectral images. In this paper, the registration of LWIR hyperspectral images is investigated to enhance applications of LWIR images such as change detection, temperature and emissivity separation, and target detection. The proposed approach first searches for the best features of hyperspectral image pixels for extraction and matching in the LWIR range and then performs a global registration over two-dimensional maps of three-dimensional hyperspectral cubes. The performances of temperature and emissivity features in the thermal domain along with the average energy and principal components of spectral radiance are investigated. The global registration performed over whole 2D maps is further improved by blockwise local refinements. Among the two proposed approaches, the geometric refinement seeks the best keypoint combination in the neighborhood of each block to estimate the transformation for that block. The alternative optimization-based refinement iteratively finds the best transformation by maximizing the similarity of the reference and transformed blocks. The possible blocking artifacts due to blockwise mapping are finally eliminated by pixelwise refinement. The experiments are evaluated with respect to the (i) utilized similarity metrics in the LWIR range between transformed and reference blocks, (ii) proposed geometric- and optimization-based methods, and (iii) image pairs captured on the same and different days. The better performance of the proposed approach compared to manual, GPU-IMU-based, and state-of-the-art image registration methods is verified.},
DOI = {10.3390/rs13132465}
}



@Article{rs13132468,
AUTHOR = {Anochi, Juliana Aparecida and de Almeida, Vinícius Albuquerque and de Campos Velho, Haroldo Fraga},
TITLE = {Machine Learning for Climate Precipitation Prediction Modeling over South America},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2468},
URL = {https://www.mdpi.com/2072-4292/13/13/2468},
ISSN = {2072-4292},
ABSTRACT = {Many natural disasters in South America are linked to meteorological phenomena. Therefore, forecasting and monitoring climatic events are fundamental issues for society and various sectors of the economy. In the last decades, machine learning models have been developed to tackle different issues in society, but there is still a gap in applications to applied physics. Here, different machine learning models are evaluated for precipitation prediction over South America. Currently, numerical weather prediction models are unable to precisely reproduce the precipitation patterns in South America due to many factors such as the lack of region-specific parametrizations and data availability. The results are compared to the general circulation atmospheric model currently used operationally in the National Institute for Space Research (INPE: Instituto Nacional de Pesquisas Espaciais), Brazil. Machine learning models are able to produce predictions with errors under 2 mm in most of the continent in comparison to satellite-observed precipitation patterns for different climate seasons, and also outperform INPE’s model for some regions (e.g., reduction of errors from 8 to 2 mm in central South America in winter). Another advantage is the computational performance from machine learning models, running faster with much lower computer resources than models based on differential equations currently used in operational centers. Therefore, it is important to consider machine learning models for precipitation forecasts in operational centers as a way to improve forecast quality and to reduce computation costs.},
DOI = {10.3390/rs13132468}
}



@Article{electronics10131539,
AUTHOR = {Ding, Qianao and Zhu, Rongbo and Liu, Hao and Ma, Maode},
TITLE = {An Overview of Machine Learning-Based Energy-Efficient Routing Algorithms in Wireless Sensor Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1539},
URL = {https://www.mdpi.com/2079-9292/10/13/1539},
ISSN = {2079-9292},
ABSTRACT = {Machine learning (ML) technology has shown its unique advantages in many fields and has excellent performance in many applications, such as image recognition, speech recognition, recommendation systems, and natural language processing. Recently, the applicability of ML in wireless sensor networks (WSNs) has attracted much attention. As resources are limited in WSNs, identifying how to improve resource utilization and achieve power-efficient load balancing is becoming a critical issue in WSNs. Traditional green routing algorithms aim to achieve this by reducing energy consumption and prolonging network lifetime through optimized routing schemes in WSNs. However, there are usually problems such as poor flexibility, a single consideration factor, and a reliance on accurate mathematical models. ML techniques can quickly adapt to environmental changes and integrate multiple factors for routing decisions, which provides new ideas for intelligent energy-efficient routing algorithms in WSNs. In this paper, we survey and propose a theoretical hypothetic model formulation of ML as an effective method for creating a power-efficient green routing model that can overcome the limitations of traditional green routing methods. In addition, the study also provides an overview of past, present, and future progress in green routing schemes in WSNs. The contents of this paper will appeal to a wide range of audiences interested in ML-based WSNs.},
DOI = {10.3390/electronics10131539}
}



@Article{jsan10030040,
AUTHOR = {Helfer, Gilson Augusto and Barbosa, Jorge Luis Victória and Alves, Douglas and da Costa, Adilson Ben and Beko, Marko and Leithardt, Valderi Reis Quietinho},
TITLE = {Multispectral Cameras and Machine Learning Integrated into Portable Devices as Clay Prediction Technology},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {40},
URL = {https://www.mdpi.com/2224-2708/10/3/40},
ISSN = {2224-2708},
ABSTRACT = {The present work proposed a low-cost portable device as an enabling technology for agriculture using multispectral imaging and machine learning in soil texture. Clay is an important factor for the verification and monitoring of soil use due to its fast reaction to chemical and surface changes. The system developed uses the analysis of reflectance in wavebands for clay prediction. The selection of each wavelength is performed through an LED lamp panel. A NoIR microcamera controlled by a Raspberry Pi device is employed to acquire the image and unfold it in RGB histograms. Results showed a good prediction performance with R2 of 0.96, RMSEC of 3.66% and RMSECV of 16.87%. The high portability allows the equipment to be used in a field providing strategic information related to soil sciences.},
DOI = {10.3390/jsan10030040}
}



@Article{app11135911,
AUTHOR = {Martos, Vanesa and Ahmad, Ali and Cartujo, Pedro and Ordoñez, Javier},
TITLE = {Ensuring Agricultural Sustainability through Remote Sensing in the Era of Agriculture 5.0},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5911},
URL = {https://www.mdpi.com/2076-3417/11/13/5911},
ISSN = {2076-3417},
ABSTRACT = {Timely and reliable information about crop management, production, and yield is considered of great utility by stakeholders (e.g., national and international authorities, farmers, commercial units, etc.) to ensure food safety and security. By 2050, according to Food and Agriculture Organization (FAO) estimates, around 70% more production of agricultural products will be needed to fulfil the demands of the world population. Likewise, to meet the Sustainable Development Goals (SDGs), especially the second goal of “zero hunger”, potential technologies like remote sensing (RS) need to be efficiently integrated into agriculture. The application of RS is indispensable today for a highly productive and sustainable agriculture. Therefore, the present study draws a general overview of RS technology with a special focus on the principal platforms of this technology, i.e., satellites and remotely piloted aircrafts (RPAs), and the sensors used, in relation to the 5th industrial revolution. Nevertheless, since 1957, RS technology has found applications, through the use of satellite imagery, in agriculture, which was later enriched by the incorporation of remotely piloted aircrafts (RPAs), which is further pushing the boundaries of proficiency through the upgrading of sensors capable of higher spectral, spatial, and temporal resolutions. More prominently, wireless sensor technologies (WST) have streamlined real time information acquisition and programming for respective measures. Improved algorithms and sensors can, not only add significant value to crop data acquisition, but can also devise simulations on yield, harvesting and irrigation periods, metrological data, etc., by making use of cloud computing. The RS technology generates huge sets of data that necessitate the incorporation of artificial intelligence (AI) and big data to extract useful products, thereby augmenting the adeptness and efficiency of agriculture to ensure its sustainability. These technologies have made the orientation of current research towards the estimation of plant physiological traits rather than the structural parameters possible. Futuristic approaches for benefiting from these cutting-edge technologies are discussed in this study. This study can be helpful for researchers, academics, and young students aspiring to play a role in the achievement of sustainable agriculture.},
DOI = {10.3390/app11135911}
}



@Article{rs13132483,
AUTHOR = {Meng, Baoping and Yang, Zhigui and Yu, Hongyan and Qin, Yu and Sun, Yi and Zhang, Jianguo and Chen, Jianjun and Wang, Zhiwei and Zhang, Wei and Li, Meng and Lv, Yanyan and Yi, Shuhua},
TITLE = {Mapping of Kobresia pygmaea Community Based on Umanned Aerial Vehicle Technology and Gaofen Remote Sensing Data in Alpine Meadow Grassland: A Case Study in Eastern of Qinghai–Tibetan Plateau},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2483},
URL = {https://www.mdpi.com/2072-4292/13/13/2483},
ISSN = {2072-4292},
ABSTRACT = {The Kobresia pygmaea (KP) community is a key succession stage of alpine meadow degradation on the Qinghai–Tibet Plateau (QTP). However, most of the grassland classification and mapping studies have been performed at the grassland type level. The spatial distribution and impact factors of KP on the QTP are still unclear. In this study, field measurements of the grassland vegetation community in the eastern part of the QTP (Counties of Zeku, Henan and Maqu) from 2015 to 2019 were acquired using unmanned aerial vehicle (UAV) technology. The machine learning algorithms for grassland vegetation community classification were constructed by combining Gaofen satellite images and topographic indices. Then, the spatial distribution of KP community was mapped. The results showed that: (1) For all field observed sites, the alpine meadow vegetation communities demonstrated a considerable spatial heterogeneity. The traditional classification methods can hardly distinguish those communities due to the high similarity of their spectral characteristics. (2) The random forest method based on the combination of satellite vegetation indices, texture feature and topographic indices exhibited the best performance in three counties, with overall accuracy and Kappa coefficient ranged from 74.06% to 83.92% and 0.65 to 0.80, respectively. (3) As a whole, the area of KP community reached 1434.07 km2, and accounted for 7.20% of the study area. We concluded that the combination of satellite remote sensing, UAV surveying and machine learning can be used for KP classification and mapping at community level.},
DOI = {10.3390/rs13132483}
}



@Article{rs13132482,
AUTHOR = {Zamboni, Pedro and Junior, José Marcato and Silva, Jonathan de Andrade and Miyoshi, Gabriela Takahashi and Matsubara, Edson Takashi and Nogueira, Keiller and Gonçalves, Wesley Nunes},
TITLE = {Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2482},
URL = {https://www.mdpi.com/2072-4292/13/13/2482},
ISSN = {2072-4292},
ABSTRACT = {Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 × 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications.},
DOI = {10.3390/rs13132482}
}



@Article{rs13132486,
AUTHOR = {Ouhami, Maryam and Hafiane, Adel and Es-Saady, Youssef and El Hajji, Mohamed and Canals, Raphael},
TITLE = {Computer Vision, IoT and Data Fusion for Crop Disease Detection Using Machine Learning: A Survey and Ongoing Research},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2486},
URL = {https://www.mdpi.com/2072-4292/13/13/2486},
ISSN = {2072-4292},
ABSTRACT = {Crop diseases constitute a serious issue in agriculture, affecting both quality and quantity of agriculture production. Disease control has been a research object in many scientific and technologic domains. Technological advances in sensors, data storage, computing resources and artificial intelligence have shown enormous potential to control diseases effectively. A growing body of literature recognizes the importance of using data from different types of sensors and machine learning approaches to build models for detection, prediction, analysis, assessment, etc. However, the increasing number and diversity of research studies requires a literature review for further developments and contributions in this area. This paper reviews state-of-the-art machine learning methods that use different data sources, applied to plant disease detection. It lists traditional and deep learning methods associated with the main data acquisition modalities, namely IoT, ground imaging, unmanned aerial vehicle imaging and satellite imaging. In addition, this study examines the role of data fusion for ongoing research in the context of disease detection. It highlights the advantage of intelligent data fusion techniques, from heterogeneous data sources, to improve plant health status prediction and presents the main challenges facing this field. The study concludes with a discussion of several current issues and research trends.},
DOI = {10.3390/rs13132486}
}



@Article{app11135928,
AUTHOR = {Karaaslan, Enes and Bagci, Ulas and Catbas, Necati},
TITLE = {A Novel Decision Support System for Long-Term Management of Bridge Networks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5928},
URL = {https://www.mdpi.com/2076-3417/11/13/5928},
ISSN = {2076-3417},
ABSTRACT = {Developing a bridge management strategy at the network level with efficient use of capital is very important for optimal infrastructure remediation. This paper introduces a novel decision support system that considers many aspects of bridge management and successfully implements the investigated methodology in a web-based platform. The proposed decision support system uses advanced prediction models, decision trees, and incremental machine learning algorithms to generate an optimal decision strategy. The system aims to achieve adaptive and flexible decision making while entailing powerful utilization of nondestructive evaluation (NDE) methods. The NDE data integration and visualization allow automatic retrieval of inspection results and overlaying the defects on a 3D bridge model. Furthermore, a deep learning-based damage growth prediction model estimates the future condition of the bridge elements and utilizes this information in the decision-making process. The decision ranking takes into account a wide range of factors including structural safety, serviceability, rehabilitation cost, life cycle cost, and societal and political factors to generate optimal maintenance strategies with multiple decision alternatives. This study aims to bring a complementary solution to currently in-use systems with the utilization of advanced machine-learning models and NDE data integration while still equipped with main bridge management functions of bridge management systems and capable of transferring data to other systems.},
DOI = {10.3390/app11135928}
}



@Article{e23070812,
AUTHOR = {Fu, Wei and Yu, Shuang and Wang, Xin},
TITLE = {A Novel Method to Determine Basic Probability Assignment Based on Adaboost and Its Application in Classification},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {812},
URL = {https://www.mdpi.com/1099-4300/23/7/812},
PubMedID = {34202212},
ISSN = {1099-4300},
ABSTRACT = {In the framework of evidence theory, one of the open and crucial issues is how to determine the basic probability assignment (BPA), which is directly related to whether the decision result is correct. This paper proposes a novel method for obtaining BPA based on Adaboost. The method uses training data to generate multiple strong classifiers for each attribute model, which is used to determine the BPA of the singleton proposition since the weights of classification provide necessary information for fundamental hypotheses. The BPA of the composite proposition is quantified by calculating the area ratio of the singleton proposition’s intersection region. The recursive formula of the area ratio of the intersection region is proposed, which is very useful for computer calculation. Finally, BPAs are combined by Dempster’s rule of combination. Using the proposed method to classify the Iris dataset, the experiment concludes that the total recognition rate is 96.53% and the classification accuracy is 90% when the training percentage is 10%. For the other datasets, the experiment results also show that the proposed method is reasonable and effective, and the proposed method performs well in the case of insufficient samples.},
DOI = {10.3390/e23070812}
}



@Article{s21134363,
AUTHOR = {Nabwire, Shona and Suh, Hyun-Kwon and Kim, Moon S. and Baek, Insuck and Cho, Byoung-Kwan},
TITLE = {Review: Application of Artificial Intelligence in Phenomics},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4363},
URL = {https://www.mdpi.com/1424-8220/21/13/4363},
PubMedID = {34202291},
ISSN = {1424-8220},
ABSTRACT = {Plant phenomics has been rapidly advancing over the past few years. This advancement is attributed to the increased innovation and availability of new technologies which can enable the high-throughput phenotyping of complex plant traits. The application of artificial intelligence in various domains of science has also grown exponentially in recent years. Notably, the computer vision, machine learning, and deep learning aspects of artificial intelligence have been successfully integrated into non-invasive imaging techniques. This integration is gradually improving the efficiency of data collection and analysis through the application of machine and deep learning for robust image analysis. In addition, artificial intelligence has fostered the development of software and tools applied in field phenotyping for data collection and management. These include open-source devices and tools which are enabling community driven research and data-sharing, thereby availing the large amounts of data required for the accurate study of phenotypes. This paper reviews more than one hundred current state-of-the-art papers concerning AI-applied plant phenotyping published between 2010 and 2020. It provides an overview of current phenotyping technologies and the ongoing integration of artificial intelligence into plant phenotyping. Lastly, the limitations of the current approaches/methods and future directions are discussed.},
DOI = {10.3390/s21134363}
}



@Article{drones5030054,
AUTHOR = {Casabianca, Pietro and Zhang, Yu},
TITLE = {Acoustic-Based UAV Detection Using Late Fusion of Deep Neural Networks},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2504-446X/5/3/54},
ISSN = {2504-446X},
ABSTRACT = {Multirotor UAVs have become ubiquitous in commercial and public use. As they become more affordable and more available, the associated security risks further increase, especially in relation to airspace breaches and the danger of drone-to-aircraft collisions. Thus, robust systems must be set in place to detect and deal with hostile drones. This paper investigates the use of deep learning methods to detect UAVs using acoustic signals. Deep neural network models are trained with mel-spectrograms as inputs. In this case, Convolutional Neural Networks (CNNs) are shown to be the better performing network, compared with Recurrent Neural Networks (RNNs) and Convolutional Recurrent Neural Networks (CRNNs). Furthermore, late fusion methods have been evaluated using an ensemble of deep neural networks, where the weighted soft voting mechanism has achieved the highest average accuracy of 94.7%, which has outperformed the solo models. In future work, the developed late fusion technique could be utilized with radar and visual methods to further improve the UAV detection performance.},
DOI = {10.3390/drones5030054}
}



@Article{electronics10131549,
AUTHOR = {Shrestha, Rakesh and Omidkar, Atefeh and Roudi, Sajjad Ahmadi and Abbas, Robert and Kim, Shiho},
TITLE = {Machine-Learning-Enabled Intrusion Detection System for Cellular Connected UAV Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1549},
URL = {https://www.mdpi.com/2079-9292/10/13/1549},
ISSN = {2079-9292},
ABSTRACT = {The recent development and adoption of unmanned aerial vehicles (UAVs) is due to its wide variety of applications in public and private sector from parcel delivery to wildlife conservation. The integration of UAVs, 5G, and satellite technologies has prompted telecommunication networks to evolve to provide higher-quality and more stable service to remote areas. However, security concerns with UAVs are growing as UAV nodes are becoming attractive targets for cyberattacks due to enormously growing volumes and poor and weak inbuilt security. In this paper, we propose a UAV- and satellite-based 5G-network security model that can harness machine learning to effectively detect of vulnerabilities and cyberattacks. The solution is divided into two main parts: the model creation for intrusion detection using various machine learning (ML) algorithms and the implementation of ML-based model into terrestrial or satellite gateways. The system identifies various attack types using realistic CSE-CIC IDS-2018 network datasets published by Canadian Establishment for Cybersecurity (CIC). It consists of seven different types of new and contemporary attack types. This paper demonstrates that ML algorithms can be used to classify benign or malicious packets in UAV networks to enhance security. Finally, the tested ML algorithms are compared for effectiveness in terms of accuracy rate, precision, recall, F1-score, and false-negative rate. The decision tree algorithm performed well by obtaining a maximum accuracy rate of 99.99% and a minimum false negative rate of 0% in detecting various attacks as compared to all other types of ML classifiers.},
DOI = {10.3390/electronics10131549}
}



@Article{rs13132496,
AUTHOR = {Khoroshevsky, Faina and Khoroshevsky, Stanislav and Bar-Hillel, Aharon},
TITLE = {Parts-per-Object Count in Agricultural Images: Solving Phenotyping Problems via a Single Deep Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2496},
URL = {https://www.mdpi.com/2072-4292/13/13/2496},
ISSN = {2072-4292},
ABSTRACT = {Solving many phenotyping problems involves not only automatic detection of objects in an image, but also counting the number of parts per object. We propose a solution in the form of a single deep network, tested for three agricultural datasets pertaining to bananas-per-bunch, spikelets-per-wheat-spike, and berries-per-grape-cluster. The suggested network incorporates object detection, object resizing, and part counting as modules in a single deep network, with several variants tested. The detection module is based on a Retina-Net architecture, whereas for the counting modules, two different architectures are examined: the first based on direct regression of the predicted count, and the other on explicit parts detection and counting. The results are promising, with the mean relative deviation between estimated and visible part count in the range of 9.2% to 11.5%. Further inference of count-based yield related statistics is considered. For banana bunches, the actual banana count (including occluded bananas) is inferred from the count of visible bananas. For spikelets-per-wheat-spike, robust estimation methods are employed to get the average spikelet count across the field, which is an effective yield estimator.},
DOI = {10.3390/rs13132496}
}



@Article{app11135941,
AUTHOR = {Lee, Mun-yong and Lee, Sang-ha and Jung, Kye-dong and Lee, Seung-hyun and Kwon, Soon-chul},
TITLE = {A Novel Preprocessing Method for Dynamic Point-Cloud Compression},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5941},
URL = {https://www.mdpi.com/2076-3417/11/13/5941},
ISSN = {2076-3417},
ABSTRACT = {Computer-based data processing capabilities have evolved to handle a lot of information. As such, the complexity of three-dimensional (3D) models (e.g., animations or real-time voxels) containing large volumes of information has increased exponentially. This rapid increase in complexity has led to problems with recording and transmission. In this study, we propose a method of efficiently managing and compressing animation information stored in the 3D point-clouds sequence. A compressed point-cloud is created by reconfiguring the points based on their voxels. Compared with the original point-cloud, noise caused by errors is removed, and a preprocessing procedure that achieves high performance in a redundant processing algorithm is proposed. The results of experiments and rendering demonstrate an average file-size reduction of 40% using the proposed algorithm. Moreover, 13% of the over-lap data are extracted and removed, and the file size is further reduced.},
DOI = {10.3390/app11135941}
}



@Article{s21134384,
AUTHOR = {Liu, Weihua and Zeng, Shan and Wu, Guiju and Li, Hao and Chen, Feifei},
TITLE = {Rice Seed Purity Identification Technology Using Hyperspectral Image with LASSO Logistic Regression Model},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4384},
URL = {https://www.mdpi.com/1424-8220/21/13/4384},
PubMedID = {34206783},
ISSN = {1424-8220},
ABSTRACT = {Hyperspectral technology is used to obtain spectral and spatial information of samples simultaneously and demonstrates significant potential for use in seed purity identification. However, it has certain limitations, such as high acquisition cost and massive redundant information. This study integrates the advantages of the sparse feature of the least absolute shrinkage and selection operator (LASSO) algorithm and the classification feature of the logistic regression model (LRM). We propose a hyperspectral rice seed purity identification method based on the LASSO logistic regression model (LLRM). The feasibility of using LLRM for the selection of feature wavelength bands and seed purity identification are discussed using four types of rice seeds as research objects. The results of 13 different adulteration cases revealed that the value of the regularisation parameter was different in each case. The recognition accuracy of LLRM and average recognition accuracy were 91.67–100% and 98.47%, respectively. Furthermore, the recognition accuracy of full-band LRM was 71.60–100%. However, the average recognition accuracy was merely 89.63%. These results indicate that LLRM can select the feature wavelength bands stably and improve the recognition accuracy of rice seeds, demonstrating the feasibility of developing a hyperspectral technology with LLRM for seed purity identification.},
DOI = {10.3390/s21134384}
}



@Article{rs13132501,
AUTHOR = {Rahimzad, Maryam and Homayouni, Saeid and Alizadeh Naeini, Amin and Nadi, Saeed},
TITLE = {An Efficient Multi-Sensor Remote Sensing Image Clustering in Urban Areas via Boosted Convolutional Autoencoder (BCAE)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2501},
URL = {https://www.mdpi.com/2072-4292/13/13/2501},
ISSN = {2072-4292},
ABSTRACT = {High-resolution urban image clustering has remained a challenging task. This is mainly because its performance strongly depends on the discrimination power of features. Recently, several studies focused on unsupervised learning methods by autoencoders to learn and extract more efficient features for clustering purposes. This paper proposes a Boosted Convolutional AutoEncoder (BCAE) method based on feature learning for efficient urban image clustering. The proposed method was applied to multi-sensor remote-sensing images through a multistep workflow. The optical data were first preprocessed by applying a Minimum Noise Fraction (MNF) transformation. Then, these MNF features, in addition to the normalized Digital Surface Model (nDSM) and vegetation indexes such as Normalized Difference Vegetation Index (NDVI) and Excess Green (ExG(2)), were used as the inputs of the BCAE model. Next, our proposed convolutional autoencoder was trained to automatically encode upgraded features and boost the hand-crafted features for producing more clustering-friendly ones. Then, we employed the Mini Batch K-Means algorithm to cluster deep features. Finally, the comparative feature sets were manually designed in three modes to prove the efficiency of the proposed method in extracting compelling features. Experiments on three datasets show the efficiency of BCAE for feature learning. According to the experimental results, by applying the proposed method, the ultimate features become more suitable for clustering, and spatial correlation among the pixels in the feature learning process is also considered.},
DOI = {10.3390/rs13132501}
}



@Article{rs13132517,
AUTHOR = {Wang, Lijun and Wang, Jiayao and Qin, Fen},
TITLE = {Feature Fusion Approach for Temporal Land Use Mapping in Complex Agricultural Areas},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2517},
URL = {https://www.mdpi.com/2072-4292/13/13/2517},
ISSN = {2072-4292},
ABSTRACT = {Accurate temporal land use mapping provides important and timely information for decision making for large-scale management of land and crop production. At present, temporal land cover and crop classifications within a study area have neglected the differences between subregions. In this paper, we propose a classification rule by integrating the terrain, time series characteristics, priority, and seasonality (TTPSR) with Sentinel-2 satellite imagery. Based on the time series of Normalized Difference Water Index (NDWI) and Vegetation Index (NDVI), a dynamic decision tree for forests, cultivation, urban, and water was created in Google Earth Engine (GEE) for each subregion to extract cultivated land. Then, with or without this cultivated land mask data, the original classification results for each subregion were completed based on composite image acquisition with five vegetation indices using Random Forest. During the post-reclassification process, a 4-bit coding rule based on terrain, type, seasonal rhythm, and priority was generated by analyzing the characteristics of the original results. Finally, statistical results and temporal mapping were processed. The results showed that feature importance was dominated by B2, NDWI, RENDVI, B11, and B12 over winter, and B11, B12, NDBI, B2, and B8A over summer. Meanwhile, the cultivated land mask improved the overall accuracy for multicategories (seven to eight and nine to 13 during winter and summer, respectively) in each subregion, with average ranges in the overall accuracy for winter and summer of 0.857–0.935 and 0.873–0.963, respectively, and kappa coefficients of 0.803–0.902 and 0.835–0.950, respectively. The analysis of the above results and the comparison with resampling plots identified various sources of error for classification accuracy, including spectral differences, degree of field fragmentation, and planting complexity. The results demonstrated the capability of the TTPSR rule in temporal land use mapping, especially with regard to complex crops classification and automated post-processing, thereby providing a viable option for large-scale land use mapping.},
DOI = {10.3390/rs13132517}
}



@Article{s21134408,
AUTHOR = {Salehi Hikouei, Iman and Kim, S. Sonny and Mishra, Deepak R.},
TITLE = {Machine-Learning Classification of Soil Bulk Density in Salt Marsh Environments},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4408},
URL = {https://www.mdpi.com/1424-8220/21/13/4408},
PubMedID = {34199102},
ISSN = {1424-8220},
ABSTRACT = {Remotely sensed data from both in situ and satellite platforms in visible, near-infrared, and shortwave infrared (VNIR–SWIR, 400–2500 nm) regions have been widely used to characterize and model soil properties in a direct, cost-effective, and rapid manner at different scales. In this study, we assess the performance of machine-learning algorithms including random forest (RF), extreme gradient boosting machines (XGBoost), and support vector machines (SVM) to model salt marsh soil bulk density using multispectral remote-sensing data from the Landsat-7 Enhanced Thematic Mapper Plus (ETM+) platform. To our knowledge, use of remote-sensing data for estimating salt marsh soil bulk density at the vegetation rooting zone has not been investigated before. Our study reveals that blue (band 1; 450–520 nm) and NIR (band 4; 770–900 nm) bands of Landsat-7 ETM+ ranked as the most important spectral features for bulk density prediction by XGBoost and RF, respectively. According to XGBoost, band 1 and band 4 had relative importance of around 41% and 39%, respectively. We tested two soil bulk density classes in order to differentiate salt marshes in terms of their capability to support vegetation that grows in either low (0.032 to 0.752 g/cm3) or high (0.752 g/cm3 to 1.893 g/cm3) bulk density areas. XGBoost produced a higher classification accuracy (88%) compared to RF (87%) and SVM (86%), although discrepancies in accuracy between these models were small (&lt;2%). XGBoost correctly classified 178 out of 186 soil samples labeled as low bulk density and 37 out of 62 soil samples labeled as high bulk density. We conclude that remote-sensing-based machine-learning models can be a valuable tool for ecologists and engineers to map the soil bulk density in wetlands to select suitable sites for effective restoration and successful re-establishment practices.},
DOI = {10.3390/s21134408}
}



@Article{s21134417,
AUTHOR = {Ukaegbu, Uchechi F. and Tartibu, Lagouge K. and Okwu, Modestus O. and Olayode, Isaac O.},
TITLE = {Development of a Light-Weight Unmanned Aerial Vehicle for Precision Agriculture},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4417},
URL = {https://www.mdpi.com/1424-8220/21/13/4417},
PubMedID = {34203187},
ISSN = {1424-8220},
ABSTRACT = {This paper describes the development of a modular unmanned aerial vehicle for the detection and eradication of weeds on farmland. Precision agriculture entails solving the problem of poor agricultural yield due to competition for nutrients by weeds and provides a faster approach to eliminating the problematic weeds using emerging technologies. This research has addressed the aforementioned problem. A quadcopter was built, and components were assembled with light-weight materials. The system consists of the electric motor, electronic speed controller, propellers, frame, lithium polymer (li-po) battery, flight controller, a global positioning system (GPS), and receiver. A sprayer module which consists of a relay, Raspberry Pi 3, spray pump, 12 V DC source, water hose, and the tank was built. It operated in such a way that when a weed is detected based on the deep learning algorithms deployed on the Raspberry Pi, general purpose input/output (GPIO) 17 or GPIO 18 (of the Raspberry Pi) were activated to supply 3.3 V, which turned on a DC relay to spray herbicides accordingly. The sprayer module was mounted on the quadcopter and from the test-running operation conducted, broadleaf and grass weeds were accurately detected and the spraying of herbicides according to the weed type occurred in less than a second.},
DOI = {10.3390/s21134417}
}



@Article{rs13132520,
AUTHOR = {Ma, Dongdong and Rehman, Tanzeel U. and Zhang, Libo and Maki, Hideki and Tuinstra, Mitchell R. and Jin, Jian},
TITLE = {Modeling of Environmental Impacts on Aerial Hyperspectral Images for Corn Plant Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2520},
URL = {https://www.mdpi.com/2072-4292/13/13/2520},
ISSN = {2072-4292},
ABSTRACT = {Aerial imaging technologies have been widely applied in agricultural plant remote sensing. However, an as yet unexplored challenge with field imaging is that the environmental conditions, such as sun angle, cloud coverage, temperature, and so on, can significantly alter plant appearance and thus affect the imaging sensor’s accuracy toward extracting plant feature measurements. These image alterations result from the complicated interaction between the real-time environments and plants. Analysis of these impacts requires continuous monitoring of the changes through various environmental conditions, which has been difficult with current aerial remote sensing systems. This paper aimed to propose a modeling method to comprehensively understand and model the environmental influences on hyperspectral imaging data. In 2019, a fixed hyperspectral imaging gantry was constructed in Purdue University’s research farm, and over 8000 repetitive images of the same corn field were taken with a 2.5 min interval for 31 days. Time-tagged local environment data, including solar zenith angle, solar irradiation, temperature, wind speed, and so on, were also recorded during the imaging time. The images were processed for phenotyping data, and the time series decomposition method was applied to extract the phenotyping data variation caused by the changing environments. An artificial neural network (ANN) was then built to model the relationship between the phenotyping data variation and environmental changes. The ANN model was able to accurately predict the environmental effects in remote sensing results, and thus could be used to effectively eliminate the environment-induced variation in the phenotyping features. The test of the normalized difference vegetation index (NDVI) calculated from the hyperspectral images showed that variance in NDVI was reduced by 79%. A similar performance was confirmed with the relative water content (RWC) predictions. Therefore, this modeling method shows great potential for application in aerial remote sensing applications in agriculture, to significantly improve the imaging quality by effectively eliminating the effects from the changing environmental conditions.},
DOI = {10.3390/rs13132520}
}



@Article{rs13132523,
AUTHOR = {Gili, Piero and Civera, Marco and Roy, Rinto and Surace, Cecilia},
TITLE = {An Unmanned Lighter-Than-Air Platform for Large Scale Land Monitoring},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2523},
URL = {https://www.mdpi.com/2072-4292/13/13/2523},
ISSN = {2072-4292},
ABSTRACT = {The concept and preliminary design of an unmanned lighter-than-air (LTA) platform instrumented with different remote sensing technologies is presented. The aim is to assess the feasibility of using a remotely controlled airship for the land monitoring of medium sized (up to 107 m2) urban or rural areas at relatively low altitudes (below 1000 m) and its potential convenience with respect to other standard remote and in-situ sensing systems. The proposal includes equipment for high-definition visual, thermal, and hyperspectral imaging as well as LiDAR scanning. The data collected from these different sources can be then combined to obtain geo-referenced products such as land use land cover (LULC), soil water content (SWC), land surface temperature (LSC), and leaf area index (LAI) maps, among others. The potential uses for diffuse structural health monitoring over built-up areas are discussed as well. Several mission typologies are considered.},
DOI = {10.3390/rs13132523}
}



@Article{rs13132524,
AUTHOR = {Chen, Ziyi and Li, Dilong and Fan, Wentao and Guan, Haiyan and Wang, Cheng and Li, Jonathan},
TITLE = {Self-Attention in Reconstruction Bias U-Net for Semantic Segmentation of Building Rooftops in Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2524},
URL = {https://www.mdpi.com/2072-4292/13/13/2524},
ISSN = {2072-4292},
ABSTRACT = {Deep learning models have brought great breakthroughs in building extraction from high-resolution optical remote-sensing images. Among recent research, the self-attention module has called up a storm in many fields, including building extraction. However, most current deep learning models loading with the self-attention module still lose sight of the reconstruction bias’s effectiveness. Through tipping the balance between the abilities of encoding and decoding, i.e., making the decoding network be much more complex than the encoding network, the semantic segmentation ability will be reinforced. To remedy the research weakness in combing self-attention and reconstruction-bias modules for building extraction, this paper presents a U-Net architecture that combines self-attention and reconstruction-bias modules. In the encoding part, a self-attention module is added to learn the attention weights of the inputs. Through the self-attention module, the network will pay more attention to positions where there may be salient regions. In the decoding part, multiple large convolutional up-sampling operations are used for increasing the reconstruction ability. We test our model on two open available datasets: the WHU and Massachusetts Building datasets. We achieve IoU scores of 89.39% and 73.49% for the WHU and Massachusetts Building datasets, respectively. Compared with several recently famous semantic segmentation methods and representative building extraction methods, our method’s results are satisfactory.},
DOI = {10.3390/rs13132524}
}



@Article{jsan10030042,
AUTHOR = {Al-Nuaimi, Mohammed and Wibowo, Sapto and Qu, Hongyang and Aitken, Jonathan and Veres, Sandor},
TITLE = {Hybrid Verification Technique for Decision-Making of Self-Driving Vehicles},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {42},
URL = {https://www.mdpi.com/2224-2708/10/3/42},
ISSN = {2224-2708},
ABSTRACT = {The evolution of driving technology has recently progressed from active safety features and ADAS systems to fully sensor-guided autonomous driving. Bringing such a vehicle to market requires not only simulation and testing but formal verification to account for all possible traffic scenarios. A new verification approach, which combines the use of two well-known model checkers: model checker for multi-agent systems (MCMAS) and probabilistic model checker (PRISM), is presented for this purpose. The overall structure of our autonomous vehicle (AV) system consists of: (1) A perception system of sensors that feeds data into (2) a rational agent (RA) based on a belief–desire–intention (BDI) architecture, which uses a model of the environment and is connected to the RA for verification of decision-making, and (3) a feedback control systems for following a self-planned path. MCMAS is used to check the consistency and stability of the BDI agent logic during design-time. PRISM is used to provide the RA with the probability of success while it decides to take action during run-time operation. This allows the RA to select movements of the highest probability of success from several generated alternatives. This framework has been tested on a new AV software platform built using the robot operating system (ROS) and virtual reality (VR) Gazebo Simulator. It also includes a parking lot scenario to test the feasibility of this approach in a realistic environment. A practical implementation of the AV system was also carried out on the experimental testbed.},
DOI = {10.3390/jsan10030042}
}



@Article{s21134442,
AUTHOR = {Niu, Zijie and Deng, Juntao and Zhang, Xu and Zhang, Jun and Pan, Shijia and Mu, Haotian},
TITLE = {Identifying the Branch of Kiwifruit Based on Unmanned Aerial Vehicle (UAV) Images Using Deep Learning Method},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4442},
URL = {https://www.mdpi.com/1424-8220/21/13/4442},
PubMedID = {34209571},
ISSN = {1424-8220},
ABSTRACT = {It is important to obtain accurate information about kiwifruit vines to monitoring their physiological states and undertake precise orchard operations. However, because vines are small and cling to trellises, and have branches laying on the ground, numerous challenges exist in the acquisition of accurate data for kiwifruit vines. In this paper, a kiwifruit canopy distribution prediction model is proposed on the basis of low-altitude unmanned aerial vehicle (UAV) images and deep learning techniques. First, the location of the kiwifruit plants and vine distribution are extracted from high-precision images collected by UAV. The canopy gradient distribution maps with different noise reduction and distribution effects are generated by modifying the threshold and sampling size using the resampling normalization method. The results showed that the accuracies of the vine segmentation using PSPnet, support vector machine, and random forest classification were 71.2%, 85.8%, and 75.26%, respectively. However, the segmentation image obtained using depth semantic segmentation had a higher signal-to-noise ratio and was closer to the real situation. The average intersection over union of the deep semantic segmentation was more than or equal to 80% in distribution maps, whereas, in traditional machine learning, the average intersection was between 20% and 60%. This indicates the proposed model can quickly extract the vine distribution and plant position, and is thus able to perform dynamic monitoring of orchards to provide real-time operation guidance.},
DOI = {10.3390/s21134442}
}



@Article{d13070296,
AUTHOR = {Kady, Charbel and Chedid, Anna Maria and Kortbawi, Ingred and Yaacoub, Charles and Akl, Adib and Daclin, Nicolas and Trousset, François and Pfister, François and Zacharewicz, Gregory},
TITLE = {IoT-Driven Workflows for Risk Management and Control of Beehives},
JOURNAL = {Diversity},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {296},
URL = {https://www.mdpi.com/1424-2818/13/7/296},
ISSN = {1424-2818},
ABSTRACT = {The internet of things (IoT) and Industry 4.0 technologies are becoming widely used in the field of apiculture to enhance honey production and reduce colony losses using connected scales combined with additional data, such as relative humidity and internal temperature. This paper exploits beehive weight measurements and builds appropriate business rules using two instruments. The first is an IoT fixed scale installed on one hive, taking rich continuous measurements, and used as a reference. The second is a portable nomad scale communicating with a smartphone and used for the remaining hives. A key contribution will be the run and triggering of a business process model based on apicultural business rules learned from experience and system observed events. Later, the evolution of the weight of each individual hive, obtained by either measurement or inference, will be associated with a graphical workflow diagram expressed with the business process model and notation (BPMN) language, and will trigger events that inform beekeepers to initiate relevant action. Finally, the BPMN processes will be transformed into executable models for model driven decision support. This contribution improves amateur and professional user-experience for honeybee keeping and opens the door for interoperability between the suggested model and other available simulations (weather, humidity, bee colony behavior, etc.).},
DOI = {10.3390/d13070296}
}



@Article{s21134447,
AUTHOR = {Shin, Jisun and Jo, Young-Heon and Ryu, Joo-Hyung and Khim, Boo-Keun and Kim, Soo Mee},
TITLE = {High Spatial-Resolution Red Tide Detection in the Southern Coast of Korea Using U-Net from PlanetScope Imagery},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4447},
URL = {https://www.mdpi.com/1424-8220/21/13/4447},
PubMedID = {34209710},
ISSN = {1424-8220},
ABSTRACT = {Red tides caused by Margalefidinium polykrikoides occur continuously along the southern coast of Korea, where there are many aquaculture cages, and therefore, prompt monitoring of bloom water is required to prevent considerable damage. Satellite-based ocean-color sensors are widely used for detecting red tide blooms, but their low spatial resolution restricts coastal observations. Contrarily, terrestrial sensors with a high spatial resolution are good candidate sensors, despite the lack of spectral resolution and bands for red tide detection. In this study, we developed a U-Net deep learning model for detecting M. polykrikoides blooms along the southern coast of Korea from PlanetScope imagery with a high spatial resolution of 3 m. The U-Net model was trained with four different datasets that were constructed with randomly or non-randomly chosen patches consisting of different ratios of red tide and non-red tide pixels. The qualitative and quantitative assessments of the conventional red tide index (RTI) and four U-Net models suggest that the U-Net model, which was trained with a dataset of non-randomly chosen patches including non-red tide patches, outperformed RTI in terms of sensitivity, precision, and F-measure level, accounting for an increase of 19.84%, 44.84%, and 28.52%, respectively. The M. polykrikoides map derived from U-Net provides the most reasonable red tide patterns in all water areas. Combining high spatial resolution images and deep learning approaches represents a good solution for the monitoring of red tides over coastal regions.},
DOI = {10.3390/s21134447}
}



@Article{rs13132548,
AUTHOR = {Habibi, Luthfan Nur and Watanabe, Tomoya and Matsui, Tsutomu and Tanaka, Takashi S. T.},
TITLE = {Machine Learning Techniques to Predict Soybean Plant Density Using UAV and Satellite-Based Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2548},
URL = {https://www.mdpi.com/2072-4292/13/13/2548},
ISSN = {2072-4292},
ABSTRACT = {The plant density of soybean is a critical factor affecting plant canopy structure and yield. Predicting the spatial variability of plant density would be valuable for improving agronomic practices. The objective of this study was to develop a model for plant density measurement using several data sets with different spatial resolutions, including unmanned aerial vehicle (UAV) imagery, PlanetScope satellite imagery, and climate data. The model establishment process includes (1) performing the high-throughput measurement of actual plant density from UAV imagery with the You Only Look Once version 3 (YOLOv3) object detection algorithm, which was further treated as a response variable of the estimation models in the next step, and (2) developing regression models to estimate plant density in the extended areas using various combinations of predictors derived from PlanetScope imagery and climate data. Our results showed that the YOLOv3 model can accurately measure actual soybean plant density from UAV imagery data with a root mean square error (RMSE) value of 0.96 plants m−2. Furthermore, the two regression models, partial least squares and random forest (RF), successfully expanded the plant density prediction areas with RMSE values ranging from 1.78 to 3.67 plant m−2. Model improvement was conducted using the variable importance feature in RF, which improved prediction accuracy with an RMSE value of 1.72 plant m−2. These results demonstrated that the established model had an acceptable prediction accuracy for estimating plant density. Although the model could not often evaluate the within-field spatial variability of soybean plant density, the predicted values were sufficient for informing the field-specific status.},
DOI = {10.3390/rs13132548}
}



@Article{su13137309,
AUTHOR = {Giray, Görkem and Catal, Cagatay},
TITLE = {Design of a Data Management Reference Architecture for Sustainable Agriculture},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {7309},
URL = {https://www.mdpi.com/2071-1050/13/13/7309},
ISSN = {2071-1050},
ABSTRACT = {Effective and efficient data management is crucial for smart farming and precision agriculture. To realize operational efficiency, full automation, and high productivity in agricultural systems, different kinds of data are collected from operational systems using different sensors, stored in different systems, and processed using advanced techniques, such as machine learning and deep learning. Due to the complexity of data management operations, a data management reference architecture is required. While there are different initiatives to design data management reference architectures, a data management reference architecture for sustainable agriculture is missing. In this study, we follow domain scoping, domain modeling, and reference architecture design stages to design the reference architecture for sustainable agriculture. Four case studies were performed to demonstrate the applicability of the reference architecture. This study shows that the proposed data management reference architecture is practical and effective for sustainable agriculture.},
DOI = {10.3390/su13137309}
}



@Article{app11136079,
AUTHOR = {Elgamoudi, Abulasad and Benzerrouk, Hamza and Elango, G. Arul and Landry, René},
TITLE = {A Survey for Recent Techniques and Algorithms of Geolocation and Target Tracking in Wireless and Satellite Systems},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6079},
URL = {https://www.mdpi.com/2076-3417/11/13/6079},
ISSN = {2076-3417},
ABSTRACT = {A single Radio-Frequency Interference (RFI) is a disturbance source of modern wireless systems depending on Global Navigation Satellite Systems (GNSS) and Satellite Communication (SatCom). In particular, significant applications such as aeronautics and satellite communication can be severely affected by intentional and unintentional interference, which are unmitigated. The matter requires finding a radical and effective solution to overcome this problem. The methods used for overcoming the RFI include interference detection, interference classification, interference geolocation, tracking and interference mitigation. RFI source geolocation and tracking methodology gained universal attention from numerous researchers, specialists, and scientists. In the last decade, various conventional techniques and algorithms have been adopted in geolocation and target tracking in civil and military operations. Previous conventional techniques did not address the challenges and demand for novel algorithms. Hence there is a necessity for focussing on the issues associated with this. This survey introduces a review of various conventional geolocation techniques, current orientations, and state-of-the-art techniques and highlights some approaches and algorithms employed in wireless and satellite systems for geolocation and target tracking that may be extremely beneficial. In addition, a comparison between different conventional geolocation techniques has been revealed, and the comparisons between various approaches and algorithms of geolocation and target tracking have been addressed, including H∞ and Kalman Filtering versions that have been implemented and investigated by authors.},
DOI = {10.3390/app11136079}
}



@Article{rs13132555,
AUTHOR = {Yoosefzadeh-Najafabadi, Mohsen and Tulpan, Dan and Eskandari, Milad},
TITLE = {Using Hybrid Artificial Intelligence and Evolutionary Optimization Algorithms for Estimating Soybean Yield and Fresh Biomass Using Hyperspectral Vegetation Indices},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2555},
URL = {https://www.mdpi.com/2072-4292/13/13/2555},
ISSN = {2072-4292},
ABSTRACT = {Recent advanced high-throughput field phenotyping combined with sophisticated big data analysis methods have provided plant breeders with unprecedented tools for a better prediction of important agronomic traits, such as yield and fresh biomass (FBIO), at early growth stages. This study aimed to demonstrate the potential use of 35 selected hyperspectral vegetation indices (HVI), collected at the R5 growth stage, for predicting soybean seed yield and FBIO. Two artificial intelligence algorithms, ensemble-bagging (EB) and deep neural network (DNN), were used to predict soybean seed yield and FBIO using HVI. Considering HVI as input variables, the coefficients of determination (R2) of 0.76 and 0.77 for yield and 0.91 and 0.89 for FBIO were obtained using DNN and EB, respectively. In this study, we also used hybrid DNN-SPEA2 to estimate the optimum HVI values in soybeans with maximized yield and FBIO productions. In addition, to identify the most informative HVI in predicting yield and FBIO, the feature recursive elimination wrapper method was used and the top ranking HVI were determined to be associated with red, 670 nm and near-infrared, 800 nm, regions. Overall, this study introduced hybrid DNN-SPEA2 as a robust mathematical tool for optimizing and using informative HVI for estimating soybean seed yield and FBIO at early growth stages, which can be employed by soybean breeders for discriminating superior genotypes in large breeding populations.},
DOI = {10.3390/rs13132555}
}



@Article{agriculture11070617,
AUTHOR = {Bansal, Prakhar and Kumar, Rahul and Kumar, Somesh},
TITLE = {Disease Detection in Apple Leaves Using Deep Convolutional Neural Network},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {617},
URL = {https://www.mdpi.com/2077-0472/11/7/617},
ISSN = {2077-0472},
ABSTRACT = {The automatic detection of diseases in plants is necessary, as it reduces the tedious work of monitoring large farms and it will detect the disease at an early stage of its occurrence to minimize further degradation of plants. Besides the decline of plant health, a country’s economy is highly affected by this scenario due to lower production. The current approach to identify diseases by an expert is slow and non-optimal for large farms. Our proposed model is an ensemble of pre-trained DenseNet121, EfficientNetB7, and EfficientNet NoisyStudent, which aims to classify leaves of apple trees into one of the following categories: healthy, apple scab, apple cedar rust, and multiple diseases, using its images. Various Image Augmentation techniques are included in this research to increase the dataset size, and subsequentially, the model’s accuracy increases. Our proposed model achieves an accuracy of 96.25% on the validation dataset. The proposed model can identify leaves with multiple diseases with 90% accuracy. Our proposed model achieved a good performance on different metrics and can be deployed in the agricultural domain to identify plant health accurately and timely.},
DOI = {10.3390/agriculture11070617}
}



@Article{app11136112,
AUTHOR = {Mbiydzenyuy, Gideon and Nowaczyk, Sławomir and Knutsson, Håkan and Vanhoudt, Dirk and Brage, Jens and Calikus, Ece},
TITLE = {Opportunities for Machine Learning in District Heating},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6112},
URL = {https://www.mdpi.com/2076-3417/11/13/6112},
ISSN = {2076-3417},
ABSTRACT = {The district heating (DH) industry is facing an important transformation towards more efficient networks that utilise significantly lower water temperatures to distribute the heat. This change requires taking advantage of new technologies, and Machine Learning (ML) is a popular direction. In the last decade, we have witnessed an extreme growth in the number of published research papers that focus on applying ML techniques to the DH domain. However, based on our experience in the field, and an extensive review of the state-of-the-art, we perceive a mismatch between the most popular research directions, such as forecasting, and the challenges faced by the DH industry. In this work, we present our findings, explain and demonstrate the key gaps between the two communities and suggest a road-map ahead towards increasing the impact of ML research in the DH industry.},
DOI = {10.3390/app11136112}
}



@Article{rs13132567,
AUTHOR = {Oh, Sungchan and Lee, Da-Young and Gongora-Canul, Carlos and Ashapure, Akash and Carpenter, Joshua and Cruz, A. P. and Fernandez-Campos, Mariela and Lane, Brenden Z. and Telenko, Darcy E. P. and Jung, Jinha and Cruz, C. D.},
TITLE = {Tar Spot Disease Quantification Using Unmanned Aircraft Systems (UAS) Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2567},
URL = {https://www.mdpi.com/2072-4292/13/13/2567},
ISSN = {2072-4292},
ABSTRACT = {Tar spot is a foliar disease of corn characterized by fungal fruiting bodies that resemble tar spots. The disease emerged in the U.S. in 2015, and severe outbreaks in 2018 caused an economic impact on corn yields throughout the Midwest. Adequate epidemiological surveillance and disease quantification are necessary to develop immediate and long-term management strategies. This study presents a measurement framework that evaluates the disease severity of tar spot using unmanned aircraft systems (UAS)-based plant phenotyping and regression techniques. UAS-based plant phenotypic information, such as canopy cover, canopy volume, and vegetation indices, were used as explanatory variables. Visual estimations of disease severity were performed by expert plant pathologists per experiment plot basis and used as response variables. Three regression methods, namely ordinary least squares (OLS), support vector regression (SVR), and multilayer perceptron (MLP), were used to determine an optimal regression method for UAS-based tar spot measurement. The cross-validation results showed that the regression model based on MLP provides the highest accuracy of disease measurements. By training and testing the model with spatially separated datasets, the proposed regression model achieved a Lin’s concordance correlation coefficient (ρc) of 0.82 and a root mean square error (RMSE) of 6.42. This study demonstrated that we could use the proposed UAS-based method for the disease quantification of tar spot, which shows a gradual spectral response as the disease develops.},
DOI = {10.3390/rs13132567}
}



@Article{s21134511,
AUTHOR = {Bauer, Martin and Sanchez, Luis and Song, JaeSeung},
TITLE = {IoT-Enabled Smart Cities: Evolution and Outlook},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4511},
URL = {https://www.mdpi.com/1424-8220/21/13/4511},
PubMedID = {34209436},
ISSN = {1424-8220},
ABSTRACT = {For the last decade the Smart City concept has been under development, fostered by the growing urbanization of the world’s population and the need to handle the challenges that such a scenario raises. During this time many Smart City projects have been executed–some as proof-of-concept, but a growing number resulting in permanent, production-level deployments, improving the operation of the city and the quality of life of its citizens. Thus, Smart Cities are still a highly relevant paradigm which needs further development before it reaches its full potential and provides robust and resilient solutions. In this paper, the focus is set on the Internet of Things (IoT) as an enabling technology for the Smart City. In this sense, the paper reviews the current landscape of IoT-enabled Smart Cities, surveying relevant experiences and city initiatives that have embedded IoT within their city services and how they have generated an impact. The paper discusses the key technologies that have been developed and how they are contributing to the realization of the Smart City. Moreover, it presents some challenges that remain open ahead of us and which are the initiatives and technologies that are under development to tackle them.},
DOI = {10.3390/s21134511}
}



@Article{info12070272,
AUTHOR = {Ackerson, Joseph M. and Dave, Rushit and Seliya, Naeem},
TITLE = {Applications of Recurrent Neural Network for Biometric Authentication &amp; Anomaly Detection},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {272},
URL = {https://www.mdpi.com/2078-2489/12/7/272},
ISSN = {2078-2489},
ABSTRACT = {Recurrent Neural Networks are powerful machine learning frameworks that allow for data to be saved and referenced in a temporal sequence. This opens many new possibilities in fields such as handwriting analysis and speech recognition. This paper seeks to explore current research being conducted on RNNs in four very important areas, being biometric authentication, expression recognition, anomaly detection, and applications to aircraft. This paper reviews the methodologies, purpose, results, and the benefits and drawbacks of each proposed method below. These various methodologies all focus on how they can leverage distinct RNN architectures such as the popular Long Short-Term Memory (LSTM) RNN or a Deep-Residual RNN. This paper also examines which frameworks work best in certain situations, and the advantages and disadvantages of each proposed model.},
DOI = {10.3390/info12070272}
}



@Article{telecom2030017,
AUTHOR = {Pourroostaei Ardakani, Saeid and Cheshmehzangi, Ali},
TITLE = {Reinforcement Learning-Enabled UAV Itinerary Planning for Remote Sensing Applications in Smart Farming},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {255--270},
URL = {https://www.mdpi.com/2673-4001/2/3/17},
ISSN = {2673-4001},
ABSTRACT = {UAV path planning for remote sensing aims to find the best-fitted routes to complete a data collection mission. UAVs plan the routes and move through them to remotely collect environmental data from particular target zones by using sensory devices such as cameras. Route planning may utilize machine learning techniques to autonomously find/select cost-effective and/or best-fitted routes and achieve optimized results including: minimized data collection delay, reduced UAV power consumption, decreased flight traversed distance and maximized number of collected data samples. This paper utilizes a reinforcement learning technique (location and energy-aware Q-learning) to plan UAV routes for remote sensing in smart farms. Through this, the UAV avoids heuristically or blindly moving throughout a farm, but this takes the benefits of environment exploration–exploitation to explore the farm and find the shortest and most cost-effective paths into target locations with interesting data samples to collect. According to the simulation results, utilizing the Q-learning technique increases data collection robustness and reduces UAV resource consumption (e.g., power), traversed paths, and remote sensing latency as compared to two well-known benchmarks, IEMF and TBID, especially if the target locations are dense and crowded in a farm.},
DOI = {10.3390/telecom2030017}
}



@Article{math9131541,
AUTHOR = {Korchagin, Sergey and Romanova, Ekaterina and Serdechnyy, Denis and Nikitin, Petr and Dolgov, Vitaliy and Feklin, Vadim},
TITLE = {Mathematical Modeling of Layered Nanocomposite of Fractal Structure},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1541},
URL = {https://www.mdpi.com/2227-7390/9/13/1541},
ISSN = {2227-7390},
ABSTRACT = {A model of a layered hierarchically constructed composite is presented, the structure of which demonstrates the properties of similarity at different scales. For the proposed model of the composite, fractal analysis was carried out, including an assessment of the permissible range of scales, calculation of fractal capacity, Hausdorff and Minkovsky dimensions, calculation of the Hurst exponent. The maximum and minimum sizes at which fractal properties are observed are investigated, and a quantitative assessment of the complexity of the proposed model is carried out. A software package is developed that allows calculating the fractal characteristics of hierarchically constructed composite media. A qualitative analysis of the calculated fractal characteristics is carried out.},
DOI = {10.3390/math9131541}
}



@Article{aerospace8070179,
AUTHOR = {Swinney, Carolyn J. and Woods, John C.},
TITLE = {The Effect of Real-World Interference on CNN Feature Extraction and Machine Learning Classification of Unmanned Aerial Systems},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {179},
URL = {https://www.mdpi.com/2226-4310/8/7/179},
ISSN = {2226-4310},
ABSTRACT = {Small unmanned aerial systems (UASs) present many potential solutions and enhancements to industry today but equally pose a significant security challenge. We only need to look at the levels of disruption caused by UASs at airports in recent years. The accuracy of UAS detection and classification systems based on radio frequency (RF) signals can be hindered by other interfering signals present in the same frequency band, such as Bluetooth and Wi-Fi devices. In this paper, we evaluate the effect of real-world interference from Bluetooth and Wi-Fi signals concurrently on convolutional neural network (CNN) feature extraction and machine learning classification of UASs. We assess multiple UASs that operate using different transmission systems: Wi-Fi, Lightbridge 2.0, OcuSync 1.0, OcuSync 2.0 and the recently released OcuSync 3.0. We consider 7 popular UASs, evaluating 2 class UAS detection, 8 class UAS type classification and 21 class UAS flight mode classification. Our results show that the process of CNN feature extraction using transfer learning and machine learning classification is fairly robust in the presence of real-world interference. We also show that UASs that are operating using the same transmission system can be distinguished. In the presence of interference from both Bluetooth and Wi-Fi signals, our results show 100% accuracy for UAV detection (2 classes), 98.1% (+/−0.4%) for UAV type classification (8 classes) and 95.4% (+/−0.3%) for UAV flight mode classification (21 classes).},
DOI = {10.3390/aerospace8070179}
}



@Article{math9131542,
AUTHOR = {Xie, Xuelin and Shen, Jingfang},
TITLE = {Waterlogging Resistance Evaluation Index and Photosynthesis Characteristics Selection: Using Machine Learning Methods to Judge Poplar’s Waterlogging Resistance},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1542},
URL = {https://www.mdpi.com/2227-7390/9/13/1542},
ISSN = {2227-7390},
ABSTRACT = {Flood disasters are the major natural disaster that affects the growth of agriculture and forestry crops. Due to rapid growth and strong waterlogging resistance characteristics, many studies have explained the waterlogging resistance mechanism of poplar from different perspectives. However, there is no accurate method to define the evaluation index of waterlogging resistance. In addition, there is also a lack of research on predicting the waterlogging resistance of poplars. Based on the changes of poplar biomass and seedling height, the evaluation index of poplar resistance to waterlogging was well determined, and the characteristics of photosynthesis were used to predict the waterlogging resistance of poplars. First, four methods of hierarchical clustering, lasso, stepwise regression and all-subsets regression were used to extract the photosynthesis characteristics. After that, the support vector regression model of poplar resistance to waterlogging was established by using the characteristic parameters of photosynthesis. Finally, the results show that the SVR model based on Stepwise regression and Lasso method has high precision. On the test set, the coefficient of determination (R2) was 0.8581 and 0.8492, the mean square error (MSE) was 0.0104 and 0.0341, and the mean relative error (MRE) was 9.78% and 9.85%, respectively. Therefore, using the characteristic parameters of photosynthesis to predict the waterlogging resistance of poplars is feasible.},
DOI = {10.3390/math9131542}
}



@Article{s21134522,
AUTHOR = {Zhang, Cong and Li, Dongguang and Qi, Jiashuo and Liu, Jingtao and Wang, Yu},
TITLE = {Infrared Small Target Detection Method with Trajectory Correction Fuze Based on Infrared Image Sensor},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4522},
URL = {https://www.mdpi.com/1424-8220/21/13/4522},
PubMedID = {34282797},
ISSN = {1424-8220},
ABSTRACT = {Due to the complexity of background and diversity of small targets, robust detection of infrared small targets for the trajectory correction fuze has become a challenge. To solve this problem, different from the traditional method, a state-of-the-art detection method based on density-distance space is proposed to apply to the trajectory correction fuze. First, parameters of the infrared image sensor on the fuze are calculated to set the boundary limitations for the target detection method. Second, the density-distance space method is proposed to detect the candidate targets. Finally, the adaptive pixel growth (APG) algorithm is used to suppress the clutter so as to detect the real targets. Three experiments, including equivalent detection, simulation and hardware-in-loop, were implemented to verify the effectiveness of this method. Results illustrated that the infrared image sensor on the fuze has a stable field of view under rotation of the projectile, and could clearly observe the infrared small target. The proposed method has superior anti-noise, different size target detection, multi-target detection and various clutter suppression capability. Compared with six novel algorithms, our algorithm shows a perfect detection performance and acceptable time consumption.},
DOI = {10.3390/s21134522}
}



@Article{electronics10131592,
AUTHOR = {Kim, Jonguk and Bae, Hyansu and Kang, Hyunwoo and Lee, Suk Gyu},
TITLE = {CNN Algorithm for Roof Detection and Material Classification in Satellite Images},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1592},
URL = {https://www.mdpi.com/2079-9292/10/13/1592},
ISSN = {2079-9292},
ABSTRACT = {This paper suggests an algorithm for extracting the location of a building from satellite imagery and using that information to modify the roof content. The materials are determined by measuring the conditions where the building is located and detecting the position of a building in broad satellite images. Depending on the incomplete roof or material, there is a greater possibility of great damage caused by disaster situations or external shocks. To address these problems, we propose an algorithm to detect roofs and classify materials in satellite images. Satellite imaging locates areas where buildings are likely to exist based on roads. Using images of the detected buildings, we classify the material of the roof using a proposed convolutional neural network (CNN) model algorithm consisting of 43 layers. In this paper, we propose a CNN structure to detect areas with buildings in large images and classify roof materials in the detected areas.},
DOI = {10.3390/electronics10131592}
}



@Article{rs13132588,
AUTHOR = {Wang, Zhihao and Brenning, Alexander},
TITLE = {Active-Learning Approaches for Landslide Mapping Using Support Vector Machines},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2588},
URL = {https://www.mdpi.com/2072-4292/13/13/2588},
ISSN = {2072-4292},
ABSTRACT = {Ex post landslide mapping for emergency response and ex ante landslide susceptibility modelling for hazard mitigation are two important application scenarios that require the development of accurate, yet cost-effective spatial landslide models. However, the manual labelling of instances for training machine learning models is time-consuming given the data requirements of flexible data-driven algorithms and the small percentage of area covered by landslides. Active learning aims to reduce labelling costs by selecting more informative instances. In this study, two common active-learning strategies, uncertainty sampling and query by committee, are combined with the support vector machine (SVM), a state-of-the-art machine-learning technique, in a landslide mapping case study in order to assess their possible benefits compared to simple random sampling of training locations. By selecting more “informative” instances, the SVMs with active learning based on uncertainty sampling outperformed both random sampling and query-by-committee strategies when considering mean AUROC (area under the receiver operating characteristic curve) as performance measure. Uncertainty sampling also produced more stable performances with a smaller AUROC standard deviation across repetitions. In conclusion, under limited data conditions, uncertainty sampling reduces the amount of expert time needed by selecting more informative instances for SVM training. We therefore recommend incorporating active learning with uncertainty sampling into interactive landslide modelling workflows, especially in emergency response settings, but also in landslide susceptibility modelling.},
DOI = {10.3390/rs13132588}
}



@Article{s21134542,
AUTHOR = {Kaczorowska, Monika and Karczmarek, Paweł and Plechawska-Wójcik, Małgorzata and Tokovarov, Mikhail},
TITLE = {On the Improvement of Eye Tracking-Based Cognitive Workload Estimation Using Aggregation Functions},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4542},
URL = {https://www.mdpi.com/1424-8220/21/13/4542},
PubMedID = {34283098},
ISSN = {1424-8220},
ABSTRACT = {Cognitive workload, being a quantitative measure of mental effort, draws significant interest of researchers, as it allows to monitor the state of mental fatigue. Estimation of cognitive workload becomes especially important for job positions requiring outstanding engagement and responsibility, e.g., air-traffic dispatchers, pilots, car or train drivers. Cognitive workload estimation finds its applications also in the field of education material preparation. It allows to monitor the difficulty degree for specific tasks enabling to adjust the level of education materials to typical abilities of students. In this study, we present the results of research conducted with the goal of examining the influence of various fuzzy or non-fuzzy aggregation functions upon the quality of cognitive workload estimation. Various classic machine learning models were successfully applied to the problem. The results of extensive in-depth experiments with over 2000 aggregation operators shows the applicability of the approach based on the aggregation functions. Moreover, the approach based on aggregation process allows for further improvement of classification results. A wide range of aggregation functions is considered and the results suggest that the combination of classical machine learning models and aggregation methods allows to achieve high quality of cognitive workload level recognition preserving low computational cost.},
DOI = {10.3390/s21134542}
}



@Article{rs13132591,
AUTHOR = {Maxwell, Aaron E. and Warner, Timothy A. and Guillén, Luis Andrés},
TITLE = {Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies—Part 2: Recommendations and Best Practices},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2591},
URL = {https://www.mdpi.com/2072-4292/13/13/2591},
ISSN = {2072-4292},
ABSTRACT = {Convolutional neural network (CNN)-based deep learning (DL) has a wide variety of applications in the geospatial and remote sensing (RS) sciences, and consequently has been a focus of many recent studies. However, a review of accuracy assessment methods used in recently published RS DL studies, focusing on scene classification, object detection, semantic segmentation, and instance segmentation, indicates that RS DL papers appear to follow an accuracy assessment approach that diverges from that of traditional RS studies. Papers reporting on RS DL studies have largely abandoned traditional RS accuracy assessment terminology; they rarely reported a complete confusion matrix; and sampling designs and analysis protocols generally did not provide a population-based confusion matrix, in which the table entries are estimates of the probabilities of occurrence of the mapped landscape. These issues indicate the need for the RS community to develop guidance on best practices for accuracy assessment for CNN-based DL thematic mapping and object detection. As a first step in that process, we explore key issues, including the observation that accuracy assessments should not be biased by the CNN-based training and inference processes that rely on image chips. Furthermore, accuracy assessments should be consistent with prior recommendations and standards in the field, should support the estimation of a population confusion matrix, and should allow for assessment of model generalization. This paper draws from our review of the RS DL literature and the rich record of traditional remote sensing accuracy assessment research while considering the unique nature of CNN-based deep learning to propose accuracy assessment best practices that use appropriate sampling methods, training and validation data partitioning, assessment metrics, and reporting standards.},
DOI = {10.3390/rs13132591}
}



@Article{fi13070174,
AUTHOR = {Li, Xiaohui and Savkin, Andrey V.},
TITLE = {Networked Unmanned Aerial Vehicles for Surveillance and Monitoring: A Survey},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {174},
URL = {https://www.mdpi.com/1999-5903/13/7/174},
ISSN = {1999-5903},
ABSTRACT = {As a typical cyber-physical system, networked unmanned aerial vehicles (UAVs) have received much attention in recent years. Emerging communication technologies and high-performance control methods enable networked UAVs to operate as aerial sensor networks to collect more complete and consistent information with significantly improved mobility and flexibility than traditional sensing platforms. One of the main applications of networked UAVs is surveillance and monitoring, which constitute essential components of a well-functioning public safety system and many industrial applications. Although the existing literature on surveillance and monitoring UAVs is extensive, a comprehensive survey on this topic is lacking. This article classifies publications on networked UAVs for surveillance and monitoring using the targets of interest and analyzes several typical problems on this topic, including the control, navigation, and deployment optimization of UAVs. The related research gaps and future directions are also presented.},
DOI = {10.3390/fi13070174}
}



@Article{horticulturae7070176,
AUTHOR = {Duarte-Carvajalino, Julio Martin and Silva-Arero, Elías Alexander and Góez-Vinasco, Gerardo Antonio and Torres-Delgado, Laura Marcela and Ocampo-Paez, Oscar Dubán and Castaño-Marín, Angela María},
TITLE = {Estimation of Water Stress in Potato Plants Using Hyperspectral Imagery and Machine Learning Algorithms},
JOURNAL = {Horticulturae},
VOLUME = {7},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {176},
URL = {https://www.mdpi.com/2311-7524/7/7/176},
ISSN = {2311-7524},
ABSTRACT = {This work presents quantitative detection of water stress and estimation of the water stress level: none, light, moderate, and severe on potato crops. We use hyperspectral imagery and state of the art machine learning algorithms: random decision forest, multilayer perceptron, convolutional neural networks, support vector machines, extreme gradient boost, and AdaBoost. The detection and estimation of water stress in potato crops is carried out on two different phenological stages of the plants: tubers differentiation and maximum tuberization. The machine learning algorithms are trained with a small subset of each hyperspectral image corresponding to the plant canopy. The results are improved using majority voting to classify all the canopy pixels in the hyperspectral images. The results indicate that both detection of water stress and estimation of the level of water stress can be obtained with good accuracy, improved further by majority voting. The importance of each band of the hyperspectral images in the classification of the images is assessed by random forest and extreme gradient boost, which are the machine learning algorithms that perform best overall on both phenological stages and detection and estimation of water stress in potato crops.},
DOI = {10.3390/horticulturae7070176}
}



@Article{rs13132596,
AUTHOR = {Mohan, Midhun and Richardson, Gabriella and Gopan, Gopika and Aghai, Matthew Mehdi and Bajaj, Shaurya and Galgamuwa, G. A. Pabodha and Vastaranta, Mikko and Arachchige, Pavithra S. Pitumpe and Amorós, Lot and Corte, Ana Paula Dalla and de-Miguel, Sergio and Leite, Rodrigo Vieira and Kganyago, Mahlatse and Broadbent, Eben North and Doaemo, Willie and Shorab, Mohammed Abdullah Bin and Cardil, Adrian},
TITLE = {UAV-Supported Forest Regeneration: Current Trends, Challenges and Implications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2596},
URL = {https://www.mdpi.com/2072-4292/13/13/2596},
ISSN = {2072-4292},
ABSTRACT = {Replanting trees helps with avoiding desertification, reducing the chances of soil erosion and flooding, minimizing the risks of zoonotic disease outbreaks, and providing ecosystem services and livelihood to the indigenous people, in addition to sequestering carbon dioxide for mitigating climate change. Consequently, it is important to explore new methods and technologies that are aiming to upscale and fast-track afforestation and reforestation (A/R) endeavors, given that many of the current tree planting strategies are not cost effective over large landscapes, and suffer from constraints associated with time, energy, manpower, and nursery-based seedling production. UAV (unmanned aerial vehicle)-supported seed sowing (UAVsSS) can promote rapid A/R in a safe, cost-effective, fast and environmentally friendly manner, if performed correctly, even in otherwise unsafe and/or inaccessible terrains, supplementing the overall manual planting efforts globally. In this study, we reviewed the recent literature on UAVsSS, to analyze the current status of the technology. Primary UAVsSS applications were found to be in areas of post-wildfire reforestation, mangrove restoration, forest restoration after degradation, weed eradication, and desert greening. Nonetheless, low survival rates of the seeds, future forest diversity, weather limitations, financial constraints, and seed-firing accuracy concerns were determined as major challenges to operationalization. Based on our literature survey and qualitative analysis, twelve recommendations—ranging from the need for publishing germination results to linking UAVsSS operations with carbon offset markets—are provided for the advancement of UAVsSS applications.},
DOI = {10.3390/rs13132596}
}



@Article{s21134549,
AUTHOR = {Hussein, Burhan Rashid and Malik, Owais Ahmed and Ong, Wee-Hong and Slik, Johan Willem Frederik},
TITLE = {Automated Extraction of Phenotypic Leaf Traits of Individual Intact Herbarium Leaves from Herbarium Specimen Images Using Deep Learning Based Semantic Segmentation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4549},
URL = {https://www.mdpi.com/1424-8220/21/13/4549},
PubMedID = {34283110},
ISSN = {1424-8220},
ABSTRACT = {With the increase in the digitization efforts of herbarium collections worldwide, dataset repositories such as iDigBio and GBIF now have hundreds of thousands of herbarium sheet images ready for exploration. Although this serves as a new source of plant leaves data, herbarium datasets have an inherent challenge to deal with the sheets containing other non-plant objects such as color charts, barcodes, and labels. Even for the plant part itself, a combination of different overlapping, damaged, and intact individual leaves exist together with other plant organs such as stems and fruits, which increases the complexity of leaf trait extraction and analysis. Focusing on segmentation and trait extraction on individual intact herbarium leaves, this study proposes a pipeline consisting of deep learning semantic segmentation model (DeepLabv3+), connected component analysis, and a single-leaf classifier trained on binary images to automate the extraction of an intact individual leaf with phenotypic traits. The proposed method achieved a higher F1-score for both the in-house dataset (96%) and on a publicly available herbarium dataset (93%) compared to object detection-based approaches including Faster R-CNN and YOLOv5. Furthermore, using the proposed approach, the phenotypic measurements extracted from the segmented individual leaves were closer to the ground truth measurements, which suggests the importance of the segmentation process in handling background noise. Compared to the object detection-based approaches, the proposed method showed a promising direction toward an autonomous tool for the extraction of individual leaves together with their trait data directly from herbarium specimen images.},
DOI = {10.3390/s21134549}
}



@Article{rs13132598,
AUTHOR = {Li, Jian and Chen, Baozhang},
TITLE = {Optimal Solar Zenith Angle Definition for Combined Landsat-8 and Sentinel-2A/2B Data Angular Normalization Using Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2598},
URL = {https://www.mdpi.com/2072-4292/13/13/2598},
ISSN = {2072-4292},
ABSTRACT = {Data from Landsat-8 and Sentinel-2A/2B are often combined for terrestrial monitoring because of their similar spectral bands. The bidirectional reflectance distribution function (BRDF) effect has been observed in both Landsat-8 and Sentinel-2A/2B reflectance data. However, there is currently no definition of solar zenith angle (θsz) that is suitable for the normalization of the BRDF-adjusted reflectance from the three sensors’ combined data. This paper describes the use of four machine learning (ML) models to predict a global θsz that is suitable for the normalization of bidirectional reflectance from the combined data in 2018. The observed θsz collected globally, and the three locations in the Democratic Republic of Congo (26.622°E, 0.356°N), Texas in the USA (99.406°W 30.751°N), and Finland (25.194°E, 61.653°N), are chosen to compare the performance of the ML models. At a global scale, the ML models of Support Vector Regression (SVR), Multi-Layer Perception (MLP), and Gaussian Process Regression (GPR) exhibit comparably good performance to that of polynomial regression, considering center latitude as the input to predict the global θsz. GPR achieves the best overall performance considering the center latitude and acquisition time as inputs, with a root mean square error (RMSE) of 1.390°, a mean absolute error (MAE) of 0.689°, and a coefficient of determination (R2) of 0.994. SVR shows an RMSE of 1.396°, an MAE of 0.638°, and an R2 of 0.994, following GPR. For a specific location, the SVR and GPR models have higher accuracy than the polynomial regression, with GPR exhibiting the best performance, when center latitude and acquisition time are considered as inputs. GPR is recommended for predicting the global θsz using the three sensors’ combined data.},
DOI = {10.3390/rs13132598}
}



@Article{agronomy11071363,
AUTHOR = {Bahrami, Hazhir and Homayouni, Saeid and Safari, Abdolreza and Mirzaei, Sayeh and Mahdianpari, Masoud and Reisi-Gahrouei, Omid},
TITLE = {Deep Learning-Based Estimation of Crop Biophysical Parameters Using Multi-Source and Multi-Temporal Remote Sensing Observations},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1363},
URL = {https://www.mdpi.com/2073-4395/11/7/1363},
ISSN = {2073-4395},
ABSTRACT = {Remote sensing data are considered as one of the primary data sources for precise agriculture. Several studies have demonstrated the excellent capability of radar and optical imagery for crop mapping and biophysical parameter estimation. This paper aims at modeling the crop biophysical parameters, e.g., Leaf Area Index (LAI) and biomass, using a combination of radar and optical Earth observations. We extracted several radar features from polarimetric Synthetic Aperture Radar (SAR) data and Vegetation Indices (VIs) from optical images to model crops’ LAI and dry biomass. Then, the mutual correlations between these features and Random Forest feature importance were calculated. We considered two scenarios to estimate crop parameters. First, Machine Learning (ML) algorithms, e.g., Support Vector Regression (SVR), Random Forest (RF), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB), were utilized to estimate two crop biophysical parameters. To this end, crops’ dry biomass and LAI were estimated using three input data; (1) SAR polarimetric features; (2) spectral VIs; (3) integrating both SAR and optical features. Second, a deep artificial neural network was created. These input data were fed to the mentioned algorithms and evaluated using the in-situ measurements. These observations of three cash crops, including soybean, corn, and canola, have been collected over Manitoba, Canada, during the Soil Moisture Active Validation Experimental 2012 (SMAPVEX-12) campaign. The results showed that GB and XGB have great potential in parameter estimation and remarkably improved accuracy. Our results also demonstrated a significant improvement in the dry biomass and LAI estimation compared to the previous studies. For LAI, the validation Root Mean Square Error (RMSE) was reported as 0.557 m2/m2 for canola using GB, and 0.298 m2/m2 for corn using GB, 0.233 m2/m2 for soybean using XGB. RMSE was reported for dry biomass as 26.29 g/m2 for canola utilizing SVR, 57.97 g/m2 for corn using RF, and 5.00 g/m2 for soybean using GB. The results revealed that the deep artificial neural network had a better potential to estimate crop parameters than the ML algorithms.},
DOI = {10.3390/agronomy11071363}
}



@Article{rs13132627,
AUTHOR = {Moura, Marks Melo and de Oliveira, Luiz Eduardo Soares and Sanquetta, Carlos Roberto and Bastos, Alexis and Mohan, Midhun and Corte, Ana Paula Dalla},
TITLE = {Towards Amazon Forest Restoration: Automatic Detection of Species from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2627},
URL = {https://www.mdpi.com/2072-4292/13/13/2627},
ISSN = {2072-4292},
ABSTRACT = {Precise assessments of forest species’ composition help analyze biodiversity patterns, estimate wood stocks, and improve carbon stock estimates. Therefore, the objective of this work was to evaluate the use of high-resolution images obtained from Unmanned Aerial Vehicle (UAV) for the identification of forest species in areas of forest regeneration in the Amazon. For this purpose, convolutional neural networks (CNN) were trained using the Keras–Tensorflow package with the faster_rcnn_inception_v2_pets model. Samples of six forest species were used to train CNN. From these, attempts were made with the number of thresholds, which is the cutoff value of the function; any value below this output is considered 0, and values above are treated as an output 1; that is, values above the value stipulated in the Threshold are considered as identified species. The results showed that the reduction in the threshold decreases the accuracy of identification, as well as the overlap of the polygons of species identification. However, in comparison with the data collected in the field, it was observed that there exists a high correlation between the trees identified by the CNN and those observed in the plots. The statistical metrics used to validate the classification results showed that CNN are able to identify species with accuracy above 90%. Based on our results, which demonstrate good accuracy and precision in the identification of species, we conclude that convolutional neural networks are an effective tool in classifying objects from UAV images.},
DOI = {10.3390/rs13132627}
}



@Article{rs13132631,
AUTHOR = {Grybas, Heather and Congalton, Russell G.},
TITLE = {A Comparison of Multi-Temporal RGB and Multispectral UAS Imagery for Tree Species Classification in Heterogeneous New Hampshire Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2631},
URL = {https://www.mdpi.com/2072-4292/13/13/2631},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial systems (UASs) have recently become an affordable means to map forests at the species level, but research into the performance of different classification methodologies and sensors is necessary so users can make informed choices that maximize accuracy. This study investigated whether multi-temporal UAS data improved the classified accuracy of 14 species examined the optimal time-window for data collection, and compared the performance of a consumer-grade RGB sensor to that of a multispectral sensor. A time series of UAS data was collected from early spring to mid-summer and a sequence of mono-temporal and multi-temporal classifications were carried out. Kappa comparisons were conducted to ascertain whether the multi-temporal classifications significantly improved accuracy and whether there were significant differences between the RGB and multispectral classifications. The multi-temporal classification approach significantly improved accuracy; however, there was no significant benefit when more than three dates were used. Mid- to late spring imagery produced the highest accuracies, potentially due to high spectral heterogeneity between species and homogeneity within species during this time. The RGB sensor exhibited significantly higher accuracies, probably due to the blue band, which was found to be very important for classification accuracy and lacking in the multispectral sensor employed here.},
DOI = {10.3390/rs13132631}
}



@Article{electronics10131605,
AUTHOR = {Kathen, Micaela Jara Ten and Flores, Isabel Jurado and Reina, Daniel Gutiérrez},
TITLE = {An Informative Path Planner for a Swarm of ASVs Based on an Enhanced PSO with Gaussian Surrogate Model Components Intended for Water Monitoring Applications},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1605},
URL = {https://www.mdpi.com/2079-9292/10/13/1605},
ISSN = {2079-9292},
ABSTRACT = {Controlling the water quality of water supplies has always been a critical challenge, and water resource monitoring has become a need in recent years. Manual monitoring is not recommended in the case of large water surfaces for a variety of reasons, including expense and time consumption. In the last few years, researchers have proposed the use of autonomous vehicles for monitoring tasks. Fleets or swarms of vehicles can be deployed to conduct water resource explorations by using path planning techniques to guide the movements of each vehicle. The main idea of this work is the development of a monitoring system for Ypacarai Lake, where a fleet of autonomous surface vehicles will be guided by an improved particle swarm optimization based on the Gaussian process as a surrogate model. The purpose of using the surrogate model is to model water quality parameter behavior and to guide the movements of the vehicles toward areas where samples have not yet been collected; these areas are considered areas with high uncertainty or unexplored areas and areas with high contamination levels of the lake. The results show that the proposed approach, namely the enhanced GP-based PSO, balances appropriately the exploration and exploitation of the surface of Ypacarai Lake. In addition, the proposed approach has been compared with other techniques like the original particle swarm optimization and the particle swarm optimization with Gaussian process uncertainty component in a simulated Ypacarai Lake environment. The obtained results demonstrate the superiority of the proposed enhanced GP-based PSO in terms of mean square error with respect to the other techniques.},
DOI = {10.3390/electronics10131605}
}



@Article{su13137497,
AUTHOR = {Wang, Hao and Ren, Yaxin and Meng, Zhijun},
TITLE = {A Farm Management Information System for Semi-Supervised Path Planning and Autonomous Vehicle Control},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {7497},
URL = {https://www.mdpi.com/2071-1050/13/13/7497},
ISSN = {2071-1050},
ABSTRACT = {This paper presents a farm management information system targeting improvements in the ease of use and sustainability of robot farming systems. The system integrates the functionalities of field survey, path planning, monitoring, and controlling agricultural vehicles in real time. Firstly, a Grabcut-based semi-supervised field registration method is proposed for arable field detection from the orthoimage taken by the drone with an RGB camera. It partitions a complex field into simple geometric entities with simple user interaction. The average Mean Intersection over Union is about 0.95 when the field size ranges from 2.74 ha to 5.06 ha. In addition, a desktop software and a web application are developed as the entity of an FMIS. Compared to existing FMISs, this system provides more advanced features in robot farming, while providing simpler user interaction and better results. It allows clients to invoke web services and receive responses independent of programming language and platforms. Moreover, the system is compatible with other services, users, and devices following the open-source access protocol. We have evaluated the system by controlling 5 robot tractors with a 2 Hz communication frequency. The communication protocols will be publicly available to protentional users.},
DOI = {10.3390/su13137497}
}



@Article{rs13132643,
AUTHOR = {Pedro, Dário and Matos-Carvalho, João P. and Fonseca, José M. and Mora, André},
TITLE = {Collision Avoidance on Unmanned Aerial Vehicles Using Neural Network Pipelines and Flow Clustering Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2643},
URL = {https://www.mdpi.com/2072-4292/13/13/2643},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Autonomous Vehicles (UAV), while not a recent invention, have recently acquired a prominent position in many industries, and they are increasingly used not only by avid customers, but also in high-demand technical use-cases, and will have a significant societal effect in the coming years. However, the use of UAVs is fraught with significant safety threats, such as collisions with dynamic obstacles (other UAVs, birds, or randomly thrown objects). This research focuses on a safety problem that is often overlooked due to a lack of technology and solutions to address it: collisions with non-stationary objects. A novel approach is described that employs deep learning techniques to solve the computationally intensive problem of real-time collision avoidance with dynamic objects using off-the-shelf commercial vision sensors. The suggested approach’s viability was corroborated by multiple experiments, firstly in simulation, and afterward in a concrete real-world case, that consists of dodging a thrown ball. A novel video dataset was created and made available for this purpose, and transfer learning was also tested, with positive results.},
DOI = {10.3390/rs13132643}
}



@Article{en14144070,
AUTHOR = {Cichowicz, Robert and Dobrzański, Maciej},
TITLE = {3D Spatial Analysis of Particulate Matter (PM10, PM2.5 and PM1.0) and Gaseous Pollutants (H2S, SO2 and VOC) in Urban Areas Surrounding a Large Heat and Power Plant},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4070},
URL = {https://www.mdpi.com/1996-1073/14/14/4070},
ISSN = {1996-1073},
ABSTRACT = {In many regions of the world, the winter period is a time of poor air quality, due primarily to the increased use of individual and district heating systems. As a consequence, the atmospheric air contains increased concentrations of both particulate matter and gaseous pollutants (as a result of “low” emissions at altitudes of up to 40 m and “high” emissions more than 40 m above ground level). In winter, the increased pollution is very often exacerbated by meteorological conditions, including air temperature, pressure, air speed, wind direction, and thermal inversion. Here, we analyze the concentrations of particulate matter (PM10, PM2.5, and PM1.0) and gaseous pollutants (H2S, SO2, and VOC) in the immediate vicinity of a large solid fuel-fired heat and power plant located in an urban agglomeration. Two locations were selected for analysis. The first was close to an air quality measurement station in the center of a multi-family housing estate. The second was the intersection of two main communication routes. To determine the impact of “low” and “high” emissions on air quality, the selected pollutants were measured at heights of between 2 and 50 m using an unmanned aerial vehicle. The results were compared with permissible standards for the concentration of pollutants. Temperature inversion was found to have a strong influence on the level of pollutants at various heights, with higher concentrations of particulate matter registered at altitudes above 40 m. The source of PM, H2S, and SO2 pollutants was confirmed to be “low emission” from local transport, industrial plant areas, and the housing estate comprising detached houses located in the vicinity of the measuring points. “High emission” was found to be responsible for the high concentrations of VOC at altitudes of more than 40 m above the intersection and in the area of the housing estate.},
DOI = {10.3390/en14144070}
}



@Article{su13147547,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Qayyum, Siddra and Khan, Sara Imran and Mojtahedi, Mohammad},
TITLE = {UAVs in Disaster Management: Application of Integrated Aerial Imagery and Convolutional Neural Network for Flood Detection},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {7547},
URL = {https://www.mdpi.com/2071-1050/13/14/7547},
ISSN = {2071-1050},
ABSTRACT = {Floods have been a major cause of destruction, instigating fatalities and massive damage to the infrastructure and overall economy of the affected country. Flood-related devastation results in the loss of homes, buildings, and critical infrastructure, leaving no means of communication or travel for the people stuck in such disasters. Thus, it is essential to develop systems that can detect floods in a region to provide timely aid and relief to stranded people, save their livelihoods, homes, and buildings, and protect key city infrastructure. Flood prediction and warning systems have been implemented in developed countries, but the manufacturing cost of such systems is too high for developing countries. Remote sensing, satellite imagery, global positioning system, and geographical information systems are currently used for flood detection to assess the flood-related damages. These techniques use neural networks, machine learning, or deep learning methods. However, unmanned aerial vehicles (UAVs) coupled with convolution neural networks have not been explored in these contexts to instigate a swift disaster management response to minimize damage to infrastructure. Accordingly, this paper uses UAV-based aerial imagery as a flood detection method based on Convolutional Neural Network (CNN) to extract flood-related features from the images of the disaster zone. This method is effective in assessing the damage to local infrastructures in the disaster zones. The study area is based on a flood-prone region of the Indus River in Pakistan, where both pre-and post-disaster images are collected through UAVs. For the training phase, 2150 image patches are created by resizing and cropping the source images. These patches in the training dataset train the CNN model to detect and extract the regions where a flood-related change has occurred. The model is tested against both pre-and post-disaster images to validate it, which has positive flood detection results with an accuracy of 91%. Disaster management organizations can use this model to assess the damages to critical city infrastructure and other assets worldwide to instigate proper disaster responses and minimize the damages. This can help with the smart governance of the cities where all emergent disasters are addressed promptly.},
DOI = {10.3390/su13147547}
}



@Article{rs13142658,
AUTHOR = {Jozdani, Shahab and Chen, Dongmei and Chen, Wenjun and Leblanc, Sylvain G. and Prévost, Christian and Lovitt, Julie and He, Liming and Johnson, Brian A.},
TITLE = {Leveraging Deep Neural Networks to Map Caribou Lichen in High-Resolution Satellite Images Based on a Small-Scale, Noisy UAV-Derived Map},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2658},
URL = {https://www.mdpi.com/2072-4292/13/14/2658},
ISSN = {2072-4292},
ABSTRACT = {Lichen is an important food source for caribou in Canada. Lichen mapping using remote sensing (RS) images could be a challenging task, however, as lichens generally appear in unevenly distributed, small patches, and could resemble surficial features. Moreover, collecting lichen labeled data (reference data) is expensive, which restricts the application of many robust supervised classification models that generally demand a large quantity of labeled data. The goal of this study was to investigate the potential of using a very-high-spatial resolution (1-cm) lichen map of a small sample site (e.g., generated based on a single UAV scene and using field data) to train a subsequent classifier to map caribou lichen over a much larger area (~0.04 km2 vs. ~195 km2) and a lower spatial resolution image (in this case, a 50-cm WorldView-2 image). The limited labeled data from the sample site were also partially noisy due to spatial and temporal mismatching issues. For this, we deployed a recently proposed Teacher-Student semi-supervised learning (SSL) approach (based on U-Net and U-Net++ networks) involving unlabeled data to assist with improving the model performance. Our experiments showed that it was possible to scale-up the UAV-derived lichen map to the WorldView-2 scale with reasonable accuracy (overall accuracy of 85.28% and F1-socre of 84.38%) without collecting any samples directly in the WorldView-2 scene. We also found that our noisy labels were partially beneficial to the SSL robustness because they improved the false positive rate compared to the use of a cleaner training set directly collected within the same area in the WorldView-2 image. As a result, this research opens new insights into how current very high-resolution, small-scale caribou lichen maps can be used for generating more accurate large-scale caribou lichen maps from high-resolution satellite imagery.},
DOI = {10.3390/rs13142658}
}



@Article{rs13142661,
AUTHOR = {Liu, Wen-Cheng and Lu, Chien-Hsing and Huang, Wei-Che},
TITLE = {Large-Scale Particle Image Velocimetry to Measure Streamflow from Videos Recorded from Unmanned Aerial Vehicle and Fixed Imaging System},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2661},
URL = {https://www.mdpi.com/2072-4292/13/14/2661},
ISSN = {2072-4292},
ABSTRACT = {The accuracy of river velocity measurements plays an important role in the effective management of water resources. Various methods have been developed to measure river velocity. Currently, image-based techniques provide a promising approach to avoid physical contact with targeted water bodies by researchers. In this study, measured surface velocities collected under low flow and high flow conditions in the Houlong River, Taiwan, using large-scale particle image velocimetry (LSPIV) captured by an unmanned aerial vehicle (UAV) and a terrestrial fixed station were analyzed and compared. Under low flow conditions, the mean absolute errors of the measured surface velocities using LSPIV from a UAV with shooting heights of 9, 12, and 15 m fell within 0.055 ± 0.015 m/s, which was lower than that obtained using LSPIV on video recorded from a terrestrial fixed station (i.e., 0.34 m/s). The mean absolute errors obtained using LSPIV derived from UAV aerial photography at a flight height of 12 m without seeding particles and with different seeding particle densities were slightly different, and fell within the range of 0.095 ± 0.025 m/s. Under high flow conditions, the mean absolute errors associated with using LSPIV derived from terrestrial fixed photography and LSPIV derived from a UAV with flight heights of 32, 62, and 112 m were 0.46 m/s and 0.49 m/s, 0.27 m, and 0.97 m/s, respectively. A UAV flight height of 62 m yielded the best measured surface velocity result. Moreover, we also demonstrated that the optimal appropriate interrogation area and image acquisition time interval using LSPIV with a UAV were 16 × 16 pixels and 1/8 s, respectively. These two parameters should be carefully adopted to accurately measure the surface velocity of rivers.},
DOI = {10.3390/rs13142661}
}



@Article{rs13142663,
AUTHOR = {Chen, Chuanfa and Guo, Jiaojiao and Wu, Huiming and Li, Yanyan and Shi, Bo},
TITLE = {Performance Comparison of Filtering Algorithms for High-Density Airborne LiDAR Point Clouds over Complex LandScapes},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2663},
URL = {https://www.mdpi.com/2072-4292/13/14/2663},
ISSN = {2072-4292},
ABSTRACT = {Airborne light detection and ranging (LiDAR) technology has become the mainstream data source in geosciences and environmental sciences. Point cloud filtering is a prerequisite for almost all LiDAR-based applications. However, it is challenging to select a suitable filtering algorithm for handling high-density point clouds over complex landscapes. Therefore, to determine an appropriate filter on a specific environment, this paper comparatively assessed the performance of five representative filtering algorithms on six study sites with different terrain characteristics, where three plots are located in urban areas and three in forest areas. The representative filtering methods include simple morphological filter (SMRF), multiresolution hierarchical filter (MHF), slope-based filter (SBF), progressive TIN densification (PTD) and segmentation-based filter (SegBF). Results demonstrate that SMRF performs the best in urban areas, and compared to MHF, SBF, PTD and SegBF, the total error of SMRF is reduced by 1.38%, 48.21%, 48.25% and 31.03%, respectively. MHF outperforms the others in forest areas, and compared to SMRF, SBF, PTD and SegBF, the total error of MHF is reduced by 1.98%, 35.87%, 45.11% and 9.42%, respectively. Moreover, both SMRF and MHF keep a good balance between type I and II errors, which makes the produced DEMs much similar to the references. Overall, SMRF and MHF are recommended for urban and forest areas, respectively, and MHF averagely performs slightly better than SMRF on all areas with respect to kappa coefficient.},
DOI = {10.3390/rs13142663}
}



@Article{rs13142665,
AUTHOR = {Mirzazade, Ali and Popescu, Cosmin and Blanksvärd, Thomas and Täljsten, Björn},
TITLE = {Workflow for Off-Site Bridge Inspection Using Automatic Damage Detection-Case Study of the Pahtajokk Bridge},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2665},
URL = {https://www.mdpi.com/2072-4292/13/14/2665},
ISSN = {2072-4292},
ABSTRACT = {For the inspection of structures, particularly bridges, it is becoming common to replace humans with autonomous systems that use unmanned aerial vehicles (UAV). In this paper, a framework for autonomous bridge inspection using a UAV is proposed with a four-step workflow: (a) data acquisition with an efficient UAV flight path, (b) computer vision comprising training, testing and validation of convolutional neural networks (ConvNets), (c) point cloud generation using intelligent hierarchical dense structure from motion (DSfM), and (d) damage quantification. This workflow starts with planning the most efficient flight path that allows for capturing of the minimum number of images required to achieve the maximum accuracy for the desired defect size, then followed by bridge and damage recognition. Three types of autonomous detection are used: masking the background of the images, detecting areas of potential damage, and pixel-wise damage segmentation. Detection of bridge components by masking extraneous parts of the image, such as vegetation, sky, roads or rivers, can improve the 3D reconstruction in the feature detection and matching stages. In addition, detecting damaged areas involves the UAV capturing close-range images of these critical regions, and damage segmentation facilitates damage quantification using 2D images. By application of DSfM, a denser and more accurate point cloud can be generated for these detected areas, and aligned to the overall point cloud to create a digital model of the bridge. Then, this generated point cloud is evaluated in terms of outlier noise, and surface deviation. Finally, damage that has been detected is quantified and verified, based on the point cloud generated using the Terrestrial Laser Scanning (TLS) method. The results indicate this workflow for autonomous bridge inspection has potential.},
DOI = {10.3390/rs13142665}
}



@Article{s21144654,
AUTHOR = {Łabędź, Piotr and Skabek, Krzysztof and Ozimek, Paweł and Nytko, Mateusz},
TITLE = {Histogram Adjustment of Images for Improving Photogrammetric Reconstruction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4654},
URL = {https://www.mdpi.com/1424-8220/21/14/4654},
PubMedID = {34300393},
ISSN = {1424-8220},
ABSTRACT = {The accuracy of photogrammetric reconstruction depends largely on the acquisition conditions and on the quality of input photographs. This paper proposes methods of improving raster images that increase photogrammetric reconstruction accuracy. These methods are based on modifying color image histograms. Special emphasis was placed on the selection of channels of the RGB and CIE L*a*b* color models for further improvement of the reconstruction process. A methodology was proposed for assessing the quality of reconstruction based on premade reference models using positional statistics. The analysis of the influence of image enhancement on reconstruction was carried out for various types of objects. The proposed methods can significantly improve the quality of reconstruction. The superiority of methods based on the luminance channel of the L*a*b* model was demonstrated. Our studies indicated high efficiency of the histogram equalization method (HE), although these results were not highly distinctive for all performed tests.},
DOI = {10.3390/s21144654}
}



@Article{rs13142678,
AUTHOR = {Ge, Haixiao and Ma, Fei and Li, Zhenwang and Tan, Zhengzheng and Du, Changwen},
TITLE = {Improved Accuracy of Phenological Detection in Rice Breeding by Using Ensemble Models of Machine Learning Based on UAV-RGB Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2678},
URL = {https://www.mdpi.com/2072-4292/13/14/2678},
ISSN = {2072-4292},
ABSTRACT = {Accurate and timely detection of phenology at plot scale in rice breeding trails is crucial for understanding the heterogeneity of varieties and guiding field management. Traditionally, remote sensing studies of phenology detection have heavily relied on the time-series vegetation index (VI) data. However, the methodology based on time-series VI data was often limited by the temporal resolution. In this study, three types of ensemble models including hard voting (majority voting), soft voting (weighted majority voting) and model stacking, were proposed to identify the principal phenological stages of rice based on unmanned aerial vehicle (UAV) RGB imagery. These ensemble models combined RGB-VIs, color space (e.g., RGB and HSV) and textures derived from UAV-RGB imagery, and five machine learning algorithms (random forest; k-nearest neighbors; Gaussian naïve Bayes; support vector machine and logistic regression) as base models to estimate phenological stages in rice breeding. The phenological estimation models were trained on the dataset of late-maturity cultivars and tested independently on the dataset of early-medium-maturity cultivars. The results indicated that all ensemble models outperform individual machine learning models in all datasets. The soft voting strategy provided the best performance for identifying phenology with the overall accuracy of 90% and 93%, and the mean F1-scores of 0.79 and 0.81, respectively, in calibration and validation datasets, which meant that the overall accuracy and mean F1-scores improved by 5% and 7%, respectively, in comparison with those of the best individual model (GNB), tested in this study. Therefore, the ensemble models demonstrated great potential in improving the accuracy of phenology detection in rice breeding.},
DOI = {10.3390/rs13142678}
}



@Article{rs13142670,
AUTHOR = {Herzig, Paul and Borrmann, Peter and Knauer, Uwe and Klück, Hans-Christian and Kilias, David and Seiffert, Udo and Pillen, Klaus and Maurer, Andreas},
TITLE = {Evaluation of RGB and Multispectral Unmanned Aerial Vehicle (UAV) Imagery for High-Throughput Phenotyping and Yield Prediction in Barley Breeding},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2670},
URL = {https://www.mdpi.com/2072-4292/13/14/2670},
ISSN = {2072-4292},
ABSTRACT = {With advances in plant genomics, plant phenotyping has become a new bottleneck in plant breeding and the need for reliable high-throughput plant phenotyping techniques has emerged. In the face of future climatic challenges, it does not seem appropriate to continue to solely select for grain yield and a few agronomically important traits. Therefore, new sensor-based high-throughput phenotyping has been increasingly used in plant breeding research, with the potential to provide non-destructive, objective and continuous plant characterization that reveals the formation of the final grain yield and provides insights into the physiology of the plant during the growth phase. In this context, we present the comparison of two sensor systems, Red-Green-Blue (RGB) and multispectral cameras, attached to unmanned aerial vehicles (UAV), and investigate their suitability for yield prediction using different modelling approaches in a segregating barley introgression population at three environments with weekly data collection during the entire vegetation period. In addition to vegetation indices, morphological traits such as canopy height, vegetation cover and growth dynamics traits were used for yield prediction. Repeatability analyses and genotype association studies of sensor-based traits were compared with reference values from ground-based phenotyping to test the use of conventional and new traits for barley breeding. The relative height estimation of the canopy by UAV achieved high precision (up to r = 0.93) and repeatability (up to R2 = 0.98). In addition, we found a great overlap of detected significant genotypes between the reference heights and sensor-based heights. The yield prediction accuracy of both sensor systems was at the same level and reached a maximum prediction accuracy of r2 = 0.82 with a continuous increase in precision throughout the entire vegetation period. Due to the lower costs and the consumer-friendly handling of image acquisition and processing, the RGB imagery seems to be more suitable for yield prediction in this study.},
DOI = {10.3390/rs13142670}
}



@Article{rs13142683,
AUTHOR = {Liu, Zhi and Yang, Shuyuan and Feng, Zhixi and Gao, Quanwei and Wang, Min},
TITLE = {Fast SAR Autofocus Based on Ensemble Convolutional Extreme Learning Machine},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2683},
URL = {https://www.mdpi.com/2072-4292/13/14/2683},
ISSN = {2072-4292},
ABSTRACT = {Inaccurate Synthetic Aperture Radar (SAR) navigation information will lead to unknown phase errors in SAR data. Uncompensated phase errors can blur the SAR images. Autofocus is a technique that can automatically estimate phase errors from data. However, existing autofocus algorithms either have poor focusing quality or a slow focusing speed. In this paper, an ensemble learning-based autofocus method is proposed. Convolutional Extreme Learning Machine (CELM) is constructed and utilized to estimate the phase error. However, the performance of a single CELM is poor. To overcome this, a novel, metric-based combination strategy is proposed, combining multiple CELMs to further improve the estimation accuracy. The proposed model is trained with the classical bagging-based ensemble learning method. The training and testing process is non-iterative and fast. Experimental results conducted on real SAR data show that the proposed method has a good trade-off between focusing quality and speed.},
DOI = {10.3390/rs13142683}
}



@Article{su13147662,
AUTHOR = {Zhang, Jingyi and Liu, Jiaxin and Chen, Yaqi and Feng, Xiaochun and Sun, Zilai},
TITLE = {Knowledge Mapping of Machine Learning Approaches Applied in Agricultural Management—A Scientometric Review with CiteSpace},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {7662},
URL = {https://www.mdpi.com/2071-1050/13/14/7662},
ISSN = {2071-1050},
ABSTRACT = {With the continuous development of the Internet of Things, artificial intelligence, big data technology, and intelligent agriculture have become hot topics in agricultural science and technology research. Machine learning is one of the core topics in artificial intelligence, and its application has penetrated every aspect of human social life. In modern agricultural intelligent management and decision making, machine learning plays an important role in crop classification, crop disease and insect pest prediction, agricultural product price prediction, and other aspects of management and decision-making processes in agriculture. To detect and recognize the latest research developing features in a quantitative and visual way, and based on machine learning methods in agricultural management, the authors of this paper used CiteSpace bibliometric methods to analyze relevant studies on the development process and hot spots. High-value references, productive authors, country and institution distributions, journal visualizations, research topics, and emerging trends were reviewed and analyzed. According to the keyword visualization and high-value references, machine learning approaches focus on sustainable agriculture, water resources, remote sensing, and machine learning methods. The research mainly focuses on six topics: learning technology, land environment, reference evapotranspiration, decision support systems for river geography, soil management, and winter wheat, while learning technology has been the most popular in recent years.},
DOI = {10.3390/su13147662}
}



@Article{rs13142705,
AUTHOR = {Mhango, Joseph K. and Harris, Edwin W. and Green, Richard and Monaghan, James M.},
TITLE = {Mapping Potato Plant Density Variation Using Aerial Imagery and Deep Learning Techniques for Precision Agriculture},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2705},
URL = {https://www.mdpi.com/2072-4292/13/14/2705},
ISSN = {2072-4292},
ABSTRACT = {In potato (Solanum tuberosum) production, the number of tubers harvested and their sizes are related to the plant population. Field maps of the spatial variation in plant density can therefore provide a decision support tool for spatially variable harvest timing to optimize tuber sizes by allowing densely populated management zones more tuber-bulking time. Computer vision has been proposed to enumerate plant numbers using images from unmanned aerial vehicles (UAV) but inaccurate predictions in images of merged canopies remains a challenge. Some research has been done on individual potato plant bounding box prediction but there is currently no information on the spatial structure of plant density that these models may reveal and its relationship with potato yield quality attributes. In this study, the Faster Region-based Convolutional Neural Network (FRCNN) framework was used to produce a plant detection model and estimate plant densities across a UAV orthomosaic. Using aerial images of 2 mm ground sampling distance (GSD) collected from potatoes at 40 days after planting, the FRCNN model was trained to an average precision (aP) of 0.78 on unseen testing data. The model was then used to generate predictions on quadrants imposed on orthorectified rasters captured at 14 and 18 days after emergence. After spatially interpolating the plant densities, the resultant surfaces were highly correlated to manually-determined plant density (R2 = 0.80). Further correlations were observed with tuber number (r = 0.54 at Butter Hill; r = 0.53 at Horse Foxhole), marketable tuber weight per plant (r = −0.57 at Buttery Hill; r = −0.56 at Horse Foxhole) and the normalized difference vegetation index (r = 0.61). These results show that accurate two-dimensional maps of plant density can be constructed from UAV imagery with high correlation to important yield components, despite the loss of accuracy of FRCNN models in partially merged canopies.},
DOI = {10.3390/rs13142705}
}



@Article{rs13142706,
AUTHOR = {Huang, Shenjin and Han, Wenting and Chen, Haipeng and Li, Guang and Tang, Jiandong},
TITLE = {Recognizing Zucchinis Intercropped with Sunflowers in UAV Visible Images Using an Improved Method Based on OCRNet},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2706},
URL = {https://www.mdpi.com/2072-4292/13/14/2706},
ISSN = {2072-4292},
ABSTRACT = {An improved semantic segmentation method based on object contextual representations network (OCRNet) is proposed to accurately identify zucchinis intercropped with sunflowers from unmanned aerial vehicle (UAV) visible images taken over Hetao Irrigation District, Inner Mongolia, China. The proposed method improves on the performance of OCRNet in two respects. First, based on the object region context extraction structure of the OCRNet, a branch that uses the channel attention module was added in parallel to rationally use channel feature maps with different weights and reduce the noise of invalid channel features. Secondly, Lovász-Softmax loss was introduced to improve the accuracy of the object region representation in the OCRNet and optimize the final segmentation result at the object level. We compared the proposed method with extant advanced semantic segmentation methods (PSPNet, DeepLabV3+, DNLNet, and OCRNet) in two test areas to test its effectiveness. The results showed that the proposed method achieved the best semantic segmentation effect in the two test areas. More specifically, our method performed better in processing image details, segmenting field edges, and identifying intercropping fields. The proposed method has significant advantages for crop classification and intercropping recognition based on UAV visible images, and these advantages are more substantive in object-level evaluation metrics (mIoU and intercropping IoU).},
DOI = {10.3390/rs13142706}
}



@Article{land10070723,
AUTHOR = {Kelm, Kathrine and Antos, Sarah and McLaren, Robin},
TITLE = {Applying the FFP Approach to Wider Land Management Functions},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {723},
URL = {https://www.mdpi.com/2073-445X/10/7/723},
ISSN = {2073-445X},
ABSTRACT = {The initial focus of implementing the Fit-for-Purpose Land Administration (FFPLA) methodology was to address the significant, global security of tenure divide. We argue that this land tenure methodology is proving successful in scaling up the provision of security of tenure for developing countries. The increasing adoption of the FFPLA methodology has also opened opportunities and provided flexibility for the innovative use of emerging technologies to accelerate the global roll out of security of tenure, such as the use of autonomous drones and machine learning techniques applied to image analysis. Despite wider adoption of participatory approaches to the recording of land tenure, similar FFP solutions for the other components of land administration services (land value, land use and land development) and land management functions are still evolving. This article therefore explores how the FFP approach can be applied to this wider set of land administration services and land management functions. A case study methodology, using three case studies, is used to determine if the case study approaches meet the FFP criteria. The focus is on the urban environment, drawing mostly from experiences and case studies in the Urban, Disaster Risk Management, Resilience &amp; Land Global Practice of the World Bank. These opportunities for the wider application of the FFP approach and associated principles are being triggered by the innovative use of emerging new data capture technology developments. The paper examines the innovative use of these emerging technologies to identify a common set of data capture techniques and geospatial data that can be shared across a range of urban land administration and management activities. Finally, the paper discusses how individual land projects could be integrated into a more holistic land administration and management program approach and deliver a significant set of socio-economic benefits more quickly. It is found that the FFP approach can be more widely adopted across land administration and land management and in many cases can share a common set of geospatial data. The authors argue that the wider adoption and integration of these new, innovative FFP urban management approaches will require a significant cultural, professional, and institutional change from all stakeholders. Future work will explore more deeply these institutional weaknesses, which will provide a basis for guidance to the World Bank and similar institutions.},
DOI = {10.3390/land10070723}
}



@Article{atmos12070894,
AUTHOR = {Jiang, Feng and Han, Xingyu and Zhang, Wenya and Chen, Guici},
TITLE = {Atmospheric PM2.5 Prediction Using DeepAR Optimized by Sparrow Search Algorithm with Opposition-Based and Fitness-Based Learning},
JOURNAL = {Atmosphere},
VOLUME = {12},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {894},
URL = {https://www.mdpi.com/2073-4433/12/7/894},
ISSN = {2073-4433},
ABSTRACT = {There is an important significance for human health in predicting atmospheric concentration precisely. However, due to the complexity and influence of contingency, atmospheric concentration prediction is a challenging topic. In this paper, we propose a novel hybrid learning method to make point and interval predictions of PM2.5 concentration simultaneously. Firstly, we optimize Sparrow Search Algorithm (SSA) by opposition-based learning, fitness-based learning, and Lévy flight. The experiments show that the improved Sparrow Search Algorithm (FOSSA) outperforms SSA-based algorithms. In addition, the improved Sparrow Search Algorithm (FOSSA) is employed to optimize the initial weights of probabilistic forecasting model with autoregressive recurrent network (DeepAR). Then, the FOSSA–DeepAR learning method is utilized to achieve the point prediction and interval prediction of PM2.5 concentration in Beijing, China. The performance of FOSSA–DeepAR is compared with other hybrid models and a single DeepAR model. Furthermore, hourly data of PM2.5 and O3 concentration in Taian of China, O3 concentration in Beijing, China are used to verify the effectiveness and robustness of the proposed FOSSA–DeepAR learning method. Finally, the empirical results illustrate that the proposed FOSSA–DeepAR learning model can achieve more efficient and accurate predictions in both interval and point prediction.},
DOI = {10.3390/atmos12070894}
}



@Article{s21144726,
AUTHOR = {Pytka, Jarosław Alexander and Budzyński, Piotr and Tomiło, Paweł and Michałowska, Joanna and Gnapowski, Ernest and Błażejczak, Dariusz and Łukaszewicz, Andrzej},
TITLE = {IMUMETER—A Convolution Neural Network-Based Sensor for Measurement of Aircraft Ground Performance},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4726},
URL = {https://www.mdpi.com/1424-8220/21/14/4726},
PubMedID = {34300466},
ISSN = {1424-8220},
ABSTRACT = {The paper presents the development of the IMUMETER sensor, designed to study the dynamics of aircraft movement, in particular, to measure the ground performance of the aircraft. A motivation of this study was to develop a sensor capable of airplane motion measurement, especially for airfield performance, takeoff and landing. The IMUMETER sensor was designed on the basis of the method of artificial neural networks. The use of a neural network is justified by the fact that the automation of the measurement of the airplane’s ground distance during landing based on acceleration data is possible thanks to the recognition of the touchdown and stopping points, using artificial intelligence. The hardware is based on a single-board computer that works with the inertial navigation platform and a satellite navigation sensor. In the development of the IMUMETER device, original software solutions were developed and tested. The paper describes the development of the Convolution Neural Network, including the learning process based on the measurement results during flight tests of the PZL 104 Wilga 35A aircraft. The ground distance of the test airplane during landing on a grass runway was calculated using the developed neural network model. Additionally included are exemplary measurements of the landing distance of the test airplane during landing on a grass runway. The results obtained in this study can be useful in the development of artificial intelligence-based sensors, especially those for the measurement and analysis of aircraft flight dynamics.},
DOI = {10.3390/s21144726}
}



@Article{electronics10141647,
AUTHOR = {Rahmaniar, Wahyu and Wang, Wen-June and Caesarendra, Wahyu and Glowacz, Adam and Oprzędkiewicz, Krzysztof and Sułowicz, Maciej and Irfan, Muhammad},
TITLE = {Distance Measurement of Unmanned Aerial Vehicles Using Vision-Based Systems in Unknown Environments},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1647},
URL = {https://www.mdpi.com/2079-9292/10/14/1647},
ISSN = {2079-9292},
ABSTRACT = {Localization for the indoor aerial robot remains a challenging issue because global positioning system (GPS) signals often cannot reach several buildings. In previous studies, navigation of mobile robots without the GPS required the registration of building maps beforehand. This paper proposes a novel framework for addressing indoor positioning for unmanned aerial vehicles (UAV) in unknown environments using a camera. First, the UAV attitude is estimated to determine whether the robot is moving forward. Then, the camera position is estimated based on optical flow and the Kalman filter. Semantic segmentation using deep learning is carried out to get the position of the wall in front of the robot. The UAV distance is measured using the comparison of the image size ratio based on the corresponding feature points between the current and the reference of the wall images. The UAV is equipped with ultrasonic sensors to measure the distance of the UAV from the surrounded wall. The ground station receives information from the UAV to show the obstacles around the UAV and its current location. The algorithm is verified by capture the images with distance information and compared with the current image and UAV position. The experimental results show that the proposed method achieves an accuracy of 91.7% and a computation time of 8 frames per second (fps).},
DOI = {10.3390/electronics10141647}
}



@Article{rs13142721,
AUTHOR = {Li, Guang and Han, Wenting and Huang, Shenjin and Ma, Weitong and Ma, Qian and Cui, Xin},
TITLE = {Extraction of Sunflower Lodging Information Based on UAV Multi-Spectral Remote Sensing and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2721},
URL = {https://www.mdpi.com/2072-4292/13/14/2721},
ISSN = {2072-4292},
ABSTRACT = {The rapid and accurate identification of sunflower lodging is important for the assessment of damage to sunflower crops. To develop a fast and accurate method of extraction of information on sunflower lodging, this study improves the inputs to SegNet and U-Net to render them suitable for multi-band image processing. Random forest and two improved deep learning methods are combined with RGB, RGB + NIR, RGB + red-edge, and RGB + NIR + red-edge bands of multi-spectral images captured by a UAV (unmanned aerial vehicle) to construct 12 models to extract information on sunflower lodging. These models are then combined with the method used to ignore edge-related information to predict sunflower lodging. The results of experiments show that the deep learning methods were superior to the random forest method in terms of the obtained lodging information and accuracy. The predictive accuracy of the model constructed by using a combination of SegNet and RGB + NIR had the highest overall accuracy of 88.23%. Adding NIR to RGB improved the accuracy of extraction of the lodging information whereas adding red-edge reduced it. An overlay analysis of the results for the lodging area shows that the extraction error was mainly caused by the failure of the model to recognize lodging in mixed areas and low-coverage areas. The predictive accuracy of information on sunflower lodging when edge-related information was ignored was about 2% higher than that obtained by using the direct splicing method.},
DOI = {10.3390/rs13142721}
}



@Article{s21144738,
AUTHOR = {Abdollahi, Abolfazl and Pradhan, Biswajeet},
TITLE = {Urban Vegetation Mapping from Aerial Imagery Using Explainable AI (XAI)},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4738},
URL = {https://www.mdpi.com/1424-8220/21/14/4738},
PubMedID = {34300478},
ISSN = {1424-8220},
ABSTRACT = {Urban vegetation mapping is critical in many applications, i.e., preserving biodiversity, maintaining ecological balance, and minimizing the urban heat island effect. It is still challenging to extract accurate vegetation covers from aerial imagery using traditional classification approaches, because urban vegetation categories have complex spatial structures and similar spectral properties. Deep neural networks (DNNs) have shown a significant improvement in remote sensing image classification outcomes during the last few years. These methods are promising in this domain, yet unreliable for various reasons, such as the use of irrelevant descriptor features in the building of the models and lack of quality in the labeled image. Explainable AI (XAI) can help us gain insight into these limits and, as a result, adjust the training dataset and model as needed. Thus, in this work, we explain how an explanation model called Shapley additive explanations (SHAP) can be utilized for interpreting the output of the DNN model that is designed for classifying vegetation covers. We want to not only produce high-quality vegetation maps, but also rank the input parameters and select appropriate features for classification. Therefore, we test our method on vegetation mapping from aerial imagery based on spectral and textural features. Texture features can help overcome the limitations of poor spectral resolution in aerial imagery for vegetation mapping. The model was capable of obtaining an overall accuracy (OA) of 94.44% for vegetation cover mapping. The conclusions derived from SHAP plots demonstrate the high contribution of features, such as Hue, Brightness, GLCM_Dissimilarity, GLCM_Homogeneity, and GLCM_Mean to the output of the proposed model for vegetation mapping. Therefore, the study indicates that existing vegetation mapping strategies based only on spectral characteristics are insufficient to appropriately classify vegetation covers.},
DOI = {10.3390/s21144738}
}



@Article{rs13142731,
AUTHOR = {Hashemi-Beni, Leila and Kurkalova, Lyubov A. and Mulrooney, Timothy J. and Azubike, Chinazor S.},
TITLE = {Combining Multiple Geospatial Data for Estimating Aboveground Biomass in North Carolina Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2731},
URL = {https://www.mdpi.com/2072-4292/13/14/2731},
ISSN = {2072-4292},
ABSTRACT = {Mapping and quantifying forest inventories are critical for the management and development of forests for natural resource conservation and for the evaluation of the aboveground forest biomass (AGFB) technically available for bioenergy production. The AGFB estimation procedures that rely on traditional, spatially sparse field inventory samples constitute a problem for geographically diverse regions such as the state of North Carolina in the southeastern U.S. We propose an alternative AGFB estimation procedure that combines multiple geospatial data. The procedure uses land cover maps to allocate forested land areas to alternative forest types; uses the light detection and ranging (LiDAR) data to evaluate tree heights; calculates the area-total AGFB using region- and tree-type-specific functions that relate the tree heights to the AGFB. We demonstrate the procedure for a selected North Carolina region, a 2.3 km2 area randomly chosen in Duplin County. The tree diameter functions are statistically estimated based on the Forest Inventory Analysis (FIA) data, and two publicly available, open source land cover maps, Crop Data Layer (CDL) and National Land Cover Database (NLCD), are compared and contrasted as a source of information on the location and typology of forests in the study area. The assessment of the consistency of forestland mapping derived from the CDL and the NLCD data lets us estimate how the disagreement between the two alternative, widely used maps affects the AGFB estimation. The methodology and the results we present are expected to complement and inform large-scale assessments of woody biomass in the region.},
DOI = {10.3390/rs13142731}
}



@Article{ijgi10070482,
AUTHOR = {Xing, Zhizhong and Zhao, Shuanfeng and Guo, Wei and Guo, Xiaojun and Wang, Yuan},
TITLE = {Processing Laser Point Cloud in Fully Mechanized Mining Face Based on DGCNN},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {482},
URL = {https://www.mdpi.com/2220-9964/10/7/482},
ISSN = {2220-9964},
ABSTRACT = {Point cloud data can accurately and intuitively reflect the spatial relationship between the coal wall and underground fully mechanized mining equipment. However, the indirect method of point cloud feature extraction based on deep neural networks will lose some of the spatial information of the point cloud, while the direct method will lose some of the local information of the point cloud. Therefore, we propose the use of dynamic graph convolution neural network (DGCNN) to extract the geometric features of the sphere in the point cloud of the fully mechanized mining face (FMMF) in order to obtain the position of the sphere (marker) in the point cloud of the FMMF, thus providing a direct basis for the subsequent transformation of the FMMF coordinates to the national geodetic coordinates with the sphere as the intermediate medium. Firstly, we completed the production of a diversity sphere point cloud (training set) and an FMMF point cloud (test set). Secondly, we further improved the DGCNN to enhance the effect of extracting the geometric features of the sphere in the FMMF. Finally, we compared the effect of the improved DGCNN with that of PointNet and PointNet++. The results show the correctness and feasibility of using DGCNN to extract the geometric features of point clouds in the FMMF and provide a new method for the feature extraction of point clouds in the FMMF. At the same time, the results provide a direct early guarantee for analyzing the point cloud data of the FMMF under the national geodetic coordinate system in the future. This can provide an effective basis for the straightening and inclining adjustment of scraper conveyors, and it is of great significance for the transparent, unmanned, and intelligent mining of the FMMF.},
DOI = {10.3390/ijgi10070482}
}



@Article{s21144798,
AUTHOR = {Chen, Fangni and Wang, Anding and Zhang, Yu and Ni, Zhengwei and Hua, Jingyu},
TITLE = {Energy Efficient SWIPT Based Mobile Edge Computing Framework for WSN-Assisted IoT},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4798},
URL = {https://www.mdpi.com/1424-8220/21/14/4798},
PubMedID = {34300538},
ISSN = {1424-8220},
ABSTRACT = {With the increasing deployment of IoT devices and applications, a large number of devices that can sense and monitor the environment in IoT network are needed. This trend also brings great challenges, such as data explosion and energy insufficiency. This paper proposes a system that integrates mobile edge computing (MEC) technology and simultaneous wireless information and power transfer (SWIPT) technology to improve the service supply capability of WSN-assisted IoT applications. A novel optimization problem is formulated to minimize the total system energy consumption under the constraints of data transmission rate and transmitting power requirements by jointly considering power allocation, CPU frequency, offloading weight factor and energy harvest weight factor. Since the problem is non-convex, we propose a novel alternate group iteration optimization (AGIO) algorithm, which decomposes the original problem into three subproblems, and alternately optimizes each subproblem using the group interior point iterative algorithm. Numerical simulations validate that the energy consumption of our proposed design is much lower than the two benchmark algorithms. The relationship between system variables and energy consumption of the system is also discussed.},
DOI = {10.3390/s21144798}
}



@Article{agronomy11071409,
AUTHOR = {Anderson, Nicholas Todd and Walsh, Kerry Brian and Wulfsohn, Dvoralai},
TITLE = {Technologies for Forecasting Tree Fruit Load and Harvest Timing—From Ground, Sky and Time},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1409},
URL = {https://www.mdpi.com/2073-4395/11/7/1409},
ISSN = {2073-4395},
ABSTRACT = {The management and marketing of fruit requires data on expected numbers, size, quality and timing. Current practice estimates orchard fruit load based on the qualitative assessment of fruit number per tree and historical orchard yield, or manually counting a subsample of trees. This review considers technological aids assisting these estimates, in terms of: (i) improving sampling strategies by the number of units to be counted and their selection; (ii) machine vision for the direct measurement of fruit number and size on the canopy; (iii) aerial or satellite imagery for the acquisition of information on tree structural parameters and spectral indices, with the indirect assessment of fruit load; (iv) models extrapolating historical yield data with knowledge of tree management and climate parameters, and (v) technologies relevant to the estimation of harvest timing such as heat units and the proximal sensing of fruit maturity attributes. Machine vision is currently dominating research outputs on fruit load estimation, while the improvement of sampling strategies has potential for a widespread impact. Techniques based on tree parameters and modeling offer scalability, but tree crops are complicated (perennialism). The use of machine vision for flowering estimates, fruit sizing, external quality evaluation is also considered. The potential synergies between technologies are highlighted.},
DOI = {10.3390/agronomy11071409}
}



@Article{rs13142776,
AUTHOR = {Li, Yong and Shao, Zhenfeng and Huang, Xiao and Cai, Bowen and Peng, Song},
TITLE = {Meta-FSEO: A Meta-Learning Fast Adaptation with Self-Supervised Embedding Optimization for Few-Shot Remote Sensing Scene Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2776},
URL = {https://www.mdpi.com/2072-4292/13/14/2776},
ISSN = {2072-4292},
ABSTRACT = {The performance of deep learning is heavily influenced by the size of the learning samples, whose labeling process is time consuming and laborious. Deep learning algorithms typically assume that the training and prediction data are independent and uniformly distributed, which is rarely the case given the attributes and properties of different data sources. In remote sensing images, representations of urban land surfaces can vary across regions and by season, demanding rapid generalization of these surfaces in remote sensing data. In this study, we propose Meta-FSEO, a novel model for improving the performance of few-shot remote sensing scene classification in varying urban scenes. The proposed Meta-FSEO model deploys self-supervised embedding optimization for adaptive generalization in new tasks such as classifying features in new urban regions that have never been encountered during the training phase, thus balancing the requirements for feature classification tasks between multiple images collected at different times and places. We also created a loss function by weighting the contrast losses and cross-entropy losses. The proposed Meta-FSEO demonstrates a great generalization capability in remote sensing scene classification among different cities. In a five-way one-shot classification experiment with the Sentinel-1/2 Multi-Spectral (SEN12MS) dataset, the accuracy reached 63.08%. In a five-way five-shot experiment on the same dataset, the accuracy reached 74.29%. These results indicated that the proposed Meta-FSEO model outperformed both the transfer learning-based algorithm and two popular meta-learning-based methods, i.e., MAML and Meta-SGD.},
DOI = {10.3390/rs13142776}
}



@Article{rs13142778,
AUTHOR = {Lai, Zhengchao and Liu, Fei and Guo, Shangwei and Meng, Xiantong and Han, Shaokun and Li, Wenhao},
TITLE = {Onboard Real-Time Dense Reconstruction in Large Terrain Scene Using Embedded UAV Platform},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2778},
URL = {https://www.mdpi.com/2072-4292/13/14/2778},
ISSN = {2072-4292},
ABSTRACT = {Using unmanned aerial vehicles (UAVs) for remote sensing has the advantages of high flexibility, convenient operation, low cost, and wide application range. It fills the need for rapid acquisition of high-resolution aerial images in modern photogrammetry applications. Due to the insufficient parallaxes and the computation-intensive process, dense real-time reconstruction for large terrain scenes is a considerable challenge. To address these problems, we proposed a novel SLAM-based MVS (Multi-View-Stereo) approach, which can incrementally generate a dense 3D (three-dimensional) model of the terrain by using the continuous image stream during the flight. The pipeline of the proposed methodology starts with pose estimation based on SLAM algorithm. The tracked frames were then selected by a novel scene-adaptive keyframe selection method to construct a sliding window frame-set. This was followed by depth estimation using a flexible search domain approach, which can improve accuracy without increasing the iterate time or memory consumption. The whole system proposed in this study was implemented on the embedded GPU based on an UAV platform. We proposed a highly parallel and memory-efficient CUDA-based depth computing architecture, enabling the system to achieve good real-time performance. The evaluation experiments were carried out in both simulation and real-world environments. A virtual large terrain scene was built using the Gazebo simulator. The simulated UAV equipped with an RGB-D camera was used to obtain synthetic evaluation datasets, which were divided by flight altitudes (800-, 1000-, 1200 m) and terrain height difference (100-, 200-, 300 m). In addition, the system has been extensively tested on various types of real scenes. Comparison with commercial 3D reconstruction software is carried out to evaluate the precision in real-world data. According to the results on the synthetic datasets, over 93.462% of the estimation with absolute error distance of less then 0.9%. In the real-world dataset captured at 800 m flight height, more than 81.27% of our estimated point cloud are less then 5 m difference with the results of Photoscan. All evaluation experiments show that the proposed approach outperforms the state-of-the-art ones in terms of accuracy and efficiency.},
DOI = {10.3390/rs13142778}
}



@Article{rs13142780,
AUTHOR = {Shukla, Shivang and Tiddeman, Bernard and Miles, Helen C.},
TITLE = {A Wide Area Multiview Static Crowd Estimation System Using UAV and 3D Training Simulator},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2780},
URL = {https://www.mdpi.com/2072-4292/13/14/2780},
ISSN = {2072-4292},
ABSTRACT = {Crowd size estimation is a challenging problem, especially when the crowd is spread over a significant geographical area. It has applications in monitoring of rallies and demonstrations and in calculating the assistance requirements in humanitarian disasters. Therefore, accomplishing a crowd surveillance system for large crowds constitutes a significant issue. UAV-based techniques are an appealing choice for crowd estimation over a large region, but they present a variety of interesting challenges, such as integrating per-frame estimates through a video without counting individuals twice. Large quantities of annotated training data are required to design, train, and test such a system. In this paper, we have first reviewed several crowd estimation techniques, existing crowd simulators and data sets available for crowd analysis. Later, we have described a simulation system to provide such data, avoiding the need for tedious and error-prone manual annotation. Then, we have evaluated synthetic video from the simulator using various existing single-frame crowd estimation techniques. Our findings show that the simulated data can be used to train and test crowd estimation, thereby providing a suitable platform to develop such techniques. We also propose an automated UAV-based 3D crowd estimation system that can be used for approximately static or slow-moving crowds, such as public events, political rallies, and natural or man-made disasters. We evaluate the results by applying our new framework to a variety of scenarios with varying crowd sizes. The proposed system gives promising results using widely accepted metrics including MAE, RMSE, Precision, Recall, and F1 score to validate the results.},
DOI = {10.3390/rs13142780}
}



@Article{rs13142787,
AUTHOR = {Gibril, Mohamed Barakat A. and Shafri, Helmi Zulhaidi Mohd and Shanableh, Abdallah and Al-Ruzouq, Rami and Wayayok, Aimrun and Hashim, Shaiful Jahari},
TITLE = {Deep Convolutional Neural Network for Large-Scale Date Palm Tree Mapping from UAV-Based Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2787},
URL = {https://www.mdpi.com/2072-4292/13/14/2787},
ISSN = {2072-4292},
ABSTRACT = {Large-scale mapping of date palm trees is vital for their consistent monitoring and sustainable management, considering their substantial commercial, environmental, and cultural value. This study presents an automatic approach for the large-scale mapping of date palm trees from very-high-spatial-resolution (VHSR) unmanned aerial vehicle (UAV) datasets, based on a deep learning approach. A U-Shape convolutional neural network (U-Net), based on a deep residual learning framework, was developed for the semantic segmentation of date palm trees. A comprehensive set of labeled data was established to enable the training and evaluation of the proposed segmentation model and increase its generalization capability. The performance of the proposed approach was compared with those of various state-of-the-art fully convolutional networks (FCNs) with different encoder architectures, including U-Net (based on VGG-16 backbone), pyramid scene parsing network, and two variants of DeepLab V3+. Experimental results showed that the proposed model outperformed other FCNs in the validation and testing datasets. The generalizability evaluation of the proposed approach on a comprehensive and complex testing dataset exhibited higher classification accuracy and showed that date palm trees could be automatically mapped from VHSR UAV images with an F-score, mean intersection over union, precision, and recall of 91%, 85%, 0.91, and 0.92, respectively. The proposed approach provides an efficient deep learning architecture for the automatic mapping of date palm trees from VHSR UAV-based images.},
DOI = {10.3390/rs13142787}
}



@Article{su13147925,
AUTHOR = {Munawar, Hafiz Suliman and Hammad, Ahmed W. A. and Waller, S. Travis and Thaheem, Muhammad Jamaluddin and Shrestha, Asheem},
TITLE = {An Integrated Approach for Post-Disaster Flood Management Via the Use of Cutting-Edge Technologies and UAVs: A Review},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {7925},
URL = {https://www.mdpi.com/2071-1050/13/14/7925},
ISSN = {2071-1050},
ABSTRACT = {Rapid advances that improve flood management have facilitated the disaster response by providing first aid services, finding safe routes, maintaining communication and developing flood maps. Different technologies such as image processing, satellite imagery, synthetic imagery and integrated approaches have been extensively analysed in the literature for disaster operations. There is a need to review cutting-edge technologies for flood management. This paper presents a review of the latest advancements in the flood management domain based on image processing, artificial intelligence and integrated approaches with a focus on post-disaster. It answers the following research questions: (1) What are the latest developments in image processing for flood management in a post-disaster scenario? (2) What are the latest techniques for flood management based on artificial intelligence in a post-disaster scenario? (3) What are the existing gaps in the selected technologies for post-disaster? (4) How can the authorities improve the existing post-disaster management operation with cutting-edge technologies? A novel framework has been proposed to optimise flood management with the application of a holistic approach.},
DOI = {10.3390/su13147925}
}



@Article{app11146524,
AUTHOR = {Pérez-González, Andrés and Jaramillo-Duque, Álvaro and Cano-Quintero, Juan Bernardo},
TITLE = {Automatic Boundary Extraction for Photovoltaic Plants Using the Deep Learning U-Net Model},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {6524},
URL = {https://www.mdpi.com/2076-3417/11/14/6524},
ISSN = {2076-3417},
ABSTRACT = {Nowadays, the world is in a transition towards renewable energy solar being one of the most promising sources used today. However, Solar Photovoltaic (PV) systems present great challenges for their proper performance such as dirt and environmental conditions that may reduce the output energy of the PV plants. For this reason, inspection and periodic maintenance are essential to extend useful life. The use of unmanned aerial vehicles (UAV) for inspection and maintenance of PV plants favor a timely diagnosis. UAV path planning algorithm over a PV facility is required to better perform this task. Therefore, it is necessary to explore how to extract the boundary of PV facilities with some techniques. This research work focuses on an automatic boundary extraction method of PV plants from imagery using a deep neural network model with a U-net structure. The results obtained were evaluated by comparing them with other reported works. Additionally, to achieve the boundary extraction processes, the standard metrics Intersection over Union (IoU) and the Dice Coefficient (DC) were considered to make a better conclusion among all methods. The experimental results evaluated on the Amir dataset show that the proposed approach can significantly improve the boundary and segmentation performance in the test stage up to 90.42% and 91.42% as calculated by IoU and DC metrics, respectively. Furthermore, the training period was faster. Consequently, it is envisaged that the proposed U-Net model will be an advantage in remote sensing image segmentation.},
DOI = {10.3390/app11146524}
}



@Article{s21144845,
AUTHOR = {Li, Jingbo and Li, Changchun and Fei, Shuaipeng and Ma, Chunyan and Chen, Weinan and Ding, Fan and Wang, Yilin and Li, Yacong and Shi, Jinjin and Xiao, Zhen},
TITLE = {Wheat Ear Recognition Based on RetinaNet and Transfer Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4845},
URL = {https://www.mdpi.com/1424-8220/21/14/4845},
PubMedID = {34300585},
ISSN = {1424-8220},
ABSTRACT = {The number of wheat ears is an essential indicator for wheat production and yield estimation, but accurately obtaining wheat ears requires expensive manual cost and labor time. Meanwhile, the characteristics of wheat ears provide less information, and the color is consistent with the background, which can be challenging to obtain the number of wheat ears required. In this paper, the performance of Faster regions with convolutional neural networks (Faster R-CNN) and RetinaNet to predict the number of wheat ears for wheat at different growth stages under different conditions is investigated. The results show that using the Global WHEAT dataset for recognition, the RetinaNet method, and the Faster R-CNN method achieve an average accuracy of 0.82 and 0.72, with the RetinaNet method obtaining the highest recognition accuracy. Secondly, using the collected image data for recognition, the R2 of RetinaNet and Faster R-CNN after transfer learning is 0.9722 and 0.8702, respectively, indicating that the recognition accuracy of the RetinaNet method is higher on different data sets. We also tested wheat ears at both the filling and maturity stages; our proposed method has proven to be very robust (the R2 is above 90). This study provides technical support and a reference for automatic wheat ear recognition and yield estimation.},
DOI = {10.3390/s21144845}
}



@Article{rs13142794,
AUTHOR = {Ran, Shuhao and Gao, Xianjun and Yang, Yuanwei and Li, Shaohua and Zhang, Guangbin and Wang, Ping},
TITLE = {Building Multi-Feature Fusion Refined Network for Building Extraction from High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2794},
URL = {https://www.mdpi.com/2072-4292/13/14/2794},
ISSN = {2072-4292},
ABSTRACT = {Deep learning approaches have been widely used in building automatic extraction tasks and have made great progress in recent years. However, the missing detection and wrong detection causing by spectrum confusion is still a great challenge. The existing fully convolutional networks (FCNs) cannot effectively distinguish whether the feature differences are from one building or the building and its adjacent non-building objects. In order to overcome the limitations, a building multi-feature fusion refined network (BMFR-Net) was presented in this paper to extract buildings accurately and completely. BMFR-Net is based on an encoding and decoding structure, mainly consisting of two parts: the continuous atrous convolution pyramid (CACP) module and the multiscale output fusion constraint (MOFC) structure. The CACP module is positioned at the end of the contracting path and it effectively minimizes the loss of effective information in multiscale feature extraction and fusion by using parallel continuous small-scale atrous convolution. To improve the ability to aggregate semantic information from the context, the MOFC structure performs predictive output at each stage of the expanding path and integrates the results into the network. Furthermore, the multilevel joint weighted loss function effectively updates parameters well away from the output layer, enhancing the learning capacity of the network for low-level abstract features. The experimental results demonstrate that the proposed BMFR-Net outperforms the other five state-of-the-art approaches in both visual interpretation and quantitative evaluation.},
DOI = {10.3390/rs13142794}
}



@Article{rs13142796,
AUTHOR = {Vandendaele, Bastien and Fournier, Richard A. and Vepakomma, Udayalakshmi and Pelletier, Gaetan and Lejeune, Philippe and Martin-Ducup, Olivier},
TITLE = {Estimation of Northern Hardwood Forest Inventory Attributes Using UAV Laser Scanning (ULS): Transferability of Laser Scanning Methods and Comparison of Automated Approaches at the Tree- and Stand-Level},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2796},
URL = {https://www.mdpi.com/2072-4292/13/14/2796},
ISSN = {2072-4292},
ABSTRACT = {UAV laser scanning (ULS) has the potential to support forest operations since it provides high-density data with flexible operational conditions. This study examined the use of ULS systems to estimate several tree attributes from an uneven-aged northern hardwood stand. We investigated: (1) the transferability of raster-based and bottom-up point cloud-based individual tree detection (ITD) algorithms to ULS data; and (2) automated approaches to the retrieval of tree-level (i.e., height, crown diameter (CD), DBH) and stand-level (i.e., tree count, basal area (BA), DBH-distribution) forest inventory attributes. These objectives were studied under leaf-on and leaf-off canopy conditions. Results achieved from ULS data were cross-compared with ALS and TLS to better understand the potential and challenges faced by different laser scanning systems and methodological approaches in hardwood forest environments. The best results that characterized individual trees from ULS data were achieved under leaf-off conditions using a point cloud-based bottom-up ITD. The latter outperformed the raster-based ITD, improving the accuracy of tree detection (from 50% to 71%), crown delineation (from R2 = 0.29 to R2 = 0.61), and prediction of tree DBH (from R2 = 0.36 to R2 = 0.67), when compared with values that were estimated from reference TLS data. Major improvements were observed for the detection of trees in the lower canopy layer (from 9% with raster-based ITD to 51% with point cloud-based ITD) and in the intermediate canopy layer (from 24% with raster-based ITD to 59% with point cloud-based ITD). Under leaf-on conditions, LiDAR data from aerial systems include substantial signal occlusion incurred by the upper canopy. Under these conditions, the raster-based ITD was unable to detect low-level canopy trees (from 5% to 15% of trees detected from lower and intermediate canopy layers, respectively), resulting in a tree detection rate of about 40% for both ULS and ALS data. The cylinder-fitting method used to estimate tree DBH under leaf-off conditions did not meet inventory standards when compared to TLS DBH, resulting in RMSE = 7.4 cm, Bias = 3.1 cm, and R2 = 0.75. Yet, it yielded more accurate estimates of the BA (+3.5%) and DBH-distribution of the stand than did allometric models −12.9%), when compared with in situ field measurements. Results suggest that the use of bottom-up ITD on high-density ULS data from leaf-off hardwood forest leads to promising results when estimating trees and stand attributes, which opens up new possibilities for supporting forest inventories and operations.},
DOI = {10.3390/rs13142796}
}



@Article{jimaging7070118,
AUTHOR = {Banfi, Fabrizio and Mandelli, Alessandro},
TITLE = {Computer Vision Meets Image Processing and UAS PhotoGrammetric Data Integration: From HBIM to the eXtended Reality Project of Arco della Pace in Milan and Its Decorative Complexity},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {118},
URL = {https://www.mdpi.com/2313-433X/7/7/118},
ISSN = {2313-433X},
ABSTRACT = {This study aims to enrich the knowledge of the monument Arco della Pace in Milan, surveying and modelling the sculpture that crowns the upper part of the building. The statues and the decorative apparatus are recorded with the photogrammetric technique using both a terrestrial camera and an Unmanned Aerial Vehicle (UAV). Research results and performance are oriented to improve computer vision and image processing integration with Unmanned Aerial System (UAS) photogrammetric data to enhance interactivity and information sharing between user and digital heritage models. The vast number of images captured from terrestrial and aerial photogrammetry will also permit to use of the Historic Building Information Modelling (HBIM) model in an eXtended Reality (XR) project developed ad-hoc, allowing different types of users (professionals, non-expert users, virtual tourists, and students) and devices (mobile phones, tablets, PCs, VR headsets) to access details and information that are not visible from the ground.},
DOI = {10.3390/jimaging7070118}
}



@Article{f12070943,
AUTHOR = {Vásquez, Felipe and Cravero, Ania and Castro, Manuel and Acevedo, Patricio},
TITLE = {Decision Support System Development of Wildland Fire: A Systematic Mapping},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {943},
URL = {https://www.mdpi.com/1999-4907/12/7/943},
ISSN = {1999-4907},
ABSTRACT = {Wildland fires have been a rising problem on the worldwide level, generating ecological and economic losses. Specifically, between wildland fire types, uncontrolled fires are critical due to the potential damage to the ecosystem and their effects on the soil, and, in the last decade, different technologies have been applied to fight them. Selecting a specific technology and Decision Support Systems (DSS) is fundamental, since the results and validity of this could drastically oscillate according to the different environmental and geographic factors of the terrain to be studied. Given the above, a systematic mapping was realized, with the purpose of recognizing the most-used DSS and context where they have been applied. One hundred and eighty-three studies were found that used different types of DSS to solve problems of detection, prediction, prevention, monitoring, simulation, administration, and access to routes. The concepts key to the type of solution are related to the use or development of systems or Information and Communication Technologies (ICT) in the computer science area. Although the use of BA and Big Data has increased in recent years, there are still many challenges to face, such as staff training, the friendly environment of DSS, and real-time decision-making.},
DOI = {10.3390/f12070943}
}



@Article{rs13142822,
AUTHOR = {Lin, Zhe and Guo, Wenxuan},
TITLE = {Cotton Stand Counting from Unmanned Aerial System Imagery Using MobileNet and CenterNet Deep Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2822},
URL = {https://www.mdpi.com/2072-4292/13/14/2822},
ISSN = {2072-4292},
ABSTRACT = {An accurate stand count is a prerequisite to determining the emergence rate, assessing seedling vigor, and facilitating site-specific management for optimal crop production. Traditional manual counting methods in stand assessment are labor intensive and time consuming for large-scale breeding programs or production field operations. This study aimed to apply two deep learning models, the MobileNet and CenterNet, to detect and count cotton plants at the seedling stage with unmanned aerial system (UAS) images. These models were trained with two datasets containing 400 and 900 images with variations in plant size and soil background brightness. The performance of these models was assessed with two testing datasets of different dimensions, testing dataset 1 with 300 by 400 pixels and testing dataset 2 with 250 by 1200 pixels. The model validation results showed that the mean average precision (mAP) and average recall (AR) were 79% and 73% for the CenterNet model, and 86% and 72% for the MobileNet model with 900 training images. The accuracy of cotton plant detection and counting was higher with testing dataset 1 for both CenterNet and MobileNet models. The results showed that the CenterNet model had a better overall performance for cotton plant detection and counting with 900 training images. The results also indicated that more training images are required when applying object detection models on images with different dimensions from training datasets. The mean absolute percentage error (MAPE), coefficient of determination (R2), and the root mean squared error (RMSE) values of the cotton plant counting were 0.07%, 0.98 and 0.37, respectively, with testing dataset 1 for the CenterNet model with 900 training images. Both MobileNet and CenterNet models have the potential to accurately and timely detect and count cotton plants based on high-resolution UAS images at the seedling stage. This study provides valuable information for selecting the right deep learning tools and the appropriate number of training images for object detection projects in agricultural applications.},
DOI = {10.3390/rs13142822}
}



@Article{rs13142827,
AUTHOR = {Hu, Pengcheng and Chapman, Scott C. and Jin, Huidong and Guo, Yan and Zheng, Bangyou},
TITLE = {Comparison of Modelling Strategies to Estimate Phenotypic Values from an Unmanned Aerial Vehicle with Spectral and Temporal Vegetation Indexes},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2827},
URL = {https://www.mdpi.com/2072-4292/13/14/2827},
ISSN = {2072-4292},
ABSTRACT = {Aboveground dry weight (AGDW) and leaf area index (LAI) are indicators of crop growth status and grain yield as affected by interactions of genotype, environment, and management. Unmanned aerial vehicle (UAV) based remote sensing provides cost-effective and non-destructive methods for the high-throughput phenotyping of crop traits (e.g., AGDW and LAI) through the integration of UAV-derived vegetation indexes (VIs) with statistical models. However, the effects of different modelling strategies that use different dataset compositions of explanatory variables (i.e., combinations of sources and temporal combinations of the VI datasets) on estimates of AGDW and LAI have rarely been evaluated. In this study, we evaluated the effects of three sources of VIs (visible, spectral, and combined) and three types of temporal combinations of the VI datasets (mono-, multi-, and full-temporal) on estimates of AGDW and LAI. The VIs were derived from visible (RGB) and multi-spectral imageries, which were acquired by a UAV-based platform over a wheat trial at five sampling dates before flowering. Partial least squares regression models were built with different modelling strategies to estimate AGDW and LAI at each prediction date. The results showed that models built with the three sources of mono-temporal VIs obtained similar performances for estimating AGDW (RRMSE = 11.86% to 15.80% for visible, 10.25% to 16.70% for spectral, and 10.25% to 16.70% for combined VIs) and LAI (RRMSE = 13.30% to 22.56% for visible, 12.04% to 22.85% for spectral, and 13.45% to 22.85% for combined VIs) across prediction dates. Mono-temporal models built with visible VIs outperformed the other two sources of VIs in general. Models built with mono-temporal VIs generally obtained better estimates than models with multi- and full-temporal VIs. The results suggested that the use of UAV-derived visible VIs can be an alternative to multi-spectral VIs for high-throughput and in-season estimates of AGDW and LAI. The combination of modelling strategies that used mono-temporal datasets and a self-calibration method demonstrated the potential for in-season estimates of AGDW and LAI (RRMSE normally less than 15%) in breeding or agronomy trials.},
DOI = {10.3390/rs13142827}
}



@Article{agronomy11071435,
AUTHOR = {Che’Ya, Nik Norasma and Dunwoody, Ernest and Gupta, Madan},
TITLE = {Assessment of Weed Classification Using Hyperspectral Reflectance and Optimal Multispectral UAV Imagery},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1435},
URL = {https://www.mdpi.com/2073-4395/11/7/1435},
ISSN = {2073-4395},
ABSTRACT = {Weeds compete with crops and are hard to differentiate and identify due to their similarities in color, shape, and size. In this study, the weed species present in sorghum (sorghum bicolor (L.) Moench) fields, such as amaranth (Amaranthus macrocarpus), pigweed (Portulaca oleracea), mallow weed (Malva sp.), nutgrass (Cyperus rotundus), liver seed grass (Urochoa panicoides), and Bellive (Ipomea plebeian), were discriminated using hyperspectral data and were detected and analyzed using multispectral images. Discriminant analysis (DA) was used to identify the most significant spectral bands in order to discriminate weeds from sorghum using hyperspectral data. The results demonstrated good separation accuracy for Amaranthus macrocarpus, Urochoa panicoides, Malva sp., Cyperus rotundus, and Sorghum bicolor (L.) Moench at 440, 560, 680, 710, 720, and 850 nm. Later, the multispectral images of these six bands were collected to detect weeds in the sorghum crop fields using object-based image analysis (OBIA). The results showed that the differences between sorghum and weed species were detectable using the six selected bands, with data collected using an unmanned aerial vehicle. Here, the highest spatial resolution had the highest accuracy for weed detection. It was concluded that each weed was successfully discriminated using hyperspectral data and was detectable using multispectral data with higher spatial resolution.},
DOI = {10.3390/agronomy11071435}
}



@Article{rs13142837,
AUTHOR = {Diez, Yago and Kentsch, Sarah and Fukuda, Motohisa and Caceres, Maximo Larry Lopez and Moritake, Koma and Cabezas, Mariano},
TITLE = {Deep Learning in Forestry Using UAV-Acquired RGB Data: A Practical Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2837},
URL = {https://www.mdpi.com/2072-4292/13/14/2837},
ISSN = {2072-4292},
ABSTRACT = {Forests are the planet’s main CO2 filtering agent as well as important economical, environmental and social assets. Climate change is exerting an increased stress, resulting in a need for improved research methodologies to study their health, composition or evolution. Traditionally, information about forests has been collected using expensive and work-intensive field inventories, but in recent years unoccupied autonomous vehicles (UAVs) have become very popular as they represent a simple and inexpensive way to gather high resolution data of large forested areas. In addition to this trend, deep learning (DL) has also been gaining much attention in the field of forestry as a way to include the knowledge of forestry experts into automatic software pipelines tackling problems such as tree detection or tree health/species classification. Among the many sensors that UAVs can carry, RGB cameras are fast, cost-effective and allow for straightforward data interpretation. This has resulted in a large increase in the amount of UAV-acquired RGB data available for forest studies. In this review, we focus on studies that use DL and RGB images gathered by UAVs to solve practical forestry research problems. We summarize the existing studies, provide a detailed analysis of their strengths paired with a critical assessment on common methodological problems and include other information, such as available public data and code resources that we believe can be useful for researchers that want to start working in this area. We structure our discussion using three main families of forestry problems: (1) individual Tree Detection, (2) tree Species Classification, and (3) forest Anomaly Detection (forest fires and insect Infestation).},
DOI = {10.3390/rs13142837}
}



@Article{electronics10141737,
AUTHOR = {Lee, Wooseop and Kang, Min-Hee and Song, Jaein and Hwang, Keeyeon},
TITLE = {The Design of Preventive Automated Driving Systems Based on Convolutional Neural Network},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1737},
URL = {https://www.mdpi.com/2079-9292/10/14/1737},
ISSN = {2079-9292},
ABSTRACT = {As automated vehicles have been considered one of the important trends in intelligent transportation systems, various research is being conducted to enhance their safety. In particular, the importance of technologies for the design of preventive automated driving systems, such as detection of surrounding objects and estimation of distance between vehicles. Object detection is mainly performed through cameras and LiDAR, but due to the cost and limits of LiDAR’s recognition distance, the need to improve Camera recognition technique, which is relatively convenient for commercialization, is increasing. This study learned convolutional neural network (CNN)-based faster regions with CNN (Faster R-CNN) and You Only Look Once (YOLO) V2 to improve the recognition techniques of vehicle-mounted monocular cameras for the design of preventive automated driving systems, recognizing surrounding vehicles in black box highway driving videos and estimating distances from surrounding vehicles through more suitable models for automated driving systems. Moreover, we learned the PASCAL visual object classes (VOC) dataset for model comparison. Faster R-CNN showed similar accuracy, with a mean average precision (mAP) of 76.4 to YOLO with a mAP of 78.6, but with a Frame Per Second (FPS) of 5, showing slower processing speed than YOLO V2 with an FPS of 40, and a Faster R-CNN, which we had difficulty detecting. As a result, YOLO V2, which shows better performance in accuracy and processing speed, was determined to be a more suitable model for automated driving systems, further progressing in estimating the distance between vehicles. For distance estimation, we conducted coordinate value conversion through camera calibration and perspective transform, set the threshold to 0.7, and performed object detection and distance estimation, showing more than 80% accuracy for near-distance vehicles. Through this study, it is believed that it will be able to help prevent accidents in automated vehicles, and it is expected that additional research will provide various accident prevention alternatives such as calculating and securing appropriate safety distances, depending on the vehicle types.},
DOI = {10.3390/electronics10141737}
}



@Article{en14144365,
AUTHOR = {Liu, Jingjing and Liu, Chuanyang and Wu, Yiquan and Xu, Huajie and Sun, Zuo},
TITLE = {An Improved Method Based on Deep Learning for Insulator Fault Detection in Diverse Aerial Images},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4365},
URL = {https://www.mdpi.com/1996-1073/14/14/4365},
ISSN = {1996-1073},
ABSTRACT = {Insulators play a significant role in high-voltage transmission lines, and detecting insulator faults timely and accurately is important for the safe and stable operation of power grids. Since insulator faults are extremely small and the backgrounds of aerial images are complex, insulator fault detection is a challenging task for automatically inspecting transmission lines. In this paper, a method based on deep learning is proposed for insulator fault detection in diverse aerial images. Firstly, to provide sufficient insulator fault images for training, a novel insulator fault dataset named “InSF-detection” is constructed. Secondly, an improved YOLOv3 model is proposed to reuse features and prevent feature loss. To improve the accuracy of insulator fault detection, SPP-networks and a multi-scale prediction network are employed for the improved YOLOv3 model. Finally, the improved YOLOv3 model and the compared models are trained and tested on the “InSF-detection”. The average precision (AP) of the improved YOLOv3 model is superior to YOLOv3 and YOLOv3-dense models, and just a little (1.2%) lower than that of CSPD-YOLO model; more importantly, the memory usage of the improved YOLOv3 model is 225 MB, which is the smallest between the four compared models. The experimental results and analysis validate that the improved YOLOv3 model achieves good performance for insulator fault detection in aerial images with diverse backgrounds.},
DOI = {10.3390/en14144365}
}



@Article{s21144929,
AUTHOR = {Hallee, Mitchell J. and Napolitano, Rebecca K. and Reinhart, Wesley F. and Glisic, Branko},
TITLE = {Crack Detection in Images of Masonry Using CNNs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4929},
URL = {https://www.mdpi.com/1424-8220/21/14/4929},
PubMedID = {34300668},
ISSN = {1424-8220},
ABSTRACT = {While there is a significant body of research on crack detection by computer vision methods in concrete and asphalt, less attention has been given to masonry. We train a convolutional neural network (CNN) on images of brick walls built in a laboratory environment and test its ability to detect cracks in images of brick-and-mortar structures both in the laboratory and on real-world images taken from the internet. We also compare the performance of the CNN to a variety of simpler classifiers operating on handcrafted features. We find that the CNN performed better on the domain adaptation from laboratory to real-world images than these simple models. However, we also find that performance is significantly better in performing the reverse domain adaptation task, where the simple classifiers are trained on real-world images and tested on the laboratory images. This work demonstrates the ability to detect cracks in images of masonry using a variety of machine learning methods and provides guidance for improving the reliability of such models when performing domain adaptation for crack detection in masonry.},
DOI = {10.3390/s21144929}
}



@Article{electronics10141744,
AUTHOR = {Wazirali, Raniyah and Ahmad, Rami and Al-Amayreh, Ahmed and Al-Madi, Mohammad and Khalifeh, Ala’},
TITLE = {Secure Watermarking Schemes and Their Approaches in the IoT Technology: An Overview},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1744},
URL = {https://www.mdpi.com/2079-9292/10/14/1744},
ISSN = {2079-9292},
ABSTRACT = {Information security is considered one of the most important issues in various infrastructures related to the field of data communication where most of the modern studies focus on finding effective and low-weight secure approaches. Digital watermarking is a trend in security techniques that hides data by using data embedding and data extraction processes. Watermarking technology is integrated into different frames without adding an overheard as in the conventional encryption. Therefore, it is efficient to be used in data encryption for applications that run over limited resources such as the Internet of Things (IoT). In this paper, different digital watermarking algorithms and approaches are presented. Additionally, watermarking requirements and challenges are illustrated in detail. Moreover, the common architecture of the watermarking system is described. Furthermore, IoT technology and its challenges are highlighted. Finally, the paper provides the motivations, objectives and applications of the recent secure watermarking techniques in IoT and summarises them into one table. In addition, the paper highlights the potential to apply the modified watermark algorithms to secure IoT networks.},
DOI = {10.3390/electronics10141744}
}



@Article{electronics10151748,
AUTHOR = {Wei, Baoquan and Zuo, Yong and Liu, Yande and Luo, Wei and Wen, Kaiyun and Deng, Fangming},
TITLE = {Novel MOA Fault Detection Technology Based on Small Sample Infrared Image},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {1748},
URL = {https://www.mdpi.com/2079-9292/10/15/1748},
ISSN = {2079-9292},
ABSTRACT = {This paper proposes a novel metal oxide arrester (MOA) fault detection technology based on a small sample infrared image. The research is carried out from the detection process and data enhancement. A lightweight MOA identification and location algorithm is designed at the edge, which can not only reduce the amount of data uploaded, but also reduce the search space of cloud algorithm. In order to improve the accuracy and generalization ability of the defect detection model under the condition of small samples, a multi-model fusion detection algorithm is proposed. Different features of the image are extracted by multiple convolutional neural networks, and then multiple classifiers are trained. Finally, the weighted voting strategy is used for fault diagnosis. In addition, the extended model of fault samples is constructed by transfer learning and deep convolutional generative adversarial networks (DCGAN) to solve the problem of unbalanced training data sets. The experimental results show that the proposed method can realize the accurate location of arrester under the condition of small samples, and after the data expansion, the recognition rate of arrester anomalies can be improved from 83% to 85%, showing high effectiveness and reliability.},
DOI = {10.3390/electronics10151748}
}



@Article{s21154961,
AUTHOR = {Chen, Xiao and Chen, Zhuang and Liu, Guoxiang and Chen, Kun and Wang, Lu and Xiang, Wei and Zhang, Rui},
TITLE = {Railway Overhead Contact System Point Cloud Classification},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {4961},
URL = {https://www.mdpi.com/1424-8220/21/15/4961},
PubMedID = {34372197},
ISSN = {1424-8220},
ABSTRACT = {As the railway overhead contact system (OCS) is the key component along the high-speed railway, it is crucial to detect the quality of the OCS. Compared with conventional manual OCS detection, the vehicle-mounted Light Detection and Ranging (LiDAR) technology has advantages such as high efficiency and precision, which can solve the problems of OCS detection difficulty, low efficiency, and high risk. Aiming at the contact cables, return current cables, and catenary cables in the railway vehicle-mounted LiDAR OCS point cloud, this paper used a scale adaptive feature classification algorithm and the DBSCAN (density-based spatial clustering of applications with noise) algorithm considering OCS characteristics to classify the OCS point cloud. Finally, the return current cables, catenary cables, and contact cables in the OCS were accurately classified and extracted. To verify the accuracy of the method presented in this paper, we compared the experimental results of this article with the classification results of TerraSolid, and the classification results were evaluated in terms of four accuracy indicators. According to statistics, the average accuracy of using this method to extract two sets of OCS point clouds is 99.83% and 99.89%, respectively; the average precision is 100% and 99.97%, respectively; the average recall is 99.16% and 99.42%, respectively; and the average overall accuracy is 99.58% and 99.69% respectively, which is overall better than TerraSolid. The experimental results showed that this approach could accurately and quickly extract the complete OCS from the point cloud. It provides a new method for processing railway OCS point clouds and has high engineering application value in railway component detection.},
DOI = {10.3390/s21154961}
}



@Article{agriculture11080687,
AUTHOR = {Mesa, Armacheska Rivero and Chiang, John Y.},
TITLE = {Multi-Input Deep Learning Model with RGB and Hyperspectral Imaging for Banana Grading},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {687},
URL = {https://www.mdpi.com/2077-0472/11/8/687},
ISSN = {2077-0472},
ABSTRACT = {Grading is a vital process during the postharvest of horticultural products as it dramatically affects consumer preference and satisfaction when goods reach the market. Manual grading is time-consuming, uneconomical, and potentially destructive. A non-invasive automated system for export-quality banana tiers was developed, which utilized RGB, hyperspectral imaging, and deep learning techniques. A real dataset of pre-classified banana tiers based on quality and size (Class 1 for export quality bananas, Class 2 for the local market, and Class 3 for defective fruits) was utilized using international standards. The multi-input model achieved an excellent overall accuracy of 98.45% using only a minimal number of samples compared to other methods in the literature. The model was able to incorporate both external and internal properties of the fruit. The size of the banana was used as a feature for grade classification as well as other morphological features using RGB imaging, while reflectance values that offer valuable information and have shown a high correlation with the internal features of fruits were obtained through hyperspectral imaging. This study highlighted the combined strengths of RGB and hyperspectral imaging in grading bananas, and this can serve as a paradigm for grading other horticultural crops. The fast-processing time of the multi-input model developed can be advantageous when it comes to actual farm postharvest processes.},
DOI = {10.3390/agriculture11080687}
}



@Article{fi13080187,
AUTHOR = {Cheng, Wei-Min and Wang, Sheng-Ming},
TITLE = {Research on the General Algorithm and Benefit of Real-Time Positioning Advertising System—Based on the Use of 5G Base Station Data},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {187},
URL = {https://www.mdpi.com/1999-5903/13/8/187},
ISSN = {1999-5903},
ABSTRACT = {Due to the limitations of mobile phone positioning technology in the past, it is difficult for an advertising system to obtain users’ locations. Although there are many advertising delivery ideas in the market, time and location-based advertising delivery system have not been able to meet the delivery needs of advertisers. This study investigates various time and location-based advertising needs in the market and finds a universal data processing model that can quickly organize user positioning data from telecom base stations into screening information. It helps us to find specific audiences and to deliver advertisements to target audiences quickly. This study evaluated various advertising scenarios’ effectiveness and explained why there is a performance gap usually between five to ten times worse compared to the past. This study discusses the problems encountered when implementing the advertising system and how to improve them. This study also examines advertising interaction methods based on 5G features, business cooperation and profit splitting with telecom operators, and more diversified data application orientations and possibilities such as market surveys, urban traffic, security issues, and epidemic prevention. The future challenge is to develop footprint tracking technologies based on 5G and provide smart cities with lives-protecting or cost-saving solutions.},
DOI = {10.3390/fi13080187}
}



@Article{electronics10151758,
AUTHOR = {Yang, Shangyi and Sun, Chao and Kim, Youngok},
TITLE = {Indoor 3D Localization Scheme Based on BLE Signal Fingerprinting and 1D Convolutional Neural Network},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {1758},
URL = {https://www.mdpi.com/2079-9292/10/15/1758},
ISSN = {2079-9292},
ABSTRACT = {Indoor localization schemes have significant potential for use in location-based services in areas such as smart factories, mixed reality, and indoor navigation. In particular, received signal strength (RSS)-based fingerprinting is used widely, given its simplicity and low hardware requirements. However, most studies tend to focus on estimating the 2D position of the target. Moreover, it is known that the fingerprinting scheme is computationally costly, and its positioning accuracy is readily affected by random fluctuations in the RSS values caused by fading and the multipath effect. We propose an indoor 3D localization scheme based on both fingerprinting and a 1D convolutional neural network (CNN). Instead of using the conventional fingerprint matching method, we transform the 3D positioning problem into a classification problem and use the 1D CNN model with the RSS time-series data from Bluetooth low-energy beacons for classification. By using the 1D CNN with the time-series data from multiple beacons, the inherent drawback of RSS-based fingerprinting, namely, its susceptibility to noise and randomness, is overcome, resulting in enhanced positioning accuracy. To evaluate the proposed scheme, we developed a 3D positioning system and performed comprehensive tests, whose results confirmed that the scheme significantly outperforms the conventional common spatial pattern classification algorithm.},
DOI = {10.3390/electronics10151758}
}



@Article{agronomy11081458,
AUTHOR = {Ammar, Adel and Koubaa, Anis and Benjdira, Bilel},
TITLE = {Deep-Learning-Based Automated Palm Tree Counting and Geolocation in Large Farms from Aerial Geotagged Images},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1458},
URL = {https://www.mdpi.com/2073-4395/11/8/1458},
ISSN = {2073-4395},
ABSTRACT = {In this paper, we propose an original deep learning framework for the automated counting and geolocation of palm trees from aerial images using convolutional neural networks. For this purpose, we collected aerial images from two different regions in Saudi Arabia, using two DJI drones, and we built a dataset of around 11,000 instances of palm trees. Then, we applied several recent convolutional neural network models (Faster R-CNN, YOLOv3, YOLOv4, and EfficientDet) to detect palms and other trees, and we conducted a complete comparative evaluation in terms of average precision and inference speed. YOLOv4 and EfficientDet-D5 yielded the best trade-off between accuracy and speed (up to 99% mean average precision and 7.4 FPS). Furthermore, using the geotagged metadata of aerial images, we used photogrammetry concepts and distance corrections to automatically detect the geographical location of detected palm trees. This geolocation technique was tested on two different types of drones (DJI Mavic Pro and Phantom 4 pro) and was assessed to provide an average geolocation accuracy that attains 1.6 m. This GPS tagging allows us to uniquely identify palm trees and count their number from a series of drone images, while correctly dealing with the issue of image overlapping. Moreover, this innovative combination between deep learning object detection and geolocalization can be generalized to any other objects in UAV images.},
DOI = {10.3390/agronomy11081458}
}



@Article{app11156745,
AUTHOR = {Díez-Pastor, José Francisco and Latorre-Carmona, Pedro and Garrido-Labrador, José Luis and Ramírez-Sanz, José Miguel and Rodríguez, Juan J.},
TITLE = {Experimental Assessment of Feature Extraction Techniques Applied to the Identification of Properties of Common Objects, Using a Radar System},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {6745},
URL = {https://www.mdpi.com/2076-3417/11/15/6745},
ISSN = {2076-3417},
ABSTRACT = {Radar technology has evolved considerably in the last few decades. There are many areas where radar systems are applied, including air traffic control in airports, ocean surveillance, and research systems, to cite a few. Other types of sensors have recently appeared, which allow tracking sub-millimeter motion with high speed and accuracy rates. These millimeter-wave radars are giving rise to myriad new applications, from the recognition of the material close objects are made, to the recognition of hand gestures. They have also been recently used to identify how a person interacts with digital devices through the physical environment (Tangible User Interfaces, TUIs). In this case, the radar is used to detect the orientation, movement, or distance from the objects to the user’s hands or the digital device. This paper presents a thoughtful comparative analysis of different feature extraction techniques and classification strategies applied on a series of datasets that cover problems such as the identification of materials, element counting, or determining the orientation and distance of objects to the sensor. The results outperform previous works using these datasets, especially when the accuracy was lowest, showing the benefits feature extraction techniques have on classification performance.},
DOI = {10.3390/app11156745}
}



@Article{rs13152881,
AUTHOR = {Karami, Azam and Quijano, Karoll and Crawford, Melba},
TITLE = {Advancing Tassel Detection and Counting: Annotation and Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2881},
URL = {https://www.mdpi.com/2072-4292/13/15/2881},
ISSN = {2072-4292},
ABSTRACT = {Tassel counts provide valuable information related to flowering and yield prediction in maize, but are expensive and time-consuming to acquire via traditional manual approaches. High-resolution RGB imagery acquired by unmanned aerial vehicles (UAVs), coupled with advanced machine learning approaches, including deep learning (DL), provides a new capability for monitoring flowering. In this article, three state-of-the-art DL techniques, CenterNet based on point annotation, task-aware spatial disentanglement (TSD), and detecting objects with recursive feature pyramids and switchable atrous convolution (DetectoRS) based on bounding box annotation, are modified to improve their performance for this application and evaluated for tassel detection relative to Tasselnetv2+. The dataset for the experiments is comprised of RGB images of maize tassels from plant breeding experiments, which vary in size, complexity, and overlap. Results show that the point annotations are more accurate and simpler to acquire than the bounding boxes, and bounding box-based approaches are more sensitive to the size of the bounding boxes and background than point-based approaches. Overall, CenterNet has high accuracy in comparison to the other techniques, but DetectoRS can better detect early-stage tassels. The results for these experiments were more robust than Tasselnetv2+, which is sensitive to the number of tassels in the image.},
DOI = {10.3390/rs13152881}
}



@Article{rs13152887,
AUTHOR = {Nassar, Ayman and Torres-Rua, Alfonso and Kustas, William and Alfieri, Joseph and Hipps, Lawrence and Prueger, John and Nieto, Héctor and Alsina, Maria Mar and White, William and McKee, Lynn and Coopmans, Calvin and Sanchez, Luis and Dokoozlian, Nick},
TITLE = {Assessing Daily Evapotranspiration Methodologies from One-Time-of-Day sUAS and EC Information in the GRAPEX Project},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2887},
URL = {https://www.mdpi.com/2072-4292/13/15/2887},
ISSN = {2072-4292},
ABSTRACT = {Daily evapotranspiration (ETd) plays a key role in irrigation water management and is particularly important in drought-stricken areas, such as California and high-value crops. Remote sensing allows for the cost-effective estimation of spatial evapotranspiration (ET), and the advent of small unmanned aerial systems (sUAS) technology has made it possible to estimate instantaneous high-resolution ET at the plant, row, and subfield scales. sUAS estimates ET using “instantaneous” remote sensing measurements with half-hourly/hourly forcing micrometeorological data, yielding hourly fluxes in W/m2 that are then translated to a daily scale (mm/day) under two assumptions: (a) relative rates, such as the ratios of ET-to-net radiation (Rn) or ET-to-solar radiation (Rs), are assumed to be constant rather than absolute, and (b) nighttime evaporation (E) and transpiration (T) contributions are negligible. While assumption (a) may be reasonable for unstressed, full cover crops (no exposed soil), the E and T rates may significantly vary over the course of the day for partially vegetated cover conditions due to diurnal variations of soil and crop temperatures and interactions between soil and vegetation elements in agricultural environments, such as vineyards and orchards. In this study, five existing extrapolation approaches that compute the daily ET from the “instantaneous” remotely sensed sUAS&nbsp;ET estimates and the eddy covariance (EC) flux tower measurements were evaluated under different weather, grapevine variety, and trellis designs. Per assumption (b), the nighttime ET contribution was ignored. Each extrapolation technique (evaporative fraction (EF), solar radiation (Rs), net radiation-to-solar radiation (Rn/Rs) ratio, Gaussian (GA), and Sine) makes use of clear skies and quasi-sinusoidal diurnal variations of hourly ET and other meteorological parameters. The sUAS&nbsp;ET estimates and EC&nbsp;ET measurements were collected over multiple years and times from different vineyard sites in California as part of the USDA Agricultural Research Service Grape Remote Sensing Atmospheric Profile and Evapotranspiration eXperiment (GRAPEX). Optical and thermal sUAS imagery data at 10 cm and 60 cm, respectively, were collected by the Utah State University AggieAir sUAS Program and used in the Two-Source Energy Balance (TSEB) model to estimate the instantaneous or hourly sUAS&nbsp;ET at overpass time. The hourly ET from the EC measurements was also used to validate the extrapolation techniques. Overall, the analysis using EC measurements indicates that the Rs, EF, and GA approaches presented the best goodness-of-fit statistics for a window of time between 1030 and 1330 PST (Pacific Standard Time), with the Rs approach yielding better agreement with the EC measurements. Similar results were found using TSEB and sUAS data. The 1030–1330 time window also provided the greatest agreement between the actual daily EC ET and the extrapolated TSEB daily ET, with the Rs approach again yielding better agreement with the ground measurements. The expected accuracy of the upscaled TSEB daily ET estimates across all vineyard sites in California is below 0.5 mm/day, (EC extrapolation accuracy was found to be 0.34 mm/day), making the daily scale results from TSEB reliable and suitable for day-to-day water management applications.},
DOI = {10.3390/rs13152887}
}



@Article{drones5030066,
AUTHOR = {Walambe, Rahee and Marathe, Aboli and Kotecha, Ketan},
TITLE = {Multiscale Object Detection from Drone Imagery Using Ensemble Transfer Learning},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/2504-446X/5/3/66},
ISSN = {2504-446X},
ABSTRACT = {Object detection in uncrewed aerial vehicle (UAV) images has been a longstanding challenge in the field of computer vision. Specifically, object detection in drone images is a complex task due to objects of various scales such as humans, buildings, water bodies, and hills. In this paper, we present an implementation of ensemble transfer learning to enhance the performance of the base models for multiscale object detection in drone imagery. Combined with a test-time augmentation pipeline, the algorithm combines different models and applies voting strategies to detect objects of various scales in UAV images. The data augmentation also presents a solution to the deficiency of drone image datasets. We experimented with two specific datasets in the open domain: the VisDrone dataset and the AU-AIR Dataset. Our approach is more practical and efficient due to the use of transfer learning and two-level voting strategy ensemble instead of training custom models on entire datasets. The experimentation shows significant improvement in the mAP for both VisDrone and AU-AIR datasets by employing the ensemble transfer learning method. Furthermore, the utilization of voting strategies further increases the 3reliability of the ensemble as the end-user can select and trace the effects of the mechanism for bounding box predictions.},
DOI = {10.3390/drones5030066}
}



@Article{rs13152912,
AUTHOR = {Wang, Jingrui and Wang, Shuqing and Zou, Dongxiao and Chen, Huimin and Zhong, Run and Li, Hanliang and Zhou, Wei and Yan, Kai},
TITLE = {Social Network and Bibliometric Analysis of Unmanned Aerial Vehicle Remote Sensing Applications from 2010 to 2021},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2912},
URL = {https://www.mdpi.com/2072-4292/13/15/2912},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) Remote sensing (RS) has unique advantages over traditional satellite RS, including convenience, high resolution, affordability and fast acquisition speed, making it widely used in many fields. To provide an overview of the development of UAV RS applications during the past decade, we screened related publications from the Web of Science core database from 2010 to 2021, built co-author networks, a discipline interaction network, a keywords timeline view, a co-citation cluster, and detected burst citations using bibliometrics and social network analysis. Our results show that: (1) The number of UAV RS publications had an increasing trend, with explosive growth in the past five years. The number of papers published by China and the United States (US) is far ahead in this field; (2) The US has currently the greatest influence in this field through the largest number of international cooperations. Cooperation is mainly concentrated in countries and institutions with a large number of publications but is not widely distributed. (3) The application of UAV RS involves multiple interdisciplinary subjects, among which “Environmental Science and Ecology” ranks first; (4) Future research trends of UAV RS are expected to be related to artificial intelligence (e.g., artificial neural networks-based research). This paper provides a scientific basis and guidance for future developments of UAV RS applications, which can help the research community to better grasp the developments of this field.},
DOI = {10.3390/rs13152912}
}



@Article{en14154477,
AUTHOR = {Vodovozov, Valery and Raud, Zoja and Petlenkov, Eduard},
TITLE = {Review on Braking Energy Management in Electric Vehicles},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {4477},
URL = {https://www.mdpi.com/1996-1073/14/15/4477},
ISSN = {1996-1073},
ABSTRACT = {The adoption of electric vehicles promises numerous benefits for modern society. At the same time, there remain significant hurdles to their wide distribution, primarily related to battery-based energy sources. This review concerns the systematization of knowledge in one of the areas of the electric vehicle control, namely, the energy management issues when using braking controllers. The braking process optimization is summarized from two aspects. First, the advantageous solutions are presented that were identified in the field of gradual and urgent braking. Second, several findings discovered in adjacent fields of automation are debated as prospects for their possible application in braking control. Following the specific classification of braking methods, a generalized braking system composition is offered, and all publications are evaluated primarily in terms of their energy recovery abilities as a global target. Then, conventional and intelligent classes of braking controllers are compared. In the first category, classic PID, threshold, and sliding-mode controllers are reviewed in terms of their energy management restrictions. The second group relates to the issues of the tire friction-slip identification and braking torque allocation between the hydraulic and electrical brakes. From this perspective, several intelligent systems are analyzed in detail, especially fuzzy logic, neural network, and their numerous associations.},
DOI = {10.3390/en14154477}
}



@Article{rs13152914,
AUTHOR = {Cruz-Ramos, Clara and Garcia-Salgado, Beatriz P. and Reyes-Reyes, Rogelio and Ponomaryov, Volodymyr and Sadovnychiy, Sergiy},
TITLE = {Gabor Features Extraction and Land-Cover Classification of Urban Hyperspectral Images for Remote Sensing Applications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2914},
URL = {https://www.mdpi.com/2072-4292/13/15/2914},
ISSN = {2072-4292},
ABSTRACT = {The principles of the transform stage of the extract, transform and load (ETL) process can be applied to index the data in functional structures for the decision-making inherent in an urban remote sensing application. This work proposes a method that can be utilised as an organisation stage by reducing the data dimension with Gabor texture features extracted from grey-scale representations of the Hue, Saturation and Value (HSV) colour space and the Normalised Difference Vegetation Index (NDVI). Additionally, the texture features are reduced using the Linear Discriminant Analysis (LDA) method. Afterwards, an Artificial Neural Network (ANN) is employed to classify the data and build a tick data matrix indexed by the belonging class of the observations, which could be retrieved for further analysis according to the class selected to explore. The proposed method is compared in terms of classification rates, reduction efficiency and training time against the utilisation of other grey-scale representations and classifiers. This method compresses up to 87% of the original features and achieves similar classification results to non-reduced features but at a higher training time.},
DOI = {10.3390/rs13152914}
}



@Article{rs13152917,
AUTHOR = {Wei, Lifei and Wang, Kun and Lu, Qikai and Liang, Yajing and Li, Haibo and Wang, Zhengxiang and Wang, Run and Cao, Liqin},
TITLE = {Crops Fine Classification in Airborne Hyperspectral Imagery Based on Multi-Feature Fusion and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2917},
URL = {https://www.mdpi.com/2072-4292/13/15/2917},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imagery has been widely used in precision agriculture due to its rich spectral characteristics. With the rapid development of remote sensing technology, the airborne hyperspectral imagery shows detailed spatial information and temporal flexibility, which open a new way to accurate agricultural monitoring. To extract crop types from the airborne hyperspectral images, we propose a fine classification method based on multi-feature fusion and deep learning. In this research, the morphological profiles, GLCM texture and endmember abundance features are leveraged to exploit the spatial information of the hyperspectral imagery. Then, the multiple spatial information is fused with the original spectral information to generate classification result by using the deep neural network with conditional random field (DNN+CRF) model. Specifically, the deep neural network (DNN) is a deep recognition model which can extract depth features and mine the potential information of data. As a discriminant model, conditional random field (CRF) considers both spatial and contextual information to reduce the misclassification noises while keeping the object boundaries. Moreover, three multiple feature fusion approaches, namely feature stacking, decision fusion and probability fusion, are taken into account. In the experiments, two airborne hyperspectral remote sensing datasets (Honghu dataset and Xiong’an dataset) are used. The experimental results show that the classification performance of the proposed method is satisfactory, where the salt and pepper noise is decreased, and the boundary of the ground object is preserved.},
DOI = {10.3390/rs13152917}
}



@Article{rs13152918,
AUTHOR = {Banerjee, Bikram P. and Sharma, Vikas and Spangenberg, German and Kant, Surya},
TITLE = {Machine Learning Regression Analysis for Estimation of Crop Emergence Using Multispectral UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2918},
URL = {https://www.mdpi.com/2072-4292/13/15/2918},
ISSN = {2072-4292},
ABSTRACT = {Optimal crop emergence is an important trait in crop breeding for genotypic screening and for achieving potential growth and yield. Emergence is conventionally quantified manually by counting the sub-sections of field plots or scoring; these are less reliable, laborious and inefficient. Remote sensing technology is being increasingly used for high-throughput estimation of agronomic traits in field crops. This study developed a method for estimating wheat seedlings using multispectral images captured from an unmanned aerial vehicle. A machine learning regression (MLR) analysis was used by combining spectral and morphological information extracted from the multispectral images. The approach was tested on diverse wheat genotypes varying in seedling emergence. In this study, three supervised MLR models including regression trees, support vector regression and Gaussian process regression (GPR) were evaluated for estimating wheat seedling emergence. The GPR model was the most effective compared to the other methods, with R2 = 0.86, RMSE = 4.07 and MAE = 3.21 when correlated to the manual seedling count. In addition, imagery data collected at multiple flight altitudes and different wheat growth stages suggested that 10 m altitude and 20 days after sowing were desirable for optimal spatial resolution and image analysis. The method is deployable on larger field trials and other crops for effective and reliable seedling emergence estimates.},
DOI = {10.3390/rs13152918}
}



@Article{agronomy11081480,
AUTHOR = {Liu, Jizhan and Abbas, Irfan and Noor, Rana Shahzad},
TITLE = {Development of Deep Learning-Based Variable Rate Agrochemical Spraying System for Targeted Weeds Control in Strawberry Crop},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1480},
URL = {https://www.mdpi.com/2073-4395/11/8/1480},
ISSN = {2073-4395},
ABSTRACT = {Agrochemical application is an important tool in the agricultural industry for the protection of crops. Agrochemical application with conventional sprayers results in the waste of applied agrochemicals, which not only increases financial losses but also contaminates the environment. Targeted agrochemical sprayers using smart control systems can substantially decrease the chemical input, weed control cost, and destructive environmental contamination. A variable rate spraying system was developed using deep learning methods for the development of new models to classify weeds and to accurately spray on desired weeds target. Laboratory and field experiments were conducted to assess the sprayer performance for weed classification and precise spraying of the target weeds using three classification CNNs (Convolutional Neural Networks) models. The DCNNs models (AlexNet, VGG-16, and GoogleNet) were trained using a dataset containing a total of 12,443 images captured from the strawberry field (4200 images with spotted spurge, 4265 images with Shepherd’s purse, and 4178 strawberry plants). The VGG-16 model attained higher values of precision, recall and F1-score as compared to AlexNet and GoogleNet. Additionally VGG-16 model recorded higher percentage of completely sprayed weeds target (CS = 93%) values. Overall in all experiments, VGG-16 performed better than AlexNet and GoogleNet for real-time weeds target classification and precision spraying. The experiments results revealed that the Sprayer performance decreased with the increase of sprayer traveling speed above 3 km/h. Experimental results recommended that the sprayer with the VGG-16 model can achieve high performance that makes it more ideal for a real-time spraying application. It is concluded that the advanced variable rate spraying system has the potential for spot application of agrochemicals to control weeds in a strawberry field. It can reduce the crop input costs and the environmental pollution risks.},
DOI = {10.3390/agronomy11081480}
}



@Article{s21155044,
AUTHOR = {Behjati, Mehran and Mohd Noh, Aishah Binti and Alobaidy, Haider A. H. and Zulkifley, Muhammad Aidiel and Nordin, Rosdiadee and Abdullah, Nor Fadzilah},
TITLE = {LoRa Communications as an Enabler for Internet of Drones towards Large-Scale Livestock Monitoring in Rural Farms},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5044},
URL = {https://www.mdpi.com/1424-8220/21/15/5044},
PubMedID = {34372281},
ISSN = {1424-8220},
ABSTRACT = {Currently, smart farming is considered an effective solution to enhance the productivity of farms; thereby, it has recently received broad interest from service providers to offer a wide range of applications, from pest identification to asset monitoring. Although the emergence of digital technologies, such as the Internet of Things (IoT) and low-power wide-area networks (LPWANs), has led to significant advances in the smart farming industry, farming operations still need more efficient solutions. On the other hand, the utilization of unmanned aerial vehicles (UAVs), also known as drones, is growing rapidly across many civil application domains. This paper aims to develop a farm monitoring system that incorporates UAV, LPWAN, and IoT technologies to transform the current farm management approach and aid farmers in obtaining actionable data from their farm operations. In this regard, an IoT-based water quality monitoring system was developed because water is an essential aspect in livestock development. Then, based on the Long-Range Wide-Area Network (LoRaWAN®) technology, a multi-channel LoRaWAN® gateway was developed and integrated into a vertical takeoff and landing drone to convey collected data from the sensors to the cloud for further analysis. In addition, to develop LoRaWAN®-based aerial communication, a series of measurements and simulations were performed under different configurations and scenarios. Finally, to enhance the efficiency of aerial-based data collection, the UAV path planning was optimized. Measurement results showed that the maximum achievable LoRa coverage when operating on-air via the drone is about 10 km, and the Longley–Rice irregular terrain model provides the most suitable path loss model for the scenario of large-scale farms, and a multi-channel gateway with a spreading factor of 12 provides the most reliable communication link at a high drone speed (up to 95 km/h). Simulation results showed that the developed system can overcome the coverage limitation of LoRaWAN® and it can establish a reliable communication link over large-scale wireless sensor networks. In addition, it was shown that by optimizing flight paths, aerial data collection could be performed in a much shorter time than industrial mission planning (up to four times in our case).},
DOI = {10.3390/s21155044}
}



@Article{agriculture11080707,
AUTHOR = {Lu, Jinzhu and Tan, Lijuan and Jiang, Huanyu},
TITLE = {Review on Convolutional Neural Network (CNN) Applied to Plant Leaf Disease Classification},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {707},
URL = {https://www.mdpi.com/2077-0472/11/8/707},
ISSN = {2077-0472},
ABSTRACT = {Crop production can be greatly reduced due to various diseases, which seriously endangers food security. Thus, detecting plant diseases accurately is necessary and urgent. Traditional classification methods, such as naked-eye observation and laboratory tests, have many limitations, such as being time consuming and subjective. Currently, deep learning (DL) methods, especially those based on convolutional neural network (CNN), have gained widespread application in plant disease classification. They have solved or partially solved the problems of traditional classification methods and represent state-of-the-art technology in this field. In this work, we reviewed the latest CNN networks pertinent to plant leaf disease classification. We summarized DL principles involved in plant disease classification. Additionally, we summarized the main problems and corresponding solutions of CNN used for plant disease classification. Furthermore, we discussed the future development direction in plant disease classification.},
DOI = {10.3390/agriculture11080707}
}



@Article{rs13152948,
AUTHOR = {Fernández, Claudio I. and Leblon, Brigitte and Wang, Jinfei and Haddadi, Ata and Wang, Keri},
TITLE = {Detecting Infected Cucumber Plants with Close-Range Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2948},
URL = {https://www.mdpi.com/2072-4292/13/15/2948},
ISSN = {2072-4292},
ABSTRACT = {This study used close-range multispectral imagery over cucumber plants inside a commercial greenhouse to detect powdery mildew due to Podosphaera xanthii. It was collected using a MicaSense® RedEdge camera at 1.5 m over the top of the plant. Image registration was performed using Speeded-Up Robust Features (SURF) with an affine geometric transformation. The image background was removed using a binary mask created with the aligned NIR band of each image, and the illumination was corrected using Cheng et al.’s algorithm. Different features were computed, including RGB, image reflectance values, and several vegetation indices. For each feature, a fine Gaussian Support Vector Machines algorithm was trained and validated to classify healthy and infected pixels. The data set to train and validate the SVM was composed of 1000 healthy and 1000 infected pixels, split 70–30% into training and validation datasets, respectively. The overall validation accuracy was 89, 73, 82, 51, and 48%, respectively, for blue, green, red, red-edge, and NIR band image. With the RGB images, we obtained an overall validation accuracy of 89%, while the best vegetation index image was the PMVI-2 image which produced an overall accuracy of 81%. Using the five bands together, overall accuracy dropped from 99% in the training to 57% in the validation dataset. While the results of this work are promising, further research should be considered to increase the number of images to achieve better training and validation datasets.},
DOI = {10.3390/rs13152948}
}



@Article{rs13152965,
AUTHOR = {Ghaffarian, Saman and Valente, João and van der Voort, Mariska and Tekinerdogan, Bedir},
TITLE = {Effect of Attention Mechanism in Deep Learning-Based Remote Sensing Image Processing: A Systematic Literature Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2965},
URL = {https://www.mdpi.com/2072-4292/13/15/2965},
ISSN = {2072-4292},
ABSTRACT = {Machine learning, particularly deep learning (DL), has become a central and state-of-the-art method for several computer vision applications and remote sensing (RS) image processing. Researchers are continually trying to improve the performance of the DL methods by developing new architectural designs of the networks and/or developing new techniques, such as attention mechanisms. Since the attention mechanism has been proposed, regardless of its type, it has been increasingly used for diverse RS applications to improve the performances of the existing DL methods. However, these methods are scattered over different studies impeding the selection and application of the feasible approaches. This study provides an overview of the developed attention mechanisms and how to integrate them with different deep learning neural network architectures. In addition, it aims to investigate the effect of the attention mechanism on deep learning-based RS image processing. We identified and analyzed the advances in the corresponding attention mechanism-based deep learning (At-DL) methods. A systematic literature review was performed to identify the trends in publications, publishers, improved DL methods, data types used, attention types used, overall accuracies achieved using At-DL methods, and extracted the current research directions, weaknesses, and open problems to provide insights and recommendations for future studies. For this, five main research questions were formulated to extract the required data and information from the literature. Furthermore, we categorized the papers regarding the addressed RS image processing tasks (e.g., image classification, object detection, and change detection) and discussed the results within each group. In total, 270 papers were retrieved, of which 176 papers were selected according to the defined exclusion criteria for further analysis and detailed review. The results reveal that most of the papers reported an increase in overall accuracy when using the attention mechanism within the DL methods for image classification, image segmentation, change detection, and object detection using remote sensing images.},
DOI = {10.3390/rs13152965}
}



@Article{s21155132,
AUTHOR = {Kuo, Ping-Huan and Lin, Ssu-Ting and Hu, Jun and Huang, Chiou-Jye},
TITLE = {Multi-Sensor Context-Aware Based Chatbot Model: An Application of Humanoid Companion Robot},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5132},
URL = {https://www.mdpi.com/1424-8220/21/15/5132},
PubMedID = {34372368},
ISSN = {1424-8220},
ABSTRACT = {In aspect of the natural language processing field, previous studies have generally analyzed sound signals and provided related responses. However, in various conversation scenarios, image information is still vital. Without the image information, misunderstanding may occur, and lead to wrong responses. In order to address this problem, this study proposes a recurrent neural network (RNNs) based multi-sensor context-aware chatbot technology. The proposed chatbot model incorporates image information with sound signals and gives appropriate responses to the user. In order to improve the performance of the proposed model, the long short-term memory (LSTM) structure is replaced by gated recurrent unit (GRU). Moreover, a VGG16 model is also chosen for a feature extractor for the image information. The experimental results demonstrate that the integrative technology of sound and image information, which are obtained by the image sensor and sound sensor in a companion robot, is helpful for the chatbot model proposed in this study. The feasibility of the proposed technology was also confirmed in the experiment.},
DOI = {10.3390/s21155132}
}



@Article{rs13152986,
AUTHOR = {Li, Xin and Xu, Feng and Xia, Runliang and Lyu, Xin and Gao, Hongmin and Tong, Yao},
TITLE = {Hybridizing Cross-Level Contextual and Attentive Representations for Remote Sensing Imagery Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2986},
URL = {https://www.mdpi.com/2072-4292/13/15/2986},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation of remote sensing imagery is a fundamental task in intelligent interpretation. Since deep convolutional neural networks (DCNNs) performed considerable insight in learning implicit representations from data, numerous works in recent years have transferred the DCNN-based model to remote sensing data analysis. However, the wide-range observation areas, complex and diverse objects and illumination and imaging angle influence the pixels easily confused, leading to undesirable results. Therefore, a remote sensing imagery semantic segmentation neural network, named HCANet, is proposed to generate representative and discriminative representations for dense predictions. HCANet hybridizes cross-level contextual and attentive representations to emphasize the distinguishability of learned features. First of all, a cross-level contextual representation module (CCRM) is devised to exploit and harness the superpixel contextual information. Moreover, a hybrid representation enhancement module (HREM) is designed to fuse cross-level contextual and self-attentive representations flexibly. Furthermore, the decoder incorporates DUpsampling operation to boost the efficiency losslessly. The extensive experiments are implemented on the Vaihingen and Potsdam benchmarks. In addition, the results indicate that HCANet achieves excellent performance on overall accuracy and mean intersection over union. In addition, the ablation study further verifies the superiority of CCRM.},
DOI = {10.3390/rs13152986}
}



