
@Article{app11093737,
AUTHOR = {Hrúz, Michal and Bugaj, Martin and Novák, Andrej and Kandera, Branislav and Badánik, Benedikt},
TITLE = {The Use of UAV with Infrared Camera and RFID for Airframe Condition Monitoring},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3737},
URL = {https://www.mdpi.com/2076-3417/11/9/3737},
ISSN = {2076-3417},
ABSTRACT = {The new progressive smart technologies announced in the fourth industrial revolution in aviation—Aviation 4.0—represent new possibilities and big challenges in aircraft maintenance processes. The main benefit of these technologies is the possibility to monitor, transfer, store, and analyze huge datasets. Based on analysis outputs, there is a possibility to improve current preventive maintenance processes and implement predictive maintenance processes. These solutions lower the downtime, save manpower, and extend the components’ lifetime; thus, the maximum effectivity and safety is achieved. The article deals with the possible implementation of an unmanned aerial vehicle (UAV) with an infrared camera and Radio Frequency Identification (RFID) as two of the smart hangar technologies for airframe condition monitoring. The presented implementations of smart technologies follow up the specific results of a case study focused on trainer aircraft failure monitoring and its impact on maintenance strategy changes. The case study failure indexes show the critical parts of aircraft that are subjected to damage the most. The aim of the article was to justify the need for thorough monitoring of critical parts of the aircraft and then analyze and propose a more effective and the most suitable form of technical condition monitoring of aircraft critical parts. The article describes the whole process of visual inspection performed by an unmanned aerial vehicle (UAV) with an IR camera and its related processes; in addition, it covers the possible usage of RFID tags as a labeling tool supporting the visual inspection. The implementations criteria apply to the repair and overhaul small aircraft maintenance organization, and later, it can also increase operational efficiency. The final suggestions describe the possible usage of proposed solutions, their main benefits, and also the limitations of their implementations in maintenance of trainer aircraft.},
DOI = {10.3390/app11093737}
}



@Article{rs13091619,
AUTHOR = {Yan, Bin and Fan, Pan and Lei, Xiaoyan and Liu, Zhijie and Yang, Fuzeng},
TITLE = {A Real-Time Apple Targets Detection Method for Picking Robot Based on Improved YOLOv5},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1619},
URL = {https://www.mdpi.com/2072-4292/13/9/1619},
ISSN = {2072-4292},
ABSTRACT = {The apple target recognition algorithm is one of the core technologies of the apple picking robot. However, most of the existing apple detection algorithms cannot distinguish between the apples that are occluded by tree branches and occluded by other apples. The apples, grasping end-effector and mechanical picking arm of the robot are very likely to be damaged if the algorithm is directly applied to the picking robot. Based on this practical problem, in order to automatically recognize the graspable and ungraspable apples in an apple tree image, a light-weight apple targets detection method was proposed for picking robot using improved YOLOv5s. Firstly, BottleneckCSP module was improved designed to BottleneckCSP-2 module which was used to replace the BottleneckCSP module in backbone architecture of original YOLOv5s network. Secondly, SE module, which belonged to the visual attention mechanism network, was inserted to the proposed improved backbone network. Thirdly, the bonding fusion mode of feature maps, which were inputs to the target detection layer of medium size in the original YOLOv5s network, were improved. Finally, the initial anchor box size of the original network was improved. The experimental results indicated that the graspable apples, which were unoccluded or only occluded by tree leaves, and the ungraspable apples, which were occluded by tree branches or occluded by other fruits, could be identified effectively using the proposed improved network model in this study. Specifically, the recognition recall, precision, mAP and F1 were 91.48%, 83.83%, 86.75% and 87.49%, respectively. The average recognition time was 0.015 s per image. Contrasted with original YOLOv5s, YOLOv3, YOLOv4 and EfficientDet-D0 model, the mAP of the proposed improved YOLOv5s model increased by 5.05%, 14.95%, 4.74% and 6.75% respectively, the size of the model compressed by 9.29%, 94.6%, 94.8% and 15.3% respectively. The average recognition speeds per image of the proposed improved YOLOv5s model were 2.53, 1.13 and 3.53 times of EfficientDet-D0, YOLOv4 and YOLOv3 and model, respectively. The proposed method can provide technical support for the real-time accurate detection of multiple fruit targets for the apple picking robot.},
DOI = {10.3390/rs13091619}
}



@Article{rs13091629,
AUTHOR = {Kwak, Geun-Ho and Park, Chan-won and Lee, Kyung-do and Na, Sang-il and Ahn, Ho-yong and Park, No-Wook},
TITLE = {Potential of Hybrid CNN-RF Model for Early Crop Mapping with Limited Input Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1629},
URL = {https://www.mdpi.com/2072-4292/13/9/1629},
ISSN = {2072-4292},
ABSTRACT = {When sufficient time-series images and training data are unavailable for crop classification, features extracted from convolutional neural network (CNN)-based representative learning may not provide useful information to discriminate crops with similar spectral characteristics, leading to poor classification accuracy. In particular, limited input data are the main obstacles to obtain reliable classification results for early crop mapping. This study investigates the potential of a hybrid classification approach, i.e., CNN-random forest (CNN-RF), in the context of early crop mapping, that combines the automatic feature extraction capability of CNN with the superior discrimination capability of an RF classifier. Two experiments on incremental crop classification with unmanned aerial vehicle images were conducted to compare the performance of CNN-RF with that of CNN and RF with respect to the length of the time-series and training data sizes. When sufficient time-series images and training data were used for the classification, the accuracy of CNN-RF was slightly higher or comparable with that of CNN. In contrast, when fewer images and the smallest training data were used at the early crop growth stage, CNN-RF was substantially beneficial and the overall accuracy increased by maximum 6.7%p and 4.6%p in the two study areas, respectively, compared to CNN. This is attributed to its ability to discriminate crops from features with insufficient information using a more sophisticated classifier. The experimental results demonstrate that CNN-RF is an effective classifier for early crop mapping when only limited input images and training samples are available.},
DOI = {10.3390/rs13091629}
}



@Article{rs13091620,
AUTHOR = {Ge, Haixiao and Xiang, Haitao and Ma, Fei and Li, Zhenwang and Qiu, Zhengchao and Tan, Zhengzheng and Du, Changwen},
TITLE = {Estimating Plant Nitrogen Concentration of Rice through Fusing Vegetation Indices and Color Moments Derived from UAV-RGB Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1620},
URL = {https://www.mdpi.com/2072-4292/13/9/1620},
ISSN = {2072-4292},
ABSTRACT = {Estimating plant nitrogen concentration (PNC) has been conducted using vegetation indices (VIs) from UAV-based imagery, but color features have been rarely considered as additional variables. In this study, the VIs and color moments (color feature) were calculated from UAV-based RGB images, then partial least square regression (PLSR) and random forest regression (RF) models were established to estimate PNC through fusing VIs and color moments. The results demonstrated that the fusion of VIs and color moments as inputs yielded higher accuracies of PNC estimation compared to VIs or color moments as input; the RF models based on the combination of VIs and color moments (R2 ranging from 0.69 to 0.91 and NRMSE ranging from 0.07 to 0.13) showed similar performances to the PLSR models (R2 ranging from 0.68 to 0.87 and NRMSE ranging from 0.10 to 0.29); Among the top five important variables in the RF models, there was at least one variable which belonged to the color moments in different datasets, indicating the significant contribution of color moments in improving PNC estimation accuracy. This revealed the great potential of combination of RGB-VIs and color moments for the estimation of rice PNC.},
DOI = {10.3390/rs13091620}
}



@Article{pr9050735,
AUTHOR = {Urrea, Claudio and Páez, Felipe},
TITLE = {Design and Comparison of Strategies for Level Control in a Nonlinear Tank},
JOURNAL = {Processes},
VOLUME = {9},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {735},
URL = {https://www.mdpi.com/2227-9717/9/5/735},
ISSN = {2227-9717},
ABSTRACT = {In this work, a study of the water level control of an inverted conical tank system is presented. This type of tank has highly nonlinear mathematical and dynamic characteristics. Four control strategies are designed, applied, and compared, namely classical Proportional–Integral–Derivative (PID), Gain Scheduling (GS), Internal Model Control (IMC), and Fuzzy Logic (FL). To determine which of the designed control strategies are the most suitable for an inverted conical tank, a comparative study of the behavior of the system is carried out. With this purpose, and considering situations much closer to reality, a variety of scenarios, such as step responses, random input disturbances, and momentary load disturbances, are conducted. Additionally, performance indexes (error- and statistics-based) are calculated to assess the system’s response.},
DOI = {10.3390/pr9050735}
}



@Article{f12050517,
AUTHOR = {Kelley, Jason and Trofymow, John A. (Tony) and Metsaranta, Juha M. and Filipescu, Cosmin N. and Bone, Christopher},
TITLE = {Use of Multi-Temporal LiDAR to Quantify Fertilization Effects on Stand Volume and Biomass in Late-Rotation Coastal Douglas-Fir Forests},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {517},
URL = {https://www.mdpi.com/1999-4907/12/5/517},
ISSN = {1999-4907},
ABSTRACT = {Forest fertilization is common in coastal British Columbia as a means to increase wood production and potentially enhance carbon sequestration. Generally, the effects of fertilization are determined by measuring sample plots pre- and post-treatment, resulting in fertilization effects being determined for a limited portion of the treatment area. Applications of remote sensing-based enhanced forest inventories have allowed for estimations to expand to the wider forested area. However, these applications have not focused on monitoring the effects of silvicultural treatments. The objective of this research was to examine if a multi-temporal application of the LiDAR area-based method can be used to detect the fertilization effects on volume, biomass, and height in a second-growth Douglas-fir (Pseudotsuga menziesii) stand. The study area on Vancouver Island was fertilized in January 2007, and sample plots were established in 2011. LiDAR acquisitions were made in 2004, prior to fertilization, and in 2008, 2011, and 2016, covering both treated and untreated areas. A total of 29 paired LiDAR blocks, comprised of four 20 m resolution raster cells, were selected on either side of the fertilization boundary for analysis of the effects across several different stand types differing in the percentage of Douglas-fir, site index, and age. Random forest (RF) plot-level models were developed to estimate total stem volume and total stem biomass for each year of LiDAR acquisition using an area-based approach. Plot level results showed an increase in stem volume by 13% fertilized over control from 2005 to 2011, which was similar to a 14% increase in above-ground carbon stocks estimated using a tree-ring stand reconstruction approach. Plot-level RF models showed R2 values of 0.86 (volume) and 0.92 (biomass) with relative cross-validated root mean square errors of 12.5% (volume) and 11.9% (biomass). For both the sample plots and LiDAR blocks, statistical results indicated no significant differences in volume or biomass between treatments. However, significant differences in height increments were detected between treatments in LiDAR blocks. The results from this research highlight the promising potential for the use of enhanced forest inventory methods to rapidly expand the assessment of treatment effects beyond sample plots to the stand, block, or landscape level.},
DOI = {10.3390/f12050517}
}



@Article{electronics10090999,
AUTHOR = {Azar, Ahmad Taher and Koubaa, Anis and Ali Mohamed, Nada and Ibrahim, Habiba A. and Ibrahim, Zahra Fathy and Kazim, Muhammad and Ammar, Adel and Benjdira, Bilel and Khamis, Alaa M. and Hameed, Ibrahim A. and Casalino, Gabriella},
TITLE = {Drone Deep Reinforcement Learning: A Review},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {999},
URL = {https://www.mdpi.com/2079-9292/10/9/999},
ISSN = {2079-9292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are increasingly being used in many challenging and diversified applications. These applications belong to the civilian and the military fields. To name a few; infrastructure inspection, traffic patrolling, remote sensing, mapping, surveillance, rescuing humans and animals, environment monitoring, and Intelligence, Surveillance, Target Acquisition, and Reconnaissance (ISTAR) operations. However, the use of UAVs in these applications needs a substantial level of autonomy. In other words, UAVs should have the ability to accomplish planned missions in unexpected situations without requiring human intervention. To ensure this level of autonomy, many artificial intelligence algorithms were designed. These algorithms targeted the guidance, navigation, and control (GNC) of UAVs. In this paper, we described the state of the art of one subset of these algorithms: the deep reinforcement learning (DRL) techniques. We made a detailed description of them, and we deduced the current limitations in this area. We noted that most of these DRL methods were designed to ensure stable and smooth UAV navigation by training computer-simulated environments. We realized that further research efforts are needed to address the challenges that restrain their deployment in real-life scenarios.},
DOI = {10.3390/electronics10090999}
}



@Article{rs13091642,
AUTHOR = {Wang, Peng and Niu, Yanxiong and Xiong, Rui and Ma, Fu and Zhang, Chunxi},
TITLE = {DGANet: Dynamic Gradient Adjustment Anchor-Free Object Detection in Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1642},
URL = {https://www.mdpi.com/2072-4292/13/9/1642},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing image object detection has been studied by many researchers in recent years using deep neural networks. However, optical remote sensing images contain many scenes with small and dense objects, resulting in a high rate of misrecognition. Firstly, in this work we selected a deep layer aggregation network with updated deformable convolution layers as the backbone to extract object features. The detection and classification of objects was based on the center-point network without non-maximum suppression. Secondly, the dynamic gradient adjustment embedded into the classification loss function was put forward to harmonize the quantity imbalance between easy and hard examples, as well as between positive and negative examples. Furthermore, the complete intersection over union (CIoU) loss function was selected as the objective function of bounding box regression, which achieves better convergence speed and accuracy. Finally, in order to validate the effectiveness and precision of the dynamic gradient adjustment network (DGANet), we conducted a series of experiments in remote sensing public datasets UCAS-AOD and LEVIR. The comparison experiments demonstrate that the DGANet achieves a more accurate detection result in optical remote sensing images.},
DOI = {10.3390/rs13091642}
}



@Article{inventions6020029,
AUTHOR = {Kashyap, Bhuwan and Kumar, Ratnesh},
TITLE = {Sensing Methodologies in Agriculture for Monitoring Biotic Stress in Plants Due to Pathogens and Pests},
JOURNAL = {Inventions},
VOLUME = {6},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {29},
URL = {https://www.mdpi.com/2411-5134/6/2/29},
ISSN = {2411-5134},
ABSTRACT = {Reducing agricultural losses is an effective way to sustainably increase agricultural output efficiency to meet our present and future needs for food, fiber, fodder, and fuel. Our ever-improving understanding of the ways in which plants respond to stress, biotic and abiotic, has led to the development of innovative sensing technologies for detecting crop stresses/stressors and deploying efficient measures. This article aims to present the current state of the methodologies applied in the field of agriculture towards the detection of biotic stress in crops. Key sensing methodologies for plant pathogen (or phytopathogen), as well as herbivorous insects/pests are presented, where the working principles are described, and key recent works discussed. The detection methods overviewed for phytopathogen-related stress identification include nucleic acid-based methods, immunological methods, imaging-based techniques, spectroscopic methods, phytohormone biosensing methods, monitoring methods for plant volatiles, and active remote sensing technologies. Whereas the pest-related sensing techniques include machine-vision-based methods, pest acoustic-emission sensors, and volatile organic compound-based stress monitoring methods. Additionally, Comparisons have been made between different sensing techniques as well as recently reported works, where the strengths and limitations are identified. Finally, the prospective future directions for monitoring biotic stress in crops are discussed.},
DOI = {10.3390/inventions6020029}
}



@Article{en14092397,
AUTHOR = {Murillo-Soto, Luis D. and Meza, Carlos},
TITLE = {Automated Fault Management System in a Photovoltaic Array: A Reconfiguration-Based Approach},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {2397},
URL = {https://www.mdpi.com/1996-1073/14/9/2397},
ISSN = {1996-1073},
ABSTRACT = {This work proposes an automated reconfiguration system to manage two types of faults in any position inside the solar arrays. The faults studied are the short-circuit to ground and the open wires in the string. These faults were selected because they severely affect power production. By identifying the affected panels and isolating the faulty one, it is possible to recover part of the power loss. Among other types of faults that the system can detect and locate are: diode short-circuit, internal open-circuit, and the degradation of the internal parasitic serial resistance. The reconfiguration system can detect, locate the above faults, and switch the distributed commutators to recover most of the power loss. Moreover, the system can return automatically to the previous state when the fault has been repaired. A SIMULINK model has been built to prove this automatic system, and a simulated numerical experiment has been executed to test the system response to the faults mentioned. The results show that the recovery of power is more than 90%, and the diagnosis accuracy and sensitivity are both 100% for this numerical experiment.},
DOI = {10.3390/en14092397}
}



@Article{rs13091661,
AUTHOR = {Gargees, Rasha S. and Scott, Grant J.},
TITLE = {Large-Scale, Multiple Level-of-Detail Change Detection from Remote Sensing Imagery Using Deep Visual Feature Clustering},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1661},
URL = {https://www.mdpi.com/2072-4292/13/9/1661},
ISSN = {2072-4292},
ABSTRACT = {In the era of big data, where massive amounts of remotely sensed imagery can be obtained from various satellites accompanied by the rapid change in the surface of the Earth, new techniques for large-scale change detection are necessary to facilitate timely and effective human understanding of natural and human-made phenomena. In this research, we propose a chip-based change detection method that is enabled by using deep neural networks to extract visual features. These features are transformed into deep orthogonal visual features that are then clustered based on land cover characteristics. The resulting chip cluster memberships allow arbitrary level-of-detail change analysis that can also support irregular geospatial extent based agglomerations. The proposed methods naturally support cross-resolution temporal scenes without requiring normalization of the pixel resolution across scenes and without requiring pixel-level coregistration processes. This is achieved with configurable spatial locality comparisons between years, where the aperture of a unit of measure can be a single chip, a small neighborhood of chips, or a large irregular geospatial region. The performance of our proposed method has been validated using various quantitative and statistical metrics in addition to presenting the visual geo-maps and the percentage of the change. The results show that our proposed method efficiently detected the change from a large scale area.},
DOI = {10.3390/rs13091661}
}



@Article{app11093871,
AUTHOR = {Morio, Jérôme and Levasseur, Baptiste and Bertrand, Sylvain},
TITLE = {Drone Ground Impact Footprints with Importance Sampling: Estimation and Sensitivity Analysis},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3871},
URL = {https://www.mdpi.com/2076-3417/11/9/3871},
ISSN = {2076-3417},
ABSTRACT = {This paper addresses the estimation of accurate extreme ground impact footprints and probabilistic maps due to a total loss of control of fixed-wing unmanned aerial vehicles after a main engine failure. In this paper, we focus on the ground impact footprints that contains 95%, 99% and 99.9% of the drone impacts. These regions are defined here with density minimum volume sets and may be estimated by Monte Carlo methods. As Monte Carlo approaches lead to an underestimation of extreme ground impact footprints, we consider in this article multiple importance sampling to evaluate them. Then, we perform a reliability oriented sensitivity analysis, to estimate the most influential uncertain parameters on the ground impact position. We show the results of these estimations on a realistic drone flight scenario.},
DOI = {10.3390/app11093871}
}



@Article{electronics10091021,
AUTHOR = {Chan, Teck Kai and Chin, Cheng Siong},
TITLE = {Review of Autonomous Intelligent Vehicles for Urban Driving and Parking},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1021},
URL = {https://www.mdpi.com/2079-9292/10/9/1021},
ISSN = {2079-9292},
ABSTRACT = {With the concept of Internet-of-Things, autonomous vehicles can provide higher driving efficiency, traffic safety, and freedom for the driver to perform other tasks. This paper first covers enabling technology involving a vehicle moving out of parking, traveling on the road, and parking at the destination. The development of autonomous vehicles relies on the data collected for deployment in actual road conditions. Research gaps and recommendations for autonomous intelligent vehicles are included. For example, a sudden obstacle while the autonomous vehicle executes the parking trajectory on the road is discussed. Several aspects of social problems, such as the liability of an accident affecting the autonomous vehicle, are described. A smart device to detect abnormal driving behaviors to prevent possible accidents is briefly discussed.},
DOI = {10.3390/electronics10091021}
}



@Article{rs13091669,
AUTHOR = {Chen, Zhiang and Wagner, Melissa and Das, Jnaneshwar and Doe, Robert K. and Cerveny, Randall S.},
TITLE = {Data-Driven Approaches for Tornado Damage Estimation with Unpiloted Aerial Systems},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1669},
URL = {https://www.mdpi.com/2072-4292/13/9/1669},
ISSN = {2072-4292},
ABSTRACT = {Tornado damage estimation is important for providing insights into tornado studies and assisting rapid disaster response. However, it is challenging to precisely estimate tornado damage because of the large volumes of perishable data. This study presents data-driven approaches to tornado damage estimation using imagery collected from Unpiloted Aerial Systems (UASs) following the 26 June 2018 Eureka Kansas tornado. High-resolution orthomosaics were generated from Structure from Motion (SfM). We applied deep neural networks (DNNs) on the orthomosaics to estimate tornado damage and assessed their performance in four scenarios: (1) object detection with binary categories, (2) object detection with multiple categories, (3) image classification with binary categories, and (4) image classification with multiple categories. Additionally, two types of tornado damage heatmaps were generated. By directly stitching the resulting image tiles from the DNN inference, we produced the first type of tornado damage heatmaps where damage estimates are accurately georeferenced. We also presented a Gaussian process (GP) regression model to build the second type of tornado damage heatmap (a spatially continuous tornado damage heatmap) by merging the first type of object detection and image classification heatmaps. The GP regression results were assessed with ground-truth annotations and National Weather Service (NWS) ground surveys. This detailed information can help NWS Weather Forecast Offices and emergency managers with their damage assessments and better inform disaster response and recovery.},
DOI = {10.3390/rs13091669}
}



@Article{s21093012,
AUTHOR = {Wang, Wenbo and Aguilar Sanchez, Ignacio and Caparra, Gianluca and McKeown, Andy and Whitworth, Tim and Lohan, Elena Simona},
TITLE = {A Survey of Spoofer Detection Techniques via Radio Frequency Fingerprinting with Focus on the GNSS Pre-Correlation Sampled Data},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3012},
URL = {https://www.mdpi.com/1424-8220/21/9/3012},
PubMedID = {33923015},
ISSN = {1424-8220},
ABSTRACT = {Radio frequency fingerprinting (RFF) methods are becoming more and more popular in the context of identifying genuine transmitters and distinguishing them from malicious or non-authorized transmitters, such as spoofers and jammers. RFF approaches have been studied to a moderate-to-great extent in the context of non-GNSS transmitters, such as WiFi, IoT, or cellular transmitters, but they have not yet been addressed much in the context of GNSS transmitters. In addition, the few RFF-related works in GNSS context are based on post-correlation or navigation data and no author has yet addressed the RFF problem in GNSS with pre-correlation data. Moreover, RFF methods in any of the three domains (pre-correlation, post-correlation, or navigation) are still hard to be found in the context of GNSS. The goal of this paper was two-fold: first, to provide a comprehensive survey of the RFF methods applicable in the GNSS context; and secondly, to propose a novel RFF methodology for spoofing detection, with a focus on GNSS pre-correlation data, but also applicable in a wider context. In order to support our proposed methodology, we qualitatively investigated the capability of different methods to be used in the context of pre-correlation sampled GNSS data, and we present a simulation-based example, under ideal noise conditions, of how the feature down selection can be done. We are also pointing out which of the transmitter features are likely to play the biggest roles in the RFF in GNSS, and which features are likely to fail in helping RFF-based spoofing detection.},
DOI = {10.3390/s21093012}
}



@Article{agriculture11050387,
AUTHOR = {Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu, Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A. and Moore, Steven and Rahman, Sk Mostafizur},
TITLE = {Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {387},
URL = {https://www.mdpi.com/2077-0472/11/5/387},
ISSN = {2077-0472},
ABSTRACT = {This paper explores the potential of machine learning algorithms for weed and crop classification from UAV images. The identification of weeds in crops is a challenging task that has been addressed through orthomosaicing of images, feature extraction and labelling of images to train machine learning algorithms. In this paper, the performances of several machine learning algorithms, random forest (RF), support vector machine (SVM) and k-nearest neighbours (KNN), are analysed to detect weeds using UAV images collected from a chilli crop field located in Australia. The evaluation metrics used in the comparison of performance were accuracy, precision, recall, false positive rate and kappa coefficient. MATLAB is used for simulating the machine learning algorithms; and the achieved weed detection accuracies are 96% using RF, 94% using SVM and 63% using KNN. Based on this study, RF and SVM algorithms are efficient and practical to use, and can be implemented easily for detecting weed from UAV images.},
DOI = {10.3390/agriculture11050387}
}



@Article{w13091191,
AUTHOR = {Lee, Jaeyeong and Kim, Byunghyun},
TITLE = {Scenario-Based Real-Time Flood Prediction with Logistic Regression},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1191},
URL = {https://www.mdpi.com/2073-4441/13/9/1191},
ISSN = {2073-4441},
ABSTRACT = {This study proposed a real-time flood extent prediction method to shorten the time it takes from the flood occurrence to an alert issuance. This method uses logistic regression to generate a flood probability discriminant for each grid constituting the study area, and then predicts the flood extent with the amount of runoff caused by rainfall. In order to generate the flood probability discriminant for each grid, a two-dimensional (2D) flood inundation model was verified by applying the Typhoon Chaba, which caused great damage to the study area in 2016. Then, 100 probability rainfall scenarios were created by combining the return period, duration, and time distribution using past observation rainfall data, and rainfall-runoff–inundation relation databases were built for each scenario by applying hydrodynamic and hydrological models. A flood probability discriminant based on logistic regression was generated for each grid by using whether the grid was flooded (1 or 0) for the runoff amount in the database. When the runoff amount is input to the generated discriminant, the flood probability on the target grid is calculated by the coefficients, so that the flood extent is quickly predicted. The proposed method predicted the flood extent in a few seconds in both cases and showed high accuracy with 83.6~98.4% and 74.4~99.1%, respectively, in the application of scenario rainfall and actual rainfall.},
DOI = {10.3390/w13091191}
}



@Article{rs13091672,
AUTHOR = {Pan, Erting and Ma, Yong and Fan, Fan and Mei, Xiaoguang and Huang, Jun},
TITLE = {Hyperspectral Image Classification across Different Datasets: A Generalization to Unseen Categories},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1672},
URL = {https://www.mdpi.com/2072-4292/13/9/1672},
ISSN = {2072-4292},
ABSTRACT = {With the rapid developments of hyperspectral imaging, the cost of collecting hyperspectral data has been lower, while the demand for reliable and detailed hyperspectral annotations has been much more substantial. However, limited by the difficulties of labelling annotations, most existing hyperspectral image (HSI) classification methods are trained and evaluated on a single hyperspectral data cube. It brings two significant challenges. On the one hand, many algorithms have reached a nearly perfect classification accuracy, but their trained models are hard to generalize to other datasets. On the other hand, since different hyperspectral datasets are usually not collected in the same scene, different datasets will contain different classes. To address these issues, in this paper, we propose a new paradigm for HSI classification, which is training and evaluating separately across different hyperspectral datasets. It is of great help to labelling hyperspectral data. However, it has rarely been studied in the hyperspectral community. In this work, we utilize a three-phase scheme, including feature embedding, feature mapping, and label reasoning. More specifically, we select a pair of datasets acquired by the same hyperspectral sensor, and the classifier learns from one dataset and then evaluated it on the other. Inspired by the latest advances in zero-shot learning, we introduce label semantic representation to establish associations between seen categories in the training set and unseen categories in the testing set. Extensive experiments on two pairs of datasets with different comparative methods have shown the effectiveness and potential of zero-shot learning in HSI classification.},
DOI = {10.3390/rs13091672}
}



@Article{drones5020031,
AUTHOR = {Song, Bonggeun and Park, Kyunghun},
TITLE = {Comparison of Outdoor Compost Pile Detection Using Unmanned Aerial Vehicle Images and Various Machine Learning Techniques},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {31},
URL = {https://www.mdpi.com/2504-446X/5/2/31},
ISSN = {2504-446X},
ABSTRACT = {Since outdoor compost piles (OCPs) contain large amounts of nitrogen and phosphorus, they act as a major pollutant that deteriorates water quality, such as eutrophication and green algae, when the OCPs enter the river during rainfall. In South Korea, OCPs are frequently used, but there is a limitation that a lot of manpower and budget are consumed to investigate the current situation, so it is necessary to efficiently investigate the OCPs. This study compared the accuracy of various machine learning techniques for the efficient detection and management of outdoor compost piles (OCPs), a non-point pollution source in agricultural areas in South Korea, using unmanned aerial vehicle (UAV) images. RGB, multispectral, and thermal infrared UAV images were taken in August and October 2019. Additionally, vegetation indices (NDVI, NDRE, ENDVI, and GNDVI) and surface temperature were also considered. Four machine learning techniques, including support vector machine (SVM), decision tree (DT), random forest (RF), and k-NN, were implemented, and the machine learning technique with the highest accuracy was identified by adjusting several variables. The accuracy of all machine learning techniques was very high, reaching values of up to 0.96. Particularly, the accuracy of the RF method with the number of estimators set to 10 was highest, reaching 0.989 in August and 0.987 in October. The proposed method allows for the prediction of OCP location and area over large regions, thereby foregoing the need for OCP field measurements. Therefore, our findings provide highly useful data for the improvement of OCP management strategies and water quality.},
DOI = {10.3390/drones5020031}
}



@Article{rs13091679,
AUTHOR = {Elsayed, Salah and El-Hendawy, Salah and Khadr, Mosaad and Elsherbiny, Osama and Al-Suhaibani, Nasser and Alotaibi, Majed and Tahir, Muhammad Usman and Darwish, Waleed},
TITLE = {Combining Thermal and RGB Imaging Indices with Multivariate and Data-Driven Modeling to Estimate the Growth, Water Status, and Yield of Potato under Different Drip Irrigation Regimes},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1679},
URL = {https://www.mdpi.com/2072-4292/13/9/1679},
ISSN = {2072-4292},
ABSTRACT = {Advances in proximal hyperspectral sensing tools, chemometric techniques, and data-driven modeling have enhanced precision irrigation management by facilitating the monitoring of several plant traits. This study investigated the performance of remote sensing indices derived from thermal and red-green-blue (RGB) images combined with stepwise multiple linear regression (SMLR) and an integrated adaptive neuro-fuzzy inference system with a genetic algorithm (ANFIS-GA) for monitoring the biomass fresh weight (BFW), biomass dry weight (BDW), biomass water content (BWC), and total tuber yield (TTY) of two potato varieties under 100%, 75%, and 50% of the estimated crop evapotranspiration (ETc). Results showed that the plant traits and indices varied significantly between the three irrigation regimes. Furthermore, all of the indices exhibited strong relationships with BFW, CWC, and TTY (R2 = 0.80–0.92) and moderate to weak relationships with BDW (R2 = 0.25–0.65) when considered for each variety across the irrigation regimes, for each season across the varieties and irrigation regimes, and across all data combined, but none of the indices successfully assessed any of the plant traits when considered for each irrigation regime across the two varieties. The SMLR and ANFIS-GA models gave the best predictions for the four plant traits in the calibration and testing stages, with the exception of the SMLR testing model for BDW. Thus, the use of thermal and RGB imaging indices with ANFIS-GA models could be a practical tool for managing the growth and production of potato crops under deficit irrigation regimes.},
DOI = {10.3390/rs13091679}
}



@Article{aerospace8050125,
AUTHOR = {Shauqee, Mohamad Norherman and Rajendran, Parvathy and Suhadis, Nurulasikin Mohd},
TITLE = {An Explosion Based Algorithm to Solve the Optimization Problem in Quadcopter Control},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {125},
URL = {https://www.mdpi.com/2226-4310/8/5/125},
ISSN = {2226-4310},
ABSTRACT = {This paper presents an optimization algorithm named Random Explosion Algorithm (REA). The fundamental idea of this algorithm is based on a simple concept of the explosion of an object. This object is commonly known as a particle: when exploded, it will randomly disperse fragments around the particle within the explosion radius. The fragment that will be considered as a search agent will fill the local space and search that particular region for the best fitness solution. The proposed algorithm was tested on 23 benchmark test functions, and the results are validated by a comparative study with eight well-known algorithms, which are Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC), Genetic Algorithm (GA), Differential Evolution (DE), Multi-Verse Optimizer (MVO), Moth Flame Optimizer (MFO), Firefly Algorithm (FA), and Sooty Tern Optimization Algorithm (STOA). After that, the algorithm was implemented and analyzed for a quadrotor control application. Similarly, a comparative study with the other algorithms stated was done. The findings reveal that the REA can yield very competitive results. It also shows that the convergence analysis has proved that the REA can converge more quickly toward the global optimum than the other metaheuristic algorithms. For the control application result, the REA controller can better track the desired reference input with shorter rise time and settling time, lower percentage overshoot, and minimal steady-state error and root mean square error (RMSE).},
DOI = {10.3390/aerospace8050125}
}



@Article{rs13091682,
AUTHOR = {Sodango, Terefe Hanchiso and Sha, Jinming and Li, Xiaomei and Noszczyk, Tomasz and Shang, Jiali and Aneseyee, Abreham Berta and Bao, Zhongcong},
TITLE = {Modeling the Spatial Dynamics of Soil Organic Carbon Using Remotely-Sensed Predictors in Fuzhou City, China},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1682},
URL = {https://www.mdpi.com/2072-4292/13/9/1682},
ISSN = {2072-4292},
ABSTRACT = {Assessing the spatial dynamics of soil organic carbon (SOC) is essential for carbon monitoring. Since variability of SOC is mainly attributed to biophysical land surface variables, integrating a compressive set of such indices may support the pursuit of an optimum set of predictor variables. Therefore, this study was aimed at predicting the spatial distribution of SOC in relation to remotely sensed variables and other covariates. Hence, the land surface variables were combined from remote sensing, topographic, and soil spectral sources. Moreover, the most influential variables for prediction were selected using the random forest (RF) and classification and regression tree (CART). The results indicated that the RF model has good prediction performance with corresponding R2 and root-mean-square error (RMSE) values of 0.96 and 0.91 mg·g−1, respectively. The distribution of SOC content showed variability across landforms (CV = 78.67%), land use (CV = 93%), and lithology (CV = 64.67%). Forestland had the highest SOC (13.60 mg·g−1) followed by agriculture (10.43 mg·g−1), urban (9.74 mg·g−1), and water body (4.55 mg·g−1) land uses. Furthermore, soils developed in bauxite and laterite lithology had the highest SOC content (14.69 mg·g−1). The SOC content was remarkably lower in soils developed in sandstones; however, the values obtained in soils from the rest of the lithologies could not be significantly differentiated. The mean SOC concentration was 11.70 mg·g−1, where the majority of soils in the study area were classified as highly humus and extremely humus. The soils with the highest SOC content (extremely humus) were distributed in the mountainous regions of the study area. The biophysical land surface indices, brightness removed vegetation indices, topographic indices, and soil spectral bands were the most influential predictors of SOC in the study area. The spatial variability of SOC may be influenced by landform, land use, and lithology of the study area. Remotely sensed predictors including land moisture, land surface temperature, and built-up indices added valuable information for the prediction of SOC. Hence, the land surface indices may provide new insights into SOC modeling in complex landscapes of warm subtropical urban regions.},
DOI = {10.3390/rs13091682}
}



@Article{en14092484,
AUTHOR = {Rinaldi, Giovanni and Thies, Philipp R. and Johanning, Lars},
TITLE = {Current Status and Future Trends in the Operation and Maintenance of Offshore Wind Turbines: A Review},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {2484},
URL = {https://www.mdpi.com/1996-1073/14/9/2484},
ISSN = {1996-1073},
ABSTRACT = {Operation and maintenance constitute a substantial share of the lifecycle expenditures of an offshore renewable energy farm. A noteworthy number of methods and techniques have been developed to provide decision-making support in strategic planning and asset management. Condition monitoring instrumentation is commonly used, especially in offshore wind farms, due to the benefits it provides in terms of fault identification and performance evaluation and improvement. Incorporating technology advancements, a shift towards automation and digitalisation is taking place in the offshore maintenance sector. This paper reviews the existing literature and novel approaches in the operation and maintenance planning and the condition monitoring of offshore renewable energy farms, with an emphasis on the offshore wind sector, discussing their benefits and limitations. The state-of-the-art in industrial condition-based maintenance is reviewed, together with deterioration models and fault diagnosis and prognosis techniques. Future scenarios in robotics, artificial intelligence and data processing are investigated. The application challenges of these strategies and Industry 4.0 concepts in the offshore renewables sector are scrutinised, together with the potential implications of early-stage project integration. The identified technologies are ranked against a series of indicators, providing a reference for a range of industry stakeholders.},
DOI = {10.3390/en14092484}
}



@Article{technologies9020031,
AUTHOR = {Ahsan, Md Manjurul and Li, Yueqing and Zhang, Jing and Ahad, Md Tanvir and Gupta, Kishor Datta},
TITLE = {Evaluating the Performance of Eigenface, Fisherface, and Local Binary Pattern Histogram-Based Facial Recognition Methods under Various Weather Conditions},
JOURNAL = {Technologies},
VOLUME = {9},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {31},
URL = {https://www.mdpi.com/2227-7080/9/2/31},
ISSN = {2227-7080},
ABSTRACT = {Facial recognition (FR) in unconstrained weather is still challenging and surprisingly ignored by many researchers and practitioners over the past few decades. Therefore, this paper aims to evaluate the performance of three existing popular facial recognition methods considering different weather conditions. As a result, a new face dataset (Lamar University database (LUDB)) was developed that contains face images captured under various weather conditions such as foggy, cloudy, rainy, and sunny. Three very popular FR methods—Eigenface (EF), Fisherface (FF), and Local binary pattern histogram (LBPH)—were evaluated considering two other face datasets, AT&amp;T and 5_Celebrity, along with LUDB in term of accuracy, precision, recall, and F1 score with 95% confidence interval (CI). Computational results show a significant difference among the three FR techniques in terms of overall time complexity and accuracy. LBPH outperforms the other two FR algorithms on both LUDB and 5_Celebrity datasets by achieving 40% and 95% accuracy, respectively. On the other hand, with minimum execution time of 1.37, 1.37, and 1.44 s per image on AT&amp;T,5_Celebrity, and LUDB, respectively, Fisherface achieved the best result.},
DOI = {10.3390/technologies9020031}
}



@Article{robotics10020063,
AUTHOR = {Madokoro, Hirokazu and Yamamoto, Satoshi and Nishimura, Yo and Nix, Stephanie and Woo, Hanwool and Sato, Kazuhito},
TITLE = {Prototype Development of Small Mobile Robots for Mallard Navigation in Paddy Fields: Toward Realizing Remote Farming},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {63},
URL = {https://www.mdpi.com/2218-6581/10/2/63},
ISSN = {2218-6581},
ABSTRACT = {This study was conducted to develop robot prototypes of three models that navigate mallards to achieve high-efficiency rice-duck farming. We examined two robotics navigation approaches based on imprinting and feeding. As the first approach, we used imprinting applied to baby mallards. They exhibited follow behavior to our first prototype after imprinting. Experimentally obtained observation results revealed the importance of providing imprinting immediately up to one week after hatching. As another approach, we used feed placed on the top of our second prototype. Experimentally obtained results showed that adult mallards exhibited wariness not only against the robot, but also against the feeder. After relieving wariness with provision of more than one week time to become accustomed, adult mallards ate feed in the box on the robot. However, they ran away immediately at a slight movement. Based on this confirmation, we developed the third prototype as an autonomous mobile robot aimed for mallard navigation in a paddy field. The body width is less than the length between rice stalks. After checking the waterproof capability of a body waterproof box, we conducted an indoor driving test for manual operation. Moreover, we conducted outdoor evaluation tests to assess running on an actual paddy field. We developed indoor and outdoor image datasets using an onboard monocular camera. For the outdoor image datasets, our segmentation method based on SegNet achieved semantic segmentation for three semantic categories. For the indoor image datasets, our prediction method based on CNN and LSTM achieved visual prediction for three motion categories.},
DOI = {10.3390/robotics10020063}
}



@Article{ani11051263,
AUTHOR = {Wang, Zhaojun and Wang, Jiangning and Lin, Congtian and Han, Yan and Wang, Zhaosheng and Ji, Liqiang},
TITLE = {Identifying Habitat Elements from Bird Images Using Deep Convolutional Neural Networks},
JOURNAL = {Animals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1263},
URL = {https://www.mdpi.com/2076-2615/11/5/1263},
PubMedID = {33925654},
ISSN = {2076-2615},
ABSTRACT = {With the rapid development of digital technology, bird images have become an important part of ornithology research data. However, due to the rapid growth of bird image data, it has become a major challenge to effectively process such a large amount of data. In recent years, deep convolutional neural networks (DCNNs) have shown great potential and effectiveness in a variety of tasks regarding the automatic processing of bird images. However, no research has been conducted on the recognition of habitat elements in bird images, which is of great help when extracting habitat information from bird images. Here, we demonstrate the recognition of habitat elements using four DCNN models trained end-to-end directly based on images. To carry out this research, an image database called Habitat Elements of Bird Images (HEOBs-10) and composed of 10 categories of habitat elements was built, making future benchmarks and evaluations possible. Experiments showed that good results can be obtained by all the tested models. ResNet-152-based models yielded the best test accuracy rate (95.52%); the AlexNet-based model yielded the lowest test accuracy rate (89.48%). We conclude that DCNNs could be efficient and useful for automatically identifying habitat elements from bird images, and we believe that the practical application of this technology will be helpful for studying the relationships between birds and habitat elements.},
DOI = {10.3390/ani11051263}
}



@Article{geomatics1020013,
AUTHOR = {Duarte, Lia and Teodoro, Ana Cláudia},
TITLE = {GIS Open-Source Plugins Development: A 10-Year Bibliometric Analysis on Scientific Literature},
JOURNAL = {Geomatics},
VOLUME = {1},
YEAR = {2021},
NUMBER = {2},
PAGES = {206--245},
URL = {https://www.mdpi.com/2673-7418/1/2/13},
ISSN = {2673-7418},
ABSTRACT = {The advent of Geographical Information Systems (GIS) has changed the way people think and interact with the world. The main objectives of this paper are: (i) to provide an overview of 10 years (2010–2020) regarding the creation/development of GIS open-source applications; and (ii) to evaluate the GIS open-source plugins for environmental science. In the first objective, we evaluate the publications regarding the development of GIS open-source geospatial software in the last 10 years, considering desktop, web GIS and mobile applications, so that we can analyze the impact of this type of application for different research areas. In the second objective, we analyze the development of GIS open-source applications in the field of environmental sciences (with more focus on QGIS plugins) in the last 10 years and discuss the applicability and usability of these GIS solutions in different environmental domains. A bibliometric analysis was performed using Web of Science database and VOSViewer software. We concluded that, in general, the development of GIS open-source applications has increased in the last 10 years, especially GIS mobile applications, since the big data and Internet of Things (IoT) era, which was expected given the new advanced technologies available in every area, especially in GIS.},
DOI = {10.3390/geomatics1020013}
}



@Article{rs13091704,
AUTHOR = {de Camargo, Tibor and Schirrmann, Michael and Landwehr, Niels and Dammer, Karl-Heinz and Pflanz, Michael},
TITLE = {Optimized Deep Learning Model as a Basis for Fast UAV Mapping of Weed Species in Winter Wheat Crops},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1704},
URL = {https://www.mdpi.com/2072-4292/13/9/1704},
ISSN = {2072-4292},
ABSTRACT = {Weed maps should be available quickly, reliably, and with high detail to be useful for site-specific management in crop protection and to promote more sustainable agriculture by reducing pesticide use. Here, the optimization of a deep residual convolutional neural network (ResNet-18) for the classification of weed and crop plants in UAV imagery is proposed. The target was to reach sufficient performance on an embedded system by maintaining the same features of the ResNet-18 model as a basis for fast UAV mapping. This would enable online recognition and subsequent mapping of weeds during UAV flying operation. Optimization was achieved mainly by avoiding redundant computations that arise when a classification model is applied on overlapping tiles in a larger input image. The model was trained and tested with imagery obtained from a UAV flight campaign at low altitude over a winter wheat field, and classification was performed on species level with the weed species Matricaria chamomilla L., Papaver rhoeas L., Veronica hederifolia L., and Viola arvensis ssp. arvensis observed in that field. The ResNet-18 model with the optimized image-level prediction pipeline reached a performance of 2.2 frames per second with an NVIDIA Jetson AGX Xavier on the full resolution UAV image, which would amount to about 1.78 ha h−1 area output for continuous field mapping. The overall accuracy for determining crop, soil, and weed species was 94%. There were some limitations in the detection of species unknown to the model. When shifting from 16-bit to 32-bit model precision, no improvement in classification accuracy was observed, but a strong decline in speed performance, especially when a higher number of filters was used in the ResNet-18 model. Future work should be directed towards the integration of the mapping process on UAV platforms, guiding UAVs autonomously for mapping purpose, and ensuring the transferability of the models to other crop fields.},
DOI = {10.3390/rs13091704}
}



@Article{f12050550,
AUTHOR = {Xu, Dandan and Wang, Haobin and Xu, Weixin and Luan, Zhaoqing and Xu, Xia},
TITLE = {LiDAR Applications to Estimate Forest Biomass at Individual Tree Scale: Opportunities, Challenges and Future Perspectives},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {550},
URL = {https://www.mdpi.com/1999-4907/12/5/550},
ISSN = {1999-4907},
ABSTRACT = {Accurate forest biomass estimation at the individual tree scale is the foundation of timber industry and forest management. It plays an important role in explaining ecological issues and small-scale processes. Remotely sensed images, across a range of spatial and temporal resolutions, with their advantages of non-destructive monitoring, are widely applied in forest biomass monitoring at global, ecoregion or community scales. However, the development of remote sensing applications for forest biomass at the individual tree scale has been relatively slow due to the constraints of spatial resolution and evaluation accuracy of remotely sensed data. With the improvements in platforms and spatial resolutions, as well as the development of remote sensing techniques, the potential for forest biomass estimation at the single tree level has been demonstrated. However, a comprehensive review of remote sensing of forest biomass scaled at individual trees has not been done. This review highlights the theoretical bases, challenges and future perspectives for Light Detection and Ranging (LiDAR) applications of individual trees scaled to whole forests. We summarize research on estimating individual tree volume and aboveground biomass (AGB) using Terrestrial Laser Scanning (TLS), Airborne Laser Scanning (ALS), Unmanned Aerial Vehicle Laser Scanning (UAV-LS) and Mobile Laser Scanning (MLS, including Vehicle-borne Laser Scanning (VLS) and Backpack Laser Scanning (BLS)) data.},
DOI = {10.3390/f12050550}
}



@Article{rs13091712,
AUTHOR = {Ramsauer, Thomas and Weiß, Thomas and Löw, Alexander and Marzahn, Philip},
TITLE = {RADOLAN_API: An Hourly Soil Moisture Data Set Based on Weather Radar, Soil Properties and Reanalysis Temperature Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1712},
URL = {https://www.mdpi.com/2072-4292/13/9/1712},
ISSN = {2072-4292},
ABSTRACT = {Soil moisture is a key variable in the terrestrial water and energy system. This study presents an hourly index that provides soil moisture estimates on a high spatial and temporal resolution (1 km × 1 km). The long established Antecedent Precipitation Index (API) is extended with soil characteristic and temperature dependent loss functions. The Soilgrids and ERA5 data sets are used to provide the controlling variables. Precipitation as main driver is provided by the German weather radar data set RADOLAN. Empiric variables in the equations are fitted in a optimization effort using 23 in-situ soil moisture measurement stations from the Terrestial Environmental Observatories (TERENO) and a separately conducted field campaign. The volumetric soil moisture estimation results show error values of 3.45 Vol% mean ubRMSD between RADOLAN_API and station data with a high temporal accordance especially of soil moisture upsurge. Further potential of the improved API algorithm is shown with a per-station calibration of applied empirical variables. In addition, the RADOLAN_API data set was spatially compared to the ESA CCI soil moisture product where it altogether demonstrates good agreement. The resulting data set is provided as open access data.},
DOI = {10.3390/rs13091712}
}



@Article{rs13091723,
AUTHOR = {Kuzmin, Anton and Korhonen, Lauri and Kivinen, Sonja and Hurskainen, Pekka and Korpelainen, Pasi and Tanhuanpää, Topi and Maltamo, Matti and Vihervaara, Petteri and Kumpula, Timo},
TITLE = {Detection of European Aspen (Populus tremula L.) Based on an Unmanned Aerial Vehicle Approach in Boreal Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1723},
URL = {https://www.mdpi.com/2072-4292/13/9/1723},
ISSN = {2072-4292},
ABSTRACT = {European aspen (Populus tremula L.) is a keystone species for biodiversity of boreal forests. Large-diameter aspens maintain the diversity of hundreds of species, many of which are threatened in Fennoscandia. Due to a low economic value and relatively sparse and scattered occurrence of aspen in boreal forests, there is a lack of information of the spatial and temporal distribution of aspen, which hampers efficient planning and implementation of sustainable forest management practices and conservation efforts. Our objective was to assess identification of European aspen at the individual tree level in a southern boreal forest using high-resolution photogrammetric point cloud (PPC) and multispectral (MSP) orthomosaics acquired with an unmanned aerial vehicle (UAV). The structure-from-motion approach was applied to generate RGB imagery-based PPC to be used for individual tree-crown delineation. Multispectral data were collected using two UAV cameras: Parrot Sequoia and MicaSense RedEdge-M. Tree-crown outlines were obtained from watershed segmentation of PPC data and intersected with multispectral mosaics to extract and calculate spectral metrics for individual trees. We assessed the role of spectral data features extracted from PPC and multispectral mosaics and a combination of it, using a machine learning classifier—Support Vector Machine (SVM) to perform two different classifications: discrimination of aspen from the other species combined into one class and classification of all four species (aspen, birch, pine, spruce) simultaneously. In the first scenario, the highest classification accuracy of 84% (F1-score) for aspen and overall accuracy of 90.1% was achieved using only RGB features from PPC, whereas in the second scenario, the highest classification accuracy of 86 % (F1-score) for aspen and overall accuracy of 83.3% was achieved using the combination of RGB and MSP features. The proposed method provides a new possibility for the rapid assessment of aspen occurrence to enable more efficient forest management as well as contribute to biodiversity monitoring and conservation efforts in boreal forests.},
DOI = {10.3390/rs13091723}
}



@Article{app11094084,
AUTHOR = {Hua, Lianghao and Zhang, Jianfeng and Li, Dejie and Xi, Xiaobo},
TITLE = {Fault-Tolerant Active Disturbance Rejection Control of Plant Protection of Unmanned Aerial Vehicles Based on a Spatio-Temporal RBF Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4084},
URL = {https://www.mdpi.com/2076-3417/11/9/4084},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a fault-tolerant flight control method for a multi-rotor UAV under actuator failure and external wind disturbances. The control method is based on an active disturbance rejection controller (ADRC) and spatio-temporal radial basis function neural networks, which can be used to achieve the stable control of the system when the parameters of the UAV mathematical model change. Firstly, an active disturbance rejection controller with an optimized parameter design is designed for rthe obust control of a multi-rotor vehicle. Secondly, a spatio-temporal radial basis function neural network with a new adaptive kernel is designed. In addition, the output of the novel radial basis function neural network is used to estimate fusion parameters containing actuator faults and model uncertainties and, consequently, to design an active fault-tolerant controller for a multi-rotor vehicle. Finally, fault injection experiments are carried out with the Qball-X4 quadrotor UAV as a specific research object, and the experimental results show the effectiveness of the proposed self-tolerant, fault-tolerant control method.},
DOI = {10.3390/app11094084}
}



@Article{rs13091741,
AUTHOR = {Hobley, Brandon and Arosio, Riccardo and French, Geoffrey and Bremner, Julie and Dolphin, Tony and Mackiewicz, Michal},
TITLE = {Semi-Supervised Segmentation for Coastal Monitoring Seagrass Using RPA Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1741},
URL = {https://www.mdpi.com/2072-4292/13/9/1741},
ISSN = {2072-4292},
ABSTRACT = {Intertidal seagrass plays a vital role in estimating the overall health and dynamics of coastal environments due to its interaction with tidal changes. However, most seagrass habitats around the globe have been in steady decline due to human impacts, disturbing the already delicate balance in the environmental conditions that sustain seagrass. Miniaturization of multi-spectral sensors has facilitated very high resolution mapping of seagrass meadows, which significantly improves the potential for ecologists to monitor changes. In this study, two analytical approaches used for classifying intertidal seagrass habitats are compared—Object-based Image Analysis (OBIA) and Fully Convolutional Neural Networks (FCNNs). Both methods produce pixel-wise classifications in order to create segmented maps. FCNNs are an emerging set of algorithms within Deep Learning. Conversely, OBIA has been a prominent solution within this field, with many studies leveraging in-situ data and multiresolution segmentation to create habitat maps. This work demonstrates the utility of FCNNs in a semi-supervised setting to map seagrass and other coastal features from an optical drone survey conducted at Budle Bay, Northumberland, England. Semi-supervision is also an emerging field within Deep Learning that has practical benefits of achieving state of the art results using only subsets of labelled data. This is especially beneficial for remote sensing applications where in-situ data is an expensive commodity. For our results, we show that FCNNs have comparable performance with the standard OBIA method used by ecologists.},
DOI = {10.3390/rs13091741}
}



@Article{rs13091749,
AUTHOR = {Wang, Zhe and Fan, Chao and Xian, Min},
TITLE = {Application and Evaluation of a Deep Learning Architecture to Urban Tree Canopy Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1749},
URL = {https://www.mdpi.com/2072-4292/13/9/1749},
ISSN = {2072-4292},
ABSTRACT = {Urban forest is a dynamic urban ecosystem that provides critical benefits to urban residents and the environment. Accurate mapping of urban forest plays an important role in greenspace management. In this study, we apply a deep learning model, the U-net, to urban tree canopy mapping using high-resolution aerial photographs. We evaluate the feasibility and effectiveness of the U-net in tree canopy mapping through experiments at four spatial scales—16 cm, 32 cm, 50 cm, and 100 cm. The overall performance of all approaches is validated on the ISPRS Vaihingen 2D Semantic Labeling dataset using four quantitative metrics, Dice, Intersection over Union, Overall Accuracy, and Kappa Coefficient. Two evaluations are performed to assess the model performance. Experimental results show that the U-net with the 32-cm input images perform the best with an overall accuracy of 0.9914 and an Intersection over Union of 0.9638. The U-net achieves the state-of-the-art overall performance in comparison with object-based image analysis approach and other deep learning frameworks. The outstanding performance of the U-net indicates a possibility of applying it to urban tree segmentation at a wide range of spatial scales. The U-net accurately recognizes and delineates tree canopy for different land cover features and has great potential to be adopted as an effective tool for high-resolution land cover mapping.},
DOI = {10.3390/rs13091749}
}



@Article{s21093152,
AUTHOR = {Liang, Peng and Shi, Wenzhong and Ding, Yixing and Liu, Zhiqiang and Shang, Haolv},
TITLE = {Road Extraction from High Resolution Remote Sensing Images Based on Vector Field Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3152},
URL = {https://www.mdpi.com/1424-8220/21/9/3152},
PubMedID = {34062917},
ISSN = {1424-8220},
ABSTRACT = {Accurate and up-to-date road network information is very important for the Geographic Information System (GIS) database, traffic management and planning, automatic vehicle navigation, emergency response and urban pollution sources investigation. In this paper, we use vector field learning to extract roads from high resolution remote sensing imaging. This method is usually used for skeleton extraction in nature image, but seldom used in road extraction. In order to improve the accuracy of road extraction, three vector fields are constructed and combined respectively with the normal road mask learning by a two-task network. The results show that all the vector fields are able to significantly improve the accuracy of road extraction, no matter the field is constructed in the road area or completely outside the road. The highest F1 score is 0.7618, increased by 0.053 compared with using only mask learning.},
DOI = {10.3390/s21093152}
}



@Article{s21093169,
AUTHOR = {Caldeira, Rafael Faria and Santiago, Wesley Esdras and Teruel, Barbara},
TITLE = {Identification of Cotton Leaf Lesions Using Deep Learning Techniques},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3169},
URL = {https://www.mdpi.com/1424-8220/21/9/3169},
PubMedID = {34063578},
ISSN = {1424-8220},
ABSTRACT = {The use of deep learning models to identify lesions on cotton leaves on the basis of images of the crop in the field is proposed in this article. Cultivated in most of the world, cotton is one of the economically most important agricultural crops. Its cultivation in tropical regions has made it the target of a wide spectrum of agricultural pests and diseases, and efficient solutions are required. Moreover, the symptoms of the main pests and diseases cannot be differentiated in the initial stages, and the correct identification of a lesion can be difficult for the producer. To help resolve the problem, the present research provides a solution based on deep learning in the screening of cotton leaves which makes it possible to monitor the health of the cotton crop and make better decisions for its management. With the learning models GoogleNet and Resnet50 using convolutional neural networks, a precision of 86.6% and 89.2%, respectively, was obtained. Compared with traditional approaches for the processing of images such as support vector machines (SVM), Closest k-neighbors (KNN), artificial neural networks (ANN) and neuro-fuzzy (NFC), the convolutional neural networks proved to be up to 25% more precise, suggesting that this method can contribute to a more rapid and reliable inspection of the plants growing in the field.},
DOI = {10.3390/s21093169}
}



@Article{ijgi10050293,
AUTHOR = {Maxwell, Aaron E. and Sharma, Maneesh and Kite, J. Steven and Donaldson, Kurt A. and Maynard, Shannon M. and Malay, Caleb M.},
TITLE = {Assessing the Generalization of Machine Learning-Based Slope Failure Prediction to New Geographic Extents},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {293},
URL = {https://www.mdpi.com/2220-9964/10/5/293},
ISSN = {2220-9964},
ABSTRACT = {Slope failure probabilistic models generated using random forest (RF) machine learning (ML), manually interpreted incident points, and light detection and ranging (LiDAR) digital terrain variables are assessed for predicting and generalizing to new geographic extents. Specifically, models for four Major Land Resource Areas (MLRAs) in the state of West Virginia in the United States (US) were created. All region-specific models were then used to predict withheld validation data within all four MLRAs. For all validation datasets, the model trained using data from the same MLRA provided the highest reported overall accuracy (OA), Kappa statistic, F1 Score, area under the receiver operating characteristic curve (AUC ROC), and area under the precision-recall curve (AUC PR). However, the model from the same MLRA as the validation dataset did not always provide the highest precision, recall, and/or specificity, suggesting that models extrapolated to new geographic extents tend to either overpredict or underpredict the land area of slope failure occurrence whereas they offer a better balance between omission and commission error within the region in which they were trained. This study highlights the value of developing region-specific inventories, models, and high resolution and detailed digital elevation data, since models may not generalize well to new geographic extents, potentially resulting from spatial heterogeneity in landscape and/or slope failure characteristics.},
DOI = {10.3390/ijgi10050293}
}



@Article{rs13091792,
AUTHOR = {Wang, Li and Chen, Shuisen and Peng, Zhiping and Huang, Jichuan and Wang, Chongyang and Jiang, Hao and Zheng, Qiong and Li, Dan},
TITLE = {Phenology Effects on Physically Based Estimation of Paddy Rice Canopy Traits from UAV Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1792},
URL = {https://www.mdpi.com/2072-4292/13/9/1792},
ISSN = {2072-4292},
ABSTRACT = {Radiation transform models such as PROSAIL are widely used for crop canopy reflectance simulation and biophysical parameter inversion. The PROSAIL model basically assumes that the canopy is turbid homogenous media with a bare soil background. However, the canopy structure changes when crop growth stages develop, which is more or less a departure from this assumption. In addition, a paddy rice field is inundated most of the time with flooded soil background. In this study, field-scale paddy rice leaf area index (LAI), leaf cholorphyll content (LCC), and canopy chlorophyll content (CCC) were retrieved from unmanned-aerial-vehicle-based hyperspectral images by the PROSAIL radiation transform model using a lookup table (LUT) strategy, with a special focus on the effects of growth-stage development and soil-background signature selection. Results show that involving flooded soil reflectance as background reflectance for PROSAIL could improve estimation accuracy. When using a LUT with the flooded soil reflectance signature (LUTflooded) the coefficients of determination (R2) between observed and estimation variables are 0.70, 0.11, and 0.79 for LAI, LCC, and CCC, respectively, for the entire growing season (from tillering to heading growth stages), and the corresponding mean absolute errors (MAEs) are 21.87%, 16.27%, and 12.52%. For LAI and LCC, high model bias mainly occurred in tillering growth stages. There is an obvious overestimation of LAI and underestimation of LCC for in the tillering growth stage. The estimation accuracy of CCC is relatively consistent from tillering to heading growth stages.},
DOI = {10.3390/rs13091792}
}



@Article{s21093204,
AUTHOR = {Shin, Sungtae and Yoon, Han Ul and Yoo, Byungseok},
TITLE = {Hand Gesture Recognition Using EGaIn-Silicone Soft Sensors},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3204},
URL = {https://www.mdpi.com/1424-8220/21/9/3204},
PubMedID = {34063055},
ISSN = {1424-8220},
ABSTRACT = {Exploiting hand gestures for non-verbal communication has extraordinary potential in HCI. A data glove is an apparatus widely used to recognize hand gestures. To improve the functionality of the data glove, a highly stretchable and reliable signal-to-noise ratio sensor is indispensable. To do this, the study focused on the development of soft silicone microchannel sensors using a Eutectic Gallium-Indium (EGaIn) liquid metal alloy and a hand gesture recognition system via the proposed data glove using the soft sensor. The EGaIn-silicone sensor was uniquely designed to include two sensing channels to monitor the finger joint movements and to facilitate the EGaIn alloy injection into the meander-type microchannels. We recruited 15 participants to collect hand gesture dataset investigating 12 static hand gestures. The dataset was exploited to estimate the performance of the proposed data glove in hand gesture recognition. Additionally, six traditional classification algorithms were studied. From the results, a random forest shows the highest classification accuracy of 97.3% and a linear discriminant analysis shows the lowest accuracy of 87.4%. The non-linearity of the proposed sensor deteriorated the accuracy of LDA, however, the other classifiers adequately overcame it and performed high accuracies (&gt;90%).},
DOI = {10.3390/s21093204}
}



@Article{drones5020034,
AUTHOR = {Görlich, Florian and Marks, Elias and Mahlein, Anne-Katrin and König, Kathrin and Lottes, Philipp and Stachniss, Cyrill},
TITLE = {UAV-Based Classification of Cercospora Leaf Spot Using RGB Images},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {34},
URL = {https://www.mdpi.com/2504-446X/5/2/34},
ISSN = {2504-446X},
ABSTRACT = {Plant diseases can impact crop yield. Thus, the detection of plant diseases using sensors that can be mounted on aerial vehicles is in the interest of farmers to support decision-making in integrated pest management and to breeders for selecting tolerant or resistant genotypes. This paper investigated the detection of Cercospora leaf spot (CLS), caused by Cercospora beticola in sugar beet using RGB imagery. We proposed an approach to tackle the CLS detection problem using fully convolutional neural networks, which operate directly on RGB images captured by a UAV. This efficient approach does not require complex multi- or hyper-spectral sensors, but provides reliable results and high sensitivity. We provided a detection pipeline for pixel-wise semantic segmentation of CLS symptoms, healthy vegetation, and background so that our approach can automatically quantify the grade of infestation. We thoroughly evaluated our system using multiple UAV datasets recorded from different sugar beet trial fields. The dataset consisted of a training and a test dataset and originated from different fields. We used it to evaluate our approach under realistic conditions and analyzed its generalization capabilities to unseen environments. The obtained results correlated to visual estimation by human experts significantly. The presented study underlined the potential of high-resolution RGB imaging and convolutional neural networks for plant disease detection under field conditions. The demonstrated procedure is particularly interesting for applications under practical conditions, as no complex and cost-intensive measuring system is required.},
DOI = {10.3390/drones5020034}
}



@Article{s21093220,
AUTHOR = {Han, Aru and Lu, Xiaoling and Qing, Song and Bao, Yongbin and Bao, Yuhai and Ma, Qing and Liu, Xingpeng and Zhang, Jiquan},
TITLE = {Rapid Determination of Low Heavy Metal Concentrations in Grassland Soils around Mining Using Vis–NIR Spectroscopy: A Case Study of Inner Mongolia, China},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3220},
URL = {https://www.mdpi.com/1424-8220/21/9/3220},
PubMedID = {34066493},
ISSN = {1424-8220},
ABSTRACT = {Proximal sensing offers a novel means for determination of the heavy metal concentration in soil, facilitating low cost and rapid analysis over large areas. In this respect, spectral data and model variables play an important role. Thus far, no attempts have been made to estimate soil heavy metal content using continuum-removal (CR), different preprocessing and statistical methods, and different modeling variables. Considering the adsorption and retention of heavy metals in spectrally active constituents in soil, this study proposes a method for determining low heavy metal concentrations in soil using spectral bands associated with soil organic matter (SOM) and visible–near-infrared (Vis–NIR). To rapidly determine the concentration of heavy metals using hyperspectral data, partial least squares regression (PLSR), principal component regression (PCR), and support vector machine regression (SVMR) statistical methods and 16 preprocessing combinations were developed and explored to determine an optimal combination. The results showed that the multiplicative scatter correction and standard normal variate preprocessing methods evaluated with the second derivative spectral transformation method could accurately determine soil Cr and Ni concentrations. The root-mean-square error (RMSE) values of Vis–NIR model combinations with PLSR, PCR, and SVMR were 0.34, 3.42, and 2.15 for Cr, and 0.07, 1.78, and 1.14 for Ni, respectively. Soil Cr and Ni showed strong spectral responses to the Vis–NIR spectral band. The R2 value of the Vis–NIR-based PLSR model was higher than 0.99, and the RMSE value was 0.07–0.34, suggesting higher stability and accuracy. The results were more accurate for Ni than Cr, and PLSR showed the best performance, followed by SVMR and PCR. This perspective has critical implications for guiding quantitative biogeochemical analysis using proximal sensing data.},
DOI = {10.3390/s21093220}
}



@Article{rs13091809,
AUTHOR = {Feroz, Sainab and Abu Dabous, Saleh},
TITLE = {UAV-Based Remote Sensing Applications for Bridge Condition Assessment},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1809},
URL = {https://www.mdpi.com/2072-4292/13/9/1809},
ISSN = {2072-4292},
ABSTRACT = {Deterioration of bridge infrastructure is a serious concern to transport and government agencies as it declines serviceability and reliability of bridges and jeopardizes public safety. Maintenance and rehabilitation needs of bridge infrastructure are periodically monitored and assessed, typically every two years. Existing inspection techniques, such as visual inspection, are time-consuming, subjective, and often incomplete. Non-destructive testing (NDT) using Unmanned Aerial Vehicles (UAVs) have been gaining momentum for bridge monitoring in the recent years, particularly due to enhanced accessibility and cost efficiency, deterrence of traffic closure, and improved safety during inspection. The primary objective of this study is to conduct a comprehensive review of the application of UAVs in bridge condition monitoring, used in conjunction with remote sensing technologies. Remote sensing technologies such as visual imagery, infrared thermography, LiDAR, and other sensors, integrated with UAVs for data acquisition are analyzed in depth. This study compiled sixty-five journal and conference papers published in the last two decades scrutinizing NDT-based UAV systems. In addition to comparison of stand-alone and integrated NDT-UAV methods, the facilitation of bridge inspection using UAVs is thoroughly discussed in the present article in terms of ease of use, accuracy, cost-efficiency, employed data collection tools, and simulation platforms. Additionally, challenges and future perspectives of the reviewed UAV-NDT technologies are highlighted.},
DOI = {10.3390/rs13091809}
}



@Article{agriculture11050420,
AUTHOR = {Chen, Shuo and Zhang, Kefei and Zhao, Yindi and Sun, Yaqin and Ban, Wei and Chen, Yu and Zhuang, Huifu and Zhang, Xuewei and Liu, Jinxiang and Yang, Tao},
TITLE = {An Approach for Rice Bacterial Leaf Streak Disease Segmentation and Disease Severity Estimation},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {420},
URL = {https://www.mdpi.com/2077-0472/11/5/420},
ISSN = {2077-0472},
ABSTRACT = {Rice bacterial leaf streak (BLS) is a serious disease in rice leaves and can seriously affect the quality and quantity of rice growth. Automatic estimation of disease severity is a crucial requirement in agricultural production. To address this, a new method (termed BLSNet) was proposed for rice and BLS leaf lesion recognition and segmentation based on a UNet network in semantic segmentation. An attention mechanism and multi-scale extraction integration were used in BLSNet to improve the accuracy of lesion segmentation. We compared the performance of the proposed network with that of DeepLabv3+ and UNet as benchmark models used in semantic segmentation. It was found that the proposed BLSNet model demonstrated higher segmentation and class accuracy. A preliminary investigation of BLS disease severity estimation was carried out based on our BLS segmentation results, and it was found that the proposed BLSNet method has strong potential to be a reliable automatic estimator of BLS disease severity.},
DOI = {10.3390/agriculture11050420}
}



@Article{rs13091819,
AUTHOR = {Qi, Tianjun and Zhao, Yan and Meng, Xingmin and Chen, Guan and Dijkstra, Tom},
TITLE = {AI-Based Susceptibility Analysis of Shallow Landslides Induced by Heavy Rainfall in Tianshui, China},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1819},
URL = {https://www.mdpi.com/2072-4292/13/9/1819},
ISSN = {2072-4292},
ABSTRACT = {Groups of landslides induced by heavy rainfall are widely distributed on a global basis and they usually result in major losses of human life and economic damage. However, compared with landslides induced by earthquakes, inventories of landslides induced by heavy rainfall are much less common. In this study we used high-precision remote sensing images before and after continuous heavy rainfall in southern Tianshui, China, from 20 June to 25 July 2013, to produce an inventory of 14,397 shallow landslides. Based on the results of landslide inventory, we utilized machine learning and the geographic information system (GIS) to map landslide susceptibility in this area and evaluated the relative weight of various factors affecting landslide development. First, 18 variables related to geomorphic conditions, slope material, geological conditions, and human activities were selected through collinearity analysis; second, 21 selected machine learning models were trained and optimized in the Python environment to evaluate the susceptibility of landslides. The results showed that the ExtraTrees model was the most effective for landslide susceptibility assessment, with an accuracy of 0.91. This predictive ability means that our landslide susceptibility results can be used in the implementation of landslide prevention and mitigation measures in the region. Analysis of the importance of the factors showed that the contribution of slope aspect (SA) was significantly higher than that of the other factors, followed by planar curvature (PLC), distance to river (DR), distance to fault (DTF), normalized difference vehicle index (NDVI), distance to road (DTR), and other factors. We conclude that factors related to geomorphic conditions are principally responsible for controlling landslide susceptibility in the study area.},
DOI = {10.3390/rs13091819}
}



@Article{drones5020035,
AUTHOR = {Bollas, Nikolaos and Kokinou, Eleni and Polychronos, Vassilios},
TITLE = {Comparison of Sentinel-2 and UAV Multispectral Data for Use in Precision Agriculture: An Application from Northern Greece},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {35},
URL = {https://www.mdpi.com/2504-446X/5/2/35},
ISSN = {2504-446X},
ABSTRACT = {The scope of this work is to compare Sentinel-2 and unmanned aerial vehicles (UAV) imagery from northern Greece for use in precision agriculture by implementing statistical analysis and 2D visualization. Surveys took place on five dates with a difference between the sensing dates for the two techniques ranging from 1 to 4 days. Using the acquired images, we initially computed the maps of the Normalized Difference Vegetation Index (NDVI), then the values of this index for fifteen points and four polygons (areas). The UAV images were not resampled, aiming to compare both techniques based on their initial standards, as they are used by the farmers. Similarities between the two techniques are depicted on the trend of the NDVI means for both satellite and UAV techniques, considering the points and the polygons. The differences are in the a) mean NDVI values of the points and b) range of the NDVI values of the polygons probably because of the difference in the spatial resolution of the two techniques. The correlation coefficient of the NDVI values, considering both points and polygons, ranges between 83.5% and 98.26%. In conclusion, both techniques provide important information in precision agriculture depending on the spatial extent, resolution, and cost, as well as the requirements of the survey.},
DOI = {10.3390/drones5020035}
}



@Article{rs13091828,
AUTHOR = {Wei, Hongjian and Huang, Yingping and Hu, Fuzhi and Zhao, Baigan and Guo, Zhiyang and Zhang, Rui},
TITLE = {Motion Estimation Using Region-Level Segmentation and Extended Kalman Filter for Autonomous Driving},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1828},
URL = {https://www.mdpi.com/2072-4292/13/9/1828},
ISSN = {2072-4292},
ABSTRACT = {Motion estimation is crucial to predict where other traffic participants will be at a certain period of time, and accordingly plan the route of the ego-vehicle. This paper presents a novel approach to estimate the motion state by using region-level instance segmentation and extended Kalman filter (EKF). Motion estimation involves three stages of object detection, tracking and parameter estimate. We first use a region-level segmentation to accurately locate the object region for the latter two stages. The region-level segmentation combines color, temporal (optical flow), and spatial (depth) information as the basis for segmentation by using super-pixels and Conditional Random Field. The optical flow is then employed to track the feature points within the object area. In the stage of parameter estimate, we develop a relative motion model of the ego-vehicle and the object, and accordingly establish an EKF model for point tracking and parameter estimate. The EKF model integrates the ego-motion, optical flow, and disparity to generate optimized motion parameters. During tracking and parameter estimate, we apply edge point constraint and consistency constraint to eliminate outliers of tracking points so that the feature points used for tracking are ensured within the object body and the parameter estimates are refined by inner points. Experiments have been conducted on the KITTI dataset, and the results demonstrate that our method presents excellent performance and outperforms the other state-of-the-art methods either in object segmentation and parameter estimate.},
DOI = {10.3390/rs13091828}
}



@Article{app11094263,
AUTHOR = {Ni, Minna and Sun, Zhihong and Luo, Yuhan and Yi, Qi and Zhang, Yiqing and Wang, Zhongyi},
TITLE = {Evaluation Model of Parking Equipment Planning and Design Based on Object-Oriented Technology},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4263},
URL = {https://www.mdpi.com/2076-3417/11/9/4263},
ISSN = {2076-3417},
ABSTRACT = {Stereo parking equipment has become an important means to solve the problem of parking difficulties, so it is necessary to study the planning of stereo parking equipment. This paper proposes an evaluation model for parking equipment planning and design, and verifies the feasibility of the model through an example. First, obtain the surface information of the planned area through object-oriented technology, and then complete the design layout of the area that can accommodate the most parking spaces according to the plan information map of the study area. Next, calculate the number of parking spaces required for each building in the area, and the number of available parking spaces within the maximum acceptable time for each building. Finally, compare the two to design the number and location of parking equipment. This method can quickly and accurately obtain the ground plane information map of the study area, while ensuring the capacity of parking spaces to meet the needs of users, it also improves the rationality and suitability of the planning and layout of stereo parking equipment, which can effectively guide the planning and construction of urban parking equipment.},
DOI = {10.3390/app11094263}
}



@Article{s21093262,
AUTHOR = {Mahmud, Md Sultan and Zahid, Azlan and He, Long and Martin, Phillip},
TITLE = {Opportunities and Possibilities of Developing an Advanced Precision Spraying System for Tree Fruits},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3262},
URL = {https://www.mdpi.com/1424-8220/21/9/3262},
PubMedID = {34066785},
ISSN = {1424-8220},
ABSTRACT = {Reducing risk from pesticide applications has been gaining serious attention in the last few decades due to the significant damage to human health, environment, and ecosystems. Pesticide applications are an essential part of current agriculture, enhancing cultivated crop productivity and quality and preventing losses of up to 45% of the world food supply. However, inappropriate and excessive use of pesticides is a major rising concern. Precision spraying addresses these concerns by precisely and efficiently applying pesticides to the target area and substantially reducing pesticide usage while maintaining efficacy at preventing crop losses. This review provides a systematic summary of current technologies used for precision spraying in tree fruits and highlights their potential, briefly discusses factors affecting spraying parameters, and concludes with possible solutions to reduce excessive agrochemical uses. We conclude there is a critical need for appropriate sensing techniques that can accurately detect the target. In addition, air jet velocity, travel speed, wind speed and direction, droplet size, and canopy characteristics need to be considered for successful droplet deposition by the spraying system. Assessment of terrain is important when field elevation has significant variability. Control of airflow during spraying is another important parameter that needs to be considered. Incorporation of these variables in precision spraying systems will optimize spray decisions and help reduce excessive agrochemical applications.},
DOI = {10.3390/s21093262}
}



@Article{rs13091837,
AUTHOR = {Laroche-Pinel, Eve and Duthoit, Sylvie and Albughdadi, Mohanad and Costard, Anne D. and Rousseau, Jacques and Chéret, Véronique and Clenet, Harold},
TITLE = {Towards Vine Water Status Monitoring on a Large Scale Using Sentinel-2 Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1837},
URL = {https://www.mdpi.com/2072-4292/13/9/1837},
ISSN = {2072-4292},
ABSTRACT = {Wine growing needs to adapt to confront climate change. In fact, the lack of water becomes more and more important in many regions. Whereas vineyards have been located in dry areas for decades, so they need special resilient varieties and/or a sufficient water supply at key development stages in case of severe drought. With climate change and the decrease of water availability, some vineyard regions face difficulties because of unsuitable variety, wrong vine management or due to the limited water access. Decision support tools are therefore required to optimize water use or to adapt agronomic practices. This study aimed at monitoring vine water status at a large scale with Sentinel-2 images. The goal was to provide a solution that would give spatialized and temporal information throughout the season on the water status of the vines. For this purpose, thirty six plots were monitored in total over three years (2018, 2019 and 2020). Vine water status was measured with stem water potential in field measurements from pea size to ripening stage. Simultaneously Sentinel-2 images were downloaded and processed to extract band reflectance values and compute vegetation indices. In our study, we tested five supervised regression machine learning algorithms to find possible relationships between stem water potential and data acquired from Sentinel-2 images (bands reflectance values and vegetation indices). Regression model using Red, NIR, Red-Edge and SWIR bands gave promising result to predict stem water potential (R2=0.40, RMSE=0.26).},
DOI = {10.3390/rs13091837}
}



@Article{drones5020037,
AUTHOR = {Wei, Bingsheng and Barczyk, Martin},
TITLE = {Experimental Evaluation of Computer Vision and Machine Learning-Based UAV Detection and Ranging},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {37},
URL = {https://www.mdpi.com/2504-446X/5/2/37},
ISSN = {2504-446X},
ABSTRACT = {We consider the problem of vision-based detection and ranging of a target UAV using the video feed from a monocular camera onboard a pursuer UAV. Our previously published work in this area employed a cascade classifier algorithm to locate the target UAV, which was found to perform poorly in complex background scenes. We thus study the replacement of the cascade classifier algorithm with newer machine learning-based object detection algorithms. Five candidate algorithms are implemented and quantitatively tested in terms of their efficiency (measured as frames per second processing rate), accuracy (measured as the root mean squared error between ground truth and detected location), and consistency (measured as mean average precision) in a variety of flight patterns, backgrounds, and test conditions. Assigning relative weights of 20%, 40% and 40% to these three criteria, we find that when flying over a white background, the top three performers are YOLO v2 (76.73 out of 100), Faster RCNN v2 (63.65 out of 100), and Tiny YOLO (59.50 out of 100), while over a realistic background, the top three performers are Faster RCNN v2 (54.35 out of 100, SSD MobileNet v1 (51.68 out of 100) and SSD Inception v2 (50.72 out of 100), leading us to recommend Faster RCNN v2 as the recommended solution. We then provide a roadmap for further work in integrating the object detector into our vision-based UAV tracking system.},
DOI = {10.3390/drones5020037}
}



@Article{app11094292,
AUTHOR = {Moreno-Revelo, Mónica Y. and Guachi-Guachi, Lorena and Gómez-Mendoza, Juan Bernardo and Revelo-Fuelagán, Javier and Peluffo-Ordóñez, Diego H.},
TITLE = {Enhanced Convolutional-Neural-Network Architecture for Crop Classification},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4292},
URL = {https://www.mdpi.com/2076-3417/11/9/4292},
ISSN = {2076-3417},
ABSTRACT = {Automatic crop identification and monitoring is a key element in enhancing food production processes as well as diminishing the related environmental impact. Although several efficient deep learning techniques have emerged in the field of multispectral imagery analysis, the crop classification problem still needs more accurate solutions. This work introduces a competitive methodology for crop classification from multispectral satellite imagery mainly using an enhanced 2D convolutional neural network (2D-CNN) designed at a smaller-scale architecture, as well as a novel post-processing step. The proposed methodology contains four steps: image stacking, patch extraction, classification model design (based on a 2D-CNN architecture), and post-processing. First, the images are stacked to increase the number of features. Second, the input images are split into patches and fed into the 2D-CNN model. Then, the 2D-CNN model is constructed within a small-scale framework, and properly trained to recognize 10 different types of crops. Finally, a post-processing step is performed in order to reduce the classification error caused by lower-spatial-resolution images. Experiments were carried over the so-named Campo Verde database, which consists of a set of satellite images captured by Landsat and Sentinel satellites from the municipality of Campo Verde, Brazil. In contrast to the maximum accuracy values reached by remarkable works reported in the literature (amounting to an overall accuracy of about 81%, a f1 score of 75.89%, and average accuracy of 73.35%), the proposed methodology achieves a competitive overall accuracy of 81.20%, a f1 score of 75.89%, and an average accuracy of 88.72% when classifying 10 different crops, while ensuring an adequate trade-off between the number of multiply-accumulate operations (MACs) and accuracy. Furthermore, given its ability to effectively classify patches from two image sequences, this methodology may result appealing for other real-world applications, such as the classification of urban materials.},
DOI = {10.3390/app11094292}
}



@Article{app11094294,
AUTHOR = {Loss, Theresa and Bergmann, Alexander},
TITLE = {Vibration-Based Fingerprint Algorithm for Structural Health Monitoring of Wind Turbine Blades},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4294},
URL = {https://www.mdpi.com/2076-3417/11/9/4294},
ISSN = {2076-3417},
ABSTRACT = {Monitoring the structural health of wind turbine blades is essential to increase energy capture and operational safety of turbines, and therewith enhance competitiveness of wind energy. With the current trends of designing blades ever longer, detailed knowledge of the vibrational characteristics at any point along the blade is desirable. In our approach, we monitor vibrations during operation of the turbine by wirelessly measuring accelerations on the outside of the blades. We propose an algorithm to extract so-called vibration-based fingerprints from those measurements, i.e., dominant vibrations such as eigenfrequencies and narrow-band noise. These fingerprints can then be used for subsequent analysis and visualisation, e.g., for comparing fingerprints across several sensor positions and for identifying vibrations as global or local properties. In this study, data were collected by sensors on two test turbines and fingerprints were successfully extracted for vibrations with both low and high operational variability. An analysis of sensors on the same blade indicates that fingerprints deviate for positions at large radial distance or at different blade sides and, hence, an evaluation with larger datasets of sensors at different positions is promising. In addition, the results show that distributed measurements on the blades are needed to gain a detailed understanding of blade vibrations and thereby reduce loads, increase energy harvesting and improve future blade design. In doing so, our method provides a tool for analysing vibrations with relation to environmental and operational variability in a comprehensive manner.},
DOI = {10.3390/app11094294}
}



@Article{rs13091853,
AUTHOR = {Jin, Xing and Tang, Ping and Zhang, Zheng},
TITLE = {Sequence Image Datasets Construction via Deep Convolution Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1853},
URL = {https://www.mdpi.com/2072-4292/13/9/1853},
ISSN = {2072-4292},
ABSTRACT = {Remote-sensing time-series datasets are significant for global change research and a better understanding of the Earth. However, remote-sensing acquisitions often provide sparse time series due to sensor resolution limitations and environmental factors such as cloud noise for optical data. Image transformation is the method that is often used to deal with this issue. This paper considers the deep convolution networks to learn the complex mapping between sequence images, called adaptive filter generation network (AdaFG), convolution long short-term memory network (CLSTM), and cycle-consistent generative adversarial network (CyGAN) for construction of sequence image datasets. AdaFG network uses a separable 1D convolution kernel instead of 2D kernels to capture the spatial characteristics of input sequence images and then is trained end-to-end using sequence images. CLSTM network can map between different images using the state information of multiple time-series images. CyGAN network can map an image from a source domain to a target domain without additional information. Our experiments, which were performed with unmanned aerial vehicle (UAV) and Landsat-8 datasets, show that the deep convolution networks are effective to produce high-quality time-series image datasets, and the data-driven deep convolution networks can better simulate complex and diverse nonlinear data information.},
DOI = {10.3390/rs13091853}
}



@Article{agriculture11050431,
AUTHOR = {Cheng, Zhenzhen and Qi, Lijun and Cheng, Yifan},
TITLE = {Cherry Tree Crown Extraction from Natural Orchard Images with Complex Backgrounds},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {431},
URL = {https://www.mdpi.com/2077-0472/11/5/431},
ISSN = {2077-0472},
ABSTRACT = {Highly effective pesticide applications require a continual adjustment of the pesticide spray flow rate that attends to different canopy characterizations. Real-time image processing with rapid target detection and data-processing technologies is vital for precision pesticide application. However, the extant studies do not provide an efficient and reliable method of extracting individual trees with irregular tree-crown shapes and complicated backgrounds. This paper on our study proposes a Mahalanobis distance and conditional random field (CRF)-based segmentation model to extract cherry trees accurately in a natural orchard environment. This study computed Mahalanobis distance from the image’s color, brightness and location features to acquire an initial classification of the canopy and background. A CRF was then created by using the Mahalanobis distance calculations as unary potential energy and the Gaussian kernel function based on the image color and pixels distance as binary potential energy. Finally, the study completed image segmentation using mean-field approximation. The results show that the proposed method displays a higher accuracy rate than the traditional algorithms K-means and GrabCut algorithms and lower labeling and training costs than the deep learning algorithm DeepLabv3+, with 92.1%, 94.5% and 93.3% of the average P, R and F1-score, respectively. Moreover, experiments on datasets with different overlap conditions and image acquisition times, as well as in different years and seasons, show that this method performs well under complex background conditions, with an average F1-score higher than 87.7%.},
DOI = {10.3390/agriculture11050431}
}



@Article{rs13091854,
AUTHOR = {Bashir, Syed Muhammad Arsalan and Wang, Yi},
TITLE = {Small Object Detection in Remote Sensing Images with Residual Feature Aggregation-Based Super-Resolution and Object Detector Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1854},
URL = {https://www.mdpi.com/2072-4292/13/9/1854},
ISSN = {2072-4292},
ABSTRACT = {This paper deals with detecting small objects in remote sensing images from satellites or any aerial vehicle by utilizing the concept of image super-resolution for image resolution enhancement using a deep-learning-based detection method. This paper provides a rationale for image super-resolution for small objects by improving the current super-resolution (SR) framework by incorporating a cyclic generative adversarial network (GAN) and residual feature aggregation (RFA) to improve detection performance. The novelty of the method is threefold: first, a framework is proposed, independent of the final object detector used in research, i.e., YOLOv3 could be replaced with Faster R-CNN or any object detector to perform object detection; second, a residual feature aggregation network was used in the generator, which significantly improved the detection performance as the RFA network detected complex features; and third, the whole network was transformed into a cyclic GAN. The image super-resolution cyclic GAN with RFA and YOLO as the detection network is termed as SRCGAN-RFA-YOLO, which is compared with the detection accuracies of other methods. Rigorous experiments on both satellite images and aerial images (ISPRS Potsdam, VAID, and Draper Satellite Image Chronology datasets) were performed, and the results showed that the detection performance increased by using super-resolution methods for spatial resolution enhancement; for an IoU of 0.10, AP of 0.7867 was achieved for a scale factor of 16.},
DOI = {10.3390/rs13091854}
}



@Article{app11104332,
AUTHOR = {Park, Gun and Lee, Jae Hyuk and Yoon, Hyungchul},
TITLE = {Semantic Structure from Motion for Railroad Bridges Using Deep Learning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4332},
URL = {https://www.mdpi.com/2076-3417/11/10/4332},
ISSN = {2076-3417},
ABSTRACT = {Current maintenance practices consume significant time, cost, and manpower. Thus, a new technique for maintenance is required. Construction information technologies, including building information modeling (BIM), have recently been applied to the field to carry out systematic and productive planning, design, construction, and maintenance. Although BIM is increasingly being applied to new structures, its application to existing structures has been limited. To apply BIM to an existing structure, a three-dimensional (3D) model of the structure that accurately represents the as-is status should be constructed and each structural component should be specified manually. This study proposes a method that constructs a 3D model and specifies the structural component automatically using photographic data with a camera installed on an unmanned aerial vehicle. This procedure is referred to as semantic structure from motion because it constructs a 3D point cloud model together with semantic information. A validation test was carried out on a railroad bridge to validate the performance of the proposed system. The average precision, intersection over union, and BF scores were 80.87%, 66.66%, and 56.33%, respectively. The proposed method could improve the current scan-to-BIM procedure by generating the as-is 3D point cloud model by specifying the structural component automatically.},
DOI = {10.3390/app11104332}
}



@Article{s21103313,
AUTHOR = {Sobczak, Łukasz and Filus, Katarzyna and Domański, Adam and Domańska, Joanna},
TITLE = {LiDAR Point Cloud Generation for SLAM Algorithm Evaluation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3313},
URL = {https://www.mdpi.com/1424-8220/21/10/3313},
PubMedID = {34064712},
ISSN = {1424-8220},
ABSTRACT = {With the emerging interest in the autonomous driving level at 4 and 5 comes a necessity to provide accurate and versatile frameworks to evaluate the algorithms used in autonomous vehicles. There is a clear gap in the field of autonomous driving simulators. It covers testing and parameter tuning of a key component of autonomous driving systems, SLAM, frameworks targeting off-road and safety-critical environments. It also includes taking into consideration the non-idealistic nature of the real-life sensors, associated phenomena and measurement errors. We created a LiDAR simulator that delivers accurate 3D point clouds in real time. The point clouds are generated based on the sensor placement and the LiDAR type that can be set using configurable parameters. We evaluate our solution based on comparison of the results using an actual device, Velodyne VLP-16, on real-life tracks and the corresponding simulations. We measure the error values obtained using Google Cartographer SLAM algorithm and the distance between the simulated and real point clouds to verify their accuracy. The results show that our simulation (which incorporates measurement errors and the rolling shutter effect) produces data that can successfully imitate the real-life point clouds. Due to dedicated mechanisms, it is compatible with the Robotic Operating System (ROS) and can be used interchangeably with data from actual sensors, which enables easy testing, SLAM algorithm parameter tuning and deployment.},
DOI = {10.3390/s21103313}
}



@Article{rs13101868,
AUTHOR = {Deur, Martina and Gašparović, Mateo and Balenović, Ivan},
TITLE = {An Evaluation of Pixel- and Object-Based Tree Species Classification in Mixed Deciduous Forests Using Pansharpened Very High Spatial Resolution Satellite Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1868},
URL = {https://www.mdpi.com/2072-4292/13/10/1868},
ISSN = {2072-4292},
ABSTRACT = {Quality tree species information gathering is the basis for making proper decisions in forest management. By applying new technologies and remote sensing methods, very high resolution (VHR) satellite imagery can give sufficient spatial detail to achieve accurate species-level classification. In this study, the influence of pansharpening of the WorldView-3 (WV-3) satellite imagery on classification results of three main tree species (Quercus robur L., Carpinus betulus L., and Alnus glutinosa (L.) Geartn.) has been evaluated. In order to increase tree species classification accuracy, three different pansharpening algorithms (Bayes, RCS, and LMVM) have been conducted. The LMVM algorithm proved the most effective pansharpening technique. The pixel- and object-based classification were applied to three pansharpened imageries using a random forest (RF) algorithm. The results showed a very high overall accuracy (OA) for LMVM pansharpened imagery: 92% and 96% for tree species classification based on pixel- and object-based approach, respectively. As expected, the object-based exceeded the pixel-based approach (OA increased by 4%). The influence of fusion on classification results was analyzed as well. Overall classification accuracy was improved by the spatial resolution of pansharpened images (OA increased by 7% for pixel-based approach). Also, regardless of pixel- or object-based classification approaches, the influence of the use of pansharpening is highly beneficial to classifying complex, natural, and mixed deciduous forest areas.},
DOI = {10.3390/rs13101868}
}



@Article{rs13101869,
AUTHOR = {Mattivi, Pietro and Pappalardo, Salvatore Eugenio and Nikolić, Nebojša and Mandolesi, Luca and Persichetti, Antonio and De Marchi, Massimo and Masin, Roberta},
TITLE = {Can Commercial Low-Cost Drones and Open-Source GIS Technologies Be Suitable for Semi-Automatic Weed Mapping for Smart Farming? A Case Study in NE Italy},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1869},
URL = {https://www.mdpi.com/2072-4292/13/10/1869},
ISSN = {2072-4292},
ABSTRACT = {Weed management is a crucial issue in agriculture, resulting in environmental in-field and off-field impacts. Within Agriculture 4.0, adoption of UASs combined with spatially explicit approaches may drastically reduce doses of herbicides, increasing sustainability in weed management. However, Agriculture 4.0 technologies are barely adopted in small-medium size farms. Recently, small and low-cost UASs, together with open-source software packages, may represent a low-cost spatially explicit system to map weed distribution in crop fields. The general aim is to map weed distribution by a low-cost UASs and a replicable workflow, completely based on open GIS software and algorithms: OpenDroneMap, QGIS, SAGA and OpenCV classification algorithms. Specific objectives are: (i) testing a low-cost UAS for weed mapping; (ii) assessing open-source packages for semi-automatic weed classification; (iii) performing a sustainable management scenario by prescription maps. Results showed high performances along the whole process: in orthomosaic generation at very high spatial resolution (0.01 m/pixel), in testing weed detection (Matthews Correlation Coefficient: 0.67–0.74), and in the production of prescription maps, reducing herbicide treatment to only 3.47% of the entire field. This study reveals the feasibility of low-cost UASs combined with open-source software, enabling a spatially explicit approach for weed management in small-medium size farmlands.},
DOI = {10.3390/rs13101869}
}



@Article{s21103317,
AUTHOR = {Wu, Xiaotian and Li, Jiongcheng and Zhou, Guanxing and Lü, Bo and Li, Qingqing and Yang, Hang},
TITLE = {RRG-GAN Restoring Network for Simple Lens Imaging System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3317},
URL = {https://www.mdpi.com/1424-8220/21/10/3317},
PubMedID = {34064779},
ISSN = {1424-8220},
ABSTRACT = {The simple lens computational imaging method provides an alternative way to achieve high-quality photography. It simplifies the design of the optical-front-end to a single-convex-lens and delivers the correction of optical aberration to a dedicated computational restoring algorithm. Traditional single-convex-lens image restoration is based on optimization theory, which has some shortcomings in efficiency and efficacy. In this paper, we propose a novel Recursive Residual Groups network under Generative Adversarial Network framework (RRG-GAN) to generate a clear image from the aberrations-degraded blurry image. The RRG-GAN network includes dual attention module, selective kernel network module, and residual resizing module to make it more suitable for the non-uniform deblurring task. To validate the evaluation algorithm, we collect sharp/aberration-degraded datasets by CODE V simulation. To test the practical application performance, we built a display-capture lab setup and reconstruct a manual registering dataset. Relevant experimental comparisons and actual tests verify the effectiveness of our proposed method.},
DOI = {10.3390/s21103317}
}



@Article{s21103374,
AUTHOR = {Liu, Hansen and Fan, Kuangang and Ouyang, Qinghua and Li, Na},
TITLE = {Real-Time Small Drones Detection Based on Pruned YOLOv4},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3374},
URL = {https://www.mdpi.com/1424-8220/21/10/3374},
PubMedID = {34066267},
ISSN = {1424-8220},
ABSTRACT = {To address the threat of drones intruding into high-security areas, the real-time detection of drones is urgently required to protect these areas. There are two main difficulties in real-time detection of drones. One of them is that the drones move quickly, which leads to requiring faster detectors. Another problem is that small drones are difficult to detect. In this paper, firstly, we achieve high detection accuracy by evaluating three state-of-the-art object detection methods: RetinaNet, FCOS, YOLOv3 and YOLOv4. Then, to address the first problem, we prune the convolutional channel and shortcut layer of YOLOv4 to develop thinner and shallower models. Furthermore, to improve the accuracy of small drone detection, we implement a special augmentation for small object detection by copying and pasting small drones. Experimental results verify that compared to YOLOv4, our pruned-YOLOv4 model, with 0.8 channel prune rate and 24 layers prune, achieves 90.5% mAP and its processing speed is increased by 60.4%. Additionally, after small object augmentation, the precision and recall of the pruned-YOLOv4 almost increases by 22.8% and 12.7%, respectively. Experiment results verify that our pruned-YOLOv4 is an effective and accurate approach for drone detection.},
DOI = {10.3390/s21103374}
}



@Article{s21103394,
AUTHOR = {Xie, Xiuchuan and Yang, Tao and Ning, Yajia and Zhang, Fangbing and Zhang, Yanning},
TITLE = {A Monocular Visual Odometry Method Based on Virtual-Real Hybrid Map in Low-Texture Outdoor Environment},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3394},
URL = {https://www.mdpi.com/1424-8220/21/10/3394},
PubMedID = {34068098},
ISSN = {1424-8220},
ABSTRACT = {With the extensive application of robots, such as unmanned aerial vehicle (UAV) in exploring unknown environments, visual odometry (VO) algorithms have played an increasingly important role. The environments are diverse, not always textured, or low-textured with insufficient features, making them challenging for mainstream VO. However, for low-texture environment, due to the structural characteristics of man-made scene, the lines are usually abundant. In this paper, we propose a virtual-real hybrid map based monocular visual odometry algorithm. The core idea is that we reprocess line segment features to generate the virtual intersection matching points, which can be used to build the virtual map. Introducing virtual map can improve the stability of the visual odometry algorithm in low-texture environment. Specifically, we first combine unparallel matched line segments to generate virtual intersection matching points, then, based on the virtual intersection matching points, we triangulate to get a virtual map, combined with the real map built upon the ordinary point features to form a virtual-real hybrid 3D map. Finally, using the hybrid map, the continuous camera pose estimation can be solved. Extensive experimental results have demonstrated the robustness and effectiveness of the proposed method in various low-texture scenes.},
DOI = {10.3390/s21103394}
}



@Article{s21103389,
AUTHOR = {Quan, Longzhe and Wu, Bing and Mao, Shouren and Yang, Chunjie and Li, Hengda},
TITLE = {An Instance Segmentation-Based Method to Obtain the Leaf Age and Plant Centre of Weeds in Complex Field Environments},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3389},
URL = {https://www.mdpi.com/1424-8220/21/10/3389},
PubMedID = {34068108},
ISSN = {1424-8220},
ABSTRACT = {Leaf age and plant centre are important phenotypic information of weeds, and accurate identification of them plays an important role in understanding the morphological structure of weeds, guiding precise targeted spraying and reducing the use of herbicides. In this work, a weed segmentation method based on BlendMask is proposed to obtain the phenotypic information of weeds under complex field conditions. This study collected images from different angles (front, side, and top views) of three kinds of weeds (Solanum nigrum, barnyard grass (Echinochloa crus-galli), and Abutilon theophrasti Medicus) in a maize field. Two datasets (with and without data enhancement) and two backbone networks (ResNet50 and ResNet101) were replaced to improve model performance. Finally, seven evaluation indicators are used to evaluate the segmentation results of the model under different angles. The results indicated that data enhancement and ResNet101 as the backbone network could enhance the model performance. The F1 value of the plant centre is 0.9330, and the recognition accuracy of leaf age can reach 0.957. The mIOU value of the top view is 0.642. Therefore, deep learning methods can effectively identify weed leaf age and plant centre, which is of great significance for variable spraying.},
DOI = {10.3390/s21103389}
}



@Article{s21103407,
AUTHOR = {He, Wei and Yang, Dehang and Peng, Haoqi and Liang, Songhong and Lin, Yingcheng},
TITLE = {An Efficient Ensemble Binarized Deep Neural Network on Chip with Perception-Control Integrated},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3407},
URL = {https://www.mdpi.com/1424-8220/21/10/3407},
PubMedID = {34068351},
ISSN = {1424-8220},
ABSTRACT = {Lightweight UAVs equipped with deep learning models have become a trend, which can be deployed for automatic navigation in a wide range of civilian and military missions. However, real-time applications usually need to process a large amount of image data, which leads to a very large computational complexity and storage consumption, and restricts its deployment on resource-constrained embedded edge devices. To reduce the computing requirements and storage occupancy of the neural network model, we proposed the ensemble binarized DroNet (EBDN) model, which implemented the reconstructed DroNet with the binarized and ensemble learning method, so that the model size of DroNet was effectively compressed, and ensemble learning method was used to overcome the defect of the poor performance of the low-precision network. Compared to the original DroNet, EBDN saves more than 7 times of memory footprint with similar model accuracy. Meanwhile, we also proposed a novel and high-efficiency hardware architecture to realize the EBDN on the chip (EBDNoC) system, which perfectly realizes the mapping of an algorithm model to hardware architecture. Compared to other solutions, the proposed architecture achieves about 10.21 GOP/s/kLUTs resource efficiency and 208.1 GOP/s/W energy efficiency, while also providing a good trade-off between model performance and resource utilization.},
DOI = {10.3390/s21103407}
}



@Article{app11104493,
AUTHOR = {Jo, Yongwon and Lee, Soobin and Lee, Youngjae and Kahng, Hyungu and Park, Seonghun and Bae, Seounghun and Kim, Minkwan and Han, Sungwon and Kim, Seoungbum},
TITLE = {Semantic Segmentation of Cabbage in the South Korea Highlands with Images by Unmanned Aerial Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4493},
URL = {https://www.mdpi.com/2076-3417/11/10/4493},
ISSN = {2076-3417},
ABSTRACT = {Identifying agricultural fields that grow cabbage in the highlands of South Korea is critical for accurate crop yield estimation. Only grown for a limited time during the summer, highland cabbage accounts for a significant proportion of South Korea’s annual cabbage production. Thus, it has a profound effect on the formation of cabbage prices. Traditionally, labor-extensive and time-consuming field surveys are manually carried out to derive agricultural field maps of the highlands. Recently, high-resolution overhead images of the highlands have become readily available with the rapid development of unmanned aerial vehicles (UAV) and remote sensing technology. In addition, deep learning-based semantic segmentation models have quickly advanced by recent improvements in algorithms and computational resources. In this study, we propose a semantic segmentation framework based on state-of-the-art deep learning techniques to automate the process of identifying cabbage cultivation fields. We operated UAVs and collected 2010 multispectral images under different spatiotemporal conditions to measure how well semantic segmentation models generalize. Next, we manually labeled these images at a pixel-level to obtain ground truth labels for training. Our results demonstrate that our framework performs well in detecting cabbage fields not only in areas included in the training data but also in unseen areas not included in the training data. Moreover, we analyzed the effects of infrared wavelengths on the performance of identifying cabbage fields. Based on the results of our framework, we expect agricultural officials to reduce time and manpower when identifying information about highlands cabbage fields by replacing field surveys.},
DOI = {10.3390/app11104493}
}



@Article{agriculture11050451,
AUTHOR = {Cui, Hongwei and Zhang, Qiang and Zhang, Jinsong and Wu, Zidan and Wu, Wenfu},
TITLE = {Classification of Grain Storage Inventory Modes Based on Temperature Contour Map of Grain Bulk Using Back Propagation Neural Network},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {451},
URL = {https://www.mdpi.com/2077-0472/11/5/451},
ISSN = {2077-0472},
ABSTRACT = {Inventory modes classification can reduce the workload of grain depot management and it is time-saving, not labor-intensive. This paper proposed a method of using a temperature contour map converted from digital temperature data to classify stored grain inventory modes in a large bulk grain warehouse, which mainly included detection of inventory changes and routine operations performed (aeration). The back propagation (BP) neural network was used in this method to identify and classify grain storage inventory modes based on the temperature contour map for helping grain depot management work. The method extracted and combined color coherence vector (CCV), texture feature vector (TFV) and smoothness feature vector (SFV) of temperature contour maps as the input vector of the BP neural network, and used inventory modes as the output vector. The experimental results indicated that the accuracy of the BP neural network with vector (CCV and TFV and SFV) as the input vector was about 93.9%, and its training time and prediction time were 320 and 0.12 s, respectively.},
DOI = {10.3390/agriculture11050451}
}



@Article{su13105548,
AUTHOR = {Awad, Mohamad M. and Lauteri, Marco},
TITLE = {Self-Organizing Deep Learning (SO-UNet)—A Novel Framework to Classify Urban and Peri-Urban Forests},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {5548},
URL = {https://www.mdpi.com/2071-1050/13/10/5548},
ISSN = {2071-1050},
ABSTRACT = {Forest-type classification is a very complex and difficult subject. The complexity increases with urban and peri-urban forests because of the variety of features that exist in remote sensing images. The success of forest management that includes forest preservation depends strongly on the accuracy of forest-type classification. Several classification methods are used to map urban and peri-urban forests and to identify healthy and non-healthy ones. Some of these methods have shown success in the classification of forests where others failed. The successful methods used specific remote sensing data technology, such as hyper-spectral and very high spatial resolution (VHR) images. However, both VHR and hyper-spectral sensors are very expensive, and hyper-spectral sensors are not widely available on satellite platforms, unlike multi-spectral sensors. Moreover, aerial images are limited in use, very expensive, and hard to arrange and manage. To solve the aforementioned problems, an advanced method, self-organizing–deep learning (SO-UNet), was created to classify forests in the urban and peri-urban environment using multi-spectral, multi-temporal, and medium spatial resolution Sentinel-2 images. SO-UNet is a combination of two different machine learning technologies: artificial neural network unsupervised self-organizing maps and deep learning UNet. Many experiments have been conducted, and the results showed that SO-UNet overwhelms UNet significantly. The experiments encompassed different settings for the parameters that control the algorithms.},
DOI = {10.3390/su13105548}
}



@Article{ijgi10050338,
AUTHOR = {Nguyen, Dinh Dung and Rohacs, Jozsef and Rohacs, Daniel},
TITLE = {Autonomous Flight Trajectory Control System for Drones in Smart City Traffic Management},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {338},
URL = {https://www.mdpi.com/2220-9964/10/5/338},
ISSN = {2220-9964},
ABSTRACT = {With the exponential growth of numerous drone operations ranging from infrastructure monitoring to even package delivery services, the integration of UAS in the smart city transportation systems is an actual task that requires radically new, sustainable (safe, secure, with minimum environmental impact and life cycle cost) solutions. The primary objective of this proposed option is the definition of routes as desired and commanded trajectories and their autonomous execution. The airspace structure and fixed routes are given in the global GPS reference system with supporting GIS mapping. The concept application requires a series of further studies and solutions as drone trajectory (or corridor) following by an autonomous trajectory tracking control system, coupled with autonomous conflict detection, resolution, safe drone following, and formation flight options. The second part of the paper introduces such possible models and shows some results of their verification tests. Drones will be connected with the agency, designed trajectories to support them with factual information on trajectories and corridors. While the agency will use trajectory elements to design fixed or desired trajectories, drones may use the conventional GPS, infrared, acoustic, and visual sensors for positioning and advanced navigation. The accuracy can be improved by unique markers integrated into the infrastructure.},
DOI = {10.3390/ijgi10050338}
}



@Article{robotics10020071,
AUTHOR = {Lucas, Nathan and Pandya, Abhilash},
TITLE = {Multirobot Confidence and Behavior Modeling: An Evaluation of Semiautonomous Task Performance and Efficiency},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {71},
URL = {https://www.mdpi.com/2218-6581/10/2/71},
ISSN = {2218-6581},
ABSTRACT = {There is considerable interest in multirobot systems capable of performing spatially distributed, hazardous, and complex tasks as a team leveraging the unique abilities of humans and automated machines working alongside each other. The limitations of human perception and cognition affect operators’ ability to integrate information from multiple mobile robots, switch between their spatial frames of reference, and divide attention among many sensory inputs and command outputs. Automation is necessary to help the operator manage increasing demands as the number of robots (and humans) scales up. However, more automation does not necessarily equate to better performance. A generalized robot confidence model was developed, which transforms key operator attention indicators to a robot confidence value for each robot to enable the robots’ adaptive behaviors. This model was implemented in a multirobot test platform with the operator commanding robot trajectories using a computer mouse and an eye tracker providing gaze data used to estimate dynamic operator attention. The human-attention-based robot confidence model dynamically adapted the behavior of individual robots in response to operator attention. The model was successfully evaluated to reveal evidence linking average robot confidence to multirobot search task performance and efficiency. The contributions of this work provide essential steps toward effective human operation of multiple unmanned vehicles to perform spatially distributed and hazardous tasks in complex environments for space exploration, defense, homeland security, search and rescue, and other real-world applications.},
DOI = {10.3390/robotics10020071}
}



@Article{rs13101956,
AUTHOR = {Cong, Jingyu and Wang, Xianpeng and Lan, Xiang and Huang, Mengxing and Wan, Liangtian},
TITLE = {Fast Target Localization Method for FMCW MIMO Radar via VDSR Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1956},
URL = {https://www.mdpi.com/2072-4292/13/10/1956},
ISSN = {2072-4292},
ABSTRACT = {The traditional frequency-modulated continuous wave (FMCW) multiple-input multiple-output (MIMO) radar two-dimensional (2D) super-resolution (SR) estimation algorithm for target localization has high computational complexity, which runs counter to the increasing demand for real-time radar imaging. In this paper, a fast joint direction-of-arrival (DOA) and range estimation framework for target localization is proposed; it utilizes a very deep super-resolution (VDSR) neural network (NN) framework to accelerate the imaging process while ensuring estimation accuracy. Firstly, we propose a fast low-resolution imaging algorithm based on the Nystrom method. The approximate signal subspace matrix is obtained from partial data, and low-resolution imaging is performed on a low-density grid. Then, the bicubic interpolation algorithm is used to expand the low-resolution image to the desired dimensions. Next, the deep SR network is used to obtain the high-resolution image, and the final joint DOA and range estimation is achieved based on the reconstructed image. Simulations and experiments were carried out to validate the computational efficiency and effectiveness of the proposed framework.},
DOI = {10.3390/rs13101956}
}



@Article{smartcities4020040,
AUTHOR = {Englund, Cristofer and Aksoy, Eren Erdal and Alonso-Fernandez, Fernando and Cooney, Martin Daniel and Pashami, Sepideh and Åstrand, Björn},
TITLE = {AI Perspectives in Smart Cities and Communities to Enable Road Vehicle Automation and Smart Traffic Control},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {2},
PAGES = {783--802},
URL = {https://www.mdpi.com/2624-6511/4/2/40},
ISSN = {2624-6511},
ABSTRACT = {Smart cities and communities (SCC) constitute a new paradigm in urban development. SCC ideate a data-centered society aimed at improving efficiency by automating and optimizing activities and utilities. Information and communication technology along with Internet of Things enables data collection and with the help of artificial intelligence (AI) situation awareness can be obtained to feed the SCC actors with enriched knowledge. This paper describes AI perspectives in SCC and gives an overview of AI-based technologies used in traffic to enable road vehicle automation and smart traffic control. Perception, smart traffic control and driver modeling are described along with open research challenges and standardization to help introduce advanced driver assistance systems and automated vehicle functionality in traffic. To fully realize the potential of SCC, to create a holistic view on a city level, availability of data from different stakeholders is necessary. Further, though AI technologies provide accurate predictions and classifications, there is an ambiguity regarding the correctness of their outputs. This can make it difficult for the human operator to trust the system. Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model. Another challenge related to trust is explainability: models can have difficulty explaining how they came to certain conclusions, so it is difficult for humans to trust them.},
DOI = {10.3390/smartcities4020040}
}



@Article{app11104618,
AUTHOR = {Wang, Xun and Cai, Libing and Kong, Longxing and Wang, Binfeng and Huang, Shaohua and Lin, Chengdi},
TITLE = {Path Following and Obstacle Avoidance for Unmanned Aerial Vehicles Using a Virtual-Force-Based Guidance Law},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4618},
URL = {https://www.mdpi.com/2076-3417/11/10/4618},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a virtual-force-based guidance law (VFGL) for path following and obstacle avoidance in unmanned aerial vehicles. First, a virtual spring force and a virtual drag force are designed for straight-line following; then, the dynamic of the cross-track-error is equivalent to a spring mass system, which is easy to tune to acquire stability and non-overshoot convergence. Secondly, an additional virtual centripetal force is designed to counteract the influence of the curvature of the planned path so that the guidance law can accurately track a curve with a time-varying curvature. Thirdly, an extra virtual repulsive force is designed directly according to the sensor inputs; the virtual repulsive force pushes the vehicle away to move around obstacles. The use of artificial physics means the guidance law is founded on solid physical theory and is computationally simple. The physical meanings of the parameters are definite, and the VFGL has a large parameter adaptation. These make the guidance law easy to tune in application. Both the numerical and hardware-in-the-loop simulation results demonstrated the effectiveness of the proposed guidance law for path following and obstacle avoidance in unmanned aerial vehicles.},
DOI = {10.3390/app11104618}
}



@Article{rs13101975,
AUTHOR = {Wang, Lin and Zhou, Yuzhen and Hu, Qiao and Tang, Zhenghong and Ge, Yufeng and Smith, Adam and Awada, Tala and Shi, Yeyin},
TITLE = {Early Detection of Encroaching Woody Juniperus virginiana and Its Classification in Multi-Species Forest Using UAS Imagery and Semantic Segmentation Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1975},
URL = {https://www.mdpi.com/2072-4292/13/10/1975},
ISSN = {2072-4292},
ABSTRACT = {Woody plant encroachment into grasslands ecosystems causes significantly ecological destruction and economic losses. Effective and efficient management largely benefits from accurate and timely detection of encroaching species at an early development stage. Recent advances in unmanned aircraft systems (UAS) enabled easier access to ultra-high spatial resolution images at a centimeter level, together with the latest machine learning based image segmentation algorithms, making it possible to detect small-sized individuals of target species at early development stage and identify them when mixed with other species. However, few studies have investigated the optimal practical spatial resolution of early encroaching species detection. Hence, we investigated the performance of four popular semantic segmentation algorithms (decision tree, DT; random forest, RF; AlexNet; and ResNet) on a multi-species forest classification case with UAS-collected RGB images in original and down-sampled coarser spatial resolutions. The objective of this study was to explore the optimal segmentation algorithm and spatial resolution for eastern redcedar (Juniperus virginiana, ERC) early detection and its classification within a multi-species forest context. To be specific, firstly, we implemented and compared the performance of the four semantic segmentation algorithms with images in the original spatial resolution (0.694 cm). The highest overall accuracy was 0.918 achieved by ResNet with a mean interaction over union at 85.0%. Secondly, we evaluated the performance of ResNet algorithm with images in down-sampled spatial resolutions (1 cm to 5 cm with 0.5 cm interval). When applied on the down-sampled images, ERC segmentation performance decreased with decreasing spatial resolution, especially for those images coarser than 3 cm spatial resolution. The UAS together with the state-of-the-art semantic segmentation algorithms provides a promising tool for early-stage detection and localization of ERC and the development of effective management strategies for mixed-species forest management.},
DOI = {10.3390/rs13101975}
}



@Article{s21103533,
AUTHOR = {Ashraf, Imran and Din, Sadia and Hur, Soojung and Kim, Gunzung and Park, Yongwan},
TITLE = {Empirical Overview of Benchmark Datasets for Geomagnetic Field-Based Indoor Positioning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3533},
URL = {https://www.mdpi.com/1424-8220/21/10/3533},
PubMedID = {34069507},
ISSN = {1424-8220},
ABSTRACT = {Indoor positioning and localization have been regarded as some of the most widely researched areas during the last decade. The wide proliferation of smartphones and the availability of fast-speed internet have initiated several location-based services. Concerning the importance of precise location information, many sensors are embedded into modern smartphones. Besides Wi-Fi positioning, a rich variety of technologies have been introduced or adopted for indoor positioning such as ultrawideband, infrared, radio frequency identification, Bluetooth beacons, pedestrian dead reckoning, and magnetic field, etc. However, special emphasis is put on infrastructureless approaches like Wi-Fi and magnetic field-based positioning, as they do not require additional infrastructure. Magnetic field positioning is an attractive solution for indoors; yet lack of public benchmarks and selection of suitable benchmarks are among the big challenges. While several benchmarks have been introduced over time, the selection criteria of a benchmark are not properly defined, which leads to positioning results that lack generalization. This study aims at analyzing various public benchmarks for magnetic field positioning and highlights their pros and cons for evaluation positioning algorithms. The concept of DUST (device, user, space, time) and DOWTS (dynamicity, orientation, walk, trajectory, and sensor fusion) is introduced which divides the characteristics of the magnetic field dataset into basic and advanced groups and discusses the publicly available datasets accordingly.},
DOI = {10.3390/s21103533}
}



@Article{jimaging7050090,
AUTHOR = {Hamdi, Slim and Bouindour, Samir and Snoussi, Hichem and Wang, Tian and Abid, Mohamed},
TITLE = {End-to-End Deep One-Class Learning for Anomaly Detection in UAV Video Stream},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {90},
URL = {https://www.mdpi.com/2313-433X/7/5/90},
ISSN = {2313-433X},
ABSTRACT = {In recent years, the use of drones for surveillance tasks has been on the rise worldwide. However, in the context of anomaly detection, only normal events are available for the learning process. Therefore, the implementation of a generative learning method in an unsupervised mode to solve this problem becomes fundamental. In this context, we propose a new end-to-end architecture capable of generating optical flow images from original UAV images and extracting compact spatio-temporal characteristics for anomaly detection purposes. It is designed with a custom loss function as a sum of three terms, the reconstruction loss (Rl), the generation loss (Gl) and the compactness loss (Cl) to ensure an efficient classification of the “deep-one” class. In addition, we propose to minimize the effect of UAV motion in video processing by applying background subtraction on optical flow images. We tested our method on very complex datasets called the mini-drone video dataset, and obtained results surpassing existing techniques’ performances with an AUC of 85.3.},
DOI = {10.3390/jimaging7050090}
}



@Article{app11104647,
AUTHOR = {Liu, Chuanyang and Wu, Yiquan and Liu, Jingjing and Sun, Zuo and Xu, Huajie},
TITLE = {Insulator Faults Detection in Aerial Images from High-Voltage Transmission Lines Based on Deep Learning Model},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4647},
URL = {https://www.mdpi.com/2076-3417/11/10/4647},
ISSN = {2076-3417},
ABSTRACT = {Insulator fault detection is one of the essential tasks for high-voltage transmission lines’ intelligent inspection. In this study, a modified model based on You Only Look Once (YOLO) is proposed for detecting insulator faults in aerial images with a complex background. Firstly, aerial images with one fault or multiple faults are collected in diverse scenes, and then a novel dataset is established. Secondly, to increase feature reuse and propagation in the low-resolution feature layers, a Cross Stage Partial Dense YOLO (CSPD-YOLO) model is proposed based on YOLO-v3 and the Cross Stage Partial Network. The feature pyramid network and improved loss function are adopted to the CSPD-YOLO model, improving the accuracy of insulator fault detection. Finally, the proposed CSPD-YOLO model and compared models are trained and tested on the established dataset. The average precision of CSPD-YOLO model is 4.9% and 1.8% higher than that of YOLO-v3 and YOLO-v4, and the running time of CSPD-YOLO (0.011 s) model is slightly longer than that of YOLO-v3 (0.01 s) and YOLO-v4 (0.01 s). Compared with the excellent object detection models YOLO-v3 and YOLO-v4, the experimental results and analysis demonstrate that the proposed CSPD-YOLO model performs better in insulator fault detection from high-voltage transmission lines with a complex background.},
DOI = {10.3390/app11104647}
}



@Article{genes12050783,
AUTHOR = {Cortés, Andrés J. and López-Hernández, Felipe},
TITLE = {Harnessing Crop Wild Diversity for Climate Change Adaptation},
JOURNAL = {Genes},
VOLUME = {12},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {783},
URL = {https://www.mdpi.com/2073-4425/12/5/783},
PubMedID = {34065368},
ISSN = {2073-4425},
ABSTRACT = {Warming and drought are reducing global crop production with a potential to substantially worsen global malnutrition. As with the green revolution in the last century, plant genetics may offer concrete opportunities to increase yield and crop adaptability. However, the rate at which the threat is happening requires powering new strategies in order to meet the global food demand. In this review, we highlight major recent ‘big data’ developments from both empirical and theoretical genomics that may speed up the identification, conservation, and breeding of exotic and elite crop varieties with the potential to feed humans. We first emphasize the major bottlenecks to capture and utilize novel sources of variation in abiotic stress (i.e., heat and drought) tolerance. We argue that adaptation of crop wild relatives to dry environments could be informative on how plant phenotypes may react to a drier climate because natural selection has already tested more options than humans ever will. Because isolated pockets of cryptic diversity may still persist in remote semi-arid regions, we encourage new habitat-based population-guided collections for genebanks. We continue discussing how to systematically study abiotic stress tolerance in these crop collections of wild and landraces using geo-referencing and extensive environmental data. By uncovering the genes that underlie the tolerance adaptive trait, natural variation has the potential to be introgressed into elite cultivars. However, unlocking adaptive genetic variation hidden in related wild species and early landraces remains a major challenge for complex traits that, as abiotic stress tolerance, are polygenic (i.e., regulated by many low-effect genes). Therefore, we finish prospecting modern analytical approaches that will serve to overcome this issue. Concretely, genomic prediction, machine learning, and multi-trait gene editing, all offer innovative alternatives to speed up more accurate pre- and breeding efforts toward the increase in crop adaptability and yield, while matching future global food demands in the face of increased heat and drought. In order for these ‘big data’ approaches to succeed, we advocate for a trans-disciplinary approach with open-source data and long-term funding. The recent developments and perspectives discussed throughout this review ultimately aim to contribute to increased crop adaptability and yield in the face of heat waves and drought events.},
DOI = {10.3390/genes12050783}
}



@Article{app11104706,
AUTHOR = {Tullu, Abera and Endale, Bedada and Wondosen, Assefinew and Hwang, Ho-Yon},
TITLE = {Machine Learning Approach to Real-Time 3D Path Planning for Autonomous Navigation of Unmanned Aerial Vehicle},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4706},
URL = {https://www.mdpi.com/2076-3417/11/10/4706},
ISSN = {2076-3417},
ABSTRACT = {The need for civilian use of Unmanned Aerial Vehicles (UAVs) has drastically increased in recent years. Their potential applications for civilian use include door-to-door package delivery, law enforcement, first aid, and emergency services in urban areas, which put the UAVs into obstacle collision risk. Therefore, UAVs are required to be equipped with sensors so as to acquire Artificial Intelligence (AI) to avoid potential risks during mission execution. The AI comes with intensive training of an on-board machine that is responsible to autonomously navigate the UAV. The training enables the UAV to develop humanoid perception of the environment it is to be navigating in. During the mission, this perception detects and localizes objects in the environment. It is based on this AI that this work proposes a real-time three-dimensional (3D) path planner that maneuvers the UAV towards destination through obstacle-free path. The proposed path planner has a heuristic sense of A⋆ algorithm, but requires no frontier nodes to be stored in a memory unlike A⋆. The planner relies on relative locations of detected objects (obstacles) and determines collision-free paths. This path planner is light-weight and hence a fast guidance method for real-time purposes. Its performance efficiency is proved through rigorous Software-In-The-Loop (SITL) simulations in constrained-environment and preliminary real flight tests.},
DOI = {10.3390/app11104706}
}



@Article{agriengineering3020020,
AUTHOR = {Chowdhury, Muhammad E. H. and Rahman, Tawsifur and Khandakar, Amith and Ayari, Mohamed Arselene and Khan, Aftab Ullah and Khan, Muhammad Salman and Al-Emadi, Nasser and Reaz, Mamun Bin Ibne and Islam, Mohammad Tariqul and Ali, Sawal Hamid Md},
TITLE = {Automatic and Reliable Leaf Disease Detection Using Deep Learning Techniques},
JOURNAL = {AgriEngineering},
VOLUME = {3},
YEAR = {2021},
NUMBER = {2},
PAGES = {294--312},
URL = {https://www.mdpi.com/2624-7402/3/2/20},
ISSN = {2624-7402},
ABSTRACT = {Plants are a major source of food for the world population. Plant diseases contribute to production loss, which can be tackled with continuous monitoring. Manual plant disease monitoring is both laborious and error-prone. Early detection of plant diseases using computer vision and artificial intelligence (AI) can help to reduce the adverse effects of diseases and also overcome the shortcomings of continuous human monitoring. In this work, we propose the use of a deep learning architecture based on a recent convolutional neural network called EfficientNet on 18,161 plain and segmented tomato leaf images to classify tomato diseases. The performance of two segmentation models i.e., U-net and Modified U-net, for the segmentation of leaves is reported. The comparative performance of the models for binary classification (healthy and unhealthy leaves), six-class classification (healthy and various groups of diseased leaves), and ten-class classification (healthy and various types of unhealthy leaves) are also reported. The modified U-net segmentation model showed accuracy, IoU, and Dice score of 98.66%, 98.5%, and 98.73%, respectively, for the segmentation of leaf images. EfficientNet-B7 showed superior performance for the binary classification and six-class classification using segmented images with an accuracy of 99.95% and 99.12%, respectively. Finally, EfficientNet-B4 achieved an accuracy of 99.89% for ten-class classification using segmented images. It can be concluded that all the architectures performed better in classifying the diseases when trained with deeper networks on segmented images. The performance of each of the experimental studies reported in this work outperforms the existing literature.},
DOI = {10.3390/agriengineering3020020}
}



@Article{ijms22115423,
AUTHOR = {Mores, Antonia and Borrelli, Grazia Maria and Laidò, Giovanni and Petruzzino, Giuseppe and Pecchioni, Nicola and Amoroso, Luca Giuseppe Maria and Desiderio, Francesca and Mazzucotelli, Elisabetta and Mastrangelo, Anna Maria and Marone, Daniela},
TITLE = {Genomic Approaches to Identify Molecular Bases of Crop Resistance to Diseases and to Develop Future Breeding Strategies},
JOURNAL = {International Journal of Molecular Sciences},
VOLUME = {22},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5423},
URL = {https://www.mdpi.com/1422-0067/22/11/5423},
PubMedID = {34063853},
ISSN = {1422-0067},
ABSTRACT = {Plant diseases are responsible for substantial crop losses each year and affect food security and agricultural sustainability. The improvement of crop resistance to pathogens through breeding represents an environmentally sound method for managing disease and minimizing these losses. The challenge is to breed varieties with a stable and broad-spectrum resistance. Different approaches, from markers to recent genomic and ‘post-genomic era’ technologies, will be reviewed in order to contribute to a better understanding of the complexity of host–pathogen interactions and genes, including those with small phenotypic effects and mechanisms that underlie resistance. An efficient combination of these approaches is herein proposed as the basis to develop a successful breeding strategy to obtain resistant crop varieties that yield higher in increasing disease scenarios.},
DOI = {10.3390/ijms22115423}
}



@Article{rs13112029,
AUTHOR = {Kok, Zhi Hong and Shariff, Abdul Rashid Bin Mohamed and Khairunniza-Bejo, Siti and Kim, Hyeon-Tae and Ahamed, Tofael and Cheah, See Siang and Wahid, Siti Aishah Abd},
TITLE = {Plot-Based Classification of Macronutrient Levels in Oil Palm Trees with Landsat-8 Images and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2029},
URL = {https://www.mdpi.com/2072-4292/13/11/2029},
ISSN = {2072-4292},
ABSTRACT = {Oil palm crops are essential for ensuring sustainable edible oil production, in which production is highly dependent on fertilizer applications. Using Landsat-8 imageries, the feasibility of macronutrient level classification with Machine Learning (ML) was studied. Variable rates of compost and inorganic fertilizer were applied to experimental plots and the following nutrients were studied: nitrogen (N), phosphorus (P), potassium (K), magnesium (Mg) and calcium (Ca). By applying image filters, separability metrics, vegetation indices (VI) and feature selection, spectral features for each plot were acquired and used with ML models to classify macronutrient levels of palm stands from chemical foliar analysis of their 17th frond. The models were calibrated and validated with 30 repetitions, with the best mean overall accuracy reported for N and K at 79.7 ± 4.3% and 76.6 ± 4.1% respectively, while accuracies for P, Mg and Ca could not be accurately classified due to the limitations of the dataset used. The study highlighted the effectiveness of separability metrics in quantifying class separability, the importance of indices for N and K level classification, and the effects of filter and feature selection on model performance, as well as concluding RF or SVM models for excessive N and K level detection. Future improvements should focus on further model validation and the use of higher-resolution imaging.},
DOI = {10.3390/rs13112029}
}



@Article{rs13112031,
AUTHOR = {Huerta, Roberto E. and Yépez, Fabiola D. and Lozano-García, Diego F. and Guerra Cobián, Víctor H. and Ferriño Fierro, Adrián L. and de León Gómez, Héctor and Cavazos González, Ricardo A. and Vargas-Martínez, Adriana},
TITLE = {Mapping Urban Green Spaces at the Metropolitan Level Using Very High Resolution Satellite Imagery and Deep Learning Techniques for Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2031},
URL = {https://www.mdpi.com/2072-4292/13/11/2031},
ISSN = {2072-4292},
ABSTRACT = {Urban green spaces (UGSs) provide essential environmental services for the well-being of ecosystems and society. Due to the constant environmental, social, and economic transformations of cities, UGSs pose new challenges for management, particularly in fast-growing metropolitan areas. With technological advancement and the evolution of deep learning, it is possible to optimize the acquisition of UGS inventories through the detection of geometric patterns present in satellite imagery. This research evaluates two deep learning model techniques for semantic segmentation of UGS polygons with the use of different convolutional neural network encoders on the U-Net architecture and very high resolution (VHR) imagery to obtain updated information on UGS polygons at the metropolitan area level. The best model yielded a Dice coefficient of 0.57, IoU of 0.75, recall of 0.80, and kappa coefficient of 0.94 with an overall accuracy of 0.97, which reflects a reliable performance of the network in detecting patterns that make up the varied geometry of UGSs. A complete database of UGS polygons was quantified and categorized by types with location and delimited by municipality, allowing for the standardization of the information at the metropolitan level, which will be useful for comparative analysis with a homogenized and updated database. This is of particular interest to urban planners and UGS decision-makers.},
DOI = {10.3390/rs13112031}
}



@Article{en14113004,
AUTHOR = {Alibabaei, Khadijeh and Gaspar, Pedro D. and Lima, Tânia M.},
TITLE = {Crop Yield Estimation Using Deep Learning Based on Climate Big Data and Irrigation Scheduling},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3004},
URL = {https://www.mdpi.com/1996-1073/14/11/3004},
ISSN = {1996-1073},
ABSTRACT = {Deep learning has already been successfully used in the development of decision support systems in various domains. Therefore, there is an incentive to apply it in other important domains such as agriculture. Fertilizers, electricity, chemicals, human labor, and water are the components of total energy consumption in agriculture. Yield estimates are critical for food security, crop management, irrigation scheduling, and estimating labor requirements for harvesting and storage. Therefore, estimating product yield can reduce energy consumption. Two deep learning models, Long Short-Term Memory and Gated Recurrent Units, have been developed for the analysis of time-series data such as agricultural datasets. In this paper, the capabilities of these models and their extensions, called Bidirectional Long Short-Term Memory and Bidirectional Gated Recurrent Units, to predict end-of-season yields are investigated. The models use historical data, including climate data, irrigation scheduling, and soil water content, to estimate end-of-season yield. The application of this technique was tested for tomato and potato yields at a site in Portugal. The Bidirectional Long Short-Term memory outperformed the Gated Recurrent Units network, the Long Short-Term Memory, and the Bidirectional Gated Recurrent Units network on the validation dataset. The model was able to capture the nonlinear relationship between irrigation amount, climate data, and soil water content and predict yield with an MSE of 0.017 to 0.039. The performance of the Bidirectional Long Short-Term Memory in the test was compared with the most commonly used deep learning method, the Convolutional Neural Network, and machine learning methods including a Multi-Layer Perceptrons model and Random Forest Regression. The Bidirectional Long Short-Term Memory outperformed the other models with an R2 score between 0.97 and 0.99. The results show that analyzing agricultural data with the Long Short-Term Memory model improves the performance of the model in terms of accuracy. The Convolutional Neural Network model achieved the second-best performance. Therefore, the deep learning model has a remarkable ability to predict the yield at the end of the season.},
DOI = {10.3390/en14113004}
}



@Article{rs13112052,
AUTHOR = {Yan, Dongchuan and Li, Guoqing and Li, Xiangqiang and Zhang, Hao and Lei, Hua and Lu, Kaixuan and Cheng, Minghua and Zhu, Fuxiao},
TITLE = {An Improved Faster R-CNN Method to Detect Tailings Ponds from High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2052},
URL = {https://www.mdpi.com/2072-4292/13/11/2052},
ISSN = {2072-4292},
ABSTRACT = {Dam failure of tailings ponds can result in serious casualties and environmental pollution. Therefore, timely and accurate monitoring is crucial for managing tailings ponds and preventing damage from tailings pond accidents. Remote sensing technology facilitates the regular extraction and monitoring of tailings pond information. However, traditional remote sensing techniques are inefficient and have low levels of automation, which hinders the large-scale, high-frequency, and high-precision extraction of tailings pond information. Moreover, research into the automatic and intelligent extraction of tailings pond information from high-resolution remote sensing images is relatively rare. However, the deep learning end-to-end model offers a solution to this problem. This study proposes an intelligent and high-precision method for extracting tailings pond information from high-resolution images, which improves deep learning target detection model: faster region-based convolutional neural network (Faster R-CNN). A comparison study is conducted and the model input size with the highest precision is selected. The feature pyramid network (FPN) is adopted to obtain multiscale feature maps with rich context information, the attention mechanism is used to improve the FPN, and the contribution degrees of feature channels are recalibrated. The model test results based on GoogleEarth high-resolution remote sensing images indicate a significant increase in the average precision (AP) and recall of tailings pond detection from that of Faster R-CNN by 5.6% and 10.9%, reaching 85.7% and 62.9%, respectively. Considering the current rapid increase in high-resolution remote sensing images, this method will be important for large-scale, high-precision, and intelligent monitoring of tailings ponds, which will greatly improve the decision-making efficiency in tailings pond management.},
DOI = {10.3390/rs13112052}
}



@Article{su13115877,
AUTHOR = {Adnan, Rana Muhammad and Jaafari, Abolfazl and Mohanavelu, Aadhityaa and Kisi, Ozgur and Elbeltagi, Ahmed},
TITLE = {Novel Ensemble Forecasting of Streamflow Using Locally Weighted Learning Algorithm},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5877},
URL = {https://www.mdpi.com/2071-1050/13/11/5877},
ISSN = {2071-1050},
ABSTRACT = {The development of advanced computational models for improving the accuracy of streamflow forecasting could save time and cost for sustainable water resource management. In this study, a locally weighted learning (LWL) algorithm is combined with the Additive Regression (AR), Bagging (BG), Dagging (DG), Random Subspace (RS), and Rotation Forest (RF) ensemble techniques for the streamflow forecasting in the Jhelum Catchment, Pakistan. To build the models, we grouped the initial parameters into four different scenarios (M1–M4) of input data with a five-fold cross-validation (I–V) approach. To evaluate the accuracy of the developed ensemble models, previous lagged values of streamflow were used as inputs whereas the cross-validation technique and periodicity input were used to examine prediction accuracy on the basis of root correlation coefficient (R), root mean squared error (RMSE), mean absolute error (MAE), relative absolute error (RAE), and root relative squared error (RRSE). The results showed that the incorporation of periodicity (i.e., MN) as an additional input variable considerably improved both the training performance and predictive performance of the models. A comparison between the results obtained from the input combinations III and IV revealed a significant performance improvement. The cross-validation revealed that the dataset M3 provided more accurate results compared to the other datasets. While all the ensemble models successfully outperformed the standalone LWL model, the ensemble LWL-AR model was identified as the best model. Our study demonstrated that the ensemble modeling approach is a robust and promising alternative to the single forecasting of streamflow that should be further investigated with different datasets from other regions around the world.},
DOI = {10.3390/su13115877}
}



@Article{s21113647,
AUTHOR = {Wu, Zhangnan and Chen, Yajun and Zhao, Bo and Kang, Xiaobing and Ding, Yuanyuan},
TITLE = {Review of Weed Detection Methods Based on Computer Vision},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3647},
URL = {https://www.mdpi.com/1424-8220/21/11/3647},
PubMedID = {34073867},
ISSN = {1424-8220},
ABSTRACT = {Weeds are one of the most important factors affecting agricultural production. The waste and pollution of farmland ecological environment caused by full-coverage chemical herbicide spraying are becoming increasingly evident. With the continuous improvement in the agricultural production level, accurately distinguishing crops from weeds and achieving precise spraying only for weeds are important. However, precise spraying depends on accurately identifying and locating weeds and crops. In recent years, some scholars have used various computer vision methods to achieve this purpose. This review elaborates the two aspects of using traditional image-processing methods and deep learning-based methods to solve weed detection problems. It provides an overview of various methods for weed detection in recent years, analyzes the advantages and disadvantages of existing methods, and introduces several related plant leaves, weed datasets, and weeding machinery. Lastly, the problems and difficulties of the existing weed detection methods are analyzed, and the development trend of future research is prospected.},
DOI = {10.3390/s21113647}
}



@Article{environments8060048,
AUTHOR = {Teodoro, Ana and Santos, Patrícia and Espinha Marques, Jorge and Ribeiro, Joana and Mansilha, Catarina and Melo, Armindo and Duarte, Lia and Rodrigues de Almeida, Cátia and Flores, Deolinda},
TITLE = {An Integrated Multi-Approach to Environmental Monitoring of a Self-Burning Coal Waste Pile: The São Pedro da Cova Mine (Porto, Portugal) Study Case},
JOURNAL = {Environments},
VOLUME = {8},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {48},
URL = {https://www.mdpi.com/2076-3298/8/6/48},
ISSN = {2076-3298},
ABSTRACT = {The São Pedro da Cova waste pile (Porto, Portugal) is composed of coal mining residues that have been self-burning since 2005 and is located close to an inhabited area and social infrastructures, further adding to effects on the environment and human health. Therefore, there is a great interest in the environmental monitoring of this waste pile. This work describes an integrative multi-approach that allows the environmental monitoring of several parameters of the waste pile, applying several technologies. The temperature measurements were obtained by a thermal infrared (TIR) sensor on board an unmanned aerial vehicle (UAV) and supplemented with field measurements. In order to evaluate the altimetric variations, for each flight, a digital elevation model (DEM) was generated considering a multispectral sensor also on board the UAV. The hydrogeochemical characterization was performed through the analysis of groundwater and surface water samples, with and without the influence of mine drainage. The soil monitoring included the analysis of waste material as well as the surface soil in the surrounding area of the waste pile. All the data were analyzed and integrated in a geographical information system (GIS) open-source application. The adopted multi-approach methodology, given its intrinsic interdisciplinary character, has proven to be an effective way of encompassing the complexity of this type of environmental problem.},
DOI = {10.3390/environments8060048}
}



@Article{rs13112077,
AUTHOR = {Fetai, Bujar and Račič, Matej and Lisec, Anka},
TITLE = {Deep Learning for Detection of Visible Land Boundaries from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2077},
URL = {https://www.mdpi.com/2072-4292/13/11/2077},
ISSN = {2072-4292},
ABSTRACT = {Current efforts aim to accelerate cadastral mapping through innovative and automated approaches and can be used to both create and update cadastral maps. This research aims to automate the detection of visible land boundaries from unmanned aerial vehicle (UAV) imagery using deep learning. In addition, we wanted to evaluate the advantages and disadvantages of programming-based deep learning compared to commercial software-based deep learning. For the first case, we used the convolutional neural network U-Net, implemented in Keras, written in Python using the TensorFlow library. For commercial software-based deep learning, we used ENVINet5. UAV imageries from different areas were used to train the U-Net model, which was performed in Google Collaboratory and tested in the study area in Odranci, Slovenia. The results were compared with the results of ENVINet5 using the same datasets. The results showed that both models achieved an overall accuracy of over 95%. The high accuracy is due to the problem of unbalanced classes, which is usually present in boundary detection tasks. U-Net provided a recall of 0.35 and a precision of 0.68 when the threshold was set to 0.5. A threshold can be viewed as a tool for filtering predicted boundary maps and balancing recall and precision. For equitable comparison with ENVINet5, the threshold was increased. U-Net provided more balanced results, a recall of 0.65 and a precision of 0.41, compared to ENVINet5 recall of 0.84 and a precision of 0.35. Programming-based deep learning provides a more flexible yet complex approach to boundary mapping than software-based, which is rigid and does not require programming. The predicted visible land boundaries can be used both to speed up the creation of cadastral maps and to automate the revision of existing cadastral maps and define areas where updates are needed. The predicted boundaries cannot be considered final at this stage but can be used as preliminary cadastral boundaries.},
DOI = {10.3390/rs13112077}
}



@Article{drones5020045,
AUTHOR = {Dronova, Iryna and Kislik, Chippie and Dinh, Zack and Kelly, Maggi},
TITLE = {A Review of Unoccupied Aerial Vehicle Use in Wetland Applications: Emerging Opportunities in Approach, Technology, and Data},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {45},
URL = {https://www.mdpi.com/2504-446X/5/2/45},
ISSN = {2504-446X},
ABSTRACT = {Recent developments in technology and data processing for Unoccupied Aerial Vehicles (UAVs) have revolutionized the scope of ecosystem monitoring, providing novel pathways to fill the critical gap between limited-scope field surveys and limited-customization satellite and piloted aerial platforms. These advances are especially ground-breaking for supporting management, restoration, and conservation of landscapes with limited field access and vulnerable ecological systems, particularly wetlands. This study presents a scoping review of the current status and emerging opportunities in wetland UAV applications, with particular emphasis on ecosystem management goals and remaining research, technology, and data needs to even better support these goals in the future. Using 122 case studies from 29 countries, we discuss which wetland monitoring and management objectives are most served by this rapidly developing technology, and what workflows were employed to analyze these data. This review showcases many ways in which UAVs may help reduce or replace logistically demanding field surveys and can help improve the efficiency of UAV-based workflows to support longer-term monitoring in the face of wetland environmental challenges and management constraints. We also highlight several emerging trends in applications, technology, and data and offer insights into future needs.},
DOI = {10.3390/drones5020045}
}



@Article{agronomy11061069,
AUTHOR = {Ahmed, Shibbir and Qiu, Baijing and Ahmad, Fiaz and Kong, Chun-Wei and Xin, Huang},
TITLE = {A State-of-the-Art Analysis of Obstacle Avoidance Methods from the Perspective of an Agricultural Sprayer UAV’s Operation Scenario},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1069},
URL = {https://www.mdpi.com/2073-4395/11/6/1069},
ISSN = {2073-4395},
ABSTRACT = {Over the last decade, Unmanned Aerial Vehicles (UAVs), also known as drones, have been broadly utilized in various agricultural fields, such as crop management, crop monitoring, seed sowing, and pesticide spraying. Nonetheless, autonomy is still a crucial limitation faced by the Internet of Things (IoT) UAV systems, especially when used as sprayer UAVs, where data needs to be captured and preprocessed for robust real-time obstacle detection and collision avoidance. Moreover, because of the objective and operational difference between general UAVs and sprayer UAVs, not every obstacle detection and collision avoidance method will be sufficient for sprayer UAVs. In this regard, this article seeks to review the most relevant developments on all correlated branches of the obstacle avoidance scenarios for agricultural sprayer UAVs, including a UAV sprayer’s structural details. Furthermore, the most relevant open challenges for current UAV sprayer solutions are enumerated, thus paving the way for future researchers to define a roadmap for devising new-generation, affordable autonomous sprayer UAV solutions. Agricultural UAV sprayers require data-intensive algorithms for the processing of the images acquired, and expertise in the field of autonomous flight is usually needed. The present study concludes that UAV sprayers are still facing obstacle detection challenges due to their dynamic operating and loading conditions.},
DOI = {10.3390/agronomy11061069}
}



@Article{s21113698,
AUTHOR = {Kawai, Takaaki},
TITLE = {Video Slice: Image Compression and Transmission for Agricultural Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3698},
URL = {https://www.mdpi.com/1424-8220/21/11/3698},
PubMedID = {34073404},
ISSN = {1424-8220},
ABSTRACT = {When agricultural automation systems are required to send cultivation field images to the cloud for field monitoring, pay-as-you-go mobile communication leads to high operation costs. To minimize cost, one can exploit a characteristic of cultivation field images wherein the landscape does not change considerably besides the appearance of the plants. Therefore, this paper presents a method that transmits only the difference data between the past and current images to minimize the amount of transmitted data. This method is easy to implement because the difference data are generated using an existing video encoder. Further, the difference data are generated based on an image at a specific time instead of the images at adjacent times, and thus the subsequent images can be reproduced even if the previous difference data are lost because of unstable mobile communication. A prototype of the proposed method was implemented with a MPEG-4 Visual video encoder. The amount of transmitted and received data on the medium access control layer was decreased to approximately 1/4 of that when using the secure copy protocol. The transmission time for one image was 5.6 s; thus, the proposed method achieved a reasonable processing time and a reduction of transmitted data.},
DOI = {10.3390/s21113698}
}



@Article{rs13112099,
AUTHOR = {Greifeneder, Felix and Notarnicola, Claudia and Wagner, Wolfgang},
TITLE = {A Machine Learning-Based Approach for Surface Soil Moisture Estimations with Google Earth Engine},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2099},
URL = {https://www.mdpi.com/2072-4292/13/11/2099},
ISSN = {2072-4292},
ABSTRACT = {Due to its relation to the Earth’s climate and weather and phenomena like drought, flooding, or landslides, knowledge of the soil moisture content is valuable to many scientific and professional users. Remote-sensing offers the unique possibility for continuous measurements of this variable. Especially for agriculture, there is a strong demand for high spatial resolution mapping. However, operationally available soil moisture products exist with medium to coarse spatial resolution only (≥1 km). This study introduces a machine learning (ML)—based approach for the high spatial resolution (50 m) mapping of soil moisture based on the integration of Landsat-8 optical and thermal images, Copernicus Sentinel-1 C-Band SAR images, and modelled data, executable in the Google Earth Engine. The novelty of this approach lies in applying an entirely data-driven ML concept for global estimation of the surface soil moisture content. Globally distributed in situ data from the International Soil Moisture Network acted as an input for model training. Based on the independent validation dataset, the resulting overall estimation accuracy, in terms of Root-Mean-Squared-Error and R², was 0.04 m3·m−3 and 0.81, respectively. Beyond the retrieval model itself, this article introduces a framework for collecting training data and a stand-alone Python package for soil moisture mapping. The Google Earth Engine Python API facilitates the execution of data collection and retrieval which is entirely cloud-based. For soil moisture retrieval, it eliminates the requirement to download or preprocess any input datasets.},
DOI = {10.3390/rs13112099}
}



@Article{f12060692,
AUTHOR = {Choudhury, MD Abdul Mueed and Marcheggiani, Ernesto and Galli, Andrea and Modica, Giuseppe and Somers, Ben},
TITLE = {Mapping the Urban Atmospheric Carbon Stock by LiDAR and WorldView-3 Data},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {692},
URL = {https://www.mdpi.com/1999-4907/12/6/692},
ISSN = {1999-4907},
ABSTRACT = {Currently, the worsening impacts of urbanizations have been impelled to the importance of monitoring and management of existing urban trees, securing sustainable use of the available green spaces. Urban tree species identification and evaluation of their roles in atmospheric Carbon Stock (CS) are still among the prime concerns for city planners regarding initiating a convenient and easily adaptive urban green planning and management system. A detailed methodology on the urban tree carbon stock calibration and mapping was conducted in the urban area of Brussels, Belgium. A comparative analysis of the mapping outcomes was assessed to define the convenience and efficiency of two different remote sensing data sources, Light Detection and Ranging (LiDAR) and WorldView-3 (WV-3), in a unique urban area. The mapping results were validated against field estimated carbon stocks. At the initial stage, dominant tree species were identified and classified using the high-resolution WorldView3 image, leading to the final carbon stock mapping based on the dominant species. An object-based image analysis approach was employed to attain an overall accuracy (OA) of 71% during the classification of the dominant species. The field estimations of carbon stock for each plot were done utilizing an allometric model based on the field tree dendrometric data. Later based on the correlation among the field data and the variables (i.e., Normalized Difference Vegetation Index, NDVI and Crown Height Model, CHM) extracted from the available remote sensing data, the carbon stock mapping and validation had been done in a GIS environment. The calibrated NDVI and CHM had been used to compute possible carbon stock in either case of the WV-3 image and LiDAR data, respectively. A comparative discussion has been introduced to bring out the issues, especially for the developing countries, where WV-3 data could be a better solution over the hardly available LiDAR data. This study could assist city planners in understanding and deciding the applicability of remote sensing data sources based on their availability and the level of expediency, ensuring a sustainable urban green management system.},
DOI = {10.3390/f12060692}
}



@Article{rs13112123,
AUTHOR = {Aeberli, Aaron and Johansen, Kasper and Robson, Andrew and Lamb, David W. and Phinn, Stuart},
TITLE = {Detection of Banana Plants Using Multi-Temporal Multispectral UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2123},
URL = {https://www.mdpi.com/2072-4292/13/11/2123},
ISSN = {2072-4292},
ABSTRACT = {Unoccupied aerial vehicles (UAVs) have become increasingly commonplace in aiding planning and management decisions in agricultural and horticultural crop production. The ability of UAV-based sensing technologies to provide high spatial (&lt;1 m) and temporal (on-demand) resolution data facilitates monitoring of individual plants over time and can provide essential information about health, yield, and growth in a timely and quantifiable manner. Such applications would be beneficial for cropped banana plants due to their distinctive growth characteristics. Limited studies have employed UAV data for mapping banana crops and to our knowledge only one other investigation features multi-temporal detection of banana crowns. The purpose of this study was to determine the suitability of multiple-date UAV-captured multi-spectral data for the automated detection of individual plants using convolutional neural network (CNN), template matching (TM), and local maximum filter (LMF) methods in a geographic object-based image analysis (GEOBIA) software framework coupled with basic classification refinement. The results indicate that CNN returns the highest plant detection accuracies, with the developed rule set and model providing greater transferability between dates (F-score ranging between 0.93 and 0.85) than TM (0.86–0.74) and LMF (0.86–0.73) approaches. The findings provide a foundation for UAV-based individual banana plant counting and crop monitoring, which may be used for precision agricultural applications to monitor health, estimate yield, and to inform on fertilizer, pesticide, and other input requirements for optimized farm management.},
DOI = {10.3390/rs13112123}
}



@Article{rs13112126,
AUTHOR = {Wang, Yuliang and Li, Mingshi},
TITLE = {Annually Urban Fractional Vegetation Cover Dynamic Mapping in Hefei, China (1999–2018)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2126},
URL = {https://www.mdpi.com/2072-4292/13/11/2126},
ISSN = {2072-4292},
ABSTRACT = {Vegetation measures are crucial for assessing changes in the ecological environment. Fractional vegetation cover (FVC) provides information on the growth status, distribution characteristics, and structural changes of vegetation. An in-depth understanding of the dynamic changes in urban FVC contributes to the sustainable development of ecological civilization in the urbanization process. However, dynamic change detection of urban FVC using multi-temporal remote sensing images is a complex process and challenge. This paper proposed an improved FVC estimation model by fusing the optimized dynamic range vegetation index (ODRVI) model. The ODRVI model improved sensitivity to the water content, roughness degree, and soil type by minimizing the influence of bare soil in areas of sparse vegetation cover. The ODRVI model enhanced the stability of FVC estimation in the near-infrared (NIR) band in areas of dense and sparse vegetation cover through introducing the vegetation canopy vertical porosity (VCVP) model. The verification results confirmed that the proposed model had better performance than typical vegetation index (VI) models for multi-temporal Landsat images. The coefficient of determination (R2) between the ODRVI model and the FVC was 0.9572, which was 7.4% higher than the average R2 of other typical VI models. Moreover, the annual urban FVC dynamics were mapped using the proposed improved FVC estimation model in Hefei, China (1999–2018). The total area of all grades FVC decreased by 33.08% during the past 20 years in Hefei, China. The areas of the extremely low, low, and medium grades FVC exhibited apparent inter-annual fluctuations. The maximum standard deviation of the area change of the medium grade FVC was 13.35%. For other grades of FVC, the order of standard deviation of the change ratio was extremely low FVC &gt; low FVC &gt; medium-high FVC &gt; high FVC. The dynamic mapping of FVC revealed the influence intensity and direction of the urban sprawl on vegetation coverage, which contributes to the strategic development of sustainable urban management plans.},
DOI = {10.3390/rs13112126}
}



@Article{rs13112139,
AUTHOR = {de Castro, Ana I. and Shi, Yeyin and Maja, Joe Mari and Peña, Jose M.},
TITLE = {UAVs for Vegetation Monitoring: Overview and Recent Scientific Contributions},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2139},
URL = {https://www.mdpi.com/2072-4292/13/11/2139},
ISSN = {2072-4292},
ABSTRACT = {This paper reviewed a set of twenty-one original and innovative papers included in a special issue on UAVs for vegetation monitoring, which proposed new methods and techniques applied to diverse agricultural and forestry scenarios. Three general categories were considered: (1) sensors and vegetation indices used, (2) technological goals pursued, and (3) agroforestry applications. Some investigations focused on issues related to UAV flight operations, spatial resolution requirements, and computation and data analytics, while others studied the ability of UAVs for characterizing relevant vegetation features (mainly canopy cover and crop height) or for detecting different plant/crop stressors, such as nutrient content/deficiencies, water needs, weeds, and diseases. The general goal was proposing UAV-based technological solutions for a better use of agricultural and forestry resources and more efficient production with relevant economic and environmental benefits.},
DOI = {10.3390/rs13112139}
}



@Article{s21113777,
AUTHOR = {Zhang, Yani and Zhao, Huailin and Duan, Zuodong and Huang, Liangjun and Deng, Jiahao and Zhang, Qing},
TITLE = {Congested Crowd Counting via Adaptive Multi-Scale Context Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3777},
URL = {https://www.mdpi.com/1424-8220/21/11/3777},
PubMedID = {34072408},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we propose a novel congested crowd counting network for crowd density estimation, i.e., the Adaptive Multi-scale Context Aggregation Network (MSCANet). MSCANet efficiently leverages the spatial context information to accomplish crowd density estimation in a complicated crowd scene. To achieve this, a multi-scale context learning block, called the Multi-scale Context Aggregation module (MSCA), is proposed to first extract different scale information and then adaptively aggregate it to capture the full scale of the crowd. Employing multiple MSCAs in a cascaded manner, the MSCANet can deeply utilize the spatial context information and modulate preliminary features into more distinguishing and scale-sensitive features, which are finally applied to a 1 × 1 convolution operation to obtain the crowd density results. Extensive experiments on three challenging crowd counting benchmarks showed that our model yielded compelling performance against the other state-of-the-art methods. To thoroughly prove the generality of MSCANet, we extend our method to two relevant tasks: crowd localization and remote sensing object counting. The extension experiment results also confirmed the effectiveness of MSCANet.},
DOI = {10.3390/s21113777}
}



@Article{rs13112146,
AUTHOR = {Zhang, Peng and Hu, Shougeng and Li, Weidong and Zhang, Chuanrong and Cheng, Peikun},
TITLE = {Improving Parcel-Level Mapping of Smallholder Crops from VHSR Imagery: An Ensemble Machine-Learning-Based Framework},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2146},
URL = {https://www.mdpi.com/2072-4292/13/11/2146},
ISSN = {2072-4292},
ABSTRACT = {Explicit spatial information about crop types on smallholder farms is important for the development of local precision agriculture. However, due to highly fragmented and heterogeneous cropland landscapes, fine-scale mapping of smallholder crops, based on low- and medium-resolution satellite images and relying on a single machine learning (ML) classifier, generally fails to achieve satisfactory performance. This paper develops an ensemble ML-based framework to improve the accuracy of parcel-level smallholder crop mapping from very high spatial resolution (VHSR) images. A typical smallholder agricultural area in central China covered by WorldView-2 images is selected to demonstrate our approach. This approach involves the task of distinguishing eight crop-level agricultural land use types. To this end, six widely used individual ML classifiers are evaluated. We further improved their performance by independently implementing bagging and stacking ensemble learning (EL) techniques. The results show that the bagging models improved the performance of unstable classifiers, but these improvements are limited. In contrast, the stacking models perform better, and the Stacking #2 model (overall accuracy = 83.91%, kappa = 0.812), which integrates the three best-performing individual classifiers, performs the best of all of the built models and improves the classwise accuracy of almost all of the land use types. Since classification performance can be significantly improved without adding costly data collection, stacking-ensemble mapping approaches are valuable for the spatial management of complex agricultural areas. We also demonstrate that using geometric and textural features extracted from VHSR images can improve the accuracy of parcel-level smallholder crop mapping. The proposed framework shows the great potential of combining EL technology with VHSR imagery for accurate mapping of smallholder crops, which could facilitate the development of parcel-level crop identification systems in countries dominated by smallholder agriculture.},
DOI = {10.3390/rs13112146}
}



@Article{app11115054,
AUTHOR = {Budholiya, Sejal and Bhat, Aayush and Raj, S. Aravind and Hameed Sultan, Mohamed Thariq and Md Shah, Ain Umaira and A. Basri, Adi},
TITLE = {State of the Art Review about Bio-Inspired Design and Applications: An Aerospace Perspective},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5054},
URL = {https://www.mdpi.com/2076-3417/11/11/5054},
ISSN = {2076-3417},
ABSTRACT = {The field of bio-inspired design has tremendously transitioned into newer automated methods, yet there are methods being discovered which can elucidate underlying principles in design, materials, and manufacturing. Bio-inspired design aims to translate knowledge from the natural world to the current trends in industry. The recent growth in additive manufacturing (AM)methods has fueled the tremendous growth of bio-inspired products. It has enabled the production of intricate and complicated features notably used in the aerospace industry. Numerous methodologies were adopted to analyse the process of bio-inspired material selection, manufacturing methods, design, and applications. In the current review, different approaches are implemented to utilize bio-inspired designs that have revolutionized the aerospace industry, focusing on AM methods.},
DOI = {10.3390/app11115054}
}



@Article{plants10061099,
AUTHOR = {Gong, Bin and Shu, Cheng and Han, Song and Cheng, Sheng-Gao},
TITLE = {Mine Vegetation Identification via Ecological Monitoring and Deep Belief Network},
JOURNAL = {Plants},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1099},
URL = {https://www.mdpi.com/2223-7747/10/6/1099},
PubMedID = {34070739},
ISSN = {2223-7747},
ABSTRACT = {Based on the characteristics of remote sensing images of mine vegetation, this research studied the application of deep belief network model in mine vegetation identification. Through vegetation identification and classification, the ecological environment index of mining area was determined according to the analysis of vegetation and coverage. Deep learning algorithm is adopted to improve the depth study, the vegetation coverage in the analysis was studied. Parameters and parameter values were selected for identification by establishing the optimal experimental design. The experimental results were compared with remote sensing images to determine the accuracy of deep learning identification and the effectiveness of the algorithm. When the sample size is 2,000,000 pixels, through repeated tests and classification effect comparison, the optimal parameter setting suitable for mine vegetation identification is obtained. Parameter setting: the number of network layers is 3 layers; the number of hidden layer neurons is 60. The learning rate is 0.01 and the number of iterations is 2. The average recognition rate of vegetation coverage was 95.95%, outperforming some other models, and the accuracy rate of kappa coefficient was 0.95, which can accurately reflect the vegetation coverage. The clearer the satellite image is, the more accurate the recognition result is, and the accuracy is closer to 100%. The identification of vegetation coverage has important guiding significance for determining the area and area of ecological restoration.},
DOI = {10.3390/plants10061099}
}



@Article{s21113830,
AUTHOR = {Almadhor, Ahmad and Rauf, Hafiz Tayyab and Lali, Muhammad Ikram Ullah and Damaševičius, Robertas and Alouffi, Bader and Alharbi, Abdullah},
TITLE = {AI-Driven Framework for Recognition of Guava Plant Diseases through Machine Learning from DSLR Camera Sensor Based High Resolution Imagery},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3830},
URL = {https://www.mdpi.com/1424-8220/21/11/3830},
PubMedID = {34205885},
ISSN = {1424-8220},
ABSTRACT = {Plant diseases can cause a considerable reduction in the quality and number of agricultural products. Guava, well known to be the tropics’ apple, is one significant fruit cultivated in tropical regions. It is attacked by 177 pathogens, including 167 fungal and others such as bacterial, algal, and nematodes. In addition, postharvest diseases may cause crucial production loss. Due to minor variations in various guava disease symptoms, an expert opinion is required for disease analysis. Improper diagnosis may cause economic losses to farmers’ improper use of pesticides. Automatic detection of diseases in plants once they emerge on the plants’ leaves and fruit is required to maintain high crop fields. In this paper, an artificial intelligence (AI) driven framework is presented to detect and classify the most common guava plant diseases. The proposed framework employs the ΔE color difference image segmentation to segregate the areas infected by the disease. Furthermore, color (RGB, HSV) histogram and textural (LBP) features are applied to extract rich, informative feature vectors. The combination of color and textural features are used to identify and attain similar outcomes compared to individual channels, while disease recognition is performed by employing advanced machine-learning classifiers (Fine KNN, Complex Tree, Boosted Tree, Bagged Tree, Cubic SVM). The proposed framework is evaluated on a high-resolution (18 MP) image dataset of guava leaves and fruit. The best recognition results were obtained by Bagged Tree classifier on a set of RGB, HSV, and LBP features (99% accuracy in recognizing four guava fruit diseases (Canker, Mummification, Dot, and Rust) against healthy fruit). The proposed framework may help the farmers to avoid possible production loss by taking early precautions.},
DOI = {10.3390/s21113830}
}



@Article{rs13112169,
AUTHOR = {Lee, Seunghyeon and Song, Youngkeun and Kil, Sung-Ho},
TITLE = {Feasibility Analyses of Real-Time Detection of Wildlife Using UAV-Derived Thermal and RGB Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2169},
URL = {https://www.mdpi.com/2072-4292/13/11/2169},
ISSN = {2072-4292},
ABSTRACT = {Wildlife monitoring is carried out for diverse reasons, and monitoring methods have gradually advanced through technological development. Direct field investigations have been replaced by remote monitoring methods, and unmanned aerial vehicles (UAVs) have recently become the most important tool for wildlife monitoring. Many previous studies on detecting wild animals have used RGB images acquired from UAVs, with most of the analyses depending on machine learning–deep learning (ML–DL) methods. These methods provide relatively accurate results, and when thermal sensors are used as a supplement, even more accurate detection results can be obtained through complementation with RGB images. However, because most previous analyses were based on ML–DL methods, a lot of time was required to generate training data and train detection models. This drawback makes ML–DL methods unsuitable for real-time detection in the field. To compensate for the disadvantages of the previous methods, this paper proposes a real-time animal detection method that generates a total of six applicable input images depending on the context and uses them for detection. The proposed method is based on the Sobel edge algorithm, which is simple but can detect edges quickly based on change values. The method can detect animals in a single image without training data. The fastest detection time per image was 0.033 s, and all frames of a thermal video could be analyzed. Furthermore, because of the synchronization of the properties of the thermal and RGB images, the performance of the method was above average in comparison with previous studies. With target images acquired at heights below 100 m, the maximum detection precision and detection recall of the most accurate input image were 0.804 and 0.699, respectively. However, the low resolution of the thermal sensor and its shooting height limitation were hindrances to wildlife detection. The aim of future research will be to develop a detection method that can improve these shortcomings.},
DOI = {10.3390/rs13112169}
}



@Article{en14113258,
AUTHOR = {Szurgacz, Dawid and Zhironkin, Sergey and Vöth, Stefan and Pokorný, Jiří and Spearing, A.J.S. (Sam) and Cehlár, Michal and Stempniak, Marta and Sobik, Leszek},
TITLE = {Thermal Imaging Study to Determine the Operational Condition of a Conveyor Belt Drive System Structure},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3258},
URL = {https://www.mdpi.com/1996-1073/14/11/3258},
ISSN = {1996-1073},
ABSTRACT = {The paper discusses the results of a study carried out to determine the thermal condition of a conveyor power unit using a thermal imaging camera. The tests covered conveyors in the main haulage system carrying coal from a longwall. The measurements were taken with a thermal imaging diagnostic method which measures infrared radiation emitted by an object. This technology provides a means of assessing the imminence and severity of a possible failure or damage. The method is a non-contact measuring technique and offers great advantages in an underground mine. The thermograms were analysed by comparing the temperature distribution. An analysis of the operating time of the conveyors was also carried out and the causes of the thermal condition were determined. The main purpose of the research was to detect changes in thermal state during the operation of a belt conveyor that could indicate failure and permit early maintenance and eliminate the chance of a fire. The article also discusses the construction and principle of operation of a thermal imaging camera. The findings obtained from the research analysis on determining the thermal condition of the conveyor drive unit are a valuable source of information for the mine’s maintenance service.},
DOI = {10.3390/en14113258}
}



@Article{rs13112181,
AUTHOR = {Illarionova , Svetlana and Nesteruk , Sergey and Shadrin, Dmitrii and Ignatiev , Vladimir and Pukalchik , Maria and Oseledets, Ivan},
TITLE = {MixChannel: Advanced Augmentation for Multispectral Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2181},
URL = {https://www.mdpi.com/2072-4292/13/11/2181},
ISSN = {2072-4292},
ABSTRACT = {Usage of multispectral satellite imaging data opens vast possibilities for monitoring and quantitatively assessing properties or objects of interest on a global scale. Machine learning and computer vision (CV) approaches show themselves as promising tools for automatizing satellite image analysis. However, there are limitations in using CV for satellite data. Mainly, the crucial one is the amount of data available for model training. This paper presents a novel image augmentation approach called MixChannel that helps to address this limitation and improve the accuracy of solving segmentation and classification tasks with multispectral satellite images. The core idea is to utilize the fact that there is usually more than one image for each location in remote sensing tasks, and this extra data can be mixed to achieve the more robust performance of the trained models. The proposed approach substitutes some channels of the original training image with channels from other images of the exact location to mix auxiliary data. This augmentation technique preserves the spatial features of the original image and adds natural color variability with some probability. We also show an efficient algorithm to tune channel substitution probabilities. We report that the MixChannel image augmentation method provides a noticeable increase in performance of all the considered models in the studied forest types classification problem.},
DOI = {10.3390/rs13112181}
}



@Article{math9111287,
AUTHOR = {Kelemen, Miroslav and Polishchuk, Volodymyr and Gavurová, Beáta and Rozenberg, Róbert and Bartok, Juraj and Gaál, Ladislav and Gera, Martin and Kelemen, Martin},
TITLE = {Model of Evaluation and Selection of Expert Group Members for Smart Cities, Green Transportation and Mobility: From Safe Times to Pandemic Times},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {1287},
URL = {https://www.mdpi.com/2227-7390/9/11/1287},
ISSN = {2227-7390},
ABSTRACT = {This paper presents the development of technologies to support the decision-making of local government executives and smart city concept managers in selecting and evaluating the competencies of new members for advisory groups for solving problems that are implemented in safe times in individual areas or in crises, such as pandemics. The reason for developing effective urban transformation strategies and for the transparent selection of independent experts (non-politicians) for policymaking, decision-making, and implementation teams is not only the heterogeneity of smart city dimensions together with the necessary complexity and systems approach, but also the nature of the capacities and tools needed for smart city concepts. The innovative hybrid competency assessment model is based on fuzzy logic and a network for neuro-fuzzy assessment. It is a technological model for evaluating the competencies of specialists, taking into account the influence of human factors on the processes of personnel selection and system management. An innovative web platform named “Smart City Concept Personnel Selection” has been designed, which can be adapted to various users of municipalities or regional institutions for the transparent selection of qualified personnel for effective decision-making and the use of public funds during safe times or emergencies, such as the COVID-19 pandemic.},
DOI = {10.3390/math9111287}
}



@Article{ijgi10060382,
AUTHOR = {Al-Fugara, A’kif and Mabdeh, Ali Nouh and Ahmadlou, Mohammad and Pourghasemi, Hamid Reza and Al-Adamat, Rida and Pradhan, Biswajeet and Al-Shabeeb, Abdel Rahman},
TITLE = {Wildland Fire Susceptibility Mapping Using Support Vector Regression and Adaptive Neuro-Fuzzy Inference System-Based Whale Optimization Algorithm and Simulated Annealing},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {382},
URL = {https://www.mdpi.com/2220-9964/10/6/382},
ISSN = {2220-9964},
ABSTRACT = {Fires are one of the most destructive forces in natural ecosystems. This study aims to develop and compare four hybrid models using two well-known machine learning models, support vector regression (SVR) and the adaptive neuro-fuzzy inference system (ANFIS), as well as two meta-heuristic models, the whale optimization algorithm (WOA) and simulated annealing (SA) to map wildland fires in Jerash Province, Jordan. For modeling, 109 fire locations were used along with 14 relevant factors, including elevation, slope, aspect, land use, normalized difference vegetation index (NDVI), rainfall, temperature, wind speed, solar radiation, soil texture, topographic wetness index (TWI), distance to drainage, and population density, as the variables affecting the fire occurrence. The area under the receiver operating characteristic (AUROC) was used to evaluate the accuracy of the models. The findings indicated that SVR-based hybrid models yielded a higher AUROC value (0.965 and 0.949) than the ANFIS-based hybrid models (0.904 and 0.894, respectively). Wildland fire susceptibility maps can play a major role in shaping firefighting tactics.},
DOI = {10.3390/ijgi10060382}
}



@Article{rs13112194,
AUTHOR = {Khan, Asim and Asim, Warda and Ulhaq, Anwaar and Ghazi, Bilal and Robinson, Randall W.},
TITLE = {Health Assessment of Eucalyptus Trees Using Siamese Network from Google Street and Ground Truth Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2194},
URL = {https://www.mdpi.com/2072-4292/13/11/2194},
ISSN = {2072-4292},
ABSTRACT = {Urban greenery is an essential characteristic of the urban ecosystem, which offers various advantages, such as improved air quality, human health facilities, storm-water run-off control, carbon reduction, and an increase in property values. Therefore, identification and continuous monitoring of the vegetation (trees) is of vital importance for our urban lifestyle. This paper proposes a deep learning-based network, Siamese convolutional neural network (SCNN), combined with a modified brute-force-based line-of-bearing (LOB) algorithm that evaluates the health of Eucalyptus trees as healthy or unhealthy and identifies their geolocation in real time from Google Street View (GSV) and ground truth images. Our dataset represents Eucalyptus trees’ various details from multiple viewpoints, scales and different shapes to texture. The experiments were carried out in the Wyndham city council area in the state of Victoria, Australia. Our approach obtained an average accuracy of 93.2% in identifying healthy and unhealthy trees after training on around 4500 images and testing on 500 images. This study helps in identifying the Eucalyptus tree with health issues or dead trees in an automated way that can facilitate urban green management and assist the local council to make decisions about plantation and improvements in looking after trees. Overall, this study shows that even in a complex background, most healthy and unhealthy Eucalyptus trees can be detected by our deep learning algorithm in real time.},
DOI = {10.3390/rs13112194}
}



@Article{rs13112197,
AUTHOR = {Waldner, François and Diakogiannis, Foivos I. and Batchelor, Kathryn and Ciccotosto-Camp, Michael and Cooper-Williams, Elizabeth and Herrmann, Chris and Mata, Gonzalo and Toovey, Andrew},
TITLE = {Detect, Consolidate, Delineate: Scalable Mapping of Field Boundaries Using Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2197},
URL = {https://www.mdpi.com/2072-4292/13/11/2197},
ISSN = {2072-4292},
ABSTRACT = {Digital agriculture services can greatly assist growers to monitor their fields and optimize their use throughout the growing season. Thus, knowing the exact location of fields and their boundaries is a prerequisite. Unlike property boundaries, which are recorded in local council or title records, field boundaries are not historically recorded. As a result, digital services currently ask their users to manually draw their field, which is time-consuming and creates disincentives. Here, we present a generalized method, hereafter referred to as DECODE (DEtect, COnsolidate, and DElinetate), that automatically extracts accurate field boundary data from satellite imagery using deep learning based on spatial, spectral, and temporal cues. We introduce a new convolutional neural network (FracTAL ResUNet) as well as two uncertainty metrics to characterize the confidence of the field detection and field delineation processes. We finally propose a new methodology to compare and summarize field-based accuracy metrics. To demonstrate the performance and scalability of our method, we extracted fields across the Australian grains zone with a pixel-based accuracy of 0.87 and a field-based accuracy of up to 0.88 depending on the metric. We also trained a model on data from South Africa instead of Australia and found it transferred well to unseen Australian landscapes. We conclude that the accuracy, scalability and transferability of DECODE shows that large-scale field boundary extraction based on deep learning has reached operational maturity. This opens the door to new agricultural services that provide routine, near-real time field-based analytics.},
DOI = {10.3390/rs13112197}
}



@Article{s21113905,
AUTHOR = {Burdziakowski, Pawel and Zima, Piotr and Wielgat, Pawel and Kalinowska, Dominika},
TITLE = {Tracking Fluorescent Dye Dispersion from an Unmanned Aerial Vehicle},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3905},
URL = {https://www.mdpi.com/1424-8220/21/11/3905},
PubMedID = {34198799},
ISSN = {1424-8220},
ABSTRACT = {Commercial unmanned aerial vehicles continue to gain popularity and their use for collecting image data and recording new phenomena is becoming more frequent. This study presents an effective method for measuring the concentration of fluorescent dyes (fluorescein and Rhodamine WT) for the purpose of providing a mathematical dispersion model. Image data obtained using a typical visible-light camera was used to measure the concentration of the dye floating on water. The reference measurement was taken using a laboratory fluorometer. The article presents the details of three extensive measurement sessions and presents elements of a newly developed method for measuring fluorescent tracer concentrations. The said method provides tracer concentration maps presented on the example of an orthophoto within a 2 × 2 m discrete grid.},
DOI = {10.3390/s21113905}
}



@Article{rs13112220,
AUTHOR = {Bai, Yanbing and Wu, Wenqi and Yang, Zhengxin and Yu, Jinze and Zhao, Bo and Liu, Xing and Yang, Hanfang and Mas, Erick and Koshimura, Shunichi},
TITLE = {Enhancement of Detecting Permanent Water and Temporary Water in Flood Disasters by Fusing Sentinel-1 and Sentinel-2 Imagery Using Deep Learning Algorithms: Demonstration of Sen1Floods11 Benchmark Datasets},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2220},
URL = {https://www.mdpi.com/2072-4292/13/11/2220},
ISSN = {2072-4292},
ABSTRACT = {Identifying permanent water and temporary water in flood disasters efficiently has mainly relied on change detection method from multi-temporal remote sensing imageries, but estimating the water type in flood disaster events from only post-flood remote sensing imageries still remains challenging. Research progress in recent years has demonstrated the excellent potential of multi-source data fusion and deep learning algorithms in improving flood detection, while this field has only been studied initially due to the lack of large-scale labelled remote sensing images of flood events. Here, we present new deep learning algorithms and a multi-source data fusion driven flood inundation mapping approach by leveraging a large-scale publicly available Sen1Flood11 dataset consisting of roughly 4831 labelled Sentinel-1 SAR and Sentinel-2 optical imagery gathered from flood events worldwide in recent years. Specifically, we proposed an automatic segmentation method for surface water, permanent water, and temporary water identification, and all tasks share the same convolutional neural network architecture. We utilize focal loss to deal with the class (water/non-water) imbalance problem. Thorough ablation experiments and analysis confirmed the effectiveness of various proposed designs. In comparison experiments, the method proposed in this paper is superior to other classical models. Our model achieves a mean Intersection over Union (mIoU) of 52.99%, Intersection over Union (IoU) of 52.30%, and Overall Accuracy (OA) of 92.81% on the Sen1Flood11 test set. On the Sen1Flood11 Bolivia test set, our model also achieves very high mIoU (47.88%), IoU (76.74%), and OA (95.59%) and shows good generalization ability.},
DOI = {10.3390/rs13112220}
}



@Article{s21113919,
AUTHOR = {Yang, Xiaoyu and Bao, Nisha and Li, Wenwen and Liu, Shanjun and Fu, Yanhua and Mao, Yachun},
TITLE = {Soil Nutrient Estimation and Mapping in Farmland Based on UAV Imaging Spectrometry},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3919},
URL = {https://www.mdpi.com/1424-8220/21/11/3919},
PubMedID = {34204160},
ISSN = {1424-8220},
ABSTRACT = {Soil nutrient is one of the most important properties for improving farmland quality and product. Imaging spectrometry has the potential for rapid acquisition and real-time monitoring of soil characteristics. This study aims to explore the preprocessing and modeling methods of hyperspectral images obtained from an unmanned aerial vehicle (UAV) platform for estimating the soil organic matter (SOM) and soil total nitrogen (STN) in farmland. The results showed that: (1) Multiplicative Scattering Correction (MSC) performed better in reducing image scattering noise than Standard Normal Variate (SNV) transformation or spectral derivatives, and it yielded a result with higher correlation and lower signal-to-noise ratio; (2) The proposed feature selection method combining Successive Projections Algorithm (SPA) and Competitive Adaptive Reweighted Sampling algorithm (CARS), could provide selective preference for hyperspectral bands. Exploiting this method, 24 and 22 feature bands were selected for SOM and STN estimation, respectively; (3) The particle swarm optimization (PSO) algorithm was employed to obtain optimized input weights and bias values of the extreme learning machine (ELM) model for more accurate prediction of SOM and STN. The improved PSO-ELM model based on the selected preference bands achieved higher prediction accuracy (R2 of 0.73 and RPD of 1.91 for SOM, R2 of 0.63, and RPD of 1.53 for STN) than support vector machine (SVM), partial least squares regression (PLSR), and the ELM model. This study provides an important guideline for monitoring soil nutrient for precision agriculture with imaging spectrometry.},
DOI = {10.3390/s21113919}
}



@Article{app11115303,
AUTHOR = {Huh, Eui-Nam and Hossain, Md Imtiaz},
TITLE = {Brainware Computing: Concepts, Scopes and Challenges},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5303},
URL = {https://www.mdpi.com/2076-3417/11/11/5303},
ISSN = {2076-3417},
ABSTRACT = {Over the decades, robotics technology has acquired sufficient advancement through the progression of 5G Internet, Artificial Intelligence (AI), Internet of Things (IoT), Cloud, and Edge Computing. Though nowadays, Cobot and Service Oriented Architecture (SOA) supported robots with edge computing paradigms have achieved remarkable performances in diverse applications, the existing SOA robotics technology fails to develop a multi-domain expert with high performing robots and demands improvement to Service-Oriented Brain, SOB (including AI model, driving service application and metadata) enabling robot for deploying brain and a new computing model with more scalability and flexibility. In this paper, instead of focusing on SOA and Robot as a Service (RaaS) model, we propose a novel computing architecture, addressed as Brainware Computing, for driving multiple domain-specific brains one-at-a-time in a single hardware robot according to the service, addressed as Brain as a Service (BaaS). In Brainware Computing, each robot can install and remove the virtual machine, which contains SOB and operating applications from the nearest edge cloud. Secondly, we provide an extensive explanation of the scope and possibilities of Brainware Computing. Finally, we demonstrate several challenges and opportunities and then concluded with future research directions in the field of Brainware Computing.},
DOI = {10.3390/app11115303}
}



@Article{rs13112232,
AUTHOR = {Paturkar, Abhipray and Sen Gupta, Gourab and Bailey, Donald},
TITLE = {Making Use of 3D Models for Plant Physiognomic Analysis: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2232},
URL = {https://www.mdpi.com/2072-4292/13/11/2232},
ISSN = {2072-4292},
ABSTRACT = {Use of 3D sensors in plant phenotyping has increased in the last few years. Various image acquisition, 3D representations, 3D model processing and analysis techniques exist to help the researchers. However, a review of approaches, algorithms, and techniques used for 3D plant physiognomic analysis is lacking. In this paper, we investigate the techniques and algorithms used at various stages of processing and analysing 3D models of plants, and identify their current limiting factors. This review will serve potential users as well as new researchers in this field. The focus is on exploring studies monitoring the plant growth of single plants or small scale canopies as opposed to large scale monitoring in the field.},
DOI = {10.3390/rs13112232}
}



@Article{land10060609,
AUTHOR = {Hara, Patryk and Piekutowska, Magdalena and Niedbała, Gniewko},
TITLE = {Selection of Independent Variables for Crop Yield Prediction Using Artificial Neural Network Models with Remote Sensing Data},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {609},
URL = {https://www.mdpi.com/2073-445X/10/6/609},
ISSN = {2073-445X},
ABSTRACT = {Knowing the expected crop yield in the current growing season provides valuable information for farmers, policy makers, and food processing plants. One of the main benefits of using reliable forecasting tools is generating more income from grown crops. Information on the amount of crop yielding before harvesting helps to guide the adoption of an appropriate strategy for managing agricultural products. The difficulty in creating forecasting models is related to the appropriate selection of independent variables. Their proper selection requires a perfect knowledge of the research object. The following article presents and discusses the most commonly used independent variables in agricultural crop yield prediction modeling based on artificial neural networks (ANNs). Particular attention is paid to environmental variables, such as climatic data, air temperature, total precipitation, insolation, and soil parameters. The possibility of using plant productivity indices and vegetation indices, which are valuable predictors obtained due to the application of remote sensing techniques, are analyzed in detail. The paper emphasizes that the increasingly common use of remote sensing and photogrammetric tools enables the development of precision agriculture. In addition, some limitations in the application of certain input variables are specified, as well as further possibilities for the development of non-linear modeling, using artificial neural networks as a tool supporting the practical use of and improvement in precision farming techniques.},
DOI = {10.3390/land10060609}
}



@Article{app11125320,
AUTHOR = {Al-amri, Redhwan and Murugesan, Raja Kumar and Man, Mustafa and Abdulateef, Alaa Fareed and Al-Sharafi, Mohammed A. and Alkahtani, Ammar Ahmed},
TITLE = {A Review of Machine Learning and Deep Learning Techniques for Anomaly Detection in IoT Data},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5320},
URL = {https://www.mdpi.com/2076-3417/11/12/5320},
ISSN = {2076-3417},
ABSTRACT = {Anomaly detection has gained considerable attention in the past couple of years. Emerging technologies, such as the Internet of Things (IoT), are known to be among the most critical sources of data streams that produce massive amounts of data continuously from numerous applications. Examining these collected data to detect suspicious events can reduce functional threats and avoid unseen issues that cause downtime in the applications. Due to the dynamic nature of the data stream characteristics, many unresolved problems persist. In the existing literature, methods have been designed and developed to evaluate certain anomalous behaviors in IoT data stream sources. However, there is a lack of comprehensive studies that discuss all the aspects of IoT data processing. Thus, this paper attempts to fill this gap by providing a complete image of various state-of-the-art techniques on the major problems and core challenges in IoT data. The nature of data, anomaly types, learning mode, window model, datasets, and evaluation criteria are also presented. Research challenges related to data evolving, feature-evolving, windowing, ensemble approaches, nature of input data, data complexity and noise, parameters selection, data visualizations, heterogeneity of data, accuracy, and large-scale and high-dimensional data are investigated. Finally, the challenges that require substantial research efforts and future directions are summarized.},
DOI = {10.3390/app11125320}
}



@Article{rs13122237,
AUTHOR = {Alonso, Laura and Picos, Juan and Armesto, Julia},
TITLE = {Forest Land Cover Mapping at a Regional Scale Using Multi-Temporal Sentinel-2 Imagery and RF Models},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2237},
URL = {https://www.mdpi.com/2072-4292/13/12/2237},
ISSN = {2072-4292},
ABSTRACT = {Over the last several decades, thanks to improvements in and the diversification of open-access satellite imagery, land cover mapping techniques have evolved significantly. Notable changes in these techniques involve the automation of different steps, yielding promising results in terms of accuracy, class detection and efficiency. The most successful methodologies that have arisen rely on the use of multi-temporal data. Several different approaches have proven successful. In this study, one of the most recently developed methodologies is tested in the region of Galicia (in Northwestern Spain), with the aim of filling gaps in the mapping needs of the Galician forestry sector. The methodology mainly consists of performing a supervised classification of individual images from a selected time series and then combining them through aggregation using decision criteria. Several of the steps of the methodology can be addressed in multiple ways: pixel resolution selection, classification model building and aggregation methods. The effectiveness of these three tasks as well as some others are tested and evaluated and the most accurate and efficient parameters for the case study area are highlighted. The final land cover map that is obtained for Galicia has high accuracy metrics (an overall accuracy of 91.6%), which is in line with previous studies that have followed this methodology in other regions. This study has led to the development of an efficient open-access solution to support the mapping needs of the forestry sector.},
DOI = {10.3390/rs13122237}
}



@Article{data6060060,
AUTHOR = {Becerra, Miguel A. and Tobón, Catalina and Castro-Ospina, Andrés Eduardo and Peluffo-Ordóñez, Diego H.},
TITLE = {Information Quality Assessment for Data Fusion Systems},
JOURNAL = {Data},
VOLUME = {6},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {60},
URL = {https://www.mdpi.com/2306-5729/6/6/60},
ISSN = {2306-5729},
ABSTRACT = {This paper provides a comprehensive description of the current literature on data fusion, with an emphasis on Information Quality (IQ) and performance evaluation. This literature review highlights recent studies that reveal existing gaps, the need to find a synergy between data fusion and IQ, several research issues, and the challenges and pitfalls in this field. First, the main models, frameworks, architectures, algorithms, solutions, problems, and requirements are analyzed. Second, a general data fusion engineering process is presented to show how complex it is to design a framework for a specific application. Third, an IQ approach, as well as the different methodologies and frameworks used to assess IQ in information systems are addressed; in addition, data fusion systems are presented along with their related criteria. Furthermore, information on the context in data fusion systems and its IQ assessment are discussed. Subsequently, the issue of data fusion systems’ performance is reviewed. Finally, some key aspects and concluding remarks are outlined, and some future lines of work are gathered.},
DOI = {10.3390/data6060060}
}



@Article{s21123971,
AUTHOR = {de Oliveira, Gabriel Silva and Marcato Junior, José and Polidoro, Caio and Osco, Lucas Prado and Siqueira, Henrique and Rodrigues, Lucas and Jank, Liana and Barrios, Sanzio and Valle, Cacilda and Simeão, Rosângela and Carromeu, Camilo and Silveira, Eloise and  André de Castro Jorge, Lúcio and Gonçalves, Wesley and Santos, Mateus and Matsubara, Edson},
TITLE = {Convolutional Neural Networks to Estimate Dry Matter Yield in a Guineagrass Breeding Program Using UAV Remote Sensing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {3971},
URL = {https://www.mdpi.com/1424-8220/21/12/3971},
PubMedID = {34207543},
ISSN = {1424-8220},
ABSTRACT = {Forage dry matter is the main source of nutrients in the diet of ruminant animals. Thus, this trait is evaluated in most forage breeding programs with the objective of increasing the yield. Novel solutions combining unmanned aerial vehicles (UAVs) and computer vision are crucial to increase the efficiency of forage breeding programs, to support high-throughput phenotyping (HTP), aiming to estimate parameters correlated to important traits. The main goal of this study was to propose a convolutional neural network (CNN) approach using UAV-RGB imagery to estimate dry matter yield traits in a guineagrass breeding program. For this, an experiment composed of 330 plots of full-sib families and checks conducted at Embrapa Beef Cattle, Brazil, was used. The image dataset was composed of images obtained with an RGB sensor embedded in a Phantom 4 PRO. The traits leaf dry matter yield (LDMY) and total dry matter yield (TDMY) were obtained by conventional agronomic methodology and considered as the ground-truth data. Different CNN architectures were analyzed, such as AlexNet, ResNeXt50, DarkNet53, and two networks proposed recently for related tasks named MaCNN and LF-CNN. Pretrained AlexNet and ResNeXt50 architectures were also studied. Ten-fold cross-validation was used for training and testing the model. Estimates of DMY traits by each CNN architecture were considered as new HTP traits to compare with real traits. Pearson correlation coefficient r between real and HTP traits ranged from 0.62 to 0.79 for LDMY and from 0.60 to 0.76 for TDMY; root square mean error (RSME) ranged from 286.24 to 366.93 kg·ha−1 for LDMY and from 413.07 to 506.56 kg·ha−1 for TDMY. All the CNNs generated heritable HTP traits, except LF-CNN for LDMY and AlexNet for TDMY. Genetic correlations between real and HTP traits were high but varied according to the CNN architecture. HTP trait from ResNeXt50 pretrained achieved the best results for indirect selection regardless of the dry matter trait. This demonstrates that CNNs with remote sensing data are highly promising for HTP for dry matter yield traits in forage breeding programs.},
DOI = {10.3390/s21123971}
}



@Article{s21123982,
AUTHOR = {Lazzeri, Giacomo and Frodella, William and Rossi, Guglielmo and Moretti, Sandro},
TITLE = {Multitemporal Mapping of Post-Fire Land Cover Using Multiplatform PRISMA Hyperspectral and Sentinel-UAV Multispectral Data: Insights from Case Studies in Portugal and Italy},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {3982},
URL = {https://www.mdpi.com/1424-8220/21/12/3982},
PubMedID = {34207736},
ISSN = {1424-8220},
ABSTRACT = {Wildfires have affected global forests and the Mediterranean area with increasing recurrency and intensity in the last years, with climate change resulting in reduced precipitations and higher temperatures. To assess the impact of wildfires on the environment, burned area mapping has become progressively more relevant. Initially carried out via field sketches, the advent of satellite remote sensing opened new possibilities, reducing the cost uncertainty and safety of the previous techniques. In the present study an experimental methodology was adopted to test the potential of advanced remote sensing techniques such as multispectral Sentinel-2, PRISMA hyperspectral satellite, and UAV (unmanned aerial vehicle) remotely-sensed data for the multitemporal mapping of burned areas by soil–vegetation recovery analysis in two test sites in Portugal and Italy. In case study one, innovative multiplatform data classification was performed with the correlation between Sentinel-2 RBR (relativized burn ratio) fire severity classes and the scene hyperspectral signature, performed with a pixel-by-pixel comparison leading to a converging classification. In the adopted methodology, RBR burned area analysis and vegetation recovery was tested for accordance with biophysical vegetation parameters (LAI, fCover, and fAPAR). In case study two, a UAV-sensed NDVI index was adopted for high-resolution mapping data collection. At a large scale, the Sentinel-2 RBR index proved to be efficient for burned area analysis, from both fire severity and vegetation recovery phenomena perspectives. Despite the elapsed time between the event and the acquisition, PRISMA hyperspectral converging classification based on Sentinel-2 was able to detect and discriminate different spectral signatures corresponding to different fire severity classes. At a slope scale, the UAV platform proved to be an effective tool for mapping and characterizing the burned area, giving clear advantage with respect to filed GPS mapping. Results highlighted that UAV platforms, if equipped with a hyperspectral sensor and used in a synergistic approach with PRISMA, would create a useful tool for satellite acquired data scene classification, allowing for the acquisition of a ground truth.},
DOI = {10.3390/s21123982}
}



@Article{rs13122261,
AUTHOR = {Vayssade, Jehan-Antoine and Paoli, Jean-Noël and Gée, Christelle and Jones, Gawain},
TITLE = {DeepIndices: Remote Sensing Indices Based on Approximation of Functions through Deep-Learning, Application to Uncalibrated Vegetation Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2261},
URL = {https://www.mdpi.com/2072-4292/13/12/2261},
ISSN = {2072-4292},
ABSTRACT = {The form of a remote sensing index is generally empirically defined, whether by choosing specific reflectance bands, equation forms or its coefficients. These spectral indices are used as preprocessing stage before object detection/classification. But no study seems to search for the best form through function approximation in order to optimize the classification and/or segmentation. The objective of this study is to develop a method to find the optimal index, using a statistical approach by gradient descent on different forms of generic equations. From six wavebands images, five equations have been tested, namely: linear, linear ratio, polynomial, universal function approximator and dense morphological. Few techniques in signal processing and image analysis are also deployed within a deep-learning framework. Performances of standard indices and DeepIndices were evaluated using two metrics, the dice (similar to f1-score) and the mean intersection over union (mIoU) scores. The study focuses on a specific multispectral camera used in near-field acquisition of soil and vegetation surfaces. These DeepIndices are built and compared to 89 common vegetation indices using the same vegetation dataset and metrics. As an illustration the most used index for vegetation, NDVI (Normalized Difference Vegetation Indices) offers a mIoU score of 63.98% whereas our best models gives an analytic solution to reconstruct an index with a mIoU of 82.19%. This difference is significant enough to improve the segmentation and robustness of the index from various external factors, as well as the shape of detected elements.},
DOI = {10.3390/rs13122261}
}



@Article{rs13122273,
AUTHOR = {Meng, Xiangtian and Bao, Yilin and Ye, Qiang and Liu, Huanjun and Zhang, Xinle and Tang, Haitao and Zhang, Xiaohan},
TITLE = {Soil Organic Matter Prediction Model with Satellite Hyperspectral Image Based on Optimized Denoising Method},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2273},
URL = {https://www.mdpi.com/2072-4292/13/12/2273},
ISSN = {2072-4292},
ABSTRACT = {In order to improve the signal-to-noise ratio of the hyperspectral sensors and exploit the potential of satellite hyperspectral data for predicting soil properties, we took MingShui County as the study area, which the study area is approximately 1481 km2, and we selected Gaofen-5 (GF-5) satellite hyperspectral image of the study area to explore an applicable and accurate denoising method that can effectively improve the prediction accuracy of soil organic matter (SOM) content. First, fractional-order derivative (FOD) processing is performed on the original reflectance (OR) to evaluate the optimal FOD. Second, singular value decomposition (SVD), Fourier transform (FT) and discrete wavelet transform (DWT) are used to denoise the OR and optimal FOD reflectance. Third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination (RFE) algorithm. Finally, the SOM content is predicted by a random forest prediction model. The results reveal that 0.6-order reflectance describes more useful details in satellite hyperspectral data. Five spectral indexes extracted from the reflectance under different denoising methods have a strong correlation with the SOM content, which is helpful for realizing high-accuracy SOM predictions. All three denoising methods can reduce the noise in hyperspectral data, and the accuracies of the different denoising methods are ranked DWT &gt; FT &gt; SVD, where 0.6-order-DWT has the highest accuracy (R2 = 0.84, RMSE = 3.36 g kg−1, and RPIQ = 1.71). This paper is relatively novel, in that GF-5 satellite hyperspectral data based on different denoising methods are used to predict SOM, and the results provide a highly robust and novel method for mapping the spatial distribution of SOM content at the regional scale.},
DOI = {10.3390/rs13122273}
}



@Article{rs13122272,
AUTHOR = {Xiong, Jinghua and Guo, Shenglian and Yin, Jiabo},
TITLE = {Discharge Estimation Using Integrated Satellite Data and Hybrid Model in the Midstream Yangtze River},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2272},
URL = {https://www.mdpi.com/2072-4292/13/12/2272},
ISSN = {2072-4292},
ABSTRACT = {Remotely sensing data have advantages in filling spatiotemporal gaps of in situ observation networks, showing potential application for monitoring floods in data-sparse regions. By using the water level retrievals of Jason-2/3 altimetry satellites, this study estimates discharge at a 10-day timescale for the virtual station (VS) 012 and 077 across the midstream Yangtze River Basin during 2009–2016 based on the developed Manning formula. Moreover, we calibrate a hybrid model combined with Gravity Recovery and Climate Experiment (GRACE) data, by coupling the GR6J hydrological model with a machine learning model to simulate discharge. To physically capture the flood processes, the random forest (RF) model is employed to downscale the 10-day discharge into a daily scale. The results show that: (1) discharge estimates from the developed Manning formula show good accuracy for the VS012 and VS077 based on the improved Multi-subwaveform Multi-weight Threshold Retracker; (2) the combination of the GR6J and the LSTM models substantially improves the performance of the discharge estimates solely from either the GR6J or LSTM models; (3) RF-downscaled daily discharge demonstrates a general consistency with in situ data, where NSE/KGE between them are as high as 0.69/0.83. Our approach, based on multi-source remotely sensing data and machine learning techniques, may benefit flood monitoring in poorly gauged areas.},
DOI = {10.3390/rs13122272}
}



@Article{app11125410,
AUTHOR = {Zheng, Ke and Jia, Guozhu and Yang, Linchao and Wang, Jiaqing},
TITLE = {A Compound Fault Labeling and Diagnosis Method Based on Flight Data and BIT Record of UAV},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5410},
URL = {https://www.mdpi.com/2076-3417/11/12/5410},
ISSN = {2076-3417},
ABSTRACT = {In the process of Unmanned Aerial Vehicle (UAV) flight testing, plenty of compound faults exist, which could be composed of concurrent single faults or over-limit states alarmed by Built-In-Test (BIT) equipment. At present, there still lacks a suitable automatic labeling approach for UAV flight data, effectively utilizing the information of the BIT record. The performance of the originally employed flight data-driven fault diagnosis models based on machine learning needs to be improved as well. A compound fault labeling and diagnosis method based on actual flight data and the BIT record of the UAV during flight test phase is proposed, through labeling the flight data with compound fault modes corresponding to concurrent single faults recorded by the BIT system, and upgrading the original diagnosis model based on Gradient Boosting Decision Tree (GBDT) and Fully Convolutional Network (FCNN), to eXtreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and modified Convolutional Neural Network (CNN). The experimental results based on actual test flight data show that the proposed method could effectively label the flight data and obtain a significant improvement in diagnostic performance, appearing to be practical in the UAV test flight process.},
DOI = {10.3390/app11125410}
}



@Article{e23060737,
AUTHOR = {Sun, Fengjie and Wang, Xianchang and Zhang, Rui},
TITLE = {Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {737},
URL = {https://www.mdpi.com/1099-4300/23/6/737},
PubMedID = {34207944},
ISSN = {1099-4300},
ABSTRACT = {An Unmanned Aerial Vehicle (UAV) can greatly reduce manpower in the agricultural plant protection such as watering, sowing, and pesticide spraying. It is essential to develop a Decision-making Support System (DSS) for UAVs to help them choose the correct action in states according to the policy. In an unknown environment, the method of formulating rules for UAVs to help them choose actions is not applicable, and it is a feasible solution to obtain the optimal policy through reinforcement learning. However, experiments show that the existing reinforcement learning algorithms cannot get the optimal policy for a UAV in the agricultural plant protection environment. In this work we propose an improved Q-learning algorithm based on similar state matching, and we prove theoretically that there has a greater probability for UAV choosing the optimal action according to the policy learned by the algorithm we proposed than the classic Q-learning algorithm in the agricultural plant protection environment. This proposed algorithm is implemented and tested on datasets that are evenly distributed based on real UAV parameters and real farm information. The performance evaluation of the algorithm is discussed in detail. Experimental results show that the algorithm we proposed can efficiently learn the optimal policy for UAVs in the agricultural plant protection environment.},
DOI = {10.3390/e23060737}
}



@Article{s21124026,
AUTHOR = {Brandoli, Bruno and de Geus, André R. and Souza, Jefferson R. and Spadon, Gabriel and Soares, Amilcar and Rodrigues, Jose F. and Komorowski, Jerzy and Matwin, Stan},
TITLE = {Aircraft Fuselage Corrosion Detection Using Artificial Intelligence},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4026},
URL = {https://www.mdpi.com/1424-8220/21/12/4026},
PubMedID = {34207959},
ISSN = {1424-8220},
ABSTRACT = {Corrosion identification and repair is a vital task in aircraft maintenance to ensure continued structural integrity. Regarding fuselage lap joints, typically, visual inspections are followed by non-destructive methodologies, which are time-consuming. The visual inspection of large areas suffers not only from subjectivity but also from the variable probability of corrosion detection, which is aggravated by the multiple layers used in fuselage construction. In this paper, we propose a methodology for automatic image-based corrosion detection of aircraft structures using deep neural networks. For machine learning, we use a dataset that consists of D-Sight Aircraft Inspection System (DAIS) images from different lap joints of Boeing and Airbus aircrafts. We also employ transfer learning to overcome the shortage of aircraft corrosion images. With precision of over 93%, we demonstrate that our approach detects corrosion with a precision comparable to that of trained operators, aiding to reduce the uncertainties related to operator fatigue or inadequate training. Our results indicate that our methodology can support specialists and engineers in corrosion monitoring in the aerospace industry, potentially contributing to the automation of condition-based maintenance protocols.},
DOI = {10.3390/s21124026}
}



@Article{en14123457,
AUTHOR = {Burdziakowski, Pawel},
TITLE = {Polymodal Method of Improving the Quality of Photogrammetric Images and Models},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {3457},
URL = {https://www.mdpi.com/1996-1073/14/12/3457},
ISSN = {1996-1073},
ABSTRACT = {Photogrammetry using unmanned aerial vehicles has become very popular and is already commonly used. The most frequent photogrammetry products are an orthoimage, digital terrain model and a 3D object model. When executing measurement flights, it may happen that there are unsuitable lighting conditions, and the flight itself is fast and not very stable. As a result, noise and blur appear on the images, and the images themselves can have too low of a resolution to satisfy the quality requirements for a photogrammetric product. In such cases, the obtained images are useless or will significantly reduce the quality of the end-product of low-level photogrammetry. A new polymodal method of improving measurement image quality has been proposed to avoid such issues. The method discussed in this article removes degrading factors from the images and, as a consequence, improves the geometric and interpretative quality of a photogrammetric product. The author analyzed 17 various image degradation cases, developed 34 models based on degraded and recovered images, and conducted an objective analysis of the quality of the recovered images and models. As evidenced, the result was a significant improvement in the interpretative quality of the images themselves and a better geometry model.},
DOI = {10.3390/en14123457}
}



@Article{rs13122288,
AUTHOR = {Quan, Longzhe and Li, Hengda and Li, Hailong and Jiang, Wei and Lou, Zhaoxia and Chen, Liqing},
TITLE = {Two-Stream Dense Feature Fusion Network Based on RGB-D Data for the Real-Time Prediction of Weed Aboveground Fresh Weight in a Field Environment},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2288},
URL = {https://www.mdpi.com/2072-4292/13/12/2288},
ISSN = {2072-4292},
ABSTRACT = {The aboveground fresh weight of weeds is an important indicator that reflects their biomass and physiological activity and directly affects the criteria for determining the amount of herbicides to apply. In precision agriculture, the development of models that can accurately locate weeds and predict their fresh weight can provide visual support for accurate, variable herbicide application in real time. In this work, we develop a two-stream dense feature fusion convolutional network model based on RGB-D data for the real-time prediction of the fresh weight of weeds. A data collection method is developed for the compilation and production of RGB-D data sets. The acquired images undergo data enhancement, and a depth transformation data enhancement method suitable for depth data is proposed. The main idea behind the approach in this study is to use the YOLO-V4 model to locate weeds and use the two-stream dense feature fusion network to predict their aboveground fresh weight. In the two-stream dense feature fusion network, DenseNet and NiN methods are used to construct a Dense-NiN-Block structure for deep feature extraction and fusion. The Dense-NiN-Block module was embedded in five convolutional neural networks for comparison, and the best results were achieved with DenseNet201. The test results show that the predictive ability of the convolutional network using RGB-D as the input is better than that of the network using RGB as the input without the Dense-NiN-Block module. The mAP of the proposed network is 75.34% (IoU value of 0.5), the IoU is 86.36%, the detection speed of the fastest model with a RTX2080Ti NVIDIA graphics card is 17.8 fps, and the average relative error is approximately 4%. The model proposed in this paper can provide visual technical support for precise, variable herbicide application. The model can also provide a reference method for the non-destructive prediction of crop fresh weight in the field and can contribute to crop breeding and genetic improvement.},
DOI = {10.3390/rs13122288}
}



@Article{rs13122290,
AUTHOR = {Zhang, Tao and Tang, Hong and Ding, Yi and Li, Penglong and Ji, Chao and Xu, Penglei},
TITLE = {FSRSS-Net: High-Resolution Mapping of Buildings from Middle-Resolution Satellite Images Using a Super-Resolution Semantic Segmentation Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2290},
URL = {https://www.mdpi.com/2072-4292/13/12/2290},
ISSN = {2072-4292},
ABSTRACT = {Satellite mapping of buildings and built-up areas used to be delineated from high spatial resolution (e.g., meters or sub-meters) and middle spatial resolution (e.g., tens of meters or hundreds of meters) satellite images, respectively. To the best of our knowledge, it is important to explore a deep-learning approach to delineate high-resolution semantic maps of buildings from middle-resolution satellite images. The approach is termed as super-resolution semantic segmentation in this paper. Specifically, we design a neural network with integrated low-level image features of super-resolution and high-level semantic features of super-resolution, which is trained with Sentinel-2A images (i.e., 10 m) and higher-resolution semantic maps (i.e., 2.5 m). The network, based on super-resolution semantic segmentation features is called FSRSS-Net. In China, the 35 cities are partitioned into three groups, i.e., 19 cities for model training, four cities for quantitative testing and the other 12 cities for qualitative generalization ability analysis of the learned networks. A large-scale sample dataset is created and utilized to train and validate the performance of the FSRSS-Net, which includes 8597 training samples and 766 quantitative accuracy evaluation samples. Quantitative evaluation results show that: (1) based on the 10 m Sentinel-2A image, the FSRSS-Net can achieve super-resolution semantic segmentation and produce 2.5 m building recognition results, and there is little difference between the accuracy of 2.5 m results by FSRSS-Net and 10 m results by U-Net. More importantly, the 2.5 m building recognition results by FSRSS-Net have higher accuracy than the 2.5 m results by U-Net 10 m building recognition results interpolation up-sampling; (2) from the spatial visualization of the results, the building recognition results of 2.5 m are more precise than those of 10 m, and the outline of the building is better depicted. Qualitative analysis shows that: (1) the learned FSRSS-Net can be also well generalized to other cities that are far from training regions; (2) the FSRSS-Net can still achieve comparable results to the U-Net 2 m building recognition results, even when the U-Net is directly trained using both 2-meter resolution GF2 satellite images and corresponding semantic labels.},
DOI = {10.3390/rs13122290}
}



@Article{s21124043,
AUTHOR = {Zhang, Wentao and Liu, Yucheng and Zhang, Shaohui and Long, Tuzhi and Liang, Jinglun},
TITLE = {Error Fusion of Hybrid Neural Networks for Mechanical Condition Dynamic Prediction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4043},
URL = {https://www.mdpi.com/1424-8220/21/12/4043},
PubMedID = {34208262},
ISSN = {1424-8220},
ABSTRACT = {It is important for equipment to operate safely and reliably so that the working state of mechanical parts pushes forward an immense influence. Therefore, in order to enhance the dependability and security of mechanical equipment, to accurately predict the changing trend of mechanical components in advance plays a significant role. This paper introduces a novel condition prediction method, named error fusion of hybrid neural networks (EFHNN), by combining the error fusion of multiple sparse auto-encoders with convolutional neural networks for predicting the mechanical condition. First, to improve prediction accuracy, we can use the error fusion of multiple sparse auto-encoders to collect multi-feature information, and obtain a trend curve representing machine condition as well as a threshold line that can indicate the beginning of mechanical failure by computing the square prediction error (SPE). Then, convolutional neural networks predict the state of the machine according to the original data when the SPE value exceeds the threshold line. It can be seen from this result that the EFHNN method in the prediction of mechanical fault time series is available and superior.},
DOI = {10.3390/s21124043}
}



@Article{app11125468,
AUTHOR = {Shmalko, Elizaveta and Diveev, Askhat},
TITLE = {Control Synthesis as Machine Learning Control by Symbolic Regression Methods},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5468},
URL = {https://www.mdpi.com/2076-3417/11/12/5468},
ISSN = {2076-3417},
ABSTRACT = {The problem of control synthesis is considered as machine learning control. The paper proposes a mathematical formulation of machine learning control, discusses approaches of supervised and unsupervised learning by symbolic regression methods. The principle of small variation of the basic solution is presented to set up the neighbourhood of the search and to increase search efficiency of symbolic regression methods. Different symbolic regression methods such as genetic programming, network operator, Cartesian and binary genetic programming are presented in details. It is shown on the computational example the possibilities of symbolic regression methods as unsupervised machine learning control technique to the solution of MLC problem of control synthesis for obtaining the stabilization system for a mobile robot.},
DOI = {10.3390/app11125468}
}



@Article{agronomy11061202,
AUTHOR = {Yang, Baohua and Gao, Zhiwei and Gao, Yuan and Zhu, Yue},
TITLE = {Rapid Detection and Counting of Wheat Ears in the Field Using YOLOv4 with Attention Module},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1202},
URL = {https://www.mdpi.com/2073-4395/11/6/1202},
ISSN = {2073-4395},
ABSTRACT = {The detection and counting of wheat ears are very important for crop field management, yield estimation, and phenotypic analysis. Previous studies have shown that most methods for detecting wheat ears were based on shallow features such as color and texture extracted by machine learning methods, which have obtained good results. However, due to the lack of robustness of these features, it was difficult for the above-mentioned methods to meet the detection and counting of wheat ears in natural scenes. Other studies have shown that convolutional neural network (CNN) methods could be used to achieve wheat ear detection and counting. However, the adhesion and occlusion of wheat ears limit the accuracy of detection. Therefore, to improve the accuracy of wheat ear detection and counting in the field, an improved YOLOv4 (you only look once v4) with CBAM (convolutional block attention module) including spatial and channel attention model was proposed that could enhance the feature extraction capabilities of the network by adding receptive field modules. In addition, to improve the generalization ability of the model, not only local wheat data (WD), but also two public data sets (WEDD and GWHDD) were used to construct the training set, the validation set, and the test set. The results showed that the model could effectively overcome the noise in the field environment and realize accurate detection and counting of wheat ears with different density distributions. The average accuracy of wheat ear detection was 94%, 96.04%, and 93.11%. Moreover, the wheat ears were counted on 60 wheat images. The results showed that R2 = 0.8968 for WD, 0.955 for WEDD, and 0.9884 for GWHDD. In short, the CBAM-YOLOv4 model could meet the actual requirements of wheat ear detection and counting, which provided technical support for other high-throughput parameters of the extraction of crops.},
DOI = {10.3390/agronomy11061202}
}



@Article{rs13122318,
AUTHOR = {Lema, Darío G. and Pedrayes, Oscar D. and Usamentiaga, Rubén and García, Daniel F. and Alonso, Ángela},
TITLE = {Cost-Performance Evaluation of a Recognition Service of Livestock Activity Using Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2318},
URL = {https://www.mdpi.com/2072-4292/13/12/2318},
ISSN = {2072-4292},
ABSTRACT = {The recognition of livestock activity is essential to be eligible for subsides, to automatically supervise critical activities and to locate stray animals. In recent decades, research has been carried out into animal detection, but this paper also analyzes the detection of other key elements that can be used to verify the presence of livestock activity in a given terrain: manure piles, feeders, silage balls, silage storage areas, and slurry pits. In recent years, the trend is to apply Convolutional Neuronal Networks (CNN) as they offer significantly better results than those obtained by traditional techniques. To implement a livestock activity detection service, the following object detection algorithms have been evaluated: YOLOv2, YOLOv4, YOLOv5, SSD, and Azure Custom Vision. Since YOLOv5 offers the best results, producing a mean average precision (mAP) of 0.94, this detector is selected for the creation of a livestock activity recognition service. In order to deploy the service in the best infrastructure, the performance/cost ratio of various Azure cloud infrastructures are analyzed and compared with a local solution. The result is an efficient and accurate service that can help to identify the presence of livestock activity in a specified terrain.},
DOI = {10.3390/rs13122318}
}



@Article{s21124089,
AUTHOR = {Kim, Jingyeom and Lee, Joohyung and Kim, Taeyeon},
TITLE = {AdaMM: Adaptive Object Movement and Motion Tracking in Hierarchical Edge Computing System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4089},
URL = {https://www.mdpi.com/1424-8220/21/12/4089},
PubMedID = {34198526},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a novel adaptive object movement and motion tracking (AdaMM) framework in a hierarchical edge computing system for achieving GPU memory footprint reduction of deep learning (DL)-based video surveillance services. DL-based object movement and motion tracking requires a significant amount of resources, such as (1) GPU processing power for the inference phase and (2) GPU memory for model loading. Despite the absence of an object in the video, if the DL model is loaded, the GPU memory must be kept allocated for the loaded model. Moreover, in several cases, video surveillance tries to capture events that rarely occur (e.g., abnormal object behaviors); therefore, such standby GPU memory might be easily wasted. To alleviate this problem, the proposed AdaMM framework categorizes the tasks used for the object movement and motion tracking procedure in an increasing order of the required processing and memory resources as task (1) frame difference calculation, task (2) object detection, and task (3) object motion and movement tracking. The proposed framework aims to adaptively release the unnecessary standby object motion and movement tracking model to save GPU memory by utilizing light tasks, such as frame difference calculation and object detection in a hierarchical manner. Consequently, object movement and motion tracking are adaptively triggered if the object is detected within the specified threshold time; otherwise, the GPU memory for the model of task (3) can be released. Moreover, object detection is also adaptively performed if the frame difference over time is greater than the specified threshold. We implemented the proposed AdaMM framework using commercial edge devices by considering a three-tier system, such as the 1st edge node for both tasks (1) and (2), the 2nd edge node for task (3), and the cloud for sending a push alarm. A measurement-based experiment reveals that the proposed framework achieves a maximum GPU memory reduction of 76.8% compared to the baseline system, while requiring a 2680 ms delay for loading the model for object movement and motion tracking.},
DOI = {10.3390/s21124089}
}



@Article{geosciences11060256,
AUTHOR = {Kakavas, Maria P. and Nikolakopoulos, Konstantinos G.},
TITLE = {Digital Elevation Models of Rockfalls and Landslides: A Review and Meta-Analysis},
JOURNAL = {Geosciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {256},
URL = {https://www.mdpi.com/2076-3263/11/6/256},
ISSN = {2076-3263},
ABSTRACT = {The scope of this paper is to summarize previous research pertaining to the use of digital elevation models (DEMs) and digital terrain models (DTMs) in the study of rockfalls and landslides. Research from 1983 to 2020 was surveyed in order to understand how the spatial resolution of DEMs and DTMs affects landslide detection, validation, and mapping. Another major question examined was the relationship between the DEM resolution and the extent of the rockfall or landslide event. It emerged from the study that, for landslides, the majority of researchers used DEMs with a spatial resolution of between 10 m and 30 m, while for rockfalls, they used DEMs with a spatial resolution of between 5 m and 20 m. We concluded that DEMs with a very high resolution (less than 5 m) are suitable for local-scale occurrences, while medium-resolution (from 20 m to 30 m) DEMs are suitable for regional-scale events. High resolution is associated with high accuracy and detailed structural characteristics, while medium accuracy better illustrates the topographic features. A low pixel size (more than 90 m) is not recommended for this type of research. Susceptibility maps, inventory maps, hazard risk zones, and vulnerability assessments are some of the main tools used in landslide/rockfall investigations, and topographic indexes, methods, models, and software optimize the reliability of the results. All of these parameters are closely related to DEMs and DTMs as the cell size affects the credibility of the final outcome.},
DOI = {10.3390/geosciences11060256}
}



@Article{rs13122351,
AUTHOR = {Torresani, Alessandro and Menna, Fabio and Battisti, Roberto and Remondino, Fabio},
TITLE = {A V-SLAM Guided and Portable System for Photogrammetric Applications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2351},
URL = {https://www.mdpi.com/2072-4292/13/12/2351},
ISSN = {2072-4292},
ABSTRACT = {Mobile and handheld mapping systems are becoming widely used nowadays as fast and cost-effective data acquisition systems for 3D reconstruction purposes. While most of the research and commercial systems are based on active sensors, solutions employing only cameras and photogrammetry are attracting more and more interest due to their significantly minor costs, size and power consumption. In this work we propose an ARM-based, low-cost and lightweight stereo vision mobile mapping system based on a Visual Simultaneous Localization And Mapping (V-SLAM) algorithm. The prototype system, named GuPho (Guided Photogrammetric System), also integrates an in-house guidance system which enables optimized image acquisitions, robust management of the cameras and feedback on positioning and acquisition speed. The presented results show the effectiveness of the developed prototype in mapping large scenarios, enabling motion blur prevention, robust camera exposure control and achieving accurate 3D results.},
DOI = {10.3390/rs13122351}
}



@Article{rs13122352,
AUTHOR = {Geng, Liying and Che, Tao and Ma, Mingguo and Tan, Junlei and Wang, Haibo},
TITLE = {Corn Biomass Estimation by Integrating Remote Sensing and Long-Term Observation Data Based on Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2352},
URL = {https://www.mdpi.com/2072-4292/13/12/2352},
ISSN = {2072-4292},
ABSTRACT = {The accurate and timely estimation of regional crop biomass at different growth stages is of great importance in guiding crop management decision making. The recent availability of long time series of remote sensing data offers opportunities for crop monitoring. In this paper, four machine learning models, namely random forest (RF), support vector machine (SVM), artificial neural network (ANN), and extreme gradient boosting (XGBoost) were adopted to estimate the seasonal corn biomass based on field observation data and moderate resolution imaging spectroradiometer (MODIS) reflectance data from 2012 to 2019 in the middle reaches of the Heihe River basin, China. Nine variables were selected with the forward feature selection approach from among twenty-seven variables potentially influencing corn biomass: soil-adjusted total vegetation index (SATVI), green ratio vegetation index (GRVI), Nadir_B7 (2105–2155 nm), Nadir_B6 (1628–1652 nm), land surface water index (LSWI), normalized difference vegetation index (NDVI), Nadir_B4 (545–565 nm), and Nadir_B3 (459–479 nm). The results indicated that the corn biomass was suitably estimated (the coefficient of determination (R2) was between 0.72 and 0.78) with the four machine learning models. The XGBoost model performed better than the other three models (R2 = 0.78, root mean squared error (RMSE) = 2.86 t/ha and mean absolute error (MAE) = 1.86 t/ha). Moreover, the RF model was an effective method (R2 = 0.77, RMSE = 2.91 t/ha and MAE = 1.91 t/ha), with a performance comparable to that of the XGBoost model. This study provides a reference for estimating crop biomass from MOD43A4 datasets. In addition, the research demonstrates the potential of machine learning techniques to achieve a relatively accurate estimation of daily corn biomass at a large scale.},
DOI = {10.3390/rs13122352}
}



@Article{computers10060081,
AUTHOR = {Planke, Lars J. and Gardi, Alessandro and Sabatini, Roberto and Kistan, Trevor and Ezer, Neta},
TITLE = {Online Multimodal Inference of Mental Workload for Cognitive Human Machine Systems},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {81},
URL = {https://www.mdpi.com/2073-431X/10/6/81},
ISSN = {2073-431X},
ABSTRACT = {With increasingly higher levels of automation in aerospace decision support systems, it is imperative that the human operator maintains the required level of situational awareness in different operational conditions and a central role in the decision-making process. While current aerospace systems and interfaces are limited in their adaptability, a Cognitive Human Machine System (CHMS) aims to perform dynamic, real-time system adaptation by estimating the cognitive states of the human operator. Nevertheless, to reliably drive system adaptation of current and emerging aerospace systems, there is a need to accurately and repeatably estimate cognitive states, particularly for Mental Workload (MWL), in real-time. As part of this study, two sessions were performed during a Multi-Attribute Task Battery (MATB) scenario, including a session for offline calibration and validation and a session for online validation of eleven multimodal inference models of MWL. The multimodal inference model implemented included an Adaptive Neuro Fuzzy Inference System (ANFIS), which was used in different configurations to fuse data from an Electroencephalogram (EEG) model’s output, four eye activity features and a control input feature. The online validation of the ANFIS models produced good results, while the best performing model (containing all four eye activity features and the control input feature) showed an average Mean Absolute Error (MAE) = 0.67 ± 0.18 and Correlation Coefficient (CC) = 0.71 ± 0.15. The remaining six ANFIS models included data from the EEG model’s output, which had an offset discrepancy. This resulted in an equivalent offset for the online multimodal fusion. Nonetheless, the efficacy of these ANFIS models could be confirmed by the pairwise correlation with the task level, where one model demonstrated a CC = 0.77 ± 0.06, which was the highest among all of the ANFIS models tested. Hence, this study demonstrates the suitability for online multimodal fusion of features extracted from EEG signals, eye activity and control inputs to produce an accurate and repeatable inference of MWL.},
DOI = {10.3390/computers10060081}
}



@Article{agronomy11061227,
AUTHOR = {Linaza, Maria Teresa and Posada, Jorge and Bund, Jürgen and Eisert, Peter and Quartulli, Marco and Döllner, Jürgen and Pagani, Alain and G. Olaizola, Igor and Barriguinha, Andre and Moysiadis, Theocharis and Lucat, Laurent},
TITLE = {Data-Driven Artificial Intelligence Applications for Sustainable Precision Agriculture},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1227},
URL = {https://www.mdpi.com/2073-4395/11/6/1227},
ISSN = {2073-4395},
ABSTRACT = {One of the main challenges for the implementation of artificial intelligence (AI) in agriculture includes the low replicability and the corresponding difficulty in systematic data gathering, as no two fields are exactly alike. Therefore, the comparison of several pilot experiments in different fields, weather conditions and farming techniques enhances the collective knowledge. Thus, this work provides a summary of the most recent research activities in the form of research projects implemented and validated by the authors in several European countries, with the objective of presenting the already achieved results, the current investigations and the still open technical challenges. As an overall conclusion, it can be mentioned that even though in their primary stages in some cases, AI technologies improve decision support at farm level, monitoring conditions and optimizing production to allow farmers to apply the optimal number of inputs for each crop, thereby boosting yields and reducing water use and greenhouse gas emissions. Future extensions of this work will include new concepts based on autonomous and intelligent robots for plant and soil sample retrieval, and effective livestock management.},
DOI = {10.3390/agronomy11061227}
}



@Article{s21124150,
AUTHOR = {Siemiatkowska, Barbara and Stecz, Wojciech},
TITLE = {A Framework for Planning and Execution of Drone Swarm Missions in a Hostile Environment},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4150},
URL = {https://www.mdpi.com/1424-8220/21/12/4150},
PubMedID = {34204272},
ISSN = {1424-8220},
ABSTRACT = {This article presents a framework for planning a drone swarm mission in a hostile environment. Elements of the planning framework are discussed in detail, including methods of planning routes for drone swarms using mixed integer linear programming (MILP) and methods of detecting potentially dangerous objects using EO/IR camera images and synthetic aperture radar (SAR). Methods of detecting objects in the field are used in the mission planning process to re-plan the swarm’s flight paths. The route planning model is discussed using the example of drone formations managed by one UAV that communicates through another UAV with the ground control station (GCS). This article presents practical examples of using algorithms for detecting dangerous objects for re-planning of swarm routes. A novelty in the work is the development of these algorithms in such a way that they can be implemented on mobile computers used by UAVs and integrated with MILP tasks. The methods of detection and classification of objects in real time by UAVs equipped with SAR and EO/IR are presented. Different sensors require different methods to detect objects. In the case of infrared or optoelectronic sensors, a convolutional neural network is used. For SAR images, a rule-based system is applied. The experimental results confirm that the stream of images can be analyzed in real-time.},
DOI = {10.3390/s21124150}
}



@Article{drones5020052,
AUTHOR = {Lee, Thomas and Mckeever, Susan and Courtney, Jane},
TITLE = {Flying Free: A Research Overview of Deep Learning in Drone Navigation Autonomy},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2504-446X/5/2/52},
ISSN = {2504-446X},
ABSTRACT = {With the rise of Deep Learning approaches in computer vision applications, significant strides have been made towards vehicular autonomy. Research activity in autonomous drone navigation has increased rapidly in the past five years, and drones are moving fast towards the ultimate goal of near-complete autonomy. However, while much work in the area focuses on specific tasks in drone navigation, the contribution to the overall goal of autonomy is often not assessed, and a comprehensive overview is needed. In this work, a taxonomy of drone navigation autonomy is established by mapping the definitions of vehicular autonomy levels, as defined by the Society of Automotive Engineers, to specific drone tasks in order to create a clear definition of autonomy when applied to drones. A top–down examination of research work in the area is conducted, focusing on drone navigation tasks, in order to understand the extent of research activity in each area. Autonomy levels are cross-checked against the drone navigation tasks addressed in each work to provide a framework for understanding the trajectory of current research. This work serves as a guide to research in drone autonomy with a particular focus on Deep Learning-based solutions, indicating key works and areas of opportunity for development of this area in the future.},
DOI = {10.3390/drones5020052}
}



@Article{brainsci11060803,
AUTHOR = {Chai, Jie and Ruan, Xiaogang and Huang, Jing},
TITLE = {NLM-HS: Navigation Learning Model Based on a Hippocampal–Striatal Circuit for Explaining Navigation Mechanisms in Animal Brains},
JOURNAL = {Brain Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {803},
URL = {https://www.mdpi.com/2076-3425/11/6/803},
PubMedID = {34204482},
ISSN = {2076-3425},
ABSTRACT = {Neurophysiological studies have shown that the hippocampus, striatum, and prefrontal cortex play different roles in animal navigation, but it is still less clear how these structures work together. In this paper, we establish a navigation learning model based on the hippocampal–striatal circuit (NLM-HS), which provides a possible explanation for the navigation mechanism in the animal brain. The hippocampal model generates a cognitive map of the environment and performs goal-directed navigation by using a place cell sequence planning algorithm. The striatal model performs reward-related habitual navigation by using the classic temporal difference learning algorithm. Since the two models may produce inconsistent behavioral decisions, the prefrontal cortex model chooses the most appropriate strategies by using a strategy arbitration mechanism. The cognitive and learning mechanism of the NLM-HS works in two stages of exploration and navigation. First, the agent uses a hippocampal model to construct the cognitive map of the unknown environment. Then, the agent uses the strategy arbitration mechanism in the prefrontal cortex model to directly decide which strategy to choose. To test the validity of the NLM-HS, the classical Tolman detour experiment was reproduced. The results show that the NLM-HS not only makes agents show environmental cognition and navigation behavior similar to animals, but also makes behavioral decisions faster and achieves better adaptivity than hippocampal or striatal models alone.},
DOI = {10.3390/brainsci11060803}
}



@Article{app11125621,
AUTHOR = {An, Kang and Chen, Yixin and Wang, Suhong and Xiao, Zhifeng},
TITLE = {RCBi-CenterNet: An Absolute Pose Policy for 3D Object Detection in Autonomous Driving},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5621},
URL = {https://www.mdpi.com/2076-3417/11/12/5621},
ISSN = {2076-3417},
ABSTRACT = {3D Object detection is a critical mission of the perception system of a self-driving vehicle. Existing bounding box-based methods are hard to train due to the need to remove duplicated detections in the post-processing stage. In this paper, we propose a center point-based deep neural network (DNN) architecture named RCBi-CenterNet that predicts the absolute pose for each detected object in the 3D world space. RCBi-CenterNet is composed of a recursive composite network with a dual-backbone feature extractor and a bi-directional feature pyramid network (BiFPN) for cross-scale feature fusion. In the detection head, we predict a confidence heatmap that is used to determine the position of detected objects. The other pose information, including depth and orientation, is regressed. We conducted extensive experiments on the Peking University/Baidu-Autonomous Driving dataset, which contains more than 60,000 labeled 3D vehicle instances from 5277 real-world images, and each vehicle object is annotated with the absolute pose described by the six degrees of freedom (6DOF). We validated the design choices of various data augmentation methods and the backbone options. Through an ablation study and an overall comparison with the state-of-the-art (SOTA), namely CenterNet, we showed that the proposed RCBi-CenterNet presents performance gains of 2.16%, 2.76%, and 5.24% in Top 1, Top 3, and Top 10 mean average precision (mAP). The model and the result could serve as a credible benchmark for future research in center point-based object detection.},
DOI = {10.3390/app11125621}
}



@Article{rs13122377,
AUTHOR = {Huang, Yixin and Mu, Zhongcheng and Wu, Shufan and Cui, Benjie and Duan, Yuxiao},
TITLE = {Revising the Observation Satellite Scheduling Problem Based on Deep Reinforcement Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2377},
URL = {https://www.mdpi.com/2072-4292/13/12/2377},
ISSN = {2072-4292},
ABSTRACT = {Earth observation satellite task scheduling research plays a key role in space-based remote sensing services. An effective task scheduling strategy can maximize the utilization of satellite resources and obtain larger objective observation profits. In this paper, inspired by the success of deep reinforcement learning in optimization domains, the deep deterministic policy gradient algorithm is adopted to solve a time-continuous satellite task scheduling problem. Moreover, an improved graph-based minimum clique partition algorithm is proposed for preprocessing in the task clustering phase by considering the maximum task priority and the minimum observation slewing angle under constraint conditions. Experimental simulation results demonstrate that the deep reinforcement learning-based task scheduling method is feasible and performs much better than traditional metaheuristic optimization algorithms, especially in large-scale problems.},
DOI = {10.3390/rs13122377}
}



@Article{rs13122379,
AUTHOR = {Arumugam, Ponraj and Chemura, Abel and Schauberger, Bernhard and Gornott, Christoph},
TITLE = {Remote Sensing Based Yield Estimation of Rice (Oryza Sativa L.) Using Gradient Boosted Regression in India},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2379},
URL = {https://www.mdpi.com/2072-4292/13/12/2379},
ISSN = {2072-4292},
ABSTRACT = {Accurate and spatially explicit yield information is required to ensure farmers’ income and food security at local and national levels. Current approaches based on crop cutting experiments are expensive and usually too late for timely income stabilization measures like crop insurances. We, therefore, utilized a Gradient Boosted Regression (GBR), a machine learning technique, to estimate rice yields at ~500 m spatial resolution for rice-producing areas in India with potential application for near real-time estimates. We used resampled intermediate resolution (~5 km) images of the Moderate Resolution Imaging Spectroradiometer (MODIS) Leaf Area Index (LAI) and observed yields at the district level in India for calibrating GBR models. These GBRs were then used to downscale district yields to 500 m resolution. Downscaled yields were re-aggregated for validation against out-of-sample district yields not used for model training and an additional independent data set of block-level (below district-level) yields. Our downscaled and re-aggregated yields agree well with reported district-level observations from 2003 to 2015 (r = 0.85 &amp; MAE = 0.15 t/ha). The model performance improved further when estimating separate models for different rice cropping densities (up to r = 0.93). An additional out-of-sample validation for the years 2016 and 2017, proved successful with r = 0.84 and r = 0.77, respectively. Simulated yield accuracy was higher in water-limited, rainfed agricultural systems. We conclude that this downscaling approach of rice yield estimation using GBR is feasible across India and may complement current approaches for timely rice yield estimation required by insurance companies and government agencies.},
DOI = {10.3390/rs13122379}
}



@Article{s21124186,
AUTHOR = {Tanveer, Muhammad Hassan and Thomas, Antony and Ahmed, Waqar and Zhu, Hongxiao},
TITLE = {Estimate the Unknown Environment with Biosonar Echoes—A Simulation Study},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4186},
URL = {https://www.mdpi.com/1424-8220/21/12/4186},
PubMedID = {34207193},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have shown great potential in various applications such as surveillance, search and rescue. To perform safe and efficient navigation, it is vitally important for a UAV to evaluate the environment accurately and promptly. In this work, we present a simulation study for the estimation of foliage distribution as a UAV equipped with biosonar navigates through a forest. Based on a simulated forest environment, foliage echoes are generated by using a bat-inspired bisonar simulator. These biosonar echoes are then used to estimate the spatial distribution of both sparsely and densely distributed tree leaves. While a simple batch processing method is able to estimate sparsely distributed leaf locations well, a wavelet scattering technique coupled with a support vector machine (SVM) classifier is shown to be effective to estimate densely distributed leaves. Our approach is validated by using multiple setups of leaf distributions in the simulated forest environment. Ninety-seven percent accuracy is obtained while estimating thickly distributed foliage.},
DOI = {10.3390/s21124186}
}



@Article{s21124195,
AUTHOR = {Pilipović, Ratko and Risojević, Vladimir and Božič, Janko and Bulić, Patricio and Lotrič, Uroš},
TITLE = {An Approximate GEMM Unit for Energy-Efficient Object Detection},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4195},
URL = {https://www.mdpi.com/1424-8220/21/12/4195},
PubMedID = {34207295},
ISSN = {1424-8220},
ABSTRACT = {Edge computing brings artificial intelligence algorithms and graphics processing units closer to data sources, making autonomy and energy-efficient processing vital for their design. Approximate computing has emerged as a popular strategy for energy-efficient circuit design, where the challenge is to achieve the best tradeoff between design efficiency and accuracy. The essential operation in artificial intelligence algorithms is the general matrix multiplication (GEMM) operation comprised of matrix multiplication and accumulation. This paper presents an approximate general matrix multiplication (AGEMM) unit that employs approximate multipliers to perform matrix–matrix operations on four-by-four matrices given in sixteen-bit signed fixed-point format. The synthesis of the proposed AGEMM unit to the 45 nm Nangate Open Cell Library revealed that it consumed only up to 36% of the area and 25% of the energy required by the exact general matrix multiplication unit. The AGEMM unit is ideally suited to convolutional neural networks, which can adapt to the error induced in the computation. We evaluated the AGEMM units’ usability for honeybee detection with the YOLOv4-tiny convolutional neural network. The results implied that we can deploy the AGEMM units in convolutional neural networks without noticeable performance degradation. Moreover, the AGEMM unit’s employment can lead to more area- and energy-efficient convolutional neural network processing, which in turn could prolong sensors’ and edge nodes’ autonomy.},
DOI = {10.3390/s21124195}
}



@Article{rs13122401,
AUTHOR = {Albuquerque, Rafael Walter and Ferreira, Manuel Eduardo and Olsen, Søren Ingvor and Tymus, Julio Ricardo Caetano and Balieiro, Cintia Palheta and Mansur, Hendrik and Moura, Ciro José Ribeiro and Costa, João Vitor Silva and Branco, Maurício Ruiz Castello and Grohmann, Carlos Henrique},
TITLE = {Forest Restoration Monitoring Protocol with a Low-Cost Remotely Piloted Aircraft: Lessons Learned from a Case Study in the Brazilian Atlantic Forest},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2401},
URL = {https://www.mdpi.com/2072-4292/13/12/2401},
ISSN = {2072-4292},
ABSTRACT = {Traditional forest restoration (FR) monitoring methods employ spreadsheets and photos taken at the ground level. Since remotely piloted aircraft (RPA) generate a panoramic high resolution and georeferenced view of the entire area of interest, this technology has high potential to improve the traditional FR monitoring methods. This study evaluates how low-cost RPA data may contribute to FR monitoring of the Brazilian Atlantic Forest by the automatic remote measurement of Tree Density, Tree Height, Vegetation Cover (area covered by trees), and Grass Infestation. The point cloud data was processed to map the Tree Density, Tree Height, and Vegetation Cover parameters. The orthomosaic was used for a Random Forest classification that considered trees and grasses as a single land cover class. The Grass Infestation parameter was mapped by the difference between this land cover class (which considered trees and grasses) and the Vegetation Cover results (obtained by the point cloud data processing). Tree Density, Vegetation Cover, and Grass Infestation parameters presented F_scores of 0.92, 0.85, and 0.64, respectively. Tree Height accuracy was indicated by the Error Percentage considering the traditional fieldwork and the RPA results. The Error Percentage was equal to 0.13 and was considered accurate because it estimated a 13% shorter height for trees that averaged 1.93 m tall. Thus, this study showed that the FR structural parameters were accurately measured by the low-cost RPA, a technology that contributes to FR monitoring. Despite accurately measuring the structural parameters, this study reinforced the challenge of measuring the Biodiversity parameter via remote sensing because the classification of tree species was not possible. After all, the Brazilian Atlantic Forest is a biodiversity hotspot, and thus different species have similar spectral responses in the visible spectrum and similar geometric forms. Therefore, until improved automatic classification methods become available for tree species, traditional fieldwork remains necessary for a complete FR monitoring diagnostic.},
DOI = {10.3390/rs13122401}
}



@Article{rs13122404,
AUTHOR = {Perich, Gregor and Aasen, Helge and Verrelst, Jochem and Argento, Francesco and Walter, Achim and Liebisch, Frank},
TITLE = {Crop Nitrogen Retrieval Methods for Simulated Sentinel-2 Data Using In-Field Spectrometer Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2404},
URL = {https://www.mdpi.com/2072-4292/13/12/2404},
ISSN = {2072-4292},
ABSTRACT = {Nitrogen (N) is one of the key nutrients supplied in agricultural production worldwide. Over-fertilization can have negative influences on the field and the regional level (e.g., agro-ecosystems). Remote sensing of the plant N of field crops presents a valuable tool for the monitoring of N flows in agro-ecosystems. Available data for validation of satellite-based remote sensing of N is scarce. Therefore, in this study, field spectrometer measurements were used to simulate data of the Sentinel-2 (S2) satellites developed for vegetation monitoring by the ESA. The prediction performance of normalized ratio indices (NRIs), random forest regression (RFR) and Gaussian processes regression (GPR) for plant-N-related traits was assessed on a diverse real-world dataset including multiple crops, field sites and years. The plant N traits included the mass-based N measure, N concentration in the biomass (Nconc), and an area-based N measure approximating the plant N uptake (NUP). Spectral indices such as normalized ratio indices (NRIs) performed well, but the RFR and GPR methods outperformed the NRIs. Key spectral bands for each trait were identified using the RFR variable importance measure and the Gaussian processes regression band analysis tool (GPR-BAT), highlighting the importance of the short-wave infrared (SWIR) region for estimation of plant Nconc—and to a lesser extent the NUP. The red edge (RE) region was also important. The GPR-BAT showed that five bands were sufficient for plant N trait and leaf area index (LAI) estimation and that a surplus of bands effectively reduced prediction performance. A global sensitivity analysis (GSA) was performed on all traits simultaneously, showing the dominance of the LAI in the mixed remote sensing signal. To delineate the plant-N-related traits from this signal, regional and/or national data collection campaigns producing large crop spectral libraries (CSL) are needed. An improved database will likely enable the mapping of N at the agro-ecosystem level or for use in precision farming by farmers in the future.},
DOI = {10.3390/rs13122404}
}



@Article{rs13122408,
AUTHOR = {Tian, Luo and Qu, Yonghua and Qi, Jianbo},
TITLE = {Estimation of Forest LAI Using Discrete Airborne LiDAR: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2408},
URL = {https://www.mdpi.com/2072-4292/13/12/2408},
ISSN = {2072-4292},
ABSTRACT = {The leaf area index (LAI) is an essential input parameter for quantitatively studying the energy and mass balance in soil-vegetation-atmosphere transfer systems. As an active remote sensing technology, light detection and ranging (LiDAR) provides a new method to describe forest canopy LAI. This paper reviewed the primary LAI retrieval methods using point cloud data (PCD) obtained by discrete airborne LiDAR scanner (DALS), its validation scheme, and its limitations. There are two types of LAI retrieval methods based on DALS PCD, i.e., the empirical regression and the gap fraction (GF) model. In the empirical model, tree height-related variables, LiDAR penetration indexes (LPIs), and canopy cover are the most widely used proxy variables. The height-related proxies are used most frequently; however, the LPIs proved the most efficient proxy. The GF model based on the Beer-Lambert law has been proven useful to estimate LAI; however, the suitability of LPIs is site-, tree species-, and LiDAR system-dependent. In the local validation in previous studies, poor scalability of both empirical and GF models in time, space, and across different DALS systems was observed, which means that field measurements are still needed to calibrate both types of models. The method to correct the impact from the clumping effect and woody material using DALS PCD and the saturation effect for both empirical and GF models still needs further exploration. Of most importance, further work is desired to emphasize assessing the transferability of published methods to new geographic contexts, different DALS sensors, and survey characteristics, based on figuring out the influence of each factor on the LAI retrieval process using DALS PCD. In addition, from a methodological perspective, taking advantage of DALS PCD in characterizing the 3D structure of the canopy, making full use of the ability of machine learning methods in the fusion of multisource data, developing a spatiotemporal scalable model of canopy structure parameters including LAI, and using multisource and heterogeneous data are promising areas of research.},
DOI = {10.3390/rs13122408}
}



@Article{agriculture11060563,
AUTHOR = {Li, Minhui and Shamshiri, Redmond R. and Schirrmann, Michael and Weltzien, Cornelia},
TITLE = {Impact of Camera Viewing Angle for Estimating Leaf Parameters of Wheat Plants from 3D Point Clouds},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {563},
URL = {https://www.mdpi.com/2077-0472/11/6/563},
ISSN = {2077-0472},
ABSTRACT = {Estimation of plant canopy using low-altitude imagery can help monitor the normal growth status of crops and is highly beneficial for various digital farming applications such as precision crop protection. However, extracting 3D canopy information from raw images requires studying the effect of sensor viewing angle by taking into accounts the limitations of the mobile platform routes inside the field. The main objective of this research was to estimate wheat (Triticum aestivum L.) leaf parameters, including leaf length and width, from the 3D model representation of the plants. For this purpose, experiments with different camera viewing angles were conducted to find the optimum setup of a mono-camera system that would result in the best 3D point clouds. The angle-control analytical study was conducted on a four-row wheat plot with a row spacing of 0.17 m and with two seeding densities and growth stages as factors. Nadir and six oblique view image datasets were acquired from the plot with 88% overlapping and were then reconstructed to point clouds using Structure from Motion (SfM) and Multi-View Stereo (MVS) methods. Point clouds were first categorized into three classes as wheat canopy, soil background, and experimental plot. The wheat canopy class was then used to extract leaf parameters, which were then compared with those values from manual measurements. The comparison between results showed that (i) multiple-view dataset provided the best estimation for leaf length and leaf width, (ii) among the single-view dataset, canopy, and leaf parameters were best modeled with angles vertically at −45° and horizontally at 0° (VA −45, HA 0), while (iii) in nadir view, fewer underlying 3D points were obtained with a missing leaf rate of 70%. It was concluded that oblique imagery is a promising approach to effectively estimate wheat canopy 3D representation with SfM-MVS using a single camera platform for crop monitoring. This study contributes to the improvement of the proximal sensing platform for crop health assessment.},
DOI = {10.3390/agriculture11060563}
}



@Article{s21124226,
AUTHOR = {Baek, Seongmin and Jung, Yunho and Lee, Seongjoo},
TITLE = {Signal Expansion Method in Indoor FMCW Radar Systems for Improving Range Resolution},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4226},
URL = {https://www.mdpi.com/1424-8220/21/12/4226},
PubMedID = {34203035},
ISSN = {1424-8220},
ABSTRACT = {As various unmanned autonomous driving technologies such as autonomous vehicles and autonomous driving drones are being developed, research on FMCW radar, a sensor related to these technologies, is actively being conducted. The range resolution, which is a parameter for accurately detecting an object in the FMCW radar system, depends on the modulation bandwidth. Expensive radars have a large modulation bandwidth, use the band above 77 GHz, and are mainly used as in-vehicle radar sensors. However, these high-performance radars have the disadvantage of being expensive and burdensome for use in areas that require precise sensors, such as indoor environment motion detection and autonomous drones. In this paper, the range resolution is improved beyond the limited modulation bandwidth by extending the beat frequency signal in the time domain through the proposed Adaptive Mirror Padding and Phase Correction Padding. The proposed algorithm has similar performance in the existing Zero Padding, Mirror Padding, and Range RMSE, but improved results were confirmed through the ρs indicating the size of the side lobe compared to the main lobe and the accurate detection rate of the OS CFAR. In the case of ρs, it was confirmed that with single targets, Adaptive Mirror Padding was improved by about 3 times and Phase Correct Padding was improved by about 6 times compared to the existing algorithm. The results of the OS CFAR were divided into single targets and multiple targets to confirm the performance. In single targets, Adaptive Mirror Padding improved by about 10% and Phase Correct Padding by about 20% compared to the existing algorithm. In multiple targets, Phase Correct Padding improved by about 20% compared to the existing algorithm. The proposed algorithm was verified through the MATLAB Tool and the actual FMCW radar. As the results were similar in the two experimental environments, it was verified that the algorithm works in real radar as well.},
DOI = {10.3390/s21124226}
}



@Article{ijerph18136705,
AUTHOR = {Pishgar, Maryam and Issa, Salah Fuad and Sietsema, Margaret and Pratap, Preethi and Darabi, Houshang},
TITLE = {REDECA: A Novel Framework to Review Artificial Intelligence and Its Applications in Occupational Safety and Health},
JOURNAL = {International Journal of Environmental Research and Public Health},
VOLUME = {18},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6705},
URL = {https://www.mdpi.com/1660-4601/18/13/6705},
PubMedID = {34206378},
ISSN = {1660-4601},
ABSTRACT = {Introduction: The field of artificial intelligence (AI) is rapidly expanding, with many applications seen routinely in health care, industry, and education, and increasingly in workplaces. Although there is growing evidence of applications of AI in workplaces across all industries to simplify and/or automate tasks there is a limited understanding of the role that AI contributes in addressing occupational safety and health (OSH) concerns. Methods: This paper introduces a new framework called Risk Evolution, Detection, Evaluation, and Control of Accidents (REDECA) that highlights the role that AI plays in the anticipation and control of exposure risks in a worker’s immediate environment. Two hundred and sixty AI papers across five sectors (oil and gas, mining, transportation, construction, and agriculture) were reviewed using the REDECA framework to highlight current applications and gaps in OSH and AI fields. Results: The REDECA framework highlighted the unique attributes and research focus of each of the five industrial sectors. The majority of evidence of AI in OSH research within the oil/gas and transportation sectors focused on the development of sensors to detect hazardous situations. In construction the focus was on the use of sensors to detect incidents. The research in the agriculture sector focused on sensors and actuators that removed workers from hazardous conditions. Application of the REDECA framework highlighted AI/OSH strengths and opportunities in various industries and potential areas for collaboration. Conclusions: As AI applications across industries continue to increase, further exploration of the benefits and challenges of AI applications in OSH is needed to optimally protect worker health, safety and well-being.},
DOI = {10.3390/ijerph18136705}
}



@Article{rs13132436,
AUTHOR = {Calamita, Federico and Imran, Hafiz Ali and Vescovo, Loris and Mekhalfi, Mohamed Lamine and La Porta, Nicola},
TITLE = {Early Identification of Root Rot Disease by Using Hyperspectral Reflectance: The Case of Pathosystem Grapevine/Armillaria},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2436},
URL = {https://www.mdpi.com/2072-4292/13/13/2436},
ISSN = {2072-4292},
ABSTRACT = {Armillaria genus represents one of the most common causes of chronic root rot disease in woody plants. Prompt recognition of diseased plants is crucial to control the pathogen. However, the current disease detection methods are limited at a field scale. Therefore, an alternative approach is needed. In this study, we investigated the potential of hyperspectral techniques to identify fungi-infected vs. healthy plants of Vitis vinifera. We used the hyperspectral imaging sensor Specim-IQ to acquire leaves’ reflectance data of the Teroldego Rotaliano grapevine cultivar. We analyzed three different groups of plants: healthy, asymptomatic, and diseased. Highly significant differences were found in the near-infrared (NIR) spectral region with a decreasing pattern from healthy to diseased plants attributable to the leaf mesophyll changes. Asymptomatic plants emerged from the other groups due to a lower reflectance in the red edge spectrum (around 705 nm), ascribable to an accumulation of secondary metabolites involved in plant defense strategies. Further significant differences were observed in the wavelengths close to 550 nm in diseased vs. asymptomatic plants. We evaluated several machine learning paradigms to differentiate the plant groups. The Naïve Bayes (NB) algorithm, combined with the most discriminant variables among vegetation indices and spectral narrow bands, provided the best results with an overall accuracy of 90% and 75% in healthy vs. diseased and healthy vs. asymptomatic plants, respectively. To our knowledge, this study represents the first report on the possibility of using hyperspectral data for root rot disease diagnosis in woody plants. Although further validation studies are required, it appears that the spectral reflectance technique, possibly implemented on unmanned aerial vehicles (UAVs), could be a promising tool for a cost-effective, non-invasive method of Armillaria disease diagnosis and mapping in-field, contributing to a significant step forward in precision viticulture.},
DOI = {10.3390/rs13132436}
}



@Article{electronics10131513,
AUTHOR = {Aziez, Sameir A. and Al-Hemeary, Nawar and Reja, Ahmed Hameed and Zsedrovits, Tamás and Cserey, György},
TITLE = {Using KNN Algorithm Predictor for Data Synchronization of Ultra-Tight GNSS/INS Integration},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1513},
URL = {https://www.mdpi.com/2079-9292/10/13/1513},
ISSN = {2079-9292},
ABSTRACT = {The INS system’s update rate is faster than that of the GNSS receiver. Additionally, GNSS receiver data may suffer from blocking for a few seconds for different reasons, affecting architecture integrations between GNSS and INS. This paper proposes a novel GNSS data prediction method using the k nearest neighbor (KNN) predictor algorithm to treat data synchronization between the INS sensors and GNSS receiver and overcome those GNSS receiver’s blocking, which may occur for a few seconds. The experimental work was conducted on a flying drone over a minor Hungarian (Mátyásföld, 47.4992 N, 19.1977 E) model airfield. The GNSS data are predicted by four different scenarios: the first is no blocking of data, and the other three have blocking periods of 1, 4, and 8 s, respectively. Ultra-tight architecture integration is used to perform the GNSS/INS integration to deal with the INS sensors’ inaccuracy and their divergence throughout the operation. The results show that using the GNSS/INS integration system yields better positioning data (in three axes (X, Y, and Z)) than using a stand-alone INS system or GNSS without a predictor.},
DOI = {10.3390/electronics10131513}
}



@Article{s21134281,
AUTHOR = {López-Ardao, J. Carlos and Rodríguez-Rubio, Raúl F. and Suárez-González, Andrés and Rodríguez-Pérez, Miguel and Sousa-Vieira, M. Estrella},
TITLE = {Current Trends on Green Wireless Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4281},
URL = {https://www.mdpi.com/1424-8220/21/13/4281},
PubMedID = {34201485},
ISSN = {1424-8220},
ABSTRACT = {The issue of energy balancing in Wireless Sensor Networks is a pivotal one, crucial in their deployment. This problem can be subdivided in three areas: (i) energy conservation techniques, usually implying minimizing the cost of communication at the nodes since it is known that the radio is the biggest consumer of the available energy; (ii) energy-harvesting techniques, converting energy from not full-time available environmental sources and usually storing it; and (iii) energy transfer techniques, sharing energy resources from one node (either specialized or not) to another one. In this article, we survey the main contributions in these three areas and identify the main trending topics in recent research. A discussion and some future directions are also included.},
DOI = {10.3390/s21134281}
}



@Article{rs13132450,
AUTHOR = {Maxwell, Aaron E. and Warner, Timothy A. and Guillén, Luis Andrés},
TITLE = {Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies—Part 1: Literature Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2450},
URL = {https://www.mdpi.com/2072-4292/13/13/2450},
ISSN = {2072-4292},
ABSTRACT = {Convolutional neural network (CNN)-based deep learning (DL) is a powerful, recently developed image classification approach. With origins in the computer vision and image processing communities, the accuracy assessment methods developed for CNN-based DL use a wide range of metrics that may be unfamiliar to the remote sensing (RS) community. To explore the differences between traditional RS and DL RS methods, we surveyed a random selection of 100 papers from the RS DL literature. The results show that RS DL studies have largely abandoned traditional RS accuracy assessment terminology, though some of the accuracy measures typically used in DL papers, most notably precision and recall, have direct equivalents in traditional RS terminology. Some of the DL accuracy terms have multiple names, or are equivalent to another measure. In our sample, DL studies only rarely reported a complete confusion matrix, and when they did so, it was even more rare that the confusion matrix estimated population properties. On the other hand, some DL studies are increasingly paying attention to the role of class prevalence in designing accuracy assessment approaches. DL studies that evaluate the decision boundary threshold over a range of values tend to use the precision-recall (P-R) curve, the associated area under the curve (AUC) measures of average precision (AP) and mean average precision (mAP), rather than the traditional receiver operating characteristic (ROC) curve and its AUC. DL studies are also notable for testing the generalization of their models on entirely new datasets, including data from new areas, new acquisition times, or even new sensors.},
DOI = {10.3390/rs13132450}
}



@Article{diagnostics11071155,
AUTHOR = {El-Rashidy, Nora and Abdelrazik, Samir and Abuhmed, Tamer and Amer, Eslam and Ali, Farman and Hu, Jong-Wan and El-Sappagh, Shaker},
TITLE = {Comprehensive Survey of Using Machine Learning in the COVID-19 Pandemic},
JOURNAL = {Diagnostics},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1155},
URL = {https://www.mdpi.com/2075-4418/11/7/1155},
PubMedID = {34202587},
ISSN = {2075-4418},
ABSTRACT = {Since December 2019, the global health population has faced the rapid spreading of coronavirus disease (COVID-19). With the incremental acceleration of the number of infected cases, the World Health Organization (WHO) has reported COVID-19 as an epidemic that puts a heavy burden on healthcare sectors in almost every country. The potential of artificial intelligence (AI) in this context is difficult to ignore. AI companies have been racing to develop innovative tools that contribute to arm the world against this pandemic and minimize the disruption that it may cause. The main objective of this study is to survey the decisive role of AI as a technology used to fight against the COVID-19 pandemic. Five significant applications of AI for COVID-19 were found, including (1) COVID-19 diagnosis using various data types (e.g., images, sound, and text); (2) estimation of the possible future spread of the disease based on the current confirmed cases; (3) association between COVID-19 infection and patient characteristics; (4) vaccine development and drug interaction; and (5) development of supporting applications. This study also introduces a comparison between current COVID-19 datasets. Based on the limitations of the current literature, this review highlights the open research challenges that could inspire the future application of AI in COVID-19.},
DOI = {10.3390/diagnostics11071155}
}



@Article{rs13132465,
AUTHOR = {Koz, Alper and Efe, Ufuk},
TITLE = {Geometric- and Optimization-Based Registration Methods for Long-Wave Infrared Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2465},
URL = {https://www.mdpi.com/2072-4292/13/13/2465},
ISSN = {2072-4292},
ABSTRACT = {Registration of long-wave infrared (LWIR) hyperspectral images with their thermal and emissivity components has until now received comparatively less attention with respect to the visible near and short wave infrared hyperspectral images. In this paper, the registration of LWIR hyperspectral images is investigated to enhance applications of LWIR images such as change detection, temperature and emissivity separation, and target detection. The proposed approach first searches for the best features of hyperspectral image pixels for extraction and matching in the LWIR range and then performs a global registration over two-dimensional maps of three-dimensional hyperspectral cubes. The performances of temperature and emissivity features in the thermal domain along with the average energy and principal components of spectral radiance are investigated. The global registration performed over whole 2D maps is further improved by blockwise local refinements. Among the two proposed approaches, the geometric refinement seeks the best keypoint combination in the neighborhood of each block to estimate the transformation for that block. The alternative optimization-based refinement iteratively finds the best transformation by maximizing the similarity of the reference and transformed blocks. The possible blocking artifacts due to blockwise mapping are finally eliminated by pixelwise refinement. The experiments are evaluated with respect to the (i) utilized similarity metrics in the LWIR range between transformed and reference blocks, (ii) proposed geometric- and optimization-based methods, and (iii) image pairs captured on the same and different days. The better performance of the proposed approach compared to manual, GPU-IMU-based, and state-of-the-art image registration methods is verified.},
DOI = {10.3390/rs13132465}
}



@Article{rs13132468,
AUTHOR = {Anochi, Juliana Aparecida and de Almeida, Vinícius Albuquerque and de Campos Velho, Haroldo Fraga},
TITLE = {Machine Learning for Climate Precipitation Prediction Modeling over South America},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2468},
URL = {https://www.mdpi.com/2072-4292/13/13/2468},
ISSN = {2072-4292},
ABSTRACT = {Many natural disasters in South America are linked to meteorological phenomena. Therefore, forecasting and monitoring climatic events are fundamental issues for society and various sectors of the economy. In the last decades, machine learning models have been developed to tackle different issues in society, but there is still a gap in applications to applied physics. Here, different machine learning models are evaluated for precipitation prediction over South America. Currently, numerical weather prediction models are unable to precisely reproduce the precipitation patterns in South America due to many factors such as the lack of region-specific parametrizations and data availability. The results are compared to the general circulation atmospheric model currently used operationally in the National Institute for Space Research (INPE: Instituto Nacional de Pesquisas Espaciais), Brazil. Machine learning models are able to produce predictions with errors under 2 mm in most of the continent in comparison to satellite-observed precipitation patterns for different climate seasons, and also outperform INPE’s model for some regions (e.g., reduction of errors from 8 to 2 mm in central South America in winter). Another advantage is the computational performance from machine learning models, running faster with much lower computer resources than models based on differential equations currently used in operational centers. Therefore, it is important to consider machine learning models for precipitation forecasts in operational centers as a way to improve forecast quality and to reduce computation costs.},
DOI = {10.3390/rs13132468}
}



@Article{electronics10131539,
AUTHOR = {Ding, Qianao and Zhu, Rongbo and Liu, Hao and Ma, Maode},
TITLE = {An Overview of Machine Learning-Based Energy-Efficient Routing Algorithms in Wireless Sensor Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1539},
URL = {https://www.mdpi.com/2079-9292/10/13/1539},
ISSN = {2079-9292},
ABSTRACT = {Machine learning (ML) technology has shown its unique advantages in many fields and has excellent performance in many applications, such as image recognition, speech recognition, recommendation systems, and natural language processing. Recently, the applicability of ML in wireless sensor networks (WSNs) has attracted much attention. As resources are limited in WSNs, identifying how to improve resource utilization and achieve power-efficient load balancing is becoming a critical issue in WSNs. Traditional green routing algorithms aim to achieve this by reducing energy consumption and prolonging network lifetime through optimized routing schemes in WSNs. However, there are usually problems such as poor flexibility, a single consideration factor, and a reliance on accurate mathematical models. ML techniques can quickly adapt to environmental changes and integrate multiple factors for routing decisions, which provides new ideas for intelligent energy-efficient routing algorithms in WSNs. In this paper, we survey and propose a theoretical hypothetic model formulation of ML as an effective method for creating a power-efficient green routing model that can overcome the limitations of traditional green routing methods. In addition, the study also provides an overview of past, present, and future progress in green routing schemes in WSNs. The contents of this paper will appeal to a wide range of audiences interested in ML-based WSNs.},
DOI = {10.3390/electronics10131539}
}



@Article{jsan10030040,
AUTHOR = {Helfer, Gilson Augusto and Barbosa, Jorge Luis Victória and Alves, Douglas and da Costa, Adilson Ben and Beko, Marko and Leithardt, Valderi Reis Quietinho},
TITLE = {Multispectral Cameras and Machine Learning Integrated into Portable Devices as Clay Prediction Technology},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {40},
URL = {https://www.mdpi.com/2224-2708/10/3/40},
ISSN = {2224-2708},
ABSTRACT = {The present work proposed a low-cost portable device as an enabling technology for agriculture using multispectral imaging and machine learning in soil texture. Clay is an important factor for the verification and monitoring of soil use due to its fast reaction to chemical and surface changes. The system developed uses the analysis of reflectance in wavebands for clay prediction. The selection of each wavelength is performed through an LED lamp panel. A NoIR microcamera controlled by a Raspberry Pi device is employed to acquire the image and unfold it in RGB histograms. Results showed a good prediction performance with R2 of 0.96, RMSEC of 3.66% and RMSECV of 16.87%. The high portability allows the equipment to be used in a field providing strategic information related to soil sciences.},
DOI = {10.3390/jsan10030040}
}



@Article{app11135911,
AUTHOR = {Martos, Vanesa and Ahmad, Ali and Cartujo, Pedro and Ordoñez, Javier},
TITLE = {Ensuring Agricultural Sustainability through Remote Sensing in the Era of Agriculture 5.0},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5911},
URL = {https://www.mdpi.com/2076-3417/11/13/5911},
ISSN = {2076-3417},
ABSTRACT = {Timely and reliable information about crop management, production, and yield is considered of great utility by stakeholders (e.g., national and international authorities, farmers, commercial units, etc.) to ensure food safety and security. By 2050, according to Food and Agriculture Organization (FAO) estimates, around 70% more production of agricultural products will be needed to fulfil the demands of the world population. Likewise, to meet the Sustainable Development Goals (SDGs), especially the second goal of “zero hunger”, potential technologies like remote sensing (RS) need to be efficiently integrated into agriculture. The application of RS is indispensable today for a highly productive and sustainable agriculture. Therefore, the present study draws a general overview of RS technology with a special focus on the principal platforms of this technology, i.e., satellites and remotely piloted aircrafts (RPAs), and the sensors used, in relation to the 5th industrial revolution. Nevertheless, since 1957, RS technology has found applications, through the use of satellite imagery, in agriculture, which was later enriched by the incorporation of remotely piloted aircrafts (RPAs), which is further pushing the boundaries of proficiency through the upgrading of sensors capable of higher spectral, spatial, and temporal resolutions. More prominently, wireless sensor technologies (WST) have streamlined real time information acquisition and programming for respective measures. Improved algorithms and sensors can, not only add significant value to crop data acquisition, but can also devise simulations on yield, harvesting and irrigation periods, metrological data, etc., by making use of cloud computing. The RS technology generates huge sets of data that necessitate the incorporation of artificial intelligence (AI) and big data to extract useful products, thereby augmenting the adeptness and efficiency of agriculture to ensure its sustainability. These technologies have made the orientation of current research towards the estimation of plant physiological traits rather than the structural parameters possible. Futuristic approaches for benefiting from these cutting-edge technologies are discussed in this study. This study can be helpful for researchers, academics, and young students aspiring to play a role in the achievement of sustainable agriculture.},
DOI = {10.3390/app11135911}
}



@Article{rs13132482,
AUTHOR = {Zamboni, Pedro and Junior, José Marcato and Silva, Jonathan de Andrade and Miyoshi, Gabriela Takahashi and Matsubara, Edson Takashi and Nogueira, Keiller and Gonçalves, Wesley Nunes},
TITLE = {Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2482},
URL = {https://www.mdpi.com/2072-4292/13/13/2482},
ISSN = {2072-4292},
ABSTRACT = {Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 × 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications.},
DOI = {10.3390/rs13132482}
}



@Article{rs13132486,
AUTHOR = {Ouhami, Maryam and Hafiane, Adel and Es-Saady, Youssef and El Hajji, Mohamed and Canals, Raphael},
TITLE = {Computer Vision, IoT and Data Fusion for Crop Disease Detection Using Machine Learning: A Survey and Ongoing Research},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2486},
URL = {https://www.mdpi.com/2072-4292/13/13/2486},
ISSN = {2072-4292},
ABSTRACT = {Crop diseases constitute a serious issue in agriculture, affecting both quality and quantity of agriculture production. Disease control has been a research object in many scientific and technologic domains. Technological advances in sensors, data storage, computing resources and artificial intelligence have shown enormous potential to control diseases effectively. A growing body of literature recognizes the importance of using data from different types of sensors and machine learning approaches to build models for detection, prediction, analysis, assessment, etc. However, the increasing number and diversity of research studies requires a literature review for further developments and contributions in this area. This paper reviews state-of-the-art machine learning methods that use different data sources, applied to plant disease detection. It lists traditional and deep learning methods associated with the main data acquisition modalities, namely IoT, ground imaging, unmanned aerial vehicle imaging and satellite imaging. In addition, this study examines the role of data fusion for ongoing research in the context of disease detection. It highlights the advantage of intelligent data fusion techniques, from heterogeneous data sources, to improve plant health status prediction and presents the main challenges facing this field. The study concludes with a discussion of several current issues and research trends.},
DOI = {10.3390/rs13132486}
}



@Article{app11135928,
AUTHOR = {Karaaslan, Enes and Bagci, Ulas and Catbas, Necati},
TITLE = {A Novel Decision Support System for Long-Term Management of Bridge Networks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5928},
URL = {https://www.mdpi.com/2076-3417/11/13/5928},
ISSN = {2076-3417},
ABSTRACT = {Developing a bridge management strategy at the network level with efficient use of capital is very important for optimal infrastructure remediation. This paper introduces a novel decision support system that considers many aspects of bridge management and successfully implements the investigated methodology in a web-based platform. The proposed decision support system uses advanced prediction models, decision trees, and incremental machine learning algorithms to generate an optimal decision strategy. The system aims to achieve adaptive and flexible decision making while entailing powerful utilization of nondestructive evaluation (NDE) methods. The NDE data integration and visualization allow automatic retrieval of inspection results and overlaying the defects on a 3D bridge model. Furthermore, a deep learning-based damage growth prediction model estimates the future condition of the bridge elements and utilizes this information in the decision-making process. The decision ranking takes into account a wide range of factors including structural safety, serviceability, rehabilitation cost, life cycle cost, and societal and political factors to generate optimal maintenance strategies with multiple decision alternatives. This study aims to bring a complementary solution to currently in-use systems with the utilization of advanced machine-learning models and NDE data integration while still equipped with main bridge management functions of bridge management systems and capable of transferring data to other systems.},
DOI = {10.3390/app11135928}
}



@Article{s21134363,
AUTHOR = {Nabwire, Shona and Suh, Hyun-Kwon and Kim, Moon S. and Baek, Insuck and Cho, Byoung-Kwan},
TITLE = {Review: Application of Artificial Intelligence in Phenomics},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4363},
URL = {https://www.mdpi.com/1424-8220/21/13/4363},
PubMedID = {34202291},
ISSN = {1424-8220},
ABSTRACT = {Plant phenomics has been rapidly advancing over the past few years. This advancement is attributed to the increased innovation and availability of new technologies which can enable the high-throughput phenotyping of complex plant traits. The application of artificial intelligence in various domains of science has also grown exponentially in recent years. Notably, the computer vision, machine learning, and deep learning aspects of artificial intelligence have been successfully integrated into non-invasive imaging techniques. This integration is gradually improving the efficiency of data collection and analysis through the application of machine and deep learning for robust image analysis. In addition, artificial intelligence has fostered the development of software and tools applied in field phenotyping for data collection and management. These include open-source devices and tools which are enabling community driven research and data-sharing, thereby availing the large amounts of data required for the accurate study of phenotypes. This paper reviews more than one hundred current state-of-the-art papers concerning AI-applied plant phenotyping published between 2010 and 2020. It provides an overview of current phenotyping technologies and the ongoing integration of artificial intelligence into plant phenotyping. Lastly, the limitations of the current approaches/methods and future directions are discussed.},
DOI = {10.3390/s21134363}
}



@Article{drones5030054,
AUTHOR = {Casabianca, Pietro and Zhang, Yu},
TITLE = {Acoustic-Based UAV Detection Using Late Fusion of Deep Neural Networks},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2504-446X/5/3/54},
ISSN = {2504-446X},
ABSTRACT = {Multirotor UAVs have become ubiquitous in commercial and public use. As they become more affordable and more available, the associated security risks further increase, especially in relation to airspace breaches and the danger of drone-to-aircraft collisions. Thus, robust systems must be set in place to detect and deal with hostile drones. This paper investigates the use of deep learning methods to detect UAVs using acoustic signals. Deep neural network models are trained with mel-spectrograms as inputs. In this case, Convolutional Neural Networks (CNNs) are shown to be the better performing network, compared with Recurrent Neural Networks (RNNs) and Convolutional Recurrent Neural Networks (CRNNs). Furthermore, late fusion methods have been evaluated using an ensemble of deep neural networks, where the weighted soft voting mechanism has achieved the highest average accuracy of 94.7%, which has outperformed the solo models. In future work, the developed late fusion technique could be utilized with radar and visual methods to further improve the UAV detection performance.},
DOI = {10.3390/drones5030054}
}



@Article{electronics10131549,
AUTHOR = {Shrestha, Rakesh and Omidkar, Atefeh and Roudi, Sajjad Ahmadi and Abbas, Robert and Kim, Shiho},
TITLE = {Machine-Learning-Enabled Intrusion Detection System for Cellular Connected UAV Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1549},
URL = {https://www.mdpi.com/2079-9292/10/13/1549},
ISSN = {2079-9292},
ABSTRACT = {The recent development and adoption of unmanned aerial vehicles (UAVs) is due to its wide variety of applications in public and private sector from parcel delivery to wildlife conservation. The integration of UAVs, 5G, and satellite technologies has prompted telecommunication networks to evolve to provide higher-quality and more stable service to remote areas. However, security concerns with UAVs are growing as UAV nodes are becoming attractive targets for cyberattacks due to enormously growing volumes and poor and weak inbuilt security. In this paper, we propose a UAV- and satellite-based 5G-network security model that can harness machine learning to effectively detect of vulnerabilities and cyberattacks. The solution is divided into two main parts: the model creation for intrusion detection using various machine learning (ML) algorithms and the implementation of ML-based model into terrestrial or satellite gateways. The system identifies various attack types using realistic CSE-CIC IDS-2018 network datasets published by Canadian Establishment for Cybersecurity (CIC). It consists of seven different types of new and contemporary attack types. This paper demonstrates that ML algorithms can be used to classify benign or malicious packets in UAV networks to enhance security. Finally, the tested ML algorithms are compared for effectiveness in terms of accuracy rate, precision, recall, F1-score, and false-negative rate. The decision tree algorithm performed well by obtaining a maximum accuracy rate of 99.99% and a minimum false negative rate of 0% in detecting various attacks as compared to all other types of ML classifiers.},
DOI = {10.3390/electronics10131549}
}



@Article{rs13132496,
AUTHOR = {Khoroshevsky, Faina and Khoroshevsky, Stanislav and Bar-Hillel, Aharon},
TITLE = {Parts-per-Object Count in Agricultural Images: Solving Phenotyping Problems via a Single Deep Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2496},
URL = {https://www.mdpi.com/2072-4292/13/13/2496},
ISSN = {2072-4292},
ABSTRACT = {Solving many phenotyping problems involves not only automatic detection of objects in an image, but also counting the number of parts per object. We propose a solution in the form of a single deep network, tested for three agricultural datasets pertaining to bananas-per-bunch, spikelets-per-wheat-spike, and berries-per-grape-cluster. The suggested network incorporates object detection, object resizing, and part counting as modules in a single deep network, with several variants tested. The detection module is based on a Retina-Net architecture, whereas for the counting modules, two different architectures are examined: the first based on direct regression of the predicted count, and the other on explicit parts detection and counting. The results are promising, with the mean relative deviation between estimated and visible part count in the range of 9.2% to 11.5%. Further inference of count-based yield related statistics is considered. For banana bunches, the actual banana count (including occluded bananas) is inferred from the count of visible bananas. For spikelets-per-wheat-spike, robust estimation methods are employed to get the average spikelet count across the field, which is an effective yield estimator.},
DOI = {10.3390/rs13132496}
}



@Article{app11135941,
AUTHOR = {Lee, Mun-yong and Lee, Sang-ha and Jung, Kye-dong and Lee, Seung-hyun and Kwon, Soon-chul},
TITLE = {A Novel Preprocessing Method for Dynamic Point-Cloud Compression},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5941},
URL = {https://www.mdpi.com/2076-3417/11/13/5941},
ISSN = {2076-3417},
ABSTRACT = {Computer-based data processing capabilities have evolved to handle a lot of information. As such, the complexity of three-dimensional (3D) models (e.g., animations or real-time voxels) containing large volumes of information has increased exponentially. This rapid increase in complexity has led to problems with recording and transmission. In this study, we propose a method of efficiently managing and compressing animation information stored in the 3D point-clouds sequence. A compressed point-cloud is created by reconfiguring the points based on their voxels. Compared with the original point-cloud, noise caused by errors is removed, and a preprocessing procedure that achieves high performance in a redundant processing algorithm is proposed. The results of experiments and rendering demonstrate an average file-size reduction of 40% using the proposed algorithm. Moreover, 13% of the over-lap data are extracted and removed, and the file size is further reduced.},
DOI = {10.3390/app11135941}
}



@Article{s21134384,
AUTHOR = {Liu, Weihua and Zeng, Shan and Wu, Guiju and Li, Hao and Chen, Feifei},
TITLE = {Rice Seed Purity Identification Technology Using Hyperspectral Image with LASSO Logistic Regression Model},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4384},
URL = {https://www.mdpi.com/1424-8220/21/13/4384},
PubMedID = {34206783},
ISSN = {1424-8220},
ABSTRACT = {Hyperspectral technology is used to obtain spectral and spatial information of samples simultaneously and demonstrates significant potential for use in seed purity identification. However, it has certain limitations, such as high acquisition cost and massive redundant information. This study integrates the advantages of the sparse feature of the least absolute shrinkage and selection operator (LASSO) algorithm and the classification feature of the logistic regression model (LRM). We propose a hyperspectral rice seed purity identification method based on the LASSO logistic regression model (LLRM). The feasibility of using LLRM for the selection of feature wavelength bands and seed purity identification are discussed using four types of rice seeds as research objects. The results of 13 different adulteration cases revealed that the value of the regularisation parameter was different in each case. The recognition accuracy of LLRM and average recognition accuracy were 91.67–100% and 98.47%, respectively. Furthermore, the recognition accuracy of full-band LRM was 71.60–100%. However, the average recognition accuracy was merely 89.63%. These results indicate that LLRM can select the feature wavelength bands stably and improve the recognition accuracy of rice seeds, demonstrating the feasibility of developing a hyperspectral technology with LLRM for seed purity identification.},
DOI = {10.3390/s21134384}
}



@Article{rs13132501,
AUTHOR = {Rahimzad, Maryam and Homayouni, Saeid and Alizadeh Naeini, Amin and Nadi, Saeed},
TITLE = {An Efficient Multi-Sensor Remote Sensing Image Clustering in Urban Areas via Boosted Convolutional Autoencoder (BCAE)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2501},
URL = {https://www.mdpi.com/2072-4292/13/13/2501},
ISSN = {2072-4292},
ABSTRACT = {High-resolution urban image clustering has remained a challenging task. This is mainly because its performance strongly depends on the discrimination power of features. Recently, several studies focused on unsupervised learning methods by autoencoders to learn and extract more efficient features for clustering purposes. This paper proposes a Boosted Convolutional AutoEncoder (BCAE) method based on feature learning for efficient urban image clustering. The proposed method was applied to multi-sensor remote-sensing images through a multistep workflow. The optical data were first preprocessed by applying a Minimum Noise Fraction (MNF) transformation. Then, these MNF features, in addition to the normalized Digital Surface Model (nDSM) and vegetation indexes such as Normalized Difference Vegetation Index (NDVI) and Excess Green (ExG(2)), were used as the inputs of the BCAE model. Next, our proposed convolutional autoencoder was trained to automatically encode upgraded features and boost the hand-crafted features for producing more clustering-friendly ones. Then, we employed the Mini Batch K-Means algorithm to cluster deep features. Finally, the comparative feature sets were manually designed in three modes to prove the efficiency of the proposed method in extracting compelling features. Experiments on three datasets show the efficiency of BCAE for feature learning. According to the experimental results, by applying the proposed method, the ultimate features become more suitable for clustering, and spatial correlation among the pixels in the feature learning process is also considered.},
DOI = {10.3390/rs13132501}
}



@Article{rs13132517,
AUTHOR = {Wang, Lijun and Wang, Jiayao and Qin, Fen},
TITLE = {Feature Fusion Approach for Temporal Land Use Mapping in Complex Agricultural Areas},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2517},
URL = {https://www.mdpi.com/2072-4292/13/13/2517},
ISSN = {2072-4292},
ABSTRACT = {Accurate temporal land use mapping provides important and timely information for decision making for large-scale management of land and crop production. At present, temporal land cover and crop classifications within a study area have neglected the differences between subregions. In this paper, we propose a classification rule by integrating the terrain, time series characteristics, priority, and seasonality (TTPSR) with Sentinel-2 satellite imagery. Based on the time series of Normalized Difference Water Index (NDWI) and Vegetation Index (NDVI), a dynamic decision tree for forests, cultivation, urban, and water was created in Google Earth Engine (GEE) for each subregion to extract cultivated land. Then, with or without this cultivated land mask data, the original classification results for each subregion were completed based on composite image acquisition with five vegetation indices using Random Forest. During the post-reclassification process, a 4-bit coding rule based on terrain, type, seasonal rhythm, and priority was generated by analyzing the characteristics of the original results. Finally, statistical results and temporal mapping were processed. The results showed that feature importance was dominated by B2, NDWI, RENDVI, B11, and B12 over winter, and B11, B12, NDBI, B2, and B8A over summer. Meanwhile, the cultivated land mask improved the overall accuracy for multicategories (seven to eight and nine to 13 during winter and summer, respectively) in each subregion, with average ranges in the overall accuracy for winter and summer of 0.857–0.935 and 0.873–0.963, respectively, and kappa coefficients of 0.803–0.902 and 0.835–0.950, respectively. The analysis of the above results and the comparison with resampling plots identified various sources of error for classification accuracy, including spectral differences, degree of field fragmentation, and planting complexity. The results demonstrated the capability of the TTPSR rule in temporal land use mapping, especially with regard to complex crops classification and automated post-processing, thereby providing a viable option for large-scale land use mapping.},
DOI = {10.3390/rs13132517}
}



@Article{s21134417,
AUTHOR = {Ukaegbu, Uchechi F. and Tartibu, Lagouge K. and Okwu, Modestus O. and Olayode, Isaac O.},
TITLE = {Development of a Light-Weight Unmanned Aerial Vehicle for Precision Agriculture},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4417},
URL = {https://www.mdpi.com/1424-8220/21/13/4417},
PubMedID = {34203187},
ISSN = {1424-8220},
ABSTRACT = {This paper describes the development of a modular unmanned aerial vehicle for the detection and eradication of weeds on farmland. Precision agriculture entails solving the problem of poor agricultural yield due to competition for nutrients by weeds and provides a faster approach to eliminating the problematic weeds using emerging technologies. This research has addressed the aforementioned problem. A quadcopter was built, and components were assembled with light-weight materials. The system consists of the electric motor, electronic speed controller, propellers, frame, lithium polymer (li-po) battery, flight controller, a global positioning system (GPS), and receiver. A sprayer module which consists of a relay, Raspberry Pi 3, spray pump, 12 V DC source, water hose, and the tank was built. It operated in such a way that when a weed is detected based on the deep learning algorithms deployed on the Raspberry Pi, general purpose input/output (GPIO) 17 or GPIO 18 (of the Raspberry Pi) were activated to supply 3.3 V, which turned on a DC relay to spray herbicides accordingly. The sprayer module was mounted on the quadcopter and from the test-running operation conducted, broadleaf and grass weeds were accurately detected and the spraying of herbicides according to the weed type occurred in less than a second.},
DOI = {10.3390/s21134417}
}



@Article{rs13132520,
AUTHOR = {Ma, Dongdong and Rehman, Tanzeel U. and Zhang, Libo and Maki, Hideki and Tuinstra, Mitchell R. and Jin, Jian},
TITLE = {Modeling of Environmental Impacts on Aerial Hyperspectral Images for Corn Plant Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2520},
URL = {https://www.mdpi.com/2072-4292/13/13/2520},
ISSN = {2072-4292},
ABSTRACT = {Aerial imaging technologies have been widely applied in agricultural plant remote sensing. However, an as yet unexplored challenge with field imaging is that the environmental conditions, such as sun angle, cloud coverage, temperature, and so on, can significantly alter plant appearance and thus affect the imaging sensor’s accuracy toward extracting plant feature measurements. These image alterations result from the complicated interaction between the real-time environments and plants. Analysis of these impacts requires continuous monitoring of the changes through various environmental conditions, which has been difficult with current aerial remote sensing systems. This paper aimed to propose a modeling method to comprehensively understand and model the environmental influences on hyperspectral imaging data. In 2019, a fixed hyperspectral imaging gantry was constructed in Purdue University’s research farm, and over 8000 repetitive images of the same corn field were taken with a 2.5 min interval for 31 days. Time-tagged local environment data, including solar zenith angle, solar irradiation, temperature, wind speed, and so on, were also recorded during the imaging time. The images were processed for phenotyping data, and the time series decomposition method was applied to extract the phenotyping data variation caused by the changing environments. An artificial neural network (ANN) was then built to model the relationship between the phenotyping data variation and environmental changes. The ANN model was able to accurately predict the environmental effects in remote sensing results, and thus could be used to effectively eliminate the environment-induced variation in the phenotyping features. The test of the normalized difference vegetation index (NDVI) calculated from the hyperspectral images showed that variance in NDVI was reduced by 79%. A similar performance was confirmed with the relative water content (RWC) predictions. Therefore, this modeling method shows great potential for application in aerial remote sensing applications in agriculture, to significantly improve the imaging quality by effectively eliminating the effects from the changing environmental conditions.},
DOI = {10.3390/rs13132520}
}



@Article{rs13132524,
AUTHOR = {Chen, Ziyi and Li, Dilong and Fan, Wentao and Guan, Haiyan and Wang, Cheng and Li, Jonathan},
TITLE = {Self-Attention in Reconstruction Bias U-Net for Semantic Segmentation of Building Rooftops in Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2524},
URL = {https://www.mdpi.com/2072-4292/13/13/2524},
ISSN = {2072-4292},
ABSTRACT = {Deep learning models have brought great breakthroughs in building extraction from high-resolution optical remote-sensing images. Among recent research, the self-attention module has called up a storm in many fields, including building extraction. However, most current deep learning models loading with the self-attention module still lose sight of the reconstruction bias’s effectiveness. Through tipping the balance between the abilities of encoding and decoding, i.e., making the decoding network be much more complex than the encoding network, the semantic segmentation ability will be reinforced. To remedy the research weakness in combing self-attention and reconstruction-bias modules for building extraction, this paper presents a U-Net architecture that combines self-attention and reconstruction-bias modules. In the encoding part, a self-attention module is added to learn the attention weights of the inputs. Through the self-attention module, the network will pay more attention to positions where there may be salient regions. In the decoding part, multiple large convolutional up-sampling operations are used for increasing the reconstruction ability. We test our model on two open available datasets: the WHU and Massachusetts Building datasets. We achieve IoU scores of 89.39% and 73.49% for the WHU and Massachusetts Building datasets, respectively. Compared with several recently famous semantic segmentation methods and representative building extraction methods, our method’s results are satisfactory.},
DOI = {10.3390/rs13132524}
}



@Article{jsan10030042,
AUTHOR = {Al-Nuaimi, Mohammed and Wibowo, Sapto and Qu, Hongyang and Aitken, Jonathan and Veres, Sandor},
TITLE = {Hybrid Verification Technique for Decision-Making of Self-Driving Vehicles},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {42},
URL = {https://www.mdpi.com/2224-2708/10/3/42},
ISSN = {2224-2708},
ABSTRACT = {The evolution of driving technology has recently progressed from active safety features and ADAS systems to fully sensor-guided autonomous driving. Bringing such a vehicle to market requires not only simulation and testing but formal verification to account for all possible traffic scenarios. A new verification approach, which combines the use of two well-known model checkers: model checker for multi-agent systems (MCMAS) and probabilistic model checker (PRISM), is presented for this purpose. The overall structure of our autonomous vehicle (AV) system consists of: (1) A perception system of sensors that feeds data into (2) a rational agent (RA) based on a belief–desire–intention (BDI) architecture, which uses a model of the environment and is connected to the RA for verification of decision-making, and (3) a feedback control systems for following a self-planned path. MCMAS is used to check the consistency and stability of the BDI agent logic during design-time. PRISM is used to provide the RA with the probability of success while it decides to take action during run-time operation. This allows the RA to select movements of the highest probability of success from several generated alternatives. This framework has been tested on a new AV software platform built using the robot operating system (ROS) and virtual reality (VR) Gazebo Simulator. It also includes a parking lot scenario to test the feasibility of this approach in a realistic environment. A practical implementation of the AV system was also carried out on the experimental testbed.},
DOI = {10.3390/jsan10030042}
}



@Article{s21134442,
AUTHOR = {Niu, Zijie and Deng, Juntao and Zhang, Xu and Zhang, Jun and Pan, Shijia and Mu, Haotian},
TITLE = {Identifying the Branch of Kiwifruit Based on Unmanned Aerial Vehicle (UAV) Images Using Deep Learning Method},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4442},
URL = {https://www.mdpi.com/1424-8220/21/13/4442},
PubMedID = {34209571},
ISSN = {1424-8220},
ABSTRACT = {It is important to obtain accurate information about kiwifruit vines to monitoring their physiological states and undertake precise orchard operations. However, because vines are small and cling to trellises, and have branches laying on the ground, numerous challenges exist in the acquisition of accurate data for kiwifruit vines. In this paper, a kiwifruit canopy distribution prediction model is proposed on the basis of low-altitude unmanned aerial vehicle (UAV) images and deep learning techniques. First, the location of the kiwifruit plants and vine distribution are extracted from high-precision images collected by UAV. The canopy gradient distribution maps with different noise reduction and distribution effects are generated by modifying the threshold and sampling size using the resampling normalization method. The results showed that the accuracies of the vine segmentation using PSPnet, support vector machine, and random forest classification were 71.2%, 85.8%, and 75.26%, respectively. However, the segmentation image obtained using depth semantic segmentation had a higher signal-to-noise ratio and was closer to the real situation. The average intersection over union of the deep semantic segmentation was more than or equal to 80% in distribution maps, whereas, in traditional machine learning, the average intersection was between 20% and 60%. This indicates the proposed model can quickly extract the vine distribution and plant position, and is thus able to perform dynamic monitoring of orchards to provide real-time operation guidance.},
DOI = {10.3390/s21134442}
}



@Article{rs13132540,
AUTHOR = {Kong, Deheng and Wu, Faquan and Saroglou, Charalampos and Sha, Peng and Li, Bo},
TITLE = {In-Situ Block Characterization of Jointed Rock Exposures Based on a 3D Point Cloud Model},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2540},
URL = {https://www.mdpi.com/2072-4292/13/13/2540},
ISSN = {2072-4292},
ABSTRACT = {The importance of in-situ rock block characterization has been realized for decades in rock mechanics and engineering, yet how to reliably measure and characterize the geometrical properties of blocks in varied forms of exposures and patterns of jointing is still a challenging task. Using a point cloud model (PCM) of rock exposures generated from remote sensing techniques, we developed a consistent and comprehensive method for rock block characterization that is composed of two different procedures and a block indicator system. A semi-automatic procedure towards the robust extraction of in-situ rock blocks created by the deterministic discontinuity network on rock exposures (PCM-DDN) was developed. A 3D stochastic discrete fracture network (DFN) simulation (PCM-SDS) procedure was built based on the statistically valid representation of the discontinuity network geometry. A multi-dimensional block indicator system, i.e., the block size, shape, orientation, and spatial distribution pattern for systematic and objective block characterization, was then established. The developed method was applied to a synthetic model of cardboard boxes and three different rock engineering scenarios, including a road cut slope from Spain and two open-pit mining slopes from China. Compared with existing empirical methods, the proposed procedures and the block indicator system are dependable and practically feasible, which can help enhance our understanding of block geometry characteristics in related applications.},
DOI = {10.3390/rs13132540}
}



@Article{rs13132548,
AUTHOR = {Habibi, Luthfan Nur and Watanabe, Tomoya and Matsui, Tsutomu and Tanaka, Takashi S. T.},
TITLE = {Machine Learning Techniques to Predict Soybean Plant Density Using UAV and Satellite-Based Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2548},
URL = {https://www.mdpi.com/2072-4292/13/13/2548},
ISSN = {2072-4292},
ABSTRACT = {The plant density of soybean is a critical factor affecting plant canopy structure and yield. Predicting the spatial variability of plant density would be valuable for improving agronomic practices. The objective of this study was to develop a model for plant density measurement using several data sets with different spatial resolutions, including unmanned aerial vehicle (UAV) imagery, PlanetScope satellite imagery, and climate data. The model establishment process includes (1) performing the high-throughput measurement of actual plant density from UAV imagery with the You Only Look Once version 3 (YOLOv3) object detection algorithm, which was further treated as a response variable of the estimation models in the next step, and (2) developing regression models to estimate plant density in the extended areas using various combinations of predictors derived from PlanetScope imagery and climate data. Our results showed that the YOLOv3 model can accurately measure actual soybean plant density from UAV imagery data with a root mean square error (RMSE) value of 0.96 plants m−2. Furthermore, the two regression models, partial least squares and random forest (RF), successfully expanded the plant density prediction areas with RMSE values ranging from 1.78 to 3.67 plant m−2. Model improvement was conducted using the variable importance feature in RF, which improved prediction accuracy with an RMSE value of 1.72 plant m−2. These results demonstrated that the established model had an acceptable prediction accuracy for estimating plant density. Although the model could not often evaluate the within-field spatial variability of soybean plant density, the predicted values were sufficient for informing the field-specific status.},
DOI = {10.3390/rs13132548}
}



@Article{drones5030055,
AUTHOR = {Bigazzi, Luca and Basso, Michele and Boni, Enrico and Innocenti, Giacomo and Pieraccini, Massimiliano},
TITLE = {A Multilevel Architecture for Autonomous UAVs},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {55},
URL = {https://www.mdpi.com/2504-446X/5/3/55},
ISSN = {2504-446X},
ABSTRACT = {In this paper, a multilevel architecture able to interface an on-board computer with a generic UAV flight controller and its radio receiver is proposed. The computer board exploits the same standard communication protocol of UAV flight controllers and can easily access additional data, such as: (i) inertial sensor measurements coming from a multi-sensor board; (ii) global navigation satellite system (GNSS) coordinates; (iii) streaming video from one or more cameras; and (iv) operator commands from the remote control. In specific operating scenarios, the proposed platform is able to act as a “cyber pilot” which replaces the role of a human UAV operator, thus simplifying the development of complex tasks such as those based on computer vision and artificial intelligence (AI) algorithms which are typically employed in autonomous flight operations.},
DOI = {10.3390/drones5030055}
}



@Article{su13137309,
AUTHOR = {Giray, Görkem and Catal, Cagatay},
TITLE = {Design of a Data Management Reference Architecture for Sustainable Agriculture},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {7309},
URL = {https://www.mdpi.com/2071-1050/13/13/7309},
ISSN = {2071-1050},
ABSTRACT = {Effective and efficient data management is crucial for smart farming and precision agriculture. To realize operational efficiency, full automation, and high productivity in agricultural systems, different kinds of data are collected from operational systems using different sensors, stored in different systems, and processed using advanced techniques, such as machine learning and deep learning. Due to the complexity of data management operations, a data management reference architecture is required. While there are different initiatives to design data management reference architectures, a data management reference architecture for sustainable agriculture is missing. In this study, we follow domain scoping, domain modeling, and reference architecture design stages to design the reference architecture for sustainable agriculture. Four case studies were performed to demonstrate the applicability of the reference architecture. This study shows that the proposed data management reference architecture is practical and effective for sustainable agriculture.},
DOI = {10.3390/su13137309}
}



@Article{app11136079,
AUTHOR = {Elgamoudi, Abulasad and Benzerrouk, Hamza and Elango, G. Arul and Landry, René},
TITLE = {A Survey for Recent Techniques and Algorithms of Geolocation and Target Tracking in Wireless and Satellite Systems},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6079},
URL = {https://www.mdpi.com/2076-3417/11/13/6079},
ISSN = {2076-3417},
ABSTRACT = {A single Radio-Frequency Interference (RFI) is a disturbance source of modern wireless systems depending on Global Navigation Satellite Systems (GNSS) and Satellite Communication (SatCom). In particular, significant applications such as aeronautics and satellite communication can be severely affected by intentional and unintentional interference, which are unmitigated. The matter requires finding a radical and effective solution to overcome this problem. The methods used for overcoming the RFI include interference detection, interference classification, interference geolocation, tracking and interference mitigation. RFI source geolocation and tracking methodology gained universal attention from numerous researchers, specialists, and scientists. In the last decade, various conventional techniques and algorithms have been adopted in geolocation and target tracking in civil and military operations. Previous conventional techniques did not address the challenges and demand for novel algorithms. Hence there is a necessity for focussing on the issues associated with this. This survey introduces a review of various conventional geolocation techniques, current orientations, and state-of-the-art techniques and highlights some approaches and algorithms employed in wireless and satellite systems for geolocation and target tracking that may be extremely beneficial. In addition, a comparison between different conventional geolocation techniques has been revealed, and the comparisons between various approaches and algorithms of geolocation and target tracking have been addressed, including H∞ and Kalman Filtering versions that have been implemented and investigated by authors.},
DOI = {10.3390/app11136079}
}



@Article{rs13132555,
AUTHOR = {Yoosefzadeh-Najafabadi, Mohsen and Tulpan, Dan and Eskandari, Milad},
TITLE = {Using Hybrid Artificial Intelligence and Evolutionary Optimization Algorithms for Estimating Soybean Yield and Fresh Biomass Using Hyperspectral Vegetation Indices},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2555},
URL = {https://www.mdpi.com/2072-4292/13/13/2555},
ISSN = {2072-4292},
ABSTRACT = {Recent advanced high-throughput field phenotyping combined with sophisticated big data analysis methods have provided plant breeders with unprecedented tools for a better prediction of important agronomic traits, such as yield and fresh biomass (FBIO), at early growth stages. This study aimed to demonstrate the potential use of 35 selected hyperspectral vegetation indices (HVI), collected at the R5 growth stage, for predicting soybean seed yield and FBIO. Two artificial intelligence algorithms, ensemble-bagging (EB) and deep neural network (DNN), were used to predict soybean seed yield and FBIO using HVI. Considering HVI as input variables, the coefficients of determination (R2) of 0.76 and 0.77 for yield and 0.91 and 0.89 for FBIO were obtained using DNN and EB, respectively. In this study, we also used hybrid DNN-SPEA2 to estimate the optimum HVI values in soybeans with maximized yield and FBIO productions. In addition, to identify the most informative HVI in predicting yield and FBIO, the feature recursive elimination wrapper method was used and the top ranking HVI were determined to be associated with red, 670 nm and near-infrared, 800 nm, regions. Overall, this study introduced hybrid DNN-SPEA2 as a robust mathematical tool for optimizing and using informative HVI for estimating soybean seed yield and FBIO at early growth stages, which can be employed by soybean breeders for discriminating superior genotypes in large breeding populations.},
DOI = {10.3390/rs13132555}
}



@Article{s21134489,
AUTHOR = {Matin, Sahar S. and Pradhan, Biswajeet},
TITLE = {Earthquake-Induced Building-Damage Mapping Using Explainable AI (XAI)},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4489},
URL = {https://www.mdpi.com/1424-8220/21/13/4489},
PubMedID = {34209169},
ISSN = {1424-8220},
ABSTRACT = {Building-damage mapping using remote sensing images plays a critical role in providing quick and accurate information for the first responders after major earthquakes. In recent years, there has been an increasing interest in generating post-earthquake building-damage maps automatically using different artificial intelligence (AI)-based frameworks. These frameworks in this domain are promising, yet not reliable for several reasons, including but not limited to the site-specific design of the methods, the lack of transparency in the AI-model, the lack of quality in the labelled image, and the use of irrelevant descriptor features in building the AI-model. Using explainable AI (XAI) can lead us to gain insight into identifying these limitations and therefore, to modify the training dataset and the model accordingly. This paper proposes the use of SHAP (Shapley additive explanation) to interpret the outputs of a multilayer perceptron (MLP)—a machine learning model—and analyse the impact of each feature descriptor included in the model for building-damage assessment to examine the reliability of the model. In this study, a post-event satellite image from the 2018 Palu earthquake was used. The results show that MLP can classify the collapsed and non-collapsed buildings with an overall accuracy of 84% after removing the redundant features. Further, spectral features are found to be more important than texture features in distinguishing the collapsed and non-collapsed buildings. Finally, we argue that constructing an explainable model would help to understand the model’s decision to classify the buildings as collapsed and non-collapsed and open avenues to build a transferable AI model.},
DOI = {10.3390/s21134489}
}



@Article{agriculture11070617,
AUTHOR = {Bansal, Prakhar and Kumar, Rahul and Kumar, Somesh},
TITLE = {Disease Detection in Apple Leaves Using Deep Convolutional Neural Network},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {617},
URL = {https://www.mdpi.com/2077-0472/11/7/617},
ISSN = {2077-0472},
ABSTRACT = {The automatic detection of diseases in plants is necessary, as it reduces the tedious work of monitoring large farms and it will detect the disease at an early stage of its occurrence to minimize further degradation of plants. Besides the decline of plant health, a country’s economy is highly affected by this scenario due to lower production. The current approach to identify diseases by an expert is slow and non-optimal for large farms. Our proposed model is an ensemble of pre-trained DenseNet121, EfficientNetB7, and EfficientNet NoisyStudent, which aims to classify leaves of apple trees into one of the following categories: healthy, apple scab, apple cedar rust, and multiple diseases, using its images. Various Image Augmentation techniques are included in this research to increase the dataset size, and subsequentially, the model’s accuracy increases. Our proposed model achieves an accuracy of 96.25% on the validation dataset. The proposed model can identify leaves with multiple diseases with 90% accuracy. Our proposed model achieved a good performance on different metrics and can be deployed in the agricultural domain to identify plant health accurately and timely.},
DOI = {10.3390/agriculture11070617}
}



@Article{app11136112,
AUTHOR = {Mbiydzenyuy, Gideon and Nowaczyk, Sławomir and Knutsson, Håkan and Vanhoudt, Dirk and Brage, Jens and Calikus, Ece},
TITLE = {Opportunities for Machine Learning in District Heating},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6112},
URL = {https://www.mdpi.com/2076-3417/11/13/6112},
ISSN = {2076-3417},
ABSTRACT = {The district heating (DH) industry is facing an important transformation towards more efficient networks that utilise significantly lower water temperatures to distribute the heat. This change requires taking advantage of new technologies, and Machine Learning (ML) is a popular direction. In the last decade, we have witnessed an extreme growth in the number of published research papers that focus on applying ML techniques to the DH domain. However, based on our experience in the field, and an extensive review of the state-of-the-art, we perceive a mismatch between the most popular research directions, such as forecasting, and the challenges faced by the DH industry. In this work, we present our findings, explain and demonstrate the key gaps between the two communities and suggest a road-map ahead towards increasing the impact of ML research in the DH industry.},
DOI = {10.3390/app11136112}
}



@Article{rs13132567,
AUTHOR = {Oh, Sungchan and Lee, Da-Young and Gongora-Canul, Carlos and Ashapure, Akash and Carpenter, Joshua and Cruz, A. P. and Fernandez-Campos, Mariela and Lane, Brenden Z. and Telenko, Darcy E. P. and Jung, Jinha and Cruz, C. D.},
TITLE = {Tar Spot Disease Quantification Using Unmanned Aircraft Systems (UAS) Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2567},
URL = {https://www.mdpi.com/2072-4292/13/13/2567},
ISSN = {2072-4292},
ABSTRACT = {Tar spot is a foliar disease of corn characterized by fungal fruiting bodies that resemble tar spots. The disease emerged in the U.S. in 2015, and severe outbreaks in 2018 caused an economic impact on corn yields throughout the Midwest. Adequate epidemiological surveillance and disease quantification are necessary to develop immediate and long-term management strategies. This study presents a measurement framework that evaluates the disease severity of tar spot using unmanned aircraft systems (UAS)-based plant phenotyping and regression techniques. UAS-based plant phenotypic information, such as canopy cover, canopy volume, and vegetation indices, were used as explanatory variables. Visual estimations of disease severity were performed by expert plant pathologists per experiment plot basis and used as response variables. Three regression methods, namely ordinary least squares (OLS), support vector regression (SVR), and multilayer perceptron (MLP), were used to determine an optimal regression method for UAS-based tar spot measurement. The cross-validation results showed that the regression model based on MLP provides the highest accuracy of disease measurements. By training and testing the model with spatially separated datasets, the proposed regression model achieved a Lin’s concordance correlation coefficient (ρc) of 0.82 and a root mean square error (RMSE) of 6.42. This study demonstrated that we could use the proposed UAS-based method for the disease quantification of tar spot, which shows a gradual spectral response as the disease develops.},
DOI = {10.3390/rs13132567}
}



@Article{s21134511,
AUTHOR = {Bauer, Martin and Sanchez, Luis and Song, JaeSeung},
TITLE = {IoT-Enabled Smart Cities: Evolution and Outlook},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4511},
URL = {https://www.mdpi.com/1424-8220/21/13/4511},
PubMedID = {34209436},
ISSN = {1424-8220},
ABSTRACT = {For the last decade the Smart City concept has been under development, fostered by the growing urbanization of the world’s population and the need to handle the challenges that such a scenario raises. During this time many Smart City projects have been executed–some as proof-of-concept, but a growing number resulting in permanent, production-level deployments, improving the operation of the city and the quality of life of its citizens. Thus, Smart Cities are still a highly relevant paradigm which needs further development before it reaches its full potential and provides robust and resilient solutions. In this paper, the focus is set on the Internet of Things (IoT) as an enabling technology for the Smart City. In this sense, the paper reviews the current landscape of IoT-enabled Smart Cities, surveying relevant experiences and city initiatives that have embedded IoT within their city services and how they have generated an impact. The paper discusses the key technologies that have been developed and how they are contributing to the realization of the Smart City. Moreover, it presents some challenges that remain open ahead of us and which are the initiatives and technologies that are under development to tackle them.},
DOI = {10.3390/s21134511}
}



@Article{info12070272,
AUTHOR = {Ackerson, Joseph M. and Dave, Rushit and Seliya, Naeem},
TITLE = {Applications of Recurrent Neural Network for Biometric Authentication &amp; Anomaly Detection},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {272},
URL = {https://www.mdpi.com/2078-2489/12/7/272},
ISSN = {2078-2489},
ABSTRACT = {Recurrent Neural Networks are powerful machine learning frameworks that allow for data to be saved and referenced in a temporal sequence. This opens many new possibilities in fields such as handwriting analysis and speech recognition. This paper seeks to explore current research being conducted on RNNs in four very important areas, being biometric authentication, expression recognition, anomaly detection, and applications to aircraft. This paper reviews the methodologies, purpose, results, and the benefits and drawbacks of each proposed method below. These various methodologies all focus on how they can leverage distinct RNN architectures such as the popular Long Short-Term Memory (LSTM) RNN or a Deep-Residual RNN. This paper also examines which frameworks work best in certain situations, and the advantages and disadvantages of each proposed model.},
DOI = {10.3390/info12070272}
}



@Article{aerospace8070179,
AUTHOR = {Swinney, Carolyn J. and Woods, John C.},
TITLE = {The Effect of Real-World Interference on CNN Feature Extraction and Machine Learning Classification of Unmanned Aerial Systems},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {179},
URL = {https://www.mdpi.com/2226-4310/8/7/179},
ISSN = {2226-4310},
ABSTRACT = {Small unmanned aerial systems (UASs) present many potential solutions and enhancements to industry today but equally pose a significant security challenge. We only need to look at the levels of disruption caused by UASs at airports in recent years. The accuracy of UAS detection and classification systems based on radio frequency (RF) signals can be hindered by other interfering signals present in the same frequency band, such as Bluetooth and Wi-Fi devices. In this paper, we evaluate the effect of real-world interference from Bluetooth and Wi-Fi signals concurrently on convolutional neural network (CNN) feature extraction and machine learning classification of UASs. We assess multiple UASs that operate using different transmission systems: Wi-Fi, Lightbridge 2.0, OcuSync 1.0, OcuSync 2.0 and the recently released OcuSync 3.0. We consider 7 popular UASs, evaluating 2 class UAS detection, 8 class UAS type classification and 21 class UAS flight mode classification. Our results show that the process of CNN feature extraction using transfer learning and machine learning classification is fairly robust in the presence of real-world interference. We also show that UASs that are operating using the same transmission system can be distinguished. In the presence of interference from both Bluetooth and Wi-Fi signals, our results show 100% accuracy for UAV detection (2 classes), 98.1% (+/−0.4%) for UAV type classification (8 classes) and 95.4% (+/−0.3%) for UAV flight mode classification (21 classes).},
DOI = {10.3390/aerospace8070179}
}



@Article{s21134522,
AUTHOR = {Zhang, Cong and Li, Dongguang and Qi, Jiashuo and Liu, Jingtao and Wang, Yu},
TITLE = {Infrared Small Target Detection Method with Trajectory Correction Fuze Based on Infrared Image Sensor},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4522},
URL = {https://www.mdpi.com/1424-8220/21/13/4522},
PubMedID = {34282797},
ISSN = {1424-8220},
ABSTRACT = {Due to the complexity of background and diversity of small targets, robust detection of infrared small targets for the trajectory correction fuze has become a challenge. To solve this problem, different from the traditional method, a state-of-the-art detection method based on density-distance space is proposed to apply to the trajectory correction fuze. First, parameters of the infrared image sensor on the fuze are calculated to set the boundary limitations for the target detection method. Second, the density-distance space method is proposed to detect the candidate targets. Finally, the adaptive pixel growth (APG) algorithm is used to suppress the clutter so as to detect the real targets. Three experiments, including equivalent detection, simulation and hardware-in-loop, were implemented to verify the effectiveness of this method. Results illustrated that the infrared image sensor on the fuze has a stable field of view under rotation of the projectile, and could clearly observe the infrared small target. The proposed method has superior anti-noise, different size target detection, multi-target detection and various clutter suppression capability. Compared with six novel algorithms, our algorithm shows a perfect detection performance and acceptable time consumption.},
DOI = {10.3390/s21134522}
}



@Article{rs13132588,
AUTHOR = {Wang, Zhihao and Brenning, Alexander},
TITLE = {Active-Learning Approaches for Landslide Mapping Using Support Vector Machines},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2588},
URL = {https://www.mdpi.com/2072-4292/13/13/2588},
ISSN = {2072-4292},
ABSTRACT = {Ex post landslide mapping for emergency response and ex ante landslide susceptibility modelling for hazard mitigation are two important application scenarios that require the development of accurate, yet cost-effective spatial landslide models. However, the manual labelling of instances for training machine learning models is time-consuming given the data requirements of flexible data-driven algorithms and the small percentage of area covered by landslides. Active learning aims to reduce labelling costs by selecting more informative instances. In this study, two common active-learning strategies, uncertainty sampling and query by committee, are combined with the support vector machine (SVM), a state-of-the-art machine-learning technique, in a landslide mapping case study in order to assess their possible benefits compared to simple random sampling of training locations. By selecting more “informative” instances, the SVMs with active learning based on uncertainty sampling outperformed both random sampling and query-by-committee strategies when considering mean AUROC (area under the receiver operating characteristic curve) as performance measure. Uncertainty sampling also produced more stable performances with a smaller AUROC standard deviation across repetitions. In conclusion, under limited data conditions, uncertainty sampling reduces the amount of expert time needed by selecting more informative instances for SVM training. We therefore recommend incorporating active learning with uncertainty sampling into interactive landslide modelling workflows, especially in emergency response settings, but also in landslide susceptibility modelling.},
DOI = {10.3390/rs13132588}
}



@Article{s21134542,
AUTHOR = {Kaczorowska, Monika and Karczmarek, Paweł and Plechawska-Wójcik, Małgorzata and Tokovarov, Mikhail},
TITLE = {On the Improvement of Eye Tracking-Based Cognitive Workload Estimation Using Aggregation Functions},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4542},
URL = {https://www.mdpi.com/1424-8220/21/13/4542},
PubMedID = {34283098},
ISSN = {1424-8220},
ABSTRACT = {Cognitive workload, being a quantitative measure of mental effort, draws significant interest of researchers, as it allows to monitor the state of mental fatigue. Estimation of cognitive workload becomes especially important for job positions requiring outstanding engagement and responsibility, e.g., air-traffic dispatchers, pilots, car or train drivers. Cognitive workload estimation finds its applications also in the field of education material preparation. It allows to monitor the difficulty degree for specific tasks enabling to adjust the level of education materials to typical abilities of students. In this study, we present the results of research conducted with the goal of examining the influence of various fuzzy or non-fuzzy aggregation functions upon the quality of cognitive workload estimation. Various classic machine learning models were successfully applied to the problem. The results of extensive in-depth experiments with over 2000 aggregation operators shows the applicability of the approach based on the aggregation functions. Moreover, the approach based on aggregation process allows for further improvement of classification results. A wide range of aggregation functions is considered and the results suggest that the combination of classical machine learning models and aggregation methods allows to achieve high quality of cognitive workload level recognition preserving low computational cost.},
DOI = {10.3390/s21134542}
}



@Article{rs13132591,
AUTHOR = {Maxwell, Aaron E. and Warner, Timothy A. and Guillén, Luis Andrés},
TITLE = {Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies—Part 2: Recommendations and Best Practices},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2591},
URL = {https://www.mdpi.com/2072-4292/13/13/2591},
ISSN = {2072-4292},
ABSTRACT = {Convolutional neural network (CNN)-based deep learning (DL) has a wide variety of applications in the geospatial and remote sensing (RS) sciences, and consequently has been a focus of many recent studies. However, a review of accuracy assessment methods used in recently published RS DL studies, focusing on scene classification, object detection, semantic segmentation, and instance segmentation, indicates that RS DL papers appear to follow an accuracy assessment approach that diverges from that of traditional RS studies. Papers reporting on RS DL studies have largely abandoned traditional RS accuracy assessment terminology; they rarely reported a complete confusion matrix; and sampling designs and analysis protocols generally did not provide a population-based confusion matrix, in which the table entries are estimates of the probabilities of occurrence of the mapped landscape. These issues indicate the need for the RS community to develop guidance on best practices for accuracy assessment for CNN-based DL thematic mapping and object detection. As a first step in that process, we explore key issues, including the observation that accuracy assessments should not be biased by the CNN-based training and inference processes that rely on image chips. Furthermore, accuracy assessments should be consistent with prior recommendations and standards in the field, should support the estimation of a population confusion matrix, and should allow for assessment of model generalization. This paper draws from our review of the RS DL literature and the rich record of traditional remote sensing accuracy assessment research while considering the unique nature of CNN-based deep learning to propose accuracy assessment best practices that use appropriate sampling methods, training and validation data partitioning, assessment metrics, and reporting standards.},
DOI = {10.3390/rs13132591}
}



@Article{rs13132592,
AUTHOR = {Liu, Xiaobang and Liang, Shunlin and Li, Bing and Ma, Han and He, Tao},
TITLE = {Mapping 30 m Fractional Forest Cover over China’s Three-North Region from Landsat-8 Data Using Ensemble Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2592},
URL = {https://www.mdpi.com/2072-4292/13/13/2592},
ISSN = {2072-4292},
ABSTRACT = {The accurate monitoring of forest cover and its changes are essential for environmental change research, but current satellite products for forest coverage carry many uncertainties. This study used 30-m Landsat-8 data, and aggregated 1-m GaoFen-2 (GF-2) satellite images to construct the training samples and used multiple machine learning algorithms (MLAs) to estimate the fractional forest cover (FFC) in China’s Three North Region (TNR). In this study, multiple MLAs were merged to construct stacked generalization (SG) models based on the idea of SG, and the performances of the MLAs in the FFC estimation were evaluated. The results of the 10-fold cross-validation showed that all non-linear algorithms had a good performance, with an R2 value of greater than 0.8 and a root-mean square error (RMSE) of less than 0.05. In the bagging ensemble, the random forest (RF) (R2 = 0.993, RMSE = 0.020) model performed the best and in the boosting ensemble, the light gradient boosted machine (LGBM) (R2 = 0.992, RMSE = 0.022) performed the best. Although the evaluation index of the RF is slightly better than that of the LGBM, the independent validation results show that the two models have similar performances. The model evaluation results of the independent datasets showed that, in the SG model, the performance of the SG(LGBM) (R2 = 0.991, RMSE = 0.034) was better than that of the single or non-ensemble model. Comparing the FFC estimates of our model with those of existing datasets showed that our model exhibited more forest spatial distribution details and higher accuracy in complex landscapes. Overall, in this study, the method of using high-resolution remote sensing (RS) images to extract samples for FFC estimation is feasible. Our results demonstrate the potential of the ensemble MLAs to map the FFC. The research results also show that among many MALs, the RF algorithm is the most suitable algorithm for estimating FFC, which provides a reference for future research.},
DOI = {10.3390/rs13132592}
}



@Article{horticulturae7070176,
AUTHOR = {Duarte-Carvajalino, Julio Martin and Silva-Arero, Elías Alexander and Góez-Vinasco, Gerardo Antonio and Torres-Delgado, Laura Marcela and Ocampo-Paez, Oscar Dubán and Castaño-Marín, Angela María},
TITLE = {Estimation of Water Stress in Potato Plants Using Hyperspectral Imagery and Machine Learning Algorithms},
JOURNAL = {Horticulturae},
VOLUME = {7},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {176},
URL = {https://www.mdpi.com/2311-7524/7/7/176},
ISSN = {2311-7524},
ABSTRACT = {This work presents quantitative detection of water stress and estimation of the water stress level: none, light, moderate, and severe on potato crops. We use hyperspectral imagery and state of the art machine learning algorithms: random decision forest, multilayer perceptron, convolutional neural networks, support vector machines, extreme gradient boost, and AdaBoost. The detection and estimation of water stress in potato crops is carried out on two different phenological stages of the plants: tubers differentiation and maximum tuberization. The machine learning algorithms are trained with a small subset of each hyperspectral image corresponding to the plant canopy. The results are improved using majority voting to classify all the canopy pixels in the hyperspectral images. The results indicate that both detection of water stress and estimation of the level of water stress can be obtained with good accuracy, improved further by majority voting. The importance of each band of the hyperspectral images in the classification of the images is assessed by random forest and extreme gradient boost, which are the machine learning algorithms that perform best overall on both phenological stages and detection and estimation of water stress in potato crops.},
DOI = {10.3390/horticulturae7070176}
}



@Article{rs13132596,
AUTHOR = {Mohan, Midhun and Richardson, Gabriella and Gopan, Gopika and Aghai, Matthew Mehdi and Bajaj, Shaurya and Galgamuwa, G. A. Pabodha and Vastaranta, Mikko and Arachchige, Pavithra S. Pitumpe and Amorós, Lot and Corte, Ana Paula Dalla and de-Miguel, Sergio and Leite, Rodrigo Vieira and Kganyago, Mahlatse and Broadbent, Eben North and Doaemo, Willie and Shorab, Mohammed Abdullah Bin and Cardil, Adrian},
TITLE = {UAV-Supported Forest Regeneration: Current Trends, Challenges and Implications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2596},
URL = {https://www.mdpi.com/2072-4292/13/13/2596},
ISSN = {2072-4292},
ABSTRACT = {Replanting trees helps with avoiding desertification, reducing the chances of soil erosion and flooding, minimizing the risks of zoonotic disease outbreaks, and providing ecosystem services and livelihood to the indigenous people, in addition to sequestering carbon dioxide for mitigating climate change. Consequently, it is important to explore new methods and technologies that are aiming to upscale and fast-track afforestation and reforestation (A/R) endeavors, given that many of the current tree planting strategies are not cost effective over large landscapes, and suffer from constraints associated with time, energy, manpower, and nursery-based seedling production. UAV (unmanned aerial vehicle)-supported seed sowing (UAVsSS) can promote rapid A/R in a safe, cost-effective, fast and environmentally friendly manner, if performed correctly, even in otherwise unsafe and/or inaccessible terrains, supplementing the overall manual planting efforts globally. In this study, we reviewed the recent literature on UAVsSS, to analyze the current status of the technology. Primary UAVsSS applications were found to be in areas of post-wildfire reforestation, mangrove restoration, forest restoration after degradation, weed eradication, and desert greening. Nonetheless, low survival rates of the seeds, future forest diversity, weather limitations, financial constraints, and seed-firing accuracy concerns were determined as major challenges to operationalization. Based on our literature survey and qualitative analysis, twelve recommendations—ranging from the need for publishing germination results to linking UAVsSS operations with carbon offset markets—are provided for the advancement of UAVsSS applications.},
DOI = {10.3390/rs13132596}
}



@Article{s21134549,
AUTHOR = {Hussein, Burhan Rashid and Malik, Owais Ahmed and Ong, Wee-Hong and Slik, Johan Willem Frederik},
TITLE = {Automated Extraction of Phenotypic Leaf Traits of Individual Intact Herbarium Leaves from Herbarium Specimen Images Using Deep Learning Based Semantic Segmentation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4549},
URL = {https://www.mdpi.com/1424-8220/21/13/4549},
PubMedID = {34283110},
ISSN = {1424-8220},
ABSTRACT = {With the increase in the digitization efforts of herbarium collections worldwide, dataset repositories such as iDigBio and GBIF now have hundreds of thousands of herbarium sheet images ready for exploration. Although this serves as a new source of plant leaves data, herbarium datasets have an inherent challenge to deal with the sheets containing other non-plant objects such as color charts, barcodes, and labels. Even for the plant part itself, a combination of different overlapping, damaged, and intact individual leaves exist together with other plant organs such as stems and fruits, which increases the complexity of leaf trait extraction and analysis. Focusing on segmentation and trait extraction on individual intact herbarium leaves, this study proposes a pipeline consisting of deep learning semantic segmentation model (DeepLabv3+), connected component analysis, and a single-leaf classifier trained on binary images to automate the extraction of an intact individual leaf with phenotypic traits. The proposed method achieved a higher F1-score for both the in-house dataset (96%) and on a publicly available herbarium dataset (93%) compared to object detection-based approaches including Faster R-CNN and YOLOv5. Furthermore, using the proposed approach, the phenotypic measurements extracted from the segmented individual leaves were closer to the ground truth measurements, which suggests the importance of the segmentation process in handling background noise. Compared to the object detection-based approaches, the proposed method showed a promising direction toward an autonomous tool for the extraction of individual leaves together with their trait data directly from herbarium specimen images.},
DOI = {10.3390/s21134549}
}



@Article{rs13132598,
AUTHOR = {Li, Jian and Chen, Baozhang},
TITLE = {Optimal Solar Zenith Angle Definition for Combined Landsat-8 and Sentinel-2A/2B Data Angular Normalization Using Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2598},
URL = {https://www.mdpi.com/2072-4292/13/13/2598},
ISSN = {2072-4292},
ABSTRACT = {Data from Landsat-8 and Sentinel-2A/2B are often combined for terrestrial monitoring because of their similar spectral bands. The bidirectional reflectance distribution function (BRDF) effect has been observed in both Landsat-8 and Sentinel-2A/2B reflectance data. However, there is currently no definition of solar zenith angle (θsz) that is suitable for the normalization of the BRDF-adjusted reflectance from the three sensors’ combined data. This paper describes the use of four machine learning (ML) models to predict a global θsz that is suitable for the normalization of bidirectional reflectance from the combined data in 2018. The observed θsz collected globally, and the three locations in the Democratic Republic of Congo (26.622°E, 0.356°N), Texas in the USA (99.406°W 30.751°N), and Finland (25.194°E, 61.653°N), are chosen to compare the performance of the ML models. At a global scale, the ML models of Support Vector Regression (SVR), Multi-Layer Perception (MLP), and Gaussian Process Regression (GPR) exhibit comparably good performance to that of polynomial regression, considering center latitude as the input to predict the global θsz. GPR achieves the best overall performance considering the center latitude and acquisition time as inputs, with a root mean square error (RMSE) of 1.390°, a mean absolute error (MAE) of 0.689°, and a coefficient of determination (R2) of 0.994. SVR shows an RMSE of 1.396°, an MAE of 0.638°, and an R2 of 0.994, following GPR. For a specific location, the SVR and GPR models have higher accuracy than the polynomial regression, with GPR exhibiting the best performance, when center latitude and acquisition time are considered as inputs. GPR is recommended for predicting the global θsz using the three sensors’ combined data.},
DOI = {10.3390/rs13132598}
}



