
@Article{s18030712,
AUTHOR = {Zhao, Yi and Ma, Jiale and Li, Xiaohui and Zhang, Jie},
TITLE = {Saliency Detection and Deep Learning-Based Wildfire Identification in UAV Imagery},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {712},
URL = {https://www.mdpi.com/1424-8220/18/3/712},
ISSN = {1424-8220},
ABSTRACT = {An unmanned aerial vehicle (UAV) equipped with global positioning systems (GPS) can provide direct georeferenced imagery, mapping an area with high resolution. So far, the major difficulty in wildfire image classification is the lack of unified identification marks, the fire features of color, shape, texture (smoke, flame, or both) and background can vary significantly from one scene to another. Deep learning (e.g., DCNN for Deep Convolutional Neural Network) is very effective in high-level feature learning, however, a substantial amount of training images dataset is obligatory in optimizing its weights value and coefficients. In this work, we proposed a new saliency detection algorithm for fast location and segmentation of core fire area in aerial images. As the proposed method can effectively avoid feature loss caused by direct resizing; it is used in data augmentation and formation of a standard fire image dataset ‘UAV_Fire’. A 15-layered self-learning DCNN architecture named ‘Fire_Net’ is then presented as a self-learning fire feature exactor and classifier. We evaluated different architectures and several key parameters (drop out ratio, batch size, etc.) of the DCNN model regarding its validation accuracy. The proposed architecture outperformed previous methods by achieving an overall accuracy of 98%. Furthermore, ‘Fire_Net’ guarantied an average processing speed of 41.5 ms per image for real-time wildfire inspection. To demonstrate its practical utility, Fire_Net is tested on 40 sampled images in wildfire news reports and all of them have been accurately identified.},
DOI = {10.3390/s18030712}
}



@Article{s18030801,
AUTHOR = {Czúni, László and Rashad, Metwally},
TITLE = {Lightweight Active Object Retrieval with Weak Classifiers},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {801},
URL = {https://www.mdpi.com/1424-8220/18/3/801},
ISSN = {1424-8220},
ABSTRACT = {In the last few years, there has been a steadily growing interest in autonomous vehicles and robotic systems. While many of these agents are expected to have limited resources, these systems should be able to dynamically interact with other objects in their environment. We present an approach where lightweight sensory and processing techniques, requiring very limited memory and processing power, can be successfully applied to the task of object retrieval using sensors of different modalities. We use the Hough framework to fuse optical and orientation information of the different views of the objects. In the presented spatio-temporal perception technique, we apply active vision, where, based on the analysis of initial measurements, the direction of the next view is determined to increase the hit-rate of retrieval. The performance of the proposed methods is shown on three datasets loaded with heavy noise.},
DOI = {10.3390/s18030801}
}



@Article{w10030297,
AUTHOR = {Ridolfi, Elena and Manciola, Piergiorgio},
TITLE = {Water Level Measurements from Drones: A Pilot Case Study at a Dam Site},
JOURNAL = {Water},
VOLUME = {10},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {297},
URL = {https://www.mdpi.com/2073-4441/10/3/297},
ISSN = {2073-4441},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are now filling in the gaps between spaceborne and ground-based observations and enhancing the spatial resolution and temporal coverage of data acquisition. In the realm of hydrological observations, UAVs play a key role in quantitatively characterizing the surface flow, allowing for remotely accessing the water body of interest. In this paper, we propose a technology that uses a sensing platform encompassing a drone and a camera to determine the water level. The images acquired by means of the sensing platform are then analyzed using the Canny method to detect the edges of water level and of Ground Control Points (GCPs) used as reference points. The water level is then retrieved from images and compared to a benchmark value obtained by a traditional device. The method is tested at four locations in an artificial lake in central Italy. Results are encouraging, as the overall mean error between estimated and true water level values is around 0.05 m. This technology is well suited to improve hydraulic modeling and thus provides reliable support to flood mitigation strategies.},
DOI = {10.3390/w10030297}
}



@Article{info9030061,
AUTHOR = {Wei, Jian and Liu, Feng},
TITLE = {Online Learning of Discriminative Correlation Filter Bank for Visual Tracking},
JOURNAL = {Information},
VOLUME = {9},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {61},
URL = {https://www.mdpi.com/2078-2489/9/3/61},
ISSN = {2078-2489},
ABSTRACT = {Accurate visual tracking is a challenging research topic in the field of computer vision. The challenge emanates from various issues, such as target deformation, background clutter, scale variations, and occlusion. In this setting, discriminative correlation filter (DCF)-based trackers have demonstrated excellent performance in terms of speed. However, existing correlation filter-based trackers cannot handle major changes in appearance due to severe occlusions, which eventually result in the development of a bounding box for target drift tracking. In this study, we use a set of DCFs called discriminative correlation filter bank (DCFB) for visual tracking to address the key causes of object occlusion and drift in a tracking-by-detection framework. In this work, we treat thxe current location of the target frame as the center, extract several samples around the target, and perform online learning of DCFB. The sliding window then extracts numerous samples within a large radius of the area where the object in the next frame is previously located. These samples are used for the DCFB to perform correlation operation in the Fourier domain to estimate the location of the new object; the coordinates of the largest correlation scores indicate the position of the new target. The DCFB is updated according to the location of the new target. Experimental results on the quantitative and qualitative evaluations on the challenging benchmark sequences show that the proposed framework improves tracking performance compared with several state-of-the-art trackers.},
DOI = {10.3390/info9030061}
}



@Article{s18030924,
AUTHOR = {Zhang, Duona and Ding, Wenrui and Zhang, Baochang and Xie, Chunyu and Li, Hongguang and Liu, Chunhui and Han, Jungong},
TITLE = {Automatic Modulation Classification Based on Deep Learning for Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {924},
URL = {https://www.mdpi.com/1424-8220/18/3/924},
ISSN = {1424-8220},
ABSTRACT = {Deep learning has recently attracted much attention due to its excellent performance in processing audio, image, and video data. However, few studies are devoted to the field of automatic modulation classification (AMC). It is one of the most well-known research topics in communication signal recognition and remains challenging for traditional methods due to complex disturbance from other sources. This paper proposes a heterogeneous deep model fusion (HDMF) method to solve the problem in a unified framework. The contributions include the following: (1) a convolutional neural network (CNN) and long short-term memory (LSTM) are combined by two different ways without prior knowledge involved; (2) a large database, including eleven types of single-carrier modulation signals with various noises as well as a fading channel, is collected with various signal-to-noise ratios (SNRs) based on a real geographical environment; and (3) experimental results demonstrate that HDMF is very capable of coping with the AMC problem, and achieves much better performance when compared with the independent network.},
DOI = {10.3390/s18030924}
}



@Article{s18040967,
AUTHOR = {Pang, Jingyue and Liu, Datong and Peng, Yu and Peng, Xiyuan},
TITLE = {Optimize the Coverage Probability of Prediction Interval for Anomaly Detection of Sensor-Based Monitoring Series},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {967},
URL = {https://www.mdpi.com/1424-8220/18/4/967},
ISSN = {1424-8220},
ABSTRACT = {Effective anomaly detection of sensing data is essential for identifying potential system failures. Because they require no prior knowledge or accumulated labels, and provide uncertainty presentation, the probability prediction methods (e.g., Gaussian process regression (GPR) and relevance vector machine (RVM)) are especially adaptable to perform anomaly detection for sensing series. Generally, one key parameter of prediction models is coverage probability (CP), which controls the judging threshold of the testing sample and is generally set to a default value (e.g., 90% or 95%). There are few criteria to determine the optimal CP for anomaly detection. Therefore, this paper designs a graphic indicator of the receiver operating characteristic curve of prediction interval (ROC-PI) based on the definition of the ROC curve which can depict the trade-off between the PI width and PI coverage probability across a series of cut-off points. Furthermore, the Youden index is modified to assess the performance of different CPs, by the minimization of which the optimal CP is derived by the simulated annealing (SA) algorithm. Experiments conducted on two simulation datasets demonstrate the validity of the proposed method. Especially, an actual case study on sensing series from an on-orbit satellite illustrates its significant performance in practical application.},
DOI = {10.3390/s18040967}
}



@Article{w10040391,
AUTHOR = {Lee, Yoo Jin and Park, Chuljin and Lee, Mi Lim},
TITLE = {Identification of a Contaminant Source Location in a River System Using Random Forest Models},
JOURNAL = {Water},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {391},
URL = {https://www.mdpi.com/2073-4441/10/4/391},
ISSN = {2073-4441},
ABSTRACT = {We consider the problem of identifying the source location of a contaminant via analyzing changes in concentration levels observed by a sensor network in a river system. To address this problem, we propose a framework including two main steps: (i) pre-processing data; and (ii) training and testing a classification model. Specifically, we first obtain a data set presenting concentration levels of a contaminant from a simulation model, and extract numerical characteristics from the data set. Then, random forest models are generated and assessed to identify the source location of a contaminant. By using the numerical characteristics from the prior step as their inputs, the models provide outputs representing the possibility, i.e., a value between 0 and 1, of a spill event at each candidate location. The performance of the framework is tested on a part of the Altamaha river system in the state of Georgia, United States of America.},
DOI = {10.3390/w10040391}
}



@Article{su10041072,
AUTHOR = {Kim, KeumJi and Yoon, SeongHwan},
TITLE = {Assessment of Building Damage Risk by Natural Disasters in South Korea Using Decision Tree Analysis},
JOURNAL = {Sustainability},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {1072},
URL = {https://www.mdpi.com/2071-1050/10/4/1072},
ISSN = {2071-1050},
ABSTRACT = {The purpose of this study is to identify the relationship between weather variables and buildings damaged in natural disasters. We used four datasets on building damage history and 33 weather datasets from 230 regions in South Korea in a decision tree analysis to evaluate the risk of building damage. We generated the decision tree model to determine the risk of rain, gale, and typhoon (excluding gale with less damage). Using the weight and limit values of the weather variables derived using the decision tree model, the risk of building damage was assessed for 230 regions in South Korea until 2100. The number of regions at risk of rain damage increased by more than 30% on average. Conversely, regions at risk of damage from snowfall decreased by more than 90%. The regions at risk of typhoons decreased by 57.5% on average, while those at high risk of the same increased by up to 62.5% under RCP 8.5. The results of this study are highly fluid since they are based on the uncertainty of future climate change. However, the study is meaningful because it suggests a new method for assessing disaster risk using weather indices.},
DOI = {10.3390/su10041072}
}



@Article{s18041120,
AUTHOR = {Li, He and Liu, Gaohuan and Liu, Qingsheng and Chen, Zhongxin and Huang, Chong},
TITLE = {Retrieval of Winter Wheat Leaf Area Index from Chinese GF-1 Satellite Data Using the PROSAIL Model},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {1120},
URL = {https://www.mdpi.com/1424-8220/18/4/1120},
ISSN = {1424-8220},
ABSTRACT = {Leaf area index (LAI) is one of the key biophysical parameters in crop structure. The accurate quantitative estimation of crop LAI is essential to verify crop growth and health. The PROSAIL radiative transfer model (RTM) is one of the most established methods for estimating crop LAI. In this study, a look-up table (LUT) based on the PROSAIL RTM was first used to estimate winter wheat LAI from GF-1 data, which accounted for some available prior knowledge relating to the distribution of winter wheat characteristics. Next, the effects of 15 LAI-LUT strategies with reflectance bands and 10 LAI-LUT strategies with vegetation indexes on the accuracy of the winter wheat LAI retrieval with different phenological stages were evaluated against in situ LAI measurements. The results showed that the LUT strategies of LAI-GNDVI were optimal and had the highest accuracy with a root mean squared error (RMSE) value of 0.34, and a coefficient of determination (R2) of 0.61 during the elongation stages, and the LUT strategies of LAI-Green were optimal with a RMSE of 0.74, and R2 of 0.20 during the grain-filling stages. The results demonstrated that the PROSAIL RTM had great potential in winter wheat LAI inversion with GF-1 satellite data and the performance could be improved by selecting the appropriate LUT inversion strategies in different growth periods.},
DOI = {10.3390/s18041120}
}



@Article{computers7020023,
AUTHOR = {Bowkett, Mark and Thanapalan, Kary and Constant, Ewen},
TITLE = {Failure Detection of Composites with Control System Corrective Response in Drone System Applications},
JOURNAL = {Computers},
VOLUME = {7},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2073-431X/7/2/23},
ISSN = {2073-431X},
ABSTRACT = {The paper describes a novel method for the detection of damage in carbon composites as used in drone frames. When damage is detected a further novel corrective response is initiated in the quadcopter flight controller to switch from a four-arm control system to a three-arm control system. This is made possible as a symmetrical frame is utilized, which allows for a balanced weight distribution between both the undamaged quadcopter and the fallback tri-copter layout. The resulting work allows for continued flight where this was not previously possible. Further developing work includes improved flight stability with the aid of an underslung load model. This is beneficial to the quadcopter as a damaged arm attached to the main body by the motor wires behaves as an underslung load. The underslung load works are also transferable in a dual master and slave drone system where the master drone transports a smaller slave drone by a tether, which acts as an underslung load.},
DOI = {10.3390/computers7020023}
}



@Article{drones2020015,
AUTHOR = {Polvara, Riccardo and Sharma, Sanjay and Wan, Jian and Manning, Andrew and Sutton, Robert},
TITLE = {Vision-Based Autonomous Landing of a Quadrotor on the Perturbed Deck of an Unmanned Surface Vehicle},
JOURNAL = {Drones},
VOLUME = {2},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2504-446X/2/2/15},
ISSN = {2504-446X},
ABSTRACT = {Autonomous landing on the deck of an unmanned surface vehicle (USV) is still a major challenge for unmanned aerial vehicles (UAVs). In this paper, a fiducial marker is located on the platform so as to facilitate the task since it is possible to retrieve its six-degrees of freedom relative-pose in an easy way. To compensate interruption in the marker’s observations, an extended Kalman filter (EKF) estimates the current USV’s position with reference to the last known position. Validation experiments have been performed in a simulated environment under various marine conditions. The results confirmed that the EKF provides estimates accurate enough to direct the UAV in proximity of the autonomous vessel such that the marker becomes visible again. Using only the odometry and the inertial measurements for the estimation, this method is found to be applicable even under adverse weather conditions in the absence of the global positioning system.},
DOI = {10.3390/drones2020015}
}



@Article{rs10040618,
AUTHOR = {Al-Saddik, Hania and Laybros, Anthony and Billiot, Bastien and Cointault, Frederic},
TITLE = {Using Image Texture and Spectral Reflectance Analysis to Detect Yellowness and Esca in Grapevines at Leaf-Level},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {618},
URL = {https://www.mdpi.com/2072-4292/10/4/618},
ISSN = {2072-4292},
ABSTRACT = {Plant diseases are one of the main reasons behind major economic and production losses in the agricultural field. Current research activities enable large fields monitoring and plant disease detection using innovative and robust technologies. French grapevines have a reputation for producing premium quality wines, however, these major fruit crops are susceptible to many diseases, including Esca, Downy mildew, Powdery mildew, Yellowing, and many others. In this study, we focused on two main infections (Esca and Yellowing), and data were gathered from fields that were located in Aquitaine and Burgundy regions, France. Since plant diseases can be diagnosed from the properties of the leaf, we acquired both Red-Green-Blue (RGB) digital image and hyperspectral reflectance data from infected and healthy leaves. Biophysical parameters that were produced by the PROSPECT model inversion together with texture parameters compiled from the literature were deduced. Then we investigated their relationship to damage caused by Yellowing and Esca. This study examined whether spectral and textural data can identify the two diseases through the use of Neural Networks. We obtained an overall accuracy of 99% for both of the diseases when textural and spectral data are combined. These results suggest that, first, biophysical parameters present a valid dimension reduction tool that could replace the use of complete hyperspectral data. Second, remote sensing using spectral reflectance and digital images can make an overall nondestructive, rapid, cost-effective, and reproducible technique to determine diseases in grapevines with a good level of accuracy.},
DOI = {10.3390/rs10040618}
}



@Article{rs10040624,
AUTHOR = {Zhuo, Xiangyu and Fraundorfer, Friedrich and Kurz, Franz and Reinartz, Peter},
TITLE = {Optimization of OpenStreetMap Building Footprints Based on Semantic Information of Oblique UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {624},
URL = {https://www.mdpi.com/2072-4292/10/4/624},
ISSN = {2072-4292},
ABSTRACT = {Building footprint information is vital for 3D building modeling. Traditionally, in remote sensing, building footprints are extracted and delineated from aerial imagery and/or LiDAR point cloud. Taking a different approach, this paper is dedicated to the optimization of OpenStreetMap (OSM) building footprints exploiting the contour information, which is derived from deep learning-based semantic segmentation of oblique images acquired by the Unmanned Aerial Vehicle (UAV). First, a simplified 3D building model of Level of Detail 1 (LoD 1) is initialized using the footprint information from OSM and the elevation information from Digital Surface Model (DSM). In parallel, a deep neural network for pixel-wise semantic image segmentation is trained in order to extract the building boundaries as contour evidence. Subsequently, an optimization integrating the contour evidence from multi-view images as a constraint results in a refined 3D building model with optimized footprints and height. Our method is leveraged to optimize OSM building footprints for four datasets with different building types, demonstrating robust performance for both individual buildings and multiple buildings regardless of image resolution. Finally, we compare our result with reference data from German Authority Topographic-Cartographic Information System (ATKIS). Quantitative and qualitative evaluations reveal that the original OSM building footprints have large offset, but can be significantly improved from meter level to decimeter level after optimization.},
DOI = {10.3390/rs10040624}
}



@Article{rs10040641,
AUTHOR = {Manfreda, Salvatore and McCabe, Matthew F. and Miller, Pauline E. and Lucas, Richard and Pajuelo Madrigal, Victor and Mallinis, Giorgos and Ben Dor, Eyal and Helman, David and Estes, Lyndon and Ciraolo, Giuseppe and Müllerová, Jana and Tauro, Flavia and De Lima, M. Isabel and De Lima, João L. M. P. and Maltese, Antonino and Frances, Felix and Caylor, Kelly and Kohv, Marko and Perks, Matthew and Ruiz-Pérez, Guiomar and Su, Zhongbo and Vico, Giulia and Toth, Brigitta},
TITLE = {On the Use of Unmanned Aerial Systems for Environmental Monitoring},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {641},
URL = {https://www.mdpi.com/2072-4292/10/4/641},
ISSN = {2072-4292},
ABSTRACT = {Environmental monitoring plays a central role in diagnosing climate and management impacts on natural and agricultural systems; enhancing the understanding of hydrological processes; optimizing the allocation and distribution of water resources; and assessing, forecasting, and even preventing natural disasters. Nowadays, most monitoring and data collection systems are based upon a combination of ground-based measurements, manned airborne sensors, and satellite observations. These data are utilized in describing both small- and large-scale processes, but have spatiotemporal constraints inherent to each respective collection system. Bridging the unique spatial and temporal divides that limit current monitoring platforms is key to improving our understanding of environmental systems. In this context, Unmanned Aerial Systems (UAS) have considerable potential to radically improve environmental monitoring. UAS-mounted sensors offer an extraordinary opportunity to bridge the existing gap between field observations and traditional air- and space-borne remote sensing, by providing high spatial detail over relatively large areas in a cost-effective way and an entirely new capacity for enhanced temporal retrieval. As well as showcasing recent advances in the field, there is also a need to identify and understand the potential limitations of UAS technology. For these platforms to reach their monitoring potential, a wide spectrum of unresolved issues and application-specific challenges require focused community attention. Indeed, to leverage the full potential of UAS-based approaches, sensing technologies, measurement protocols, postprocessing techniques, retrieval algorithms, and evaluation techniques need to be harmonized. The aim of this paper is to provide an overview of the existing research and applications of UAS in natural and agricultural ecosystem monitoring in order to identify future directions, applications, developments, and challenges.},
DOI = {10.3390/rs10040641}
}



@Article{rs10050661,
AUTHOR = {Krylov, Vladimir A. and Kenny, Eamonn and Dahyot, Rozenn},
TITLE = {Automatic Discovery and Geotagging of Objects from Street View Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {661},
URL = {https://www.mdpi.com/2072-4292/10/5/661},
ISSN = {2072-4292},
ABSTRACT = {Many applications, such as autonomous navigation, urban planning, and asset monitoring, rely on the availability of accurate information about objects and their geolocations. In this paper, we propose the automatic detection and computation of the coordinates of recurring stationary objects of interest using street view imagery. Our processing pipeline relies on two fully convolutional neural networks: the first segments objects in the images, while the second estimates their distance from the camera. To geolocate all the detected objects coherently we propose a novel custom Markov random field model to estimate the objects&rsquo; geolocation. The novelty of the resulting pipeline is the combined use of monocular depth estimation and triangulation to enable automatic mapping of complex scenes with the simultaneous presence of multiple, visually similar objects of interest. We validate experimentally the effectiveness of our approach on two object classes: traffic lights and telegraph poles. The experiments report high object recall rates and position precision of approximately 2 m, which is approaching the precision of single-frequency GPS receivers.},
DOI = {10.3390/rs10050661}
}



@Article{s18051307,
AUTHOR = {Na, Wongi S. and Baek, Jongdae},
TITLE = {A Review of the Piezoelectric Electromechanical Impedance Based Structural Health Monitoring Technique for Engineering Structures},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1307},
URL = {https://www.mdpi.com/1424-8220/18/5/1307},
ISSN = {1424-8220},
ABSTRACT = {The birth of smart materials such as piezoelectric (PZT) transducers has aided in revolutionizing the field of structural health monitoring (SHM) based on non-destructive testing (NDT) methods. While a relatively new NDT method known as the electromechanical (EMI) technique has been investigated for more than two decades, there are still various problems that must be solved before it is applied to real structures. The technique, which has a significant potential to contribute to the creation of one of the most effective SHM systems, involves the use of a single PZT for exciting and sensing of the host structure. In this paper, studies applied for the past decade related to the EMI technique have been reviewed to understand its trend. In addition, new concepts and ideas proposed by various authors are also surveyed, and the paper concludes with a discussion of the potential directions for future works.},
DOI = {10.3390/s18051307}
}



@Article{machines6020018,
AUTHOR = {De Simone, Marco Claudio and Rivera, Zandra Betzabe and Guida, Domenico},
TITLE = {Obstacle Avoidance System for Unmanned Ground Vehicles by Using Ultrasonic Sensors},
JOURNAL = {Machines},
VOLUME = {6},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2075-1702/6/2/18},
ISSN = {2075-1702},
ABSTRACT = {Artificial intelligence is the ability of a computer to perform the functions and reasoning typical of the human mind. In its purely informatic aspect, it includes the theory and techniques for the development of algorithms that allow machines to show an intelligent ability and/or perform an intelligent activity, at least in specific areas. In particular, there are automatic learning algorithms based on the same mechanisms that are thought to be the basis of all the cognitive processes developed by the human brain. Such a powerful tool has already started to produce a new class of self-driving vehicles. With the projections of population growth that will increase until the year 2100 up to 11.2 billion, research on innovating agricultural techniques must be continued. In order to improve the efficiency regarding precision agriculture, the use of autonomous agricultural machines must become an important issue. For this reason, it was decided to test the use of the &ldquo;Neural Network Toolbox&rdquo; tool already present in MATLAB to design an artificial neural network with supervised learning suitable for classification and pattern recognition by using data collected by an ultrasonic sensor. The idea is to use such a protocol to retrofit kits for agricultural machines already present on the market.},
DOI = {10.3390/machines6020018}
}



@Article{s18051379,
AUTHOR = {Chen, Xi and Kopsaftopoulos, Fotis and Wu, Qi and Ren, He and Chang, Fu-Kuo},
TITLE = {Flight State Identification of a Self-Sensing Wing via an Improved Feature Selection Method and Machine Learning Approaches},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1379},
URL = {https://www.mdpi.com/1424-8220/18/5/1379},
ISSN = {1424-8220},
ABSTRACT = {In this work, a data-driven approach for identifying the flight state of a self-sensing wing structure with an embedded multi-functional sensing network is proposed. The flight state is characterized by the structural vibration signals recorded from a series of wind tunnel experiments under varying angles of attack and airspeeds. A large feature pool is created by extracting potential features from the signals covering the time domain, the frequency domain as well as the information domain. Special emphasis is given to feature selection in which a novel filter method is developed based on the combination of a modified distance evaluation algorithm and a variance inflation factor. Machine learning algorithms are then employed to establish the mapping relationship from the feature space to the practical state space. Results from two case studies demonstrate the high identification accuracy and the effectiveness of the model complexity reduction via the proposed method, thus providing new perspectives of self-awareness towards the next generation of intelligent air vehicles.},
DOI = {10.3390/s18051379}
}



@Article{rs10050706,
AUTHOR = {Moy de Vitry, Matthew and Schindler, Konrad and Rieckermann, Jörg and Leitão, João P.},
TITLE = {Sewer Inlet Localization in UAV Image Clouds: Improving Performance with Multiview Detection},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {706},
URL = {https://www.mdpi.com/2072-4292/10/5/706},
ISSN = {2072-4292},
ABSTRACT = {Sewer and drainage infrastructure are often not as well catalogued as they should be, considering the immense investment they represent. In this work, we present a fully automatic framework for localizing sewer inlets from image clouds captured from an unmanned aerial vehicle (UAV). The framework exploits the high image overlap of UAV imaging surveys with a multiview approach to improve detection performance. The framework uses a Viola–Jones classifier trained to detect sewer inlets in aerial images with a ground sampling distance of 3–3.5 cm/pixel. The detections are then projected into three-dimensional space where they are clustered and reclassified to discard false positives. The method is evaluated by cross-validating results from an image cloud of 252 UAV images captured over a 0.57-km2 study area with 228 sewer inlets. Compared to an equivalent single-view detector, the multiview approach improves both recall and precision, increasing average precision from 0.65 to 0.73. The source code and case study data are publicly available for reuse.},
DOI = {10.3390/rs10050706}
}



@Article{s18051474,
AUTHOR = {Kamminga, Jacob and Ayele, Eyuel and Meratnia, Nirvana and Havinga, Paul},
TITLE = {Poaching Detection Technologies—A Survey},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1474},
URL = {https://www.mdpi.com/1424-8220/18/5/1474},
ISSN = {1424-8220},
ABSTRACT = {Between 1960 and 1990, 95% of the black rhino population in the world was killed. In South Africa, a rhino was killed every 8 h for its horn throughout 2016. Wild animals, rhinos and elephants, in particular, are facing an ever increasing poaching crisis. In this paper, we review poaching detection technologies that aim to save endangered species from extinction. We present requirements for effective poacher detection and identify research challenges through the survey. We describe poaching detection technologies in four domains: perimeter based, ground based, aerial based, and animal tagging based technologies. Moreover, we discuss the different types of sensor technologies that are used in intruder detection systems such as: radar, magnetic, acoustic, optic, infrared and thermal, radio frequency, motion, seismic, chemical, and animal sentinels. The ultimate long-term solution for the poaching crisis is to remove the drivers of demand by educating people in demanding countries and raising awareness of the poaching crisis. Until prevention of poaching takes effect, there will be a continuous urgent need for new (combined) approaches that take up the research challenges and provide better protection against poaching in wildlife areas.},
DOI = {10.3390/s18051474}
}



@Article{ijgi7050182,
AUTHOR = {Deng, Zhipeng and Sun, Hao and Zhou, Shilin},
TITLE = {Semi-Supervised Ground-to-Aerial Adaptation with Heterogeneous Features Learning for Scene Classification},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {182},
URL = {https://www.mdpi.com/2220-9964/7/5/182},
ISSN = {2220-9964},
ABSTRACT = {Currently, huge quantities of remote sensing images (RSIs) are becoming available. Nevertheless, the scarcity of labeled samples hinders the semantic understanding of RSIs. Fortunately, many ground-level image datasets with detailed semantic annotations have been collected in the vision community. In this paper, we attempt to exploit the abundant labeled ground-level images to build discriminative models for overhead-view RSI classification. However, images from the ground-level and overhead view are represented by heterogeneous features with different distributions; how to effectively combine multiple features and reduce the mismatch of distributions are two key problems in this scene-model transfer task. Specifically, a semi-supervised manifold-regularized multiple-kernel-learning (SMRMKL) algorithm is proposed for solving these problems. We employ multiple kernels over several features to learn an optimal combined model automatically. Multi-kernel Maximum Mean Discrepancy (MK-MMD) is utilized to measure the data mismatch. To make use of unlabeled target samples, a manifold regularized semi-supervised learning process is incorporated into our framework. Extensive experimental results on both cross-view and aerial-to-satellite scene datasets demonstrate that: (1) SMRMKL has an appealing extension ability to effectively fuse different types of visual features; and (2) manifold regularization can improve the adaptation performance by utilizing unlabeled target samples.},
DOI = {10.3390/ijgi7050182}
}



@Article{f9050268,
AUTHOR = {Lee, Junghee and Im, Jungho and Kim, Kyungmin and Quackenbush, Lindi J.},
TITLE = {Machine Learning Approaches for Estimating Forest Stand Height Using Plot-Based Observations and Airborne LiDAR Data},
JOURNAL = {Forests},
VOLUME = {9},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {268},
URL = {https://www.mdpi.com/1999-4907/9/5/268},
ISSN = {1999-4907},
ABSTRACT = {Effective sustainable forest management for broad areas needs consistent country-wide forest inventory data. A stand-level inventory is appropriate as a minimum unit for local and regional forest management. South Korea currently produces a forest type map that contains only four categorical parameters. Stand height is a crucial forest attribute for understanding forest ecosystems that is currently missing and should be included in future forest type maps. Estimation of forest stand height is challenging in South Korea because stands exist in small and irregular patches on highly rugged terrain. In this study, we proposed stand height estimation models suitable for rugged terrain with highly mixed tree species. An arithmetic mean height was used as a target variable. Plot-level height estimation models were first developed using 20 descriptive statistics from airborne Light Detection and Ranging (LiDAR) data and three machine learning approaches—support vector regression (SVR), modified regression trees (RT) and random forest (RF). Two schemes (i.e., central plot-based (Scheme 1) and stand-based (Scheme 2)) for expanding from the plot level to the stand level were then investigated. The results showed varied performance metrics (i.e., coefficient of determination, root mean square error, and mean bias) by model for forest height estimation at the plot level. There was no statistically significant difference among the three mean plot height models (i.e., SVR, RT and RF) in terms of estimated heights and bias (p-values &gt; 0.05). The stand-level validation based on all tree measurements for three selected stands produced varied results by scheme and machine learning used. It implies that additional reference data should be used for a more thorough stand-level validation to identify statistically robust approaches in the future. Nonetheless, the research findings from this study can be used as a guide for estimating stand heights for forests in rugged terrain and with complex composition of tree species.},
DOI = {10.3390/f9050268}
}



@Article{rs10050761,
AUTHOR = {Louargant, Marine and Jones, Gawain and Faroux, Romain and Paoli, Jean-Noël and Maillot, Thibault and Gée, Christelle and Villette, Sylvain},
TITLE = {Unsupervised Classification Algorithm for Early Weed Detection in Row-Crops by Combining Spatial and Spectral Information},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {761},
URL = {https://www.mdpi.com/2072-4292/10/5/761},
ISSN = {2072-4292},
ABSTRACT = {In agriculture, reducing herbicide use is a challenge to reduce health and environmental risks while maintaining production yield and quality. Site-specific weed management is a promising way to reach this objective but requires efficient weed detection methods. In this paper, an automatic image processing has been developed to discriminate between crop and weed pixels combining spatial and spectral information extracted from four-band multispectral images. Image data was captured at 3 m above ground, with a camera (multiSPEC 4C, AIRINOV, Paris) mounted on a pole kept manually. For each image, the field of view was approximately 4 m × 3 m and the resolution was 6 mm/pix. The row crop arrangement was first used to discriminate between some crop and weed pixels depending on their location inside or outside of crop rows. Then, these pixels were used to automatically build the training dataset concerning the multispectral features of crop and weed pixel classes. For each image, a specific training dataset was used by a supervised classifier (Support Vector Machine) to classify pixels that cannot be correctly discriminated using only the initial spatial approach. Finally, inter-row pixels were classified as weed and in-row pixels were classified as crop or weed depending on their spectral characteristics. The method was assessed on 14 images captured on maize and sugar beet fields. The contribution of the spatial, spectral and combined information was studied with respect to the classification quality. Our results show the better ability of the spatial and spectral combination algorithm to detect weeds between and within crop rows. They demonstrate the improvement of the weed detection rate and the improvement of its robustness. On all images, the mean value of the weed detection rate was 89% for spatial and spectral combination method, 79% for spatial method, and 75% for spectral method. Moreover, our work shows that the plant in-line sowing can be used to design an automatic image processing and classification algorithm to detect weed without requiring any manual data selection and labelling. Since the method required crop row identification, the method is suitable for wide-row crops and high spatial resolution images (at least 6 mm/pix).},
DOI = {10.3390/rs10050761}
}



@Article{rs10050771,
AUTHOR = {Sukhova, Ekaterina and Sukhov, Vladimir},
TITLE = {Connection of the Photochemical Reflectance Index (PRI) with the Photosystem II Quantum Yield and Nonphotochemical Quenching Can Be Dependent on Variations of Photosynthetic Parameters among Investigated Plants: A Meta-Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {771},
URL = {https://www.mdpi.com/2072-4292/10/5/771},
ISSN = {2072-4292},
ABSTRACT = {The development of spectral methods of remote sensing, including measurement of a photochemical reflectance index (PRI), is a prospective trend in precision agriculture. There are many works which have investigated the connection between photosynthetic parameters and PRI; however, their results varied and were sometimes contradictory. For this paper, we performed a meta-analysis of works in this field. Here, only linear correlations of PRI with photosynthetic parameters—including quantum yield of photosystem II (ΔF/Fm’), nonphotochemical quenching of chlorophyll fluorescence (NPQ), and light use efficiency (LUE)—were investigated. First, it was shown that the correlations were dependent on conditions of PRI measurements (leaf or canopy; artificial light or sunlight). Second, it was shown that a minimal level of the photosynthetic stress, and the variation of this level among investigated plants, can influence the linear correlation of PRI with ΔF/Fm’ and NPQ; the effect was dependent on conditions of measurements. In contrast, the distribution of LUE among plants did not influence its correlation with PRI. Thus, the meta-analysis shows that the distribution of photosynthetic parameters among investigated plants can be an important factor that influences the efficiency of remote sensing on the basis of the PRI measurement.},
DOI = {10.3390/rs10050771}
}



@Article{f9050275,
AUTHOR = {Li, Dan and Gu, Xingfa and Pang, Yong and Chen, Bowei and Liu, Luxia},
TITLE = {Estimation of Forest Aboveground Biomass and Leaf Area Index Based on Digital Aerial Photograph Data in Northeast China},
JOURNAL = {Forests},
VOLUME = {9},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {275},
URL = {https://www.mdpi.com/1999-4907/9/5/275},
ISSN = {1999-4907},
ABSTRACT = {Forest aboveground biomass (AGB) and leaf area index (LAI) are two important parameters for evaluating forest growth and health. It is of great significance to estimate AGB and LAI accurately using remote sensing technology. Considering the temporal resolution and data acquisition costs, digital aerial photographs (DAPs) from a digital camera mounted on an unmanned aerial vehicle or light, small aircraft have been widely used in forest inventory. In this study, the aerial photograph data was acquired on 5 and 9 June, 2017 by a Hasselblad60 digital camera of the CAF-LiCHy system in a Y-5 aircraft in the Mengjiagang forest farm of Northeast China, and the digital orthophoto mosaic (DOM) and photogrammetric point cloud (PPC) were generated from an aerial overlap photograph. Forest red-green-blue (RGB) vegetation indices and textural factors were extracted from the DOM. Forest vertical structure features and canopy cover were extracted from normalized PPC. Regression analysis was carried out considering only DOM data, only PPC data, and a combination of both. A recursive feature elimination (RFE) method using a random forest was used for variable selection. Four different machine-learning (ML) algorithms (random forest, k-nearest neighbor, Cubist and supporting vector machine) were used to build regression models. Experimental results showed that PPC data alone could estimate AGB, and DOM data alone could estimate LAI with relatively high accuracy. The combination of features from DOM and PPC data was the most effective, in all the experiments considered, for the estimation of AGB and LAI. The results showed that the height and coverage variables of PPC, texture mean value, and the visible differential vegetation index (VDVI) of the DOM are significantly related to the estimated AGB (R2 = 0.73, RMSE = 20 t/ha). The results also showed that the canopy cover of PPC and green red ratio index (GRRI) of DOM are the most strongly related to the estimated LAI, and the height and coverage variables of PPC, the texture mean value and visible atmospherically resistant index (VARI), and the VDVI of DOM followed (R2 = 0.79, RMSE = 0.48).},
DOI = {10.3390/f9050275}
}



@Article{electronics7060078,
AUTHOR = {Liu, Xiaofei and Yang, Tao and Li, Jing},
TITLE = {Real-Time Ground Vehicle Detection in Aerial Infrared Imagery Based on Convolutional Neural Network},
JOURNAL = {Electronics},
VOLUME = {7},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {78},
URL = {https://www.mdpi.com/2079-9292/7/6/78},
ISSN = {2079-9292},
ABSTRACT = {An infrared sensor is a commonly used imaging device. Unmanned aerial vehicles, the most promising moving platform, each play a vital role in their own field, respectively. However, the two devices are seldom combined in automatic ground vehicle detection tasks. Therefore, how to make full use of them&mdash;especially in ground vehicle detection based on aerial imagery&ndash;has aroused wide academic concern. However, due to the aerial imagery&rsquo;s low-resolution and the vehicle detection&rsquo;s complexity, how to extract remarkable features and handle pose variations, view changes as well as surrounding radiation remains a challenge. In fact, these typical abstract features extracted by convolutional neural networks are more recognizable than the engineering features, and those complex conditions involved can be learned and memorized before. In this paper, a novel approach towards ground vehicle detection in aerial infrared images based on a convolutional neural network is proposed. The UAV and the infrared sensor used in this application are firstly introduced. Then, a novel aerial moving platform is built and an aerial infrared vehicle dataset is unprecedentedly constructed. We publicly release this dataset (NPU_CS_UAV_IR_DATA), which can be used for the following research in this field. Next, an end-to-end convolutional neural network is built. With large amounts of recognized features being iteratively learned, a real-time ground vehicle model is constructed. It has the unique ability to detect both the stationary vehicles and moving vehicles in real urban environments. We evaluate the proposed algorithm on some low&ndash;resolution aerial infrared images. Experiments on the NPU_CS_UAV_IR_DATA dataset demonstrate that the proposed method is effective and efficient to recognize the ground vehicles. Moreover it can accomplish the task in real-time while achieving superior performances in leak and false alarm ratio.},
DOI = {10.3390/electronics7060078}
}



@Article{s18061703,
AUTHOR = {Nguyen, Phong Ha and Arsalan, Muhammad and Koo, Ja Hyung and Naqvi, Rizwan Ali and Truong, Noi Quang and Park, Kang Ryoung},
TITLE = {LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1703},
URL = {https://www.mdpi.com/1424-8220/18/6/1703},
ISSN = {1424-8220},
ABSTRACT = {Autonomous landing of an unmanned aerial vehicle or a drone is a challenging problem for the robotics research community. Previous researchers have attempted to solve this problem by combining multiple sensors such as global positioning system (GPS) receivers, inertial measurement unit, and multiple camera systems. Although these approaches successfully estimate an unmanned aerial vehicle location during landing, many calibration processes are required to achieve good detection accuracy. In addition, cases where drones operate in heterogeneous areas with no GPS signal should be considered. To overcome these problems, we determined how to safely land a drone in a GPS-denied environment using our remote-marker-based tracking algorithm based on a single visible-light-camera sensor. Instead of using hand-crafted features, our algorithm includes a convolutional neural network named lightDenseYOLO to extract trained features from an input image to predict a marker&rsquo;s location by visible light camera sensor on drone. Experimental results show that our method significantly outperforms state-of-the-art object trackers both using and not using convolutional neural network in terms of both accuracy and processing time.},
DOI = {10.3390/s18061703}
}



@Article{app8060876,
AUTHOR = {Akram, Muhammad Adeel and Liu, Peilin and Wang, Yuze and Qian, Jiuchao},
TITLE = {GNSS Positioning Accuracy Enhancement Based on Robust Statistical MM Estimation Theory for Ground Vehicles in Challenging Environments},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {876},
URL = {https://www.mdpi.com/2076-3417/8/6/876},
ISSN = {2076-3417},
ABSTRACT = {Global Navigation Satellite System (GNSS) is the most reliable navigation system for location-based applications where accuracy and consistency is an essential requirement. The LSE (least squares estimator) has been used since the start of GNSS for position estimation. However; LSE is affected by outliers and errors in GNSS measurements and results in wrong user position. In this paper; we proposed a novel three-phase estimator for enhancing GNSS positioning accuracy in the presence of outliers and errors; relying upon the robust MM estimation theory. In the first phase; a subsampling process is proposed on available observations. IRWLS (iterative reweighted LS) is applied to all subsamples up to a predefined number of observations to obtain a positioning estimate and a scale factor. Secondly; IRWLS is applied up to the convergence point on a set of selected subsamples. The third phase involves the selection of optimum positioning solution having minimum scale factor. An outlier detection and exclusion process is applied on a probabilistic set of outlying observations to maintain the integrity and reliability of the position. Multiple simulated and real scenarios are tested. Results show high accuracy and reliability of the proposed algorithm in challenging environments.},
DOI = {10.3390/app8060876}
}



@Article{rs10060887,
AUTHOR = {Zhu, Jiasong and Sun, Ke and Jia, Sen and Lin, Weidong and Hou, Xianxu and Liu, Bozhi and Qiu, Guoping},
TITLE = {Bidirectional Long Short-Term Memory Network for Vehicle Behavior Recognition},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {887},
URL = {https://www.mdpi.com/2072-4292/10/6/887},
ISSN = {2072-4292},
ABSTRACT = {Vehicle behavior recognition is an attractive research field which is useful for many computer vision and intelligent traffic analysis tasks. This paper presents an all-in-one behavior recognition framework for moving vehicles based on the latest deep learning techniques. Unlike traditional traffic analysis methods which rely on low-resolution videos captured by road cameras, we capture 4K (    3840 × 2178    ) traffic videos at a busy road intersection of a modern megacity by flying a unmanned aerial vehicle (UAV) during the rush hours. We then manually annotate locations and types of road vehicles. The proposed method consists of the following three steps: (1) vehicle detection and type recognition based on deep neural networks; (2) vehicle tracking by data association and vehicle trajectory modeling; (3) vehicle behavior recognition by nearest neighbor search and by bidirectional long short-term memory network, respectively. This paper also presents experimental results of the proposed framework in comparison with state-of-the-art approaches on the 4K testing traffic video, which demonstrated the effectiveness and superiority of the proposed method.},
DOI = {10.3390/rs10060887}
}



@Article{rs10060900,
AUTHOR = {Gao, Lipeng and Shi, Wenzhong and Miao, Zelang and Lv, Zhiyong},
TITLE = {Method Based on Edge Constraint and Fast Marching for Road Centerline Extraction from Very High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {900},
URL = {https://www.mdpi.com/2072-4292/10/6/900},
ISSN = {2072-4292},
ABSTRACT = {In recent decades, road extraction from very high-resolution (VHR) remote sensing images has become popular and has attracted extensive research efforts. However, the very high spatial resolution, complex urban structure, and contextual background effect of road images complicate the process of road extraction. For example, shadows, vehicles, or other objects may occlude a road located in a developed urban area. To address the problem of occlusion, this study proposes a semiautomatic approach for road extraction from VHR remote sensing images. First, guided image filtering is employed to reduce the negative effects of nonroad pixels while preserving edge smoothness. Then, an edge-constraint-based weighted fusion model is adopted to trace and refine the road centerline. An edge-constraint fast marching method, which sequentially links discrete seed points, is presented to maintain road-point connectivity. Six experiments with eight VHR remote sensing images (spatial resolution of 0.3 m/pixel to 2 m/pixel) are conducted to evaluate the efficiency and robustness of the proposed approach. Compared with state-of-the-art methods, the proposed approach presents superior extraction quality, time consumption, and seed-point requirements.},
DOI = {10.3390/rs10060900}
}



@Article{s18061881,
AUTHOR = {Kim, In-Ho and Jeon, Haemin and Baek, Seung-Chan and Hong, Won-Hwa and Jung, Hyung-Jo},
TITLE = {Application of Crack Identification Techniques for an Aging Concrete Bridge Inspection Using an Unmanned Aerial Vehicle},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1881},
URL = {https://www.mdpi.com/1424-8220/18/6/1881},
ISSN = {1424-8220},
ABSTRACT = {Bridge inspection using unmanned aerial vehicles (UAV) with high performance vision sensors has received considerable attention due to its safety and reliability. As bridges become obsolete, the number of bridges that need to be inspected increases, and they require much maintenance cost. Therefore, a bridge inspection method based on UAV with vision sensors is proposed as one of the promising strategies to maintain bridges. In this paper, a crack identification method by using a commercial UAV with a high resolution vision sensor is investigated in an aging concrete bridge. First, a point cloud-based background model is generated in the preliminary flight. Then, cracks on the structural surface are detected with the deep learning algorithm, and their thickness and length are calculated. In the deep learning method, region with convolutional neural networks (R-CNN)-based transfer learning is applied. As a result, a new network for the 384 collected crack images of 256 &times; 256 pixel resolution is generated from the pre-trained network. A field test is conducted to verify the proposed approach, and the experimental results proved that the UAV-based bridge inspection is effective at identifying and quantifying the cracks on the structures.},
DOI = {10.3390/s18061881}
}



@Article{s18072013,
AUTHOR = {Zhou, Yi and Zhang, Rui and Wang, Shixin and Wang, Futao},
TITLE = {Feature Selection Method Based on High-Resolution Remote Sensing Images and the Effect of Sensitive Features on Classification Accuracy},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2013},
URL = {https://www.mdpi.com/1424-8220/18/7/2013},
ISSN = {1424-8220},
ABSTRACT = {With the advent of high spatial resolution remote sensing imagery, numerous image features can be utilized. Applying a reasonable feature selection approach is critical to effectively reduce feature redundancy and improve the efficiency and accuracy of classification. This paper proposes a novel feature selection approach, in which ReliefF, genetic algorithm, and support vector machine (RFGASVM) are integrated to extract buildings. We adopt the ReliefF algorithm to preliminary filter high-dimensional features in the feature database. After eliminating the sorted features, the feature subset and the C and &gamma; parameters of support vector machine (SVM) are encoded into the chromosome of the genetic algorithm. A fitness function is constructed considering the sample identification accuracy, the number of selected features, and the feature cost. The proposed method was applied to high-resolution images obtained from different sensors, GF-2, BJ-2, and unmanned aerial vehicles (UAV). The confusion matrix, precision, recall and F1-score were applied to assess the accuracy. The results showed that the proposed method achieved feature reduction, and the overall accuracy (OA) was more than 85%, with Kappa coefficient values of 0.80, 0.83 and 0.85, respectively. The precision of each image was more than 85%. The time efficiency of the proposed method was two-fold greater than SVM with all the features. The RFGASVM method has the advantages of large feature reduction and high extraction performance and can be applied in feature selection.},
DOI = {10.3390/s18072013}
}



@Article{app8071028,
AUTHOR = {Wu, Yunpeng and Qin, Yong and Wang, Zhipeng and Jia, Limin},
TITLE = {A UAV-Based Visual Inspection Method for Rail Surface Defects},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1028},
URL = {https://www.mdpi.com/2076-3417/8/7/1028},
ISSN = {2076-3417},
ABSTRACT = {Rail surface defects seriously affect the safety of railway systems. At present, human inspection and rail vehicle inspection are the main approaches for the detection of rail surface defects. However, there are many shortcomings to these approaches, such as low efficiency, high cost, and so on. This paper presents a novel visual inspection approach based on unmanned aerial vehicle (UAV) images, and focuses on two key issues of UAV-based rail images: image enhancement and defects segmentation. With regards to the first aspect, a novel image enhancement algorithm named Local Weber-like Contrast (LWLC) is proposed to enhance rail images. The rail surface defects and backgrounds can be highlighted and homogenized under various sunlight intensity by LWLC, due to its illuminance independent, local nonlinear and other advantages. With regards to the second, a new threshold segmentation method named gray stretch maximum entropy (GSME) is presented in this paper. The proposed GSME method emphasizes gray stretch and de-noising on UAV-based rail images, and selects an optimal segmentation threshold for defects detection. Two visual comparison experiments were carried out to demonstrate the efficiency of the proposed methods. Finally, a quantitative comparison experiment shows the LWLC-GSME model achieves a recall of 93.75% for T-I defects and of 94.26% for T-II defects. Therefore, LWLC for image enhancement, in conjunction with GSME for defects segmentation, is efficient and feasible for the detection of rail surface defects based on UAV Images.},
DOI = {10.3390/app8071028}
}



@Article{s18072048,
AUTHOR = {Rivas, Alberto and Chamoso, Pablo and González-Briones, Alfonso and Corchado, Juan Manuel},
TITLE = {Detection of Cattle Using Drones and Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2048},
URL = {https://www.mdpi.com/1424-8220/18/7/2048},
ISSN = {1424-8220},
ABSTRACT = {Multirotor drones have been one of the most important technological advances of the last decade. Their mechanics are simple compared to other types of drones and their possibilities in flight are greater. For example, they can take-off vertically. Their capabilities have therefore brought progress to many professional activities. Moreover, advances in computing and telecommunications have also broadened the range of activities in which drones may be used. Currently, artificial intelligence and information analysis are the main areas of research in the field of computing. The case study presented in this article employed artificial intelligence techniques in the analysis of information captured by drones. More specifically, the camera installed in the drone took images which were later analyzed using Convolutional Neural Networks (CNNs) to identify the objects captured in the images. In this research, a CNN was trained to detect cattle, however the same training process could be followed to develop a CNN for the detection of any other object. This article describes the design of the platform for real-time analysis of information and its performance in the detection of cattle.},
DOI = {10.3390/s18072048}
}



@Article{s18072071,
AUTHOR = {Guerra, Edmundo and Munguía, Rodrigo and Grau, Antoni},
TITLE = {UAV Visual and Laser Sensors Fusion for Detection and Positioning in Industrial Applications},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2071},
URL = {https://www.mdpi.com/1424-8220/18/7/2071},
ISSN = {1424-8220},
ABSTRACT = {This work presents a solution to localize Unmanned Autonomous Vehicles with respect to pipes and other cylindrical elements found in inspection and maintenance tasks both in industrial and civilian infrastructures. The proposed system exploits the different features of vision and laser based sensors, combining them to obtain accurate positioning of the robot with respect to the cylindrical structures. A probabilistic (RANSAC-based) procedure is used to segment possible cylinders found in the laser scans, and this is used as a seed to accurately determine the robot position through a computer vision system. The priors obtained from the laser scan registration help to solve the problem of determining the apparent contour of the cylinders. In turn this apparent contour is used in a degenerate quadratic conic estimation, enabling to visually estimate the pose of the cylinder.},
DOI = {10.3390/s18072071}
}



@Article{app8071062,
AUTHOR = {Adege, Abebe Belay and Lin, Hsin-Piao and Tarekegn, Getaneh Berie and Jeng, Shiann-Shiun},
TITLE = {Applying Deep Neural Network (DNN) for Robust Indoor Localization in Multi-Building Environment},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1062},
URL = {https://www.mdpi.com/2076-3417/8/7/1062},
ISSN = {2076-3417},
ABSTRACT = {In the Internet of Things (IoT) era, indoor localization plays a vital role in academia and industry. Wi-Fi is a promising scheme for indoor localization as it is easy and free of charge, even for private networks. However, Wi-Fi has signal fluctuation problems because of dynamic changes of environments and shadowing effects. In this paper, we propose to use a deep neural network (DNN) to achieve accurate localization in Wi-Fi environments. In the localization process, we primarily construct a database having all reachable received signal strengths (RSSs), and basic service set identifiers (BSSIDs). Secondly, we fill the missed RSS values using regression, and then apply linear discriminant analysis (LDA) to reduce features. Thirdly, the 5-BSSIDs having the strongest RSS values are appended with reduced RSS vector. Finally, a DNN is applied for localizing Wi-Fi users. The proposed system is evaluated in the classification and regression schemes using the python programming language. The results show that 99.15% of the localization accuracy is correctly classified. Moreover, the coordinate-based localization provides 50%, 75%, and 93.10% accuracies for errors less than 0.50 m, 0.75 m, and 0.90 m respectively. The proposed method is compared with other algorithms, and our method provides motivated results. The simulation results also show that the proposed method can robustly localize Wi-Fi users in hierarchical and complex wireless environments.},
DOI = {10.3390/app8071062}
}



@Article{s18072113,
AUTHOR = {Huang, Huasheng and Lan, Yubin and Deng, Jizhong and Yang, Aqing and Deng, Xiaoling and Zhang, Lei and Wen, Sheng},
TITLE = {A Semantic Labeling Approach for Accurate Weed Mapping of High Resolution UAV Imagery},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2113},
URL = {https://www.mdpi.com/1424-8220/18/7/2113},
ISSN = {1424-8220},
ABSTRACT = {Weed control is necessary in rice cultivation, but the excessive use of herbicide treatments has led to serious agronomic and environmental problems. Suitable site-specific weed management (SSWM) is a solution to address this problem while maintaining the rice production quality and quantity. In the context of SSWM, an accurate weed distribution map is needed to provide decision support information for herbicide treatment. UAV remote sensing offers an efficient and effective platform to monitor weeds thanks to its high spatial resolution. In this work, UAV imagery was captured in a rice field located in South China. A semantic labeling approach was adopted to generate the weed distribution maps of the UAV imagery. An ImageNet pre-trained CNN with residual framework was adapted in a fully convolutional form, and transferred to our dataset by fine-tuning. Atrous convolution was applied to extend the field of view of convolutional filters; the performance of multi-scale processing was evaluated; and a fully connected conditional random field (CRF) was applied after the CNN to further refine the spatial details. Finally, our approach was compared with the pixel-based-SVM and the classical FCN-8s. Experimental results demonstrated that our approach achieved the best performance in terms of accuracy. Especially for the detection of small weed patches in the imagery, our approach significantly outperformed other methods. The mean intersection over union (mean IU), overall accuracy, and Kappa coefficient of our method were 0.7751, 0.9445, and 0.9128, respectively. The experiments showed that our approach has high potential in accurate weed mapping of UAV imagery.},
DOI = {10.3390/s18072113}
}



@Article{geosciences8070244,
AUTHOR = {Buscombe, Daniel and Ritchie, Andrew C.},
TITLE = {Landscape Classification with Deep Neural Networks},
JOURNAL = {Geosciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {244},
URL = {https://www.mdpi.com/2076-3263/8/7/244},
ISSN = {2076-3263},
ABSTRACT = {The application of deep learning, specifically deep convolutional neural networks (DCNNs), to the classification of remotely-sensed imagery of natural landscapes has the potential to greatly assist in the analysis and interpretation of geomorphic processes. However, the general usefulness of deep learning applied to conventional photographic imagery at a landscape scale is, at yet, largely unproven. If DCNN-based image classification is to gain wider application and acceptance within the geoscience community, demonstrable successes need to be coupled with accessible tools to retrain deep neural networks to discriminate landforms and land uses in landscape imagery. Here, we present an efficient approach to train/apply DCNNs with/on sets of photographic images, using a powerful graphical method called a conditional random field (CRF), to generate DCNN training and testing data using minimal manual supervision. We apply the method to several sets of images of natural landscapes, acquired from satellites, aircraft, unmanned aerial vehicles, and fixed camera installations. We synthesize our findings to examine the general effectiveness of transfer learning to landscape-scale image classification. Finally, we show how DCNN predictions on small regions of images might be used in conjunction with a CRF for highly accurate pixel-level classification of images.},
DOI = {10.3390/geosciences8070244}
}



@Article{s18072184,
AUTHOR = {Ismail, Adiel and Bagula, Bigomokero Antoine and Tuyishimire, Emmanuel},
TITLE = {Internet-Of-Things in Motion: A UAV Coalition Model for Remote Sensing in Smart Cities},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2184},
URL = {https://www.mdpi.com/1424-8220/18/7/2184},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) or drones are increasingly used in cities to provide service tasks that are too dangerous, expensive or difficult for human beings. Drones are also used in cases where a task can be performed more economically and or more efficiently than if done by humans. These include remote sensing tasks where drones can be required to form coalitions by pooling their resources to meet the service requirements at different locations of interest in a city. During such coalition formation, finding the shortest path from a source to a location of interest is key to efficient service delivery. For fixed-wing UAVs, Dubins curves can be applied to find the shortest flight path. When a UAV flies to a location of interest, the angle or orientation of the UAV upon its arrival is often not important. In such a case, a simplified version of the Dubins curve consisting of two instead of three parts can be used. This paper proposes a novel model for UAV coalition and an algorithm derived from basic geometry that generates a path derived from the original Dubins curve for application in remote sensing missions of fixed-wing UAVs. The algorithm is tested by incorporating it into three cooperative coalition formation algorithms. The performance of the model is evaluated by varying the number of types of resources and the sensor ranges of the UAVs to reveal the relevance and practicality of the proposed model.},
DOI = {10.3390/s18072184}
}



@Article{s18072194,
AUTHOR = {Bachmann, Daniel and Weichert, Frank and Rinkenauer, Gerhard},
TITLE = {Review of Three-Dimensional Human-Computer Interaction with Focus on the Leap Motion Controller},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2194},
URL = {https://www.mdpi.com/1424-8220/18/7/2194},
ISSN = {1424-8220},
ABSTRACT = {Modern hardware and software development has led to an evolution of user interfaces from command-line to natural user interfaces for virtual immersive environments. Gestures imitating real-world interaction tasks increasingly replace classical two-dimensional interfaces based on Windows/Icons/Menus/Pointers (WIMP) or touch metaphors. Thus, the purpose of this paper is to survey the state-of-the-art Human-Computer Interaction (HCI) techniques with a focus on the special field of three-dimensional interaction. This includes an overview of currently available interaction devices, their applications of usage and underlying methods for gesture design and recognition. Focus is on interfaces based on the Leap Motion Controller (LMC) and corresponding methods of gesture design and recognition. Further, a review of evaluation methods for the proposed natural user interfaces is given.},
DOI = {10.3390/s18072194}
}



@Article{sym10070270,
AUTHOR = {Petrellis, Nikos},
TITLE = {A Review of Image Processing Techniques Common in Human and Plant Disease Diagnosis},
JOURNAL = {Symmetry},
VOLUME = {10},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {270},
URL = {https://www.mdpi.com/2073-8994/10/7/270},
ISSN = {2073-8994},
ABSTRACT = {Image processing has been extensively used in various (human, animal, plant) disease diagnosis approaches, assisting experts to select the right treatment. It has been applied to both images captured from cameras of visible light and from equipment that captures information in invisible wavelengths (magnetic/ultrasonic sensors, microscopes, etc.). In most of the referenced diagnosis applications, the image is enhanced by various filtering methods and segmentation follows isolating the regions of interest. Classification of the input image is performed at the final stage. The disease diagnosis approaches based on these steps and the common methods are described. The features extracted from a plant/skin disease diagnosis framework developed by the author are used here to demonstrate various techniques adopted in the literature. The various metrics along with the available experimental conditions and results presented in the referenced approaches are also discussed. The accuracy achieved in the diagnosis methods that are based on image processing is often higher than 90%. The motivation for this review is to highlight the most common and efficient methods that have been employed in various disease diagnosis approaches and suggest how they can be used in similar or different applications.},
DOI = {10.3390/sym10070270}
}



@Article{s18072244,
AUTHOR = {De Oliveira, Diulhio Candido and Wehrmeister, Marco Aurelio},
TITLE = {Using Deep Learning and Low-Cost RGB and Thermal Cameras to Detect Pedestrians in Aerial Images Captured by Multirotor UAV},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2244},
URL = {https://www.mdpi.com/1424-8220/18/7/2244},
ISSN = {1424-8220},
ABSTRACT = {The use of Unmanned Aerial Vehicles (UAV) has been increasing over the last few years in many sorts of applications due mainly to the decreasing cost of this technology. One can see the use of the UAV in several civilian applications such as surveillance and search and rescue. Automatic detection of pedestrians in aerial images is a challenging task. The computing vision system must deal with many sources of variability in the aerial images captured with the UAV, e.g., low-resolution images of pedestrians, images captured at distinct angles due to the degrees of freedom that a UAV can move, the camera platform possibly experiencing some instability while the UAV flies, among others. In this work, we created and evaluated different implementations of Pattern Recognition Systems (PRS) aiming at the automatic detection of pedestrians in aerial images captured with multirotor UAV. The main goal is to assess the feasibility and suitability of distinct PRS implementations running on top of low-cost computing platforms, e.g., single-board computers such as the Raspberry Pi or regular laptops without a GPU. For that, we used four machine learning techniques in the feature extraction and classification steps, namely Haar cascade, LBP cascade, HOG + SVM and Convolutional Neural Networks (CNN). In order to improve the system performance (especially the processing time) and also to decrease the rate of false alarms, we applied the Saliency Map (SM) and Thermal Image Processing (TIP) within the segmentation and detection steps of the PRS. The classification results show the CNN to be the best technique with 99.7% accuracy, followed by HOG + SVM with 92.3%. In situations of partial occlusion, the CNN showed 71.1% sensitivity, which can be considered a good result in comparison with the current state-of-the-art, since part of the original image data is missing. As demonstrated in the experiments, by combining TIP with CNN, the PRS can process more than two frames per second (fps), whereas the PRS that combines TIP with HOG + SVM was able to process 100 fps. It is important to mention that our experiments show that a trade-off analysis must be performed during the design of a pedestrian detection PRS. The faster implementations lead to a decrease in the PRS accuracy. For instance, by using HOG + SVM with TIP, the PRS presented the best performance results, but the obtained accuracy was 35 percentage points lower than the CNN. The obtained results indicate that the best detection technique (i.e., the CNN) requires more computational resources to decrease the PRS computation time. Therefore, this work shows and discusses the pros/cons of each technique and trade-off situations, and hence, one can use such an analysis to improve and tailor the design of a PRS to detect pedestrians in aerial images.},
DOI = {10.3390/s18072244}
}



@Article{s18072258,
AUTHOR = {Xiang, Xuezhi and Lv, Ning and Guo, Xinli and Wang, Shuai and El Saddik, Abdulmotaleb},
TITLE = {Engineering Vehicles Detection Based on Modified Faster R-CNN for Power Grid Surveillance},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2258},
URL = {https://www.mdpi.com/1424-8220/18/7/2258},
ISSN = {1424-8220},
ABSTRACT = {Engineering vehicles intrusion detection is a key problem for the security of power grid operation, which can warn of the regional invasion and prevent external damage from architectural construction. In this paper, we propose an intelligent surveillance method based on the framework of Faster R-CNN for locating and identifying the invading engineering vehicles. In our detection task, the type of the objects is varied and the monitoring scene is large and complex. In order to solve these challenging problems, we modify the network structure of the object detection model by adjusting the position of the ROI pooling layer. The convolutional layer is added to the feature classification part to improve the accuracy of the detection model. We verify that increasing the depth of the feature classification part is effective for detecting engineering vehicles in realistic transmission lines corridors. We also collect plenty of scene images taken from the monitor site and label the objects to create a fine-tuned dataset. We train the modified deep detection model based on the technology of transfer learning and conduct training and test on the newly labeled dataset. Experimental results show that the proposed intelligent surveillance method can detect engineering vehicles with high accuracy and a low false alarm rate, which can be used for the early warning of power grid surveillance.},
DOI = {10.3390/s18072258}
}



@Article{rs10071120,
AUTHOR = {Lausch, Angela and Borg, Erik and Bumberger, Jan and Dietrich, Peter and Heurich, Marco and Huth, Andreas and Jung, András and Klenke, Reinhard and Knapp, Sonja and Mollenhauer, Hannes and Paasche, Hendrik and Paulheim, Heiko and Pause, Marion and Schweitzer, Christian and Schmulius, Christiane and Settele, Josef and Skidmore, Andrew K. and Wegmann, Martin and Zacharias, Steffen and Kirsten, Toralf and Schaepman, Michael E.},
TITLE = {Understanding Forest Health with Remote Sensing, Part III: Requirements for a Scalable Multi-Source Forest Health Monitoring Network Based on Data Science Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1120},
URL = {https://www.mdpi.com/2072-4292/10/7/1120},
ISSN = {2072-4292},
ABSTRACT = {Forest ecosystems fulfill a whole host of ecosystem functions that are essential for life on our planet. However, an unprecedented level of anthropogenic influences is reducing the resilience and stability of our forest ecosystems as well as their ecosystem functions. The relationships between drivers, stress, and ecosystem functions in forest ecosystems are complex, multi-faceted, and often non-linear, and yet forest managers, decision makers, and politicians need to be able to make rapid decisions that are data-driven and based on short and long-term monitoring information, complex modeling, and analysis approaches. A huge number of long-standing and standardized forest health inventory approaches already exist, and are increasingly integrating remote-sensing based monitoring approaches. Unfortunately, these approaches in monitoring, data storage, analysis, prognosis, and assessment still do not satisfy the future requirements of information and digital knowledge processing of the 21st century. Therefore, this paper discusses and presents in detail five sets of requirements, including their relevance, necessity, and the possible solutions that would be necessary for establishing a feasible multi-source forest health monitoring network for the 21st century. Namely, these requirements are: (1) understanding the effects of multiple stressors on forest health; (2) using remote sensing (RS) approaches to monitor forest health; (3) coupling different monitoring approaches; (4) using data science as a bridge between complex and multidimensional big forest health (FH) data; and (5) a future multi-source forest health monitoring network. It became apparent that no existing monitoring approach, technique, model, or platform is sufficient on its own to monitor, model, forecast, or assess forest health and its resilience. In order to advance the development of a multi-source forest health monitoring network, we argue that in order to gain a better understanding of forest health in our complex world, it would be conducive to implement the concepts of data science with the components: (i) digitalization; (ii) standardization with metadata management after the FAIR (Findability, Accessibility, Interoperability, and Reusability) principles; (iii) Semantic Web; (iv) proof, trust, and uncertainties; (v) tools for data science analysis; and (vi) easy tools for scientists, data managers, and stakeholders for decision-making support.},
DOI = {10.3390/rs10071120}
}



@Article{data3030028,
AUTHOR = {Gopalakrishnan, Kasthurirangan},
TITLE = {Deep Learning in Data-Driven Pavement Image Analysis and Automated Distress Detection: A Review},
JOURNAL = {Data},
VOLUME = {3},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {28},
URL = {https://www.mdpi.com/2306-5729/3/3/28},
ISSN = {2306-5729},
ABSTRACT = {Deep learning, more specifically deep convolutional neural networks, is fast becoming a popular choice for computer vision-based automated pavement distress detection. While pavement image analysis has been extensively researched over the past three decades or so, recent ground-breaking achievements of deep learning algorithms in the areas of machine translation, speech recognition, and computer vision has sparked interest in the application of deep learning to automated detection of distresses in pavement images. This paper provides a narrative review of recently published studies in this field, highlighting the current achievements and challenges. A comparison of the deep learning software frameworks, network architecture, hyper-parameters employed by each study, and crack detection performance is provided, which is expected to provide a good foundation for driving further research on this important topic in the context of smart pavement or asset management systems. The review concludes with potential avenues for future research; especially in the application of deep learning to not only detect, but also characterize the type, extent, and severity of distresses from 2D and 3D pavement images.},
DOI = {10.3390/data3030028}
}



@Article{ijgi7080294,
AUTHOR = {Chabot, Dominique and Dillon, Christopher and Shemrock, Adam and Weissflog, Nicholas and Sager, Eric P. S.},
TITLE = {An Object-Based Image Analysis Workflow for Monitoring Shallow-Water Aquatic Vegetation in Multispectral Drone Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {294},
URL = {https://www.mdpi.com/2220-9964/7/8/294},
ISSN = {2220-9964},
ABSTRACT = {High-resolution drone aerial surveys combined with object-based image analysis are transforming our capacity to monitor and manage aquatic vegetation in an era of invasive species. To better exploit the potential of these technologies, there is a need to develop more efficient and accessible analysis workflows and focus more efforts on the distinct challenge of mapping submerged vegetation. We present a straightforward workflow developed to monitor emergent and submerged invasive water soldier (Stratiotes aloides) in shallow waters of the Trent-Severn Waterway in Ontario, Canada. The main elements of the workflow are: (1) collection of radiometrically calibrated multispectral imagery including a near-infrared band; (2) multistage segmentation of the imagery involving an initial separation of above-water from submerged features; and (3) automated classification of features with a supervised machine-learning classifier. The approach yielded excellent classification accuracy for emergent features (overall accuracy = 92%; kappa = 88%; water soldier producer&rsquo;s accuracy = 92%; user&rsquo;s accuracy = 91%) and good accuracy for submerged features (overall accuracy = 84%; kappa = 75%; water soldier producer&rsquo;s accuracy = 71%; user&rsquo;s accuracy = 84%). The workflow employs off-the-shelf graphical software tools requiring no programming or coding, and could therefore be used by anyone with basic GIS and image analysis skills for a potentially wide variety of aquatic vegetation monitoring operations.},
DOI = {10.3390/ijgi7080294}
}



@Article{rs10081187,
AUTHOR = {Zhou, Jianmin and Zhang, Shan and Yang, Hua and Xiao, Zhiqiang and Gao, Feng},
TITLE = {The Retrieval of 30-m Resolution LAI from Landsat Data by Combining MODIS Products},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1187},
URL = {https://www.mdpi.com/2072-4292/10/8/1187},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is a critical vegetation structural parameter in biogeochemical and biophysical ecosystems. High-resolution LAI products play an essential role in regional studies. Empirical methods, which normally use field measurements as their training samples and have been identified as the most commonly used approaches to retrieve structural parameters of vegetation from high-resolution remote-sensing data, are limited by the quality of training samples. Few efforts have been made to generate training samples from existing global LAI products. In this study, two methods (a homogeneous and pure pixel filter method (method A) and a pixel unmixing method (method B)) were developed to extract training samples from moderate-resolution imaging spectroradiometer (MODIS) surface reflectance and LAI products, and a support vector regression (SVR) algorithm trained by the samples was used to retrieve the high-resolution LAI from Landsat data at Baoding, situated in the Hebei Province in China, and Des Moines, situated in Iowa, United States. For the homogeneous and pure pixel filter method, two different sets of training samples were designed. One was composed of upscaled Landsat reflectance at the 500-m resolution and MODIS LAI products (dataset A1); the other was composed of MODIS reflectance and LAI products (dataset A2). With them, two inversion models were developed using SVR. For the pixel unmixing method, the training samples (dataset B) were extracted from unmixed MODIS surface reflectance and LAI products at 30-m resolution, and the third inversion model was obtained with them. LAI inversion results showed that good agreement with field measurements was achieved using these three inversion models. The R2 (coefficient of determination) value and the root mean square error (RMSE) value were computed to assess the results. For all tests, the R2 values are higher than 0.74 and RMSE values are less than 0.73. These tests showed that three models for the two methods combined with MODIS products can retrieve 30-m resolution LAI from Landsat data. The results of the pixel unmixing method was slightly better than that of the homogeneous and pure pixel filter method.},
DOI = {10.3390/rs10081187}
}



@Article{rs10081192,
AUTHOR = {Feng, Chen-Chieh and Guo, Zhou},
TITLE = {Automating Parameter Learning for Classifying Terrestrial LiDAR Point Cloud Using 2D Land Cover Maps},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1192},
URL = {https://www.mdpi.com/2072-4292/10/8/1192},
ISSN = {2072-4292},
ABSTRACT = {The automating classification of point clouds capturing urban scenes is critical for supporting applications that demand three-dimensional (3D) models. Achieving this goal, however, is met with challenges because of the varying densities of the point clouds and the complexity of the 3D data. In order to increase the level of automation in the point cloud classification, this study proposes a segment-based parameter learning method that incorporates a two-dimensional (2D) land cover map, in which a strategy of fusing the 2D land cover map and the 3D points is first adopted to create labelled samples, and a formalized procedure is then implemented to automatically learn the following parameters of point cloud classification: the optimal scale of the neighborhood for segmentation, optimal feature set, and the training classifier. It comprises four main steps, namely: (1) point cloud segmentation; (2) sample selection; (3) optimal feature set selection; and (4) point cloud classification. Three datasets containing the point cloud data were used in this study to validate the efficiency of the proposed method. The first two datasets cover two areas of the National University of Singapore (NUS) campus while the third dataset is a widely used benchmark point cloud dataset of Oakland, Pennsylvania. The classification parameters were learned from the first dataset consisting of a terrestrial laser-scanning data and a 2D land cover map, and were subsequently used to classify both of the NUS datasets. The evaluation of the classification results showed overall accuracies of 94.07% and 91.13%, respectively, indicating that the transition of the knowledge learned from one dataset to another was satisfactory. The classification of the Oakland dataset achieved an overall accuracy of 97.08%, which further verified the transferability of the proposed approach. An experiment of the point-based classification was also conducted on the first dataset and the result was compared to that of the segment-based classification. The evaluation revealed that the overall accuracy of the segment-based classification is indeed higher than that of the point-based classification, demonstrating the advantage of the segment-based approaches.},
DOI = {10.3390/rs10081192}
}



@Article{s18082484,
AUTHOR = {Zhang, Weixing and Witharana, Chandi and Li, Weidong and Zhang, Chuanrong and Li, Xiaojiang and Parent, Jason},
TITLE = {Using Deep Learning to Identify Utility Poles with Crossarms and Estimate Their Locations from Google Street View Images},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2484},
URL = {https://www.mdpi.com/1424-8220/18/8/2484},
ISSN = {1424-8220},
ABSTRACT = {Traditional methods of detecting and mapping utility poles are inefficient and costly because of the demand for visual interpretation with quality data sources or intense field inspection. The advent of deep learning for object detection provides an opportunity for detecting utility poles from side-view optical images. In this study, we proposed using a deep learning-based method for automatically mapping roadside utility poles with crossarms (UPCs) from Google Street View (GSV) images. The method combines the state-of-the-art DL object detection algorithm (i.e., the RetinaNet object detection algorithm) and a modified brute-force-based line-of-bearing (LOB, a LOB stands for the ray towards the location of the target [UPC at here] from the original location of the sensor [GSV mobile platform]) measurement method to estimate the locations of detected roadside UPCs from GSV. Experimental results indicate that: (1) both the average precision (AP) and the overall accuracy (OA) are around 0.78 when the intersection-over-union (IoU) threshold is greater than 0.3, based on the testing of 500 GSV images with a total number of 937 objects; and (2) around 2.6%, 47%, and 79% of estimated locations of utility poles are within 1 m, 5 m, and 10 m buffer zones, respectively, around the referenced locations of utility poles. In general, this study indicates that even in a complex background, most utility poles can be detected with the use of DL, and the LOB measurement method can estimate the locations of most UPCs.},
DOI = {10.3390/s18082484}
}



@Article{rs10081216,
AUTHOR = {Dash, Jonathan P. and Pearse, Grant D. and Watt, Michael S.},
TITLE = {UAV Multispectral Imagery Can Complement Satellite Data for Monitoring Forest Health},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1216},
URL = {https://www.mdpi.com/2072-4292/10/8/1216},
ISSN = {2072-4292},
ABSTRACT = {The development of methods that can accurately detect physiological stress in forest trees caused by biotic or abiotic factors is vital for ensuring productive forest systems that can meet the demands of the Earth&rsquo;s population. The emergence of new sensors and platforms presents opportunities to augment traditional practices by combining remotely-sensed data products to provide enhanced information on forest condition. We tested the sensitivity of multispectral imagery collected from time-series unmanned aerial vehicle (UAV) and satellite imagery to detect herbicide-induced stress in a carefully controlled experiment carried out in a mature Pinus radiata D. Don plantation. The results revealed that both data sources were sensitive to physiological stress in the study trees. The UAV data were more sensitive to changes at a finer spatial resolution and could detect stress down to the level of individual trees. The satellite data tested could only detect physiological stress in clusters of four or more trees. Resampling the UAV imagery to the same spatial resolution as the satellite imagery revealed that the differences in sensitivity were not solely the result of spatial resolution. Instead, vegetation indices suited to the sensor characteristics of each platform were required to optimise the detection of physiological stress from each data source. Our results define both the spatial detection threshold and the optimum vegetation indices required to implement monitoring of this forest type. A comparison between time-series datasets of different spectral indices showed that the two sensors are compatible and can be used to deliver an enhanced method for monitoring physiological stress in forest trees at various scales. We found that the higher resolution UAV imagery was more sensitive to fine-scale instances of herbicide induced physiological stress than the RapidEye imagery. Although less sensitive to smaller phenomena the satellite imagery was found to be very useful for observing trends in physiological stress over larger areas.},
DOI = {10.3390/rs10081216}
}



@Article{rs10081229,
AUTHOR = {Zhao, Qi and Zhang, Boxue and Lyu, Shuchang and Zhang, Hong and Sun, Daniel and Li, Guoqiang and Feng, Wenquan},
TITLE = {A CNN-SIFT Hybrid Pedestrian Navigation Method Based on First-Person Vision},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1229},
URL = {https://www.mdpi.com/2072-4292/10/8/1229},
ISSN = {2072-4292},
ABSTRACT = {The emergence of new wearable technologies, such as action cameras and smart glasses, has driven the use of the first-person perspective in computer applications. This field is now attracting the attention and investment of researchers aiming to develop methods to process first-person vision (FPV) video. The current approaches present particular combinations of different image features and quantitative methods to accomplish specific objectives, such as object detection, activity recognition, user&ndash;machine interaction, etc. FPV-based navigation is necessary in some special areas, where Global Position System (GPS) or other radio-wave strength methods are blocked, and is especially helpful for visually impaired people. In this paper, we propose a hybrid structure with a convolutional neural network (CNN) and local image features to achieve FPV pedestrian navigation. A novel end-to-end trainable global pooling operator, called AlphaMEX, has been designed to improve the scene classification accuracy of CNNs. A scale-invariant feature transform (SIFT)-based tracking algorithm is employed for movement estimation and trajectory tracking of the person through each frame of FPV images. Experimental results demonstrate the effectiveness of the proposed method. The top-1 error rate of the proposed AlphaMEX-ResNet outperforms the original ResNet (k = 12) by 1.7% on the ImageNet dataset. The CNN-SIFT hybrid pedestrian navigation system reaches 0.57 m average absolute error, which is an adequate accuracy for pedestrian navigation. Both positions and movements can be well estimated by the proposed pedestrian navigation algorithm with a single wearable camera.},
DOI = {10.3390/rs10081229}
}



@Article{rs10081234,
AUTHOR = {Fu, Zhitao and Qin, Qianqing and Luo, Bin and Sun, Hong and Wu, Chun},
TITLE = {HOMPC: A Local Feature Descriptor Based on the Combination of Magnitude and Phase Congruency Information for Multi-Sensor Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1234},
URL = {https://www.mdpi.com/2072-4292/10/8/1234},
ISSN = {2072-4292},
ABSTRACT = {Local region description of multi-sensor images remains a challenging task in remote sensing image analysis and applications due to the non-linear radiation variations between images. This paper presents a novel descriptor based on the combination of the magnitude and phase congruency information of local regions to capture the common features of images with non-linear radiation changes. We first propose oriented phase congruency maps (PCMs) and oriented magnitude binary maps (MBMs) using the multi-oriented phase congruency and magnitude information of log-Gabor filters. The two feature vectors are then quickly constructed based on the convolved PCMs and MBMs. Finally, a dense descriptor named the histograms of oriented magnitude and phase congruency (HOMPC) is developed by combining the histograms of oriented phase congruency (HPC) and the histograms of oriented magnitude (HOM) to capture the structure and shape properties of local regions. HOMPC was evaluated with three datasets composed of multi-sensor remote sensing images obtained from unmanned ground vehicle, unmanned aerial vehicle, and satellite platforms. The descriptor performance was evaluated by recall, precision, F1-measure, and area under the precision-recall curve. The experimental results showed the advantages of the HOM and HPC combination and confirmed that HOMPC is far superior to the current state-of-the-art local feature descriptors.},
DOI = {10.3390/rs10081234}
}



@Article{s18082640,
AUTHOR = {Gallo, Mariano and De Luca, Giuseppina},
TITLE = {Spatial Extension of Road Traffic Sensor Data with Artificial Neural Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2640},
URL = {https://www.mdpi.com/1424-8220/18/8/2640},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a method for estimating traffic flows on some links of a road network knowing the data on other links that are monitored with sensors. In this way, it is possible to obtain more information on traffic conditions without increasing the number of monitored links. The proposed method is based on artificial neural networks (ANNs), wherein the input data are the traffic flows on some monitored road links and the output data are the traffic flows on some unmonitored links. We have implemented and tested several single-layer feed-forward ANNs that differ in the number of neurons and the method of generating datasets for training. The proposed ANNs were trained with a supervised learning approach where input and output example datasets were generated through traffic simulation techniques. The proposed method was tested on a real-scale network and gave very good results if the travel demand patterns were known and used for generating example datasets, and promising results if the demand patterns were not considered in the procedure. Numerical results have underlined that the ANNs with few neurons were more effective than the ones with many neurons in this specific problem.},
DOI = {10.3390/s18082640}
}



@Article{s18082674,
AUTHOR = {Liakos, Konstantinos G. and Busato, Patrizia and Moshou, Dimitrios and Pearson, Simon and Bochtis, Dionysis},
TITLE = {Machine Learning in Agriculture: A Review},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2674},
URL = {https://www.mdpi.com/1424-8220/18/8/2674},
ISSN = {1424-8220},
ABSTRACT = {Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.},
DOI = {10.3390/s18082674}
}



@Article{rs10081319,
AUTHOR = {Hidayat, Sarip and MATSUOKA, Masayuki and Baja, Sumbangan and Rampisela, Dorothea Agnes},
TITLE = {Object-Based Image Analysis for Sago Palm Classification: The Most Important Features from High-Resolution Satellite Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1319},
URL = {https://www.mdpi.com/2072-4292/10/8/1319},
ISSN = {2072-4292},
ABSTRACT = {Sago palm (Metroxylon sagu) is a palm tree species originating in Indonesia. In the future, this starch-producing tree will play an important role in food security and biodiversity. Local governments have begun to emphasize the sustainable development of sago palm plantations; therefore, they require near-real-time geospatial information on palm stands. We developed a semi-automated classification scheme for mapping sago palm using machine learning within an object-based image analysis framework with Pleiades-1A imagery. In addition to spectral information, arithmetic, geometric, and textural features were employed to enhance the classification accuracy. Recursive feature elimination was applied to samples to rank the importance of 26 input features. A support vector machine (SVM) was used to perform classifications and resulted in the highest overall accuracy of 85.00% after inclusion of the eight most important features, including three spectral features, three arithmetic features, and two textural features. The SVM classifier showed normal fitting up to the eighth most important feature. According to the McNemar test results, using the top seven to 14 features provided a better classification accuracy. The significance of this research is the revelation of the most important features in recognizing sago palm among other similar tree species.},
DOI = {10.3390/rs10081319}
}



@Article{s18092751,
AUTHOR = {Xue, Xizhe and Li, Ying and Shen, Qiang},
TITLE = {Unmanned Aerial Vehicle Object Tracking by Correlation Filter with Adaptive Appearance Model},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2751},
URL = {https://www.mdpi.com/1424-8220/18/9/2751},
ISSN = {1424-8220},
ABSTRACT = {With the increasing availability of low-cost, commercially available unmanned aerial vehicles (UAVs), visual tracking using UAVs has become more and more important due to its many new applications, including automatic navigation, obstacle avoidance, traffic monitoring, search and rescue, etc. However, real-world aerial tracking poses many challenges due to platform motion and image instability, such as aspect ratio change, viewpoint change, fast motion, scale variation and so on. In this paper, an efficient object tracking method for UAV videos is proposed to tackle these challenges. We construct the fused features to capture the gradient information and color characteristics simultaneously. Furthermore, cellular automata is introduced to update the appearance template of target accurately and sparsely. In particular, a high confidence model updating strategy is developed according to the stability function. Systematic comparative evaluations performed on the popular UAV123 dataset show the efficiency of the proposed approach.},
DOI = {10.3390/s18092751}
}



@Article{s18092753,
AUTHOR = {Hu, Jie and Wu, Zhongli and Qin, Xiongzhen and Geng, Huangzheng and Gao, Zhangbin},
TITLE = {An Extended Kalman Filter and Back Propagation Neural Network Algorithm Positioning Method Based on Anti-lock Brake Sensor and Global Navigation Satellite System Information},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2753},
URL = {https://www.mdpi.com/1424-8220/18/9/2753},
ISSN = {1424-8220},
ABSTRACT = {Telematics box (T-Box) chip-level Global Navigation Satellite System (GNSS) receiver modules usually suffer from GNSS information failure or noise in urban environments. In order to resolve this issue, this paper presents a real-time positioning method for Extended Kalman Filter (EKF) and Back Propagation Neural Network (BPNN) algorithms based on Antilock Brake System (ABS) sensor and GNSS information. Experiments were performed using an assembly in the vehicle with a T-Box. The T-Box firstly use automotive kinematical Pre-EKF to fuse the four wheel speed, yaw rate and steering wheel angle data from the ABS sensor to obtain a more accurate vehicle speed and heading angle velocity. In order to reduce the noise of the GNSS information, After-EKF fusion vehicle speed, heading angle velocity and GNSS data were used and low-noise positioning data were obtained. The heading angle speed error is extracted as target and part of low-noise positioning data were used as input for training a BPNN model. When the positioning is invalid, the well-trained BPNN corrected heading angle velocity output and vehicle speed add the synthesized relative displacement to the previous absolute position to realize a new position. With the data of high-precision real-time kinematic differential positioning equipment as the reference, the use of the dual EKF can reduce the noise range of GNSS information and concentrate good-positioning signals of the road within 5 m (i.e. the positioning status is valid). When the GNSS information was shielded (making the positioning status invalid), and the previous data was regarded as a training sample, it is found that the vehicle achieved 15 minutes position without GNSS information on the recycling line. The results indicated this new position method can reduce the vehicle positioning noise when GNSS information is valid and determine the position during long periods of invalid GNSS information.},
DOI = {10.3390/s18092753}
}



@Article{rs10091339,
AUTHOR = {Liu, Shuo and Ding, Wenrui and Liu, Chunhui and Liu, Yu and Wang, Yufeng and Li, Hongguang},
TITLE = {ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1339},
URL = {https://www.mdpi.com/2072-4292/10/9/1339},
ISSN = {2072-4292},
ABSTRACT = {The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved.},
DOI = {10.3390/rs10091339}
}



@Article{s18092770,
AUTHOR = {Tamouridou, Afroditi Alexandra and Pantazi, Xanthoula Eirini and Alexandridis, Thomas and Lagopodi, Anastasia and Kontouris, Giorgos and Moshou, Dimitrios},
TITLE = {Spectral Identification of Disease in Weeds Using Multilayer Perceptron with Automatic Relevance Determination},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2770},
URL = {https://www.mdpi.com/1424-8220/18/9/2770},
ISSN = {1424-8220},
ABSTRACT = {Microbotryum silybum, a smut fungus, is studied as an agent for the biological control of Silybum marianum (milk thistle) weed. Confirmation of the systemic infection is essential in order to assess the effectiveness of the biological control application and assist decision-making. Nonetheless, in situ diagnosis is challenging. The presently demonstrated research illustrates the identification process of systemically infected S. marianum plants by means of field spectroscopy and the multilayer perceptron/automatic relevance determination (MLP-ARD) network. Leaf spectral signatures were obtained from both healthy and infected S. marianum plants using a portable visible and near-infrared spectrometer (310&ndash;1100 nm). The MLP-ARD algorithm was applied for the recognition of the infected S. marianum plants. Pre-processed spectral signatures served as input features. The spectra pre-processing consisted of normalization, and second derivative and principal component extraction. MLP-ARD reached a high overall accuracy (90.32%) in the identification process. The research results establish the capacity of MLP-ARD to precisely identify systemically infected S. marianum weeds during their vegetative growth stage.},
DOI = {10.3390/s18092770}
}



@Article{rs10091347,
AUTHOR = {Chen, Ting and Pennisi, Andrea and Li, Zhi and Zhang, Yanning and Sahli, Hichem},
TITLE = {A Hierarchical Association Framework for Multi-Object Tracking in Airborne Videos},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1347},
URL = {https://www.mdpi.com/2072-4292/10/9/1347},
ISSN = {2072-4292},
ABSTRACT = {Multi-Object Tracking (MOT) in airborne videos is a challenging problem due to the uncertain airborne vehicle motion, vibrations of the mounted camera, unreliable detections, changes of size, appearance and motion of the moving objects and occlusions caused by the interaction between moving and static objects in the scene. To deal with these problems, this work proposes a four-stage hierarchical association framework for multiple object tracking in airborne video. The proposed framework combines Data Association-based Tracking (DAT) methods and target tracking using a compressive tracking approach, to robustly track objects in complex airborne surveillance scenes. In each association stage, different sets of tracklets and detections are associated to efficiently handle local tracklet generation, local trajectory construction, global drifting tracklet correction and global fragmented tracklet linking. Experiments with challenging airborne videos show significant tracking improvement compared to existing state-of-the-art methods.},
DOI = {10.3390/rs10091347}
}



