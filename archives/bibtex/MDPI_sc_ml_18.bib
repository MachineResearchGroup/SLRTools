
@Article{en13133371,
AUTHOR = {Jawad, Shafqat and Liu, Junyong},
TITLE = {Electrical Vehicle Charging Services Planning and Operation with Interdependent Power Networks and Transportation Networks: A Review of the Current Scenario and Future Trends},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3371},
URL = {https://www.mdpi.com/1996-1073/13/13/3371},
ISSN = {1996-1073},
ABSTRACT = {The growing trend in electrical vehicle (EV) deployment has transformed independent power network and transportation network studies into highly congested interdependent network performance evaluations assessing their impact on power and transportation systems. Electrified transportation is highly capable of intensifying the interdependent correlations across charging service, transportation, and power networks. However, the evaluation of the complex coupled relationship across charging services, transportation, and power networks poses several challenges, including an impact on charging scheduling, traffic congestion, charging loads on the power grid, and high costs. Therefore, this article presents comparative survey analytics of large-scale EV integration&rsquo;s impact on charging service network scheduling, transportation networks, and power networks. Moreover, price mechanism strategies to determine the charging fares, minimize investment profits, diminish traffic congestion, and reduce power distribution constraints under the influence of various factors were carried out. Additionally, the survey analysis stipulates the interdependent network performance index, ascertaining travel distance, route selection, long-term and short-term planning, and different infrastructure strategies. Finally, the limitations of the proposed study, potential research trends, and critical technologies are demonstrated for future inquiries.},
DOI = {10.3390/en13133371}
}



@Article{app10144735,
AUTHOR = {McClellan, Miranda and Cervelló-Pastor, Cristina and Sallent, Sebastià},
TITLE = {Deep Learning at the Mobile Edge: Opportunities for 5G Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4735},
URL = {https://www.mdpi.com/2076-3417/10/14/4735},
ISSN = {2076-3417},
ABSTRACT = {Mobile edge computing (MEC) within 5G networks brings the power of cloud computing, storage, and analysis closer to the end user. The increased speeds and reduced delay enable novel applications such as connected vehicles, large-scale IoT, video streaming, and industry robotics. Machine Learning (ML) is leveraged within mobile edge computing to predict changes in demand based on cultural events, natural disasters, or daily commute patterns, and it prepares the network by automatically scaling up network resources as needed. Together, mobile edge computing and ML enable seamless automation of network management to reduce operational costs and enhance user experience. In this paper, we discuss the state of the art for ML within mobile edge computing and the advances needed in automating adaptive resource allocation, mobility modeling, security, and energy efficiency for 5G networks.},
DOI = {10.3390/app10144735}
}



@Article{su12145561,
AUTHOR = {Silva, Bhagya Nathali and Khan, Murad and Han, Kijun},
TITLE = {Futuristic Sustainable Energy Management in Smart Environments: A Review of Peak Load Shaving and Demand Response Strategies, Challenges, and Opportunities},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {5561},
URL = {https://www.mdpi.com/2071-1050/12/14/5561},
ISSN = {2071-1050},
ABSTRACT = {The emergence of the Internet of Things (IoT) notion pioneered the implementation of various smart environments. Smart environments intelligibly accommodate inhabitants&rsquo; requirements. With rapid resource shrinkage, energy management has recently become an essential concern for all smart environments. Energy management aims to assure ecosystem sustainability, while benefiting both consumers and utility providers. Although energy management emerged as a solution that addresses challenges that arise with increasing energy demand and resource deterioration, further evolution and expansion are hindered due to technological, economical, and social barriers. This review aggregates energy management approaches in smart environments and extensively reviews a variety of recent literature reports on peak load shaving and demand response. Significant benefits and challenges of these energy management strategies were identified through the literature survey. Finally, a critical discussion summarizing trends and opportunities is given as a thread for future research.},
DOI = {10.3390/su12145561}
}



@Article{su12156142,
AUTHOR = {Vogiatzaki, Maria and Zerefos, Stelios and Hoque Tania, Marzia},
TITLE = {Enhancing City Sustainability through Smart Technologies: A Framework for Automatic Pre-Emptive Action to Promote Safety and Security Using Lighting and ICT-Based Surveillance},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {6142},
URL = {https://www.mdpi.com/2071-1050/12/15/6142},
ISSN = {2071-1050},
ABSTRACT = {The scope of the present paper is to promote social, cultural and environmental sustainability in cities by establishing a conceptual framework and the relationship amongst safety in urban public space (UPS), lighting and Information and Communication Technology (ICT)-based surveillance. This framework uses available technologies and tools, as these can be found in urban equipment such as lighting posts, to enhance security and safety in UPS, ensuring protection against attempted criminal activity. Through detailed literary research, publications on security and safety concerning crime and lighting can be divided into two periods, the first one pre-1994, and the second one from 2004&ndash;2008. Since then, a significant reduction in the number of publications dealing with lighting and crime is observed, while at the same time, the urban nightscape has been reshaped with the immersion of light-emitting diode (LED) technologies. Especially in the last decade, where most municipalities in the EU28 (European Union of all the member states from the accession of Croatia in 2013 to the withdrawal of the United Kingdom in 2020) are refurbishing their road lighting with LED technology and the consideration of smart networks and surveillance is under development, the use of lighting to deter possible attempted felonies in UPS is not addressed. To capitalize on the potential of lighting as a deterrent, this paper proposes a framework that uses existing technology, namely, dimmable LED light sources, presence sensors, security cameras, as well as emerging techniques such as artificial intelligence (AI)-enabled image recognition algorithms and big data analytics and presents a possible system that could be developed as a stand-alone product to alert possible dangerous situations, deter criminal activity and promote the perception of safety thus linking lighting and ICT-based surveillance towards safety and security in UPS.},
DOI = {10.3390/su12156142}
}



@Article{iot1010003,
AUTHOR = {Michailidis, Emmanouel T. and Potirakis, Stelios M. and Kanatas, Athanasios G.},
TITLE = {AI-Inspired Non-Terrestrial Networks for IIoT: Review on Enabling Technologies and Applications},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {1},
PAGES = {21--48},
URL = {https://www.mdpi.com/2624-831X/1/1/3},
ISSN = {2624-831X},
ABSTRACT = {During the last few years, various Industrial Internet of Things (IIoT) applications have emerged with numerous network elements interconnected using wired and wireless communication technologies and equipped with strategically placed sensors and actuators. This paper justifies why non-terrestrial networks (NTNs) will bring the IIoT vision closer to reality by providing improved data acquisition and massive connectivity to sensor fields in large and remote areas. NTNs are engineered to utilize satellites, airships, and aircrafts, which can be employed to extend the radio coverage and provide remote monitoring and sensing services. Additionally, this paper describes indicative delay-tolerant massive IIoT and delay-sensitive mission-critical IIoT applications spanning a large number of vertical markets with diverse and stringent requirements. As the heterogeneous nature of NTNs and the complex and dynamic communications scenarios lead to uncertainty and a high degree of variability, conventional wireless communication technologies cannot sufficiently support ultra-reliable and low-latency communications (URLLC) and offer ubiquitous and uninterrupted interconnectivity. In this regard, this paper sheds light on the potential role of artificial intelligence (AI) techniques, including machine learning (ML) and deep learning (DL), in the provision of challenging NTN-based IIoT services and provides a thorough review of the relevant research works. By adding intelligence and facilitating the decision-making and prediction procedures, the NTNs can effectively adapt to their surrounding environment, thus enhancing the performance of various metrics with significantly lower complexity compared to typical optimization methods.},
DOI = {10.3390/iot1010003}
}



@Article{s20154291,
AUTHOR = {Wu, Qiang and Wu, Jianqing and Shen, Jun and Yong, Binbin and Zhou, Qingguo},
TITLE = {An Edge Based Multi-Agent Auto Communication Method for Traffic Light Control},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4291},
URL = {https://www.mdpi.com/1424-8220/20/15/4291},
ISSN = {1424-8220},
ABSTRACT = {With smart city infrastructures growing, the Internet of Things (IoT) has been widely used in the intelligent transportation systems (ITS). The traditional adaptive traffic signal control method based on reinforcement learning (RL) has expanded from one intersection to multiple intersections. In this paper, we propose a multi-agent auto communication (MAAC) algorithm, which is an innovative adaptive global traffic light control method based on multi-agent reinforcement learning (MARL) and an auto communication protocol in edge computing architecture. The MAAC algorithm combines multi-agent auto communication protocol with MARL, allowing an agent to communicate the learned strategies with others for achieving global optimization in traffic signal control. In addition, we present a practicable edge computing architecture for industrial deployment on IoT, considering the limitations of the capabilities of network transmission bandwidth. We demonstrate that our algorithm outperforms other methods over 17% in experiments in a real traffic simulation environment.},
DOI = {10.3390/s20154291}
}



@Article{en13153928,
AUTHOR = {Toubeau, Jean-François and Bakhshideh Zad, Bashir and Hupez, Martin and De Grève, Zacharie and Vallée, François},
TITLE = {Deep Reinforcement Learning-Based Voltage Control to Deal with Model Uncertainties in Distribution Networks},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {3928},
URL = {https://www.mdpi.com/1996-1073/13/15/3928},
ISSN = {1996-1073},
ABSTRACT = {This paper addresses the voltage control problem in medium-voltage distribution networks. The objective is to cost-efficiently maintain the voltage profile within a safe range, in presence of uncertainties in both the future working conditions, as well as the physical parameters of the system. Indeed, the voltage profile depends not only on the fluctuating renewable-based power generation and load demand, but also on the physical parameters of the system components. In reality, the characteristics of loads, lines and transformers are subject to complex and dynamic dependencies, which are difficult to model. In such a context, the quality of the control strategy depends on the accuracy of the power flow representation, which requires to capture the non-linear behavior of the power network. Relying on the detailed analytical models (which are still subject to uncertainties) introduces a high computational power that does not comply with the real-time constraint of the voltage control task. To address this issue, while avoiding arbitrary modeling approximations, we leverage a deep reinforcement learning model to ensure an autonomous grid operational control. Outcomes show that the proposed model-free approach offers a promising alternative to find a compromise between calculation time, conservativeness and economic performance.},
DOI = {10.3390/en13153928}
}



@Article{electronics9081306,
AUTHOR = {Ijiga, Owoicho E. and Malekian, Reza and Chude-Okonkwo, Uche A. K.},
TITLE = {Enabling Emergent Configurations in the Industrial Internet of Things for Oil and Gas Explorations: A Survey},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1306},
URL = {https://www.mdpi.com/2079-9292/9/8/1306},
ISSN = {2079-9292},
ABSTRACT = {Several heterogeneous, intelligent, and distributed devices can be connected to interact with one another over the Internet in what is termed internet of things (IoT). Also, the concept of IoT can be exploited in the industrial environment for enhancing the production of goods and services and for mitigating the risk of disaster occurrences. This application of IoT for enhancing industrial production is known as industrial IoT (IIoT). Emergent configuration (EC) is a technology that can be adopted to enhance the operation and collaboration of IoT connected devices in order to improve the efficiency of the connected IoT systems for maximum user satisfaction. To meet user goals, the connected devices are required to cooperate with one another in an adaptive, interoperable, and homogeneous manner. In this paper, a survey of the concept of IoT is presented in addition to a review of IIoT systems. The application of ubiquitous computing-aided software define networking (SDN)-based EC architecture is propounded for enhancing the throughput of oil and gas production in the maritime ecosystems by managing the exploration process especially in emergency situations that involve anthropogenic oil and gas spillages.},
DOI = {10.3390/electronics9081306}
}



@Article{cleantechnol2030019,
AUTHOR = {Lai, Chun Sing and Jia, Youwei and Dong, Zhekang and Wang, Dongxiao and Tao, Yingshan and Lai, Qi Hong and Wong, Richard T. K. and Zobaa, Ahmed F. and Wu, Ruiheng and Lai, Loi Lei},
TITLE = {A Review of Technical Standards for Smart Cities},
JOURNAL = {Clean Technologies},
VOLUME = {2},
YEAR = {2020},
NUMBER = {3},
PAGES = {290--310},
URL = {https://www.mdpi.com/2571-8797/2/3/19},
ISSN = {2571-8797},
ABSTRACT = {Smart cities employ technology and data to increase efficiencies, economic development, sustainability, and life quality for citizens in urban areas. Inevitably, clean technologies promote smart cities development including for energy, transportation and health. The smart city concept is ambitious and is being refined with standards. Standards are used to help with regulating how smart cities function and contributing to define a smart city. Smart cities must be officially recognized by national and international authorities and organizations in order to promote societal advancement. There are many research and review articles on smart cities. However, technical standards are seldom discussed in the current literature. This review firstly presents the study of smart city definitions and domain. The well-known smart city standards will be presented to better recognize the smart city concept. Well-defined standards allow meaningful comparisons among smart cities implementation. How smart city initiatives make a city smarter and improve the quality of life will be discussed for various countries. This review highlights that technical standards are important for smart cities implementation. This paper serves as a guide to the most recent developments of smart cities standards.},
DOI = {10.3390/cleantechnol2030019}
}



@Article{app10175773,
AUTHOR = {Alparslan, Onur and Arakawa, Shin’ichi and Murata, Masayuki},
TITLE = {SDN-Based Control of IoT Network by Brain-Inspired Bayesian Attractor Model and Network Slicing},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {5773},
URL = {https://www.mdpi.com/2076-3417/10/17/5773},
ISSN = {2076-3417},
ABSTRACT = {One of the models in the literature for modeling the behavior of the brain is the Bayesian attractor model, which is a kind of machine-learning algorithm. According to this model, the brain assigns stochastic variables to possible decisions (attractors) and chooses one of them when enough evidence is collected from sensory systems to achieve a confidence level high enough to make a decision. In this paper, we introduce a software defined networking (SDN) application based on a brain-inspired Bayesian attractor model for identification of the current traffic pattern for the supervision and automation of Internet of things (IoT) networks that exhibit a limited number of traffic patterns. In a real SDN testbed, we demonstrate that our SDN application can identify the traffic patterns using a limited set of fluctuating network statistics of edge link utilization. Moreover, we show that our application can improve core link utilization and the power efficiency of IoT networks by immediately applying a pre-calculated network configuration optimized by traffic engineering with network slicing for the identified pattern.},
DOI = {10.3390/app10175773}
}



@Article{a13090208,
AUTHOR = {Santos, Guto Leoni and Endo, Patricia Takako and Sadok, Djamel and Kelner, Judith},
TITLE = {When 5G Meets Deep Learning: A Systematic Review},
JOURNAL = {Algorithms},
VOLUME = {13},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {208},
URL = {https://www.mdpi.com/1999-4893/13/9/208},
ISSN = {1999-4893},
ABSTRACT = {This last decade, the amount of data exchanged on the Internet increased by over a staggering factor of 100, and is expected to exceed well over the 500 exabytes by 2020. This phenomenon is mainly due to the evolution of high-speed broadband Internet and, more specifically, the popularization and wide spread use of smartphones and associated accessible data plans. Although 4G with its long-term evolution (LTE) technology is seen as a mature technology, there is continual improvement to its radio technology and architecture such as in the scope of the LTE Advanced standard, a major enhancement of LTE. However, for the long run, the next generation of telecommunication (5G) is considered and is gaining considerable momentum from both industry and researchers. In addition, with the deployment of the Internet of Things (IoT) applications, smart cities, vehicular networks, e-health systems, and Industry 4.0, a new plethora of 5G services has emerged with very diverging and technologically challenging design requirements. These include high mobile data volume per area, high number of devices connected per area, high data rates, longer battery life for low-power devices, and reduced end-to-end latency. Several technologies are being developed to meet these new requirements, and each of these technologies brings its own design issues and challenges. In this context, deep learning models could be seen as one of the main tools that can be used to process monitoring data and automate decisions. As these models are able to extract relevant features from raw data (images, texts, and other types of unstructured data), the integration between 5G and DL looks promising and one that requires exploring. As main contribution, this paper presents a systematic review about how DL is being applied to solve some 5G issues. Differently from the current literature, we examine data from the last decade and the works that address diverse 5G specific problems, such as physical medium state estimation, network traffic prediction, user device location prediction, self network management, among others. We also discuss the main research challenges when using deep learning models in 5G scenarios and identify several issues that deserve further consideration.},
DOI = {10.3390/a13090208}
}



@Article{fi12090147,
AUTHOR = {Isyaku, Babangida and Mohd Zahid, Mohd Soperi and Bte Kamat, Maznah and Abu Bakar, Kamalrulnizam and Ghaleb, Fuad A.},
TITLE = {Software Defined Networking Flow Table Management of OpenFlow Switches Performance and Security Challenges: A Survey},
JOURNAL = {Future Internet},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {147},
URL = {https://www.mdpi.com/1999-5903/12/9/147},
ISSN = {1999-5903},
ABSTRACT = {Software defined networking (SDN) is an emerging network paradigm that decouples the control plane from the data plane. The data plane is composed of forwarding elements called switches and the control plane is composed of controllers. SDN is gaining popularity from industry and academics due to its advantages such as centralized, flexible, and programmable network management. The increasing number of traffics due to the proliferation of the Internet of Thing (IoT) devices may result in two problems: (1) increased processing load of the controller, and (2) insufficient space in the switches’ flow table to accommodate the flow entries. These problems may cause undesired network behavior and unstable network performance, especially in large-scale networks. Many solutions have been proposed to improve the management of the flow table, reducing controller processing load, and mitigating security threats and vulnerabilities on the controllers and switches. This paper provides comprehensive surveys of existing schemes to ensure SDN meets the quality of service (QoS) demands of various applications and cloud services. Finally, potential future research directions are identified and discussed such as management of flow table using machine learning.},
DOI = {10.3390/fi12090147}
}



@Article{s20185044,
AUTHOR = {Kufakunesu, Rachel and Hancke, Gerhard P. and Abu-Mahfouz, Adnan M.},
TITLE = {A Survey on Adaptive Data Rate Optimization in LoRaWAN: Recent Solutions and Major Challenges},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5044},
URL = {https://www.mdpi.com/1424-8220/20/18/5044},
ISSN = {1424-8220},
ABSTRACT = {Long-Range Wide Area Network (LoRaWAN) is a fast-growing communication system for Low Power Wide Area Networks (LPWAN) in the Internet of Things (IoTs) deployments. LoRaWAN is built to optimize LPWANs for battery lifetime, capacity, range, and cost. LoRaWAN employs an Adaptive Data Rate (ADR) scheme that dynamically optimizes data rate, airtime, and energy consumption. The major challenge in LoRaWAN is that the LoRa specification does not state how the network server must command end nodes pertaining rate adaptation. As a result, numerous ADR schemes have been proposed to cater for the many applications of IoT technology, the quality of service requirements, different metrics, and radio frequency (RF) conditions. This offers a challenge for the reliability and suitability of these schemes. This paper presents a comprehensive review of the research on ADR algorithms for LoRaWAN technology. First, we provide an overview of LoRaWAN network performance that has been explored and documented in the literature and then focus on recent solutions for ADR as an optimization approach to improve throughput, energy efficiency and scalability. We then distinguish the approaches used, highlight their strengths and drawbacks, and provide a comparison of these approaches. Finally, we identify some research gaps and future directions.},
DOI = {10.3390/s20185044}
}



@Article{su12187300,
AUTHOR = {Leiva-Padilla, Paulina and Moreno-Navarro, Fernando and Iglesias, Guillermo and Rubio-Gamez, Mª Carmen},
TITLE = {Interpretation of the Magnetic Field Signals Emitted by Encoded Asphalt Pavement Materials},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {7300},
URL = {https://www.mdpi.com/2071-1050/12/18/7300},
ISSN = {2071-1050},
ABSTRACT = {Asphalt materials modified with different types and dosages of magnetically responsive materials can emit patterns of magnetic signals easily detectable by magnetic field sensors. These patterns could be used to encode roads and improve infrastructure-to-vehicle (I2V)/road-to-vehicle (R2V) communications. In this sense, this paper presents a laboratory study addressed to analyze the magnetic field signals emitted by encoded asphalt specimens manufactured with various dosages of steel fibers. The analysis consisted in the evaluation of the influence of three parameters: (1) the height of placement of the magnetic field sensors, (2) the approach speed of the encoded specimen/vehicle and (3) the distance from signal detection. Results show that, for each one of the parameters evaluated, there is a limit value below which it is possible to work with the magnetic signal emitted by the encoded samples. A proof of concept was used to validate the results obtained.},
DOI = {10.3390/su12187300}
}



@Article{computers9030073,
AUTHOR = {Sarasa-Cabezuelo, Antonio},
TITLE = {The Use of Geolocation to Manage Passenger Mobility between Airports and Cities},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {73},
URL = {https://www.mdpi.com/2073-431X/9/3/73},
ISSN = {2073-431X},
ABSTRACT = {A general problem in large cities is mobility. Every day, there are incidents (accidents, construction, or meteorological events) that increase the duration of the journeys in a city and exert negative effects on the lives of citizens. A particular case of this situation is communications with airports. Shuttles are a type of private transport service that operates in airports. The number of passengers carried by shuttles is small. Moreover, shuttles transport passengers to their final destinations and their prices are more competitive than those other private services. However, shuttle services have a limitation with respect to their routes due to the many different destinations of passengers. For this reason, it is important to be able to calculate the best route to optimize the journey. This work presents an Android application that implements a value-added service to plan the route of a shuttle and manage its service (passengers, journeys, routes, etc.). This application combines information from different sources, such as geolocation information from the shuttle driver’s mobile phone, information on the passengers, and results obtained from the service offered by MapBox to calculate the optimized route between several destinations.},
DOI = {10.3390/computers9030073}
}



@Article{electronics9091501,
AUTHOR = {Neelakantam, Gone and Onthoni, Djeane Debora and Sahoo, Prasan Kumar},
TITLE = {Reinforcement Learning Based Passengers Assistance System for Crowded Public Transportation in Fog Enabled Smart City},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1501},
URL = {https://www.mdpi.com/2079-9292/9/9/1501},
ISSN = {2079-9292},
ABSTRACT = {Crowding in city public transportation systems is a primary issue that causes delay in the mobility of passengers. Moreover, scheduled and unscheduled events in a city lead to excess crowding situations at the metro or bus stations. The Internet of Things (IoT) devices could be used for data collection, which are related to crowding situations in a smart city. The fog computing data centers located in different zones of a smart city can process and analyze the collected data to assist the passengers how to commute smoothly with minimum waiting time in the crowded situation. In this paper, Q-learning based passengers assistance system is designed to assist the commuters in finding less crowded bus and metro stations to avoid long queues of waiting. The traffic congestion and crowded situation data are processed in the fog computing data centers. From our experimental results, it is found that our proposed method can achieve higher reward values, which can be used to minimize the passengers&rsquo; waiting time with minimum computational delay as compared to the cloud computing platform.},
DOI = {10.3390/electronics9091501}
}



@Article{info11090453,
AUTHOR = {Carta, Salvatore and Podda, Alessandro Sebastian and Recupero, Diego Reforgiato and Saia, Roberto and Usai, Giovanni},
TITLE = {Popularity Prediction of Instagram Posts},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {453},
URL = {https://www.mdpi.com/2078-2489/11/9/453},
ISSN = {2078-2489},
ABSTRACT = {Predicting the popularity of posts on social networks has taken on significant importance in recent years, and several social media management tools now offer solutions to improve and optimize the quality of published content and to enhance the attractiveness of companies and organizations. Scientific research has recently moved in this direction, with the aim of exploiting advanced techniques such as machine learning, deep learning, natural language processing, etc., to support such tools. In light of the above, in this work we aim to address the challenge of predicting the popularity of a future post on Instagram, by defining the problem as a classification task and by proposing an original approach based on Gradient Boosting and feature engineering, which led us to promising experimental results. The proposed approach exploits big data technologies for scalability and efficiency, and it is general enough to be applied to other social media as well.},
DOI = {10.3390/info11090453}
}



@Article{s20185393,
AUTHOR = {Fattah, Salmah and Gani, Abdullah and Ahmedy, Ismail and Idris, Mohd Yamani Idna and Targio Hashem, Ibrahim Abaker},
TITLE = {A Survey on Underwater Wireless Sensor Networks: Requirements, Taxonomy, Recent Advances, and Open Research Challenges},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5393},
URL = {https://www.mdpi.com/1424-8220/20/18/5393},
ISSN = {1424-8220},
ABSTRACT = {The domain of underwater wireless sensor networks (UWSNs) had received a lot of attention recently due to its significant advanced capabilities in the ocean surveillance, marine monitoring and application deployment for detecting underwater targets. However, the literature have not compiled the state-of-the-art along its direction to discover the recent advancements which were fuelled by the underwater sensor technologies. Hence, this paper offers the newest analysis on the available evidences by reviewing studies in the past five years on various aspects that support network activities and applications in UWSN environments. This work was motivated by the need for robust and flexible solutions that can satisfy the requirements for the rapid development of the underwater wireless sensor networks. This paper identifies the key requirements for achieving essential services as well as common platforms for UWSN. It also contributes a taxonomy of the critical elements in UWSNs by devising a classification on architectural elements, communications, routing protocol and standards, security, and applications of UWSNs. Finally, the major challenges that remain open are presented as a guide for future research directions.},
DOI = {10.3390/s20185393}
}



@Article{e22091057,
AUTHOR = {Dias Canedo, Edna and Cordeiro Mendes, Bruno},
TITLE = {Software Requirements Classification Using Machine Learning Algorithms},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1057},
URL = {https://www.mdpi.com/1099-4300/22/9/1057},
ISSN = {1099-4300},
ABSTRACT = {The correct classification of requirements has become an essential task within software engineering. This study shows a comparison among the text feature extraction techniques, and machine learning algorithms to the problem of requirements engineer classification to answer the two major questions &ldquo;Which works best (Bag of Words (BoW) vs. Term Frequency&ndash;Inverse Document Frequency (TF-IDF) vs. Chi Squared (CHI2)) for classifying Software Requirements into Functional Requirements (FR) and Non-Functional Requirements (NF), and the sub-classes of Non-Functional Requirements?&rdquo; and &ldquo;Which Machine Learning Algorithm provides the best performance for the requirements classification task?&rdquo;. The data used to perform the research was the PROMISE_exp, a recently made dataset that expands the already known PROMISE repository, a repository that contains labeled software requirements. All the documents from the database were cleaned with a set of normalization steps and the two feature extractions, and feature selection techniques used were BoW, TF-IDF and CHI2 respectively. The algorithms used for classification were Logist Regression (LR), Support Vector Machine (SVM), Multinomial Naive Bayes (MNB) and k-Nearest Neighbors (kNN). The novelty of our work is the data used to perform the experiment, the details of the steps used to reproduce the classification, and the comparison between BoW, TF-IDF and CHI2 for this repository not having been covered by other studies. This work will serve as a reference for the software engineering community and will help other researchers to understand the requirement classification process. We noticed that the use of TF-IDF followed by the use of LR had a better classification result to differentiate requirements, with an F-measure of 0.91 in binary classification (tying with SVM in that case), 0.74 in NF classification and 0.78 in general classification. As future work we intend to compare more algorithms and new forms to improve the precision of our models.},
DOI = {10.3390/e22091057}
}



@Article{s20195449,
AUTHOR = {Pereira, Rickson S. and Lieira, Douglas D. and Silva, Marco A. C. da and Pimenta, Adinovam H. M. and da Costa, Joahannes B. D. and Rosário, Denis and Villas, Leandro and Meneguette, Rodolfo I.},
TITLE = {RELIABLE: Resource Allocation Mechanism for 5G Network using Mobile Edge Computing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5449},
URL = {https://www.mdpi.com/1424-8220/20/19/5449},
ISSN = {1424-8220},
ABSTRACT = {Technological advancement is currently focused on the miniaturization of devices, and integrated circuits allow us to observe the increase in the number of Internet of Things (IoT) devices. Most IoT services and devices require an Internet connection, which needs to provide the minimum processing, storage and networking requirements to best serve a requested service. One of the main goals of 5G networks is to comply with the user&rsquo;s various Quality of Service (QoS) requirements in different application scenarios. Fifth-generation networks use Network Function Virtualization (NFV) and Mobile Edge Computing (MEC) concepts to achieve these QoS requirements. However, the computational resource allocation mechanisms required by the services are considered very complex. Thus, in this paper, we propose an allocation and management resources mechanism for 5G networks that uses MEC and simple mathematical methods to reduce the model complexity. The mechanism decides to allocate the resource in MEC to meet the requirements requested by the user. The simulation results show that the proposed mechanism provides a larger amount of services, leading to a reduction in the service lock number and as a reduction in the blocking ratio of services due to the accuracy of the approach and its load balancing in the process of resource allocation.},
DOI = {10.3390/s20195449}
}



@Article{s20195498,
AUTHOR = {Diyan, Muhammad and Khan, Murad and Nathali Silva, Bhagya and Han, Kijun},
TITLE = {Scheduling Sensor Duty Cycling Based on Event Detection Using Bi-Directional Long Short-Term Memory and Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5498},
URL = {https://www.mdpi.com/1424-8220/20/19/5498},
ISSN = {1424-8220},
ABSTRACT = {A smart home provides a facilitated environment for the detection of human activity with appropriate Deep Learning algorithms to manipulate data collected from numerous sensors attached to various smart things in a smart home environment. Human activities comprise expected and unexpected behavior events; therefore, detecting these events consisting of mutual dependent activities poses a key challenge in the activities detection paradigm. Besides, the battery-powered sensor ubiquitously and extensively monitors activities, disputes, and sensor energy depletion. Therefore, to address these challenges, we propose an Energy and Event Aware-Sensor Duty Cycling scheme. The proposed model predicts the future expected event using the Bi-Directional Long-Short Term Memory model and allocates Predictive Sensors to the predicted event. To detect the unexpected events, the proposed model localizes a Monitor Sensor within a cluster of Hibernate Sensors using the Jaccard Similarity Index. Finally, we optimize the performance of our proposed scheme by employing the Q-Learning algorithm to track the missed or undetected events. The simulation is executed against the conventional Machine Learning algorithms for the sensor duty cycle, scheduling to reduce the sensor energy consumption and improve the activity detection accuracy. The experimental evaluation of our proposed scheme shows significant improvement in activity detection accuracy from 94.12% to 96.12%. Besides, the effective rotation of the Monitor Sensor significantly improves the energy consumption of each sensor with the entire network lifetime.},
DOI = {10.3390/s20195498}
}



@Article{s20195603,
AUTHOR = {Ungurean, Ioan and Gaitan, Nicoleta Cristina},
TITLE = {A Software Architecture for the Industrial Internet of Things—A Conceptual Model},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5603},
URL = {https://www.mdpi.com/1424-8220/20/19/5603},
ISSN = {1424-8220},
ABSTRACT = {The Internet of Things (IoT) is an emerging concept that has revolutionized the use of new technologies in everyday life. The economic impact of IoT becoming very important, and it began to be used in the industrial environment under the name of the Industrial Internet of Things (IIoT) concept, which is a sub-domain of IoT. The IIoT changes the way industrial processes are controlled and monitored, increasing operating efficiency. This article proposes a software architecture for IIoT that has a low degree of abstraction compared to the reference architectures presented in the literature. The architecture is organized on four-layer and it integrates the latest concepts related to fog and edge computing. These concepts are activated through the use of fog/edge/gateway nodes, where the processing of data acquired from things is performed and it is the place where things interact with each other in the virtual environment. The main contributions of this paper are the proposal and description of a complete IIoT software architecture, the use of a unified address space, and the use of the computing platform based on SoC (System on Chip) with specialized co-processors in order to be able to execute in real-time certain time-critical operations specific to the industrial environment.},
DOI = {10.3390/s20195603}
}



@Article{app10207120,
AUTHOR = {Mohammed, Thaha and Albeshri, Aiiad and Katib, Iyad and Mehmood, Rashid},
TITLE = {UbiPriSEQ—Deep Reinforcement Learning to Manage Privacy, Security, Energy, and QoS in 5G IoT HetNets},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7120},
URL = {https://www.mdpi.com/2076-3417/10/20/7120},
ISSN = {2076-3417},
ABSTRACT = {5G networks and Internet of Things (IoT) offer a powerful platform for ubiquitous environments with their ubiquitous sensing, high speeds and other benefits. The data, analytics, and other computations need to be optimally moved and placed in these environments, dynamically, such that energy-efficiency and QoS demands are best satisfied. A particular challenge in this context is to preserve privacy and security while delivering quality of service (QoS) and energy-efficiency. Many works have tried to address these challenges but without a focus on optimizing all of them and assuming fixed models of environments and security threats. This paper proposes the UbiPriSEQ framework that uses Deep Reinforcement Learning (DRL) to adaptively, dynamically, and holistically optimize QoS, energy-efficiency, security, and privacy. UbiPriSEQ is built on a three-layered model and comprises two modules. UbiPriSEQ devises policies and makes decisions related to important parameters including local processing and offloading rates for data and computations, radio channel states, transmit power, task priority, and selection of fog nodes for offloading, data migration, and so forth. UbiPriSEQ is implemented in Python over the TensorFlow platform and is evaluated using a real-life application in terms of SINR, privacy metric, latency, and utility function, manifesting great promise.},
DOI = {10.3390/app10207120}
}



@Article{app10207361,
AUTHOR = {Cabrera, Daniel and Cubillos, Claudio and Urra, Enrique and Mellado, Rafael},
TITLE = {Framework for Incorporating Artificial Somatic Markers in the Decision-Making of Autonomous Agents},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7361},
URL = {https://www.mdpi.com/2076-3417/10/20/7361},
ISSN = {2076-3417},
ABSTRACT = {The somatic marker hypothesis proposes that when a person faces a decision scenario, many thoughts arise and different &ldquo;physical consequences&rdquo; are fleetingly observable. It is generally accepted that affective dimension influences cognitive capacities. Several proposals for including affectivity within artificial systems have been presented. However, to the best of our knowledge, a proposal that considers the incorporation of artificial somatic markers in a disaggregated and specialized way for the different phases that make up a decision-making process has not been observed yet. Thus, this research work proposes a framework that considers the incorporation of artificial somatic markers in different phases of the decision-making of autonomous agents: recognition of decision point; determination of the courses of action; analysis of decision options; decision selection and performing; memory management. Additionally, a unified decision-making process and a general architecture for autonomous agents are presented. This proposal offers a qualitative perspective following an approach of grounded theory, which is suggested when existing theories or models cannot fully explain or understand a phenomenon or circumstance under study. This research work represents a novel contribution to the body of knowledge in guiding the incorporation of this biological concept in artificial terms within autonomous agents.},
DOI = {10.3390/app10207361}
}



@Article{electronics9111818,
AUTHOR = {Song, Jaein and Cho, Yun Ji and Kang, Min Hee and Hwang, Kee Yeon},
TITLE = {An Application of Reinforced Learning-Based Dynamic Pricing for Improvement of Ridesharing Platform Service in Seoul},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1818},
URL = {https://www.mdpi.com/2079-9292/9/11/1818},
ISSN = {2079-9292},
ABSTRACT = {As ridesharing services (including taxi) are often run by private companies, profitability is the top priority in operation. This leads to an increase in the driver&rsquo;s refusal to take passengers to areas with low demand where they will have difficulties finding subsequent passengers, causing problems such as an extended waiting time when hailing a vehicle for passengers bound for these regions. The study used Seoul&rsquo;s taxi data to find appropriate surge rates of ridesharing services between 10:00 p.m. and 4:00 a.m. by region using a reinforcement learning algorithm to resolve this problem during the worst time period. In reinforcement learning, the outcome of centrality analysis was applied as a weight affecting drivers&rsquo; destination choice probability. Furthermore, the reward function used in the learning was adjusted according to whether the passenger waiting time value was applied or not. The profit was used for reward value. By using a negative reward for the passenger waiting time, the study was able to identify a more appropriate surge level. Across the region, the surge averaged a value of 1.6. To be more specific, those located on the outskirts of the city and in residential areas showed a higher surge, while central areas had a lower surge. Due to this different surge, a driver&rsquo;s refusal to take passengers can be lessened and the passenger waiting time can be shortened. The supply of ridesharing services in low-demand regions can be increased by as much as 7.5%, allowing regional equity problems related to ridesharing services in Seoul to be reduced to a greater extent.},
DOI = {10.3390/electronics9111818}
}



@Article{s20216335,
AUTHOR = {Cecilia, José M. and Cano, Juan-Carlos and Morales-García, Juan and Llanes, Antonio and Imbernón, Baldomero},
TITLE = {Evaluation of Clustering Algorithms on GPU-Based Edge Computing Platforms},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6335},
URL = {https://www.mdpi.com/1424-8220/20/21/6335},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things (IoT) is becoming a new socioeconomic revolution in which data and immediacy are the main ingredients. IoT generates large datasets on a daily basis but it is currently considered as &ldquo;dark data&rdquo;, i.e., data generated but never analyzed. The efficient analysis of this data is mandatory to create intelligent applications for the next generation of IoT applications that benefits society. Artificial Intelligence (AI) techniques are very well suited to identifying hidden patterns and correlations in this data deluge. In particular, clustering algorithms are of the utmost importance for performing exploratory data analysis to identify a set (a.k.a., cluster) of similar objects. Clustering algorithms are computationally heavy workloads and require to be executed on high-performance computing clusters, especially to deal with large datasets. This execution on HPC infrastructures is an energy hungry procedure with additional issues, such as high-latency communications or privacy. Edge computing is a paradigm to enable light-weight computations at the edge of the network that has been proposed recently to solve these issues. In this paper, we provide an in-depth analysis of emergent edge computing architectures that include low-power Graphics Processing Units (GPUs) to speed-up these workloads. Our analysis includes performance and power consumption figures of the latest Nvidia&rsquo;s AGX Xavier to compare the energy-performance ratio of these low-cost platforms with a high-performance cloud-based counterpart version. Three different clustering algorithms (i.e., k-means, Fuzzy Minimals (FM), and Fuzzy C-Means (FCM)) are designed to be optimally executed on edge and cloud platforms, showing a speed-up factor of up to 11&times; for the GPU code compared to sequential counterpart versions in the edge platforms and energy savings of up to 150% between the edge computing and HPC platforms.},
DOI = {10.3390/s20216335}
}



@Article{en13225875,
AUTHOR = {Ren, Yuan and Zhang, Xuewei and Lu, Guangyue},
TITLE = {The Wireless Solution to Realize Green IoT: Cellular Networks with Energy Efficient and Energy Harvesting Schemes},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {5875},
URL = {https://www.mdpi.com/1996-1073/13/22/5875},
ISSN = {1996-1073},
ABSTRACT = {With the tremendous increase of heterogeneous Internet of Things (IoT) devices and the different service requirements of these IoT applications, machine-type communication (MTC) has attracted considerable attention from both industry and academia. Owing to the prominent advantages of supporting pervasive connectivity and wide area coverage, the cellular network is advocated as the potential wireless solution to realize IoT deployment for MTC, and this creative network paradigm is called the cellular IoT (C-IoT). In this paper, we propose the three-layer structured C-IoT architecture for MTC and review the challenges for deploying green C-IoT. Then, effective strategies for realizing green C-IoT are presented, including the energy efficient and energy harvesting schemes. We put forward several strategies to make the C-IoT run in an energy-saving manner, such as efficient random access and barring mechanisms, self-adapting machine learning predictions, scheduling optimization, resource allocation, fog computing, and group-oriented transmission. As for the energy harvesting schemes, the ambient and dedicated energy harvesting strategies are investigated. Afterwards, we give a detailed case study, which shows the effectiveness of reducing power consumption for the proposed layered C-IoT architecture. Additionally, for real-time and non-real-time applications, the power consumption of different on-off states for MTC devices is discussed.},
DOI = {10.3390/en13225875}
}



@Article{signals1020010,
AUTHOR = {Asad, Syed Muhammad and Ansari, Shuja and Ozturk, Metin and Rais, Rao Naveed Bin and Dashtipour, Kia and Hussain, Sajjad and Abbasi, Qammer H. and Imran, Muhammad Ali},
TITLE = {Mobility Management-Based Autonomous Energy-Aware Framework Using Machine Learning Approach in Dense Mobile Networks},
JOURNAL = {Signals},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {170--187},
URL = {https://www.mdpi.com/2624-6120/1/2/10},
ISSN = {2624-6120},
ABSTRACT = {A paramount challenge of prohibiting increased CO2 emissions for network densification is to deliver the Fifth Generation (5G) cellular capacity and connectivity demands, while maintaining a greener, healthier and prosperous environment. Energy consumption is a demanding consideration in the 5G era to combat several challenges such as reactive mode of operation, high latency wake up times, incorrect user association with the cells, multiple cross-functional operation of Self-Organising Networks (SON), etc. To address this challenge, we propose a novel Mobility Management-Based Autonomous Energy-Aware Framework for analysing bus passengers ridership through statistical Machine Learning (ML) and proactive energy savings coupled with CO2 emissions in Heterogeneous Network (HetNet) architecture using Reinforcement Learning (RL). Furthermore, we compare and report various ML algorithms using bus passengers ridership obtained from London Overground (LO) dataset. Extensive spatiotemporal simulations show that our proposed framework can achieve up to 98.82% prediction accuracy and CO2 reduction gains of up to 31.83%.},
DOI = {10.3390/signals1020010}
}



@Article{app10228220,
AUTHOR = {Umair, Areeba and Sarfraz, Muhammad Shahzad and Ahmad, Muhammad and Habib, Usman and Ullah, Muhammad Habib and Mazzara, Manuel},
TITLE = {Spatiotemporal Analysis of Web News Archives for Crime Prediction},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8220},
URL = {https://www.mdpi.com/2076-3417/10/22/8220},
ISSN = {2076-3417},
ABSTRACT = {In today&rsquo;s world, security is the most prominent aspect which has been given higher priority. Despite the rapid growth and usage of digital devices, lucrative measurement of crimes in under-developing countries is still challenging. In this work, unstructural crime data (900 records) from the news archives of the previous eight years were extracted to predict the behavior of criminals&rsquo; networks and transform it into useful information using natural language processing (NLP). To estimate the next move of criminals in Pakistan, we performed hotspot-based spatial analysis. Later, this information is fed to two different classifiers for possible identification and prediction. We achieved the maximum accuracy of 92% using K-Nearest Neighbor (KNN) and 62% using the Random Forest algorithm. In terms of crimes, the results showed that the most prevalent crime events are robberies. Thus, the usage of digital information archives, spatial analysis, and machine learning techniques can open new ways of handling a peaceful and sustainable society in eradicating crimes for countries having paucity of financial resources.},
DOI = {10.3390/app10228220}
}



@Article{inventions5040056,
AUTHOR = {Abdi, Hamdi and Shahbazitabar, Maryam and Mohammadi-Ivatloo, Behnam},
TITLE = {Food, Energy and Water Nexus: A Brief Review of Definitions, Research, and Challenges},
JOURNAL = {Inventions},
VOLUME = {5},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {56},
URL = {https://www.mdpi.com/2411-5134/5/4/56},
ISSN = {2411-5134},
ABSTRACT = {Vast expansion in consumption is leading to natural resource scarcity and global warming. The integrated management of natural resources, such as food, energy, food (FEW) as one of the most important aspects has been proposed as a solution to meet these challenges. The FEW nexus is a world-wide solution for simultaneously assessing the development and implementation of various approaches focusing on energy, water and food security, sufficiency. This approach is intended to foster sustainable development and improve the quality of life of communities while preserving the natural, human and social capital, address the long-term sustainability challenges and protecting all-natural resources. This paper tries to review some recent research on this topic. For this purpose, first, we describe some facts about demand growth and exponential consumption in these three areas, with emphasis on presented statistics. Then, the most critical research published in this field is reviewed, considering that it took a decade or so before that the original idea was introduced. The most important policymakers of this emerging concept, including committees and conferences, and finally significant challenges and opportunities to the implementation along with future insights, are addressed.},
DOI = {10.3390/inventions5040056}
}



@Article{s20236758,
AUTHOR = {Guo, Zihan and Han, Dezhi},
TITLE = {Multi-Modal Explicit Sparse Attention Networks for Visual Question Answering},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6758},
URL = {https://www.mdpi.com/1424-8220/20/23/6758},
ISSN = {1424-8220},
ABSTRACT = {Visual question answering (VQA) is a multi-modal task involving natural language processing (NLP) and computer vision (CV), which requires models to understand of both visual information and textual information simultaneously to predict the correct answer for the input visual image and textual question, and has been widely used in smart and intelligent transport systems, smart city, and other fields. Today, advanced VQA approaches model dense interactions between image regions and question words by designing co-attention mechanisms to achieve better accuracy. However, modeling interactions between each image region and each question word will force the model to calculate irrelevant information, thus causing the model&rsquo;s attention to be distracted. In this paper, to solve this problem, we propose a novel model called Multi-modal Explicit Sparse Attention Networks (MESAN), which concentrates the model&rsquo;s attention by explicitly selecting the parts of the input features that are the most relevant to answering the input question. We consider that this method based on top-k selection can reduce the interference caused by irrelevant information and ultimately help the model to achieve better performance. The experimental results on the benchmark dataset VQA v2 demonstrate the effectiveness of our model. Our best single model delivers 70.71% and 71.08% overall accuracy on the test-dev and test-std sets, respectively. In addition, we also demonstrate that our model can obtain better attended features than other advanced models through attention visualization. Our work proves that the models with sparse attention mechanisms can also achieve competitive results on VQA datasets. We hope that it can promote the development of VQA models and the application of artificial intelligence (AI) technology related to VQA in various aspects.},
DOI = {10.3390/s20236758}
}



@Article{e22121352,
AUTHOR = {Castro-Medina, Felipe and Rodríguez-Mazahua, Lisbeth and López-Chau, Asdrúbal and Cervantes, Jair and Alor-Hernández, Giner and Machorro-Cano, Isaac},
TITLE = {Application of Dynamic Fragmentation Methods in Multimedia Databases: A Review},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1352},
URL = {https://www.mdpi.com/1099-4300/22/12/1352},
PubMedID = {33266019},
ISSN = {1099-4300},
ABSTRACT = {Fragmentation is a design technique widely used in multimedia databases, because it produces substantial benefits in reducing response times, causing lower execution costs in each operation performed. Multimedia databases include data whose main characteristic is their large size, therefore, database administrators face a challenge of great importance, since they must contemplate the different qualities of non-trivial data. These databases over time undergo changes in their access patterns. Different fragmentation techniques presented in related studies show adequate workflows, however, some do not contemplate changes in access patterns. This paper aims to provide an in-depth review of the literature related to dynamic fragmentation of multimedia databases, to identify the main challenges, technologies employed, types of fragmentation used, and characteristics of the cost model. This review provides valuable information for database administrators by showing essential characteristics to perform proper fragmentation and to improve the performance of fragmentation schemes. The reduction of costs in fragmentation methods is one of the most desired main properties. To fulfill this objective, the works include cost models, covering different qualities. In this analysis, a set of characteristics used in the cost models of each work is presented to facilitate the creation of a new cost model including the most used qualities. In addition, different data sets or reference points used in the testing stage of each work analyzed are presented.},
DOI = {10.3390/e22121352}
}



@Article{electronics9122083,
AUTHOR = {Byabazaire, John and O’Hare, Gregory and Delaney, Declan},
TITLE = {Data Quality and Trust: Review of Challenges and Opportunities for Data Sharing in IoT},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2083},
URL = {https://www.mdpi.com/2079-9292/9/12/2083},
ISSN = {2079-9292},
ABSTRACT = {Existing research recognizes the critical role of quality data in the current big-data and Internet of Things (IoT) era. Quality data has a direct impact on model results and hence business decisions. The growth in the number of IoT-connected devices makes it hard to access data quality using traditional assessments methods. This is exacerbated by the need to share data across different IoT domains as it increases the heterogeneity of the data. Data-shared IoT defines a new perspective of IoT applications which benefit from sharing data among different domains of IoT to create new use-case applications. For example, sharing data between smart transport and smart industry can lead to other use-case applications such as intelligent logistics management and warehouse management. The benefits of such applications, however, can only be achieved if the shared data is of acceptable quality. There are three main practices in data quality (DQ) determination approaches that are restricting their effective use in data-shared platforms: (1) most DQ techniques validate test data against a known quantity considered to be a reference; a gold reference. (2) narrow sets of static metrics are used to describe the quality. Each consumer uses these metrics in similar ways. (3) data quality is evaluated in isolated stages throughout the processing pipeline. Data-shared IoT presents unique challenges; (1) each application and use-case in shared IoT has a unique description of data quality and requires a different set of metrics. This leads to an extensive list of DQ dimensions which are difficult to implement in real-world applications. (2) most data in IoT scenarios does not have a gold reference. (3) factors endangering DQ in shared IoT exist throughout the entire big-data model from data collection to data visualization, and data use. This paper aims to describe data-shared IoT and shared data pools while highlighting the importance of sharing quality data across various domains. The article examines how we can use trust as a measure of quality in data-shared IoT. We conclude that researchers can combine such trust-based techniques with blockchain for secure end-to-end data quality assessment.},
DOI = {10.3390/electronics9122083}
}



@Article{electronics9122146,
AUTHOR = {Zhou, Qianru and Pezaros, Dimitrios},
TITLE = {A Prediction-Based Model for Consistent Adaptive Routing in Back-Bone Networks at Extreme Situations},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2146},
URL = {https://www.mdpi.com/2079-9292/9/12/2146},
ISSN = {2079-9292},
ABSTRACT = {To reduce congestion, numerous routing solutions have been proposed for backbone networks, but how to select paths that stay consistently optimal for a long time in extremely congested situations, avoiding the unnecessary path reroutings, has not yet been investigated much. To solve that issue, a model that can measure the consistency of path latency difference is needed. In this paper, we make a humble step towards a consistent differential path latency model and by predicting base on that model, a metric Path Swap Indicator (PSI) is proposed. By learning the history latency of all optional paths, PSI is able to predict the onset of an obvious and steady channel deterioration and make the decision to switch paths. The effect of PSI is evaluated from the following aspects: (1) the consistency of the path selected, by measuring the time interval between PSI changes; (2) the accuracy of the channel congestion situation prediction; and (3) the improvement of the congestion situation. Experiments were carried out on a testbed using real-life Abilene traffic datasets collected at different times and locations. Results show that the proposed PSI can stay consistent for over 1000 s on average, and more than 3000 s at the longest in our experiment, while at the same time achieving a congestion situation improvement of more than 300% on average, and more than 200% at the least. It is evident that the proposed PSI metric is able to provide a consistent channel congestion prediction with satisfiable channel improvement at the same time. The results also demonstrate how different parameter values impact the result, both in terms of prediction consistency and the congestion improvement.},
DOI = {10.3390/electronics9122146}
}



@Article{jsan9040059,
AUTHOR = {De Vita, Fabrizio and Bruneo, Dario},
TITLE = {Leveraging Stack4Things for Federated Learning in Intelligent Cyber Physical Systems},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2224-2708/9/4/59},
ISSN = {2224-2708},
ABSTRACT = {During the last decade, the Internet of Things acted as catalyst for the big data phenomenon. As result, modern edge devices can access a huge amount of data that can be exploited to build useful services. In such a context, artificial intelligence has a key role to develop intelligent systems (e.g., intelligent cyber physical systems) that create a connecting bridge with the physical world. However, as time goes by, machine and deep learning applications are becoming more complex, requiring increasing amounts of data and training time, which makes the use of centralized approaches unsuitable. Federated learning is an emerging paradigm which enables the cooperation of edge devices to learn a shared model (while keeping private their training data), thereby abating the training time. Although federated learning is a promising technique, its implementation is difficult and brings a lot of challenges. In this paper, we present an extension of Stack4Things, a cloud platform developed in our department; leveraging its functionalities, we enabled the deployment of federated learning on edge devices without caring their heterogeneity. Experimental results show a comparison with a centralized approach and demonstrate the effectiveness of the proposed approach in terms of both training time and model accuracy.},
DOI = {10.3390/jsan9040059}
}



@Article{electronics9122207,
AUTHOR = {Dujić Rodić, Lea and Perković, Toni and Županović, Tomislav and Šolić, Petar},
TITLE = {Sensing Occupancy through Software: Smart Parking Proof of Concept},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2207},
URL = {https://www.mdpi.com/2079-9292/9/12/2207},
ISSN = {2079-9292},
ABSTRACT = {In order to detect the vehicle presence in parking slots, different approaches have been utilized, which range from image recognition to sensing via detection nodes. The last one is usually based on getting the presence data from one or more sensors (commonly magnetic or IR-based), controlled and processed by a micro-controller that sends the data through radio interface. Consequently, given nodes have multiple components, adequate software is required for its control and state-machine to communicate its status to the receiver. This paper presents an alternative, cost-effective beacon-based mechanism for sensing the vehicle presence. It is based on the well-known effect that, once the metallic obstacle (i.e., vehicle) is on top of the sensing node, the signal strength will be attenuated, while the same shall be recognized at the receiver side. Therefore, the signal strength change conveys the information regarding the presence. Algorithms processing signal strength change at the receiver side to estimate the presence are required due to the stochastic nature of signal strength parameters. In order to prove the concept, experimental setup based on LoRa-based parking sensors was used to gather occupancy/signal strength data. In order to extract the information of presence, the Hidden Markov Model (HMM) was employed with accuracy of up to 96%, while the Neural Network (NN) approach reaches an accuracy of up to 97%. The given approach reduces the costs of the sensor production by at least 50%.},
DOI = {10.3390/electronics9122207}
}



@Article{app11010339,
AUTHOR = {Hou, Lei and Wu, Shaoze and Zhang, Guomin (Kevin) and Tan, Yongtao and Wang, Xiangyu},
TITLE = {Literature Review of Digital Twins Applications in Construction Workforce Safety},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {339},
URL = {https://www.mdpi.com/2076-3417/11/1/339},
ISSN = {2076-3417},
ABSTRACT = {For many decades, safety has been a challenge in the construction sector. Despite extensive efforts to improve overall safety, the sector&rsquo;s casualty rate still remains high. In practice, dynamic and complex construction processes may lead to on-site risks and safety plans being overlooked, likely leading to a variety of safety accidents. Nowadays, under the guidance of the digital twins (DT) concept, the advent of state-of-the-art sensing and visualisation technologies has offered the possibility to improve construction health and safety in the workplace. To understand the research advances of these technologies, identify their gaps and challenges, and propose solutions to further advance the industry&rsquo;s safety, we conducted and report a thorough review on the state-of-the-art technological studies, and elaborate upon the key findings in detail. For instance, despite DT being proven to be effective in improving construction workforce safety, the construction industry has yet to fully exploit and streamline these innovations in practice. Overall, this review provides insights into technological clustering, improvement strategies, as well as workforce safety, which can benefit from formulating effective digital technology paradigms.},
DOI = {10.3390/app11010339}
}



@Article{app11010363,
AUTHOR = {Roldán-Gómez, Juan Jesús and González-Gironda, Eduardo and Barrientos, Antonio},
TITLE = {A Survey on Robotic Technologies for Forest Firefighting: Applying Drone Swarms to Improve Firefighters’ Efficiency and Safety},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {363},
URL = {https://www.mdpi.com/2076-3417/11/1/363},
ISSN = {2076-3417},
ABSTRACT = {Forest firefighting missions encompass multiple tasks related to prevention, surveillance, and extinguishing. This work presents a complete survey of firefighters on the current problems in their work and the potential technological solutions. Additionally, it reviews the efforts performed by the academy and industry to apply different types of robots in the context of firefighting missions. Finally, all this information is used to propose a concept of operation for the comprehensive application of drone swarms in firefighting. The proposed system is a fleet of quadcopters that individually are only able to visit waypoints and use payloads, but collectively can perform tasks of surveillance, mapping, monitoring, etc. Three operator roles are defined, each one with different access to information and functions in the mission: mission commander, team leaders, and team members. These operators take advantage of virtual and augmented reality interfaces to intuitively get the information of the scenario and, in the case of the mission commander, control the drone swarm.},
DOI = {10.3390/app11010363}
}



@Article{s21020359,
AUTHOR = {Honar Pajooh, Houshyar and Rashid, Mohammad and Alam, Fakhrul and Demidenko, Serge},
TITLE = {Hyperledger Fabric Blockchain for Securing the Edge Internet of Things},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {359},
URL = {https://www.mdpi.com/1424-8220/21/2/359},
PubMedID = {33430274},
ISSN = {1424-8220},
ABSTRACT = {Providing security and privacy to the Internet of Things (IoT) networks while achieving it with minimum performance requirements is an open research challenge. Blockchain technology, as a distributed and decentralized ledger, is a potential solution to tackle the limitations of the current peer-to-peer IoT networks. This paper presents the development of an integrated IoT system implementing the permissioned blockchain Hyperledger Fabric (HLF) to secure the edge computing devices by employing a local authentication process. In addition, the proposed model provides traceability for the data generated by the IoT devices. The presented solution also addresses the IoT systems&rsquo; scalability challenges, the processing power and storage issues of the IoT edge devices in the blockchain network. A set of built-in queries is leveraged by smart-contracts technology to define the rules and conditions. The paper validates the performance of the proposed model with practical implementation by measuring performance metrics such as transaction throughput and latency, resource consumption, and network use. The results show that the proposed platform with the HLF implementation is promising for the security of resource-constrained IoT devices and is scalable for deployment in various IoT scenarios.},
DOI = {10.3390/s21020359}
}



@Article{s21020463,
AUTHOR = {Sharma, Sparsh and Ahmed, Suhaib and Naseem, Mohd and Alnumay, Waleed S. and Singh, Saurabh and Cho, Gi Hwan},
TITLE = {A Survey on Applications of Artificial Intelligence for Pre-Parametric Project Cost and Soil Shear-Strength Estimation in Construction and Geotechnical Engineering},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {463},
URL = {https://www.mdpi.com/1424-8220/21/2/463},
PubMedID = {33440731},
ISSN = {1424-8220},
ABSTRACT = {Ensuring soil strength, as well as preliminary construction cost and duration prediction, is a very crucial and preliminary aspect of any construction project. Similarly, building strong structures is very important in geotechnical engineering to ensure the bearing capability of structures against external forces. Hence, in this first-of-its-kind state-of-the-art review, the capability of various artificial intelligence (AI)-based models toward accurate prediction and estimation of preliminary construction cost, duration, and shear strength is explored. Initially, background regarding the revolutionary AI technology along with its different models suited for geotechnical and construction engineering is presented. Various existing works in the literature on the usage of AI-based models for the abovementioned applications of construction and maintenance are presented along with their advantages, limitations, and future work. Through analysis, various crucial input parameters with great impact on the estimation of preliminary construction cost, duration, and soil shear strength are enumerated and presented. Lastly, various challenges in using AI-based models for accurate predictions in these applications, as well as factors contributing to the cost-overrun issues, are presented. This study can, thus, greatly assist civil engineers in efficiently using the capabilities of AI for solving complex and risk-sensitive tasks, and it can also be used in Internet of things (IoT) environments for automated applications such as smart structural health-monitoring systems.},
DOI = {10.3390/s21020463}
}



@Article{en14020375,
AUTHOR = {Cheng, Jiayu and Duan, Dongliang and Cheng, Xiang and Yang, Liuqing and Cui, Shuguang},
TITLE = {Adaptive Control for Energy Exchange with Probabilistic Interval Predictors in Isolated Microgrids},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {375},
URL = {https://www.mdpi.com/1996-1073/14/2/375},
ISSN = {1996-1073},
ABSTRACT = {Stability and reliability are of the most important concern for isolated microgrid systems that have no support from the utility grid. Interval predictions are often applied to ensure the system stability of isolated microgrids as they cover more uncertainties and robust control can be achieved based on more sufficient information. In this paper, we propose a probabilistic microgrid energy exchange method based on the Model Predictive Control (MPC) approach to make better use of the prediction intervals so that the system stability and cost efficiency of isolated microgrids are improved simultaneously. Appropriate scenarios are selected from the predictions according to the evaluation of future trends and system capacity. In the meantime, a two-stage adaptive reserve strategy is adopted to further utilize the potential of interval predictions and maintain the system security adaptively. Reserves are determined at the optimization stage to prepare some extra capacity for the fluctuations in the renewable generation and load demand at the operation stage based on the aggressive and conservative level of the system, which is automatically updated at each step. The optimal dispatch problem is finally formulated using the mixed-integer linear programming model and the MPC is formulated as an optimization problem with a discount factor introduced to adjust the weights. Case studies show that the proposed method could effectively guarantee the stability of the system and improve economic performance.},
DOI = {10.3390/en14020375}
}



@Article{rs13020245,
AUTHOR = {Guo, Yujuan and Liao, Jingjuan and Shen, Guozhuang},
TITLE = {Mapping Large-Scale Mangroves along the Maritime Silk Road from 1990 to 2015 Using a Novel Deep Learning Model and Landsat Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {245},
URL = {https://www.mdpi.com/2072-4292/13/2/245},
ISSN = {2072-4292},
ABSTRACT = {Mangroves are important ecosystems and their distribution and dynamics can provide an understanding of the processes of ecological change. Meanwhile, mangroves protection is also an important element of the Maritime Silk Road (MSR) Cooperation Project. Large amounts of accessible satellite remote sensing data can provide timely and accurate information on the dynamics of mangroves, offering significant advantages in space, time, and characterization. In view of the capability of deep learning in processing massive data in recent years, we developed a new deep learning model&mdash;Capsules-Unet, which introduces the capsule concept into U-net to extract mangroves with high accuracy by learning the spatial relationship between objects in images. This model can significantly reduce the number of network parameters to improve the efficiency of data processing. This study uses Landsat data combined with Capsules-Unet to map the dynamics of mangrove changes over the 25 years (1990&ndash;2015) along the MSR. The results show that there was a loss in the mangrove area of 1,356,686 ha (about 21.5%) between 1990 and 2015, with anthropic activities such as agriculture, aquaculture, tourism, urban development, and over-development appearing to be the likely drivers of this decline. This information contributes to the understanding of ecological conditions, variability characteristics, and influencing factors along the MSR.},
DOI = {10.3390/rs13020245}
}



@Article{su13020751,
AUTHOR = {Andronie, Mihai and Lăzăroiu, George and Iatagan, Mariana and Hurloiu, Iulian and Dijmărescu, Irina},
TITLE = {Sustainable Cyber-Physical Production Systems in Big Data-Driven Smart Urban Economy: A Systematic Literature Review},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {751},
URL = {https://www.mdpi.com/2071-1050/13/2/751},
ISSN = {2071-1050},
ABSTRACT = {In this article, we cumulate previous research findings indicating that cyber-physical production systems bring about operations shaping social sustainability performance technologically. We contribute to the literature on sustainable cyber-physical production systems by showing that the technological and operations management features of cyber-physical systems constitute the components of data-driven sustainable smart manufacturing. Throughout September 2020, we performed a quantitative literature review of the Web of Science, Scopus, and ProQuest databases, with search terms including &ldquo;sustainable industrial value creation&rdquo;, &ldquo;cyber-physical production systems&rdquo;, &ldquo;sustainable smart manufacturing&rdquo;, &ldquo;smart economy&rdquo;, &ldquo;industrial big data analytics&rdquo;, &ldquo;sustainable Internet of Things&rdquo;, and &ldquo;sustainable Industry 4.0&rdquo;. As we inspected research published only in 2019 and 2020, only 323 articles satisfied the eligibility criteria. By eliminating controversial findings, outcomes unsubstantiated by replication, too imprecise material, or having similar titles, we decided upon 119, generally empirical, sources. Future research should investigate whether Industry 4.0-based manufacturing technologies can ensure the sustainability of big data-driven production systems by use of Internet of Things sensing networks and deep learning-assisted smart process planning.},
DOI = {10.3390/su13020751}
}



@Article{g12010008,
AUTHOR = {Chica-Pedraza, Gustavo and Mojica-Nava, Eduardo and Cadena-Muñoz, Ernesto},
TITLE = {Boltzmann Distributed Replicator Dynamics: Population Games in a Microgrid Context},
JOURNAL = {Games},
VOLUME = {12},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {8},
URL = {https://www.mdpi.com/2073-4336/12/1/8},
ISSN = {2073-4336},
ABSTRACT = {Multi-Agent Systems (MAS) have been used to solve several optimization problems in control systems. MAS allow understanding the interactions between agents and the complexity of the system, thus generating functional models that are closer to reality. However, these approaches assume that information between agents is always available, which means the employment of a full-information model. Some tendencies have been growing in importance to tackle scenarios where information constraints are relevant issues. In this sense, game theory approaches appear as a useful technique that use a strategy concept to analyze the interactions of the agents and achieve the maximization of agent outcomes. In this paper, we propose a distributed control method of learning that allows analyzing the effect of the exploration concept in MAS. The dynamics obtained use Q-learning from reinforcement learning as a way to include the concept of exploration into the classic exploration-less Replicator Dynamics equation. Then, the Boltzmann distribution is used to introduce the Boltzmann-Based Distributed Replicator Dynamics as a tool for controlling agents behaviors. This distributed approach can be used in several engineering applications, where communications constraints between agents are considered. The behavior of the proposed method is analyzed using a smart grid application for validation purposes. Results show that despite the lack of full information of the system, by controlling some parameters of the method, it has similar behavior to the traditional centralized approaches.},
DOI = {10.3390/g12010008}
}



@Article{electronics10030227,
AUTHOR = {Neelakantam, Gone and Onthoni, Djeane Debora and Sahoo, Prasan Kumar},
TITLE = {Fog Computing Enabled Locality Based Product Demand Prediction and Decision Making Using Reinforcement Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {227},
URL = {https://www.mdpi.com/2079-9292/10/3/227},
ISSN = {2079-9292},
ABSTRACT = {Wastage of perishable and non-perishable products due to manual monitoring in shopping malls creates huge revenue loss in supermarket industry. Besides, internal and external factors such as calendar events and weather condition contribute to excess wastage of products in different regions of supermarket. It is a challenging job to know about the wastage of the products manually in different supermarkets region-wise. Therefore, the supermarket management needs to take appropriate decision and action to prevent the wastage of products. The fog computing data centers located in each region can collect, process and analyze data for demand prediction and decision making. In this paper, a product-demand prediction model is designed using integrated Principal Component Analysis (PCA) and K-means Unsupervised Learning (UL) algorithms and a decision making model is developed using State-Action-Reward-State-Action (SARSA) Reinforcement Learning (RL) algorithm. Our proposed method can cluster the products into low, medium, and high-demand product by learning from the designed features. Taking the derived cluster model, decision making for distributing low-demand to high-demand product can be made using SARSA. Experimental results show that our proposed method can cluster the datasets well with a Silhouette score of &ge;60%. Besides, our adopted SARSA-based decision making model outperforms over Q-Learning, Monte-Carlo, Deep Q-Network (DQN), and Actor-Critic algorithms in terms of maximum cumulative reward, average cumulative reward and execution time.},
DOI = {10.3390/electronics10030227}
}



@Article{su13031210,
AUTHOR = {Limsoonthrakul, Somphop and Dailey, Matthew N. and Marikhu, Ramesh and Timtong, Vasan and Chairat, Aphinya and Suphavilai, Anant and Seetamanotch, Wiwat and Ekpanyapong, Mongkol},
TITLE = {Design and Implementation of a Highly Scalable, Low-Cost Distributed Traffic Violation Enforcement System in Phuket, Thailand},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1210},
URL = {https://www.mdpi.com/2071-1050/13/3/1210},
ISSN = {2071-1050},
ABSTRACT = {The number of global road traffic accidents is rising every year and remains undesirably high. One of the main reasons for this trend is that, in many countries, road users violate road safety regulations and traffic laws. Despite improvements in road safety legislation, enforcement is still a major challenge in low- and middle-income countries. Information technology solutions have emerged for automated traffic enforcement systems in the last decade. They have been tested on a small scale, but until now, the cost of deployment of these systems is generally too high for nation-wide adoption in low- and middle-income countries that need them the most. We present the architectural design of a traffic violation enforcement system that can optimize the cost of deployment and resource utilization. Based on the proposed architecture, we describe the implementation and deployment of the system, and perform a comparison of two different versions of the video-based enforcement system, one using classical computer vision methods and another using deep learning techniques. Finally, we analyze the impact of the system deployed in Phuket, Thailand from 2017 to the present in terms of local road users&rsquo; compliance and the road safety situation. We conclude that the system has had a positive impact on road safety in Phuket at a moderate cost.},
DOI = {10.3390/su13031210}
}



@Article{app11031109,
AUTHOR = {Xiong, Gang and Li, Zhishuai and Wu, Huaiyu and Chen, Shichao and Dong, Xisong and Zhu, Fenghua and Lv, Yisheng},
TITLE = {Building Urban Public Traffic Dynamic Network Based on CPSS: An Integrated Approach of Big Data and AI},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1109},
URL = {https://www.mdpi.com/2076-3417/11/3/1109},
ISSN = {2076-3417},
ABSTRACT = {The extensive proliferation of urban transit cards and smartphones has witnessed the feasibility of the collection of citywide travel behaviors and the estimation of traffic status in real-time. In this paper, an urban public traffic dynamic network based on the cyber-physical-social system (CPSS-UPTDN) is proposed as a universal framework for advanced public transportation systems, which can optimize the urban public transportation based on big data and AI methods. Firstly, we introduce three modules and two loops which composes of the novel framework. Then, the key technologies in CPSS-UPTDN are studied, especially collecting and analyzing traffic information by big data and AI methods, and a particular implementation of CPSS-UPTDN is discussed, namely the artificial system, computational experiments, and parallel execution (ACP) method. Finally, a case study is performed. The data sources include both traffic congestion data from physical space and cellular data from social space, which can improve the prediction performance for traffic status. Furthermore, the service quality of urban public transportation can be promoted by optimizing the bus dispatching based on the parallel execution in our framework.},
DOI = {10.3390/app11031109}
}



@Article{electronics10030323,
AUTHOR = {Abdelaal, Marwa A. and Ebrahim, Gamal A. and Anis, Wagdy R.},
TITLE = {Efficient Placement of Service Function Chains in Cloud Computing Environments},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2079-9292/10/3/323},
ISSN = {2079-9292},
ABSTRACT = {The widespread adoption of network function virtualization (NFV) leads to providing network services through a chain of virtual network functions (VNFs). This architecture is called service function chain (SFC), which can be hosted on top of commodity servers and switches located at the cloud. Meanwhile, software-defined networking (SDN) can be utilized to manage VNFs to handle traffic flows through SFC. One of the most critical issues that needs to be addressed in NFV is VNF placement that optimizes physical link bandwidth consumption. Moreover, deploying SFCs enables service providers to consider different goals, such as minimizing the overall cost and service response time. In this paper, a novel approach for the VNF placement problem for SFCs, called virtual network functions and their replica placement (VNFRP), is introduced. It tries to achieve load balancing over the core links while considering multiple resource constraints. Hence, the VNF placement problem is first formulated as an integer linear programming (ILP) optimization problem, aiming to minimize link bandwidth consumption, energy consumption, and SFC placement cost. Then, a heuristic algorithm is proposed to find a near-optimal solution for this optimization problem. Simulation studies are conducted to evaluate the performance of the proposed approach. The simulation results show that VNFRP can significantly improve load balancing by 80% when the number of replicas is increased. Additionally, VNFRP provides more than a 54% reduction in network energy consumption. Furthermore, it can efficiently reduce the SFC placement cost by more than 67%. Moreover, with the advantages of a fast response time and rapid convergence, VNFRP can be considered as a scalable solution for large networking environments.},
DOI = {10.3390/electronics10030323}
}



@Article{smartcities4010014,
AUTHOR = {Gomez-Rosero, Santiago and Capretz, Miriam A. M. and Mir, Syed},
TITLE = {Transfer Learning by Similarity Centred Architecture Evolution for Multiple Residential Load Forecasting},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {1},
PAGES = {217--240},
URL = {https://www.mdpi.com/2624-6511/4/1/14},
ISSN = {2624-6511},
ABSTRACT = {The development from traditional low voltage grids to smart systems has become extensive and adopted worldwide. Expanding the demand response program to cover the residential sector raises a wide range of challenges. Short term load forecasting for residential consumers in a neighbourhood could lead to a better understanding of low voltage consumption behaviour. Nevertheless, users with similar characteristics can present diversity in consumption patterns. Consequently, transfer learning methods have become a useful tool to tackle differences among residential time series. This paper proposes a method combining evolutionary algorithms for neural architecture search with transfer learning to perform short term load forecasting in a neighbourhood with multiple household load consumption. The approach centres its efforts on neural architecture search using evolutionary algorithms. The neural architecture evolution process retains the patterns of the centre-most house, and later the architecture weights are adjusted for each house in a multihouse set from a neighbourhood. In addition, a sensitivity analysis was conducted to ensure model performance. Experimental results on a large dataset containing hourly load consumption for ten houses in London, Ontario showed that the performance of the proposed approach performs better than the compared techniques. Moreover, the proposed method presents the average accuracy performance of 3.17 points higher than the state-of-the-art LSTM one shot method.},
DOI = {10.3390/smartcities4010014}
}



@Article{su13041821,
AUTHOR = {Islam, Nahina and Rashid, Md Mamunur and Pasandideh, Faezeh and Ray, Biplob and Moore, Steven and Kadel, Rajan},
TITLE = {A Review of Applications and Communication Technologies for Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) Based Sustainable Smart Farming},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1821},
URL = {https://www.mdpi.com/2071-1050/13/4/1821},
ISSN = {2071-1050},
ABSTRACT = {To reach the goal of sustainable agriculture, smart farming is taking advantage of the Unmanned Aerial Vehicles (UAVs) and Internet of Things (IoT) paradigm. These smart farms are designed to be run by interconnected devices and vehicles. Some enormous potentials can be achieved by the integration of different IoT technologies to achieve automated operations with minimum supervision. This paper outlines some major applications of IoT and UAV in smart farming, explores the communication technologies, network functionalities and connectivity requirements for Smart farming. The connectivity limitations of smart agriculture and it’s solutions are analysed with two case studies. In case study-1, we propose and evaluate meshed Long Range Wide Area Network (LoRaWAN) gateways to address connectivity limitations of Smart Farming. While in case study-2, we explore satellite communication systems to provide connectivity to smart farms in remote areas of Australia. Finally, we conclude the paper by identifying future research challenges on this topic and outlining directions to address those challenges.},
DOI = {10.3390/su13041821}
}



@Article{technologies9010014,
AUTHOR = {Gadze, James Dzisi and Bamfo-Asante, Akua Acheampomaa and Agyemang, Justice Owusu and Nunoo-Mensah, Henry and Opare, Kwasi Adu-Boahen},
TITLE = {An Investigation into the Application of Deep Learning in the Detection and Mitigation of DDOS Attack on SDN Controllers},
JOURNAL = {Technologies},
VOLUME = {9},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {14},
URL = {https://www.mdpi.com/2227-7080/9/1/14},
ISSN = {2227-7080},
ABSTRACT = {Software-Defined Networking (SDN) is a new paradigm that revolutionizes the idea of a software-driven network through the separation of control and data planes. It addresses the problems of traditional network architecture. Nevertheless, this brilliant architecture is exposed to several security threats, e.g., the distributed denial of service (DDoS) attack, which is hard to contain in such software-based networks. The concept of a centralized controller in SDN makes it a single point of attack as well as a single point of failure. In this paper, deep learning-based models, long-short term memory (LSTM) and convolutional neural network (CNN), are investigated. It illustrates their possibility and efficiency in being used in detecting and mitigating DDoS attack. The paper focuses on TCP, UDP, and ICMP flood attacks that target the controller. The performance of the models was evaluated based on the accuracy, recall, and true negative rate. We compared the performance of the deep learning models with classical machine learning models. We further provide details on the time taken to detect and mitigate the attack. Our results show that RNN LSTM is a viable deep learning algorithm that can be applied in the detection and mitigation of DDoS in the SDN controller. Our proposed model produced an accuracy of 89.63%, which outperformed linear-based models such as SVM (86.85%) and Naive Bayes (82.61%). Although KNN, which is a linear-based model, outperformed our proposed model (achieving an accuracy of 99.4%), our proposed model provides a good trade-off between precision and recall, which makes it suitable for DDoS classification. In addition, it was realized that the split ratio of the training and testing datasets can give different results in the performance of a deep learning algorithm used in a specific work. The model achieved the best performance when a split of 70/30 was used in comparison to 80/20 and 60/40 split ratios.},
DOI = {10.3390/technologies9010014}
}



@Article{math9040387,
AUTHOR = {Li, Shuyu and Sung, Yunsick},
TITLE = {INCO-GAN: Variable-Length Music Generation Method Based on Inception Model-Based Conditional GAN},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {387},
URL = {https://www.mdpi.com/2227-7390/9/4/387},
ISSN = {2227-7390},
ABSTRACT = {Deep learning has made significant progress in the field of automatic music generation. At present, the research on music generation via deep learning can be divided into two categories: predictive models and generative models. However, both categories have the same problems that need to be resolved. First, the length of the music must be determined artificially prior to generation. Second, although the convolutional neural network (CNN) is unexpectedly superior to the recurrent neural network (RNN), CNN still has several disadvantages. This paper proposes a conditional generative adversarial network approach using an inception model (INCO-GAN), which enables the generation of complete variable-length music automatically. By adding a time distribution layer that considers sequential data, CNN considers the time relationship in a manner similar to RNN. In addition, the inception model obtains richer features, which improves the quality of the generated music. In experiments conducted, the music generated by the proposed method and that by human composers were compared. High cosine similarity of up to 0.987 was achieved between the frequency vectors, indicating that the music generated by the proposed method is very similar to that created by a human composer.},
DOI = {10.3390/math9040387}
}



@Article{atmos12020261,
AUTHOR = {Jeong, Chang Hoo and Kim, Wonsu and Joo, Wonkyun and Jang, Dongmin and Yi, Mun Yong},
TITLE = {Enhancing the Encoding-Forecasting Model for Precipitation Nowcasting by Putting High Emphasis on the Latest Data of the Time Step},
JOURNAL = {Atmosphere},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {261},
URL = {https://www.mdpi.com/2073-4433/12/2/261},
ISSN = {2073-4433},
ABSTRACT = {Nowcasting is an important technique for weather forecasting because sudden weather changes significantly affect human life. The encoding-forecasting model, which is a state-of-the-art architecture in the field of data-driven radar extrapolation, does not particularly focus on the latest data when forecasting natural phenomena. This paper proposes a weighted broadcasting method that emphasizes the latest data of the time step to improve the nowcasting performance. This weighted broadcasting method allows the most recent rainfall patterns to have a greater impact on the forecasting network by extending the architecture of the existing encoding-forecasting model. Experimental results show that the proposed model is 1.74% and 2.20% better than the existing encoding-forecasting model in terms of mean absolute error and critical success index, respectively. In the case of heavy rainfall with an intensity of 30 mm/h or higher, the proposed model was more than 30% superior to the existing encoding-forecasting model. Therefore, applying the weighted broadcasting method, which explicitly places a high emphasis on the latest information, to the encoding-forecasting model is considered as an improvement that is applicable to the state-of-the-art implementation of data-driven radar-based precipitation nowcasting.},
DOI = {10.3390/atmos12020261}
}



@Article{en14041045,
AUTHOR = {Bhatt, Dhowmya and D, Danalakshmi and Hariharasudan, A. and Lis, Marcin and Grabowska, Marlena},
TITLE = {Forecasting of Energy Demands for Smart Home Applications},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1045},
URL = {https://www.mdpi.com/1996-1073/14/4/1045},
ISSN = {1996-1073},
ABSTRACT = {The utilization of energy is on the rise in current trends due to increasing consumptions by households. Smart buildings, on the other hand, aim to optimize energy, and hence, the aim of the study is to forecast the cost of energy consumption in smart buildings by effectively addressing the minimal energy consumption. However, smart buildings are restricted, with limited power access and capacity associated with Heating, Ventilation and Air Conditioning (HVAC) units. It further suffers from low communication capability due to device limitations. In this paper, a balanced deep learning architecture is used to offer solutions to address these constraints. The deep learning algorithm considers three constraints, such as a multi-objective optimization problem and a fitness function, to resolve the price management problem and high-level energy consumption in HVAC systems. The study analyzes and optimizes the consumption of power in smart buildings by the HVAC systems in terms of power loss, price management and reactive power. Experiments are conducted over various scenarios to check the integrity of the system over various smart buildings and in high-rise buildings. The results are compared in terms of various HVAC devices on various metrics and communication protocols, where the proposed system is considered more effective than other methods. The results of the Li-Fi communication protocols show improved results compared to the other communication protocols.},
DOI = {10.3390/en14041045}
}



@Article{app11041900,
AUTHOR = {Aljohani, Sarah L. and Alenazi, Mohammed J. F.},
TITLE = {MPResiSDN: Multipath Resilient Routing Scheme for SDN-Enabled Smart Cities Networks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1900},
URL = {https://www.mdpi.com/2076-3417/11/4/1900},
ISSN = {2076-3417},
ABSTRACT = {The number of smart cities is increasing rapidly around the world with the continuous increase of governments’ interest in exploiting Information and Communication Technologies (ICT) to solve issues arising from rapid urbanization. Most smart city services rely fundamentally on ubiquitous sensing, enabled by Wireless Sensor Network (WSN) technologies. However, WSNs in smart cities are naturally vulnerable to unavoidable external challenges like storms, fires, and other natural disasters. Such challenges pose a great threat to smart city infrastructure, including WSNs, as they might affect network connectivity or result in complete blockages of network services. However, some particular smart city services are critical, to the point where they must remain available in all situations, especially during disasters; to monitor the disaster and obtain sensory information needed for controlling it, limiting its danger, or for decision-making during rescue operations. Thus, it is crucial to design a smart-city network to maintain connectivity against such challenges. In this paper, we introduce MPResiSDN, a MultiPath Resilient routing system based on Software Defined Networking (SDN). The system introduced exploits SDN’s capabilities and aided-multipath routing to reactively provide connectivity in smart city networks in the presence of challenges. We evaluated our proposed system under simulations of different natural disasters. The results demonstrate that the system improved data delivery under the challenges by as much as 100% compared to the Spanning Tree Protocol when a suitable value for k diverse paths was selected.},
DOI = {10.3390/app11041900}
}



@Article{s21051583,
AUTHOR = {Goyal, Shanky and Bhushan, Shashi and Kumar, Yogesh and Rana, Abu ul Hassan S. and Bhutta, Muhammad Raheel and Ijaz, Muhammad Fazal and Son, Youngdoo},
TITLE = {An Optimized Framework for Energy-Resource Allocation in a Cloud Environment based on the Whale Optimization Algorithm},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1583},
URL = {https://www.mdpi.com/1424-8220/21/5/1583},
PubMedID = {33668282},
ISSN = {1424-8220},
ABSTRACT = {Cloud computing offers the services to access, manipulate and configure data online over the web. The cloud term refers to an internet network which is remotely available and accessible at anytime from anywhere. Cloud computing is undoubtedly an innovation as the investment in the real and physical infrastructure is much greater than the cloud technology investment. The present work addresses the issue of power consumption done by cloud infrastructure. As there is a need for algorithms and techniques that can reduce energy consumption and schedule resource for the effectiveness of servers. Load balancing is also a significant part of cloud technology that enables the balanced distribution of load among multiple servers to fulfill users’ growing demand. The present work used various optimization algorithms such as particle swarm optimization (PSO), cat swarm optimization (CSO), BAT, cuckoo search algorithm (CSA) optimization algorithm and the whale optimization algorithm (WOA) for balancing the load, energy efficiency, and better resource scheduling to make an efficient cloud environment. In the case of seven servers and eight server’s settings, the results revealed that whale optimization algorithm outperformed other algorithms in terms of response time, energy consumption, execution time and throughput.},
DOI = {10.3390/s21051583}
}



@Article{math9050500,
AUTHOR = {Lydia, E. Laxmi and Jovith, A. Arokiaraj and Devaraj, A. Francis Saviour and Seo, Changho and Joshi, Gyanendra Prasad},
TITLE = {Green Energy Efficient Routing with Deep Learning Based Anomaly Detection for Internet of Things (IoT) Communications},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {500},
URL = {https://www.mdpi.com/2227-7390/9/5/500},
ISSN = {2227-7390},
ABSTRACT = {Presently, a green Internet of Things (IoT) based energy aware network plays a significant part in the sensing technology. The development of IoT has a major impact on several application areas such as healthcare, smart city, transportation, etc. The exponential rise in the sensor nodes might result in enhanced energy dissipation. So, the minimization of environmental impact in green media networks is a challenging issue for both researchers and business people. Energy efficiency and security remain crucial in the design of IoT applications. This paper presents a new green energy-efficient routing with DL based anomaly detection (GEER-DLAD) technique for IoT applications. The presented model enables IoT devices to utilize energy effectively in such a way as to increase the network span. The GEER-DLAD technique performs error lossy compression (ELC) technique to lessen the quantity of data communication over the network. In addition, the moth flame swarm optimization (MSO) algorithm is applied for the optimal selection of routes in the network. Besides, DLAD process takes place via the recurrent neural network-long short term memory (RNN-LSTM) model to detect anomalies in the IoT communication networks. A detailed experimental validation process is carried out and the results ensured the betterment of the GEER-DLAD model in terms of energy efficiency and detection performance.},
DOI = {10.3390/math9050500}
}



@Article{asi4010017,
AUTHOR = {Singh, Jaideep and Khushi, Matloob},
TITLE = {Feature Learning for Stock Price Prediction Shows a Significant Role of Analyst Rating},
JOURNAL = {Applied System Innovation},
VOLUME = {4},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {17},
URL = {https://www.mdpi.com/2571-5577/4/1/17},
ISSN = {2571-5577},
ABSTRACT = {Efficient Market Hypothesis states that stock prices are a reflection of all the information present in the world and generating excess returns is not possible by merely analysing trade data which is already available to all public. Yet to further the research rejecting this idea, a rigorous literature review was conducted and a set of five technical indicators and 23 fundamental indicators was identified to establish the possibility of generating excess returns on the stock market. Leveraging these data points and various classification machine learning models, trading data of the 505 equities on the US S&amp;P500 over the past 20 years was analysed to develop a classifier effective for our cause. From any given day, we were able to predict the direction of change in price by 1% up to 10 days in the future. The predictions had an overall accuracy of 83.62% with a precision of 85% for buy signals and a recall of 100% for sell signals. Moreover, we grouped equities by their sector and repeated the experiment to see if grouping similar assets together positively effected the results but concluded that it showed no significant improvements in the performance—rejecting the idea of sector-based analysis. Also, using feature ranking we could identify an even smaller set of 6 indicators while maintaining similar accuracies as that from the original 28 features and also uncovered the importance of buy, hold and sell analyst ratings as they came out to be the top contributors in the model. Finally, to evaluate the effectiveness of the classifier in real-life situations, it was backtested on FAANG (Facebook, Amazon, Apple, Netflix &amp; Google) equities using a modest trading strategy where it generated high returns of above 60% over the term of the testing dataset. In conclusion, our proposed methodology with the combination of purposefully picked features shows an improvement over the previous studies, and our model predicts the direction of 1% price changes on the 10th day with high confidence and with enough buffer to even build a robotic trading system.},
DOI = {10.3390/asi4010017}
}



@Article{electronics10060668,
AUTHOR = {Rastenis, Justinas and Ramanauskaitė, Simona and Suzdalev, Ivan and Tunaitytė, Kornelija and Janulevičius, Justinas and Čenys, Antanas},
TITLE = {Multi-Language Spam/Phishing Classification by Email Body Text: Toward Automated Security Incident Investigation},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {668},
URL = {https://www.mdpi.com/2079-9292/10/6/668},
ISSN = {2079-9292},
ABSTRACT = {Spamming and phishing are two types of emailing that are annoying and unwanted, differing by the potential threat and impact to the user. Automated classification of these categories can increase the users’ awareness as well as to be used for incident investigation prioritization or automated fact gathering. However, currently there are no scientific papers focusing on email classification concerning these two categories of spam and phishing emails. Therefore this paper presents a solution, based on email message body text automated classification into spam and phishing emails. We apply the proposed solution for email classification, written in three languages: English, Russian, and Lithuanian. As most public email datasets almost exclusively collect English emails, we investigate the suitability of automated dataset translation to adapt it to email classification, written in other languages. Experiments on public dataset usage limitations for a specific organization are executed in this paper to evaluate the need of dataset updates for more accurate classification results.},
DOI = {10.3390/electronics10060668}
}



@Article{logistics5010017,
AUTHOR = {Muñoz-Villamizar, Andrés and Solano-Charris, Elyn L. and Reyes-Rubiano, Lorena and Faulin, Javier},
TITLE = {Measuring Disruptions in Last-Mile Delivery Operations},
JOURNAL = {Logistics},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {17},
URL = {https://www.mdpi.com/2305-6290/5/1/17},
ISSN = {2305-6290},
ABSTRACT = {The rapid growth of urbanisation and e-commerce has increased the number of home deliveries that need to be made in retail operations. Consequently, there is also an increase in unexpected incidents, such as adverse traffic, unavailability of parking space, and vehicle breakdowns. These disruptions result in delays, higher costs, and lower service levels in the last-mile delivery operation. Motivated by free, innovative, and efficient tools, such as the Google application programming interface (API) and Google OR, we built a model to measure the impact of disruptions in the last-mile delivery operation. Our model considers customers’ geographic information, speed estimation between nodes, routing optimisation, and disruption evaluation. Disruptions are considered here as external factors such as accidents and road works that imply the closure of or slow access to certain roads. Computational experiments, based on a set of real data from three different cities around the world, which contrast in size and characteristics (i.e., Boston, US; Bogotá, Colombia; and Pamplona, Spain), were conducted to validate our approach. The tests consider 50 different instances of up to 100 customers per city and analyse the impact of disruptions in terms of travelled time and distance. Our results provide managerial insights for key stakeholders (i.e., carriers, consumers, and government) to define policies and development plans that improve the resilience and capabilities of cities’ transportation systems.},
DOI = {10.3390/logistics5010017}
}



@Article{s21062143,
AUTHOR = {Paiva, Sara and Ahad, Mohd Abdul and Tripathi, Gautami and Feroz, Noushaba and Casalino, Gabriella},
TITLE = {Enabling Technologies for Urban Smart Mobility: Recent Trends, Opportunities and Challenges},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2143},
URL = {https://www.mdpi.com/1424-8220/21/6/2143},
PubMedID = {33803903},
ISSN = {1424-8220},
ABSTRACT = {The increasing population across the globe makes it essential to link smart and sustainable city planning with the logistics of transporting people and goods, which will significantly contribute to how societies will face mobility in the coming years. The concept of smart mobility emerged with the popularity of smart cities and is aligned with the sustainable development goals defined by the United Nations. A reduction in traffic congestion and new route optimizations with reduced ecological footprint are some of the essential factors of smart mobility; however, other aspects must also be taken into account, such as the promotion of active mobility and inclusive mobility, encouraging the use of other types of environmentally friendly fuels and engagement with citizens. The Internet of Things (IoT), Artificial Intelligence (AI), Blockchain and Big Data technology will serve as the main entry points and fundamental pillars to promote the rise of new innovative solutions that will change the current paradigm for cities and their citizens. Mobility-as-a-service, traffic flow optimization, the optimization of logistics and autonomous vehicles are some of the services and applications that will encompass several changes in the coming years with the transition of existing cities into smart cities. This paper provides an extensive review of the current trends and solutions presented in the scope of smart mobility and enabling technologies that support it. An overview of how smart mobility fits into smart cities is provided by characterizing its main attributes and the key benefits of using smart mobility in a smart city ecosystem. Further, this paper highlights other various opportunities and challenges related to smart mobility. Lastly, the major services and applications that are expected to arise in the coming years within smart mobility are explored with the prospective future trends and scope.},
DOI = {10.3390/s21062143}
}



@Article{s21062152,
AUTHOR = {Yaïci, Wahiba and Krishnamurthy, Karthik and Entchev, Evgueniy and Longo, Michela},
TITLE = {Recent Advances in Internet of Things (IoT) Infrastructures for Building Energy Systems: A Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2152},
URL = {https://www.mdpi.com/1424-8220/21/6/2152},
PubMedID = {33808558},
ISSN = {1424-8220},
ABSTRACT = {This paper summarises a literature review on the applications of Internet of Things (IoT) with the aim of enhancing building energy use and reducing greenhouse gas emissions (GHGs). A detailed assessment of contemporary practical reviews and works was conducted to understand how different IoT systems and technologies are being developed to increase energy efficiencies in both residential and commercial buildings. Most of the reviewed works were invariably related to the dilemma of efficient heating systems in buildings. Several features of the central components of IoT, namely, the hardware and software needed for building controls, are analysed. Common design factors across the many IoT systems comprise the selection of sensors and actuators and their powering techniques, control strategies for collecting information and activating appliances, monitoring of actual data to forecast prospect energy consumption and communication methods amongst IoT components. Some building energy applications using IoT are provided. It was found that each application presented has the potential for significant energy reduction and user comfort improvement. This is confirmed in two case studies summarised, which report the energy savings resulting from implementing IoT systems. Results revealed that a few elements are user-specific that need to be considered in the decision processes. Last, based on the studies reviewed, a few aspects of prospective research were recommended.},
DOI = {10.3390/s21062152}
}



@Article{s21062233,
AUTHOR = {Li, Ke and Zhang, Kun and Zhang, Zhenchong and Liu, Zekun and Hua, Shuai and He, Jianliang},
TITLE = {A UAV Maneuver Decision-Making Algorithm for Autonomous Airdrop Based on Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2233},
URL = {https://www.mdpi.com/1424-8220/21/6/2233},
PubMedID = {33806886},
ISSN = {1424-8220},
ABSTRACT = {How to operate an unmanned aerial vehicle (UAV) safely and efficiently in an interactive environment is challenging. A large amount of research has been devoted to improve the intelligence of a UAV while performing a mission, where finding an optimal maneuver decision-making policy of the UAV has become one of the key issues when we attempt to enable the UAV autonomy. In this paper, we propose a maneuver decision-making algorithm based on deep reinforcement learning, which generates efficient maneuvers for a UAV agent to execute the airdrop mission autonomously in an interactive environment. Particularly, the training set of the learning algorithm by the Prioritized Experience Replay is constructed, that can accelerate the convergence speed of decision network training in the algorithm. It is shown that a desirable and effective maneuver decision-making policy can be found by extensive experimental results.},
DOI = {10.3390/s21062233}
}



@Article{electronics10070824,
AUTHOR = {Wang, Peng and Deng, Zhenkai and Cui, Ruilong},
TITLE = {TDJEE: A Document-Level Joint Model for Financial Event Extraction},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {824},
URL = {https://www.mdpi.com/2079-9292/10/7/824},
ISSN = {2079-9292},
ABSTRACT = {Extracting financial events from numerous financial announcements is very important for investors to make right decisions. However, it is still challenging that event arguments always scatter in multiple sentences in a financial announcement, while most existing event extraction models only work in sentence-level scenarios. To address this problem, this paper proposes a relation-aware Transformer-based Document-level Joint Event Extraction model (TDJEE), which encodes relations between words into the context and leverages modified Transformer to capture document-level information to fill event arguments. Meanwhile, the absence of labeled data in financial domain could lead models be unstable in extraction results, which is known as the cold start problem. Furthermore, a Fonduer-based knowledge base combined with the distant supervision method is proposed to simplify the event labeling and provide high quality labeled training corpus for model training and evaluating. Experimental results on real-world Chinese financial announcement show that, compared with other models, TDJEE achieves competitive results and can effectively extract event arguments across multiple sentences.},
DOI = {10.3390/electronics10070824}
}



@Article{su13073977,
AUTHOR = {Tichý, Tomáš and Brož, Jiří and Bělinová, Zuzana and Pirník, Rastislav},
TITLE = {Analysis of Predictive Maintenance for Tunnel Systems},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3977},
URL = {https://www.mdpi.com/2071-1050/13/7/3977},
ISSN = {2071-1050},
ABSTRACT = {Smart and automated maintenance could make the system and its parts more sustainable by extending their lifecycle, failure detection, smart control of the equipment, and precise detection and reaction to unexpected circumstances. This article focuses on the analysis of data, particularly on logs captured in several Czech tunnel systems. The objective of the analysis is to find useful information in the logs for predicting upcoming situations, and furthermore, to check the possibilities of predictive diagnostics and to design the process of predictive maintenance. The main goal of the article is to summarize the possibilities of optimizing system maintenance that are based on data analysis as well as expert analysis based on the experience with the equipment in the tunnel. The results, findings, and conclusions could primarily be used in the tunnels; secondarily, these principles could be applied in telematics and lead to the optimization and improvement of system sustainability.},
DOI = {10.3390/su13073977}
}



@Article{electronics10080880,
AUTHOR = {Imran and Ghaffar, Zeba and Alshahrani, Abdullah and Fayaz, Muhammad and Alghamdi, Ahmed Mohammed and Gwak, Jeonghwan},
TITLE = {A Topical Review on Machine Learning, Software Defined Networking, Internet of Things Applications: Research Limitations and Challenges},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {880},
URL = {https://www.mdpi.com/2079-9292/10/8/880},
ISSN = {2079-9292},
ABSTRACT = {In recent years, rapid development has been made to the Internet of Things communication technologies, infrastructure, and physical resources management. These developments and research trends address challenges such as heterogeneous communication, quality of service requirements, unpredictable network conditions, and a massive influx of data. One major contribution to the research world is in the form of software-defined networking applications, which aim to deploy rule-based management to control and add intelligence to the network using high-level policies to have integral control of the network without knowing issues related to low-level configurations. Machine learning techniques coupled with software-defined networking can make the networking decision more intelligent and robust. The Internet of Things application has recently adopted virtualization of resources and network control with software-defined networking policies to make the traffic more controlled and maintainable. However, the requirements of software-defined networking and the Internet of Things must be aligned to make the adaptations possible. This paper aims to discuss the possible ways to make software-defined networking enabled Internet of Things application and discusses the challenges solved using the Internet of Things leveraging the software-defined network. We provide a topical survey of the application and impact of software-defined networking on the Internet of things networks. We also study the impact of machine learning techniques applied to software-defined networking and its application perspective. The study is carried out from the different perspectives of software-based Internet of Things networks, including wide-area networks, edge networks, and access networks. Machine learning techniques are presented from the perspective of network resources management, security, classification of traffic, quality of experience, and quality of service prediction. Finally, we discuss challenges and issues in adopting machine learning and software-defined networking for the Internet of Things applications.},
DOI = {10.3390/electronics10080880}
}



@Article{en14082120,
AUTHOR = {Ji, Ying and Wang, Jianhui and Xu, Jiacan and Li, Donglin},
TITLE = {Data-Driven Online Energy Scheduling of a Microgrid Based on Deep Reinforcement Learning},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2120},
URL = {https://www.mdpi.com/1996-1073/14/8/2120},
ISSN = {1996-1073},
ABSTRACT = {The proliferation of distributed renewable energy resources (RESs) poses major challenges to the operation of microgrids due to uncertainty. Traditional online scheduling approaches relying on accurate forecasts become difficult to implement due to the increase of uncertain RESs. Although several data-driven methods have been proposed recently to overcome the challenge, they generally suffer from a scalability issue due to the limited ability to optimize high-dimensional continuous control variables. To address these issues, we propose a data-driven online scheduling method for microgrid energy optimization based on continuous-control deep reinforcement learning (DRL). We formulate the online scheduling problem as a Markov decision process (MDP). The objective is to minimize the operating cost of the microgrid considering the uncertainty of RESs generation, load demand, and electricity prices. To learn the optimal scheduling strategy, a Gated Recurrent Unit (GRU)-based network is designed to extract temporal features of uncertainty and generate the optimal scheduling decisions in an end-to-end manner. To optimize the policy with high-dimensional and continuous actions, proximal policy optimization (PPO) is employed to train the neural network-based policy in a data-driven fashion. The proposed method does not require any forecasting information on the uncertainty or a prior knowledge of the physical model of the microgrid. Simulation results using realistic power system data of California Independent System Operator (CAISO) demonstrate the effectiveness of the proposed method.},
DOI = {10.3390/en14082120}
}



@Article{en14082233,
AUTHOR = {Amara-Ouali, Yvenn and Goude, Yannig and Massart, Pascal and Poggi, Jean-Michel and Yan, Hui},
TITLE = {A Review of Electric Vehicle Load Open Data and Models},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2233},
URL = {https://www.mdpi.com/1996-1073/14/8/2233},
ISSN = {1996-1073},
ABSTRACT = {The field of electric vehicle charging load modelling has been growing rapidly in the last decade. In light of the Paris Agreement, it is crucial to keep encouraging better modelling techniques for successful electric vehicle adoption. Additionally, numerous papers highlight the lack of charging station data available in order to build models that are consistent with reality. In this context, the purpose of this article is threefold. First, to provide the reader with an overview of the open datasets available and ready to be used in order to foster reproducible research in the field. Second, to review electric vehicle charging load models with their strengths and weaknesses. Third, to provide suggestions on matching the models reviewed to six datasets found in this research that have not previously been explored in the literature. The open data search covered more than 860 repositories and yielded around 60 datasets that are relevant for modelling electric vehicle charging load. These datasets include information on charging point locations, historical and real-time charging sessions, traffic counts, travel surveys and registered vehicles. The models reviewed range from statistical characterization to stochastic processes and machine learning and the context of their application is assessed.},
DOI = {10.3390/en14082233}
}



@Article{rs13081547,
AUTHOR = {He, Yixin and Zhai, Daosen and Huang, Fanghui and Wang, Dawei and Tang, Xiao and Zhang, Ruonan},
TITLE = {Joint Task Offloading, Resource Allocation, and Security Assurance for Mobile Edge Computing-Enabled UAV-Assisted VANETs},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1547},
URL = {https://www.mdpi.com/2072-4292/13/8/1547},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we propose a mobile edge computing (MEC)-enabled unmanned aerial vehicle (UAV)-assisted vehicular ad hoc network (VANET) architecture, based on which a number of vehicles are served by UAVs equipped with computation resource. Each vehicle has to offload its computing tasks to the proper MEC server on the UAV due to the limited computation ability. To counter the problems above, we first model and analyze the transmission model and the security assurance model from the vehicle to the MEC server on UAV, and the task computation model of the local vehicle and the edge UAV. Then, the vehicle offloading problem is formulated as a multi-objective optimization problem by jointly considering the task offloading, the resource allocation, and the security assurance. For tackling this hard problem, we decouple the multi-objective optimization problem as two subproblems and propose an efficient iterative algorithm to jointly make the MEC selection decision based on the criteria of load balancing and optimize the offloading ratio and the computation resource according to the Lagrangian dual decomposition. Finally, the simulation results demonstrate that our proposed scheme achieves significant performance superiority compared with other schemes in terms of the successful task processing ratio and the task processing delay.},
DOI = {10.3390/rs13081547}
}



@Article{electronics10091012,
AUTHOR = {Sharma, Himanshu and Haque, Ahteshamul and Blaabjerg, Frede},
TITLE = {Machine Learning in Wireless Sensor Networks for Smart Cities: A Survey},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1012},
URL = {https://www.mdpi.com/2079-9292/10/9/1012},
ISSN = {2079-9292},
ABSTRACT = {Artificial intelligence (AI) and machine learning (ML) techniques have huge potential to efficiently manage the automated operation of the internet of things (IoT) nodes deployed in smart cities. In smart cities, the major IoT applications are smart traffic monitoring, smart waste management, smart buildings and patient healthcare monitoring. The small size IoT nodes based on low power Bluetooth (IEEE 802.15.1) standard and wireless sensor networks (WSN) (IEEE 802.15.4) standard are generally used for transmission of data to a remote location using gateways. The WSN based IoT (WSN-IoT) design problems include network coverage and connectivity issues, energy consumption, bandwidth requirement, network lifetime maximization, communication protocols and state of the art infrastructure. In this paper, the authors propose machine learning methods as an optimization tool for regular WSN-IoT nodes deployed in smart city applications. As per the author’s knowledge, this is the first in-depth literature survey of all ML techniques in the field of low power consumption WSN-IoT for smart cities. The results of this unique survey article show that the supervised learning algorithms have been most widely used (61%) as compared to reinforcement learning (27%) and unsupervised learning (12%) for smart city applications.},
DOI = {10.3390/electronics10091012}
}



@Article{s21093096,
AUTHOR = {Wang, Jian and Guo, Xinyu and Yang, Xinyu},
TITLE = {Efficient and Safe Strategies for Intersection Management: A Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3096},
URL = {https://www.mdpi.com/1424-8220/21/9/3096},
PubMedID = {33946781},
ISSN = {1424-8220},
ABSTRACT = {Intersection management is a sophisticated event in the intelligent transportation system due to a variety of behavior for traffic participants. This paper primarily overviews recent studies on the scenes of intersection, aiming at improving the efficiency or guaranteeing the safety when vehicles pass the crossing. These studies are respectively surveyed from the perspectives of efficiency and safety. Firstly, recent contributions to efficiency-oriented, intersection management overviews from four scenes, including congestion avoidance, green light optimized speed advisory (GLOSA), trajectory planning, and emergency vehicle priority preemption control. Furthermore, the studies on intersection collision detection and abnormal information warning are surveyed in the safety category. The corresponding algorithms for velocity and route management presented in the surveyed works are discussed.},
DOI = {10.3390/s21093096}
}



@Article{electronics10091068,
AUTHOR = {Memon, Imran and Hasan, Mohammad Kamrul and Shaikh, Riaz Ahmed and Nebhen, Jamel and Bakar, Khairul Azmi Abu and Hossain, Eklas and Tunio, Muhammad Hanif},
TITLE = {Energy-Efficient Fuzzy Management System for Internet of Things Connected Vehicular Ad Hoc Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1068},
URL = {https://www.mdpi.com/2079-9292/10/9/1068},
ISSN = {2079-9292},
ABSTRACT = {Many algorithms use clustering to improve vehicular ad hoc network performance. The expected points of many of these approaches support multiple rounds of data to the roadside unit and constantly include clustering in every round of single-hop data transmission towards the road side unit; however, the clustering in every round maximizes the number of control messages and there could be the possibility of collision and decreases in network energy. Multi-hop transmission prolongs the cluster head node’s lifetime and boosts the network’s efficiency. Accordingly, this article proposes a new fuzzy-clustering-based routing algorithm to benefit from multi-hop transmission clustering simultaneously. This research has analyzed the limitation of clustering in each round, different algorithms were used to perform the clustering, and multi-hop routing was used to transfer the data of every cluster to the road side unit. The fuzzy logic was used to choose the head node of each cluster. Three parameters, (1) distance of each node, (2) remaining energy, and (3) number of neighbors of every node, were considered as fuzzy criteria. The results of this research were compared to various other algorithms in relation to parameters like dead node in every round, first node expire, half node expire, last node expire, and the network lifetime. The simulation results show that the proposed approach outperforms other methods. On the other hand, the vehicular ad hoc network (VANET) environment is vulnerable at the time of data transmission. The NS-2 software tool was used to simulate and evaluate the proposed fuzzy logic opportunistic routing’s performance results concerning end-to-end delay, packet delivery, and network throughput. We compare to the existing protocols, such as fuzzy Internet of Things (IoT), two fuzzy, and Fuzzy-Based Driver Monitoring System (FDMS). The performance comparison also emphasizes an effective utilization of the resources. Simulations on the highway environment show that the suggested protocol has an improved Quality of Service (QoS) efficiency compared to the above published methods in the literature.},
DOI = {10.3390/electronics10091068}
}



@Article{s21093261,
AUTHOR = {Sultan, Salman Md and Waleed, Muhammad and Pyun, Jae-Young and Um, Tai-Won},
TITLE = {Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3261},
URL = {https://www.mdpi.com/1424-8220/21/9/3261},
PubMedID = {34066766},
ISSN = {1424-8220},
ABSTRACT = {The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.},
DOI = {10.3390/s21093261}
}



@Article{smartcities4020040,
AUTHOR = {Englund, Cristofer and Aksoy, Eren Erdal and Alonso-Fernandez, Fernando and Cooney, Martin Daniel and Pashami, Sepideh and Åstrand, Björn},
TITLE = {AI Perspectives in Smart Cities and Communities to Enable Road Vehicle Automation and Smart Traffic Control},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {2},
PAGES = {783--802},
URL = {https://www.mdpi.com/2624-6511/4/2/40},
ISSN = {2624-6511},
ABSTRACT = {Smart cities and communities (SCC) constitute a new paradigm in urban development. SCC ideate a data-centered society aimed at improving efficiency by automating and optimizing activities and utilities. Information and communication technology along with Internet of Things enables data collection and with the help of artificial intelligence (AI) situation awareness can be obtained to feed the SCC actors with enriched knowledge. This paper describes AI perspectives in SCC and gives an overview of AI-based technologies used in traffic to enable road vehicle automation and smart traffic control. Perception, smart traffic control and driver modeling are described along with open research challenges and standardization to help introduce advanced driver assistance systems and automated vehicle functionality in traffic. To fully realize the potential of SCC, to create a holistic view on a city level, availability of data from different stakeholders is necessary. Further, though AI technologies provide accurate predictions and classifications, there is an ambiguity regarding the correctness of their outputs. This can make it difficult for the human operator to trust the system. Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model. Another challenge related to trust is explainability: models can have difficulty explaining how they came to certain conclusions, so it is difficult for humans to trust them.},
DOI = {10.3390/smartcities4020040}
}



@Article{electronics10101208,
AUTHOR = {Alonso, Francisco and Faus, Mireia and Esteban, Cristina and Useche, Sergio A.},
TITLE = {Is There a Predisposition towards the Use of New Technologies within the Traffic Field of Emerging Countries? The Case of the Dominican Republic},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1208},
URL = {https://www.mdpi.com/2079-9292/10/10/1208},
ISSN = {2079-9292},
ABSTRACT = {Technological devices are becoming more and more integrated in the management and control of traffic in big cities. The population perceives the benefits provided by these systems, and, therefore, citizens usually have a favorable opinion of them. However, emerging countries, which have fewer available infrastructures, could present a certain lack of trust. The objective of this work is to detect the level of knowledge and predisposition towards the use of new technologies in the transportation field of the Dominican Republic. For this study, the National Survey on Mobility was administered to a sample of Dominican citizens, proportional to the ONE census and to sex, age and province. The knowledge of ITS topics, as well as the use of mobile applications for mobility, are scarce; however, there was a significant increase that can be observed in only one year. Moreover, technology is, in general, positively assessed for what concerns the improvement of the traffic field, even though there is a lack of predisposition to provide one’s personal data, which is necessary for these devices. The process of technological development in the country must be backed up by laws that protect the citizens’ privacy. Thus, technologies that can improve road safety, mobility and sustainability can be implemented in the country.},
DOI = {10.3390/electronics10101208}
}



@Article{smartcities4020042,
AUTHOR = {Elvas, Luís B. and Mataloto, Bruno Miguel and Martins, Ana Lúcia and Ferreira, João C.},
TITLE = {Disaster Management in Smart Cities},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {2},
PAGES = {819--839},
URL = {https://www.mdpi.com/2624-6511/4/2/42},
ISSN = {2624-6511},
ABSTRACT = {The smart city concept, in which data from different systems are available, contains a multitude of critical infrastructures. This data availability opens new research opportunities in the study of the interdependency between those critical infrastructures and cascading effects solutions and focuses on the smart city as a network of critical infrastructures. This paper proposes an integrated resilience system linking interconnected critical infrastructures in a smart city to improve disaster resilience. A data-driven approach is considered, using artificial intelligence and methods to minimize cascading effects and the destruction of failing critical infrastructures and their components (at a city level). The proposed approach allows rapid recovery of infrastructures’ service performance levels after disasters while keeping the coverage of the assessment of risks, prevention, detection, response, and mitigation of consequences. The proposed approach has the originality and the practical implication of providing a decision support system that handles the infrastructures that will support the city disaster management system—make the city prepare, adapt, absorb, respond, and recover from disasters by taking advantage of the interconnections between its various critical infrastructures to increase the overall resilience capacity. The city of Lisbon (Portugal) is used as a case to show the practical application of the approach.},
DOI = {10.3390/smartcities4020042}
}



@Article{s21103559,
AUTHOR = {Massoud, Rana and Berta, Riccardo and Poslad, Stefan and De Gloria, Alessandro and Bellotti, Francesco},
TITLE = {IoT Sensing for Reality-Enhanced Serious Games, a Fuel-Efficient Drive Use Case},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3559},
URL = {https://www.mdpi.com/1424-8220/21/10/3559},
PubMedID = {34065354},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things technologies are spurring new types of instructional games, namely reality-enhanced serious games (RESGs), that support training directly in the field. This paper investigates a key feature of RESGs, i.e., user performance evaluation using real data, and studies an application of RESGs for promoting fuel-efficient driving, using fuel consumption as an indicator of driver performance. In particular, we propose a reference model for supporting a novel smart sensing dataflow involving the combination of two modules, based on machine learning, to be employed in RESGs in parallel and in real-time. The first module concerns quantitative performance assessment, while the second one targets verbal recommendation. For the assessment module, we compared the performance of three well-established machine learning algorithms: support vector regression, random forest and artificial neural networks. The experiments show that random forest achieves a slightly better performance assessment correlation than the others but requires a higher inference time. The instant recommendation module, implemented using fuzzy logic, triggers advice when inefficient driving patterns are detected. The dataflow has been tested with data from the enviroCar public dataset, exploiting on board diagnostic II (OBD II) standard vehicular interface information. The data covers various driving environments and vehicle models, which makes the system robust for real-world conditions. The results show the feasibility and effectiveness of the proposed approach, attaining a high estimation correlation (R2 = 0.99, with random forest) and punctual verbal feedback to the driver. An important word of caution concerns users’ privacy, as the modules rely on sensitive personal data, and provide information that by no means should be misused.},
DOI = {10.3390/s21103559}
}



@Article{electronics10101219,
AUTHOR = {Guru, Divya and Perumal, Supraja and Varadarajan, Vijayakumar},
TITLE = {Approaches towards Blockchain Innovation: A Survey and Future Directions},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1219},
URL = {https://www.mdpi.com/2079-9292/10/10/1219},
ISSN = {2079-9292},
ABSTRACT = {A blockchain is a decentralized peer to peer platform which provides security services based on some key concepts, namely authentication, confidentiality, integrity and authorization. It is the process of recording and keeping track of the resources without the intervention of a centralized authority. This paper provides an overview of blockchains, the structure of blockchains, consensus algorithms, etc., It compares the algorithms based on their utility and limitations. Though blockchains provide secure communication, there are some minimal data leaks which are discussed. Various security issues in blockchains are discussed such as denial of service attacks, etc., In addition to security, some other blockchain challenges are presented like scalability, reliability, interoperability, privacy and consensus mechanisms for integration with AI, IoT and edge computing. This paper also explains about the importance of blockchains in the fields of smart healthcare, smart grid, and smart financial systems. Overall, this paper gives the glimpse of various protocols, algorithms, applications, challenges and opportunities that are found in the blockchain domain.},
DOI = {10.3390/electronics10101219}
}



@Article{e23060668,
AUTHOR = {Troussas, Christos and Krouska, Akrivi and Sgouropoulou, Cleo},
TITLE = {Improving Learner-Computer Interaction through Intelligent Learning Material Delivery Using Instructional Design Modeling},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {668},
URL = {https://www.mdpi.com/1099-4300/23/6/668},
PubMedID = {34073243},
ISSN = {1099-4300},
ABSTRACT = {This paper describes an innovative and sophisticated approach for improving learner-computer interaction in the tutoring of Java programming through the delivery of adequate learning material to learners. To achieve this, an instructional theory and intelligent techniques are combined, namely the Component Display Theory along with content-based filtering and multiple-criteria decision analysis, with the intention of providing personalized learning material and thus, improving student interaction. Until now, the majority of the research efforts mainly focus on adapting the presentation of learning material based on students’ characteristics. As such, there is free space for researching issues like delivering the appropriate type of learning material, in order to maintain the pedagogical affordance of the educational software. The blending of instructional design theories and sophisticated techniques can offer a more personalized and adaptive learning experience to learners of computer programming. The paper presents a fully operating intelligent educational software. It merges pedagogical and technological approaches for sophisticated learning material delivery to students. Moreover, it was used by undergraduate university students to learn Java programming for a semester during the COVID-19 lockdown. The findings of the evaluation showed that the presented way for delivering the Java learning material surpassed other approaches incorporating merely instructional models or intelligent tools, in terms of satisfaction and knowledge acquisition.},
DOI = {10.3390/e23060668}
}



@Article{su13116187,
AUTHOR = {Al-Absi, Mohammed Abdulhakim and Al-Absi, Ahmed Abdulhakim and Sain, Mangal and Lee, Hoonjae},
TITLE = {Moving Ad Hoc Networks—A Comparative Study},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {6187},
URL = {https://www.mdpi.com/2071-1050/13/11/6187},
ISSN = {2071-1050},
ABSTRACT = {An ad hoc network is a wireless mobile communication network composed of a group of mobile nodes with wireless transceivers. It does not rely on preset infrastructure and is established temporarily. The mobile nodes of the network use their own wireless transceivers to exchange information; when the information is not within the communication range, other intermediate nodes can be used to relay to achieve communication. They can be widely used in environments that cannot be supported by wired networks or which require communication temporarily, such as military applications, sensor networks, rescue and disaster relief, and emergency response. In MANET, each node acts as a host and as a router, and the nodes are linked through wireless channels in the network. One of the scenarios of MANET is VANET; VANET is supported by several types of fixed infrastructure. Due to its limitations, this infrastructure can support some VANET services and provide fixed network access. FANET is a subset of VANET. SANET is one of the common types of ad hoc networks. This paper could serve as a guide and reference so that readers have a comprehensive and general understanding of wireless ad hoc networks and their routing protocols at a macro level with a lot of good, related papers for reference. However, this is the first paper that discusses the popular types of ad hoc networks along with comparisons and simulation tools for Ad Hoc Networks.},
DOI = {10.3390/su13116187}
}



@Article{s21113893,
AUTHOR = {Nouh, Rayan and Singh, Madhusudan and Singh, Dhananjay},
TITLE = {SafeDrive: Hybrid Recommendation System Architecture for Early Safety Predication Using Internet of Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3893},
URL = {https://www.mdpi.com/1424-8220/21/11/3893},
PubMedID = {34199981},
ISSN = {1424-8220},
ABSTRACT = {The Internet of vehicles (IoV) is a rapidly emerging technological evolution of Intelligent Transportation System (ITS). This paper proposes SafeDrive, a dynamic driver profile (DDP) using a hybrid recommendation system. DDP is a set of functional modules, to analyses individual driver’s behaviors, using prior violation and accident records, to identify driving risk patterns. In this paper, we have considered three synthetic data-sets for 1500 drivers based on their profile information, risk parameters information, and risk likelihood. In addition, we have also considered the driver’s historical violation/accident data-set records based on four risk-score levels such as high-risk, medium-risk, low-risk, and no-risk to predict current and future driver risk scores. Several error calculation methods have been applied in this study to analyze our proposed hybrid recommendation systems’ performance to classify the driver’s data with higher accuracy based on various criteria. The evaluated results help to improve the driving behavior and broadcast early warning alarm to the other vehicles in IoV environment for the overall road safety. Moreover, the propoed model helps to provide a safe and predicted environment for vehicles, pedestrians, and road objects, with the help of regular monitoring of vehicle motion, driver behavior, and road conditions. It also enables accurate prediction of accidents beforehand, and also minimizes the complexity of on-road vehicles and latency due to fog/cloud computing servers.},
DOI = {10.3390/s21113893}
}



@Article{ai2020017,
AUTHOR = {Hati, Anirban Jyoti and Singh, Rajiv Ranjan},
TITLE = {Artificial Intelligence in Smart Farms: Plant Phenotyping for Species Recognition and Health Condition Identification Using Deep Learning},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {2},
PAGES = {274--289},
URL = {https://www.mdpi.com/2673-2688/2/2/17},
ISSN = {2673-2688},
ABSTRACT = {This paper analyses the contribution of residual network (ResNet) based convolutional neural network (CNN) architecture employed in two tasks related to plant phenotyping. Among the contemporary works for species recognition (SR) and infection detection of plants, the majority of them have performed experiments on balanced datasets and used accuracy as the evaluation parameter. However, this work used an imbalanced dataset having an unequal number of images, applied data augmentation to increase accuracy, organised data as multiple test cases and classes, and, most importantly, employed multiclass classifier evaluation parameters useful for asymmetric class distribution. Additionally, the work addresses typical issues faced such as selecting the size of the dataset, depth of classifiers, training time needed, and analysing the classifier’s performance if various test cases are deployed. In this work, ResNet 20 (V2) architecture has performed significantly well in the tasks of Species Recognition (SR) and Identification of Healthy and Infected Leaves (IHIL) with a Precision of 91.84% and 84.00%, Recall of 91.67% and 83.14% and F1 Score of 91.49% and 83.19%, respectively.},
DOI = {10.3390/ai2020017}
}



@Article{app11115303,
AUTHOR = {Huh, Eui-Nam and Hossain, Md Imtiaz},
TITLE = {Brainware Computing: Concepts, Scopes and Challenges},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5303},
URL = {https://www.mdpi.com/2076-3417/11/11/5303},
ISSN = {2076-3417},
ABSTRACT = {Over the decades, robotics technology has acquired sufficient advancement through the progression of 5G Internet, Artificial Intelligence (AI), Internet of Things (IoT), Cloud, and Edge Computing. Though nowadays, Cobot and Service Oriented Architecture (SOA) supported robots with edge computing paradigms have achieved remarkable performances in diverse applications, the existing SOA robotics technology fails to develop a multi-domain expert with high performing robots and demands improvement to Service-Oriented Brain, SOB (including AI model, driving service application and metadata) enabling robot for deploying brain and a new computing model with more scalability and flexibility. In this paper, instead of focusing on SOA and Robot as a Service (RaaS) model, we propose a novel computing architecture, addressed as Brainware Computing, for driving multiple domain-specific brains one-at-a-time in a single hardware robot according to the service, addressed as Brain as a Service (BaaS). In Brainware Computing, each robot can install and remove the virtual machine, which contains SOB and operating applications from the nearest edge cloud. Secondly, we provide an extensive explanation of the scope and possibilities of Brainware Computing. Finally, we demonstrate several challenges and opportunities and then concluded with future research directions in the field of Brainware Computing.},
DOI = {10.3390/app11115303}
}



@Article{app11125320,
AUTHOR = {Al-amri, Redhwan and Murugesan, Raja Kumar and Man, Mustafa and Abdulateef, Alaa Fareed and Al-Sharafi, Mohammed A. and Alkahtani, Ammar Ahmed},
TITLE = {A Review of Machine Learning and Deep Learning Techniques for Anomaly Detection in IoT Data},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5320},
URL = {https://www.mdpi.com/2076-3417/11/12/5320},
ISSN = {2076-3417},
ABSTRACT = {Anomaly detection has gained considerable attention in the past couple of years. Emerging technologies, such as the Internet of Things (IoT), are known to be among the most critical sources of data streams that produce massive amounts of data continuously from numerous applications. Examining these collected data to detect suspicious events can reduce functional threats and avoid unseen issues that cause downtime in the applications. Due to the dynamic nature of the data stream characteristics, many unresolved problems persist. In the existing literature, methods have been designed and developed to evaluate certain anomalous behaviors in IoT data stream sources. However, there is a lack of comprehensive studies that discuss all the aspects of IoT data processing. Thus, this paper attempts to fill this gap by providing a complete image of various state-of-the-art techniques on the major problems and core challenges in IoT data. The nature of data, anomaly types, learning mode, window model, datasets, and evaluation criteria are also presented. Research challenges related to data evolving, feature-evolving, windowing, ensemble approaches, nature of input data, data complexity and noise, parameters selection, data visualizations, heterogeneity of data, accuracy, and large-scale and high-dimensional data are investigated. Finally, the challenges that require substantial research efforts and future directions are summarized.},
DOI = {10.3390/app11125320}
}



@Article{app11125523,
AUTHOR = {Ye, Qian and Lu, Minyan},
TITLE = {s2p: Provenance Research for Stream Processing System},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {5523},
URL = {https://www.mdpi.com/2076-3417/11/12/5523},
ISSN = {2076-3417},
ABSTRACT = {The main purpose of our provenance research for DSP (distributed stream processing) systems is to analyze abnormal results. Provenance for these systems is not nontrivial because of the ephemerality of stream data and instant data processing mode in modern DSP systems. Challenges include but are not limited to an optimization solution for avoiding excessive runtime overhead, reducing provenance-related data storage, and providing it in an easy-to-use fashion. Without any prior knowledge about which kinds of data may finally lead to the abnormal, we have to track all transformations in detail, which potentially causes hard system burden. This paper proposes s2p (Stream Process Provenance), which mainly consists of online provenance and offline provenance, to provide fine- and coarse-grained provenance in different precision. We base our design of s2p on the fact that, for a mature online DSP system, the abnormal results are rare, and the results that require a detailed analysis are even rarer. We also consider state transition in our provenance explanation. We implement s2p on Apache Flink named as s2p-flink and conduct three experiments to evaluate its scalability, efficiency, and overhead from end-to-end cost, throughput, and space overhead. Our evaluation shows that s2p-flink incurs a 13% to 32% cost overhead, 11% to 24% decline in throughput, and few additional space costs in the online provenance phase. Experiments also demonstrates the s2p-flink can scale well. A case study is presented to demonstrate the feasibility of the whole s2p solution.},
DOI = {10.3390/app11125523}
}



@Article{s21124153,
AUTHOR = {Signoretti, Gabriel and Silva, Marianne and Andrade, Pedro and Silva, Ivanovitch and Sisinni, Emiliano and Ferrari, Paolo},
TITLE = {An Evolving TinyML Compression Algorithm for IoT Environments Based on Data Eccentricity},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4153},
URL = {https://www.mdpi.com/1424-8220/21/12/4153},
PubMedID = {34204300},
ISSN = {1424-8220},
ABSTRACT = {Currently, the applications of the Internet of Things (IoT) generate a large amount of sensor data at a very high pace, making it a challenge to collect and store the data. This scenario brings about the need for effective data compression algorithms to make the data manageable among tiny and battery-powered devices and, more importantly, shareable across the network. Additionally, considering that, very often, wireless communications (e.g., low-power wide-area networks) are adopted to connect field devices, user payload compression can also provide benefits derived from better spectrum usage, which in turn can result in advantages for high-density application scenarios. As a result of this increase in the number of connected devices, a new concept has emerged, called TinyML. It enables the use of machine learning on tiny, computationally restrained devices. This allows intelligent devices to analyze and interpret data locally and in real time. Therefore, this work presents a new data compression solution (algorithm) for the IoT that leverages the TinyML perspective. The new approach is called the Tiny Anomaly Compressor (TAC) and is based on data eccentricity. TAC does not require previously established mathematical models or any assumptions about the underlying data distribution. In order to test the effectiveness of the proposed solution and validate it, a comparative analysis was performed on two real-world datasets with two other algorithms from the literature (namely Swing Door Trending (SDT) and the Discrete Cosine Transform (DCT)). It was found that the TAC algorithm showed promising results, achieving a maximum compression rate of 98.33%. Additionally, it also surpassed the two other models regarding the compression error and peak signal-to-noise ratio in all cases.},
DOI = {10.3390/s21124153}
}



@Article{electronics10121460,
AUTHOR = {Xu, Rongxu and Jin, Wenquan and Hong, Yonggeun and Kim, Do-Hyeun},
TITLE = {Intelligent Optimization Mechanism Based on an Objective Function for Efficient Home Appliances Control in an Embedded Edge Platform},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {1460},
URL = {https://www.mdpi.com/2079-9292/10/12/1460},
ISSN = {2079-9292},
ABSTRACT = {In recent years the ever-expanding internet of things (IoT) is becoming more empowered to revolutionize our world with the advent of cutting-edge features and intelligence in an IoT ecosystem. Thanks to the development of the IoT, researchers have devoted themselves to technologies that convert a conventional home into an intelligent occupants-aware place to manage electric resources with autonomous devices to deal with excess energy consumption and providing a comfortable living environment. There are studies to supplement the innate shortcomings of the IoT and improve intelligence by using cloud computing and machine learning. However, the machine learning-based autonomous control devices lack flexibility, and cloud computing is challenging with latency and security. In this paper, we propose a rule-based optimization mechanism on an embedded edge platform to provide dynamic home appliance control and advanced intelligence in a smart home. To provide actional control ability, we design and developed a rule-based objective function in the EdgeX edge computing platform to control the temperature states of the smart home. Compared to cloud computing, edge computing can provide faster response and higher quality of services. The edge computing paradigm provides better analysis, processing, and storage abilities to the data generated from the IoT sensors to enhance the capability of IoT devices concerning computing, storage, and network resources. In order to satisfy the paradigm of distributed edge computing, all the services are implemented as microservices. The microservices are connected to each other through REST APIs based on the constrained IoT devices to provide all the functionalities that accomplish a trade-off between energy consumption and occupant-desired environment setting for the smart home appliances. We simulated our proposed system to control the temperature of a smart home; through experimental findings, we investigated the application against the delay time and overall memory consumption by the embedded edge system of EdgeX. The result of this research work suggests that the implemented services operated efficiently in the raspberry pi 3 hardware of IoT devices.},
DOI = {10.3390/electronics10121460}
}



@Article{s21124234,
AUTHOR = {Vivas, Fulvio Yesid and Caicedo, Oscar Mauricio and Nieves, Juan Carlos},
TITLE = {A Semantic and Knowledge-Based Approach for Handover Management},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {4234},
URL = {https://www.mdpi.com/1424-8220/21/12/4234},
PubMedID = {34205492},
ISSN = {1424-8220},
ABSTRACT = {Handover Management (HM) is pivotal for providing service continuity, enormous reliability and extreme-low latency, and meeting sky-high data rates, in wireless communications. Current HM approaches based on a single criterion may lead to unnecessary and frequent handovers due to a partial network view that is constrained to information about link quality. In turn, HM approaches based on multicriteria may present a failure of handovers and wrong network selection, decreasing the throughput and increasing the packet loss in the network. This paper proposes SIM-Know, an approach for improving HM. SIM-Know improves HM by including a Semantic Information Model (SIM) that enables context-aware and multicriteria handover decisions. SIM-Know also introduces a SIM-based distributed Knowledge Base Profile (KBP) that provides local and global intelligence to make contextual and proactive handover decisions. We evaluated SIM-Know in an emulated wireless network. When the end-user device moves at low and moderate speeds, the results show that our approach outperforms the Signal Strong First (SSF, single criterion approach) and behaves similarly to the Analytic Hierarchy Process combined with the Technique for Order Preferences by Similarity to the Ideal Solution (AHP-TOPSIS, multicriteria approach) regarding the number of handovers and the number of throughput drops. SSF outperforms SIM-Know and AHP-TOPSIS regarding the handover latency metric because SSF runs a straightforward process for making handover decisions. At high speeds, SIM-Know outperforms SSF and AHP-TOPSIS regarding the number of handovers and the number of throughput drops and, further, improves the throughput, delay, jitter, and packet loss in the network. Considering the obtained results, we conclude that SIM-Know is a practical and attractive solution for cognitive HM.},
DOI = {10.3390/s21124234}
}



@Article{app11135861,
AUTHOR = {Li, Gen and Nguyen, Tri-Hai and Jung, Jason J.},
TITLE = {Traffic Incident Detection Based on Dynamic Graph Embedding in Vehicular Edge Computing},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {5861},
URL = {https://www.mdpi.com/2076-3417/11/13/5861},
ISSN = {2076-3417},
ABSTRACT = {With a large of time series dataset from the Internet of Things in Ambient Intelligence-enabled smart environments, many supervised learning-based anomaly detection methods have been investigated but ignored the correlation among the time series. To address this issue, we present a new idea for anomaly detection based on dynamic graph embedding, in which the dynamic graph comprises the multiple time series and their correlation in each time interval. We propose an entropy for measuring a graph’s information injunction with a correlation matrix to define similarity between graphs. A dynamic graph embedding model based on the graph similarity is proposed to cluster the graphs for anomaly detection. We implement the proposed model in vehicular edge computing for traffic incident detection. The experiments are carried out using traffic data produced by the Simulation of Urban Mobility framework. The experimental findings reveal that the proposed method achieves better results than the baselines by 14.5% and 18.1% on average with respect to F1-score and accuracy, respectively.},
DOI = {10.3390/app11135861}
}



@Article{buildings11070275,
AUTHOR = {Shen, Yachen and Chen, Jianping and Fu, Qiming and Wu, Hongjie and Wang, Yunzhe and Lu, You},
TITLE = {Detection of District Heating Pipe Network Leakage Fault Using UCB Arm Selection Method},
JOURNAL = {Buildings},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {275},
URL = {https://www.mdpi.com/2075-5309/11/7/275},
ISSN = {2075-5309},
ABSTRACT = {District heating networks make up an important public energy service, in which leakage is the main problem affecting the safety of pipeline network operation. This paper proposes a Leakage Fault Detection (LFD) method based on the Linear Upper Confidence Bound (LinUCB) which is used for arm selection in the Contextual Bandit (CB) algorithm. With data collected from end-users’ pressure and flow information in the simulation model, the LinUCB method is adopted to locate the leakage faults. Firstly, we use a hydraulic simulation model to simulate all failure conditions that can occur in the network, and these change rate vectors of observed data form a dataset. Secondly, the LinUCB method is used to train an agent for the arm selection, and the outcome of arm selection is the leaking pipe label. Thirdly, the experiment results show that this method can detect the leaking pipe accurately and effectively. Furthermore, it allows operators to evaluate the system performance, supports troubleshooting of decision mechanisms, and provides guidance in the arrangement of maintenance.},
DOI = {10.3390/buildings11070275}
}



@Article{en14133900,
AUTHOR = {Mohapatra, Sunil Kumar and Mishra, Sushruta and Tripathy, Hrudaya Kumar and Bhoi, Akash Kumar and Barsocchi, Paolo},
TITLE = {A Pragmatic Investigation of Energy Consumption and Utilization Models in the Urban Sector Using Predictive Intelligence Approaches},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {3900},
URL = {https://www.mdpi.com/1996-1073/14/13/3900},
ISSN = {1996-1073},
ABSTRACT = {Energy consumption is a crucial domain in energy system management. Recently, it was observed that there has been a rapid rise in the consumption of energy throughout the world. Thus, almost every nation devises its strategies and models to limit energy usage in various areas, ranging from large buildings to industrial firms and vehicles. With technological advancements, computational intelligence models have been successfully contributing to the prediction of the consumption of energy. Machine learning and deep learning-based models enhance the precision and robustness compared to traditional approaches, making it more reliable. This article performs a review analysis of the various computational intelligence approaches currently being utilized to predict energy consumption. An extensive survey procedure is conducted and presented in this study, and relevant works are discussed. Different criteria are considered during the aggregation of the relevant studies relating to the work. The author’s perspective, future trends and various novel approaches are also presented as a part of the discussion. This article thereby lays a foundation stone for further research works to be undertaken for energy prediction.},
DOI = {10.3390/en14133900}
}



@Article{app11136079,
AUTHOR = {Elgamoudi, Abulasad and Benzerrouk, Hamza and Elango, G. Arul and Landry, René},
TITLE = {A Survey for Recent Techniques and Algorithms of Geolocation and Target Tracking in Wireless and Satellite Systems},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6079},
URL = {https://www.mdpi.com/2076-3417/11/13/6079},
ISSN = {2076-3417},
ABSTRACT = {A single Radio-Frequency Interference (RFI) is a disturbance source of modern wireless systems depending on Global Navigation Satellite Systems (GNSS) and Satellite Communication (SatCom). In particular, significant applications such as aeronautics and satellite communication can be severely affected by intentional and unintentional interference, which are unmitigated. The matter requires finding a radical and effective solution to overcome this problem. The methods used for overcoming the RFI include interference detection, interference classification, interference geolocation, tracking and interference mitigation. RFI source geolocation and tracking methodology gained universal attention from numerous researchers, specialists, and scientists. In the last decade, various conventional techniques and algorithms have been adopted in geolocation and target tracking in civil and military operations. Previous conventional techniques did not address the challenges and demand for novel algorithms. Hence there is a necessity for focussing on the issues associated with this. This survey introduces a review of various conventional geolocation techniques, current orientations, and state-of-the-art techniques and highlights some approaches and algorithms employed in wireless and satellite systems for geolocation and target tracking that may be extremely beneficial. In addition, a comparison between different conventional geolocation techniques has been revealed, and the comparisons between various approaches and algorithms of geolocation and target tracking have been addressed, including H∞ and Kalman Filtering versions that have been implemented and investigated by authors.},
DOI = {10.3390/app11136079}
}



@Article{app11136112,
AUTHOR = {Mbiydzenyuy, Gideon and Nowaczyk, Sławomir and Knutsson, Håkan and Vanhoudt, Dirk and Brage, Jens and Calikus, Ece},
TITLE = {Opportunities for Machine Learning in District Heating},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {6112},
URL = {https://www.mdpi.com/2076-3417/11/13/6112},
ISSN = {2076-3417},
ABSTRACT = {The district heating (DH) industry is facing an important transformation towards more efficient networks that utilise significantly lower water temperatures to distribute the heat. This change requires taking advantage of new technologies, and Machine Learning (ML) is a popular direction. In the last decade, we have witnessed an extreme growth in the number of published research papers that focus on applying ML techniques to the DH domain. However, based on our experience in the field, and an extensive review of the state-of-the-art, we perceive a mismatch between the most popular research directions, such as forecasting, and the challenges faced by the DH industry. In this work, we present our findings, explain and demonstrate the key gaps between the two communities and suggest a road-map ahead towards increasing the impact of ML research in the DH industry.},
DOI = {10.3390/app11136112}
}



@Article{s21134511,
AUTHOR = {Bauer, Martin and Sanchez, Luis and Song, JaeSeung},
TITLE = {IoT-Enabled Smart Cities: Evolution and Outlook},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4511},
URL = {https://www.mdpi.com/1424-8220/21/13/4511},
PubMedID = {34209436},
ISSN = {1424-8220},
ABSTRACT = {For the last decade the Smart City concept has been under development, fostered by the growing urbanization of the world’s population and the need to handle the challenges that such a scenario raises. During this time many Smart City projects have been executed–some as proof-of-concept, but a growing number resulting in permanent, production-level deployments, improving the operation of the city and the quality of life of its citizens. Thus, Smart Cities are still a highly relevant paradigm which needs further development before it reaches its full potential and provides robust and resilient solutions. In this paper, the focus is set on the Internet of Things (IoT) as an enabling technology for the Smart City. In this sense, the paper reviews the current landscape of IoT-enabled Smart Cities, surveying relevant experiences and city initiatives that have embedded IoT within their city services and how they have generated an impact. The paper discusses the key technologies that have been developed and how they are contributing to the realization of the Smart City. Moreover, it presents some challenges that remain open ahead of us and which are the initiatives and technologies that are under development to tackle them.},
DOI = {10.3390/s21134511}
}



@Article{vehicles3030021,
AUTHOR = {Rajput, Daizy and Herreros, Jose M. and Innocente, Mauro S. and Schaub, Joschka and Dizqah, Arash M.},
TITLE = {Electrified Powertrain with Multiple Planetary Gears and Corresponding Energy Management Strategy},
JOURNAL = {Vehicles},
VOLUME = {3},
YEAR = {2021},
NUMBER = {3},
PAGES = {341--356},
URL = {https://www.mdpi.com/2624-8921/3/3/21},
ISSN = {2624-8921},
ABSTRACT = {Modern hybrid electric vehicles (HEVs) like the fourth generation of Toyota Prius incorporate multiple planetary gears (PG) to interconnect various power components. Previous studies reported that increasing the number of planetary gears from one to two reduces energy consumption. However, these studies did not compare one PG and two PGs topologies at their optimal operation. Moreover, the size of the powertrain components are not the same and hence the source of reduction in energy consumption is not clear. This paper investigates the effect of the number of planetary gears on energy consumption under optimal operation of the powertrain components. The powertrains with one and two PGs are considered and an optimal simultaneous torque distribution and mode selection strategy is proposed. The proposed energy management strategy (EMS) optimally distributes torque demands amongst the power components whilst also controlling clutches (i.e., mode selection). Results show that increasing from one to two PGs reduces energy consumption by 4%.},
DOI = {10.3390/vehicles3030021}
}



@Article{s21134553,
AUTHOR = {Moon, Junhyung and Yang, Minyeol and Jeong, Jongpil},
TITLE = {A Novel Approach to the Job Shop Scheduling Problem Based on the Deep Q-Network in a Cooperative Multi-Access Edge Computing Ecosystem},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4553},
URL = {https://www.mdpi.com/1424-8220/21/13/4553},
PubMedID = {34283102},
ISSN = {1424-8220},
ABSTRACT = {In this study, based on multi-access edge computing (MEC), we provided the possibility of cooperating manufacturing processes. We tried to solve the job shop scheduling problem by applying DQN (deep Q-network), a reinforcement learning model, to this method. Here, to alleviate the overload of computing resources, an efficient DQN was used for the experiments using transfer learning data. Additionally, we conducted scheduling studies in the edge computing ecosystem of our manufacturing processes without the help of cloud centers. Cloud computing, an environment in which scheduling processing is performed, has issues sensitive to the manufacturing process in general, such as security issues and communication delay time, and research is being conducted in various fields, such as the introduction of an edge computing system that can replace them. We proposed a method of independently performing scheduling at the edge of the network through cooperative scheduling between edge devices within a multi-access edge computing structure. The proposed framework was evaluated, analyzed, and compared with existing frameworks in terms of providing solutions and services.},
DOI = {10.3390/s21134553}
}



@Article{electronics10131606,
AUTHOR = {Senanayake, Janaka and Kalutarage, Harsha and Al-Kadri, Mhd Omar},
TITLE = {Android Mobile Malware Detection Using Machine Learning: A Systematic Review},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1606},
URL = {https://www.mdpi.com/2079-9292/10/13/1606},
ISSN = {2079-9292},
ABSTRACT = {With the increasing use of mobile devices, malware attacks are rising, especially on Android phones, which account for 72.2% of the total market share. Hackers try to attack smartphones with various methods such as credential theft, surveillance, and malicious advertising. Among numerous countermeasures, machine learning (ML)-based methods have proven to be an effective means of detecting these attacks, as they are able to derive a classifier from a set of training examples, thus eliminating the need for an explicit definition of the signatures when developing malware detectors. This paper provides a systematic review of ML-based Android malware detection techniques. It critically evaluates 106 carefully selected articles and highlights their strengths and weaknesses as well as potential improvements. Finally, the ML-based methods for detecting source code vulnerabilities are discussed, because it might be more difficult to add security after the app is deployed. Therefore, this paper aims to enable researchers to acquire in-depth knowledge in the field and to identify potential future research and development directions.},
DOI = {10.3390/electronics10131606}
}



@Article{electronics10141615,
AUTHOR = {Khan, Zeeshan Ali and Abbasi, Ubaid and Kim, Sung Won},
TITLE = {Machine Learning and LPWAN Based Internet of Things Applications in Healthcare Sector during COVID-19 Pandemic},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1615},
URL = {https://www.mdpi.com/2079-9292/10/14/1615},
ISSN = {2079-9292},
ABSTRACT = {Low power wide area networks (LPWAN) are comprised of small devices having restricted processing resources and limited energy budget. These devices are connected with each other using communication protocols. Considering their available resources, these devices can be used in a number of different Internet of Things (IoT) applications. Another interesting paradigm is machine learning, which can also be integrated with LPWAN technology to embed intelligence into these IoT applications. These machine learning-based applications combine intelligence with LPWAN and prove to be a useful tool. One such IoT application is in the medical field, where they can be used to provide multiple services. In the scenario of the COVID-19 pandemic, the importance of LPWAN-based medical services has gained particular attention. This article describes various COVID-19-related healthcare services, using the the applications of machine learning and LPWAN in improving the medical domain during the current COVID-19 pandemic. We validate our idea with the help of a case study that describes a way to reduce the spread of any pandemic using LPWAN technology and machine learning. The case study compares k-Nearest Neighbors (KNN) and trust-based algorithms for mitigating the flow of virus spread. The simulation results show the effectiveness of KNN for curtailing the COVID-19 spread.},
DOI = {10.3390/electronics10141615}
}



@Article{app11146399,
AUTHOR = {Bai, Luchang and Zhang, Youtong and Wei, Hongqian and Dong, Junbo and Tian, Wei},
TITLE = {Digital Twin Modeling of a Solar Car Based on the Hybrid Model Method with Data-Driven and Mechanistic},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {6399},
URL = {https://www.mdpi.com/2076-3417/11/14/6399},
ISSN = {2076-3417},
ABSTRACT = {Solar cars are energy-sensitive and affected by many factors. In order to achieve optimal energy management of solar cars, it is necessary to comprehensively characterize the energy flow of vehicular components. To model these components which are hard to formulate, this study stimulates a solar car with the digital twin (DT) technology to accurately characterize energy. Based on the hybrid modeling approach combining mechanistic and data-driven technologies, the DT model of a solar car is established with a designed cloud platform server based on Transmission Control Protocol (TCP) to realize data interaction between physical and virtual entities. The DT model is further modified by the offline optimization data of drive motors, and the energy consumption is evaluated with the DT system in the real-world experiment. Specifically, the energy consumption error between the experiment and simulation is less than 5.17%, which suggests that the established DT model can accurately stimulate energy consumption. Generally, this study lays the foundation for subsequent performance optimization research.},
DOI = {10.3390/app11146399}
}



@Article{s21144772,
AUTHOR = {Rudd-Orthner, Richard N. M. and Mihaylova, Lyudmila},
TITLE = {Deep ConvNet: Non-Random Weight Initialization for Repeatable Determinism, Examined with FSGM},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4772},
URL = {https://www.mdpi.com/1424-8220/21/14/4772},
PubMedID = {34300512},
ISSN = {1424-8220},
ABSTRACT = {A repeatable and deterministic non-random weight initialization method in convolutional layers of neural networks examined with the Fast Gradient Sign Method (FSGM). Using the FSGM approach as a technique to measure the initialization effect with controlled distortions in transferred learning, varying the dataset numerical similarity. The focus is on convolutional layers with induced earlier learning through the use of striped forms for image classification. Which provided a higher performing accuracy in the first epoch, with improvements of between 3–5% in a well known benchmark model, and also ~10% in a color image dataset (MTARSI2), using a dissimilar model architecture. The proposed method is robust to limit optimization approaches like Glorot/Xavier and He initialization. Arguably the approach is within a new category of weight initialization methods, as a number sequence substitution of random numbers, without a tether to the dataset. When examined under the FGSM approach with transferred learning, the proposed method when used with higher distortions (numerically dissimilar datasets), is less compromised against the original cross-validation dataset, at ~31% accuracy instead of ~9%. This is an indication of higher retention of the original fitting in transferred learning.},
DOI = {10.3390/s21144772}
}



@Article{electronics10141677,
AUTHOR = {Elbasi, Ersin and Topcu, Ahmet E. and Mathew, Shinu},
TITLE = {Prediction of COVID-19 Risk in Public Areas Using IoT and Machine Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1677},
URL = {https://www.mdpi.com/2079-9292/10/14/1677},
ISSN = {2079-9292},
ABSTRACT = {COVID-19 is a community-acquired infection with symptoms that resemble those of influenza and bacterial pneumonia. Creating an infection control policy involving isolation, disinfection of surfaces, and identification of contagions is crucial in eradicating such pandemics. Incorporating social distancing could also help stop the spread of community-acquired infections like COVID-19. Social distancing entails maintaining certain distances between people and reducing the frequency of contact between people. Meanwhile, a significant increase in the development of different Internet of Things (IoT) devices has been seen together with cyber-physical systems that connect with physical environments. Machine learning is strengthening current technologies by adding new approaches to quickly and correctly solve problems utilizing this surge of available IoT devices. We propose a new approach using machine learning algorithms for monitoring the risk of COVID-19 in public areas. Extracted features from IoT sensors are used as input for several machine learning algorithms such as decision tree, neural network, naïve Bayes classifier, support vector machine, and random forest to predict the risks of the COVID-19 pandemic and calculate the risk probability of public places. This research aims to find vulnerable populations and reduce the impact of the disease on certain groups using machine learning models. We build a model to calculate and predict the risk factors of populated areas. This model generates automated alerts for security authorities in the case of any abnormal detection. Experimental results show that we have high accuracy with random forest of 97.32%, with decision tree of 94.50%, and with the naïve Bayes classifier of 99.37%. These algorithms indicate great potential for crowd risk prediction in public areas.},
DOI = {10.3390/electronics10141677}
}



@Article{s21144798,
AUTHOR = {Chen, Fangni and Wang, Anding and Zhang, Yu and Ni, Zhengwei and Hua, Jingyu},
TITLE = {Energy Efficient SWIPT Based Mobile Edge Computing Framework for WSN-Assisted IoT},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4798},
URL = {https://www.mdpi.com/1424-8220/21/14/4798},
PubMedID = {34300538},
ISSN = {1424-8220},
ABSTRACT = {With the increasing deployment of IoT devices and applications, a large number of devices that can sense and monitor the environment in IoT network are needed. This trend also brings great challenges, such as data explosion and energy insufficiency. This paper proposes a system that integrates mobile edge computing (MEC) technology and simultaneous wireless information and power transfer (SWIPT) technology to improve the service supply capability of WSN-assisted IoT applications. A novel optimization problem is formulated to minimize the total system energy consumption under the constraints of data transmission rate and transmitting power requirements by jointly considering power allocation, CPU frequency, offloading weight factor and energy harvest weight factor. Since the problem is non-convex, we propose a novel alternate group iteration optimization (AGIO) algorithm, which decomposes the original problem into three subproblems, and alternately optimizes each subproblem using the group interior point iterative algorithm. Numerical simulations validate that the energy consumption of our proposed design is much lower than the two benchmark algorithms. The relationship between system variables and energy consumption of the system is also discussed.},
DOI = {10.3390/s21144798}
}



@Article{electronics10141744,
AUTHOR = {Wazirali, Raniyah and Ahmad, Rami and Al-Amayreh, Ahmed and Al-Madi, Mohammad and Khalifeh, Ala’},
TITLE = {Secure Watermarking Schemes and Their Approaches in the IoT Technology: An Overview},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1744},
URL = {https://www.mdpi.com/2079-9292/10/14/1744},
ISSN = {2079-9292},
ABSTRACT = {Information security is considered one of the most important issues in various infrastructures related to the field of data communication where most of the modern studies focus on finding effective and low-weight secure approaches. Digital watermarking is a trend in security techniques that hides data by using data embedding and data extraction processes. Watermarking technology is integrated into different frames without adding an overheard as in the conventional encryption. Therefore, it is efficient to be used in data encryption for applications that run over limited resources such as the Internet of Things (IoT). In this paper, different digital watermarking algorithms and approaches are presented. Additionally, watermarking requirements and challenges are illustrated in detail. Moreover, the common architecture of the watermarking system is described. Furthermore, IoT technology and its challenges are highlighted. Finally, the paper provides the motivations, objectives and applications of the recent secure watermarking techniques in IoT and summarises them into one table. In addition, the paper highlights the potential to apply the modified watermark algorithms to secure IoT networks.},
DOI = {10.3390/electronics10141744}
}



@Article{su13158141,
AUTHOR = {Rehman Khan, Haseeb Ur and Lim, Chen Kim and Ahmed, Minhaz Farid and Tan, Kian Lam and Bin Mokhtar, Mazlin},
TITLE = {Systematic Review of Contextual Suggestion and Recommendation Systems for Sustainable e-Tourism},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {8141},
URL = {https://www.mdpi.com/2071-1050/13/15/8141},
ISSN = {2071-1050},
ABSTRACT = {Agenda 2030 of Sustainable Development Goals (SDGs) 9 and 11 recognizes tourism as one of the central industries to global development to tackle global challenges. With the transformation of information and communication technologies (ICT), e-tourism has evolved globally to establish commercial relationships using the Internet for offering tourism-related products, including giving personalised suggestions. The contextual suggestion has emerged as a modified recommendation system that is integrated with information-retrieval techniques within large databases to provide tourists with a list of suggestions based on contexts, such as location, time of day, or day of the week (weekdays or weekends). This study surveyed literature in the field of contextual suggestion and recommendation systems with a focus on e-tourism. The concerns linked with approaches used in contextual suggestion and recommendation systems are highlighted in this systematic review, while motivations, recommendations, and practical implications in e-tourism are also discussed in this paper. A query search using the keywords “contextual suggestion system”, “recommendation system”, and “tourism” identified 143 relevant articles published from 2012 to 2020. Four major repositories are considered for searching, namely, (i) Science Direct, (ii) Scopus, (iii) IEEE, and (iv) Web of Science. This review was carried out under the protocols of four phases, namely, (i) query searching in major article repositories, (ii) removal of duplicates, (iii) scan of title and abstract, and (iv) complete reading of articles. To identify the gaps in current research, a taxonomy analysis was exemplified into categories and subcategories. The main categories were highlighted as (i) review articles, (ii) model/framework, and (iii) applications. Critical analysis was carried out on the basis of the available literature on the limitations of approaches used in contextual suggestion and recommendation systems. In conclusion, the approaches used are mainly based on content-based filtering, collaborative filtering, preference-based product ranking, and language modelling. The evaluation measures for the contextual suggestion system include precision, normalized discounted cumulative, and mean reciprocal rank, while test collections comprise Internet resources. Given that the tourism industry contributed to the environmental and social-economic development, contextual suggestion and recommendation systems have presented themselves to be relevant in integrating and achieving SDG 9 and SDG 11 in many ways such as web-based e-services by the government sector and smart gadgets based on reliable and real-time data and information for city planners as well as law enforcement personnel in a sustainable city.},
DOI = {10.3390/su13158141}
}



@Article{info12080292,
AUTHOR = {Sari, Riri Fitri and Rosyidi, Lukman and Susilo, Bambang and Asvial, Muhamad},
TITLE = {A Comprehensive Review on Network Protocol Design for Autonomic Internet of Things},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {292},
URL = {https://www.mdpi.com/2078-2489/12/8/292},
ISSN = {2078-2489},
ABSTRACT = {The autonomic Internet of Things is the creation of self-management capability in the Internet of Things system by embedding some autonomic properties, with the goal of freeing humans from all detail of the operation and management of the system. At same time, this provides a system to always operate on the best performance. This paper presents a review of the recent studies related to the design of network communication protocol, which can support autonomic Internet of Things. Many of the studies come from the research and development in Wireless Sensor Network protocols, as it becomes one of the key technologies for the Internet of Things. The identified autonomic properties are self-organization, self-optimization, and self-protection. We review some protocols with the objective of energy consumption reduction and energy harvesting awareness, as it can support the self-energy-awareness property. As the result, the protocol designs are mapped according to each autonomic property supported, including protocols for MAC layer, protocols for clustering, protocols for routing, and protocols for security. This can be used to map the advances of communication protocol research for the autonomic Internet of Things and to identify the opportunities for future research.},
DOI = {10.3390/info12080292}
}



@Article{s21154999,
AUTHOR = {Khan, Manzoor Ahmed and Alkaabi, Najla},
TITLE = {Rebirth of Distributed AI—A Review of eHealth Research},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {4999},
URL = {https://www.mdpi.com/1424-8220/21/15/4999},
PubMedID = {34372236},
ISSN = {1424-8220},
ABSTRACT = {The envisioned smart city domains are expected to rely heavily on artificial intelligence and machine learning (ML) approaches for their operations, where the basic ingredient is data. Privacy of the data and training time have been major roadblocks to achieving the specific goals of each application domain. Policy makers, the research community, and the industrial sector have been putting their efforts into addressing these issues. Federated learning, with its distributed and local training approach, stands out as a potential solution to these challenges. In this article, we discuss the potential interplay of different technologies and AI for achieving the required features of future smart city services. Having discussed a few use-cases for future eHealth, we list design goals and technical requirements of the enabling technologies. The paper confines its focus on federated learning. After providing the tutorial on federated learning, we analyze the Federated Learning research literature. We also highlight the challenges. A solution sketch and high-level research directions may be instrumental in addressing the challenges.},
DOI = {10.3390/s21154999}
}



@Article{s21155044,
AUTHOR = {Behjati, Mehran and Mohd Noh, Aishah Binti and Alobaidy, Haider A. H. and Zulkifley, Muhammad Aidiel and Nordin, Rosdiadee and Abdullah, Nor Fadzilah},
TITLE = {LoRa Communications as an Enabler for Internet of Drones towards Large-Scale Livestock Monitoring in Rural Farms},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5044},
URL = {https://www.mdpi.com/1424-8220/21/15/5044},
PubMedID = {34372281},
ISSN = {1424-8220},
ABSTRACT = {Currently, smart farming is considered an effective solution to enhance the productivity of farms; thereby, it has recently received broad interest from service providers to offer a wide range of applications, from pest identification to asset monitoring. Although the emergence of digital technologies, such as the Internet of Things (IoT) and low-power wide-area networks (LPWANs), has led to significant advances in the smart farming industry, farming operations still need more efficient solutions. On the other hand, the utilization of unmanned aerial vehicles (UAVs), also known as drones, is growing rapidly across many civil application domains. This paper aims to develop a farm monitoring system that incorporates UAV, LPWAN, and IoT technologies to transform the current farm management approach and aid farmers in obtaining actionable data from their farm operations. In this regard, an IoT-based water quality monitoring system was developed because water is an essential aspect in livestock development. Then, based on the Long-Range Wide-Area Network (LoRaWAN®) technology, a multi-channel LoRaWAN® gateway was developed and integrated into a vertical takeoff and landing drone to convey collected data from the sensors to the cloud for further analysis. In addition, to develop LoRaWAN®-based aerial communication, a series of measurements and simulations were performed under different configurations and scenarios. Finally, to enhance the efficiency of aerial-based data collection, the UAV path planning was optimized. Measurement results showed that the maximum achievable LoRa coverage when operating on-air via the drone is about 10 km, and the Longley–Rice irregular terrain model provides the most suitable path loss model for the scenario of large-scale farms, and a multi-channel gateway with a spreading factor of 12 provides the most reliable communication link at a high drone speed (up to 95 km/h). Simulation results showed that the developed system can overcome the coverage limitation of LoRaWAN® and it can establish a reliable communication link over large-scale wireless sensor networks. In addition, it was shown that by optimizing flight paths, aerial data collection could be performed in a much shorter time than industrial mission planning (up to four times in our case).},
DOI = {10.3390/s21155044}
}



@Article{s21155060,
AUTHOR = {Khan, Malak Abid Ali and Ma, Hongbin and Aamir, Syed Muhammad and Jin, Ying},
TITLE = {Optimizing the Performance of Pure ALOHA for LoRa-Based ESL},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5060},
URL = {https://www.mdpi.com/1424-8220/21/15/5060},
PubMedID = {34372295},
ISSN = {1424-8220},
ABSTRACT = {(1) Background: The scientific development in the field of industrialization demands the automization of electronic shelf labels (ESLs). COVID-19 has limited the manpower responsible for the frequent updating of the ESL system. The current ESL uses QR (quick response) codes, NFC (near-field communication), and RFID (radio-frequency identification). These technologies have a short range or need more manpower. LoRa is one of the prominent contenders in this category as it provides long-range connectivity with less energy harvesting and location tracking. It uses many gateways (GWs) to transmit the same data packet to a node, which causes collision at the receiver side. The restriction of the duty cycle (DC) and dependency of acknowledgment makes it unsuitable for use by the common person. The maximum efficiency of pure ALOHA is 18.4%, while that of slotted ALOHA is 36.8%, which makes LoRa unsuitable for industrial use. It can be used for applications that need a low data rate, i.e., up to approximately 27 Kbps. The ALOHA mechanism can cause inefficiency by not eliminating fast saturation even with the increasing number of gateways. The increasing number of gateways can only improve the global performance for generating packets with Poisson law having a uniform distribution of payload of 1~51 bytes. The maximum expected channel capacity usage is similar to the pure ALOHA throughput. (2) Methods: In this paper, the improved ALOHA mechanism is used, which is based on the orthogonal combination of spreading factor (SF) and bandwidth (BW), to maximize the throughput of LoRa for ESL. The varying distances (D) of the end nodes (ENs) are arranged based on the K-means machine learning algorithm (MLA) using the parameter selection principle of ISM (industrial, scientific and medical) regulation with a 1% DC for transmission to minimize the saturation. (3) Results: The performance of the improved ALOHA degraded with the increasing number of SFs and as well ENs. However, after using K-mapping, the network changes and the different number of gateways had a greater impact on the probability of successful transmission. The saturation decreased from 57% to 1~2% by using MLA. The RSSI (Received Signal Strength Indicator) plays a key role in determining the exact position of the ENs, which helps to improve the possibility of successful transmission and synchronization at higher BW (250 kHz). In addition, a high BW has lower energy consumption than a low BW at the same DC with a double-bit rate and almost half the ToA (time on-air).},
DOI = {10.3390/s21155060}
}



@Article{iot2030022,
AUTHOR = {Ullah, Imtiaz and Ullah, Ayaz and Sajjad, Mazhar},
TITLE = {Towards a Hybrid Deep Learning Model for Anomalous Activities Detection in Internet of Things Networks},
JOURNAL = {IoT},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {428--448},
URL = {https://www.mdpi.com/2624-831X/2/3/22},
ISSN = {2624-831X},
ABSTRACT = {The tremendous number of Internet of Things (IoT) applications, with their ubiquity, has provided us with unprecedented productivity and simplified our daily life. At the same time, the insecurity of these technologies ensures that our daily lives are surrounded by vulnerable computers, allowing for the launch of multiple attacks via large-scale botnets through the IoT. These attacks have been successful in achieving their heinous objectives. A strong identification strategy is essential to keep devices secured. This paper proposes and implements a model for anomaly-based intrusion detection in IoT networks that uses a convolutional neural network (CNN) and gated recurrent unit (GRU) to detect and classify binary and multiclass IoT network data. The proposed model is validated using the BoT-IoT, IoT Network Intrusion, MQTT-IoT-IDS2020, and IoT-23 intrusion detection datasets. Our proposed binary and multiclass classification model achieved an exceptionally high level of accuracy, precision, recall, and F1 score.},
DOI = {10.3390/iot2030022}
}



@Article{s21155111,
AUTHOR = {Kim, Youngboo and Kwon, Lam and Park, Eun-Chan},
TITLE = {OFDMA Backoff Control Scheme for Improving Channel Efficiency in the Dynamic Network Environment of IEEE 802.11ax WLANs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5111},
URL = {https://www.mdpi.com/1424-8220/21/15/5111},
PubMedID = {34372346},
ISSN = {1424-8220},
ABSTRACT = {IEEE 802.11ax uplink orthogonal frequency division multiple access (OFDMA)-based random access (UORA) is a new feature for random channel access in wireless local area networks (WLANs). Similar to the legacy random access scheme in WLANs, UORA performs the OFDMA backoff (OBO) procedure to access the channel and decides on a random OBO counter within the OFDMA contention window (OCW) value. An access point (AP) can determine the OCW range and inform each station (STA) of it. However, how to determine a reasonable OCW range is beyond the scope of the IEEE 802.11ax standard. The OCW range is crucial to the UORA performance, and it primarily depends on the number of contending STAs, but it is challenging for the AP to accurately and quickly estimate or keep track of the number of contending STAs without the aid of a specific signaling mechanism. In addition, the one for this purpose incurs an additional delay and overhead in the channel access procedure. Therefore, the performance of a UORA scheme can be degraded by an improper OCW range, especially when the number of contending STAs changes dynamically. We first observed the effect of OCW values on channel efficiency and derived its optimal value from an analytical model. Next, we proposed a simple yet effective OBO control scheme where each STA determines its own OBO counter in a distributed manner rather than adjusting the OCW value globally. In the proposed scheme, each STA determines an appropriate OBO counter depending on whether the previous transmission was successful or not so that collisions can be mitigated without leaving OFDMA resource units unnecessarily idle. The results of a simulation study confirm that the throughput of the proposed scheme is comparable to the optimal OCW-based scheme and is improved by up to 15 times compared to the standard UORA scheme.},
DOI = {10.3390/s21155111}
}



@Article{s21155122,
AUTHOR = {Istiaque Ahmed, Kazi and Tahir, Mohammad and Hadi Habaebi, Mohamed and Lun Lau, Sian and Ahad, Abdul},
TITLE = {Machine Learning for Authentication and Authorization in IoT: Taxonomy, Challenges and Future Research Direction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5122},
URL = {https://www.mdpi.com/1424-8220/21/15/5122},
PubMedID = {34372360},
ISSN = {1424-8220},
ABSTRACT = {With the ongoing efforts for widespread Internet of Things (IoT) adoption, one of the key factors hindering the wide acceptance of IoT is security. Securing IoT networks such as the electric power grid or water supply systems has emerged as a major national and global priority. To address the security issue of IoT, several studies are being carried out that involve the use of, but are not limited to, blockchain, artificial intelligence, and edge/fog computing. Authentication and authorization are crucial aspects of the CIA triad to protect the network from malicious parties. However, existing authorization and authentication schemes are not sufficient for handling security, due to the scale of the IoT networks and the resource-constrained nature of devices. In order to overcome challenges due to various constraints of IoT networks, there is a significant interest in using machine learning techniques to assist in the authentication and authorization process for IoT. In this paper, recent advances in authentication and authorization techniques for IoT networks are reviewed. Based on the review, we present a taxonomy of authentication and authorization schemes in IoT focusing on machine learning-based schemes. Using the presented taxonomy, a thorough analysis is provided of the authentication and authorization (AA) security threats and challenges for IoT. Furthermore, various criteria to achieve a high degree of AA resiliency in IoT implementations to enhance IoT security are evaluated. Lastly, a detailed discussion on open issues, challenges, and future research directions is presented for enabling secure communication among IoT nodes.},
DOI = {10.3390/s21155122}
}



@Article{asi4030052,
AUTHOR = {Manzoor, Bilal and Othman, Idris and Durdyev, Serdar and Ismail, Syuhaida and Wahab, Mohammad Hussaini},
TITLE = {Influence of Artificial Intelligence in Civil Engineering toward Sustainable Development—A Systematic Literature Review},
JOURNAL = {Applied System Innovation},
VOLUME = {4},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2571-5577/4/3/52},
ISSN = {2571-5577},
ABSTRACT = {The widespread use of artificial intelligence (AI) in civil engineering has provided civil engineers with various benefits and opportunities, including a rich data collection, sustainable assessment, and productivity. The trend of construction is diverted toward sustainability with the aid of digital technologies. In this regard, this paper presents a systematic literature review (SLR) in order to explore the influence of AI in civil engineering toward sustainable development. In addition, SLR was carried out by using academic publications from Scopus (i.e., 3478 publications). Furthermore, screening is carried out, and eventually, 105 research publications in the field of AI were selected. Keywords were searched through Boolean operation “Artificial Intelligence” OR “Machine intelligence” OR “Machine Learning” OR “Computational intelligence” OR “Computer vision” OR “Expert systems” OR “Neural networks” AND “Civil Engineering” OR “Construction Engineering” OR “Sustainable Development” OR “Sustainability”. According to the findings, it was revealed that the trend of publications received its high intention of researchers in 2020, the most important contribution of publications on AI toward sustainability by the Automation in Construction, the United States has the major influence among all the other countries, the main features of civil engineering toward sustainability are interconnectivity, functionality, unpredictability, and individuality. This research adds to the body of knowledge in civil engineering by visualizing and comprehending trends and patterns, as well as defining major research goals, journals, and countries. In addition, a theoretical framework has been proposed in light of the results for prospective researchers and scholars.},
DOI = {10.3390/asi4030052}
}



@Article{sym13081453,
AUTHOR = {Lyu, Renjian and He, Mingshu and Zhang, Yu and Jin, Lei and Wang, Xinlei},
TITLE = {Network Intrusion Detection Based on an Efficient Neural Architecture Search},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1453},
URL = {https://www.mdpi.com/2073-8994/13/8/1453},
ISSN = {2073-8994},
ABSTRACT = {Deep learning has been applied in the field of network intrusion detection and has yielded good results. In malicious network traffic classification tasks, many studies have achieved good performance with respect to the accuracy and recall rate of classification through self-designed models. In deep learning, the design of the model architecture greatly influences the results. However, the design of the network model architecture usually requires substantial professional knowledge. At present, the focus of research in the field of traffic monitoring is often directed elsewhere. Therefore, in the classification task of the network intrusion detection field, there is much room for improvement in the design and optimization of the model architecture. A neural architecture search (NAS) can automatically search the architecture of the model under the premise of a given optimization goal. For this reason, we propose a model that can perform NAS in the field of network traffic classification and search for the optimal architecture suitable for traffic detection based on the network traffic dataset. Each layer of our depth model is constructed according to the principle of maximum coding rate attenuation, which has strong consistency and symmetry in structure. Compared with some manually designed network architectures, classification indicators, such as Top-1 accuracy and F1 score, are also greatly improved while ensuring the lightweight nature of the model. In addition, we introduce a surrogate model in the search task. Compared to using the traditional NAS model to search the network traffic classification model, our NAS model greatly improves the search efficiency under the premise of ensuring that the results are not substantially different. We also manually adjust some operations in the search space of the architecture search to find a set of model operations that are more suitable for traffic classification. Finally, we apply the searched model to other traffic datasets to verify the universality of the model. Compared with several common network models in the traffic field, the searched model (NAS-Net) performs better, and the classification effect is more accurate.},
DOI = {10.3390/sym13081453}
}



@Article{app11167351,
AUTHOR = {Ahmad, Hussain and Zubair Islam, Muhammad and Ali, Rashid and Haider, Amir and Kim, Hyungseok},
TITLE = {Intelligent Stretch Optimization in Information Centric Networking-Based Tactile Internet Applications},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7351},
URL = {https://www.mdpi.com/2076-3417/11/16/7351},
ISSN = {2076-3417},
ABSTRACT = {The fifth-generation (5G) mobile network services are currently being made available for different use case scenarios like enhanced mobile broadband, ultra-reliable and low latency communication, and massive machine-type communication. The ever-increasing data requests from the users have shifted the communication paradigm to be based on the type of the requested data content or the so-called information-centric networking (ICN). The ICN primarily aims to enhance the performance of the network infrastructure in terms of the stretch to opt for the best routing path. Reduction in stretch merely reduces the end-to-end (E2E) latency to ensure the requirements of the 5G-enabled tactile internet (TI) services. The foremost challenge tackled by the ICN-based system is to minimize the stretch while selecting an optimal routing path. Therefore, in this work, a reinforcement learning-based intelligent stretch optimization (ISO) strategy has been proposed to reduce stretch and obtain an optimal routing path in ICN-based systems for the realization of 5G-enabled TI services. A Q-learning algorithm is utilized to explore and exploit the different routing paths within the ICN infrastructure. The problem is designed as a Markov decision process and solved with the help of the Q-learning algorithm. The simulation results indicate that the proposed strategy finds the optimal routing path for the delay-sensitive haptic-driven services of 5G-enabled TI based upon their stretch profile over ICN, such as the augmented reality /virtual reality applications. Moreover, we compare and evaluate the simulation results of propsoed ISO strategy with random routing strategy and history aware routing protocol (HARP). The proposed ISO strategy reduces 33.33% and 33.69% delay as compared to random routing and HARP, respectively. Thus, the proposed strategy suggests an optimal routing path with lesser stretch to minimize the E2E latency.},
DOI = {10.3390/app11167351}
}



@Article{su13169092,
AUTHOR = {Rehman, Amjad and Haseeb, Khalid and Saba, Tanzila and Lloret, Jaime and Ahmed, Zara},
TITLE = {Mobility Support 5G Architecture with Real-Time Routing for Sustainable Smart Cities},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {9092},
URL = {https://www.mdpi.com/2071-1050/13/16/9092},
ISSN = {2071-1050},
ABSTRACT = {The Internet of Things (IoT) is an emerging technology and provides connectivity among physical objects with the support of 5G communication. In recent decades, there have been a lot of applications based on IoT technology for the sustainability of smart cities, such as farming, e-healthcare, education, smart homes, weather monitoring, etc. These applications communicate in a collaborative manner between embedded IoT devices and systematize daily routine tasks. In the literature, many solutions facilitate remote users to gather the observed data by accessing the stored information on the cloud network and lead to smart systems. However, most of the solutions raise significant research challenges regarding information sharing in mobile IoT networks and must be able to stabilize the performance of smart operations in terms of security and intelligence. Many solutions are based on 5G communication to support high user mobility and increase the connectivity among a huge number of IoT devices. However, such approaches lack user and data privacy against anonymous threats and incur resource costs. In this paper, we present a mobility support 5G architecture with real-time routing for sustainable smart cities that aims to decrease the loss of data against network disconnectivity and increase the reliability for 5G-based public healthcare networks. The proposed architecture firstly establishes a mutual relationship among the nodes and mobile sink with shared secret information and lightweight processing. Secondly, multi-secured levels are proposed to protect the interaction with smart transmission systems by increasing the trust threshold over the insecure channels. The conducted experiments are analyzed, and it is concluded that their performance significantly increases the information sustainability for mobile networks in terms of security and routing.},
DOI = {10.3390/su13169092}
}



@Article{s21165491,
AUTHOR = {Gupta, Divya and Rani, Shalli and Ahmed, Syed Hassan and Verma, Sahil and Ijaz, Muhammad Fazal and Shafi, Jana},
TITLE = {Edge Caching Based on Collaborative Filtering for Heterogeneous ICN-IoT Applications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5491},
URL = {https://www.mdpi.com/1424-8220/21/16/5491},
PubMedID = {34450933},
ISSN = {1424-8220},
ABSTRACT = {The substantial advancements offered by the edge computing has indicated serious evolutionary improvements for the internet of things (IoT) technology. The rigid design philosophy of the traditional network architecture limits its scope to meet future demands. However, information centric networking (ICN) is envisioned as a promising architecture to bridge the huge gaps and maintain IoT networks, mostly referred as ICN-IoT. The edge-enabled ICN-IoT architecture always demands efficient in-network caching techniques for supporting better user’s quality of experience (QoE). In this paper, we propose an enhanced ICN-IoT content caching strategy by enabling artificial intelligence (AI)-based collaborative filtering within the edge cloud to support heterogeneous IoT architecture. This collaborative filtering-based content caching strategy would intelligently cache content on edge nodes for traffic management at cloud databases. The evaluations has been conducted to check the performance of the proposed strategy over various benchmark strategies, such as LCE, LCD, CL4M, and ProbCache. The analytical results demonstrate the better performance of our proposed strategy with average gain of 15% for cache hit ratio, 12% reduction in content retrieval delay, and 28% reduced average hop count in comparison to best considered LCD. We believe that the proposed strategy will contribute an effective solution to the related studies in this domain.},
DOI = {10.3390/s21165491}
}



@Article{fi13080210,
AUTHOR = {Ghorpade, Sheetal and Zennaro, Marco and Chaudhari, Bharat},
TITLE = {Survey of Localization for Internet of Things Nodes: Approaches, Challenges and Open Issues},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {210},
URL = {https://www.mdpi.com/1999-5903/13/8/210},
ISSN = {1999-5903},
ABSTRACT = {With exponential growth in the deployment of Internet of Things (IoT) devices, many new innovative and real-life applications are being developed. IoT supports such applications with the help of resource-constrained fixed as well as mobile nodes. These nodes can be placed in anything from vehicles to the human body to smart homes to smart factories. Mobility of the nodes enhances the network coverage and connectivity. One of the crucial requirements in IoT systems is the accurate and fast localization of its nodes with high energy efficiency and low cost. The localization process has several challenges. These challenges keep changing depending on the location and movement of nodes such as outdoor, indoor, with or without obstacles and so on. The performance of localization techniques greatly depends on the scenarios and conditions from which the nodes are traversing. Precise localization of nodes is very much required in many unique applications. Although several localization techniques and algorithms are available, there are still many challenges for the precise and efficient localization of the nodes. This paper classifies and discusses various state-of-the-art techniques proposed for IoT node localization in detail. It includes the different approaches such as centralized, distributed, iterative, ranged based, range free, device-based, device-free and their subtypes. Furthermore, the different performance metrics that can be used for localization, comparison of the different techniques, some prominent applications in smart cities and future directions are also covered.},
DOI = {10.3390/fi13080210}
}



@Article{electronics10161974,
AUTHOR = {Lakhan, Abdullah and Mastoi, Qurat-ul-ain and Dootio, Mazhar Ali and Alqahtani, Fehaid and Alzahrani, Ibrahim R. and Baothman, Fatmah and Shah, Syed Yaseen and Shah, Syed Aziz and Anjum, Nadeem and Abbasi, Qammer Hussain and Khokhar, Muhammad Saddam},
TITLE = {Hybrid Workload Enabled and Secure Healthcare Monitoring Sensing Framework in Distributed Fog-Cloud Network},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {1974},
URL = {https://www.mdpi.com/2079-9292/10/16/1974},
ISSN = {2079-9292},
ABSTRACT = {The Internet of Medical Things (IoMT) workflow applications have been rapidly growing in practice. These internet-based applications can run on the distributed healthcare sensing system, which combines mobile computing, edge computing and cloud computing. Offloading and scheduling are the required methods in the distributed network. However, a security issue exists and it is hard to run different types of tasks (e.g., security, delay-sensitive, and delay-tolerant tasks) of IoMT applications on heterogeneous computing nodes. This work proposes a new healthcare architecture for workflow applications based on heterogeneous computing nodes layers: an application layer, management layer, and resource layer. The goal is to minimize the makespan of all applications. Based on these layers, the work proposes a secure offloading-efficient task scheduling (SEOS) algorithm framework, which includes the deadline division method, task sequencing rules, homomorphic security scheme, initial scheduling, and the variable neighbourhood searching method. The performance evaluation results show that the proposed plans outperform all existing baseline approaches for healthcare applications in terms of makespan.},
DOI = {10.3390/electronics10161974}
}



@Article{a14080242,
AUTHOR = {Kousis, Anestis and Tjortjis, Christos},
TITLE = {Data Mining Algorithms for Smart Cities: A Bibliometric Analysis},
JOURNAL = {Algorithms},
VOLUME = {14},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {242},
URL = {https://www.mdpi.com/1999-4893/14/8/242},
ISSN = {1999-4893},
ABSTRACT = {Smart cities connect people and places using innovative technologies such as Data Mining (DM), Machine Learning (ML), big data, and the Internet of Things (IoT). This paper presents a bibliometric analysis to provide a comprehensive overview of studies associated with DM technologies used in smart cities applications. The study aims to identify the main DM techniques used in the context of smart cities and how the research field of DM for smart cities evolves over time. We adopted both qualitative and quantitative methods to explore the topic. We used the Scopus database to find relative articles published in scientific journals. This study covers 197 articles published over the period from 2013 to 2021. For the bibliometric analysis, we used the Biliometrix library, developed in R. Our findings show that there is a wide range of DM technologies used in every layer of a smart city project. Several ML algorithms, supervised or unsupervised, are adopted for operating the instrumentation, middleware, and application layer. The bibliometric analysis shows that DM for smart cities is a fast-growing scientific field. Scientists from all over the world show a great interest in researching and collaborating on this interdisciplinary scientific field.},
DOI = {10.3390/a14080242}
}



@Article{app11167561,
AUTHOR = {Iqbal, Umair and Barthelemy, Johan and Li, Wanqing and Perez, Pascal},
TITLE = {Automating Visual Blockage Classification of Culverts with Deep Learning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7561},
URL = {https://www.mdpi.com/2076-3417/11/16/7561},
ISSN = {2076-3417},
ABSTRACT = {Blockage of culverts by transported debris materials is reported as the salient contributor in originating urban flash floods. Conventional hydraulic modeling approaches had no success in addressing the problem primarily because of the unavailability of peak floods hydraulic data and the highly non-linear behavior of debris at the culvert. This article explores a new dimension to investigate the issue by proposing the use of intelligent video analytics (IVA) algorithms for extracting blockage related information. The presented research aims to automate the process of manual visual blockage classification of culverts from a maintenance perspective by remotely applying deep learning models. The potential of using existing convolutional neural network (CNN) algorithms (i.e., DarkNet53, DenseNet121, InceptionResNetV2, InceptionV3, MobileNet, ResNet50, VGG16, EfficientNetB3, NASNet) is investigated over a dataset from three different sources (i.e., images of culvert openings and blockage (ICOB), visual hydrology-lab dataset (VHD), synthetic images of culverts (SIC)) to predict the blockage in a given image. Models were evaluated based on their performance on the test dataset (i.e., accuracy, loss, precision, recall, F1 score, Jaccard Index, region of convergence (ROC) curve), floating point operations per second (FLOPs) and response times to process a single test instance. Furthermore, the performance of deep learning models was benchmarked against conventional machine learning algorithms (i.e., SVM, RF, xgboost). In addition, the idea of classifying deep visual features extracted by CNN models (i.e., ResNet50, MobileNet) using conventional machine learning approaches was also implemented in this article. From the results, NASNet was reported most efficient in classifying the blockage images with the 5-fold accuracy of 85%; however, MobileNet was recommended for the hardware implementation because of its improved response time with 5-fold accuracy comparable to NASNet (i.e., 78%). Comparable performance to standard CNN models was achieved for the case where deep visual features were classified using conventional machine learning approaches. False negative (FN) instances, false positive (FP) instances and CNN layers activation suggested that background noise and oversimplified labelling criteria were two contributing factors in the degraded performance of existing CNN algorithms. A framework for partial automation of the visual blockage classification process was proposed, given that none of the existing models was able to achieve high enough accuracy to completely automate the manual process. In addition, a detection-classification pipeline with higher blockage classification accuracy (i.e., 94%) has been proposed as a potential future direction for practical implementation.},
DOI = {10.3390/app11167561}
}



@Article{s21165578,
AUTHOR = {Trakadas, Panagiotis and Sarakis, Lambros and Giannopoulos, Anastasios and Spantideas, Sotirios and Capsalis, Nikolaos and Gkonis, Panagiotis and Karkazis, Panagiotis and Rigazzi, Giovanni and Antonopoulos, Angelos and Cambeiro, Marta Amor and Gonzalez-Diaz, Sergio and Conceição, Luís},
TITLE = {A Cost-Efficient 5G Non-Public Network Architectural Approach: Key Concepts and Enablers, Building Blocks and Potential Use Cases},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5578},
URL = {https://www.mdpi.com/1424-8220/21/16/5578},
PubMedID = {34451020},
ISSN = {1424-8220},
ABSTRACT = {The provision of high data rate services to mobile users combined with improved quality of experience (i.e., zero latency multimedia content) drives technological evolution towards the design and implementation of fifth generation (5G) broadband wireless networks. To this end, a dynamic network design approach is adopted whereby network topology is configured according to service demands. In parallel, many private companies are interested in developing their own 5G networks, also referred to as non-public networks (NPNs), since this deployment is expected to leverage holistic production monitoring and support critical applications. In this context, this paper introduces a 5G NPN architectural approach, supporting among others various key enabling technologies, such as cell densification, disaggregated RAN with open interfaces, edge computing, and AI/ML-based network optimization. In the same framework, potential applications of our proposed approach in real world scenarios (e.g., support of mission critical services and computer vision analytics for emergencies) are described. Finally, scalability issues are also highlighted since a deployment framework of our architectural design in an additional real-world scenario related to Industry 4.0 (smart manufacturing) is also analyzed.},
DOI = {10.3390/s21165578}
}



@Article{a14080245,
AUTHOR = {Albeshri, Aiiad},
TITLE = {SVSL: A Human Activity Recognition Method Using Soft-Voting and Self-Learning},
JOURNAL = {Algorithms},
VOLUME = {14},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {245},
URL = {https://www.mdpi.com/1999-4893/14/8/245},
ISSN = {1999-4893},
ABSTRACT = {Many smart city and society applications such as smart health (elderly care, medical applications), smart surveillance, sports, and robotics require the recognition of user activities, an important class of problems known as human activity recognition (HAR). Several issues have hindered progress in HAR research, particularly due to the emergence of fog and edge computing, which brings many new opportunities (a low latency, dynamic and real-time decision making, etc.) but comes with its challenges. This paper focuses on addressing two important research gaps in HAR research: (i) improving the HAR prediction accuracy and (ii) managing the frequent changes in the environment and data related to user activities. To address this, we propose an HAR method based on Soft-Voting and Self-Learning (SVSL). SVSL uses two strategies. First, to enhance accuracy, it combines the capabilities of Deep Learning (DL), Generalized Linear Model (GLM), Random Forest (RF), and AdaBoost classifiers using soft-voting. Second, to classify the most challenging data instances, the SVSL method is equipped with a self-training mechanism that generates training data and retrains itself. We investigate the performance of our proposed SVSL method using two publicly available datasets on six human activities related to lying, sitting, and walking positions. The first dataset consists of 562 features and the second dataset consists of five features. The data are collected using the accelerometer and gyroscope smartphone sensors. The results show that the proposed method provides 6.26%, 1.75%, 1.51%, and 4.40% better prediction accuracy (average over the two datasets) compared to GLM, DL, RF, and AdaBoost, respectively. We also analyze and compare the class-wise performance of the SVSL methods with that of DL, GLM, RF, and AdaBoost.},
DOI = {10.3390/a14080245}
}



@Article{su13169351,
AUTHOR = {Cho, Yunji and Song, Jaein and Kang, Minhee and Hwang, Keeyeon},
TITLE = {An Application of a Deep Q-Network Based Dynamic Fare Bidding System to Improve the Use of Taxi Services during Off-Peak Hours in Seoul},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {9351},
URL = {https://www.mdpi.com/2071-1050/13/16/9351},
ISSN = {2071-1050},
ABSTRACT = {The problem of structural imbalance in terms of supply and demand due to changes in traffic patterns by time zone has been continuously raised in the mobility market. In Korea, unlike large overseas cities, the waiting time tolerance increases during the daytime when supply far exceeds demand, resulting in a large loss of operating profit. The purpose of this study is to increase taxi demand and further improve driver’s profits through real-time fare discounts during off-peak daytime hours in Seoul, Korea. To this end, we propose a real-time fare bidding system among taxi drivers based on a dynamic pricing scheme and simulate the appropriate fare discount level for each regional time zone. The driver-to-driver fare competition system consists of simulating fare competition based on the multi-agent Deep Q-Network method after developing a fare discount index that reflects the supply and demand level of each region in 25 districts in Seoul. According to the optimal fare discount level analysis in the off-peak hours, the lower the OI Index, which means the level of demand relative to supply, the higher the fare discount rate. In addition, an analysis of drivers’ profits and matching rates according to the distance between the origin and destination of each region showed up to 89% and 65% of drivers who actively offered discounts on fares. The results of this study in the future can serve as the foundation of a fare adjustment system for varying demand and supply situations in the Korean mobility market.},
DOI = {10.3390/su13169351}
}



@Article{s21165619,
AUTHOR = {Mendoza, Jessica and de-la-Bandera, Isabel and Palacios, David and Barco, Raquel},
TITLE = {QoE Optimization in a Live Cellular Network through RLC Parameter Tuning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5619},
URL = {https://www.mdpi.com/1424-8220/21/16/5619},
PubMedID = {34451060},
ISSN = {1424-8220},
ABSTRACT = {The mobile communication networks sector has experienced a great evolution during the last few years. The emergence of new services as well as the growth in the number of subscribers have motivated the search for new ways to optimize mobile networks. In this way, the objective pursued by optimization techniques has been evolving, shifting from the traditional optimization of radio parameters to the improvement of the quality perceived by users, known as quality of experience (QoE). In mobile networks, the radio link control (RLC) layer provides a reliable link between both ends of the communication and has a great impact on the QoE. In this paper, the optimization of the QoE for users based on the adjustment of the RLC layer is proposed. For this purpose, two typical services demanded by the users of mobile networks have been selected: the real-time video streaming service and file transfer service. For a broader view of the behavior of the QoE in relation to RLC, optimization tests have been carried out in scenarios with different system bandwidths. In this way, the relationship between the QoE and the optimal configuration of RLC in different network load situations has been analyzed. A proof of concept has been carried out to show the capability of this optimization. To that end, both a cellular network simulator and a live cellular network devised for research purposes have been used.},
DOI = {10.3390/s21165619}
}



@Article{fi13080218,
AUTHOR = {Ghazal, Taher M. and Hasan, Mohammad Kamrul and Alshurideh, Muhammad Turki and Alzoubi, Haitham M. and Ahmad, Munir and Akbar, Syed Shehryar and Al Kurdi, Barween and Akour, Iman A.},
TITLE = {IoT for Smart Cities: Machine Learning Approaches in Smart Healthcare—A Review},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {218},
URL = {https://www.mdpi.com/1999-5903/13/8/218},
ISSN = {1999-5903},
ABSTRACT = {Smart city is a collective term for technologies and concepts that are directed toward making cities efficient, technologically more advanced, greener and more socially inclusive. These concepts include technical, economic and social innovations. This term has been tossed around by various actors in politics, business, administration and urban planning since the 2000s to establish tech-based changes and innovations in urban areas. The idea of the smart city is used in conjunction with the utilization of digital technologies and at the same time represents a reaction to the economic, social and political challenges that post-industrial societies are confronted with at the start of the new millennium. The key focus is on dealing with challenges faced by urban society, such as environmental pollution, demographic change, population growth, healthcare, the financial crisis or scarcity of resources. In a broader sense, the term also includes non-technical innovations that make urban life more sustainable. So far, the idea of using IoT-based sensor networks for healthcare applications is a promising one with the potential of minimizing inefficiencies in the existing infrastructure. A machine learning approach is key to successful implementation of the IoT-powered wireless sensor networks for this purpose since there is large amount of data to be handled intelligently. Throughout this paper, it will be discussed in detail how AI-powered IoT and WSNs are applied in the healthcare sector. This research will be a baseline study for understanding the role of the IoT in smart cities, in particular in the healthcare sector, for future research works.},
DOI = {10.3390/fi13080218}
}



@Article{s21165660,
AUTHOR = {Santos, Brena and Soares, André and Nguyen, Tuan-Anh and Min, Dug-Ki and Lee, Jae-Woo and Silva, Francisco-Airton},
TITLE = {IoT Sensor Networks in Smart Buildings: A Performance Assessment Using Queuing Models},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5660},
URL = {https://www.mdpi.com/1424-8220/21/16/5660},
PubMedID = {34451103},
ISSN = {1424-8220},
ABSTRACT = {Smart buildings in big cities are now equipped with an internet of things (IoT) infrastructure to constantly monitor different aspects of people’s daily lives via IoT devices and sensor networks. The malfunction and low quality of service (QoS) of such devices and networks can severely cause property damage and perhaps loss of life. Therefore, it is important to quantify different metrics related to the operational performance of the systems that make up such computational architecture even in advance of the building construction. Previous studies used analytical models considering different aspects to assess the performance of building monitoring systems. However, some critical points are still missing in the literature, such as (i) analyzing the capacity of computational resources adequate to the data demand, (ii) representing the number of cores per machine, and (iii) the clustering of sensors by location. This work proposes a queuing network based message exchange architecture to evaluate the performance of an intelligent building infrastructure associated with multiple processing layers: edge and fog. We consider an architecture of a building that has several floors and several rooms in each of them, where all rooms are equipped with sensors and an edge device. A comprehensive sensitivity analysis of the model was performed using the Design of Experiments (DoE) method to identify bottlenecks in the proposal. A series of case studies were conducted based on the DoE results. The DoE results allowed us to conclude, for example, that the number of cores can have more impact on the response time than the number of nodes. Simulations of scenarios defined through DoE allow observing the behavior of the following metrics: average response time, resource utilization rate, flow rate, discard rate, and the number of messages in the system. Three scenarios were explored: (i) scenario A (varying the number of cores), (ii) scenario B (varying the number of fog nodes), and (iii) scenario C (varying the nodes and cores simultaneously). Depending on the number of resources (nodes or cores), the system can become so overloaded that no new requests are supported. The queuing network based message exchange architecture and the analyses carried out can help system designers optimize their computational architectures before building construction.},
DOI = {10.3390/s21165660}
}



@Article{en14175286,
AUTHOR = {Akpudo, Ugochukwu Ejike and Hur, Jang-Wook},
TITLE = {D-dCNN: A Novel Hybrid Deep Learning-Based Tool for Vibration-Based Diagnostics},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5286},
URL = {https://www.mdpi.com/1996-1073/14/17/5286},
ISSN = {1996-1073},
ABSTRACT = {This paper develops a novel hybrid feature learner and classifier for vibration-based fault detection and isolation (FDI) of industrial apartments. The trained model extracts high-level discriminative features from vibration signals and predicts equipment state. Against the limitations of traditional machine learning (ML)-based classifiers, the convolutional neural network (CNN) and deep neural network (DNN) are not only superior for real-time applications, but they also come with other benefits including ease-of-use, automated feature learning, and higher predictive accuracies. This study proposes a hybrid DNN and one-dimensional CNN diagnostics model (D-dCNN) which automatically extracts high-level discriminative features from vibration signals for FDI. Via Softmax averaging at the output layer, the model mitigates the limitations of the standalone classifiers. A diagnostic case study demonstrates the efficiency of the model with a significant accuracy of 92% (F1 score) and extensive comparative empirical validations.},
DOI = {10.3390/en14175286}
}



@Article{app11177900,
AUTHOR = {Maharjan, Manisha and Tamrakar, Ujjwol and Ni, Zhen and Bhattarai, Bishnu and Tonkoski, Reinaldo},
TITLE = {Overvoltage Prevention and Curtailment Reduction Using Adaptive Droop-Based Supplementary Control in Smart Inverters},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {7900},
URL = {https://www.mdpi.com/2076-3417/11/17/7900},
ISSN = {2076-3417},
ABSTRACT = {Recent developments in the renewable energy sector have seen an unprecedented growth in residential photovoltaic (PV) installations. However, high PV penetration levels often lead to overvoltage problems in low-voltage (LV) distribution feeders. Smart inverter control such as active power curtailment (APC)-based overvoltage control can be implemented to overcome these challenges. The APC technique utilizes a constant droop-based approach which curtails power rigidly, which can lead to significant energy curtailment in the LV distribution feeders. In this paper, different variations of the APC technique with linear, quadratic, and exponential droops have been analyzed from the point-of-view of energy curtailment for a LV distribution network in North America. Further, a combinatorial approach using various droop-based APC methods in conjunction with adaptive dynamic programming (ADP) as a supplementary control scheme has also been proposed. The proposed approach minimizes energy curtailment in the LV distribution network by adjusting the droop gains. Simulation results depict that ADP in conjunction with exponential droop reduces the energy curtailment to approximately 50% compared to using the standard linear droop.},
DOI = {10.3390/app11177900}
}



@Article{app11177976,
AUTHOR = {Amicone, Donatello and Cannas, Andrea and Marci, Alberto and Tortora, Giuseppe},
TITLE = {A Smart Capsule Equipped with Artificial Intelligence for Autonomous Delivery of Medical Material through Drones},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {7976},
URL = {https://www.mdpi.com/2076-3417/11/17/7976},
ISSN = {2076-3417},
ABSTRACT = {In the last few years, many examples of blood and medicine delivery drones have been demonstrated worldwide, which mainly rely on aeronautical experience that is not common in the medical world. Speaking about drone delivery, attention should focus on the most important thing: the transported lifesaving good. Traditional boxes that monitor temperature are not usually in real time, and are not suitable for drone transportation because they are heavy and bulky. This means that the biomedical characteristics of delivery are of primary importance. A Smart Capsule, equipped with artificial intelligence (AI), is the first system ever proposed to provide a fully autonomous drone delivery service for perishable and high-value medical products, integrating real-time quality monitoring and control. It consists in a smart casing that is able to guide any autonomous aerial vehicle attached to it, specifically designed for transporting blood, organs, tissues, test samples and drugs, among others. The system monitors the conditions of the product (e.g., temperature, agitation and humidity) and adjusts them when needed by exploiting, for instance, vibrations to maintain the required agitation, ensuring that goods are ready to be used as soon as they are delivered. The Smart Capsule also leverages external temperature to reduce energy uptake from the drone, thus improving the drone’s battery life and flight range. The system replaces the need for specialized drivers and traditional road-bound transportation means, while guaranteeing compliance with all applicable safety regulations. A series of 16 experimental tests was performed to demonstrate the possibility of using the smart capsule to manage the flight and internal good delivery. Eighty-one missions were carried out for a total of 364 min of flight. The Smart Capsule greatly improves emergency response and efficiency of healthcare systems by reducing delivery times by up to 80% and costs by at least 28%. The Smart Capsule and its enabling technology based on AI for drone deliveries are discussed in this paper. The aim of this work is to show the possibility of managing drone delivery with an AI-based device.},
DOI = {10.3390/app11177976}
}



@Article{electronics10172155,
AUTHOR = {Ruan, Jinjia and Xie, Dongliang},
TITLE = {A Survey on QoE-Oriented VR Video Streaming: Some Research Issues and Challenges},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2155},
URL = {https://www.mdpi.com/2079-9292/10/17/2155},
ISSN = {2079-9292},
ABSTRACT = {With the advent of the information age, VR video streaming services have emerged in large numbers in scenarios such as immersive entertainment, smart education, and the Internet of Vehicles. People are also demanding an increasing number of virtual-reality (VR) services, and service providers must ensure a good user experience. Therefore, the quality of the VR user’s experience is receiving increasing attention from academia and industry. The review in this paper focuses on a comprehensive summary of the current state of quality-of-experience (QoE) technologies applied to VR video streaming. First, we review the main influencing factors of QoE and VR video streaming. Second, the user QoE for VR evaluation is discussed. Third, the modeling of QoE for VR video streaming, the QoE-oriented VR optimization problem, and enabling techniques of machine learning for VR video streaming improvement are summarized. Lastly, we present current challenges and possible future research directions.},
DOI = {10.3390/electronics10172155}
}



@Article{signals2030035,
AUTHOR = {Lami, Ihsan and Abdulkhudhur, Alnoman},
TITLE = {DXN: Dynamic AI-Based Analysis and Optimisation of IoT Networks’ Connectivity and Sensor Nodes’ Performance},
JOURNAL = {Signals},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {570--585},
URL = {https://www.mdpi.com/2624-6120/2/3/35},
ISSN = {2624-6120},
ABSTRACT = {Most IoT networks implement one-way messages from the sensor nodes to the “application host server” via a gateway. Messages from any sensor node in the network are sent when its sensor is triggered or at regular intervals as dictated by the application, such as a Smart-City deployment of LoRaWAN traps/sensors for rat detection. However, these traps can, due to the nature of this application, be moved out of signal range from their original location, or obstructed by objects, resulting in under 69% of the messages reaching the gateway. Therefore, applications of this type would benefit from control messages from the “application host server” back to the sensor nodes for enhancing their performance/connectivity. This paper has implemented a cloud-based AI engine, as part of the “application host server”, that dynamically analyses all received messages from the sensor nodes and exchanges data/enhancement back and forth with them, when necessary. Hundreds of sensor nodes in various blocked/obstructed IoT network connectivity scenarios are used to test our DXN solution. We achieved 100% reporting success if access to any blocked sensor node was possible via a neighbouring node. DXN is based on DNN and Time Series models.},
DOI = {10.3390/signals2030035}
}



@Article{su131810026,
AUTHOR = {Tavera Romero, Carlos Andrés and Ortiz, Jesús Hamilton and Khalaf, Osamah Ibrahim and Ríos Prado, Andrea},
TITLE = {Business Intelligence: Business Evolution after Industry 4.0},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {10026},
URL = {https://www.mdpi.com/2071-1050/13/18/10026},
ISSN = {2071-1050},
ABSTRACT = {Industry 4.0 is a set of technologies that companies require to promote innovation strategies and obtain a rapid response in dynamic markets. It focuses mainly on interconnectivity, digital technology, predictive analytics and machine learning to revolutionize the way companies operate and develop. Therefore, this article proposes and motivates the implementation of Industry 4.0 in organizations. Studying the state of the art and reviewing the current situation of business intelligence (BI) technology, the way it has positively impacted organizations at the economic and business level in terms of decision-making and some success stories implemented in different business, academic, social and governmental environments. Moreover, it addresses the future expected for Industry 4.0 primarily in BI and how companies should face this revolution. This article provides knowledge contribution about the current state and positive consequences of Industry 4.0, and high development in technology when implemented in the organization and the harmonization between production and intelligent digital technology.},
DOI = {10.3390/su131810026}
}



@Article{fi13090232,
AUTHOR = {Jaramillo-Ramirez, Daniel and Perez, Manuel},
TITLE = {Spectrum Demand Forecasting for IoT Services},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {232},
URL = {https://www.mdpi.com/1999-5903/13/9/232},
ISSN = {1999-5903},
ABSTRACT = {The evolution of IoT has come with the challenge of connecting not only a massive number of devices, but also providing an always wider variety of services. In the next few years, a big increase in the number of connected devices is expected, together with an important increase in the amount of traffic generated. Never before have wireless communications permeated so deeply in all industries and economic sectors. Therefore, it is crucial to correctly forecast the spectrum needs, which bands should be used for which services, and the economic potential of its utilization. This paper proposes a methodology for spectrum forecasting consisting of two phases: a market study and a spectrum forecasting model. The market study determines the main drivers of the IoT industry for any country: services, technologies, frequency bands, and the number of devices that will require IoT connectivity. The forecasting model takes the market study as the input and calculates the spectrum demand in 5 steps: Defining scenarios for spectrum contention, calculating the offered traffic load, calculating a capacity for some QoS requirements, finding the spectrum required, and adjusting according to key spectral efficiency determinants. This methodology is applied for Colombia’s IoT spectrum forecast. We provide a complete step-by-step implementation in fourteen independent spectrum contention scenarios, calculating offered traffic, required capacity, and spectrum for cellular licensed bands and non-cellular unlicensed bands in a 10-year period. Detailed results are presented specifying coverage area requirements per economic sector, frequency band, and service. The need for higher teledensity and higher spectral efficiency turns out to be a determining factor for spectrum savings.},
DOI = {10.3390/fi13090232}
}



@Article{s21186023,
AUTHOR = {Shah, Sayed Khushal and Tariq, Zeenat and Lee, Jeehwan and Lee, Yugyung},
TITLE = {Event-Driven Deep Learning for Edge Intelligence (EDL-EI)},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6023},
URL = {https://www.mdpi.com/1424-8220/21/18/6023},
PubMedID = {34577228},
ISSN = {1424-8220},
ABSTRACT = {Edge intelligence (EI) has received a lot of interest because it can reduce latency, increase efficiency, and preserve privacy. More significantly, as the Internet of Things (IoT) has proliferated, billions of portable and embedded devices have been interconnected, producing zillions of gigabytes on edge networks. Thus, there is an immediate need to push AI (artificial intelligence) breakthroughs within edge networks to achieve the full promise of edge data analytics. EI solutions have supported digital technology workloads and applications from the infrastructure level to edge networks; however, there are still many challenges with the heterogeneity of computational capabilities and the spread of information sources. We propose a novel event-driven deep-learning framework, called EDL-EI (event-driven deep learning for edge intelligence), via the design of a novel event model by defining events using correlation analysis with multiple sensors in real-world settings and incorporating multi-sensor fusion techniques, a transformation method for sensor streams into images, and lightweight 2-dimensional convolutional neural network (CNN) models. To demonstrate the feasibility of the EDL-EI framework, we presented an IoT-based prototype system that we developed with multiple sensors and edge devices. To verify the proposed framework, we have a case study of air-quality scenarios based on the benchmark data provided by the USA Environmental Protection Agency for the most polluted cities in South Korea and China. We have obtained outstanding predictive accuracy (97.65% and 97.19%) from two deep-learning models on the cities’ air-quality patterns. Furthermore, the air-quality changes from 2019 to 2020 have been analyzed to check the effects of the COVID-19 pandemic lockdown.},
DOI = {10.3390/s21186023}
}



@Article{s21196340,
AUTHOR = {Huang, Ziqi and Shen, Yang and Li, Jiayi and Fey, Marcel and Brecher, Christian},
TITLE = {A Survey on AI-Driven Digital Twins in Industry 4.0: Smart Manufacturing and Advanced Robotics},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6340},
URL = {https://www.mdpi.com/1424-8220/21/19/6340},
PubMedID = {34640660},
ISSN = {1424-8220},
ABSTRACT = {Digital twin (DT) and artificial intelligence (AI) technologies have grown rapidly in recent years and are considered by both academia and industry to be key enablers for Industry 4.0. As a digital replica of a physical entity, the basis of DT is the infrastructure and data, the core is the algorithm and model, and the application is the software and service. The grounding of DT and AI in industrial sectors is even more dependent on the systematic and in-depth integration of domain-specific expertise. This survey comprehensively reviews over 300 manuscripts on AI-driven DT technologies of Industry 4.0 used over the past five years and summarizes their general developments and the current state of AI-integration in the fields of smart manufacturing and advanced robotics. These cover conventional sophisticated metal machining and industrial automation as well as emerging techniques, such as 3D printing and human–robot interaction/cooperation. Furthermore, advantages of AI-driven DTs in the context of sustainable development are elaborated. Practical challenges and development prospects of AI-driven DTs are discussed with a respective focus on different levels. A route for AI-integration in multiscale/fidelity DTs with multiscale/fidelity data sources in Industry 4.0 is outlined.},
DOI = {10.3390/s21196340}
}



@Article{infrastructures6100138,
AUTHOR = {Borges, Fábio de Souza Pereira and Fonseca, Adelayda Pallavicini and Garcia, Reinaldo Crispiniano},
TITLE = {Deep Reinforcement Learning Model to Mitigate Congestion in Real-Time Traffic Light Networks},
JOURNAL = {Infrastructures},
VOLUME = {6},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {138},
URL = {https://www.mdpi.com/2412-3811/6/10/138},
ISSN = {2412-3811},
ABSTRACT = {Urban traffic congestion has a significant detrimental impact on the environment, public health and the economy, with at a high cost to society worldwide. Moreover, it is not possible to continually modify urban road infrastructure in order to mitigate increasing traffic demand. Therefore, it is important to develop traffic control models that can handle high-volume traffic data and synchronize traffic lights in an urban network in real time, without interfering with other initiatives. Within this context, this study proposes a model, based on deep reinforcement learning, for synchronizing the traffic signals of an urban traffic network composed of two intersections. The calibration of this model, including training of its neural network, was performed using real traffic data collected at the approach to each intersection. The results achieved through simulations were very promising, yielding significant improvements in indicators measured in relation to the pre-existing conditions in the network. The model was able to deal with a broad spectrum of traffic flows and, in peak demand periods, reduced delays and queue lengths by more than 28% and 42%, respectively.},
DOI = {10.3390/infrastructures6100138}
}



@Article{s21196447,
AUTHOR = {Long, Keliu and Nsalo Kong, Darryl Franck and Zhang, Kun and Tian, Chuan and Shen, Chong},
TITLE = {A CSI-Based Indoor Positioning System Using Single UWB Ranging Correction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6447},
URL = {https://www.mdpi.com/1424-8220/21/19/6447},
PubMedID = {34640767},
ISSN = {1424-8220},
ABSTRACT = {A fingerprint-based localization system is an economic way to solve an indoor positioning problem. However, the traditional off-line fingerprint collection stage is a time-consuming and laborious process which limits the use of fingerprint-based localization systems. In this paper, based on ubiquitous Wireless Fidelity (Wi-Fi) equipment and a low-cost Ultra-Wideband (UWB) ranging system (with only one UWB anchor), a ready-to-use indoor localization system is proposed to realize long-term and high-accuracy indoor positioning. More specifically, in this system, it is divided into two stages: (1) an initial stage, and (2) a positioning stage. In the initial stage, an Inertial Measure Unit (IMU) is used to calculate the position using Pedestrian Dead Reckon (PDR) algorithm within a preset number of steps, and the location-related fingerprints are collected to train a Convolutional Neural Network (CNN) regression model; simultaneously, in order to make the UWB ranging system adapt to the Non-Line-of-Sight (NLoS) environment, the increments of acceleration and angular velocity in IMU and the increments of single UWB ranging measures are correlated to pre-train a Supported Vector Regression (SVR). After reaching the threshold of time or step number, the system is changed into a positioning stage, and the CNN predicts the position calibrated by corrected UWB ranging. At last, a series of practical experiments are conducted in the real environment; the experiment results show that, due to the corrected UWB ranging measures calibrating the CNN parameters in every positioning period, this system has stable localization results in a comparative long-term range. Additionally, it has the advantages of stability, low cost, anti-noise, etc.},
DOI = {10.3390/s21196447}
}



@Article{electronics10192363,
AUTHOR = {Hurtado-Gómez, Julián and Romo, Juan David and Salazar-Cabrera, Ricardo and Pachón de la Cruz, Álvaro and Madrid Molina, Juan Manuel},
TITLE = {Traffic Signal Control System Based on Intelligent Transportation System and Reinforcement Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {2363},
URL = {https://www.mdpi.com/2079-9292/10/19/2363},
ISSN = {2079-9292},
ABSTRACT = {Traffic congestion has several causes, including insufficient road capacity, unrestricted demand and improper scheduling of traffic signal phases. A great variety of efforts have been made to properly program such phases. Some of them are based on traditional transportation assumptions, and others are adaptive, allowing the system to learn the control law (signal program) from data obtained from different sources. Reinforcement Learning (RL) is a technique commonly used in previous research. However, properly determining the states and the reward is key to obtain good results and to have a real chance to implement it. This paper proposes and implements a traffic signal control system (TSCS), detailing its development stages: (a) Intelligent Transportation System (ITS) architecture design for the TSCS; (b) design and development of a system prototype, including an RL algorithm to minimize the vehicle queue at intersections, and detection and calculation of such queues by adapting a computer vision algorithm; and (c) design and development of system tests to validate operation of the algorithms and the system prototype. Results include the development of the tests for each module (vehicle queue measurement and RL algorithm) and real-time integration tests. Finally, the article presents a system simulation in the context of a medium-sized city in a developing country, showing that the proposed system allowed reduction of vehicle queues by 29%, of waiting time by 50%, and of lost time by 50%, when compared to fixed phase times in traffic signals.},
DOI = {10.3390/electronics10192363}
}



@Article{computers10100121,
AUTHOR = {Sánchez-Aguayo, Marco and Urquiza-Aguiar, Luis and Estrada-Jiménez, José},
TITLE = {Fraud Detection Using the Fraud Triangle Theory and Data Mining Techniques: A Literature Review},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {121},
URL = {https://www.mdpi.com/2073-431X/10/10/121},
ISSN = {2073-431X},
ABSTRACT = {Fraud entails deception in order to obtain illegal gains; thus, it is mainly evidenced within financial institutions and is a matter of general interest. The problem is particularly complex, since perpetrators of fraud could belong to any position, from top managers to payroll employees. Fraud detection has traditionally been performed by auditors, who mainly employ manual techniques. These could take too long to process fraud-related evidence. Data mining, machine learning, and, as of recently, deep learning strategies are being used to automate this type of processing. Many related techniques have been developed to analyze, detect, and prevent fraud-related behavior, with the fraud triangle associated with the classic auditing model being one of the most important of these. This work aims to review current work related to fraud detection that uses the fraud triangle in addition to machine learning and deep learning techniques. We used the Kitchenham methodology to analyze the research works related to fraud detection from the last decade. This review provides evidence that fraud is an area of active investigation. Several works related to fraud detection using machine learning techniques were identified without the evidence that they incorporated the fraud triangle as a method for more efficient analysis.},
DOI = {10.3390/computers10100121}
}



@Article{en14196309,
AUTHOR = {Peyman, Mohammad and Copado, Pedro J. and Tordecilla, Rafael D. and Martins, Leandro do C. and Xhafa, Fatos and Juan, Angel A.},
TITLE = {Edge Computing and IoT Analytics for Agile Optimization in Intelligent Transportation Systems},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6309},
URL = {https://www.mdpi.com/1996-1073/14/19/6309},
ISSN = {1996-1073},
ABSTRACT = {With the emergence of fog and edge computing, new possibilities arise regarding the data-driven management of citizens’ mobility in smart cities. Internet of Things (IoT) analytics refers to the use of these technologies, data, and analytical models to describe the current status of the city traffic, to predict its evolution over the coming hours, and to make decisions that increase the efficiency of the transportation system. It involves many challenges such as how to deal and manage real and huge amounts of data, and improving security, privacy, scalability, reliability, and quality of services in the cloud and vehicular network. In this paper, we review the state of the art of IoT in intelligent transportation systems (ITS), identify challenges posed by cloud, fog, and edge computing in ITS, and develop a methodology based on agile optimization algorithms for solving a dynamic ride-sharing problem (DRSP) in the context of edge/fog computing. These algorithms allow us to process, in real time, the data gathered from IoT systems in order to optimize automatic decisions in the city transportation system, including: optimizing the vehicle routing, recommending customized transportation modes to the citizens, generating efficient ride-sharing and car-sharing strategies, create optimal charging station for electric vehicles and different services within urban and interurban areas. A numerical example considering a DRSP is provided, in which the potential of employing edge/fog computing, open data, and agile algorithms is illustrated.},
DOI = {10.3390/en14196309}
}



@Article{app11209360,
AUTHOR = {Li, Kaibin and Peng, Zhiping and Cui, Delong and Li, Qirui},
TITLE = {SLA-DQTS: SLA Constrained Adaptive Online Task Scheduling Based on DDQN in Cloud Computing},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {9360},
URL = {https://www.mdpi.com/2076-3417/11/20/9360},
ISSN = {2076-3417},
ABSTRACT = {Task scheduling is key to performance optimization and resource management in cloud computing systems. Because of its complexity, it has been defined as an NP problem. We introduce an online scheme to solve the problem of task scheduling under a dynamic load in the cloud environment. After analyzing the process, we propose a server level agreement constraint adaptive online task scheduling algorithm based on double deep Q-learning (SLA-DQTS) to reduce the makespan, cost, and average overdue time under the constraints of virtual machine (VM) resources and deadlines. In the algorithm, we prevent the change of the model input dimension with the number of VMs by taking the Gaussian distribution of related parameters as a part of the state space. Through the design of the reward function, the model can be optimized for different goals and task loads. We evaluate the performance of the algorithm by comparing it with three heuristic algorithms (Min-Min, random, and round robin) under different loads. The results show that the algorithm in this paper can achieve similar or better results than the comparison algorithms at a lower cost.},
DOI = {10.3390/app11209360}
}



@Article{su132011331,
AUTHOR = {Ko, Kwangho and Lee, Tongwon and Jeong, Seunghyun},
TITLE = {A Deep Learning Method for Monitoring Vehicle Energy Consumption with GPS Data},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {11331},
URL = {https://www.mdpi.com/2071-1050/13/20/11331},
ISSN = {2071-1050},
ABSTRACT = {A monitoring method for energy consumption of vehicles is proposed in the study. It is necessary to have parameters estimating fuel economy with GPS data obtained while driving in the proposed method. The parameters are trained by fuel consumption data measured with a data logger for the reference cars. The data logger is equipped with a GPS sensor and OBD connection capability. The GPS sensor measures vehicle speed, acceleration rate and road gradient. The OBD connector gathers the fuel consumption signaled from OBD port built in the car. The parameters are trained by a 5-layer deep-learning construction with input data (speed, acceleration, gradient) and labels (fuel consumption data) in the typical classification approach. The number of labels is about 6–8 and the number of neurons for hidden layers increases in proportionate to the label numbers. There are about 160–200 parameters. The parameters are calibrated to consider the wide range of fuel efficiency and deterioration degree in age for various test cars. The calibration factor is made from the certified fuel economy and model year taken from the car registration form. The error range of the estimated fuel economy from the measured value is about −6% to +7% for the eight test cars. It is accurate enough to capture the vehicle dynamics for using the input and output data in point-to-point classification style for training steps. Further, it is simple enough to hit fuel economy of the other test cars because fuel economy is a kind of averaged value of fuel consumption for the time period or driven distance for monitoring steps. You can predict or monitor energy consumption for any vehicle with the GPS-measured speed/acceleration/gradient data by the pre-trained parameters and calibration factors of the reference vehicles according to fuel types such as gasoline, diesel and electric. The proposed method requires just a GPS sensor that is cheap and common, and the calculating procedure is so simple that you can monitor energy consumption of various vehicles in real-time with ease. However, it does not consider weight, weather and auxiliary changes and these effects will be addressed in the future works with a monitoring service system under preparation.},
DOI = {10.3390/su132011331}
}



@Article{iot2040031,
AUTHOR = {Ajayi, Oluwashina Joseph and Rafferty, Joseph and Santos, Jose and Garcia-Constantino, Matias and Cui, Zhan},
TITLE = {BECA: A Blockchain-Based Edge Computing Architecture for Internet of Things Systems},
JOURNAL = {IoT},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {610--632},
URL = {https://www.mdpi.com/2624-831X/2/4/31},
ISSN = {2624-831X},
ABSTRACT = {The scale of Internet of Things (IoT) systems has expanded in recent times and, in tandem with this, IoT solutions have developed symbiotic relationships with technologies, such as edge Computing. IoT has leveraged edge computing capabilities to improve the capabilities of IoT solutions, such as facilitating quick data retrieval, low latency response, and advanced computation, among others. However, in contrast with the benefits offered by edge computing capabilities, there are several detractors, such as centralized data storage, data ownership, privacy, data auditability, and security, which concern the IoT community. This study leveraged blockchain&rsquo;s inherent capabilities, including distributed storage system, non-repudiation, privacy, security, and immutability, to provide a novel, advanced edge computing architecture for IoT systems. Specifically, this blockchain-based edge computing architecture addressed centralized data storage, data auditability, privacy, data ownership, and security. Following implementation, the performance of this solution was evaluated to quantify performance in terms of response time and resource utilization. The results show the viability of the proposed and implemented architecture, characterized by improved privacy, device data ownership, security, and data auditability while implementing decentralized storage.},
DOI = {10.3390/iot2040031}
}



@Article{computers10100130,
AUTHOR = {Kjorveziroski, Vojdan and Filiposka, Sonja and Trajkovik, Vladimir},
TITLE = {IoT Serverless Computing at the Edge: A Systematic Mapping Review},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {130},
URL = {https://www.mdpi.com/2073-431X/10/10/130},
ISSN = {2073-431X},
ABSTRACT = {Serverless computing is a new concept allowing developers to focus on the core functionality of their code, while abstracting away the underlying infrastructure. Even though there are existing commercial serverless cloud providers and open-source solutions, dealing with the explosive growth of new Internet of Things (IoT) devices requires more efficient bandwidth utilization, reduced latency, and data preprocessing closer to the source, thus reducing the overall data volume and meeting privacy regulations. Moving serverless computing to the edge of the network is a topic that is actively being researched with the aim of solving these issues. This study presents a systematic mapping review of current progress made to this effect, analyzing work published between 1 January 2015 and 1 September 2021. Using a document selection methodology which emphasizes the quality of the papers obtained through querying several popular databases with relevant search terms, we have included 64 entries, which we then further categorized into eight main categories. Results show that there is an increasing interest in this area with rapid progress being made to solve the remaining open issues, which have also been summarized in this paper. Special attention is paid to open-source efforts, as well as open-access contributions.},
DOI = {10.3390/computers10100130}
}



@Article{ijgi10100703,
AUTHOR = {You, Lan and Guan, Zhengyi and Li, Na and Zhang, Jiahe and Cui, Haibo and Claramunt, Christophe and Cao, Rui},
TITLE = {A Spatio-Temporal Schedule-Based Neural Network for Urban Taxi Waiting Time Prediction},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {703},
URL = {https://www.mdpi.com/2220-9964/10/10/703},
ISSN = {2220-9964},
ABSTRACT = {Taxi waiting times is an important criterion for taxi passengers to choose appropriate pick-up locations in urban environments. How to predict the taxi waiting time accurately at a certain time and location is the key solution for the imbalance between the taxis’ supplies and demands. Considering the life schedule of urban residents and the different functions of geogrid regions, the research developed in this paper introduces a spatio-temporal schedule-based neural network for urban taxi waiting time prediction. The approach integrates a series of multi-source data from taxi trajectories to city points of interest, different time frames and human behaviors in the city. We apply a grid-based and functional structuration of an urban space that provides a lower-level data representation. Overall, the neural network model can dynamically predict the waiting time of taxi passengers in real time under some given spatio-temporal constraints. The experimental results show that the granular-based grids and spatio-temporal neural network can effectively predict and optimize the accuracy of taxi waiting times. This work provides a decision support for intelligent travel predictions of taxi waiting time in a smart city.},
DOI = {10.3390/ijgi10100703}
}



@Article{bdcc5040056,
AUTHOR = {Hao, Yixue and Miao, Yiming and Chen, Min and Gharavi, Hamid and Leung, Victor C. M.},
TITLE = {6G Cognitive Information Theory: A Mailbox Perspective},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {56},
URL = {https://www.mdpi.com/2504-2289/5/4/56},
ISSN = {2504-2289},
ABSTRACT = {With the rapid development of 5G communications, enhanced mobile broadband, massive machine type communications and ultra-reliable low latency communications are widely supported. However, a 5G communication system is still based on Shannon’s information theory, while the meaning and value of information itself are not taken into account in the process of transmission. Therefore, it is difficult to meet the requirements of intelligence, customization, and value transmission of 6G networks. In order to solve the above challenges, we propose a 6G mailbox theory, namely a cognitive information carrier to enable distributed algorithm embedding for intelligence networking. Based on Mailbox, a 6G network will form an intelligent agent with self-organization, self-learning, self-adaptation, and continuous evolution capabilities. With the intelligent agent, redundant transmission of data can be reduced while the value transmission of information can be improved. Then, the features of mailbox principle are introduced, including polarity, traceability, dynamics, convergence, figurability, and dependence. Furthermore, key technologies with which value transmission of information can be realized are introduced, including knowledge graph, distributed learning, and blockchain. Finally, we establish a cognitive communication system assisted by deep learning. The experimental results show that, compared with a traditional communication system, our communication system performs less data transmission quantity and error.},
DOI = {10.3390/bdcc5040056}
}



@Article{app11209680,
AUTHOR = {Zhou, Xuan and Ke, Ruimin and Yang, Hao and Liu, Chenxi},
TITLE = {When Intelligent Transportation Systems Sensing Meets Edge Computing: Vision and Challenges},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {9680},
URL = {https://www.mdpi.com/2076-3417/11/20/9680},
ISSN = {2076-3417},
ABSTRACT = {The widespread use of mobile devices and sensors has motivated data-driven applications that can leverage the power of big data to benefit many aspects of our daily life, such as health, transportation, economy, and environment. Under the context of smart city, intelligent transportation systems (ITS), as a main building block of modern cities, and edge computing (EC), as an emerging computing service that targets addressing the limitations of cloud computing, have attracted increasing attention in the research community in recent years. It is well believed that the application of EC in ITS will have considerable benefits to transportation systems regarding efficiency, safety, and sustainability. Despite the growing trend in ITS and EC research, a big gap in the existing literature is identified: the intersection between these two promising directions has been far from well explored. In this paper, we focus on a critical part of ITS, i.e., sensing, and conducting a review on the recent advances in ITS sensing and EC applications in this field. The key challenges in ITS sensing and future directions with the integration of edge computing are discussed.},
DOI = {10.3390/app11209680}
}



@Article{educsci11110666,
AUTHOR = {da Silva, Lidia Martins and Dias, Lucas Pfeiffer Salomão and Rigo, Sandro and Barbosa, Jorge Luis Victória and Leithardt, Daiana R. F. and Leithardt, Valderi Reis Quietinho},
TITLE = {A Literature Review on Intelligent Services Applied to Distance Learning},
JOURNAL = {Education Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {666},
URL = {https://www.mdpi.com/2227-7102/11/11/666},
ISSN = {2227-7102},
ABSTRACT = {Distance learning has assumed a relevant role in the educational scenario. The use of Virtual Learning Environments contributes to obtaining a substantial amount of educational data. In this sense, the analyzed data generate knowledge used by institutions to assist managers and professors in strategic planning and teaching. The discovery of students’ behaviors enables a wide variety of intelligent services for assisting in the learning process. This article presents a literature review in order to identify the intelligent services applied in distance learning. The research covers the period from January 2010 to May 2021. The initial search found 1316 articles, among which 51 were selected for further studies. Considering the selected articles, 33% (17/51) focus on learning systems, 35% (18/51) propose recommendation systems, 26% (13/51) approach predictive systems or models, and 6% (3/51) use assessment tools. This review allowed for the observation that the principal services offered are recommendation systems and learning systems. In these services, the analysis of student profiles stands out to identify patterns of behavior, detect low performance, and identify probabilities of dropouts from courses.},
DOI = {10.3390/educsci11110666}
}



@Article{smartcities4040072,
AUTHOR = {Carneiro, Davide and Amaral, António and Carvalho, Mariana and Barreto, Luís},
TITLE = {An Anthropocentric and Enhanced Predictive Approach to Smart City Management},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {4},
PAGES = {1366--1390},
URL = {https://www.mdpi.com/2624-6511/4/4/72},
ISSN = {2624-6511},
ABSTRACT = {Cities are becoming increasingly complex to manage, as they increase in size and must provide higher living standards for their populations. New technology-based solutions must be developed towards attending this growth and ensuring that it is socially sustainable. This paper puts forward the notion that these solutions must share some properties: they should be anthropocentric, holistic, horizontal, multi-dimensional, multi-modal, and predictive. We propose an architecture in which streaming data sources that characterize the city context are used to feed a real-time graph of the city’s assets and states, as well as to train predictive models that hint into near future states of the city. This allows human decision-makers and automated services to take decisions, both for the present and for the future. To achieve this, multiple data sources about a city were gradually connected to a message broker, that enables increasingly rich decision-support. Results show that it is possible to predict future states of a city, in aspects such as traffic, air pollution, and other ambient variables. The key innovative aspect of this work is that, as opposed to the majority of existing approaches which focus on a real-time view of the city, we also provide insights into the near-future state of the city, thus allowing city services to plan ahead and adapt accordingly. The main goal is to optimize decision-making by anticipating future states of the city and make decisions accordingly.},
DOI = {10.3390/smartcities4040072}
}



@Article{s21217053,
AUTHOR = {Mobasheri, Motahareh and Kim, Yangwoo and Kim, Woongsup},
TITLE = {Toward an Adaptive Threshold on Cooperative Bandwidth Management Based on Hierarchical Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7053},
URL = {https://www.mdpi.com/1424-8220/21/21/7053},
PubMedID = {34770360},
ISSN = {1424-8220},
ABSTRACT = {With the increase in Internet of Things (IoT) devices and network communications, but with less bandwidth growth, the resulting constraints must be overcome. Due to the network complexity and uncertainty of emergency distribution parameters in smart environments, using predetermined rules seems illogical. Reinforcement learning (RL), as a powerful machine learning approach, can handle such smart environments without a trainer or supervisor. Recently, we worked on bandwidth management in a smart environment with several fog fragments using limited shared bandwidth, where IoT devices may experience uncertain emergencies in terms of the time and sequence needed for more bandwidth for further higher-level communication. We introduced fog fragment cooperation using an RL approach under a predefined fixed threshold constraint. In this study, we promote this approach by removing the fixed level of restriction of the threshold through hierarchical reinforcement learning (HRL) and completing the cooperation qualification. At the first learning hierarchy level of the proposed approach, the best threshold level is learned over time, and the final results are used by the second learning hierarchy level, where the fog node learns the best device for helping an emergency device by temporarily lending the bandwidth. Although equipping the method to the adaptive threshold and restricting fog fragment cooperation make the learning procedure more difficult, the HRL approach increases the method’s efficiency in terms of time and performance.},
DOI = {10.3390/s21217053}
}



@Article{su132111772,
AUTHOR = {Nawaz, Afifa and Zafar, Nazir Ahmad and Alkhammash, Eman H.},
TITLE = {Formal Modeling of Responsive Traffic Signaling System Using Graph Theory and VDM-SL},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {11772},
URL = {https://www.mdpi.com/2071-1050/13/21/11772},
ISSN = {2071-1050},
ABSTRACT = {Internet of things (IoT) is playing a major role in smart cities to make a digital environment. Traffic congestion is a serious road issue because of an increasing number of vehicles in urban areas. Some crucial traffic problems include accidents and traffic jams that cause waste of fuel, health diseases, and a waste of time. Present traffic signaling systems are not efficient in resolving congestion problems because of the lack of traffic signals. Nowadays, traffic signaling systems are modeled with fixed time intervals in which no proper mechanism for emergency vehicles is available. Such traffic mechanisms failed to deal with traffic problems effectively. The major objective is to establish a robust traffic monitoring and signaling system that improves signal efficiency by providing a responsive scheme; appropriate routes; a mechanism for emergency vehicles and pedestrians in real-time using Vienna Development Method Specification Language (VDM-SL) formal method and graph theory. A formal model is constructed by considering objects, such as wireless sensors and cameras that are used for collecting information. Graph theory is used to represent the network and find appropriate routes. Unified Modeling Language is used to design the system requirements. The graph-based framework is converted into a formal model by using VDM-SL. The model has been validated and analyzed using many facilities available in the VDM-SL toolbox.},
DOI = {10.3390/su132111772}
}



@Article{s21217070,
AUTHOR = {Aljabri, Malak and Aljameel, Sumayh S. and Mohammad, Rami Mustafa A. and Almotiri, Sultan H. and Mirza, Samiha and Anis, Fatima M. and Aboulnour, Menna and Alomari, Dorieh M. and Alhamed, Dina H. and Altamimi, Hanan S.},
TITLE = {Intelligent Techniques for Detecting Network Attacks: Review and Research Directions},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7070},
URL = {https://www.mdpi.com/1424-8220/21/21/7070},
PubMedID = {34770375},
ISSN = {1424-8220},
ABSTRACT = {The significant growth in the use of the Internet and the rapid development of network technologies are associated with an increased risk of network attacks. Network attacks refer to all types of unauthorized access to a network including any attempts to damage and disrupt the network, often leading to serious consequences. Network attack detection is an active area of research in the community of cybersecurity. In the literature, there are various descriptions of network attack detection systems involving various intelligent-based techniques including machine learning (ML) and deep learning (DL) models. However, although such techniques have proved useful within specific domains, no technique has proved useful in mitigating all kinds of network attacks. This is because some intelligent-based approaches lack essential capabilities that render them reliable systems that are able to confront different types of network attacks. This was the main motivation behind this research, which evaluates contemporary intelligent-based research directions to address the gap that still exists in the field. The main components of any intelligent-based system are the training datasets, the algorithms, and the evaluation metrics; these were the main benchmark criteria used to assess the intelligent-based systems included in this research article. This research provides a rich source of references for scholars seeking to determine their scope of research in this field. Furthermore, although the paper does present a set of suggestions about future inductive directions, it leaves the reader free to derive additional insights about how to develop intelligent-based systems to counter current and future network attacks.},
DOI = {10.3390/s21217070}
}



@Article{su132111840,
AUTHOR = {Khanna, Abhirup and Sah, Anushree and Bolshev, Vadim and Jasinski, Michal and Vinogradov, Alexander and Leonowicz, Zbigniew and Jasiński, Marek},
TITLE = {Blockchain: Future of e-Governance in Smart Cities},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {11840},
URL = {https://www.mdpi.com/2071-1050/13/21/11840},
ISSN = {2071-1050},
ABSTRACT = {In recent times, Blockchain has emerged as a transformational technology with the ability to disrupt and evolve multiple domains. As a decentralized, immutable distributed ledger, Blockchain technology is one of the most recent entrants to the comprehensive ideology of Smart Cities. The rise of urbanization and increased citizen participation have led to various technology integrations in our present-day cities. For cities to become smart, we need standard frameworks and procedures for integrating technology, citizens and governments. In this paper, we explore the potential of Blockchain technology as an enabler for e-governance in smart cities. We examine the daily challenges of citizens and compare them with the benefits being offered by Blockchain integration. On the basis of a comprehensive literature review, we identified four key areas of e-governance wherein Blockchain can provide monumental advantages. In the context of Blockchain integration for e-governance, the paper presents a survey of prominent published works discussing various urban applications.},
DOI = {10.3390/su132111840}
}



@Article{info12110447,
AUTHOR = {Solter, Alex and Lin, Fuhua and Wen, Dunwei and Zhou, Xiaokang},
TITLE = {Data-Driven Multi-Agent Vehicle Routing in a Congested City},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {447},
URL = {https://www.mdpi.com/2078-2489/12/11/447},
ISSN = {2078-2489},
ABSTRACT = {Navigation in a traffic congested city can prove to be a difficult task. Often a path that may appear to be the fastest option is much slower due to congestion. If we can predict the effects of congestion, it may be possible to develop a better route that allows us to reach our destination more quickly. This paper studies the possibility of using a centralized real-time traffic information system containing travel time data collected from each road user. These data are made available to all users, such that they may be able to learn and predict the effects of congestion for building a route adaptively. This method is further enhanced by combining the traffic information system data with previous routing experiences to determine the fastest route with less exploration. We test our method using a multi-agent simulation, demonstrating that this method produces a lower total route time for all vehicles than when using either a centralized traffic information system or direct experience alone.},
DOI = {10.3390/info12110447}
}



@Article{make3040043,
AUTHOR = {Xiang, Xuanchen and Foo, Simon and Zang, Huanyu},
TITLE = {Recent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems Part 2—Applications in Transportation, Industries, Communications and Networking and More Topics},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {3},
YEAR = {2021},
NUMBER = {4},
PAGES = {863--878},
URL = {https://www.mdpi.com/2504-4990/3/4/43},
ISSN = {2504-4990},
ABSTRACT = {The two-part series of papers provides a survey on recent advances in Deep Reinforcement Learning (DRL) for solving partially observable Markov decision processes (POMDP) problems. Reinforcement Learning (RL) is an approach to simulate the human’s natural learning process, whose key is to let the agent learn by interacting with the stochastic environment. The fact that the agent has limited access to the information of the environment enables AI to be applied efficiently in most fields that require self-learning. It’s essential to have an organized investigation—we can make good comparisons and choose the best structures or algorithms when applying DRL in various applications. The first part of the overview introduces Markov Decision Processes (MDP) problems and Reinforcement Learning and applications of DRL for solving POMDP problems in games, robotics, and natural language processing. In part two, we continue to introduce applications in transportation, industries, communications and networking, etc. and discuss the limitations of DRL.},
DOI = {10.3390/make3040043}
}



@Article{heritage4040222,
AUTHOR = {McLennan, Sam and Brown, Andre},
TITLE = {A Smart Heritage System to Re-Generate New Zealand’s 19th Century Timber Churches},
JOURNAL = {Heritage},
VOLUME = {4},
YEAR = {2021},
NUMBER = {4},
PAGES = {4040--4055},
URL = {https://www.mdpi.com/2571-9408/4/4/222},
ISSN = {2571-9408},
ABSTRACT = {This article describes a Smart Heritage computational system that automatically produces a wide range of design proposals for new timber Gothic churches based on an intelligent interpretation of an architectural database of historic churches. The system enlists the software ‘Houdini’ and a digitally archived dataset of 19th Century timber Gothic churches. The cases presented here focus primarily on timber churches built in Wellington, New Zealand. Through a process of analysis and deconstruction of these historic churches into their characteristic architectural components, spatial organisation and geometric relationships, the system assembles them into novel designs based on high-level design parameters. This paper details this computational system, its development, its operation and its outputs. The role of the system that has been developed is two-fold. One is designing in an architectural heritage context, and one is as an aid to historical architectural investigations, or what can be called digital forensics. The particular outputs are automatically generated hybrid churches that capture the historical design values and complexities of Gothic inspired churches in New Zealand. However, the broader applications are as an investigative tool for historians, and as an objective generative tool for those involved in heritage reconstruction.},
DOI = {10.3390/heritage4040222}
}



@Article{sym13112040,
AUTHOR = {Serey, Joel and Quezada, Luis and Alfaro, Miguel and Fuertes, Guillermo and Vargas, Manuel and Ternero, Rodrigo and Sabattin, Jorge and Duran, Claudia and Gutierrez, Sebastian},
TITLE = {Artificial Intelligence Methodologies for Data Management},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2040},
URL = {https://www.mdpi.com/2073-8994/13/11/2040},
ISSN = {2073-8994},
ABSTRACT = {This study analyses the main challenges, trends, technological approaches, and artificial intelligence methods developed by new researchers and professionals in the field of machine learning, with an emphasis on the most outstanding and relevant works to date. This literature review evaluates the main methodological contributions of artificial intelligence through machine learning. The methodology used to study the documents was content analysis; the basic terminology of the study corresponds to machine learning, artificial intelligence, and big data between the years 2017 and 2021. For this study, we selected 181 references, of which 120 are part of the literature review. The conceptual framework includes 12 categories, four groups, and eight subgroups. The study of data management using AI methodologies presents symmetry in the four machine learning groups: supervised learning, unsupervised learning, semi-supervised learning, and reinforced learning. Furthermore, the artificial intelligence methods with more symmetry in all groups are artificial neural networks, Support Vector Machines, K-means, and Bayesian Methods. Finally, five research avenues are presented to improve the prediction of machine learning.},
DOI = {10.3390/sym13112040}
}



@Article{en14217141,
AUTHOR = {Zouloumis, Leonidas and Stergianakos, Georgios and Ploskas, Nikolaos and Panaras, Giorgos},
TITLE = {Dynamic Simulation-Based Surrogate Model for the Dimensioning of Building Energy Systems},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7141},
URL = {https://www.mdpi.com/1996-1073/14/21/7141},
ISSN = {1996-1073},
ABSTRACT = {In recent decades, building design and operation have been an important field of study, due to the significant share of buildings in global primary energy consumption and the time that most people spend indoors. As such, multiple studies focus on aspects of building energy consumption and occupant comfort optimization. The scientific community has discerned the importance of operation optimization through retrofitting actions for on-site building energy systems, achieved by the use of simulation techniques, surrogate modeling, as well as the guidance of existing building performance and indoor occupancy standards. However, more knowledge should be attained on the matter of whether this methodology can be extended towards the early stages of thermal system and/or building design. To this end, the present study provides a building thermal system design optimization methodology. A data set of minimum thermal system power, for a typical range of building characteristics, is generated, according to the criterion of occupant discomfort in degree hours. Respectively, a surrogate model, providing a configurable correlation of the above set of thermal system dimensioning solutions is developed, using regression model fitting techniques. Computational results indicate that such a model could provide both desirable calculative simplification and accuracy on par with existing respective thermal load calculation standards and simplified system dimensioning methods.},
DOI = {10.3390/en14217141}
}



@Article{s21217330,
AUTHOR = {Zhang, Le and Khalgui, Mohamed and Li, Zhiwu},
TITLE = {Predictive Intelligent Transportation: Alleviating Traffic Congestion in the Internet of Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {7330},
URL = {https://www.mdpi.com/1424-8220/21/21/7330},
PubMedID = {34770637},
ISSN = {1424-8220},
ABSTRACT = {Due to the limitations of data transfer technologies, existing studies on urban traffic control mainly focused on isolated dimension control such as traffic signal control or vehicle route guidance to alleviate traffic congestion. However, in real traffic, the distribution of traffic flow is the result of multiple dimensions whose future state is influenced by each dimension’s decisions. Presently, the development of the Internet of Vehicles enables an integrated intelligent transportation system. This paper proposes an integrated intelligent transportation model that can optimize predictive traffic signal control and predictive vehicle route guidance simultaneously to alleviate traffic congestion based on their feedback regulation relationship. The challenges of this model lie in that the formulation of the nonlinear feedback relationship between various dimensions is hard to describe and the design of a corresponding solving algorithm that can obtain Pareto optimality for multi-dimension control is complex. In the integrated model, we introduce two medium variables—predictive traffic flow and the predictive waiting time—to two-way link the traffic signal control and vehicle route guidance. Inspired by game theory, an asymmetric information exchange framework-based updating distributed algorithm is designed to solve the integrated model. Finally, an experimental study in two typical traffic scenarios shows that more than 73.33% of the considered cases adopting the integrated model achieve Pareto optimality.},
DOI = {10.3390/s21217330}
}



@Article{computers10110147,
AUTHOR = {Giannoutakis, Konstantinos M. and Filelis-Papadopoulos, Christos K. and Gravvanis, George A. and Tzovaras, Dimitrios},
TITLE = {On the Optimization of Self-Organization and Self-Management Hardware Resource Allocation for Heterogeneous Clouds},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {147},
URL = {https://www.mdpi.com/2073-431X/10/11/147},
ISSN = {2073-431X},
ABSTRACT = {There is a tendency, during the last years, to migrate from the traditional homogeneous clouds and centralized provisioning of resources to heterogeneous clouds with specialized hardware governed in a distributed and autonomous manner. The CloudLightning architecture proposed recently introduced a dynamic way to provision heterogeneous cloud resources, by shifting the selection of underlying resources from the end-user to the system in an efficient way. In this work, an optimized Suitability Index and assessment function are proposed, along with their theoretical analysis, for improving the computational efficiency, energy consumption, service delivery and scalability of the distributed orchestration. The effectiveness of the proposed scheme is being evaluated with the use of simulation, by comparing the optimized methods with the original approach and the traditional centralized resource management, on real and synthetic High Performance Computing applications. Finally, numerical results are presented and discussed regarding the improvements over the defined evaluation criteria.},
DOI = {10.3390/computers10110147}
}



@Article{app112210517,
AUTHOR = {Sivasankarareddy, V. and Sundari, G. and Rami Reddy, Ch. and Aymen, Flah and Bortoni, Edson C.},
TITLE = {Grid-Based Routing Model for Energy Efficient and Secure Data Transmission in WSN for Smart Building Applications},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {10517},
URL = {https://www.mdpi.com/2076-3417/11/22/10517},
ISSN = {2076-3417},
ABSTRACT = {Presently, due to the establishment of a sensor network, residual buildings in urban areas are being converted into smart buildings. Many sensors are deployed in various buildings to perform different functions, such as water quality monitoring and temperature monitoring. However, the major concern faced in smart building Wireless Sensor Networks (WSNs) is energy depletion and security threats. Many researchers have attempted to solve these issues by various authors in different applications of WSNs. However, limited research has been conducted on smart buildings. Thus, the present research is focused on designing an energy-efficient and secure routing protocol for smart building WSNs. The process in the proposed framework is carried out in two stages. The first stage is the design of the optimal routing protocol based on the grid-clustering approach. In the grid-based model, a grid organizer was selected based on the sailfish optimization algorithm. Subsequently, a fuzzy expert system is used to select the relay node to reach the shortest path for data transmission. The second stage involves designing a trust model for secure data transmission using the two-fish algorithm. A simulation study of the proposed framework was conducted to evaluate its performance. Some metrics, such as the packet delivery ratio, end-end delay, and average residual energy, were calculated for the proposed model. The average residual energy for the proposed framework was 96%, which demonstrates the effectiveness of the proposed routing design.},
DOI = {10.3390/app112210517}
}



@Article{electronics10222752,
AUTHOR = {Sagar, Md. Samiul Islam and Ouassal, Hassna and Omi, Asif I. and Wisniewska, Anna and Jalajamony, Harikrishnan M. and Fernandez, Renny E. and Sekhar, Praveen K.},
TITLE = {Application of Machine Learning in Electromagnetics: Mini-Review},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {2752},
URL = {https://www.mdpi.com/2079-9292/10/22/2752},
ISSN = {2079-9292},
ABSTRACT = {As an integral part of the electromagnetic system, antennas are becoming more advanced and versatile than ever before, thus making it necessary to adopt new techniques to enhance their performance. Machine Learning (ML), a branch of artificial intelligence, is a method of data analysis that automates analytical model building with minimal human intervention. The potential for ML to solve unpredictable and non-linear complex challenges is attracting researchers in the field of electromagnetics (EM), especially in antenna and antenna-based systems. Numerous antenna simulations, synthesis, and pattern recognition of radiations as well as non-linear inverse scattering-based object identifications are now leveraging ML techniques. Although the accuracy of ML algorithms depends on the availability of sufficient data and expert handling of the model and hyperparameters, it is gradually becoming the desired solution when researchers are aiming for a cost-effective solution without excessive time consumption. In this context, this paper aims to present an overview of machine learning, and its applications in Electromagnetics, including communication, radar, and sensing. It extensively discusses recent research progress in the development and use of intelligent algorithms for antenna design, synthesis and analysis, electromagnetic inverse scattering, synthetic aperture radar target recognition, and fault detection systems. It also provides limitations of this emerging field of study. The unique aspect of this work is that it surveys the state-of the art and recent advances in ML techniques as applied to EM.},
DOI = {10.3390/electronics10222752}
}



@Article{en14227756,
AUTHOR = {Zhong , Liang and Zhang , Shizhong and Zhang , Yidu and Chen , Guang and Liu , Yong},
TITLE = {Joint Acquisition Time Design and Sensor Association for Wireless Sensor Networks in Microgrids},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {7756},
URL = {https://www.mdpi.com/1996-1073/14/22/7756},
ISSN = {1996-1073},
ABSTRACT = {Wireless sensor networks are used to monitor the operating status of the microgrids, which can effectively improve the stability of power supplies. The topology control is a critical issue of wireless sensor networks, which affects monitoring data transmission reliability and lifetime of wireless sensor networks. Meanwhile, the data acquisition accuracy of wireless sensor networks has a great impact on the quality of monitoring. Therefore, this paper focuses on improving wireless sensor networks data acquisition satisfaction and energy efficiency. A joint acquisition time design and sensor association optimization algorithm is proposed to prolong the lifetime of wireless sensor networks and enhance the stability of monitoring, which considers the cluster heads selection, data collection satisfaction and sensor association. First, a multi-constrained mixed-integer programming problem, which combines acquisition time design and sensor association, is formulated to maximize data acquisition satisfaction and minimize energy consumption. To solve this problem, we propose an iterative algorithm based on block coordinate descent technology. In each iteration, the acquisition time is obtained by Lagrangian duality. After that, the sensor association is modeled as a 0–1 knapsack problem, and the three different methods are proposed to solve it. Finally, the simulations are provided to demonstrate the efficiency of the algorithm proposed in this paper.},
DOI = {10.3390/en14227756}
}



@Article{info12110480,
AUTHOR = {Viktoratos, Iosif and Tsadiras, Athanasios},
TITLE = {Personalized Advertising Computational Techniques: A Systematic Literature Review, Findings, and a Design Framework},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {480},
URL = {https://www.mdpi.com/2078-2489/12/11/480},
ISSN = {2078-2489},
ABSTRACT = {This work conducts a systematic literature review about the domain of personalized advertisement, and more specifically, about the techniques that are used for this purpose. State-of-the-art publications and techniques are presented in detail, and the relationship of this domain with other related domains such as artificial intelligence (AI), semantic web, etc., is investigated. Important issues such as (a) business data utilization in personalized advertisement models, (b) the cold start problem in the domain, (c) advertisement visualization issues, (d) psychological factors in the personalization models, (e) the lack of rich datasets, and (f) user privacy are highlighted and are pinpointed to help and inspire researchers for future work. Finally, a design framework for personalized advertisement systems has been designed based on these findings.},
DOI = {10.3390/info12110480}
}



@Article{s21227705,
AUTHOR = {Reza, Selim and Oliveira, Hugo S. and Machado, José J. M. and Tavares, João Manuel R. S.},
TITLE = {Urban Safety: An Image-Processing and Deep-Learning-Based Intelligent Traffic Management and Control System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {7705},
URL = {https://www.mdpi.com/1424-8220/21/22/7705},
PubMedID = {34833794},
ISSN = {1424-8220},
ABSTRACT = {With the rapid growth and development of cities, Intelligent Traffic Management and Control (ITMC) is becoming a fundamental component to address the challenges of modern urban traffic management, where a wide range of daily problems need to be addressed in a prompt and expedited manner. Issues such as unpredictable traffic dynamics, resource constraints, and abnormal events pose difficulties to city managers. ITMC aims to increase the efficiency of traffic management by minimizing the odds of traffic problems, by providing real-time traffic state forecasts to better schedule the intersection signal controls. Reliable implementations of ITMC improve the safety of inhabitants and the quality of life, leading to economic growth. In recent years, researchers have proposed different solutions to address specific problems concerning traffic management, ranging from image-processing and deep-learning techniques to forecasting the traffic state and deriving policies to control intersection signals. This review article studies the primary public datasets helpful in developing models to address the identified problems, complemented with a deep analysis of the works related to traffic state forecast and intersection-signal-control models. Our analysis found that deep-learning-based approaches for short-term traffic state forecast and multi-intersection signal control showed reasonable results, but lacked robustness for unusual scenarios, particularly during oversaturated situations, which can be resolved by explicitly addressing these cases, potentially leading to significant improvements of the systems overall. However, there is arguably a long path until these models can be used safely and effectively in real-world scenarios.},
DOI = {10.3390/s21227705}
}



@Article{electronics10232918,
AUTHOR = {Mohamed, Nader and Al-Jaroodi, Jameela and Lazarova-Molnar, Sanja and Jawhar, Imad},
TITLE = {Applications of Integrated IoT-Fog-Cloud Systems to Smart Cities: A Survey},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {2918},
URL = {https://www.mdpi.com/2079-9292/10/23/2918},
ISSN = {2079-9292},
ABSTRACT = {Several cities have recently moved towards becoming smart cities for better services and quality of life for residents and visitors, with: optimized resource utilization; increased environmental protection; enhanced infrastructure operations and maintenance; and strong safety and security measures. Smart cities depend on deploying current and new technologies and different optimization methods to enhance services and performance in their different sectors. Some of the technologies assisting smart city applications are the Internet of Things (IoT), fog computing, and cloud computing. Integrating these three to serve one system (we will refer to it as integrated IoT-fog-cloud system (iIFC)) creates an advanced platform to develop and operate various types of smart city applications. This platform will allow applications to use the best features from the IoT devices, fog nodes, and cloud services to deliver best capabilities and performance. Utilizing this powerful platform will provide many opportunities for enhancing and optimizing applications in energy, transportation, healthcare, and other areas. In this paper we survey various applications of iIFCs for smart cities. We identify different common issues associated with utilizing iIFCs for smart city applications. These issues arise due to the characteristics of iIFCs on the one side and the requirements of different smart city applications on the other. In addition, we outline the main requirements to effectively utilize iIFCs for smart city applications. These requirements are related to optimization, networking, and security.},
DOI = {10.3390/electronics10232918}
}



@Article{rs13234839,
AUTHOR = {Zheng, Lianming and Lin, Rui and Wang, Xuemei and Chen, Weihua},
TITLE = {The Development and Application of Machine Learning in Atmospheric Environment Studies},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {4839},
URL = {https://www.mdpi.com/2072-4292/13/23/4839},
ISSN = {2072-4292},
ABSTRACT = {Machine learning (ML) plays an important role in atmospheric environment prediction, having been widely applied in atmospheric science with significant progress in algorithms and hardware. In this paper, we present a brief overview of the development of ML models as well as their application to atmospheric environment studies. ML model performance is then compared based on the main air pollutants (i.e., PM2.5, O3, and NO2) and model type. Moreover, we identify the key driving variables for ML models in predicting particulate matter (PM) pollutants by quantitative statistics. Additionally, a case study for wet nitrogen deposition estimation is carried out based on ML models. Finally, the prospects of ML for atmospheric prediction are discussed.},
DOI = {10.3390/rs13234839}
}



@Article{app112311382,
AUTHOR = {Osman, Radwa Ahmed and Saleh, Sherine Nagy and Saleh, Yasmine N. M. and Elagamy, Mazen Nabil},
TITLE = {Enhancing the Reliability of Communication between Vehicle and Everything (V2X) Based on Deep Learning for Providing Efficient Road Traffic Information},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {11382},
URL = {https://www.mdpi.com/2076-3417/11/23/11382},
ISSN = {2076-3417},
ABSTRACT = {Developing efficient communication between vehicles and everything (V2X) is a challenging task, mainly due to the characteristics of vehicular networks, which include rapid topology changes, large-scale sizes, and frequent link disconnections. This article proposes a deep learning model to enhance V2X communication. Various channel conditions such as interference, channel noise, and path loss affect the communication between a vehicle (V) and everything (X). Thus, the proposed model aims to determine the required optimum interference power to enhance connectivity, comply with the quality of service (QoS) constraints, and improve the communication link reliability. The proposed model fulfills the best QoS in terms of four metrics, namely, achievable data rate (Rb), packet delivery ratio (PDR), packet loss rate (PLR), and average end-to-end delay (E2E). The factors to be considered are the distribution and density of vehicles, average length, and minimum safety distance between vehicles. A mathematical formulation of the optimum required interference power is presented to achieve the given objectives as a constrained optimization problem, and accordingly, the proposed deep learning model is trained. The obtained results show the ability of the proposed model to enhance the connectivity between V2X for improving road traffic information efficiency and increasing road traffic safety.},
DOI = {10.3390/app112311382}
}



@Article{electronics10232997,
AUTHOR = {Hurbean, Luminita and Danaiata, Doina and Militaru, Florin and Dodea, Andrei-Mihail and Negovan, Ana-Maria},
TITLE = {Open Data Based Machine Learning Applications in Smart Cities: A Systematic Literature Review},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {2997},
URL = {https://www.mdpi.com/2079-9292/10/23/2997},
ISSN = {2079-9292},
ABSTRACT = {Machine learning (ML) has already gained the attention of the researchers involved in smart city (SC) initiatives, along with other advanced technologies such as IoT, big data, cloud computing, or analytics. In this context, researchers also realized that data can help in making the SC happen but also, the open data movement has encouraged more research works using machine learning. Based on this line of reasoning, the aim of this paper is to conduct a systematic literature review to investigate open data-based machine learning applications in the six different areas of smart cities. The results of this research reveal that: (a) machine learning applications using open data came out in all the SC areas and specific ML techniques are discovered for each area, with deep learning and supervised learning being the first choices. (b) Open data platforms represent the most frequently used source of data. (c) The challenges associated with open data utilization vary from quality of data, to frequency of data collection, to consistency of data, and data format. Overall, the data synopsis as well as the in-depth analysis may be a valuable support and inspiration for the future smart city projects.},
DOI = {10.3390/electronics10232997}
}



@Article{su132313322,
AUTHOR = {Ponnusamy, Vinoth Kumar and Kasinathan, Padmanathan and Madurai Elavarasan, Rajvikram and Ramanathan, Vinoth and Anandan, Ranjith Kumar and Subramaniam, Umashankar and Ghosh, Aritra and Hossain, Eklas},
TITLE = {A Comprehensive Review on Sustainable Aspects of Big Data Analytics for the Smart Grid},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {13322},
URL = {https://www.mdpi.com/2071-1050/13/23/13322},
ISSN = {2071-1050},
ABSTRACT = {The role of energy is cardinal for achieving the Sustainable Development Goals (SDGs) through the enhancement and modernization of energy generation and management practices. The smart grid enables efficient communication between utilities and the end- users, and enhances the user experience by monitoring and controlling the energy transmission. The smart grid deals with an enormous amount of energy data, and the absence of proper techniques for data collection, processing, monitoring and decision-making ultimately makes the system ineffective. Big data analytics, in association with the smart grid, enable better grid visualization and contribute toward the attainment of sustainability. The current research work deals with the achievement of sustainability in the smart grid and efficient data management using big data analytics, that has social, economic, technical and political impacts. This study provides clear insights into energy data generated in the grid and the possibilities of energy theft affecting the sustainable future. The paper provides insights about the importance of big data analytics, with their effects on the smart grids&rsquo; performance towards the achievement of SDGs. The work highlights efficient real-time energy data management involving artificial intelligence and machine learning for a better future, to short out the effects of the conventional smart grid without big data analytics. Finally, the work discusses the challenges and future directions to improve smart grid technologies with big data analytics in action.},
DOI = {10.3390/su132313322}
}



@Article{math9233117,
AUTHOR = {Herich, Dušan and Vaščák, Ján and Zolotová, Iveta and Brecko, Alexander},
TITLE = {Automatic Path Planning Offloading Mechanism in Edge-Enabled Environments},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {3117},
URL = {https://www.mdpi.com/2227-7390/9/23/3117},
ISSN = {2227-7390},
ABSTRACT = {The utilization of edge-enabled cloud computing in unmanned aerial vehicles has facilitated advances in autonomous control by employing computationally intensive algorithms frequently related to traversal among different locations in an environment. A significant problem remains in designing an effective strategy to offload tasks from the edge to the cloud. This work focuses on creating such a strategy by employing a network evaluation method built on the mean opinion score metrics in concoction with machine learning algorithms for path length prediction to assess computational complexity and classification models to perform an offloading decision on the data provided by both network metrics and solution depth prediction. The proposed system is applied to the A* path planning algorithm, and the presented results demonstrate up to 94% accuracy in offloading decisions.},
DOI = {10.3390/math9233117}
}



@Article{telecom2040028,
AUTHOR = {Gomes, Eliza and Costa, Felipe and De Rolt, Carlos and Plentz, Patricia and Dantas, Mario},
TITLE = {A Survey from Real-Time to Near Real-Time Applications in Fog Computing Environments},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {489--517},
URL = {https://www.mdpi.com/2673-4001/2/4/28},
ISSN = {2673-4001},
ABSTRACT = {In this article, we present a comprehensive survey on time-sensitive applications implemented in fog computing environments. The goal is to research what applications are being implemented in fog computing architectures and how the temporal requirements of these applications are being addressed. We also carried out a comprehensive analysis of the articles surveyed and separate them into categories, according to a pattern found in them. Our research is important for the area of real-time systems since the concept of systems that respond in real time has presented various understandings and concepts. This variability of concept has been due to the growing requirements for fast data communication and processing. Therefore, we present different concepts of real-time and near real-time systems found in the literature and currently accepted by the academic-scientific community. Finally, we conduct an analytical discussion of the characteristics and proposal of articles.},
DOI = {10.3390/telecom2040028}
}



@Article{w13233470,
AUTHOR = {Alenezi, Fayadh and Armghan, Ammar and Mohanty, Sachi Nandan and Jhaveri, Rutvij H. and Tiwari, Prayag},
TITLE = {Block-Greedy and CNN Based Underwater Image Dehazing for Novel Depth Estimation and Optimal Ambient Light},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {3470},
URL = {https://www.mdpi.com/2073-4441/13/23/3470},
ISSN = {2073-4441},
ABSTRACT = {A lack of adequate consideration of underwater image enhancement gives room for more research into the field. The global background light has not been adequately addressed amid the presence of backscattering. This paper presents a technique based on pixel differences between global and local patches in scene depth estimation. The pixel variance is based on green and red, green and blue, and red and blue channels besides the absolute mean intensity functions. The global background light is extracted based on a moving average of the impact of suspended light and the brightest pixels within the image color channels. We introduce the block-greedy algorithm in a novel Convolutional Neural Network (CNN) proposed to normalize different color channels&rsquo; attenuation ratios and select regions with the lowest variance. We address the discontinuity associated with underwater images by transforming both local and global pixel values. We minimize energy in the proposed CNN via a novel Markov random field to smooth edges and improve the final underwater image features. A comparison of the performance of the proposed technique against existing state-of-the-art algorithms using entropy, Underwater Color Image Quality Evaluation (UCIQE), Underwater Image Quality Measure (UIQM), Underwater Image Colorfulness Measure (UICM), and Underwater Image Sharpness Measure (UISM) indicate better performance of the proposed approach in terms of average and consistency. As it concerns to averagely, UICM has higher values in the technique than the reference methods, which explainsits higher color balance. The &mu; values of UCIQE, UISM, and UICM of the proposed method supersede those of the existing techniques. The proposed noted a percent improvement of 0.4%, 4.8%, 9.7%, 5.1% and 7.2% in entropy, UCIQE, UIQM, UICM and UISM respectively compared to the best existing techniques. Consequently, dehazed images have sharp, colorful, and clear features in most images when compared to those resulting from the existing state-of-the-art methods. Stable &sigma; values explain the consistency in visual analysis in terms of sharpness of color and clarity of features in most of the proposed image results when compared with reference methods. Our own assessment shows that only weakness of the proposed technique is that it only applies to underwater images. Future research could seek to establish edge strengthening without color saturation enhancement.},
DOI = {10.3390/w13233470}
}



@Article{pr9122205,
AUTHOR = {Jafari, Sadiqa and Shahbazi, Zeinab and Byun, Yung-Cheol},
TITLE = {Traffic Control Prediction Design Based on Fuzzy Logic and Lyapunov Approaches to Improve the Performance of Road Intersection},
JOURNAL = {Processes},
VOLUME = {9},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {2205},
URL = {https://www.mdpi.com/2227-9717/9/12/2205},
ISSN = {2227-9717},
ABSTRACT = {Due to the increasing use of private cars for urbanization and urban transport, the travel time of urban transportation is increasing. People spend a lot of time in the streets, and the queue length of waiting increases accordingly; this has direct effects on fuel consumption too. Traffic flow forecasts and traffic light schedules were studied separately in the urban traffic system. This paper presents a new stable TS (Takagi&ndash;Sugeno) fuzzy controller for urban traffic. The state-space dynamics are utilized to formulate both the vehicle&rsquo;s average waiting time at an isolated intersection and the length of queues. A fuzzy intelligent controller is designed for light control based upon the length of the queue, and eventually, the system&rsquo;s stability is proved using the Lyapunov theorem. Moreover, the input variables are the length of queue and number of input or output vehicles from each lane. The simulation results describe the appearance of the proposed controller. An illustrative example is also given to show the proposed method&rsquo;s effectiveness; the suggested method is more efficient than both the conventional fuzzy traffic controllers and the fixed time controller.},
DOI = {10.3390/pr9122205}
}



@Article{s21248178,
AUTHOR = {Azhar, Irfan and Sharif, Muhammad and Raza, Mudassar and Khan, Muhammad Attique and Yong, Hwan-Seung},
TITLE = {A Decision Support System for Face Sketch Synthesis Using Deep Learning and Artificial Intelligence},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {8178},
URL = {https://www.mdpi.com/1424-8220/21/24/8178},
PubMedID = {34960274},
ISSN = {1424-8220},
ABSTRACT = {The recent development in the area of IoT technologies is likely to be implemented extensively in the next decade. There is a great increase in the crime rate, and the handling officers are responsible for dealing with a broad range of cyber and Internet issues during investigation. IoT technologies are helpful in the identification of suspects, and few technologies are available that use IoT and deep learning together for face sketch synthesis. Convolutional neural networks (CNNs) and other constructs of deep learning have become major tools in recent approaches. A new-found architecture of the neural network is anticipated in this work. It is called Spiral-Net, which is a modified version of U-Net fto perform face sketch synthesis (the phase is known as the compiler network C here). Spiral-Net performs in combination with a pre-trained Vgg-19 network called the feature extractor F. It first identifies the top n matches from viewed sketches to a given photo. F is again used to formulate a feature map based on the cosine distance of a candidate sketch formed by C from the top n matches. A customized CNN configuration (called the discriminator D) then computes loss functions based on differences between the candidate sketch and the feature. Values of these loss functions alternately update C and F. The ensemble of these nets is trained and tested on selected datasets, including CUFS, CUFSF, and a part of the IIT photo&ndash;sketch dataset. Results of this modified U-Net are acquired by the legacy NLDA (1998) scheme of face recognition and its newer version, OpenBR (2013), which demonstrate an improvement of 5% compared with the current state of the art in its relevant domain.},
DOI = {10.3390/s21248178}
}



@Article{w13243488,
AUTHOR = {Jeon, Minsu and Guerra, Heidi B. and Choi, Hyeseon and Kwon, Donghyun and Kim, Hayong and Kim, Lee-Hyung},
TITLE = {Stormwater Runoff Treatment Using Rain Garden: Performance Monitoring and Development of Deep Learning-Based Water Quality Prediction Models},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {3488},
URL = {https://www.mdpi.com/2073-4441/13/24/3488},
ISSN = {2073-4441},
ABSTRACT = {Twenty-three rainfall events were monitored to determine the characteristics of the stormwater runoff entering a rain garden facility and evaluate its performance in terms of pollutant removal and volume reduction. Data gathered during the five-year monitoring period were utilized to develop a deep learning-based model that can predict the concentrations of Total Suspended Solids (TSS), Chemical Oxygen Demand (COD), Total Nitrogen (TN), and Total Phosphorus (TP). Findings revealed that the rain garden was capable of effectively reducing solids, organics, nutrients, and heavy metals from stormwater runoff during the five-year period when hydrologic and climate conditions have changed. Volume reduction was also high but can decrease over time due to the accumulation of solids in the facility which reduced the infiltration capacity and increased ponding and overflows especially during heavy rainfalls. A preliminary development of a water quality prediction model based on long short-term memory (LSTM) architecture was also developed to be able to potentially reduce the labor and costs associated with on-site monitoring in the future. The LSTM model predicted pollutant concentrations that are close to the actual values with a mean square error of 0.36 during calibration and a less than 10% difference from the measured values during validation. The study showed the potential of using deep learning architecture for the prediction of stormwater quality parameters entering rain gardens. While this study is still in the preliminary stage, it can potentially be improved for use in performance monitoring, decision-making regarding maintenance, and design of similar technologies in the future.},
DOI = {10.3390/w13243488}
}



@Article{app112411637,
AUTHOR = {Karnati, Yashaswi and Sengupta, Rahul and Ranka, Sanjay},
TITLE = {InterTwin: Deep Learning Approaches for Computing Measures of Effectiveness for Traffic Intersections},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {11637},
URL = {https://www.mdpi.com/2076-3417/11/24/11637},
ISSN = {2076-3417},
ABSTRACT = {Microscopic simulation-based approaches are extensively used for determining good signal timing plans on traffic intersections. Measures of Effectiveness (MOEs) such as wait time, throughput, fuel consumption, emission, and delays can be derived for variable signal timing parameters, traffic flow patterns, etc. However, these techniques are computationally intensive, especially when the number of signal timing scenarios to be simulated are large. In this paper, we propose InterTwin, a Deep Neural Network architecture based on Spatial Graph Convolution and Encoder-Decoder Recurrent networks that can predict the MOEs efficiently and accurately for a wide variety of signal timing and traffic patterns. Our methods can generate probability distributions of MOEs and are not limited to mean and standard deviation. Additionally, GPU implementations using InterTwin can derive MOEs, at least four to five orders of magnitude faster than microscopic simulations on a conventional 32 core CPU machine.},
DOI = {10.3390/app112411637}
}



@Article{smartcities4040078,
AUTHOR = {Shinde, Swapnil Sadashiv and Tarchi, Daniele},
TITLE = {Towards a Novel Air&ndash;Ground Intelligent Platform for Vehicular Networks: Technologies, Scenarios, and Challenges},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {4},
PAGES = {1469--1495},
URL = {https://www.mdpi.com/2624-6511/4/4/78},
ISSN = {2624-6511},
ABSTRACT = {Modern cities require a tighter integration with Information and Communication Technologies (ICT) for bringing new services to the citizens. The Smart City is the revolutionary paradigm aiming at integrating the ICT with the citizen life; among several urban services, transports are one of the most important in modern cities, introducing several challenges to the Smart City paradigm. In order to satisfy the stringent requirements of new vehicular applications and services, Edge Computing (EC) is one of the most promising technologies when integrated into the Vehicular Networks (VNs). EC-enabled VNs can facilitate new latency-critical and data-intensive applications and services. However, ground-based EC platforms (i.e., Road Side Units&mdash;RSUs, 5G Base Stations&mdash;5G BS) can only serve a reduced number of Vehicular Users (VUs), due to short coverage ranges and resource shortage. In the recent past, several new aerial platforms with integrated EC facilities have been deployed for achieving global connectivity. Such air-based EC platforms can complement the ground-based EC facilities for creating a futuristic VN able to deploy several new applications and services. The goal of this work is to explore the possibility of creating a novel joint air-ground EC platform within a VN architecture for helping VUs with new intelligent applications and services. By exploiting most modern technologies, with particular attention towards network softwarization, vehicular edge computing, and machine learning, we propose here three possible layered air-ground EC-enabled VN scenarios. For each of the discussed scenarios, a list of the possible challenges is considered, as well possible solutions allowing to overcome all or some of the considered challenges. A proper comparison is also done, through the use of tables, where all the proposed scenarios, and the proposed solutions, are discussed.},
DOI = {10.3390/smartcities4040078}
}



@Article{electronics10243079,
AUTHOR = {Sengan, Sudhakar and Kotecha, Ketan and Vairavasundaram, Indragandhi and Velayutham, Priya and Varadarajan, Vijayakumar and Ravi, Logesh and Vairavasundaram, Subramaniyaswamy},
TITLE = {Real-Time Automatic Investigation of Indian Roadway Animals by 3D Reconstruction Detection Using Deep Learning for R-3D-YOLOv3 Image Classification and Filtering},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {3079},
URL = {https://www.mdpi.com/2079-9292/10/24/3079},
ISSN = {2079-9292},
ABSTRACT = {Statistical reports say that, from 2011 to 2021, more than 11,915 stray animals, such as cats, dogs, goats, cows, etc., and wild animals were wounded in road accidents. Most of the accidents occurred due to negligence and doziness of drivers. These issues can be handled brilliantly using stray and wild animals-vehicle interaction and the pedestrians&rsquo; awareness. This paper briefs a detailed forum on GPU-based embedded systems and ODT real-time applications. ML trains machines to recognize images more accurately than humans. This provides a unique and real-time solution using deep-learning real 3D motion-based YOLOv3 (DL-R-3D-YOLOv3) ODT of images on mobility. Besides, it discovers methods for multiple views of flexible objects using 3D reconstruction, especially for stray and wild animals. Computer vision-based IoT devices are also besieged by this DL-R-3D-YOLOv3 model. It seeks solutions by forecasting image filters to find object properties and semantics for object recognition methods leading to closed-loop ODT.},
DOI = {10.3390/electronics10243079}
}



@Article{fi13120313,
AUTHOR = {Kapassa, Evgenia and Themistocleous, Marinos and Christodoulou, Klitos and Iosif, Elias},
TITLE = {Blockchain Application in Internet of Vehicles: Challenges, Contributions and Current Limitations},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {313},
URL = {https://www.mdpi.com/1999-5903/13/12/313},
ISSN = {1999-5903},
ABSTRACT = {Blockchain technology is highly coupled with cryptocurrencies; however, it provides several other potential use cases, related to energy and sustainability, Internet of Things (IoT), smart cities, smart mobility and more. Blockchain can offer security for Electric Vehicle (EV) transactions in the Internet of Vehicles (IoV) concept, allowing electricity trading to be performed in a decentralized, transparent and secure way. Additionally, blockchain provides the necessary functionalities for IoV decentralized application development, such as data exchange, personal digital identity, sharing economy and optimized charging pattern. Moreover, blockchain technology has the potential to significantly increase energy efficiency, decrease management costs and guarantee the effective use of the energy recourses. Therefore, its application in the IoV concept provides secure, autonomous and automated energy trading between EVs. While several studies on blockchain technology in smart grids have been conducted, insufficient attention has been given to conducting a detailed review and state-of-the-art analysis of blockchain application in the IoV domain. To this end, this work provides a systematic literature review of blockchain-based applications in the IoV domain. The aim is to investigate the current challenges of IoV and to highlight how blockchain characteristics can contribute to this emerging paradigm. In addition, limitations and future research directions related to the integration of blockchain technology within the IoV are discussed. To this end, this study incorporates the theoretical foundations of several research articles published in scientific publications over the previous five years, as a method of simplifying our assessment and capturing the ever-expanding blockchain area. We present a comprehensive taxonomy of blockchain-enabled applications in the IoV domain, such as privacy and security, data protection and management, vehicle management, charging optimization and P2P energy trading, based on a structured, systematic review and content analysis of the discovered literature, and we identify key trends and emerging areas for research. The contribution of this article is two-fold: (a) we highlight the limitations presented in the relevant literature, particularly the barriers of blockchain technology and how they influence its integration into the IoV and (b) we present a number of research gaps and suggest future exploratory areas.},
DOI = {10.3390/fi13120313}
}



@Article{s21248262,
AUTHOR = {Mary, Delphin Raj Kesari and Ko, Eunbi and Kim, Seung-Geun and Yum, Sun-Ho and Shin, Soo-Young and Park, Soo-Hyun},
TITLE = {A Systematic Review on Recent Trends, Challenges, Privacy and Security Issues of Underwater Internet of Things},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {8262},
URL = {https://www.mdpi.com/1424-8220/21/24/8262},
PubMedID = {34960366},
ISSN = {1424-8220},
ABSTRACT = {Owing to the hasty growth of communication technologies in the Underwater Internet of Things (UIoT), many researchers and industries focus on enhancing the existing technologies of UIoT systems for developing numerous applications such as oceanography, diver networks monitoring, deep-sea exploration and early warning systems. In a constrained UIoT environment, communication media such as acoustic, infrared (IR), visible light, radiofrequency (RF) and magnet induction (MI) are generally used to transmit information via digitally linked underwater devices. However, each medium has its technical limitations: for example, the acoustic medium has challenges such as narrow-channel bandwidth, low data rate, high cost, etc., and optical medium has challenges such as high absorption, scattering, long-distance data transmission, etc. Moreover, the malicious node can steal the underwater data by employing blackhole attacks, routing attacks, Sybil attacks, etc. Furthermore, due to heavyweight, the existing privacy and security mechanism of the terrestrial internet of things (IoT) cannot be applied directly to UIoT environment. Hence, this paper aims to provide a systematic review of recent trends, applications, communication technologies, challenges, security threats and privacy issues of UIoT system. Additionally, this paper highlights the methods of preventing the technical challenges and security attacks of the UIoT environment. Finally, this systematic review contributes much to the profit of researchers to analyze and improve the performance of services in UIoT applications.},
DOI = {10.3390/s21248262}
}



@Article{drones5040148,
AUTHOR = {Yazid, Yassine and Ez-Zazi, Imad and Guerrero-González, Antonio and El Oualkadi, Ahmed and Arioua, Mounir},
TITLE = {UAV-Enabled Mobile Edge-Computing for IoT Based on AI: A Comprehensive Review},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {148},
URL = {https://www.mdpi.com/2504-446X/5/4/148},
ISSN = {2504-446X},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are becoming integrated into a wide range of modern IoT applications. The growing number of networked IoT devices generates a large amount of data. However, processing and memorizing this massive volume of data at local nodes have been deemed critical challenges, especially when using artificial intelligence (AI) systems to extract and exploit valuable information. In this context, mobile edge computing (MEC) has emerged as a way to bring cloud computing (CC) processes within reach of users, to address computation-intensive offloading and latency issues. This paper provides a comprehensive review of the most relevant research works related to UAV technology applications in terms of enabled or assisted MEC architectures. It details the utility of UAV-enabled MEC architecture regarding emerging IoT applications and the role of both deep learning (DL) and machine learning (ML) in meeting various limitations related to latency, task offloading, energy demand, and security. Furthermore, throughout this article, the reader gains an insight into the future of UAV-enabled MEC, the advantages and the critical challenges to be tackled when using AI.},
DOI = {10.3390/drones5040148}
}



@Article{e23121678,
AUTHOR = {Yang, Shubo and Luo, Yang and Miao, Wang and Ge, Changhao and Sun, Wenjian and Luo, Chunbo},
TITLE = {RF Signal-Based UAV Detection and Mode Classification: A Joint Feature Engineering Generator and Multi-Channel Deep Neural Network Approach},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {1678},
URL = {https://www.mdpi.com/1099-4300/23/12/1678},
PubMedID = {34945985},
ISSN = {1099-4300},
ABSTRACT = {With the proliferation of Unmanned Aerial Vehicles (UAVs) to provide diverse critical services, such as surveillance, disaster management, and medicine delivery, the accurate detection of these small devices and the efficient classification of their flight modes are of paramount importance to guarantee their safe operation in our sky. Among the existing approaches, Radio Frequency (RF) based methods are less affected by complex environmental factors. The similarities between UAV RF signals and the diversity of frequency components make accurate detection and classification a particularly difficult task. To bridge this gap, we propose a joint Feature Engineering Generator (FEG) and Multi-Channel Deep Neural Network (MC-DNN) approach. Specifically, in FEG, data truncation and normalization separate different frequency components, the moving average filter reduces the outliers in the RF signal, and the concatenation fully exploits the details of the dataset. In addition, the multi-channel input in MC-DNN separates multiple frequency components and reduces the interference between them. A novel dataset that contains ten categories of RF signals from three types of UAVs is used to verify the effectiveness. Experiments show that the proposed method outperforms the state-of-the-art UAV detection and classification approaches in terms of 98.4% and F1 score of 98.3%.},
DOI = {10.3390/e23121678}
}



@Article{s21248467,
AUTHOR = {Elsisi, Mahmoud and Tran, Minh-Quang},
TITLE = {Development of an IoT Architecture Based on a Deep Neural Network against Cyber Attacks for Automated Guided Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {8467},
URL = {https://www.mdpi.com/1424-8220/21/24/8467},
PubMedID = {34960561},
ISSN = {1424-8220},
ABSTRACT = {This paper introduces an integrated IoT architecture to handle the problem of cyber attacks based on a developed deep neural network (DNN) with a rectified linear unit in order to provide reliable and secure online monitoring for automated guided vehicles (AGVs). The developed IoT architecture based on a DNN introduces a new approach for the online monitoring of AGVs against cyber attacks with a cheap and easy implementation instead of the traditional cyber attack detection schemes in the literature. The proposed DNN is trained based on experimental AGV data that represent the real state of the AGV and different types of cyber attacks including a random attack, ramp attack, pulse attack, and sinusoidal attack that is injected by the attacker into the internet network. The proposed DNN is compared with different deep learning and machine learning algorithms such as a one dimension convolutional neural network (1D-CNN), a supported vector machine model (SVM), random forest, extreme gradient boosting (XGBoost), and a decision tree for greater validation. Furthermore, the proposed IoT architecture based on a DNN can provide an effective detection for the AGV status with an excellent accuracy of 96.77% that is significantly greater than the accuracy based on the traditional schemes. The AGV status based on the proposed IoT architecture with a DNN is visualized by an advanced IoT platform named CONTACT Elements for IoT. Different test scenarios with a practical setup of an AGV with IoT are carried out to emphasize the performance of the suggested IoT architecture based on a DNN. The results approve the usefulness of the proposed IoT to provide effective cybersecurity for data visualization and tracking of the AGV status that enhances decision-making and improves industrial productivity.},
DOI = {10.3390/s21248467}
}



@Article{s22010026,
AUTHOR = {Dangi, Ramraj and Lalwani, Praveen and Choudhary, Gaurav and You, Ilsun and Pau, Giovanni},
TITLE = {Study and Investigation on 5G Technology: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {26},
URL = {https://www.mdpi.com/1424-8220/22/1/26},
PubMedID = {35009569},
ISSN = {1424-8220},
ABSTRACT = {In wireless communication, Fifth Generation (5G) Technology is a recent generation of mobile networks. In this paper, evaluations in the field of mobile communication technology are presented. In each evolution, multiple challenges were faced that were captured with the help of next-generation mobile networks. Among all the previously existing mobile networks, 5G provides a high-speed internet facility, anytime, anywhere, for everyone. 5G is slightly different due to its novel features such as interconnecting people, controlling devices, objects, and machines. 5G mobile system will bring diverse levels of performance and capability, which will serve as new user experiences and connect new enterprises. Therefore, it is essential to know where the enterprise can utilize the benefits of 5G. In this research article, it was observed that extensive research and analysis unfolds different aspects, namely, millimeter wave (mmWave), massive multiple-input and multiple-output (Massive-MIMO), small cell, mobile edge computing (MEC), beamforming, different antenna technology, etc. This article&rsquo;s main aim is to highlight some of the most recent enhancements made towards the 5G mobile system and discuss its future research objectives.},
DOI = {10.3390/s22010026}
}



@Article{math10010101,
AUTHOR = {Attanasio, Barbara and Mazayev, Andriy and du Plessis, Shani and Correia, Noélia},
TITLE = {Cognitive Load Balancing Approach for 6G MEC Serving IoT Mashups},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {101},
URL = {https://www.mdpi.com/2227-7390/10/1/101},
ISSN = {2227-7390},
ABSTRACT = {The sixth generation (6G) of communication networks represents more of a revolution than an evolution of the previous generations, providing new directions and innovative approaches to face the network challenges of the future. A crucial aspect is to make the best use of available resources for the support of an entirely new generation of services. From this viewpoint, the Web of Things (WoT), which enables Things to become Web Things to chain, use and re-use in IoT mashups, allows interoperability among IoT platforms. At the same time, Multi-access Edge Computing (MEC) brings computing and data storage to the edge of the network, which creates the so-called distributed and collective edge intelligence. Such intelligence is created in order to deal with the huge amount of data to be collected, analyzed and processed, from real word contexts, such as smart cities, which are evolving into dynamic and networked systems of people and things. To better exploit this architecture, it is crucial to break monolithic applications into modular microservices, which can be executed independently. Here, we propose an approach based on complex network theory and two weighted and interdependent multiplex networks to address the Microservices-compliant Load Balancing (McLB) problem in MEC infrastructure. Our findings show that the multiplex network representation represents an extra dimension of analysis, allowing to capture the complexity in WoT mashup organization and its impact on the organizational aspect of MEC servers. The impact of this extracted knowledge on the cognitive organization of MEC is quantified, through the use of heuristics that are engineered to guarantee load balancing and, consequently, QoS.},
DOI = {10.3390/math10010101}
}



@Article{e24010058,
AUTHOR = {Balicki, Jerzy},
TITLE = {Many-Objective Quantum-Inspired Particle Swarm Optimization Algorithm for Placement of Virtual Machines in Smart Computing Cloud},
JOURNAL = {Entropy},
VOLUME = {24},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {58},
URL = {https://www.mdpi.com/1099-4300/24/1/58},
ISSN = {1099-4300},
ABSTRACT = {Particle swarm optimization algorithm (PSO) is an effective metaheuristic that can determine Pareto-optimal solutions. We propose an extended PSO by introducing quantum gates in order to ensure the diversity of particle populations that are looking for efficient alternatives. The quality of solutions was verified in the issue of assignment of resources in the computing cloud to improve the live migration of virtual machines. We consider the multi-criteria optimization problem of deep learning-based models embedded into virtual machines. Computing clouds with deep learning agents can support several areas of education, smart city or economy. Because deep learning agents require lots of computer resources, seven criteria are studied such as electric power of hosts, reliability of cloud, CPU workload of the bottleneck host, communication capacity of the critical node, a free RAM capacity of the most loaded memory, a free disc memory capacity of the most busy storage, and overall computer costs. Quantum gates modify an accepted position for the current location of a particle. To verify the above concept, various simulations have been carried out on the laboratory cloud based on the OpenStack platform. Numerical experiments have confirmed that multi-objective quantum-inspired particle swarm optimization algorithm provides better solutions than the other metaheuristics.},
DOI = {10.3390/e24010058}
}



@Article{s22010208,
AUTHOR = {Muntean, Maria Viorela},
TITLE = {Multi-Agent System for Intelligent Urban Traffic Management Using Wireless Sensor Networks Data},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {208},
URL = {https://www.mdpi.com/1424-8220/22/1/208},
PubMedID = {35009750},
ISSN = {1424-8220},
ABSTRACT = {Intelligent traffic management is an important issue for smart cities. City councils try to implement the newest techniques and performant technologies in order to avoid traffic congestion, to optimize the use of traffic lights, to efficiently use car parking, etc. To find the best solution to this problem, Birmingham City Council decided to allow open-source predictive traffic forecasting by making the real-time datasets available. This paper proposes a multi-agent system (MAS) approach for intelligent urban traffic management in Birmingham using forecasting and classification techniques. The designed agents have the following tasks: forecast the occupancy rates for traffic flow, road junctions and car parking; classify the faults; control and monitor the entire process. The experimental results show that k-nearest neighbor forecasts with high accuracy rates for the traffic data and decision trees build the most accurate model for classifying the faults for their detection and repair in the shortest possible time. The whole learning process is coordinated by a monitoring agent in order to automate Birmingham city&rsquo;s traffic management.},
DOI = {10.3390/s22010208}
}



@Article{electronics11010121,
AUTHOR = {Tanveer, Jawad and Haider, Amir and Ali, Rashid and Kim, Ajung},
TITLE = {Machine Learning for Physical Layer in 5G and beyond Wireless Networks: A Survey},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {121},
URL = {https://www.mdpi.com/2079-9292/11/1/121},
ISSN = {2079-9292},
ABSTRACT = {Fifth-generation (5G) technology will play a vital role in future wireless networks. The breakthrough 5G technology will unleash a massive Internet of Everything (IoE), where billions of connected devices, people, and processes will be simultaneously served. The services provided by 5G include several use cases enabled by the enhanced mobile broadband, massive machine-type communications, and ultra-reliable low-latency communication. Fifth-generation networks potentially merge multiple networks on a single platform, providing a landscape for seamless connectivity, particularly for high-mobility devices. With their enhanced speed, 5G networks are prone to various research challenges. In this context, we provide a comprehensive survey on 5G technologies that emphasize machine learning-based solutions to cope with existing and future challenges. First, we discuss 5G network architecture and outline the key performance indicators compared to the previous and upcoming network generations. Second, we discuss next-generation wireless networks and their characteristics, applications, and use cases for fast connectivity to billions of devices. Then, we confer physical layer services, functions, and issues that decrease the signal quality. We also present studies on 5G network technologies, 5G propelling trends, and architectures that help to achieve the goals of 5G. Moreover, we discuss signaling techniques for 5G massive multiple-input and multiple-output and beam-forming techniques to enhance data rates with efficient spectrum sharing. Further, we review security and privacy concerns in 5G and standard bodies&rsquo; actionable recommendations for policy makers. Finally, we also discuss emerging challenges and future directions.},
DOI = {10.3390/electronics11010121}
}



@Article{jsan11010004,
AUTHOR = {Samara, Mustafa Al and Bennis, Ismail and Abouaissa, Abdelhafid and Lorenz, Pascal},
TITLE = {A Survey of Outlier Detection Techniques in IoT: Review and Classification},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {11},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {4},
URL = {https://www.mdpi.com/2224-2708/11/1/4},
ISSN = {2224-2708},
ABSTRACT = {The Internet of Things (IoT) is a fact today where a high number of nodes are used for various applications. From small home networks to large-scale networks, the aim is the same: transmitting data from the sensors to the base station. However, these data are susceptible to different factors that may affect the collected data efficiency or the network functioning, and therefore the desired quality of service (QoS). In this context, one of the main issues requiring more research and adapted solutions is the outlier detection problem. The challenge is to detect outliers and classify them as either errors to be ignored, or important events requiring actions to prevent further service degradation. In this paper, we propose a comprehensive literature review of recent outlier detection techniques used in the IoTs context. First, we provide the fundamentals of outlier detection while discussing the different sources of an outlier, the existing approaches, how we can evaluate an outlier detection technique, and the challenges facing designing such techniques. Second, comparison and discussion of the most recent outlier detection techniques are presented and classified into seven main categories, which are: statistical-based, clustering-based, nearest neighbour-based, classification-based, artificial intelligent-based, spectral decomposition-based, and hybrid-based. For each category, available techniques are discussed, while highlighting the advantages and disadvantages of each of them. The related works for each of them are presented. Finally, a comparative study for these techniques is provided.},
DOI = {10.3390/jsan11010004}
}



@Article{s22010359,
AUTHOR = {Chen, Tzung-Shi and Chen, Jen-Jee and Gao, Xiang-You and Chen, Tzung-Cheng},
TITLE = {Mobile Charging Strategy for Wireless Rechargeable Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {359},
URL = {https://www.mdpi.com/1424-8220/22/1/359},
PubMedID = {35009897},
ISSN = {1424-8220},
ABSTRACT = {In a wireless sensor network, the sensing and data transmission for sensors will cause energy depletion, which will lead to the inability to complete the tasks. To solve this problem, wireless rechargeable sensor networks (WRSNs) have been developed to extend the lifetime of the entire network. In WRSNs, a mobile charging robot (MR) is responsible for wireless charging each sensor battery and collecting sensory data from the sensor simultaneously. Thereby, MR needs to traverse along a designed path for all sensors in the WRSNs. In this paper, dual-side charging strategies are proposed for MR traversal planning, which minimize the MR traversal path length, energy consumption, and completion time. Based on MR dual-side charging, neighboring sensors in both sides of a designated path can be wirelessly charged by MR and sensory data sent to MR simultaneously. The constructed path is based on the power diagram according to the remaining power of sensors and distances among sensors in a WRSN. While the power diagram is built, charging strategies with dual-side charging capability are determined accordingly. In addition, a clustering-based approach is proposed to improve minimizing MR moving total distance, saving charging energy and total completion time in a round. Moreover, integrated strategies that apply a clustering-based approach on the dual-side charging strategies are presented in WRSNs. The simulation results show that, no matter with or without clustering, the performances of proposed strategies outperform the baseline strategies in three respects, energy saving, total distance reduced, and completion time reduced for MR in WSRNs.},
DOI = {10.3390/s22010359}
}



@Article{telecom3010003,
AUTHOR = {Faysal, Jabed Al and Mostafa, Sk Tahmid and Tamanna, Jannatul Sultana and Mumenin, Khondoker Mirazul and Arifin, Md. Mashrur and Awal, Md. Abdul and Shome, Atanu and Mostafa, Sheikh Shanawaz},
TITLE = {XGB-RF: A Hybrid Machine Learning Approach for IoT Intrusion Detection},
JOURNAL = {Telecom},
VOLUME = {3},
YEAR = {2022},
NUMBER = {1},
PAGES = {52--69},
URL = {https://www.mdpi.com/2673-4001/3/1/3},
ISSN = {2673-4001},
ABSTRACT = {In the past few years, Internet of Things (IoT) devices have evolved faster and the use of these devices is exceedingly increasing to make our daily activities easier than ever. However, numerous security flaws persist on IoT devices due to the fact that the majority of them lack the memory and computing resources necessary for adequate security operations. As a result, IoT devices are affected by a variety of attacks. A single attack on network systems or devices can lead to significant damages in data security and privacy. However, machine-learning techniques can be applied to detect IoT attacks. In this paper, a hybrid machine learning scheme called XGB-RF is proposed for detecting intrusion attacks. The proposed hybrid method was applied to the N-BaIoT dataset containing hazardous botnet attacks. Random forest (RF) was used for the feature selection and eXtreme Gradient Boosting (XGB) classifier was used to detect different types of attacks on IoT environments. The performance of the proposed XGB-RF scheme is evaluated based on several evaluation metrics and demonstrates that the model successfully detects 99.94% of the attacks. After comparing it with state-of-the-art algorithms, our proposed model has achieved better performance for every metric. As the proposed scheme is capable of detecting botnet attacks effectively, it can significantly contribute to reducing the security concerns associated with IoT systems.},
DOI = {10.3390/telecom3010003}
}



@Article{s22020411,
AUTHOR = {Awan, Saba and Javaid, Nadeem and Ullah, Sameeh and Khan, Asad Ullah and Qamar, Ali Mustafa and Choi, Jin-Ghoo},
TITLE = {Blockchain Based Secure Routing and Trust Management in Wireless Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {411},
URL = {https://www.mdpi.com/1424-8220/22/2/411},
ISSN = {1424-8220},
ABSTRACT = {In this paper, an encryption and trust evaluation model is proposed on the basis of a blockchain in which the identities of the Aggregator Nodes (ANs) and Sensor Nodes (SNs) are stored. The authentication of ANs and SNs is performed in public and private blockchains, respectively. However, inauthentic nodes utilize the network&rsquo;s resources and perform malicious activities. Moreover, the SNs have limited energy, transmission range and computational capabilities, and are attacked by malicious nodes. Afterwards, the malicious nodes transmit wrong information of the route and increase the number of retransmissions due to which the SNs&rsquo; energy is rapidly consumed. The lifespan of the wireless sensor network is reduced due to the rapid energy dissipation of the SNs. Furthermore, the throughput increases and packet loss increase with the presence of malicious nodes in the network. The trust values of SNs are computed to eradicate the malicious nodes from the network. Secure routing in the network is performed considering residual energy and trust values of the SNs. Moreover, the Rivest&ndash;Shamir&ndash;Adleman (RSA), a cryptosystem that provides asymmetric keys, is used for securing data transmission. The simulation results show the effectiveness of the proposed model in terms of high packet delivery ratio.},
DOI = {10.3390/s22020411}
}



@Article{electronics11020173,
AUTHOR = {Vladyko, Andrei and Elagin, Vasiliy and Spirkina, Anastasia and Muthanna, Ammar and Ateya, Abdelhamied A.},
TITLE = {Distributed Edge Computing with Blockchain Technology to Enable Ultra-Reliable Low-Latency V2X Communications},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {173},
URL = {https://www.mdpi.com/2079-9292/11/2/173},
ISSN = {2079-9292},
ABSTRACT = {Vehicular communication is a promising technology that has been announced as a main use-case of the fifth-generation cellular system (5G). Vehicle-to-everything (V2X) is the vehicular communication paradigm that enables the communications and interactions between vehicles and other network entities, e.g., road-side units (RSUs). This promising technology faces many challenges related to reliability, availability and security of the exchanged data. To this end, this work aims to solve the scientific problem of building a vehicular network architecture for reliable delivery of correct and uncompromised data within the V2X concept to improve the safety of road users, using blockchain technology and mobile edge computing (MEC). The proposed work provides a formalized mathematical model of the system, taking into account the interconnection of objects and V2X information channels and an energy-efficient offloading algorithm to manage traffic offloading to the MEC server. The main applications of the blockchain and MEC technology in the developed system are discussed. Furthermore, the developed system, with the introduced sub-systems and algorithms, was evaluated over a reliable environment, for different simulation scenarios, and the obtained results are discussed.},
DOI = {10.3390/electronics11020173}
}



@Article{s22020450,
AUTHOR = {Abreha, Haftay Gebreslasie and Hayajneh, Mohammad and Serhani, Mohamed Adel},
TITLE = {Federated Learning in Edge Computing: A Systematic Survey},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {450},
URL = {https://www.mdpi.com/1424-8220/22/2/450},
ISSN = {1424-8220},
ABSTRACT = {Edge Computing (EC) is a new architecture that extends Cloud Computing (CC) services closer to data sources. EC combined with Deep Learning (DL) is a promising technology and is widely used in several applications. However, in conventional DL architectures with EC enabled, data producers must frequently send and share data with third parties, edge or cloud servers, to train their models. This architecture is often impractical due to the high bandwidth requirements, legalization, and privacy vulnerabilities. The Federated Learning (FL) concept has recently emerged as a promising solution for mitigating the problems of unwanted bandwidth loss, data privacy, and legalization. FL can co-train models across distributed clients, such as mobile phones, automobiles, hospitals, and more, through a centralized server, while maintaining data localization. FL can therefore be viewed as a stimulating factor in the EC paradigm as it enables collaborative learning and model optimization. Although the existing surveys have taken into account applications of FL in EC environments, there has not been any systematic survey discussing FL implementation and challenges in the EC paradigm. This paper aims to provide a systematic survey of the literature on the implementation of FL in EC environments with a taxonomy to identify advanced solutions and other open problems. In this survey, we review the fundamentals of EC and FL, then we review the existing related works in FL in EC. Furthermore, we describe the protocols, architecture, framework, and hardware requirements for FL implementation in the EC environment. Moreover, we discuss the applications, challenges, and related existing solutions in the edge FL. Finally, we detail two relevant case studies of applying FL in EC, and we identify open issues and potential directions for future research. We believe this survey will help researchers better understand the connection between FL and EC enabling technologies and concepts.},
DOI = {10.3390/s22020450}
}



@Article{s22020456,
AUTHOR = {Bustamante-Bello, Rogelio and García-Barba, Alec and Arce-Saenz, Luis A. and Curiel-Ramirez, Luis A. and Izquierdo-Reyes, Javier and Ramirez-Mendoza, Ricardo A.},
TITLE = {Visualizing Street Pavement Anomalies through Fog Computing V2I Networks and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {456},
URL = {https://www.mdpi.com/1424-8220/22/2/456},
ISSN = {1424-8220},
ABSTRACT = {Analyzing data related to the conditions of city streets and avenues could help to make better decisions about public spending on mobility. Generally, streets and avenues are fixed as soon as they have a citizen report or when a major incident occurs. However, it is uncommon for cities to have real-time reactive systems that detect the different problems they have to fix on the pavement. This work proposes a solution to detect anomalies in streets through state analysis using sensors within the vehicles that travel daily and connecting them to a fog-computing architecture on a V2I network. The system detects and classifies the main road problems or abnormal conditions in streets and avenues using Machine Learning Algorithms (MLA), comparing roughness against a flat reference. An instrumented vehicle obtained the reference through accelerometry sensors and then sent the data through a mid-range communication system. With these data, the system compared an Artificial Neural Network (supervised MLA) and a K-Nearest Neighbor (Supervised MLA) to select the best option to handle the acquired data. This system makes it desirable to visualize the streets&rsquo; quality and map the areas with the most significant anomalies.},
DOI = {10.3390/s22020456}
}



@Article{electronics11020198,
AUTHOR = {Abdullahi, Mujaheed and Baashar, Yahia and Alhussian, Hitham and Alwadain, Ayed and Aziz, Norshakirah and Capretz, Luiz Fernando and Abdulkadir, Said Jadid},
TITLE = {Detecting Cybersecurity Attacks in Internet of Things Using Artificial Intelligence Methods: A Systematic Literature Review},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {198},
URL = {https://www.mdpi.com/2079-9292/11/2/198},
ISSN = {2079-9292},
ABSTRACT = {In recent years, technology has advanced to the fourth industrial revolution (Industry 4.0), where the Internet of things (IoTs), fog computing, computer security, and cyberattacks have evolved exponentially on a large scale. The rapid development of IoT devices and networks in various forms generate enormous amounts of data which in turn demand careful authentication and security. Artificial intelligence (AI) is considered one of the most promising methods for addressing cybersecurity threats and providing security. In this study, we present a systematic literature review (SLR) that categorize, map and survey the existing literature on AI methods used to detect cybersecurity attacks in the IoT environment. The scope of this SLR includes an in-depth investigation on most AI trending techniques in cybersecurity and state-of-art solutions. A systematic search was performed on various electronic databases (SCOPUS, Science Direct, IEEE Xplore, Web of Science, ACM, and MDPI). Out of the identified records, 80 studies published between 2016 and 2021 were selected, surveyed and carefully assessed. This review has explored deep learning (DL) and machine learning (ML) techniques used in IoT security, and their effectiveness in detecting attacks. However, several studies have proposed smart intrusion detection systems (IDS) with intelligent architectural frameworks using AI to overcome the existing security and privacy challenges. It is found that support vector machines (SVM) and random forest (RF) are among the most used methods, due to high accuracy detection another reason may be efficient memory. In addition, other methods also provide better performance such as extreme gradient boosting (XGBoost), neural networks (NN) and recurrent neural networks (RNN). This analysis also provides an insight into the AI roadmap to detect threats based on attack categories. Finally, we present recommendations for potential future investigations.},
DOI = {10.3390/electronics11020198}
}



@Article{s22030709,
AUTHOR = {Prabakaran, Senthil and Ramar, Ramalakshmi and Hussain, Irshad and Kavin, Balasubramanian Prabhu and Alshamrani, Sultan S. and AlGhamdi, Ahmed Saeed and Alshehri, Abdullah},
TITLE = {Predicting Attack Pattern via Machine Learning by Exploiting Stateful Firewall as Virtual Network Function in an SDN Network},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {709},
URL = {https://www.mdpi.com/1424-8220/22/3/709},
ISSN = {1424-8220},
ABSTRACT = {Decoupled data and control planes in Software Defined Networks (SDN) allow them to handle an increasing number of threats by limiting harmful network links at the switching stage. As storage, high-end servers, and network devices, Network Function Virtualization (NFV) is designed to replace purpose-built network elements with VNFs (Virtualized Network Functions). A Software Defined Network Function Virtualization (SDNFV) network is designed in this paper to boost network performance. Stateful firewall services are deployed as VNFs in the SDN network in this article to offer security and boost network scalability. The SDN controller&rsquo;s role is to develop a set of guidelines and rules to avoid hazardous network connectivity. Intruder assaults that employ numerous socket addresses cannot be adequately protected by these strategies. Machine learning algorithms are trained using traditional network threat intelligence data to identify potentially malicious linkages and probable attack targets. Based on conventional network data (DT), Bayesian Network (BayesNet), Naive-Bayes, C4.5, and Decision Table (DT) algorithms are used to predict the target host that will be attacked. The experimental results shows that the Bayesian Network algorithm achieved an average prediction accuracy of 92.87%, Native&ndash;Bayes Algorithm achieved an average prediction accuracy of 87.81%, C4.5 Algorithm achieved an average prediction accuracy of 84.92%, and the Decision Tree algorithm achieved an average prediction accuracy of 83.18%. There were 451 k login attempts from 178 different countries, with over 70 k source IP addresses and 40 k source port addresses recorded in a large dataset from nine honeypot servers.},
DOI = {10.3390/s22030709}
}



@Article{sym14020195,
AUTHOR = {Ma, Xiaohang and Liao, Lingxia and Li, Zhi and Lai, Roy Xiaorong and Zhang, Miao},
TITLE = {Applying Federated Learning in Software-Defined Networks: A Survey},
JOURNAL = {Symmetry},
VOLUME = {14},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {195},
URL = {https://www.mdpi.com/2073-8994/14/2/195},
ISSN = {2073-8994},
ABSTRACT = {Federated learning (FL) is a type of distributed machine learning approacs that trains global models through the collaboration of participants. It protects data privacy as participants only contribute local models instead of sharing private local data. However, the performance of FL highly relies on the number of participants and their contributions. When applying FL over conventional computer networks, attracting more participants, encouraging participants to contribute more local resources, and enabling efficient and effective collaboration among participants become very challenging. As software-defined networks (SDNs) enable open and flexible networking architecture with separate control and data planes, SDNs provide standardized protocols and specifications to enable fine-grained collaborations among devices. Applying FL approaches over SDNs can take use such advantages to address challenges. A SDN control plane can have multiple controllers organized in layers; the controllers in the lower layer can be placed in the network edge to deal with the asymmetries in the attached switches and hosts, and the controller in the upper layer can supervise the whole network centrally and globally. Applying FL in SDNs with a layered-distributed control plane may be able to protect the data privacy of each participant while improving collaboration among participants to produce higher-quality models over asymmetric networks. Accordingly, this paper aims to make a comprehensive survey on the related mechanisms and solutions that enable FL in SDNs. It highlights three major challenges, an incentive mechanism, privacy and security, and model aggregation, which affect the quality and quantity of participants, the security and privacy in model transferring, and the performance of the global model, respectively. The state of the art in mechanisms and solutions that can be applied to address such challenges in the current literature are categorized based on the challenges they face, followed by suggestions of future research directions. To the best of our knowledge, this work is the first effort in surveying the state of the art in combining FL with SDNs.},
DOI = {10.3390/sym14020195}
}



@Article{s22030879,
AUTHOR = {Antonić, Martina and Antonić, Aleksandar and Podnar Žarko, Ivana},
TITLE = {Bloom Filter Approach for Autonomous Data Acquisition in the Edge-Based MCS Scenario},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {879},
URL = {https://www.mdpi.com/1424-8220/22/3/879},
ISSN = {1424-8220},
ABSTRACT = {Mobile crowdsensing (MCS) is a sensing paradigm that allows ordinary citizens to use mobile and wearable technologies and become active observers of their surroundings. MCS services generate a massive amount of data due to the vast number of devices engaging in MCS tasks, and the intrinsic mobility of users can quickly make information obsolete, requiring efficient data processing. Our previous work shows that the Bloom filter (BF) is a promising technique to reduce the quantity of redundant data in a hierarchical edge-based MCS ecosystem, allowing users engaging in MCS tasks to make autonomous informed decisions on whether or not to transmit data. This paper extends the proposed BF algorithm to accept multiple data readings of the same type at an exact location if the MCS task requires such functionality. In addition, we thoroughly evaluate the overall behavior of our approach by taking into account the overhead generated in communication between edge servers and end-user devices on a real-world dataset. Our results indicate that using the proposed algorithm makes it possible to significantly reduce the amount of transmitted data and achieve energy savings up to 62% compared to a baseline approach.},
DOI = {10.3390/s22030879}
}



