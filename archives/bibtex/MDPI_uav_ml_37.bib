
@Article{rs11060676,
AUTHOR = {Angelopoulou, Theodora and Tziolas, Nikolaos and Balafoutis, Athanasios and Zalidis, George and Bochtis, Dionysis},
TITLE = {Remote Sensing Techniques for Soil Organic Carbon Estimation: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {676},
URL = {https://www.mdpi.com/2072-4292/11/6/676},
ISSN = {2072-4292},
ABSTRACT = {Towards the need for sustainable development, remote sensing (RS) techniques in the Visible-Near Infrared&ndash;Shortwave Infrared (VNIR&ndash;SWIR, 400&ndash;2500 nm) region could assist in a more direct, cost-effective and rapid manner to estimate important indicators for soil monitoring purposes. Soil reflectance spectroscopy has been applied in various domains apart from laboratory conditions, e.g., sensors mounted on satellites, aircrafts and Unmanned Aerial Systems. The aim of this review is to illustrate the research made for soil organic carbon estimation, with the use of RS techniques, reporting the methodology and results of each study. It also aims to provide a comprehensive introduction in soil spectroscopy for those who are less conversant with the subject. In total, 28 journal articles were selected and further analysed. It was observed that prediction accuracy reduces from Unmanned Aerial Systems (UASs) to satellite platforms, though advances in machine learning techniques could further assist in the generation of better calibration models. There are some challenges concerning atmospheric, radiometric and geometric corrections, vegetation cover, soil moisture and roughness that still need to be addressed. The advantages and disadvantages of each approach are highlighted and future considerations are also discussed at the end.},
DOI = {10.3390/rs11060676}
}



@Article{s19061400,
AUTHOR = {Kulich, Miroslav and Kubalík, Jiří and Přeučil, Libor},
TITLE = {An Integrated Approach to Goal Selection in Mobile Robot Exploration},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {1400},
URL = {https://www.mdpi.com/1424-8220/19/6/1400},
ISSN = {1424-8220},
ABSTRACT = {This paper deals with the problem of autonomous navigation of a mobile robot in an unknown 2D environment to fully explore the environment as efficiently as possible. We assume a terrestrial mobile robot equipped with a ranging sensor with a limited range and     360 &deg;     field of view. The key part of the exploration process is formulated as the d-Watchman Route Problem which consists of two coupled tasks&mdash;candidate goals generation and finding an optimal path through a subset of goals&mdash;which are solved in each exploration step. The latter has been defined as a constrained variant of the Generalized Traveling Salesman Problem and solved using an evolutionary algorithm. An evolutionary algorithm that uses an indirect representation and the nearest neighbor based constructive procedure was proposed to solve this problem. Individuals evolved in this evolutionary algorithm do not directly code the solutions to the problem. Instead, they represent sequences of instructions to construct a feasible solution. The problems with efficiently generating feasible solutions typically arising when applying traditional evolutionary algorithms to constrained optimization problems are eliminated this way. The proposed exploration framework was evaluated in a simulated environment on three maps and the time needed to explore the whole environment was compared to state-of-the-art exploration methods. Experimental results show that our method outperforms the compared ones in environments with a low density of obstacles by up to     12.5 %    , while it is slightly worse in office-like environments by     4.5 %     at maximum. The framework has also been deployed on a real robot to demonstrate the applicability of the proposed solution with real hardware.},
DOI = {10.3390/s19061400}
}



@Article{rs11060690,
AUTHOR = {Liu, Shengjie and Qi, Zhixin and Li, Xia and Yeh, Anthony Gar-On},
TITLE = {Integration of Convolutional Neural Networks and Object-Based Post-Classification Refinement for Land Use and Land Cover Mapping with Optical and SAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {690},
URL = {https://www.mdpi.com/2072-4292/11/6/690},
ISSN = {2072-4292},
ABSTRACT = {Object-based image analysis (OBIA) has been widely used for land use and land cover (LULC) mapping using optical and synthetic aperture radar (SAR) images because it can utilize spatial information, reduce the effect of salt and pepper, and delineate LULC boundaries. With recent advances in machine learning, convolutional neural networks (CNNs) have become state-of-the-art algorithms. However, CNNs cannot be easily integrated with OBIA because the processing unit of CNNs is a rectangular image, whereas that of OBIA is an irregular image object. To obtain object-based thematic maps, this study developed a new method that integrates object-based post-classification refinement (OBPR) and CNNs for LULC mapping using Sentinel optical and SAR data. After producing the classification map by CNN, each image object was labeled with the most frequent land cover category of its pixels. The proposed method was tested on the optical-SAR Sentinel Guangzhou dataset with 10 m spatial resolution, the optical-SAR Zhuhai-Macau local climate zones (LCZ) dataset with 100 m spatial resolution, and a hyperspectral benchmark the University of Pavia with 1.3 m spatial resolution. It outperformed OBIA support vector machine (SVM) and random forest (RF). SVM and RF could benefit more from the combined use of optical and SAR data compared with CNN, whereas spatial information learned by CNN was very effective for classification. With the ability to extract spatial features and maintain object boundaries, the proposed method considerably improved the classification accuracy of urban ground targets. It achieved overall accuracy (OA) of 95.33% for the Sentinel Guangzhou dataset, OA of 77.64% for the Zhuhai-Macau LCZ dataset, and OA of 95.70% for the University of Pavia dataset with only 10 labeled samples per class.},
DOI = {10.3390/rs11060690}
}



@Article{rs11060691,
AUTHOR = {Wu, Jintao and Yang, Guijun and Yang, Xiaodong and Xu, Bo and Han, Liang and Zhu, Yaohui},
TITLE = {Automatic Counting of in situ Rice Seedlings from UAV Images Based on a Deep Fully Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {691},
URL = {https://www.mdpi.com/2072-4292/11/6/691},
ISSN = {2072-4292},
ABSTRACT = {The number of rice seedlings in the field is one of the main agronomic components for determining rice yield. This counting task, however, is still mainly performed using human vision rather than computer vision and is thus cumbersome and time-consuming. A fast and accurate alternative method of acquiring such data may contribute to monitoring the efficiency of crop management practices, to earlier estimations of rice yield, and as a phenotyping trait in breeding programs. In this paper, we propose an efficient method that uses computer vision to accurately count rice seedlings in a digital image. First, an unmanned aerial vehicle (UAV) equipped with red-green-blue (RGB) cameras was used to acquire field images at the seedling stage. Next, we use a regression network (Basic Network) inspired by a deep fully convolutional neural network to regress the density map and estimate the number of rice seedlings for a given UAV image. Finally, an improved version of the Basic Network, the Combined Network, is also proposed to further improve counting accuracy. To explore the efficacy of the proposed method, a novel rice seedling counting (RSC) dataset was built, which consisted of 40 images (where the number of seedlings varied between 3732 and 16,173) and corresponding manually-dotted annotations. The results demonstrated high average accuracy (higher than 93%) between counts according to the proposed method and manual (UAV image-based) rice seedling counts, and very good performance, with a high coefficient of determination (R2) (around 0.94). In conclusion, the results indicate that the proposed method is an efficient alternative for large-scale counting of rice seedlings, and offers a new opportunity for yield estimation. The RSC dataset and source code are available online.},
DOI = {10.3390/rs11060691}
}



@Article{rs11060704,
AUTHOR = {Mcilwaine, Ben and Casado, Monica Rivas and Leinster, Paul},
TITLE = {Using 1st Derivative Reflectance Signatures within a Remote Sensing Framework to Identify Macroalgae in Marine Environments},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {704},
URL = {https://www.mdpi.com/2072-4292/11/6/704},
ISSN = {2072-4292},
ABSTRACT = {Macroalgae blooms (MABs) are a global natural hazard that are likely to increase in occurrence with climate change and increased agricultural runoff. MABs can cause major issues for indigenous species, fish farms, nuclear power stations, and tourism activities. This project focuses on the impacts of MABs on the operations of a British nuclear power station. However, the outputs and findings are also of relevance to other coastal operators with similar problems. Through the provision of an early-warning detection system for MABs, it should be possible to minimize the damaging effects and possibly avoid them altogether. Current methods based on satellite imagery cannot be used to detect low-density mobile vegetation at various water depths. This work is the first step towards providing a system that can warn a coastal operator 6&ndash;8 h prior to a marine ingress event. A fundamental component of such a warning system is the spectral reflectance properties of the problematic macroalgae species. This is necessary to optimize the detection capability for the problematic macroalgae in the marine environment. We measured the reflectance signatures of eight species of macroalgae that we sampled in the vicinity of the power station. Only wavelengths below 900 nm (700 nm for similarity percentage (SIMPER)) were analyzed, building on current methodologies. We then derived 1st derivative spectra of these eight sampled species. A multifaceted univariate and multivariate approach was used to visualize the spectral reflectance, and an analysis of similarities (ANOSIM) provided a species-level discrimination rate of 85% for all possible pairwise comparisons. A SIMPER analysis was used to detect wavebands that consistently contributed to the simultaneous discrimination of all eight sampled macroalgae species to both a group level (535&ndash;570 nm), and to a species level (570&ndash;590 nm). Sampling locations were confirmed using a fixed-wing unmanned aerial vehicle (UAV), with the collected imagery being used to produce a single orthographic image via standard photogrammetric processes. The waveband found to contribute consistently to group-level discrimination has previously been found to be associated with photosynthetic pigmentation, whereas the species-level discriminatory waveband did not share this association. This suggests that the photosynthetic pigments were not spectrally diverse enough to successfully distinguish all eight species. We suggest that future work should investigate a Charge-Coupled Device (CCD)-based sensor using the wavebands highlighted above. This should facilitate the development of a regional-scale early-warning MAB detection system using UAVs, and help inform optimum sensor filter selection.},
DOI = {10.3390/rs11060704}
}



@Article{biom9030116,
AUTHOR = {Castro, Tarsila G. and Munteanu, Florentina-Daniela and Cavaco-Paulo, Artur},
TITLE = {Electrostatics of Tau Protein by Molecular Dynamics},
JOURNAL = {Biomolecules},
VOLUME = {9},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {116},
URL = {https://www.mdpi.com/2218-273X/9/3/116},
PubMedID = {30909607},
ISSN = {2218-273X},
ABSTRACT = {Tau is a microtubule-associated protein that promotes microtubule assembly and stability. This protein is implicated in several neurodegenerative diseases, including Alzheimer’s. To date, the three-dimensional (3D) structure of tau has not been fully solved, experimentally. Even the most recent information is sometimes controversial in regard to how this protein folds, interacts, and behaves. Predicting the tau structure and its profile sheds light on the knowledge about its properties and biological function, such as the binding to microtubules (MT) and, for instance, the effect on ionic conductivity. Our findings on the tau structure suggest a disordered protein, with discrete portions of well-defined secondary structure, mostly at the microtubule binding region. In addition, the first molecular dynamics simulation of full-length tau along with an MT section was performed, unveiling tau structure when associated with MT and interaction sites. Electrostatics and conductivity were also examined to understand how tau affects the ions in the intracellular fluid environment. Our results bring a new insight into tau and tubulin MT proteins, their characteristics, and the structure–function relationship.},
DOI = {10.3390/biom9030116}
}



@Article{rs11060714,
AUTHOR = {Salgadoe, Arachchige Surantha Ashan and Robson, Andrew James and Lamb, David William and Schneider, Derek},
TITLE = {A Non-Reference Temperature Histogram Method for Determining Tc from Ground-Based Thermal Imagery of Orchard Tree Canopies},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {714},
URL = {https://www.mdpi.com/2072-4292/11/6/714},
ISSN = {2072-4292},
ABSTRACT = {Obtaining average canopy temperature (Tc) by thresholding canopy pixels from on-ground thermal imagery has historically been undertaken using &lsquo;wet&rsquo; and &lsquo;dry&rsquo; reference surfaces in the field (reference temperature thresholding). However, this method is extremely time inefficient and can suffer inaccuracies if the surfaces are non-standardised or unable to stabilise with the environment. The research presented in this paper evaluates non-reference techniques to obtain average canopy temperature (Tc) from thermal imagery of avocado trees, both for the shaded side and sunlit side, without the need of reference temperature values. A sample of 510 thermal images (from 130 avocado trees) were acquired with a FLIR B250 handheld thermal imaging camera. Two methods based on temperature histograms were evaluated for removing non-canopy-related pixel information from the analysis, enabling Tc to be determined. These approaches included: 1) Histogram gradient thresholding based on temperature intensity changes (HG); and 2) histogram thresholding at one or more standard deviation (SD) above and below the mean. The HG method was found to be more accurate (R2 &gt; 0.95) than the SD method in defining canopy pixels and calculating Tc from each thermal image (shaded and sunlit) when compared to the standard reference temperature thresholding method. The results from this study present an alternative non-reference method for determining Tc from ground-based thermal imagery without the need of calibration surfaces. As such, it offers a more efficient and computationally autonomous method that will ultimately support the greater adoption of non-invasive thermal technologies within a precision agricultural system.},
DOI = {10.3390/rs11060714}
}



@Article{s19061479,
AUTHOR = {Jin, Ren and Jiang, Jiaqi and Qi, Yuhua and Lin, Defu and Song, Tao},
TITLE = {Drone Detection and Pose Estimation Using Relational Graph Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {1479},
URL = {https://www.mdpi.com/1424-8220/19/6/1479},
ISSN = {1424-8220},
ABSTRACT = {With the upsurge in use of Unmanned Aerial Vehicles (UAVs), drone detection and pose estimation by using optical sensors becomes an important research subject in cooperative flight and low-altitude security. The existing technology only obtains the position of the target UAV based on object detection methods. To achieve better adaptability and enhanced cooperative performance, the attitude information of the target drone becomes a key message to understand its state and intention, e.g., the acceleration of quadrotors. At present, most of the object 6D pose estimation algorithms depend on accurate pose annotation or a 3D target model, which costs a lot of human resource and is difficult to apply to non-cooperative targets. To overcome these problems, a quadrotor 6D pose estimation algorithm was proposed in this paper. It was based on keypoints detection (only need keypoints annotation), relational graph network and perspective-n-point (PnP) algorithm, which achieves state-of-the-art performance both in simulation and real scenario. In addition, the inference ability of our relational graph network to the keypoints of four motors was also evaluated. The accuracy and speed were improved significantly compared with the state-of-the-art keypoints detection algorithm.},
DOI = {10.3390/s19061479}
}



@Article{rs11060733,
AUTHOR = {Windrim, Lloyd and Bryson, Mitch and McLean, Michael and Randle, Jeremy and Stone, Christine},
TITLE = {Automated Mapping of Woody Debris over Harvested Forest Plantations Using UAVs, High-Resolution Imagery, and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/11/6/733},
ISSN = {2072-4292},
ABSTRACT = {Surveying of woody debris left over from harvesting operations on managed forests is an important step in monitoring site quality, managing the extraction of residues and reconciling differences in pre-harvest inventories and actual timber yields. Traditional methods for post-harvest survey involving manual assessment of debris on the ground over small sample plots are labor-intensive, time-consuming, and do not scale well to heterogeneous landscapes. In this paper, we propose and evaluate new automated methods for the collection and interpretation of high-resolution, Unmanned Aerial Vehicle (UAV)-borne imagery over post-harvested forests for estimating quantities of fine and coarse woody debris. Using high-resolution, geo-registered color mosaics generated from UAV-borne images, we develop manual and automated processing methods for detecting, segmenting and counting both fine and coarse woody debris, including tree stumps, exploiting state-of-the-art machine learning and image processing techniques. Results are presented using imagery over a post-harvested compartment in a Pinus radiata plantation and demonstrate the capacity for both manual image annotations and automated image processing to accurately detect and quantify coarse woody debris and stumps left over after harvest, providing a cost-effective and scalable survey method for forest managers.},
DOI = {10.3390/rs11060733}
}



@Article{ijgi8040160,
AUTHOR = {Liu, Bingxin and Li, Ying and Li, Guannan and Liu, Anling},
TITLE = {A Spectral Feature Based Convolutional Neural Network for Classification of Sea Surface Oil Spill},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {160},
URL = {https://www.mdpi.com/2220-9964/8/4/160},
ISSN = {2220-9964},
ABSTRACT = {Spectral characteristics play an important role in the classification of oil film, but the presence of too many bands can lead to information redundancy and reduced classification accuracy. In this study, a classification model that combines spectral indices-based band selection (SIs) and one-dimensional convolutional neural networks was proposed to realize automatic oil films classification using hyperspectral remote sensing images. Additionally, for comparison, the minimum Redundancy Maximum Relevance (mRMR) was tested for reducing the number of bands. The support vector machine (SVM), random forest (RF), and Hu&rsquo;s convolutional neural networks (CNN) were trained and tested. The results show that the accuracy of classifications through the one dimensional convolutional neural network (1D CNN) models surpassed the accuracy of other machine learning algorithms such as SVM and RF. The model of SIs+1D CNN could produce a relatively higher accuracy oil film distribution map within less time than other models.},
DOI = {10.3390/ijgi8040160}
}



@Article{rs11070736,
AUTHOR = {Hu, Jie and Peng, Jie and Zhou, Yin and Xu, Dongyun and Zhao, Ruiying and Jiang, Qingsong and Fu, Tingting and Wang, Fei and Shi, Zhou},
TITLE = {Quantitative Estimation of Soil Salinity Using UAV-Borne Hyperspectral and Satellite Multispectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {736},
URL = {https://www.mdpi.com/2072-4292/11/7/736},
ISSN = {2072-4292},
ABSTRACT = {Soil salinization is a global issue resulting in soil degradation, arable land loss and ecological environmental deterioration. Over the decades, multispectral and hyperspectral remote sensing have enabled efficient and cost-effective monitoring of salt-affected soils. However, the potential of hyperspectral sensors installed on an unmanned aerial vehicle (UAV) to estimate and map soil salinity has not been thoroughly explored. This study quantitatively characterized and estimated field-scale soil salinity using an electromagnetic induction (EMI) equipment and a hyperspectral camera installed on a UAV platform. In addition, 30 soil samples (0~20 cm) were collected in each field for the lab measurements of electrical conductivity. First, the apparent electrical conductivity (ECa) values measured by EMI were calibrated using the lab measured electrical conductivity derived from soil samples based on empirical line method. Second, the soil salinity was quantitatively estimated using the random forest (RF) regression method based on the reflectance factors of UAV hyperspectral images and satellite multispectral data. The performance of models was assessed by Lin&rsquo;s concordance coefficient (CC), ratio of performance to deviation (RPD), and root mean square error (RMSE). Finally, the soil salinity of three study fields with different land cover were mapped. The results showed that bare land (field A) exhibited the most severe salinity, followed by dense vegetation area (field C) and sparse vegetation area (field B). The predictive models using UAV data outperformed those derived from GF-2 data with lower RMSE, higher CC and RPD values, and the most accurate UAV-derived model was developed using 62 hyperspectral bands of the image of the field A with the RMSE, CC, and RPD values of 1.40 dS m&minus;1, 0.94, and 2.98, respectively. Our results indicated that UAV-borne hyperspectral imager is a useful tool for field-scale soil salinity monitoring and mapping. With the help of the EMI technique, quantitative estimation of surface soil salinity is critical to decision-making in arid land management and saline soil reclamation.},
DOI = {10.3390/rs11070736}
}



@Article{rs11070740,
AUTHOR = {Maimaitiyiming, Matthew and Sagan, Vasit and Sidike, Paheding and Kwasniewski, Misha T.},
TITLE = {Dual Activation Function-Based Extreme Learning Machine (ELM) for Estimating Grapevine Berry Yield and Quality},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {740},
URL = {https://www.mdpi.com/2072-4292/11/7/740},
ISSN = {2072-4292},
ABSTRACT = {Reliable assessment of grapevine productivity is a destructive and time-consuming process. In addition, the mixed effects of grapevine water status and scion-rootstock interactions on grapevine productivity are not always linear. Despite the potential opportunity of applying remote sensing and machine learning techniques to predict plant traits, there are still limitations to previously studied techniques for vine productivity due to the complexity of the system not being adequately modeled. During the 2014 and 2015 growing seasons, hyperspectral reflectance spectra were collected using a handheld spectroradiometer in a vineyard designed to investigate the effects of irrigation level (0%, 50%, and 100%) and rootstocks (1103 Paulsen, 3309 Couderc, SO4 and Chambourcin) on vine productivity. To assess vine productivity, it is necessary to measure factors related to fruit ripeness and not just yield, as an over cropped vine may produce high-yield but poor-quality fruit. Therefore, yield, Total Soluble Solids (TSS), Titratable Acidity (TA) and the ratio TSS/TA (maturation index, IMAD) were measured. A total of 20 vegetation indices were calculated from hyperspectral data and used as input for predictive model calibration. Prediction performance of linear/nonlinear multiple regression methods and Weighted Regularized Extreme Learning Machine (WRELM) were compared with our newly developed WRELM-TanhRe. The developed method is based on two activation functions: hyperbolic tangent (Tanh) and rectified linear unit (ReLU). The results revealed that WRELM and WRELM-TanhRe outperformed the widely used multiple regression methods when model performance was tested with an independent validation dataset. WRELM-TanhRe produced the highest prediction accuracy for all the berry yield and quality parameters (R2 of 0.522&ndash;0.682 and RMSE of 2&ndash;15%), except for TA, which was predicted best with WRELM (R2 of 0.545 and RMSE of 6%). The results demonstrate the value of combining hyperspectral remote sensing and machine learning methods for improving of berry yield and quality prediction.},
DOI = {10.3390/rs11070740}
}



@Article{s19071486,
AUTHOR = {Gebrehiwot, Asmamaw and Hashemi-Beni, Leila and Thompson, Gary and Kordjamshidi, Parisa and Langan, Thomas E.},
TITLE = {Deep Convolutional Neural Network for Flood Extent Mapping Using Unmanned Aerial Vehicles Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1486},
URL = {https://www.mdpi.com/1424-8220/19/7/1486},
ISSN = {1424-8220},
ABSTRACT = {Flooding is one of the leading threats of natural disasters to human life and property, especially in densely populated urban areas. Rapid and precise extraction of the flooded areas is key to supporting emergency-response planning and providing damage assessment in both spatial and temporal measurements. Unmanned Aerial Vehicles (UAV) technology has recently been recognized as an efficient photogrammetry data acquisition platform to quickly deliver high-resolution imagery because of its cost-effectiveness, ability to fly at lower altitudes, and ability to enter a hazardous area. Different image classification methods including SVM (Support Vector Machine) have been used for flood extent mapping. In recent years, there has been a significant improvement in remote sensing image classification using Convolutional Neural Networks (CNNs). CNNs have demonstrated excellent performance on various tasks including image classification, feature extraction, and segmentation. CNNs can learn features automatically from large datasets through the organization of multi-layers of neurons and have the ability to implement nonlinear decision functions. This study investigates the potential of CNN approaches to extract flooded areas from UAV imagery. A VGG-based fully convolutional network (FCN-16s) was used in this research. The model was fine-tuned and a k-fold cross-validation was applied to estimate the performance of the model on the new UAV imagery dataset. This approach allowed FCN-16s to be trained on the datasets that contained only one hundred training samples, and resulted in a highly accurate classification. Confusion matrix was calculated to estimate the accuracy of the proposed method. The image segmentation results obtained from FCN-16s were compared from the results obtained from FCN-8s, FCN-32s and SVMs. Experimental results showed that the FCNs could extract flooded areas precisely from UAV images compared to the traditional classifiers such as SVMs. The classification accuracy achieved by FCN-16s, FCN-8s, FCN-32s, and SVM for the water class was 97.52%, 97.8%, 94.20% and 89%, respectively.},
DOI = {10.3390/s19071486}
}



@Article{en12071204,
AUTHOR = {Zhao, Zhenbing and Zhen, Zhen and Zhang, Lei and Qi, Yincheng and Kong, Yinghui and Zhang, Ke},
TITLE = {Insulator Detection Method in Inspection Image Based on Improved Faster R-CNN},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1204},
URL = {https://www.mdpi.com/1996-1073/12/7/1204},
ISSN = {1996-1073},
ABSTRACT = {The detection of insulators in power transmission and transformation inspection images is the basis for insulator state detection and fault diagnosis in thereafter. Aiming at the detection of insulators with different aspect ratios and scales and ones with mutual occlusion, a method of insulator inspection image based on the improved faster region-convolutional neural network (R-CNN) is put forward in this paper. By constructing a power transmission and transformation insulation equipment detection dataset and fine-tuning the faster R-CNN model, the anchor generation method and non-maximum suppression (NMS) in the region proposal network (RPN) of the faster R-CNN model were improved, thus realizing a better detection of insulators. The experimental results show that the average precision (AP) value of the faster R-CNN model was increased to 0.818 with the improved anchor generation method under the VGG-16 Net. In addition, the detection effect of different aspect ratios and different scales of insulators in the inspection images was improved significantly, and the occlusion of insulators could be effectively distinguished and detected using the improved NMS.},
DOI = {10.3390/en12071204}
}



@Article{rs11070755,
AUTHOR = {Zhang, Xiaodong and Zhu, Kun and Chen, Guanzhou and Tan, Xiaoliang and Zhang, Lifei and Dai, Fan and Liao, Puyun and Gong, Yuanfu},
TITLE = {Geospatial Object Detection on High Resolution Remote Sensing Imagery Based on Double Multi-Scale Feature Pyramid Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {755},
URL = {https://www.mdpi.com/2072-4292/11/7/755},
ISSN = {2072-4292},
ABSTRACT = {Object detection on very-high-resolution (VHR) remote sensing imagery has attracted a lot of attention in the field of image automatic interpretation. Region-based convolutional neural networks (CNNs) have been vastly promoted in this domain, which first generate candidate regions and then accurately classify and locate the objects existing in these regions. However, the overlarge images, the complex image backgrounds and the uneven size and quantity distribution of training samples make the detection tasks more challenging, especially for small and dense objects. To solve these problems, an effective region-based VHR remote sensing imagery object detection framework named Double Multi-scale Feature Pyramid Network (DM-FPN) was proposed in this paper, which utilizes inherent multi-scale pyramidal features and combines the strong-semantic, low-resolution features and the weak-semantic, high-resolution features simultaneously. DM-FPN consists of a multi-scale region proposal network and a multi-scale object detection network, these two modules share convolutional layers and can be trained end-to-end. We proposed several multi-scale training strategies to increase the diversity of training data and overcome the size restrictions of the input images. We also proposed multi-scale inference and adaptive categorical non-maximum suppression (ACNMS) strategies to promote detection performance, especially for small and dense objects. Extensive experiments and comprehensive evaluations on large-scale DOTA dataset demonstrate the effectiveness of the proposed framework, which achieves mean average precision (mAP) value of 0.7927 on validation dataset and the best mAP value of 0.793 on testing dataset.},
DOI = {10.3390/rs11070755}
}



@Article{s19071517,
AUTHOR = {Farlik, Jan and Kratky, Miroslav and Casar, Josef and Stary, Vadim},
TITLE = {Multispectral Detection of Commercial Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1517},
URL = {https://www.mdpi.com/1424-8220/19/7/1517},
ISSN = {1424-8220},
ABSTRACT = {The fight against unmanned vehicles is nothing new; however, especially with the arrival of new technologies that are easily accessible for the wider population, new problems are arising. The deployment of small unmanned aerial vehicles (UAVs) by paramilitary organizations during conflicts around the world has become a reality, non-lethal &ldquo;paparazzi&rdquo; actions have become a common practice, and it is only a matter of time until the population faces lethal attacks. The basic prerequisite for direct defense against attacking UAVs is their detection. The authors of this paper analysed the possibility of detecting flying aircraft in several different electro-magnetic spectrum bands. Firstly, methods based on calculations and simulations were chosen, and experiments in laboratories and measurements of the exterior were subsequently performed. As a result, values of the radar cross section (RCS), the noise level, the surface temperature, and optical as well as acoustic traces of tested devices were quantified. The outputs obtained from calculated, simulated, and experimentally detected values were found via UAV detection distances using specific sensors working in corresponding parts of the frequency spectrum.},
DOI = {10.3390/s19071517}
}



@Article{rs11070759,
AUTHOR = {Li, Jin and Liu, Zilong},
TITLE = {Multispectral Transforms Using Convolution Neural Networks for Remote Sensing Multispectral Image Compression},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {759},
URL = {https://www.mdpi.com/2072-4292/11/7/759},
ISSN = {2072-4292},
ABSTRACT = {A multispectral image is a three-order tensor since it is a three-dimensional matrix, i.e., one spectral dimension and two spatial position dimensions. Multispectral image compression can be achieved by means of the advantages of tensor decomposition (TD), such as Nonnegative Tucker Decomposition (NTD). Unfortunately, the TD suffers from high calculation complexity and cannot be used in the on-board low-complexity case (e.g., multispectral cameras) that the hardware resources and power are limited. Here, we propose a low-complexity compression approach for multispectral images based on convolution neural networks (CNNs) with NTD. We construct a new spectral transform using CNNs, where the CNNs are able to transform the three-dimension spectral tensor from large-scale to a small-scale version. The NTD resources only allocate the small-scale three-dimension tensor to improve calculation efficiency. We obtain the optimized small-scale spectral tensor by the minimization of original and reconstructed three-dimension spectral tensor in self-learning CNNs. Then, the NTD is applied to the optimized three-dimension spectral tensor in the DCT domain to obtain the high compression performance. We experimentally confirmed the proposed method on multispectral images. Compared to the case that the new spectral tensor transform with CNNs is not applied to the original three-dimension spectral tensor at the same compression bit-rates, the reconstructed image quality could be improved. Compared with the full NTD-based method, the computation efficiency was obviously improved with only a small sacrifices of PSNR without affecting the quality of images.},
DOI = {10.3390/rs11070759}
}



@Article{en12071223,
AUTHOR = {Gao, Jianlei and Chai, Senchun and Zhang, Baihai and Xia, Yuanqing},
TITLE = {Research on Network Intrusion Detection Based on Incremental Extreme Learning Machine and Adaptive Principal Component Analysis},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1223},
URL = {https://www.mdpi.com/1996-1073/12/7/1223},
ISSN = {1996-1073},
ABSTRACT = {Recently, network attacks launched by malicious attackers have seriously affected modern life and enterprise production, and these network attack samples have the characteristic of type imbalance, which undoubtedly increases the difficulty of intrusion detection. In response to this problem, it would naturally be very meaningful to design an intrusion detection system (IDS) to effectively and quickly identify and detect malicious behaviors. In our work, we have proposed a method for an IDS-combined incremental extreme learning machine (I-ELM) with an adaptive principal component (A-PCA). In this method, the relevant features of network traffic are adaptively selected, where the best detection accuracy can then be obtained by I-ELM. We have used the NSL-KDD standard dataset and UNSW-NB15 standard dataset to evaluate the performance of our proposed method. Through analysis of the experimental results, we can see that our proposed method has better computation capacity, stronger generalization ability, and higher accuracy.},
DOI = {10.3390/en12071223}
}



@Article{app9071361,
AUTHOR = {Li, Yangyang and Wang, Ximing and Liu, Dianxiong and Guo, Qiuju and Liu, Xin and Zhang, Jie and Xu, Yitao},
TITLE = {On the Performance of Deep Reinforcement Learning-Based Anti-Jamming Method Confronting Intelligent Jammer},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1361},
URL = {https://www.mdpi.com/2076-3417/9/7/1361},
ISSN = {2076-3417},
ABSTRACT = {With the development of access technologies and artificial intelligence, a deep reinforcement learning (DRL) algorithm is proposed into channel accessing and anti-jamming. Assuming the jamming modes are sweeping, comb, dynamic and statistic, the DRL-based method through training can almost perfectly avoid jamming signal and communicate successfully. Instead, in this paper, from the perspective of jammers, we investigate the performance of a DRL-based anti-jamming method. First of all, we design an intelligent jamming method based on reinforcement learning to combat the DRL-based user. Then, we theoretically analyze the condition when the DRL-based anti-jamming algorithm cannot converge, and provide the proof. Finally, in order to investigate the performance of DRL-based method, various scenarios where users with different communicating modes combat jammers with different jamming modes are compared. As the simulation results show, the theoretical analysis is verified, and the proposed RL-based jamming can effectively restrict the performance of DRL-based anti-jamming method.},
DOI = {10.3390/app9071361}
}



@Article{s19071562,
AUTHOR = {Yang, Jiachen and Liu, Lin and Zhang, Linfeng and Li, Gen and Sun, Zhonghao and Song, Houbing},
TITLE = {Prediction of Marine Pycnocline Based on Kernel Support Vector Machine and Convex Optimization Technology},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1562},
URL = {https://www.mdpi.com/1424-8220/19/7/1562},
ISSN = {1424-8220},
ABSTRACT = {With the explosive growth of ocean data, it is of great significance to use ocean observation data to analyze ocean pycnocline data in military field. However, due to natural factors, most of the time the ocean hydrological data is not complete. In this case, predicting the ocean hydrological data by partial data has become a hot spot in marine science. In this paper, based on the traditional statistical analysis literature, we propose a machine-learning ocean hydrological data processing process under big data. At the same time, based on the traditional pycnocline gradient determination method, the open Argo data set is analyzed, and the local characteristics of pycnocline are verified from several aspects combined with the current research about pycnocline. Most importantly, in this paper, the combination of kernel function and support vector machine(SVM) is extended to nonlinear learning by using the idea of machine learning and convex optimization technology. Based on this, the known pycnocline training set is trained, and an accurate model is obtained to predict the pycnocline in unknown domains. In the specific steps, this paper combines the classification problem with the regression problem, and determines the proportion of training set and test formula set by polynomial regression. Subsequently, the feature scaling of the input data accelerated the gradient convergence, and a grid search algorithm with variable step size was proposed to determine the super parameter c and gamma of the SVM model. The prediction results not only used the confusion matrix to analyze the accuracy of GridSearch-SVM with variable step size, but also compared the traditional SVM and the similar algorithm. At the end of the experiment, two features which have the greatest influence on the Marine density thermocline are found out by the feature ranking algorithm based on learning.},
DOI = {10.3390/s19071562}
}



@Article{s19071579,
AUTHOR = {Ostovar, Ahmad and Talbot, Bruce and Puliti, Stefano and Astrup, Rasmus and Ringdahl, Ola},
TITLE = {Detection and Classification of Root and Butt-Rot (RBR) in Stumps of Norway Spruce Using RGB Images and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1579},
URL = {https://www.mdpi.com/1424-8220/19/7/1579},
ISSN = {1424-8220},
ABSTRACT = {Root and butt-rot (RBR) has a significant impact on both the material and economic outcome of timber harvesting, and therewith on the individual forest owner and collectively on the forest and wood processing industries. An accurate recording of the presence of RBR during timber harvesting would enable a mapping of the location and extent of the problem, providing a basis for evaluating spread in a climate anticipated to enhance pathogenic growth in the future. Therefore, a system to automatically identify and detect the presence of RBR would constitute an important contribution to addressing the problem without increasing workload complexity for the machine operator. In this study, we developed and evaluated an approach based on RGB images to automatically detect tree stumps and classify them as to the absence or presence of rot. Furthermore, since knowledge of the extent of RBR is valuable in categorizing logs, we also classify stumps into three classes of infestation; rot = 0%, 0% &lt; rot &lt; 50% and rot &ge; 50%. In this work we used deep-learning approaches and conventional machine-learning algorithms for detection and classification tasks. The results showed that tree stumps were detected with precision rate of 95% and recall of 80%. Using only the correct output (TP) of the stump detector, stumps without and with RBR were correctly classified with accuracy of 83.5% and 77.5%, respectively. Classifying rot into three classes resulted in 79.4%, 72.4%, and 74.1% accuracy for stumps with rot = 0%, 0% &lt; rot &lt; 50%, and rot &ge; 50%, respectively. With some modifications, the developed algorithm could be used either during the harvesting operation to detect RBR regions on the tree stumps or as an RBR detector for post-harvest assessment of tree stumps and logs.},
DOI = {10.3390/s19071579}
}



@Article{en12071256,
AUTHOR = {Patané, Luca},
TITLE = {Bio-Inspired Robotic Solutions for Landslide Monitoring},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1256},
URL = {https://www.mdpi.com/1996-1073/12/7/1256},
ISSN = {1996-1073},
ABSTRACT = {Bio-inspired solutions are often taken into account to solve problems that nature took millions of years to deal with. In the field of robotics, when we need to design systems able to perform in unstructured environments, bio-inspiration can be a useful instrument both for mechanical design and for the control architecture. In the proposed work the problem of landslide monitoring is addressed proposing a bio-inspired robotic structure developed to deploy a series of smart sensors on target locations with the aim of creating a sensor network capable of acquiring information on the status of the area of interest. The acquired data can be used both to create models and to generate alert signals when a landslide event is identified in the early stage. The design process of the robotic system, including dynamic simulations and robot experiments, will be presented here.},
DOI = {10.3390/en12071256}
}



@Article{rs11070789,
AUTHOR = {Laybros, Anthony and Schläpfer, Daniel and Féret, Jean-Baptiste and Descroix, Laurent and Bedeau, Caroline and Lefevre, Marie-Jose and Vincent, Grégoire},
TITLE = {Across Date Species Detection Using Airborne Imaging Spectroscopy},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {789},
URL = {https://www.mdpi.com/2072-4292/11/7/789},
ISSN = {2072-4292},
ABSTRACT = {Imaging spectroscopy is a promising tool for airborne tree species recognition in hyper-diverse tropical canopies. However, its widespread application is limited by the signal sensitivity to acquisition parameters, which may require new training data in every new area of application. This study explores how various pre-processing steps may improve species discrimination and species recognition under different operational settings. In the first experiment, a classifier was trained and applied on imaging spectroscopy data acquired on a single date, while in a second experiment, the classifier was trained on data from one date and applied to species identification on data from a different date. A radiative transfer model based on atmospheric compensation was applied with special focus on the automatic retrieval of aerosol amounts. The impact of spatial or spectral filtering and normalisation was explored as an alternative to atmospheric correction. A pixel-wise classification was performed with a linear discriminant analysis trained on individual tree crowns identified at the species level. Tree species were then identified at the crown scale based on a majority vote rule. Atmospheric corrections did not outperform simple statistical processing (i.e., filtering and normalisation) when training and testing sets were taken from the same flight date. However, atmospheric corrections became necessary for reliable species recognition when different dates were considered. Shadow masking improved species classification results in all cases. Single date classification rate was 83.9% for 1297 crowns of 20 tropical species. The loss of mean accuracy observed when using training data from one date to identify species at another date in the same area was limited to 10% when atmospheric correction was applied.},
DOI = {10.3390/rs11070789}
}



@Article{make1020033,
AUTHOR = {Azabi, Yousef and Savvaris, Al and Kipouros, Timoleon},
TITLE = {Artificial Intelligence to Enhance Aerodynamic Shape Optimisation of the Aegis UAV},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {1},
YEAR = {2019},
NUMBER = {2},
PAGES = {552--574},
URL = {https://www.mdpi.com/2504-4990/1/2/33},
ISSN = {2504-4990},
ABSTRACT = {This article presents an optimisation framework that uses stochastic multi-objective optimisation, combined with an Artificial Neural Network (ANN), and describes its application to the aerodynamic design of aircraft shapes. The framework uses the Multi-Objective Particle Swarm Optimisation (MOPSO) algorithm and the obtained results confirm that the proposed technique provides highly optimal solutions in less computational time than other approaches to the same design problem. The main idea was to focus computational effort on worthwhile design solutions rather than exploring and evaluating all possible solutions in the design space. It is shown that the number of valid solutions obtained using ANN-MOPSO compared to MOPSO for 3000 evaluations grew from 529 to 1006 (90% improvement) with a penalty of only 8.3% (11 min) in computational time. It is demonstrated that including an ANN, the ANN-MOPSO with 3000 evaluations produced a larger number of valid solutions than the MOPSO with 5500 evaluations, and in 33% less computational time (64 min). This is taken as confirmation of the potential power of ANNs when applied to this type of design problem.},
DOI = {10.3390/make1020033}
}



@Article{agronomy9040174,
AUTHOR = {de Lara, Alfonso and Longchamps, Louis and Khosla, Raj},
TITLE = {Soil Water Content and High-Resolution Imagery for Precision Irrigation: Maize Yield},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {174},
URL = {https://www.mdpi.com/2073-4395/9/4/174},
ISSN = {2073-4395},
ABSTRACT = {Improvement in water use efficiency of crops is a key component in addressing the increasing global water demand. The time and depth of the soil water monitoring are essential when defining the amount of water to be applied to irrigated crops. Precision irrigation (PI) is a relatively new concept in agriculture, and it provides a vast potential for enhancing water use efficiency, while maintaining or increasing grain yield. Neutron probes (NPs) have consistently been used as a robust and accurate method to estimate soil water content (SWC). Remote sensing derived vegetation indices have been successfully used to estimate variability of Leaf Area Index and biomass, which are related to root water uptake. Crop yield has not been evaluated on a basis of SWC, as explained by NPs in time and at different depths. The objectives of this study were (1) to determine the optimal time and depth of SWC and its relationship to maize grain yield (2) to determine if satellite-derived vegetation indices coupled with SWC could further improve the relationship between maize grain yield and SWC. Soil water and remote sensing data were collected throughout the crop season and analyzed. The results from the automated model selection of SWC readings, used to assess maize yield, consistently selected three dates spread around reproductive growth stages for most depths (p value &lt; 0.05). SWC readings at the 90 cm depth had the highest correlation with maize yield, followed closely by the 120 cm. When coupled with remote sensing data, models improved by adding vegetation indices representing the crop health status at V9, right before tasseling. Thus, SWC monitoring at reproductive stages combined with vegetation indices could be a tool for improving maize irrigation management.},
DOI = {10.3390/agronomy9040174}
}



@Article{s19071651,
AUTHOR = {Hong, Suk-Ju and Han, Yunhyeok and Kim, Sang-Yeon and Lee, Ah-Yeong and Kim, Ghiseok},
TITLE = {Application of Deep-Learning Methods to Bird Detection Using Unmanned Aerial Vehicle Imagery},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1651},
URL = {https://www.mdpi.com/1424-8220/19/7/1651},
ISSN = {1424-8220},
ABSTRACT = {Wild birds are monitored with the important objectives of identifying their habitats and estimating the size of their populations. Especially in the case of migratory bird, they are significantly recorded during specific periods of time to forecast any possible spread of animal disease such as avian influenza. This study led to the construction of deep-learning-based object-detection models with the aid of aerial photographs collected by an unmanned aerial vehicle (UAV). The dataset containing the aerial photographs includes diverse images of birds in various bird habitats and in the vicinity of lakes and on farmland. In addition, aerial images of bird decoys are captured to achieve various bird patterns and more accurate bird information. Bird detection models such as Faster Region-based Convolutional Neural Network (R-CNN), Region-based Fully Convolutional Network (R-FCN), Single Shot MultiBox Detector (SSD), Retinanet, and You Only Look Once (YOLO) were created and the performance of all models was estimated by comparing their computing speed and average precision. The test results show Faster R-CNN to be the most accurate and YOLO to be the fastest among the models. The combined results demonstrate that the use of deep-learning-based detection methods in combination with UAV aerial imagery is fairly suitable for bird detection in various environments.},
DOI = {10.3390/s19071651}
}



@Article{app9071459,
AUTHOR = {Mao, Huihui and Meng, Jihua and Ji, Fujiang and Zhang, Qiankun and Fang, Huiting},
TITLE = {Comparison of Machine Learning Regression Algorithms for Cotton Leaf Area Index Retrieval Using Sentinel-2 Spectral Bands},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1459},
URL = {https://www.mdpi.com/2076-3417/9/7/1459},
ISSN = {2076-3417},
ABSTRACT = {Leaf area index (LAI) is a crucial crop biophysical parameter that has been widely used in a variety of fields. Five state-of-the-art machine learning regression algorithms (MLRAs), namely, artificial neural network (ANN), support vector regression (SVR), Gaussian process regression (GPR), random forest (RF) and gradient boosting regression tree (GBRT), have been used in the retrieval of cotton LAI with Sentinel-2 spectral bands. The performances of the five machine learning models are compared for better applications of MLRAs in remote sensing, since challenging problems remain in the selection of MLRAs for crop LAI retrieval, as well as the decision as to the optimal number for the training sample size and spectral bands to different MLRAs. A comprehensive evaluation was employed with respect to model accuracy, computational efficiency, sensitivity to training sample size and sensitivity to spectral bands. We conducted the comparison of five MLRAs in an agricultural area of Northwest China over three cotton seasons with the corresponding field campaigns for modeling and validation. Results show that the GBRT model outperforms the other models with respect to model accuracy in average (       R 2   &macr;      = 0.854,       R M S E  &macr;      = 0.674 and       M A E  &macr;      = 0.456). SVR achieves the best performance in computational efficiency, which means it is fast to train, and to validate that it has great potentials to deliver near-real-time operational products for crop management. As for sensitivity to training sample size, GBRT behaves as the most robust model, and provides the best model accuracy on the average among the variations of training sample size, compared with other models (       R 2   &macr;      = 0.884,       R M S E  &macr;      = 0.615 and       M A E  &macr;      = 0.452). Spectral bands sensitivity analysis with dCor (distance correlation), combined with the backward elimination approach, indicates that SVR, GPR and RF provide relatively robust performance to the spectral bands, while ANN outperforms the other models in terms of model accuracy on the average among the reduction of spectral bands (       R 2   &macr;      = 0.881,       R M S E  &macr;      = 0.625 and       M A E  &macr;      = 0.480). A comprehensive evaluation indicates that GBRT is an appealing alternative for cotton LAI retrieval, except for its computational efficiency. Despite the different performance of the ML models, all models exhibited considerable potential for cotton LAI retrieval, which could offer accurate crop parameters information timely and accurately for crop fields management and agricultural production decisions.},
DOI = {10.3390/app9071459}
}



@Article{app9071461,
AUTHOR = {Wada, Daichi and Tamayama, Masato},
TITLE = {Wing Load and Angle of Attack Identification by Integrating Optical Fiber Sensing and Neural Network Approach in Wind Tunnel Test},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {1461},
URL = {https://www.mdpi.com/2076-3417/9/7/1461},
ISSN = {2076-3417},
ABSTRACT = {The load and angle of attack (AoA) for wing structures are critical parameters to be monitored for efficient operation of an aircraft. This study presents wing load and AoA identification techniques by integrating an optical fiber sensing technique and a neural network approach. We developed a 3.6-m semi-spanned wing model with eight flaps and bonded two optical fibers with 30 fiber Bragg gratings (FBGs) each along the main and aft spars. Using this model in a wind tunnel test, we demonstrate load and AoA identification through a neural network approach. We input the FBG data and the eight flap angles to a neural network and output estimated load distributions on the eight wing segments. Thereafter, we identify the AoA by using the estimated load distributions and the flap angles through another neural network. This multi-neural-network process requires only the FBG and flap angle data to be measured. We successfully identified the load distributions with an error range of &minus;1.5&ndash;1.4 N and a standard deviation of 0.57 N. The AoA was also successfully identified with error ranges of &minus;1.03&ndash;0.46&deg; and a standard deviation of 0.38&deg;.},
DOI = {10.3390/app9071461}
}



@Article{rs11070886,
AUTHOR = {Adriano, Bruno and Xia, Junshi and Baier, Gerald and Yokoya, Naoto and Koshimura, Shunichi},
TITLE = {Multi-Source Data Fusion Based on Ensemble Learning for Rapid Building Damage Mapping during the 2018 Sulawesi Earthquake and Tsunami in Palu, Indonesia},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {886},
URL = {https://www.mdpi.com/2072-4292/11/7/886},
ISSN = {2072-4292},
ABSTRACT = {This work presents a detailed analysis of building damage recognition, employing multi-source data fusion and ensemble learning algorithms for rapid damage mapping tasks. A damage classification framework is introduced and tested to categorize the building damage following the recent 2018 Sulawesi earthquake and tsunami. Three robust ensemble learning classifiers were investigated for recognizing building damage from Synthetic Aperture Radar (SAR) and optical remote sensing datasets and their derived features. The contribution of each feature dataset was also explored, considering different combinations of sensors as well as their temporal information. SAR scenes acquired by the ALOS-2 PALSAR-2 and Sentinel-1 sensors were used. The optical Sentinel-2 and PlanetScope sensors were also included in this study. A non-local filter in the preprocessing phase was used to enhance the SAR features. Our results demonstrated that the canonical correlation forests classifier performs better in comparison to the other classifiers. In the data fusion analysis, Digital Elevation Model (DEM)- and SAR-derived features contributed the most in the overall damage classification. Our proposed mapping framework successfully classifies four levels of building damage (with overall accuracy &gt;90%, average accuracy &gt;67%). The proposed framework learned the damage patterns from a limited available human-interpreted building damage annotation and expands this information to map a larger affected area. This process including pre- and post-processing phases were completed in about 3 h after acquiring all raw datasets.},
DOI = {10.3390/rs11070886}
}



@Article{rs11070888,
AUTHOR = {Du, Zhenrong and Yang, Jianyu and Ou, Cong and Zhang, Tingting},
TITLE = {Smallholder Crop Area Mapped with a Semantic Segmentation Deep Learning Method},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {888},
URL = {https://www.mdpi.com/2072-4292/11/7/888},
ISSN = {2072-4292},
ABSTRACT = {The growing population in China has led to an increasing importance of crop area (CA) protection. A powerful tool for acquiring accurate and up-to-date CA maps is automatic mapping using information extracted from high spatial resolution remote sensing (RS) images. RS image information extraction includes feature classification, which is a long-standing research issue in the RS community. Emerging deep learning techniques, such as the deep semantic segmentation network technique, are effective methods to automatically discover relevant contextual features and get better image classification results. In this study, we exploited deep semantic segmentation networks to classify and extract CA from high-resolution RS images. WorldView-2 (WV-2) images with only Red-Green-Blue (RGB) bands were used to confirm the effectiveness of the proposed semantic classification framework for information extraction and the CA mapping task. Specifically, we used the deep learning framework TensorFlow to construct a platform for sampling, training, testing, and classifying to extract and map CA on the basis of DeepLabv3+. By leveraging per-pixel and random sample point accuracy evaluation methods, we conclude that the proposed approach can efficiently obtain acceptable accuracy (Overall Accuracy = 95%, Kappa = 0.90) of CA classification in the study area, and the approach performs better than other deep semantic segmentation networks (U-Net/PspNet/SegNet/DeepLabv2) and traditional machine learning methods, such as Maximum Likelihood (ML), Support Vector Machine (SVM), and RF (Random Forest). Furthermore, the proposed approach is highly scalable for the variety of crop types in a crop area. Overall, the proposed approach can train a precise and effective model that is capable of adequately describing the small, irregular fields of smallholder agriculture and handling the great level of details in RGB high spatial resolution images.},
DOI = {10.3390/rs11070888}
}



@Article{s19081751,
AUTHOR = {Khan, Nabeel and Martini, Maria G.},
TITLE = {Bandwidth Modeling of Silicon Retinas for Next Generation Visual Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1751},
URL = {https://www.mdpi.com/1424-8220/19/8/1751},
ISSN = {1424-8220},
ABSTRACT = {Silicon retinas, also known as Dynamic Vision Sensors (DVS) or event-based visual sensors, have shown great advantages in terms of low power consumption, low bandwidth, wide dynamic range and very high temporal resolution. Owing to such advantages as compared to conventional vision sensors, DVS devices are gaining more and more attention in various applications such as drone surveillance, robotics, high-speed motion photography, etc. The output of such sensors is a sequence of events rather than a series of frames as for classical cameras. Estimating the data rate of the stream of events associated with such sensors is needed for the appropriate design of transmission systems involving such sensors. In this work, we propose to consider information about the scene content and sensor speed to support such estimation, and we identify suitable metrics to quantify the complexity of the scene for this purpose. According to the results of this study, the event rate shows an exponential relationship with the metric associated with the complexity of the scene and linear relationships with the speed of the sensor. Based on these results, we propose a two-parameter model for the dependency of the event rate on scene complexity and sensor speed. The model achieves a prediction accuracy of approximately 88.4% for the outdoor environment along with the overall prediction performance of approximately 84%.},
DOI = {10.3390/s19081751}
}



@Article{s19081758,
AUTHOR = {Wu, Qing and Shen, Xudong and Jin, Yuanzhe and Chen, Zeyu and Li, Shuai and Khan, Ameer Hamza and Chen, Dechao},
TITLE = {Intelligent Beetle Antennae Search for UAV Sensing and Avoidance of Obstacles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1758},
URL = {https://www.mdpi.com/1424-8220/19/8/1758},
ISSN = {1424-8220},
ABSTRACT = {Based on a bio-heuristic algorithm, this paper proposes a novel path planner called obstacle avoidance beetle antennae search (OABAS) algorithm, which is applied to the global path planning of unmanned aerial vehicles (UAVs). Compared with the previous bio-heuristic algorithms, the algorithm proposed in this paper has advantages of a wide search range and breakneck search speed, which resolves the contradictory requirements of the high computational complexity of the bio-heuristic algorithm and real-time path planning of UAVs. Besides, the constraints used by the proposed algorithm satisfy various characteristics of the path, such as shorter path length, maximum allowed turning angle, and obstacle avoidance. Ignoring the z-axis optimization by combining with the minimum threat surface (MTS), the resultant path meets the requirements of efficiency and safety. The effectiveness of the algorithm is substantiated by applying the proposed path planning algorithm on the UAVs. Moreover, comparisons with other existing algorithms further demonstrate the superiority of the proposed OABAS algorithm.},
DOI = {10.3390/s19081758}
}



@Article{rs11080895,
AUTHOR = {Tsai, Ya-Lun S. and Dietz, Andreas and Oppelt, Natascha and Kuenzer, Claudia},
TITLE = {Wet and Dry Snow Detection Using Sentinel-1 SAR Data for Mountainous Areas with a Machine Learning Technique},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {895},
URL = {https://www.mdpi.com/2072-4292/11/8/895},
ISSN = {2072-4292},
ABSTRACT = {Traditional studies on mapping wet snow cover extent (SCE) often feature limitations, especially in vegetated and mountainous areas. The aim of this study is to propose a new total and wet SCE mapping strategy based on freely accessible spaceborne synthetic aperture radar (SAR) data. The approach is transferable on a global scale as well as for different land cover types (including densely vegetated forest and agricultural regions), and is based on the use of backscattering coefficient, interferometric SAR coherence, and polarimetric parameters. Furthermore, four topographical factors were included in the simple tuning of random forest-based land cover type-dependent classification strategy. Results showed the classification accuracy was above 0.75, with an F-measure higher than 0.70, in all five selected regions of interest located around globally distributed mountain ranges. Whilst excluding forest-type land cover classes, the accuracy and F-measure increases to 0.80 and 0.75. In cross-location model set, the accuracy can also be maintained at 0.80 with non-forest accuracy up to 0.85. It has been found that the elevation and polarimetric parameters are the most critical factors, and that the quality of land cover information would also affect the subsequent mapping reliability. In conclusion, through comprehensive validation using optical satellite and in-situ data, our land cover-dependent total SCE mapping approach has been confirmed to be robustly applicable, and the holistic SCE map for different months were eventually derived.},
DOI = {10.3390/rs11080895}
}



@Article{sym11040533,
AUTHOR = {Zhang, Hehu and Wang, Xiushan and Chen, Ying and Jiang, Guoqiang and Lin, Shifeng},
TITLE = {Research on Vision-Based Navigation for Plant Protection UAV under the Near Color Background},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {533},
URL = {https://www.mdpi.com/2073-8994/11/4/533},
ISSN = {2073-8994},
ABSTRACT = {GPS (Global Positioning System) navigation in agriculture is facing many challenges, such as weak signals in orchards and the high cost for small plots of farmland. With the reduction of camera cost and the emergence of excellent visual algorithms, visual navigation can solve the above problems. Visual navigation is a navigation technology that uses cameras to sense environmental information as the basis of an aircraft flight. It is mainly divided into five parts: Image acquisition, landmark recognition, route planning, flight control, and obstacle avoidance. Here, landmarks are plant canopy, buildings, mountains, and rivers, with unique geographical characteristics in a place. During visual navigation, landmark location and route tracking are key links. When there are significant color-differences (for example, the differences among red, green, and blue) between a landmark and the background, the landmark can be recognized based on classical visual algorithms. However, in the case of non-significant color-differences (for example, the differences between dark green and vivid green) between a landmark and the background, there are no robust and high-precision methods for landmark identification. In view of the above problem, visual navigation in a maize field is studied. First, the block recognition method based on fine-tuned Inception-V3 is developed; then, the maize canopy landmark is recognized based on the above method; finally, local navigation lines are extracted from the landmarks based on the maize canopy grayscale gradient law. The results show that the accuracy is 0.9501. When the block number is 256, the block recognition method achieves the best segmentation. The average segmentation quality is 0.87, and time is 0.251 s. This study suggests that stable visual semantic navigation can be achieved under the near color background. It will be an important reference for the navigation of plant protection UAV (Unmanned Aerial Vehicle).},
DOI = {10.3390/sym11040533}
}



@Article{drones3020036,
AUTHOR = {Girolamo-Neto, Cesare Di and Sanches, Ieda Del’Arco and Neves, Alana Kasahara and Prudente, Victor Hugo Rohden and Körting, Thales Sehn and Picoli, Michelle Cristina Araujo and Aragão, Luiz Eduardo Oliveira e Cruz de},
TITLE = {Assessment of Texture Features for Bermudagrass (Cynodon dactylon) Detection in Sugarcane Plantations},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {36},
URL = {https://www.mdpi.com/2504-446X/3/2/36},
ISSN = {2504-446X},
ABSTRACT = {Sugarcane products contribute significantly to the Brazilian economy, generating U.S. $12.2 billion in revenue in 2018. Identifying and monitoring factors that induce yield reduction, such as weed occurrence, is thus imperative. The detection of Bermudagrass in sugarcane crops using remote sensing data, however, is a challenge considering their spectral similarity. To overcome this limitation, this paper aims to explore the potential of texture features derived from images acquired by an optical sensor onboard anunmanned aerial vehicle (UAV) to detect Bermudagrass in sugarcane. Aerial images with a spatial resolution of 2 cm were acquired from a sugarcane field in Brazil. The Green-Red Vegetation Index and several texture metrics derived from the gray-level co-occurrence matrix were calculated to perform an automatic classification using arandom forest algorithm. Adding texture metrics to the classification process improved the overall accuracy from 83.00% to 92.54%, and this improvement was greater considering larger window sizes, since they representeda texture transition between two targets. Production losses induced by Bermudagrass presence reached 12.1 tons &times; ha&minus;1 in the study site. This study not only demonstrated the capacity of UAV images to overcome the well-known limitation of detecting Bermudagrass in sugarcane crops, but also highlighted the importance of texture for high-accuracy quantification of weed invasion in sugarcane crops.},
DOI = {10.3390/drones3020036}
}



@Article{app9081544,
AUTHOR = {Katsaprakakis, Dimitris Al. and Dakanali, Irini and Zidianakis, George and Yiannakoudakis, Yiannis and Psarras, Nikolaos and Kanouras, Spyros},
TITLE = {Potential on Energy Performance Upgrade of National Stadiums: A Case Study for the Pancretan Stadium, Crete, Greece},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1544},
URL = {https://www.mdpi.com/2076-3417/9/8/1544},
ISSN = {2076-3417},
ABSTRACT = {Energy performance upgrade of stadiums constitutes a complex and demanding task because of both the size and the variety of the involved energy loads. The present article aims to summarize the basic results of the implemented study on the energy performance upgrade of the Pancretan Stadium, Crete, Greece. This target was approached with a cluster of passive and active measures: replacement of old openings, a photovoltaic station, an open loop geothermal system, installation of energy-efficient lighting devices, a solar-biomass combi system and a Building Energy Management System (BEMS) for the control of the main energy consumptions. The dimensioning of all the proposed active systems is optimized through the computational simulation of their annual operation. With the applied technologies, the achieved annual energy saving percentage exceeds 83%. The Renewable Energy Sources annual penetration percentage is calculated at 82% versus the annual energy consumption. The Stadium’s energy performance is upgraded from rank D to rank A+, according to the European Union’s directives. The set-up cost of the under consideration energy performance upgrade systems is approximately calculated at 2,700,000 €, with a payback period of 12 years, calculated versus the achieved monetary savings due to the reduction of the consumed energy resources.},
DOI = {10.3390/app9081544}
}



@Article{machines7020023,
AUTHOR = {Cammarata, Alessandro and Sinatra, Rosario and Rigato, Riccardo and Maddio, Pietro Davide},
TITLE = {Tie-System Calibration for the Experimental Setup of Large Deployable Reflectors},
JOURNAL = {Machines},
VOLUME = {7},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2075-1702/7/2/23},
ISSN = {2075-1702},
ABSTRACT = {The trade-off between the design phase and the experimental setup is crucial in satisfying the accuracy requirements of large deployable reflectors. Manufacturing errors and tolerances change the root mean square (RMS) of the reflecting surface and require careful calibration of the tie-rod system to be able to fit into the initial design specifications. To give a possible solution to this problem, two calibration methods&mdash;for rigid and flexible ring truss supports, respectively&mdash;are described in this study. Starting from the acquired experimental data on the net nodal co-ordinates, the initial problem of satisfying the static equilibrium with the measured configuration is described. Then, two constrained optimization problems (for rigid or flexible ring truss supports) are defined to meet the desired RMS accuracy of the reflecting surface by modifying the tie lengths. Finally, a case study to demonstrate the validity of the proposed methods is presented.},
DOI = {10.3390/machines7020023}
}



@Article{s19081815,
AUTHOR = {Buchaillot, Ma. Luisa and Gracia-Romero, Adrian and Vergara-Diaz, Omar and Zaman-Allah, Mainassara A. and Tarekegne, Amsal and Cairns, Jill E. and Prasanna, Boddupalli M. and Araus, Jose Luis and Kefauver, Shawn C.},
TITLE = {Evaluating Maize Genotype Performance under Low Nitrogen Conditions Using RGB UAV Phenotyping Techniques},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1815},
URL = {https://www.mdpi.com/1424-8220/19/8/1815},
ISSN = {1424-8220},
ABSTRACT = {Maize is the most cultivated cereal in Africa in terms of land area and production, but low soil nitrogen availability often constrains yields. Developing new maize varieties with high and reliable yields using traditional crop breeding techniques in field conditions can be slow and costly. Remote sensing has become an important tool in the modernization of field-based high-throughput plant phenotyping (HTPP), providing faster gains towards the improvement of yield potential and adaptation to abiotic and biotic limiting conditions. We evaluated the performance of a set of remote sensing indices derived from red&ndash;green&ndash;blue (RGB) images along with field-based multispectral normalized difference vegetation index (NDVI) and leaf chlorophyll content (SPAD values) as phenotypic traits for assessing maize performance under managed low-nitrogen conditions. HTPP measurements were conducted from the ground and from an unmanned aerial vehicle (UAV). For the ground-level RGB indices, the strongest correlations to yield were observed with hue, greener green area (GGA), and a newly developed RGB HTPP index, NDLab (normalized difference Commission Internationale de I&acute;Edairage (CIE)Lab index), while GGA and crop senescence index (CSI) correlated better with grain yield from the UAV. Regarding ground sensors, SPAD exhibited the closest correlation with grain yield, notably increasing in its correlation when measured in the vegetative stage. Additionally, we evaluated how different HTPP indices contributed to the explanation of yield in combination with agronomic data, such as anthesis silking interval (ASI), anthesis date (AD), and plant height (PH). Multivariate regression models, including RGB indices (R2 &gt; 0.60), outperformed other models using only agronomic parameters or field sensors (R2 &gt; 0.50), reinforcing RGB HTPP&rsquo;s potential to improve yield assessments. Finally, we compared the low-N results to the same panel of 64 maize genotypes grown under optimal conditions, noting that only 11% of the total genotypes appeared in the highest yield producing quartile for both trials. Furthermore, we calculated the grain yield loss index (GYLI) for each genotype, which showed a large range of variability, suggesting that low-N performance is not necessarily exclusive of high productivity in optimal conditions.},
DOI = {10.3390/s19081815}
}



@Article{s19081818,
AUTHOR = {Liu, Yuan and Sui, Xiubao and Kuang, Xiaodong and Liu, Chengwei and Gu, Guohua and Chen, Qian},
TITLE = {Object Tracking Based on Vector Convolutional Network and Discriminant Correlation Filters},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1818},
URL = {https://www.mdpi.com/1424-8220/19/8/1818},
ISSN = {1424-8220},
ABSTRACT = {Due to the fast speed and high efficiency, discriminant correlation filter (DCF) has drawn great attention in online object tracking recently. However, with the improvement of performance, the costs are the increase in parameters and the decline of speed. In this paper, we propose a novel visual tracking algorithm, namely VDCFNet, and combine DCF with a vector convolutional network (VCNN). We replace one traditional convolutional filter with two novel vector convolutional filters in the convolutional stage of our network. This enables our model with few memories (only 59 KB) trained offline to learn the generic image features. In the online tracking stage, we propose a coarse-to-fine search strategy to solve drift problems under fast motion. Besides, we update model selectively to speed up and increase robustness. The experiments on OTB benchmarks demonstrate that our proposed VDCFNet can achieve a competitive performance while running over real-time speed.},
DOI = {10.3390/s19081818}
}



@Article{rs11080928,
AUTHOR = {Swinfield, Tom and Lindsell, Jeremy A. and Williams, Jonathan V. and Harrison, Rhett D. and Agustiono and Habibi and Gemita, Elva and Schönlieb, Carola B. and Coomes, David A.},
TITLE = {Accurate Measurement of Tropical Forest Canopy Heights and Aboveground Carbon Using Structure From Motion},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {928},
URL = {https://www.mdpi.com/2072-4292/11/8/928},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles are increasingly used to monitor forests. Three-dimensional models of tropical rainforest canopies can be constructed from overlapping photos using Structure from Motion (SfM), but it is often impossible to map the ground elevation directly from such data because canopy gaps are rare in rainforests. Without knowledge of the terrain elevation, it is, thus, difficult to accurately measure the canopy height or forest properties, including the recovery stage and aboveground carbon density. Working in an Indonesian ecosystem restoration landscape, we assessed how well SfM derived the estimates of the canopy height and aboveground carbon density compared with those from an airborne laser scanning (also known as LiDAR) benchmark. SfM systematically underestimated the canopy height with a mean bias of approximately 5 m. The linear models suggested that the bias increased quadratically with the top-of-canopy height for short, even-aged, stands but linearly for tall, structurally complex canopies (&gt;10 m). The predictions based on the simple linear model were closely correlated to the field-measured heights when the approach was applied to an independent survey in a different location (    R 2     = 67% and RMSE = 1.85 m), but a negative bias of 0.89 m remained, suggesting the need to refine the model parameters with additional training data. Models that included the metrics of canopy complexity were less biased but with a reduced     R 2    . The inclusion of ground control points (GCPs) was found to be important in accurately registering SfM measurements in space, which is essential if the survey requirement is to produce small-scale restoration interventions or to track changes through time. However, at the scale of several hectares, the top-of-canopy height and above-ground carbon density estimates from SfM and LiDAR were very similar even without GCPs. The ability to produce accurate top-of-canopy height and carbon stock measurements from SfM is game changing for forest managers and restoration practitioners, providing the means to make rapid, low-cost surveys over hundreds of hectares without the need for LiDAR.},
DOI = {10.3390/rs11080928}
}



@Article{sym11040569,
AUTHOR = {Vrabel, Robert},
TITLE = {Eigenvalue Based Approach for Assessment of Global Robustness of Nonlinear Dynamical Systems},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {569},
URL = {https://www.mdpi.com/2073-8994/11/4/569},
ISSN = {2073-8994},
ABSTRACT = {In this paper we have established the sufficient conditions for asymptotic convergence of all solutions of nonlinear dynamical system (with potentially unknown and unbounded external disturbances) to zero with time     t &rarr; &infin; .     We showed here that the symmetric part of linear part of nonlinear nominal system, or, to be more precise, its time-dependent eigenvalues, play important role in assessment of the robustness of systems.},
DOI = {10.3390/sym11040569}
}



@Article{drones3020040,
AUTHOR = {Barbedo, Jayme Garcia Arnal},
TITLE = {A Review on the Use of Unmanned Aerial Vehicles and Imaging Sensors for Monitoring and Assessing Plant Stresses},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {40},
URL = {https://www.mdpi.com/2504-446X/3/2/40},
ISSN = {2504-446X},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are becoming a valuable tool to collect data in a variety of contexts. Their use in agriculture is particularly suitable, as those areas are often vast, making ground scouting difficult, and sparsely populated, which means that injury and privacy risks are not as important as in urban settings. Indeed, the use of UAVs for monitoring and assessing crops, orchards, and forests has been growing steadily during the last decade, especially for the management of stresses such as water, diseases, nutrition deficiencies, and pests. This article presents a critical overview of the main advancements on the subject, focusing on the strategies that have been used to extract the information contained in the images captured during the flights. Based on the information found in more than 100 published articles and on our own research, a discussion is provided regarding the challenges that have already been overcome and the main research gaps that still remain, together with some suggestions for future research.},
DOI = {10.3390/drones3020040}
}



@Article{info10040149,
AUTHOR = {Mylonas, Phivos and Voutos, Yorghos and Sofou, Anastasia},
TITLE = {A Collaborative Pilot Platform for Data Annotation and Enrichment in Viticulture},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {149},
URL = {https://www.mdpi.com/2078-2489/10/4/149},
ISSN = {2078-2489},
ABSTRACT = {It took some time indeed, but the research evolution and transformations that occurred in the smart agriculture field over the recent years tend to constitute the latter as the main topic of interest in the so-called Internet of Things (IoT) domain. Undoubtedly, our era is characterized by the mass production of huge amounts of data, information and content deriving from many different sources, mostly IoT devices and sensors, but also from environmentalists, agronomists, winemakers, or plain farmers and interested stakeholders themselves. Being an emerging field, only a small part of this rich content has been aggregated so far in digital platforms that serve as cross-domain hubs. The latter offer typically limited usability and accessibility of the actual content itself due to problems dealing with insufficient data and metadata availability, as well as their quality. Over our recent involvement within a precision viticulture environment and in an effort to make the notion of smart agriculture in the winery domain more accessible to and reusable from the general public, we introduce herein the model of an aggregation platform that provides enhanced services and enables human-computer collaboration for agricultural data annotations and enrichment. In principle, the proposed architecture goes beyond existing digital content aggregation platforms by advancing digital data through the combination of artificial intelligence automation and creative user engagement, thus facilitating its accessibility, visibility, and re-use. In particular, by using image and free text analysis methodologies for automatic metadata enrichment, in accordance to the human expertise for enrichment, it offers a cornerstone for future researchers focusing on improving the quality of digital agricultural information analysis and its presentation, thus establishing new ways for its efficient exploitation in a larger scale with benefits both for the agricultural and the consumer domains.},
DOI = {10.3390/info10040149}
}



@Article{rs11080967,
AUTHOR = {Wang, Sijia and Chen, Yunhao and Wang, Mingguo and Zhao, Yifei and Li, Jing},
TITLE = {SPA-Based Methods for the Quantitative Estimation of the Soil Salt Content in Saline-Alkali Land from Field Spectroscopy Data: A Case Study from the Yellow River Irrigation Regions},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {967},
URL = {https://www.mdpi.com/2072-4292/11/8/967},
ISSN = {2072-4292},
ABSTRACT = {The problem of soil salinization has always been a global problem involving resource, environmental, and ecological issues, and is closely related to the sustainable development of the social economy. Remote sensing provides an effective technical means for soil salinity identification and quantification research. This study focused on the estimation of the soil salt content in saline-alkali soils and applied the Successive Projections Algorithm (SPA) method to the estimation model; twelve spectral forms were applied in the estimation model of the spectra and soil salt content. Regression modeling was performed using the Partial Least Squares Regression (PLSR) method. Proximal-field spectral measurements data and soil samples were collected in the Yellow River Irrigation regions of Shizuishan City. A total of 60 samples were collected. The results showed that application of the SPA method improved the modeled determination coefficient (R2) and the ratio of performance to deviation (RPD), and reduced the modeled root mean square error (RMSE) and the percentage root mean square error (RMSE%); the maximum value of R2 increased by 0.22, the maximum value of RPD increased by 0.97, the maximum value of the RMSE decreased by 0.098 and the maximum value of the RMSE% decreased by 8.52%. The SPA&ndash;PLSR model, based on the first derivative of reflectivity (FD), the FD&ndash;SPA&ndash;PLSR model, showed the best results, with an R2 value of 0.89, an RPD value of 2.72, an RMSE value of 0.177, and RMSE% value of 11.81%. The results of this study demonstrated the applicability of the SPA method in the estimation of soil salinity, by using field spectroscopy data. The study provided a reference for a subsequent study of the hyperspectral estimation of soil salinity, and the proximal sensing data from a low distance, in this study, could provide detailed data for use in future remote sensing studies.},
DOI = {10.3390/rs11080967}
}



@Article{s19081933,
AUTHOR = {Pham, Tien Dat and Xia, Junshi and Ha, Nam Thang and Bui, Dieu Tien and Le, Nga Nhu and Tekeuchi, Wataru},
TITLE = {A Review of Remote Sensing Approaches for Monitoring Blue Carbon Ecosystems: Mangroves, Seagrassesand Salt Marshes during 2010–2018},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1933},
URL = {https://www.mdpi.com/1424-8220/19/8/1933},
ISSN = {1424-8220},
ABSTRACT = {Blue carbon (BC) ecosystems are an important coastal resource, as they provide a range of goods and services to the environment. They play a vital role in the global carbon cycle by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, there has been a large reduction in the global BC ecosystems due to their conversion to agriculture and aquaculture, overexploitation, and removal for human settlements. Effectively monitoring BC ecosystems at large scales remains a challenge owing to practical difficulties in monitoring and the time-consuming field measurement approaches used. As a result, sensible policies and actions for the sustainability and conservation of BC ecosystems can be hard to implement. In this context, remote sensing provides a useful tool for mapping and monitoring BC ecosystems faster and at larger scales. Numerous studies have been carried out on various sensors based on optical imagery, synthetic aperture radar (SAR), light detection and ranging (LiDAR), aerial photographs (APs), and multispectral data. Remote sensing-based approaches have been proven effective for mapping and monitoring BC ecosystems by a large number of studies. However, to the best of our knowledge, this is the first comprehensive review on the applications of remote sensing techniques for mapping and monitoring BC ecosystems. The main goal of this review is to provide an overview and summary of the key studies undertaken from 2010 onwards on remote sensing applications for mapping and monitoring BC ecosystems. Our review showed that optical imagery, such as multispectral and hyper-spectral data, is the most common for mapping BC ecosystems, while the Landsat time-series are the most widely-used data for monitoring their changes on larger scales. We investigate the limitations of current studies and suggest several key aspects for future applications of remote sensing combined with state-of-the-art machine learning techniques for mapping coastal vegetation and monitoring their extents and changes.},
DOI = {10.3390/s19081933}
}



@Article{app9081719,
AUTHOR = {El Gmili, Nada and Mjahed, Mostafa and El Kari, Abdeljalil and Ayad, Hassan},
TITLE = {Particle Swarm Optimization and Cuckoo Search-Based Approaches for Quadrotor Control and Trajectory Tracking},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1719},
URL = {https://www.mdpi.com/2076-3417/9/8/1719},
ISSN = {2076-3417},
ABSTRACT = {This paper explores the full control of a quadrotor Unmanned Aerial Vehicles (UAVs) by exploiting the nature-inspired algorithms of Particle Swarm Optimization (PSO), Cuckoo Search (CS), and the cooperative Particle Swarm Optimization-Cuckoo Search (PSO-CS). The proposed PSO-CS algorithm combines the ability of social thinking in PSO with the local search capability in CS, which helps to overcome the problem of low convergence speed of CS. First, the quadrotor dynamic modeling is defined using Newton-Euler formalism. Second, PID (Proportional, Integral, and Derivative) controllers are optimized by using the intelligent proposed approaches and the classical method of Reference Model (RM) for quadrotor full control. Finally, simulation results prove that PSO and PSO-CS are more efficient in tuning of optimal parameters for the quadrotor control. Indeed, the ability of PSO and PSO-CS to track the imposed trajectories is well seen from 3D path tracking simulations and even in presence of wind disturbances.},
DOI = {10.3390/app9081719}
}



@Article{rs11090993,
AUTHOR = {Carvajal-Ramírez, Fernando and Marques da Silva, José Rafael and Agüera-Vega, Francisco and Martínez-Carricondo, Patricio and Serrano, João and Moral, Francisco Jesús},
TITLE = {Evaluation of Fire Severity Indices Based on Pre- and Post-Fire Multispectral Imagery Sensed from UAV},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {993},
URL = {https://www.mdpi.com/2072-4292/11/9/993},
ISSN = {2072-4292},
ABSTRACT = {Fire severity is a key factor for management of post-fire vegetation regeneration strategies because it quantifies the impact of fire, describing the amount of damage. Several indices have been developed for estimation of fire severity based on terrestrial observation by satellite imagery. In order to avoid the implicit limitations of this kind of data, this work employed an Unmanned Aerial Vehicle (UAV) carrying a high-resolution multispectral sensor including green, red, near-infrared, and red edge bands. Flights were carried out pre- and post-controlled fire in a Mediterranean forest. The products obtained from the UAV-photogrammetric projects based on the Structure from Motion (SfM) algorithm were a Digital Surface Model (DSM) and multispectral images orthorectified in both periods and co-registered in the same absolute coordinate system to find the temporal differences (d) between pre- and post-fire values of the Excess Green Index (EGI), Normalized Difference Vegetation Index (NDVI), and Normalized Difference Red Edge (NDRE) index. The differences of indices (dEGI, dNDVI, and dNDRE) were reclassified into fire severity classes, which were compared with the reference data identified through the in situ fire damage location and Artificial Neural Network classification. Applying an error matrix analysis to the three difference of indices, the overall Kappa accuracies of the severity maps were 0.411, 0.563, and 0.211 and the Cramer&rsquo;s Value statistics were 0.411, 0.582, and 0.269 for dEGI, dNDVI, and dNDRE, respectively. The chi-square test, used to compare the average of each severity class, determined that there were no significant differences between the three severity maps, with a 95% confidence level. It was concluded that dNDVI was the index that best estimated the fire severity according to the UAV flight conditions and sensor specifications.},
DOI = {10.3390/rs11090993}
}



@Article{s19091962,
AUTHOR = {Jiang, Liubing and Zhou, Xiaolong and Che, Li and Rong, Shuwei and Wen, Hexin},
TITLE = {Feature Extraction and Reconstruction by Using 2D-VMD Based on Carrier-Free UWB Radar Application in Human Motion Recognition},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1962},
URL = {https://www.mdpi.com/1424-8220/19/9/1962},
ISSN = {1424-8220},
ABSTRACT = {As the size of the radar hardware platform becomes smaller and smaller, the cost becomes lower and lower. The application of indoor radar-based human motion recognition has become a reality, which can be realized in a low-cost device with simple architecture. Compared with narrow-band radar (such as continuous wave radar, etc.), the human motion echo signal of the carrier-free ultra-wideband (UWB) radar contains more abundant characteristic information of human motion, which is helpful for identifying different types of human motion. In this paper, a novel feature extraction method by two-dimensional variational mode decomposition (2D-VMD) algorithm is proposed. And it is used for extracting the primary features of human motion. The 2D-VMD algorithm is an adaptive non-recursive multiscale decomposition method for nonlinear and nonstationary signals. Firstly, the original 2D radar echo signals are decomposed by the 2D-VMD algorithm to capture several 2D intrinsic mode function (BIMFs) which represent different groups of central frequency components of a certain type of human motion. Secondly, original echo signals are reconstructed according to the several BIMFs, which not only have a certain inhibitory effect on the clutter in the echo signal, but can also further demonstrate that the BIMFs obtained by the 2D-VMD algorithm can represent the original 2D echo signal well. Finally, based on the measured ten different types of UWB radar human motion 2D echo analysis signals, the characteristics of these different types of human motion are extracted and the original echo signal are reconstructed. Then, the three indicators of the PCC, UQI, and PSNR between the original echo signals and extraction/reconstruction 2D signals are analyzed, which illustrate the effectiveness of 2D-VMD algorithm to extract feature of human motion 2D echo signals of the carrier-free UWB radar. Experimental results show that BIMFs by 2D-VMD algorithm can well represent the echo signal characteristics of this type of human motion, which is a very effective tool for human motion radar echo signal feature extraction.},
DOI = {10.3390/s19091962}
}



@Article{s19091994,
AUTHOR = {Sun, Guibin and Zhou, Rui and Di, Bin and Dong, Zhuoning and Wang, Yingxun},
TITLE = {A Novel Cooperative Path Planning for Multi-robot Persistent Coverage with Obstacles and Coverage Period Constraints},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1994},
URL = {https://www.mdpi.com/1424-8220/19/9/1994},
ISSN = {1424-8220},
ABSTRACT = {In this paper, a multi-robot persistent coverage of the region of interest is considered, where persistent coverage and cooperative coverage are addressed simultaneously. Previous works have mainly concentrated on the paths that allow for repeated coverage, but ignored the coverage period requirements of each sub-region. In contrast, this paper presents a combinatorial approach for path planning, which aims to cover mission domains with different task periods while guaranteeing both obstacle avoidance and minimizing the number of robots used. The algorithm first deploys the sensors in the region to satisfy coverage requirements with minimum cost. Then it solves the travelling salesman problem to obtain the frame of the closed path. Finally, the approach partitions the closed path into the fewest segments under the coverage period constraints, and it generates the closed route for each robot on the basis of portioned segments of the closed path. Therefore, each robot can circumnavigate one closed route to cover the different task areas completely and persistently. The numerical simulations show that the proposed approach is feasible to implement the cooperative coverage in consideration of obstacles and coverage period constraints, and the number of robots used is also minimized.},
DOI = {10.3390/s19091994}
}



@Article{rs11091017,
AUTHOR = {Zhang, Yang and Xiong, Zhangyue and Zang, Yu and Wang, Cheng and Li, Jonathan and Li, Xiang},
TITLE = {Topology-Aware Road Network Extraction via Multi-Supervised Generative Adversarial Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1017},
URL = {https://www.mdpi.com/2072-4292/11/9/1017},
ISSN = {2072-4292},
ABSTRACT = {Road network extraction from remote sensing images has played an important role in various areas. However, due to complex imaging conditions and terrain factors, such as occlusion and shades, it is very challenging to extract road networks with complete topology structures. In this paper, we propose a learning-based road network extraction framework via a Multi-supervised Generative Adversarial Network (MsGAN), which is jointly trained by the spectral and topology features of the road network. Such a design makes the network capable of learning how to &ldquo;guess&rdquo; the aberrant road cases, which is caused by occlusion and shadow, based on the relationship between the road region and centerline; thus, it is able to provide a road network with integrated topology. Additionally, we also present a sample quality measurement to efficiently generate a large number of training samples with a little human interaction. Through the experiments on images from various satellites and the comprehensive comparisons to state-of-the-art approaches on the public datasets, it is demonstrated that the proposed method is able to provide high-quality results, especially for the completeness of the road network.},
DOI = {10.3390/rs11091017}
}



@Article{rs11091018,
AUTHOR = {Li, Zhen and Zan, Qijie and Yang, Qiong and Zhu, Dehuang and Chen, Youjun and Yu, Shixiao},
TITLE = {Remote Estimation of Mangrove Aboveground Carbon Stock at the Species Level Using a Low-Cost Unmanned Aerial Vehicle System},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1018},
URL = {https://www.mdpi.com/2072-4292/11/9/1018},
ISSN = {2072-4292},
ABSTRACT = {There is ongoing interest in developing remote sensing technology to map and monitor the spatial distribution and carbon stock of mangrove forests. Previous research has demonstrated that the relationship between remote sensing derived parameters and aboveground carbon (AGC) stock varies for different species types. However, the coarse spatial resolution of satellite images has restricted the estimated AGC accuracy, especially at the individual species level. Recently, the availability of unmanned aerial vehicles (UAVs) has provided an operationally efficient approach to map the distribution of species and accurately estimate AGC stock at a fine scale in mangrove areas. In this study, we estimated mangrove AGC in the core area of northern Shenzhen Bay, South China, using four kinds of variables, including species type, canopy height metrics, vegetation indices, and texture features, derived from a low-cost UAV system. Three machine-learning algorithm models, including Random Forest (RF), Support Vector Regression (SVR), and Artificial Neural Network (ANN), were compared in this study, where a 10-fold cross-validation was used to evaluate each model&rsquo;s effectiveness. The results showed that a model that used all four type of variables, which were based on the RF algorithm, provided better AGC estimates (R2 = 0.81, relative RMSE (rRMSE) = 0.20, relative MAE (rMAE) = 0.14). The average predicted AGC from this model was 93.0 &plusmn; 24.3 Mg C ha&minus;1, and the total estimated AGC was 7903.2 Mg for the mangrove forests. The species-based model had better performance than the considered canopy-height-based model for AGC estimation, and mangrove species was the most important variable among all the considered input variables; the mean height (Hmean) the second most important variable. Additionally, the RF algorithms showed better performance in terms of mangrove AGC estimation than the SVR and ANN algorithms. Overall, a low-cost UAV system with a digital camera has the potential to enable satisfactory predictions of AGC in areas of homogenous mangrove forests.},
DOI = {10.3390/rs11091018}
}



@Article{s19092031,
AUTHOR = {Quirós Vargas, Juan José and Zhang, Chongyuan and Smitchger, Jamin A. and McGee, Rebecca J. and Sankaran, Sindhuja},
TITLE = {Phenotyping of Plant Biomass and Performance Traits Using Remote Sensing Techniques in Pea (Pisum sativum, L.)},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2031},
URL = {https://www.mdpi.com/1424-8220/19/9/2031},
ISSN = {1424-8220},
ABSTRACT = {Field pea cultivars are constantly improved through breeding programs to enhance biotic and abiotic stress tolerance and increase seed yield potential. In pea breeding, the Above Ground Biomass (AGBM) is assessed due to its influence on seed yield, canopy closure, and weed suppression. It is also the primary yield component for peas used as a cover crop and/or grazing. Measuring AGBM is destructive and labor-intensive process. Sensor-based phenotyping of such traits can greatly enhance crop breeding efficiency. In this research, high resolution RGB and multispectral images acquired with unmanned aerial systems were used to assess phenotypes in spring and winter pea breeding plots. The Green Red Vegetation Index (GRVI), Normalized Difference Vegetation Index (NDVI), Normalized Difference Red Edge Index (NDRE), plot volume, canopy height, and canopy coverage were extracted from RGB and multispectral information at five imaging times (between 365 to 1948 accumulated degree days/ADD after 1 May) in four winter field pea experiments and at three imaging times (between 1231 to 1648 ADD) in one spring field pea experiment. The image features were compared to ground-truth data including AGBM, lodging, leaf type, days to 50% flowering, days to physiological maturity, number of the first reproductive node, and seed yield. In two of the winter pea experiments, a strong correlation between image features and seed yield was observed at 1268 ADD (flowering). An increase in correlation between image features with the phenological traits such as days to 50% flowering and days to physiological maturity was observed at about 1725 ADD in these winter pea experiments. In the spring pea experiment, the plot volume estimated from images was highly correlated with ground truth canopy height (r = 0.83) at 1231 ADD. In two other winter pea experiments and the spring pea experiment, the GRVI and NDVI features were significantly correlated with AGBM at flowering. When selected image features were used to develop a least absolute shrinkage and selection operator model for AGBM estimation, the correlation coefficient between the actual and predicted AGBM was 0.60 and 0.84 in the winter and spring pea experiments, respectively. A SPOT-6 satellite image (1.5 m resolution) was also evaluated for its applicability to assess biomass and seed yield. The image features extracted from satellite imagery showed significant correlation with seed yield in two winter field pea experiments, however, the trend was not consistent. In summary, the study supports the potential of using unmanned aerial system-based imaging techniques to estimate biomass and crop performance in pea breeding programs.},
DOI = {10.3390/s19092031}
}



@Article{rs11091025,
AUTHOR = {Li, Weijia and He, Conghui and Fu, Haohuan and Zheng, Juepeng and Dong, Runmin and Xia, Maocai and Yu, Le and Luk, Wayne},
TITLE = {A Real-Time Tree Crown Detection Approach for Large-Scale Remote Sensing Images on FPGAs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1025},
URL = {https://www.mdpi.com/2072-4292/11/9/1025},
ISSN = {2072-4292},
ABSTRACT = {The on-board real-time tree crown detection from high-resolution remote sensing images is beneficial for avoiding the delay between data acquisition and processing, reducing the quantity of data transmission from the satellite to the ground, monitoring the growing condition of individual trees, and discovering the damage of trees as early as possible, etc. Existing high performance platform based tree crown detection studies either focus on processing images in a small size or suffer from high power consumption or slow processing speed. In this paper, we propose the first FPGA-based real-time tree crown detection approach for large-scale satellite images. A pipelined-friendly and resource-economic tree crown detection algorithm (PF-TCD) is designed through reconstructing and modifying the workflow of the original algorithm into three computational kernels on FPGAs. Compared with the well-optimized software implementation of the original algorithm on an Intel 12-core CPU, our proposed PF-TCD obtains the speedup of 18.75 times for a satellite image with a size of 12,188 &times; 12,576 pixels without reducing the detection accuracy. The image processing time for the large-scale remote sensing image is only 0.33 s, which satisfies the requirements of the on-board real-time data processing on satellites.},
DOI = {10.3390/rs11091025}
}



@Article{beverages5020033,
AUTHOR = {Gonzalez Viejo, Claudia and Torrico, Damir D. and Dunshea, Frank R. and Fuentes, Sigfredo},
TITLE = {Development of Artificial Neural Network Models to Assess Beer Acceptability Based on Sensory Properties Using a Robotic Pourer: A Comparative Model Approach to Achieve an Artificial Intelligence System},
JOURNAL = {Beverages},
VOLUME = {5},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {33},
URL = {https://www.mdpi.com/2306-5710/5/2/33},
ISSN = {2306-5710},
ABSTRACT = {Artificial neural networks (ANN) have become popular for optimization and prediction of parameters in foods, beverages, agriculture and medicine. For brewing, they have been explored to develop rapid methods to assess product quality and acceptability. Different beers (N = 17) were analyzed in triplicates using a robotic pourer, RoboBEER (University of Melbourne, Melbourne, Australia), to assess 15 color and foam-related parameters using computer-vision. Those samples were tested using sensory analysis for acceptability of carbonation mouthfeel, bitterness, flavor and overall liking with 30 consumers using a 9-point hedonic scale. ANN models were developed using 17 different training algorithms with 15 color and foam-related parameters as inputs and liking of four descriptors obtained from consumers as targets. Each algorithm was tested using five, seven and ten neurons and compared to select the best model based on correlation coefficients, slope and performance (mean squared error (MSE). Bayesian Regularization algorithm with seven neurons presented the best correlation (R = 0.98) and highest performance (MSE = 0.03) with no overfitting. These models may be used as a cost-effective method for fast-screening of beers during processing to assess acceptability more efficiently. The use of RoboBEER, computer-vision algorithms and ANN will allow the implementation of an artificial intelligence system for the brewing industry to assess its effectiveness.},
DOI = {10.3390/beverages5020033}
}



@Article{rs11091040,
AUTHOR = {He, Haiqing and Zhou, Junchao and Chen, Min and Chen, Ting and Li, Dajun and Cheng, Penggen},
TITLE = {Building Extraction from UAV Images Jointly Using 6D-SLIC and Multiscale Siamese Convolutional Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1040},
URL = {https://www.mdpi.com/2072-4292/11/9/1040},
ISSN = {2072-4292},
ABSTRACT = {Automatic building extraction using a single data type, either 2D remotely-sensed images or light detection and ranging 3D point clouds, remains insufficient to accurately delineate building outlines for automatic mapping, despite active research in this area and the significant progress which has been achieved in the past decade. This paper presents an effective approach to extracting buildings from Unmanned Aerial Vehicle (UAV) images through the incorporation of superpixel segmentation and semantic recognition. A framework for building extraction is constructed by jointly using an improved Simple Linear Iterative Clustering (SLIC) algorithm and Multiscale Siamese Convolutional Networks (MSCNs). The SLIC algorithm, improved by additionally imposing a digital surface model for superpixel segmentation, namely 6D-SLIC, is suited for building boundary detection under building and image backgrounds with similar radiometric signatures. The proposed MSCNs, including a feature learning network and a binary decision network, are used to automatically learn a multiscale hierarchical feature representation and detect building objects under various complex backgrounds. In addition, a gamma-transform green leaf index is proposed to truncate vegetation superpixels for further processing to improve the robustness and efficiency of building detection, the Douglas&ndash;Peucker algorithm and iterative optimization are used to eliminate jagged details generated from small structures as a result of superpixel segmentation. In the experiments, the UAV datasets, including many buildings in urban and rural areas with irregular shapes and different heights and that are obscured by trees, are collected to evaluate the proposed method. The experimental results based on the qualitative and quantitative measures confirm the effectiveness and high accuracy of the proposed framework relative to the digitized results. The proposed framework performs better than state-of-the-art building extraction methods, given its higher values of recall, precision, and intersection over Union (IoU).},
DOI = {10.3390/rs11091040}
}



@Article{rs11091042,
AUTHOR = {Hakdaoui, Sofia and Emran, Anas and Pradhan, Biswajeet and Lee, Chang-Wook and Nguemhe Fils, Salomon Cesar},
TITLE = {A Collaborative Change Detection Approach on Multi-Sensor Spatial Imagery for Desert Wetland Monitoring after a Flash Flood in Southern Morocco},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1042},
URL = {https://www.mdpi.com/2072-4292/11/9/1042},
ISSN = {2072-4292},
ABSTRACT = {This study aims to present a technique that combines multi-sensor spatial data to monitor wetland areas after a flash-flood event in a Saharan arid region. To extract the most efficient information, seven satellite images (radar and optical) taken before and after the event were used. To achieve the objectives, this study used Sentinel-1 data to discriminate water body and soil roughness, and optical data to monitor the soil moisture after the event. The proposed method combines two approaches: one based on spectral processing, and the other based on categorical processing. The first step was to extract four spectral indices and utilize change vector analysis on multispectral diachronic images from three MSI Sentinel-2 images and two Landsat-8 OLI images acquired before and after the event. The second step was performed using pattern classification techniques, namely, linear classifiers based on support vector machines (SVM) with Gaussian kernels. The results of these two approaches were fused to generate a collaborative wetland change map. The application of co-registration and supervised classification based on textural and intensity information from Radar Sentinel-1 images taken before and after the event completes this work. The results obtained demonstrate the importance of the complementarity of multi-sensor images and a multi-approach methodology to better monitor changes to a wetland area after a flash-flood disaster.},
DOI = {10.3390/rs11091042}
}



@Article{sym11050617,
AUTHOR = {Li, Zhiwei and Lu, Yu and Shi, Yun and Wang, Zengguang and Qiao, Wenxin and Liu, Yicen},
TITLE = {A Dyna-Q-Based Solution for UAV Networks Against Smart Jamming Attacks},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {617},
URL = {https://www.mdpi.com/2073-8994/11/5/617},
ISSN = {2073-8994},
ABSTRACT = {Unmanned aerial vehicle (UAV) networks have a wide range of applications, such as in the Internet of Things (IoT), 5G communications, and so forth. However, the communications between UAVs and UAVs to ground control stations mainly use radio channels, and therefore these communications are vulnerable to cyberattacks. With the advent of software-defined radio (SDR), smart attacks that can flexibly select attack strategies according to the defender&rsquo;s state information are gradually attracting the attention of researchers and potential attackers of UAV networks. The smart attack can even induce the defender to take a specific defense strategy, causing even greater damage. Inspired by symmetrical thinking, a solution using a software-defined network (SDN) to combat software-defined radio was proposed. We propose a network architecture which uses dual controllers, including a UAV flight controller and SDN controller, to achieve collaborative decision-making. Built on the top of the SDN, the state information of the whole network converges quickly and is fitted to an environment model used to develop an improved Dyna-Q-based reinforcement learning algorithm. The improved algorithm integrates the power allocation and track planning of UAVs into a unified action space. The simulation data showed that the proposed communication solution can effectively avoid smart jamming attacks and has faster learning efficiency and higher convergence performance than the compared algorithms.},
DOI = {10.3390/sym11050617}
}



@Article{rs11091046,
AUTHOR = {Jia, Heming and Xing, Zhikai and Song, Wenlong},
TITLE = {Three Dimensional Pulse Coupled Neural Network Based on Hybrid Optimization Algorithm for Oil Pollution Image Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1046},
URL = {https://www.mdpi.com/2072-4292/11/9/1046},
ISSN = {2072-4292},
ABSTRACT = {This paper proposes a three dimensional pulse coupled neural network (3DPCNN) image segmentation method based on a hybrid seagull optimization algorithm (HSOA) to solve the oil pollution image. The image of oil pollution is taken by the unmanned aerial vehicle (UAV) in the oil field area. The UAV is good at shooting the ground area, but its ability to identify the oil pollution area is poor. In order to solve this problem, a 3DPCNN-HSOA algorithm is proposed to segment the oil pollution image, and the oil pollution area is segmented to identify the dirty oil area and improve the inspection of environmental pollution. The 3DPCNN image segmentation method has simple structure and good segmentation effect, but it has many parameters and poor segmentation effect for complex oil images. Therefore, we apply HSOA algorithm to optimize the parameters of 3DPCNN algorithm, so as to improve the segmentation accuracy and solve the segmentation of oil pollution images. The experimental results show that the 3DPCNN-HSOA model can separate the oil pollution area from the complex background.},
DOI = {10.3390/rs11091046}
}



@Article{su11092580,
AUTHOR = {Guimarães, Tainá T. and Veronez, Maurício R. and Koste, Emilie C. and Souza, Eniuce M. and Brum, Diego and Gonzaga, Luiz and Mauad, Frederico F.},
TITLE = {Evaluation of Regression Analysis and Neural Networks to Predict Total Suspended Solids in Water Bodies from Unmanned Aerial Vehicle Images},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2580},
URL = {https://www.mdpi.com/2071-1050/11/9/2580},
ISSN = {2071-1050},
ABSTRACT = {The concentration of suspended solids in water is one of the quality parameters that can be recovered using remote sensing data. This paper investigates the data obtained using a sensor coupled to an unmanned aerial vehicle (UAV) in order to estimate the concentration of suspended solids in a lake in southern Brazil based on the relation of spectral images and limnological data. The water samples underwent laboratory analysis to determine the concentration of total suspended solids (TSS). The images obtained using the UAV were orthorectified and georeferenced so that the values referring to the near, green, and blue infrared channels were collected at each sampling point to relate with the laboratory data. The prediction of the TSS concentration was performed using regression analysis and artificial neural networks. The obtained results were important for two main reasons. First, although regression methods have been used in remote sensing applications, they may not be adequate to capture the linear and/or non-linear relationships of interest. Second, results show that the integration of UAV in the mapping of water bodies together with the application of neural networks in the data analysis is a promising approach to predict TSS as well as their temporal and spatial variations.},
DOI = {10.3390/su11092580}
}



@Article{soilsystems3020033,
AUTHOR = {Krenz, Juliane and Greenwood, Philip and Kuhn, Nikolaus J.},
TITLE = {Soil Degradation Mapping in Drylands Using Unmanned Aerial Vehicle (UAV) Data},
JOURNAL = {Soil Systems},
VOLUME = {3},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {33},
URL = {https://www.mdpi.com/2571-8789/3/2/33},
ISSN = {2571-8789},
ABSTRACT = {Arid and semi-arid landscapes often show a patchwork of bare and vegetated spaces. Their heterogeneous patterns can be of natural origin, but may also indicate soil degradation. This study investigates the use of unmanned aerial vehicle (UAV) imagery to identify the degradation status of soils, based on the hypothesis that vegetation cover can be used as a proxy for estimating the soils&rsquo; health status. To assess the quality of the UAV-derived products, we compare a conventional field-derived map (FM) with two modelled maps based on (i) vegetation cover (RGB map), and (ii) vegetation cover, topographic information, and a flow accumulation analysis (RGB+DEM map). All methods were able to identify areas of soil degradation but differed in the extent of classified soil degradation, with the RGB map classifying the least amount as degraded. The RGB+DEM map classified 12% more as degraded than the FM, due to the wider perspective of the UAV compared to conventional field mapping. Overall, conventional UAVs provide a valuable tool for soil mapping in heterogeneous landscapes where manual field sampling is very time consuming. Additionally, the UAVs&rsquo; planform view from a bird&rsquo;s-eye perspective can overcome the limited view from the surveyors&rsquo; (ground-based) vantage point.},
DOI = {10.3390/soilsystems3020033}
}



@Article{s19092130,
AUTHOR = {Zemmour, Elie and Kurtser, Polina and Edan, Yael},
TITLE = {Automatic Parameter Tuning for Adaptive Thresholding in Fruit Detection},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2130},
URL = {https://www.mdpi.com/1424-8220/19/9/2130},
ISSN = {1424-8220},
ABSTRACT = {This paper presents an automatic parameter tuning procedure specially developed for a dynamic adaptive thresholding algorithm for fruit detection. One of the major algorithm strengths is its high detection performances using a small set of training images. The algorithm enables robust detection in highly-variable lighting conditions. The image is dynamically split into variably-sized regions, where each region has approximately homogeneous lighting conditions. Nine thresholds were selected to accommodate three different illumination levels for three different dimensions in four color spaces: RGB, HSI, LAB, and NDI. Each color space uses a different method to represent a pixel in an image: RGB (Red, Green, Blue), HSI (Hue, Saturation, Intensity), LAB (Lightness, Green to Red and Blue to Yellow) and NDI (Normalized Difference Index, which represents the normal difference between the RGB color dimensions). The thresholds were selected by quantifying the required relation between the true positive rate and false positive rate. A tuning process was developed to determine the best fit values of the algorithm parameters to enable easy adaption to different kinds of fruits (shapes, colors) and environments (illumination conditions). Extensive analyses were conducted on three different databases acquired in natural growing conditions: red apples (nine images with 113 apples), green grape clusters (129 images with 1078 grape clusters), and yellow peppers (30 images with 73 peppers). These databases are provided as part of this paper for future developments. The algorithm was evaluated using cross-validation with 70% images for training and 30% images for testing. The algorithm successfully detected apples and peppers in variable lighting conditions resulting with an F-score of 93.17% and 99.31% respectively. Results show the importance of the tuning process for the generalization of the algorithm to different kinds of fruits and environments. In addition, this research revealed the importance of evaluating different color spaces since for each kind of fruit, a different color space might be superior over the others. The LAB color space is most robust to noise. The algorithm is robust to changes in the threshold learned by the training process and to noise effects in images.},
DOI = {10.3390/s19092130}
}



@Article{app9091908,
AUTHOR = {Zhang, Yan and Wen, Jinxiao and Yang, Guanshu and He, Zunwen and Wang, Jing},
TITLE = {Path Loss Prediction Based on Machine Learning: Principle, Method, and Data Expansion},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1908},
URL = {https://www.mdpi.com/2076-3417/9/9/1908},
ISSN = {2076-3417},
ABSTRACT = {Path loss prediction is of great significance for the performance optimization of wireless networks. With the development and deployment of the fifth-generation (5G) mobile communication systems, new path loss prediction methods with high accuracy and low complexity should be proposed. In this paper, the principle and procedure of machine-learning-based path loss prediction are presented. Measured data are used to evaluate the performance of different models such as artificial neural network, support vector regression, and random forest. It is shown that these machine-learning-based models outperform the log-distance model. In view of the fact that the volume of measured data sometimes cannot meet the requirements of machine learning algorithms, we propose two mechanisms to expand the training dataset. On one hand, old measured data can be reused in new scenarios or at different frequencies. On the other hand, the classical model can also be utilized to generate a number of training samples based on the prior information obtained from measured results. Measured data are employed to verify the feasibility of these data expansion mechanisms. Finally, some issues for future research are discussed.},
DOI = {10.3390/app9091908}
}



@Article{app9091909,
AUTHOR = {Van Pham, Hai and Asadi, Farzin and Abut, Nurettin and Kandilli, Ismet},
TITLE = {Hybrid Spiral STC-Hedge Algebras Model in Knowledge Reasonings for Robot Coverage Path Planning and Its Applications},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1909},
URL = {https://www.mdpi.com/2076-3417/9/9/1909},
ISSN = {2076-3417},
ABSTRACT = {Robotics is a highly developed field in industry, and there is a large research effort in terms of humanoid robotics, including the development of multi-functional empathetic robots as human companions. An important function of a robot is to find an optimal coverage path planning, with obstacle avoidance in dynamic environments for cleaning and monitoring robotics. This paper proposes a novel approach to enable robotic path planning. The proposed approach combines robot reasoning with knowledge reasoning techniques, hedge algebra, and the Spiral Spanning Tree Coverage (STC) algorithm, for a cleaning and monitoring robot with optimal decisions. This approach is used to apply knowledge inference and hedge algebra with the Spiral STC algorithm to enable autonomous robot control in the optimal coverage path planning, with minimum obstacle avoidance. The results of experiments show that the proposed approach in the optimal robot path planning avoids tangible and intangible obstacles for the monitoring and cleaning robot. Experimental results are compared with current methods under the same conditions. The proposed model using knowledge reasoning techniques in the optimal coverage path performs better than the conventional algorithms in terms of high robot coverage and low repetition rates. Experiments are done with real robots for cleaning in dynamic environments.},
DOI = {10.3390/app9091909}
}



@Article{s19092165,
AUTHOR = {Teng, Xichao and Yu, Qifeng and Luo, Jing and Wang, Gang and Zhang, Xiaohu},
TITLE = {Aircraft Pose Estimation Based on Geometry Structure Features and Line Correspondences},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2165},
URL = {https://www.mdpi.com/1424-8220/19/9/2165},
ISSN = {1424-8220},
ABSTRACT = {A robust and accurate aircraft pose estimation method is proposed in this paper. The aircraft pose reflects the flight status of the aircraft and accurate pose measurement is of great importance in many aerospace applications. This work aims to establish a universal framework to estimate the aircraft pose based on generic geometry structure features. In our method, line features are extracted to describe the structure of an aircraft in single images and the generic geometry features are exploited to form line groups for aircraft structure recognition. Parallel line clustering is utilized to detect the fuselage reference line and bilateral symmetry property of aircraft provides an important constraint for the extraction of wing edge lines under weak perspective projection. After identifying the main structure of the aircraft, a planes intersection method is used to obtain the 3D pose parameters based on the established line correspondences. Our proposed method can increase the measuring range of binocular vision sensors and has the advantage of not relying on 3D models, cooperative marks or other feature datasets. Experimental results show that our method can obtain reliable and accurate pose information of different types of aircraft.},
DOI = {10.3390/s19092165}
}



@Article{s19092168,
AUTHOR = {Wang, Chuanyun and Wang, Tian and Wang, Ershen and Sun, Enyan and Luo, Zhen},
TITLE = {Flying Small Target Detection for Anti-UAV Based on a Gaussian Mixture Model in a Compressive Sensing Domain},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2168},
URL = {https://www.mdpi.com/1424-8220/19/9/2168},
ISSN = {1424-8220},
ABSTRACT = {Addressing the problems of visual surveillance for anti-UAV, a new flying small target detection method is proposed based on Gaussian mixture background modeling in a compressive sensing domain and low-rank and sparse matrix decomposition of local image. First of all, images captured by stationary visual sensors are broken into patches and the candidate patches which perhaps contain targets are identified by using a Gaussian mixture background model in a compressive sensing domain. Subsequently, the candidate patches within a finite time period are separated into background images and target images by low-rank and sparse matrix decomposition. Finally, flying small target detection is achieved over separated target images by threshold segmentation. The experiment results using visible and infrared image sequences of flying UAV demonstrate that the proposed methods have effective detection performance and outperform the baseline methods in precision and recall evaluation.},
DOI = {10.3390/s19092168}
}



@Article{rs11091128,
AUTHOR = {Rahnemoonfar, Maryam and Dobbs, Dugan and Yari, Masoud and Starek, Michael J.},
TITLE = {DisCountNet: Discriminating and Counting Network for Real-Time Counting and Localization of Sparse Objects in High-Resolution UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1128},
URL = {https://www.mdpi.com/2072-4292/11/9/1128},
ISSN = {2072-4292},
ABSTRACT = {Recent deep-learning counting techniques revolve around two distinct features of data&mdash;sparse data, which favors detection networks, or dense data where density map networks are used. Both techniques fail to address a third scenario, where dense objects are sparsely located. Raw aerial images represent sparse distributions of data in most situations. To address this issue, we propose a novel and exceedingly portable end-to-end model, DisCountNet, and an example dataset to test it on. DisCountNet is a two-stage network that uses theories from both detection and heat-map networks to provide a simple yet powerful design. The first stage, DiscNet, operates on the theory of coarse detection, but does so by converting a rich and high-resolution image into a sparse representation where only important information is encoded. Following this, CountNet operates on the dense regions of the sparse matrix to generate a density map, which provides fine locations and count predictions on densities of objects. Comparing the proposed network to current state-of-the-art networks, we find that we can maintain competitive performance while using a fraction of the computational complexity, resulting in a real-time solution.},
DOI = {10.3390/rs11091128}
}



@Article{app9091952,
AUTHOR = {Petrellis, Nikos},
TITLE = {Plant Disease Diagnosis for Smart Phone Applications with Extensible Set of Diseases},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1952},
URL = {https://www.mdpi.com/2076-3417/9/9/1952},
ISSN = {2076-3417},
ABSTRACT = {A plant disease diagnosis method that can be implemented with the resources of a mobile phone application, that does not have to be connected to a remote server, is presented and evaluated on citrus diseases. It can be used both by amateur gardeners and by professional agriculturists for early detection of diseases. The features used are extracted from photographs of plant parts like leaves or fruits and include the color, the relative area and the number of the lesion spots. These classification features, along with additional information like weather metadata, form disease signatures that can be easily defined by the end user (e.g., an agronomist). These signatures are based on the statistical processing of a small number of representative training photographs. The extracted features of a test photograph are compared against the disease signatures in order to select the most likely disease. An important advantage of the proposed approach is that the diagnosis does not depend on the orientation, the scale or the resolution of the photograph. The experiments have been conducted under several light exposure conditions. The accuracy was experimentally measured between 70% and 99%. An acceptable accuracy higher than 90% can be achieved in most of the cases since the lesion spots can recognized interactively with high precision.},
DOI = {10.3390/app9091952}
}



@Article{rs11091142,
AUTHOR = {Zhan, Shuyue and Wang, Chao and Liu, Shuchang and Xia, Kaibo and Huang, Hui and Li, Xiaorun and Liu, Caicai and Xu, Ren},
TITLE = {Floating Xylene Spill Segmentation from Ultraviolet Images via Target Enhancement},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1142},
URL = {https://www.mdpi.com/2072-4292/11/9/1142},
ISSN = {2072-4292},
ABSTRACT = {Automatic colorless floating hazardous and noxious substances (HNS) spill segmentation is an emerging research topic. Xylene is one of the priority HNSs since it poses a high risk of being involved in an HNS incident. This paper presents a novel algorithm for the target enhancement of xylene spills and their segmentation in ultraviolet (UV) images. To improve the contrast between targets and backgrounds (waves, sun reflections, and shadows), we developed a global background suppression (GBS) method to remove the irrelevant objects from the background, which is followed by an adaptive target enhancement (ATE) method to enhance the target. Based on the histogram information of the processed image, we designed an automatic algorithm to calculate the optimal number of clusters, which is usually manually determined in traditional cluster segmentation methods. In addition, necessary pre-segmentation processing and post-segmentation processing were adopted in order to improve the performance. Experimental results on our UV image datasets demonstrated that the proposed method can achieve good segmentation results for chemical spills from different backgrounds, especially for images with strong waves, uneven intensities, and low contrast.},
DOI = {10.3390/rs11091142}
}



@Article{rs11101153,
AUTHOR = {Bejiga, Mesay Belete and Melgani, Farid and Beraldini, Pietro},
TITLE = {Domain Adversarial Neural Networks for Large-Scale Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1153},
URL = {https://www.mdpi.com/2072-4292/11/10/1153},
ISSN = {2072-4292},
ABSTRACT = {Learning classification models require sufficiently labeled training samples, however, collecting labeled samples for every new problem is time-consuming and costly. An alternative approach is to transfer knowledge from one problem to another, which is called transfer learning. Domain adaptation (DA) is a type of transfer learning that aims to find a new latent space where the domain discrepancy between the source and the target domain is negligible. In this work, we propose an unsupervised DA technique called domain adversarial neural networks (DANNs), composed of a feature extractor, a class predictor, and domain classifier blocks, for large-scale land cover classification. Contrary to the traditional methods that perform representation and classifier learning in separate stages, DANNs combine them into a single stage, thereby learning a new representation of the input data that is both domain-invariant and discriminative. Once trained, the classifier of a DANN can be used to predict both source and target domain labels. Additionally, we also modify the domain classifier of a DANN to evaluate its suitability for multi-target domain adaptation problems. Experimental results obtained for both single and multiple target DA problems show that the proposed method provides a performance gain of up to 40%.},
DOI = {10.3390/rs11101153}
}



@Article{rs11101157,
AUTHOR = {Fuentes-Pacheco, Jorge and Torres-Olivares, Juan and Roman-Rangel, Edgar and Cervantes, Salvador and Juarez-Lopez, Porfirio and Hermosillo-Valadez, Jorge and Rendón-Mancha, Juan Manuel},
TITLE = {Fig Plant Segmentation from Aerial Images Using a Deep Convolutional Encoder-Decoder Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1157},
URL = {https://www.mdpi.com/2072-4292/11/10/1157},
ISSN = {2072-4292},
ABSTRACT = {Crop segmentation is an important task in Precision Agriculture, where the use of aerial robots with an on-board camera has contributed to the development of new solution alternatives. We address the problem of fig plant segmentation in top-view RGB (Red-Green-Blue) images of a crop grown under open-field difficult circumstances of complex lighting conditions and non-ideal crop maintenance practices defined by local farmers. We present a Convolutional Neural Network (CNN) with an encoder-decoder architecture that classifies each pixel as crop or non-crop using only raw colour images as input. Our approach achieves a mean accuracy of 93.85% despite the complexity of the background and a highly variable visual appearance of the leaves. We make available our CNN code to the research community, as well as the aerial image data set and a hand-made ground truth segmentation with pixel precision to facilitate the comparison among different algorithms.},
DOI = {10.3390/rs11101157}
}



@Article{en12101843,
AUTHOR = {Gil-Antonio, Leopoldo and Saldivar, Belem and Portillo-Rodríguez, Otniel and Ávila-Vilchis, Juan Carlos and Martínez-Rodríguez, Pánfilo Raymundo and Martínez-Méndez, Rigoberto},
TITLE = {Flatness-Based Control for the Maximum Power Point Tracking in a Photovoltaic System},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1843},
URL = {https://www.mdpi.com/1996-1073/12/10/1843},
ISSN = {1996-1073},
ABSTRACT = {Solar energy harvesting using Photovoltaic (PV) systems is one of the most popular sources of renewable energy, however the main drawback of PV systems is their low conversion efficiency. An optimal system operation requires an efficient tracking of the Maximum Power Point (MPP), which represents the maximum energy that can be extracted from the PV panel. This paper presents a novel control approach for the Maximum Power Point Tracking (MPPT) based on the differential flatness property of the Boost converter, which is one of the most used converters in PV systems. The underlying idea of the proposed control approach is to use the classical flatness-based trajectory tracking control where a reference voltage will be defined in terms of the maximum power provided by the PV panel. The effectiveness of the proposed controller is assessed through numerical simulations and experimental tests. The results show that the controller based on differential flatness is capable of converging in less than 0.15 s and, compared with other MPPT techniques, such as Incremental Conductance and Perturb and Observe, it improves the response against sudden changes in load or weather conditions, reducing the ringing in the output of the system. Based on the results, it can be inferred that the new flatness-based controller represents an alternative to improve the MPPT in PV systems, especially when they are subject to sudden load or weather changes.},
DOI = {10.3390/en12101843}
}



@Article{app9102009,
AUTHOR = {Han, Jiaming and Yang, Zhong and Zhang, Qiuyan and Chen, Cong and Li, Hongchen and Lai, Shangxiang and Hu, Guoxiong and Xu, Changliang and Xu, Hao and Wang, Di and Chen, Rui},
TITLE = {A Method of Insulator Faults Detection in Aerial Images for High-Voltage Transmission Lines Inspection},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2009},
URL = {https://www.mdpi.com/2076-3417/9/10/2009},
ISSN = {2076-3417},
ABSTRACT = {Insulator faults detection is an important task for high-voltage transmission line inspection. However, current methods often suffer from the lack of accuracy and robustness. Moreover, these methods can only detect one fault in the insulator string, but cannot detect a multi-fault. In this paper, a novel method is proposed for insulator one fault and multi-fault detection in UAV-based aerial images, the backgrounds of which usually contain much complex interference. The shapes of the insulators also vary obviously due to the changes in filming angle and distance. To reduce the impact of complex interference on insulator faults detection, we make full use of the deep neural network to distinguish between insulators and background interference. First of all, plenty of insulator aerial images with manually labelled ground-truth are collected to construct a standard insulator detection dataset &lsquo;InST_detection&rsquo;. Secondly, a new convolutional network is proposed to obtain accurate insulator string positions in the aerial image. Finally, a novel fault detection method is proposed that can detect both insulator one fault and multi-fault in aerial images. Experimental results on a large number of aerial images show that our proposed method is more effective and efficient than the state-of-the-art insulator fault detection methods.},
DOI = {10.3390/app9102009}
}



@Article{s19102271,
AUTHOR = {Bi, Fukun and Hou, Jinyuan and Chen, Liang and Yang, Zhihua and Wang, Yanping},
TITLE = {Ship Detection for Optical Remote Sensing Images Based on Visual Attention Enhanced Network},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2271},
URL = {https://www.mdpi.com/1424-8220/19/10/2271},
ISSN = {1424-8220},
ABSTRACT = {Ship detection plays a significant role in military and civil fields. Although some state-of-the-art detection methods, based on convolutional neural networks (CNN) have certain advantages, they still cannot solve the challenge well, including the large size of images, complex scene structure, a large amount of false alarm interference, and inshore ships. This paper proposes a ship detection method from optical remote sensing images, based on visual attention enhanced network. To effectively reduce false alarm in non-ship area and improve the detection efficiency from remote sensing images, we developed a light-weight local candidate scene network(     L 2     CSN) to extract the local candidate scenes with ships. Then, for the selected local candidate scenes, we propose a ship detection method, based on the visual attention DSOD(VA-DSOD). Here, to enhance the detection performance and positioning accuracy of inshore ships, we both extract semantic features, based on DSOD and embed a visual attention enhanced network in DSOD to extract the visual features. We test the detection method on a large number of typical remote sensing datasets, which consist of Google Earth images and GaoFen-2 images. We regard the state-of-the-art method [sliding window DSOD (SW+DSOD)] as a baseline, which achieves the average precision (AP) of 82.33%. The AP of the proposed method increases by 7.53%. The detection and location performance of our proposed method outperforms the baseline in complex remote sensing scenes.},
DOI = {10.3390/s19102271}
}



@Article{rs11101174,
AUTHOR = {Sheykhmousa, Mohammadreza and Kerle, Norman and Kuffer, Monika and Ghaffarian, Saman},
TITLE = {Post-Disaster Recovery Assessment with Machine Learning-Derived Land Cover and Land Use Information},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1174},
URL = {https://www.mdpi.com/2072-4292/11/10/1174},
ISSN = {2072-4292},
ABSTRACT = {Post-disaster recovery (PDR) is a complex, long-lasting, resource intensive, and poorly understood process. PDR goes beyond physical reconstruction (physical recovery) and includes relevant processes such as economic and social (functional recovery) processes. Knowing the size and location of the places that positively or negatively recovered is important to effectively support policymakers to help readjust planning and resource allocation to rebuild better. Disasters and the subsequent recovery are mainly expressed through unique land cover and land use changes (LCLUCs). Although LCLUCs have been widely studied in remote sensing, their value for recovery assessment has not yet been explored, which is the focus of this paper. An RS-based methodology was created for PDR assessment based on multi-temporal, very high-resolution satellite images. Different trajectories of change were analyzed and evaluated, i.e., transition patterns (TPs) that signal positive or negative recovery. Experimental analysis was carried out on three WorldView-2 images acquired over Tacloban city, Philippines, which was heavily affected by Typhoon Haiyan in 2013. Support vector machine, a robust machine learning algorithm, was employed with texture features extracted from the grey level co-occurrence matrix and local binary patterns. Although classification results for the images before and four years after the typhoon show high accuracy, substantial uncertainties mark the results for the immediate post-event image. All land cover (LC) and land use (LU) classified maps were stacked, and only changes related to TPs were extracted. The final products are LC and LU recovery maps that quantify the PDR process at the pixel level. It was found that physical and functional recovery can be mainly explained through the LCLUC information. In addition, LC and LU-based recovery maps support a general and a detailed recovery understanding, respectively. It is therefore suggested to use the LC and LU-based recovery maps to monitor and support the short and the long-term recovery, respectively.},
DOI = {10.3390/rs11101174}
}



@Article{ijgi8050240,
AUTHOR = {Kim, Nari and Ha, Kyung-Ja and Park, No-Wook and Cho, Jaeil and Hong, Sungwook and Lee, Yang-Won},
TITLE = {A Comparison Between Major Artificial Intelligence Models for Crop Yield Prediction: Case Study of the Midwestern United States, 2006–2015},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {240},
URL = {https://www.mdpi.com/2220-9964/8/5/240},
ISSN = {2220-9964},
ABSTRACT = {This paper compares different artificial intelligence (AI) models in order to develop the best crop yield prediction model for the Midwestern United States (US). Through experiments to examine the effects of phenology using three different periods, we selected the July&ndash;August (JA) database as the best months to predict corn and soybean yields. Six different AI models for crop yield prediction are tested in this research. Then, a comprehensive and objective comparison is conducted between the AI models. Particularly for the deep neural network (DNN) model, we performed an optimization process to ensure the best configurations for the layer structure, cost function, optimizer, activation function, and drop-out ratio. In terms of mean absolute error (MAE), our DNN model with the JA database was approximately 21&ndash;33% and 17&ndash;22% more accurate for corn and soybean yields, respectively, than the other five AI models. This indicates that corn and soybean yields for a given year can be forecasted in advance, at the beginning of September, approximately a month or more ahead of harvesting time. A combination of the optimized DNN model and spatial statistical methods should be investigated in future work, to mitigate partly clustered errors in some regions.},
DOI = {10.3390/ijgi8050240}
}



@Article{rs11101241,
AUTHOR = {Li, Jing and Chen, Shuo and Zhang, Fangbing and Li, Erkang and Yang, Tao and Lu, Zhaoyang},
TITLE = {An Adaptive Framework for Multi-Vehicle Ground Speed Estimation in Airborne Videos},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1241},
URL = {https://www.mdpi.com/2072-4292/11/10/1241},
ISSN = {2072-4292},
ABSTRACT = {With the rapid development of unmanned aerial vehicles (UAVs), UAV-based intelligent airborne surveillance systems represented by real-time ground vehicle speed estimation have attracted wide attention from researchers. However, there are still many challenges in extracting speed information from UAV videos, including the dynamic moving background, small target size, complicated environment, and diverse scenes. In this paper, we propose a novel adaptive framework for multi-vehicle ground speed estimation in airborne videos. Firstly, we build a traffic dataset based on UAV. Then, we use the deep learning detection algorithm to detect the vehicle in the UAV field of view and obtain the trajectory in the image through the tracking-by-detection algorithm. Thereafter, we present a motion compensation method based on homography. This method obtains matching feature points by an optical flow method and eliminates the influence of the detected target to accurately calculate the homography matrix to determine the real motion trajectory in the current frame. Finally, vehicle speed is estimated based on the mapping relationship between the pixel distance and the actual distance. The method regards the actual size of the car as prior information and adaptively recovers the pixel scale by estimating the vehicle size in the image; it then calculates the vehicle speed. In order to evaluate the performance of the proposed system, we carry out a large number of experiments on the AirSim Simulation platform as well as real UAV aerial surveillance experiments. Through quantitative and qualitative analysis of the simulation results and real experiments, we verify that the proposed system has a unique ability to detect, track, and estimate the speed of ground vehicles simultaneously even with a single downward-looking camera. Additionally, the system can obtain effective and accurate speed estimation results, even in various complex scenes.},
DOI = {10.3390/rs11101241}
}



@Article{rs11101243,
AUTHOR = {Ge, Xuming and Wu, Bo and Li, Yuan and Hu, Han},
TITLE = {A Multi-Primitive-Based Hierarchical Optimal Approach for Semantic Labeling of ALS Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1243},
URL = {https://www.mdpi.com/2072-4292/11/10/1243},
ISSN = {2072-4292},
ABSTRACT = {There are normally three main steps to carrying out the labeling of airborne laser scanning (ALS) point clouds. The first step is to use appropriate primitives to represent the scanning scenes, the second is to calculate the discriminative features of each primitive, and the third is to introduce a classifier to label the point clouds. This paper investigates multiple primitives to effectively represent scenes and exploit their geometric relationships. Relationships are graded according to the properties of related primitives. Then, based on initial labeling results, a novel, hierarchical, and optimal strategy is developed to optimize semantic labeling results. The proposed approach was tested using two sets of representative ALS point clouds, namely the Vaihingen datasets and Hong Kong&rsquo;s Central District dataset. The results were compared with those generated by other typical methods in previous work. Quantitative assessments for the two experimental datasets showed that the performance of the proposed approach was superior to reference methods in both datasets. The scores for correctness attained over 98% in all cases of the Vaihingen datasets and up to 96% in the Hong Kong dataset. The results reveal that our approach of labeling different classes in terms of ALS point clouds is robust and bears significance for future applications, such as 3D modeling and change detection from point clouds.},
DOI = {10.3390/rs11101243}
}



@Article{electronics8050576,
AUTHOR = {You, Shixun and Diao, Ming and Gao, Lipeng},
TITLE = {Completing Explorer Games with a Deep Reinforcement Learning Framework Based on Behavior Angle Navigation},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {576},
URL = {https://www.mdpi.com/2079-9292/8/5/576},
ISSN = {2079-9292},
ABSTRACT = {In cognitive electronic warfare, when a typical combat vehicle, such as an unmanned combat air vehicle (UCAV), uses radar sensors to explore an unknown space, the target-searching fails due to an inefficient servoing/tracking system. Thus, to solve this problem, we developed an autonomous reasoning search method that can generate efficient decision-making actions and guide the UCAV as early as possible to the target area. For high-dimensional continuous action space, the UCAV&rsquo;s maneuvering strategies are subject to certain physical constraints. We first record the path histories of the UCAV as a sample set of supervised experiments and then construct a grid cell network using long short-term memory (LSTM) to generate a new displacement prediction to replace the target location estimation. Finally, we enable a variety of continuous-control-based deep reinforcement learning algorithms to output optimal/sub-optimal decision-making actions. All these tasks are performed in a three-dimensional target-searching simulator, i.e., the Explorer game. Please note that we use the behavior angle (BHA) for the first time as the main factor of the reward-shaping of the deep reinforcement learning framework and successfully make the trained UCAV achieve a 99.96% target destruction rate, i.e., the game win rate, in a 0.1 s operating cycle.},
DOI = {10.3390/electronics8050576}
}



@Article{s19102396,
AUTHOR = {Lin, Shijie and Wang, Jinwang and Peng, Rui and Yang, Wen},
TITLE = {Development of an Autonomous Unmanned Aerial Manipulator Based on a Real-Time Oriented-Object Detection Method},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2396},
URL = {https://www.mdpi.com/1424-8220/19/10/2396},
ISSN = {1424-8220},
ABSTRACT = {Autonomous Unmanned Aerial Manipulators (UAMs) have shown promising potential in mobile 3-dimensional grasping applications, but they still suffer from some difficulties impeding their board applications, such as target detection and indoor positioning. For the autonomous grasping mission, the UAMs need ability to recognize the objects and grasp them. Considering the efficiency and precision, we present a novel oriented-object detection method called Rotation-SqueezeDet. This method can run on embedded-platforms in near real-time. Besides, this method can give the oriented bounding box of an object in images to enable a rotation-aware grasping. Based on this method, a UAM platform was designed and built. We have given the formulation, positioning, control, and planning of the whole UAM system. All the mechanical designs are fully provided as open-source hardware for reuse by the community. Finally, the effectiveness of the proposed scheme was validated in multiple experimental trials, highlighting its applicability of autonomous aerial rotational grasping in Global Positioning System (GPS) denied environments. We believe this system can be deployed to many potential workplaces which need UAM to accomplish difficult manipulation tasks.},
DOI = {10.3390/s19102396}
}



@Article{molecules24102025,
AUTHOR = {Tan, Jin Yeong and Ker, Pin Jern and Lau, K. Y. and Hannan, M. A. and Tang, Shirley Gee Hoon},
TITLE = {Applications of Photonics in Agriculture Sector: A Review},
JOURNAL = {Molecules},
VOLUME = {24},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2025},
URL = {https://www.mdpi.com/1420-3049/24/10/2025},
PubMedID = {31137897},
ISSN = {1420-3049},
ABSTRACT = {The agricultural industry has made a tremendous contribution to the foundations of civilization. Basic essentials such as food, beverages, clothes and domestic materials are enriched by the agricultural industry. However, the traditional method in agriculture cultivation is labor-intensive and inadequate to meet the accelerating nature of human demands. This scenario raises the need to explore state-of-the-art crop cultivation and harvesting technologies. In this regard, optics and photonics technologies have proven to be effective solutions. This paper aims to present a comprehensive review of three photonic techniques, namely imaging, spectroscopy and spectral imaging, in a comparative manner for agriculture applications. Essentially, the spectral imaging technique is a robust solution which combines the benefits of both imaging and spectroscopy but faces the risk of underutilization. This review also comprehends the practicality of all three techniques by presenting existing examples in agricultural applications. Furthermore, the potential of these techniques is reviewed and critiqued by looking into agricultural activities involving palm oil, rubber, and agro-food crops. All the possible issues and challenges in implementing the photonic techniques in agriculture are given prominence with a few selective recommendations. The highlighted insights in this review will hopefully lead to an increased effort in the development of photonics applications for the future agricultural industry.},
DOI = {10.3390/molecules24102025}
}



@Article{rs11111269,
AUTHOR = {Brabant, Charlotte and Alvarez-Vanhard, Emilien and Laribi, Achour and Morin, Gwénaël and Thanh Nguyen, Kim and Thomas, Alban and Houet, Thomas},
TITLE = {Comparison of Hyperspectral Techniques for Urban Tree Diversity Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1269},
URL = {https://www.mdpi.com/2072-4292/11/11/1269},
ISSN = {2072-4292},
ABSTRACT = {This research aims to assess the capabilities of Very High Spatial Resolution (VHSR) hyperspectral satellite data in order to discriminate urban tree diversity. Four dimension reduction methods and two classifiers are tested, using two learning methods and applied with four in situ sample datasets. An airborne HySpex image (408 bands/2 m) was acquired in July 2015 from which prototypal spaceborne hyperspectral images (named HYPXIM) at 4 m and 8 m and a multispectral Sentinel2 image at 10 m have been simulated for the purpose of this study. A comparison is made using these methods and datasets. The influence of dimension reduction methods is assessed on hyperspectral (HySpex and HYPXIM) and Sentinel2 datasets. The influence of conventional classifiers (Support Vector Machine &ndash;SVM&ndash; and Random Forest &ndash;RF&ndash;) and learning methods is evaluated on all image datasets (reduced and non-reduced hyperspectral and Sentinel2 datasets). Results show that HYPXIM 4 m and HySpex 2 m reduced by Minimum Noise Fraction (MNF) provide the greatest classification of 14 species using the SVM with an overall accuracy of 78.4% (&plusmn;1.5) and a kappa index of agreement of 0.7. More generally, the learning methods have a stronger influence than classifiers, or even than dimensional reduction methods, on urban tree diversity classification. Prototypal HYPXIM images appear to present a great compromise (192 spectral bands/4 m resolution) for urban vegetation applications compared to HySpex or Sentinel2 images.},
DOI = {10.3390/rs11111269}
}



@Article{s19112448,
AUTHOR = {Xiong, Xin and Zhang, Jingjin and Guo, Doudou and Chang, Liying and Huang, Danfeng},
TITLE = {Non-Invasive Sensing of Nitrogen in Plant Using Digital Images and Machine Learning for Brassica Campestris ssp. Chinensis L.},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2448},
URL = {https://www.mdpi.com/1424-8220/19/11/2448},
ISSN = {1424-8220},
ABSTRACT = {Monitoring plant nitrogen (N) in a timely way and accurately is critical for precision fertilization. The imaging technology based on visible light is relatively inexpensive and ubiquitous, and open-source analysis tools have proliferated. In this study, texture- and geometry-related phenotyping combined with color properties were investigated for their potential use in evaluating N in pakchoi (Brassica campestris ssp. chinensis L.). Potted pakchoi treated with four levels of N were cultivated in a greenhouse. Their top-view images were acquired using a camera at six growth stages. The corresponding plant N concentration was determined destructively. The quantitative relationships between the nitrogen nutrition index (NNI) and the image-based phenotyping features were established using the following algorithms: random forest (RF), support vector regression (SVR), and neural network (NN). The results showed the full model based on the color, texture, and geometry-related features outperforms the model based on only the color-related feature in predicting the NNI. The RF full model exhibited the most robust performance in both the seedling and harvest stages, reaching prediction accuracies of 0.823 and 0.943, respectively. The high prediction accuracy of the model allows for a low-cost, non-destructive monitoring of N in the field of precision crop management.},
DOI = {10.3390/s19112448}
}



@Article{cancers11060756,
AUTHOR = {Halicek, Martin and Fabelo, Himar and Ortega, Samuel and Callico, Gustavo M. and Fei, Baowei},
TITLE = {In-Vivo and Ex-Vivo Tissue Analysis through Hyperspectral Imaging Techniques: Revealing the Invisible Features of Cancer},
JOURNAL = {Cancers},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {756},
URL = {https://www.mdpi.com/2072-6694/11/6/756},
ISSN = {2072-6694},
ABSTRACT = {In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology.},
DOI = {10.3390/cancers11060756}
}



@Article{ijgi8060259,
AUTHOR = {Biswas, Debojit and Su, Hongbo and Wang, Chengyi and Stevanovic, Aleksandar},
TITLE = {Speed Estimation of Multiple Moving Objects from a Moving UAV Platform},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {259},
URL = {https://www.mdpi.com/2220-9964/8/6/259},
ISSN = {2220-9964},
ABSTRACT = {Speed detection of a moving object using an optical camera has always been an important subject to study in computer vision. This is one of the key components to address in many application areas, such as transportation systems, military and naval applications, and robotics. In this study, we implemented a speed detection system for multiple moving objects on the ground from a moving platform in the air. A detect-and-track approach is used for primary tracking of the objects. Faster R-CNN (region-based convolutional neural network) is applied to detect the objects, and a discriminative correlation filter with CSRT (channel and spatial reliability tracking) is used for tracking. Feature-based image alignment (FBIA) is done for each frame to get the proper object location. In addition, SSIM (structural similarity index measurement) is performed to check how similar the current frame is with respect to the object detection frame. This measurement is necessary because the platform is moving, and new objects may be captured in a new frame. We achieved a speed accuracy of 96.80% with our framework with respect to the real speed of the objects.},
DOI = {10.3390/ijgi8060259}
}



@Article{rs11111308,
AUTHOR = {Wang, Dongliang and Shao, Quanqin and Yue, Huanyin},
TITLE = {Surveying Wild Animals from Satellites, Manned Aircraft and Unmanned Aerial Systems (UASs): A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1308},
URL = {https://www.mdpi.com/2072-4292/11/11/1308},
ISSN = {2072-4292},
ABSTRACT = {This article reviews studies regarding wild animal surveys based on multiple platforms, including satellites, manned aircraft, and unmanned aircraft systems (UASs), and focuses on the data used, animal detection methods, and their accuracies. We also discuss the advantages and limitations of each type of remote sensing data and highlight some new research opportunities and challenges. Submeter very-high-resolution (VHR) spaceborne imagery has potential in modeling the population dynamics of large (&gt;0.6 m) wild animals at large spatial and temporal scales, but has difficulty discerning small (&lt;0.6 m) animals at the species level, although high-resolution commercial satellites, such as WorldView-3 and -4, have been able to collect images with a ground resolution of up to 0.31 m in panchromatic mode. This situation will not change unless the satellite image resolution is greatly improved in the future. Manned aerial surveys have long been employed to capture the centimeter-scale images required for animal censuses over large areas. However, such aerial surveys are costly to implement in small areas and can cause significant disturbances to wild animals because of their noise. In contrast, UAS surveys are seen as a safe, convenient and less expensive alternative to ground-based and conventional manned aerial surveys, but most UASs can cover only small areas. The proposed use of UAS imagery in combination with VHR satellite imagery would produce critical population data for large wild animal species and colonies over large areas. The development of software systems for automatically producing image mosaics and recognizing wild animals will further improve survey efficiency.},
DOI = {10.3390/rs11111308}
}



@Article{rs11111309,
AUTHOR = {Weinstein, Ben G. and Marconi, Sergio and Bohlman, Stephanie and Zare, Alina and White, Ethan},
TITLE = {Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1309},
URL = {https://www.mdpi.com/2072-4292/11/11/1309},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing can transform the speed, scale, and cost of biodiversity and forestry surveys. Data acquisition currently outpaces the ability to identify individual organisms in high resolution imagery. We outline an approach for identifying tree-crowns in RGB imagery while using a semi-supervised deep learning detection network. Individual crown delineation has been a long-standing challenge in remote sensing and available algorithms produce mixed results. We show that deep learning models can leverage existing Light Detection and Ranging (LIDAR)-based unsupervised delineation to generate trees that are used for training an initial RGB crown detection model. Despite limitations in the original unsupervised detection approach, this noisy training data may contain information from which the neural network can learn initial tree features. We then refine the initial model using a small number of higher-quality hand-annotated RGB images. We validate our proposed approach while using an open-canopy site in the National Ecological Observation Network. Our results show that a model using 434,551 self-generated trees with the addition of 2848 hand-annotated trees yields accurate predictions in natural landscapes. Using an intersection-over-union threshold of 0.5, the full model had an average tree crown recall of 0.69, with a precision of 0.61 for the visually-annotated data. The model had an average tree detection rate of 0.82 for the field collected stems. The addition of a small number of hand-annotated trees improved the performance over the initial self-supervised model. This semi-supervised deep learning approach demonstrates that remote sensing can overcome a lack of labeled training data by generating noisy data for initial training using unsupervised methods and retraining the resulting models with high quality labeled data.},
DOI = {10.3390/rs11111309}
}



@Article{app9112276,
AUTHOR = {Liu, Wending and Liu, Hanxing and Wang, Yuan and Zheng, Xiaorui and Zhang, Junguo},
TITLE = {A Novel Extraction Method for Wildlife Monitoring Images with Wireless Multimedia Sensor Networks (WMSNs)},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2276},
URL = {https://www.mdpi.com/2076-3417/9/11/2276},
ISSN = {2076-3417},
ABSTRACT = {In remote areas, wireless multimedia sensor networks (WMSNs) have limited energy, and the data processing of wildlife monitoring images always suffers from energy consumption limitations. Generally, only part of each wildlife image is valuable. Therefore, the above mentioned issue could be avoided by transmitting the target area. Inspired by this transport strategy, in this paper, we propose an image extraction method with a low computational complexity, which can be adapted to extract the target area (i.e., the animal) and its background area according to the characteristics of the image pixels. Specifically, we first reconstruct a color space model via a CIELUV (LUV) color space framework to extract the color parameters. Next, according to the importance of the Hermite polynomial, a Hermite filter is utilized to extract the texture features, which ensures the accuracy of the split extraction of wildlife images. Then, an adaptive mean-shift algorithm is introduced to cluster texture features and color space information, realizing the extraction of the foreground area in the monitoring image. To verify the performance of the algorithm, a demonstration of the extraction of field-captured wildlife images is presented. Further, we conduct a comparative experiment with N-cuts (N-cuts), the existing aggregating super-pixels (SAS) algorithm, and the histogram contrast saliency detection (HCS) algorithm. A comparison of the results shows that the proposed algorithm for monitoring image target area extraction increased the average pixel accuracy by 11.25%, 5.46%, and 10.39%, respectively; improved the relative limit measurement accuracy by 1.83%, 5.28%, and 12.05%, respectively; and increased the average mean intersection over the union by 7.09%, 14.96%, and 19.14%, respectively.},
DOI = {10.3390/app9112276}
}



@Article{s19112530,
AUTHOR = {Chen, Shichao and Liu, Ming and Lu, Fugang and Xing, Mengdao},
TITLE = {A Target Identification Method for the Millimeter Wave Seeker via Correlation Matching and Beam Pointing},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2530},
URL = {https://www.mdpi.com/1424-8220/19/11/2530},
ISSN = {1424-8220},
ABSTRACT = {Target identification is a challenging task under land backgrounds for the millimeter wave (MMW) seeker, especially under complex backgrounds. Focusing on the problem, an effective method combining correlation matching and beam pointing is proposed in this paper. In the beginning, seeker scanning for target detection is conducted in two rounds, and target information of the detected targets is stored for correlation matching. Point or body feature judgment is implemented by using high resolution range profile (HRRP). Then, the error distribution zone is constructed with the beam pointing as the origin. In the end, we identify the target by searching the one which lies in the closest error distribution from the beam pointing center. The effectiveness of the proposed method is verified by using mooring test-fly and real flight data.},
DOI = {10.3390/s19112530}
}



@Article{rs11111338,
AUTHOR = {Sothe, Camile and Dalponte, Michele and Almeida, Cláudia Maria de and Schimalski, Marcos Benedito and Lima, Carla Luciane and Liesenberg, Veraldo and Miyoshi, Gabriela Takahashi and Tommaselli, Antonio Maria Garcia},
TITLE = {Tree Species Classification in a Highly Diverse Subtropical Forest Integrating UAV-Based Photogrammetric Point Cloud and Hyperspectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1338},
URL = {https://www.mdpi.com/2072-4292/11/11/1338},
ISSN = {2072-4292},
ABSTRACT = {The use of remote sensing data for tree species classification in tropical forests is still a challenging task, due to their high floristic and spectral diversity. In this sense, novel sensors on board of unmanned aerial vehicle (UAV) platforms are a rapidly evolving technology that provides new possibilities for tropical tree species mapping. Besides the acquisition of high spatial and spectral resolution images, UAV-hyperspectral cameras operating in frame format enable to produce 3D hyperspectral point clouds. This study investigated the use of UAV-acquired hyperspectral images and UAV-photogrammetric point cloud (PPC) for classification of 12 major tree species in a subtropical forest fragment in Southern Brazil. Different datasets containing hyperspectral visible/near-infrared (VNIR) bands, PPC features, canopy height model (CHM), and other features extracted from hyperspectral data (i.e., texture, vegetation indices-VIs, and minimum noise fraction-MNF) were tested using a support vector machine (SVM) classifier. The results showed that the use of VNIR hyperspectral bands alone reached an overall accuracy (OA) of 57% (Kappa index of 0.53). Adding PPC features to the VNIR hyperspectral bands increased the OA by 11%. The best result was achieved combining VNIR bands, PPC features, CHM, and VIs (OA of 72.4% and Kappa index of 0.70). When only the CHM was added to VNIR bands, the OA increased by 4.2%. Among the hyperspectral features, besides all the VNIR bands and the two VIs (NDVI and PSSR), the first four MNF features and the textural mean of 565 and 679 nm spectral bands were pointed out as more important to discriminate the tree species according to Jeffries&ndash;Matusita (JM) distance. The SVM method proved to be a good classifier for the tree species recognition task, even in the presence of a high number of classes and a small dataset.},
DOI = {10.3390/rs11111338}
}



@Article{e21060568,
AUTHOR = {Shekaramiz, Mohammad and Moon, Todd K. and Gunther, Jacob H.},
TITLE = {Exploration vs. Data Refinement via Multiple Mobile Sensors},
JOURNAL = {Entropy},
VOLUME = {21},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {568},
URL = {https://www.mdpi.com/1099-4300/21/6/568},
ISSN = {1099-4300},
ABSTRACT = {We examine the deployment of multiple mobile sensors to explore an unknown region to map regions containing concentration of a physical quantity such as heat, electron density, and so on. The exploration trades off between two desiderata: to continue taking data in a region known to contain the quantity of interest with the intent of refining the measurements vs. taking data in unobserved areas to attempt to discover new regions where the quantity may exist. Making reasonable and practical decisions to simultaneously fulfill both goals of exploration and data refinement seem to be hard and contradictory. For this purpose, we propose a general framework that makes value-laden decisions for the trajectory of mobile sensors. The framework employs a Gaussian process regression model to predict the distribution of the physical quantity of interest at unseen locations. Then, the decision-making on the trajectories of sensors is performed using an epistemic utility controller. An example is provided to illustrate the merit and applicability of the proposed framework.},
DOI = {10.3390/e21060568}
}



@Article{s19112596,
AUTHOR = {Jung, Dae-Hyun and Kim, Hak-Jin and Kim, Hyoung Seok and Choi, Jaeyoung and Kim, Jeong Do and Park, Soo Hyun},
TITLE = {Fusion of Spectroscopy and Cobalt Electrochemistry Data for Estimating Phosphate Concentration in Hydroponic Solution},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2596},
URL = {https://www.mdpi.com/1424-8220/19/11/2596},
ISSN = {1424-8220},
ABSTRACT = {Phosphate is a key element affecting plant growth. Therefore, the accurate determination of phosphate concentration in hydroponic nutrient solutions is essential for providing a balanced set of nutrients to plants within a suitable range. This study aimed to develop a data fusion approach for determining phosphate concentrations in a paprika nutrient solution. As a conventional multivariate analysis approach using spectral data, partial least squares regression (PLSR) and principal components regression (PCR) models were developed using 56 samples for calibration and 24 samples for evaluation. The R2 values of estimation models using PCR and PLSR ranged from 0.44 to 0.64. Furthermore, an estimation model using raw electromotive force (EMF) data from cobalt electrodes gave R2 values of 0.58–0.71. To improve the model performance, a data fusion method was developed to estimate phosphate concentration using near infrared (NIR) spectral and cobalt electrochemical data. Raw EMF data from cobalt electrodes and principle component values from the spectral data were combined. Results of calibration and evaluation tests using an artificial neural network estimation model showed that R2 = 0.90 and 0.89 and root mean square error (RMSE) = 96.70 and 119.50 mg/L, respectively. These values are sufficiently high for application to measuring phosphate concentration in hydroponic solutions.},
DOI = {10.3390/s19112596}
}



@Article{rs11111371,
AUTHOR = {Wang, Yanyu and Zhang, Ke and Tang, Chunlan and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
TITLE = {Estimation of Rice Growth Parameters Based on Linear Mixed-Effect Model Using Multispectral Images from Fixed-Wing Unmanned Aerial Vehicles},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1371},
URL = {https://www.mdpi.com/2072-4292/11/11/1371},
ISSN = {2072-4292},
ABSTRACT = {The accurate estimation of aboveground biomass (AGB) and leaf area index (LAI) is critical to characterize crop growth status and predict grain yield. Unmanned aerial vehicle (UAV) -based remote sensing has attracted significant interest due to its high flexibility and easiness of operation. The mixed effect model introduced in this study can capture secondary factors that cannot be captured by standard empirical relationships. The objective of this study was to explore the potential benefit of using a linear mixed-effect (LME) model and multispectral images from a fixed-wing UAV to estimate both AGB and LAI of rice. Field experiments were conducted over two consecutive years (2017&ndash;2018), that involved different N rates, planting patterns and rice cultivars. Images were collected by a compact multispectral camera mounted on a fixed-wing UAV during key rice growth stages. LME, simple regression (SR), artificial neural networks (ANN) and random forests (RF) models were developed relating growth parameters (AGB and LAI) to spectral information. Cultivar (C), growth stage (S) and planting pattern (P) were selected as candidates of random effects for the LME models due to their significant effects on rice growth. Compared to other regression models (SR, ANN and RF), the LME model improved the AGB estimation accuracy for all stage groups to varying degrees: the R2 increased by 0.14&ndash;0.35 and the RMSE decreased by 0.88&ndash;1.80 t ha&minus;1 for the whole season, the R2 increased by 0.07&ndash;0.15 and the RMSE decreased by 0.31&ndash;0.61 t ha&minus;1 for pre-heading stages and the R2 increased by 0.21&ndash;0.53 and the RMSE decreased by 0.72&ndash;1.52 t ha&minus;1 for post-heading stages. Further analysis suggested that the LME model also successfully predicted within the groups when the number of groups was suitable. More importantly, depending on the availability of C, S, P or combinations thereof, mixed effects could lead to an outperformance of baseline retrieval methods (SR, ANN or RF) due to the inclusion of secondary effects. Satisfactory results were also obtained for the LAI estimation while the superiority of the LME model was not as significant as that for AGB estimation. This study demonstrates that the LME model could accurately estimate rice AGB and LAI and fixed-wing UAVs are promising for the monitoring of the crop growth status over large-scale farmland.},
DOI = {10.3390/rs11111371}
}



@Article{rs11111373,
AUTHOR = {Abdulridha, Jaafar and Batuman, Ozgur and Ampatzidis, Yiannis},
TITLE = {UAV-Based Remote Sensing Technique to Detect Citrus Canker Disease Utilizing Hyperspectral Imaging and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1373},
URL = {https://www.mdpi.com/2072-4292/11/11/1373},
ISSN = {2072-4292},
ABSTRACT = {A remote sensing technique was developed to detect citrus canker in laboratory conditions and was verified in the grove by utilizing an unmanned aerial vehicle (UAV). In the laboratory, a hyperspectral (400&ndash;1000 nm) imaging system was utilized for the detection of citrus canker in several disease development stages (i.e., asymptomatic, early, and late symptoms) on Sugar Belle leaves and immature (green) fruit by using two classification methods: (i) radial basis function (RBF) and (ii) K nearest neighbor (KNN). The same imaging system mounted on an UAV was used to detect citrus canker on tree canopies in the orchard. The overall classification accuracy of the RBF was higher (94%, 96%, and 100%) than the KNN method (94%, 95%, and 96%) for detecting canker in leaves. Among the 31 studied vegetation indices, the water index (WI) and the Modified Chlorophyll Absorption in Reflectance Index (ARI and TCARI 1) more accurately detected canker in laboratory and in orchard conditions, respectively. Immature fruit was not a reliable tissue for early detection of canker. However, the proposed technique successfully distinguished the late stage canker-infected fruit with 92% classification accuracy. The UAV-based technique achieved 100% classification accuracy for identifying healthy and canker-infected trees.},
DOI = {10.3390/rs11111373}
}



@Article{rs11111380,
AUTHOR = {Abeysinghe, Tharindu and Simic Milas, Anita and Arend, Kristin and Hohman, Breann and Reil, Patrick and Gregory, Andrew and Vázquez-Ortega, Angélica},
TITLE = {Mapping Invasive Phragmites australis in the Old Woman Creek Estuary Using UAV Remote Sensing and Machine Learning Classifiers},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1380},
URL = {https://www.mdpi.com/2072-4292/11/11/1380},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAV) are increasingly used for spatiotemporal monitoring of invasive plants in coastal wetlands. Early identification of invasive species is necessary in planning, restoring, and managing wetlands. This study assessed the effectiveness of UAV technology to identify invasive Phragmites australis in the Old Woman Creek (OWC) estuary using machine learning (ML) algorithms: Neural network (NN), support vector machine (SVM), and k-nearest neighbor (kNN). The ML algorithms were compared with the parametric maximum likelihood classifier (MLC) using pixel- and object-based methods. Pixel-based NN was identified as the best classifier with an overall accuracy of 94.80% and the lowest error of omission of 1.59%, the outcome desirable for effective eradication of Phragmites. The results were reached combining Sequoia multispectral imagery (green, red, red edge, and near-infrared bands) combined with the canopy height model (CHM) acquired in the mid-growing season and normalized difference vegetation index (NDVI) acquired later in the season. The sensitivity analysis, using various vegetation indices, image texture, CHM, and principal components (PC), demonstrated the impact of various feature layers on the classifiers. The study emphasizes the necessity of a suitable sampling and cross-validation methods, as well as the importance of optimum classification parameters.},
DOI = {10.3390/rs11111380}
}



@Article{rs11121405,
AUTHOR = {Bazine, Razika and Wu, Huayi and Boukhechba, Kamel},
TITLE = {Spatial Filtering in DCT Domain-Based Frameworks for Hyperspectral Imagery Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1405},
URL = {https://www.mdpi.com/2072-4292/11/12/1405},
ISSN = {2072-4292},
ABSTRACT = {In this article, we propose two effective frameworks for hyperspectral imagery classification based on spatial filtering in Discrete Cosine Transform (DCT) domain. In the proposed approaches, spectral DCT is performed on the hyperspectral image to obtain a spectral profile representation, where the most significant information in the transform domain is concentrated in a few low-frequency components. The high-frequency components that generally represent noisy data are further processed using a spatial filter to extract the remaining useful information. For the spatial filtering step, both two-dimensional DCT (2D-DCT) and two-dimensional adaptive Wiener filter (2D-AWF) are explored. After performing the spatial filter, an inverse spectral DCT is applied on all transformed bands including the filtered bands to obtain the final preprocessed hyperspectral data, which is subsequently fed into a linear Support Vector Machine (SVM) classifier. Experimental results using three hyperspectral datasets show that the proposed framework Cascade Spectral DCT Spatial Wiener Filter (CDCT-WF_SVM) outperforms several state-of-the-art methods in terms of classification accuracy, the sensitivity regarding different sizes of the training samples, and computational time.},
DOI = {10.3390/rs11121405}
}



@Article{app9122410,
AUTHOR = {Patel, Maharshi and Jernigan, Shaphan and Richardson, Rob and Ferguson, Scott and Buckner, Gregory},
TITLE = {Autonomous Robotics for Identification and Management of Invasive Aquatic Plant Species},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2410},
URL = {https://www.mdpi.com/2076-3417/9/12/2410},
ISSN = {2076-3417},
ABSTRACT = {Invasive aquatic plant species can expand rapidly throughout water bodies and cause severely adverse economic and ecological impacts. While mechanical, chemical, and biological methods exist for the identification and treatment of these invasive species, they are manually intensive, inefficient, costly, and can cause collateral ecological damage. To address current deficiencies in aquatic weed management, this paper details the development of a small fleet of fully autonomous boats capable of subsurface hydroacoustic imaging (to scan aquatic vegetation), machine learning (for automated weed identification), and herbicide deployment (for vegetation control). These capabilities aim to minimize manual labor and provide more efficient, safe (reduced chemical exposure to personnel), and timely weed management. Geotagged hydroacoustic imagery of three aquatic plant varieties (Hydrilla, Cabomba, and Coontail) was collected and used to create a software pipeline for subsurface aquatic weed classification and distribution mapping. Employing deep learning, the novel software achieved a classification accuracy of 99.06% after training.},
DOI = {10.3390/app9122410}
}



@Article{su11123278,
AUTHOR = {Voutos, Yorghos and Mylonas, Phivos and Katheniotis, John and Sofou, Anastasia},
TITLE = {A Survey on Intelligent Agricultural Information Handling Methodologies},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {3278},
URL = {https://www.mdpi.com/2071-1050/11/12/3278},
ISSN = {2071-1050},
ABSTRACT = {The term intelligent agriculture, or smart farming, typically involves the incorporation of computer science and information technologies into the traditional notion of farming. The latter utilizes plain machinery and equipment used for many decades and the only significant improvement made over the years has been the introduction of automation in the process. Still, at the beginning of the new century, there are ways and room for further vast improvements. More specifically, the low cost of rather advanced sensors and small-scale devices, now even connected to the Internet of Things (IoT), allowed them to be introduced in the process and used within agricultural production systems. New and emerging technologies and methodologies, like the utilization of cheap network storage, are expected to advance this development. In this sense, the main goals of this paper may be summarized as follows: (a) To identify, group, and acknowledge the current state-of-the-art research knowledge about intelligent agriculture approaches, (b) to categorize them according to meaningful data sources categories, and (c) to describe current efficient data processing and utilization aspects from the perspective of the main trends in the field.},
DOI = {10.3390/su11123278}
}



@Article{s19122722,
AUTHOR = {Higa, Kyota and Iwamoto, Kota},
TITLE = {Robust Shelf Monitoring Using Supervised Learning for Improving On-Shelf Availability in Retail Stores},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2722},
URL = {https://www.mdpi.com/1424-8220/19/12/2722},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a method to robustly monitor shelves in retail stores using supervised learning for improving on-shelf availability. To ensure high on-shelf availability, which is a key factor for improving profits in retail stores, we focus on understanding changes in products regarding increases/decreases in product amounts on the shelves. Our method first detects changed regions of products in an image by using background subtraction followed by moving object removal. It then classifies the detected change regions into several classes representing the actual changes on the shelves, such as &ldquo;product taken (decrease)&rdquo; and &ldquo;product replenished/returned (increase)&rdquo;, by supervised learning using convolutional neural networks. It finally updates the shelf condition representing the presence/absence of products using classification results and computes the product amount visible in the image as on-shelf availability using the updated shelf condition. Three experiments were conducted using two videos captured from a surveillance camera on the ceiling in a real store. Results of the first and second experiments show the effectiveness of the product change classification in our method. Results of the third experiment show that our method achieves a success rate of 89.6% for on-shelf availability when an error margin is within one product. With high accuracy, store clerks can maintain high on-shelf availability, enabling retail stores to increase profits.},
DOI = {10.3390/s19122722}
}



@Article{rs11121443,
AUTHOR = {Yao, Huang and Qin, Rongjun and Chen, Xiaoyu},
TITLE = {Unmanned Aerial Vehicle for Remote Sensing Applications—A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1443},
URL = {https://www.mdpi.com/2072-4292/11/12/1443},
ISSN = {2072-4292},
ABSTRACT = {The unmanned aerial vehicle (UAV) sensors and platforms nowadays are being used in almost every application (e.g., agriculture, forestry, and mining) that needs observed information from the top or oblique views. While they intend to be a general remote sensing (RS) tool, the relevant RS data processing and analysis methods are still largely ad-hoc to applications. Although the obvious advantages of UAV data are their high spatial resolution and flexibility in acquisition and sensor integration, there is in general a lack of systematic analysis on how these characteristics alter solutions for typical RS tasks such as land-cover classification, change detection, and thematic mapping. For instance, the ultra-high-resolution data (less than 10 cm of Ground Sampling Distance (GSD)) bring more unwanted classes of objects (e.g., pedestrian and cars) in land-cover classification; the often available 3D data generated from photogrammetric images call for more advanced techniques for geometric and spectral analysis. In this paper, we perform a critical review on RS tasks that involve UAV data and their derived products as their main sources including raw perspective images, digital surface models, and orthophotos. In particular, we focus on solutions that address the &ldquo;new&rdquo; aspects of the UAV data including (1) ultra-high resolution; (2) availability of coherent geometric and spectral data; and (3) capability of simultaneously using multi-sensor data for fusion. Based on these solutions, we provide a brief summary of existing examples of UAV-based RS in agricultural, environmental, urban, and hazards assessment applications, etc., and by discussing their practical potentials, we share our views in their future research directions and draw conclusive remarks.},
DOI = {10.3390/rs11121443}
}



@Article{s19122734,
AUTHOR = {Zou, Xiaojun and Lian, Baowang and Wu, Peng},
TITLE = {Fault Identification Ability of a Robust Deeply Integrated GNSS/INS System Assisted by Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2734},
URL = {https://www.mdpi.com/1424-8220/19/12/2734},
ISSN = {1424-8220},
ABSTRACT = {The problem of fault propagation which exists in the deeply integrated GNSS (Global Navigation Satellite System)/INS (Inertial Navigation System) system makes it difficult to identify faults. Once a fault occurs, system performance will be degraded due to the inability to identify and isolate the fault accurately. After analyzing the causes of fault propagation and the difficulty of fault identification, maintaining correct navigation solution is found to be the key to prevent fault propagation from occurring. In order to solve the problem, a novel robust algorithm based on convolutional neural network (CNN) is proposed. The optimal expansion factor of the robust algorithm is obtained adaptively by utilizing CNN, thus the adverse effect of fault on navigation solution can be reduced as much as possible. At last, the fault identification ability is verified by two types of experiments: artificial fault injection and outdoor occlusion. Experiment results show that the proposed robust algorithm which can successfully suppress the fault propagation is an effective solution. The accuracy of fault identification is increased by more than 20% compared with that before improvement, and the robustness of deep GNSS/INS integration is also improved.},
DOI = {10.3390/s19122734}
}



@Article{robotics8020047,
AUTHOR = {Groves, Keir and West, Andrew and Gornicki, Konrad and Watson, Simon and Carrasco, Joaquin and Lennox, Barry},
TITLE = {MallARD: An Autonomous Aquatic Surface Vehicle for Inspection and Monitoring of Wet Nuclear Storage Facilities},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {47},
URL = {https://www.mdpi.com/2218-6581/8/2/47},
ISSN = {2218-6581},
ABSTRACT = {Inspection and monitoring of wet nuclear storage facilities such as spent fuel pools or wet silos is performed for a variety of reasons, including nuclear security and characterisation of storage facilities prior to decommissioning. Until now such tasks have been performed by personnel or, if the risk to health is too high, avoided. Tasks are often repetitive, time-consuming and potentially dangerous, making them suited to being performed by an autonomous robot. Previous autonomous surface vehicles (ASVs) have been designed for operation in natural outdoor environments and lack the localisation and tracking accuracy necessary for operation in a wet nuclear storage facility. In this paper the environmental and operational conditions are analysed, applicable localisation technologies selected and a unique aquatic autonomous surface vehicle (ASV) is designed and constructed. The ASV developed is holonomic, uses a LiDAR for localisation and features a robust trajectory tracking controller. In a series of experiments the mean error between the present ASV&rsquo;s planned path and the actual path is approximately 1 cm, which is two orders of magnitude lower than previous ASVs. As well as lab testing, the ASV has been used in two deployments, one of which was in an active spent fuel pool.},
DOI = {10.3390/robotics8020047}
}



@Article{s19122775,
AUTHOR = {Munaye, Yirga Yayeh and Lin, Hsin-Piao and Adege, Abebe Belay and Tarekegn, Getaneh Berie},
TITLE = {UAV Positioning for Throughput Maximization Using Deep Learning Approaches},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2775},
URL = {https://www.mdpi.com/1424-8220/19/12/2775},
ISSN = {1424-8220},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) as a communication platform has great practical importance for future wireless networks, especially for on-demand deployment for temporary and emergency conditions. The user throughput estimation in a wireless system depends on the data traffic load and the available capacity to support that load. In UAV-assisted communication, the position of the UAV is one major factor that affects the capacity available to the data flows being served. This study applies multi-layer perceptron (MLP) and long short term memory (LSTM) approaches to determine the position of a UAV that maximizes the overall system performance and user throughput. To analyze and evaluate the system performance, we apply the hybrid of MLP-LSTM for classification regression tasks and K-means algorithms for automatic clustering of classes. The implementation of our work is done through TensorFlow packages. The performance of our proposed system is compared with other approaches to give accurate and novel results for both classification and regression tasks of the user throughput maximization and UAV positioning. According to the results, 98% of the user throughput maximization accuracy is correctly classified. Moreover, the UAV positioning provides accuracy levels of 94.73%, 98.33%, and 99.53% for original datasets (scenario 1), reduced features on the estimated values of user throughput at each grid point (scenario 2), and reduced feature datasets collected on different days and grid points achieved maximum throughput (scenario 3), respectively.},
DOI = {10.3390/s19122775}
}



@Article{rs11121468,
AUTHOR = {Vanbrabant, Yasmin and Tits, Laurent and Delalieux, Stephanie and Pauly, Klaas and Verjans, Wim and Somers, Ben},
TITLE = {Multitemporal Chlorophyll Mapping in Pome Fruit Orchards from Remotely Piloted Aircraft Systems},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1468},
URL = {https://www.mdpi.com/2072-4292/11/12/1468},
ISSN = {2072-4292},
ABSTRACT = {Early and precise spatio-temporal monitoring of tree vitality is key for steering management decisions in pome fruit orchards. Spaceborne remote sensing instruments face a tradeoff between spatial and spectral resolution, while manned aircraft sensor-platform systems are very expensive. In order to address the shortcomings of these platforms, this study investigates the potential of Remotely Piloted Aircraft Systems (RPAS) to facilitate rapid, low cost, and flexible chlorophyll monitoring. Due to the complexity of orchard scenery a robust chlorophyll retrieval model on RPAS level has not yet been developed. In this study, specific focus therefore lies on evaluating the sensitivity of retrieval models to confounding factors. For this study, multispectral and hyperspectral imagery was collected over pome fruit orchards. Sensitivities of both univariate and multivariate retrieval models were demonstrated under different species, phenology, shade, and illumination scenes. Results illustrate that multivariate models have a significantly higher accuracy than univariate models as the former provide accuracies for the canopy chlorophyll content retrieval of R2 = 0.80 and Relative Root Mean Square Error (RRMSE) = 12% for the hyperspectral sensor. Random forest regression on multispectral imagery (R2 &gt; 0.9 for May, June, July, and August, and R2 = 0.5 for October) and hyperspectral imagery (0.6 &lt; R2 &lt; 0.9) led to satisfactory high and consistent accuracies for all months.},
DOI = {10.3390/rs11121468}
}



@Article{s19122823,
AUTHOR = {Stodola, Petr and Drozd, Jan and Nohel, Jan and Hodický, Jan and Procházka, Dalibor},
TITLE = {Trajectory Optimization in a Cooperative Aerial Reconnaissance Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2823},
URL = {https://www.mdpi.com/1424-8220/19/12/2823},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the use of modern technology in military operations has become standard practice. Unmanned systems play an important role in operations such as reconnaissance and surveillance. This article examines a model for planning aerial reconnaissance using a fleet of mutually cooperating unmanned aerial vehicles to increase the effectiveness of the task. The model deploys a number of waypoints such that, when every waypoint is visited by any vehicle in the fleet, the area of interest is fully explored. The deployment of waypoints must meet the conditions arising from the technical parameters of the sensory systems used and tactical requirements of the task at hand. This paper proposes an improvement of the model by optimizing the number and position of waypoints deployed in the area of interest, the effect of which is to improve the trajectories of individual unmanned systems, and thus increase the efficiency of the operation. To achieve this optimization, a modified simulated annealing algorithm is proposed. The improvement of the model is verified by several experiments. Two sets of benchmark problems were designed: (a) benchmark problems for verifying the proposed algorithm for optimizing waypoints, and (b) benchmark problems based on typical reconnaissance scenarios in the real environment to prove the increased effectiveness of the reconnaissance operation. Moreover, an experiment in the SteelBeast simulation system was also conducted.},
DOI = {10.3390/s19122823}
}



@Article{rs11121505,
AUTHOR = {Zhang, Heng and Eziz, Anwar and Xiao, Jian and Tao, Shengli and Wang, Shaopeng and Tang, Zhiyao and Zhu, Jiangling and Fang, Jingyun},
TITLE = {High-Resolution Vegetation Mapping Using eXtreme Gradient Boosting Based on Extensive Features},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1505},
URL = {https://www.mdpi.com/2072-4292/11/12/1505},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of vegetation is a premise for conserving, managing, and sustainably using vegetation resources, especially in conditions of intensive human activities and accelerating global changes. However, it is still challenging to produce high-resolution multiclass vegetation map in high accuracy, due to the incapacity of traditional mapping techniques in distinguishing mosaic vegetation classes with subtle differences and the paucity of fieldwork data. This study created a workflow by adopting a promising classifier, extreme gradient boosting (XGBoost), to produce accurate vegetation maps of two strikingly different cases (the Dzungarian Basin in China and New Zealand) based on extensive features and abundant vegetation data. For the Dzungarian Basin, a vegetation map with seven vegetation types, 17 subtypes, and 43 associations was produced with an overall accuracy of 0.907, 0.801, and 0.748, respectively. For New Zealand, a map of 10 habitats and a map of 41 vegetation classes were produced with 0.946, and 0.703 overall accuracy, respectively. The workflow incorporating simplified field survey procedures outperformed conventional field survey and remote sensing based methods in terms of accuracy and efficiency. In addition, it opens a possibility of building large-scale, high-resolution, and timely vegetation monitoring platforms for most terrestrial ecosystems worldwide with the aid of Google Earth Engine and citizen science programs.},
DOI = {10.3390/rs11121505}
}



@Article{rs11121507,
AUTHOR = {Cardenal, Javier and Fernández, Tomás and Pérez-García, José Luis and Gómez-López, José Miguel},
TITLE = {Measurement of Road Surface Deformation Using Images Captured from UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1507},
URL = {https://www.mdpi.com/2072-4292/11/12/1507},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a methodology for measuring road surface deformation due to terrain instability processes. The methodology is based on ultra-high resolution images acquired from unmanned aerial vehicles (UAVs). Flights are georeferenced by means of Structure from Motion (SfM) techniques. Dense point clouds, obtained using the multiple-view stereo (MVS) approach, are used to generate digital surface models (DSM) and high resolution orthophotographs (0.02 m GSD). The methodology has been applied to an unstable area located in La Guardia (Jaen, Southern Spain), where an active landslide was identified. This landslide affected some roads and accesses to a highway at the landslide foot. The detailed road deformation was monitored between 2012 and 2015 by means of eleven UAV flights of ultrahigh resolution covering an area of about 260 m × 90 m. The accuracy of the analysis has been established in 0.02 ± 0.01 m in XY and 0.04 ± 0.02 m in Z. Large deformations in the order of two meters were registered in the total period analyzed that resulted in maximum average rates of 0.62 m/month in the unstable area. Some boundary conditions were considered because of the low required flying height (&lt;50 m above ground level) in order to achieve a suitable image GSD, the fast landslide dynamic, continuous maintenance works on the affected roads and dramatic seasonal vegetation changes throughout the monitoring period. Finally, we have analyzed the relation of displacements to rainfalls in the area, finding a significant correlation between the two variables, as well as two different reactivation episodes.},
DOI = {10.3390/rs11121507}
}



@Article{electronics8070723,
AUTHOR = {Qiao, Dalei and Liu, Guangzhong and Zhang, Jun and Zhang, Qiangyong and Wu, Gongxing and Dong, Feng},
TITLE = {M3C: Multimodel-and-Multicue-Based Tracking by Detection of Surrounding Vessels in Maritime Environment for USV},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {723},
URL = {https://www.mdpi.com/2079-9292/8/7/723},
ISSN = {2079-9292},
ABSTRACT = {It is crucial for unmanned surface vessels (USVs) to detect and track surrounding vessels in real time to avoid collisions at sea. However, the harsh maritime environment poses great challenges to multitarget tracking (MTT). In this paper, a novel tracking by detection framework that integrates the multimodel and multicue (M3C) pipeline is proposed, which aims at improving the detection and tracking performance. Regarding the multimodel, we predicted the maneuver probability of a target vessel via the gated recurrent unit (GRU) model with an attention mechanism, and fused their respective outputs as the output of a kinematic filter. We developed a hybrid affinity model based on multi cues, such as the motion, appearance, and attitude of the ego vessel in the data association stage. By using the proposed ship re-identification approach, the tracker had the capability of appearance matching via metric learning. Experimental evaluation of two public maritime datasets showed that our method achieved state-of-the-art performance, not only in identity switches (IDS) but also in frame rates.},
DOI = {10.3390/electronics8070723}
}



@Article{app9132621,
AUTHOR = {Shao, Zhuang and Yan, Fei and Zhou, Zhou and Zhu, Xiaoping},
TITLE = {Path Planning for Multi-UAV Formation Rendezvous Based on Distributed Cooperative Particle Swarm Optimization},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2621},
URL = {https://www.mdpi.com/2076-3417/9/13/2621},
ISSN = {2076-3417},
ABSTRACT = {This paper studies the problem of generating cooperative feasible paths for formation rendezvous of unmanned aerial vehicles (UAVs). Cooperative path-planning for multi-UAV formation rendezvous is mostly a complicated multi-objective optimization problem with many coupled constraints. In order to satisfy the kinematic constraints, i.e., the maximum curvature constraint and the requirement of continuous curvature of the UAV path, the Pythagorean hodograph (PH) curve is adopted as the parameterized path because of its curvature continuity and rational intrinsic properties. Inspired by the co-evolutionary theory, a distributed cooperative particle swarm optimization (DCPSO) algorithm with an elite keeping strategy is proposed to generate a flyable and safe path for each UAV. This proposed algorithm can meet the kinematic constraints of UAVs and the cooperation requirements among UAVs. Meanwhile, the optimal or sub-optimal paths can be obtained. Finally, numerical simulations in 2-D and 3-D environments are conducted to demonstrate the feasibility and stability of the proposed algorithm. Simulation results show that the paths generated by the proposed DCPSO can not only meet the kinematic constraints of UAVs and safety requirements, but also achieve the simultaneous arrival and collision avoidance between UAVs for formation rendezvous. Compared with the cooperative co-evolutionary genetic algorithm (CCGA), the proposed DCPSO has better stability and a higher searching success rate.},
DOI = {10.3390/app9132621}
}



@Article{rs11131550,
AUTHOR = {Koch, Tobias and Körner, Marco and Fraundorfer, Friedrich},
TITLE = {Automatic and Semantically-Aware 3D UAV Flight Planning for Image-Based 3D Reconstruction},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1550},
URL = {https://www.mdpi.com/2072-4292/11/13/1550},
ISSN = {2072-4292},
ABSTRACT = {Small-scaled unmanned aerial vehicles (UAVs) emerge as ideal image acquisition platforms due to their high maneuverability even in complex and tightly built environments. The acquired images can be utilized to generate high-quality 3D models using current multi-view stereo approaches. However, the quality of the resulting 3D model highly depends on the preceding flight plan which still requires human expert knowledge, especially in complex urban and hazardous environments. In terms of safe flight plans, practical considerations often define prohibited and restricted airspaces to be accessed with the vehicle. We propose a 3D UAV path planning framework designed for detailed and complete small-scaled 3D reconstructions considering the semantic properties of the environment allowing for user-specified restrictions on the airspace. The generated trajectories account for the desired model resolution and the demands on a successful photogrammetric reconstruction. We exploit semantics from an initial flight to extract the target object and to define restricted and prohibited airspaces which have to be avoided during the path planning process to ensure a safe and short UAV path, while still aiming to maximize the object reconstruction quality. The path planning problem is formulated as an orienteering problem and solved via discrete optimization exploiting submodularity and photogrammetrical relevant heuristics. An evaluation of our method on a customized synthetic scene and on outdoor experiments suggests the real-world capability of our methodology by providing feasible, short and safe flight plans for the generation of detailed 3D reconstruction models.},
DOI = {10.3390/rs11131550}
}



@Article{app9132656,
AUTHOR = {Zhang, Hanxue and Shen, Chong and Chen, Xuemei and Cao, Huiliang and Zhao, Donghua and Huang, Haoqian and Guo, Xiaoting},
TITLE = {An Enhanced Fusion Strategy for Reliable Attitude Measurement Utilizing Vision and Inertial Sensors},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2656},
URL = {https://www.mdpi.com/2076-3417/9/13/2656},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we present a radial basis function (RBF) and cubature Kalman filter (CKF) based enhanced fusion strategy for vision and inertial integrated attitude measurement for sampling frequency discrepancy and divergence. First, the multi-frequency problem of the integrated system and the reason for attitude divergence are analyzed. Second, the filter equation and attitude differential equation are constructed to calculate attitudes separately in time series when visual and inertial data are available or when there are only inertial data. Third, attitude errors between inertial and vision are sent to the input layer of RBF for training. After this, through the activation function of the hidden layer, the errors are transferred to the output layer for weighting the sums, and the training model is established. To overcome the problem of divergence inherent in a multi-frequency system, the well-trained RBF, which can output the attitude errors, is utilized to compensate the attitudes calculated by pure inertial data. Finally, semi-physical simulation experiments under different scenarios are performed to validate the effectiveness and superiority of the proposed scheme in accurate attitude measurements and enhanced anti-divergence capability.},
DOI = {10.3390/app9132656}
}



@Article{act8030053,
AUTHOR = {Kidd, Robert},
TITLE = {Artificial Immune Systems: An Overview for Faulting Actuators},
JOURNAL = {Actuators},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {53},
URL = {https://www.mdpi.com/2076-0825/8/3/53},
ISSN = {2076-0825},
ABSTRACT = {This paper reviews Artificial Immune Systems (AIS) that can be implemented to compensate for actuators that are in a faulted state or operating abnormally. Eventually, all actuators will fail or wear out, and these actuator faults must be managed if a system is to operate safely. The AIS are adaptive algorithms which are inherently well-suited to these situations by treating these faults as infections that must be combated. However, the computational intensity of these algorithms has caused them to have limited success in real-time situations. With the advent of distributed and cloud-based computing these algorithms have begun to be feasible for diagnosing faulted actuators and then generating compensating controllers in near-real-time. To encourage the application of AIS to these situations, this work presents research for the fundamental operating principles of AIS, their applications, and a brief case-study on their applicability to fault compensation by considering an overactuated rover with four independent drive wheels and independent front and rear steering.},
DOI = {10.3390/act8030053}
}



@Article{rs11131554,
AUTHOR = {Zhang, Xin and Han, Liangxiu and Dong, Yingying and Shi, Yue and Huang, Wenjiang and Han, Lianghao and González-Moreno, Pablo and Ma, Huiqin and Ye, Huichun and Sobeih, Tam},
TITLE = {A Deep Learning-Based Approach for Automated Yellow Rust Disease Detection from High-Resolution Hyperspectral UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1554},
URL = {https://www.mdpi.com/2072-4292/11/13/1554},
ISSN = {2072-4292},
ABSTRACT = {Yellow rust in winter wheat is a widespread and serious fungal disease, resulting in significant yield losses globally. Effective monitoring and accurate detection of yellow rust are crucial to ensure stable and reliable wheat production and food security. The existing standard methods often rely on manual inspection of disease symptoms in a small crop area by agronomists or trained surveyors. This is costly, time consuming and prone to error due to the subjectivity of surveyors. Recent advances in unmanned aerial vehicles (UAVs) mounted with hyperspectral image sensors have the potential to address these issues with low cost and high efficiency. This work proposed a new deep convolutional neural network (DCNN) based approach for automated crop disease detection using very high spatial resolution hyperspectral images captured with UAVs. The proposed model introduced multiple Inception-Resnet layers for feature extraction and was optimized to establish the most suitable depth and width of the network. Benefiting from the ability of convolution layers to handle three-dimensional data, the model used both spatial and spectral information for yellow rust detection. The model was calibrated with hyperspectral imagery collected by UAVs in five different dates across a whole crop cycle over a well-controlled field experiment with healthy and rust infected wheat plots. Its performance was compared across sampling dates and with random forest, a representative of traditional classification methods in which only spectral information was used. It was found that the method has high performance across all the growing cycle, particularly at late stages of the disease spread. The overall accuracy of the proposed model (0.85) was higher than that of the random forest classifier (0.77). These results showed that combining both spectral and spatial information is a suitable approach to improving the accuracy of crop disease detection with high resolution UAV hyperspectral images.},
DOI = {10.3390/rs11131554}
}



@Article{su11133637,
AUTHOR = {Lee, SangSik and Jeong, YiNa and Son, SuRak and Lee, ByungKwan},
TITLE = {A Self-Predictable Crop Yield Platform (SCYP) Based On Crop Diseases Using Deep Learning},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3637},
URL = {https://www.mdpi.com/2071-1050/11/13/3637},
ISSN = {2071-1050},
ABSTRACT = {This paper proposes a self-predictable crop yield platform (SCYP) based on crop diseases using deep learning that collects weather information (temperature, humidity, sunshine, precipitation, etc.) and farm status information (harvest date, disease information, crop status, ground temperature, etc.), diagnoses crop diseases by using convolutional neural network (CNN), and predicts crop yield based on factors such as climate change, crop diseases, and others by using artificial neural network (ANN). The SCYP consists of an image preprocessing module (IPM) to determine crop diseases through the Google Vision API and image resizing, a crop disease diagnosis module (CDDM) based on CNN to diagnose the types and extent of crop diseases through photographs, and a crop yield prediction module (CYPM) based on ANN by using information of crop diseases, remaining time until harvest (based on the date), current temperature, humidity and precipitation (amount of snowfall) in the area, sunshine amount, ground temperature, atmospheric pressure, moisture evaporation in the ground, etc. Four experiments were conducted to verify the efficiency of the SCYP. In the CDMM, the accuracy and operation time of each model were measured using three neural network models: CNN, region-CNN(R-CNN), and you only look once (YOLO). In the CYPM, rectified linear unit (ReLU), Sigmoid, and Step activation functions were compared to measure ANN accuracy. The accuracy of CNN was about 3.5% higher than that of R-CNN and about 5.4% higher than that of YOLO. The operation time of CNN was about 37 s less than that of R-CNN and about 72 s less than that of YOLO. The CDDM had slightly less operation time, but in this paper, we prefer accuracy over operation time to diagnose crop diseases efficiently and accurately. When the activation function of the ANN used in the CYPM was ReLU, the accuracy of the ANN was 2% higher than that of Sigmoid and 7% higher than that of Step. The CYPM prediction was about 34% more accurate when using multiple diseases than when not using them. Therefore, the SCYP can predict farm yields more accurately than traditional methods.},
DOI = {10.3390/su11133637}
}



@Article{agronomy9070354,
AUTHOR = {Zhang, Xiuhua and Derival, Magda and Albrecht, Ute and Ampatzidis, Yiannis},
TITLE = {Evaluation of a Ground Penetrating Radar to Map the Root Architecture of HLB-Infected Citrus Trees},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {354},
URL = {https://www.mdpi.com/2073-4395/9/7/354},
ISSN = {2073-4395},
ABSTRACT = {This paper investigates the influences of several limiting factors on the performance of ground penetrating radar (GPR) in accurately detecting huanglongbing (HLB)-infected citrus roots and determining their main structural characteristics. First, single-factor experiments were conducted to evaluate GPR performance. The factors that were evaluated were (i) root diameter; (ii) root moisture level; (iii) root depth; (iv) root spacing; (v) survey angle; and, (vi) soil moisture level. Second, two multi-factor field experiments were conducted to evaluate the performance of the GPR in complex orchard environments. The GPR generated a hyperbola in the radar profile upon root detection; the diameter of the root was successfully determined according to the width of the hyperbola when the roots were larger than 6 mm in diameter. The GPR also distinguished live from dead roots, a capability that is indispensable for studying the effects of soil-borne and other diseases on the citrus tree root system. The GPR can distinguish the roots only if their horizontal distance is greater than 10 cm and their vertical distance is greater than 5 cm if two or more roots are in proximity. GPR technology can be applied to determine the efficacy of advanced crop production strategies, especially under the pressures of disease and environmental stresses.},
DOI = {10.3390/agronomy9070354}
}



@Article{rs11131584,
AUTHOR = {Chen, Yang and Lee, Won Suk and Gan, Hao and Peres, Natalia and Fraisse, Clyde and Zhang, Yanchao and He, Yong},
TITLE = {Strawberry Yield Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoimages},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1584},
URL = {https://www.mdpi.com/2072-4292/11/13/1584},
ISSN = {2072-4292},
ABSTRACT = {Strawberry growers in Florida suffer from a lack of efficient and accurate yield forecasts for strawberries, which would allow them to allocate optimal labor and equipment, as well as other resources for harvesting, transportation, and marketing. Accurate estimation of the number of strawberry flowers and their distribution in a strawberry field is, therefore, imperative for predicting the coming strawberry yield. Usually, the number of flowers and their distribution are estimated manually, which is time-consuming, labor-intensive, and subjective. In this paper, we develop an automatic strawberry flower detection system for yield prediction with minimal labor and time costs. The system used a small unmanned aerial vehicle (UAV) (DJI Technology Co., Ltd., Shenzhen, China) equipped with an RGB (red, green, blue) camera to capture near-ground images of two varieties (Sensation and Radiance) at two different heights (2 m and 3 m) and built orthoimages of a 402 m2 strawberry field. The orthoimages were automatically processed using the Pix4D software and split into sequential pieces for deep learning detection. A faster region-based convolutional neural network (R-CNN), a state-of-the-art deep neural network model, was chosen for the detection and counting of the number of flowers, mature strawberries, and immature strawberries. The mean average precision (mAP) was 0.83 for all detected objects at 2 m heights and 0.72 for all detected objects at 3 m heights. We adopted this model to count strawberry flowers in November and December from 2 m aerial images and compared the results with a manual count. The average deep learning counting accuracy was 84.1% with average occlusion of 13.5%. Using this system could provide accurate counts of strawberry flowers, which can be used to forecast future yields and build distribution maps to help farmers observe the growth cycle of strawberry fields.},
DOI = {10.3390/rs11131584}
}



@Article{su11133678,
AUTHOR = {Roșca, Sanda and Șimonca, Vasile and Bilașco, Ștefan and Vescan, Iuliu and Fodorean, Ioan and Petrea, Dănuț},
TITLE = {The Assessment of Favourability and Spatio-Temporal Dynamics of Pinus Mugo in the Romanian Carpathians Using GIS Technology and Landsat Images},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3678},
URL = {https://www.mdpi.com/2071-1050/11/13/3678},
ISSN = {2071-1050},
ABSTRACT = {Favourability classification for forest species represents a fundamental activity for deriving technological solutions in forestry, as specialists need detailed information about the ecological requirements of forest species from environmental factors: climate, pedological characteristics and morphometric characteristics of the study area. The purpose of the present study was the use of the qualitative data extracted from the ecological records of the Pinus mugo species and the generation of a complex geospatial database for the entire territory of Romania. The results were represented by a collection of thematic maps generated on favourability classes for the Romanian Carpathians, as well as for the major landform subunits which had been the basis for the statistical analysis of the results. The validation of the results was performed by comparing the results obtained through the application of the model which used the frequency points reported in the European Atlas of the Forest Tree Species from Europe, 2016. In order to identify the spatio-temporal dynamics, LANDSAT satellite images from 30 years were used, which enabled the identification of the expansion and the reduction in size of the Pinus mugo area at a zonal level, a process which is dependent on natural factors, like climatic variations, or anthropic factors (overgrazing or works of cleaning the montain pastures).},
DOI = {10.3390/su11133678}
}



@Article{s19132955,
AUTHOR = {Fakhrulddin, Saif Saad and Gharghan, Sadik Kamel and Al-Naji, Ali and Chahl, Javaan},
TITLE = {An Advanced First Aid System Based on an Unmanned Aerial Vehicles and a Wireless Body Area Sensor Network for Elderly Persons in Outdoor Environments},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2955},
URL = {https://www.mdpi.com/1424-8220/19/13/2955},
ISSN = {1424-8220},
ABSTRACT = {For elderly persons, a fall can cause serious injuries such as a hip fracture or head injury. Here, an advanced first aid system is proposed for monitoring elderly patients with heart conditions that puts them at risk of falling and for providing first aid supplies using an unmanned aerial vehicle. A hybridized fall detection algorithm (FDB-HRT) is proposed based on a combination of acceleration and a heart rate threshold. Five volunteers were invited to evaluate the performance of the heartbeat sensor relative to a benchmark device, and the extracted data was validated using statistical analysis. In addition, the accuracy of fall detections and the recorded locations of fall incidents were validated. The proposed FDB-HRT algorithm was 99.16% and 99.2% accurate with regard to heart rate measurement and fall detection, respectively. In addition, the geolocation error of patient fall incidents based on a GPS module was evaluated by mean absolute error analysis for 17 different locations in three cities in Iraq. Mean absolute error was 1.08 &times; 10&minus;5&deg; and 2.01 &times; 10&minus;5&deg; for latitude and longitude data relative to data from the GPS Benchmark system. In addition, the results revealed that in urban areas, the UAV succeeded in all missions and arrived at the patient&rsquo;s locations before the ambulance, with an average time savings of 105 s. Moreover, a time saving of 31.81% was achieved when using the UAV to transport a first aid kit to the patient compared to an ambulance. As a result, we can conclude that when compared to delivering first aid via ambulance, our design greatly reduces delivery time. The proposed advanced first aid system outperformed previous systems presented in the literature in terms of accuracy of heart rate measurement, fall detection, and information messages and UAV arrival time.},
DOI = {10.3390/s19132955}
}



@Article{rs11131594,
AUTHOR = {Qiu, Heqian and Li, Hongliang and Wu, Qingbo and Meng, Fanman and Ngan, King Ngi and Shi, Hengcan},
TITLE = {A2RMNet: Adaptively Aspect Ratio Multi-Scale Network for Object Detection in Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1594},
URL = {https://www.mdpi.com/2072-4292/11/13/1594},
ISSN = {2072-4292},
ABSTRACT = {Object detection is a significant and challenging problem in the study area of remote sensing and image analysis. However, most existing methods are easy to miss or incorrectly locate objects due to the various sizes and aspect ratios of objects. In this paper, we propose a novel end-to-end Adaptively Aspect Ratio Multi-Scale Network (A     2    RMNet) to solve this problem. On the one hand, we design a multi-scale feature gate fusion network to adaptively integrate the multi-scale features of objects. This network is composed of gate fusion modules, refine blocks and region proposal networks. On the other hand, an aspect ratio attention network is leveraged to preserve the aspect ratios of objects, which alleviates the excessive shape distortions of objects caused by aspect ratio changes during training. Experiments show that the proposed A     2    RMNet significantly outperforms the previous state of the arts on the DOTA dataset, NWPU VHR-10 dataset, RSOD dataset and UCAS-AOD dataset by     5.73 %    ,     7.06 %    ,     3.27 %     and     2.24 %    , respectively.},
DOI = {10.3390/rs11131594}
}



@Article{s19132965,
AUTHOR = {Contreras-Cruz, Marco Antonio and Ramirez-Paredes, Juan Pablo and Hernandez-Belmonte, Uriel Haile and Ayala-Ramirez, Victor},
TITLE = {Vision-Based Novelty Detection Using Deep Features and Evolved Novelty Filters for Specific Robotic Exploration and Inspection Tasks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2965},
URL = {https://www.mdpi.com/1424-8220/19/13/2965},
ISSN = {1424-8220},
ABSTRACT = {One of the essential abilities in animals is to detect novelties within their environment. From the computational point of view, novelty detection consists of finding data that are different in some aspect to the known data. In robotics, researchers have incorporated novelty modules in robots to develop automatic exploration and inspection tasks. The visual sensor is one of the preferred sensors to perform this task. However, there exist problems as illumination changes, occlusion, and scale, among others. Besides, novelty detectors vary their performance depending on the specific application scenario. In this work, we propose a visual novelty detection framework for specific exploration and inspection tasks based on evolved novelty detectors. The system uses deep features to represent the visual information captured by the robots and applies a global optimization technique to design novelty detectors for specific robotics applications. We verified the performance of the proposed system against well-established state-of-the-art methods in a challenging scenario. This scenario was an outdoor environment covering typical problems in computer vision such as illumination changes, occlusion, and geometric transformations. The proposed framework presented high-novelty detection accuracy with competitive or even better results than the baseline methods.},
DOI = {10.3390/s19132965}
}



@Article{rs11131617,
AUTHOR = {Wang, Jicheng and Shen, Li and Qiao, Wenfan and Dai, Yanshuai and Li, Zhilin},
TITLE = {Deep Feature Fusion with Integration of Residual Connection and Attention Model for Classification of VHR Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1617},
URL = {https://www.mdpi.com/2072-4292/11/13/1617},
ISSN = {2072-4292},
ABSTRACT = {The classification of very-high-resolution (VHR) remote sensing images is essential in many applications. However, high intraclass and low interclass variations in these kinds of images pose serious challenges. Fully convolutional network (FCN) models, which benefit from a powerful feature learning ability, have shown impressive performance and great potential. Nevertheless, only classification results with coarse resolution can be obtained from the original FCN method. Deep feature fusion is often employed to improve the resolution of outputs. Existing strategies for such fusion are not capable of properly utilizing the low-level features and considering the importance of features at different scales. This paper proposes a novel, end-to-end, fully convolutional network to integrate a multiconnection ResNet model and a class-specific attention model into a unified framework to overcome these problems. The former fuses multilevel deep features without introducing any redundant information from low-level features. The latter can learn the contributions from different features of each geo-object at each scale. Extensive experiments on two open datasets indicate that the proposed method can achieve class-specific scale-adaptive classification results and it outperforms other state-of-the-art methods. The results were submitted to the International Society for Photogrammetry and Remote Sensing (ISPRS) online contest for comparison with more than 50 other methods. The results indicate that the proposed method (ID: SWJ_2) ranks #1 in terms of overall accuracy, even though no additional digital surface model (DSM) data that were offered by ISPRS were used and no postprocessing was applied.},
DOI = {10.3390/rs11131617}
}



@Article{s19133014,
AUTHOR = {Jalil, Bushra and Leone, Giuseppe Riccardo and Martinelli, Massimo and Moroni, Davide and Pascali, Maria Antonietta and Berton, Andrea},
TITLE = {Fault Detection in Power Equipment via an Unmanned Aerial System Using Multi Modal Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3014},
URL = {https://www.mdpi.com/1424-8220/19/13/3014},
ISSN = {1424-8220},
ABSTRACT = {The power transmission lines are the link between power plants and the points of consumption, through substations. Most importantly, the assessment of damaged aerial power lines and rusted conductors is of extreme importance for public safety; hence, power lines and associated components must be periodically inspected to ensure a continuous supply and to identify any fault and defect. To achieve these objectives, recently, Unmanned Aerial Vehicles (UAVs) have been widely used; in fact, they provide a safe way to bring sensors close to the power transmission lines and their associated components without halting the equipment during the inspection, and reducing operational cost and risk. In this work, a drone, equipped with multi-modal sensors, captures images in the visible and infrared domain and transmits them to the ground station. We used state-of-the-art computer vision methods to highlight expected faults (i.e., hot spots) or damaged components of the electrical infrastructure (i.e., damaged insulators). Infrared imaging, which is invariant to large scale and illumination changes in the real operating environment, supported the identification of faults in power transmission lines; while a neural network is adapted and trained to detect and classify insulators from an optical video stream. We demonstrate our approach on data captured by a drone in Parma, Italy.},
DOI = {10.3390/s19133014}
}



@Article{rs11141636,
AUTHOR = {Lai, Xudong and Yang, Jingru and Li, Yongxu and Wang, Mingwei},
TITLE = {A Building Extraction Approach Based on the Fusion of LiDAR Point Cloud and Elevation Map Texture Features},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1636},
URL = {https://www.mdpi.com/2072-4292/11/14/1636},
ISSN = {2072-4292},
ABSTRACT = {Building extraction is an important way to obtain information in urban planning, land management, and other fields. As remote sensing has various advantages such as large coverage and real-time capability, it becomes an essential approach for building extraction. Among various remote sensing technologies, the capability of providing 3D features makes the LiDAR point cloud become a crucial means for building extraction. However, the LiDAR point cloud has difficulty distinguishing objects with similar heights, in which case texture features are able to extract different objects in a 2D image. In this paper, a building extraction method based on the fusion of point cloud and texture features is proposed, and the texture features are extracted by using an elevation map that expresses the height of each point. The experimental results show that the proposed method obtains better extraction results than that of other texture feature extraction methods and ENVI software in all experimental areas, and the extraction accuracy is always higher than 87%, which is satisfactory for some practical work.},
DOI = {10.3390/rs11141636}
}



@Article{s19143096,
AUTHOR = {Xin, Junfeng and Li, Shixin and Sheng, Jinlu and Zhang, Yongbo and Cui, Ying},
TITLE = {Application of Improved Particle Swarm Optimization for Navigation of Unmanned Surface Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3096},
URL = {https://www.mdpi.com/1424-8220/19/14/3096},
ISSN = {1424-8220},
ABSTRACT = {Multi-sensor fusion for unmanned surface vehicles (USVs) is an important issue for autonomous navigation of USVs. In this paper, an improved particle swarm optimization (PSO) is proposed for real-time autonomous navigation of a USV in real maritime environment. To overcome the conventional PSO&rsquo;s inherent shortcomings, such as easy occurrence of premature convergence and human experience-determined parameters, and to enhance the precision and algorithm robustness of the solution, this work proposes three optimization strategies: linearly descending inertia weight, adaptively controlled acceleration coefficients, and random grouping inversion. Their respective or combinational effects on the effectiveness of path planning are investigated by Monte Carlo simulations for five TSPLIB instances and application tests for the navigation of a self-developed unmanned surface vehicle on the basis of multi-sensor data. Comparative results show that the adaptively controlled acceleration coefficients play a substantial role in reducing the path length and the linearly descending inertia weight help improve the algorithm robustness. Meanwhile, the random grouping inversion optimizes the capacity of local search and maintains the population diversity by stochastically dividing the single swarm into several subgroups. Moreover, the PSO combined with all three strategies shows the best performance with the shortest trajectory and the superior robustness, although retaining solution precision and avoiding being trapped in local optima require more time consumption. The experimental results of our USV demonstrate the effectiveness and efficiency of the proposed method for real-time navigation based on multi-sensor fusion.},
DOI = {10.3390/s19143096}
}



@Article{app9142808,
AUTHOR = {Peng, Yahui and Liu, Xiaochen and Shen, Chong and Huang, Haoqian and Zhao, Donghua and Cao, Huiliang and Guo, Xiaoting},
TITLE = {An Improved Optical Flow Algorithm Based on Mask-R-CNN and K-Means for Velocity Calculation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2808},
URL = {https://www.mdpi.com/2076-3417/9/14/2808},
ISSN = {2076-3417},
ABSTRACT = {Aiming at enhancing the accuracy and reliability of velocity calculation in vision navigation, an improved method is proposed in this paper. The method integrates Mask-R-CNN (Mask Region-based Convolutional Neural Network) and K-Means with the pyramid Lucas Kanade algorithm in order to reduce the harmful effect of moving objects on velocity calculation. Firstly, Mask-R-CNN is used to recognize the objects which have motions relative to the ground and covers them with masks to enhance the similarity between pixels and to reduce the impacts of the noisy moving pixels. Then, the pyramid Lucas Kanade algorithm is used to calculate the optical flow value. Finally, the value is clustered by the K-Means algorithm to abandon the outliers, and vehicle velocity is calculated by the processed optical flow. The prominent advantages of the proposed algorithm are (i) decreasing the bad impacts to velocity calculation, due to the objects which have relative motions; (ii) obtaining the correct optical flow sets and velocity calculation outputs with less fluctuation; and (iii) the applicability enhancement of the optical flow algorithm in complex navigation environment. The proposed algorithm is tested by actual experiments. Results with superior precision and reliability show the feasibility and effectiveness of the proposed method for vehicle velocity calculation in vision navigation system.},
DOI = {10.3390/app9142808}
}



@Article{s19143106,
AUTHOR = {Zhou, Chengquan and Ye, Hongbao and Hu, Jun and Shi, Xiaoyan and Hua, Shan and Yue, Jibo and Xu, Zhifu and Yang, Guijun},
TITLE = {Automated Counting of Rice Panicle by Applying Deep Learning Model to Images from Unmanned Aerial Vehicle Platform},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3106},
URL = {https://www.mdpi.com/1424-8220/19/14/3106},
ISSN = {1424-8220},
ABSTRACT = {The number of panicles per unit area is a common indicator of rice yield and is of great significance to yield estimation, breeding, and phenotype analysis. Traditional counting methods have various drawbacks, such as long delay times and high subjectivity, and they are easily perturbed by noise. To improve the accuracy of rice detection and counting in the field, we developed and implemented a panicle detection and counting system that is based on improved region-based fully convolutional networks, and we use the system to automate rice-phenotype measurements. The field experiments were conducted in target areas to train and test the system and used a rotor light unmanned aerial vehicle equipped with a high-definition RGB camera to collect images. The trained model achieved a precision of 0.868 on a held-out test set, which demonstrates the feasibility of this approach. The algorithm can deal with the irregular edge of the rice panicle, the significantly different appearance between the different varieties and growing periods, the interference due to color overlapping between panicle and leaves, and the variations in illumination intensity and shading effects in the field. The result is more accurate and efficient recognition of rice-panicles, which facilitates rice breeding. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a global scale.},
DOI = {10.3390/s19143106}
}



@Article{rs11141678,
AUTHOR = {Fu, Yongyong and Ye, Ziran and Deng, Jinsong and Zheng, Xinyu and Huang, Yibo and Yang, Wu and Wang, Yaohua and Wang, Ke},
TITLE = {Finer Resolution Mapping of Marine Aquaculture Areas Using WorldView-2 Imagery and a Hierarchical Cascade Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1678},
URL = {https://www.mdpi.com/2072-4292/11/14/1678},
ISSN = {2072-4292},
ABSTRACT = {Marine aquaculture plays an important role in seafood supplement, economic development, and coastal ecosystem service provision. The precise delineation of marine aquaculture areas from high spatial resolution (HSR) imagery is vital for the sustainable development and management of coastal marine resources. However, various sizes and detailed structures of marine objects make it difficult for accurate mapping from HSR images by using conventional methods. Therefore, this study attempts to extract marine aquaculture areas by using an automatic labeling method based on the convolutional neural network (CNN), i.e., an end-to-end hierarchical cascade network (HCNet). Specifically, for marine objects of various sizes, we propose to improve the classification performance by utilizing multi-scale contextual information. Technically, based on the output of a CNN encoder, we employ atrous convolutions to capture multi-scale contextual information and aggregate them in a hierarchical cascade way. Meanwhile, for marine objects with detailed structures, we propose to refine the detailed information gradually by using a series of long-span connections with fine resolution features from the shallow layers. In addition, to decrease the semantic gaps between features in different levels, we propose to refine the feature space (i.e., channel and spatial dimensions) using an attention-based module. Experimental results show that our proposed HCNet can effectively identify and distinguish different kinds of marine aquaculture, with 98% of overall accuracy. It also achieves better classification performance compared with object-based support vector machine and state-of-the-art CNN-based methods, such as FCN-32s, U-Net, and DeeplabV2. Our developed method lays a solid foundation for the intelligent monitoring and management of coastal marine resources.},
DOI = {10.3390/rs11141678}
}



@Article{make1030046,
AUTHOR = {Manzo, Mario},
TITLE = {Graph-Based Image Matching for Indoor Localization},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {1},
YEAR = {2019},
NUMBER = {3},
PAGES = {785--804},
URL = {https://www.mdpi.com/2504-4990/1/3/46},
ISSN = {2504-4990},
ABSTRACT = {Graphs are a very useful framework for representing information. In general, these data structures are used in different application domains where data of interest are described in terms of local and spatial relations. In this context, the aim is to propose an alternative graph-based image representation. An image is encoded by a Region Adjacency Graph (RAG), based on Multicolored Neighborhood (MCN) clustering. This representation is integrated into a Content-Based Image Retrieval (CBIR) system, designed for the vision-based positioning task. The image matching phase, in the CBIR system, is managed with an approach of attributed graph matching, named the extended-VF algorithm. Evaluated in a context of indoor localization, the proposed system reports remarkable performance.},
DOI = {10.3390/make1030046}
}



@Article{en12142706,
AUTHOR = {Ejaz, Waleed and Azam, Muhammad Awais and Saadat, Salman and Iqbal, Farkhund and Hanan, Abdul},
TITLE = {Unmanned Aerial Vehicles enabled IoT Platform for Disaster Management},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2706},
URL = {https://www.mdpi.com/1996-1073/12/14/2706},
ISSN = {1996-1073},
ABSTRACT = {Efficient and reliable systems are required to detect and monitor disasters such as wildfires as well as to notify the people in the disaster-affected areas. Internet of Things (IoT) is the key paradigm that can address the multitude problems related to disaster management. In addition, an unmanned aerial vehicles (UAVs)-enabled IoT platform connected via cellular network can further enhance the robustness of the disaster management system. The UAV-enabled IoT platform is based on three main research areas: (i) ground IoT network; (ii) communication technologies for ground and aerial connectivity; and (iii) data analytics. In this paper, we provide a holistic view of a UAVs-enabled IoT platform which can provide ubiquitous connectivity to both aerial and ground users in challenging environments such as wildfire management. We then highlight key challenges for the design of an efficient and reliable IoT platform. We detail a case study targeting the design of an efficient ground IoT network that can detect and monitor fire and send notifications to people using named data networking (NDN) architecture. The use of NDN architecture in a sensor network for IoT integrates pull-based communication to enable reliable and efficient message dissemination in the network and to notify the users as soon as possible in case of disastrous situations. The results of the case study show the enormous impact on the performance of IoT platform for wildfire management. Lastly, we draw the conclusion and outline future research directions in this field.},
DOI = {10.3390/en12142706}
}



@Article{drones3030058,
AUTHOR = {Akhloufi, Moulay A. and Arola, Sebastien and Bonnet, Alexandre},
TITLE = {Drones Chasing Drones: Reinforcement Learning and Deep Search Area Proposal},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {58},
URL = {https://www.mdpi.com/2504-446X/3/3/58},
ISSN = {2504-446X},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are very popular and increasingly used in different applications. Today, the use of multiple UAVs and UAV swarms are attracting more interest from the research community, leading to the exploration of topics such as UAV cooperation, multi-drone autonomous navigation, etc. In this work, we propose two approaches for UAV pursuit-evasion. The first approach uses deep reinforcement learning to predict the actions to apply to the follower UAV to keep track of the target UAV. The second approach uses a deep object detector and a search area proposal (SAP) to predict the position of the target UAV in the next frame for tracking purposes. The two approaches are promising and lead to a higher tracking accuracy with an intersection over union (IoU) above the selected threshold. We also show that the deep SAP-based approach improves the detection of distant objects that cover small areas in the image. The efficiency of the proposed algorithms is demonstrated in outdoor tracking scenarios using real UAVs.},
DOI = {10.3390/drones3030058}
}



@Article{s19143140,
AUTHOR = {Tjhai, Chandra and O’Keefe, Kyle},
TITLE = {Using Step Size and Lower Limb Segment Orientation from Multiple Low-Cost Wearable Inertial/Magnetic Sensors for Pedestrian Navigation},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3140},
URL = {https://www.mdpi.com/1424-8220/19/14/3140},
ISSN = {1424-8220},
ABSTRACT = {This paper demonstrates the use of multiple low-cost inertial/magnetic sensors as a pedestrian navigation system for indoor positioning. This research looks at the problem of pedestrian navigation in a practical manner by investigating dead-reckoning methods using low-cost sensors. This work uses the estimated sensor orientation angles to compute the step size from the kinematics of a skeletal model. The orientations of limbs are represented by the tilt angles estimated from the inertial measurements, especially the pitch angle. In addition, different step size estimation methods are compared. A sensor data logging system is developed in order to record all motion data from every limb segment using a single platform and similar types of sensors. A skeletal model of five segments is chosen to model the forward kinematics of the lower limbs. A treadmill walk experiment with an optical motion capture system is conducted for algorithm evaluation. The mean error of the estimated orientation angles of the limbs is less than 6 degrees. The results show that the step length mean error is 3.2 cm, the left stride length mean error is 12.5 cm, and the right stride length mean error is 9 cm. The expected positioning error is less than 5% of the total distance travelled.},
DOI = {10.3390/s19143140}
}



@Article{rs11141692,
AUTHOR = {Farooq, Adnan and Jia, Xiuping and Hu, Jiankun and Zhou, Jun},
TITLE = {Multi-Resolution Weed Classification via Convolutional Neural Network and Superpixel Based Local Binary Pattern Using Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1692},
URL = {https://www.mdpi.com/2072-4292/11/14/1692},
ISSN = {2072-4292},
ABSTRACT = {Automatic weed detection and classification faces the challenges of large intraclass variation and high spectral similarity to other vegetation. With the availability of new high-resolution remote sensing data from various platforms and sensors, it is possible to capture both spectral and spatial characteristics of weed species at multiple scales. Effective multi-resolution feature learning is then desirable to extract distinctive intensity, texture and shape features of each category of weed to enhance the weed separability. We propose a feature extraction method using a Convolutional Neural Network (CNN) and superpixel based Local Binary Pattern (LBP). Both middle and high level spatial features are learned using the CNN. Local texture features from superpixel-based LBP are extracted, and are also used as input to Support Vector Machines (SVM) for weed classification. Experimental results on the hyperspectral and remote sensing datasets verify the effectiveness of the proposed method, and show that it outperforms several feature extraction approaches.},
DOI = {10.3390/rs11141692}
}



@Article{rs11141694,
AUTHOR = {Mekhalfi, Mohamed Lamine and Bejiga, Mesay Belete and Soresina, Davide and Melgani, Farid and Demir, Begüm},
TITLE = {Capsule Networks for Object Detection in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1694},
URL = {https://www.mdpi.com/2072-4292/11/14/1694},
ISSN = {2072-4292},
ABSTRACT = {Recent advances in Convolutional Neural Networks (CNNs) have attracted great attention in remote sensing due to their high capability to model high-level semantic content of Remote Sensing (RS) images. However, CNNs do not explicitly retain the relative position of objects in an image and, thus, the effectiveness of the obtained features is limited in the framework of the complex object detection problems. To address this problem, in this paper we introduce Capsule Networks (CapsNets) for object detection in Unmanned Aerial Vehicle-acquired images. Unlike CNNs, CapsNets extract and exploit the information content about objects&rsquo; relative position across several layers, which enables parsing crowded scenes with overlapping objects. Experimental results obtained on two datasets for car and solar panel detection problems show that CapsNets provide similar object detection accuracies when compared to state-of-the-art deep models with significantly reduced computational time. This is due to the fact that CapsNets emphasize dynamic routine instead of the depth.},
DOI = {10.3390/rs11141694}
}



@Article{rs11141708,
AUTHOR = {Cao, Shuang and Yu, Yongtao and Guan, Haiyan and Peng, Daifeng and Yan, Wanqian},
TITLE = {Affine-Function Transformation-Based Object Matching for Vehicle Detection from Unmanned Aerial Vehicle Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1708},
URL = {https://www.mdpi.com/2072-4292/11/14/1708},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection from remote sensing images plays a significant role in transportation related applications. However, the scale variations, orientation variations, illumination variations, and partial occlusions of vehicles, as well as the image qualities, bring great challenges for accurate vehicle detection. In this paper, we present an affine-function transformation-based object matching framework for vehicle detection from unmanned aerial vehicle (UAV) images. First, meaningful and non-redundant patches are generated through a superpixel segmentation strategy. Then, the affine-function transformation-based object matching framework is applied to a vehicle template and each of the patches for vehicle existence estimation. Finally, vehicles are detected and located after matching cost thresholding, vehicle location estimation, and multiple response elimination. Quantitative evaluations on two UAV image datasets show that the proposed method achieves an average completeness, correctness, quality, and F1-measure of 0.909, 0.969, 0.883, and 0.938, respectively. Comparative studies also demonstrate that the proposed method achieves compatible performance with the Faster R-CNN and outperforms the other eight existing methods in accurately detecting vehicles of various conditions.},
DOI = {10.3390/rs11141708}
}



@Article{rs11141713,
AUTHOR = {Jozdani, Shahab Eddin and Johnson, Brian Alan and Chen, Dongmei},
TITLE = {Comparing Deep Neural Networks, Ensemble Classifiers, and Support Vector Machine Algorithms for Object-Based Urban Land Use/Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1713},
URL = {https://www.mdpi.com/2072-4292/11/14/1713},
ISSN = {2072-4292},
ABSTRACT = {With the advent of high-spatial resolution (HSR) satellite imagery, urban land use/land cover (LULC) mapping has become one of the most popular applications in remote sensing. Due to the importance of context information (e.g., size/shape/texture) for classifying urban LULC features, Geographic Object-Based Image Analysis (GEOBIA) techniques are commonly employed for mapping urban areas. Regardless of adopting a pixel- or object-based framework, the selection of a suitable classifier is of critical importance for urban mapping. The popularity of deep learning (DL) (or deep neural networks (DNNs)) for image classification has recently skyrocketed, but it is still arguable if, or to what extent, DL methods can outperform other state-of-the art ensemble and/or Support Vector Machines (SVM) algorithms in the context of urban LULC classification using GEOBIA. In this study, we carried out an experimental comparison among different architectures of DNNs (i.e., regular deep multilayer perceptron (MLP), regular autoencoder (RAE), sparse, autoencoder (SAE), variational autoencoder (AE), convolutional neural networks (CNN)), common ensemble algorithms (Random Forests (RF), Bagging Trees (BT), Gradient Boosting Trees (GB), and Extreme Gradient Boosting (XGB)), and SVM to investigate their potential for urban mapping using a GEOBIA approach. We tested the classifiers on two RS images (with spatial resolutions of 30 cm and 50 cm). Based on our experiments, we drew three main conclusions: First, we found that the MLP model was the most accurate classifier. Second, unsupervised pretraining with the use of autoencoders led to no improvement in the classification result. In addition, the small difference in the classification accuracies of MLP from those of other models like SVM, GB, and XGB classifiers demonstrated that other state-of-the-art machine learning classifiers are still versatile enough to handle mapping of complex landscapes. Finally, the experiments showed that the integration of CNN and GEOBIA could not lead to more accurate results than the other classifiers applied.},
DOI = {10.3390/rs11141713}
}



@Article{s19143185,
AUTHOR = {Yu, Yang and Hou, Qingyu and Zhang, Wei and Zhang, Jinxiu},
TITLE = {A Sequential Two-Stage Track-to-Track Association Method in Asynchronous Bearings-Only Sensor Networks for Aerial Targets Surveillance},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3185},
URL = {https://www.mdpi.com/1424-8220/19/14/3185},
ISSN = {1424-8220},
ABSTRACT = {Successful track-to-track association (TTTA) in a multisensor and multitarget scenario is predicated on a reasonable likelihood function to evaluate the similarity of asynchronous mono tracks. To deal with the lack of synchronous data and prior knowledge of the targets in practical applications, this paper investigates a global optimization method with a novel likelihood function constructed by finite asynchronous measurements with joint temporal and spatial constraints (JTSC). For a scenario with more than two independent sensors, a sequential two-stage strategy is proposed to calculate the similarity of multiple asynchronous mono tracks. For the first stage, based on the temporal features of measurements from different sensors, a pairwise fusion model to estimate the position of the target with two mono tracks is established based on the asynchronous crossing location approach. For the other stage, to evaluate the similarity of the outputs, a pairwise similarity model is constructed by searching for the optimal matching points by setting temporal and spatial constraints. Thus, the likelihood of multiple asynchronous tracks is obtained. Simulations are performed to verify that the proposed method can achieve favorable performance without data-synchronization, and also demonstrate the superiority over the methods based on hinge angle differences (HADs) in some scenarios.},
DOI = {10.3390/s19143185}
}



@Article{pr7070464,
AUTHOR = {Gong, Qingwu and Tan, Si and Wang, Yubo and Liu, Dong and Qiao, Hui and Wu, Liuchuang},
TITLE = {Online Operation Risk Assessment of the Wind Power System of the Convolution Neural Network (CNN) Considering Multiple Random Factors},
JOURNAL = {Processes},
VOLUME = {7},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {464},
URL = {https://www.mdpi.com/2227-9717/7/7/464},
ISSN = {2227-9717},
ABSTRACT = {In order to solve the problem of the inaccuracy of the traditional online operation risk assessment model based on a physical mechanism and the inability to adapt to the actual operation of massive online operation monitoring data, this paper proposes an online operation risk assessment of the wind power system of the convolution neural network (CNN) considering multiple random factors. This paper analyzes multiple random factors of the wind power system, including uncertain wind power output, load fluctuations, frequent changes in operation patterns, and the electrical equipment failure rate, and generates the sample data based on multi-random factors. It uses the CNN algorithm network, offline training to obtain the risk assessment model, and online application to obtain the real-time online operation risk state of the wind power system. Finally, the online operation risk assessment model is verified by simulation using the standard network of 39 nodes of 10 machines New England system. The results prove that the risk assessment model presented in this paper is more rapid and suitable for online application.},
DOI = {10.3390/pr7070464}
}



@Article{robotics8030059,
AUTHOR = {Iannace, Gino and Ciaburro, Giuseppe and Trematerra, Amelia},
TITLE = {Fault Diagnosis for UAV Blades Using Artificial Neural Network},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2218-6581/8/3/59},
ISSN = {2218-6581},
ABSTRACT = {In recent years, unmanned aerial vehicles (UAVs) have been used in several fields including, for example, archaeology, cargo transport, conservation, healthcare, filmmaking, hobbies and recreational use. UAVs are aircraft characterized by the absence of a human pilot on board. The extensive use of these devices has highlighted maintenance problems with regard to the propellers, which represent the source of propulsion of the aircraft. A defect in the propellers of a drone can cause the aircraft to fall to the ground and its consequent destruction, and it also constitutes a safety problem for objects and people that are in the range of action of the aircraft. In this study, the measurements of the noise emitted by a UAV were used to build a classification model to detect unbalanced blades in a UAV propeller. To simulate the fault condition, two strips of paper tape were applied to the upper surface of a blade. The paper tape created a substantial modification of the aerodynamics of the blade, and this modification characterized the noise produced by the blade in its rotation. Then, a model based on artificial neural network algorithms was built to detect unbalanced blades in a UAV propeller. This model showed high accuracy (0.9763), indicating a high number of correct detections and suggests the adoption of this tool to verify the operating conditions of a UAV. The test must be performed indoors; from the measurements of the noise produced by the UAV it is possible to identify an imbalance in the propeller blade.},
DOI = {10.3390/robotics8030059}
}



@Article{rs11141725,
AUTHOR = {Xia, Xue and Persello, Claudio and Koeva, Mila},
TITLE = {Deep Fully Convolutional Networks for Cadastral Boundary Detection from UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1725},
URL = {https://www.mdpi.com/2072-4292/11/14/1725},
ISSN = {2072-4292},
ABSTRACT = {There is a growing demand for cheap and fast cadastral mapping methods to face the challenge of 70% global unregistered land rights. As traditional on-site field surveying is time-consuming and labor intensive, imagery-based cadastral mapping has in recent years been advocated by fit-for-purpose (FFP) land administration. However, owing to the semantic gap between the high-level cadastral boundary concept and low-level visual cues in the imagery, improving the accuracy of automatic boundary delineation remains a major challenge. In this research, we use imageries acquired by Unmanned Aerial Vehicles (UAV) to explore the potential of deep Fully Convolutional Networks (FCNs) for cadastral boundary detection in urban and semi-urban areas. We test the performance of FCNs against other state-of-the-art techniques, including Multi-Resolution Segmentation (MRS) and Globalized Probability of Boundary (gPb) in two case study sites in Rwanda. Experimental results show that FCNs outperformed MRS and gPb in both study areas and achieved an average accuracy of 0.79 in precision, 0.37 in recall and 0.50 in F-score. In conclusion, FCNs are able to effectively extract cadastral boundaries, especially when a large proportion of cadastral boundaries are visible. This automated method could minimize manual digitization and reduce field work, thus facilitating the current cadastral mapping and updating practices.},
DOI = {10.3390/rs11141725}
}



@Article{app9142917,
AUTHOR = {Chen, Yan and Zhang, Chengming and Wang, Shouyi and Li, Jianping and Li, Feng and Yang, Xiaoxia and Wang, Yuanyuan and Yin, Leikun},
TITLE = {Extracting Crop Spatial Distribution from Gaofen 2 Imagery Using a Convolutional Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2917},
URL = {https://www.mdpi.com/2076-3417/9/14/2917},
ISSN = {2076-3417},
ABSTRACT = {Using satellite remote sensing has become a mainstream approach for extracting crop spatial distribution. Making edges finer is a challenge, while simultaneously extracting crop spatial distribution information from high-resolution remote sensing images using a convolutional neural network (CNN). Based on the characteristics of the crop area in the Gaofen 2 (GF-2) images, this paper proposes an improved CNN to extract fine crop areas. The CNN comprises a feature extractor and a classifier. The feature extractor employs a spectral feature extraction unit to generate spectral features, and five coding-decoding-pair units to generate five level features. A linear model is used to fuse features of different levels, and the fusion results are up-sampled to obtain a feature map consistent with the structure of the input image. This feature map is used by the classifier to perform pixel-by-pixel classification. In this study, the SegNet and RefineNet models and 21 GF-2 images of Feicheng County, Shandong Province, China, were chosen for comparison experiment. Our approach had an accuracy of 93.26%, which is higher than those of the existing SegNet (78.12%) and RefineNet (86.54%) models. This demonstrates the superiority of the proposed method in extracting crop spatial distribution information from GF-2 remote sensing images.},
DOI = {10.3390/app9142917}
}



@Article{s19143217,
AUTHOR = {Cho, Jaechan and Jung, Yongchul and Kim, Dong-Sun and Lee, Seongjoo and Jung, Yunho},
TITLE = {Moving Object Detection Based on Optical Flow Estimation and a Gaussian Mixture Model for Advanced Driver Assistance Systems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3217},
URL = {https://www.mdpi.com/1424-8220/19/14/3217},
ISSN = {1424-8220},
ABSTRACT = {Most approaches for moving object detection (MOD) based on computer vision are limited to stationary camera environments. In advanced driver assistance systems (ADAS), however, ego-motion is added to image frames owing to the use of a moving camera. This results in mixed motion in the image frames and makes it difficult to classify target objects and background. In this paper, we propose an efficient MOD algorithm that can cope with moving camera environments. In addition, we present a hardware design and implementation results for the real-time processing of the proposed algorithm. The proposed moving object detector was designed using hardware description language (HDL) and its real-time performance was evaluated using an FPGA based test system. Experimental results demonstrate that our design achieves better detection performance than existing MOD systems. The proposed moving object detector was implemented with 13.2K logic slices, 104 DSP48s, and 163 BRAM and can support real-time processing of 30 fps at an operating frequency of 200 MHz.},
DOI = {10.3390/s19143217}
}



@Article{geosciences9070323,
AUTHOR = {Jakovljevic, Gordana and Govedarica, Miro and Alvarez-Taboada, Flor and Pajic, Vladimir},
TITLE = {Accuracy Assessment of Deep Learning Based Classification of LiDAR and UAV Points Clouds for DTM Creation and Flood Risk Mapping},
JOURNAL = {Geosciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2076-3263/9/7/323},
ISSN = {2076-3263},
ABSTRACT = {Digital elevation model (DEM) has been frequently used for the reduction and management of flood risk. Various classification methods have been developed to extract DEM from point clouds. However, the accuracy and computational efficiency need to be improved. The objectives of this study were as follows: (1) to determine the suitability of a new method to produce DEM from unmanned aerial vehicle (UAV) and light detection and ranging (LiDAR) data, using a raw point cloud classification and ground point filtering based on deep learning and neural networks (NN); (2) to test the convenience of rebalancing datasets for point cloud classification; (3) to evaluate the effect of the land cover class on the algorithm performance and the elevation accuracy; and (4) to assess the usability of the LiDAR and UAV structure from motion (SfM) DEM in flood risk mapping. In this paper, a new method of raw point cloud classification and ground point filtering based on deep learning using NN is proposed and tested on LiDAR and UAV data. The NN was trained on approximately 6 million points from which local and global geometric features and intensity data were extracted. Pixel-by-pixel accuracy assessment and visual inspection confirmed that filtering point clouds based on deep learning using NN is an appropriate technique for ground classification and producing DEM, as for the test and validation areas, both ground and non-ground classes achieved high recall (&gt;0.70) and high precision values (&gt;0.85), which showed that the two classes were well handled by the model. The type of method used for balancing the original dataset did not have a significant influence in the algorithm accuracy, and it was suggested not to use any of them unless the distribution of the generated and real data set will remain the same. Furthermore, the comparisons between true data and LiDAR and a UAV structure from motion (UAV SfM) point clouds were analyzed, as well as the derived DEM. The root mean square error (RMSE) and the mean average error (MAE) of the DEM were 0.25 m and 0.05 m, respectively, for LiDAR data, and 0.59 m and &ndash;0.28 m, respectively, for UAV data. For all land cover classes, the UAV DEM overestimated the elevation, whereas the LIDAR DEM underestimated it. The accuracy was not significantly different in the LiDAR DEM for the different vegetation classes, while for the UAV DEM, the RMSE increased with the height of the vegetation class. The comparison of the inundation areas derived from true LiDAR and UAV data for different water levels showed that in all cases, the largest differences were obtained for the lowest water level tested, while they performed best for very high water levels. Overall, the approach presented in this work produced DEM from LiDAR and UAV data with the required accuracy for flood mapping according to European Flood Directive standards. Although LiDAR is the recommended technology for point cloud acquisition, a suitable alternative is also UAV SfM in hilly areas.},
DOI = {10.3390/geosciences9070323}
}



@Article{w11081527,
AUTHOR = {Priori, Simone and Barbetti, Roberto and Meini, Luca and Morelli, Annalisa and Zampolli, Andrea and D’Avino, Lorenzo},
TITLE = {Towards Economic Land Evaluation at the Farm Scale Based on Soil Physical-Hydrological Features and Ecosystem Services},
JOURNAL = {Water},
VOLUME = {11},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {1527},
URL = {https://www.mdpi.com/2073-4441/11/8/1527},
ISSN = {2073-4441},
ABSTRACT = {The economic evaluation of a land parcel is mainly based on the local economy, as well as on the topography, distance to the main streets, distance to the river, and presence of irrigation. Spatial variability of soil features and functionalities are often left behind during economic land evaluation, probably due to a scarce awareness of soil function’s economic value. The paper shows an approach for economic land evaluation of irrigated croplands in the Po River plain (Northern Italy), based on spatial variability of soil functions, namely biomass production and carbon sequestration, as well as taking into account the river flood risk. The soil spatial variability was mapped using proximal sensing technology and few calibration points (one every 5 hectares). Biomass production of the main crops of the area, namely maize, soybean, and sorghum, was monitored and mapped for three years (2016, 2017, and 2018) using precision agriculture technologies. The results showed that the available water capacity (AWC) reached the highest correlation with biomass production, additionally, soil texture and cation exchange capacity were significantly correlated. Economic evaluation of the land parcels was computed considering the mean land market value of the area, the site-specific deviations due to the spatial variability of the biomass production by capitalization rate, and carbon sequestration soil functions, applying a natural capital approach by the mean annual value of the carbon market. This site-specific methodology could be applied to many other arable lands.},
DOI = {10.3390/w11081527}
}



@Article{app9152961,
AUTHOR = {Cao, Mingwei and Jia, Wei and Lv, Zhihan and Zheng, Liping and Liu, Xiaoping},
TITLE = {Superpixel-Based Feature Tracking for Structure from Motion},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2961},
URL = {https://www.mdpi.com/2076-3417/9/15/2961},
ISSN = {2076-3417},
ABSTRACT = {Feature tracking in image collections significantly affects the efficiency and accuracy of Structure from Motion (SFM). Insufficient correspondences may result in disconnected structures and incomplete components, while the redundant correspondences containing incorrect ones may yield to folded and superimposed structures. In this paper, we present a Superpixel-based feature tracking method for structure from motion. In the proposed method, we first propose to use a joint approach to detect local keypoints and compute descriptors. Second, the superpixel-based approach is used to generate labels for the input image. Third, we combine the Speed Up Robust Feature and binary test in the generated label regions to produce a set of combined descriptors for the detected keypoints. Fourth, the locality-sensitive hash (LSH)-based k nearest neighboring matching (KNN) is utilized to produce feature correspondences, and then the ratio test approach is used to remove outliers from the previous matching collection. Finally, we conduct comprehensive experiments on several challenging benchmarking datasets including highly ambiguous and duplicated scenes. Experimental results show that the proposed method gets better performances with respect to the state of the art methods.},
DOI = {10.3390/app9152961}
}



@Article{rs11151748,
AUTHOR = {Hariharan, Jeanette and Fuller, John and Ampatzidis, Yiannis and Abdulridha, Jaafar and Lerwill, Andrew},
TITLE = {Finite Difference Analysis and Bivariate Correlation of Hyperspectral Data for Detecting Laurel Wilt Disease and Nutritional Deficiency in Avocado},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1748},
URL = {https://www.mdpi.com/2072-4292/11/15/1748},
ISSN = {2072-4292},
ABSTRACT = {Laurel wilt (Lw) is a very destructive disease and poses a serious threat to the commercial production of avocado in Florida, USA. External symptoms of Lw are similar to those that are caused by other diseases and disorders. A rapid technique to distinguish Lw infected avocado from healthy trees and trees with other abiotic stressors is presented in this paper. A novel method was developed to analyze data from hyperspectral data using finite difference approximation (FDA) and bivariate correlation (BC) to discriminate Lw, Nitrogen (N), and Iron (Fe) deficiencies from healthy avocado plants. Several combinatorial methods were used in preprocessing the data, such as standard normal transformation of data, smoothing of the data, and polynomial fit. The FDA technique was derived using a Taylor Polynomial finite difference approximation. This FDA accentuates inflection points in the spectrum. These, in turn, reveal variance in the data that can be used to identify spectral signature associated with healthy and diseased states. By statistical correlation using the bivariate correlation coefficient of these enhanced spectral patterns, an algorithm (FDA-BC) for distinguishing Lw avocado leaves from all other categories of healthy or mineral deficient avocado leaves is achieved with an overall accuracy of 100%.},
DOI = {10.3390/rs11151748}
}



@Article{app9152976,
AUTHOR = {Luo, Cai and Zhao, Weikang and Du, Zhenpeng and Yu, Leijian},
TITLE = {A Neural Network Based Landing Method for an Unmanned Aerial Vehicle with Soft Landing Gears},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2976},
URL = {https://www.mdpi.com/2076-3417/9/15/2976},
ISSN = {2076-3417},
ABSTRACT = {This paper presents the design, implementation, and testing of a soft landing gear together with a neural network-based control method for replicating avian landing behavior on non-flat surfaces. With full consideration of unmanned aerial vehicles and landing gear requirements, a quadrotor helicopter, comprised of one flying unit and one landing assistance unit, is employed. Considering the touchdown speed and posture, a novel design of a soft mechanism for non-flat surfaces is proposed, in order to absorb the remaining landing impact. The framework of the control strategy is designed based on a derived dynamic model. A neural network-based backstepping controller is applied to achieve the desired trajectory. The simulation and outdoor testing results attest to the effectiveness and reliability of the proposed control method.},
DOI = {10.3390/app9152976}
}



@Article{drones3030059,
AUTHOR = {Hildmann, Hanno and Kovacs, Ernö},
TITLE = {Review: Using Unmanned Aerial Vehicles (UAVs) as Mobile Sensing Platforms (MSPs) for Disaster Response, Civil Security and Public Safety},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2504-446X/3/3/59},
ISSN = {2504-446X},
ABSTRACT = {The use of UAVs in areas ranging from agriculture over urban services to entertainment or simply as a hobby has rapidly grown over the last years. Regarding serious/commercial applications, UAVs have been considered in the literature, especially as mobile sensing/actuation platforms (i.e., as a delivery platform for an increasingly wide range of sensors and actuators). With regard to timely, cost-effective and very rich data acquisition, both, NEC Research as well as TNO are pursuing investigations into the use of UAVs and swarms of UAVs for scenarios where high-resolution requirements, prohibiting environments or tight time constraints render traditional approaches ineffective. In this review article, we provide a brief overview of safety and security-focused application areas that we identified as main targets for industrial and commercial projects, especially in the context of intelligent autonomous systems and autonomous/semi-autonomously operating swarms. We discuss a number of challenges related to the deployment of UAVs in general and to their deployment within the identified application areas in particular. As such, this article is meant to serve as a review and overview of the literature and the state-of-the-art, but also to offer an outlook over our possible (near-term) future work and the challenges that we will face there.},
DOI = {10.3390/drones3030059}
}



@Article{s19153306,
AUTHOR = {Byun, Yeun-Sub and Kim, Baek-Hyun and Jeong, Rag-Gyo},
TITLE = {Sensor Fault Detection and Signal Restoration in Intelligent Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3306},
URL = {https://www.mdpi.com/1424-8220/19/15/3306},
ISSN = {1424-8220},
ABSTRACT = {This paper presents fault diagnosis logic and signal restoration algorithms for vehicle motion sensors. Because various sensors are equipped to realize automatic operation of the vehicle, defects in these sensors lead to severe safety issues. Therefore, an effective and reliable fault detection and recovery system should be developed. The primary idea of the proposed fault detection system is the conversion of measured wheel speeds into vehicle central axis information and the selection of a reference central axis speed based on this information. Thus, the obtained results are employed to estimate the speed for all wheel sides, which are compared with measured values to identify fault and recover the fault signal. For fault diagnosis logic, a conditional expression is derived with only two variables to distinguish between normal and fault; further, an analytical redundancy structure and a simple diagnostic logic structure are presented. Finally, an off-line test is conducted using test vehicle information to validate the proposed method; it demonstrates that the proposed fault detection and signal restoration algorithm can satisfy the control performance required for each sensor failure.},
DOI = {10.3390/s19153306}
}



@Article{rs11151774,
AUTHOR = {Yi, Yaning and Zhang, Zhijie and Zhang, Wanchang and Zhang, Chuanrong and Li, Weidong and Zhao, Tian},
TITLE = {Semantic Segmentation of Urban Buildings from VHR Remote Sensing Imagery Using a Deep Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1774},
URL = {https://www.mdpi.com/2072-4292/11/15/1774},
ISSN = {2072-4292},
ABSTRACT = {Urban building segmentation is a prevalent research domain for very high resolution (VHR) remote sensing; however, various appearances and complicated background of VHR remote sensing imagery make accurate semantic segmentation of urban buildings a challenge in relevant applications. Following the basic architecture of U-Net, an end-to-end deep convolutional neural network (denoted as DeepResUnet) was proposed, which can effectively perform urban building segmentation at pixel scale from VHR imagery and generate accurate segmentation results. The method contains two sub-networks: One is a cascade down-sampling network for extracting feature maps of buildings from the VHR image, and the other is an up-sampling network for reconstructing those extracted feature maps back to the same size of the input VHR image. The deep residual learning approach was adopted to facilitate training in order to alleviate the degradation problem that often occurred in the model training process. The proposed DeepResUnet was tested with aerial images with a spatial resolution of 0.075 m and was compared in performance under the exact same conditions with six other state-of-the-art networks&mdash;FCN-8s, SegNet, DeconvNet, U-Net, ResUNet and DeepUNet. Results of extensive experiments indicated that the proposed DeepResUnet outperformed the other six existing networks in semantic segmentation of urban buildings in terms of visual and quantitative evaluation, especially in labeling irregular-shape and small-size buildings with higher accuracy and entirety. Compared with the U-Net, the F1 score, Kappa coefficient and overall accuracy of DeepResUnet were improved by 3.52%, 4.67% and 1.72%, respectively. Moreover, the proposed DeepResUnet required much fewer parameters than the U-Net, highlighting its significant improvement among U-Net applications. Nevertheless, the inference time of DeepResUnet is slightly longer than that of the U-Net, which is subject to further improvement.},
DOI = {10.3390/rs11151774}
}



@Article{s19153318,
AUTHOR = {Martínez, Carlos and Jiménez, Felipe},
TITLE = {Implementation of a Potential Field-Based Decision-Making Algorithm on Autonomous Vehicles for Driving in Complex Environments},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3318},
URL = {https://www.mdpi.com/1424-8220/19/15/3318},
ISSN = {1424-8220},
ABSTRACT = {Autonomous driving is undergoing huge developments nowadays. It is expected that its implementation will bring many benefits. Autonomous cars must deal with tasks at different levels. Although some of them are currently solved, and perception systems provide quite an accurate and complete description of the environment, high-level decisions are hard to obtain in challenging scenarios. Moreover, they must comply with safety, reliability and predictability requirements, road user acceptance, and comfort specifications. This paper presents a path planning algorithm based on potential fields. Potential models are adjusted so that their behavior is appropriate to the environment and the dynamics of the vehicle and they can face almost any unexpected scenarios. The response of the system considers the road characteristics (e.g., maximum speed, lane line curvature, etc.) and the presence of obstacles and other users. The algorithm has been tested on an automated vehicle equipped with a GPS receiver, an inertial measurement unit and a computer vision system in real environments with satisfactory results.},
DOI = {10.3390/s19153318}
}



@Article{s19153316,
AUTHOR = {Salhaoui, Marouane and Guerrero-González, Antonio and Arioua, Mounir and Ortiz, Francisco J. and El Oualkadi, Ahmed and Torregrosa, Carlos Luis},
TITLE = {Smart Industrial IoT Monitoring and Control System Based on UAV and Cloud Computing Applied to a Concrete Plant},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3316},
URL = {https://www.mdpi.com/1424-8220/19/15/3316},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are now considered one of the best remote sensing techniques for gathering data over large areas. They are now being used in the industry sector as sensing tools for proactively solving or preventing many issues, besides quantifying production and helping to make decisions. UAVs are a highly consistent technological platform for efficient and cost-effective data collection and event monitoring. The industrial Internet of things (IIoT) sends data from systems that monitor and control the physical world to data processing systems that cloud computing has shown to be important tools for meeting processing requirements. In fog computing, the IoT gateway links different objects to the internet. It can operate as a joint interface for different networks and support different communication protocols. A great deal of effort has been put into developing UAVs and multi-UAV systems. This paper introduces a smart IIoT monitoring and control system based on an unmanned aerial vehicle that uses cloud computing services and exploits fog computing as the bridge between IIoT layers. Its novelty lies in the fact that the UAV is automatically integrated into an industrial control system through an IoT gateway platform, while UAV photos are systematically and instantly computed and analyzed in the cloud. Visual supervision of the plant by drones and cloud services is integrated in real-time into the control loop of the industrial control system. As a proof of concept, the platform was used in a case study in an industrial concrete plant. The results obtained clearly illustrate the feasibility of the proposed platform in providing a reliable and efficient system for UAV remote control to improve product quality and reduce waste. For this, we studied the communication latency between the different IIoT layers in different IoT gateways.},
DOI = {10.3390/s19153316}
}



@Article{robotics8030062,
AUTHOR = {Thomas, Ajith and Hedley, John},
TITLE = {FumeBot: A Deep Convolutional Neural Network Controlled Robot},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {62},
URL = {https://www.mdpi.com/2218-6581/8/3/62},
ISSN = {2218-6581},
ABSTRACT = {This paper describes the development of a convolutional neural network for the control of a home monitoring robot (FumeBot). The robot is fitted with a Raspberry Pi for on board control and a Raspberry Pi camera is used as the data feed for the neural network. A wireless connection between the robot and a graphical user interface running on a laptop allows for the diagnostics and development of the neural network. The neural network, running on the laptop, was trained using a supervised training method. The robot was put through a series of obstacle courses to test its robustness, with the tests demonstrating that the controller has learned to navigate the obstacles to a reasonable level. The main problem identified in this work was that the neural controller did not have memory of past actions it took and a past state of the world resulting in obstacle collisions. Options to rectify this issue are suggested.},
DOI = {10.3390/robotics8030062}
}



@Article{electronics8080847,
AUTHOR = {Zhang, Dong and Raven, Lindsey Ann and Lee, Dah-Jye and Yu, Meng and Desai, Alok},
TITLE = {Hardware Friendly Robust Synthetic Basis Feature Descriptor},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {847},
URL = {https://www.mdpi.com/2079-9292/8/8/847},
ISSN = {2079-9292},
ABSTRACT = {Finding corresponding image features between two images is often the first step for many computer vision algorithms. This paper introduces an improved synthetic basis feature descriptor algorithm that describes and compares image features in an efficient and discrete manner with rotation and scale invariance. It works by performing a number of similarity tests between the feature region surrounding the feature point and a predetermined number of synthetic basis images to generate a feature descriptor that uniquely describes the feature region. Features in two images are matched by comparing their descriptors. By only storing the similarity of the feature region to each synthetic basis image, the overall storage size is greatly reduced. In short, this new binary feature descriptor is designed to provide high feature matching accuracy with computational simplicity, relatively low resource usage, and a hardware friendly design for real-time vision applications. Experimental results show that our algorithm produces higher precision rates and larger number of correct matches than the original version and other mainstream algorithms and is a good alternative for common computer vision applications. Two applications that often have to cope with scaling and rotation variations are included in this work to demonstrate its performance.},
DOI = {10.3390/electronics8080847}
}



@Article{electronics8080856,
AUTHOR = {Konecny, Jaromir and Kromer, Pavel and Prauzek, Michal and Musilek, Petr},
TITLE = {Scan Matching by Cross-Correlation and Differential Evolution},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {856},
URL = {https://www.mdpi.com/2079-9292/8/8/856},
ISSN = {2079-9292},
ABSTRACT = {Scan matching is an important task, solved in the context of many high-level problems including pose estimation, indoor localization, simultaneous localization and mapping and others. Methods that are accurate and adaptive and at the same time computationally efficient are required to enable location-based services in autonomous mobile devices. Such devices usually have a wide range of high-resolution sensors but only a limited processing power and constrained energy supply. This work introduces a novel high-level scan matching strategy that uses a combination of two advanced algorithms recently used in this field: cross-correlation and differential evolution. The cross-correlation between two laser range scans is used as an efficient measure of scan alignment and the differential evolution algorithm is used to search for the parameters of a transformation that aligns the scans. The proposed method was experimentally validated and showed good ability to match laser range scans taken shortly after each other and an excellent ability to match laser range scans taken with longer time intervals between them.},
DOI = {10.3390/electronics8080856}
}



@Article{rs11151816,
AUTHOR = {Iizuka, Kotaro and Kato, Tsuyoshi and Silsigia, Sisva and Soufiningrum, Alifia Yuni and Kozan, Osamu},
TITLE = {Estimating and Examining the Sensitivity of Different Vegetation Indices to Fractions of Vegetation Cover at Different Scaling Grids for Early Stage Acacia Plantation Forests Using a Fixed-Wing UAS},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1816},
URL = {https://www.mdpi.com/2072-4292/11/15/1816},
ISSN = {2072-4292},
ABSTRACT = {Understanding the information on land conditions and especially green vegetation cover is important for monitoring ecosystem dynamics. The fraction of vegetation cover (FVC) is a key variable that can be used to observe vegetation cover trends. Conventionally, satellite data are utilized to compute these variables, although computations in regions such as the tropics can limit the amount of available observation information due to frequent cloud coverage. Unmanned aerial systems (UASs) have become increasingly prominent in recent research and can remotely sense using the same methods as satellites but at a lower altitude. UASs are not limited by clouds and have a much higher resolution. This study utilizes a UAS to determine the emerging trends for FVC estimates at an industrial plantation site in Indonesia, which utilizes fast-growing Acacia trees that can rapidly change the land conditions. First, the UAS was utilized to collect high-resolution RGB imagery and multispectral images for the study area. The data were used to develop general land use/land cover (LULC) information for the site. Multispectral data were converted to various vegetation indices, and within the determined resolution grid (5, 10, 30 and 60 m), the fraction of each LULC type was analyzed for its correlation between the different vegetation indices (Vis). Finally, a simple empirical model was developed to estimate the FVC from the UAS data. The results show the correlation between the FVC (acacias) and different Vis ranging from R2 = 0.66&ndash;0.74, 0.76&ndash;0.8, 0.84&ndash;0.89 and 0.93&ndash;0.94 for 5, 10, 30 and 60 m grid resolutions, respectively. This study indicates that UAS-based FVC estimations can be used for observing fast-growing acacia trees at a fine scale resolution, which may assist current restoration programs in Indonesia.},
DOI = {10.3390/rs11151816}
}



@Article{rs11151837,
AUTHOR = {Brinkhoff, James and Dunn, Brian W. and Robson, Andrew J. and Dunn, Tina S. and Dehaan, Remy L.},
TITLE = {Modeling Mid-Season Rice Nitrogen Uptake Using Multispectral Satellite Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1837},
URL = {https://www.mdpi.com/2072-4292/11/15/1837},
ISSN = {2072-4292},
ABSTRACT = {Mid-season nitrogen (N) application in rice crops can maximize yield and profitability. This requires accurate and efficient methods of determining rice N uptake in order to prescribe optimal N amounts for topdressing. This study aims to determine the accuracy of using remotely sensed multispectral data from satellites to predict N uptake of rice at the panicle initiation (PI) growth stage, with a view to providing optimum variable-rate N topdressing prescriptions without needing physical sampling. Field experiments over 4 years, 4&ndash;6 N rates, 4 varieties and 2 sites were conducted, with at least 3 replicates of each plot. One WorldView satellite image for each year was acquired, close to the date of PI. Numerous single- and multi-variable models were investigated. Among single-variable models, the square of the NDRE vegetation index was shown to be a good predictor of N uptake (R     2     = 0.75, RMSE = 22.8 kg/ha for data pooled from all years and experiments). For multi-variable models, Lasso regularization was used to ensure an interpretable and compact model was chosen and to avoid over fitting. Combinations of remotely sensed reflectances and spectral indexes as well as variety, climate and management data as input variables for model training achieved R     2    &lt; 0.9 and RMSE &lt; 15 kg/ha for the pooled data set. The ability of remotely sensed data to predict N uptake in new seasons where no physical sample data has yet been obtained was tested. A methodology to extract models that generalize well to new seasons was developed, avoiding model overfitting. Lasso regularization selected four or less input variables, and yielded R     2     of better than 0.67 and RMSE better than 27.4 kg/ha over four test seasons that weren&rsquo;t used to train the models.},
DOI = {10.3390/rs11151837}
}



@Article{s19163451,
AUTHOR = {Lay, Usman Salihu and Pradhan, Biswajeet and Yusoff, Zainuddin Bin Md and Abdallah, Ahmad Fikri Bin and Aryal, Jagannath and Park, Hyuck-Jin},
TITLE = {Data Mining and Statistical Approaches in Debris-Flow Susceptibility Modelling Using Airborne LiDAR Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3451},
URL = {https://www.mdpi.com/1424-8220/19/16/3451},
ISSN = {1424-8220},
ABSTRACT = {Cameron Highland is a popular tourist hub in the mountainous area of Peninsular Malaysia. Most communities in this area suffer frequent incidence of debris flow, especially during monsoon seasons. Despite the loss of lives and properties recorded annually from debris flow, most studies in the region concentrate on landslides and flood susceptibilities. In this study, debris-flow susceptibility prediction was carried out using two data mining techniques; Multivariate Adaptive Regression Splines (MARS) and Support Vector Regression (SVR) models. The existing inventory of debris-flow events (640 points) were selected for training 70% (448) and validation 30% (192). Twelve conditioning factors namely; elevation, plan-curvature, slope angle, total curvature, slope aspect, Stream Transport Index (STI), profile curvature, roughness index, Stream Catchment Area (SCA), Stream Power Index (SPI), Topographic Wetness Index (TWI) and Topographic Position Index (TPI) were selected from Light Detection and Ranging (LiDAR)-derived Digital Elevation Model (DEM) data. Multi-collinearity was checked using Information Factor, Cramer&rsquo;s V, and Gini Index to identify the relative importance of conditioning factors. The susceptibility models were produced and categorized into five classes; not-susceptible, low, moderate, high and very-high classes. Models performances were evaluated using success and prediction rates where the area under the curve (AUC) showed a higher performance of MARS (93% and 83%) over SVR (76% and 72%). The result of this study will be important in contingency hazards and risks management plans to reduce the loss of lives and properties in the area.},
DOI = {10.3390/s19163451}
}



@Article{s19163465,
AUTHOR = {Pongsakornsathien, Nichakorn and Lim, Yixiang and Gardi, Alessandro and Hilton, Samuel and Planke, Lars and Sabatini, Roberto and Kistan, Trevor and Ezer, Neta},
TITLE = {Sensor Networks for Aerospace Human-Machine Systems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3465},
URL = {https://www.mdpi.com/1424-8220/19/16/3465},
ISSN = {1424-8220},
ABSTRACT = {Intelligent automation and trusted autonomy are being introduced in aerospace cyber-physical systems to support diverse tasks including data processing, decision-making, information sharing and mission execution. Due to the increasing level of integration/collaboration between humans and automation in these tasks, the operational performance of closed-loop human-machine systems can be enhanced when the machine monitors the operator’s cognitive states and adapts to them in order to maximise the effectiveness of the Human-Machine Interfaces and Interactions (HMI2). Technological developments have led to neurophysiological observations becoming a reliable methodology to evaluate the human operator’s states using a variety of wearable and remote sensors. The adoption of sensor networks can be seen as an evolution of this approach, as there are notable advantages if these sensors collect and exchange data in real-time, while their operation is controlled remotely and synchronised. This paper discusses recent advances in sensor networks for aerospace cyber-physical systems, focusing on Cognitive HMI2 (CHMI2) implementations. The key neurophysiological measurements used in this context and their relationship with the operator’s cognitive states are discussed. Suitable data analysis techniques based on machine learning and statistical inference are also presented, as these techniques allow processing both neurophysiological and operational data to obtain accurate cognitive state estimations. Lastly, to support the development of sensor networks for CHMI2 applications, the paper addresses the performance characterisation of various state-of-the-art sensors and the propagation of measurement uncertainties through a machine learning-based inference engine. Results show that a proper sensor selection and integration can support the implementation of effective human-machine systems for various challenging aerospace applications, including Air Traffic Management (ATM), commercial airliner Single-Pilot Operations (SIPO), one-to-many Unmanned Aircraft Systems (UAS), and space operations management.},
DOI = {10.3390/s19163465}
}



@Article{rs11161856,
AUTHOR = {Ghoussein, Youssra and Nicolas, Hervé and Haury, Jacques and Fadel, Ali and Pichelin, Pascal and Abou Hamdan, Hussein and Faour, Ghaleb},
TITLE = {Multitemporal Remote Sensing Based on an FVC Reference Period Using Sentinel-2 for Monitoring Eichhornia crassipes on a Mediterranean River},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1856},
URL = {https://www.mdpi.com/2072-4292/11/16/1856},
ISSN = {2072-4292},
ABSTRACT = {Invasive aquatic plants are a serious global ecological and socio-economic problem because they can cause local extinction of native species and alter navigation and fishing. Eichhornia crassipes (water hyacinth) is a dangerous invasive floating plant that is widely distributed throughout the world. In Lebanon, it has spread since 2006 in the Al Kabir River. Remote sensing techniques have been widely developed to detect and monitor dynamics and extents of invasive plants such as water hyacinth over large areas. However, they become challenging to use in narrow areas such as the Al Kabir River and we developed a new image-analysis method to extract water hyacinth areas on the river. The method is based on a time series of a biophysical variable obtained from Sentinel-2 images. After defining a reference period between two growing cycles, we used the fractional vegetation cover (FVC) to estimate the water hyacinth surface area in the river. This method makes it possible to monitor water hyacinth development and estimate the total area it colonizes in the river corridor. This method can help ecologists and other stakeholders to map invasive plants in rivers and improve their control.},
DOI = {10.3390/rs11161856}
}



@Article{rs11161859,
AUTHOR = {Dyson, Jack and Mancini, Adriano and Frontoni, Emanuele and Zingaretti, Primo},
TITLE = {Deep Learning for Soil and Crop Segmentation from Remotely Sensed Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1859},
URL = {https://www.mdpi.com/2072-4292/11/16/1859},
ISSN = {2072-4292},
ABSTRACT = {One of the most challenging problems in precision agriculture is to correctly identify and separate crops from the soil. Current precision farming algorithms based on artificially intelligent networks use multi-spectral or hyper-spectral data to derive radiometric indices that guide the operational management of agricultural complexes. Deep learning applications using these big data require sensitive filtering of raw data to effectively drive their hidden layer neural network architectures. Threshold techniques based on the normalized difference vegetation index (NDVI) or other similar metrics are generally used to simplify the development and training of deep learning neural networks. They have the advantage of being natural transformations of hyper-spectral or multi-spectral images that filter the data stream into a neural network, while reducing training requirements and increasing system classification performance. In this paper, to calculate a detailed crop/soil segmentation based on high resolution Digital Surface Model (DSM) data, we propose the redefinition of the radiometric index using a directional mathematical filter. To further refine the analysis, we feed this new radiometric index image of about 3500 &times; 4500 pixels into a relatively small Convolution Neural Network (CNN) designed for general image pattern recognition at 28 &times; 28 pixels to evaluate and resolve the vegetation correctly. We show that the result of applying a DSM filter to the NDVI radiometric index before feeding it into a Convolutional Neural Network can potentially improve crop separation hit rate by 65%.},
DOI = {10.3390/rs11161859}
}



@Article{geosciences9080350,
AUTHOR = {Polykretis, Christos and Kalogeropoulos, Kleomenis and Andreopoulos, Panagiotis and Faka, Antigoni and Tsatsaris, Andreas and Chalkias, Christos},
TITLE = {Comparison of Statistical Analysis Models for Susceptibility Assessment of Earthquake-Triggered Landslides: A Case Study from 2015 Earthquake in Lefkada Island},
JOURNAL = {Geosciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {350},
URL = {https://www.mdpi.com/2076-3263/9/8/350},
ISSN = {2076-3263},
ABSTRACT = {The main purpose of this study is to comparatively assess the susceptibility of earthquake-triggered landslides in the island of Lefkada (Ionian Islands, Greece) using two different statistical analysis models, a bivariate model represented by frequency ratio (FR), and a multivariate model represented by logistic regression (LR). For the implementation of the models, the relationship between geo-environmental factors contributing to landslides and documented events related to the 17th November 2015 earthquake was investigated by geographic information systems (GIS)-based analysis. A landslide inventory with events attributed to the specific earthquake was prepared using satellite imagery interpretation and field surveys. Eight factors: Elevation, slope angle, slope aspect, distance to main road network, distance to faults, land cover, geology, and peak ground acceleration (PGA), were considered and used as thematic data layers. The prediction capability of the models and the accuracy of the resulting susceptibility maps were tested by a standard validation method, the receiver operator characteristic (ROC) analysis. Based on the validation results, the output map with the highest reliability could potentially constitute an ideal basis for use within regional spatial planning as well as for the organization of emergency actions by local authorities.},
DOI = {10.3390/geosciences9080350}
}



@Article{s19163517,
AUTHOR = {Ji, Zheng and Liao, Yifan and Zheng, Li and Wu, Liang and Yu, Manzhu and Feng, Yanjie},
TITLE = {An Assembled Detector Based on Geometrical Constraint for Power Component Recognition},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3517},
URL = {https://www.mdpi.com/1424-8220/19/16/3517},
ISSN = {1424-8220},
ABSTRACT = {The intelligent inspection of power lines and other difficult-to-access structures and facilities has been greatly enhanced by the use of Unmanned Aerial Vehicles (UAVs), which allow inspection in a safe, efficient, and high-quality fashion. This paper analyzes the characteristics of a scene containing power equipment and the operation mode of UAVs. A low-cost virtual scene is created, and a training sample for the power-line components is generated quickly. Taking a vibration-damper as the main object, an assembled detector based on geometrical constraint (ADGC) is proposed and is used to analyze the virtual dataset. The geometric positional relationship is used as the constraint, and the Faster Region with Convolutional Neural Network (R-CNN), Deformable Part Model (DPM), and Haar cascade classifiers are combined, which allows the features of different classifiers, such as contour, shape, and texture to be fully used. By combining the characteristics of virtual data and real data using UAV images, the power components are detected by the ADGC. The result produced by the detector with relatively good performance can help expand the training set and achieve a better detection model. Moreover, this method can be smoothly transferred to other power-line facilities and other power-line scenarios.},
DOI = {10.3390/s19163517}
}



@Article{s19163542,
AUTHOR = {Lygouras, Eleftherios and Santavas, Nicholas and Taitzoglou, Anastasios and Tarchanidis, Konstantinos and Mitropoulos, Athanasios and Gasteratos, Antonios},
TITLE = {Unsupervised Human Detection with an Embedded Vision System on a Fully Autonomous UAV for Search and Rescue Operations},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3542},
URL = {https://www.mdpi.com/1424-8220/19/16/3542},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play a primary role in a plethora of technical and scientific fields owing to their wide range of applications. In particular, the provision of emergency services during the occurrence of a crisis event is a vital application domain where such aerial robots can contribute, sending out valuable assistance to both distressed humans and rescue teams. Bearing in mind that time constraints constitute a crucial parameter in search and rescue (SAR) missions, the punctual and precise detection of humans in peril is of paramount importance. The paper in hand deals with real-time human detection onboard a fully autonomous rescue UAV. Using deep learning techniques, the implemented embedded system was capable of detecting open water swimmers. This allowed the UAV to provide assistance accurately in a fully unsupervised manner, thus enhancing first responder operational capabilities. The novelty of the proposed system is the combination of global navigation satellite system (GNSS) techniques and computer vision algorithms for both precise human detection and rescue apparatus release. Details about hardware configuration as well as the system&rsquo;s performance evaluation are fully discussed.},
DOI = {10.3390/s19163542}
}



@Article{s19163552,
AUTHOR = {Guo, Hui and Hong, Huajie},
TITLE = {Research on Filtering Algorithm of MEMS Gyroscope Based on Information Fusion},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3552},
URL = {https://www.mdpi.com/1424-8220/19/16/3552},
ISSN = {1424-8220},
ABSTRACT = {As an important inertial sensor, the gyroscope is mainly used to measure angular velocity in inertial space. However, due to the influence of semiconductor thermal noise and electromagnetic interference, the output of the gyroscope has a certain random noise and drift, which affects the accuracy of the detected angular velocity signal, thus interfering with the accuracy of the stability of the whole system. In order to reduce the noise and compensate for the drift of the MEMS (Micro Electromechanical System) gyroscope during usage, this paper proposes a Kalman filtering method based on information fusion, which uses the MEMS gyroscope and line accelerometer signals to implement the filtering function under the Kalman algorithm. The experimental results show that compared with the commonly used filtering methods, this method allows significant reduction of the noise of the gyroscope signal and accurate estimation of the drift of the gyroscope signal, and thus improves the control performance of the system and the stability accuracy.},
DOI = {10.3390/s19163552}
}



@Article{electronics8080904,
AUTHOR = {Li, Qingyu and Dai, Keren and Wang, Xiaofeng and Zhang, Yu and Zhang, He and Jiang, Defu},
TITLE = {Low-Complexity Failed Element Diagnosis for Radar-Communication mmWave Antenna Array with Low SNR},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {904},
URL = {https://www.mdpi.com/2079-9292/8/8/904},
ISSN = {2079-9292},
ABSTRACT = {The millimeter-wave (mmWave) antenna array plays an important role in the excellent performance of wireless sensors networks (WSN) or unmanned aerial vehicle (UAV) clusters. However, the array elements are easily damaged in its harsh working environment but hard to be repaired or exchanged timely, resulting in a serious decline in the beamforming performance. Thus, accurate self-diagnosis of the failed elements is of great importance. In previous studies, there are still significant difficulties for large-scale arrays under extremely low SNR. In this paper, a diagnosis algorithm with low complexity and high reliability for the failed elements is proposed, which is based on a joint decision of communication signal and sensing echoes. Compared with the previous studies, the complexity of the algorithm is reduced by the construction of low-dimensional feature vectors for classification, the decoupling of the degree of arrival (DOA) estimation and the failed pattern diagnosis, with the help of the sub-array division. Simulation results show that, under an ultra-low SNR of &minus;12.5 dB for communication signals and &minus;16 dB for sensing echoes, an accurate self-diagnosis with a block error rate lower than 8% can be realized. The study in this paper will effectively promote the long-term and reliable operation of the mmWave antenna array in WSN, UAV clusters and other similar fields.},
DOI = {10.3390/electronics8080904}
}



@Article{rs11161916,
AUTHOR = {Corradino, Claudia and Ganci, Gaetana and Cappello, Annalisa and Bilotta, Giuseppe and Hérault, Alexis and Del Negro, Ciro},
TITLE = {Mapping Recent Lava Flows at Mount Etna Using Multispectral Sentinel-2 Images and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1916},
URL = {https://www.mdpi.com/2072-4292/11/16/1916},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of recent lava flows can provide significant insight into the development of flow fields that may aid in predicting future flow behavior. The task is challenging, due to both intrinsic properties of the phenomenon (e.g., lava flow resurfacing processes) and technical issues (e.g., the difficulty to survey a spatially extended lava flow with either aerial or ground instruments while avoiding hazardous locations). The huge amount of moderate to high resolution multispectral satellite data currently provides new opportunities for monitoring of extreme thermal events, such as eruptive phenomena. While retrieving boundaries of an active lava flow is relatively straightforward, problems arise when discriminating a recently cooled lava flow from older lava flow fields. Here, we present a new supervised classifier based on machine learning techniques to discriminate recent lava imaged in the MultiSpectral Imager (MSI) onboard Sentinel-2 satellite. Automated classification evaluates each pixel in a scene and then groups the pixels with similar values (e.g., digital number, reflectance, radiance) into a specified number of classes. Bands at the spatial resolution of 10 m (bands 2, 3, 4, 8) are used as input to the classifier. The training phase is performed on a small number of pixels manually labeled as covered by fresh lava, while the testing characterizes the entire lava flow field. Compared with ground-based measurements and actual lava flows of Mount Etna emplaced in 2017 and 2018, our automatic procedure provides excellent results in terms of accuracy, precision, and sensitivity.},
DOI = {10.3390/rs11161916}
}



@Article{rs11161922,
AUTHOR = {Guo, Shichen and Jin, Qizhao and Wang, Hongzhen and Wang, Xuezhi and Wang, Yangang and Xiang, Shiming},
TITLE = {Learnable Gated Convolutional Neural Network for Semantic Segmentation in Remote-Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1922},
URL = {https://www.mdpi.com/2072-4292/11/16/1922},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation in high-resolution remote-sensing (RS) images is a fundamental task for RS-based urban understanding and planning. However, various types of artificial objects in urban areas make this task quite challenging. Recently, the use of Deep Convolutional Neural Networks (DCNNs) with multiscale information fusion has demonstrated great potential in enhancing performance. Technically, however, existing fusions are usually implemented by summing or concatenating feature maps in a straightforward way. Seldom do works consider the spatial importance for global-to-local context-information aggregation. This paper proposes a Learnable-Gated CNN (L-GCNN) to address this issue. Methodologically, the Taylor expression of the information-entropy function is first parameterized to design the gate function, which is employed to generate pixelwise weights for coarse-to-fine refinement in the L-GCNN. Accordingly, a Parameterized Gate Module (PGM) was designed to achieve this goal. Then, the single PGM and its densely connected extension were embedded into different levels of the encoder in the L-GCNN to help identify the discriminative feature maps at different scales. With the above designs, the L-GCNN is finally organized as a self-cascaded end-to-end architecture that is able to sequentially aggregate context information for fine segmentation. The proposed model was evaluated on two public challenging benchmarks, the ISPRS 2Dsemantic segmentation challenge Potsdam dataset and the Massachusetts building dataset. The experiment results demonstrate that the proposed method exhibited significant improvement compared with several related segmentation networks, including the FCN, SegNet, RefineNet, PSPNet, DeepLab and GSN.For example, on the Potsdam dataset, our method achieved a 93.65%     F 1     score and 88.06%     I o U     score for the segmentation of tiny cars in high-resolution RS images. As a conclusion, the proposed model showed potential for object segmentation from the RS images of buildings, impervious surfaces, low vegetation, trees and cars in urban settings, which largely varies in size and have confusing appearances.},
DOI = {10.3390/rs11161922}
}



@Article{s19163595,
AUTHOR = {Santos, Anderson Aparecido dos and Marcato Junior, José and Araújo, Márcio Santos and Di Martini, David Robledo and Tetila, Everton Castelão and Siqueira, Henrique Lopes and Aoki, Camila and Eltner, Anette and Matsubara, Edson Takashi and Pistori, Hemerson and Feitosa, Raul Queiroz and Liesenberg, Veraldo and Gonçalves, Wesley Nunes},
TITLE = {Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3595},
URL = {https://www.mdpi.com/1424-8220/19/16/3595},
ISSN = {1424-8220},
ABSTRACT = {Detection and classification of tree species from remote sensing data were performed using mainly multispectral and hyperspectral images and Light Detection And Ranging (LiDAR) data. Despite the comparatively lower cost and higher spatial resolution, few studies focused on images captured by Red-Green-Blue (RGB) sensors. Besides, the recent years have witnessed an impressive progress of deep learning methods for object detection. Motivated by this scenario, we proposed and evaluated the usage of Convolutional Neural Network (CNN)-based methods combined with Unmanned Aerial Vehicle (UAV) high spatial resolution RGB imagery for the detection of law protected tree species. Three state-of-the-art object detection methods were evaluated: Faster Region-based Convolutional Neural Network (Faster R-CNN), YOLOv3 and RetinaNet. A dataset was built to assess the selected methods, comprising 392 RBG images captured from August 2018 to February 2019, over a forested urban area in midwest Brazil. The target object is an important tree species threatened by extinction known as Dipteryx alata Vogel (Fabaceae). The experimental analysis delivered average precision around 92% with an associated processing times below 30 miliseconds.},
DOI = {10.3390/s19163595}
}



@Article{math7080755,
AUTHOR = {Ran, Xiangjin and Xue, Linfu and Zhang, Yanyan and Liu, Zeyu and Sang, Xuejia and He, Jinxin},
TITLE = {Rock Classification from Field Image Patches Analyzed Using a Deep Convolutional Neural Network},
JOURNAL = {Mathematics},
VOLUME = {7},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {755},
URL = {https://www.mdpi.com/2227-7390/7/8/755},
ISSN = {2227-7390},
ABSTRACT = {The automatic identification of rock type in the field would aid geological surveying, education, and automatic mapping. Deep learning is receiving significant research attention for pattern recognition and machine learning. Its application here has effectively identified rock types from images captured in the field. This paper proposes an accurate approach for identifying rock types in the field based on image analysis using deep convolutional neural networks. The proposed approach can identify six common rock types with an overall classification accuracy of 97.96%, thus outperforming other established deep-learning models and a linear model. The results show that the proposed approach based on deep learning represents an improvement in intelligent rock-type identification and solves several difficulties facing the automated identification of rock types in the field.},
DOI = {10.3390/math7080755}
}



@Article{app9173448,
AUTHOR = {Zhang, Changwu and Tang, Yuchen and Zhou, Li and Liu, Hengzhu},
TITLE = {Late Line-of-Sight Check and Prioritized Trees for Path Planning},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3448},
URL = {https://www.mdpi.com/2076-3417/9/17/3448},
ISSN = {2076-3417},
ABSTRACT = {How to generate a safe and high-quality path for a moving robot is a classical issue. The relative solution is also suitable for computer games and animation. To make the problem simpler, it is common to discretize the continuous planning space into grids, with blocked cells to represent obstacles. However, grid-based path-finding algorithms usually cannot find the truly shortest path because the extending paths are constrained to grid edges. Meanwhile, Line-of-Sight Check (LoS-Check) is an efficient operation to find the shorter any-angle path by relaxing the constraint of grid edges, which has been successfully used in Theta*. Through reducing the number of LoS-Check operations in Theta*, a variant version called LazyTheta* speeds up the planning especially in the 3D environment. We propose Late LoS-Check A* (LLA*) to further reduce the LoS-Check amount. It uses the structure of the prioritized trees to partially update the gvalues of different successors that share the same parent (i.e., the parent of the current vertex waiting to be extended). The sufficient experiments on various benchmark maps show that LLA* costs less execution time than Lazy Theta* while generating shorter paths. If we just delay the LoS-Check, the path planned by LLA* will hardly be shorter than that of Lazy Theta*. Therefore, the key of LLA* is the discriminatory strategy, and we empirically explain the reason why both the path length and execution time of LLA* are shorter than those of Lazy Theta*.},
DOI = {10.3390/app9173448}
}



@Article{electronics8090915,
AUTHOR = {Maldonado-Bascón, Saturnino and Iglesias-Iglesias, Cristian and Martín-Martín, Pilar and Lafuente-Arroyo, Sergio},
TITLE = {Fallen People Detection Capabilities Using Assistive Robot},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {915},
URL = {https://www.mdpi.com/2079-9292/8/9/915},
ISSN = {2079-9292},
ABSTRACT = {One of the main problems in the elderly population and for people with functional disabilities is falling when they are not supervised. Therefore, there is a need for monitoring systems with fall detection functionality. Mobile robots are a good solution for keeping the person in sight when compared to static-view sensors. Mobile-patrol robots can be used for a group of people and systems are less intrusive than ones based on mobile robots. In this paper, we propose a novel vision-based solution for fall detection based on a mobile-patrol robot that can correct its position in case of doubt. The overall approach can be formulated as an end-to-end solution based on two stages: person detection and fall classification. Deep learning-based computer vision is used for person detection and fall classification is done by using a learning-based Support Vector Machine (SVM) classifier. This approach mainly fulfills the following design requirements&mdash;simple to apply, adaptable, high performance, independent of person size, clothes, or the environment, low cost and real-time computing. Important to highlight is the ability to distinguish between a simple resting position and a real fall scene. One of the main contributions of this paper is the input feature vector to the SVM-based classifier. We evaluated the robustness of the approach using a realistic public dataset proposed in this paper called the Fallen Person Dataset (FPDS), with 2062 images and 1072 falls. The results obtained from different experiments indicate that the system has a high success rate in fall classification (precision of 100% and recall of 99.74%). Training the algorithm using our Fallen Person Dataset (FPDS) and testing it with other datasets showed that the algorithm is independent of the camera setup.},
DOI = {10.3390/electronics8090915}
}



@Article{su11174557,
AUTHOR = {Liu, Chunting and Jia, Guozhu},
TITLE = {Industrial Big Data and Computational Sustainability: Multi-Method Comparison Driven by High-Dimensional Data for Improving Reliability and Sustainability of Complex Systems},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {4557},
URL = {https://www.mdpi.com/2071-1050/11/17/4557},
ISSN = {2071-1050},
ABSTRACT = {Sustainable development is of great significance. The emerging research on data-driven computational sustainability has become an effective way to solve this problem. This paper presents a fault diagnosis and prediction framework for complex systems based on multi-dimensional data and multi-method comparison, aimed at improving the reliability and sustainability of the system by selecting methods with relatively superior performance. This study took the avionics system in the industrial field as an example. Based on the literature research on typical fault modes and fault diagnosis requirements of avionics systems, three popular high-dimensional data-driven fault diagnosis methods&mdash;support vector machine, convolutional neural network, and long- and short-term memory neural network&mdash;were comprehensively analyzed and compared. Finally, the actual bearing failure data were used for programming in order to verify and compare various methods and the process of selecting the superior method driven by high-dimensional data was fully demonstrated. We attempt to provide a sustainable development idea that continuously explores multi-method integration and comparison, aimed at improving the calculation efficiency and accuracy of reliability assessments, optimizing system performance, and ultimately achieving the goal of long-term improvement of system reliability and sustainability.},
DOI = {10.3390/su11174557}
}



@Article{rs11171976,
AUTHOR = {Hamdi, Zayd Mahmoud and Brandmeier, Melanie and Straub, Christoph},
TITLE = {Forest Damage Assessment Using Deep Learning on High Resolution Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1976},
URL = {https://www.mdpi.com/2072-4292/11/17/1976},
ISSN = {2072-4292},
ABSTRACT = {Storms can cause significant damage to forest areas, affecting biodiversity and infrastructure and leading to economic loss. Thus, rapid detection and mapping of windthrows are crucially important for forest management. Recent advances in computer vision have led to highly-accurate image classification algorithms such as Convolutional Neural Network (CNN) architectures. In this study, we tested and implemented an algorithm based on CNNs in an ArcGIS environment for automatic detection and mapping of damaged areas. The algorithm was trained and tested on a forest area in Bavaria, Germany. . It is a based on a modified U-Net architecture that was optimized for the pixelwise classification of multispectral aerial remote sensing data. The neural network was trained on labeled damaged areas from after-storm aerial orthophotos of a ca.     109 k  m 2      forest area with RGB and NIR bands and 0.2-m spatial resolution. Around     10 7     pixels of labeled data were used in the process. Once the network is trained, predictions on further datasets can be computed within seconds, depending on the size of the input raster and the computational power used. The overall accuracy on our test dataset was     92 %    . During visual validation, labeling errors were found in the reference data that somewhat biased the results because the algorithm in some instance performed better than the human labeling procedure, while missing areas affected by shadows. Our results are very good in terms of precision, and the methods introduced in this paper have several additional advantages compared to traditional methods: CNNs automatically detect high- and low-level features in the data, leading to high classification accuracies, while only one after-storm image is needed in comparison to two images for approaches based on change detection. Furthermore, flight parameters do not affect the results in the same way as for approaches that require DSMs and DTMs as the classification is only based on the image data themselves, and errors occurring in the computation of DSMs and DTMs do not affect the results with respect to the z component. The integration into the ArcGIS Platform allows a streamlined workflow for forest management, as the results can be accessed by mobile devices in the field to allow for high-accuracy ground-truthing and additional mapping that can be synchronized back into the database. Our results and the provided automatic workflow highlight the potential of deep learning on high-resolution imagery and GIS for fast and efficient post-disaster damage assessment as a first step of disaster management.},
DOI = {10.3390/rs11171976}
}



@Article{electronics8090919,
AUTHOR = {Wu, Ruidong and Liu, Bing and Fu, Jiafeng and Xu, Mingzhu and Fu, Ping and Li, Junbao},
TITLE = {Research and Implementation of ε-SVR Training Method Based on FPGA},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {919},
URL = {https://www.mdpi.com/2079-9292/8/9/919},
ISSN = {2079-9292},
ABSTRACT = {Online training of Support Vector Regression (SVR) in the field of machine learning is a computationally complex algorithm. Due to the need for multiple iterative processing in training, SVR training is usually implemented on computer, and the existing training methods cannot be directly implemented on Field-Programmable Gate Array (FPGA), which restricts the application range. This paper reconstructs the training framework and implementation without precision loss to reduce the total latency required for matrix update, reducing time consumption by 90%. A general &epsilon;-SVR training system with low latency is implemented on Zynq platform. Taking the regression of samples in two-dimensional as an example, the maximum acceleration ratio is 27.014&times; compared with microcontroller platform and the energy consumption is 12.449% of microcontroller. From the experiments for the University of California, Riverside (UCR) time series data set. The regression results obtain excellent regression effects. The minimum coefficient of determination is 0.996, and running time is less than 30 ms, which can meet the requirements of different applications for real-time regression.},
DOI = {10.3390/electronics8090919}
}



@Article{rs11171979,
AUTHOR = {Lu, Bing and He, Yuhong},
TITLE = {Evaluating Empirical Regression, Machine Learning, and Radiative Transfer Modelling for Estimating Vegetation Chlorophyll Content Using Bi-Seasonal Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1979},
URL = {https://www.mdpi.com/2072-4292/11/17/1979},
ISSN = {2072-4292},
ABSTRACT = {Different types of methods have been developed to retrieve vegetation attributes from remote sensing data, including conventional empirical regressions (i.e., linear regression (LR)), advanced empirical regressions (e.g., multivariable linear regression (MLR), partial least square regression (PLSR)), machine learning (e.g., random forest regression (RFR), decision tree regression (DTR)), and radiative transfer modelling (RTM, e.g., PROSAIL). Given that each algorithm has its own strengths and weaknesses, it is essential to compare them and evaluate their effectiveness. Previous studies have mainly used single-date multispectral imagery or ground-based hyperspectral reflectance data for evaluating the models, while multi-seasonal hyperspectral images have been rarely used. Extensive spectral and spatial information in hyperspectral images, as well as temporal variations of landscapes, potentially influence the model performance. In this research, LR, PLSR, RFR, and PROSAIL, representing different types of methods, were evaluated for estimating vegetation chlorophyll content from bi-seasonal hyperspectral images (i.e., a middle- and a late-growing season image, respectively). Results show that the PLSR and RFR generally performed better than LR and PROSAIL. RFR achieved the highest accuracy for both images. This research provides insights on the effectiveness of different models for estimating vegetation chlorophyll content using hyperspectral images, aiming to support future vegetation monitoring research.},
DOI = {10.3390/rs11171979}
}



@Article{rs11171990,
AUTHOR = {Mansour, Mostafa and Davidson, Pavel and Stepanov, Oleg and Piché, Robert},
TITLE = {Relative Importance of Binocular Disparity and Motion Parallax for Depth Estimation: A Computer Vision Approach},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1990},
URL = {https://www.mdpi.com/2072-4292/11/17/1990},
ISSN = {2072-4292},
ABSTRACT = {Binocular disparity and motion parallax are the most important cues for depth estimation in human and computer vision. Here, we present an experimental study to evaluate the accuracy of these two cues in depth estimation to stationary objects in a static environment. Depth estimation via binocular disparity is most commonly implemented using stereo vision, which uses images from two or more cameras to triangulate and estimate distances. We use a commercial stereo camera mounted on a wheeled robot to create a depth map of the environment. The sequence of images obtained by one of these two cameras as well as the camera motion parameters serve as the input to our motion parallax-based depth estimation algorithm. The measured camera motion parameters include translational and angular velocities. Reference distance to the tracked features is provided by a LiDAR. Overall, our results show that at short distances stereo vision is more accurate, but at large distances the combination of parallax and camera motion provide better depth estimation. Therefore, by combining the two cues, one obtains depth estimation with greater range than is possible using either cue individually.},
DOI = {10.3390/rs11171990}
}



@Article{app9173488,
AUTHOR = {Ćwiąkała, Paweł},
TITLE = {Testing Procedure of Unmanned Aerial Vehicles (UAVs) Trajectory in Automatic Missions},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3488},
URL = {https://www.mdpi.com/2076-3417/9/17/3488},
ISSN = {2076-3417},
ABSTRACT = {This paper describes an experimental test campaign while using an Unmanned Aerial Vehicle (UAV) and measuring the obtained UAV positions during different flight tasks and in different operative conditions. A new test procedure has been presented and tested for different devices in various weather conditions. This paper describes and analyses the measurements of the flight trajectory of the UAV that was performed with the use of a robotic total station (RTS), as compared to the design data and the data recorded in the internal memory of the UAV. Five different test tasks have been conducted. The obtained results have allowed for the assessment of the correctness of task performance as compared to the design and to determine the flying accuracy of the entire UAV set. The proposed set of tasks can be successfully utilised to control the correctness of operation of various types of UAVs and it may be implemented as a universal test to verify the algorithms optimising take-offs and landings, test flights of the objects, as well as flight planning in various terrain and weather conditions, which will increase the safety of the flights while using UAVs.},
DOI = {10.3390/app9173488}
}



@Article{rs11171997,
AUTHOR = {Jeziorska, Justyna},
TITLE = {UAS for Wetland Mapping and Hydrological Modeling},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1997},
URL = {https://www.mdpi.com/2072-4292/11/17/1997},
ISSN = {2072-4292},
ABSTRACT = {The miniaturization and affordable production of integrated microelectronics have improved in recent years, making unmanned aerial systems (UAS) accessible to consumers and igniting their interest. Researchers have proposed UAS-based solutions for almost any conceivable problem, but the greatest impact will likely be in applications that exploit the unique advantages of the technology: work in dangerous or difficult-to-access areas, high spatial resolution and/or frequent measurements of environmental phenomena, and deployment of novel sensing technology over small to moderate spatial scales. Examples of such applications may be the identification of wetland areas and use of high-resolution spatial data for hydrological modeling. However, because of the large&mdash;and growing&mdash;assortment of aircraft and sensors available on the market, an evolving regulatory environment, and limited practical guidance or examples of wetland mapping with UAS, it has been difficult to confidently devise or recommend UAS-based monitoring strategies for these applications. This paper provides a comprehensive review of UAS hardware, software, regulations, scientific applications, and data collection/post-processing procedures that are relevant for wetland monitoring and hydrological modeling.},
DOI = {10.3390/rs11171997}
}



@Article{drones3030066,
AUTHOR = {Khoufi, Ines and Laouiti, Anis and Adjih, Cedric},
TITLE = {A Survey of Recent Extended Variants of the Traveling Salesman and Vehicle Routing Problems for Unmanned Aerial Vehicles},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/2504-446X/3/3/66},
ISSN = {2504-446X},
ABSTRACT = {The use of Unmanned Aerial Vehicles (UAVs) is rapidly growing in popularity. Initially introduced for military purposes, over the past few years, UAVs and related technologies have successfully transitioned to a whole new range of civilian applications such as delivery, logistics, surveillance, entertainment, and so forth. They have opened new possibilities such as allowing operation in otherwise difficult or hazardous areas, for instance. For all applications, one foremost concern is the selection of the paths and trajectories of UAVs, and at the same time, UAVs control comes with many challenges, as they have limited energy, limited load capacity and are vulnerable to difficult weather conditions. Generally, efficiently operating a drone can be mathematically formalized as a path optimization problem under some constraints. This shares some commonalities with similar problems that have been extensively studied in the context of urban vehicles and it is only natural that the recent literature has extended the latter to fit aerial vehicle constraints. The knowledge of such problems, their formulation, the resolution methods proposed—through the variants induced specifically by UAVs features—are of interest for practitioners for any UAV application. Hence, in this study, we propose a review of existing literature devoted to such UAV path optimization problems, focusing specifically on the sub-class of problems that consider the mobility on a macroscopic scale. These are related to the two existing general classic ones—the Traveling Salesman Problem and the Vehicle Routing Problem. We analyze the recent literature that adapted the problems to the UAV context, provide an extensive classification and taxonomy of their problems and their formulation and also give a synthetic overview of the resolution techniques, performance metrics and obtained numerical results.},
DOI = {10.3390/drones3030066}
}



@Article{rs11172008,
AUTHOR = {Yang, Qinchen and Liu, Man and Zhang, Zhitao and Yang, Shuqin and Ning, Jifeng and Han, Wenting},
TITLE = {Mapping Plastic Mulched Farmland for High Resolution Images of Unmanned Aerial Vehicle Using Deep Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2008},
URL = {https://www.mdpi.com/2072-4292/11/17/2008},
ISSN = {2072-4292},
ABSTRACT = {With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images.},
DOI = {10.3390/rs11172008}
}



@Article{rs11172046,
AUTHOR = {Ghorbanzadeh, Omid and Meena, Sansar Raj and Blaschke, Thomas and Aryal, Jagannath},
TITLE = {UAV-Based Slope Failure Detection Using Deep-Learning Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2046},
URL = {https://www.mdpi.com/2072-4292/11/17/2046},
ISSN = {2072-4292},
ABSTRACT = {Slope failures occur when parts of a slope collapse abruptly under the influence of gravity, often triggered by a rainfall event or earthquake. The resulting slope failures often cause problems in mountainous or hilly regions, and the detection of slope failure is therefore an important topic for research. Most of the methods currently used for mapping and modelling slope failures rely on classification algorithms or feature extraction, but the spatial complexity of slope failures, the uncertainties inherent in expert knowledge, and problems in transferability, all combine to inhibit slope failure detection. In an attempt to overcome some of these problems we have analyzed the potential of deep learning convolutional neural networks (CNNs) for slope failure detection, in an area along a road section in the northern Himalayas, India. We used optical data from unmanned aerial vehicles (UAVs) over two separate study areas. Different CNN designs were used to produce eight different slope failure distribution maps, which were then compared with manually extracted slope failure polygons using different accuracy assessment metrics such as the precision, F-score, and mean intersection-over-union (mIOU). A slope failure inventory data set was produced for each of the study areas using a frequency-area distribution (FAD). The CNN approach that was found to perform best (precision accuracy assessment of almost 90% precision, F-score 85%, mIOU 74%) was one that used a window size of 64 &times; 64 pixels for the sample patches, and included slope data as an additional input layer. The additional information from the slope data helped to discriminate between slope failure areas and roads, which had similar spectral characteristics in the optical imagery. We concluded that the effectiveness of CNNs for slope failure detection was strongly dependent on their design (i.e., the window size selected for the sample patch, the data used, and the training strategies), but that CNNs are currently only designed by trial and error. While CNNs can be powerful tools, such trial and error strategies make it difficult to explain why a particular pooling or layer numbering works better than any other.},
DOI = {10.3390/rs11172046}
}



@Article{a12090183,
AUTHOR = {Li, Kexin and Wang, Jun and Qi, Dawei},
TITLE = {An Intelligent Warning Method for Diagnosing Underwater Structural Damage},
JOURNAL = {Algorithms},
VOLUME = {12},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {183},
URL = {https://www.mdpi.com/1999-4893/12/9/183},
ISSN = {1999-4893},
ABSTRACT = {A number of intelligent warning techniques have been implemented for detecting underwater infrastructure diagnosis to partially replace human-conducted on-site inspections. However, the extensively varying real-world situation (e.g., the adverse environmental conditions, the limited sample space, and the complex defect types) can lead to challenges to the wide adoption of intelligent warning techniques. To overcome these challenges, this paper proposed an intelligent algorithm combing gray level co-occurrence matrix (GLCM) with self-organization map (SOM) for accurate diagnosis of the underwater structural damage. In order to optimize the generative criterion for GLCM construction, a triangle algorithm was proposed based on orthogonal experiments. The constructed GLCM were utilized to evaluate the texture features of the regions of interest (ROI) of micro-injury images of underwater structures and extracted damage image texture characteristic parameters. The digital feature screening (DFS) method was used to obtain the most relevant features as the input for the SOM network. According to the unique topology information of the SOM network, the classification result, recognition efficiency, parameters, such as the network layer number, hidden layer node, and learning step, were optimized. The robustness and adaptability of the proposed approach were tested on underwater structure images through the DFS method. The results showed that the proposed method revealed quite better performances and can diagnose structure damage in underwater realistic situations.},
DOI = {10.3390/a12090183}
}



@Article{s19173754,
AUTHOR = {Stodola, Petr and Drozd, Jan and Mazal, Jan and Hodický, Jan and Procházka, Dalibor},
TITLE = {Cooperative Unmanned Aerial System Reconnaissance in a Complex Urban Environment and Uneven Terrain},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3754},
URL = {https://www.mdpi.com/1424-8220/19/17/3754},
ISSN = {1424-8220},
ABSTRACT = {Using unmanned robotic systems in military operations such as reconnaissance or surveillance, as well as in many civil applications, is common practice. In this article, the problem of monitoring the specified area of interest by a fleet of unmanned aerial systems is examined. The monitoring is planned via the Cooperative Aerial Model, which deploys a number of waypoints in the area; these waypoints are visited successively by unmanned systems. The original model proposed in the past assumed that the area to be explored is perfectly flat. A new formulation of this model is introduced in this article so that the model can be used in a complex environment with uneven terrain and/or with many obstacles, which may occlude some parts of the area of interest. The optimization algorithm based on the simulated annealing principles is proposed for positioning of waypoints to cover as large an area as possible. A set of scenarios has been designed to verify and evaluate the proposed approach. The key experiments are aimed at finding the minimum number of waypoints needed to explore at least the minimum requested portion of the area. Furthermore, the results are compared to the algorithm based on the lawnmower pattern.},
DOI = {10.3390/s19173754}
}



@Article{s19173796,
AUTHOR = {Shafi, Uferah and Mumtaz, Rafia and García-Nieto, José and Hassan, Syed Ali and Zaidi, Syed Ali Raza and Iqbal, Naveed},
TITLE = {Precision Agriculture Techniques and Practices: From Considerations to Applications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3796},
URL = {https://www.mdpi.com/1424-8220/19/17/3796},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things (IoT)-based automation of agricultural events can change the agriculture sector from being static and manual to dynamic and smart, leading to enhanced production with reduced human efforts. Precision Agriculture (PA) along with Wireless Sensor Network (WSN) are the main drivers of automation in the agriculture domain. PA uses specific sensors and software to ensure that the crops receive exactly what they need to optimize productivity and sustainability. PA includes retrieving real data about the conditions of soil, crops and weather from the sensors deployed in the fields. High-resolution images of crops are obtained from satellite or air-borne platforms (manned or unmanned), which are further processed to extract information used to provide future decisions. In this paper, a review of near and remote sensor networks in the agriculture domain is presented along with several considerations and challenges. This survey includes wireless communication technologies, sensors, and wireless nodes used to assess the environmental behaviour, the platforms used to obtain spectral images of crops, the common vegetation indices used to analyse spectral images and applications of WSN in agriculture. As a proof of concept, we present a case study showing how WSN-based PA system can be implemented. We propose an IoT-based smart solution for crop health monitoring, which is comprised of two modules. The first module is a wireless sensor network-based system to monitor real-time crop health status. The second module uses a low altitude remote sensing platform to obtain multi-spectral imagery, which is further processed to classify healthy and unhealthy crops. We also highlight the results obtained using a case study and list the challenges and future directions based on our work.},
DOI = {10.3390/s19173796}
}



@Article{s19183827,
AUTHOR = {Kim, Minwoo and Cho, Jaechan and Lee, Seongjoo and Jung, Yunho},
TITLE = {IMU Sensor-Based Hand Gesture Recognition for Human-Machine Interfaces},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3827},
URL = {https://www.mdpi.com/1424-8220/19/18/3827},
ISSN = {1424-8220},
ABSTRACT = {We propose an efficient hand gesture recognition (HGR) algorithm, which can cope with time-dependent data from an inertial measurement unit (IMU) sensor and support real-time learning for various human-machine interface (HMI) applications. Although the data extracted from IMU sensors are time-dependent, most existing HGR algorithms do not consider this characteristic, which results in the degradation of recognition performance. Because the dynamic time warping (DTW) technique considers the time-dependent characteristic of IMU sensor data, the recognition performance of DTW-based algorithms is better than that of others. However, the DTW technique requires a very complex learning algorithm, which makes it difficult to support real-time learning. To solve this issue, the proposed HGR algorithm is based on a restricted column energy (RCE) neural network, which has a very simple learning scheme in which neurons are activated when necessary. By replacing the metric calculation of the RCE neural network with DTW distance, the proposed algorithm exhibits superior recognition performance for time-dependent sensor data while supporting real-time learning. Our verification results on a field-programmable gate array (FPGA)-based test platform show that the proposed HGR algorithm can achieve a recognition accuracy of 98.6% and supports real-time learning and recognition at an operating frequency of 150 MHz.},
DOI = {10.3390/s19183827}
}



@Article{app9183708,
AUTHOR = {Tan, Liguo and Wu, Juncheng and Yang, Xiaoyan and Song, Senmin},
TITLE = {Research on Optimal Landing Trajectory Planning Method between an UAV and a Moving Vessel},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3708},
URL = {https://www.mdpi.com/2076-3417/9/18/3708},
ISSN = {2076-3417},
ABSTRACT = {The location, velocity, and flight path angle of an autonomous unmanned aerial vehicle (UAV) landing on a moving vessel are key factors for an optimal landing trajectory. To tackle this challenge, this paper proposes a method for calculating the optimal approach landing trajectory between an UAV and a small vessel. A numerical approach (iterative method) is used to calculate the optimal approach landing trajectory, and the initial lead is introduced in the calculation process of the UAV trajectory for the inclination and heading angle for accuracy improvement, so that the UAV can track and calculate the optimal landing trajectory with high precision. Compared with the variational method, the proposed method can calculate an optimal turning direction angle for the UAV during the landing. Simulation experiments verify the effectiveness of the proposed algorithm and give optimal initialization values.},
DOI = {10.3390/app9183708}
}



@Article{rs11182104,
AUTHOR = {Bhardwaj, Anshuman and Sam, Lydia and Martín-Torres, F. Javier and Zorzano, María-Paz and Ramírez Luque, Juan Antonio},
TITLE = {UAV Imaging of a Martian Brine Analogue Environment in a Fluvio-Aeolian Setting},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2104},
URL = {https://www.mdpi.com/2072-4292/11/18/2104},
ISSN = {2072-4292},
ABSTRACT = {Understanding extraterrestrial environments and landforms through remote sensing and terrestrial analogy has gained momentum in recent years due to advances in remote sensing platforms, sensors, and computing efficiency. The seasonal brines of the largest salt plateau on Earth in Salar de Uyuni (Bolivian Altiplano) have been inadequately studied for their localized hydrodynamics and the regolith volume transport across the freshwater-brine mixing zones. These brines have recently been projected as a new analogue site for the proposed Martian brines, such as recurring slope lineae (RSL) and slope streaks. The Martian brines have been postulated to be the result of ongoing deliquescence-based salt-hydrology processes on contemporary Mars, similar to the studied Salar de Uyuni brines. As part of a field-site campaign during the cold and dry season in the latter half of August 2017, we deployed an unmanned aerial vehicle (UAV) at two sites of the Salar de Uyuni to perform detailed terrain mapping and geomorphometry. We generated high-resolution (2 cm/pixel) photogrammetric digital elevation models (DEMs) for observing and quantifying short-term terrain changes within the brines and their surroundings. The achieved co-registration for the temporal DEMs was considerably high, from which precise inferences regarding the terrain dynamics were derived. The observed average rate of bottom surface elevation change for brines was ~1.02 mm/day, with localized signs of erosion and deposition. Additionally, we observed short-term changes in the adjacent geomorphology and salt cracks. We conclude that the transferred regolith volume via such brines can be extremely low, well within the resolution limits of the remote sensors that are currently orbiting Mars, thereby making it difficult to resolve the topographic relief and terrain perturbations that are produced by such flows on Mars. Thus, the absence of observable erosion and deposition features within or around most of the proposed Martian RSL and slope streaks cannot be used to dismiss the possibility of fluidized flow within these features.},
DOI = {10.3390/rs11182104}
}



@Article{app9183789,
AUTHOR = {Moon, Jiyoun and Lee, Beom-Hee},
TITLE = {PDDL Planning with Natural Language-Based Scene Understanding for UAV-UGV Cooperation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2076-3417/9/18/3789},
ISSN = {2076-3417},
ABSTRACT = {Natural-language-based scene understanding can enable heterogeneous robots to cooperate efficiently in large and unconstructed environments. However, studies on symbolic planning rarely consider the semantic knowledge acquisition problem associated with the surrounding environments. Further, recent developments in deep learning methods show outstanding performance for semantic scene understanding using natural language. In this paper, a cooperation framework that connects deep learning techniques and a symbolic planner for heterogeneous robots is proposed. The framework is largely composed of the scene understanding engine, planning agent, and knowledge engine. We employ neural networks for natural-language-based scene understanding to share environmental information among robots. We then generate a sequence of actions for each robot using a planning domain definition language planner. JENA-TDB is used for knowledge acquisition storage. The proposed method is validated using simulation results obtained from one unmanned aerial and three ground vehicles.},
DOI = {10.3390/app9183789}
}



@Article{a12090193,
AUTHOR = {Torres-Sospedra, Joaquín and Nebot, Patricio},
TITLE = {Combining Satellite Images and Cadastral Information for Outdoor Autonomous Mapping and Navigation: A Proof-of-Concept Study in Citric Groves},
JOURNAL = {Algorithms},
VOLUME = {12},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {193},
URL = {https://www.mdpi.com/1999-4893/12/9/193},
ISSN = {1999-4893},
ABSTRACT = {The development of robotic applications for agricultural environments has several problems which are not present in the robotic systems used for indoor environments. Some of these problems can be solved with an efficient navigation system. In this paper, a new system is introduced to improve the navigation tasks for those robots which operate in agricultural environments. Concretely, the paper focuses on the problem related to the autonomous mapping of agricultural parcels (i.e., an orange grove). The map created by the system will be used to help the robots navigate into the parcel to perform maintenance tasks such as weed removal, harvest, or pest inspection. The proposed system connects to a satellite positioning service to obtain the real coordinates where the robotic system is placed. With these coordinates, the parcel information is downloaded from an online map service in order to autonomously obtain a map of the parcel in a readable format for the robot. Finally, path planning is performed by means of Fast Marching techniques using the robot or a team of two robots. This paper introduces the proof-of-concept and describes all the necessary steps and algorithms to obtain the path planning just from the initial coordinates of the robot.},
DOI = {10.3390/a12090193}
}



@Article{s19183917,
AUTHOR = {Fan, Shurui and Li, Zirui and Xia, Kewen and Hao, Dongxia},
TITLE = {Quantitative and Qualitative Analysis of Multicomponent Gas Using Sensor Array},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3917},
URL = {https://www.mdpi.com/1424-8220/19/18/3917},
ISSN = {1424-8220},
ABSTRACT = {The gas sensor array has long been a major tool for measuring gas due to its high sensitivity, quick response, and low power consumption. This goal, however, faces a difficult challenge because of the cross-sensitivity of the gas sensor. This paper presents a novel gas mixture analysis method for gas sensor array applications. The features extracted from the raw data utilizing principal component analysis (PCA) were used to complete random forest (RF) modeling, which enabled qualitative identification. Support vector regression (SVR), optimized by the particle swarm optimization (PSO) algorithm, was used to select hyperparameters C and &gamma; to establish the optimal regression model for the purpose of quantitative analysis. Utilizing the dataset, we evaluated the effectiveness of our approach. Compared with logistic regression (LR) and support vector machine (SVM), the average recognition rate of PCA combined with RF was the highest (97%). The fitting effect of SVR optimized by PSO for gas concentration was better than that of SVR and solved the problem of hyperparameters selection.},
DOI = {10.3390/s19183917}
}



@Article{s19183935,
AUTHOR = {Liu, Xiaolei and Liu, Liansheng and Wang, Lulu and Guo, Qing and Peng, Xiyuan},
TITLE = {Performance Sensing Data Prediction for an Aircraft Auxiliary Power Unit Using the Optimized Extreme Learning Machine},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3935},
URL = {https://www.mdpi.com/1424-8220/19/18/3935},
ISSN = {1424-8220},
ABSTRACT = {The aircraft auxiliary power unit (APU) is responsible for environmental control in the cabin and the main engines starting the aircraft. The prediction of its performance sensing data is significant for condition-based maintenance. As a complex system, its performance sensing data have a typically nonlinear feature. In order to monitor this process, a model with strong nonlinear fitting ability needs to be formulated. A neural network has advantages of solving a nonlinear problem. Compared with the traditional back propagation neural network algorithm, an extreme learning machine (ELM) has features of a faster learning speed and better generalization performance. To enhance the training of the neural network with a back propagation algorithm, an ELM is employed to predict the performance sensing data of the APU in this study. However, the randomly generated weights and thresholds of the ELM often may result in unstable prediction results. To address this problem, a restricted Boltzmann machine (RBM) is utilized to optimize the ELM. In this way, a stable performance parameter prediction model of the APU can be obtained and better performance parameter prediction results can be achieved. The proposed method is evaluated by the real APU sensing data of China Southern Airlines Company Limited Shenyang Maintenance Base. Experimental results show that the optimized ELM with an RBM is more stable and can obtain more accurate prediction results.},
DOI = {10.3390/s19183935}
}



@Article{ijgi8090409,
AUTHOR = {Tan, Yumin and Li, Yunxin},
TITLE = {UAV Photogrammetry-Based 3D Road Distress Detection},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {409},
URL = {https://www.mdpi.com/2220-9964/8/9/409},
ISSN = {2220-9964},
ABSTRACT = {The timely and proper rehabilitation of damaged roads is essential for road maintenance, and an effective method to detect road surface distress with high efficiency and low cost is urgently needed. Meanwhile, unmanned aerial vehicles (UAVs), with the advantages of high flexibility, low cost, and easy maneuverability, are a new fascinating choice for road condition monitoring. In this paper, road images from UAV oblique photogrammetry are used to reconstruct road three-dimensional (3D) models, from which road pavement distress is automatically detected and the corresponding dimensions are extracted using the developed algorithm. Compared with a field survey, the detection result presents a high precision with an error of around 1 cm in the height dimension for most cases, demonstrating the potential of the proposed method for future engineering practice.},
DOI = {10.3390/ijgi8090409}
}



@Article{s19183958,
AUTHOR = {Han, Seongkyun and Yoo, Jisang and Kwon, Soonchul},
TITLE = {Real-Time Vehicle-Detection Method in Bird-View Unmanned-Aerial-Vehicle Imagery},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3958},
URL = {https://www.mdpi.com/1424-8220/19/18/3958},
ISSN = {1424-8220},
ABSTRACT = {Vehicle detection is an important research area that provides background information for the diversity of unmanned-aerial-vehicle (UAV) applications. In this paper, we propose a vehicle-detection method using a convolutional-neural-network (CNN)-based object detector. We design our method, DRFBNet300, with a Deeper Receptive Field Block (DRFB) module that enhances the expressiveness of feature maps to detect small objects in the UAV imagery. We also propose the UAV-cars dataset that includes the composition and angular distortion of vehicles in UAV imagery to train our DRFBNet300. Lastly, we propose a Split Image Processing (SIP) method to improve the accuracy of the detection model. Our DRFBNet300 achieves 21 mAP with 45 FPS in the MS COCO metric, which is the highest score compared to other lightweight single-stage methods running in real time. In addition, DRFBNet300, trained on the UAV-cars dataset, obtains the highest AP score at altitudes of 20&ndash;50 m. The gap of accuracy improvement by applying the SIP method became larger when the altitude increases. The DRFBNet300 trained on the UAV-cars dataset with SIP method operates at 33 FPS, enabling real-time vehicle detection.},
DOI = {10.3390/s19183958}
}



@Article{en12183539,
AUTHOR = {Bjaoui, Marwen and Khiari, Brahim and Benadli, Ridha and Memni, Mouad and Sellami, Anis},
TITLE = {Practical Implementation of the Backstepping Sliding Mode Controller MPPT for a PV-Storage Application},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3539},
URL = {https://www.mdpi.com/1996-1073/12/18/3539},
ISSN = {1996-1073},
ABSTRACT = {This study presents a design and an implementation of a robust Maximum Power Point Tracking (MPPT) for a stand-alone photovoltaic (PV) system with battery storage. A new control scheme is applied for the boost converter based on the combination of the adaptive perturb and observe fuzzy logic controller (P&amp;O-FLC) MPPT technique and the backstepping sliding mode control (BS-SMC) approach. The MPPT controller design was used to accurately track the PV operating point to its maximum power point (MPP) under changing climatic conditions. The presented MPPT based on the P&amp;O-FLC technique generates the reference PV voltage and then a cascade control loop type, based on the BS-SMC approach is used. The aims of this approach are applied to regulate the inductor current and then the PV voltage to its reference values. In order to reduce system costs and complexity, a high gain observer (HGO) was designed, based on the model of the PV system, to estimate online the real value of the boost converter&rsquo;s inductor current. The performance and the robustness of the BS-SMC approach are evaluated using a comparative simulation with a conventional proportional integral (PI) controller implemented in the MATLAB/Simulink environment. The obtained results demonstrate that the proposed approach not only provides a near-perfect tracking performance (dynamic response, overshoot, steady-state error), but also offers greater robustness and stability than the conventional PI controller. Experimental results fitted with dSPACE software reveal that the PV module could reach the MPP and achieve the performance and robustness of the designed BS-SMC MPPT controller.},
DOI = {10.3390/en12183539}
}



@Article{rs11182155,
AUTHOR = {Wang, Jie and Simeonova, Sandra and Shahbazi, Mozhdeh},
TITLE = {Orientation- and Scale-Invariant Multi-Vehicle Detection and Tracking from Unmanned Aerial Videos},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2155},
URL = {https://www.mdpi.com/2072-4292/11/18/2155},
ISSN = {2072-4292},
ABSTRACT = {Along with the advancement of light-weight sensing and processing technologies, unmanned aerial vehicles (UAVs) have recently become popular platforms for intelligent traffic monitoring and control. UAV-mounted cameras can capture traffic-flow videos from various perspectives providing a comprehensive insight into road conditions. To analyze the traffic flow from remotely captured videos, a reliable and accurate vehicle detection-and-tracking approach is required. In this paper, we propose a deep-learning framework for vehicle detection and tracking from UAV videos for monitoring traffic flow in complex road structures. This approach is designed to be invariant to significant orientation and scale variations in the videos. The detection procedure is performed by fine-tuning a state-of-the-art object detector, You Only Look Once (YOLOv3), using several custom-labeled traffic datasets. Vehicle tracking is conducted following a tracking-by-detection paradigm, where deep appearance features are used for vehicle re-identification, and Kalman filtering is used for motion estimation. The proposed methodology is tested on a variety of real videos collected by UAVs under various conditions, e.g., in late afternoons with long vehicle shadows, in dawn with vehicles lights being on, over roundabouts and interchange roads where vehicle directions change considerably, and from various viewpoints where vehicles&rsquo; appearance undergo substantial perspective distortions. The proposed tracking-by-detection approach performs efficiently at 11 frames per second on color videos of 2720p resolution. Experiments demonstrated that high detection accuracy could be achieved with an average F1-score of 92.1%. Besides, the tracking technique performs accurately, with an average multiple-object tracking accuracy (MOTA) of 81.3%. The proposed approach also addressed the shortcomings of the state-of-the-art in multi-object tracking regarding frequent identity switching, resulting in a total of only one identity switch over every 305 tracked vehicles.},
DOI = {10.3390/rs11182155}
}



@Article{e21090912,
AUTHOR = {Mei, Wenjuan and Liu, Zhen and Su, Yuanzhang and Du, Li and Huang, Jianguo},
TITLE = {Evolved-Cooperative Correntropy-Based Extreme Learning Machine for Robust Prediction},
JOURNAL = {Entropy},
VOLUME = {21},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {912},
URL = {https://www.mdpi.com/1099-4300/21/9/912},
ISSN = {1099-4300},
ABSTRACT = {In recent years, the correntropy instead of the mean squared error has been widely taken as a powerful tool for enhancing the robustness against noise and outliers by forming the local similarity measurements. However, most correntropy-based models either have too simple descriptions of the correntropy or require too many parameters to adjust in advance, which is likely to cause poor performance since the correntropy fails to reflect the probability distributions of the signals. Therefore, in this paper, a novel correntropy-based extreme learning machine (ELM) called ECC-ELM has been proposed to provide a more robust training strategy based on the newly developed multi-kernel correntropy with the parameters that are generated using cooperative evolution. To achieve an accurate description of the correntropy, the method adopts a cooperative evolution which optimizes the bandwidths by switching delayed particle swarm optimization (SDPSO) and generates the corresponding influence coefficients that minimizes the minimum integrated error (MIE) to adaptively provide the best solution. The simulated experiments and real-world applications show that cooperative evolution can achieve the optimal solution which provides an accurate description on the probability distribution of the current error in the model. Therefore, the multi-kernel correntropy that is built with the optimal solution results in more robustness against the noise and outliers when training the model, which increases the accuracy of the predictions compared with other methods.},
DOI = {10.3390/e21090912}
}



@Article{rs11192212,
AUTHOR = {Salameh, Edward and Frappart, Frédéric and Almar, Rafael and Baptista, Paulo and Heygster, Georg and Lubac, Bertrand and Raucoules, Daniel and Almeida, Luis Pedro and Bergsma, Erwin W. J. and Capo, Sylvain and De Michele, Marcello and Idier, Deborah and Li, Zhen and Marieu, Vincent and Poupardin, Adrien and Silva, Paulo A. and Turki, Imen and Laignel, Benoit},
TITLE = {Monitoring Beach Topography and Nearshore Bathymetry Using Spaceborne Remote Sensing: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2212},
URL = {https://www.mdpi.com/2072-4292/11/19/2212},
ISSN = {2072-4292},
ABSTRACT = {With high anthropogenic pressure and the effects of climate change (e.g., sea level rise) on coastal regions, there is a greater need for accurate and up-to-date information about the topography of these systems. Reliable topography and bathymetry information are fundamental parameters for modelling the morpho-hydrodynamics of coastal areas, for flood forecasting, and for coastal management. Traditional methods such as ground, ship-borne, and airborne surveys suffer from limited spatial coverage and temporal sampling due to logistical constraints and high costs which limit their ability to provide the needed information. The recent advancements of spaceborne remote sensing techniques, along with their ability to acquire data over large spatial areas and to provide high frequency temporal monitoring, has made them very attractive for topography and bathymetry mapping. In this review, we present an overview of the current state of spaceborne-based remote sensing techniques used to estimate the topography and bathymetry of beaches, intertidal, and nearshore areas. We also provide some insights about the potential of these techniques when using data provided by new and future satellite missions.},
DOI = {10.3390/rs11192212}
}



@Article{s19194091,
AUTHOR = {Guo, Qiwei and Chen, Yayong and Tang, Yu and Zhuang, Jiajun and He, Yong and Hou, Chaojun and Chu, Xuan and Zhong, Zhenyu and Luo, Shaoming},
TITLE = {Lychee Fruit Detection Based on Monocular Machine Vision in Orchard Environment},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4091},
URL = {https://www.mdpi.com/1424-8220/19/19/4091},
ISSN = {1424-8220},
ABSTRACT = {Due to the change of illumination environment and overlapping conditions caused by the neighboring fruits and other background objects, the simple application of the traditional machine vision method limits the detection accuracy of lychee fruits in natural orchard environments. Therefore, this research presented a detection method based on monocular machine vision to detect lychee fruits growing in overlapped conditions. Specifically, a combination of contrast limited adaptive histogram equalization (CLAHE), red/blue chromatic mapping, Otsu thresholding and morphology operations were adopted to segment the foreground regions of the lychees. A stepwise method was proposed for extracting individual lychee fruit from the lychee foreground region. The first step in this process was based on the relative position relation of the Hough circle and an equivalent area circle (equal to the area of the potential lychee foreground region) and was designed to distinguish lychee fruits growing in isolated or overlapped states. Then, a process based on the three-point definite circle theorem was performed to extract individual lychee fruits from the foreground regions of overlapped lychee fruit clusters. Finally, to enhance the robustness of the detection method, a local binary pattern support vector machine (LBP-SVM) was adopted to filter out the false positive detections generated by background chaff interferences. The performance of the presented method was evaluated using 485 images captured in a natural lychee orchard in Conghua (Area), Guangzhou. The detection results showed that the recall rate was 86.66%, the precision rate was greater than 87% and the F1-score was 87.07%.},
DOI = {10.3390/s19194091}
}



@Article{rs11192211,
AUTHOR = {Petliak, Helen and Cerovski-Darriau, Corina and Zaliva, Vadim and Stock, Jonathan},
TITLE = {Where’s the Rock: Using Convolutional Neural Networks to Improve Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2211},
URL = {https://www.mdpi.com/2072-4292/11/19/2211},
ISSN = {2072-4292},
ABSTRACT = {While machine learning techniques have been increasingly applied to land cover classification problems, these techniques have not focused on separating exposed bare rock from soil covered areas. Therefore, we built a convolutional neural network (CNN) to differentiate exposed bare rock (rock) from soil cover (other). We made a training dataset by mapping exposed rock at eight test sites across the Sierra Nevada Mountains (California, USA) using USDA’s 0.6 m National Aerial Inventory Program (NAIP) orthoimagery. These areas were then used to train and test the CNN. The resulting machine learning approach classifies bare rock in NAIP orthoimagery with a 0.95     F 1     score. Comparatively, the classical OBIA approach gives only a 0.84     F 1     score. This is an improvement over existing land cover maps, which underestimate rock by almost 90%. The resulting CNN approach is likely scalable but dependent on high-quality imagery and high-performance algorithms using representative training sets informed by expert mapping. As image quality and quantity continue to increase globally, machine learning models that incorporate high-quality training data informed by geologic, topographic, or other topical maps may be applied to more effectively identify exposed rock in large image collections.},
DOI = {10.3390/rs11192211}
}



@Article{robotics8040082,
AUTHOR = {Abouheaf, Mohammed and Gueaieb, Wail and Spinello, Davide},
TITLE = {Online Multi-Objective Model-Independent Adaptive Tracking Mechanism for Dynamical Systems},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {82},
URL = {https://www.mdpi.com/2218-6581/8/4/82},
ISSN = {2218-6581},
ABSTRACT = {The optimal tracking problem is addressed in the robotics literature by using a variety of robust and adaptive control approaches. However, these schemes are associated with implementation limitations such as applicability in uncertain dynamical environments with complete or partial model-based control structures, complexity and integrity in discrete-time environments, and scalability in complex coupled dynamical systems. An online adaptive learning mechanism is developed to tackle the above limitations and provide a generalized solution platform for a class of tracking control problems. This scheme minimizes the tracking errors and optimizes the overall dynamical behavior using simultaneous linear feedback control strategies. Reinforcement learning approaches based on value iteration processes are adopted to solve the underlying Bellman optimality equations. The resulting control strategies are updated in real time in an interactive manner without requiring any information about the dynamics of the underlying systems. Means of adaptive critics are employed to approximate the optimal solving value functions and the associated control strategies in real time. The proposed adaptive tracking mechanism is illustrated in simulation to control a flexible wing aircraft under uncertain aerodynamic learning environment.},
DOI = {10.3390/robotics8040082}
}



@Article{electronics8101077,
AUTHOR = {Bai, Guoxing and Meng, Yu and Liu, Li and Luo, Weidong and Gu, Qing and Liu, Li},
TITLE = {Review and Comparison of Path Tracking Based on Model Predictive Control},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1077},
URL = {https://www.mdpi.com/2079-9292/8/10/1077},
ISSN = {2079-9292},
ABSTRACT = {Recently, model predictive control (MPC) is increasingly applied to path tracking of mobile devices, such as mobile robots. The characteristics of these MPC-based controllers are not identical due to the different approaches taken during design. According to the differences in the prediction models, we believe that the existing MPC-based path tracking controllers can be divided into four categories. We named them linear model predictive control (LMPC), linear error model predictive control (LEMPC), nonlinear model predictive control (NMPC), and nonlinear error model predictive control (NEMPC). Subsequently, we built these four controllers for the same mobile robot and compared them. By comparison, we got some conclusions. The real-time performance of LMPC and LEMPC is good, but they are less robust to reference paths and positioning errors. NMPC performs well when the reference velocity is high and the radius of the reference path is small. It is also robust to positioning errors. However, the real-time performance of NMPC is slightly worse. NEMPC has many disadvantages. Like LMPC and LEMPC, it performs poorly when the reference velocity is high and the radius of the reference path is small. Its real-time performance is also not good enough.},
DOI = {10.3390/electronics8101077}
}



