
@Article{agriculture10040112,
AUTHOR = {Przybylak, Andrzej and Kozłowski, Radosław and Osuch, Ewa and Osuch, Andrzej and Rybacki, Piotr and Przygodziński, Przemysław},
TITLE = {Quality Evaluation of Potato Tubers Using Neural Image Analysis Method},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {112},
URL = {https://www.mdpi.com/2077-0472/10/4/112},
ISSN = {2077-0472},
ABSTRACT = {This paper describes the research aimed at developing an effective quality assessment method for potato tubers using neural image analysis techniques. Nowadays, the methods used to identify damage and diseases are time-consuming, require specialized knowledge, and often rely on subjective judgment. This study showed the use of the developed neural model as a tool supporting the evaluation of potato tubers during the sorting process in the storage room.},
DOI = {10.3390/agriculture10040112}
}



@Article{sym12040561,
AUTHOR = {Xue, Yiming and Zeng, Dan and Chen, Fansheng and Wang, Yueming and Zhang, Zhijiang},
TITLE = {A New Dataset and Deep Residual Spectral Spatial Network for Hyperspectral Image Classification},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {561},
URL = {https://www.mdpi.com/2073-8994/12/4/561},
ISSN = {2073-8994},
ABSTRACT = {Due to the limited varieties and sizes of existing public hyperspectral image (HSI) datasets, the classification accuracies are higher than 99% with convolutional neural networks (CNNs). In this paper, we presented a new HSI dataset named Shandong Feicheng, whose size and pixel quantity are much larger. It also has a larger intra-class variance and a smaller inter-class variance. State-of-the-art methods were compared on it to verify its diversity. Otherwise, to reduce overfitting caused by the imbalance between high dimension and small quantity of labeled HSI data, existing CNNs for HSI classification are relatively shallow and suffer from low capacity of feature learning. To solve this problem, we proposed an HSI classification framework named deep residual spectral spatial setwork (DRSSN). By using shortcut connection structure, which is an asymmetry structure, DRSSN can be deeper to extract features with better discrimination. In addition, to alleviate insufficient training caused by unbalanced sample sizes between easily and hard classified samples, we proposed a novel training loss function named sample balanced loss, which allocated weights to the losses of samples according to their prediction confidence. Experimental results on two popular datasets and our proposed dataset showed that our proposed network could provide competitive results compared with state-of-the-art methods.},
DOI = {10.3390/sym12040561}
}



@Article{rs12071176,
AUTHOR = {Lin, Yukun and Zhu, Zhe and Guo, Wenxuan and Sun, Yazhou and Yang, Xiaoyuan and Kovalskyy, Valeriy},
TITLE = {Continuous Monitoring of Cotton Stem Water Potential using Sentinel-2 Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1176},
URL = {https://www.mdpi.com/2072-4292/12/7/1176},
ISSN = {2072-4292},
ABSTRACT = {Monitoring cotton status during the growing season is critical in increasing production efficiency. The water status in cotton is a key factor for yield and cotton quality. Stem water potential (SWP) is a precise indicator for assessing cotton water status. Satellite remote sensing is an effective approach for monitoring cotton growth at a large scale. The aim of this study is to estimate cotton water stress at a high temporal frequency and at a large scale. In this study, we measured midday SWP samples according to the acquisition dates of Sentinel-2 images and used them to build linear-regression-based and machine-learning-based models to estimate cotton water stress during the growing season (June to August, 2018). For the linear-regression-based method, we estimated SWP based on different Sentinel-2 spectral bands and vegetation indices, where the normalized difference index 45 (NDI45) achieved the best performance (R2 = 0.6269; RMSE = 3.6802 (-1*swp (bars))). For the machine-learning-based method, we used random forest regression to estimate SWP and received even better results (R2 = 0.6709; RMSE = 3.3742 (-1*swp (bars))). To find the best selection of input variables for the machine-learning-based approach, we tried three different data input datasets, including (1) 9 original spectral bands (e.g., blue, green, red, red edge, near infrared (NIR), and shortwave infrared (SWIR)), (2) 21 vegetation indices, and (3) a combination of original Sentinel-2 spectral bands and vegetation indices. The highest accuracy was achieved when only the original spectral bands were used. We also found the SWIR and red edge band were the most important spectral bands, and the vegetation indices based on red edge and NIR bands were particularly helpful. Finally, we applied the best approach for the linear-regression-based and the machine-learning-based methods to generate cotton water potential maps at a large scale and high temporal frequency. Results suggests that the methods developed here has the potential for continuous monitoring of SWP at large scales and the machine-learning-based method is preferred.},
DOI = {10.3390/rs12071176}
}



@Article{app10072528,
AUTHOR = {Deng, Lu and Chu, Hong-Hu and Shi, Peng and Wang, Wei and Kong, Xuan},
TITLE = {Region-Based CNN Method with Deformable Modules for Visually Classifying Concrete Cracks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2528},
URL = {https://www.mdpi.com/2076-3417/10/7/2528},
ISSN = {2076-3417},
ABSTRACT = {Cracks are often the most intuitive indicators for assessing the condition of in-service structures. Intelligent detection methods based on regular convolutional neural networks (CNNs) have been widely applied to the field of crack detection in recently years; however, these methods exhibit unsatisfying performance on the detection of out-of-plane cracks. To overcome this drawback, a new type of region-based CNN (R-CNN) crack detector with deformable modules is proposed in the present study. The core idea of the method is to replace the traditional regular convolution and pooling operation with a deformable convolution operation and a deformable pooling operation. The idea is implemented on three different regular detectors, namely the Faster R-CNN, region-based fully convolutional networks (R-FCN), and feature pyramid network (FPN)-based Faster R-CNN. To examine the advantages of the proposed method, the results obtained from the proposed detector and corresponding regular detectors are compared. The results show that the addition of deformable modules improves the mean average precisions (mAPs) achieved by the Faster R-CNN, R-FCN, and FPN-based Faster R-CNN for crack detection. More importantly, adding deformable modules enables these detectors to detect the out-of-plane cracks that are difficult for regular detectors to detect.},
DOI = {10.3390/app10072528}
}



@Article{en13071768,
AUTHOR = {Zhao, Xiaochun and Huang, Xianghua and Xia, Tianqian},
TITLE = {Research on Modeling of a Micro Variable-Pitch Turboprop Engine Based on Rig Test Data},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1768},
URL = {https://www.mdpi.com/1996-1073/13/7/1768},
ISSN = {1996-1073},
ABSTRACT = {Exact component characteristics are required for establishing an accurate component level aeroengine model. When component characteristics is lacking, the dynamic coefficient method based on test data, is suitable for establishing a single-input and single-output aeroengine model. When it is applied to build multiple-input, multiple-output aeroengine models, some parameters are assumed to be unchanged, which causes large error. An improved modeling method based on rig data is proposed to establish a double-input, double-output model for a micro variable-pitch turboprop engine. The input variables are fuel flow and pitch angle, and the output variables are rotational speeds of the core engine and the propeller. First, in order to gather modeling data, a test bench is designed and rig tests are carried out. Then, two conclusions are obtained by analyzing the rig data, based on which, the power turbine output is taken as the function of the core speed and the propeller speed. The established model has the property that the input variables can vary arbitrarily within the defined domain, without any restriction to the output variables. Simulation results showed that the model has a high dynamic and steady-state accuracy. The maximum error was less than 8%. The real-time performance was greatly improved, compared to the component level model.},
DOI = {10.3390/en13071768}
}



@Article{s20072068,
AUTHOR = {Debeunne, César and Vivet, Damien},
TITLE = {A Review of Visual-LiDAR Fusion based Simultaneous Localization and Mapping},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2068},
URL = {https://www.mdpi.com/1424-8220/20/7/2068},
ISSN = {1424-8220},
ABSTRACT = {Autonomous navigation requires both a precise and robust mapping and localization solution. In this context, Simultaneous Localization and Mapping (SLAM) is a very well-suited solution. SLAM is used for many applications including mobile robotics, self-driving cars, unmanned aerial vehicles, or autonomous underwater vehicles. In these domains, both visual and visual-IMU SLAM are well studied, and improvements are regularly proposed in the literature. However, LiDAR-SLAM techniques seem to be relatively the same as ten or twenty years ago. Moreover, few research works focus on vision-LiDAR approaches, whereas such a fusion would have many advantages. Indeed, hybridized solutions offer improvements in the performance of SLAM, especially with respect to aggressive motion, lack of light, or lack of visual features. This study provides a comprehensive survey on visual-LiDAR SLAM. After a summary of the basic idea of SLAM and its implementation, we give a complete review of the state-of-the-art of SLAM research, focusing on solutions using vision, LiDAR, and a sensor fusion of both modalities.},
DOI = {10.3390/s20072068}
}



@Article{s20072069,
AUTHOR = {Feng, Chuncheng and Zhang, Hua and Wang, Haoran and Wang, Shuang and Li, Yonglong},
TITLE = {Automatic Pixel-Level Crack Detection on Dam Surface Using Deep Convolutional Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2069},
URL = {https://www.mdpi.com/1424-8220/20/7/2069},
ISSN = {1424-8220},
ABSTRACT = {Crack detection on dam surfaces is an important task for safe inspection of hydropower stations. More and more object detection methods based on deep learning are being applied to crack detection. However, most of the methods can only achieve the classification and rough location of cracks. Pixel-level crack detection can provide more intuitive and accurate detection results for dam health assessment. To realize pixel-level crack detection, a method of crack detection on dam surface (CDDS) using deep convolution network is proposed. First, we use an unmanned aerial vehicle (UAV) to collect dam surface images along a predetermined trajectory. Second, raw images are cropped. Then crack regions are manually labelled on cropped images to create the crack dataset, and the architecture of CDDS network is designed. Finally, the CDDS network is trained, validated and tested using the crack dataset. To validate the performance of the CDDS network, the predicted results are compared with ResNet152-based, SegNet, UNet and fully convolutional network (FCN). In terms of crack segmentation, the recall, precision, F-measure and IoU are 80.45%, 80.31%, 79.16%, and 66.76%. The results on test dataset show that the CDDS network has better performance for crack detection of dam surfaces.},
DOI = {10.3390/s20072069}
}



@Article{asi3020019,
AUTHOR = {Kounas, Dimitrios and Voutyras, Orfefs and Palaiokrassas, Georgios and Litke, Antonios and Varvarigou, Theodora},
TITLE = {QuietPlace: An Ultrasound-Based Proof of Location Protocol with Strong Identities},
JOURNAL = {Applied System Innovation},
VOLUME = {3},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {19},
URL = {https://www.mdpi.com/2571-5577/3/2/19},
ISSN = {2571-5577},
ABSTRACT = {Location-based services are becoming extremely popular due to the widespread use of smartphones and other mobile and portable devices. These services mainly rely on the sincerity of users, who can spoof the location they report to them. For applications with higher security requirements, the user should be unable to report a location different than the real one. Proof of Location protocols provide a solution to secure localization by validating the device&rsquo;s location with the help of nearby nodes. We propose QuietPlace, a novel protocol that is based on ultrasound and provides strong identities, proving the location of the owner of a device, without exposing though their identity. QuietPlace provides unforgeable proof that is able to resist to various attacks while respecting the users&rsquo; privacy. It can work regardless of certificate authority and location-based service and is able to support trust schemas that evaluate the participants&rsquo; behavior. We implement and validate the protocol for Android devices, showing that ultrasound-based profiles offer a better performance in terms of maximum receiving distance than audible profiles, and discuss its strengths and weaknesses, making suggestions about future work.},
DOI = {10.3390/asi3020019}
}



@Article{e22040419,
AUTHOR = {Zhao, Zhenbing and Qi, Hongyu and Fan, Xiaoqing and Xu, Guozhi and Qi, Yincheng and Zhai, Yongjie and Zhang, Ke},
TITLE = {Image Representation Method Based on Relative Layer Entropy for Insulator Recognition},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {419},
URL = {https://www.mdpi.com/1099-4300/22/4/419},
ISSN = {1099-4300},
ABSTRACT = {Deep convolutional neural networks (DCNNs) with alternating convolutional, pooling and decimation layers are widely used in computer vision, yet current works tend to focus on deeper networks with many layers and neurons, resulting in a high computational complexity. However, the recognition task is still challenging for insufficient and uncomprehensive object appearance and training sample types such as infrared insulators. In view of this, more attention is focused on the application of a pretrained network for image feature representation, but the rules on how to select the feature representation layer are scarce. In this paper, we proposed a new concept, the layer entropy and relative layer entropy, which can be referred to as an image representation method based on relative layer entropy (IRM_RLE). It was designed to excavate the most suitable convolution layer for image recognition. First, the image was fed into an ImageNet pretrained DCNN model, and deep convolutional activations were extracted. Then, the appropriate feature layer was selected by calculating the layer entropy and relative layer entropy of each convolution layer. Finally, the number of the feature map was selected according to the importance degree and the feature maps of the convolution layer, which were vectorized and pooled by VLAD (vector of locally aggregated descriptors) coding and quantifying for final image representation. The experimental results show that the proposed approach performs competitively against previous methods across all datasets. Furthermore, for the indoor scenes and actions datasets, the proposed approach outperforms the state-of-the-art methods.},
DOI = {10.3390/e22040419}
}



@Article{rs12071210,
AUTHOR = {Mallinis, Giorgos and Chrysafis, Irene and Korakis, Georgios and Pana, Eleanna and Kyriazopoulos, Apostolos P.},
TITLE = {A Random Forest Modelling Procedure for a Multi-Sensor Assessment of Tree Species Diversity},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1210},
URL = {https://www.mdpi.com/2072-4292/12/7/1210},
ISSN = {2072-4292},
ABSTRACT = {Earth observation data can provide important information for tree species diversity mapping and monitoring. The relatively recent advances in remote sensing data characteristics and processing systems elevate the potential of satellite imagery for providing accurate, timely, consistent, and robust spatially explicit estimates of tree species diversity over forest ecosystems. This study was conducted in Northern Pindos National Park, the largest terrestrial park in Greece and aimed to assess the potential of four satellite sensors with different instrumental characteristics, for the estimation of tree diversity. Through field measurements, we originally quantified two diversity indices, namely the Shannon diversity index (H&rsquo;) and Simpson&rsquo;s diversity (D1). Random forest regression models were developed for associating remotely sensed spectral signal with tree species diversity within the area. The models generated from the use of the WorldView-2 image were the most accurate with a coefficient of determination of up to 0.44 for H&rsquo; and 0.37 for D1. The Sentinel-2 -based models of tree species diversity performed slightly worse, but were better than the Landsat-8 and RapidEye models. The coefficient of variation quantifying internal variability of spectral values within each plot provided little or no usage for improving the modelling accuracy. Our results suggest that very-high-spatial-resolution imagery provides the most important information for the assessment of tree species diversity in heterogeneous Mediterranean ecosystems.},
DOI = {10.3390/rs12071210}
}



@Article{rs12071213,
AUTHOR = {Raza, Muhammad M. and Harding, Chris and Liebman, Matt and Leandro, Leonor F.},
TITLE = {Exploring the Potential of High-Resolution Satellite Imagery for the Detection of Soybean Sudden Death Syndrome},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1213},
URL = {https://www.mdpi.com/2072-4292/12/7/1213},
ISSN = {2072-4292},
ABSTRACT = {Sudden death syndrome (SDS) is one of the major yield-limiting soybean diseases in the Midwestern United States. Effective management for SDS requires accurate detection in soybean fields. Since traditional scouting methods are time-consuming, labor-intensive, and often destructive, alternative methods to monitor SDS in large soybean fields are needed. This study explores the potential of using high-resolution (3 m) PlanetScope satellite imagery for detection of SDS using the random forest classification algorithm. Image data from blue, green, red, and near-infrared (NIR) spectral bands, the calculated normalized difference vegetation index (NDVI), and crop rotation information were used to detect healthy and SDS-infected quadrats in a soybean field experiment with different rotation treatments, located in Boone County, Iowa. Datasets collected during the 2016, 2017, and 2018 soybean growing seasons were analyzed. The results indicate that spectral features, when combined with ground-based information, can detect areas in soybean plots that are at risk for disease, even before foliar symptoms develop. The classification of healthy and diseased soybean quadrats was &gt;75% accurate and the area under the receiver operating characteristic curve (AUROC) was &gt;70%. Our results indicate that high-resolution satellite imagery and random forest analyses have the potential to detect SDS in soybean fields, and that this approach may facilitate large-scale monitoring of SDS (and possibly other economically important soybean diseases). It may also be useful for guiding recommendations for site-specific management in current and future seasons.},
DOI = {10.3390/rs12071213}
}



@Article{s20072125,
AUTHOR = {Silveira Kupssinskü, Lucas and Thomassim Guimarães, Tainá and Menezes de Souza, Eniuce and C. Zanotta, Daniel and Roberto Veronez, Mauricio and Gonzaga, Luiz and Mauad, Frederico Fábio},
TITLE = {A Method for Chlorophyll-a and Suspended Solids Prediction through Remote Sensing and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2125},
URL = {https://www.mdpi.com/1424-8220/20/7/2125},
ISSN = {1424-8220},
ABSTRACT = {Total Suspended Solids (TSS) and chlorophyll-a concentration are two critical parameters to monitor water quality. Since directly collecting samples for laboratory analysis can be expensive, this paper presents a methodology to estimate this information through remote sensing and Machine Learning (ML) techniques. TSS and chlorophyll-a are optically active components, therefore enabling measurement by remote sensing. Two study cases in distinct water bodies are performed, and those cases use different spatial resolution data from Sentinel-2 spectral images and unmanned aerial vehicles together with laboratory analysis data. In consonance with the methodology, supervised ML algorithms are trained to predict the concentration of TSS and chlorophyll-a. The predictions are evaluated separately in both study areas, where both TSS and chlorophyll-a models achieved R-squared values above 0.8.},
DOI = {10.3390/s20072125}
}



@Article{s20072126,
AUTHOR = {Barbedo, Jayme Garcia Arnal and Koenigkan, Luciano Vieira and Santos, Patrícia Menezes and Ribeiro, Andrea Roberto Bueno},
TITLE = {Counting Cattle in UAV Images—Dealing with Clustered Animals and Animal/Background Contrast Changes},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2126},
URL = {https://www.mdpi.com/1424-8220/20/7/2126},
ISSN = {1424-8220},
ABSTRACT = {The management of livestock in extensive production systems may be challenging, especially in large areas. Using Unmanned Aerial Vehicles (UAVs) to collect images from the area of interest is quickly becoming a viable alternative, but suitable algorithms for extraction of relevant information from the images are still rare. This article proposes a method for counting cattle which combines a deep learning model for rough animal location, color space manipulation to increase contrast between animals and background, mathematical morphology to isolate the animals and infer the number of individuals in clustered groups, and image matching to take into account image overlap. Using Nelore and Canchim breeds as a case study, the proposed approach yields accuracies over 90% under a wide variety of conditions and backgrounds.},
DOI = {10.3390/s20072126}
}



@Article{ijgi9040238,
AUTHOR = {Xu, Zhiqiang and Chen, Yumin and Yang, Fan and Chu, Tianyou and Zhou, Hongyan},
TITLE = {A Postearthquake Multiple Scene Recognition Model Based on Classical SSD Method and Transfer Learning},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {238},
URL = {https://www.mdpi.com/2220-9964/9/4/238},
ISSN = {2220-9964},
ABSTRACT = {The recognition of postearthquake scenes plays an important role in postearthquake rescue and reconstruction. To overcome the over-reliance on expert visual interpretation and the poor recognition performance of traditional machine learning in postearthquake scene recognition, this paper proposes a postearthquake multiple scene recognition (PEMSR) model based on the classical deep learning Single Shot MultiBox Detector (SSD) method. In this paper, a labeled postearthquake scenes dataset is constructed by segmenting acquired remote sensing images, which are classified into six categories: landslide, houses, ruins, trees, clogged and ponding. Due to the insufficiency and imbalance of the original dataset, transfer learning and a data augmentation and balancing strategy are utilized in the PEMSR model. To evaluate the PEMSR model, the evaluation metrics of precision, recall and F1 score are used in the experiment. Multiple experimental test results demonstrate that the PEMSR model shows a stronger performance in postearthquake scene recognition. The PEMSR model improves the detection accuracy of each scene compared with SSD by transfer learning and data augmentation strategy. In addition, the average detection time of the PEMSR model only needs 0.4565s, which is far less than the 8.3472s of the traditional Histogram of Oriented Gradient + Support Vector Machine (HOG+SVM) method.},
DOI = {10.3390/ijgi9040238}
}



@Article{jmse8040274,
AUTHOR = {Pan, Xinliang and Jiang, Tao and Zhang, Zhen and Sui, Baikai and Liu, Chenxi and Zhang, Linjing},
TITLE = {A New Method for Extracting Laver Culture Carriers Based on Inaccurate Supervised Classification with FCN-CRF},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {8},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {274},
URL = {https://www.mdpi.com/2077-1312/8/4/274},
ISSN = {2077-1312},
ABSTRACT = {Timely monitoring of marine aquaculture has considerable significance for marine ecological protection and maritime safety and security. Considering that supervised learning needs to rely on a large number of training samples and the characteristics of intensive and regular distribution of the laver aquaculture zone, in this paper, an inaccurate supervised classification model based on fully convolutional neural network and conditional random filed (FCN-CRF) is designed for the study of a laver aquaculture zone in Lianyungang, Jiangsu Province. The proposed model can extract the aquaculture zone and calculate the area and quantity of laver aquaculture net simultaneously. The FCN is used to extract the laver aquaculture zone by roughly making the training label. Then, the CRF is used to extract the isolated laver aquaculture net with high precision. The results show that the     k a p p a     coefficient of the proposed model is 0.984, the      F 1      is 0.99, and the recognition effect is outstanding. For label production, the fault tolerance rate is high and does not affect the final classification accuracy, thereby saving more label production time. The findings provide a data basis for future aquaculture yield estimation and offshore resource planning as well as technical support for marine ecological supervision and marine traffic management.},
DOI = {10.3390/jmse8040274}
}



@Article{rs12081242,
AUTHOR = {Chatterjee, Sumanta and Huang, Jingyi and Hartemink, Alfred E.},
TITLE = {Establishing an Empirical Model for Surface Soil Moisture Retrieval at the U.S. Climate Reference Network Using Sentinel-1 Backscatter and Ancillary Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1242},
URL = {https://www.mdpi.com/2072-4292/12/8/1242},
ISSN = {2072-4292},
ABSTRACT = {Progress in sensor technologies has allowed real-time monitoring of soil water. It is a challenge to model soil water content based on remote sensing data. Here, we retrieved and modeled surface soil moisture (SSM) at the U.S. Climate Reference Network (USCRN) stations using Sentinel-1 backscatter data from 2016 to 2018 and ancillary data. Empirical machine learning models were established between soil water content measured at the USCRN stations with Sentinel-1 data from 2016 to 2017, the National Land Cover Dataset, terrain parameters, and Polaris soil data, and were evaluated in 2018 at the same USCRN stations. The Cubist model performed better than the multiple linear regression (MLR) and Random Forest (RF) model (R2 = 0.68 and RMSE = 0.06 m3 m-3 for validation). The Cubist model performed best in Shrub/Scrub, followed by Herbaceous and Cultivated Crops but poorly in Hay/Pasture. The success of SSM retrieval was mostly attributed to soil properties, followed by Sentinel-1 backscatter data, terrain parameters, and land cover. The approach shows the potential for retrieving SSM using Sentinel-1 data in a combination of high-resolution ancillary data across the conterminous United States (CONUS). Future work is required to improve the model performance by including more SSM network measurements, assimilating Sentinel-1 data with other microwave, optical and thermal remote sensing products. There is also a need to improve the spatial resolution and accuracy of land surface parameter products (e.g., soil properties and terrain parameters) at the regional and global scales.},
DOI = {10.3390/rs12081242}
}



@Article{rs12081288,
AUTHOR = {G. Braga, José R. and Peripato, Vinícius and Dalagnol, Ricardo and P. Ferreira, Matheus and Tarabalka, Yuliya and O. C. Aragão, Luiz E. and F. de Campos Velho, Haroldo and Shiguemori, Elcio H. and Wagner, Fabien H.},
TITLE = {Tree Crown Delineation Algorithm Based on a Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1288},
URL = {https://www.mdpi.com/2072-4292/12/8/1288},
ISSN = {2072-4292},
ABSTRACT = {Tropical forests concentrate the largest diversity of species on the planet and play a key role in maintaining environmental processes. Due to the importance of those forests, there is growing interest in mapping their components and getting information at an individual tree level to conduct reliable satellite-based forest inventory for biomass and species distribution qualification. Individual tree crown information could be manually gathered from high resolution satellite images; however, to achieve this task at large-scale, an algorithm to identify and delineate each tree crown individually, with high accuracy, is a prerequisite. In this study, we propose the application of a convolutional neural network&mdash;Mask R-CNN algorithm&mdash;to perform the tree crown detection and delineation. The algorithm uses very high-resolution satellite images from tropical forests. The results obtained are promising&mdash;the     R e c a l l    ,     P r e c i s i o n    , and     F 1     score values obtained were were     0.81    ,     0.91    , and     0.86    , respectively. In the study site, the total of tree crowns delineated was     59,062    . These results suggest that this algorithm can be used to assist the planning and conduction of forest inventories. As the algorithm is based on a Deep Learning approach, it can be systematically trained and used for other regions.},
DOI = {10.3390/rs12081288}
}



@Article{rs12081294,
AUTHOR = {Miyoshi, Gabriela Takahashi and Arruda, Mauro dos Santos and Osco, Lucas Prado and Marcato Junior, José and Gonçalves, Diogo Nunes and Imai, Nilton Nobuhiro and Tommaselli, Antonio Maria Garcia and Honkavaara, Eija and Gonçalves, Wesley Nunes},
TITLE = {A Novel Deep Learning Method to Identify Single Tree Species in UAV-Based Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1294},
URL = {https://www.mdpi.com/2072-4292/12/8/1294},
ISSN = {2072-4292},
ABSTRACT = {Deep neural networks are currently the focus of many remote sensing approaches related to forest management. Although they return satisfactory results in most tasks, some challenges related to hyperspectral data remain, like the curse of data dimensionality. In forested areas, another common problem is the highly-dense distribution of trees. In this paper, we propose a novel deep learning approach for hyperspectral imagery to identify single-tree species in highly-dense areas. We evaluated images with 25 spectral bands ranging from 506 to 820 nm taken over a semideciduous forest of the Brazilian Atlantic biome. We included in our network&rsquo;s architecture a band combination selection phase. This phase learns from multiple combinations between bands which contributed the most for the tree identification task. This is followed by a feature map extraction and a multi-stage model refinement of the confidence map to produce accurate results of a highly-dense target. Our method returned an f-measure, precision and recall values of 0.959, 0.973, and 0.945, respectively. The results were superior when compared with a principal component analysis (PCA) approach. Compared to other learning methods, ours estimate a combination of hyperspectral bands that most contribute to the mentioned task within the network&rsquo;s architecture. With this, the proposed method achieved state-of-the-art performance for detecting and geolocating individual tree-species in UAV-based hyperspectral images in a complex forest.},
DOI = {10.3390/rs12081294}
}



@Article{app10082846,
AUTHOR = {Moselhi, Osama and Bardareh, Hassan and Zhu, Zhenhua},
TITLE = {Automated Data Acquisition in Construction with Remote Sensing Technologies},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2846},
URL = {https://www.mdpi.com/2076-3417/10/8/2846},
ISSN = {2076-3417},
ABSTRACT = {Near real-time tracking of construction operations and timely progress reporting are essential for effective management of construction projects. This does not only mitigate potential negative impact of schedule delays and cost overruns but also helps to improve safety on site. Such timely tracking circumvents the drawbacks of conventional methods for data acquisition, which are manual, labor-intensive, and not reliable enough for various construction purposes. To address these issues, a wide range of automated site data acquisition, including remote sensing (RS) technologies, has been introduced. This review article describes the capabilities and limitations of various scenarios employing RS enabling technologies for localization, with a focus on multi-sensor data fusion models. In particular, we have considered integration of real-time location systems (RTLSs) including GPS and UWB with other sensing technologies such as RFID, WSN, and digital imaging for their use in construction. This integrated use of technologies, along with information models (e.g., BIM models) is expected to enhance the efficiency of automated site data acquisition. It is also hoped that this review will prompt researchers to investigate fusion-based data capturing and processing.},
DOI = {10.3390/app10082846}
}



@Article{s20082355,
AUTHOR = {Redweik, Paula and de Sanjosé Blasco, José Juan and Sánchez-Fernández, Manuel and Atkinson, Alan D. and Martínez Corrales, Luís Francisco},
TITLE = {Tower of Belém (Lisbon)–Status Quo 3D Documentation and Material Origin Determination},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2355},
URL = {https://www.mdpi.com/1424-8220/20/8/2355},
ISSN = {1424-8220},
ABSTRACT = {The Tower of Bel&eacute;m, an early 16th century defense tower located at the mouth of the Tagus river, is the iconic symbol of Lisbon. It belongs to the Bel&eacute;m complex, classified since 1983 as a World Heritage Site by the UNESCO, and it is the second most visited monument in Portugal. On November 1st, 1755, there was a heavy earthquake in Lisbon followed by a tsunami, causing between 60,000 and 100,000 deaths. There is a possibility of a repetition of such a catastrophe, which could bring about the collapse of the structure. This was the reasoning behind the decision to evaluate the Tower of Bel&eacute;m by means of surveys using Terrestrial Laser Scanning and photogrammetry. Until now, there was no high-resolution 3D model of the interior and exterior of the tower. A complete 3D documentation of the state of the Tower was achieved with a cloud of more than 6,200 million 3D points in the ETRS89 PT-TM06 coordinate system. Additionally, measurements were made using a hyperspectral camera and a spectroradiometer to characterize the stone material used in the Tower. The result is a digital 3D representation of the Tower of Bel&eacute;m, and the identification of the quarries that may have been used to extract its stone. The work carried out combines geometrical and material analysis. The methods used may constitute a guide when documenting and intervening in similar heritage elements. Finally, the information contained therein will allow an eventual reconstruction of the Tower in the case of another catastrophe.},
DOI = {10.3390/s20082355}
}



@Article{app10082878,
AUTHOR = {Seo, Jihyun and Ahn, Hanse and Kim, Daewon and Lee, Sungju and Chung, Yongwha and Park, Daihee},
TITLE = {EmbeddedPigDet—Fast and Accurate Pig Detection for Embedded Board Implementations},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2878},
URL = {https://www.mdpi.com/2076-3417/10/8/2878},
ISSN = {2076-3417},
ABSTRACT = {Automated pig monitoring is an important issue in the surveillance environment of a pig farm. For a large-scale pig farm in particular, practical issues such as monitoring cost should be considered but such consideration based on low-cost embedded boards has not yet been reported. Since low-cost embedded boards have more limited computing power than typical PCs and have tradeoffs between execution speed and accuracy, achieving fast and accurate detection of individual pigs for &ldquo;on-device&rdquo; pig monitoring applications is very challenging. Therefore, in this paper, we propose a method for the fast detection of individual pigs by reducing the computational workload of 3 &times; 3 convolution in widely-used, deep learning-based object detectors. Then, in order to recover the accuracy of the &ldquo;light-weight&rdquo; deep learning-based object detector, we generate a three-channel composite image as its input image, through &ldquo;simple&rdquo; image preprocessing techniques. Our experimental results on an NVIDIA Jetson Nano embedded board show that the proposed method can improve the integrated performance of both execution speed and accuracy of widely-used, deep learning-based object detectors, by a factor of up to 8.7.},
DOI = {10.3390/app10082878}
}



@Article{rs12081333,
AUTHOR = {Ayhan, Bulent and Kwan, Chiman},
TITLE = {Tree, Shrub, and Grass Classification Using Only RGB Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1333},
URL = {https://www.mdpi.com/2072-4292/12/8/1333},
ISSN = {2072-4292},
ABSTRACT = {In this work, a semantic segmentation-based deep learning method, DeepLabV3+, is applied to classify three vegetation land covers, which are tree, shrub, and grass using only three band color (RGB) images. DeepLabV3+&rsquo;s detection performance has been studied on low and high resolution datasets that both contain tree, shrub, and grass and some other land cover types. The two datasets are heavily imbalanced where shrub pixels are much fewer than tree and grass pixels. A simple weighting strategy known as median frequency weighting was incorporated into DeepLabV3+ to mitigate the data imbalance issue, which originally used uniform weights. The tree, shrub, grass classification performances are compared when all land cover types are included in the classification and also when classification is limited to the three vegetation classes with both uniform and median frequency weights. Among the three vegetation types, shrub is found to be the most challenging one to classify correctly whereas correct classification accuracy was highest for tree. It is observed that even though the median frequency weighting did not improve the overall accuracy, it resulted in better classification accuracy for the underrepresented classes such as shrub in our case and it also significantly increased the average class accuracy. The classification performance and computation time comparison of DeepLabV3+ with two other pixel-based classification methods on sampled pixels of the three vegetation classes showed that DeepLabV3+ achieves significantly higher accuracy than these methods with a trade-off for longer model training time.},
DOI = {10.3390/rs12081333}
}



@Article{s20082401,
AUTHOR = {Kapuscinski, Tomasz and Szczerba, Piotr and Rogalski, Tomasz and Rzucidlo, Pawel and Szczerba, Zygmunt},
TITLE = {A Vision-Based Method for Determining Aircraft State during Spin Recovery},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2401},
URL = {https://www.mdpi.com/1424-8220/20/8/2401},
ISSN = {1424-8220},
ABSTRACT = {This article proposes a vision-based method of determining in which of the three states, defined in the spin recovery process, is an aircraft. The correct identification of this state is necessary to make the right decisions during the spin recovery maneuver. The proposed solution employs a keypoints displacements analysis in consecutive frames taken from the on-board camera. The idea of voting on the temporary location of the rotation axis and dominant displacement direction was used. The decision about the state is made based on a proposed set of rules employing the histogram spread measure. To validate the method, experiments on flight simulator videos, recorded at varying altitudes and in different lighting, background, and visibility conditions, were carried out. For the selected conditions, the first flight tests were also performed. Qualitative and quantitative assessments were conducted using a multimedia data annotation tool and the Jaccard index, respectively. The proposed approach could be the basis for creating a solution supporting the pilot in the process of aircraft spin recovery and, in the future, the development of an autonomous method.},
DOI = {10.3390/s20082401}
}



@Article{rs12081334,
AUTHOR = {Pham, Tien Dat and Yokoya, Naoto and Xia, Junshi and Ha, Nam Thang and Le, Nga Nhu and Nguyen, Thi Thu Trang and Dao, Thi Huong and Vu, Thuy Thi Phuong and Pham, Tien Duc and Takeuchi, Wataru},
TITLE = {Comparison of Machine Learning Methods for Estimating Mangrove Above-Ground Biomass Using Multiple Source Remote Sensing Data in the Red River Delta Biosphere Reserve, Vietnam},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1334},
URL = {https://www.mdpi.com/2072-4292/12/8/1334},
ISSN = {2072-4292},
ABSTRACT = {This study proposes a hybrid intelligence approach based on an extreme gradient boosting regression and genetic algorithm, namely, the XGBR-GA model, incorporating Sentinel-2, Sentinel-1, and ALOS-2 PALSAR-2 data to estimate the mangrove above-ground biomass (AGB), including small and shrub mangrove patches in the Red River Delta biosphere reserve across the northern coast of Vietnam. We used the novel extreme gradient boosting decision tree (XGBR) technique together with genetic algorithm (GA) optimization for feature selection to construct and verify a mangrove AGB model using data from a field survey of 105 sampling plots conducted in November and December of 2018 and incorporated the dual polarimetric (HH and HV) data of the ALOS-2 PALSAR-2 L-band and the Sentinel-2 multispectral data combined with Sentinel-1 (C-band VV and VH) data. We employed the root-mean-square error (RMSE) and coefficient of determination (R2) to evaluate the performance of the proposed model. The capability of the XGBR-GA model was assessed via a comparison with other machine-learning (ML) techniques, i.e., the CatBoost regression (CBR), gradient boosted regression tree (GBRT), support vector regression (SVR), and random forest regression (RFR) models. The XGBR-GA model yielded a promising result (R2 = 0.683, RMSE = 25.08 Mg&middot;ha&minus;1) and outperformed the four other ML models. The XGBR-GA model retrieved a mangrove AGB ranging from 17 Mg&middot;ha&minus;1 to 142 Mg&middot;ha&minus;1 (with an average of 72.47 Mg&middot;ha&minus;1). Therefore, multisource optical and synthetic aperture radar (SAR) combined with the XGBR-GA model can be used to estimate the mangrove AGB in North Vietnam. The effectiveness of the proposed method needs to be further tested and compared to other mangrove ecosystems in the tropics.},
DOI = {10.3390/rs12081334}
}



@Article{electronics9040689,
AUTHOR = {Kouhdaragh, Vahid and Verde, Francesco and Gelli, Giacinto and Abouei, Jamshid},
TITLE = {On the Application of Machine Learning to the Design of UAV-Based 5G Radio Access Networks},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {689},
URL = {https://www.mdpi.com/2079-9292/9/4/689},
ISSN = {2079-9292},
ABSTRACT = {A groundbreaking design of radio access networks (RANs) is needed to fulfill 5G traffic requirements. To this aim, a cost-effective and flexible strategy consists of complementing terrestrial RANs with unmanned aerial vehicles (UAVs). However, several problems must be solved in order to effectively deploy such UAV-based RANs (U-RANs). Indeed, due to the high complexity and heterogeneity of these networks, model-based design approaches, often relying on restrictive assumptions and constraints, exhibit severe limitation in real-world scenarios. Moreover, design of a set of appropriate protocols for such U-RANs is a highly sophisticated task. In this context, machine learning (ML) emerges as a useful tool to obtain practical and effective solutions. In this paper, we discuss why, how, and which types of ML methods are useful for designing U-RANs, by focusing in particular on supervised and reinforcement learning strategies.},
DOI = {10.3390/electronics9040689}
}



@Article{sym12040676,
AUTHOR = {Alsharif, Mohammed H. and Kelechi, Anabi Hilary and Albreem, Mahmoud A. and Chaudhry, Shehzad Ashraf and Zia, M. Sultan and Kim, Sunghwan},
TITLE = {Sixth Generation (6G) Wireless Networks: Vision, Research Activities, Challenges and Potential Solutions},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {676},
URL = {https://www.mdpi.com/2073-8994/12/4/676},
ISSN = {2073-8994},
ABSTRACT = {The standardization activities of the fifth generation communications are clearly over and deployment has commenced globally. To sustain the competitive edge of wireless networks, industrial and academia synergy have begun to conceptualize the next generation of wireless communication systems (namely, sixth generation, (6G)) aimed at laying the foundation for the stratification of the communication needs of the 2030s. In support of this vision, this study highlights the most promising lines of research from the recent literature in common directions for the 6G project. Its core contribution involves exploring the critical issues and key potential features of 6G communications, including: (i) vision and key features; (ii) challenges and potential solutions; and (iii) research activities. These controversial research topics were profoundly examined in relation to the motivation of their various sub-domains to achieve a precise, concrete, and concise conclusion. Thus, this article will contribute significantly to opening new horizons for future research directions.},
DOI = {10.3390/sym12040676}
}



@Article{su12093501,
AUTHOR = {Lin, Mengyi and Li, Fu-Yuan and Zhou, Haibin},
TITLE = {A Research on the Combination of Oblique Photography and Mobile Applications Based on the Sustainable Development of Tourism},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {3501},
URL = {https://www.mdpi.com/2071-1050/12/9/3501},
ISSN = {2071-1050},
ABSTRACT = {Tourism is one of the world&rsquo;s fastest driving forces of economic development, playing an important role in achieving sustainable development goals. In modern society, mobile social media is a communication and decision-making platform for users and a source of big data information about travel. Obtaining and analyzing travel data can provide customer-oriented information about travel destinations and comprehensive services for both tourism operators and tourists. It has a positive impact on the sustainable development of society, economy, environment, and humanities. Starting with theoretical analysis and empirical research, this study combines social media and oblique photography, conducts a case study of the Pingtan comprehensive experimental area in China, and develops an app about online travelling to provide corresponding information for consumers&rsquo; decisions. This study also discusses the potential value of the app, i.e., assisting the development of smart travel in city, achieving sustainable development of tourism, and contributing to tourism globally.},
DOI = {10.3390/su12093501}
}



@Article{ai1020010,
AUTHOR = {Tang, Ziyang and Liu, Xiang and Chen, Hanlin and Hupy, Joseph and Yang, Baijian},
TITLE = {Deep Learning Based Wildfire Event Object Detection from 4K Aerial Images Acquired by UAS},
JOURNAL = {AI},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {166--179},
URL = {https://www.mdpi.com/2673-2688/1/2/10},
ISSN = {2673-2688},
ABSTRACT = {Unmanned Aerial Systems, hereafter referred to as UAS, are of great use in hazard events such as wildfire due to their ability to provide high-resolution video imagery over areas deemed too dangerous for manned aircraft and ground crews. This aerial perspective allows for identification of ground-based hazards such as spot fires and fire lines, and to communicate this information with fire fighting crews. Current technology relies on visual interpretation of UAS imagery, with little to no computer-assisted automatic detection. With the help of big labeled data and the significant increase of computing power, deep learning has seen great successes on object detection with fixed patterns, such as people and vehicles. However, little has been done for objects, such as spot fires, with amorphous and irregular shapes. Additional challenges arise when data are collected via UAS as high-resolution aerial images or videos; an ample solution must provide reasonable accuracy with low delays. In this paper, we examined 4K (    3840 × 2160    ) videos collected by UAS from a controlled burn and created a set of labeled video sets to be shared for public use. We introduce a coarse-to-fine framework to auto-detect wildfires that are sparse, small, and irregularly-shaped. The coarse detector adaptively selects the sub-regions that are likely to contain the objects of interest while the fine detector passes only the details of the sub-regions, rather than the entire 4K region, for further scrutiny. The proposed two-phase learning therefore greatly reduced time overhead and is capable of maintaining high accuracy. Compared against the real-time one-stage object backbone of YoloV3, the proposed methods improved the mean average precision(mAP) from     0 . 29     to     0 . 67    , with an average inference speed of 7.44 frames per second. Limitations and future work are discussed with regard to the design and the experiment results.},
DOI = {10.3390/ai1020010}
}



@Article{app10093079,
AUTHOR = {Huang, Yi-Qi and Zheng, Jia-Chun and Sun, Shi-Dan and Yang, Cheng-Fu and Liu, Jing},
TITLE = {Optimized YOLOv3 Algorithm and Its Application in Traffic Flow Detections},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {3079},
URL = {https://www.mdpi.com/2076-3417/10/9/3079},
ISSN = {2076-3417},
ABSTRACT = {In the intelligent traffic system, real-time and accurate detections of vehicles in images and video data are very important and challenging work. Especially in situations with complex scenes, different models, and high density, it is difficult to accurately locate and classify these vehicles during traffic flows. Therefore, we propose a single-stage deep neural network YOLOv3-DL, which is based on the Tensorflow framework to improve this problem. The network structure is optimized by introducing the idea of spatial pyramid pooling, then the loss function is redefined, and a weight regularization method is introduced, for that, the real-time detections and statistics of traffic flows can be implemented effectively. The optimization algorithm we use is the DL-CAR data set for end-to-end network training and experiments with data sets under different scenarios and weathers. The analyses of experimental data show that the optimized algorithm can improve the vehicles&rsquo; detection accuracy on the test set by 3.86%. Experiments on test sets in different environments have improved the detection accuracy rate by 4.53%, indicating that the algorithm has high robustness. At the same time, the detection accuracy and speed of the investigated algorithm are higher than other algorithms, indicating that the algorithm has higher detection performance.},
DOI = {10.3390/app10093079}
}



@Article{rs12091411,
AUTHOR = {Risbøl, Ole and Langhammer, Daniel and Schlosser Mauritsen, Esben and Seitsonen, Oula},
TITLE = {Employment, Utilization, and Development of Airborne Laser Scanning in Fenno-Scandinavian Archaeology—A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1411},
URL = {https://www.mdpi.com/2072-4292/12/9/1411},
ISSN = {2072-4292},
ABSTRACT = {This paper gives a presentation of how airborne laser scanning (ALS) has been adopted in archaeology in the North over the period 2005&ndash;2019. Almost two decades have passed since ALS first emerged as a potential tool to add to the archaeologist&rsquo;s toolbox. Soon after, it attracted the attention of researchers within archaeological communities engaged with remote sensing in the Fenno-Scandinavian region. The first archaeological ALS projects gave immediate good results and led to further use, research, and development through new projects that followed various tracks. The bulk of the research and development focused on studying how well-suited ALS is for identifying, mapping, and documenting archaeological features in outfield land, mainly in forested areas. The poor situation in terms of lack of information on archaeological records in outfield areas has been challenging for research and especially for cultural heritage management for a long period of time. Consequently, an obvious direction was to study how ALS-based mapping of cultural features in forests could help to improve the survey situation. This led to various statistical analyses and studies covering research questions related to for instance effects on detection success of laser pulse density, and the size and shape of the targeted features. Substantial research has also been devoted to the development and assessment of semi-automatic detection of archaeological features based on the use of algorithms. This has been studied as an alternative approach to human desk-based visual analyses and interpretations of ALS data. This approach has considerable potential for detecting sites over large regions such as the vast roadless and unbuilt wilderness regions of northern Fennoscandia, and has proven highly successful. In addition, the current review presents how ALS has been employed for monitoring purposes and for landscape studies, including how it can influence landscape understanding. Finally, the most recent advance within ALS research and development has been discussed: testing of the use of drones for data acquisition. In conclusion, aspects related to the utilization of ALS in archaeological research and cultural heritage management are summarized and discussed, together with thoughts about future perspectives.},
DOI = {10.3390/rs12091411}
}



@Article{e22050510,
AUTHOR = {Liu, Longlong and Ma, Di and Azar, Ahmad Taher and Zhu, Quanmin},
TITLE = {Neural Computing Enhanced Parameter Estimation for Multi-Input and Multi-Output Total Non-Linear Dynamic Models},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {510},
URL = {https://www.mdpi.com/1099-4300/22/5/510},
ISSN = {1099-4300},
ABSTRACT = {In this paper, a gradient descent algorithm is proposed for the parameter estimation of multi-input and multi-output (MIMO) total non-linear dynamic models. Firstly, the MIMO total non-linear model is mapped to a non-completely connected feedforward neural network, that is, the parameters of the total non-linear model are mapped to the connection weights of the neural network. Then, based on the minimization of network error, a weight-updating algorithm, that is, an estimation algorithm of model parameters, is proposed with the convergence conditions of a non-completely connected feedforward network. In further determining the variables of the model set, a method of model structure detection is proposed for selecting a group of important items from the whole variable candidate set. In order to verify the usefulness of the parameter identification process, we provide a virtual bench test example for the numerical analysis and user-friendly instructions for potential applications.},
DOI = {10.3390/e22050510}
}



@Article{s20092557,
AUTHOR = {Augustauskas, Rytis and Lipnickas, Arūnas},
TITLE = {Improved Pixel-Level Pavement-Defect Segmentation Using a Deep Autoencoder},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {2557},
URL = {https://www.mdpi.com/1424-8220/20/9/2557},
ISSN = {1424-8220},
ABSTRACT = {Convolutional neural networks perform impressively in complicated computer-vision image-segmentation tasks. Vision-based systems surpass humans in speed and accuracy in quality inspection tasks. Moreover, the maintenance of big infrastructures, such as roads, bridges, or buildings, is tedious and time-demanding work. In this research, we addressed pavement-quality evaluation by pixelwise defect segmentation using a U-Net deep autoencoder. Additionally, to the original neural network architecture, we utilized residual connections, atrous spatial pyramid pooling with parallel and &ldquo;Waterfall&rdquo; connections, and attention gates to perform better defect extraction. The proposed neural network configurations showed a segmentation performance improvement over U-Net with no significant computational overhead. Statistical and visual performance evaluation was taken into consideration for the model comparison. Experiments were conducted on CrackForest, Crack500, GAPs384, and mixed datasets.},
DOI = {10.3390/s20092557}
}



@Article{rs12091438,
AUTHOR = {Silva, Vanessa Sousa da and Silva, Carlos Alberto and Mohan, Midhun and Cardil, Adrián and Rex, Franciel Eduardo and Loureiro, Gabrielle Hambrecht and Almeida, Danilo Roberti Alves de and Broadbent, Eben North and Gorgens, Eric Bastos and Dalla Corte, Ana Paula and Silva, Emanuel Araújo and Valbuena, Rubén and Klauberg, Carine},
TITLE = {Combined Impact of Sample Size and Modeling Approaches for Predicting Stem Volume in Eucalyptus spp. Forest Plantations Using Field and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1438},
URL = {https://www.mdpi.com/2072-4292/12/9/1438},
ISSN = {2072-4292},
ABSTRACT = {Light Detection and Ranging (LiDAR) remote sensing has been established as one of the most promising tools for large-scale forest monitoring and mapping. Continuous advances in computational techniques, such as machine learning algorithms, have been increasingly improving our capability to model forest attributes accurately and at high spatial and temporal resolution. While there have been previous studies exploring the use of LiDAR and machine learning algorithms for forest inventory modeling, as yet, no studies have demonstrated the combined impact of sample size and different modeling techniques for predicting and mapping stem total volume in industrial Eucalyptus spp. tree plantations. This study aimed to compare the combined effects of parametric and nonparametric modeling methods for estimating volume in Eucalyptus spp. tree plantation using airborne LiDAR data while varying the reference data (sample size). The modeling techniques were compared in terms of root mean square error (RMSE), bias, and R2 with 500 simulations. The best performance was verified for the ordinary least-squares (OLS) method, which was able to provide comparable results to the traditional forest inventory approaches using only 40% (n = 63; ~0.04 plots/ha) of the total field plots, followed by the random forest (RF) algorithm with identical sample size values. This study provides solutions for increasing the industry efficiency in monitoring and managing forest plantation stem volume for the paper and pulp supply chain.},
DOI = {10.3390/rs12091438}
}



@Article{s20092592,
AUTHOR = {Cheng, Xuemin and Ren, Yong and Cheng, Kaichang and Cao, Jie and Hao, Qun},
TITLE = {Method for Training Convolutional Neural Networks for In Situ Plankton Image Recognition and Classification Based on the Mechanisms of the Human Eye},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {2592},
URL = {https://www.mdpi.com/1424-8220/20/9/2592},
ISSN = {1424-8220},
ABSTRACT = {In this study, we propose a method for training convolutional neural networks to make them identify and classify images with higher classification accuracy. By combining the Cartesian and polar coordinate systems when describing the images, the method of recognition and classification for plankton images is discussed. The optimized classification and recognition networks are constructed. They are available for in situ plankton images, exploiting the advantages of both coordinate systems in the network training process. Fusing the two types of vectors and using them as the input for conventional machine learning models for classification, support vector machines (SVMs) are selected as the classifiers to combine these two features of vectors, coming from different image coordinate descriptions. The accuracy of the proposed model was markedly higher than those of the initial classical convolutional neural networks when using the in situ plankton image data, with the increases in classification accuracy and recall rate being 5.3% and 5.1% respectively. In addition, the proposed training method can improve the classification performance considerably when used on the public CIFAR-10 dataset.},
DOI = {10.3390/s20092592}
}



@Article{rs12091444,
AUTHOR = {Abdollahi, Abolfazl and Pradhan, Biswajeet and Shukla, Nagesh and Chakraborty, Subrata and Alamri, Abdullah},
TITLE = {Deep Learning Approaches Applied to Remote Sensing Datasets for Road Extraction: A State-Of-The-Art Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1444},
URL = {https://www.mdpi.com/2072-4292/12/9/1444},
ISSN = {2072-4292},
ABSTRACT = {One of the most challenging research subjects in remote sensing is feature extraction, such as road features, from remote sensing images. Such an extraction influences multiple scenes, including map updating, traffic management, emergency tasks, road monitoring, and others. Therefore, a systematic review of deep learning techniques applied to common remote sensing benchmarks for road extraction is conducted in this study. The research is conducted based on four main types of deep learning methods, namely, the GANs model, deconvolutional networks, FCNs, and patch-based CNNs models. We also compare these various deep learning models applied to remote sensing datasets to show which method performs well in extracting road parts from high-resolution remote sensing images. Moreover, we describe future research directions and research gaps. Results indicate that the largest reported performance record is related to the deconvolutional nets applied to remote sensing images, and the F1 score metric of the generative adversarial network model, DenseNet method, and FCN-32 applied to UAV and Google Earth images are high: 96.08%, 95.72%, and 94.59%, respectively.},
DOI = {10.3390/rs12091444}
}



@Article{math8050717,
AUTHOR = {Christias, Panagiotis and Daliakopoulos, Ioannis N. and Manios, Thrassyvoulos and Mocanu, Mariana},
TITLE = {Comparison of Three Computational Approaches for Tree Crop Irrigation Decision Support},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {717},
URL = {https://www.mdpi.com/2227-7390/8/5/717},
ISSN = {2227-7390},
ABSTRACT = {This paper explores methodologies for developing intelligent automated decision systems for complex processes that contain uncertainties, thus requiring computational intelligence. Irrigation decision support systems (IDSS) promise to increase water efficiency while sustaining crop yields. Here, we explored methodologies for developing intelligent IDSS that exploit statistical, measured, and simulated data. A simple and a fuzzy multicriteria approach as well as a Decision Tree based system were analyzed. The methodologies were applied in a sample of olive tree farms of Heraklion in the island of Crete, Greece, where water resources are scarce and crop management is generally empirical. The objective is to support decision for optimal financial profit through high yield while conserving water resources through optimal irrigation schemes under various (or uncertain) intrinsic and extrinsic conditions. Crop irrigation requirements are modelled using the FAO-56 equation. The results demonstrate that the decision support based on probabilistic and fuzzy approaches point to strategies with low amounts and careful distributed water irrigation strategies. The decision tree shows that decision can be optimized by examining coexisting factors. We conclude that irrigation-based decisions can be highly assisted by methods such as decision trees given the right choice of attributes while keeping focus on the financial balance between cost and revenue.},
DOI = {10.3390/math8050717}
}



@Article{pr8050537,
AUTHOR = {Lin, Yao-Chin and Yeh, Ching-Chuan and Chen, Wei-Hung and Hsu, Kai-Yen},
TITLE = {Implementation Criteria for Intelligent Systems in Motor Production Line Process Management},
JOURNAL = {Processes},
VOLUME = {8},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {537},
URL = {https://www.mdpi.com/2227-9717/8/5/537},
ISSN = {2227-9717},
ABSTRACT = {In this study, the factors that affect the implementation of intelligent systems in motor production lines are analyzed. A motor production line located in Vietnam is used as the research object. The research methods include secondary data collection, field study, and interviews. This study demonstrates the following: firstly, the implementation of intelligent systems in motor production lines is heading toward Industry 4.0. Secondly, it is proposed that three functional systems&mdash;robot arm, image recognition, and big data analysis&mdash;can be introduced in the motor production line. This study analyzes the process involved in coil and motor production lines and attempts to combine intelligent system functions. It is expected that in the future, manpower will be reduced, production line productivity will increase, and intelligent production lines will be proposed. The factors that affect the introduction of intelligent systems in motor production lines are improved, and the importance of intelligent systems, which has been rarely considered in previous studies, is highlighted. In the implementation criteria of the intelligent system in the process management of the motor production line, this study provides some suggestions (to coil and motor assembly line) for the production process management. These suggestions can be provided as a reference for production lines that acquaint with intelligent systems.},
DOI = {10.3390/pr8050537}
}



@Article{drones4020018,
AUTHOR = {Gorkin, Robert and Adams, Kye and Berryman, Matthew J and Aubin, Sam and Li, Wanqing and Davis, Andrew R and Barthelemy, Johan},
TITLE = {Sharkeye: Real-Time Autonomous Personal Shark Alerting via Aerial Surveillance},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2504-446X/4/2/18},
ISSN = {2504-446X},
ABSTRACT = {While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens.},
DOI = {10.3390/drones4020018}
}



@Article{ijgi9050297,
AUTHOR = {Condorelli, Francesca and Rinaudo, Fulvio and Salvadore, Francesco and Tagliaventi, Stefano},
TITLE = {A Neural Networks Approach to Detecting Lost Heritage in Historical Video},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {297},
URL = {https://www.mdpi.com/2220-9964/9/5/297},
ISSN = {2220-9964},
ABSTRACT = {Documenting Cultural Heritage through the extraction of 3D measures with photogrammetry is fundamental for the conservation of the memory of the past. However, when the heritage has been lost the only way to recover this information is the use of historical images from archives. The aim of this study is to experiment with new ways to search for architectural heritage in video material and to save the effort of the operator in the archive in terms of efficiency and time. A workflow is proposed to automatically detect lost heritage in film footage using Deep Learning to find suitable images to process with photogrammetry for its 3D virtual reconstruction. The performance of the network was tested on two case studies considering different architectural scenarios, the Tour Saint Jacques which still exists for the tuning of the networks, and Les Halles to test the algorithms on a real case of an architecture which has been destroyed. Despite the poor quantity and low quality of the historical images available for the training of the network, it has been demonstrated that, with few frames, it was possible to reach the same results in terms of performance of a network trained on a large dataset. Moreover, with the introduction of new metrics based on time intervals the measure of the real time saving in terms of human effort was achieved. These findings represent an important innovation in the documentation of destroyed monuments and open new ways to recover information about the past.},
DOI = {10.3390/ijgi9050297}
}



@Article{s20092641,
AUTHOR = {Morar, Anca and Moldoveanu, Alin and Mocanu, Irina and Moldoveanu, Florica and Radoi, Ion Emilian and Asavei, Victor and Gradinaru, Alexandru and Butean, Alex},
TITLE = {A Comprehensive Survey of Indoor Localization Methods Based on Computer Vision},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {2641},
URL = {https://www.mdpi.com/1424-8220/20/9/2641},
ISSN = {1424-8220},
ABSTRACT = {Computer vision based indoor localization methods use either an infrastructure of static cameras to track mobile entities (e.g., people, robots) or cameras attached to the mobile entities. Methods in the first category employ object tracking, while the others map images from mobile cameras with images acquired during a configuration stage or extracted from 3D reconstructed models of the space. This paper offers an overview of the computer vision based indoor localization domain, presenting application areas, commercial tools, existing benchmarks, and other reviews. It provides a survey of indoor localization research solutions, proposing a new classification based on the configuration stage (use of known environment data), sensing devices, type of detected elements, and localization method. It groups 70 of the most recent and relevant image based indoor localization methods according to the proposed classification and discusses their advantages and drawbacks. It highlights localization methods that also offer orientation information, as this is required by an increasing number of applications of indoor localization (e.g., augmented reality).},
DOI = {10.3390/s20092641}
}



@Article{su12093807,
AUTHOR = {Zhang, Jiaqi and Shuang Chen, Sophia and Gao, Qun and Shen, Qiushi and Kimirei, Ismael Aaron and Mapunda, Damas William},
TITLE = {Morphological Characteristics of Informal Settlements and Strategic Suggestions for Urban Sustainable Development in Tanzania: Dar es Salaam, Mwanza, and Kigoma},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {3807},
URL = {https://www.mdpi.com/2071-1050/12/9/3807},
ISSN = {2071-1050},
ABSTRACT = {Rapid urbanization in developing countries has been accompanied by the spread of informal settlements, which is particularly prominent in sub-Saharan Africa. These settlements have become an important supplement to the inadequate formal housing supply in cities, and their spontaneously formed spatial patterns have important influences on sustainable development. In this study, qualitative and quantitative approaches were used to examine the morphological characteristics of informal settlements in Tanzania and the associated influences on urban development. Geographic spatial analyses, landscape pattern indices, and mathematical statistics, along with quick assessments, group discussions, and key informant interviews, were used to obtain detailed information on the spatial forms of informal settlements. The results indicate that the form of the settlements does not conform to the social, economic, or environmental characteristics of sustainable development. The disordered expansion of single-layered buildings with a single function, irregular road networks in poor condition, and a lack of consideration and protection of the ecological environment were found to negatively impact urban function and sustainable development. However, the structure and form of informal settlements could, in addition to formalization projects, be optimized to drive sustainable and socioeconomic development goals as well as environmental conservation.},
DOI = {10.3390/su12093807}
}



@Article{electronics9050788,
AUTHOR = {Deliparaschos, Kyriakos M. and Michail, Konstantinos and Zolotas, Argyrios C.},
TITLE = {Facilitating Autonomous Systems with AI-Based Fault Tolerance and Computational Resource Economy},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {788},
URL = {https://www.mdpi.com/2079-9292/9/5/788},
ISSN = {2079-9292},
ABSTRACT = {Proposed is the facilitation of fault-tolerant capability in autonomous systems with particular consideration of low computational complexity and system interface devices (sensor/actuator) performance. Traditionally model-based fault-tolerant/detection units for multiple sensor faults in automation require a bank of estimators, normally Kalman-based ones. An AI-based control framework enabling low computational power fault tolerance is presented. Contrary to the bank-of-estimators approach, the proposed framework exhibits a single unit for multiple actuator/sensor fault detection. The efficacy of the proposed scheme is shown via rigorous analysis for several sensor fault scenarios for an electro-magnetic suspension testbed.},
DOI = {10.3390/electronics9050788}
}



@Article{w12051369,
AUTHOR = {Jiang, Ling and Hu, Yang and Xia, Xilin and Liang, Qiuhua and Soltoggio, Andrea and Kabir, Syed Rezwan},
TITLE = {A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for Reconstructing High-Resolution Urban DEMs},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1369},
URL = {https://www.mdpi.com/2073-4441/12/5/1369},
ISSN = {2073-4441},
ABSTRACT = {The scarcity of high-resolution urban digital elevation model (DEM) datasets, particularly in certain developing countries, has posed a challenge for many water-related applications such as flood risk management. A solution to address this is to develop effective approaches to reconstruct high-resolution DEMs from their low-resolution equivalents that are more widely available. However, the current high-resolution DEM reconstruction approaches mainly focus on natural topography. Few attempts have been made for urban topography, which is typically an integration of complex artificial and natural features. This study proposed a novel multi-scale mapping approach based on convolutional neural network (CNN) to deal with the complex features of urban topography and to reconstruct high-resolution urban DEMs. The proposed multi-scale CNN model was firstly trained using urban DEMs that contained topographic features at different resolutions, and then used to reconstruct the urban DEM at a specified (high) resolution from a low-resolution equivalent. A two-level accuracy assessment approach was also designed to evaluate the performance of the proposed urban DEM reconstruction method, in terms of numerical accuracy and morphological accuracy. The proposed DEM reconstruction approach was applied to a 121 km2 urbanized area in London, United Kingdom. Compared with other commonly used methods, the current CNN-based approach produced superior results, providing a cost-effective innovative method to acquire high-resolution DEMs in other data-scarce regions.},
DOI = {10.3390/w12051369}
}



@Article{s20102778,
AUTHOR = {Azimi, Mohsen and Eslamlou, Armin Dadras and Pekcan, Gokhan},
TITLE = {Data-Driven Structural Health Monitoring and Damage Detection through Deep Learning: State-of-the-Art Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2778},
URL = {https://www.mdpi.com/1424-8220/20/10/2778},
ISSN = {1424-8220},
ABSTRACT = {Data-driven methods in structural health monitoring (SHM) is gaining popularity due to recent technological advancements in sensors, as well as high-speed internet and cloud-based computation. Since the introduction of deep learning (DL) in civil engineering, particularly in SHM, this emerging and promising tool has attracted significant attention among researchers. The main goal of this paper is to review the latest publications in SHM using emerging DL-based methods and provide readers with an overall understanding of various SHM applications. After a brief introduction, an overview of various DL methods (e.g., deep neural networks, transfer learning, etc.) is presented. The procedure and application of vibration-based, vision-based monitoring, along with some of the recent technologies used for SHM, such as sensors, unmanned aerial vehicles (UAVs), etc. are discussed. The review concludes with prospects and potential limitations of DL-based methods in SHM applications.},
DOI = {10.3390/s20102778}
}



@Article{agriculture10050170,
AUTHOR = {Hong, Suk-Ju and Kim, Sang-Yeon and Kim, Eungchan and Lee, Chang-Hyup and Lee, Jung-Sup and Lee, Dong-Soo and Bang, Jiwoong and Kim, Ghiseok},
TITLE = {Moth Detection from Pheromone Trap Images Using Deep Learning Object Detectors},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {170},
URL = {https://www.mdpi.com/2077-0472/10/5/170},
ISSN = {2077-0472},
ABSTRACT = {Diverse pheromones and pheromone-based traps, as well as images acquired from insects captured by pheromone-based traps, have been studied and developed to monitor the presence and abundance of pests and to protect plants. The purpose of this study is to construct models that detect three species of pest moths in pheromone trap images using deep learning object detection methods and compare their speed and accuracy. Moth images in pheromone traps were collected for training and evaluation of deep learning detectors. Collected images were then subjected to a labeling process that defines the ground truths of target objects for their box locations and classes. Because there were a few negative objects in the dataset, non-target insects were labeled as unknown class and images of non-target insects were added to the dataset. Moreover, data augmentation methods were applied to the training process, and parameters of detectors that were pre-trained with the COCO dataset were used as initial parameter values. Seven detectors&mdash;Faster R-CNN ResNet 101, Faster R-CNN ResNet 50, Faster R-CNN Inception v.2, R-FCN ResNet 101, Retinanet ResNet 50, Retinanet Mobile v.2, and SSD Inception v.2 were trained and evaluated. Faster R-CNN ResNet 101 detector exhibited the highest accuracy (mAP as 90.25), and seven different detector types showed different accuracy and speed. Furthermore, when unexpected insects were included in the collected images, a four-class detector with an unknown class (non-target insect) showed lower detection error than a three-class detector.},
DOI = {10.3390/agriculture10050170}
}



@Article{s20102799,
AUTHOR = {Hożyń, Stanisław and Zalewski, Jacek},
TITLE = {Shoreline Detection and Land Segmentation for Autonomous Surface Vehicle Navigation with the Use of an Optical System},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2799},
URL = {https://www.mdpi.com/1424-8220/20/10/2799},
ISSN = {1424-8220},
ABSTRACT = {Autonomous surface vehicles (ASVs) are a critical part of recent progressive marine technologies. Their development demands the capability of optical systems to understand and interpret the surrounding landscape. This capability plays an important role in the navigation of coastal areas a safe distance from land, which demands sophisticated image segmentation algorithms. For this purpose, some solutions, based on traditional image processing and neural networks, have been introduced. However, the solution of traditional image processing methods requires a set of parameters before execution, while the solution of a neural network demands a large database of labelled images. Our new solution, which avoids these drawbacks, is based on adaptive filtering and progressive segmentation. The adaptive filtering is deployed to suppress weak edges in the image, which is convenient for shoreline detection. Progressive segmentation is devoted to distinguishing the sky and land areas, using a probabilistic clustering model to improve performance. To verify the effectiveness of the proposed method, a set of images acquired from the vehicle&rsquo;s operative camera were utilised. The results demonstrate that the proposed method performs with high accuracy regardless of distance from land or weather conditions.},
DOI = {10.3390/s20102799}
}



@Article{agronomy10050718,
AUTHOR = {Cholula, Uriel and da Silva, Jorge A. and Marconi, Thiago and Thomasson, J. Alex and Solorzano, Jorge and Enciso, Juan},
TITLE = {Forecasting Yield and Lignocellulosic Composition of Energy Cane Using Unmanned Aerial Systems},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {718},
URL = {https://www.mdpi.com/2073-4395/10/5/718},
ISSN = {2073-4395},
ABSTRACT = {Crop monitoring and appropriate agricultural management practices of elite germplasm will enhance bioenergy&rsquo;s efficiency. Unmanned aerial systems (UAS) may be a useful tool for this purpose. The objective of this study was to assess the use of UAS with true color and multispectral imagery to predict the yield and total cellulosic content (TCC) of newly created energy cane germplasm. A trial was established in the growing season of 2016 at the Texas A&amp;M AgriLife Research Center in Weslaco, Texas, where 15 energy cane elite lines and three checks were grown on experimental plots, arranged in a complete block design and replicated four times. Four flights were executed at different growth stages in 2018, at the first ratoon crop, using two multi-rotor UAS: the DJI Phantom 4 Pro equipped with RGB camera and the DJI Matrice 100, equipped with multispectral sensor (SlantRange 3p). Canopy cover, canopy height, NDVI (Normalized Difference Vegetation Index), and ExG (Excess Green Index) were extracted from the images and used to perform a stepwise regression to obtain the yield and TCC models. The results showed a good agreement between the predicted and the measured yields (R2 = 0.88); however, a low coefficient of determination was found between the predicted and the observed TCC (R2 = 0.30). This study demonstrated the potential application of UAS to estimate energy cane yield with high accuracy, enabling plant breeders to phenotype larger populations and make selections with higher confidence.},
DOI = {10.3390/agronomy10050718}
}



@Article{en13102552,
AUTHOR = {Franko, Josef and Du, Shengzhi and Kallweit, Stephan and Duelberg, Enno and Engemann, Heiko},
TITLE = {Design of a Multi-Robot System for Wind Turbine Maintenance},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2552},
URL = {https://www.mdpi.com/1996-1073/13/10/2552},
ISSN = {1996-1073},
ABSTRACT = {The maintenance of wind turbines is of growing importance considering the transition to renewable energy. This paper presents a multi-robot-approach for automated wind turbine maintenance including a novel climbing robot. Currently, wind turbine maintenance remains a manual task, which is monotonous, dangerous, and also physically demanding due to the large scale of wind turbines. Technical climbers are required to work at significant heights, even in bad weather conditions. Furthermore, a skilled labor force with sufficient knowledge in repairing fiber composite material is rare. Autonomous mobile systems enable the digitization of the maintenance process. They can be designed for weather-independent operations. This work contributes to the development and experimental validation of a maintenance system consisting of multiple robotic platforms for a variety of tasks, such as wind turbine tower and rotor blade service. In this work, multicopters with vision and LiDAR sensors for global inspection are used to guide slower climbing robots. Light-weight magnetic climbers with surface contact were used to analyze structure parts with non-destructive inspection methods and to locally repair smaller defects. Localization was enabled by adapting odometry for conical-shaped surfaces considering additional navigation sensors. Magnets were suitable for steel towers to clamp onto the surface. A friction-based climbing ring robot (SMART&mdash; Scanning, Monitoring, Analyzing, Repair and Transportation) completed the set-up for higher payload. The maintenance period could be extended by using weather-proofed maintenance robots. The multi-robot-system was running the Robot Operating System (ROS). Additionally, first steps towards machine learning would enable maintenance staff to use pattern classification for fault diagnosis in order to operate safely from the ground in the future.},
DOI = {10.3390/en13102552}
}



@Article{electronics9050832,
AUTHOR = {Li, Shuai and Sun, Kuangyuan and Luo, Yukui and Yadav, Nandakishor and Choi, Ken},
TITLE = {Novel CNN-Based AP2D-Net Accelerator: An Area and Power Efficient Solution for Real-Time Applications on Mobile FPGA},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {832},
URL = {https://www.mdpi.com/2079-9292/9/5/832},
ISSN = {2079-9292},
ABSTRACT = {Standard convolutional neural networks (CNNs) have large amounts of data redundancy, and the same accuracy can be obtained even in lower bit weights instead of floating-point representation. Most CNNs have to be developed and executed on high-end GPU-based workstations, for which it is hard to transplant the existing implementations onto portable edge FPGAs because of the limitation of on-chip block memory storage size and battery capacity. In this paper, we present adaptive pointwise convolution and 2D convolution joint network (AP2D-Net), an ultra-low power and relatively high throughput system combined with dynamic precision weights and activation. Our system has high performance, and we make a trade-off between accuracy and power efficiency by adopting unmanned aerial vehicle (UAV) object detection scenarios. We evaluate our system on the Zynq UltraScale+ MPSoC Ultra96 mobile FPGA platform. The target board can get the real-time speed of 30 fps under 5.6 W, and the FPGA on-chip power is only 0.6 W. The power efficiency of our system is 2.8&times; better than the best system design on a Jetson TX2 GPU and 1.9&times; better than the design on a PYNQ-Z1 SoC FPGA.},
DOI = {10.3390/electronics9050832}
}



@Article{s20102891,
AUTHOR = {Pan, Hongyi and Badawi, Diaa and Cetin, Ahmet Enis},
TITLE = {Computationally Efficient Wildfire Detection Method Using a Deep Convolutional Network Pruned via Fourier Analysis},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2891},
URL = {https://www.mdpi.com/1424-8220/20/10/2891},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we propose a deep convolutional neural network for camera based wildfire detection. We train the neural network via transfer learning and use window based analysis strategy to increase the fire detection rate. To achieve computational efficiency, we calculate frequency response of the kernels in convolutional and dense layers and eliminate those filters with low energy impulse response. Moreover, to reduce the storage for edge devices, we compare the convolutional kernels in Fourier domain and discard similar filters using the cosine similarity measure in the frequency domain. We test the performance of the neural network with a variety of wildfire video clips and the pruned system performs as good as the regular network in daytime wild fire detection, and it also works well on some night wild fire video clips.},
DOI = {10.3390/s20102891}
}



@Article{app10103544,
AUTHOR = {Bahaghighat, Mahdi and Xin, Qin and Motamedi, Seyed Ahmad and Zanjireh, Morteza Mohammadi and Vacavant, Antoine},
TITLE = {Estimation of Wind Turbine Angular Velocity Remotely Found on Video Mining and Convolutional Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {3544},
URL = {https://www.mdpi.com/2076-3417/10/10/3544},
ISSN = {2076-3417},
ABSTRACT = {Today, energy issues are more important than ever. Because of the importance of environmental concerns, clean and renewable energies such as wind power have been most welcomed globally, especially in developing countries. Worldwide development of these technologies leads to the use of intelligent systems for monitoring and maintenance purposes. Besides, deep learning as a new area of machine learning is sharply developing. Its strong performance in computer vision problems has conducted us to provide a high accuracy intelligent machine vision system based on deep learning to estimate the wind turbine angular velocity, remotely. This velocity along with other information such as pitch angle and yaw angle can be used to estimate the wind farm energy production. For this purpose, we have used SSD (Single Shot Multi-Box Detector) object detection algorithm and some specific classification methods based on DenseNet, SqueezeNet, ResNet50, and InceptionV3 models. The results indicate that the proposed system can estimate rotational speed with about     99.05 %     accuracy.},
DOI = {10.3390/app10103544}
}



@Article{s20102906,
AUTHOR = {Qi, Bing and Wen, Fuzhong and Liu, Fanming and Cheng, Jianhua},
TITLE = {Modification of MTEA-Based Temperature Drift Error Compensation Model for MEMS-Gyros},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2906},
URL = {https://www.mdpi.com/1424-8220/20/10/2906},
ISSN = {1424-8220},
ABSTRACT = {The conventional temperature drift error (TDE) compensation model cannot decouple temperature dependence of Si-based materials because temperature correlated quantities (TCQ) have not been obtained comprehensively, and Micro-Electro-Mechanical System gyros&rsquo; (MEMS-gyros&rsquo;) environmental adaptability is reduced in diverse, complicated conditions. The study presents modification of TDE compensation model of MEMS-gyros based on microstructure thermal effect analysis (MTEA). First, Si-based materials&rsquo; temperature dependence was studied in microstructure with thermal expansion effect and TCQ that determines the structural deformation were extracted to modify the conventional model, including temperature variation and its square. Second, a precise TDE test method was formed by analyzing heat conduction process between MEMS-gyros and thermal chamber, and temperature experiments were designed and conducted. Third, the modified model&rsquo;s parameters were identified based on radical basis function artificial neural network (RBF ANN) and its performance was evaluated. Last, the conventional and modified models were compared in performance. The experimental results show MEMS-gyros&rsquo; bias stability was up to 10% of the conventional model, the temperature dependence of Si-based materials was decoupled better by the modified one and the environmental adaptability of MEMS-gyros was improved to expand their application in diverse complicated conditions.},
DOI = {10.3390/s20102906}
}



@Article{agriengineering2020019,
AUTHOR = {Deng, Xiaoling and Tong, Zejing and Lan, Yubin and Huang, Zixiao},
TITLE = {Detection and Location of Dead Trees with Pine Wilt Disease Based on Deep Learning and UAV Remote Sensing},
JOURNAL = {AgriEngineering},
VOLUME = {2},
YEAR = {2020},
NUMBER = {2},
PAGES = {294--307},
URL = {https://www.mdpi.com/2624-7402/2/2/19},
ISSN = {2624-7402},
ABSTRACT = {Pine wilt disease causes huge economic losses to pine wood forestry because of its destructiveness and rapid spread. This paper proposes a detection and location method of pine wood nematode disease at a large scale adopting UAV (Unmanned Aerial Vehicle) remote sensing and artificial intelligence technology. The UAV remote sensing images were enhanced by computer vision tools. A Faster-RCNN (Faster Region Convolutional Neural Networks) deep learning framework based on a RPN (Region Proposal Network) network and the ResNet residual neural network were used to train the pine wilt diseased dead tree detection model. The loss function and the anchors in the RPN of the convolutional neural network were optimized. Finally, the location of pine wood nematode dead tree was conducted, which generated the geographic information on the detection results. The results show that ResNet101 performed better than VGG16 (Visual Geometry Group 16) convolutional neural network. The detection accuracy was improved and reached to about 90% after a series of optimizations to the network, meaning that the optimization methods proposed in this paper are feasible to pine wood nematode dead tree detection.},
DOI = {10.3390/agriengineering2020019}
}



@Article{s20102958,
AUTHOR = {Diebold, Clarice Anna and Salles, Angeles and Moss, Cynthia F.},
TITLE = {Adaptive Echolocation and Flight Behaviors in Bats Can Inspire Technology Innovations for Sonar Tracking and Interception},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2958},
URL = {https://www.mdpi.com/1424-8220/20/10/2958},
ISSN = {1424-8220},
ABSTRACT = {Target tracking and interception in a dynamic world proves to be a fundamental challenge faced by both animals and artificial systems. To track moving objects under natural conditions, agents must employ strategies to mitigate interference and conditions of uncertainty. Animal studies of prey tracking and capture reveal biological solutions, which can inspire new technologies, particularly for operations in complex and noisy environments. By reviewing research on target tracking and interception by echolocating bats, we aim to highlight biological solutions that could inform new approaches to artificial sonar tracking and navigation systems. Most bat species use wideband echolocation signals to navigate dense forests and hunt for evasive insects in the dark. Importantly, bats exhibit rapid adaptations in flight trajectory, sonar beam aim, and echolocation signal design, which appear to be key to the success of these animals in a variety of tasks. The rich suite of adaptive behaviors of echolocating bats could be leveraged in new sonar tracking technologies by implementing dynamic sensorimotor feedback control of wideband sonar signal design, head, and ear movements.},
DOI = {10.3390/s20102958}
}



@Article{rs12101676,
AUTHOR = {Pradhan, Biswajeet and Al-Najjar, Husam A. H. and Sameen, Maher Ibrahim and Tsang, Ivor and Alamri, Abdullah M.},
TITLE = {Unseen Land Cover Classification from High-Resolution Orthophotos Using Integration of Zero-Shot Learning and Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1676},
URL = {https://www.mdpi.com/2072-4292/12/10/1676},
ISSN = {2072-4292},
ABSTRACT = {Zero-shot learning (ZSL) is an approach to classify objects unseen during the training phase and shown to be useful for real-world applications, especially when there is a lack of sufficient training data. Only a limited amount of works has been carried out on ZSL, especially in the field of remote sensing. This research investigates the use of a convolutional neural network (CNN) as a feature extraction and classification method for land cover mapping using high-resolution orthophotos. In the feature extraction phase, we used a CNN model with a single convolutional layer to extract discriminative features. In the second phase, we used class attributes learned from the Word2Vec model (pre-trained by Google News) to train a second CNN model that performed class signature prediction by using both the features extracted by the first CNN and class attributes during training and only the features during prediction. We trained and tested our models on datasets collected over two subareas in the Cameron Highlands (training dataset, first test dataset) and Ipoh (second test dataset) in Malaysia. Several experiments have been conducted on the feature extraction and classification models regarding the main parameters, such as the network&rsquo;s layers and depth, number of filters, and the impact of Gaussian noise. As a result, the best models were selected using various accuracy metrics such as top-k categorical accuracy for k = [1,2,3], Recall, Precision, and F1-score. The best model for feature extraction achieved 0.953 F1-score, 0.941 precision, 0.882 recall for the training dataset and 0.904 F1-score, 0.869 precision, 0.949 recall for the first test dataset, and 0.898 F1-score, 0.870 precision, 0.838 recall for the second test dataset. The best model for classification achieved an average of 0.778 top-one, 0.890 top-two and 0.942 top-three accuracy, 0.798 F1-score, 0.766 recall and 0.838 precision for the first test dataset and 0.737 top-one, 0.906 top-two, 0.924 top-three, 0.729 F1-score, 0.676 recall and 0.790 precision for the second test dataset. The results demonstrated that the proposed ZSL is a promising tool for land cover mapping based on high-resolution photos.},
DOI = {10.3390/rs12101676}
}



@Article{s20102975,
AUTHOR = {Fuentes, Sigfredo and Gonzalez Viejo, Claudia and Cullen, Brendan and Tongson, Eden and Chauhan, Surinder S. and Dunshea, Frank R.},
TITLE = {Artificial Intelligence Applied to a Robotic Dairy Farm to Model Milk Productivity and Quality based on Cow Data and Daily Environmental Parameters},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2975},
URL = {https://www.mdpi.com/1424-8220/20/10/2975},
ISSN = {1424-8220},
ABSTRACT = {Increased global temperatures and climatic anomalies, such as heatwaves, as a product of climate change, are impacting the heat stress levels of farm animals. These impacts could have detrimental effects on the milk quality and productivity of dairy cows. This research used four years of data from a robotic dairy farm from 36 cows with similar heat tolerance (Model 1), and all 312 cows from the farm (Model 2). These data consisted of programmed concentrate feed and weight combined with weather parameters to develop supervised machine learning fitting models to predict milk yield, fat and protein content, and actual cow concentrate feed intake. Results showed highly accurate models, which were developed for cows with a similar genetic heat tolerance (Model 1: n = 116, 456; R = 0.87; slope = 0.76) and for all cows (Model 2: n = 665, 836; R = 0.86; slope = 0.74). Furthermore, an artificial intelligence (AI) system was proposed to increase or maintain a targeted level of milk quality by reducing heat stress that could be applied to a conventional dairy farm with minimal technology addition.},
DOI = {10.3390/s20102975}
}



@Article{math8050855,
AUTHOR = {Teso-Fz-Betoño, Daniel and Zulueta, Ekaitz and Sánchez-Chica, Ander and Fernandez-Gamiz, Unai and Saenz-Aguirre, Aitor},
TITLE = {Semantic Segmentation to Develop an Indoor Navigation System for an Autonomous Mobile Robot},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {855},
URL = {https://www.mdpi.com/2227-7390/8/5/855},
ISSN = {2227-7390},
ABSTRACT = {In this study, a semantic segmentation network is presented to develop an indoor navigation system for a mobile robot. Semantic segmentation can be applied by adopting different techniques, such as a convolutional neural network (CNN). However, in the present work, a residual neural network is implemented by engaging in ResNet-18 transfer learning to distinguish between the floor, which is the navigation free space, and the walls, which are the obstacles. After the learning process, the semantic segmentation floor mask is used to implement indoor navigation and motion calculations for the autonomous mobile robot. This motion calculations are based on how much the estimated path differs from the center vertical line. The highest point is used to move the motors toward that direction. In this way, the robot can move in a real scenario by avoiding different obstacles. Finally, the results are collected by analyzing the motor duty cycle and the neural network execution time to review the robot&rsquo;s performance. Moreover, a different net comparison is made to determine other architectures&rsquo; reaction times and accuracy values.},
DOI = {10.3390/math8050855}
}



@Article{app10103645,
AUTHOR = {Kang, Yanyan and Lv, Wanting and He, Jinyan and Ding, Xianrong},
TITLE = {Remote Sensing of Time-Varying Tidal Flat Topography, Jiangsu Coast, China, Based on the Waterline Method and an Artificial Neural Network Model},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {3645},
URL = {https://www.mdpi.com/2076-3417/10/10/3645},
ISSN = {2076-3417},
ABSTRACT = {Measurement of beach heights in the intertidal zone has great importance for dynamic geomorphology research, coastal zone management, and the protection of ecological resources. Based on satellite images, the waterline method based on satellite images is one of the most effective methods for constructing digital elevation models (DEMs) for large-scale tidal flats. However, for fast-changing areas, such as Tiaozini in the Jiangsu coast, timely and detailed topographical data are difficult to obtain due to the insufficient images over a short period of time. In this study, as a supplement to the waterline method, an artificial neural network (ANN) model with the multi-layer feed-forward back propagation algorithm was developed to simulate the topography of variable Tiaozini tidal flats. The &ldquo;7-15-15-1&rdquo; double hidden layers with optimized training structures were confirmed via continuous training and comparisons. The input parameters included spectral bands (HJ-1 images B1~B4), geographical coordinates (X, Y), and the distance (D) to waterlines, and the output parameter was the elevation. The model training data were the HJ-1 image for 21 March 2014, and the corresponding topographic data obtained from the waterline method. Then, this ANN model was used to simulate synchronous DEMs corresponding to remote sensing images on 11 February 2012, and 11 July 2013, under low tide conditions. The height accuracy (root mean square error) of the two DEMs was about 0.3&ndash;0.4 m based on three transects of the in-situ measured data, and the horizontal accuracy was 30 m&mdash;the same as the spatial resolution of the HJ-1 image. Although its vertical accuracy is not very high, this ANN model can quickly provide the basic geomorphological framework for tidal flats based on only one image. This model, therefore, provides an effective way to monitor rapidly changing tidal flats.},
DOI = {10.3390/app10103645}
}



@Article{en13112860,
AUTHOR = {Piccinini, Fabio and Pierdicca, Roberto and Malinverni, Eva Savina},
TITLE = {A Relational Conceptual Model in GIS for the Management of Photovoltaic Systems},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {2860},
URL = {https://www.mdpi.com/1996-1073/13/11/2860},
ISSN = {1996-1073},
ABSTRACT = {The aim of this manuscript is to define an operational pipeline of work, from data acquisition to the report creation, for the smart management of PV plants. To achieve such an ambitious result, we exploit the implementation of a conceptual model, deployed through a relational database to retrieve any kind of information related to the PV plant. The motivation that drove this research is due to the increasing construction of PV plants. In fact, following European and international investments that heavily stimulated the use of clean energy, the need to maintain PV plants in their maximum efficiency for their whole lifecycle emerged, to bring about benefits from both the ecological and the economic points of view. While the research community focuses on finding new and automatic ways to detect faults automatically, few efforts have been made considering the so-called Operation and Maintenance (O&amp;M). A relational conceptual model may facilitate the management of heterogeneous sources of information, which are common in complex PV plants. The purpose of the present study is to provide companies and insiders with a GIS-based tool to maintain the energy efficiency of a PV plant. Indeed, it is a common practice used by companies dealing with O&amp;M of PV plants to create technical reports about the health status of the plants. This operation, made manually, is very time consuming and error prone. To overcome this latter drawback, this work attempts to encourage the use of GIS in the PV plants O&amp;M, which proves to be efficient to deal with fault management and to assure a good level of energy production. The developed conceptual model, tested on two real case studies, proved to be complete, cost-effective and efficient to be replicated in other existing plants.},
DOI = {10.3390/en13112860}
}



@Article{aerospace7060071,
AUTHOR = {Gomez, Victor and Gomez, Nicolas and Rodas, Jorge and Paiva, Enrique and Saad, Maarouf and Gregor, Raul},
TITLE = {Pareto Optimal PID Tuning for Px4-Based Unmanned Aerial Vehicles by Using a Multi-Objective Particle Swarm Optimization Algorithm},
JOURNAL = {Aerospace},
VOLUME = {7},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {71},
URL = {https://www.mdpi.com/2226-4310/7/6/71},
ISSN = {2226-4310},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are affordable these days. For that reason, there are currently examples of the use of UAVs in recreational, professional and research applications. Most of the commercial UAVs use Px4 for their operating system. Even though Px4 allows one to change the flight controller structure, the proportional-integral-derivative (PID) format is still by far the most popular choice. A selection of the PID controller parameters is required before the UAV can be used. Although there are guidelines for the design of PID parameters, they do not guarantee the stability of the UAV, which in many cases, leads to collisions involving the UAV during the calibration process. In this paper, an offline tuning procedure based on the multi-objective particle swarm optimization (MOPSO) algorithm for the attitude and altitude control of a Px4-based UAV is proposed. A Pareto dominance concept is used for the MOPSO to find values for the PID comparing parameters of step responses (overshoot, rise time and root-mean-square). Experimental results are provided to validate the proposed tuning procedure by using a quadrotor as a case study.},
DOI = {10.3390/aerospace7060071}
}



@Article{rs12111838,
AUTHOR = {Zhang, Zhao and Flores, Paulo and Igathinathane, C. and L. Naik, Dayakar and Kiran, Ravi and Ransom, Joel K.},
TITLE = {Wheat Lodging Detection from UAS Imagery Using Machine Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1838},
URL = {https://www.mdpi.com/2072-4292/12/11/1838},
ISSN = {2072-4292},
ABSTRACT = {The current mainstream approach of using manual measurements and visual inspections for crop lodging detection is inefficient, time-consuming, and subjective. An innovative method for wheat lodging detection that can overcome or alleviate these shortcomings would be welcomed. This study proposed a systematic approach for wheat lodging detection in research plots (372 experimental plots), which consisted of using unmanned aerial systems (UAS) for aerial imagery acquisition, manual field evaluation, and machine learning algorithms to detect the occurrence or not of lodging. UAS imagery was collected on three different dates (23 and 30 July 2019, and 8 August 2019) after lodging occurred. Traditional machine learning and deep learning were evaluated and compared in this study in terms of classification accuracy and standard deviation. For traditional machine learning, five types of features (i.e. gray level co-occurrence matrix, local binary pattern, Gabor, intensity, and Hu-moment) were extracted and fed into three traditional machine learning algorithms (i.e., random forest (RF), neural network, and support vector machine) for detecting lodged plots. For the datasets on each imagery collection date, the accuracies of the three algorithms were not significantly different from each other. For any of the three algorithms, accuracies on the first and last date datasets had the lowest and highest values, respectively. Incorporating standard deviation as a measurement of performance robustness, RF was determined as the most satisfactory. Regarding deep learning, three different convolutional neural networks (simple convolutional neural network, VGG-16, and GoogLeNet) were tested. For any of the single date datasets, GoogLeNet consistently had superior performance over the other two methods. Further comparisons between RF and GoogLeNet demonstrated that the detection accuracies of the two methods were not significantly different from each other (p &gt; 0.05); hence, the choice of any of the two would not affect the final detection accuracies. However, considering the fact that the average accuracy of GoogLeNet (93%) was larger than RF (91%), it was recommended to use GoogLeNet for wheat lodging detection. This research demonstrated that UAS RGB imagery, coupled with the GoogLeNet machine learning algorithm, can be a novel, reliable, objective, simple, low-cost, and effective (accuracy &gt; 90%) tool for wheat lodging detection.},
DOI = {10.3390/rs12111838}
}



@Article{app10113953,
AUTHOR = {de la Fuente Castillo, Víctor and Díaz-Álvarez, Alberto and Manso-Callejo, Miguel-Ángel and Serradilla García, Francisco},
TITLE = {Grammar Guided Genetic Programming for Network Architecture Search and Road Detection on Aerial Orthophotography},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3953},
URL = {https://www.mdpi.com/2076-3417/10/11/3953},
ISSN = {2076-3417},
ABSTRACT = {Photogrammetry involves aerial photography of the Earth&rsquo;s surface and subsequently processing the images to provide a more accurate depiction of the area (Orthophotography). It is used by the Spanish Instituto Geogr&aacute;fico Nacional to update road cartography but requires a significant amount of manual labor due to the need to perform visual inspection of all tiled images. Deep learning techniques (artificial neural networks with more than one hidden layer) can perform road detection but it is still unclear how to find the optimal network architecture. Our main goal is the automatic design of deep neural network architectures with grammar-guided genetic programming. In this kind of evolutive algorithm, all the population individuals (here candidate network architectures) are constrained to rules specified by a grammar that defines valid and useful structural patterns to guide the search process. Grammar used includes well-known complex structures (e.g., Inception-like modules) combined with a custom designed mutation operator (dynamically links the mutation probability to structural diversity). Pilot results show that the system is able to design models for road detection that obtain test accuracies similar to that reached by state-of-the-art models when evaluated over a dataset from the Spanish National Aerial Orthophotography Plan.},
DOI = {10.3390/app10113953}
}



@Article{s20113245,
AUTHOR = {Zhang, Tianyao and Hu, Xiaoguang and Xiao, Jin and Zhang, Guofeng},
TITLE = {A Machine Learning Method for Vision-Based Unmanned Aerial Vehicle Systems to Understand Unknown Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3245},
URL = {https://www.mdpi.com/1424-8220/20/11/3245},
ISSN = {1424-8220},
ABSTRACT = {What makes unmanned aerial vehicles (UAVs) intelligent is their capability of sensing and understanding new unknown environments. Some studies utilize computer vision algorithms like Visual Simultaneous Localization and Mapping (VSLAM) and Visual Odometry (VO) to sense the environment for pose estimation, obstacles avoidance and visual servoing. However, understanding the new environment (i.e., make the UAV recognize generic objects) is still an essential scientific problem that lacks a solution. Therefore, this paper takes a step to understand the items in an unknown environment. The aim of this research is to enable the UAV with basic understanding capability for a high-level UAV flock application in the future. Specially, firstly, the proposed understanding method combines machine learning and traditional algorithm to understand the unknown environment through RGB images; secondly, the You Only Look Once (YOLO) object detection system is integrated (based on TensorFlow) in a smartphone to perceive the position and category of 80 classes of objects in the images; thirdly, the method makes the UAV more intelligent and liberates the operator from labor; fourthly, detection accuracy and latency in working condition are quantitatively evaluated, and properties of generality (can be used in various platforms), transportability (easily deployed from one platform to another) and scalability (easily updated and maintained) for UAV flocks are qualitatively discussed. The experiments suggest that the method has enough accuracy to recognize various objects with high computational speed, and excellent properties of generality, transportability and scalability.},
DOI = {10.3390/s20113245}
}



@Article{rs12111890,
AUTHOR = {Arabameri, Alireza and Asadi Nalivan, Omid and Saha, Sunil and Roy, Jagabandhu and Pradhan, Biswajeet and Tiefenbacher, John P. and Thi Ngo, Phuong Thao},
TITLE = {Novel Ensemble Approaches of Machine Learning Techniques in Modeling the Gully Erosion Susceptibility},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1890},
URL = {https://www.mdpi.com/2072-4292/12/11/1890},
ISSN = {2072-4292},
ABSTRACT = {Gully erosion has become one of the major environmental issues, due to the severity of its impact in many parts of the world. Gully erosion directly and indirectly affects agriculture and infrastructural development. The Golestan Dam basin, where soil erosion and degradation are very severe problems, was selected as the study area. This research maps gully erosion susceptibility (GES) by integrating four models: maximum entropy (MaxEnt), artificial neural network (ANN), support vector machine (SVM), and general linear model (GLM). Of 1042 gully locations, 729 (70%) and 313 (30%) gully locations were used for modeling and validation purposes, respectively. Fourteen effective gully erosion conditioning factors (GECFs) were selected for spatial gully erosion modeling. Tolerance and variance inflation factors (VIFs) were used to examine the collinearity among the GECFs. The random forest (RF) model was used to assess factors&rsquo; effectiveness and significance in gully erosion modeling. An ensemble of techniques can provide more accurate results than can single, standalone models. Therefore, we compared two-, three-, and four-model ensembles (ANN-SVM, GLM-ANN, GLM-MaxEnt, GLM-SVM, MaxEnt-ANN, MaxEnt-SVM, ANN-SVM-GLM, GLM-MaxEnt-ANN, GLM-MaxEnt-SVM, MaxEnt-ANN-SVM and GLM-ANN-SVM-MaxEnt) for GES modeling. The susceptibility zones of the GESMs were classified as very-low, low, medium, high, and very-high using Jenks&rsquo; natural break classification method (NBM). Subsequently, the receiver operating characteristics (ROC) curve and the seed cell area index (SCAI) methods measured the reliability of the models. The success rate curve (SRC) and predication rate curve (PRC) and their area under the curve (AUC) values were obtained from the GES maps. The results show that the ANN model combined with two and three models are more accurate than the other combinations, but the ANN-SVM model had the highest accuracy. The rank of the others from best to worst accuracy is GLM, MaxEnt, SVM, GLM-ANN, GLM-MaxEnt, GLM-SVM, MaxEnt-ANN, MaxEnt-SVM, GLM-ANN-SVM-MaxEnt, GLM-MaxEnt-ANN, GLM-MaxEnt-SVM and MaxEnt-ANN-SVM. The resulting gully erosion susceptibility models (GESMs) are efficient and powerful and could be used to improve soil and water conservation and management.},
DOI = {10.3390/rs12111890}
}



@Article{drones4020021,
AUTHOR = {Rodríguez-Puerta, Francisco and Alonso Ponce, Rafael and Pérez-Rodríguez, Fernando and Águeda, Beatriz and Martín-García, Saray and Martínez-Rodrigo, Raquel and Lizarralde, Iñigo},
TITLE = {Comparison of Machine Learning Algorithms for Wildland-Urban Interface Fuelbreak Planning Integrating ALS and UAV-Borne LiDAR Data and Multispectral Images},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {21},
URL = {https://www.mdpi.com/2504-446X/4/2/21},
ISSN = {2504-446X},
ABSTRACT = {Controlling vegetation fuels around human settlements is a crucial strategy for reducing fire severity in forests, buildings and infrastructure, as well as protecting human lives. Each country has its own regulations in this respect, but they all have in common that by reducing fuel load, we in turn reduce the intensity and severity of the fire. The use of Unmanned Aerial Vehicles (UAV)-acquired data combined with other passive and active remote sensing data has the greatest performance to planning Wildland-Urban Interface (WUI) fuelbreak through machine learning algorithms. Nine remote sensing data sources (active and passive) and four supervised classification algorithms (Random Forest, Linear and Radial Support Vector Machine and Artificial Neural Networks) were tested to classify five fuel-area types. We used very high-density Light Detection and Ranging (LiDAR) data acquired by UAV (154 returns&middot;m&minus;2 and ortho-mosaic of 5-cm pixel), multispectral data from the satellites Pleiades-1B and Sentinel-2, and low-density LiDAR data acquired by Airborne Laser Scanning (ALS) (0.5 returns&middot;m&minus;2, ortho-mosaic of 25 cm pixels). Through the Variable Selection Using Random Forest (VSURF) procedure, a pre-selection of final variables was carried out to train the model. The four algorithms were compared, and it was concluded that the differences among them in overall accuracy (OA) on training datasets were negligible. Although the highest accuracy in the training step was obtained in SVML (OA=94.46%) and in testing in ANN (OA=91.91%), Random Forest was considered to be the most reliable algorithm, since it produced more consistent predictions due to the smaller differences between training and testing performance. Using a combination of Sentinel-2 and the two LiDAR data (UAV and ALS), Random Forest obtained an OA of 90.66% in training and of 91.80% in testing datasets. The differences in accuracy between the data sources used are much greater than between algorithms. LiDAR growth metrics calculated using point clouds in different dates and multispectral information from different seasons of the year are the most important variables in the classification. Our results support the essential role of UAVs in fuelbreak planning and management and thus, in the prevention of forest fires.},
DOI = {10.3390/drones4020021}
}



@Article{s20123336,
AUTHOR = {Tang, Ta-Wei and Kuo, Wei-Han and Lan, Jauh-Hsiang and Ding, Chien-Fang and Hsu, Hakiem and Young, Hong-Tsu},
TITLE = {Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3336},
URL = {https://www.mdpi.com/1424-8220/20/12/3336},
ISSN = {1424-8220},
ABSTRACT = {Recently, researchers have been studying methods to introduce deep learning into automated optical inspection (AOI) systems to reduce labor costs. However, the integration of deep learning in the industry may encounter major challenges such as sample imbalance (defective products that only account for a small proportion). Therefore, in this study, an anomaly detection neural network, dual auto-encoder generative adversarial network (DAGAN), was developed to solve the problem of sample imbalance. With skip-connection and dual auto-encoder architecture, the proposed method exhibited excellent image reconstruction ability and training stability. Three datasets, namely public industrial detection training set, MVTec AD, with mobile phone screen glass and wood defect detection datasets, were used to verify the inspection ability of DAGAN. In addition, training with a limited amount of data was proposed to verify its detection ability. The results demonstrated that the areas under the curve (AUCs) of DAGAN were better than previous generative adversarial network-based anomaly detection models in 13 out of 17 categories in these datasets, especially in categories with high variability or noise. The maximum AUC improvement was 0.250 (toothbrush). Moreover, the proposed method exhibited better detection ability than the U-Net auto-encoder, which indicates the function of discriminator in this application. Furthermore, the proposed method had a high level of AUCs when using only a small amount of training data. DAGAN can significantly reduce the time and cost of collecting and labeling data when it is applied to industrial detection.},
DOI = {10.3390/s20123336}
}



@Article{rs12121924,
AUTHOR = {Miura, Hiroyuki and Aridome, Tomohiro and Matsuoka, Masashi},
TITLE = {Deep Learning-Based Identification of Collapsed, Non-Collapsed and Blue Tarp-Covered Buildings from Post-Disaster Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1924},
URL = {https://www.mdpi.com/2072-4292/12/12/1924},
ISSN = {2072-4292},
ABSTRACT = {A methodology for the automated identification of building damage from post-disaster aerial images was developed based on convolutional neural network (CNN) and building damage inventories. The aerial images and the building damage data obtained in the 2016 Kumamoto, and the 1995 Kobe, Japan earthquakes were analyzed. Since the roofs of many moderately damaged houses are covered with blue tarps immediately after disasters, not only collapsed and non-collapsed buildings but also the buildings covered with blue tarps were identified by the proposed method. The CNN architecture developed in this study correctly classifies the building damage with the accuracy of approximately 95 % in both earthquake data. We applied the developed CNN model to aerial images in Chiba, Japan, damaged by the typhoon in September 2019. The result shows that more than 90 % of the building damage are correctly classified by the CNN model.},
DOI = {10.3390/rs12121924}
}



@Article{rs12121933,
AUTHOR = {Wang, Mingchang and Zhang, Haiming and Sun, Weiwei and Li, Sheng and Wang, Fengyan and Yang, Guodong},
TITLE = {A Coarse-to-Fine Deep Learning Based Land Use Change Detection Method for High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1933},
URL = {https://www.mdpi.com/2072-4292/12/12/1933},
ISSN = {2072-4292},
ABSTRACT = {In recent decades, high-resolution (HR) remote sensing images have shown considerable potential for providing detailed information for change detection. The traditional change detection methods based on HR remote sensing images mostly only detect a single land type or only the change range, and cannot simultaneously detect the change of all object types and pixel-level range changes in the area. To overcome this difficulty, we propose a new coarse-to-fine deep learning-based land-use change detection method. We independently created a new scene classification dataset called NS-55, and innovatively considered the adaptation relationship between the convolutional neural network (CNN) and the scene complexity by selecting the CNN that best fit the scene complexity. The CNN trained by NS-55 was used to detect the category of the scene, define the final category of the scene according to the majority voting method, and obtain the changed scene by comparison to obtain the so-called coarse change result. Then, we created a multi-scale threshold (MST) method, which is a new method for obtaining high-quality training samples. We used the high-quality samples selected by MST to train the deep belief network to obtain the pixel-level range change detection results. By mapping coarse scene changes to range changes, we could obtain fine multi-type land-use change detection results. Experiments were conducted on the Multi-temporal Scene Wuhan dataset and aerial images of a particular area of Dapeng New District, Shenzhen, where promising results were achieved by the proposed method. This demonstrates that the proposed method is practical, easy-to-implement, and the NS-55 dataset is physically justified. The proposed method has the potential to be applied in the large scale land use fine change detection problem and qualitative and quantitative research on land use/cover change based on HR remote sensing data.},
DOI = {10.3390/rs12121933}
}



@Article{s20123405,
AUTHOR = {Bilal, Diyar Khalis and Unel, Mustafa and Yildiz, Mehmet and Koc, Bahattin},
TITLE = {Realtime Localization and Estimation of Loads on Aircraft Wings from Depth Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3405},
URL = {https://www.mdpi.com/1424-8220/20/12/3405},
ISSN = {1424-8220},
ABSTRACT = {This paper deals with the development of a realtime structural health monitoring system for airframe structures to localize and estimate the magnitude of the loads causing deflections to the critical components, such as wings. To this end, a framework that is based on artificial neural networks is developed where features that are extracted from a depth camera are utilized. The localization of the load is treated as a multinomial logistic classification problem and the load magnitude estimation as a logistic regression problem. The neural networks trained for classification and regression are preceded with an autoencoder, through which maximum informative data at a much smaller scale are extracted from the depth features. The effectiveness of the proposed method is validated by an experimental study performed on a composite unmanned aerial vehicle (UAV) wing subject to concentrated and distributed loads, and the results obtained by the proposed method are superior when compared with a method based on Castigliano&rsquo;s theorem.},
DOI = {10.3390/s20123405}
}



@Article{s20123414,
AUTHOR = {Gong, Dawei and He, Zhiheng and Ye, Xiaolong and Fang, Ziyun},
TITLE = {Visual Saliency Detection for Over-Temperature Regions in 3D Space via Dual-Source Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3414},
URL = {https://www.mdpi.com/1424-8220/20/12/3414},
ISSN = {1424-8220},
ABSTRACT = {To allow mobile robots to visually observe the temperature of equipment in complex industrial environments and work on temperature anomalies in time, it is necessary to accurately find the coordinates of temperature anomalies and obtain information on the surrounding obstacles. This paper proposes a visual saliency detection method for hypertemperature in three-dimensional space through dual-source images. The key novelty of this method is that it can achieve accurate salient object detection without relying on high-performance hardware equipment. First, the redundant point clouds are removed through adaptive sampling to reduce the computational memory. Second, the original images are merged with infrared images and the dense point clouds are surface-mapped to visually display the temperature of the reconstructed surface and use infrared imaging characteristics to detect the plane coordinates of temperature anomalies. Finally, transformation mapping is coordinated according to the pose relationship to obtain the spatial position. Experimental results show that this method not only displays the temperature of the device directly but also accurately obtains the spatial coordinates of the heat source without relying on a high-performance computing platform.},
DOI = {10.3390/s20123414}
}



@Article{automation1010001,
AUTHOR = {Aull, Mark and Stough, Andy and Cohen, Kelly},
TITLE = {Design Optimization and Sizing for Fly-Gen Airborne Wind Energy Systems},
JOURNAL = {Automation},
VOLUME = {1},
YEAR = {2020},
NUMBER = {1},
PAGES = {1--16},
URL = {https://www.mdpi.com/2673-4052/1/1/1},
ISSN = {2673-4052},
ABSTRACT = {Traditional on-shore horizontal-axis wind turbines need to be large for both performance reasons (e.g., clearing ground turbulence and reaching higher wind speeds) and for economic reasons (e.g., more efficient land use, lower maintenance costs, and fewer controllers and grid attachments) while their efficiency is scale and mass independent. Airborne wind energy (AWE) system efficiency is a function of system size and AWE system operating altitude is less directly coupled to system power rating. This paper derives fly-gen AWE system parameters from small number of design parameters, which are used to optimize a design for energy cost. This paper then scales AWE systems and optimizes them at each scale to determine the relationships between size, efficiency, power output, and cost. The results indicate that physics and economics favor a larger number of small units, at least offshore or where land cost is small.},
DOI = {10.3390/automation1010001}
}



@Article{jmse8060449,
AUTHOR = {Abaspur Kazerouni, Iman and Dooly, Gerard and Toal, Daniel},
TITLE = {Underwater Image Enhancement and Mosaicking System Based on A-KAZE Feature Matching},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {8},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {449},
URL = {https://www.mdpi.com/2077-1312/8/6/449},
ISSN = {2077-1312},
ABSTRACT = {Feature extraction and matching is a key component in image stitching and a critical step in advancing image reconstructions, machine vision and robotic perception algorithms. This paper presents a fast and robust underwater image mosaicking system based on (2D)2PCA and A-KAZE key-points extraction and optimal seam-line methods. The system utilizes image enhancement as a preprocessing step to improve quality and allow for greater keyframe extraction and matching performance, leading to better quality mosaicking. The application focus of this paper is underwater imaging and it demonstrates the suitability of the developed system in advanced underwater reconstructions. The results show that the proposed method can address the problems of noise, mismatching and quality issues which are typically found in underwater image datasets. The results demonstrate the proposed method as scale-invariant and show improvements in terms of processing speed and system robustness over other methods found in the literature.},
DOI = {10.3390/jmse8060449}
}



@Article{ijgi9060403,
AUTHOR = {Zhang, Xueyan},
TITLE = {Village-Level Homestead and Building Floor Area Estimates Based on UAV Imagery and U-Net Algorithm},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {403},
URL = {https://www.mdpi.com/2220-9964/9/6/403},
ISSN = {2220-9964},
ABSTRACT = {China&rsquo;s rural population has declined markedly with the acceleration of urbanization and industrialization, but the area under rural homesteads has continued to expand. Proper rural land use and management require large-scale, efficient, and low-cost rural residential surveys; however, such surveys are time-consuming and difficult to accomplish. Unmanned aerial vehicle (UAV) technology coupled with a deep learning architecture and 3D modelling can provide a potential alternative to traditional surveys for gathering rural homestead information. In this study, a method to estimate the village-level homestead area, a 3D-based building height model (BHM), and the number of building floors based on UAV imagery and the U-net algorithm was developed, and the respective estimation accuracies were found to be 0.92, 0.99, and 0.89. This method is rapid and inexpensive compared to the traditional time-consuming and costly household surveys, and, thus, it is of great significance to the ongoing use and management of rural homestead information, especially with regards to the confirmation of homestead property rights in China. Further, the proposed combination of UAV imagery and U-net technology may have a broader application in rural household surveys, as it can provide more information for decision-makers to grasp the current state of the rural socio-economic environment.},
DOI = {10.3390/ijgi9060403}
}



@Article{rs12121984,
AUTHOR = {Hegarty-Craver, Meghan and Polly, Jason and O’Neil, Margaret and Ujeneza, Noel and Rineer, James and Beach, Robert H. and Lapidus, Daniel and Temple, Dorota S.},
TITLE = {Remote Crop Mapping at Scale: Using Satellite Imagery and UAV-Acquired Data as Ground Truth},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1984},
URL = {https://www.mdpi.com/2072-4292/12/12/1984},
ISSN = {2072-4292},
ABSTRACT = {Timely and accurate agricultural information is needed to inform resource allocation and sustainable practices to improve food security in the developing world. Obtaining this information through traditional surveys is time consuming and labor intensive, making it difficult to collect data at the frequency and resolution needed to accurately estimate the planted areas of key crops and their distribution during the growing season. Remote sensing technologies can be leveraged to provide consistent, cost-effective, and spatially disaggregated data at high temporal frequency. In this study, we used imagery acquired from unmanned aerial vehicles to create a high-fidelity ground-truth dataset that included examples of large mono-cropped fields, small intercropped fields, and natural vegetation. The imagery was acquired in three rounds of flights at six sites in different agro-ecological zones to capture growing conditions. This dataset was used to train and test a random forest model that was implemented in Google Earth Engine for classifying cropped land using freely available Sentinel-1 and -2 data. This model achieved an overall accuracy of 83%, and a 91% accuracy for maize specifically. The model results were compared with Rwanda&rsquo;s Seasonal Agricultural Survey, which highlighted biases in the dataset including a lack of examples of mixed land cover.},
DOI = {10.3390/rs12121984}
}



@Article{app10124247,
AUTHOR = {Zhong, Kefeng and Teng, Shuai and Liu, Gen and Chen, Gongfa and Cui, Fangsen},
TITLE = {Structural Damage Features Extracted by Convolutional Neural Networks from Mode Shapes},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {4247},
URL = {https://www.mdpi.com/2076-3417/10/12/4247},
ISSN = {2076-3417},
ABSTRACT = {This paper aims to locate damaged rods in a three-dimensional (3D) steel truss and reveals some internal working mechanisms of the convolutional neural network (CNN), which is based on the first-order modal parameters and CNN. The CNN training samples (including a large number of damage scenarios) are created by ABAQUS and PYTHON scripts. The mode shapes and mode curvature differences are taken as the inputs of the CNN training samples, respectively, and the damage locating accuracy of the CNN is investigated. Finally, the features extracted from each convolutional layer of the CNN are checked to reveal some internal working mechanisms of the CNN and explain the specific meanings of some features. The results show that the CNN-based damage detection method using mode shapes as the inputs has a higher locating accuracy for all damage degrees, while the method using mode curvature differences as the inputs has a lower accuracy for the targets with a low damage degree; however, with the increase of the target damage degree, it gradually achieves the same good locating accuracy as mode shapes. The features extracted from each convolutional layer show that the CNN can obtain the difference between the sample to be classified and the average of training samples in shallow layers, and then amplify the difference in the subsequent convolutional layer, which is similar to a power function, finally it produces a distinguishable peak signal at the damage location. Then a damage locating method is derived from the feature extraction of the CNN. All of these results indicate that the CNN using first-order modal parameters not only has a powerful damage location ability, but also opens up a new way to extract damage features from the measurement data.},
DOI = {10.3390/app10124247}
}



@Article{rs12121990,
AUTHOR = {Wagner, Matthias P. and Oppelt, Natascha},
TITLE = {Deep Learning and Adaptive Graph-Based Growing Contours for Agricultural Field Extraction},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1990},
URL = {https://www.mdpi.com/2072-4292/12/12/1990},
ISSN = {2072-4292},
ABSTRACT = {Field mapping and information on agricultural landscapes is of increasing importance for many applications. Monitoring schemes and national cadasters provide a rich source of information but their maintenance and regular updating is costly and labor-intensive. Automatized mapping of fields based on remote sensing imagery may aid in this task and allow for a faster and more regular observation. Although remote sensing has seen extensive use in agricultural research topics, such as plant health monitoring, crop type classification, yield prediction, and irrigation, field delineation and extraction has seen comparatively little research interest. In this study, we present a field boundary detection technique based on deep learning and a variety of image features, and combine it with the graph-based growing contours (GGC) method to extract agricultural fields in a study area in northern Germany. The boundary detection step only requires red, green, and blue (RGB) data and is therefore largely independent of the sensor used. We compare different image features based on color and luminosity information and evaluate their usefulness for the task of field boundary detection. A model based on texture metrics, gradient information, Hessian matrix eigenvalues, and local statistics showed good results with accuracies up to 88.2%, an area under the ROC curve (AUC) of up to 0.94, and F1 score of up to 0.88. The exclusive use of these universal image features may also facilitate transferability to other regions. We further present modifications to the GGC method intended to aid in upscaling of the method through process acceleration with a minimal effect on results. We combined the boundary detection results with the GGC method for field polygon extraction. Results were promising, with the new GGC version performing similarly or better than the original version while experiencing an acceleration of 1.3&times; to 2.3&times; on different subsets and input complexities. Further research may explore other applications of the GGC method outside agricultural remote sensing and field extraction.},
DOI = {10.3390/rs12121990}
}



@Article{rs12121994,
AUTHOR = {Luo, Xin and Tian, Xiaoyue and Zhang, Huijie and Hou, Weimin and Leng, Geng and Xu, Wenbo and Jia, Haitao and He, Xixu and Wang, Meng and Zhang, Jian},
TITLE = {Fast Automatic Vehicle Detection in UAV Images Using Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1994},
URL = {https://www.mdpi.com/2072-4292/12/12/1994},
ISSN = {2072-4292},
ABSTRACT = {Vehicle targets in unmanned aerial vehicle (UAV) images are generally small, so a significant amount of detailed information on targets may be lost after neural computing, which leads to the poor performances of the existing recognition algorithms. Based on convolutional neural networks that utilize the YOLOv3 algorithm, this article focuses on the development of a quick automatic vehicle detection method for UAV images. First, a vehicle dataset for target recognition is constructed. Then, a novel YOLOv3 vehicle detection framework is proposed according to the following characteristics: The vehicle targets in the UAV image are relatively small and dense. The average precision (AP) increased by 5.48%, from 92.01% to 97.49%, which still remains the rather high processing speed of the YOLO network. Finally, the proposed framework is tested using three datasets: COWC, VEDAI, and CAR. The experimental results demonstrate that our method had a better detection capability.},
DOI = {10.3390/rs12121994}
}



@Article{en13123223,
AUTHOR = {Foudeh, Husam A. and Luk, Patrick and Whidborne, James},
TITLE = {Application of Norm Optimal Iterative Learning Control to Quadrotor Unmanned Aerial Vehicle for Monitoring Overhead Power System},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3223},
URL = {https://www.mdpi.com/1996-1073/13/12/3223},
ISSN = {1996-1073},
ABSTRACT = {Wind disturbances and noise severely affect Unmanned Aerial Vehicles (UAV) when monitoring and finding faults in overhead power lines. Accordingly, we propose repetitive learning as a new solution for the problem. In particular, the performance of Iterative Learning Control (ILC) that are based on optimal approaches are examined, namely (i) Gradient-based ILC and (ii) Norm Optimal ILC. When considering the repetitive nature of fault-finding tasks for electrical overhead power lines, this study develops, implements and evaluates optimal ILC algorithms for a UAV model. Moreover, we suggest attempting a learning gain variation on the standard optimal algorithms instead of heuristically selecting from the previous range. The results of both simulations and experiments of gradient-based norm optimal control reveal that the proposed ILC algorithm has not only contributed to good trajectory tracking, but also good convergence speed and the ability to cope with exogenous disturbances such as wind gusts.},
DOI = {10.3390/en13123223}
}



@Article{rs12122000,
AUTHOR = {Kwan, Chiman and Ayhan, Bulent and Budavari, Bence and Lu, Yan and Perez, Daniel and Li, Jiang and Bernabe, Sergio and Plaza, Antonio},
TITLE = {Deep Learning for Land Cover Classification Using Only a Few Bands},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2000},
URL = {https://www.mdpi.com/2072-4292/12/12/2000},
ISSN = {2072-4292},
ABSTRACT = {There is an emerging interest in using hyperspectral data for land cover classification. The motivation behind using hyperspectral data is the notion that increasing the number of narrowband spectral channels would provide richer spectral information and thus help improve the land cover classification performance. Although hyperspectral data with hundreds of channels provide detailed spectral signatures, the curse of dimensionality might lead to degradation in the land cover classification performance. Moreover, in some practical applications, hyperspectral data may not be available due to cost, data storage, or bandwidth issues, and RGB and near infrared (NIR) could be the only image bands available for land cover classification. Light detection and ranging (LiDAR) data is another type of data to assist land cover classification especially if the land covers of interest have different heights. In this paper, we examined the performance of two Convolutional Neural Network (CNN)-based deep learning algorithms for land cover classification using only four bands (RGB+NIR) and five bands (RGB+NIR+LiDAR), where these limited number of image bands were augmented using Extended Multi-attribute Profiles (EMAP). The deep learning algorithms were applied to a well-known dataset used in the 2013 IEEE Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest. With EMAP augmentation, the two deep learning algorithms were observed to achieve better land cover classification performance using only four bands as compared to that using all 144 hyperspectral bands.},
DOI = {10.3390/rs12122000}
}



@Article{designs4020015,
AUTHOR = {Parvaresh, Ahmad and Abrazeh, Saber and Mohseni, Saeid-Reza and Zeitouni, Meisam Jahanshahi and Gheisarnejad, Meysam and Khooban, Mohammad-Hassan},
TITLE = {A Novel Deep Learning Backstepping Controller-Based Digital Twins Technology for Pitch Angle Control of Variable Speed Wind Turbine},
JOURNAL = {Designs},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2411-9660/4/2/15},
ISSN = {2411-9660},
ABSTRACT = {This paper proposes a deep deterministic policy gradient (DDPG) based nonlinear integral backstepping (NIB) in combination with model free control (MFC) for pitch angle control of variable speed wind turbine. In particular, the controller has been presented as a digital twin (DT) concept, which is an increasingly growing method in a variety of applications. In DDPG-NIB-MFC, the pitch angle is considered as the control input that depends on the optimal rotor speed, which is usually derived from effective wind speed. The system stability according to the Lyapunov theory can be achieved by the recursive nature of the backstepping theory and the integral action has been used to compensate for the steady-state error. Moreover, due to the nonlinear characteristics of wind turbines, the MFC aims to handle the un-modeled system dynamics and disturbances. The DDPG algorithm with actor-critic structure has been added in the proposed control structure to efficiently and adaptively tune the controller parameters embedded in the NIB controller. Under this effort, a digital twin of a presented controller is defined as a real-time and probabilistic model which is implemented on the digital signal processor (DSP) computing device. To ensure the performance of the proposed approach and output behavior of the system, software-in-loop (SIL) and hardware-in-loop (HIL) testing procedures have been considered. From the simulation and implementation outcomes, it can be concluded that the proposed backstepping controller based DDPG is more effective, robust, and adaptive than the backstepping and proportional-integral (PI) controllers optimized by particle swarm optimization (PSO) in the presence of uncertainties and disturbances.},
DOI = {10.3390/designs4020015}
}



@Article{rs12122002,
AUTHOR = {Panagiotou, Emmanouil and Chochlakis, Georgios and Grammatikopoulos, Lazaros and Charou, Eleni},
TITLE = {Generating Elevation Surface from a Single RGB Remotely Sensed Image Using Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2002},
URL = {https://www.mdpi.com/2072-4292/12/12/2002},
ISSN = {2072-4292},
ABSTRACT = {Generating Digital Elevation Models (DEM) from satellite imagery or other data sources constitutes an essential tool for a plethora of applications and disciplines, ranging from 3D flight planning and simulation, autonomous driving and satellite navigation, such as GPS, to modeling water flow, precision farming and forestry. The task of extracting this 3D geometry from a given surface hitherto requires a combination of appropriately collected corresponding samples and/or specialized equipment, as inferring the elevation from single image data is out of reach for contemporary approaches. On the other hand, Artificial Intelligence (AI) and Machine Learning (ML) algorithms have experienced unprecedented growth in recent years as they can extrapolate rules in a data-driven manner and retrieve convoluted, nonlinear one-to-one mappings, such as an approximate mapping from satellite imagery to DEMs. Therefore, we propose an end-to-end Deep Learning (DL) approach to construct this mapping and to generate an absolute or relative point cloud estimation of a DEM given a single RGB satellite (Sentinel-2 imagery in this work) or drone image. The model has been readily extended to incorporate available information from the non-visible electromagnetic spectrum. Unlike existing methods, we only exploit one image for the production of the elevation data, rendering our approach less restrictive and constrained, but suboptimal compared to them at the same time. Moreover, recent advances in software and hardware allow us to make the inference and the generation extremely fast, even on moderate hardware. We deploy Conditional Generative Adversarial networks (CGAN), which are the state-of-the-art approach to image-to-image translation. We expect our work to serve as a springboard for further development in this field and to foster the integration of such methods in the process of generating, updating and analyzing DEMs.},
DOI = {10.3390/rs12122002}
}



@Article{rs12122012,
AUTHOR = {Kucharczyk, Maja and Hay, Geoffrey J. and Ghaffarian, Salar and Hugenholtz, Chris H.},
TITLE = {Geographic Object-Based Image Analysis: A Primer and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2012},
URL = {https://www.mdpi.com/2072-4292/12/12/2012},
ISSN = {2072-4292},
ABSTRACT = {Geographic object-based image analysis (GEOBIA) is a remote sensing image analysis paradigm that defines and examines image-objects: groups of neighboring pixels that represent real-world geographic objects. Recent reviews have examined methodological considerations and highlighted how GEOBIA improves upon the 30+ year pixel-based approach, particularly for H-resolution imagery. However, the literature also exposes an opportunity to improve guidance on the application of GEOBIA for novice practitioners. In this paper, we describe the theoretical foundations of GEOBIA and provide a comprehensive overview of the methodological workflow, including: (i) software-specific approaches (open-source and commercial); (ii) best practices informed by research; and (iii) the current status of methodological research. Building on this foundation, we then review recent research on the convergence of GEOBIA with deep convolutional neural networks, which we suggest is a new form of GEOBIA. Specifically, we discuss general integrative approaches and offer recommendations for future research. Overall, this paper describes the past, present, and anticipated future of GEOBIA in a novice-accessible format, while providing innovation and depth to experienced practitioners.},
DOI = {10.3390/rs12122012}
}



@Article{rs12122019,
AUTHOR = {Zhao, Yibo and Lei, Shaogang and Yang, Xingchen and Gong, Chuangang and Wang, Cangjiao and Cheng, Wei and Li, Heng and She, Changchao},
TITLE = {Study on Spectral Response and Estimation of Grassland Plants Dust Retention Based on Hyperspectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2019},
URL = {https://www.mdpi.com/2072-4292/12/12/2019},
ISSN = {2072-4292},
ABSTRACT = {Accurate monitoring of plant dust retention can provide a basis for dust pollution control and environmental protection. The aims of this study were to analyze the spectral response features of grassland plants to mining dust and to predict the spatial distribution of dust retention using hyperspectral data. The dust retention content was determined by an electronic analytical balance and a leaf area meter. The leaf reflectance spectrum was measured by a handheld hyperspectral camera, and the airborne hyperspectral data were obtained using an imaging spectrometer. We analyzed the difference between the leaf spectral before and after dust removal. The sensitive spectra of dust retention on the leaf- and the canopy-scale were determined through two-dimensional correlation spectroscopy (2DCOS). The competitive adaptive reweighted sampling (CARS) algorithm was applied to select the feature bands of canopy dust retention. The estimation model of canopy dust retention was built through random forest regression (RFR), and the dust distribution map was obtained based on the airborne hyperspectral image. The results showed that dust retention enhanced the spectral reflectance of leaves in the visible wavelength but weakened the reflectance in the near-infrared wavelength. Caused by the canopy structure and multiple scattering, a slight difference in the sensitive spectra on dust retention existed between the canopy and leaves. Similarly, the sensitive spectra of leaves and the canopy were closely related to dust and plant physiological parameters. The estimation model constructed through 2DCOS-CARS-RFR showed higher precision, compared with genetic algorithm-random forest regression (GA-RFR) and simulated annealing algorithm-random forest regression (SAA-RFR). Spatially, the amount of canopy dust increased and then decreased with increasing distance from the mining area, reaching a maximum within 300&ndash;500 m. This study not only demonstrated the importance of extracting feature bands based on the response of plant physical and chemical parameters to dust, but also laid a foundation for the rapid and non-destructive monitoring of grassland plant dust retention.},
DOI = {10.3390/rs12122019}
}



@Article{ai1020021,
AUTHOR = {Barbedo, Jayme Garcia Arnal},
TITLE = {Detecting and Classifying Pests in Crops Using Proximal Images and Machine Learning: A Review},
JOURNAL = {AI},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {312--328},
URL = {https://www.mdpi.com/2673-2688/1/2/21},
ISSN = {2673-2688},
ABSTRACT = {Pest management is among the most important activities in a farm. Monitoring all different species visually may not be effective, especially in large properties. Accordingly, considerable research effort has been spent towards the development of effective ways to remotely monitor potential infestations. A growing number of solutions combine proximal digital images with machine learning techniques, but since species and conditions associated to each study vary considerably, it is difficult to draw a realistic picture of the actual state of the art on the subject. In this context, the objectives of this article are (1) to briefly describe some of the most relevant investigations on the subject of automatic pest detection using proximal digital images and machine learning; (2) to provide a unified overview of the research carried out so far, with special emphasis to research gaps that still linger; (3) to propose some possible targets for future research.},
DOI = {10.3390/ai1020021}
}



@Article{drones4020025,
AUTHOR = {Mury, Antoine and Collin, Antoine and Houet, Thomas and Alvarez-Vanhard, Emilien and James, Dorothée},
TITLE = {Using Multispectral Drone Imagery for Spatially Explicit Modeling of Wave Attenuation through a Salt Marsh Meadow},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {25},
URL = {https://www.mdpi.com/2504-446X/4/2/25},
ISSN = {2504-446X},
ABSTRACT = {Offering remarkable biodiversity, coastal salt marshes also provide a wide variety of ecosystem services: cultural services (leisure, tourist amenities), supply services (crop production, pastoralism) and regulation services including carbon sequestration and natural protection against coastal erosion and inundation. The consideration of this coastal protection ecosystem service takes part in a renewed vision of coastal risk management and especially marine flooding, with an emerging focus on &ldquo;nature-based solutions.&rdquo; Through this work, using remote-sensing methods, we propose a novel drone-based spatial modeling methodology of the salt marsh hydrodynamic attenuation at very high spatial resolution (VHSR). This indirect modeling is based on in situ measurements of significant wave heights (Hm0) that constitute the ground truth, as well as spectral and topographical predictors from VHSR multispectral drone imagery. By using simple and multiple linear regressions, we identify the contribution of predictors, taken individually, and jointly. The best individual drone-based predictor is the green waveband. Dealing with the addition of individual predictors to the red-green-blue (RGB) model, the highest gain is observed with the red edge waveband, followed by the near-infrared, then the digital surface model. The best full combination is the RGB enhanced by the red edge and the normalized difference vegetation index (coefficient of determination (R2): 0.85, root mean square error (RMSE): 0.20%/m).},
DOI = {10.3390/drones4020025}
}



@Article{rs12122026,
AUTHOR = {Bowler, Ellen and Fretwell, Peter T. and French, Geoffrey and Mackiewicz, Michal},
TITLE = {Using Deep Learning to Count Albatrosses from Space: Assessing Results in Light of Ground Truth Uncertainty},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2026},
URL = {https://www.mdpi.com/2072-4292/12/12/2026},
ISSN = {2072-4292},
ABSTRACT = {Many wildlife species inhabit inaccessible environments, limiting researchers ability to conduct essential population surveys. Recently, very high resolution (sub-metre) satellite imagery has enabled remote monitoring of certain species directly from space; however, manual analysis of the imagery is time-consuming, expensive and subjective. State-of-the-art deep learning approaches can automate this process; however, often image datasets are small, and uncertainty in ground truth labels can affect supervised training schemes and the interpretation of errors. In this paper, we investigate these challenges by conducting both manual and automated counts of nesting Wandering Albatrosses on four separate islands, captured by the 31 cm resolution WorldView-3 sensor. We collect counts from six observers, and train a convolutional neural network (U-Net) using leave-one-island-out cross-validation and different combinations of ground truth labels. We show that (1) interobserver variation in manual counts is significant and differs between the four islands, (2) the small dataset can limit the networks ability to generalise to unseen imagery and (3) the choice of ground truth labels can have a significant impact on our assessment of network performance. Our final results show the network detects albatrosses as accurately as human observers for two of the islands, while in the other two misclassifications are largely caused by the presence of noise, cloud cover and habitat, which was not present in the training dataset. While the results show promise, we stress the importance of considering these factors for any study where data is limited and observer confidence is variable.},
DOI = {10.3390/rs12122026}
}



@Article{s20133618,
AUTHOR = {Fuentes, Sigfredo and Torrico, Damir D. and Tongson, Eden and Gonzalez Viejo, Claudia},
TITLE = {Machine Learning Modeling of Wine Sensory Profiles and Color of Vertical Vintages of Pinot Noir Based on Chemical Fingerprinting, Weather and Management Data},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3618},
URL = {https://www.mdpi.com/1424-8220/20/13/3618},
ISSN = {1424-8220},
ABSTRACT = {Important wine quality traits such as sensory profile and color are the product of complex interactions between the soil, grapevine, the environment, management, and winemaking practices. Artificial intelligence (AI) and specifically machine learning (ML) could offer powerful tools to assess these complex interactions and their patterns through seasons to predict quality traits to winegrowers close to harvest and before winemaking. This study considered nine vintages (2008&ndash;2016) using near-infrared spectroscopy (NIR) of wines and corresponding weather and management information as inputs for artificial neural network (ANN) modeling of sensory profiles (Models 1 and 2 respectively). Furthermore, weather and management data were used as inputs to predict the color of wines (Model 3). Results showed high accuracy in the prediction of sensory profiles of vertical wine vintages using NIR (Model 1; R = 0.92; slope = 0.85), while better models were obtained using weather/management data for the prediction of sensory profiles (Model 2; R = 0.98; slope = 0.93) and wine color (Model 3; R = 0.99; slope = 0.98). For all models, there was no indication of overfitting as per ANN specific tests. These models may be used as powerful tools to winegrowers and winemakers close to harvest and before the winemaking process to maintain a determined wine style with high quality and acceptability by consumers.},
DOI = {10.3390/s20133618}
}



@Article{s20133664,
AUTHOR = {Zhang, Qichen and Zhu, Meiqiang and Zou, Liang and Li, Ming and Zhang, Yong},
TITLE = {Learning Reward Function with Matching Network for Mapless Navigation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3664},
URL = {https://www.mdpi.com/1424-8220/20/13/3664},
ISSN = {1424-8220},
ABSTRACT = {Deep reinforcement learning (DRL) has been successfully applied in mapless navigation. An important issue in DRL is to design a reward function for evaluating actions of agents. However, designing a robust and suitable reward function greatly depends on the designer&rsquo;s experience and intuition. To address this concern, we consider employing reward shaping from trajectories on similar navigation tasks without human supervision, and propose a general reward function based on matching network (MN). The MN-based reward function is able to gain the experience by pre-training through trajectories on different navigation tasks and accelerate the training speed of DRL in new tasks. The proposed reward function keeps the optimal strategy of DRL unchanged. The simulation results on two static maps show that the DRL converge with less iterations via the learned reward function than the state-of-the-art mapless navigation methods. The proposed method performs well in dynamic maps with partially moving obstacles. Even when test maps are different from training maps, the proposed strategy is able to complete the navigation tasks without additional training.},
DOI = {10.3390/s20133664}
}



@Article{e22070723,
AUTHOR = {Humaidi, Amjad J. and Ibraheem, Ibraheem Kasim and Azar, Ahmad Taher and Sadiq, Musaab E.},
TITLE = {A New Adaptive Synergetic Control Design for Single Link Robot Arm Actuated by Pneumatic Muscles},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {723},
URL = {https://www.mdpi.com/1099-4300/22/7/723},
ISSN = {1099-4300},
ABSTRACT = {This paper suggests a new control design based on the concept of Synergetic Control theory for controlling a one-link robot arm actuated by Pneumatic artificial muscles (PAMs) in opposing bicep/tricep positions. The synergetic control design is first established based on known system parameters. However, in real PAM-actuated systems, the uncertainties are inherited features in their parameters and hence an adaptive synergetic control algorithm is proposed and synthesized for a PAM-actuated robot arm subjected to perturbation in its parameters. The adaptive synergetic laws are developed to estimate the uncertainties and to guarantee the asymptotic stability of the adaptive synergetic controlled PAM-actuated system. The work has also presented an improvement in the performance of proposed synergetic controllers (classical and adaptive) by applying a modern optimization technique based on Particle Swarm Optimization (PSO) to tune their design parameters towards optimal dynamic performance. The effectiveness of the proposed classical and adaptive synergetic controllers has been verified via computer simulation and it has been shown that the adaptive controller could cope with uncertainties and keep the controlled system stable. The proposed optimal Adaptive Synergetic Controller (ASC) has been validated with a previous adaptive controller with the same robot structure and actuation, and it has been shown that the optimal ASC outperforms its opponent in terms of tracking speed and error.},
DOI = {10.3390/e22070723}
}



@Article{app10134574,
AUTHOR = {Ghaffarian, Saman and Rezaie Farhadabad, Ali and Kerle, Norman},
TITLE = {Post-Disaster Recovery Monitoring with Google Earth Engine},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {4574},
URL = {https://www.mdpi.com/2076-3417/10/13/4574},
ISSN = {2076-3417},
ABSTRACT = {Post-disaster recovery is a complex process in terms of measuring its progress after a disaster and understanding its components and influencing factors. During this process, disaster planners and governments need reliable information to make decisions towards building the affected region back to normal (pre-disaster), or even improved, conditions. Hence, it is essential to use methods to understand the dynamics/variables of the post-disaster recovery process, and rapid and cost-effective data and tools to monitor the process. Google Earth Engine (GEE) provides free access to vast amounts of remote sensing (RS) data and a powerful computing environment in a cloud platform, making it an attractive tool to analyze earth surface data. In this study we assessed the suitability of GEE to analyze and track recovery. To do so, we employed GEE to assess the recovery process over a three-year period after Typhoon Haiyan, which struck Leyte island, in the Philippines, in 2013. We developed an approach to (i) generate cloud and shadow-free image composites from Landsat 7 and 8 satellite imagery and produce land cover classification data using the Random Forest method, and (ii) generate damage and recovery maps based on post-classification change analysis. The method produced land cover maps with accuracies &gt;88%. We used the model to produce damage and three time-step recovery maps for 62 municipalities on Leyte island. The results showed that most of the municipalities had recovered after three years in terms of returning to the pre-disaster situation based on the selected land cover change analysis. However, more analysis (e.g., functional assessment) based on detailed data (e.g., land use maps) is needed to evaluate the more complex and subtle socio-economic aspects of the recovery. The study showed that GEE has good potential for monitoring the recovery process for extensive regions. However, the most important limitation is the lack of very-high-resolution RS data that are critical to assess the process in detail, in particular in complex urban environments.},
DOI = {10.3390/app10134574}
}



@Article{s20133750,
AUTHOR = {Tantiparimongkol, Lalida and Phasukkit, Pattarapong},
TITLE = {IR-UWB Pulse Generation Using FPGA Scheme for through Obstacle Human Detection},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3750},
URL = {https://www.mdpi.com/1424-8220/20/13/3750},
ISSN = {1424-8220},
ABSTRACT = {This research proposes a scheme of field programmable gate array (FPGA) to generate an impulse-radio ultra-wideband (IR-UWB) pulse. The FPGA scheme consists of three parts: digital clock manager, four-delay-paths stratagem, and edge combiner. The IR-UWB radar system is designed to detect human subjects from their respiration underneath the rubble in the aftermath of an earthquake and to locate the human subjects based on range estimation. The proposed IR-UWB radar system is experimented with human subjects lying underneath layers of stacked clay bricks in supine and prone position. The results reveal that the IR-UWB radar system achieves a pulse duration of 540 ps with a bandwidth of 2.073 GHz (fractional bandwidth of 1.797). In addition, the IR-UWB technology can detect human subjects underneath the rubble from respiration and identify the location of human subjects by range estimation. The novelty of this research lies in the use of the FPGA scheme to achieve an IR-UWB pulse with a 2.073 GHz (117 MHz&ndash;2.19 GHz) bandwidth, thereby rendering the technology suitable for a wide range of applications, in addition to through-obstacle detection.},
DOI = {10.3390/s20133750}
}



@Article{agriculture10070277,
AUTHOR = {García-Martínez, Héctor and Flores-Magdaleno, Héctor and Ascencio-Hernández, Roberto and Khalil-Gardezi, Abdul and Tijerina-Chávez, Leonardo and Mancilla-Villa, Oscar R. and Vázquez-Peña, Mario A.},
TITLE = {Corn Grain Yield Estimation from Vegetation Indices, Canopy Cover, Plant Density, and a Neural Network Using Multispectral and RGB Images Acquired with Unmanned Aerial Vehicles},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {277},
URL = {https://www.mdpi.com/2077-0472/10/7/277},
ISSN = {2077-0472},
ABSTRACT = {Corn yields vary spatially and temporally in the plots as a result of weather, altitude, variety, plant density, available water, nutrients, and planting date; these are the main factors that influence crop yield. In this study, different multispectral and red-green-blue (RGB) vegetation indices were analyzed, as well as the digitally estimated canopy cover and plant density, in order to estimate corn grain yield using a neural network model. The relative importance of the predictor variables was also analyzed. An experiment was established with five levels of nitrogen fertilization (140, 200, 260, 320, and 380 kg/ha) and four replicates, in a completely randomized block design, resulting in 20 experimental polygons. Crop information was captured using two sensors (Parrot Sequoia_4.9, and DJI FC6310_8.8) mounted on an unmanned aerial vehicle (UAV) for two flight dates at 47 and 79 days after sowing (DAS). The correlation coefficient between the plant density, obtained through the digital count of corn plants, and the corn grain yield was 0.94; this variable was the one with the highest relative importance in the yield estimation according to Garson&rsquo;s algorithm. The canopy cover, digitally estimated, showed a correlation coefficient of 0.77 with respect to the corn grain yield, while the relative importance of this variable in the yield estimation was 0.080 and 0.093 for 47 and 79 DAS, respectively. The wide dynamic range vegetation index (WDRVI), plant density, and canopy cover showed the highest correlation coefficient and the smallest errors (R = 0.99, mean absolute error (MAE) = 0.028 t ha&minus;1, root mean square error (RMSE) = 0.125 t ha&minus;1) in the corn grain yield estimation at 47 DAS, with the WDRVI index and the density being the variables with the highest relative importance for this crop development date. For the 79 DAS flight, the combination of the normalized difference vegetation index (NDVI), normalized difference red edge (NDRE), WDRVI, excess green (EXG), triangular greenness index (TGI), and visible atmospherically resistant index (VARI), as well as plant density and canopy cover, generated the highest correlation coefficient and the smallest errors (R = 0.97, MAE = 0.249 t ha&minus;1, RMSE = 0.425 t ha&minus;1) in the corn grain yield estimation, where the density and the NDVI were the variables with the highest relative importance, with values of 0.295 and 0.184, respectively. However, the WDRVI, plant density, and canopy cover estimated the corn grain yield with acceptable precision (R = 0.96, MAE = 0.209 t ha&minus;1, RMSE = 0.449 t ha&minus;1). The generated neural network models provided a high correlation coefficient between the estimated and the observed corn grain yield, and also showed acceptable errors in the yield estimation. The spectral information registered through remote sensors mounted on unmanned aerial vehicles and its processing in vegetation indices, canopy cover, and plant density allowed the characterization and estimation of corn grain yield. Such information is very useful for decision-making and agricultural activities planning.},
DOI = {10.3390/agriculture10070277}
}



@Article{rs12142188,
AUTHOR = {Niculescu, Simona and Boissonnat, Jean-Baptiste and Lardeux, Cédric and Roberts, Dar and Hanganu, Jenica and Billey, Antoine and Constantinescu, Adrian and Doroftei, Mihai},
TITLE = {Synergy of High-Resolution Radar and Optical Images Satellite for Identification and Mapping of Wetland Macrophytes on the Danube Delta},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2188},
URL = {https://www.mdpi.com/2072-4292/12/14/2188},
ISSN = {2072-4292},
ABSTRACT = {In wetland environments, vegetation has an important role in ecological functioning. The main goal of this work was to identify an optimal combination of Sentinel-1 (S1), Sentinel-2 (S2), and Pleiades data using ground-reference data to accurately map wetland macrophytes in the Danube Delta. We tested several combinations of optical and Synthetic Aperture Radar (SAR) data rigorously at two levels. First, in order to reduce the confusion between reed (Phragmites australis (Cav.) Trin. ex Steud.) and other macrophyte communities, a time series analysis of S1 data was performed. The potential of S1 for detection of compact reed on plaur, compact reed on plaur/reed cut, open reed on plaur, pure reed, and reed on salinized soil was evaluated through time series of backscatter coefficient and coherence ratio images, calculated mainly according to the phenology of the reed. The analysis of backscattering coefficients allowed separation of reed classes that strongly overlapped. The coherence coefficient showed that C-band SAR repeat pass interferometric coherence for cut reed detection is feasible. In the second section, random forest (RF) classification was applied to the S2, Pleiades, and S1 data and in situ observations to discriminate and map reed against other aquatic macrophytes (submerged aquatic vegetation (SAV), emergent macrophytes, some floating broad-leaved and floating vegetation of delta lakes). In addition, different optical indices were included in the RF. A total of 67 classification models were made in several sensor combinations with two series of validation samples (with the reed and without reed) using both a simple and more detailed classification schema. The results showed that reed is completely discriminable compared to other macrophyte communities with all sensor combinations. In all combinations, the model-based producer’s accuracy (PA) and user’s accuracy (UA) for reed with both nomenclatures were over 90%. The diverse combinations of sensors were valuable for improving the overall classification accuracy of all of the communities of aquatic macrophytes except Myriophyllum spicatum L.},
DOI = {10.3390/rs12142188}
}



@Article{app10144735,
AUTHOR = {McClellan, Miranda and Cervelló-Pastor, Cristina and Sallent, Sebastià},
TITLE = {Deep Learning at the Mobile Edge: Opportunities for 5G Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4735},
URL = {https://www.mdpi.com/2076-3417/10/14/4735},
ISSN = {2076-3417},
ABSTRACT = {Mobile edge computing (MEC) within 5G networks brings the power of cloud computing, storage, and analysis closer to the end user. The increased speeds and reduced delay enable novel applications such as connected vehicles, large-scale IoT, video streaming, and industry robotics. Machine Learning (ML) is leveraged within mobile edge computing to predict changes in demand based on cultural events, natural disasters, or daily commute patterns, and it prepares the network by automatically scaling up network resources as needed. Together, mobile edge computing and ML enable seamless automation of network management to reduce operational costs and enhance user experience. In this paper, we discuss the state of the art for ML within mobile edge computing and the advances needed in automating adaptive resource allocation, mobility modeling, security, and energy efficiency for 5G networks.},
DOI = {10.3390/app10144735}
}



@Article{electronics9071120,
AUTHOR = {Liang, Chao and Shanmugam, Bharanidharan and Azam, Sami and Karim, Asif and Islam, Ashraful and Zamani, Mazdak and Kavianpour, Sanaz and Idris, Norbik Bashah},
TITLE = {Intrusion Detection System for the Internet of Things Based on Blockchain and Multi-Agent Systems},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1120},
URL = {https://www.mdpi.com/2079-9292/9/7/1120},
ISSN = {2079-9292},
ABSTRACT = {With the popularity of Internet of Things (IoT) technology, the security of the IoT network has become an important issue. Traditional intrusion detection systems have their limitations when applied to the IoT network due to resource constraints and the complexity. This research focusses on the design, implementation and testing of an intrusion detection system which uses a hybrid placement strategy based on a multi-agent system, blockchain and deep learning algorithms. The system consists of the following modules: data collection, data management, analysis, and response. The National security lab&ndash;knowledge discovery and data mining NSL-KDD dataset is used to test the system. The results demonstrate the efficiency of deep learning algorithms when detecting attacks from the transport layer. The experiment indicates that deep learning algorithms are suitable for intrusion detection in IoT network environment.},
DOI = {10.3390/electronics9071120}
}



@Article{electronics9071121,
AUTHOR = {Kong, Weiren and Zhou, Deyun and Yang, Zhen and Zhao, Yiyang and Zhang, Kai},
TITLE = {UAV Autonomous Aerial Combat Maneuver Strategy Generation with Observation Error Based on State-Adversarial Deep Deterministic Policy Gradient and Inverse Reinforcement Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1121},
URL = {https://www.mdpi.com/2079-9292/9/7/1121},
ISSN = {2079-9292},
ABSTRACT = {With the development of unmanned aerial vehicle (UAV) and artificial intelligence (AI) technology, Intelligent UAV will be widely used in future autonomous aerial combat. Previous researches on autonomous aerial combat within visual range (WVR) have limitations due to simplifying assumptions, limited robustness, and ignoring sensor errors. In this paper, in order to consider the error of the aircraft sensors, we model the aerial combat WVR as a state-adversarial Markov decision process (SA-MDP), which introduce the small adversarial perturbations on state observations and these perturbations do not alter the environment directly, but can mislead the agent into making suboptimal decisions. Meanwhile, we propose a novel autonomous aerial combat maneuver strategy generation algorithm with high-performance and high-robustness based on state-adversarial deep deterministic policy gradient algorithm (SA-DDPG), which add a robustness regularizers related to an upper bound on performance loss at the actor-network. At the same time, a reward shaping method based on maximum entropy (MaxEnt) inverse reinforcement learning algorithm (IRL) is proposed to improve the aerial combat strategy generation algorithm&rsquo;s efficiency. Finally, the efficiency of the aerial combat strategy generation algorithm and the performance and robustness of the resulting aerial combat strategy is verified by simulation experiments. Our main contributions are three-fold. First, to introduce the observation errors of UAV, we are modeling air combat as SA-MDP. Second, to make the strategy network of air combat maneuver more robust in the presence of observation errors, we introduce regularizers into the policy gradient. Third, to solve the problem that air combat&rsquo;s reward function is too sparse, we use MaxEnt IRL to design a shaping reward to accelerate the convergence of SA-DDPG.},
DOI = {10.3390/electronics9071121}
}



@Article{rs12142213,
AUTHOR = {Vitrack-Tamam, Snir and Holtzman, Lilach and Dagan, Reut and Levi, Shai and Tadmor, Yuval and Azizi, Tamir and Rabinovitz, Onn and Naor, Amos and Liran, Oded},
TITLE = {Random Forest Algorithm Improves Detection of Physiological Activity Embedded within Reflectance Spectra Using Stomatal Conductance as a Test Case},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2213},
URL = {https://www.mdpi.com/2072-4292/12/14/2213},
ISSN = {2072-4292},
ABSTRACT = {Plants transpire water through their tissues in order to move nutrients and water to the cells. Transpiration includes various mechanisms, primarily stomata movement, which controls the rate of CO2 and water vapor exchange between the tissues and the atmosphere. Assessment of stomatal conductance is available for gas exchange techniques at leaf level, yet these techniques are not scalable to the whole plant let alone a large vegetation area. Hyperspectral reflectance spectroscopy, which acquires hundreds of bands in a single scan, may capture a glimpse of the crop&rsquo;s physiological activity and therefore meet the scalability challenge. In this study, classic chemometric analyses are used alongside advanced statistical learning algorithms in order to identify stomatal conductance cues in hyperspectral measurements of cotton plants experiencing a gradient of irrigation. Random forest of regression trees identified 23 wavelengths related to both structural properties of the plant as well as water content. Partial least squares regression succeeded in relating these wavelengths to stomatal conductance, but only partially (R2 &lt; 0.2). An artificial neural network algorithm reported an R2 = 0.54 with an 89% error-free performance on the same data subset. This study discusses implementation of machine learning methodologies as a benchmark for deeper analysis of spectral information, such as required when searching for plant physiology-related attenuations embedded within reflectance spectra.},
DOI = {10.3390/rs12142213}
}



@Article{e22070765,
AUTHOR = {Wang, Pengfei and Gao, Yanbin and Wu, Menghao and Zhang, Fan and Li, Guangchun and Qin, Chao},
TITLE = {A Denoising Method for Fiber Optic Gyroscope Based on Variational Mode Decomposition and Beetle Swarm Antenna Search Algorithm},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {765},
URL = {https://www.mdpi.com/1099-4300/22/7/765},
ISSN = {1099-4300},
ABSTRACT = {Fiber optic gyroscope (FOG) is one of the important components of Inertial Navigation Systems (INS). In order to improve the accuracy of the INS, it is necessary to suppress the random error of the FOG signal. In this paper, a variational mode decomposition (VMD) denoising method based on beetle swarm antenna search (BSAS) algorithm is proposed to reduce the noise in FOG signal. Firstly, the BSAS algorithm is introduced in detail. Then, the permutation entropy of the band-limited intrinsic mode functions (BLIMFs) is taken as the optimization index, and two key parameters of VMD algorithm, including decomposition mode number K and quadratic penalty factor    &alpha;   , are optimized by using the BSAS algorithm. Next, a new method based on Hausdorff distance (HD) between the probability density function (PDF) of all BLIMFs and that of the original signal is proposed in this paper to determine the relevant modes. Finally, the selected BLIMF components are reconstructed to get the denoised signal. In addition, the simulation results show that the proposed scheme is better than the existing schemes in terms of noise reduction performance. Two experiments further demonstrate the priority of the proposed scheme in the FOG noise reduction compared with other schemes.},
DOI = {10.3390/e22070765}
}



@Article{agriengineering2030029,
AUTHOR = {Gao, Zongmei and Luo, Zhongwei and Zhang, Wen and Lv, Zhenzhen and Xu, Yanlei},
TITLE = {Deep Learning Application in Plant Stress Imaging: A Review},
JOURNAL = {AgriEngineering},
VOLUME = {2},
YEAR = {2020},
NUMBER = {3},
PAGES = {430--446},
URL = {https://www.mdpi.com/2624-7402/2/3/29},
ISSN = {2624-7402},
ABSTRACT = {Plant stress is one of major issues that cause significant economic loss for growers. The labor-intensive conventional methods for identifying the stressed plants constrain their applications. To address this issue, rapid methods are in urgent needs. Developments of advanced sensing and machine learning techniques trigger revolutions for precision agriculture based on deep learning and big data. In this paper, we reviewed the latest deep learning approaches pertinent to the image analysis of crop stress diagnosis. We compiled the current sensor tools and deep learning principles involved in plant stress phenotyping. In addition, we reviewed a variety of deep learning applications/functions with plant stress imaging, including classification, object detection, and segmentation, of which are closely intertwined. Furthermore, we summarized and discussed the current challenges and future development avenues in plant phenotyping.},
DOI = {10.3390/agriengineering2030029}
}



@Article{s20143923,
AUTHOR = {Jamil, Sonain and Fawad and Rahman, MuhibUr and Ullah, Amin and Badnava, Salman and Forsat, Masoud and Mirjavadi, Seyed Sajad},
TITLE = {Malicious UAV Detection Using Integrated Audio and Visual Features for Public Safety Applications},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3923},
URL = {https://www.mdpi.com/1424-8220/20/14/3923},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have become popular in surveillance, security, and remote monitoring. However, they also pose serious security threats to public privacy. The timely detection of a malicious drone is currently an open research issue for security provisioning companies. Recently, the problem has been addressed by a plethora of schemes. However, each plan has a limitation, such as extreme weather conditions and huge dataset requirements. In this paper, we propose a novel framework consisting of the hybrid handcrafted and deep feature to detect and localize malicious drones from their sound and image information. The respective datasets include sounds and occluded images of birds, airplanes, and thunderstorms, with variations in resolution and illumination. Various kernels of the support vector machine (SVM) are applied to classify the features. Experimental results validate the improved performance of the proposed scheme compared to other related methods.},
DOI = {10.3390/s20143923}
}



@Article{rs12142268,
AUTHOR = {Zhou, Tian and Hasheminasab, Seyyed Meghdad and Ravi, Radhika and Habib, Ayman},
TITLE = {LiDAR-Aided Interior Orientation Parameters Refinement Strategy for Consumer-Grade Cameras Onboard UAV Remote Sensing Systems},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2268},
URL = {https://www.mdpi.com/2072-4292/12/14/2268},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are quickly emerging as a popular platform for 3D reconstruction/modeling in various applications such as precision agriculture, coastal monitoring, and emergency management. For such applications, LiDAR and frame cameras are the two most commonly used sensors for 3D mapping of the object space. For example, point clouds for the area of interest can be directly derived from LiDAR sensors onboard UAVs equipped with integrated global navigation satellite systems and inertial navigation systems (GNSS/INS). Imagery-based mapping, on the other hand, is considered to be a cost-effective and practical option and is often conducted by generating point clouds and orthophotos using structure from motion (SfM) techniques. Mapping with photogrammetric approaches requires accurate camera interior orientation parameters (IOPs), especially when direct georeferencing is utilized. Most state-of-the-art approaches for determining/refining camera IOPs depend on ground control points (GCPs). However, establishing GCPs is expensive and labor-intensive, and more importantly, the distribution and number of GCPs are usually less than optimal to provide adequate control for determining and/or refining camera IOPs. Moreover, consumer-grade cameras with unstable IOPs have been widely used for mapping applications. Therefore, in such scenarios, where frequent camera calibration or IOP refinement is required, GCP-based approaches are impractical. To eliminate the need for GCPs, this study uses LiDAR data as a reference surface to perform in situ refinement of camera IOPs. The proposed refinement strategy is conducted in three main steps. An image-based sparse point cloud is first generated via a GNSS/INS-assisted SfM strategy. Then, LiDAR points corresponding to the resultant image-based sparse point cloud are identified through an iterative plane fitting approach and are referred to as LiDAR control points (LCPs). Finally, IOPs of the utilized camera are refined through a GNSS/INS-assisted bundle adjustment procedure using LCPs. Seven datasets over two study sites with a variety of geomorphic features are used to evaluate the performance of the developed strategy. The results illustrate the ability of the proposed approach to achieve an object space absolute accuracy of 3&ndash;5 cm (i.e., 5&ndash;10 times the ground sampling distance) at a 41 m flying height.},
DOI = {10.3390/rs12142268}
}



@Article{rs12142278,
AUTHOR = {Segarra, Joel and González-Torralba, Jon and Aranjuelo, Íker and Araus, Jose Luis and Kefauver, Shawn C.},
TITLE = {Estimating Wheat Grain Yield Using Sentinel-2 Imagery and Exploring Topographic Features and Rainfall Effects on Wheat Performance in Navarre, Spain},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2278},
URL = {https://www.mdpi.com/2072-4292/12/14/2278},
ISSN = {2072-4292},
ABSTRACT = {Reliable methods for estimating wheat grain yield before harvest could help improve farm management and, if applied on a regional level, also help identify spatial factors that influence yield. Regional grain yield can be estimated using conventional methods, but the typical process is complex and labor-intensive. Here we describe the development of a streamlined approach using publicly accessible agricultural data, field-level yield, and remote sensing data from Sentinel-2 satellite to estimate regional wheat grain yield. We validated our method on wheat croplands in Navarre in northern Spain, which features heterogeneous topography and rainfall. First, this study developed stepwise multilinear equations to estimate grain yield based on various vegetation indices, which were measured at various phenological stages in order to determine the optimal timings. Second, the most suitable model was used to estimate grain yield in wheat parcels mapped from Sentinel-2 satellite images. We used a supervised pixel-based random forest classification and the estimates were compared to government-published post-harvest yield statistics. When tested, the model achieved an R2 of 0.83 in predicting grain yield at field level. The wheat parcels were mapped with an accuracy close to 86% for both overall accuracy and compared to official statistics. Third, the validated model was used to explore potential relationships of the mapped per-parcel grain yield estimation with topographic features and rainfall by using geographically weighted regressions. Topographic features and rainfall together accounted for an average for 11 to 20% of the observed spatial variation in grain yield in Navarre. These results highlight the ability of our method for estimating wheat grain yield before harvest and determining spatial factors that influence yield at the regional scale.},
DOI = {10.3390/rs12142278}
}



@Article{ani10071207,
AUTHOR = {Akçay, Hüseyin Gökhan and Kabasakal, Bekir and Aksu, Duygugül and Demir, Nusret and Öz, Melih and Erdoğan, Ali},
TITLE = {Automated Bird Counting with Deep Learning for Regional Bird Distribution Mapping},
JOURNAL = {Animals},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1207},
URL = {https://www.mdpi.com/2076-2615/10/7/1207},
PubMedID = {32708550},
ISSN = {2076-2615},
ABSTRACT = {A challenging problem in the field of avian ecology is deriving information on bird population movement trends. This necessitates the regular counting of birds which is usually not an easily-achievable task. A promising attempt towards solving the bird counting problem in a more consistent and fast way is to predict the number of birds in different regions from their photos. For this purpose, we exploit the ability of computers to learn from past data through deep learning which has been a leading sub-field of AI for image understanding. Our data source is a collection of on-ground photos taken during our long run of birding activity. We employ several state-of-the-art generic object-detection algorithms to learn to detect birds, each being a member of one of the 38 identified species, in natural scenes. The experiments revealed that computer-aided counting outperformed the manual counting with respect to both accuracy and time. As a real-world application of image-based bird counting, we prepared the spatial bird order distribution and species diversity maps of Turkey by utilizing the geographic information system (GIS) technology. Our results suggested that deep learning can assist humans in bird monitoring activities and increase citizen scientists&rsquo; participation in large-scale bird surveys.},
DOI = {10.3390/ani10071207}
}



@Article{app10144870,
AUTHOR = {Coviello, Luca and Cristoforetti, Marco and Jurman, Giuseppe and Furlanello, Cesare},
TITLE = {GBCNet: In-Field Grape Berries Counting for Yield Estimation by Dilated CNNs},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4870},
URL = {https://www.mdpi.com/2076-3417/10/14/4870},
ISSN = {2076-3417},
ABSTRACT = {We introduce here the Grape Berries Counting Net (GBCNet), a tool for accurate fruit yield estimation from smartphone cameras, by adapting Deep Learning algorithms originally developed for crowd counting. We test GBCNet using cross-validation procedure on two original datasets CR1 and CR2 of grape pictures taken in-field before veraison. A total of 35,668 berries have been manually annotated for the task. GBCNet achieves good performances on both the seven grape varieties dataset CR1, although with a different accuracy level depending on the variety, and on the single variety dataset CR2: in particular Mean Average Error (MAE) ranges from 0.85% for Pinot Gris to 11.73% for Marzemino on CR1 and reaches 7.24% on the Teroldego CR2 dataset.},
DOI = {10.3390/app10144870}
}



@Article{s20143948,
AUTHOR = {Fu, Wenpeng and Liu, Ran and Wang, Heng and Ali, Rashid and He, Yongping and Cao, Zhiqiang and Qin, Zhenghong},
TITLE = {A Method of Multiple Dynamic Objects Identification and Localization Based on Laser and RFID},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3948},
URL = {https://www.mdpi.com/1424-8220/20/14/3948},
ISSN = {1424-8220},
ABSTRACT = {In an indoor environment, object identification and localization are paramount for human-object interaction. Visual or laser-based sensors can achieve the identification and localization of the object based on its appearance, but these approaches are computationally expensive and not robust against the environment with obstacles. Radio Frequency Identification (RFID) has a unique tag ID to identify the object, but it cannot accurately locate it. Therefore, in this paper, the data of RFID and laser range finder are fused for the better identification and localization of multiple dynamic objects in an indoor environment. The main method is to use the laser range finder to estimate the radial velocities of objects in a certain environment, and match them with the object&rsquo;s radial velocities estimated by the RFID phase. The method also uses a fixed time series as &ldquo;sliding time window&rdquo; to find the cluster with the highest similarity of each RFID tag in each window. Moreover, the Pearson correlation coefficient (PCC) is used in the update stage of the particle filter (PF) to estimate the moving path of each cluster in order to improve the accuracy in a complex environment with obstacles. The experiments were verified by a SCITOS G5 robot. The results show that this method can achieve an matching rate of 90.18% and a localization accuracy of 0.33m in an environment with the presence of obstacles. This method effectively improves the matching rate and localization accuracy of multiple objects in indoor scenes when compared to the Bray-Curtis (BC) similarity matching-based approach as well as the particle filter-based approach.},
DOI = {10.3390/s20143948}
}



@Article{rs12142283,
AUTHOR = {Battulwar, Rushikesh and Winkelmaier, Garrett and Valencia, Jorge and Naghadehi, Masoud Zare and Peik, Bijan and Abbasi, Behrooz and Parvin, Bahram and Sattarvand, Javad},
TITLE = {A Practical Methodology for Generating High-Resolution 3D Models of Open-Pit Slopes Using UAVs: Flight Path Planning and Optimization},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2283},
URL = {https://www.mdpi.com/2072-4292/12/14/2283},
ISSN = {2072-4292},
ABSTRACT = {High-resolution terrain models of open-pit mine highwalls and benches are essential in developing new automated slope monitoring systems for operational optimization. This paper presents several contributions to the field of remote sensing in surface mines providing a practical framework for generating high-resolution images using low-trim Unmanned Aerial Vehicles (UAVs). First, a novel mobile application was developed for autonomous drone flights to follow mine terrain and capture high-resolution images of the mine surface. In this article, case study is presented showcasing the ability of developed software to import area terrain, plan the flight accordingly, and finally execute the area mapping mission autonomously. Next, to model the drone&rsquo;s battery performance, empirical studies were conducted considering various flight scenarios. A multivariate linear regression model for drone power consumption was derived from experimental data. The model has also been validated using data from a test flight. Finally, a genetic algorithm for solving the problem of flight planning and optimization has been employed. The developed power consumption model was used as the fitness function in the genetic algorithm. The designed algorithm was then validated using simulation studies. It is shown that the offered path optimization can reduce the time and energy of high-resolution imagery missions by over 50%. The current work provides a practical framework for stability monitoring of open-pit highwalls while achieving required energy optimization and imagery performance.},
DOI = {10.3390/rs12142283}
}



@Article{f11070763,
AUTHOR = {Klemmt, Hans-Joachim and Seitz, Rudolf and Straub, Christoph},
TITLE = {Application of Haralick’s Texture Features for Rapid Detection of Windthrow Hotspots in Orthophotos},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {763},
URL = {https://www.mdpi.com/1999-4907/11/7/763},
ISSN = {1999-4907},
ABSTRACT = {Windthrow and storm damage are crucial issues in practical forestry. We propose a method for rapid detection of windthrow hotspots in airborne digital orthophotos. Therefore, we apply Haralick&rsquo;s texture features on 50 &times; 50 m cells of the orthophotos and classify the cells with a random forest algorithm. We apply the classification results from a training data set on a validation set. The overall classification accuracy of the proposed method varies between 76% for fine distinction of the cells and 96% for a distinction level that tried to detect only severe damaged cells. The proposed method enables the rapid detection of windthrow hotspots in forests immediately after their occurrence in single-date data. It is not adequate for the determination of areas with only single fallen trees. Future research will investigate the possibilities and limitations when applying the method on other data sources (e.g., optical satellite data).},
DOI = {10.3390/f11070763}
}



@Article{s20143954,
AUTHOR = {Ahmed, Habib and La, Hung Manh and Gucunski, Nenad},
TITLE = {Review of Non-Destructive Civil Infrastructure Evaluation for Bridges: State-of-the-Art Robotic Platforms, Sensors and Algorithms},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3954},
URL = {https://www.mdpi.com/1424-8220/20/14/3954},
ISSN = {1424-8220},
ABSTRACT = {The non-destructive evaluation (NDE) of civil infrastructure has been an active area of research in recent decades. The traditional inspection of civil infrastructure mostly relies on visual inspection using human inspectors. To facilitate this process, different sensors for data collection and techniques for data analyses have been used to effectively carry out this task in an automated fashion. This review-based study will examine some of the recent developments in the field of autonomous robotic platforms for NDE and the structural health monitoring (SHM) of bridges. Some of the salient features of this review-based study will be discussed in the light of the existing surveys and reviews that have been published in the recent past, which will enable the clarification regarding the novelty of the present review-based study. The review methodology will be discussed in sufficient depth, which will provide insights regarding some of the primary aspects of the review methodology followed by this review-based study. In order to provide an in-depth examination of the state-of-the-art, the current research will examine the three major research streams. The first stream relates to technological robotic platforms developed for NDE of bridges. The second stream of literature examines myriad sensors used for the development of robotic platforms for the NDE of bridges. The third stream of literature highlights different algorithms for the surface- and sub-surface-level analysis of bridges that have been developed by studies in the past. A number of challenges towards the development of robotic platforms have also been discussed.},
DOI = {10.3390/s20143954}
}



@Article{rs12142285,
AUTHOR = {Hammond, Joshua E. and Vernon, Cory A. and Okeson, Trent J. and Barrett, Benjamin J. and Arce, Samuel and Newell, Valerie and Janson, Joseph and Franke, Kevin W. and Hedengren, John D.},
TITLE = {Survey of 8 UAV Set-Covering Algorithms for Terrain Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2285},
URL = {https://www.mdpi.com/2072-4292/12/14/2285},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing with unmanned aerial vehicles (UAVs) facilitates photogrammetry for environmental and infrastructural monitoring. Models are created with less computational cost by reducing the number of photos required. Optimal camera locations for reducing the number of photos needed for structure-from-motion (SfM) are determined through eight mathematical set-covering algorithms as constrained by solve time. The algorithms examined are: traditional greedy, reverse greedy, carousel greedy (CG), linear programming, particle swarm optimization, simulated annealing, genetic, and ant colony optimization. Coverage and solve time are investigated for these algorithms. CG is the best method for choosing optimal camera locations as it balances number of photos required and time required to calculate camera positions as shown through an analysis similar to a Pareto Front. CG obtains a statistically significant 3.2 fewer cameras per modeled area than base greedy algorithm while requiring just one additional order of magnitude of solve time. For comparison, linear programming is capable of fewer cameras than base greedy but takes at least three orders of magnitude longer to solve. A grid independence study serves as a sensitivity analysis of the CG algorithms    &alpha;    (iteration number) and    &beta;    (percentage to be recalculated) parameters that adjust traditional greedy heuristics, and a case study at the Rock Canyon collection dike in Provo, UT, USA, compares the results of all eight algorithms and the uniqueness (in terms of percentage comparisons based on location/angle metadata and qualitative visual comparison) of each selected set. Though this specific study uses SfM, the principles could apply to other instruments such as multi-spectral cameras or aerial LiDAR.},
DOI = {10.3390/rs12142285}
}



@Article{robotics9030054,
AUTHOR = {Al-Buraiki, Omar and Wu, Wenbo and Payeur, Pierre},
TITLE = {Probabilistic Allocation of Specialized Robots on Targets Detected Using Deep Learning Networks},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2218-6581/9/3/54},
ISSN = {2218-6581},
ABSTRACT = {Task allocation for specialized unmanned robotic agents is addressed in this paper. Based on the assumptions that each individual robotic agent possesses specialized capabilities and that targets representing the tasks to be performed in the surrounding environment impose specific requirements, the proposed approach computes task-agent fitting probabilities to efficiently match the available robotic agents with the detected targets. The framework is supported by a deep learning method with an object instance segmentation capability, Mask R-CNN, that is adapted to provide target object recognition and localization estimates from vision sensors mounted on the robotic agents. Experimental validation, for indoor search-and-rescue (SAR) scenarios, is conducted and results demonstrate the reliability and efficiency of the proposed approach.},
DOI = {10.3390/robotics9030054}
}



@Article{rs12142313,
AUTHOR = {El Mahrad, Badr and Newton, Alice and Icely, John D. and Kacimi, Ilias and Abalansa, Samuel and Snoussi, Maria},
TITLE = {Contribution of Remote Sensing Technologies to a Holistic Coastal and Marine Environmental Management Framework: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2313},
URL = {https://www.mdpi.com/2072-4292/12/14/2313},
ISSN = {2072-4292},
ABSTRACT = {Coastal and marine management require the evaluation of multiple environmental threats and issues. However, there are gaps in the necessary data and poor access or dissemination of existing data in many countries around the world. This research identifies how remote sensing can contribute to filling these gaps so that environmental agencies, such as the United Nations Environmental Programme, European Environmental Agency, and International Union for Conservation of Nature, can better implement environmental directives in a cost-effective manner. Remote sensing (RS) techniques generally allow for uniform data collection, with common acquisition and reporting methods, across large areas. Furthermore, these datasets are sometimes open-source, mainly when governments finance satellite missions. Some of these data can be used in holistic, coastal and marine environmental management frameworks, such as the DAPSI(W)R(M) framework (Drivers–Activities–Pressures–State changes–Impacts (on Welfare)–Responses (as Measures), an updated version of Drivers–Pressures–State–Impact–Responses. The framework is a useful and holistic problem-structuring framework that can be used to assess the causes, consequences, and responses to change in the marine environment. Six broad classifications of remote data collection technologies are reviewed for their potential contribution to integrated marine management, including Satellite-based Remote Sensing, Aerial Remote Sensing, Unmanned Aerial Vehicles, Unmanned Surface Vehicles, Unmanned Underwater Vehicles, and Static Sensors. A significant outcome of this study is practical inputs into each component of the DAPSI(W)R(M) framework. The RS applications are not expected to be all-inclusive; rather, they provide insight into the current use of the framework as a foundation for developing further holistic resource technologies for management strategies in the future. A significant outcome of this research will deliver practical insights for integrated coastal and marine management and demonstrate the usefulness of RS to support the implementation of environmental goals, descriptors, targets, and policies, such as the Water Framework Directive, Marine Strategy Framework Directive, Ocean Health Index, and United Nations Sustainable Development Goals. Additionally, the opportunities and challenges of these technologies are discussed.},
DOI = {10.3390/rs12142313}
}



@Article{s20144011,
AUTHOR = {Yu, Ziyang and Ustin, Susan L. and Zhang, Zhongchen and Liu, Huanjun and Zhang, Xinle and Meng, Xiangtian and Cui, Yang and Guan, Haixiang},
TITLE = {Estimation of a New Canopy Structure Parameter for Rice Using Smartphone Photography},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4011},
URL = {https://www.mdpi.com/1424-8220/20/14/4011},
ISSN = {1424-8220},
ABSTRACT = {The objective of this study was to develop a low-cost method for rice growth information obtained quickly using digital images taken with smartphone. A new canopy parameter, namely, the canopy volume parameter (CVP), was proposed and developed for rice using the leaf area index (LAI) and plant height (PH). Among these parameters, the CVP was selected as an optimal parameter to characterize rice yields during the growth period. Rice canopy images were acquired with a smartphone. Image feature parameters were extracted, including the canopy cover (CC) and numerous vegetation indices (VIs), before and after image segmentation. A rice CVP prediction model in which the CC and VIs served as independent variables was established using a random forest (RF) regression algorithm. The results revealed the following. The CVP was better than the LAI and PH for predicting the final yield. And a CVP prediction model constructed according to a local modelling method for distinguishing different types of rice varieties was the most accurate (coefficient of determination (R2) = 0.92; root mean square error (RMSE) = 0.44). These findings indicate that digital images can be used to track the growth of crops over time and provide technical support for estimating rice yields.},
DOI = {10.3390/s20144011}
}



@Article{rs12142327,
AUTHOR = {Yang, Ming-Der and Huang, Kai-Hsiang and Tsai, Hui-Ping},
TITLE = {Integrating MNF and HHT Transformations into Artificial Neural Networks for Hyperspectral Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2327},
URL = {https://www.mdpi.com/2072-4292/12/14/2327},
ISSN = {2072-4292},
ABSTRACT = {The critical issue facing hyperspectral image (HSI) classification is the imbalance between dimensionality and the number of available training samples. This study attempted to solve the issue by proposing an integrating method using minimum noise fractions (MNF) and Hilbert&ndash;Huang transform (HHT) transformations into artificial neural networks (ANNs) for HSI classification tasks. MNF and HHT function as a feature extractor and image decomposer, respectively, to minimize influences of noises and dimensionality and to maximize training sample efficiency. Experimental results using two benchmark datasets, Indian Pine (IP) and Pavia University (PaviaU) hyperspectral images, are presented. With the intention of optimizing the number of essential neurons and training samples in the ANN, 1 to 1000 neurons and four proportions of training sample were tested, and the associated classification accuracies were evaluated. For the IP dataset, the results showed a remarkable classification accuracy of 99.81% with a 30% training sample from the MNF1&ndash;14+HHT-transformed image set using 500 neurons. Additionally, a high accuracy of 97.62% using only a 5% training sample was achieved for the MNF1&ndash;14+HHT-transformed images. For the PaviaU dataset, the highest classification accuracy was 98.70% with a 30% training sample from the MNF1&ndash;14+HHT-transformed image using 800 neurons. In general, the accuracy increased as the neurons increased, and as the training samples increased. However, the accuracy improvement curve became relatively flat when more than 200 neurons were used, which revealed that using more discriminative information from transformed images can reduce the number of neurons needed to adequately describe the data as well as reducing the complexity of the ANN model. Overall, the proposed method opens new avenues in the use of MNF and HHT transformations for HSI classification with outstanding accuracy performance using an ANN.},
DOI = {10.3390/rs12142327}
}



@Article{vehicles2030026,
AUTHOR = {Yañez-Badillo, Hugo and Tapia-Olvera, Ruben and Beltran-Carbajal, Francisco},
TITLE = {Adaptive Neural Motion Control of a Quadrotor UAV},
JOURNAL = {Vehicles},
VOLUME = {2},
YEAR = {2020},
NUMBER = {3},
PAGES = {468--490},
URL = {https://www.mdpi.com/2624-8921/2/3/26},
ISSN = {2624-8921},
ABSTRACT = {Unmanned Aerial Vehicles have generated considerable interest in different research fields. The motion control problem is among the most important issues to be solved since system dynamic stability depends on the robustness of the main controller against endogenous and exogenous disturbances. In spite of different controllers have been introduced in the literature for motion control of fixed and rotary wing vehicles, there are some challenges for improving controller features such as simplicity, robustness, efficiency, adaptability, and stability. This paper outlines a novel approach to deal with the induced effects of external disturbances affecting the flight of a quadrotor unmanned aerial vehicle. The aim of our study is to further extend the current knowledge of quadrotor motion control by using both adaptive and robust control strategies. A new adaptive neural trajectory tracking control strategy based on B-spline artificial neural networks and on-line disturbance estimation for a quadrotor is proposed. A linear extended state observer is used for estimating time-varying disturbances affecting the controlled nonlinear system dynamics. B-spline artificial neural networks are properly synthesized for on-line calculating control gains of an adaptive Proportional Integral Derivative (PID) scheme. Simulation results highlight the implementation of such a controller is able to reject disturbances meanwhile perform proper motion control by exploiting the robustness, disturbance rejection, adaptability, and self-learning capabilities.},
DOI = {10.3390/vehicles2030026}
}



@Article{app10144991,
AUTHOR = {Villaseñor, Carlos and Gallegos, Alberto A. and Gomez-Avila, Javier and López-González, Gehová and Rios, Jorge D. and Arana-Daniel, Nancy},
TITLE = {Environment Classification for Unmanned Aerial Vehicle Using Convolutional Neural Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4991},
URL = {https://www.mdpi.com/2076-3417/10/14/4991},
ISSN = {2076-3417},
ABSTRACT = {Environment classification is one of the most critical tasks for Unmanned Aerial Vehicles (UAV). Since water accumulation may destabilize UAV, clouds must be detected and avoided. In a previous work presented by the authors, Superpixel Segmentation (SPS) descriptors with low computational cost are used to classify ground, sky, and clouds. In this paper, an enhanced approach to classify the environment in those three classes is presented. The proposed scheme consists of a Convolutional Neural Network (CNN) trained with a dataset generated by both, an human expert and a Support Vector Machine (SVM) to capture context and precise localization. The advantage of using this approach is that the CNN classifies each pixel, instead of a cluster like in SPS, which improves the resolution of the classification, also, is less tedious for the human expert to generate a few training samples instead of the normal amount that it is required. This proposal is implemented for images obtained from video and photographic cameras mounted on a UAV facing in the same direction of the vehicle flight. Experimental results and comparison with other approaches are shown to demonstrate the effectiveness of the algorithm.},
DOI = {10.3390/app10144991}
}



@Article{app10145018,
AUTHOR = {Kim, Sung-Min and Choi, Yosoon and Suh, Jangwon},
TITLE = {Applications of the Open-Source Hardware Arduino Platform in the Mining Industry: A Review},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {5018},
URL = {https://www.mdpi.com/2076-3417/10/14/5018},
ISSN = {2076-3417},
ABSTRACT = {In this study, applications of the Arduino platform in the mining industry were reviewed. Arduino, a representative and popular open-source hardware, can acquire information from various sensors, transmit data using communication technology, and control devices through actuators. The review was conducted by classifying previous studies into three types of Arduino applications: field monitoring systems, wearable systems, and autonomous systems. With regard to field monitoring systems, most studies in mines were classified as atmospheric or geotechnical monitoring. In wearable systems, the health status of the miner was an important consideration, in addition to the environmental conditions of the mine. Arduino can be a useful tool as an initial prototype for autonomous mine systems. Arduino has advantages in that it can be combined with various electronic products and is cost-effective. Therefore, although many studies have been conducted in the laboratory (as opposed to field tests), Arduino applications can be further expanded in the mining field in the future.},
DOI = {10.3390/app10145018}
}



@Article{en13153760,
AUTHOR = {Qi, Min-Yong and Li, Jun-Qing and Han, Yu-Yan and Dong, Jin-Xin},
TITLE = {Optimal Chiller Loading for Energy Conservation Using an Improved Fruit Fly Optimization Algorithm},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {3760},
URL = {https://www.mdpi.com/1996-1073/13/15/3760},
ISSN = {1996-1073},
ABSTRACT = {In the multi-chiller of the air conditioning system, the optimal chiller loading (OCL) is an important research topic. This research is to find the appropriate partial load ratio (PLR) for each chiller in order to minimize the total energy consumption of the multi-chiller under the system cooling load (CL) requirements. However, this optimization problem has not been well studied. In this paper, in order to solve the OCL problem, we propose an improved fruit fly optimization algorithm (IFOA). A linear generation mechanism is developed to uniformly generate candidate solutions, and a new dynamic search radius method is employed to balance the local and global search ability of IFOA. To empirically evaluate the performance of the proposed IFOA, a number of comparative experiments are conducted on three well-known cases. The experimental results show that IFOA found 14 optimal values (the optimal values among all algorithms) under a total of 17 CLs in three cases, and the ratio of the optimal values found was 82.4%, which was the highest among all algorithms. In addition, the mean value of all objective functions of IFOA is smaller and the standard deviation is equal to or close to 0, which proves that the algorithm has high stability. It can be concluded that IFOA is an ideal method to solve the OCL problem.},
DOI = {10.3390/en13153760}
}



@Article{rs12152359,
AUTHOR = {Blanco, Víctor and Blaya-Ros, Pedro José and Castillo, Cristina and Soto-Vallés, Fulgencio and Torres-Sánchez, Roque and Domingo, Rafael},
TITLE = {Potential of UAS-Based Remote Sensing for Estimating Tree Water Status and Yield in Sweet Cherry Trees},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2359},
URL = {https://www.mdpi.com/2072-4292/12/15/2359},
ISSN = {2072-4292},
ABSTRACT = {The present work aims to assess the usefulness of five vegetation indices (VI) derived from multispectral UAS imagery to capture the effects of deficit irrigation on the canopy structure of sweet cherry trees (Prunus avium L.) in southeastern Spain. Three irrigation treatments were assayed, a control treatment and two regulated deficit irrigation treatments. Four airborne flights were carried out during two consecutive seasons; to compare the results of the remote sensing VI, the conventional and continuous water status indicators commonly used to manage sweet cherry tree irrigation were measured, including midday stem water potential (&Psi;s) and maximum daily shrinkage (MDS). Simple regression between individual VIs and &Psi;s or MDS found stronger relationships in postharvest than in preharvest. Thus, the normalized difference vegetation index (NDVI), resulted in the strongest relationship with &Psi;s (r2 = 0.67) and MDS (r2 = 0.45), followed by the normalized difference red edge (NDRE). The sensitivity analysis identified the optimal soil adjusted vegetation index (OSAVI) as the VI with the highest coefficient of variation in postharvest and the difference vegetation index (DVI) in preharvest. A new index is proposed, the transformed red range vegetation index (TRRVI), which was the only VI able to statistically identify a slight water deficit applied in preharvest. The combination of the VIs studied was used in two machine learning models, decision tree and artificial neural networks, to estimate the extra labor needed for harvesting and the sweet cherry yield.},
DOI = {10.3390/rs12152359}
}



@Article{rs12152379,
AUTHOR = {Pulido, Dagoberto and Salas, Joaquín and Rös, Matthias and Puettmann, Klaus and Karaman, Sertac},
TITLE = {Assessment of Tree Detection Methods in Multispectral Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2379},
URL = {https://www.mdpi.com/2072-4292/12/15/2379},
ISSN = {2072-4292},
ABSTRACT = {Detecting individual trees and quantifying their biomass is crucial for carbon accounting procedures at the stand, landscape, and national levels. A significant challenge for many organizations is the amount of effort necessary to document carbon storage levels, especially in terms of human labor. To advance towards the goal of efficiently assessing the carbon content of forest, we evaluate methods to detect trees from high-resolution images taken from unoccupied aerial systems (UAS). In the process, we introduce the Digital Elevated Vegetation Model (DEVM), a representation that combines multispectral images, digital surface models, and digital terrain models. We show that the DEVM facilitates the development of refined synthetic data to detect individual trees using deep learning-based approaches. We carried out experiments in two tree fields located in different countries. Simultaneously, we perform comparisons among an array of classical and deep learning-based methods highlighting the precision and reliability of the DEVM.},
DOI = {10.3390/rs12152379}
}



@Article{f11080808,
AUTHOR = {Prosekov, Alexander and Kuznetsov, Alexander and Rada, Artem and Ivanova, Svetlana},
TITLE = {Methods for Monitoring Large Terrestrial Animals in the Wild},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {808},
URL = {https://www.mdpi.com/1999-4907/11/8/808},
ISSN = {1999-4907},
ABSTRACT = {Reliable information about wildlife is absolutely important for making informed management decisions. The issues with the effectiveness of the control and monitoring of both large and small wild animals are relevant to assess and protect the world&rsquo;s biodiversity. Monitoring becomes part of the methods in wildlife ecology for observation, assessment, and forecasting of the human environment. World practice reveals the potential of the joint application of both proven traditional and modern technologies using specialized equipment to organize environmental control and management processes. Monitoring large terrestrial animals require an individual approach due to their low density and larger habitat. Elk/moose are such animals. This work aims to evaluate the methods for monitoring large wild animals, suitable for controlling the number of elk/moose in the framework of nature conservation activities. Using different models allows determining the population size without affecting the animals and without significant financial costs. Although, the accuracy of each model is determined by its postulates implementation and initial conditions that need statistical data. Depending on the geographical, climatic, and economic conditions in each territory, it is possible to use different tools and equipment (e.g., cameras, GPS sensors, and unmanned aerial vehicles), a flexible variation of which will allow reaching the golden mean between the desires and capabilities of researchers.},
DOI = {10.3390/f11080808}
}



@Article{rs12152397,
AUTHOR = {Schlosser, Aletta Dóra and Szabó, Gergely and Bertalan, László and Varga, Zsolt and Enyedi, Péter and Szabó, Szilárd},
TITLE = {Building Extraction Using Orthophotos and Dense Point Cloud Derived from Visual Band Aerial Imagery Based on Machine Learning and Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2397},
URL = {https://www.mdpi.com/2072-4292/12/15/2397},
ISSN = {2072-4292},
ABSTRACT = {Urban sprawl related increase of built-in areas requires reliable monitoring methods and remote sensing can be an efficient technique. Aerial surveys, with high spatial resolution, provide detailed data for building monitoring, but archive images usually have only visible bands. We aimed to reveal the efficiency of visible orthophotographs and photogrammetric dense point clouds in building detection with segmentation-based machine learning (with five algorithms) using visible bands, texture information, and spectral and morphometric indices in different variable sets. Usually random forest (RF) had the best (99.8%) and partial least squares the worst overall accuracy (~60%). We found that &gt;95% accuracy can be gained even in class level. Recursive feature elimination (RFE) was an efficient variable selection tool, its result with six variables was like when we applied all the available 31 variables. Morphometric indices had 82% producer&rsquo;s and 85% user&rsquo;s Accuracy (PA and UA, respectively) and combining them with spectral and texture indices, it had the largest contribution in the improvement. However, morphometric indices are not always available but by adding texture and spectral indices to red-green-blue (RGB) bands the PA improved with 12% and the UA with 6%. Building extraction from visual aerial surveys can be accurate, and archive images can be involved in the time series of a monitoring.},
DOI = {10.3390/rs12152397}
}



@Article{rs12152426,
AUTHOR = {Pleșoianu, Alin-Ionuț and Stupariu, Mihai-Sorin and Șandric, Ionuț and Pătru-Stupariu, Ileana and Drăguț, Lucian},
TITLE = {Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2426},
URL = {https://www.mdpi.com/2072-4292/12/15/2426},
ISSN = {2072-4292},
ABSTRACT = {Traditional methods for individual tree-crown (ITC) detection (image classification, segmentation, template matching, etc.) applied to very high-resolution remote sensing imagery have been shown to struggle in disparate landscape types or image resolutions due to scale problems and information complexity. Deep learning promised to overcome these shortcomings due to its superior performance and versatility, proven with reported detection rates of ~90%. However, such models still find their limits in transferability across study areas, because of different tree conditions (e.g., isolated trees vs. compact forests) and/or resolutions of the input data. This study introduces a highly replicable deep learning ensemble design for ITC detection and species classification based on the established single shot detector (SSD) model. The ensemble model design is based on varying the input data for the SSD models, coupled with a voting strategy for the output predictions. Very high-resolution unmanned aerial vehicles (UAV), aerial remote sensing imagery and elevation data are used in different combinations to test the performance of the ensemble models in three study sites with highly contrasting spatial patterns. The results show that ensemble models perform better than any single SSD model, regardless of the local tree conditions or image resolution. The detection performance and the accuracy rates improved by 3&ndash;18% with only as few as two participant single models, regardless of the study site. However, when more than two models were included, the performance of the ensemble models only improved slightly and even dropped.},
DOI = {10.3390/rs12152426}
}



@Article{rs12152427,
AUTHOR = {Cai, Yiming and Ding, Yalin and Zhang, Hongwen and Xiu, Jihong and Liu, Zhiming},
TITLE = {Geo-Location Algorithm for Building Targets in Oblique Remote Sensing Images Based on Deep Learning and Height Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2427},
URL = {https://www.mdpi.com/2072-4292/12/15/2427},
ISSN = {2072-4292},
ABSTRACT = {To improve the accuracy of the geographic positioning of a single aerial remote sensing image, the height information of a building in the image must be considered. Oblique remote sensing images are essentially two-dimensional images and produce a large positioning error if a traditional positioning algorithm is used to locate the building directly. To address this problem, this study uses a convolutional neural network to automatically detect the location of buildings in remote sensing images. Moreover, it optimizes an automatic building recognition algorithm for oblique aerial remote sensing images based on You Only Look Once V4 (YOLO V4). This study also proposes a positioning algorithm for the building target, which uses the imaging angle to estimate the height of a building, and combines the spatial coordinate transformation matrix to calculate high-accuracy geo-location of target buildings. Simulation analysis shows that the traditional positioning algorithm inevitably leads to large errors in the positioning of building targets. When the target height is 50 m and the imaging angle is 70&deg;, the positioning error is 114.89 m. Flight tests show that the algorithm established in this study can improve the positioning accuracy of building targets by approximately 20%&ndash;50% depending on the difference in target height.},
DOI = {10.3390/rs12152427}
}



@Article{s20154214,
AUTHOR = {Mekhalfi, Mohamed Lamine and Nicolò, Carlo and Ianniello, Ivan and Calamita, Federico and Goller, Rino and Barazzuol, Maurizio and Melgani, Farid},
TITLE = {Vision System for Automatic On-Tree Kiwifruit Counting and Yield Estimation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4214},
URL = {https://www.mdpi.com/1424-8220/20/15/4214},
ISSN = {1424-8220},
ABSTRACT = {Yield estimation is an essential preharvest practice among most large-scale farming companies, since it enables the predetermination of essential logistics to be allocated (i.e., transportation means, supplies, labor force, among others). An overestimation may thus incur further costs, whereas an underestimation entails potential crop waste. More interestingly, an accurate yield estimation enables stakeholders to better place themselves in the market. Yet, computer-aided precision farming is set to play a pivotal role in this respect. Kiwifruit represents a major produce in several countries (e.g., Italy, China, New and Zealand). However, up to date, the relevant literature remains short of a complete as well as automatic system for kiwifruit yield estimation. In this paper, we present a fully automatic and noninvasive computer vision system for kiwifruit yield estimation across a given orchard. It consists mainly of an optical sensor mounted on a minitractor that surveys the orchard of interest at a low pace. Afterwards, the acquired images are fed to a pipeline that incorporates image preprocessing, stitching, and fruit counting stages and outputs an estimated fruit count and yield estimation. Experimental results conducted on two large kiwifruit orchards confirm a high plausibility (i.e., errors of 6% and 15%) of the proposed system. The proposed yield estimation solution has been in commercial use for about 2 years. With respect to the traditional manual yield estimation carried out by kiwifruit companies, it was demonstrated to save a significant amount of time and cut down on estimation errors, especially when speaking of large-scale farming.},
DOI = {10.3390/s20154214}
}



@Article{robotics9030059,
AUTHOR = {Chaoui, Hicham and Yadav, Sumit and Ahmadi, Rosita Sharif and Bouzid, Allal El Moubarek},
TITLE = {Adaptive Interval Type-2 Fuzzy Logic Control of a Three Degree-of-Freedom Helicopter},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2218-6581/9/3/59},
ISSN = {2218-6581},
ABSTRACT = {This paper combines interval type-2 fuzzy logic with adaptive control theory for the control of a three degree-of-freedom (DOF) helicopter. This strategy yields robustness to various kinds of uncertainties and guaranteed stability of the closed-loop control system. Thus, precise trajectory tracking is maintained under various operational conditions with the presence of various types of uncertainties. Unlike other controllers, the proposed controller approximates the helicopter&rsquo;s inverse dynamic model and assumes no a priori knowledge of the helicopter&rsquo;s dynamics or parameters. The proposed controller is applied to a 3-DOF helicopter model and compared against three other controllers, i.e., PID control, adaptive control, and adaptive sliding-mode control. Numerical results show its high performance and robustness under the presence of uncertainties. To better assess the performance of the control system, two quantitative tracking performance metrics are introduced, i.e., the integral of the tracking errors and the integral of the control signals. Comparative numerical results reveal the superiority of the proposed method by achieving the highest tracking accuracy with the lowest control effort.},
DOI = {10.3390/robotics9030059}
}



@Article{iot1010003,
AUTHOR = {Michailidis, Emmanouel T. and Potirakis, Stelios M. and Kanatas, Athanasios G.},
TITLE = {AI-Inspired Non-Terrestrial Networks for IIoT: Review on Enabling Technologies and Applications},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {1},
PAGES = {21--48},
URL = {https://www.mdpi.com/2624-831X/1/1/3},
ISSN = {2624-831X},
ABSTRACT = {During the last few years, various Industrial Internet of Things (IIoT) applications have emerged with numerous network elements interconnected using wired and wireless communication technologies and equipped with strategically placed sensors and actuators. This paper justifies why non-terrestrial networks (NTNs) will bring the IIoT vision closer to reality by providing improved data acquisition and massive connectivity to sensor fields in large and remote areas. NTNs are engineered to utilize satellites, airships, and aircrafts, which can be employed to extend the radio coverage and provide remote monitoring and sensing services. Additionally, this paper describes indicative delay-tolerant massive IIoT and delay-sensitive mission-critical IIoT applications spanning a large number of vertical markets with diverse and stringent requirements. As the heterogeneous nature of NTNs and the complex and dynamic communications scenarios lead to uncertainty and a high degree of variability, conventional wireless communication technologies cannot sufficiently support ultra-reliable and low-latency communications (URLLC) and offer ubiquitous and uninterrupted interconnectivity. In this regard, this paper sheds light on the potential role of artificial intelligence (AI) techniques, including machine learning (ML) and deep learning (DL), in the provision of challenging NTN-based IIoT services and provides a thorough review of the relevant research works. By adding intelligence and facilitating the decision-making and prediction procedures, the NTNs can effectively adapt to their surrounding environment, thus enhancing the performance of various metrics with significantly lower complexity compared to typical optimization methods.},
DOI = {10.3390/iot1010003}
}



@Article{rs12152460,
AUTHOR = {You, Yanan and Cao, Jingyi and Zhou, Wenli},
TITLE = {A Survey of Change Detection Methods Based on Remote Sensing Images for Multi-Source and Multi-Objective Scenarios},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2460},
URL = {https://www.mdpi.com/2072-4292/12/15/2460},
ISSN = {2072-4292},
ABSTRACT = {Quantities of multi-temporal remote sensing (RS) images create favorable conditions for exploring the urban change in the long term. However, diverse multi-source features and change patterns bring challenges to the change detection in urban cases. In order to sort out the development venation of urban change detection, we make an observation of the literatures on change detection in the last five years, which focuses on the disparate multi-source RS images and multi-objective scenarios determined according to scene category. Based on the survey, a general change detection framework, including change information extraction, data fusion, and analysis of multi-objective scenarios modules, is summarized. Owing to the attributes of input RS images affect the technical selection of each module, data characteristics and application domains across different categories of RS images are discussed firstly. On this basis, not only the evolution process and relationship of the representative solutions are elaborated in the module description, through emphasizing the feasibility of fusing diverse data and the manifold application scenarios, we also advocate a complete change detection pipeline. At the end of the paper, we conclude the current development situation and put forward possible research direction of urban change detection, in the hope of providing insights to the following research.},
DOI = {10.3390/rs12152460}
}



@Article{s20154282,
AUTHOR = {Jia, Guifeng and Li, Wei and Meng, Junyu and Tan, Hequn and Feng, Yaoze},
TITLE = {Non-Contact Evaluation of Pigs’ Body Temperature Incorporating Environmental Factors},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4282},
URL = {https://www.mdpi.com/1424-8220/20/15/4282},
ISSN = {1424-8220},
ABSTRACT = {Internal body temperature is the gold standard for the fever of pigs, however non-contact infrared imaging technology (IRT) can only measure the skin temperature of regions of interest (ROI). Therefore, using IRT to detect the internal body temperature should be based on a correlation model between the ROI temperature and the internal temperature. When heat exchange between the ROI and the surroundings makes the ROI temperature more correlated with the environment, merely depending on the ROI to predict the internal temperature is unreliable. To ensure a high prediction accuracy, this paper investigated the influence of air temperature and humidity on ROI temperature, then built a prediction model incorporating them. The animal test includes 18 swine. IRT was employed to collect the temperatures of the backside, eye, vulva, and ear root ROIs; meanwhile, the air temperature and humidity were recorded. Body temperature prediction models incorporating environmental factors and the ROI temperature were constructed based on Back Propagate Neural Net (BPNN), Random Forest (RF), and Support Vector Regression (SVR). All three models yielded better results regarding the maximum error, minimum error, and mean square error (MSE) when the environmental factors were considered. When environmental factors were incorporated, SVR produced the best outcome, with the maximum error at 0.478 &deg;C, the minimum error at 0.124 &deg;C, and the MSE at 0.159 &deg;C. The result demonstrated the accuracy and applicability of SVR as a prediction model of pigs&prime; internal body temperature.},
DOI = {10.3390/s20154282}
}



@Article{smartcities3030039,
AUTHOR = {Su, Wen-Hao},
TITLE = {Advanced Machine Learning in Point Spectroscopy, RGB- and Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review},
JOURNAL = {Smart Cities},
VOLUME = {3},
YEAR = {2020},
NUMBER = {3},
PAGES = {767--792},
URL = {https://www.mdpi.com/2624-6511/3/3/39},
ISSN = {2624-6511},
ABSTRACT = {Crop productivity is readily reduced by competition from weeds. It is particularly important to control weeds early to prevent yield losses. Limited herbicide choices and increasing costs of weed management are threatening the profitability of crops. Smart agriculture can use intelligent technology to accurately measure the distribution of weeds in the field and perform weed control tasks in selected areas, which cannot only improve the effectiveness of pesticides, but also increase the economic benefits of agricultural products. The most important thing for an automatic system to remove weeds within crop rows is to utilize reliable sensing technology to achieve accurate differentiation of weeds and crops at specific locations in the field. In recent years, there have been many significant achievements involving the differentiation of crops and weeds. These studies are related to the development of rapid and non-destructive sensors, as well as the analysis methods for the data obtained. This paper presents a review of the use of three sensing methods including spectroscopy, color imaging, and hyperspectral imaging in the discrimination of crops and weeds. Several algorithms of machine learning have been employed for data analysis such as convolutional neural network (CNN), artificial neural network (ANN), and support vector machine (SVM). Successful applications include the weed detection in grain crops (such as maize, wheat, and soybean), vegetable crops (such as tomato, lettuce, and radish), and fiber crops (such as cotton) with unsupervised or supervised learning. This review gives a brief introduction into proposed sensing and machine learning methods, then provides an overview of instructive examples of these techniques for weed/crop discrimination. The discussion describes the recent progress made in the development of automated technology for accurate plant identification as well as the challenges and future prospects. It is believed that this review is of great significance to those who study automatic plant care in crops using intelligent technology.},
DOI = {10.3390/smartcities3030039}
}



@Article{jimaging6080076,
AUTHOR = {Daffara, Claudia and Muradore, Riccardo and Piccinelli, Nicola and Gaburro, Nicola and de Rubeis, Tullio and Ambrosini, Dario},
TITLE = {A Cost-Effective System for Aerial 3D Thermography of Buildings},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {76},
URL = {https://www.mdpi.com/2313-433X/6/8/76},
ISSN = {2313-433X},
ABSTRACT = {Three-dimensional (3D) imaging and infrared (IR) thermography are powerful tools in many areas in engineering and sciences. Their joint use is of great interest in the buildings sector, allowing inspection and non-destructive testing of elements as well as an evaluation of the energy efficiency. When dealing with large and complex structures, as buildings (particularly historical) generally are, 3D thermography inspection is enhanced by Unmanned Aerial Vehicles (UAV&mdash;also known as drones). The aim of this paper is to propose a simple and cost-effective system for aerial 3D thermography of buildings. Special attention is thus payed to instrument and reconstruction software choice. After a very brief introduction to IR thermography for buildings and 3D thermography, the system is described. Some experimental results are given to validate the proposal.},
DOI = {10.3390/jimaging6080076}
}



@Article{electronics9081243,
AUTHOR = {Cecchetti, Riccardo and de Paulis, Francesco and Olivieri, Carlo and Orlandi, Antonio and Buecker, Markus},
TITLE = {Effective PCB Decoupling Optimization by Combining an Iterative Genetic Algorithm and Machine Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1243},
URL = {https://www.mdpi.com/2079-9292/9/8/1243},
ISSN = {2079-9292},
ABSTRACT = {An iterative optimization for decoupling capacitor placement on a power delivery network (PDN) is presented based on Genetic Algorithm (GA) and Artificial Neural Network (ANN). The ANN is first trained by an appropriate set of results obtained by a commercial simulator. Once the ANN is ready, it is used within an iterative GA process to place a minimum number of decoupling capacitors for minimizing the differences between the input impedance at one or more location, and the required target impedance. The combined GA&ndash;ANN process is shown to effectively provide results consistent with those obtained by a longer optimization based on commercial simulators. With the new approach the accuracy of the results remains at the same level, but the computational time is reduced by at least 30 times. Two test cases have been considered for validating the proposed approach, with the second one also being compared by experimental measurements.},
DOI = {10.3390/electronics9081243}
}



@Article{s20154331,
AUTHOR = {Santos, João and Oliveira, Miguel and Arrais, Rafael and Veiga, Germano},
TITLE = {Autonomous Scene Exploration for Robotics: A Conditional Random View-Sampling and Evaluation Using a Voxel-Sorting Mechanism for Efficient Ray Casting},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {4331},
URL = {https://www.mdpi.com/1424-8220/20/15/4331},
ISSN = {1424-8220},
ABSTRACT = {Carrying out the task of the exploration of a scene by an autonomous robot entails a set of complex skills, such as the ability to create and update a representation of the scene, the knowledge of the regions of the scene which are yet unexplored, the ability to estimate the most efficient point of view from the perspective of an explorer agent and, finally, the ability to physically move the system to the selected Next Best View (NBV). This paper proposes an autonomous exploration system that makes use of a dual OcTree representation to encode the regions in the scene which are occupied, free, and unknown. The NBV is estimated through a discrete approach that samples and evaluates a set of view hypotheses that are created by a conditioned random process which ensures that the views have some chance of adding novel information to the scene. The algorithm uses ray-casting defined according to the characteristics of the RGB-D sensor, and a mechanism that sorts the voxels to be tested in a way that considerably speeds up the assessment. The sampled view that is estimated to provide the largest amount of novel information is selected, and the system moves to that location, where a new exploration step begins. The exploration session is terminated when there are no more unknown regions in the scene or when those that exist cannot be observed by the system. The experimental setup consisted of a robotic manipulator with an RGB-D sensor assembled on its end-effector, all managed by a Robot Operating System (ROS) based architecture. The manipulator provides movement, while the sensor collects information about the scene. Experimental results span over three test scenarios designed to evaluate the performance of the proposed system. In particular, the exploration performance of the proposed system is compared against that of human subjects. Results show that the proposed approach is able to carry out the exploration of a scene, even when it starts from scratch, building up knowledge as the exploration progresses. Furthermore, in these experiments, the system was able to complete the exploration of the scene in less time when compared to human subjects.},
DOI = {10.3390/s20154331}
}



@Article{app10165436,
AUTHOR = {Kim, Dong-Hyun and Go, Yong-Guk and Choi, Soo-Mi},
TITLE = {An Aerial Mixed-Reality Environment for First-Person-View Drone Flying},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/2076-3417/10/16/5436},
ISSN = {2076-3417},
ABSTRACT = {A drone be able to fly without colliding to preserve the surroundings and its own safety. In addition, it must also incorporate numerous features of interest for drone users. In this paper, an aerial mixed-reality environment for first-person-view drone flying is proposed to provide an immersive experience and a safe environment for drone users by creating additional virtual obstacles when flying a drone in an open area. The proposed system is effective in perceiving the depth of obstacles, and enables bidirectional interaction between real and virtual worlds using a drone equipped with a stereo camera based on human binocular vision. In addition, it synchronizes the parameters of the real and virtual cameras to effectively and naturally create virtual objects in a real space. Based on user studies that included both general and expert users, we confirm that the proposed system successfully creates a mixed-reality environment using a flying drone by quickly recognizing real objects and stably combining them with virtual objects.},
DOI = {10.3390/app10165436}
}



@Article{s20164410,
AUTHOR = {Jamil, Faisal and Iqbal, Naeem and Ahmad, Shabir and Kim, Do-Hyeun},
TITLE = {Toward Accurate Position Estimation Using Learning to Prediction Algorithm in Indoor Navigation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4410},
URL = {https://www.mdpi.com/1424-8220/20/16/4410},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things is advancing, and the augmented role of smart navigation in automating processes is at its vanguard. Smart navigation and location tracking systems are finding increasing use in the area of the mission-critical indoor scenario, logistics, medicine, and security. A demanding emerging area is an Indoor Localization due to the increased fascination towards location-based services. Numerous inertial assessments unit-based indoor localization mechanisms have been suggested in this regard. However, these methods have many shortcomings pertaining to accuracy and consistency. In this study, we propose a novel position estimation system based on learning to the prediction model to address the above challenges. The designed system consists of two modules; learning to prediction module and position estimation using sensor fusion in an indoor environment. The prediction algorithm is attached to the learning module. Moreover, the learning module continuously controls, observes, and enhances the efficiency of the prediction algorithm by evaluating the output and taking into account the exogenous factors that may have an impact on its outcome. On top of that, we reckon a situation where the prediction algorithm can be applied to anticipate the accurate gyroscope and accelerometer reading from the noisy sensor readings. In the designed system, we consider a scenario where the learning module, based on Artificial Neural Network, and Kalman filter are used as a prediction algorithm to predict the actual accelerometer and gyroscope reading from the noisy sensor reading. Moreover, to acquire data, we use the next-generation inertial measurement unit, which contains a 3-axis accelerometer and gyroscope data. Finally, for the performance and accuracy of the proposed system, we carried out numbers of experiments, and we observed that the proposed Kalman filter with learning module performed better than the traditional Kalman filter algorithm in terms of root mean square error metric.},
DOI = {10.3390/s20164410}
}



@Article{rs12162564,
AUTHOR = {Alebele, Yeshanbele and Zhang, Xue and Wang, Wenhui and Yang, Gaoxiang and Yao, Xia and Zheng, Hengbiao and Zhu, Yan and Cao, Weixing and Cheng, Tao},
TITLE = {Estimation of Canopy Biomass Components in Paddy Rice from Combined Optical and SAR Data Using Multi-Target Gaussian Regressor Stacking},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2564},
URL = {https://www.mdpi.com/2072-4292/12/16/2564},
ISSN = {2072-4292},
ABSTRACT = {Crop biomass is a critical variable to make sound decisions about field crop monitoring activities (fertilizers and irrigation) and crop productivity forecasts. More importantly, crop biomass estimations by components are essential for crop growth monitoring as the yield formation of crops results from the accumulation and transportation of substances between different organs. Retrieval of crop biomass from synthetic aperture radar SAR or optical imagery is of paramount importance for in-season monitoring of crop growth. A combination of optical and SAR imagery can compensate for their limitations and has exhibited comparative advantages in biomass estimation. Notably, the joint estimations of biophysical parameters might be more accurate than that of an individual parameter. Previous studies have attempted to use satellite imagery to estimate aboveground biomass, but the estimation of biomass for individual organs remains a challenge. Multi-target Gaussian process regressor stacking (MGPRS), as a new machine learning method, can be suitably utilized to estimate biomass components jointly from satellite imagery data, as the model does not require a large amount of data for training and can be adjusted to the required degrees of relationship exhibited by the given data. Thus, the aim of this study was to estimate the biomass of individual organs by using MGPRS in conjunction with optical (Sentinel-2A) and SAR (Sentinel-1A) imagery. Two hybrid indices, SAR and optical multiplication vegetation index (SOMVI) and SAR and optical difference vegetation index (SODVI), have been constructed to examine their estimation performance. The hybrid vegetation indices were used as input for the MGPRS and single-target Gaussian process regression (SGPR). The accuracy of the estimation methods was analyzed by in situ measurements of aboveground biomass (AGB) and organ biomass conducted in 2018 and 2019 over the paddy rice fields of Xinghua in Jiangsu Province, China. The results showed that the combined indices (SOMVI and SODVI) performed better than those derived from either the optical or SAR data only. The best predictive accuracy was achieved by the MGPRS using SODVI as input (r2 = 0.84, RMSE = 0.4 kg/m2 for stem biomass; r2 = 0.87, RMSE = 0.16 kg/m2 for AGB). This was higher than using SOMVI as input for the MGPRS (r2 = 0.71, RMSE = 1.12 kg/m2 for stem biomass; r2 = 0.71, RMSE = 0.56 kg/m2 for AGB) or SGPR (r2 = 0.63, RMSE = 1.08 kg/m2 for stem biomass; r2 = 0.67, RMSE = 1.08 kg/m2 for AGB). Relatively, higher accuracy for leaf biomass was achieved using SOMVI (r2 = 0.83) than using SODVI (r2 = 0.73) as input for MGPRS. Our results demonstrate that the combined indices are effective by integrating SAR and optical imagery and MGPRS outperformed SGPR with the same input variable for estimating rice crop biomass. The presented workflow will improve the estimation of crops biomass components from satellite data for effective crop growth monitoring.},
DOI = {10.3390/rs12162564}
}



@Article{s20164455,
AUTHOR = {Mayor, Vicente and Estepa, Rafael and Estepa, Antonio and Madinabeitia, Germán},
TITLE = {Energy-Efficient UAVs Deployment for QoS-Guaranteed VoWiFi Service},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4455},
URL = {https://www.mdpi.com/1424-8220/20/16/4455},
ISSN = {1424-8220},
ABSTRACT = {This paper formulates a new problem for the optimal placement of Unmanned Aerial Vehicles (UAVs) geared towards wireless coverage provision for Voice over WiFi (VoWiFi) service to a set of ground users confined in an open area. Our objective function is constrained by coverage and by VoIP speech quality and minimizes the ratio between the number of UAVs deployed and energy efficiency in UAVs, hence providing the layout that requires fewer UAVs per hour of service. Solutions provide the number and position of UAVs to be deployed, and are found using well-known heuristic search methods such as genetic algorithms (used for the initial deployment of UAVs), or particle swarm optimization (used for the periodical update of the positions). We examine two communication services: (a) one bidirectional VoWiFi channel per user; (b) single broadcast VoWiFi channel for announcements. For these services, we study the results obtained for an increasing number of users confined in a small area of 100 m2 as well as in a large area of 10,000 m2. Results show that the drone turnover rate is related to both users&rsquo; sparsity and the number of users served by each UAV. For the unicast service, the ratio of UAVs per hour of service tends to increase with user sparsity and the power of radio communication represents 14&ndash;16% of the total UAV energy consumption depending on ground user density. In large areas, solutions tend to locate UAVs at higher altitudes seeking increased coverage, which increases energy consumption due to hovering. However, in the VoWiFi broadcast communication service, the traffic is scarce, and solutions are mostly constrained only by coverage. This results in fewer UAVs deployed, less total power consumption (between 20% and 75%), and less sensitivity to the number of served users.},
DOI = {10.3390/s20164455}
}



@Article{rs12162576,
AUTHOR = {de Bem, Pablo Pozzobon and de Carvalho Júnior, Osmar Abílio and de Carvalho, Osmar Luiz Ferreira and Gomes, Roberto Arnaldo Trancoso and Fontes Guimarães, Renato},
TITLE = {Performance Analysis of Deep Convolutional Autoencoders with Different Patch Sizes for Change Detection from Burnt Areas},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2576},
URL = {https://www.mdpi.com/2072-4292/12/16/2576},
ISSN = {2072-4292},
ABSTRACT = {Fire is one of the primary sources of damages to natural environments globally. Estimates show that approximately 4 million km2 of land burns yearly. Studies have shown that such estimates often underestimate the real extent of burnt land, which highlights the need to find better, state-of-the-art methods to detect and classify these areas. This study aimed to analyze the use of deep convolutional Autoencoders in the classification of burnt areas, considering different sample patch sizes. A simple Autoencoder and the U-Net and ResUnet architectures were evaluated. We collected Landsat 8 OLI+ data from three scenes in four consecutive dates to detect the changes specifically in the form of burnt land. The data were sampled according to four different sampling strategies to evaluate possible performance changes related to sampling window sizes. The training stage used two scenes, while the validation stage used the remaining scene. The ground truth change mask was created using the Normalized Burn Ratio (NBR) spectral index through a thresholding approach. The classifications were evaluated according to the F1 index, Kappa index, and mean Intersection over Union (mIoU) value. Results have shown that the U-Net and ResUnet architectures offered the best classifications with average F1, Kappa, and mIoU values of approximately 0.96, representing excellent classification results. We have also verified that a sampling window size of 256 by 256 pixels offered the best results.},
DOI = {10.3390/rs12162576}
}



@Article{s20164480,
AUTHOR = {Duan, Jian-Li and Lin, Bin and Cai, Lin X. and Liu, Yu-Xiang and Wu, Yuan},
TITLE = {Node Deployment of Marine Monitoring Networks: A Multiobjective Optimization Scheme},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4480},
URL = {https://www.mdpi.com/1424-8220/20/16/4480},
ISSN = {1424-8220},
ABSTRACT = {The increasing demands for real-time marine monitoring call for the wide deployment of Marine Monitoring Networks (MMNs). The low-rate underwater communications over a long distance, long propagation delay of underwater acoustic channel, and high deployment costs of marine sensors in a large-scale three-dimensional space bring great challenges in the network deployment and management of MMN. In this paper, we first propose a multitier, hierarchical network architecture of MMN with the support of edge computing (HMMN-EC) to enable efficient monitoring services in a harsh marine environment, taking into consideration the salient features of marine communications. Specifically, HMMN-EC is composed of three subnetworks, i.e., underwater acoustic subnetwork, the sea-surface wireless subnetwork, and the air wireless subnetwork, with a diversity of network nodes with different capabilities. We then jointly investigate the deployment diverse network nodes with various constraints in different subnetworks of HMMN-EC. To this end, we formulate a Multiobjective Optimization (MO) problem to minimize the network deployment cost while achieving the maximal network lifetime, subject to the limited energy of different marine nodes and the complex deployment environment. To solve the formulated problem, we present an Ant-Colony-based Efficient Topology Optimization (AC-ETO) algorithm to find the optimal locations of nodes in different subnetworks of MMN in a large-scale deployment. The time complexity of the proposed algorithm is also analyzed. Finally, extensive simulations are carried out to validate the superior performance of the proposed algorithm compared with some existing solutions.},
DOI = {10.3390/s20164480}
}



@Article{geosciences10080308,
AUTHOR = {Ruiz-Carulla, Roger and Corominas, Jordi and Gili, Josep A. and Matas, Gerard and Lantada, Nieves and Moya, Jose and Prades, Albert and Núñez-Andrés, M. A. and Buill, Felipe and Puig, Carol},
TITLE = {Analysis of Fragmentation of Rock Blocks from Real-Scale Tests},
JOURNAL = {Geosciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {308},
URL = {https://www.mdpi.com/2076-3263/10/8/308},
ISSN = {2076-3263},
ABSTRACT = {Real-scale fragmentation tests provide high quality data in order to study the fragmentation pattern of rock blocks. In the tests carried out, the initial rock mass, in terms of both volume and shape, was reconstructed by means of 3D photogrammetry. The fragments size distribution of the bocks tested was measured by hand using a tape. The drop tests were performed in four different sites, releasing a total of 124 blocks and measuring 2907 fragments. The obtained fragment size distributions may be well fitted using power laws. The survival rate (Sr), which is the proportion of remaining block shows a wide range of values. Observing the fragment distribution, two parameters are needed to characterize the fragmentation: the number of fragments produced and Sr. The intensity of the fragmentation is expressed by the exponent of the fitted power laws. Although the results are highly variable and show a stochastic behavior of the fragmentation, we have identified different patterns that reflect some local test conditions.},
DOI = {10.3390/geosciences10080308}
}



@Article{agriculture10080348,
AUTHOR = {Wei, Marcelo Chan Fu and Molin, José Paulo},
TITLE = {Soybean Yield Estimation and Its Components: A Linear Regression Approach},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {348},
URL = {https://www.mdpi.com/2077-0472/10/8/348},
ISSN = {2077-0472},
ABSTRACT = {Soybean yield estimation is either based on yield monitors or agro-meteorological and satellite imagery data, but they present several limiting factors regarding on-farm decision level. Aware that machine learning approaches have been largely applied to estimate soybean yield and the availability of data regarding soybean yield and its components (number of grains (NG) and thousand grains weight (TGW)), there is an opportunity to study their relationships. The objective was to explore the relationships between soybean yield and its components, generate equations to estimate yield and evaluate its prediction accuracy. The training dataset was composed of soybean yield and its components&rsquo; data from 2010 to 2019. Linear regression models based on NG, TGW and yield were fitted on the training dataset and applied to a validation dataset composed of 58 on-field collected samples. It was found that globally TGW and NG presented weak (r = 0.50) and strong (r = 0.92) linear relationships with yield, respectively. In addition to that, applying the fitted models to the validation dataset, model based on NG presented the highest accuracy, coefficient of determination (R2) of 0.70, mean absolute error (MAE) of 639.99 kg ha&minus;1 and root mean squared error (RMSE) of 726.67 kg ha&minus;1.},
DOI = {10.3390/agriculture10080348}
}



@Article{rs12162578,
AUTHOR = {Li, Daoliang and Zhang, Pan and Chen, Tao and Qin, Wei},
TITLE = {Recent Development and Challenges in Spectroscopy and Machine Vision Technologies for Crop Nitrogen Diagnosis: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2578},
URL = {https://www.mdpi.com/2072-4292/12/16/2578},
ISSN = {2072-4292},
ABSTRACT = {Recent development of non-destructive optical techniques, such as spectroscopy and machine vision technologies, have laid a good foundation for real-time monitoring and precise management of crop N status. However, their advantages and disadvantages have not been systematically summarized and evaluated. Here, we reviewed the state-of-the-art of non-destructive optical methods for monitoring the N status of crops, and summarized their advantages and disadvantages. We mainly focused on the contribution of spectral and machine vision technology to the accurate diagnosis of crop N status from three aspects: system selection, data processing, and estimation methods. Finally, we discussed the opportunities and challenges of the application of these technologies, followed by recommendations for future work to address the challenges.},
DOI = {10.3390/rs12162578}
}



@Article{ijgi9080485,
AUTHOR = {Ding, Kaimeng and Liu, Yueming and Xu, Qin and Lu, Fuqiang},
TITLE = {A Subject-Sensitive Perceptual Hash Based on MUM-Net for the Integrity Authentication of High Resolution Remote Sensing Images},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {485},
URL = {https://www.mdpi.com/2220-9964/9/8/485},
ISSN = {2220-9964},
ABSTRACT = {Data security technology is of great significance to the application of high resolution remote sensing image (HRRS) images. As an important data security technology, perceptual hash overcomes the shortcomings of cryptographic hashing that is not robust and can achieve integrity authentication of HRRS images based on perceptual content. However, the existing perceptual hash does not take into account whether the user focuses on certain types of information of the HRRS image. In this paper, we introduce the concept of subject-sensitive perceptual hash, which can be seen as a special case of conventional perceptual hash, for the integrity authentication of HRRS image. To achieve subject-sensitive perceptual hash, we propose a new deep convolutional neural network architecture, named MUM-Net, for extracting robust features of HRRS images. MUM-Net is the core of perceptual hash algorithm, and it uses focal loss as the loss function to overcome the imbalance between the positive and negative samples in the training samples. The robust features extracted by MUM-Net are further compressed and encoded to obtain the perceptual hash sequence of HRRS image. Experiments show that our algorithm has higher tamper sensitivity to subject-related malicious tampering, and the robustness is improved by about 10% compared to the existing U-net-based algorithm; compared to other deep learning-based algorithms, this algorithm achieves a better balance between robustness and tampering sensitivity, and has better overall performance.},
DOI = {10.3390/ijgi9080485}
}



@Article{app10165564,
AUTHOR = {Hu, Dada and Pei, Zhongcai and Tang, Zhiyong},
TITLE = {Single-Parameter-Tuned Attitude Control for Quadrotor with Unknown Disturbance},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5564},
URL = {https://www.mdpi.com/2076-3417/10/16/5564},
ISSN = {2076-3417},
ABSTRACT = {In this paper, methods are presented for designing a quadrotor attitude control system with disturbance rejection ability, wherein only one parameter needs to be tuned for each axis. The core difference between quadrotor platforms are extracted as critical gain parameters (CGPs). Reinforcement learning (RL) technology is introduced in order to automatically optimize the controlling law for quadrotors with different CGPs, and the CGPs are used to extend the RL state list. A deterministic policy gradient (DPG) algorithm that is based on an actor-critic structure in a model-free style is used as the learning algorithm. Mirror sampling and reward shaping methods are designed in order to eliminate the steady-state errors of the RL controller and accelerate the training process. Active disturbance rejection control (ADRC) is applied to reject unknown external disturbances. A set of extended state observers (ESOs) is designed to estimate the total disturbance to the roll and pitch axes. The covariance matrix adaptation evolution strategy (CMA-ES) algorithm is used to automatically tune the ESO parameters and improve the final performance. The complete controller is tested on an F550 quadrotor in both simulation and real flight environments. The quadrotor can hover and move around stably and accurately in the air, even with a severe disturbance.},
DOI = {10.3390/app10165564}
}



@Article{rs12162586,
AUTHOR = {Burdziakowski, Pawel},
TITLE = {A Novel Method for the Deblurring of Photogrammetric Images Using Conditional Generative Adversarial Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2586},
URL = {https://www.mdpi.com/2072-4292/12/16/2586},
ISSN = {2072-4292},
ABSTRACT = {The visual data acquisition from small unmanned aerial vehicles (UAVs) may encounter a situation in which blur appears on the images. Image blurring caused by camera motion during exposure significantly impacts the images interpretation quality and consequently the quality of photogrammetric products. On blurred images, it is difficult to visually locate ground control points, and the number of identified feature points decreases rapidly together with an increasing blur kernel. The nature of blur can be non-uniform, which makes it hard to forecast for traditional deblurring methods. Due to the above, the author of this publication concluded that the neural methods developed in recent years were able to eliminate blur on UAV images with an unpredictable or highly variable blur nature. In this research, a new, rapid method based on generative adversarial networks (GANs) was applied for deblurring. A data set for neural network training was developed based on real aerial images collected over the last few years. More than 20 full sets of photogrammetric products were developed, including point clouds, orthoimages and digital surface models. The sets were generated from both blurred and deblurred images using the presented method. The results presented in the publication show that the method for improving blurred photo quality significantly contributed to an improvement in the general quality of typical photogrammetric products. The geometric accuracy of the products generated from deblurred photos was maintained despite the rising blur kernel. The quality of textures and input photos was increased. This research proves that the developed method based on neural networks can be used for deblur, even in highly blurred images, and it significantly increases the final geometric quality of the photogrammetric products. In practical cases, it will be possible to implement an additional feature in the photogrammetric software, which will eliminate unwanted blur and allow one to use almost all blurred images in the modelling process.},
DOI = {10.3390/rs12162586}
}



@Article{s20164524,
AUTHOR = {Rojas-Perez, Leticia Oyuki and Martinez-Carranza, Jose},
TITLE = {DeepPilot: A CNN for Autonomous Drone Racing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4524},
URL = {https://www.mdpi.com/1424-8220/20/16/4524},
ISSN = {1424-8220},
ABSTRACT = {Autonomous Drone Racing (ADR) was first proposed in IROS 2016. It called for the development of an autonomous drone capable of beating a human in a drone race. After almost five years, several teams have proposed different solutions with a common pipeline: gate detection; drone localization; and stable flight control. Recently, Deep Learning (DL) has been used for gate detection and localization of the drone regarding the gate. However, recent competitions such as the Game of Drones, held at NeurIPS 2019, called for solutions where DL played a more significant role. Motivated by the latter, in this work, we propose a CNN approach called DeepPilot that takes camera images as input and predicts flight commands as output. These flight commands represent: the angular position of the drone&rsquo;s body frame in the roll and pitch angles, thus producing translation motion in those angles; rotational speed in the yaw angle; and vertical speed referred as altitude h. Values for these 4 flight commands, predicted by DeepPilot, are passed to the drone&rsquo;s inner controller, thus enabling the drone to navigate autonomously through the gates in the racetrack. For this, we assume that the next gate becomes visible immediately after the current gate has been crossed. We present evaluations in simulated racetrack environments where DeepPilot is run several times successfully to prove repeatability. In average, DeepPilot runs at 25 frames per second (fps). We also present a thorough evaluation of what we called a temporal approach, which consists of creating a mosaic image, with consecutive camera frames, that is passed as input to the DeepPilot. We argue that this helps to learn the drone&rsquo;s motion trend regarding the gate, thus acting as a local memory that leverages the prediction of the flight commands. Our results indicate that this purely DL-based artificial pilot is feasible to be used for the ADR challenge.},
DOI = {10.3390/s20164524}
}



@Article{app10165608,
AUTHOR = {Yaghoubi, Ehsan and Khezeli, Farhad and Borza, Diana and Kumar, SV Aruna and Neves, João and Proença, Hugo},
TITLE = {Human Attribute Recognition— A Comprehensive Survey},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5608},
URL = {https://www.mdpi.com/2076-3417/10/16/5608},
ISSN = {2076-3417},
ABSTRACT = {Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR&rsquo;s main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.},
DOI = {10.3390/app10165608}
}



@Article{rs12162624,
AUTHOR = {Ingman, Matias and Virtanen, Juho-Pekka and Vaaja, Matti T. and Hyyppä, Hannu},
TITLE = {A Comparison of Low-Cost Sensor Systems in Automatic Cloud-Based Indoor 3D Modeling},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2624},
URL = {https://www.mdpi.com/2072-4292/12/16/2624},
ISSN = {2072-4292},
ABSTRACT = {The automated 3D modeling of indoor spaces is a rapidly advancing field, in which recent developments have made the modeling process more accessible to consumers by lowering the cost of instruments and offering a highly automated service for 3D model creation. We compared the performance of three low-cost sensor systems; one RGB-D camera, one low-end terrestrial laser scanner (TLS), and one panoramic camera, using a cloud-based processing service to automatically create mesh models and point clouds, evaluating the accuracy of the results against a reference point cloud from a higher-end TLS. While adequately accurate results could be obtained with all three sensor systems, the TLS performed the best both in terms of reconstructing the overall room geometry and smaller details, with the panoramic camera clearly trailing the other systems and the RGB-D offering a middle ground in terms of both cost and quality. The results demonstrate the attractiveness of fully automatic cloud-based indoor 3D modeling for low-cost sensor systems, with the latter providing better model accuracy and completeness, and with all systems offering a rapid rate of data acquisition through an easy-to-use interface.},
DOI = {10.3390/rs12162624}
}



@Article{en13164224,
AUTHOR = {Seme, Sebastijan and Štumberger, Bojan and Hadžiselimović, Miralem and Sredenšek, Klemen},
TITLE = {Solar Photovoltaic Tracking Systems for Electricity Generation: A Review},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4224},
URL = {https://www.mdpi.com/1996-1073/13/16/4224},
ISSN = {1996-1073},
ABSTRACT = {This paper presents a thorough review of state-of-the-art research and literature in the field of photovoltaic tracking systems for the production of electrical energy. A review of the literature is performed mainly for the field of solar photovoltaic tracking systems, which gives this paper the necessary foundation. Solar systems can be roughly divided into three fields: the generation of thermal energy (solar collectors), the generation of electrical energy (photovoltaic systems), and the generation of electrical energy/thermal energy (hybrid systems). The development of photovoltaic systems began in the mid-19th century, followed shortly by research in the field of tracking systems. With the development of tracking systems, different types of tracking systems, drives, designs, and tracking strategies were also defined. This paper presents a comprehensive overview of photovoltaic tracking systems, as well as the latest studies that have been done in recent years. The review will be supplemented with a factual presentation of the tracking systems used at the Institute of Energy Technology of the University of Maribor.},
DOI = {10.3390/en13164224}
}



@Article{rs12162640,
AUTHOR = {Vlachopoulos, Odysseas and Leblon, Brigitte and Wang, Jinfei and Haddadi, Ataollah and LaRocque, Armand and Patterson, Greg},
TITLE = {Delineation of Crop Field Areas and Boundaries from UAS Imagery Using PBIA and GEOBIA with Random Forest Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2640},
URL = {https://www.mdpi.com/2072-4292/12/16/2640},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aircraft systems (UAS) have been proven cost- and time-effective remote-sensing platforms for precision agriculture applications. This study presents a method for automatic delineation of field areas and boundaries that uses UAS multispectral orthomosaics acquired over 7 vegetated fields having a variety of crops in Prince Edward Island (PEI). This information is needed by crop insurance agencies and growers for an accurate determination of crop insurance premiums. The field areas and boundaries were delineated by applying both a pixel-based and an object-based supervised random forest (RF) classifier applied to reflectance and vegetation index images, followed by a vectorization pipeline. Both methodologies performed exceptionally well, resulting in a mean area goodness of fit (AGoF) for the field areas greater than 98% and a mean boundary mean positional error (BMPE) lower than 0.8 m for the seven surveyed fields.},
DOI = {10.3390/rs12162640}
}



@Article{rs12162646,
AUTHOR = {Zhang, Shiyu and Zhuo, Li and Zhang, Hui and Li, Jiafeng},
TITLE = {Object Tracking in Unmanned Aerial Vehicle Videos via Multifeature Discrimination and Instance-Aware Attention Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2646},
URL = {https://www.mdpi.com/2072-4292/12/16/2646},
ISSN = {2072-4292},
ABSTRACT = {Visual object tracking in unmanned aerial vehicle (UAV) videos plays an important role in a variety of fields, such as traffic data collection, traffic monitoring, as well as film and television shooting. However, it is still challenging to track the target robustly in UAV vision task due to several factors such as appearance variation, background clutter, and severe occlusion. In this paper, we propose a novel two-stage UAV tracking framework, which includes a target detection stage based on multifeature discrimination and a bounding-box estimation stage based on the instance-aware attention network. In the target detection stage, we explore a feature representation scheme for a small target that integrates handcrafted features, low-level deep features, and high-level deep features. Then, the correlation filter is used to roughly predict target location. In the bounding-box estimation stage, an instance-aware intersection over union (IoU)-Net is integrated together with an instance-aware attention network to estimate the target size based on the bounding-box proposals generated in the target detection stage. Extensive experimental results on the UAV123 and UAVDT datasets show that our tracker, running at over 25 frames per second (FPS), has superior performance as compared with state-of-the-art UAV visual tracking approaches.},
DOI = {10.3390/rs12162646}
}



@Article{bdcc4030020,
AUTHOR = {Ciaburro, Giuseppe},
TITLE = {Sound Event Detection in Underground Parking Garage Using Convolutional Neural Network},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {4},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {20},
URL = {https://www.mdpi.com/2504-2289/4/3/20},
ISSN = {2504-2289},
ABSTRACT = {Parking is a crucial element in urban mobility management. The availability of parking areas makes it easier to use a service, determining its success. Proper parking management allows economic operators located nearby to increase their business revenue. Underground parking areas during off-peak hours are uncrowded places, where user safety is guaranteed by company overseers. Due to the large size, ensuring adequate surveillance would require many operators to increase the costs of parking fees. To reduce costs, video surveillance systems are used, in which an operator monitors many areas. However, some activities are beyond the control of this technology. In this work, a procedure to identify sound events in an underground garage is developed. The aim of the work is to detect sounds identifying dangerous situations and to activate an automatic alert that draws the attention of surveillance in that area. To do this, the sounds of a parking sector were detected with the use of sound sensors. These sounds were analyzed by a sound detector based on convolutional neural networks. The procedure returned high accuracy in identifying a car crash in an underground parking area.},
DOI = {10.3390/bdcc4030020}
}



@Article{rs12162657,
AUTHOR = {Stereńczak, Krzysztof and Zapłata, Rafał and Wójcik, Jarosław and Kraszewski, Bartłomiej and Mielcarek, Miłosz and Mitelsztedt, Krzysztof and Białczak, Małgorzata and Krok, Grzegorz and Kuberski, Łukasz and Markiewicz, Anna and Modzelewska, Aneta and Parkitna, Karolina and Piasecka, Żaneta and Pilch, Kamil and Rzeczycki, Karol and Sadkowski, Rafał and Wietecha, Martyna and Rysiak, Piotr and von Gadow, Klaus and Cieszewski, Chris J.},
TITLE = {ALS-Based Detection of Past Human Activities in the Białowieża Forest—New Evidence of Unknown Remains of Past Agricultural Systems},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2657},
URL = {https://www.mdpi.com/2072-4292/12/16/2657},
ISSN = {2072-4292},
ABSTRACT = {The Białowieża Forest (BF), a unique ecosystem of historical significance in central Europe, has a long history of assumed human settlement, with at least 200 known archaeological sites (until 2016). This study uncovers new evidence of the cultural heritage of this unique forest area using Airborne Laser Scanning (ALS) technology combined with traditional archaeological field assessment methods to verify the ALS data interpretations and to provide additional evidence about the function and origin of the newly detected archaeological sites. The results of this study include (1) a scientific approach for an improved identification of archaeological resources in forest areas; (2) new evidence about the history of the human use of the BF based on ALS data, covering the entire Polish part of the BF; and (3) an improved remote sensing infrastructure, supporting existing GIS (Geographic Information System) systems for the BF, a famous UNESCO Heritage site. Our study identified numerous locations with evidence of past human agricultural activities known in the literature as &ldquo;field systems&rdquo;, &ldquo;lynchets&rdquo; and &ldquo;Celtic fields&rdquo;. The initial identification included more than 300 km of possible field boundaries and plough headlands, many of which we have verified on the ground. Various past human activities creating those boundaries have existed since the (pre-) Roman Period up to the 13th century AD. The results of this study demonstrate that past human activities in the Polish part of the Białowieża Forest had been more prevalent than previously believed. As a practical result of the described activities, a geodatabase was created; this has practical applications for the system of monument protection in Poland, as well as for local communities and the BF&rsquo;s management and conservation. The more widely achieved results are in line with the implementation of the concept of a cultural heritage inventory in forested and protected areas&mdash;the actions taken specify (built globally) the forms of protection and management of cultural and environmental goods.},
DOI = {10.3390/rs12162657}
}



@Article{s20164655,
AUTHOR = {Sun, Meiwei and Deng, Yingbin and Li, Miao and Jiang, Hao and Huang, Haoling and Liao, Wenyue and Liu, Yangxiaoyue and Yang, Ji and Li, Yong},
TITLE = {Extraction and Analysis of Blue Steel Roofs Information Based on CNN Using Gaofen-2 Imageries},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4655},
URL = {https://www.mdpi.com/1424-8220/20/16/4655},
ISSN = {1424-8220},
ABSTRACT = {Blue steel roof is advantageous for its low cost, durability, and ease of installation. It is generally used by industrial areas. The accurate and rapid mapping of blue steel roof is important for the preliminary assessment of inefficient industrial areas and is one of the key elements for quantifying environmental issues like urban heat islands. Here, the DeeplabV3+ semantic segmentation neural network based on GaoFen-2 images was used to analyze the quantity and spatial distribution of blue steel roofs in the Nanhai district, Foshan (including the towns of Shishan, Guicheng, Dali, and Lishui), which is the important manufacturing industry base of China. We found that: (1) the DeeplabV3+ performs well with an overall accuracy of 92%, higher than the maximum likelihood classification; (2) the distribution of blue steel roofs was not even across the whole study area, but they were evenly distributed within the town scale; and (3) strong positive correlation was observed between blue steel roofs area and industrial gross output. These results not only can be used to detect the inefficient industrial areas for regional planning but also provide fundamental data for studies of urban environmental issues.},
DOI = {10.3390/s20164655}
}



@Article{rs12172683,
AUTHOR = {Jimenez-Sierra, David Alejandro and Benítez-Restrepo, Hernán Darío and Vargas-Cardona, Hernán Darío and Chanussot, Jocelyn},
TITLE = {Graph-Based Data Fusion Applied to: Change Detection and Biomass Estimation in Rice Crops},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2683},
URL = {https://www.mdpi.com/2072-4292/12/17/2683},
ISSN = {2072-4292},
ABSTRACT = {The complementary nature of different modalities and multiple bands used in remote sensing data is helpful for tasks such as change detection and the prediction of agricultural variables. Nonetheless, correctly processing a multi-modal dataset is not a simple task, owing to the presence of different data resolutions and formats. In the past few years, graph-based methods have proven to be a useful tool in capturing inherent data similarity, in spite of different data formats, and preserving relevant topological and geometric information. In this paper, we propose a graph-based data fusion algorithm for remotely sensed images applied to (i) data-driven semi-unsupervised change detection and (ii) biomass estimation in rice crops. In order to detect the change, we evaluated the performance of four competing algorithms on fourteen datasets. To estimate biomass in rice crops, we compared our proposal in terms of root mean squared error (RMSE) concerning a recent approach based on vegetation indices as features. The results confirm that the proposed graph-based data fusion algorithm outperforms state-of-the-art methods for change detection and biomass estimation in rice crops.},
DOI = {10.3390/rs12172683}
}



@Article{app10175773,
AUTHOR = {Alparslan, Onur and Arakawa, Shin’ichi and Murata, Masayuki},
TITLE = {SDN-Based Control of IoT Network by Brain-Inspired Bayesian Attractor Model and Network Slicing},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {5773},
URL = {https://www.mdpi.com/2076-3417/10/17/5773},
ISSN = {2076-3417},
ABSTRACT = {One of the models in the literature for modeling the behavior of the brain is the Bayesian attractor model, which is a kind of machine-learning algorithm. According to this model, the brain assigns stochastic variables to possible decisions (attractors) and chooses one of them when enough evidence is collected from sensory systems to achieve a confidence level high enough to make a decision. In this paper, we introduce a software defined networking (SDN) application based on a brain-inspired Bayesian attractor model for identification of the current traffic pattern for the supervision and automation of Internet of things (IoT) networks that exhibit a limited number of traffic patterns. In a real SDN testbed, we demonstrate that our SDN application can identify the traffic patterns using a limited set of fluctuating network statistics of edge link utilization. Moreover, we show that our application can improve core link utilization and the power efficiency of IoT networks by immediately applying a pre-calculated network configuration optimized by traffic engineering with network slicing for the identified pattern.},
DOI = {10.3390/app10175773}
}



@Article{s20174709,
AUTHOR = {Wang, Bin and Gu, Yinjuan},
TITLE = {An Improved FBPN-Based Detection Network for Vehicles in Aerial Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4709},
URL = {https://www.mdpi.com/1424-8220/20/17/4709},
ISSN = {1424-8220},
ABSTRACT = {With the development of artificial intelligence and big data analytics, an increasing number of researchers have tried to use deep-learning technology to train neural networks and achieved great success in the field of vehicle detection. However, as a special domain of object detection, vehicle detection in aerial images still has made limited progress because of low resolution, complex backgrounds and rotating objects. In this paper, an improved feature-balanced pyramid network (FBPN) has been proposed to enhance the network&rsquo;s ability to detect small objects. By combining FBPN with modified faster region convolutional neural network (faster-RCNN), a vehicle detection framework for aerial images is proposed. The focal loss function is adopted in the proposed framework to reduce the imbalance between easy and hard samples. The experimental results based on the VEDIA, USCAS-AOD, and DOTA datasets show that the proposed framework outperforms other state-of-the-art vehicle detection algorithms for aerial images.},
DOI = {10.3390/s20174709}
}



@Article{robotics9030064,
AUTHOR = {Wilson, Callum and Marchetti, Francesco and Di Carlo, Marilena and Riccardi, Annalisa and Minisci, Edmondo},
TITLE = {Classifying Intelligence in Machines: A Taxonomy of Intelligent Control},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {64},
URL = {https://www.mdpi.com/2218-6581/9/3/64},
ISSN = {2218-6581},
ABSTRACT = {The quest to create machines that can solve problems as humans do leads us to intelligent control. This field encompasses control systems that can adapt to changes and learn to improve their actions&mdash;traits typically associated with human intelligence. In this work we seek to determine how intelligent these classes of control systems are by quantifying their level of adaptability and learning. First we describe the stages of development towards intelligent control and present a definition based on literature. Based on the key elements of this definition, we propose a novel taxonomy of intelligent control methods, which assesses the extent to which they handle uncertainties in three areas: the environment, the controller, and the goals. This taxonomy is applicable to a variety of robotic and other autonomous systems, which we demonstrate through several examples of intelligent control methods and their classifications. Looking at the spread of classifications based on this taxonomy can help researchers identify where control systems can be made more intelligent.},
DOI = {10.3390/robotics9030064}
}



@Article{app10175799,
AUTHOR = {Yang, Yuanwei and Ran, Shuhao and Gao, Xianjun and Wang, Mingwei and Li, Xi},
TITLE = {An Automatic Shadow Compensation Method via a New Model Combined Wallis Filter with LCC Model in High Resolution Remote Sensing Images},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {5799},
URL = {https://www.mdpi.com/2076-3417/10/17/5799},
ISSN = {2076-3417},
ABSTRACT = {Current automatic shadow compensation methods often suffer because their contrast improvement processes are not self-adaptive and, consequently, the results they produce do not adequately represent the real objects. The study presented in this paper designed a new automatic shadow compensation framework based on improvements to the Wallis principle, which included an intensity coefficient and a stretching coefficient to enhance contrast and brightness more efficiently. An automatic parameter calculation strategy also is a part of this framework, which is based on searching for and matching similar feature points around shadow boundaries. Finally, a final compensation combination strategy combines the regional compensation with the local window compensation of the pixels in each shadow to improve the shaded information in a balanced way. All these strategies in our method work together to provide a better measurement for customizing suitable compensation depending on the condition of each region and pixel. The intensity component I also is automatically strengthened through the customized compensation model. Color correction is executed in a way that avoids the color bias caused by over-compensated component values, thereby better reflecting shaded information. Images with clouds shadows and ground objects shadows were utilized to test our method and six other state-of-the-art methods. The comparison results indicate that our method compensated for shaded information more effectively, accurately, and evenly than the other methods for customizing suitable models for each shadow and pixel with reasonable time-cost. Its brightness, contrast, and object color in shaded areas were approximately equalized with non-shaded regions to present a shadow-free image.},
DOI = {10.3390/app10175799}
}



@Article{rs12172722,
AUTHOR = {Wang, Yuxuan and Wu, Guangming and Guo, Yimin and Huang, Yifei and Shibasaki, Ryosuke},
TITLE = {Learn to Extract Building Outline from Misaligned Annotation through Nearest Feature Selector},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2722},
URL = {https://www.mdpi.com/2072-4292/12/17/2722},
ISSN = {2072-4292},
ABSTRACT = {For efficient building outline extraction, many algorithms, including unsupervised or supervised, have been proposed over the past decades. In recent years, due to the rapid development of the convolutional neural networks, especially fully convolutional networks, building extraction is treated as a semantic segmentation task that deals with the extremely biased positive pixels. The state-of-the-art methods, either through direct or indirect approaches, are mainly focused on better network design. The shifts and rotations, which are coarsely presented in manually created annotations, have long been ignored. Due to the limited number of positive samples, the misalignment will significantly reduce the correctness of pixel-to-pixel loss that might lead to a gradient explosion. To overcome this, we propose a nearest feature selector (NFS) to dynamically re-align the prediction and slightly misaligned annotations. The NFS can be seamlessly appended to existing loss functions and prevent misleading by the errors or misalignment of annotations. Experiments on a large scale aerial image dataset with centered buildings and corresponding building outlines indicate that the additional NFS brings higher performance when compared to existing naive loss functions. In the classic L1 loss, the addition of NFS gains increments of 8.8% of f1-score, 8.9% of kappa coefficient, and 9.8% of Jaccard index, respectively.},
DOI = {10.3390/rs12172722}
}



@Article{rs12172729,
AUTHOR = {Yang, Jianxiu and Xie, Xuemei and Shi, Guangming and Yang, Wenzhe},
TITLE = {A Feature-Enhanced Anchor-Free Network for UAV Vehicle Detection},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2729},
URL = {https://www.mdpi.com/2072-4292/12/17/2729},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection based on unmanned aerial vehicle (UAV) images is a challenging task. One reason is that the objects are small size, low-resolution, and large scale variations, resulting in weak feature representation. Another reason is the imbalance between positive and negative examples. In this paper, we propose a novel architecture for UAV vehicle detection to solve above problems. In detail, we use anchor-free mechanism to eliminate predefined anchors, which can reduce complicated computation and relieve the imbalance between positive and negative samples. Meanwhile, to enhance the features for vehicles, we design a multi-scale semantic enhancement block (MSEB) and an effective 49-layer backbone which is based on the DetNet59. The proposed network offers appropriate receptive fields that match the small-sized vehicles, and involves precise localization information provided by the contexts with high resolution. The MSEB strengthens discriminative feature representation at various scales, without reducing the spatial resolution of prediction layers. Experiments show that the proposed method achieves the state-of-the-art performance. Particularly, the main part of vehicles, much smaller ones, the accuracy is about 2% higher than other existing methods.},
DOI = {10.3390/rs12172729}
}



@Article{ijgi9090507,
AUTHOR = {Arjasakusuma, Sanjiwana and Swahyu Kusuma, Sandiaga and Phinn, Stuart},
TITLE = {Evaluating Variable Selection and Machine Learning Algorithms for Estimating Forest Heights by Combining Lidar and Hyperspectral Data},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {507},
URL = {https://www.mdpi.com/2220-9964/9/9/507},
ISSN = {2220-9964},
ABSTRACT = {Machine learning has been employed for various mapping and modeling tasks using input variables from different sources of remote sensing data. For feature selection involving high- spatial and spectral dimensionality data, various methods have been developed and incorporated into the machine learning framework to ensure an efficient and optimal computational process. This research aims to assess the accuracy of various feature selection and machine learning methods for estimating forest height using AISA (airborne imaging spectrometer for applications) hyperspectral bands (479 bands) and airborne light detection and ranging (lidar) height metrics (36 metrics), alone and combined. Feature selection and dimensionality reduction using Boruta (BO), principal component analysis (PCA), simulated annealing (SA), and genetic algorithm (GA) in combination with machine learning algorithms such as multivariate adaptive regression spline (MARS), extra trees (ET), support vector regression (SVR) with radial basis function, and extreme gradient boosting (XGB) with trees (XGbtree and XGBdart) and linear (XGBlin) classifiers were evaluated. The results demonstrated that the combinations of BO-XGBdart and BO-SVR delivered the best model performance for estimating tropical forest height by combining lidar and hyperspectral data, with R2 = 0.53 and RMSE = 1.7 m (18.4% of nRMSE and 0.046 m of bias) for BO-XGBdart and R2 = 0.51 and RMSE = 1.8 m (15.8% of nRMSE and &minus;0.244 m of bias) for BO-SVR. Our study also demonstrated the effectiveness of BO for variables selection; it could reduce 95% of the data to select the 29 most important variables from the initial 516 variables from lidar metrics and hyperspectral data.},
DOI = {10.3390/ijgi9090507}
}



@Article{rs12172732,
AUTHOR = {Abdulridha, Jaafar and Ampatzidis, Yiannis and Qureshi, Jawwad and Roberts, Pamela},
TITLE = {Laboratory and UAV-Based Identification and Classification of Tomato Yellow Leaf Curl, Bacterial Spot, and Target Spot Diseases in Tomato Utilizing Hyperspectral Imaging and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2732},
URL = {https://www.mdpi.com/2072-4292/12/17/2732},
ISSN = {2072-4292},
ABSTRACT = {Tomato crops are susceptible to multiple diseases, several of which may be present during the same season. Therefore, rapid disease identification could enhance crop management consequently increasing the yield. In this study, nondestructive methods were developed to detect diseases that affect tomato crops, such as bacterial spot (BS), target spot (TS), and tomato yellow leaf curl (TYLC) for two varieties of tomato (susceptible and tolerant to TYLC only) by using hyperspectral sensing in two conditions: a) laboratory (benchtop scanning), and b) in field using an unmanned aerial vehicle (UAV-based). The stepwise discriminant analysis (STDA) and the radial basis function were applied to classify the infected plants and distinguish them from noninfected or healthy (H) plants. Multiple vegetation indices (VIs) and the M statistic method were utilized to distinguish and classify the diseased plants. In general, the classification results between healthy and diseased plants were highly accurate for all diseases; for instance, when comparing H vs. BS, TS, and TYLC in the asymptomatic stage and laboratory conditions, the classification rates were 94%, 95%, and 100%, respectively. Similarly, in the symptomatic stage, the classification rates between healthy and infected plants were 98% for BS, and 99&ndash;100% for TS and TYLC diseases. The classification results in the field conditions also showed high values of 98%, 96%, and 100%, for BS, TS, and TYLC, respectively. The VIs that could best identify these diseases were the renormalized difference vegetation index (RDVI), and the modified triangular vegetation index 1 (MTVI 1) in both laboratory and field. The results were promising and suggest the possibility to identify these diseases using remote sensing.},
DOI = {10.3390/rs12172732}
}



@Article{insects11090565,
AUTHOR = {Zhang, Zhiliang and Zhan, Wei and He, Zhangzhang and Zou, Yafeng},
TITLE = {Application of Spatio-Temporal Context and Convolution Neural Network (CNN) in Grooming Behavior of Bactrocera minax (Diptera: Trypetidae) Detection and Statistics},
JOURNAL = {Insects},
VOLUME = {11},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {565},
URL = {https://www.mdpi.com/2075-4450/11/9/565},
PubMedID = {32846918},
ISSN = {2075-4450},
ABSTRACT = {Statistical analysis and research on insect grooming behavior can find more effective methods for pest control. Traditional manual insect grooming behavior statistical methods are time-consuming, labor-intensive, and error-prone. Based on computer vision technology, this paper uses spatio-temporal context to extract video features, uses self-built Convolution Neural Network (CNN) to train the detection model, and proposes a simple and effective Bactrocera minax grooming behavior detection method, which automatically detects the grooming behaviors of the flies and analysis results by a computer program. Applying the method training detection model proposed in this paper, the videos of 22 adult flies with a total of 1320 min of grooming behavior were detected and analyzed, and the total detection accuracy was over 95%, the standard error of the accuracy of the behavior detection of each adult flies was less than 3%, and the difference was less than 15% when compared with the results of manual observation. The experimental results show that the method in this paper greatly reduces the time of manual observation and at the same time ensures the accuracy of insect behavior detection and analysis, which proposes a new informatization analysis method for the behavior statistics of Bactrocera minax and also provides a new idea for related insect behavior identification research.},
DOI = {10.3390/insects11090565}
}



@Article{math8091415,
AUTHOR = {Li, Juan and Lei, Hong and Alavi, Amir H. and Wang, Gai-Ge},
TITLE = {Elephant Herding Optimization: Variants, Hybrids, and Applications},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1415},
URL = {https://www.mdpi.com/2227-7390/8/9/1415},
ISSN = {2227-7390},
ABSTRACT = {Elephant herding optimization (EHO) is a nature-inspired metaheuristic optimization algorithm based on the herding behavior of elephants. EHO uses a clan operator to update the distance of the elephants in each clan with respect to the position of a matriarch elephant. The superiority of the EHO method to several state-of-the-art metaheuristic algorithms has been demonstrated for many benchmark problems and in various application areas. A comprehensive review for the EHO-based algorithms and their applications are presented in this paper. Various aspects of the EHO variants for continuous optimization, combinatorial optimization, constrained optimization, and multi-objective optimization are reviewed. Future directions for research in the area of EHO are further discussed.},
DOI = {10.3390/math8091415}
}



@Article{s20174807,
AUTHOR = {Zhang, Dawei and Zheng, Zhonglong and Wang, Tianxiang and He, Yiran},
TITLE = {HROM: Learning High-Resolution Representation and Object-Aware Masks for Visual Object Tracking},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4807},
URL = {https://www.mdpi.com/1424-8220/20/17/4807},
ISSN = {1424-8220},
ABSTRACT = {Siamese network-based trackers consider tracking as features cross-correlation between the target template and the search region. Therefore, feature representation plays an important role for constructing a high-performance tracker. However, all existing Siamese networks extract the deep but low-resolution features of the entire patch, which is not robust enough to estimate the target bounding box accurately. In this work, to address this issue, we propose a novel high-resolution Siamese network, which connects the high-to-low resolution convolution streams in parallel as well as repeatedly exchanges the information across resolutions to maintain high-resolution representations. The resulting representation is semantically richer and spatially more precise by a simple yet effective multi-scale feature fusion strategy. Moreover, we exploit attention mechanisms to learn object-aware masks for adaptive feature refinement, and use deformable convolution to handle complex geometric transformations. This makes the target more discriminative against distractors and background. Without bells and whistles, extensive experiments on popular tracking benchmarks containing OTB100, UAV123, VOT2018 and LaSOT demonstrate that the proposed tracker achieves state-of-the-art performance and runs in real time, confirming its efficiency and effectiveness.},
DOI = {10.3390/s20174807}
}



@Article{rs12172767,
AUTHOR = {Chen, Yu and Wei, Yongming and Wang, Qinjun and Chen, Fang and Lu, Chunyan and Lei, Shaohua},
TITLE = {Mapping Post-Earthquake Landslide Susceptibility: A U-Net Like Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2767},
URL = {https://www.mdpi.com/2072-4292/12/17/2767},
ISSN = {2072-4292},
ABSTRACT = {A serious earthquake could trigger thousands of landslides and produce some slopes more sensitive to slide in future. Landslides could threaten human&rsquo;s lives and properties, and thus mapping the post-earthquake landslide susceptibility is very valuable for a rapid response to landslide disasters in terms of relief resource allocation and posterior earthquake reconstruction. Previous researchers have proposed many methods to map landslide susceptibility but seldom considered the spatial structure information of the factors that influence a slide. In this study, we first developed a U-net like model suitable for mapping post-earthquake landslide susceptibility. The post-earthquake high spatial airborne images were used for producing a landslide inventory. Pre-earthquake Landsat TM (Thematic Mapper) images and the influencing factors such as digital elevation model (DEM), slope, aspect, multi-scale topographic position index (mTPI), lithology, fault, road network, streams network, and macroseismic intensity (MI) were prepared as the input layers of the model. Application of the model to the heavy-hit area of the destructive 2008 Wenchuan earthquake resulted in a high validation accuracy (precision 0.77, recall 0.90, F1 score 0.83, and AUC 0.90). The performance of this U-net like model was also compared with those of traditional logistic regression (LR) and support vector machine (SVM) models on both the model area and independent testing area with the former being stronger than the two traditional models. The U-net like model introduced in this paper provides us the inspiration that balancing the environmental influence of a pixel itself and its surrounding pixels to perform a better landslide susceptibility mapping (LSM) task is useful and feasible when using remote sensing and GIS technology.},
DOI = {10.3390/rs12172767}
}



@Article{sym12091424,
AUTHOR = {Alabdulwahab, Saleh and Moon, BongKyo},
TITLE = {Feature Selection Methods Simultaneously Improve the Detection Accuracy and Model Building Time of Machine Learning Classifiers},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1424},
URL = {https://www.mdpi.com/2073-8994/12/9/1424},
ISSN = {2073-8994},
ABSTRACT = {The detection accuracy and model building time of machine learning (ML) classifiers are vital aspects for an intrusion detection system (IDS) to predict attacks in real life. Recently, researchers have introduced feature selection methods to increase the detection accuracy and minimize the model building time of a limited number of ML classifiers. Therefore, identifying more ML classifiers with very high detection accuracy and the lowest possible model building time is necessary. In this study, the authors tested six supervised classifiers on a full NSL-KDD training dataset (a benchmark record for Internet traffic) using 10-fold cross-validation in the Weka tool with and without feature selection/reduction methods. The authors aimed to identify more options to outperform and secure classifiers with the highest detection accuracy and lowest model building time. The results show that the feature selection/reduction methods, including the wrapper method in combination with the discretize filter, the filter method in combination with the discretize filter, and the discretize filter, can significantly decrease model building time without compromising detection accuracy. The suggested ML algorithms and feature selection/reduction methods are automated pattern recognition approaches to detect network attacks, which are within the scope of the Symmetry journal.},
DOI = {10.3390/sym12091424}
}



@Article{s20174870,
AUTHOR = {Dumitrescu, Cătălin and Minea, Marius and Costea, Ilona Mădălina and Cosmin Chiva, Ionut and Semenescu, Augustin},
TITLE = {Development of an Acoustic System for UAV Detection},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4870},
URL = {https://www.mdpi.com/1424-8220/20/17/4870},
ISSN = {1424-8220},
ABSTRACT = {The purpose of this paper is to investigate the possibility of developing and using an intelligent, flexible, and reliable acoustic system, designed to discover, locate, and transmit the position of unmanned aerial vehicles (UAVs). Such an application is very useful for monitoring sensitive areas and land territories subject to privacy. The software functional components of the proposed detection and location algorithm were developed employing acoustic signal analysis and concurrent neural networks (CoNNs). An analysis of the detection and tracking performance for remotely piloted aircraft systems (RPASs), measured with a dedicated spiral microphone array with MEMS microphones, was also performed. The detection and tracking algorithms were implemented based on spectrograms decomposition and adaptive filters. In this research, spectrograms with Cohen class decomposition, log-Mel spectrograms, harmonic-percussive source separation and raw audio waveforms of the audio sample, collected from the spiral microphone array&mdash;as an input to the Concurrent Neural Networks were used, in order to determine and classify the number of detected drones in the perimeter of interest.},
DOI = {10.3390/s20174870}
}



@Article{math8091456,
AUTHOR = {Martinez-Soltero, Gabriel and Alanis, Alma Y. and Arana-Daniel, Nancy and Lopez-Franco, Carlos},
TITLE = {Semantic Segmentation for Aerial Mapping},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1456},
URL = {https://www.mdpi.com/2227-7390/8/9/1456},
ISSN = {2227-7390},
ABSTRACT = {Mobile robots commonly have to traverse rough terrains. One way to find the easiest traversable path is by determining the types of terrains in the environment. The result of this process can be used by the path planning algorithms to find the best traversable path. In this work, we present an approach for terrain classification from aerial images while using a Convolutional Neural Networks at the pixel level. The segmented images can be used in robot mapping and navigation tasks. The performance of two different Convolutional Neural Networks is analyzed in order to choose the best architecture.},
DOI = {10.3390/math8091456}
}



@Article{rs12172833,
AUTHOR = {Arabameri, Alireza and Asadi Nalivan, Omid and Chandra Pal, Subodh and Chakrabortty, Rabin and Saha, Asish and Lee, Saro and Pradhan, Biswajeet and Tien Bui, Dieu},
TITLE = {Novel Machine Learning Approaches for Modelling the Gully Erosion Susceptibility},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2833},
URL = {https://www.mdpi.com/2072-4292/12/17/2833},
ISSN = {2072-4292},
ABSTRACT = {The extreme form of land degradation caused by the formation of gullies is a major challenge for the sustainability of land resources. This problem is more vulnerable in the arid and semi-arid environment and associated damage to agriculture and allied economic activities. Appropriate modeling of such erosion is therefore needed with optimum accuracy for estimating vulnerable regions and taking appropriate initiatives. The Golestan Dam has faced an acute problem of gully erosion over the last decade and has adversely affected society. Here, the artificial neural network (ANN), general linear model (GLM), maximum entropy (MaxEnt), and support vector machine (SVM) machine learning algorithm with 90/10, 80/20, 70/30, 60/40, and 50/50 random partitioning of training and validation samples was selected purposively for estimating the gully erosion susceptibility. The main objective of this work was to predict the susceptible zone with the maximum possible accuracy. For this purpose, random partitioning approaches were implemented. For this purpose, 20 gully erosion conditioning factors were considered for predicting the susceptible areas by considering the multi-collinearity test. The variance inflation factor (VIF) and tolerance (TOL) limit were considered for multi-collinearity assessment for reducing the error of the models and increase the efficiency of the outcome. The ANN with 50/50 random partitioning of the sample is the most optimal model in this analysis. The area under curve (AUC) values of receiver operating characteristics (ROC) in ANN (50/50) for the training and validation data are 0.918 and 0.868, respectively. The importance of the causative factors was estimated with the help of the Jackknife test, which reveals that the most important factor is the topography position index (TPI). Apart from this, the prioritization of all predicted models was estimated taking into account the training and validation data set, which should help future researchers to select models from this perspective. This type of outcome should help planners and local stakeholders to implement appropriate land and water conservation measures.},
DOI = {10.3390/rs12172833}
}



@Article{electronics9091416,
AUTHOR = {Qamar, Faizan and Siddiqui, Maraj Uddin Ahmed and Hindia, MHD Nour and Hassan, Rosilah and Nguyen, Quang Ngoc},
TITLE = {Issues, Challenges, and Research Trends in Spectrum Management: A Comprehensive Overview and New Vision for Designing 6G Networks},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1416},
URL = {https://www.mdpi.com/2079-9292/9/9/1416},
ISSN = {2079-9292},
ABSTRACT = {With an extensive growth in user demand for high throughput, large capacity, and low latency, the ongoing deployment of Fifth-Generation (5G) systems is continuously exposing the inherent limitations of the system, as compared with its original premises. Such limitations are encouraging researchers worldwide to focus on next-generation 6G wireless systems, which are expected to address the constraints. To meet the above demands, future radio network architecture should be effectively designed to utilize its maximum radio spectrum capacity. It must simultaneously utilize various new techniques and technologies, such as Carrier Aggregation (CA), Cognitive Radio (CR), and small cell-based Heterogeneous Networks (HetNet), high-spectrum access (mmWave), and Massive Multiple-Input-Multiple-Output (M-MIMO), to achieve the desired results. However, the concurrent operations of these techniques in current 5G cellular networks create several spectrum management issues; thus, a comprehensive overview of these emerging technologies is presented in detail in this study. Then, the problems involved in the concurrent operations of various technologies for the spectrum management of the current 5G network are highlighted. The study aims to provide a detailed review of cooperative communication among all the techniques and potential problems associated with the spectrum management that has been addressed with the possible solutions proposed by the latest researches. Future research challenges are also discussed to highlight the necessary steps that can help achieve the desired objectives for designing 6G wireless networks.},
DOI = {10.3390/electronics9091416}
}



@Article{ijgi9090527,
AUTHOR = {Liu, Jiantao and Feng, Quanlong and Wang, Ying and Batsaikhan, Bayartungalag and Gong, Jianhua and Li, Yi and Liu, Chunting and Ma, Yin},
TITLE = {Urban Green Plastic Cover Mapping Based on VHR Remote Sensing Images and a Deep Semi-Supervised Learning Framework},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {527},
URL = {https://www.mdpi.com/2220-9964/9/9/527},
ISSN = {2220-9964},
ABSTRACT = {With the rapid process of both urban sprawl and urban renewal, large numbers of old buildings have been demolished in China, leading to wide spread construction sites, which could cause severe dust contamination. To alleviate the accompanied dust pollution, green plastic mulch has been widely used by local governments of China. Therefore, timely and accurate mapping of urban green plastic covered regions is of great significance to both urban environmental management and the understanding of urban growth status. However, the complex spatial patterns of the urban landscape make it challenging to accurately identify these areas of green plastic cover. To tackle this issue, we propose a deep semi-supervised learning framework for green plastic cover mapping using very high resolution (VHR) remote sensing imagery. Specifically, a multi-scale deformable convolution neural network (CNN) was exploited to learn representative and discriminative features under complex urban landscapes. Afterwards, a semi-supervised learning strategy was proposed to integrate the limited labeled data and massive unlabeled data for model co-training. Experimental results indicate that the proposed method could accurately identify green plastic-covered regions in Jinan with an overall accuracy (OA) of 91.63%. An ablation study indicated that, compared with supervised learning, the semi-supervised learning strategy in this study could increase the OA by 6.38%. Moreover, the multi-scale deformable CNN outperforms several classic CNN models in the computer vision field. The proposed method is the first attempt to map urban green plastic-covered regions based on deep learning, which could serve as a baseline and useful reference for future research.},
DOI = {10.3390/ijgi9090527}
}



@Article{rs12182866,
AUTHOR = {Ren, Yongfeng and Yu, Yongtao and Guan, Haiyan},
TITLE = {DA-CapsUNet: A Dual-Attention Capsule U-Net for Road Extraction from Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2866},
URL = {https://www.mdpi.com/2072-4292/12/18/2866},
ISSN = {2072-4292},
ABSTRACT = {The up-to-date and information-accurate road database plays a significant role in many applications. Recently, with the improvement in image resolutions and quality, remote sensing images have provided an important data source for road extraction tasks. However, due to the topology variations, spectral diversities, and complex scenarios, it is still challenging to realize fully automated and highly accurate road extractions from remote sensing images. This paper proposes a novel dual-attention capsule U-Net (DA-CapsUNet) for road region extraction by combining the advantageous properties of capsule representations and the powerful features of attention mechanisms. By constructing a capsule U-Net architecture, the DA-CapsUNet can extract and fuse multiscale capsule features to recover a high-resolution and semantically strong feature representation. By designing the multiscale context-augmentation and two types of feature attention modules, the DA-CapsUNet can exploit multiscale contextual properties at a high-resolution perspective and generate an informative and class-specific feature encoding. Quantitative evaluations on a large dataset showed that the DA-CapsUNet provides a competitive road extraction performance with a precision of 0.9523, a recall of 0.9486, and an F-score of 0.9504, respectively. Comparative studies with eight recently developed deep learning methods also confirmed the applicability and superiority or compatibility of the DA-CapsUNet in road extraction tasks.},
DOI = {10.3390/rs12182866}
}



@Article{app10186147,
AUTHOR = {Li, Jin and Yan, Daifu and Luan, Kuan and Li, Zeyu and Liang, Hong},
TITLE = {Deep Learning-Based Bird’s Nest Detection on Transmission Lines Using UAV Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6147},
URL = {https://www.mdpi.com/2076-3417/10/18/6147},
ISSN = {2076-3417},
ABSTRACT = {In order to ensure the safety of transmission lines, the use of unmanned aerial vehicle (UAV) images for automatic object detection has important application prospects, such as the detection of birds&rsquo; nests. The traditional bird&rsquo;s nest detection methods mainly include the study of morphological characteristics of the bird&rsquo;s nest. These methods have poor applicability and low accuracy. In this work, we propose a deep learning-based birds&rsquo; nests automatic detection framework&mdash;region of interest (ROI) mining faster region-based convolutional neural networks (RCNN). First, the prior dimensions of anchors are obtained by using k-means clustering to improve the accuracy of coordinate boxes generation. Second, in order to balance the number of foreground and background samples in the training process, the focal loss function is introduced in the region proposal network (RPN) classification stage. Finally, the ROI mining module is added to solve the class imbalance problem in the classification stage, combined with the characteristics of difficult-to-classify bird&rsquo;s nest samples in the UAV images. After parameter optimization and experimental verification, the deep learning-based bird&rsquo;s nest automatic detection framework proposed in this work achieves high detection accuracy. In addition, the mean average precision (mAP) and formula 1 (F1) score of the proposed method are higher than the original faster RCNN and cascade RCNN. Our comparative analysis verifies the effectiveness of the proposed method.},
DOI = {10.3390/app10186147}
}



@Article{math8091499,
AUTHOR = {Tran, Huu Khoa and Chiou, Juing-Shian and Dang, Viet-Hung},
TITLE = {New Fusion Algorithm-Reinforced Pilot Control for an Agricultural Tricopter UAV},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1499},
URL = {https://www.mdpi.com/2227-7390/8/9/1499},
ISSN = {2227-7390},
ABSTRACT = {Currently, fuzzy proportional integral derivative (PID) controller schemes, which include simplified fuzzy reasoning decision methodologies and PID parameters, are broadly and efficaciously practiced in various fields from industrial applications, military service, to rescue operations, civilian information and also horticultural observation and agricultural surveillance. A fusion particle swarm optimization (PSO)&ndash;evolutionary programming (EP) algorithm, which is an improved version of the stochastic optimization strategy PSO, was presented for designing and optimizing controller gains in this study. The mathematical calculations of this study include the reproduction of EP with PSO. By minimizing the integral of the absolute error (IAE) criterion that is used for estimating the system response as a fitness function, the obtained integrated design of the fusion PSO&ndash;EP algorithm generated and updated the new elite parameters for proposed controller schemes. This progression was used for the complicated non-linear systems of the attitude-control pilot models of a tricopter unmanned aerial vehicle (UAV) to demonstrate an improvement on the performance in terms of rapid response, precision, reliability, and stability.},
DOI = {10.3390/math8091499}
}



@Article{app10186151,
AUTHOR = {Hung, Sheng-Chieh and Wu, Hui-Ching and Tseng, Ming-Hseng},
TITLE = {Remote Sensing Scene Classification and Explanation Using RSSCNet and LIME},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6151},
URL = {https://www.mdpi.com/2076-3417/10/18/6151},
ISSN = {2076-3417},
ABSTRACT = {Classification is needed in disaster investigation, traffic control, and land-use resource management. How to quickly and accurately classify such remote sensing imagery has become a popular research topic. However, the application of large, deep neural network models for the training of classifiers in the hope of obtaining good classification results is often very time-consuming. In this study, a new CNN (convolutional neutral networks) architecture, i.e., RSSCNet (remote sensing scene classification network), with high generalization capability was designed. Moreover, a two-stage cyclical learning rate policy and the no-freezing transfer learning method were developed to speed up model training and enhance accuracy. In addition, the manifold learning t-SNE (t-distributed stochastic neighbor embedding) algorithm was used to verify the effectiveness of the proposed model, and the LIME (local interpretable model, agnostic explanation) algorithm was applied to improve the results in cases where the model made wrong predictions. Comparing the results of three publicly available datasets in this study with those obtained in previous studies, the experimental results show that the model and method proposed in this paper can achieve better scene classification more quickly and more efficiently.},
DOI = {10.3390/app10186151}
}



@Article{s20185047,
AUTHOR = {Wei, Jiaqi and Shao, Shuai and Ma, Hui and Wang, Penghui and Zhang, Lei and Liu, Hongwei},
TITLE = {High-Resolution ISAR Imaging with Modified Joint Range Spatial-Variant Autofocus and Azimuth Scaling},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5047},
URL = {https://www.mdpi.com/1424-8220/20/18/5047},
ISSN = {1424-8220},
ABSTRACT = {Well-focused and accurately scaled high-resolution inverse synthetic aperture radar (ISAR) images provide a sound basis for feature extraction and target recognition. This paper proposes a novel high-resolution ISAR imaging algorithm, namely modified joint range spatial-variant autofocus and azimuth scaling algorithm (MJAAS). After motion compensation, the shift of the equivalent rotational center (ERC) of the target destroys the linear relationship between the azimuth chirp rates (ACR) of echo signals and the range coordinates of scattering points, thereby leading to the failure of azimuth scaling. Accordingly, a new joint equivalent rotational center position and effective rotational velocity (JERCP-ERV) signal model is established, serving as the basis of MJAAS. By recourse to the Davidon-Fletcher-Powell (DFP) algorithm, MJAAS can jointly estimate the ERCP and ERV by solving a minimum entropy optimization problem, so as to simultaneously achieve accurate azimuth scaling and range spatial-variant autofocus, which further improves the image focusing performance. MJAAS is not restricted by the modes of motion errors (coherent or non-coherent) and the motion compensation methods, so it can be widely applied to real data with the advantages of strong practicality and high accuracy. Extensive experimental results based on both simulated and real data are provided to corroborate the effectiveness of the proposed algorithm.},
DOI = {10.3390/s20185047}
}



@Article{s20185055,
AUTHOR = {Guo, Yahui and Wang, Hanxi and Wu, Zhaofei and Wang, Shuxin and Sun, Hongyong and Senthilnath, J. and Wang, Jingzhe and Robin Bryant, Christopher and Fu, Yongshuo},
TITLE = {Modified Red Blue Vegetation Index for Chlorophyll Estimation and Yield Prediction of Maize from Visible Images Captured by UAV},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5055},
URL = {https://www.mdpi.com/1424-8220/20/18/5055},
ISSN = {1424-8220},
ABSTRACT = {The vegetation index (VI) has been successfully used to monitor the growth and to predict the yield of agricultural crops. In this paper, a long-term observation was conducted for the yield prediction of maize using an unmanned aerial vehicle (UAV) and estimations of chlorophyll contents using SPAD-502. A new vegetation index termed as modified red blue VI (MRBVI) was developed to monitor the growth and to predict the yields of maize by establishing relationships between MRBVI- and SPAD-502-based chlorophyll contents. The coefficients of determination (R2s) were 0.462 and 0.570 in chlorophyll contents&rsquo; estimations and yield predictions using MRBVI, and the results were relatively better than the results from the seven other commonly used VI approaches. All VIs during the different growth stages of maize were calculated and compared with the measured values of chlorophyll contents directly, and the relative error (RE) of MRBVI is the lowest at 0.355. Further, machine learning (ML) methods such as the backpropagation neural network model (BP), support vector machine (SVM), random forest (RF), and extreme learning machine (ELM) were adopted for predicting the yields of maize. All VIs calculated for each image captured during important phenological stages of maize were set as independent variables and the corresponding yields of each plot were defined as dependent variables. The ML models used the leave one out method (LOO), where the root mean square errors (RMSEs) were 2.157, 1.099, 1.146, and 1.698 (g/hundred grain weight) for BP, SVM, RF, and ELM. The mean absolute errors (MAEs) were 1.739, 0.886, 0.925, and 1.356 (g/hundred grain weight) for BP, SVM, RF, and ELM, respectively. Thus, the SVM method performed better in predicting the yields of maize than the other ML methods. Therefore, it is strongly suggested that the MRBVI calculated from images acquired at different growth stages integrated with advanced ML methods should be used for agricultural- and ecological-related chlorophyll estimation and yield predictions.},
DOI = {10.3390/s20185055}
}



@Article{electronics9091459,
AUTHOR = {Kundid Vasić, Mirela and Papić, Vladan},
TITLE = {Multimodel Deep Learning for Person Detection in Aerial Images},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1459},
URL = {https://www.mdpi.com/2079-9292/9/9/1459},
ISSN = {2079-9292},
ABSTRACT = {In this paper, we propose a novel method for person detection in aerial images of nonurban terrain gathered by an Unmanned Aerial Vehicle (UAV), which plays an important role in Search And Rescue (SAR) missions. The UAV in SAR operations contributes significantly due to the ability to survey a larger geographical area from an aerial viewpoint. Because of the high altitude of recording, the object of interest (person) covers a small part of an image (around 0.1%), which makes this task quite challenging. To address this problem, a multimodel deep learning approach is proposed. The solution consists of two different convolutional neural networks in region proposal, as well as in the classification stage. Additionally, contextual information is used in the classification stage in order to improve the detection results. Experimental results tested on the HERIDAL dataset achieved precision of 68.89% and a recall of 94.65%, which is better than current state-of-the-art methods used for person detection in similar scenarios. Consequently, it may be concluded that this approach is suitable for usage as an auxiliary method in real SAR operations.},
DOI = {10.3390/electronics9091459}
}



@Article{s20185073,
AUTHOR = {Khan, Khalil and Albattah, Waleed and Khan, Rehan Ullah and Qamar, Ali Mustafa and Nayab, Durre},
TITLE = {Advances and Trends in Real Time Visual Crowd Analysis},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5073},
URL = {https://www.mdpi.com/1424-8220/20/18/5073},
ISSN = {1424-8220},
ABSTRACT = {Real time crowd analysis represents an active area of research within the computer vision community in general and scene analysis in particular. Over the last 10 years, various methods for crowd management in real time scenario have received immense attention due to large scale applications in people counting, public events management, disaster management, safety monitoring an so on. Although many sophisticated algorithms have been developed to address the task; crowd management in real time conditions is still a challenging problem being completely solved, particularly in wild and unconstrained conditions. In the proposed paper, we present a detailed review of crowd analysis and management, focusing on state-of-the-art methods for both controlled and unconstrained conditions. The paper illustrates both the advantages and disadvantages of state-of-the-art methods. The methods presented comprise the seminal research works on crowd management, and monitoring and then culminating state-of-the-art methods of the newly introduced deep learning methods. Comparison of the previous methods is presented, with a detailed discussion of the direction for future research work. We believe this review article will contribute to various application domains and will also augment the knowledge of the crowd analysis within the research community.},
DOI = {10.3390/s20185073}
}



@Article{app10186210,
AUTHOR = {Zheng, Ruihao and Xiong, Chen and Deng, Xiangbin and Li, Qiangsheng and Li, Yi},
TITLE = {Assessment of Earthquake Destructive Power to Structures Based on Machine Learning Methods},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6210},
URL = {https://www.mdpi.com/2076-3417/10/18/6210},
ISSN = {2076-3417},
ABSTRACT = {This study presents a machine learning-based method for the destructive power assessment of earthquake to structures. First, the analysis procedure of the method is presented, and the backpropagation neural network (BPNN) and convolutional neural network (CNN) are used as the machine learning algorithms. Second, the optimized BPNN architecture is obtained by discussing the influence of a different number of hidden layers and nodes. Third, the CNN architecture is proposed based on several classical deep learning networks. To build the machine learning models, 50,570 time-history analysis results of a structural system subjected to different ground motions are used as training, validation, and test samples. The results of the BPNN indicate that the features extraction method based on the short-time Fourier transform (STFT) can well reflect the frequency-/time-domain characteristics of ground motions. The results of the CNN indicate that the CNN exhibits better accuracy (R2 = 0.8737) compared with that of the BPNN (R2 = 0.6784). Furthermore, the CNN model exhibits remarkable computational efficiency, the prediction of 1000 structures based on the CNN model takes 0.762 s, while 507.81 s are required for the conventional time-history analysis (THA)-based simulation. Feature visualization of different layers of the CNN reveals that the shallow to deep layers of the CNN can extract the high to low-frequency features of ground motions. The proposed method can assist in the fast prediction of engineering demand parameters of large-number structures, which facilitates the damage or loss assessments of regional structures for timely emergency response and disaster relief after earthquake.},
DOI = {10.3390/app10186210}
}



@Article{s20185130,
AUTHOR = {Guo, Yahui and Yin, Guodong and Sun, Hongyong and Wang, Hanxi and Chen, Shouzhi and Senthilnath, J. and Wang, Jingzhe and Fu, Yongshuo},
TITLE = {Scaling Effects on Chlorophyll Content Estimations with RGB Camera Mounted on a UAV Platform Using Machine-Learning Methods},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5130},
URL = {https://www.mdpi.com/1424-8220/20/18/5130},
ISSN = {1424-8220},
ABSTRACT = {Timely monitoring and precise estimation of the leaf chlorophyll contents of maize are crucial for agricultural practices. The scale effects are very important as the calculated vegetation index (VI) were crucial for the quantitative remote sensing. In this study, the scale effects were investigated by analyzing the linear relationships between VI calculated from red&ndash;green&ndash;blue (RGB) images from unmanned aerial vehicles (UAV) and ground leaf chlorophyll contents of maize measured using SPAD-502. The scale impacts were assessed by applying different flight altitudes and the highest coefficient of determination (R2) can reach 0.85. We found that the VI from images acquired from flight altitude of 50 m was better to estimate the leaf chlorophyll contents using the DJI UAV platform with this specific camera (5472 &times; 3648 pixels). Moreover, three machine-learning (ML) methods including backpropagation neural network (BP), support vector machine (SVM), and random forest (RF) were applied for the grid-based chlorophyll content estimation based on the common VI. The average values of the root mean square error (RMSE) of chlorophyll content estimations using ML methods were 3.85, 3.11, and 2.90 for BP, SVM, and RF, respectively. Similarly, the mean absolute error (MAE) were 2.947, 2.460, and 2.389, for BP, SVM, and RF, respectively. Thus, the ML methods had relative high precision in chlorophyll content estimations using VI; in particular, the RF performed better than BP and SVM. Our findings suggest that the integrated ML methods with RGB images of this camera acquired at a flight altitude of 50 m (spatial resolution 0.018 m) can be perfectly applied for estimations of leaf chlorophyll content in agriculture.},
DOI = {10.3390/s20185130}
}



@Article{pr8091123,
AUTHOR = {Park, You-Jin and Fan, Shu-Kai S. and Hsu, Chia-Yu},
TITLE = {A Review on Fault Detection and Process Diagnostics in Industrial Processes},
JOURNAL = {Processes},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1123},
URL = {https://www.mdpi.com/2227-9717/8/9/1123},
ISSN = {2227-9717},
ABSTRACT = {The main roles of fault detection and diagnosis (FDD) for industrial processes are to make an effective indicator which can identify faulty status of a process and then to take a proper action against a future failure or unfavorable accidents. In order to enhance many process performances (e.g., quality and throughput), FDD has attracted great attention from various industrial sectors. Many traditional FDD techniques have been developed for checking the existence of a trend or pattern in the process or whether a certain process variable behaves normally or not. However, they might fail to produce several hidden characteristics of the process or fail to discover the faults in processes due to underlying process dynamics. In this paper, we present current research and developments of FDD approaches for process monitoring as well as a broad literature review of many useful FDD approaches.},
DOI = {10.3390/pr8091123}
}



@Article{rs12182934,
AUTHOR = {Xu, Jin and Quackenbush, Lindi J. and Volk, Timothy A. and Im, Jungho},
TITLE = {Forest and Crop Leaf Area Index Estimation Using Remote Sensing: Research Trends and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2934},
URL = {https://www.mdpi.com/2072-4292/12/18/2934},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is an important vegetation leaf structure parameter in forest and agricultural ecosystems. Remote sensing techniques can provide an effective alternative to field-based observation of LAI. Differences in canopy structure result in different sensor types (active or passive), platforms (terrestrial, airborne, or satellite), and models being appropriate for the LAI estimation of forest and agricultural systems. This study reviews the application of remote sensing-based approaches across different system configurations (passive, active, and multisource sensors on different collection platforms) that are used to estimate forest and crop LAI and explores uncertainty analysis in LAI estimation. A comparison of the difference in LAI estimation for forest and agricultural applications given the different structure of these ecosystems is presented, particularly as this relates to spatial scale. The ease of use of empirical models supports these as the preferred choice for forest and crop LAI estimation. However, performance variation among different empirical models for forest and crop LAI estimation limits the broad application of specific models. The development of models that facilitate the strategic incorporation of local physiology and biochemistry parameters for specific forests and crop growth stages from various temperature zones could improve the accuracy of LAI estimation models and help develop models that can be applied more broadly. In terms of scale issues, both spectral and spatial scales impact the estimation of LAI. Exploration of the quantitative relationship between scales of data from different sensors could help forest and crop managers more appropriately and effectively apply different data sources. Uncertainty coming from various sources results in reduced accuracy in estimating LAI. While Bayesian approaches have proven effective to quantify LAI estimation uncertainty based on the uncertainty of model inputs, there is still a need to quantify uncertainty from remote sensing data source, ground measurements and related environmental factors to mitigate the impacts of model uncertainty and improve LAI estimation.},
DOI = {10.3390/rs12182934}
}



@Article{s20185186,
AUTHOR = {Rutkowski, Adam and Kawalec, Adam},
TITLE = {Some of Problems of Direction Finding of Ground-Based Radars Using Monopulse Location System Installed on Unmanned Aerial Vehicle},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5186},
URL = {https://www.mdpi.com/1424-8220/20/18/5186},
ISSN = {1424-8220},
ABSTRACT = {Locating active radars in real environmental conditions is a very important and complex task. The efficiency of the direction finding (DF) of ground-based radars and other microwave emitters using unmanned aerial vehicles (UAV) is dependent on the parameters of applied devices for angle location of microwave emitters, and on the construction and modes of operation of the observed transmitting antenna systems. An additional factor having the influence on DF of the radar, when are used systems installed on the UAV, is the rotation of the antenna of a radar. The accuracy of estimation of direction of any microwave transmitter is determined by the terrain properties that surround the transmitter and the objects reflecting microwave signals. The exemplary shapes of the radar antenna patterns and the associated relationships with the probability of remotely detecting the radar and determining its bearings are described. The simulated patterns of the signals received at an emitter-locating device mounted on a UAV and the expected results of a monopulse DF based on these signals are presented. The novelty of this work is the analysis of the DF efficiency of radars in conditions where intense multi-path phenomena appear, and for various amplitudes and phases of the direct signal and multi-path signals that reach the UAV when assuming that so-called simple signals and linear frequency modulation (LFM) signals are transmitted by the radar. The primary focus is on multi-path phenomenon, which can make it difficult, but not entirely impossible, to detect activity and location of radar with a low-flying small UAV and using only monopulse techniques, that is, when only a single pulse emitted by a radar must be sufficient to DF of this radar. Direction of arrival (DOA) algorithms of signals in dense signal environment were not presented in the work, but relevant suggestions were made for the design of such algorithms.},
DOI = {10.3390/s20185186}
}



@Article{rs12182977,
AUTHOR = {Sapkota, Bishwa and Singh, Vijay and Neely, Clark and Rajan, Nithya and Bagavathiannan, Muthukumar},
TITLE = {Detection of Italian Ryegrass in Wheat and Prediction of Competitive Interactions Using Remote-Sensing and Machine-Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2977},
URL = {https://www.mdpi.com/2072-4292/12/18/2977},
ISSN = {2072-4292},
ABSTRACT = {Italian ryegrass (Lolium perenne ssp. multiflorum (Lam) Husnot) is a troublesome weed species in wheat (Triticum aestivum) production in the United States, severely affecting grain yields. Spatial mapping of ryegrass infestation in wheat fields and early prediction of its impact on yield can assist management decision making. In this study, unmanned aerial systems (UAS)-based red, green and blue (RGB) imageries acquired at an early wheat growth stage in two different experimental sites were used for developing predictive models. Deep neural networks (DNNs) coupled with an extensive feature selection method were used to detect ryegrass in wheat and estimate ryegrass canopy coverage. Predictive models were developed by regressing early-season ryegrass canopy coverage (%) with end-of-season (at wheat maturity) biomass and seed yield of ryegrass, as well as biomass and grain yield reduction (%) of wheat. Italian ryegrass was detected with high accuracy (precision = 95.44 &plusmn; 4.27%, recall = 95.48 &plusmn; 5.05%, F-score = 95.56 &plusmn; 4.11%) using the best model which included four features: hue, saturation, excess green index, and visible atmospheric resistant index. End-of-season ryegrass biomass was predicted with high accuracy (R2 = 0.87), whereas the other variables had moderate to high accuracy levels (R2 values of 0.74 for ryegrass seed yield, 0.73 for wheat biomass reduction, and 0.69 for wheat grain yield reduction). The methodology demonstrated in the current study shows great potential for mapping and quantifying ryegrass infestation and predicting its competitive response in wheat, allowing for timely management decisions.},
DOI = {10.3390/rs12182977}
}



@Article{rs12182982,
AUTHOR = {Gée, Christelle and Denimal, Emmanuel},
TITLE = {RGB Image-Derived Indicators for Spatial Assessment of the Impact of Broadleaf Weeds on Wheat Biomass},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2982},
URL = {https://www.mdpi.com/2072-4292/12/18/2982},
ISSN = {2072-4292},
ABSTRACT = {In precision agriculture, the development of proximal imaging systems embedded in autonomous vehicles allows to explore new weed management strategies for site-specific plant application. Accurate monitoring of weeds while controlling wheat growth requires indirect measurements of leaf area index (LAI) and above-ground dry matter biomass (BM) at early growth stages. This article explores the potential of RGB images to assess crop-weed competition in a wheat (Triticum aestivum L.) crop by generating two new indicators, the weed pressure (WP) and the local wheat biomass production (&delta;BMc). The fractional vegetation cover (FVC) of the crop and the weeds was automatically determined from the images with a SVM-RBF classifier, using bag of visual word vectors as inputs. It is based on a new vegetation index called MetaIndex, defined as a vote of six indices widely used in the literature. Beyond a simple map of weed infestation, the map of WP describes the crop-weed competition. The map of &delta;BMc, meanwhile, evaluates the local wheat above-ground biomass production and informs us about a potential stress. It is generated from the wheat FVC because it is highly correlated with LAI (r2 = 0.99) and BM (r2 = 0.93) obtained by destructive methods. By combining these two indicators, we aim at determining whether the origin of the wheat stress is due to weeds or not. This approach opens up new perspectives for the monitoring of weeds and the monitoring of their competition during crop growth with non-destructive and proximal sensing technologies in the early stages of development.},
DOI = {10.3390/rs12182982}
}



@Article{atmos11090987,
AUTHOR = {Kim, Hyun Il and Han, Kun Yeun},
TITLE = {Linking Hydraulic Modeling with a Machine Learning Approach for Extreme Flood Prediction and Response},
JOURNAL = {Atmosphere},
VOLUME = {11},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {987},
URL = {https://www.mdpi.com/2073-4433/11/9/987},
ISSN = {2073-4433},
ABSTRACT = {An emergency action plan (EAP) for reservoirs and urban areas downstream of dams can alleviate damage caused by extreme flooding. An EAP is a disaster action plan that can designate evacuation paths for vulnerable districts. Generally, calculation of dam-break discharge in accordance with dam inflow conditions, calculation of maximum water surface elevation as per hydraulic channel routing, and flood map generation using topographical data are prepared for the purposes of creating an EAP. However, rainfall and flood patterns exhibited in the context of climate change can be extremely diverse. In order to prepare an efficient flood response, techniques should be considered that are capable of generating flood maps promptly while taking dam inflow conditions into account. Therefore, this study aims to propose methodology that is capable of generating flood maps rapidly for any dam inflow conditions. The proposed methodology was performed by linking a dynamic numerical analysis model (DAMBRK) with a random forest regression technique. The previous standard method of drawing flood maps often requires a significant amount of time depending on accuracy and personnel availability; however, the technique proposed here is capable of generating a flood map within one minute. Through use of this methodology, the time taken to prepare flood maps in large-scale water-disaster situations can be reduced. Moreover, methodology for estimating flood risk via use of flood mapping has been proposed. This study would provide assistance in establishing disaster countermeasures that take various flood scenarios into account by promptly providing flood inundation information to disaster-related agencies.},
DOI = {10.3390/atmos11090987}
}



@Article{s20185262,
AUTHOR = {Li, Meizhu and Huang, Shaoguang and De Bock, Jasper and de Cooman, Gert and Pižurica, Aleksandra},
TITLE = {A Robust Dynamic Classifier Selection Approach for Hyperspectral Images with Imprecise Label Information},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5262},
URL = {https://www.mdpi.com/1424-8220/20/18/5262},
ISSN = {1424-8220},
ABSTRACT = {Supervised hyperspectral image (HSI) classification relies on accurate label information. However, it is not always possible to collect perfectly accurate labels for training samples. This motivates the development of classifiers that are sufficiently robust to some reasonable amounts of errors in data labels. Despite the growing importance of this aspect, it has not been sufficiently studied in the literature yet. In this paper, we analyze the effect of erroneous sample labels on probability distributions of the principal components of HSIs, and provide in this way a statistical analysis of the resulting uncertainty in classifiers. Building on the theory of imprecise probabilities, we develop a novel robust dynamic classifier selection (R-DCS) model for data classification with erroneous labels. Particularly, spectral and spatial features are extracted from HSIs to construct two individual classifiers for the dynamic selection, respectively. The proposed R-DCS model is based on the robustness of the classifiers&rsquo; predictions: the extent to which a classifier can be altered without changing its prediction. We provide three possible selection strategies for the proposed model with different computational complexities and apply them on three benchmark data sets. Experimental results demonstrate that the proposed model outperforms the individual classifiers it selects from and is more robust to errors in labels compared to widely adopted approaches.},
DOI = {10.3390/s20185262}
}



@Article{rs12183007,
AUTHOR = {Liu, Bo and Du, Shihong and Du, Shouji and Zhang, Xiuyuan},
TITLE = {Incorporating Deep Features into GEOBIA Paradigm for Remote Sensing Imagery Classification: A Patch-Based Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3007},
URL = {https://www.mdpi.com/2072-4292/12/18/3007},
ISSN = {2072-4292},
ABSTRACT = {The fast and accurate creation of land use/land cover maps from very-high-resolution (VHR) remote sensing imagery is crucial for urban planning and environmental monitoring. Geographic object-based image analysis methods (GEOBIA) provide an effective solution using image objects instead of individual pixels in VHR remote sensing imagery analysis. Simultaneously, convolutional neural networks (CNN) have been widely used in the image processing field because of their powerful feature extraction capabilities. This study presents a patch-based strategy for integrating deep features into GEOBIA for VHR remote sensing imagery classification. To extract deep features from irregular image objects through CNN, a patch-based approach is proposed for representing image objects and learning patch-based deep features, and a deep features aggregation method is proposed for aggregating patch-based deep features into object-based deep features. Finally, both object and deep features are integrated into a GEOBIA paradigm for classifying image objects. We explored the influences of segmentation scales and patch sizes in our method and explored the effectiveness of deep and object features in classification. Moreover, we performed 5-fold stratified cross validations 50 times to explore the uncertainty of our method. Additionally, we explored the importance of deep feature aggregation, and we evaluated our method by comparing it with three state-of-the-art methods in a Beijing dataset and Zurich dataset. The results indicate that smaller segmentation scales were more conducive to VHR remote sensing imagery classification, and it was not appropriate to select too large or too small patches as the patch size should be determined by imagery and its resolution. Moreover, we found that deep features are more effective than object features, while object features still matter for image classification, and deep feature aggregation is a critical step in our method. Finally, our method can achieve the highest overall accuracies compared with the state-of-the-art methods, and the overall accuracies are 91.21% for the Beijing dataset and 99.05% for the Zurich dataset.},
DOI = {10.3390/rs12183007}
}



@Article{rs12183015,
AUTHOR = {Machefer, Mélissande and Lemarchand, François and Bonnefond, Virginie and Hitchins, Alasdair and Sidiropoulos, Panagiotis},
TITLE = {Mask R-CNN Refitting Strategy for Plant Counting and Sizing in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3015},
URL = {https://www.mdpi.com/2072-4292/12/18/3015},
ISSN = {2072-4292},
ABSTRACT = {This work introduces a method that combines remote sensing and deep learning into a framework that is tailored for accurate, reliable and efficient counting and sizing of plants in aerial images. The investigated task focuses on two low-density crops, potato and lettuce. This double objective of counting and sizing is achieved through the detection and segmentation of individual plants by fine-tuning an existing deep learning architecture called Mask R-CNN. This paper includes a thorough discussion on the optimal parametrisation to adapt the Mask R-CNN architecture to this novel task. As we examine the correlation of the Mask R-CNN performance to the annotation volume and granularity (coarse or refined) of remotely sensed images of plants, we conclude that transfer learning can be effectively used to reduce the required amount of labelled data. Indeed, a previously trained Mask R-CNN on a low-density crop can improve performances after training on new crops. Once trained for a given crop, the Mask R-CNN solution is shown to outperform a manually-tuned computer vision algorithm. Model performances are assessed using intuitive metrics such as Mean Average Precision (mAP) from Intersection over Union (IoU) of the masks for individual plant segmentation and Multiple Object Tracking Accuracy (MOTA) for detection. The presented model reaches an mAP of 0.418 for potato plants and 0.660 for lettuces for the individual plant segmentation task. In detection, we obtain a MOTA of 0.781 for potato plants and 0.918 for lettuces.},
DOI = {10.3390/rs12183015}
}



@Article{jimaging6090097,
AUTHOR = {Bhuiyan, Md Abul Ehsan and Witharana, Chandi and Liljedahl, Anna K. and Jones, Benjamin M. and Daanen, Ronald and Epstein, Howard E. and Kent, Kelcy and Griffin, Claire G. and Agnew, Amber},
TITLE = {Understanding the Effects of Optimal Combination of Spectral Bands on Deep Learning Model Predictions: A Case Study Based on Permafrost Tundra Landform Mapping Using High Resolution Multispectral Satellite Imagery},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {97},
URL = {https://www.mdpi.com/2313-433X/6/9/97},
ISSN = {2313-433X},
ABSTRACT = {Deep learning (DL) convolutional neural networks (CNNs) have been rapidly adapted in very high spatial resolution (VHSR) satellite image analysis. DLCNN-based computer visions (CV) applications primarily aim for everyday object detection from standard red, green, blue (RGB) imagery, while earth science remote sensing applications focus on geo object detection and classification from multispectral (MS) imagery. MS imagery includes RGB and narrow spectral channels from near- and/or middle-infrared regions of reflectance spectra. The central objective of this exploratory study is to understand to what degree MS band statistics govern DLCNN model predictions. We scaffold our analysis on a case study that uses Arctic tundra permafrost landform features called ice-wedge polygons (IWPs) as candidate geo objects. We choose Mask RCNN as the DLCNN architecture to detect IWPs from eight-band Worldview-02 VHSR satellite imagery. A systematic experiment was designed to understand the impact on choosing the optimal three-band combination in model prediction. We tasked five cohorts of three-band combinations coupled with statistical measures to gauge the spectral variability of input MS bands. The candidate scenes produced high model detection accuracies for the F1 score, ranging between 0.89 to 0.95, for two different band combinations (coastal blue, blue, green (1,2,3) and green, yellow, red (3,4,5)). The mapping workflow discerned the IWPs by exhibiting low random and systematic error in the order of 0.17&ndash;0.19 and 0.20&ndash;0.21, respectively, for band combinations (1,2,3). Results suggest that the prediction accuracy of the Mask-RCNN model is significantly influenced by the input MS bands. Overall, our findings accentuate the importance of considering the image statistics of input MS bands and careful selection of optimal bands for DLCNN predictions when DLCNN architectures are restricted to three spectral channels.},
DOI = {10.3390/jimaging6090097}
}



@Article{rs12183035,
AUTHOR = {Lai, Ying-Chih and Huang, Zong-Ying},
TITLE = {Detection of a Moving UAV Based on Deep Learning-Based Distance Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3035},
URL = {https://www.mdpi.com/2072-4292/12/18/3035},
ISSN = {2072-4292},
ABSTRACT = {Distance information of an obstacle is important for obstacle avoidance in many applications, and could be used to determine the potential risk of object collision. In this study, the detection of a moving fixed-wing unmanned aerial vehicle (UAV) with deep learning-based distance estimation to conduct a feasibility study of sense and avoid (SAA) and mid-air collision avoidance of UAVs is proposed by using a monocular camera to detect and track an incoming UAV. A quadrotor is regarded as an owned UAV, and it is able to estimate the distance of an incoming fixed-wing intruder. The adopted object detection method is based on the you only look once (YOLO) object detector. Deep neural network (DNN) and convolutional neural network (CNN) methods are applied to exam their performance in the distance estimation of moving objects. The feature extraction of fixed-wing UAVs is based on the VGG-16 model, and then its result is applied to the distance network to estimate the object distance. The proposed model is trained by using synthetic images from animation software and validated by using both synthetic and real flight videos. The results show that the proposed active vision-based scheme is able to detect and track a moving UAV with high detection accuracy and low distance errors.},
DOI = {10.3390/rs12183035}
}



@Article{rs12183046,
AUTHOR = {Xiao, Yingxin and Dong, Yingying and Huang, Wenjiang and Liu, Linyi and Ma, Huiqin and Ye, Huichun and Wang, Kun},
TITLE = {Dynamic Remote Sensing Prediction for Wheat Fusarium Head Blight by Combining Host and Habitat Conditions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3046},
URL = {https://www.mdpi.com/2072-4292/12/18/3046},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing technology provides a feasible option for early prediction for wheat Fusarium head blight (FHB). This study presents a methodology for the dynamic prediction of this classic meteorological crop disease. Host and habitat conditions were comprehensively considered as inputs of the FHB prediction model, and the advantages, accuracy, and generalization ability of the model were evaluated. Firstly, multi-source satellite images were used to predict growth stages and to obtain remote sensing features, then weather features around the predicted stages were extracted. Then, with changes in the inputting features, the severity of FHB was dynamically predicted on February 18, March 6, April 23, and May 9, 2017. Compared to the results obtained by the Logistic model, the prediction with the Relevance Vector Machine performed better, with the overall accuracy on these four dates as 0.71, 0.78, 0.85, and 0.93, and with the area under the receiver operating characteristic curve as 0.66, 0.67, 0.72, and 0.75. Additionally, compared with the prediction with only one factor, the integration of multiple factors was more accurate. The results showed that when the date of the remote sensing features was closer to the heading or flowering stage, the prediction was more accurate, especially in severe areas. Though the habitat conditions were suitable for FHB, the infection can be inhibited when the host&rsquo;s growth meets certain requirements.},
DOI = {10.3390/rs12183046}
}



@Article{s20185343,
AUTHOR = {Opiela, Miroslav and Galčík, František},
TITLE = {Grid-Based Bayesian Filtering Methods for Pedestrian Dead Reckoning Indoor Positioning Using Smartphones},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5343},
URL = {https://www.mdpi.com/1424-8220/20/18/5343},
ISSN = {1424-8220},
ABSTRACT = {Indoor positioning systems for smartphones are often based on Pedestrian Dead Reckoning, which computes the current position from the previously estimated location. Noisy sensor measurements, inaccurate step length estimations, faulty direction detections, and a demand on the real-time calculation introduce the error which is suppressed using a map model and a Bayesian filtering. The main focus of this paper is on grid-based implementations of Bayes filters as an alternative to commonly used Kalman and particle filters. Our previous work regarding grid-based filters is elaborated and enriched with convolution mask calculations. More advanced implementations, the centroid grid filter, and the advanced point-mass filter are introduced. These implementations are analyzed and compared using different configurations on the same raw sensor recordings. The evaluation is performed on three sets of experiments: a custom simple path in faculty building in Slovakia, and on datasets from IPIN competitions from a shopping mall in France, 2018 and a research institute in Italy, 2019. Evaluation results suggests that proposed methods are qualified alternatives to the particle filter. Advantages, drawbacks and proper configurations of these filters are discussed in this paper.},
DOI = {10.3390/s20185343}
}



@Article{computers9030075,
AUTHOR = {Contreras, Ruben and Ayala, Angel and Cruz, Francisco},
TITLE = {Unmanned Aerial Vehicle Control through Domain-Based Automatic Speech Recognition},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {75},
URL = {https://www.mdpi.com/2073-431X/9/3/75},
ISSN = {2073-431X},
ABSTRACT = {Currently, unmanned aerial vehicles, such as drones, are becoming a part of our lives and extend to many areas of society, including the industrialized world. A common alternative for controlling the movements and actions of the drone is through unwired tactile interfaces, for which different remote control devices are used. However, control through such devices is not a natural, human-like communication interface, which sometimes is difficult to master for some users. In this research, we experimented with a domain-based speech recognition architecture to effectively control an unmanned aerial vehicle such as a drone. The drone control was performed in a more natural, human-like way to communicate the instructions. Moreover, we implemented an algorithm for command interpretation using both Spanish and English languages, as well as to control the movements of the drone in a simulated domestic environment. We conducted experiments involving participants giving voice commands to the drone in both languages in order to compare the effectiveness of each, considering the mother tongue of the participants in the experiment. Additionally, different levels of distortion were applied to the voice commands to test the proposed approach when it encountered noisy input signals. The results obtained showed that the unmanned aerial vehicle was capable of interpreting user voice instructions. Speech-to-action recognition improved for both languages with phoneme matching in comparison to only using the cloud-based algorithm without domain-based instructions. Using raw audio inputs, the cloud-based approach achieves 74.81% and 97.04% accuracy for English and Spanish instructions, respectively. However, with our phoneme matching approach the results are improved, yielding 93.33% accuracy for English and 100.00% accuracy for Spanish.},
DOI = {10.3390/computers9030075}
}



@Article{s20185374,
AUTHOR = {Zhao, Long and Ishag Mahmoud, Mubarak Adam and Ren, Honge and Zhu, Meng},
TITLE = {A Visual Tracker Offering More Solutions},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5374},
URL = {https://www.mdpi.com/1424-8220/20/18/5374},
ISSN = {1424-8220},
ABSTRACT = {Most trackers focus solely on robustness and accuracy. Visual tracking, however, is a long-term problem with a high time limitation. A tracker that is robust, accurate, with long-term sustainability and real-time processing, is of high research value and practical significance. In this paper, we comprehensively consider these requirements in order to propose a new, state-of-the-art tracker with an excellent performance. EfficientNet-B0 is adopted for the first time via neural architecture search technology as the backbone network for the tracking task. This improves the network feature extraction ability and significantly reduces the number of parameters required for the tracker backbone network. In addition, maximal Distance Intersection-over-Union is set as the target estimation method, enhancing network stability and increasing the offline training convergence rate. Channel and spatial dual attention mechanisms are employed in the target classification module to improve the discrimination of the trackers. Furthermore, the conjugate gradient optimization strategy increases the speed of the online learning target classification module. A two-stage search method combined with a screening module is proposed to enable the tracker to cope with sudden target movement and reappearance following a brief disappearance. Our proposed method has an obvious speed advantage compared with pure global searching and achieves an optimal performance on OTB2015, VOT2016, VOT2018-LT, UAV-123 and LaSOT while running at over 50 FPS.},
DOI = {10.3390/s20185374}
}



@Article{rs12183084,
AUTHOR = {Abdellatif, Mohamed and Peel, Harriet and Cohn, Anthony G. and Fuentes, Raul},
TITLE = {Pavement Crack Detection from Hyperspectral Images Using a Novel Asphalt Crack Index},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3084},
URL = {https://www.mdpi.com/2072-4292/12/18/3084},
ISSN = {2072-4292},
ABSTRACT = {Detection of road pavement cracks is important and needed at an early stage to repair the road and extend its lifetime for maintaining city roads. Cracks are hard to detect from images taken with visible spectrum cameras due to noise and ambiguity with background textures besides the lack of distinct features in cracks. Hyperspectral images are sensitive to surface material changes and their potential for road crack detection is explored here. The key observation is that road cracks reveal the interior material that is different from the worn surface material. A novel asphalt crack index is introduced here as an additional clue that is sensitive to the spectra in the range 450&ndash;550 nm. The crack index is computed and found to be strongly correlated with the appearance of fresh asphalt cracks. The new index is then used to differentiate cracks from road surfaces. Several experiments have been made, which confirmed that the proposed index is effective for crack detection. The recall-precision analysis showed an increase in the associated F1-score by an average of 21.37% compared to the VIS2 metric in the literature (a metric used to classify pavement condition from hyperspectral data).},
DOI = {10.3390/rs12183084}
}



@Article{agriculture10090416,
AUTHOR = {Chen, Pei-Chun and Chiang, Yen-Cheng and Weng, Pei-Yi},
TITLE = {Imaging Using Unmanned Aerial Vehicles for Agriculture Land Use Classification},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {416},
URL = {https://www.mdpi.com/2077-0472/10/9/416},
ISSN = {2077-0472},
ABSTRACT = {An unmanned aerial vehicle (UAV) was used to capture high-resolution aerial images of crop fields. Software-based image analysis was performed to classify land uses. The purpose was to help relevant agencies use aerial imaging in managing agricultural production. This study involves five townships in the Chianan Plain of Chiayi County, Taiwan. About 100 ha of farmland in each township was selected as a sample area, and a quadcopter and a handheld fixed-wing drone were used to capture visible-light images and multispectral images. The survey was carried out from August to October 2018 and aerial photographs were captured in clear and dry weather. This study used high-resolution images captured from a UAV to classify the uses of agricultural land, and then employed information from multispectral images and elevation data from a digital surface model. The results revealed that visible-light images led to low interpretation accuracy. However, multispectral images and elevation data increased the accuracy rate to nearly 90%. Accordingly, such images and data can effectively enhance the accuracy of land use classification. The technology can reduce costs that are associated with labor and time and can facilitate the establishment of a real-time mapping database.},
DOI = {10.3390/agriculture10090416}
}



@Article{rs12183104,
AUTHOR = {An, Gangqiang and Xing, Minfeng and He, Binbin and Liao, Chunhua and Huang, Xiaodong and Shang, Jiali and Kang, Haiqi},
TITLE = {Using Machine Learning for Estimating Rice Chlorophyll Content from In Situ Hyperspectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3104},
URL = {https://www.mdpi.com/2072-4292/12/18/3104},
ISSN = {2072-4292},
ABSTRACT = {Chlorophyll is an essential pigment for photosynthesis in crops, and leaf chlorophyll content can be used as an indicator for crop growth status and help guide nitrogen fertilizer applications. Estimating crop chlorophyll content plays an important role in precision agriculture. In this study, a variable, rate of change in reflectance between wavelengths &lsquo;a&rsquo; and &lsquo;b&rsquo; (RCRWa-b), derived from in situ hyperspectral remote sensing data combined with four advanced machine learning techniques, Gaussian process regression (GPR), random forest regression (RFR), support vector regression (SVR), and gradient boosting regression tree (GBRT), were used to estimate the chlorophyll content (measured by a portable soil&ndash;plant analysis development meter) of rice. The performances of the four machine learning models were assessed and compared using root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination (R2). The results revealed that four features of RCRWa-b, RCRW551.0&ndash;565.6, RCRW739.5&ndash;743.5, RCRW684.4&ndash;687.1 and RCRW667.9&ndash;672.0, were effective in estimating the chlorophyll content of rice, and the RFR model generated the highest prediction accuracy (training set: RMSE = 1.54, MAE =1.23 and R2 = 0.95; validation set: RMSE = 2.64, MAE = 1.99 and R2 = 0.80). The GPR model was found to have the strongest generalization (training set: RMSE = 2.83, MAE = 2.16 and R2 = 0.77; validation set: RMSE = 2.97, MAE = 2.30 and R2 = 0.76). We conclude that RCRWa-b is a useful variable to estimate chlorophyll content of rice, and RFR and GPR are powerful machine learning algorithms for estimating the chlorophyll content of rice.},
DOI = {10.3390/rs12183104}
}



