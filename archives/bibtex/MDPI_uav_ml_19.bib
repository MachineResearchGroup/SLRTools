
@Article{su13063279,
AUTHOR = {Emin, Mirzat and Anwar, Erpan and Liu, Suhong and Emin, Bilal and Mamut, Maryam and Abdukeram, Abduwali and Liu, Ting},
TITLE = {Target Detection-Based Tree Recognition in a Spruce Forest Area with a High Tree Density—Implications for Estimating Tree Numbers},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {3279},
URL = {https://www.mdpi.com/2071-1050/13/6/3279},
ISSN = {2071-1050},
ABSTRACT = {Here, unmanned aerial vehicle (UAV) remote sensing and machine vision were used to automatically, accurately, and efficiently count Tianshan spruce and improve the efficiency of scientific forest management, focusing on a typical Tianshan spruce forest on Tianshan Mountain, middle Asia. First, the UAV in the sampling area was cropped from the image, and a target-labeling tool was used. The Tianshan spruce trees were annotated to construct a data set, and four models were used to identify and verify them in three different areas (low, medium, and high canopy closures). Finally, the combined number of trees was calculated. The average accuracy of the detection frame, mean accuracy and precision (mAP), was used to determine the target detection accuracy. The Faster Region Convolutional Neural Network (Faster-RCNN) model achieved the highest accuracies (96.36%, 96.32%, and 95.54% under low, medium, and high canopy closures, respectively) and the highest mAP (85%). Canopy closure affected the detection and recognition accuracy; YOLOv3, YOLOv4, and Faster-RCNN all showed varying spruce recognition accuracies at different densities. The accuracy of the Faster-RCNN model decreased by at least 0.82%. Combining UAV remote sensing with target detection networks can identify and quantify statistics regarding Tianshan spruce. This solves the shortcomings of traditional monitoring methods and is significant for understanding and monitoring forest ecosystems.},
DOI = {10.3390/su13063279}
}



@Article{rs13061128,
AUTHOR = {Tahmasbian, Iman and Morgan, Natalie K. and Hosseini Bai, Shahla and Dunlop, Mark W. and Moss, Amy F.},
TITLE = {Comparison of Hyperspectral Imaging and Near-Infrared Spectroscopy to Determine Nitrogen and Carbon Concentrations in Wheat},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1128},
URL = {https://www.mdpi.com/2072-4292/13/6/1128},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imaging (HSI) is an emerging rapid and non-destructive technology that has promising application within feed mills and processing plants in poultry and other intensive animal industries. HSI may be advantageous over near infrared spectroscopy (NIRS) as it scans entire samples, which enables compositional gradients and sample heterogenicity to be visualised and analysed. This study was a preliminary investigation to compare the performance of HSI with that of NIRS for quality measurements of ground samples of Australian wheat and to identify the most important spectral regions for predicting carbon (C) and nitrogen (N) concentrations. In total, 69 samples were scanned using an NIRS (400–2500 nm), and two HSI cameras operated in 400–1000 nm (VNIR) and 1000–2500 nm (SWIR) spectral regions. Partial least square regression (PLSR) models were used to correlate C and N concentrations of 63 calibration samples with their spectral reflectance, with 6 additional samples used for testing the models. The accuracy of the HSI predictions (full spectra) were similar or slightly higher than those of NIRS (NIRS Rc2 for C = 0.90 and N = 0.96 vs. HSI Rc2 for C (VNIR) = 0.97 and N (SWIR) = 0.97). The most important spectral region for C prediction identified using HSI reflectance was 400–550 nm with R2 of 0.93 and RMSE of 0.17% in the calibration set and R2 of 0.86, RMSE of 0.21% and ratio of performance to deviation (RPD) of 2.03 in the test set. The most important spectral regions for predicting N concentrations in the feed samples included 1451–1600 nm, 1901–2050 nm and 2051–2200 nm, providing prediction with R2 ranging from 0.91 to 0.93, RMSE ranging from 0.06% to 0.07% in the calibration sets, R2 from 0.96 to 0.99, RMSE of 0.06% and RPD from 3.47 to 3.92 in the test sets. The prediction accuracy of HSI and NIRS were comparable possibly due to the larger statistical population (larger number of pixels) that HSI provided, despite the fact that HSI had smaller spectral range compared with that of NIRS. In addition, HSI enabled visualising the variability of C and N in the samples. Therefore, HSI is advantageous compared to NIRS as it is a multifunctional tool that poses many potential applications in data collection and quality assurance within feed mills and poultry processing plants. The ability to more accurately measure and visualise the properties of feed ingredients has potential economic benefits and therefore additional investigation and development of HSI in this application is warranted.},
DOI = {10.3390/rs13061128}
}



@Article{educsci11030126,
AUTHOR = {Danaher, Michael and Wu, Jiaping and Hewson, Michael},
TITLE = {Sustainability: A Regional Australian Experience of Educating Secondary Geography Teachers},
JOURNAL = {Education Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {126},
URL = {https://www.mdpi.com/2227-7102/11/3/126},
ISSN = {2227-7102},
ABSTRACT = {The United Nations Sustainable Development Goal (SDG) number four seeks an equitable and widespread education that enables an outcome of sustainable development by 2030. Intersecting the studies of society and earth processes, a geographical education is well placed to make cohesive sense of all the individual knowledge silos that contribute to achieving sustainability. Geography education is compulsory for the first three years of the secondary education curriculum in Australia; however, research has shown that many geography teachers are underprepared and report limitations in their teaching of sustainability. This article engages with this research problem to provide a critical reflection, using experiential knowledge as an analytical lens, on how tertiary level geography training at one Australian regional university can equip undergraduate teacher education students with the values, knowledge, and skills needed to develop their future students’ understanding and appreciation of the principles of sustainability. The authors unpacked a geography minor for a Bachelor of Secondary Education degree at Central Queensland University and, deploying content analysis, explain how three units in that minor can develop these students’ values, knowledge, and skills through fostering initiatives and activities. The analysis was framed by elements of pedagogy that offer learners a context for developing active, global citizenship and participation to understand the interdependencies of ecological, societal, and economic systems including a multisided view of sustainability and sustainable development. The study concluded that the three geography units engage student teachers in sustainable thinking in a variety of ways, which can have a wider application in the geography curricula in other teacher education courses. More importantly, however, the study found that there is a critical need for collaboration between university teachers of sustainability content and university teachers of school-based pedagogy in order to maximise the efficacy of sustainability education in schools.},
DOI = {10.3390/educsci11030126}
}



@Article{rs13061134,
AUTHOR = {El-Alem, Anas and Chokmani, Karem and Venkatesan, Aarthi and Rachid, Lhissou and Agili, Hachem and Dedieu, Jean-Pierre},
TITLE = {How Accurate Is an Unmanned Aerial Vehicle Data-Based Model Applied on Satellite Imagery for Chlorophyll-a Estimation in Freshwater Bodies?},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1134},
URL = {https://www.mdpi.com/2072-4292/13/6/1134},
ISSN = {2072-4292},
ABSTRACT = {Optical sensors are increasingly sought to estimate the amount of chlorophyll a (chl_a) in freshwater bodies. Most, whether empirical or semi-empirical, are data-oriented. Two main limitations are often encountered in the development of such models. The availability of data needed for model calibration, validation, and testing and the locality of the model developed—the majority need a re-parameterization from lake to lake. An Unmanned aerial vehicle (UAV) data-based model for chl_a estimation is developed in this work and tested on Sentinel-2 imagery without any re-parametrization. The Ensemble-based system (EBS) algorithm was used to train the model. The leave-one-out cross validation technique was applied to evaluate the EBS, at a local scale, where results were satisfactory (R2 = Nash = 0.94 and RMSE = 5.6 µg chl_a L−1). A blind database (collected over 89 lakes) was used to challenge the EBS’ Sentine-2-derived chl_a estimates at a regional scale. Results were relatively less good, yet satisfactory (R2 = 0.85, RMSE= 2.4 µg chl_a L−1, and Nash = 0.79). However, the EBS has shown some failure to correctly retrieve chl_a concentration in highly turbid waterbodies. This particularity nonetheless does not affect EBS performance, since turbid waters can easily be pre-recognized and masked before the chl_a modeling.},
DOI = {10.3390/rs13061134}
}



@Article{rs13061143,
AUTHOR = {Quan, Yinghui and Tong, Yingping and Feng, Wei and Dauphin, Gabriel and Huang, Wenjiang and Zhu, Wentao and Xing, Mengdao},
TITLE = {Relative Total Variation Structure Analysis-Based Fusion Method for Hyperspectral and LiDAR Data Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1143},
URL = {https://www.mdpi.com/2072-4292/13/6/1143},
ISSN = {2072-4292},
ABSTRACT = {The fusion of the hyperspectral image (HSI) and the light detecting and ranging (LiDAR) data has a wide range of applications. This paper proposes a novel feature fusion method for urban area classification, namely the relative total variation structure analysis (RTVSA), to combine various features derived from HSI and LiDAR data. In the feature extraction stage, a variety of high-performance methods including the extended multi-attribute profile, Gabor filter, and local binary pattern are used to extract the features of the input data. The relative total variation is then applied to remove useless texture information of the processed data. Finally, nonparametric weighted feature extraction is adopted to reduce the dimensions. Random forest and convolutional neural networks are utilized to evaluate the fusion images. Experiments conducted on two urban Houston University datasets (including Houston 2012 and the training portion of Houston 2017) demonstrate that the proposed method can extract the structural correlation from heterogeneous data, withstand a noise well, and improve the land cover classification accuracy.},
DOI = {10.3390/rs13061143}
}



@Article{s21062129,
AUTHOR = {Buja, Ilaria and Sabella, Erika and Monteduro, Anna Grazia and Chiriacò, Maria Serena and De Bellis, Luigi and Luvisi, Andrea and Maruccio, Giuseppe},
TITLE = {Advances in Plant Disease Detection and Monitoring: From Traditional Assays to In-Field Diagnostics},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2129},
URL = {https://www.mdpi.com/1424-8220/21/6/2129},
PubMedID = {33803614},
ISSN = {1424-8220},
ABSTRACT = {Human activities significantly contribute to worldwide spread of phytopathological adversities. Pathogen-related food losses are today responsible for a reduction in quantity and quality of yield and decrease value and financial returns. As a result, “early detection” in combination with “fast, accurate, and cheap” diagnostics have also become the new mantra in plant pathology, especially for emerging diseases or challenging pathogens that spread thanks to asymptomatic individuals with subtle initial symptoms but are then difficult to face. Furthermore, in a globalized market sensitive to epidemics, innovative tools suitable for field-use represent the new frontier with respect to diagnostic laboratories, ensuring that the instruments and techniques used are suitable for the operational contexts. In this framework, portable systems and interconnection with Internet of Things (IoT) play a pivotal role. Here we review innovative diagnostic methods based on nanotechnologies and new perspectives concerning information and communication technology (ICT) in agriculture, resulting in an improvement in agricultural and rural development and in the ability to revolutionize the concept of “preventive actions”, making the difference in fighting against phytopathogens, all over the world.},
DOI = {10.3390/s21062129}
}



@Article{agronomy11030575,
AUTHOR = {Sabzi, Sajad and Pourdarbani, Razieh and Rohban, Mohammad Hossein and García-Mateos, Ginés and Paliwal, Jitendra and Molina-Martínez, José Miguel},
TITLE = {Early Detection of Excess Nitrogen Consumption in Cucumber Plants Using Hyperspectral Imaging Based on Hybrid Neural Networks and the Imperialist Competitive Algorithm},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {575},
URL = {https://www.mdpi.com/2073-4395/11/3/575},
ISSN = {2073-4395},
ABSTRACT = {To achieve healthy and optimal yields of agricultural products, the principles of nutrition must be observed and appropriate fertilizers must be applied. Nutritional deficiencies or overabundance reduce the quality and yield of the products. Thus, their early detection prevents physiological disorders and associated diseases. Most research efforts have focused on spectroscopy, which extracts only spectral data from a single point of the product. The present study aims to detect early excess nitrogen in cucumber plants by using a new hyperspectral imaging technique based on a hybrid of artificial neural networks and the imperialist competitive algorithm (ANN-ICA), which can provide spectral and spatial information on the leaves at the same time. First, cucumber seeds were planted in 18 pots. The same inputs were applied to all the pots until the plants grew; after that, 30% excess nitrogen was applied to nine pots with irrigation water, while it remained constant in the other nine pots. Each day, six leaves were collected from each pot, and their images were captured using a hyperspectral camera (in the range of 400–1100 nm). The wavelengths of 715, 783 and 821 nm were determined as the most effective for early detection of excess nitrogen using a hybrid of artificial neural networks and the artificial bee colony algorithm (ANN-ABC). The parameter of days of treatment was classified using ANN-ICA. The performance of the classifier was evaluated using different criteria, namely recall, accuracy, specificity, precision and the F-measure. The results indicate that the differences between different days were statistically significant. This means that the hyperspectral imaging technique was able to detect plants with excess nitrogen in the near-infrared range (NIR), with a correct classification rate of 96.11%.},
DOI = {10.3390/agronomy11030575}
}



@Article{s21062141,
AUTHOR = {Nafea, Ohoud and Abdul, Wadood and Muhammad, Ghulam and Alsulaiman, Mansour},
TITLE = {Sensor-Based Human Activity Recognition with Spatio-Temporal Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2141},
URL = {https://www.mdpi.com/1424-8220/21/6/2141},
PubMedID = {33803891},
ISSN = {1424-8220},
ABSTRACT = {Human activity recognition (HAR) remains a challenging yet crucial problem to address in computer vision. HAR is primarily intended to be used with other technologies, such as the Internet of Things, to assist in healthcare and eldercare. With the development of deep learning, automatic high-level feature extraction has become a possibility and has been used to optimize HAR performance. Furthermore, deep-learning techniques have been applied in various fields for sensor-based HAR. This study introduces a new methodology using convolution neural networks (CNN) with varying kernel dimensions along with bi-directional long short-term memory (BiLSTM) to capture features at various resolutions. The novelty of this research lies in the effective selection of the optimal video representation and in the effective extraction of spatial and temporal features from sensor data using traditional CNN and BiLSTM. Wireless sensor data mining (WISDM) and UCI datasets are used for this proposed methodology in which data are collected through diverse methods, including accelerometers, sensors, and gyroscopes. The results indicate that the proposed scheme is efficient in improving HAR. It was thus found that unlike other available methods, the proposed method improved accuracy, attaining a higher score in the WISDM dataset compared to the UCI dataset (98.53% vs. 97.05%).},
DOI = {10.3390/s21062141}
}



@Article{s21062143,
AUTHOR = {Paiva, Sara and Ahad, Mohd Abdul and Tripathi, Gautami and Feroz, Noushaba and Casalino, Gabriella},
TITLE = {Enabling Technologies for Urban Smart Mobility: Recent Trends, Opportunities and Challenges},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2143},
URL = {https://www.mdpi.com/1424-8220/21/6/2143},
PubMedID = {33803903},
ISSN = {1424-8220},
ABSTRACT = {The increasing population across the globe makes it essential to link smart and sustainable city planning with the logistics of transporting people and goods, which will significantly contribute to how societies will face mobility in the coming years. The concept of smart mobility emerged with the popularity of smart cities and is aligned with the sustainable development goals defined by the United Nations. A reduction in traffic congestion and new route optimizations with reduced ecological footprint are some of the essential factors of smart mobility; however, other aspects must also be taken into account, such as the promotion of active mobility and inclusive mobility, encouraging the use of other types of environmentally friendly fuels and engagement with citizens. The Internet of Things (IoT), Artificial Intelligence (AI), Blockchain and Big Data technology will serve as the main entry points and fundamental pillars to promote the rise of new innovative solutions that will change the current paradigm for cities and their citizens. Mobility-as-a-service, traffic flow optimization, the optimization of logistics and autonomous vehicles are some of the services and applications that will encompass several changes in the coming years with the transition of existing cities into smart cities. This paper provides an extensive review of the current trends and solutions presented in the scope of smart mobility and enabling technologies that support it. An overview of how smart mobility fits into smart cities is provided by characterizing its main attributes and the key benefits of using smart mobility in a smart city ecosystem. Further, this paper highlights other various opportunities and challenges related to smart mobility. Lastly, the major services and applications that are expected to arise in the coming years within smart mobility are explored with the prospective future trends and scope.},
DOI = {10.3390/s21062143}
}



@Article{electronics10060724,
AUTHOR = {Yavariabdi, Amir and Kusetogullari, Huseyin and Celik, Turgay and Cicek, Hasan},
TITLE = {FastUAV-NET: A Multi-UAV Detection Algorithm for Embedded Platforms},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {724},
URL = {https://www.mdpi.com/2079-9292/10/6/724},
ISSN = {2079-9292},
ABSTRACT = {In this paper, a real-time deep learning-based framework for detecting and tracking Unmanned Aerial Vehicles (UAVs) in video streams captured by a fixed-wing UAV is proposed. The proposed framework consists of two steps, namely intra-frame multi-UAV detection and the inter-frame multi-UAV tracking. In the detection step, a new multi-scale UAV detection Convolutional Neural Network (CNN) architecture based on a shallow version of You Only Look Once version 3 (YOLOv3-tiny) widened by Inception blocks is designed to extract local and global features from input video streams. Here, the widened multi-UAV detection network architecture is termed as FastUAV-NET and aims to improve UAV detection accuracy while preserving computing time of one-step deep detection algorithms in the context of UAV-UAV tracking. To detect UAVs, the FastUAV-NET architecture uses five inception units and adopts a feature pyramid network to detect UAVs. To obtain a high frame rate, the proposed method is applied to every nth frame and then the detected UAVs are tracked in intermediate frames using scalable Kernel Correlation Filter algorithm. The results on the generated UAV-UAV dataset illustrate that the proposed framework obtains 0.7916 average precision with 29 FPS performance on Jetson-TX2. The results imply that the widening of CNN network is a much more effective way than increasing the depth of CNN and leading to a good trade-off between accurate detection and real-time performance. The FastUAV-NET model will be publicly available to the research community to further advance multi-UAV-UAV detection algorithms.},
DOI = {10.3390/electronics10060724}
}



@Article{s21062153,
AUTHOR = {Hou, Yuewu and Liu, Zhaoying and Zhang, Ting and Li, Yujian},
TITLE = {C-UNet: Complement UNet for Remote Sensing Road Extraction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2153},
URL = {https://www.mdpi.com/1424-8220/21/6/2153},
PubMedID = {33808588},
ISSN = {1424-8220},
ABSTRACT = {Roads are important mode of transportation, which are very convenient for people’s daily work and life. However, it is challenging to accuratly extract road information from a high-resolution remote sensing image. This paper presents a road extraction method for remote sensing images with a complement UNet (C-UNet). C-UNet contains four modules. Firstly, the standard UNet is used to roughly extract road information from remote sensing images, getting the first segmentation result; secondly, a fixed threshold is utilized to erase partial extracted information; thirdly, a multi-scale dense dilated convolution UNet (MD-UNet) is introduced to discover the complement road areas in the erased masks, obtaining the second segmentation result; and, finally, we fuse the extraction results of the first and the third modules, getting the final segmentation results. Experimental results on the Massachusetts Road dataset indicate that our C-UNet gets the higher results than the state-of-the-art methods, demonstrating its effectiveness.},
DOI = {10.3390/s21062153}
}



@Article{rs13061172,
AUTHOR = {Chen, De-Yue and Peng, Ling and Li, Wei-Chao and Wang, Yin-Da},
TITLE = {Building Extraction and Number Statistics in WUI Areas Based on UNet Structure and Ensemble Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1172},
URL = {https://www.mdpi.com/2072-4292/13/6/1172},
ISSN = {2072-4292},
ABSTRACT = {Following the advancement and progression of urbanization, management problems of the wildland&ndash;urban interface (WUI) have become increasingly serious. WUI regional governance issues involve many factors including climate, humanities, etc., and have attracted attention and research from all walks of life. Building research plays a vital part in the WUI area. Building location is closely related with the planning and management of the WUI area, and the number of buildings is related to the rescue arrangement. There are two major methods to obtain this building information: one is to obtain them from relevant agencies, which is slow and lacks timeliness, while the other approach is to extract them from high-resolution remote sensing images, which is relatively inexpensive and offers improved timeliness. Inspired by the recent successful application of deep learning, in this paper, we propose a method for extracting building information from high-resolution remote sensing images based on deep learning, which is combined with ensemble learning to extract the building location. Further, we use the idea of image anomaly detection to estimate the number of buildings. After verification on two datasets, we obtain superior semantic segmentation results and achieve better building contour extraction and number estimation.},
DOI = {10.3390/rs13061172}
}



@Article{rs13061176,
AUTHOR = {Zhang, Cheng and Jiang, Wanshou and Zhao, Qing},
TITLE = {Semantic Segmentation of Aerial Imagery via Split-Attention Networks with Disentangled Nonlocal and Edge Supervision},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1176},
URL = {https://www.mdpi.com/2072-4292/13/6/1176},
ISSN = {2072-4292},
ABSTRACT = {In this work, we propose a new deep convolution neural network (DCNN) architecture for semantic segmentation of aerial imagery. Taking advantage of recent research, we use split-attention networks (ResNeSt) as the backbone for high-quality feature expression. Additionally, a disentangled nonlocal (DNL) block is integrated into our pipeline to express the inter-pixel long-distance dependence and highlight the edge pixels simultaneously. Moreover, the depth-wise separable convolution and atrous spatial pyramid pooling (ASPP) modules are combined to extract and fuse multiscale contextual features. Finally, an auxiliary edge detection task is designed to provide edge constraints for semantic segmentation. Evaluation of algorithms is conducted on two benchmarks provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). Extensive experiments demonstrate the effectiveness of each module of our architecture. Precision evaluation based on the Potsdam benchmark shows that the proposed DCNN achieves competitive performance over the state-of-the-art methods.},
DOI = {10.3390/rs13061176}
}



@Article{rs13061184,
AUTHOR = {Geng, Xiaomeng and Shi, Lei and Yang, Jie and Li, Pingxiang and Zhao, Lingli and Sun, Weidong and Zhao, Jinqi},
TITLE = {Ship Detection and Feature Visualization Analysis Based on Lightweight CNN in VH and VV Polarization Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1184},
URL = {https://www.mdpi.com/2072-4292/13/6/1184},
ISSN = {2072-4292},
ABSTRACT = {Synthetic aperture radar (SAR) is a significant application in maritime monitoring, which can provide SAR data throughout the day and in all weather conditions. With the development of artificial intelligence and big data technologies, the data-driven convolutional neural network (CNN) has become widely used in ship detection. However, the accuracy, feature visualization, and analysis of ship detection need to be improved further, when the CNN method is used. In this letter, we propose a two-stage ship detection for land-contained sea area without a traditional sea-land segmentation process. First, to decrease the possibly existing false alarms from the island, an island filter is used as the first step, and then threshold segmentation is used to quickly perform candidate detection. Second, a two-layer lightweight CNN model-based classifier is built to separate false alarms from the ship object. Finally, we discuss the CNN interpretation and visualize in detail when the ship is predicted in vertical–horizontal (VH) and vertical–vertical (VV) polarization. Experiments demonstrate that the proposed method can reach an accuracy of 99.4% and an F1 score of 0.99 based on the Sentinel-1 images for a ship with a size of less than 32 × 32.},
DOI = {10.3390/rs13061184}
}



@Article{s21062180,
AUTHOR = {Liu, Chang and Szirányi, Tamás},
TITLE = {Real-Time Human Detection and Gesture Recognition for On-Board UAV Rescue},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2180},
URL = {https://www.mdpi.com/1424-8220/21/6/2180},
PubMedID = {33804718},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play an important role in numerous technical and scientific fields, especially in wilderness rescue. This paper carries out work on real-time UAV human detection and recognition of body and hand rescue gestures. We use body-featuring solutions to establish biometric communications, like yolo3-tiny for human detection. When the presence of a person is detected, the system will enter the gesture recognition phase, where the user and the drone can communicate briefly and effectively, avoiding the drawbacks of speech communication. A data-set of ten body rescue gestures (i.e., Kick, Punch, Squat, Stand, Attention, Cancel, Walk, Sit, Direction, and PhoneCall) has been created by a UAV on-board camera. The two most important gestures are the novel dynamic Attention and Cancel which represent the set and reset functions respectively. When the rescue gesture of the human body is recognized as Attention, the drone will gradually approach the user with a larger resolution for hand gesture recognition. The system achieves 99.80% accuracy on testing data in body gesture data-set and 94.71% accuracy on testing data in hand gesture data-set by using the deep learning method. Experiments conducted on real-time UAV cameras confirm our solution can achieve our expected UAV rescue purpose.},
DOI = {10.3390/s21062180}
}



@Article{rs13061192,
AUTHOR = {Holst, Christoph and Janßen, Jannik and Schmitz, Berit and Blome, Martin and Dercks, Malte and Schoch-Baumann, Anna and Blöthe, Jan and Schrott, Lothar and Kuhlmann, Heiner and Medic, Tomislav},
TITLE = {Increasing Spatio-Temporal Resolution for Monitoring Alpine Solifluction Using Terrestrial Laser Scanners and 3D Vector Fields},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1192},
URL = {https://www.mdpi.com/2072-4292/13/6/1192},
ISSN = {2072-4292},
ABSTRACT = {This article investigates the usage of terrestrial laser scanner (TLS) point clouds for monitoring the gradual movements of soil masses due to freeze–thaw activity and water saturation, commonly referred to as solifluction. Solifluction is a geomorphic process which is characteristic for hillslopes in (high-)mountain areas, primarily alpine periglacial areas and the arctic. The movement can reach millimetre-to-centimetre per year velocities, remaining well below the typical displacement mangitudes of other frequently monitored natural objects, such as landslides and glaciers. Hence, a better understanding of solifluction processes requires increased spatial and temporal resolution with relatively high measurement accuracy. To that end, we developed a workflow for TLS point cloud processing, providing a 3D vector field that can capture soil mass displacement due to solifluction with high fidelity. This is based on the common image-processing techniques of feature detection and tracking. The developed workflow is tested on a study area placed in Hohe Tauern range of the Austrian Alps with a prominent assemblage of solifluction lobes. The derived displacements were compared with the established geomonitoring approach with total station and signalized markers and point cloud deformation monitoring approaches. The comparison indicated that the achieved results were in the same accuracy range as the established methods, with an advantage of notably higher spatial resolution. This improvement allowed for new insights considering the solifluction processes.},
DOI = {10.3390/rs13061192}
}



@Article{rs13061198,
AUTHOR = {Liu, Bi-Yuan and Chen, Huai-Xin and Huang, Zhou and Liu, Xing and Yang, Yun-Zhi},
TITLE = {ZoomInNet: A Novel Small Object Detector in Drone Images with Cross-Scale Knowledge Distillation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1198},
URL = {https://www.mdpi.com/2072-4292/13/6/1198},
ISSN = {2072-4292},
ABSTRACT = {Drone-based object detection has been widely applied in ground object surveillance, urban patrol, and some other fields. However, the dramatic scale changes and complex backgrounds of drone images usually result in weak feature representation of small objects, which makes it challenging to achieve high-precision object detection. Aiming to improve small objects detection, this paper proposes a novel cross-scale knowledge distillation (CSKD) method, which enhances the features of small objects in a manner similar to image enlargement, so it is termed as ZoomInNet. First, based on an efficient feature pyramid network structure, the teacher and student network are trained with images in different scales to introduce the cross-scale feature. Then, the proposed layer adaption (LA) and feature level alignment (FA) mechanisms are applied to align the feature size of the two models. After that, the adaptive key distillation point (AKDP) algorithm is used to get the crucial positions in feature maps that need knowledge distillation. Finally, the position-aware L2 loss is used to measure the difference between feature maps from cross-scale models, realizing the cross-scale information compression in a single model. Experiments on the challenging Visdrone2018 dataset show that the proposed method draws on the advantages of the image pyramid methods, while avoids the large calculation of them and significantly improves the detection accuracy of small objects. Simultaneously, the comparison with mainstream methods proves that our method has the best performance in small object detection.},
DOI = {10.3390/rs13061198}
}



@Article{app11062797,
AUTHOR = {Muñoz, Filiberto and Cervantes-Rojas, Jorge S. and Valdovinos, Jose M. and Sandre-Hernández, Omar and Salazar, Sergio and Romero, Hugo},
TITLE = {Dynamic Neural Network-Based Adaptive Tracking Control for an Autonomous Underwater Vehicle Subject to Modeling and Parametric Uncertainties},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2797},
URL = {https://www.mdpi.com/2076-3417/11/6/2797},
ISSN = {2076-3417},
ABSTRACT = {This research presents a way to improve the autonomous maneuvering capability of a four-degrees-of-freedom (4DOF) autonomous underwater vehicle (AUV) to perform trajectory tracking tasks in a disturbed underwater environment. This study considers four second-order input-affine nonlinear equations for the translational (x,y,z) and rotational (heading) dynamics of a real AUV subject to hydrodynamic parameter uncertainties (added mass and damping coefficients), unknown damping dynamics, and external disturbances. We proposed an identification-control scheme for each dynamic named Dynamic Neural Control System (DNCS) as a combination of an adaptive neural controller based on nonparametric identification of the effect of unknown dynamics and external disturbances, and on parametric estimation of the added mass dependent input gain. Several numerical simulations validate the satisfactory performance of the proposed DNCS tracking reference trajectories in comparison with a conventional feedback controller with no adaptive compensation. Some graphics showing dynamic approximation of the lumped disturbance as well as estimation of the parametric uncertainty are depicted, validating effective operation of the proposed DNCS when the system is almost completely unknown.},
DOI = {10.3390/app11062797}
}



@Article{s21062208,
AUTHOR = {Park, Kyung Ho and Park, Eunji and Kim, Huy Kang},
TITLE = {Unsupervised Fault Detection on Unmanned Aerial Vehicles: Encoding and Thresholding Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2208},
URL = {https://www.mdpi.com/1424-8220/21/6/2208},
PubMedID = {33809830},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles are expected to create enormous benefits to society, but there are safety concerns in recognizing faults at the vehicle’s control component. Prior studies proposed various fault detection approaches leveraging heuristics-based rules and supervised learning-based models, but there were several drawbacks. The rule-based approaches required an engineer to update the rules on every type of fault, and the supervised learning-based approaches necessitated the acquisition of a finely-labeled training dataset. Moreover, both prior approaches commonly include a limit that the detection model can identify the trained type of faults only, but fail to recognize the unseen type of faults. In pursuit of resolving the aforementioned drawbacks, we proposed a fault detection model utilizing a stacked autoencoder that lies under unsupervised learning. The autoencoder was trained with data from safe UAV states, and its reconstruction loss was examined to distinguish the safe states and faulty states. The key contributions of our study are, as follows. First, we presented a series of analyses to extract essential features from raw UAV flight logs. Second, we designed a fault detection model consisting of the stacked autoencoder and the classifier. Lastly, we validated our approach’s fault detection performance with two datasets consisting of different types of UAV faults.},
DOI = {10.3390/s21062208}
}



@Article{electronics10060747,
AUTHOR = {Passafiume, Marco and Rojhani, Neda and Collodi, Giovanni and Cidronali, Alessandro},
TITLE = {Modeling Small UAV Micro-Doppler Signature Using Millimeter-Wave FMCW Radar},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {747},
URL = {https://www.mdpi.com/2079-9292/10/6/747},
ISSN = {2079-9292},
ABSTRACT = {With the increase in small unmanned aerial vehicle (UAV) applications in several technology areas, detection and small UAVs classification have become of interest. To cope with small radar cross-sections (RCSs), slow-flying speeds, and low flying altitudes, the micro-Doppler signature provides some of the most distinctive information to identify and classify targets in many radar systems. In this paper, we introduce an effective model for the micro-Doppler effect that is suitable for frequency-modulated continuous-wave (FMCW) radar applications, and exploit it to investigate UAV signatures. The latter depends on the number of UAV motors, which are considered vibrational sources, and their rotation speed. To demonstrate the reliability of the proposed model, it is used to build simulated FMCW radar images, which are compared with experimental data acquired by a 77 GHz FMCW multiple-input multiple-output (MIMO) cost-effective automotive radar platform. The experimental results confirm the model’s ability to estimate the class of the UAV, namely its number of motors, in different operative scenarios. In addition, the experimental results show that the motors rotation speed does not imprint a significant signature on the classification of the UAV; thus, the estimation of the number of motors represents the only viable parameter for small UAV classification using the micro-Doppler effect.},
DOI = {10.3390/electronics10060747}
}



@Article{rs13061205,
AUTHOR = {Zhao, Caidan and Luo, Gege and Wang, Yilin and Chen, Caiyun and Wu, Zhiqiang},
TITLE = {UAV Recognition Based on Micro-Doppler Dynamic Attribute-Guided Augmentation Algorithm},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1205},
URL = {https://www.mdpi.com/2072-4292/13/6/1205},
ISSN = {2072-4292},
ABSTRACT = {A micro-Doppler signature (m-DS) based on the rotation of drone blades is an effective way to detect and identify small drones. Deep-learning-based recognition algorithms can achieve higher recognition performance, but they needs a large amount of sample data to train models. In addition to the hovering state, the signal samples of small unmanned aerial vehicles (UAVs) should also include flight dynamics, such as vertical, pitch, forward and backward, roll, lateral, and yaw. However, it is difficult to collect all dynamic UAV signal samples under actual flight conditions, and these dynamic flight characteristics will lead to the deviation of the original features, thus affecting the performance of the recognizer. In this paper, we propose a small UAV m-DS recognition algorithm based on dynamic feature enhancement. We extract the combined principal component analysis and discrete wavelet transform (PCA-DWT) time–frequency characteristics and texture features of the UAV’s micro-Doppler signal and use a dynamic attribute-guided augmentation (DAGA) algorithm to expand the feature domain for model training to achieve an adaptive, accurate, and efficient multiclass recognition model in complex environments. After the training model is stable, the average recognition accuracy rate can reach 98% during dynamic flight.},
DOI = {10.3390/rs13061205}
}



@Article{rs13061211,
AUTHOR = {Fan, Pan and Lang, Guodong and Yan, Bin and Lei, Xiaoyan and Guo, Pengju and Liu, Zhijie and Yang, Fuzeng},
TITLE = {A Method of Segmenting Apples Based on Gray-Centered RGB Color Space},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1211},
URL = {https://www.mdpi.com/2072-4292/13/6/1211},
ISSN = {2072-4292},
ABSTRACT = {In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. The rapid and accurate identification of apple targets in an illuminated and unstructured natural orchard is still a key challenge for the picking robot’s vision system. In this paper, by combining local image features and color information, we propose a pixel patch segmentation method based on gray-centered red–green–blue (RGB) color space to address this issue. Different from the existing methods, this method presents a novel color feature selection method that accounts for the influence of illumination and shadow in apple images. By exploring both color features and local variation in apple images, the proposed method could effectively distinguish the apple fruit pixels from other pixels. Compared with the classical segmentation methods and conventional clustering algorithms as well as the popular deep-learning segmentation algorithms, the proposed method can segment apple images more accurately and effectively. The proposed method was tested on 180 apple images. It offered an average accuracy rate of 99.26%, recall rate of 98.69%, false positive rate of 0.06%, and false negative rate of 1.44%. Experimental results demonstrate the outstanding performance of the proposed method.},
DOI = {10.3390/rs13061211}
}



@Article{s21062233,
AUTHOR = {Li, Ke and Zhang, Kun and Zhang, Zhenchong and Liu, Zekun and Hua, Shuai and He, Jianliang},
TITLE = {A UAV Maneuver Decision-Making Algorithm for Autonomous Airdrop Based on Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2233},
URL = {https://www.mdpi.com/1424-8220/21/6/2233},
PubMedID = {33806886},
ISSN = {1424-8220},
ABSTRACT = {How to operate an unmanned aerial vehicle (UAV) safely and efficiently in an interactive environment is challenging. A large amount of research has been devoted to improve the intelligence of a UAV while performing a mission, where finding an optimal maneuver decision-making policy of the UAV has become one of the key issues when we attempt to enable the UAV autonomy. In this paper, we propose a maneuver decision-making algorithm based on deep reinforcement learning, which generates efficient maneuvers for a UAV agent to execute the airdrop mission autonomously in an interactive environment. Particularly, the training set of the learning algorithm by the Prioritized Experience Replay is constructed, that can accelerate the convergence speed of decision network training in the algorithm. It is shown that a desirable and effective maneuver decision-making policy can be found by extensive experimental results.},
DOI = {10.3390/s21062233}
}



@Article{rs13061217,
AUTHOR = {Philipp, Marius and Dietz, Andreas and Buchelt, Sebastian and Kuenzer, Claudia},
TITLE = {Trends in Satellite Earth Observation for Permafrost Related Analyses—A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1217},
URL = {https://www.mdpi.com/2072-4292/13/6/1217},
ISSN = {2072-4292},
ABSTRACT = {Climate change and associated Arctic amplification cause a degradation of permafrost which in turn has major implications for the environment. The potential turnover of frozen ground from a carbon sink to a carbon source, eroding coastlines, landslides, amplified surface deformation and endangerment of human infrastructure are some of the consequences connected with thawing permafrost. Satellite remote sensing is hereby a powerful tool to identify and monitor these features and processes on a spatially explicit, cheap, operational, long-term basis and up to circum-Arctic scale. By filtering after a selection of relevant keywords, a total of 325 articles from 30 international journals published during the last two decades were analyzed based on study location, spatio-temporal resolution of applied remote sensing data, platform, sensor combination and studied environmental focus for a comprehensive overview of past achievements, current efforts, together with future challenges and opportunities. The temporal development of publication frequency, utilized platforms/sensors and the addressed environmental topic is thereby highlighted. The total number of publications more than doubled since 2015. Distinct geographical study hot spots were revealed, while at the same time large portions of the continuous permafrost zone are still only sparsely covered by satellite remote sensing investigations. Moreover, studies related to Arctic greenhouse gas emissions in the context of permafrost degradation appear heavily underrepresented. New tools (e.g., Google Earth Engine (GEE)), methodologies (e.g., deep learning or data fusion etc.) and satellite data (e.g., the Methane Remote Sensing LiDAR Mission (Merlin) and the Sentinel-fleet) will thereby enable future studies to further investigate the distribution of permafrost, its thermal state and its implications on the environment such as thermokarst features and greenhouse gas emission rates on increasingly larger spatial and temporal scales.},
DOI = {10.3390/rs13061217}
}



@Article{rs13061222,
AUTHOR = {Gonçalves, Gil and Gonçalves, Diogo and Gómez-Gutiérrez, Álvaro and Andriolo, Umberto and Pérez-Alvárez, Juan Antonio},
TITLE = {3D Reconstruction of Coastal Cliffs from Fixed-Wing and Multi-Rotor UAS: Impact of SfM-MVS Processing Parameters, Image Redundancy and Acquisition Geometry},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1222},
URL = {https://www.mdpi.com/2072-4292/13/6/1222},
ISSN = {2072-4292},
ABSTRACT = {Monitoring the dynamics of coastal cliffs is fundamental for the safety of communities, buildings, utilities, and infrastructures located near the coastline. Structure-from-Motion and Multi View Stereo (SfM-MVS) photogrammetry based on Unmanned Aerial Systems (UAS) is a flexible and cost-effective surveying technique for generating a dense 3D point cloud of the whole cliff face (from bottom to top), with high spatial and temporal resolution. In this paper, in order to generate a reproducible, reliable, precise, accurate, and dense point cloud of the cliff face, a comprehensive analysis of the SfM-MVS processing parameters, image redundancy and acquisition geometry was performed. Using two different UAS, a fixed-wing and a multi-rotor, two flight missions were executed with the aim of reconstructing the geometry of an almost vertical cliff located at the central Portuguese coast. The results indicated that optimizing the processing parameters of Agisoft Metashape can improve the 3D accuracy of the point cloud up to 2 cm. Regarding the image acquisition geometry, the high off-nadir (90°) dataset taken by the multi-rotor generated a denser and more accurate point cloud, with lesser data gaps, than that generated by the low off-nadir dataset (3°) taken by the fixed wing. Yet, it was found that reducing properly the high overlap of the image dataset acquired by the multi-rotor drone permits to get an optimal image dataset, allowing to speed up the processing time without compromising the accuracy and density of the generated point cloud. The analysis and results presented in this paper improve the knowledge required for the 3D reconstruction of coastal cliffs by UAS, providing new insights into the technical aspects needed for optimizing the monitoring surveys.},
DOI = {10.3390/rs13061222}
}



@Article{rs13071231,
AUTHOR = {Agrillo, Emiliano and Filipponi, Federico and Pezzarossa, Alice and Casella, Laura and Smiraglia, Daniela and Orasi, Arianna and Attorre, Fabio and Taramelli, Andrea},
TITLE = {Earth Observation and Biodiversity Big Data for Forest Habitat Types Classification and Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1231},
URL = {https://www.mdpi.com/2072-4292/13/7/1231},
ISSN = {2072-4292},
ABSTRACT = {In the light of the “Biological Diversity” concept, habitats are cardinal pieces for biodiversity quantitative estimation at a local and global scale. In Europe EUNIS (European Nature Information System) is a system tool for habitat identification and assessment. Earth Observation (EO) data, which are acquired by satellite sensors, offer new opportunities for environmental sciences and they are revolutionizing the methodologies applied. These are providing unprecedented insights for habitat monitoring and for evaluating the Sustainable Development Goals (SDGs) indicators. This paper shows the results of a novel approach for a spatially explicit habitat mapping in Italy at a national scale, using a supervised machine learning model (SMLM), through the combination of vegetation plot database (as response variable), and both spectral and environmental predictors. The procedure integrates forest habitat data in Italy from the European Vegetation Archive (EVA), with Sentinel-2 imagery processing (vegetation indices time series, spectral indices, and single bands spectral signals) and environmental data variables (i.e., climatic and topographic), to parameterize a Random Forests (RF) classifier. The obtained results classify 24 forest habitats according to the EUNIS III level: 12 broadleaved deciduous (T1), 4 broadleaved evergreen (T2) and eight needleleaved forest habitats (T3), and achieved an overall accuracy of 87% at the EUNIS II level classes (T1, T2, T3), and an overall accuracy of 76.14% at the EUNIS III level. The highest overall accuracy value was obtained for the broadleaved evergreen forest equal to 91%, followed by 76% and 68% for needleleaved and broadleaved deciduous habitat forests, respectively. The results of the proposed methodology open the way to increase the EUNIS habitat categories to be mapped together with their geographical extent, and to test different semi-supervised machine learning algorithms and ensemble modelling methods.},
DOI = {10.3390/rs13071231}
}



@Article{rs13071238,
AUTHOR = {Kaivosoja, Jere and Hautsalo, Juho and Heikkinen, Jaakko and Hiltunen, Lea and Ruuttunen, Pentti and Näsi, Roope and Niemeläinen, Oiva and Lemsalu, Madis and Honkavaara, Eija and Salonen, Jukka},
TITLE = {Reference Measurements in Developing UAV Systems for Detecting Pests, Weeds, and Diseases},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1238},
URL = {https://www.mdpi.com/2072-4292/13/7/1238},
ISSN = {2072-4292},
ABSTRACT = {The development of UAV (unmanned aerial vehicle) imaging technologies for precision farming applications is rapid, and new studies are published frequently. In cases where measurements are based on aerial imaging, there is the need to have ground truth or reference data in order to develop reliable applications. However, in several precision farming use cases such as pests, weeds, and diseases detection, the reference data can be subjective or relatively difficult to capture. Furthermore, the collection of reference data is usually laborious and time consuming. It also appears that it is difficult to develop generalisable solutions for these areas. This review studies previous research related to pests, weeds, and diseases detection and mapping using UAV imaging in the precision farming context, underpinning the applied reference measurement techniques. The majority of the reviewed studies utilised subjective visual observations of UAV images, and only a few applied in situ measurements. The conclusion of the review is that there is a lack of quantitative and repeatable reference data measurement solutions in the areas of mapping pests, weeds, and diseases. In addition, the results that the studies present should be reflected in the applied references. An option in the future approach could be the use of synthetic data as reference.},
DOI = {10.3390/rs13071238}
}



@Article{robotics10020052,
AUTHOR = {Oliveira, Luiz F. P. and Moreira, António P. and Silva, Manuel F.},
TITLE = {Advances in Agriculture Robotics: A State-of-the-Art Review and Challenges Ahead},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2218-6581/10/2/52},
ISSN = {2218-6581},
ABSTRACT = {The constant advances in agricultural robotics aim to overcome the challenges imposed by population growth, accelerated urbanization, high competitiveness of high-quality products, environmental preservation and a lack of qualified labor. In this sense, this review paper surveys the main existing applications of agricultural robotic systems for the execution of land preparation before planting, sowing, planting, plant treatment, harvesting, yield estimation and phenotyping. In general, all robots were evaluated according to the following criteria: its locomotion system, what is the final application, if it has sensors, robotic arm and/or computer vision algorithm, what is its development stage and which country and continent they belong. After evaluating all similar characteristics, to expose the research trends, common pitfalls and the characteristics that hinder commercial development, and discover which countries are investing into Research and Development (R&amp;D) in these technologies for the future, four major areas that need future research work for enhancing the state of the art in smart agriculture were highlighted: locomotion systems, sensors, computer vision algorithms and communication technologies. The results of this research suggest that the investment in agricultural robotic systems allows to achieve short—harvest monitoring—and long-term objectives—yield estimation.},
DOI = {10.3390/robotics10020052}
}



@Article{electronics10070771,
AUTHOR = {Liu, Chuanyang and Wu, Yiquan and Liu, Jingjing and Sun, Zuo},
TITLE = {Improved YOLOv3 Network for Insulator Detection in Aerial Images with Diverse Background Interference},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {771},
URL = {https://www.mdpi.com/2079-9292/10/7/771},
ISSN = {2079-9292},
ABSTRACT = {Automatic inspection of insulators from high-voltage transmission lines is of paramount importance to the safety and reliable operation of the power grid. Due to different size insulators and the complex background of aerial images, it is a difficult task to recognize insulators in aerial views. Most of the traditional image processing methods and machine learning methods cannot achieve sufficient performance for insulator detection when diverse background interference is present. In this study, a deep learning method—based on You Only Look Once (YOLO)—will be proposed, capable of detecting insulators from aerial images with complex backgrounds. Firstly, aerial images with common aerial scenes were collected by Unmanned Aerial Vehicle (UAV), and a novel insulator dataset was constructed. Secondly, to enhance feature reuse and propagation, on the basis of YOLOv3 and Dense-Blocks, the YOLOv3-dense network was utilized for insulator detection. To improve detection accuracy for different sized insulators, a structure of multiscale feature fusion was adapted to the YOLOv3-dense network. To obtain abundant semantic information of upper and lower layers, multilevel feature mapping modules were employed across the YOLOv3-dense network. Finally, the YOLOv3-dense network and compared networks were trained and tested on the testing set. The average precision of YOLOv3-dense, YOLOv3, and YOLOv2 were 94.47%, 90.31%, and 83.43%, respectively. Experimental results and analysis validate the claim that the proposed YOLOv3-dense network achieves good performance in the detection of different size insulators amid diverse background interference.},
DOI = {10.3390/electronics10070771}
}



@Article{rs13071245,
AUTHOR = {Lin, Jinhuang and Jin, Xiaobin and Ren, Jie and Liu, Jingping and Liang, Xinyuan and Zhou, Yinkang},
TITLE = {Rapid Mapping of Large-Scale Greenhouse Based on Integrated Learning Algorithm and Google Earth Engine},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1245},
URL = {https://www.mdpi.com/2072-4292/13/7/1245},
ISSN = {2072-4292},
ABSTRACT = {A greenhouse is an important land-use type, which can effectively improve agricultural production conditions and increase crop yields. It is of great significance to obtain the spatial distribution data of greenhouses quickly and accurately for regional agricultural production and food security. Based on the Google Earth Engine cloud platform and Landsat 8 images, this study selected a total of 18 indicators from three aspects of spectral features, texture features and terrain features to construct greenhouse identification features. From a variety of classification algorithms for remote-sensing recognition of greenhouses, this study selected three classifiers with higher accuracy (classification and regression trees (CART), random forest model (randomForest) and maximum entropy model (gmoMaxEnt)) to construct an integrated classification algorithm, and then extracted the spatial distribution data of greenhouses in Jiangsu Province. The results show that: (1) Google Earth Engine with its own massive data and cloud computing capabilities, combined with integrated classification algorithms, can achieve rapid remote-sensing mapping of large-scale greenhouses under complex terrain, and the classification accuracy is higher than that of a single classification algorithm. (2) The combination of different spectral, texture and terrain features has a greater impact on the extraction of regional greenhouses, the combination of all three aspects of features has the highest accuracy. Spectral features are the key factors for greenhouse remote-sensing mapping, but terrain and texture features can also enhance classification accuracy. (3) The greenhouse in Jiangsu Province has significant spatial differentiation and spatial agglomeration characteristics. The most widely distributed greenhouses are mainly concentrated in the agriculturally developed areas such as Dongtai City, Hai’an County, Rudong County and Pizhou City.},
DOI = {10.3390/rs13071245}
}



@Article{rs13071248,
AUTHOR = {Xu, Hao and Yao, Wei and Cheng, Li and Li, Bo},
TITLE = {Multiple Spectral Resolution 3D Convolutional Neural Network for Hyperspectral Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1248},
URL = {https://www.mdpi.com/2072-4292/13/7/1248},
ISSN = {2072-4292},
ABSTRACT = {In recent years, benefiting from the rapid development of deep learning technology in the field of computer vision, the study of hyperspectral image (HSI) classification has also made great progress. However, compared with ordinary RGB images, HSIs are more like 3D cubes; therefore, it is necessary and beneficial to explore classification methods suitable for the very special data structure of HSIs. In this paper, we propose Multiple Spectral Resolution 3D Convolutional Neural Network (MSR-3DCNN) for HSI classification tasks. In MSR-3DCNN, we expand the idea of multi-scale feature fusion and dilated convolution from the spatial dimension to the spectral dimension, and combine 3D convolution and residual connection; therefore, it can better adapt to the 3D cubic form of hyperspectral data and make efficient use of spectral information in different bands. Experimental results on four benchmark datasets show the effectiveness of the proposed approach and its superiority as compared with some state-of-the-art (SOTA) HSI classification methods.},
DOI = {10.3390/rs13071248}
}



@Article{app11072930,
AUTHOR = {Perez-Sanchez, Vicente and Gomez-Tamm, Alejandro E. and Savastano, Emanuela and Arrue, Begoña C. and Ollero, Anibal},
TITLE = {Bio-Inspired Morphing Tail for Flapping-Wings Aerial Robots Using Macro Fiber Composites},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2930},
URL = {https://www.mdpi.com/2076-3417/11/7/2930},
ISSN = {2076-3417},
ABSTRACT = {The aim of this work is to present the development of a bio-inspired approach for a robotic tail using Macro Fiber Composites (MFC) as actuators. The use of this technology will allow achieving closer to the nature approach of the tail, aiming to mimic a bird tail behavior. The tail will change its shape, performing morphing, providing a new type of actuation methodology in flapping control systems. The work is intended as a first step for demonstrating the potential of these technologies for being applied in other parts of the aerials robotics systems. When compared with traditional actuation approaches, one key advantage that is given by the use of MFC is their ability to adapt to different flight conditions via geometric tailoring, imitating what birds do in nature. Theoretical explanations, design, and experimental validation of the developed concept using different methodologies will be presented in this paper.},
DOI = {10.3390/app11072930}
}



@Article{agronomy11040621,
AUTHOR = {López-Andreu, Francisco Javier and Erena, Manuel and Dominguez-Gómez, Jose Antonio and López-Morales, Juan Antonio},
TITLE = {Sentinel-2 Images and Machine Learning as Tool for Monitoring of the Common Agricultural Policy: Calasparra Rice as a Case Study},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {621},
URL = {https://www.mdpi.com/2073-4395/11/4/621},
ISSN = {2073-4395},
ABSTRACT = {The European Commission introduces the Control by Monitoring through new technologies to manage Common Agricultural Policy funds through the Regulation 2018/746. The advances in remote sensing have been considered one of these new technologies, mainly since the European Space Agency designed the Copernicus Programme. The Sentinel-1 (radar range) and Sentinel-2 (optical range) satellites have been designed for monitoring agricultural problems based on the characteristics they provide. The data provided by the Sentinel 2 missions, together with the emergence of different scientific disciplines in artificial intelligence —especially machine learning— offer the perfect basis for identifying and classifying any crop and its phenological state. Our research is based on developing and evaluating a pixel-based supervised classification scheme to produce accurate rice crop mapping in a smallholder agricultural zone in Calasparra, Murcia, Spain. Several models are considered to obtain the most suitable model for each element of the time series used; pixel-based classification is performed and finished with a statistical treatment. The highly accurate results obtained, especially across the most significant vegetative development dates, indicate the benefits of using Sentinel-2 data combined with Machine Learning techniques to identify rice crops. It should be noted that it was possible to locate rice crop areas with an overall accuracy of 94% and standard deviation of 1%, which could be increased to 96% (±1%) if we focus on the months of the crop’s highest development state. Thanks to the proposed methodology, the on-site inspections carried out, 5% of the files, have been replaced by remote sensing evaluations of 100% of the analyzed season files. Besides, by adjusting the model input data, it is possible to detect unproductive or abandoned plots.},
DOI = {10.3390/agronomy11040621}
}



@Article{rs13071261,
AUTHOR = {Roncella, Riccardo and Bruno, Nazarena and Diotri, Fabrizio and Thoeni, Klaus and Giacomini, Anna},
TITLE = {Photogrammetric Digital Surface Model Reconstruction in Extreme Low-Light Environments},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1261},
URL = {https://www.mdpi.com/2072-4292/13/7/1261},
ISSN = {2072-4292},
ABSTRACT = {Digital surface models (DSM) have become one of the main sources of geometrical information for a broad range of applications. Image-based systems typically rely on passive sensors which can represent a strong limitation in several survey activities (e.g., night-time monitoring, underground survey and night surveillance). However, recent progresses in sensor technology allow very high sensitivity which drastically improves low-light image quality by applying innovative noise reduction techniques. This work focuses on the performances of night-time photogrammetric systems devoted to the monitoring of rock slopes. The study investigates the application of different camera settings and their reliability to produce accurate DSM. A total of 672 stereo-pairs acquired with high-sensitivity cameras (Nikon D800 and D810) at three different testing sites were considered. The dataset includes different camera configurations (ISO speed, shutter speed, aperture and image under-/over-exposure). The use of image quality assessment (IQA) methods to evaluate the quality of the images prior to the 3D reconstruction is investigated. The results show that modern high-sensitivity cameras allow the reconstruction of accurate DSM in an extreme low-light environment and, exploiting the correct camera setup, achieving comparable results to daylight acquisitions. This makes imaging sensors extremely versatile for monitoring applications at generally low costs.},
DOI = {10.3390/rs13071261}
}



@Article{rs13071266,
AUTHOR = {Rudge, Mitchel L. M. and Levick, Shaun R. and Bartolo, Renee E. and Erskine, Peter D.},
TITLE = {Modelling the Diameter Distribution of Savanna Trees with Drone-Based LiDAR},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1266},
URL = {https://www.mdpi.com/2072-4292/13/7/1266},
ISSN = {2072-4292},
ABSTRACT = {The diameter distribution of savanna tree populations is a valuable indicator of savanna health because changes in the number and size of trees can signal a shift from savanna to grassland or forest. Savanna diameter distributions have traditionally been monitored with forestry techniques, where stem diameter at breast height (DBH) is measured in the field within defined sub-hectare plots. However, because the spatial scale of these plots is often misaligned with the scale of variability in tree populations, there is a need for techniques that can scale-up diameter distribution surveys. Dense point clouds collected from uncrewed aerial vehicle laser scanners (UAV-LS), also known as drone-based LiDAR (Light Detection and Ranging), can be segmented into individual tree crowns then related to stem diameter with the application of allometric scaling equations. Here, we sought to test the potential of UAV-LS tree segmentation and allometric scaling to model the diameter distributions of savanna trees. We collected both UAV-LS and field-survey data from five one-hectare savanna woodland plots in northern Australia, which were divided into two calibration and three validation plots. Within the two calibration plots, allometric scaling equations were developed by linking field-surveyed DBH to the tree metrics of manually delineated tree crowns, where the best performing model had a bias of 1.8% and the relatively high RMSE of 39.2%. A segmentation algorithm was then applied to segment individual tree crowns from UAV-LS derived point clouds, and individual tree level segmentation accuracy was assessed against the manually delineated crowns. 47% of crowns were accurately segmented within the calibration plots and 68% within the validation plots. Using the site-specific allometry, DBH was modelled from crown metrics within all five plots, and these modelled results were compared to field-surveyed diameter distributions. In all plots, there were significant differences between field-surveyed and UAV-LS modelled diameter distributions, which became similar at two of the plots when smaller trees (&lt;10 cm DBH) were excluded. Although the modelled diameter distributions followed the overall trend of field surveys, the non-significant result demonstrates a need for the adoption of remotely detectable proxies of tree size which could replace DBH, as well as more accurate tree detection and segmentation methods for savanna ecosystems.},
DOI = {10.3390/rs13071266}
}



@Article{electronics10070795,
AUTHOR = {Dike, Happiness Ugochi and Zhou, Yimin},
TITLE = {A Robust Quadruplet and Faster Region-Based CNN for UAV Video-Based Multiple Object Tracking in Crowded Environment},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {795},
URL = {https://www.mdpi.com/2079-9292/10/7/795},
ISSN = {2079-9292},
ABSTRACT = {Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) videos has faced several challenges such as motion capture and appearance, clustering, object variation, high altitudes, and abrupt motion. Consequently, the volume of objects captured by the UAV is usually quite small, and the target object appearance information is not always reliable. To solve these issues, a new technique is presented to track objects based on a deep learning technique that attains state-of-the-art performance on standard datasets, such as Stanford Drone and Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking (UAVDT) datasets. The proposed faster RCNN (region-based convolutional neural network) framework was enhanced by integrating a series of activities, including the proper calibration of key parameters, multi-scale training, hard negative mining, and feature collection to improve the region-based CNN baseline. Furthermore, a deep quadruplet network (DQN) was applied to track the movement of the captured objects from the crowded environment, and it was modelled to utilize new quadruplet loss function in order to study the feature space. A deep 6 Rectified linear units (ReLU) convolution was used in the faster RCNN to mine spatial–spectral features. The experimental results on the standard datasets demonstrated a high performance accuracy. Thus, the proposed method can be used to detect multiple objects and track their trajectories with a high accuracy.},
DOI = {10.3390/electronics10070795}
}



@Article{app11073002,
AUTHOR = {Henninger, Helen and Biggs, James and von Ellenrieder, Karl},
TITLE = {Safety-Aware Optimal Attitude Pointing for Low-Thrust Satellites},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3002},
URL = {https://www.mdpi.com/2076-3417/11/7/3002},
ISSN = {2076-3417},
ABSTRACT = {In geostationary orbit, long eclipses and the seasonal variations in the direction and intensity of the solar input can cause damage to sensitive equipment during attitude maneuvers, which may inadvertently point the equipment towards the Sun. The requirement that transmitting and receiving antennae remain pointed towards the Earth creates further restrictions to pointing directions. The aim of the study is to construct a novel geometric and reinforcement-learning-based method to determine attitude guidance maneuvers that maintain the equipment in safe and operational orientations throughout an attitude maneuver. The attitude trajectory is computed numerically using the geometric framing of Pontryagin’s maximum principle applied to the vehicle kinematics using the global matrix Lie group representation on SO(3), and the angular velocities are shaped using free parameters. The values of these free parameters are determined by a reinforcement learning algorithm to avoid the forbidden areas while maintaining the pointing in operational areas (modeled as subsets of the two-sphere of all possible pointing directions of a particular axis). The method is applied to a model geosynchronous satellite and demonstrated in a simulation.},
DOI = {10.3390/app11073002}
}



@Article{rs13071279,
AUTHOR = {Li, Tong and Cui, Lizhen and Xu, Zhihong and Hu, Ronghai and Joshi, Pawan K. and Song, Xiufang and Tang, Li and Xia, Anquan and Wang, Yanfen and Guo, Da and Zhu, Jiapei and Hao, Yanbin and Song, Lan and Cui, Xiaoyong},
TITLE = {Quantitative Analysis of the Research Trends and Areas in Grassland Remote Sensing: A Scientometrics Analysis of Web of Science from 1980 to 2020},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1279},
URL = {https://www.mdpi.com/2072-4292/13/7/1279},
ISSN = {2072-4292},
ABSTRACT = {Grassland remote sensing (GRS) is an important research topic that applies remote sensing technology to grassland ecosystems, reflects the number of grassland resources and grassland health promptly, and provides inversion information used in sustainable development management. A scientometrics analysis based on Science Citation Index-Expanded (SCI-E) was performed to understand the research trends and areas of focus in GRS research studies. A total of 2692 papers related to GRS research studies and 82,208 references published from 1980 to 2020 were selected as the research objects. A comprehensive overview of the field based on the annual documents, research areas, institutions, influential journals, core authors, and temporal trends in keywords were presented in this study. The results showed that the annual number of documents increased exponentially, and more than 100 papers were published each year since 2010. Remote sensing, environmental sciences, and ecology were the most popular Web of Science research areas. The journal Remote Sensing was one of the most popular for researchers to publish documents and shows high development and publishing potential in GRS research studies. The institution with the greatest research documents and most citations was the Chinese Academy of Sciences. Guo X.L., Hill M.J., and Zhang L. were the most productive authors across the 40-year study period in terms of the number of articles published. Seven clusters of research areas were identified that generated contributions to this topic by keyword co-occurrence analysis. We also detected 17 main future directions of GRS research studies by document co-citation analysis. Emerging or underutilized methodologies and technologies, such as unmanned aerial systems (UASs), cloud computing, and deep learning, will continue to further enhance GRS research in the process of achieving sustainable development goals. These results can help related researchers better understand the past and future of GRS research studies.},
DOI = {10.3390/rs13071279}
}



@Article{modelling2020011,
AUTHOR = {Grekhov, Andrii and Kondratiuk, Vasyl and Ilnytska, Svitlana},
TITLE = {Data Traffic Modeling in RPAS/UAV Networks with Different Architectures},
JOURNAL = {Modelling},
VOLUME = {2},
YEAR = {2021},
NUMBER = {2},
PAGES = {210--223},
URL = {https://www.mdpi.com/2673-3951/2/2/11},
ISSN = {2673-3951},
ABSTRACT = {Deploying of Fifth Generation and Beyond Fifth Generation (5G/B5G) wireless networks will require wider coverage, flexible connectivity, low latency, support for a large number of user devices, and more bandwidth. This article explores the paradigm that Remotely Piloted Air Systems (RPASs) or Unmanned Aerial Vehicles (UAVs) are integrated as a communication platform with cellular networks using radio access. It is important to know the possibilities and ways of such integration for effective interaction with RPASs. This paper studies the issues of ensuring the required Quality of Service (QoS) during heavy traffic and the choice of necessary data transmission modes for this. Models of RPAS communication channels with different architectures were created. The relationships between models’ performance and traffic parameters were obtained using the NetCracker Professional 4.1 software. The dependencies of the Average Utilization (AU) on the Transaction Size (TS) were analyzed. The effects of different bandwidths and the Bit Error Rate (BER) were studied. The traffic characteristics in all models were compared.},
DOI = {10.3390/modelling2020011}
}



@Article{s21072363,
AUTHOR = {Campos, Javier and García-Ruíz, Francisco and Gil, Emilio},
TITLE = {Assessment of Vineyard Canopy Characteristics from Vigour Maps Obtained Using UAV and Satellite Imagery},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2363},
URL = {https://www.mdpi.com/1424-8220/21/7/2363},
PubMedID = {33805351},
ISSN = {1424-8220},
ABSTRACT = {Canopy characterisation is a key factor for the success and efficiency of the pesticide application process in vineyards. Canopy measurements to determine the optimal volume rate are currently conducted manually, which is time-consuming and limits the adoption of precise methods for volume rate selection. Therefore, automated methods for canopy characterisation must be established using a rapid and reliable technology capable of providing precise information about crop structure. This research providedregression models for obtaining canopy characteristics of vineyards from unmanned aerial vehicle (UAV) and satellite images collected in three significant growth stages. Between 2018 and 2019, a total of 1400 vines were characterised manually and remotely using a UAV and a satellite-based technology. The information collected from the sampled vines was analysed by two different procedures. First, a linear relationship between the manual and remote sensing data was investigated considering every single vine as a data point. Second, the vines were clustered based on three vigour levels in the parcel, and regression models were fitted to the average values of the ground-based and remote sensing-estimated canopy parameters. Remote sensing could detect the changes in canopy characteristics associated with vegetation growth. The combination of normalised differential vegetation index (NDVI) and projected area extracted from the UAV images is correlated with the tree row volume (TRV) when raw point data were used. This relationship was improved and extended to canopy height, width, leaf wall area, and TRV when the data were clustered. Similarly, satellite-based NDVI yielded moderate coefficients of determination for canopy width with raw point data, and for canopy width, height, and TRV when the vines were clustered according to the vigour. The proposed approach should facilitate the estimation of canopy characteristics in each area of a field using a cost-effective, simple, and reliable technology, allowing variable rate application in vineyards.},
DOI = {10.3390/s21072363}
}



@Article{rs13071301,
AUTHOR = {Lee, Cheonjae and de Vries, Walter Timo},
TITLE = {Testing and Validating the Suitability of Geospatially Informed Proxies on Land Tenure in North Korea for Korean (Re-)Unification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1301},
URL = {https://www.mdpi.com/2072-4292/13/7/1301},
ISSN = {2072-4292},
ABSTRACT = {The role of remote sensing data in detecting, estimating, and monitoring socioeconomic status (SES) such as quality of life dimensions and sustainable development prospects has received increased attention. Geospatial data has emerged as powerful source of information for enabling both socio-technical assessment and socio-legal analysis in land administration domain. In the context of Korean (re-)unification, there is a notable paucity of evidence how to identify unknowns in North Korea. The main challenge is the lack of complete and adequate information when it comes to clarifying unknown land tenure relations and land governance arrangements. Deriving informative land tenure relations from geospatial data in line with socio-economic land attributes is currently the most innovative approach. In-close and in-depth investigations of validating the suitability of a set of geospatially informed proxies combining multiple values were taken into consideration, as were the forms of knowledge co-production. Thus, the primary aim is to provide empirical evidence of whether proposed proxies are scientifically valid, policy-relevant, and socially robust. We revealed differences in the distributions of agreements relating to land ownership and land transfer rights identification among scientists, bureaucrats, and stakeholders. Moreover, we were able to measure intrinsic, contextual, representational, and accessibility attributes of information quality regarding the associations between earth observation (EO) data and land tenure relations in North Korea from a number of different viewpoints. This paper offers valuable insights into new techniques for validating suitability of EO data proxies in the land administration domain off the reliance on conventional practices formed and customized to the specific artefacts and guidelines of the remote sensing community.},
DOI = {10.3390/rs13071301}
}



@Article{s21072385,
AUTHOR = {Jasinski, Tomasz and Brooker, Graham and Antipov, Irina},
TITLE = {W-Band Multi-Aspect High Resolution Range Profile Radar Target Classification Using Support Vector Machines},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2385},
URL = {https://www.mdpi.com/1424-8220/21/7/2385},
PubMedID = {33808183},
ISSN = {1424-8220},
ABSTRACT = {Millimeter-wave (W-band) radar measurements were taken for two maritime targets instrumented with attitude and heading reference systems (AHRSs) in a littoral environment with the aim of developing a multiaspect classifier. The focus was on resource-limited implementations such as short-range, tactical, unmanned aircraft systems (UASs) and dealing with limited and imbalanced datasets. Radar imaging and preprocessing consisted of recording high-resolution range profiles (HRRPs) and performing range alignment using peak detection and fast Fourier transforms (FFTs). HRRPs were used because of their simplicity, reliability, and speed. The features used were fixed-length, frequency domain range profiles. Two linear support vector machine (SVM)-based classifiers were developed which both yielded excellent results in their general forms and were simple to implement. The first approach utilized the positive predictive value (PPV) and negative predictive value (NPV) statistics of the SVM directly to generate target probabilities and consequently determine the optimal aspect transitions for classification. The second approach used the Kolmogorov–Smirnov test for dimensionality reduction, followed by concatenating feature vectors across several aspects. The latter approach is particularly well-suited to resource-constrained scenarios, potentially allowing for retraining and updating in the field.},
DOI = {10.3390/s21072385}
}



@Article{rs13071311,
AUTHOR = {Xu, Danqing and Wu, Yiquan},
TITLE = {FE-YOLO: A Feature Enhancement Network for Remote Sensing Target Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1311},
URL = {https://www.mdpi.com/2072-4292/13/7/1311},
ISSN = {2072-4292},
ABSTRACT = {In the past few decades, target detection from remote sensing images gained from aircraft or satellites has become one of the hottest topics. However, the existing algorithms are still limited by the detection of small remote sensing targets. Benefiting from the great development of computing power, deep learning has also made great breakthroughs. Due to a large number of small targets and complexity of background, the task of remote sensing target detection is still a challenge. In this work, we establish a series of feature enhancement modules for the network based on YOLO (You Only Look Once) -V3 to improve the performance of feature extraction. Therefore, we term our proposed network as FE-YOLO. In addition, to realize fast detection, the original Darknet-53 was simplified. Experimental results on remote sensing datasets show that our proposed FE-YOLO performs better than other state-of-the-art target detection models.},
DOI = {10.3390/rs13071311}
}



@Article{rs13071318,
AUTHOR = {Hou, Jie-Bo and Zhu, Xiaobin and Yin, Xu-Cheng},
TITLE = {Self-Adaptive Aspect Ratio Anchor for Oriented Object Detection in Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1318},
URL = {https://www.mdpi.com/2072-4292/13/7/1318},
ISSN = {2072-4292},
ABSTRACT = {Object detection is a significant and challenging problem in the study of remote sensing. Since remote sensing images are typically captured with a bird’s-eye view, the aspect ratios of objects in the same category may obey a Gaussian distribution. Generally, existing object detection methods ignore exploring the distribution character of aspect ratios for improving performance in remote sensing tasks. In this paper, we propose a novel Self-Adaptive Aspect Ratio Anchor (SARA) to explicitly explore aspect ratio variations of objects in remote sensing images. To be concrete, our SARA can self-adaptively learn an appropriate aspect ratio for each category. In this way, we can only utilize a simple squared anchor (related to the strides of feature maps in Feature Pyramid Networks) to regress objects in various aspect ratios. Finally, we adopt an Oriented Box Decoder (OBD) to align the feature maps and encode the orientation information of oriented objects. Our method achieves a promising mAP value of 79.91% on the DOTA dataset.},
DOI = {10.3390/rs13071318}
}



@Article{electronics10070820,
AUTHOR = {Ammar, Adel and Koubaa, Anis and Ahmed, Mohanned and Saad, Abdulrahman and Benjdira, Bilel},
TITLE = {Vehicle Detection from Aerial Images Using Deep Learning: A Comparative Study},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {820},
URL = {https://www.mdpi.com/2079-9292/10/7/820},
ISSN = {2079-9292},
ABSTRACT = {This paper addresses the problem of car detection from aerial images using Convolutional Neural Networks (CNNs). This problem presents additional challenges as compared to car (or any object) detection from ground images because the features of vehicles from aerial images are more difficult to discern. To investigate this issue, we assess the performance of three state-of-the-art CNN algorithms, namely Faster R-CNN, which is the most popular region-based algorithm, as well as YOLOv3 and YOLOv4, which are known to be the fastest detection algorithms. We analyze two datasets with different characteristics to check the impact of various factors, such as the UAV’s (unmanned aerial vehicle) altitude, camera resolution, and object size. A total of 52 training experiments were conducted to account for the effect of different hyperparameter values. The objective of this work is to conduct the most robust and exhaustive comparison between these three cutting-edge algorithms on the specific domain of aerial images. By using a variety of metrics, we show that the difference between YOLOv4 and YOLOv3 on the two datasets is statistically insignificant in terms of Average Precision (AP) (contrary to what was obtained on the COCO dataset). However, both of them yield markedly better performance than Faster R-CNN in most configurations. The only exception is that both of them exhibit a lower recall when object sizes and scales in the testing dataset differ largely from those in the training dataset.},
DOI = {10.3390/electronics10070820}
}



@Article{rs13071321,
AUTHOR = {Gong, Yiping and Zhang, Fan and Jia, Xiangyang and Huang, Xianfeng and Li, Deren and Mao, Zhu},
TITLE = {Deep Neural Networks for Quantitative Damage Evaluation of Building Losses Using Aerial Oblique Images: Case Study on the Great Wall (China)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1321},
URL = {https://www.mdpi.com/2072-4292/13/7/1321},
ISSN = {2072-4292},
ABSTRACT = {Automated damage evaluation is of great importance in the maintenance and preservation of heritage structures. Damage investigation of large cultural buildings is time-consuming and labor-intensive, meaning that many buildings are not repaired in a timely manner. Additionally, some buildings in harsh environments are impossible to reach, increasing the difficulty of damage investigation. Oblique images facilitate damage detection in large buildings, yet quantitative damage information, such as area or volume, is difficult to generate. In this paper, we propose a method for quantitative damage evaluation of large heritage buildings in wild areas with repetitive structures based on drone images. Unlike existing methods that focus on building surfaces, we study the damage of building components and extract hidden linear symmetry information, which is useful for localizing missing parts in architectural restoration. First, we reconstruct a 3D mesh model based on the photogrammetric method using high-resolution oblique images captured by drone. Second, we extract 3D objects by applying advanced deep learning methods to the images and projecting the 2D object segmentation results to 3D mesh models. For accurate 2D object extraction, we propose an edge-enhanced method to improve the segmentation accuracy of object edges. 3D object fragments from multiple views are integrated to build complete individual objects according to the geometric features. Third, the damage condition of objects is estimated in 3D space by calculating the volume reduction. To obtain the damage condition of an entire building, we define the damage degree in three levels: no or slight damage, moderate damage and severe damage, and then collect statistics on the number of damaged objects at each level. Finally, through an analysis of the building structure, we extract the linear symmetry surface from the remaining damaged objects and use the symmetry surface to localize the positions of missing objects. This procedure was tested and validated in a case study (the Jiankou Great Wall in China). The experimental results show that in terms of segmentation accuracy, our method obtains results of 93.23% mAP and 84.21% mIoU on oblique images and 72.45% mIoU on the 3D mesh model. Moreover, the proposed method shows effectiveness in performing damage assessment of objects and missing part localization.},
DOI = {10.3390/rs13071321}
}



@Article{rs13071327,
AUTHOR = {Tian, Ling and Cao, Yu and He, Bokun and Zhang, Yifan and He, Chu and Li, Deshi},
TITLE = {Image Enhancement Driven by Object Characteristics and Dense Feature Reuse Network for Ship Target Detection in Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1327},
URL = {https://www.mdpi.com/2072-4292/13/7/1327},
ISSN = {2072-4292},
ABSTRACT = {As the application scenarios of remote sensing imagery (RSI) become richer, the task of ship detection from an overhead perspective is of great significance. Compared with traditional methods, the use of deep learning ideas has more prospects. However, the Convolutional Neural Network (CNN) has poor resistance to sample differences in detection tasks, and the huge differences in the image environment, background, and quality of RSIs affect the performance for target detection tasks; on the other hand, upsampling or pooling operations result in the loss of detailed information in the features, and the CNN with outstanding results are often accompanied by a high computation and a large amount of memory storage. Considering the characteristics of ship targets in RSIs, this study proposes a detection framework combining an image enhancement module with a dense feature reuse module: (1) drawing on the ideas of the generative adversarial network (GAN), we designed an image enhancement module driven by object characteristics, which improves the quality of the ship target in the images while augmenting the training set; (2) the intensive feature extraction module was designed to integrate low-level location information and high-level semantic information of different resolutions while minimizing the computation, which can improve the efficiency of feature reuse in the network; (3) we introduced the receptive field expansion module to obtain a wider range of deep semantic information and enhance the ability to extract features of targets were at different sizes. Experiments were carried out on two types of ship datasets, optical RSI and Synthetic Aperture Radar (SAR) images. The proposed framework was implemented on classic detection networks such as You Only Look Once (YOLO) and Mask-RCNN. The experimental results verify the effectiveness of the proposed method.},
DOI = {10.3390/rs13071327}
}



@Article{electronics10070831,
AUTHOR = {Al-Darraji, Izzat and Piromalis, Dimitrios and Kakei, Ayad A. and Khan, Fazal Qudus and Stojmenovic, Milos and Tsaramirsis, Georgios and Papageorgas, Panagiotis G.},
TITLE = {Adaptive Robust Controller Design-Based RBF Neural Network for Aerial Robot Arm Model},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {831},
URL = {https://www.mdpi.com/2079-9292/10/7/831},
ISSN = {2079-9292},
ABSTRACT = {Aerial Robot Arms (ARAs) enable aerial drones to interact and influence objects in various environments. Traditional ARA controllers need the availability of a high-precision model to avoid high control chattering. Furthermore, in practical applications of aerial object manipulation, the payloads that ARAs can handle vary, depending on the nature of the task. The high uncertainties due to modeling errors and an unknown payload are inversely proportional to the stability of ARAs. To address the issue of stability, a new adaptive robust controller, based on the Radial Basis Function (RBF) neural network, is proposed. A three-tier approach is also followed. Firstly, a detailed new model for the ARA is derived using the Lagrange–d’Alembert principle. Secondly, an adaptive robust controller, based on a sliding mode, is designed to manipulate the problem of uncertainties, including modeling errors. Last, a higher stability controller, based on the RBF neural network, is implemented with the adaptive robust controller to stabilize the ARAs, avoiding modeling errors and unknown payload issues. The novelty of the proposed design is that it takes into account high nonlinearities, coupling control loops, high modeling errors, and disturbances due to payloads and environmental conditions. The model was evaluated by the simulation of a case study that includes the two proposed controllers and ARA trajectory tracking. The simulation results show the validation and notability of the presented control algorithm.},
DOI = {10.3390/electronics10070831}
}



@Article{s21072407,
AUTHOR = {You, Hojun and Kim, Dongsu},
TITLE = {Development of an Image Registration Technique for Fluvial Hyperspectral Imagery Using an Optical Flow Algorithm},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2407},
URL = {https://www.mdpi.com/1424-8220/21/7/2407},
PubMedID = {33807293},
ISSN = {1424-8220},
ABSTRACT = {Fluvial remote sensing has been used to monitor diverse riverine properties through processes such as river bathymetry and visual detection of suspended sediment, algal blooms, and bed materials more efficiently than laborious and expensive in-situ measurements. Red–green–blue (RGB) optical sensors have been widely used in traditional fluvial remote sensing. However, owing to their three confined bands, they rely on visual inspection for qualitative assessments and are limited to performing quantitative and accurate monitoring. Recent advances in hyperspectral imaging in the fluvial domain have enabled hyperspectral images to be geared with more than 150 spectral bands. Thus, various riverine properties can be quantitatively characterized using sensors in low-altitude unmanned aerial vehicles (UAVs) with a high spatial resolution. Many efforts are ongoing to take full advantage of hyperspectral band information in fluvial research. Although geo-referenced hyperspectral images can be acquired for satellites and manned airplanes, few attempts have been made using UAVs. This is mainly because the synthesis of line-scanned images on top of image registration using UAVs is more difficult owing to the highly sensitive and heavy image driven by dense spatial resolution. Therefore, in this study, we propose a practical technique for achieving high spatial accuracy in UAV-based fluvial hyperspectral imaging through efficient image registration using an optical flow algorithm. Template matching algorithms are the most common image registration technique in RGB-based remote sensing; however, they require many calculations and can be error-prone depending on the user, as decisions regarding various parameters are required. Furthermore, the spatial accuracy of this technique needs to be verified, as it has not been widely applied to hyperspectral imagery. The proposed technique resulted in an average reduction of spatial errors by 91.9%, compared to the case where the image registration technique was not applied, and by 78.7% compared to template matching.},
DOI = {10.3390/s21072407}
}



@Article{electronics10070833,
AUTHOR = {Kang, Mi-Seon and Kim, Pyong-Kun and Lim, Kil-Taek and Cho, You-Ze},
TITLE = {Method for Obtaining Better Traffic Survey Data},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {833},
URL = {https://www.mdpi.com/2079-9292/10/7/833},
ISSN = {2079-9292},
ABSTRACT = {Road traffic surveys determine the number and type of vehicles passing by a specific point over a certain period of time. The manual estimation of the number and type of vehicles from images captured by a camera is the most commonly used method. However, this method has the disadvantage of requiring high amounts of manpower and cost. Recently, methods of automating traffic volume surveys using sensors or deep learning have been widely attempted, but there is the disadvantage that a person must finally manually verify the data in order to ensure that they are reliable. In order to address these shortcomings, we propose a method for efficiently conducting road traffic volume surveys and obtaining highly reliable data. The proposed method detects vehicles on the road from CCTV (Closed-circuit television) images and classifies vehicle types using deep learning or a similar method. After that, it automatically informs the user of candidates with a high probability of error and provides a method for efficient verification. The performance of the proposed method was tested using a data set collected by an actual road traffic survey company. As a result, we proved that our method shows better accuracy than the previous method. The proposed method can reduce the labor and cost in road traffic volume surveys, and increase the reliability of the data due to more accurate results.},
DOI = {10.3390/electronics10070833}
}



@Article{aerospace8040093,
AUTHOR = {Ribeiro, Marta and Ellerbroek, Joost and Hoekstra, Jacco},
TITLE = {Velocity Obstacle Based Conflict Avoidance in Urban Environment with Variable Speed Limit},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {93},
URL = {https://www.mdpi.com/2226-4310/8/4/93},
ISSN = {2226-4310},
ABSTRACT = {Current investigations into urban aerial mobility, as well as the continuing growth of global air transportation, have renewed interest in conflict detection and resolution (CD&amp;R) methods. The use of drones for applications such as package delivery, would result in traffic densities that are orders of magnitude higher than those currently observed in manned aviation. Such densities do not only make automated conflict detection and resolution a necessity, but will also force a re-evaluation of aspects such as coordination vs. priority, or state vs. intent. This paper looks into enabling a safe introduction of drones into urban airspace by setting travelling rules in the operating airspace which benefit tactical conflict resolution. First, conflicts resulting from changes of direction are added to conflict resolution with intent trajectory propagation. Second, the likelihood of aircraft with opposing headings meeting in conflict is reduced by separating traffic into different layers per heading–altitude rules. Guidelines are set in place to make sure aircraft respect the heading ranges allowed at every crossed layer. Finally, we use a reinforcement learning agent to implement variable speed limits towards creating a more homogeneous traffic situation between cruising and climbing/descending aircraft. The effects of all of these variables were tested through fast-time simulations on an open source airspace simulation platform. Results showed that we were able to improve the operational safety of several scenarios.},
DOI = {10.3390/aerospace8040093}
}



@Article{s21072416,
AUTHOR = {Wang, Fei and Liu, Zhendong and Zhu, Hongchun and Wu, Pengda and Li, Chengming},
TITLE = {An Improved Method for Stable Feature Points Selection in Structure-from-Motion Considering Image Semantic and Structural Characteristics},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2416},
URL = {https://www.mdpi.com/1424-8220/21/7/2416},
PubMedID = {33915845},
ISSN = {1424-8220},
ABSTRACT = {Feature matching plays a crucial role in the process of 3D reconstruction based on the structure from motion (SfM) technique. For a large collection of oblique images, feature matching is one of the most time-consuming steps, and the matching result directly affects the accuracy of subsequent tasks. Therefore, how to extract the reasonable feature points robustly and efficiently to improve the matching speed and quality has received extensive attention from scholars worldwide. Most studies perform quantitative feature point selection based on image Difference-of-Gaussian (DoG) pyramids in practice. However, the stability and spatial distribution of feature points are not considered enough, resulting in selected feature points that may not adequately reflect the scene structures and cannot guarantee the matching rate and the aerial triangulation accuracy. To address these issues, an improved method for stable feature point selection in SfM considering image semantic and structural characteristics is proposed. First, the visible-band difference vegetation index is used to identify the vegetation areas from oblique images, and the line feature in the image is extracted by the optimized line segment detector algorithm. Second, the feature point two-tuple classification model is established, in which the vegetation area recognition result is used as the semantic constraint, the line feature extraction result is used as the structural constraint, and the feature points are divided into three types. Finally, a progressive selection algorithm for feature points is proposed, in which feature points in the DoG pyramid are selected by classes and levels until the number of feature points is satisfied. Oblique images of a 40-km2 area in Dongying city, China, were used for validation. The experimental results show that compared to the state-of-the-art method, the method proposed in this paper not only effectively reduces the number of feature points but also better reflects the scene structure. At the same time, the average reprojection error of the aerial triangulation decrease by 20%, the feature point matching rate increase by 3%, the selected feature points are more stable and reasonable.},
DOI = {10.3390/s21072416}
}



@Article{agronomy11040667,
AUTHOR = {Araújo, Sara Oleiro and Peres, Ricardo Silva and Barata, José and Lidon, Fernando and Ramalho, José Cochicho},
TITLE = {Characterising the Agriculture 4.0 Landscape—Emerging Trends, Challenges and Opportunities},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {667},
URL = {https://www.mdpi.com/2073-4395/11/4/667},
ISSN = {2073-4395},
ABSTRACT = {Investment in technological research is imperative to stimulate the development of sustainable solutions for the agricultural sector. Advances in Internet of Things, sensors and sensor networks, robotics, artificial intelligence, big data, cloud computing, etc. foster the transition towards the Agriculture 4.0 era. This fourth revolution is currently seen as a possible solution for improving agricultural growth, ensuring the future needs of the global population in a fair, resilient and sustainable way. In this context, this article aims at characterising the current Agriculture 4.0 landscape. Emerging trends were compiled using a semi-automated process by analysing relevant scientific publications published in the past ten years. Subsequently, a literature review focusing these trends was conducted, with a particular emphasis on their applications in real environments. From the results of the study, some challenges are discussed, as well as opportunities for future research. Finally, a high-level cloud-based IoT architecture is presented, serving as foundation for designing future smart agricultural systems. It is expected that this work will positively impact the research around Agriculture 4.0 systems, providing a clear characterisation of the concept along with guidelines to assist the actors in a successful transition towards the digitalisation of the sector.},
DOI = {10.3390/agronomy11040667}
}



@Article{rs13071341,
AUTHOR = {Appeltans, Simon and Pieters, Jan G. and Mouazen, Abdul M.},
TITLE = {Detection of Leek Rust Disease under Field Conditions Using Hyperspectral Proximal Sensing and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1341},
URL = {https://www.mdpi.com/2072-4292/13/7/1341},
ISSN = {2072-4292},
ABSTRACT = {Rust disease is an important problem for leek cultivation worldwide. It reduces market value and in extreme cases destroys the entire harvest. Farmers have to resort to periodical full-field fungicide applications to prevent the spread of disease, once every 1 to 5 weeks, depending on the cultivar and weather conditions. This implies an economic cost for the farmer and an environmental cost for society. Hyperspectral sensors have been extensively used to address this issue in research, but their application in the field has been limited to a relatively low number of crops, excluding leek, due to the high investment costs and complex data gathering and analysis associated with these sensors. To fill this gap, a methodology was developed for detecting leek rust disease using hyperspectral proximal sensing data combined with supervised machine learning. First, a hyperspectral library was constructed containing 43,416 spectra with a waveband range of 400–1000 nm, measured under field conditions. Then, an extensive evaluation of 11 common classifiers was performed using the scikit-learn machine learning library in Python, combined with a variety of wavelength selection techniques and preprocessing strategies. The best performing model was a (linear) logistic regression model that was able to correctly classify rust disease with an accuracy of 98.14%, using reflectance values at 556 and 661 nm, combined with the value of the first derivative at 511 nm. This model was used to classify unlabelled hyperspectral images, confirming that the model was able to accurately classify leek rust disease symptoms. It can be concluded that the results in this work are an important step towards the mapping of leek rust disease, and that future research is needed to overcome certain challenges before variable rate fungicide applications can be adopted against leek rust disease.},
DOI = {10.3390/rs13071341}
}



@Article{rs13071359,
AUTHOR = {Vélez-Nicolás, Mercedes and García-López, Santiago and Barbero, Luis and Ruiz-Ortiz, Verónica and Sánchez-Bellón, Ángel},
TITLE = {Applications of Unmanned Aerial Systems (UASs) in Hydrology: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1359},
URL = {https://www.mdpi.com/2072-4292/13/7/1359},
ISSN = {2072-4292},
ABSTRACT = {In less than two decades, UASs (unmanned aerial systems) have revolutionized the field of hydrology, bridging the gap between traditional satellite observations and ground-based measurements and allowing the limitations of manned aircraft to be overcome. With unparalleled spatial and temporal resolutions and product-tailoring possibilities, UAS are contributing to the acquisition of large volumes of data on water bodies, submerged parameters and their interactions in different hydrological contexts and in inaccessible or hazardous locations. This paper provides a comprehensive review of 122 works on the applications of UASs in surface water and groundwater research with a purpose-oriented approach. Concretely, the review addresses: (i) the current applications of UAS in surface and groundwater studies, (ii) the type of platforms and sensors mainly used in these tasks, (iii) types of products generated from UAS-borne data, (iv) the associated advantages and limitations, and (v) knowledge gaps and future prospects of UASs application in hydrology. The first aim of this review is to serve as a reference or introductory document for all researchers and water managers who are interested in embracing this novel technology. The second aim is to unify in a single document all the possibilities, potential approaches and results obtained by different authors through the implementation of UASs.},
DOI = {10.3390/rs13071359}
}



@Article{s21072445,
AUTHOR = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
TITLE = {Active Mapping and Robot Exploration: A Survey},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2445},
URL = {https://www.mdpi.com/1424-8220/21/7/2445},
PubMedID = {33918107},
ISSN = {1424-8220},
ABSTRACT = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data obtained from one or more sensors. In most situations, the robot is driven by a human operator, but some systems are capable of navigating autonomously while mapping, which is called native simultaneous localization and mapping. This strategy focuses on actively calculating the trajectories to explore the environment while building a map with a minimum error. In this paper, a comprehensive review of the research work developed in this field is provided, targeting the most relevant contributions in indoor mobile robotics.},
DOI = {10.3390/s21072445}
}



@Article{s21072447,
AUTHOR = {Park, Jonghyuk and Park, Jonghun and Shin, Dongmin and Choi, Yerim},
TITLE = {A BCI Based Alerting System for Attention Recovery of UAV Operators},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2447},
URL = {https://www.mdpi.com/1424-8220/21/7/2447},
PubMedID = {33918116},
ISSN = {1424-8220},
ABSTRACT = {As unmanned aerial vehicles have become popular, the number of accidents caused by an operator’s inattention have increased. To prevent such accidents, the operator should maintain an attention status. However, limited research has been conducted on the brain-computer interface (BCI)-based system with an alerting module for the operator’s attention recovery of unmanned aerial vehicles. Therefore, we introduce a detection and alerting system that prevents an unmanned aerial vehicle operator from falling into inattention status by using the operator’s electroencephalogram signal. The proposed system consists of the following three components: a signal processing module, which collects and preprocesses an electroencephalogram signal of an operator, an inattention detection module, which determines whether an inattention status occurred based on the preprocessed signal, and, lastly, an alert providing module that presents stimulus to an operator when inattention is detected. As a result of evaluating the performance with a real-world dataset, it was shown that the proposed system successfully contributed to the recovery of operator attention in the evaluating dataset, although statistical significance could not be established due to the small number of subjects.},
DOI = {10.3390/s21072447}
}



@Article{aerospace8040099,
AUTHOR = {Huang, Yixin and Xiang, Xiaojia and Zhou, Han and Tang, Dengqing and Sun, Yihao},
TITLE = {Online Identification-Verification-Prediction Method for Parallel System Control of UAVs},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {99},
URL = {https://www.mdpi.com/2226-4310/8/4/99},
ISSN = {2226-4310},
ABSTRACT = {In order to solve the problem of how to efficiently control a large-scale swarm Unmanned Aerial Vehicle (UAV) system, which performs complex tasks with limited manpower in a non-ideal environment, this paper proposes a parallel UAV swarm control method. The key technology of parallel control is to establish a one-to-one artificial UAV system corresponding to the aerial swarm UAV on the ground. This paper focuses on the computational experiments algorithm for artificial UAV system establishment, including data processing, model identification, model verification and state prediction. Furthermore, this paper performs a comprehensive flight mission with four common modes (climbing, level flighting, turning and descending) for verification. The results of the identification experiment present a good consistency between the outputs of the refined dynamics model and the real flight data. The prediction experiment results show that the prediction method in this paper can basically guarantee that the prediction states error is kept within 10% about 16 s.},
DOI = {10.3390/aerospace8040099}
}



@Article{rs13071371,
AUTHOR = {Wang, Junshu and Yang, Yue and Chen, Yuan and Han, Yuxing},
TITLE = {LighterGAN: An Illumination Enhancement Method for Urban UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1371},
URL = {https://www.mdpi.com/2072-4292/13/7/1371},
ISSN = {2072-4292},
ABSTRACT = {In unmanned aerial vehicle based urban observation and monitoring, the performance of computer vision algorithms is inevitably limited by the low illumination and light pollution caused degradation, therefore, the application image enhancement is a considerable prerequisite for the performance of subsequent image processing algorithms. Therefore, we proposed a deep learning and generative adversarial network based model for UAV low illumination image enhancement, named LighterGAN. The design of LighterGAN refers to the CycleGAN model with two improvements—attention mechanism and semantic consistency loss—having been proposed to the original structure. Additionally, an unpaired dataset that was captured by urban UAV aerial photography has been used to train this unsupervised learning model. Furthermore, in order to explore the advantages of the improvements, both the performance in the illumination enhancement task and the generalization ability improvement of LighterGAN were proven in the comparative experiments combining subjective and objective evaluations. In the experiments with five cutting edge image enhancement algorithms, in the test set, LighterGAN achieved the best results in both visual perception and PIQE (perception based image quality evaluator, a MATLAB build-in function, the lower the score, the higher the image quality) score of enhanced images, scores were 4.91 and 11.75 respectively, better than EnlightenGAN the state-of-the-art. In the enhancement of low illumination sub-dataset Y (containing 2000 images), LighterGAN also achieved the lowest PIQE score of 12.37, 2.85 points lower than second place. Moreover, compared with the CycleGAN, the improvement of generalization ability was also demonstrated. In the test set generated images, LighterGAN was 6.66 percent higher than CycleGAN in subjective authenticity assessment and 3.84 lower in PIQE score, meanwhile, in the whole dataset generated images, the PIQE score of LighterGAN is 11.67, 4.86 lower than CycleGAN.},
DOI = {10.3390/rs13071371}
}



@Article{buildings11040150,
AUTHOR = {Han, Dongyeob and Lee, Suk Bae and Song, Mihwa and Cho, Jun Sang},
TITLE = {Change Detection in Unmanned Aerial Vehicle Images for Progress Monitoring of Road Construction},
JOURNAL = {Buildings},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {150},
URL = {https://www.mdpi.com/2075-5309/11/4/150},
ISSN = {2075-5309},
ABSTRACT = {Currently, unmanned aerial vehicles are increasingly being used in various construction projects such as housing developments, road construction, and bridge maintenance. If a drone is used at a road construction site, elevation information and orthoimages can be generated to acquire the construction status quantitatively. However, the detection of detailed changes in the site owing to construction depends on visual video interpretation. This study develops a method for automatic detection of the construction area using multitemporal images and a deep learning method. First, a deep learning model was trained using images of the changing area as reference. Second, we obtained an effective application method by applying various parameters to the deep learning process. The application of the time-series images of a construction site to the selected deep learning model enabled more effective identification of the changed areas than the existing pixel-based change detection. The proposed method is expected to be very helpful in construction management by aiding in the development of smart construction technology.},
DOI = {10.3390/buildings11040150}
}



@Article{s21072505,
AUTHOR = {Wu, Rouwan and Xu, Zhiyong and Zhang, Jianlin and Zhang, Lihong},
TITLE = {Robust Global Motion Estimation for Video Stabilization Based on Improved K-Means Clustering and Superpixel},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2505},
URL = {https://www.mdpi.com/1424-8220/21/7/2505},
PubMedID = {33916773},
ISSN = {1424-8220},
ABSTRACT = {Obtaining accurate global motion is a crucial step for video stabilization. This paper proposes a robust and simple method to implement global motion estimation. We don’t extend the framework of 2D video stabilization but add a “plug and play” module to motion estimation based on feature points. Firstly, simple linear iterative clustering (SLIC) pre-segmentation is used to obtain superpixels of the video frame, clustering is performed according to the superpixel centroid motion vector and cluster center with large value is eliminated. Secondly, in order to obtain accurate global motion estimation, an improved K-means clustering is proposed. We match the feature points of the remaining superpixels between two adjacent frames, establish a feature points’ motion vector space, and use improved K-means clustering for clustering. Finally, the richest cluster is being retained, and the global motion is obtained by homography transformation. Our proposed method has been verified on different types of videos and has efficient performance than traditional approaches. The stabilization video has an average improvement of 0.24 in the structural similarity index than the original video and 0.1 higher than the traditional method.},
DOI = {10.3390/s21072505}
}



@Article{rs13071391,
AUTHOR = {Fernandez-Beltran, Ruben and Baidar, Tina and Kang, Jian and Pla, Filiberto},
TITLE = {Rice-Yield Prediction with Multi-Temporal Sentinel-2 Data and 3D CNN: A Case Study in Nepal},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1391},
URL = {https://www.mdpi.com/2072-4292/13/7/1391},
ISSN = {2072-4292},
ABSTRACT = {Crop yield estimation is a major issue of crop monitoring which remains particularly challenging in developing countries due to the problem of timely and adequate data availability. Whereas traditional agricultural systems mainly rely on scarce ground-survey data, freely available multi-temporal and multi-spectral remote sensing images are excellent tools to support these vulnerable systems by accurately monitoring and estimating crop yields before harvest. In this context, we introduce the use of Sentinel-2 (S2) imagery, with a medium spatial, spectral and temporal resolutions, to estimate rice crop yields in Nepal as a case study. Firstly, we build a new large-scale rice crop database (RicePAL) composed by multi-temporal S2 and climate/soil data from the Terai districts of Nepal. Secondly, we propose a novel 3D Convolutional Neural Network (CNN) adapted to these intrinsic data constraints for the accurate rice crop yield estimation. Thirdly, we study the effect of considering different temporal, climate and soil data configurations in terms of the performance achieved by the proposed approach and several state-of-the-art regression and CNN-based yield estimation methods. The extensive experiments conducted in this work demonstrate the suitability of the proposed CNN-based framework for rice crop yield estimation in the developing country of Nepal using S2 data.},
DOI = {10.3390/rs13071391}
}



@Article{s21072534,
AUTHOR = {Doukhi, Oualid and Lee, Deok-Jin},
TITLE = {Deep Reinforcement Learning for End-to-End Local Motion Planning of Autonomous Aerial Robots in Unknown Outdoor Environments: Real-Time Flight Experiments},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2534},
URL = {https://www.mdpi.com/1424-8220/21/7/2534},
PubMedID = {33916624},
ISSN = {1424-8220},
ABSTRACT = {Autonomous navigation and collision avoidance missions represent a significant challenge for robotics systems as they generally operate in dynamic environments that require a high level of autonomy and flexible decision-making capabilities. This challenge becomes more applicable in micro aerial vehicles (MAVs) due to their limited size and computational power. This paper presents a novel approach for enabling a micro aerial vehicle system equipped with a laser range finder to autonomously navigate among obstacles and achieve a user-specified goal location in a GPS-denied environment, without the need for mapping or path planning. The proposed system uses an actor–critic-based reinforcement learning technique to train the aerial robot in a Gazebo simulator to perform a point-goal navigation task by directly mapping the noisy MAV’s state and laser scan measurements to continuous motion control. The obtained policy can perform collision-free flight in the real world while being trained entirely on a 3D simulator. Intensive simulations and real-time experiments were conducted and compared with a nonlinear model predictive control technique to show the generalization capabilities to new unseen environments, and robustness against localization noise. The obtained results demonstrate our system’s effectiveness in flying safely and reaching the desired points by planning smooth forward linear velocity and heading rates.},
DOI = {10.3390/s21072534}
}



@Article{cli9040058,
AUTHOR = {Ghaffarian, Saman and Emtehani, Sobhan},
TITLE = {Monitoring Urban Deprived Areas with Remote Sensing and Machine Learning in Case of Disaster Recovery},
JOURNAL = {Climate},
VOLUME = {9},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {58},
URL = {https://www.mdpi.com/2225-1154/9/4/58},
ISSN = {2225-1154},
ABSTRACT = {Rapid urbanization and increasing population in cities with a large portion of them settled in deprived neighborhoods, mostly defined as slum areas, have escalated inequality and vulnerability to natural disasters. As a result, monitoring such areas is essential to provide information and support decision-makers and urban planners, especially in case of disaster recovery. Here, we developed an approach to monitor the urban deprived areas over a four-year period after super Typhoon Haiyan, which struck Tacloban city, in the Philippines, in 2013, using high-resolution satellite images and machine learning methods. A Support Vector Machine classification method supported by a local binary patterns feature extraction model was initially performed to detect slum areas in the pre-disaster, just after/event, and post-disaster images. Afterward, a dense conditional random fields model was employed to produce the final slum areas maps. The developed method detected slum areas with accuracies over 83%. We produced the damage and recovery maps based on change analysis over the detected slum areas. The results revealed that most of the slum areas were reconstructed 4 years after Typhoon Haiyan, and thus, the city returned to the pre-existing vulnerability level.},
DOI = {10.3390/cli9040058}
}



@Article{computers10040046,
AUTHOR = {Felemban, Emad and Sheikh, Adil A. and Naseer, Atif},
TITLE = {Improving Response Time for Crowd Management in Hajj},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {46},
URL = {https://www.mdpi.com/2073-431X/10/4/46},
ISSN = {2073-431X},
ABSTRACT = {Flying Adhoc Network (FANET) is a particular type of Mobile Adhoc Network (MANET) that consists of flying drones or unmanned aerial vehicles (UAVs). MANETs are especially useful in rural and remote areas, where the lack of public networks necessitates data delivery through mobile nodes. Additionally, FANETs provide better coverage where there is a lack of roads. Generally, the goal of FANETs is to provide multimedia data to applications such as search and rescue operations, forest fire detection, surveillance and patrol, environmental monitoring, and traffic and urban monitoring. The above applications’ performance and efficiency depend on the quality and timely delivery of these essential data from an area of interest to control centers. This paper presents a Priority-based Routing Framework for Flying Adhoc Networks (PRoFFAN) for the expedited delivery of essential multimedia data to control centers. PRoFFAN reduces the FANET application’s response time by prioritizing the sending and forwarding of critical image data from the UAV to the control center. Our motivation application is crowd management; we believe that having important image features as early as possible will save lives and enhance the crowd’s safety and flow. We integrated PRoFFAN over the RPL routing layer of Contiki-NG’s IPv6 network stack. We used simulations in Cooja to demonstrate the benefit of PRoFFAN over conventional ZigBee.},
DOI = {10.3390/computers10040046}
}



@Article{electricity2020007,
AUTHOR = {Speranza, Nicholas A. and Rave, Christopher J. and Pei, Yong},
TITLE = {Energy-Efficient On-Platform Target Classification for Electric Air Transportation Systems},
JOURNAL = {Electricity},
VOLUME = {2},
YEAR = {2021},
NUMBER = {2},
PAGES = {110--123},
URL = {https://www.mdpi.com/2673-4826/2/2/7},
ISSN = {2673-4826},
ABSTRACT = {Due to the predicted rise of Unmanned Aircraft Systems (UAS) in commercial, civil, and military operations, there is a desire to make UASs more energy efficient so they can proliferate with ease of deployment and maximal life per charge. To address current limitations, a three-tiered approach is investigated to mitigate Unmanned Aerial Vehicle (UAV) hover time, reduce network datalink transmission to a ground station, and provide a real-time framework for Sense-and-Avoidance (SAA) target classification. An energy-efficient UAS architecture framework is presented, and a corresponding SAA prototype is developed using commercial hardware to validate the proposed architecture using an experimental methodology. The proposed architecture utilizes classical computer vision methods within the Detection Subsystem coupled with deeply learned Convolutional Neural Networks (CNN) within the Classification Subsystem. Real-time operations of three frames per second are realized enabling UAV hover time and associated energy consumption during SAA processing to be effectively eliminated. Additional energy improvements are not addressed in the scope of this work. Inference accuracy is improved by 19% over baseline COTS models and current non-adaptive, single-stage SAA architectures. Overall, by pushing SAA processing to the edge of the sensors, network offload transmissions and reductions in processing time and energy consumption are feasible and realistic in future battery-powered electric air transportation systems.},
DOI = {10.3390/electricity2020007}
}



@Article{electronics10070868,
AUTHOR = {Martínez, Anselmo and Belmonte, Lidia M. and García, Arturo S. and Fernández-Caballero, Antonio and Morales, Rafael},
TITLE = {Facial Emotion Recognition from an Unmanned Flying Social Robot for Home Care of Dependent People},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {868},
URL = {https://www.mdpi.com/2079-9292/10/7/868},
ISSN = {2079-9292},
ABSTRACT = {This work is part of an ongoing research project to develop an unmanned flying social robot to monitor dependants at home in order to detect the person’s state and bring the necessary assistance. In this sense, this paper focuses on the description of a virtual reality (VR) simulation platform for the monitoring process of an avatar in a virtual home by a rotatory-wing autonomous unmanned aerial vehicle (UAV). This platform is based on a distributed architecture composed of three modules communicated through the message queue telemetry transport (MQTT) protocol: the UAV Simulator implemented in MATLAB/Simulink, the VR Visualiser developed in Unity, and the new emotion recognition (ER) system developed in Python. Using a face detection algorithm and a convolutional neural network (CNN), the ER System is able to detect the person’s face in the image captured by the UAV’s on-board camera and classify the emotion among seven possible ones (surprise; fear; happiness; sadness; disgust; anger; or neutral expression). The experimental results demonstrate the correct integration of this new computer vision module within the VR platform, as well as the good performance of the designed CNN, with around 85% in the F1-score, a mean of the precision and recall of the model. The developed emotion detection system can be used in the future implementation of the assistance UAV that monitors dependent people in a real environment, since the methodology used is valid for images of real people.},
DOI = {10.3390/electronics10070868}
}



@Article{rs13081416,
AUTHOR = {Bao, Min and Chala Urgessa, Guyo and Xing, Mengdao and Han, Liang and Chen, Rui},
TITLE = {Toward More Robust and Real-Time Unmanned Aerial Vehicle Detection and Tracking via Cross-Scale Feature Aggregation Based on the Center Keypoint},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1416},
URL = {https://www.mdpi.com/2072-4292/13/8/1416},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play an essential role in various applications, such as transportation and intelligent environmental sensing. However, due to camera motion and complex environments, it can be difficult to recognize the UAV from its surroundings thus, traditional methods often miss detection of UAVs and generate false alarms. To address these issues, we propose a novel method for detecting and tracking UAVs. First, a cross-scale feature aggregation CenterNet (CFACN) is constructed to recognize the UAVs. CFACN is a free anchor-based center point estimation method that can effectively decrease the false alarm rate, the misdetection of small targets, and computational complexity. Secondly, the region of interest-scale-crop-resize (RSCR) method is utilized to merge CFACN and region-of-interest (ROI) CFACN (ROI-CFACN) further, in order to improve the accuracy at a lower computational cost. Finally, the Kalman filter is adopted to track the UAV. The effectiveness of our method is validated using a collected UAV dataset. The experimental results demonstrate that our methods can achieve higher accuracy with lower computational cost, being superior to BiFPN, CenterNet, YoLo, and their variants on the same dataset.},
DOI = {10.3390/rs13081416}
}



@Article{rs13081420,
AUTHOR = {Tang, Mingliang and Esmaeili, Kamran},
TITLE = {Heap Leach Pad Surface Moisture Monitoring Using Drone-Based Aerial Images and Convolutional Neural Networks: A Case Study at the El Gallo Mine, Mexico},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1420},
URL = {https://www.mdpi.com/2072-4292/13/8/1420},
ISSN = {2072-4292},
ABSTRACT = {An efficient metal recovery in heap leach operations relies on uniform distribution of leaching reagent solution over the heap leach pad surface. However, the current practices for heap leach pad (HLP) surface moisture monitoring often rely on manual inspection, which is labor-intensive, time-consuming, discontinuous, and intermittent. In order to complement the manual monitoring process and reduce the frequency of exposing technical manpower to the hazardous leaching reagent (e.g., dilute cyanide solution in gold leaching), this manuscript describes a case study of implementing an HLP surface moisture monitoring method based on drone-based aerial images and convolutional neural networks (CNNs). Field data collection was conducted on a gold HLP at the El Gallo mine, Mexico. A commercially available hexa-copter drone was equipped with one visible-light (RGB) camera and one thermal infrared sensor to acquire RGB and thermal images from the HLP surface. The collected data had high spatial and temporal resolutions. The high-quality aerial images were used to generate surface moisture maps of the HLP based on two CNN approaches. The generated maps provide direct visualization of the different moisture zones across the HLP surface, and such information can be used to detect potential operational issues related to distribution of reagent solution and to facilitate timely decision making in heap leach operations.},
DOI = {10.3390/rs13081420}
}



@Article{rs13081424,
AUTHOR = {Terres de Lima, Lucas and Fernández-Fernández, Sandra and Gonçalves, João Francisco and Magalhães Filho, Luiz and Bernardes, Cristina},
TITLE = {Development of Tools for Coastal Management in Google Earth Engine: Uncertainty Bathtub Model and Bruun Rule},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1424},
URL = {https://www.mdpi.com/2072-4292/13/8/1424},
ISSN = {2072-4292},
ABSTRACT = {Sea-level rise is a problem increasingly affecting coastal areas worldwide. The existence of free and open-source models to estimate the sea-level impact can contribute to improve coastal management. This study aims to develop and validate two different models to predict the sea-level rise impact supported by Google Earth Engine (GEE)—a cloud-based platform for planetary-scale environmental data analysis. The first model is a Bathtub Model based on the uncertainty of projections of the sea-level rise impact module of TerrSet—Geospatial Monitoring and Modeling System software. The validation process performed in the Rio Grande do Sul coastal plain (S Brazil) resulted in correlations from 0.75 to 1.00. The second model uses the Bruun rule formula implemented in GEE and can determine the coastline retreat of a profile by creatting a simple vector line from topo-bathymetric data. The model shows a very high correlation (0.97) with a classical Bruun rule study performed in the Aveiro coast (NW Portugal). Therefore, the achieved results disclose that the GEE platform is suitable to perform these analysis. The models developed have been openly shared, enabling the continuous improvement of the code by the scientific community.},
DOI = {10.3390/rs13081424}
}



@Article{rs13081428,
AUTHOR = {Marang, Ian J. and Filippi, Patrick and Weaver, Tim B. and Evans, Bradley J. and Whelan, Brett M. and Bishop, Thomas F. A. and Murad, Mohammed O. F. and Al-Shammari, Dhahi and Roth, Guy},
TITLE = {Machine Learning Optimised Hyperspectral Remote Sensing Retrieves Cotton Nitrogen Status},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1428},
URL = {https://www.mdpi.com/2072-4292/13/8/1428},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imaging spectrometers mounted on unmanned aerial vehicle (UAV) can capture high spatial and spectral resolution to provide cotton crop nitrogen status for precision agriculture. The aim of this research was to explore machine learning use with hyperspectral datacubes over agricultural fields. Hyperspectral imagery was collected over a mature cotton crop, which had high spatial (~5.2 cm) and spectral (5 nm) resolution over the spectral range 475–925 nm that allowed discrimination of individual crop rows and field features as well as a continuous spectral range for calculating derivative spectra. The nominal reflectance and its derivatives clearly highlighted the different treatment blocks and were strongly related to N concentration in leaf and petiole samples, both in traditional vegetation indices (e.g., Vogelman 1, R2 = 0.8) and novel combinations of spectra (R2 = 0.85). The key hyperspectral bands identified were at the red-edge inflection point (695–715 nm). Satellite multispectral was compared against the UAV hyperspectral remote sensing’s performance by testing the ability of Sentinel MSI to predict N concentration using the bands in VIS-NIR spectral region. The Sentinel 2A Green band (B3; mid-point 559.8 nm) explained the same amount of variation in N as the hyperspectral data and more than the Sentinel Red Edge Point 1 (B5; mid-point 704.9 nm) with the lower 10 m resolution Green band reporting an R2 = 0.85, compared with the R2 = 0.78 of downscaled Sentinel Red Edge Point 1 at 5 m. The remaining Sentinel bands explained much lower variation (maximum was NIR at R2 = 0.48). Investigation of the red edge peak region in the first derivative showed strong promise with RIDAmid (R2 = 0.81) being the best index. The machine learning approach narrowed the range of bands required to investigate plant condition over this trial site, greatly improved processing time and reduced processing complexity. While Sentinel performed well in this comparison and would be useful in a broadacre crop production context, the impact of pixel boundaries relative to a region of interest and coarse spatial and temporal resolution impacts its utility in a research capacity.},
DOI = {10.3390/rs13081428}
}



@Article{e23040435,
AUTHOR = {Zhang, Xixin and Yang, Yuhang and Li, Zhiyong and Ning, Xin and Qin, Yilang and Cai, Weiwei},
TITLE = {An Improved Encoder-Decoder Network Based on Strip Pool Method Applied to Segmentation of Farmland Vacancy Field},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {435},
URL = {https://www.mdpi.com/1099-4300/23/4/435},
PubMedID = {33917753},
ISSN = {1099-4300},
ABSTRACT = {In the research of green vegetation coverage in the field of remote sensing image segmentation, crop planting area is often obtained by semantic segmentation of images taken from high altitude. This method can be used to obtain the rate of cultivated land in a region (such as a country), but it does not reflect the real situation of a particular farmland. Therefore, this paper takes low-altitude images of farmland to build a dataset. After comparing several mainstream semantic segmentation algorithms, a new method that is more suitable for farmland vacancy segmentation is proposed. Additionally, the Strip Pooling module (SPM) and the Mixed Pooling module (MPM), with strip pooling as their core, are designed and fused into the semantic segmentation network structure to better extract the vacancy features. Considering the high cost of manual data annotation, this paper uses an improved ResNet network as the backbone of signal transmission, and meanwhile uses data augmentation to improve the performance and robustness of the model. As a result, the accuracy of the proposed method in the test set is 95.6%, mIoU is 77.6%, and the error rate is 7%. Compared to the existing model, the mIoU value is improved by nearly 4%, reaching the level of practical application.},
DOI = {10.3390/e23040435}
}



@Article{app11083339,
AUTHOR = {Kang, Myung Soo and An, Yun-Kyu},
TITLE = {Deep Learning-Based Automated Background Removal for Structural Exterior Image Stitching},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3339},
URL = {https://www.mdpi.com/2076-3417/11/8/3339},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a deep learning-based automated background removal technique for structural exterior image stitching. In order to establish an exterior damage map of a structure using an unmanned aerial vehicle (UAV), a close-up vision scanning is typically required. However, unwanted background objects are often captured within the scanned digital images. Since the unnecessary background objects often cause serious distortion on the image stitching process, they should be removed. In this paper, the automated background removal technique using deep learning-based depth estimation is proposed. Based on the fact that the region of interest has closer working distance than the background ones from the camera, the background region within the digital images can be automatically removed using a deep learning-based depth estimation network. In addition, an optimal digital image selection based on feature matching-based overlap ratio is proposed. The proposed technique is experimentally validated using UAV-scanned digital images acquired from an in-situ high-rise building structure. The validation test results show that the optimal digital images obtained from the proposed technique produce the precise structural exterior map with computational cost reduction of 85.7%, while raw scanned digital images fail to construct the structural exterior map and cause serious stitching distortion.},
DOI = {10.3390/app11083339}
}



@Article{agriculture11040337,
AUTHOR = {Pane, Catello and Manganiello, Gelsomina and Nicastro, Nicola and Cardi, Teodoro and Carotenuto, Francesco},
TITLE = {Powdery Mildew Caused by Erysiphe cruciferarum on Wild Rocket (Diplotaxis tenuifolia): Hyperspectral Imaging and Machine Learning Modeling for Non-Destructive Disease Detection},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {337},
URL = {https://www.mdpi.com/2077-0472/11/4/337},
ISSN = {2077-0472},
ABSTRACT = {Wild rocket is a widely cultivated salad crop. Typical signs and symptoms of powdery mildew were observed on leaves of Diplotaxis tenuifolia, likely favored by climatic conditions occurring in a greenhouse. Based on morphological features and molecular analysis, the disease agent was identified as the fungal pathogen Erysiphe cruciferarum. To the best of our knowledge, this is the first report of E. cruciferarum on D. tenuifolia. Moreover, the present study provides a non-destructive high performing digital approach to efficiently detect the disease. Hyperspectral image analysis allowed to characterize the spectral response of wild rocket affected by powdery mildew and the adopted machine-learning approach (a trained Random Forest model with the four most contributory wavelengths falling in the range 403–446 nm) proved to be able to accurately discriminate between healthy and diseased wild rocket leaves. Shifts in the irradiance absorption by chlorophyll a of diseased leaves in the spectrum blue range seems to be at the base of the hyperspectral imaging detection of wild rocket powdery mildew.},
DOI = {10.3390/agriculture11040337}
}



@Article{s21082650,
AUTHOR = {Choi, Daegyun and Bell, William and Kim, Donghoon and Kim, Jichul},
TITLE = {UAV-Driven Structural Crack Detection and Location Determination Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2650},
URL = {https://www.mdpi.com/1424-8220/21/8/2650},
PubMedID = {33918951},
ISSN = {1424-8220},
ABSTRACT = {Structural cracks are a vital feature in evaluating the health of aging structures. Inspectors regularly monitor structures’ health using visual information because early detection of cracks on highly trafficked structures is critical for maintaining the public’s safety. In this work, a framework for detecting cracks along with their locations is proposed. Image data provided by an unmanned aerial vehicle (UAV) is stitched using image processing techniques to overcome limitations in the resolution of cameras. This stitched image is analyzed to identify cracks using a deep learning model that makes judgements regarding the presence of cracks in the image. Moreover, cracks’ locations are determined using data from UAV sensors. To validate the system, cracks forming on an actual building are captured by a UAV, and these images are analyzed to detect and locate cracks. The proposed framework is proven as an effective way to detect cracks and to represent the cracks’ locations.},
DOI = {10.3390/s21082650}
}



@Article{rs13081464,
AUTHOR = {Liang, Zhu and Wang, Changming and Duan, Zhijie and Liu, Hailiang and Liu, Xiaoyang and Ullah Jan Khan, Kaleem},
TITLE = {A Hybrid Model Consisting of Supervised and Unsupervised Learning for Landslide Susceptibility Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1464},
URL = {https://www.mdpi.com/2072-4292/13/8/1464},
ISSN = {2072-4292},
ABSTRACT = {Landslides cause huge damage to social economy and human beings every year. Landslide susceptibility mapping (LSM) occupies an important position in land use and risk management. This study is to investigate a hybrid model which makes full use of the advantage of supervised learning model (SLM) and unsupervised learning model (ULM). Firstly, ten continuous variables were used to develop a ULM which consisted of factor analysis (FA) and k-means cluster for a preliminary landslide susceptibility map. Secondly, 351 landslides with “1” label were collected and the same number of non-landslide samples with “0” label were selected from the very low susceptibility area in the preliminary map, constituting a new priori condition for a SLM, and thirteen factors were used for the modeling of gradient boosting decision tree (GBDT) which represented for SLM. Finally, the performance of different models was verified using related indexes. The results showed that the performance of the pretreated GBDT model was improved with sensitivity, specificity, accuracy and the area under the curve (AUC) values of 88.60%, 92.59%, 90.60% and 0.976, respectively. It can be concluded that a pretreated model with strong robustness can be constructed by increasing the purity of samples.},
DOI = {10.3390/rs13081464}
}



@Article{electronics10080905,
AUTHOR = {Rodríguez-García, Miguel Ángel and García-Sánchez, Francisco and Valencia-García, Rafael},
TITLE = {Knowledge-Based System for Crop Pests and Diseases Recognition},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {905},
URL = {https://www.mdpi.com/2079-9292/10/8/905},
ISSN = {2079-9292},
ABSTRACT = {With the rapid increase in the world’s population, there is an ever-growing need for a sustainable food supply. Agriculture is one of the pillars for worldwide food provisioning, with fruits and vegetables being essential for a healthy diet. However, in the last few years the worldwide dispersion of virulent plant pests and diseases has caused significant decreases in the yield and quality of crops, in particular fruit, cereal and vegetables. Climate change and the intensification of global trade flows further accentuate the issue. Integrated Pest Management (IPM) is an approach to pest control that aims at maintaining pest insects at tolerable levels, keeping pest populations below an economic injury level. Under these circumstances, the early identification of pests and diseases becomes crucial. In this work, we present the first step towards a fully fledged, semantically enhanced decision support system for IPM. The ultimate goal is to build a complete agricultural knowledge base by gathering data from multiple, heterogeneous sources and to develop a system to assist farmers in decision making concerning the control of pests and diseases. The pest classifier framework has been evaluated in a simulated environment, obtaining an aggregated accuracy of 98.8%.},
DOI = {10.3390/electronics10080905}
}



@Article{app11083454,
AUTHOR = {Gaspar, Pedro D. and Fernandez, Carlos M. and Soares, Vasco N. G. J. and Caldeira, João M. L. P. and Silva, Hélio},
TITLE = {Development of Technological Capabilities through the Internet of Things (IoT): Survey of Opportunities and Barriers for IoT Implementation in Portugal’s Agro-Industry},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3454},
URL = {https://www.mdpi.com/2076-3417/11/8/3454},
ISSN = {2076-3417},
ABSTRACT = {The agro-industrial sector consumes a significant amount of natural resources for farming and meat production. By 2050, population growth is expected, generating more demand and, consequently, more consumption of scarce resources. This challenging scenario is a concern of the European Commission, revealed in the Green Deal commitment and by the United Nations’ 12th goal of sustainable development. Thus, organizations must increase productivity and be more sustainable as soon as possible. Internet of Things (IoT) is introduced as a solution to facilitate agro-food companies to be more eco-efficient, mainly facing difficulties on farms, such as food loss and waste, best efficiency in management of resources, and production. The deployment of this technology depends on the stage of maturity and potential of implementation. To assess and characterize companies, with respect of IoT implementation, a survey was applied in 21 micro, small and medium agro-food companies, belonging to milk, honey, olive oil, jams, fruticulture, bakery and pastry, meat, coffee, and wine sectors, in the central region of Portugal. As results, this paper reveals the stage of maturity, level of sophistication, potential, opportunities, solutions, and barriers for implementation of IoT. Additionally, suggestions and recommendations to improve practices are discussed.},
DOI = {10.3390/app11083454}
}



@Article{s21082748,
AUTHOR = {Leon-Medina, Jersson X. and Anaya, Maribel and Parés, Núria and Tibaduiza, Diego A. and Pozo, Francesc},
TITLE = {Structural Damage Classification in a Jacket-Type Wind-Turbine Foundation Using Principal Component Analysis and Extreme Gradient Boosting},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2748},
URL = {https://www.mdpi.com/1424-8220/21/8/2748},
PubMedID = {33924654},
ISSN = {1424-8220},
ABSTRACT = {Damage classification is an important topic in the development of structural health monitoring systems. When applied to wind-turbine foundations, it provides information about the state of the structure, helps in maintenance, and prevents catastrophic failures. A data-driven pattern-recognition methodology for structural damage classification was developed in this study. The proposed methodology involves several stages: (1) data acquisition, (2) data arrangement, (3) data normalization through the mean-centered unitary group-scaling method, (4) linear feature extraction, (5) classification using the extreme gradient boosting machine learning classifier, and (6) validation applying a 5-fold cross-validation technique. The linear feature extraction capabilities of principal component analysis are employed; the original data of 58,008 features is reduced to only 21 features. The methodology is validated with an experimental test performed in a small-scale wind-turbine foundation structure that simulates the perturbation effects caused by wind and marine waves by applying an unknown white noise signal excitation to the structure. A vibration-response methodology is selected for collecting accelerometer data from both the healthy structure and the structure subjected to four different damage scenarios. The datasets are satisfactorily classified, with performance measures over 99.9% after using the proposed damage classification methodology.},
DOI = {10.3390/s21082748}
}



@Article{s21082754,
AUTHOR = {Chmielewski, Piotr and Sibilski, Krzysztof},
TITLE = {Ground Speed Optical Estimator for Miniature UAV},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2754},
URL = {https://www.mdpi.com/1424-8220/21/8/2754},
PubMedID = {33924736},
ISSN = {1424-8220},
ABSTRACT = {In a conventional Unmanned aerial vehicles (UAV) navigational system Global Navigation Satellite System (GNSS) sensor is often a main source of data for trajectory generation. Even video tracking based systems need some GNSS data for proper work. The goal of this study is to develop an optics-based system to estimate the ground speed of the UAV in the case of the GNSS failure, jamming, or unavailability. The proposed approach uses a camera mounted on the fuselage belly of the UAV. We can obtain the ground speed of the airplane by using the digital cropping, the stabilization of the real time image, and template matching algorithms. By combining the ground speed vector components with measurements of airspeed and altitude, the wind velocity and drift are computed. The obtained data were used to improve efficiency of the video-tracking based on a navigational system. An algorithm allows this computation to be performed in real time on board of a UAV. The algorithm was tested in Software-in-the-loop and implemented on the UAV hardware. Its effectiveness has been demonstrated through the experimental test results. The presented work could be useful for upgrading the existing MUAV products (with embedded cameras) already delivered to the customers only by updating their software. It is especially significant in the case when any necessary hardware upgrades would be economically unjustified or even impossible to be carried out.},
DOI = {10.3390/s21082754}
}



@Article{rs13081508,
AUTHOR = {Kang, Yeseong and Nam, Jinwoo and Kim, Younggwang and Lee, Seongtae and Seong, Deokgyeong and Jang, Sihyeong and Ryu, Chanseok},
TITLE = {Assessment of Regression Models for Predicting Rice Yield and Protein Content Using Unmanned Aerial Vehicle-Based Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1508},
URL = {https://www.mdpi.com/2072-4292/13/8/1508},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle-based multispectral imagery including five spectral bands (blue, green, red, red-edge, and near-infrared) for a rice field in the ripening stage was used to develop regression models for predicting the rice yield and protein content and to select the most suitable regression analysis method for the year-invariant model: partial least squares regression, ridge regression, and artificial neural network (ANN). The regression models developed with six vegetation indices (green normalization difference vegetation index (GNDVI), normalization difference red-edge index (NDRE), chlorophyll index red edge (CIrededge), difference NIR/Green green difference vegetation index (GDVI), green-red NDVI (GRNDVI), and medium resolution imaging spectrometer terrestrial chlorophyll index (MTCI)), calculated from the spectral bands, were applied to single years (2018, 2019, and 2020) and multiple years (2018 + 2019, 2018 + 2020, 2019 + 2020, and all years). The regression models were cross-validated through mutual prediction against the vegetation indices in nonoverlapping years, and the prediction errors were evaluated via root mean squared error of prediction (RMSEP). The ANN model was reproducible, with low and sustained prediction errors of 24.2 kg/1000 m2 ≤ RMSEP ≤ 59.1 kg/1000 m2 in rice yield and 0.14% ≤ RMSEP ≤ 0.28% in rice-protein content in all single-year and multiple-year analyses. When the importance of each vegetation index of the regression models was evaluated, only the ANN model showed the same ranking in the vegetation index of the first (MTCI in both rice yield and protein content) and second importance (CIrededge in rice yield and GRNDVI in rice-protein content). Overall, this means that the ANN model has the highest potential for developing a year-invariant model with stable RMSEP and consistent variable ranking.},
DOI = {10.3390/rs13081508}
}



@Article{rs13081509,
AUTHOR = {Hu, Xikun and Ban, Yifang and Nascetti, Andrea},
TITLE = {Uni-Temporal Multispectral Imagery for Burned Area Mapping with Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1509},
URL = {https://www.mdpi.com/2072-4292/13/8/1509},
ISSN = {2072-4292},
ABSTRACT = {Accurate burned area information is needed to assess the impacts of wildfires on people, communities, and natural ecosystems. Various burned area detection methods have been developed using satellite remote sensing measurements with wide coverage and frequent revisits. Our study aims to expound on the capability of deep learning (DL) models for automatically mapping burned areas from uni-temporal multispectral imagery. Specifically, several semantic segmentation network architectures, i.e., U-Net, HRNet, Fast-SCNN, and DeepLabv3+, and machine learning (ML) algorithms were applied to Sentinel-2 imagery and Landsat-8 imagery in three wildfire sites in two different local climate zones. The validation results show that the DL algorithms outperform the ML methods in two of the three cases with the compact burned scars, while ML methods seem to be more suitable for mapping dispersed burn in boreal forests. Using Sentinel-2 images, U-Net and HRNet exhibit comparatively identical performance with higher kappa (around 0.9) in one heterogeneous Mediterranean fire site in Greece; Fast-SCNN performs better than others with kappa over 0.79 in one compact boreal forest fire with various burn severity in Sweden. Furthermore, directly transferring the trained models to corresponding Landsat-8 data, HRNet dominates in the three test sites among DL models and can preserve the high accuracy. The results demonstrated that DL models can make full use of contextual information and capture spatial details in multiple scales from fire-sensitive spectral bands to map burned areas. Using only a post-fire image, the DL methods not only provide automatic, accurate, and bias-free large-scale mapping option with cross-sensor applicability, but also have potential to be used for onboard processing in the next Earth observation satellites.},
DOI = {10.3390/rs13081509}
}



@Article{s21082765,
AUTHOR = {Taghvaee, Hamidreza and Jain, Akshay and Timoneda, Xavier and Liaskos, Christos and Abadal, Sergi and Alarcón, Eduard and Cabellos-Aparicio, Albert},
TITLE = {Radiation Pattern Prediction for Metasurfaces: A Neural Network-Based Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2765},
URL = {https://www.mdpi.com/1424-8220/21/8/2765},
PubMedID = {33919861},
ISSN = {1424-8220},
ABSTRACT = {As the current standardization for the 5G networks nears completion, work towards understanding the potential technologies for the 6G wireless networks is already underway. One of these potential technologies for the 6G networks is reconfigurable intelligent surfaces. They offer unprecedented degrees of freedom towards engineering the wireless channel, i.e., the ability to modify the characteristics of the channel whenever and however required. Nevertheless, such properties demand that the response of the associated metasurface is well understood under all possible operational conditions. While an understanding of the radiation pattern characteristics can be obtained through either analytical models or full-wave simulations, they suffer from inaccuracy and extremely high computational complexity, respectively. Hence, in this paper, we propose a neural network-based approach that enables a fast and accurate characterization of the metasurface response. We analyze multiple scenarios and demonstrate the capabilities and utility of the proposed methodology. Concretely, we show that this method can learn and predict the parameters governing the reflected wave radiation pattern with an accuracy of a full-wave simulation (98.8–99.8%) and the time and computational complexity of an analytical model. The aforementioned result and methodology will be of specific importance for the design, fault tolerance, and maintenance of the thousands of reconfigurable intelligent surfaces that will be deployed in the 6G network environment.},
DOI = {10.3390/s21082765}
}



@Article{rs13081523,
AUTHOR = {Shao, Yang and Cooner, Austin J. and Walsh, Stephen J.},
TITLE = {Assessing Deep Convolutional Neural Networks and Assisted Machine Perception for Urban Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1523},
URL = {https://www.mdpi.com/2072-4292/13/8/1523},
ISSN = {2072-4292},
ABSTRACT = {High-spatial-resolution satellite imagery has been widely applied for detailed urban mapping. Recently, deep convolutional neural networks (DCNNs) have shown promise in certain remote sensing applications, but they are still relatively new techniques for general urban mapping. This study examines the use of two DCNNs (U-Net and VGG16) to provide an automatic schema to support high-resolution mapping of buildings, road/open built-up, and vegetation cover. Using WorldView-2 imagery as input, we first applied an established OBIA method to characterize major urban land cover classes. An OBIA-derived urban map was then divided into a training and testing region to evaluate the DCNNs’ performance. For U-Net mapping, we were particularly interested in how sample size or the number of image tiles affect mapping accuracy. U-Net generated cross-validation accuracies ranging from 40.5 to 95.2% for training sample sizes from 32 to 4096 image tiles (each tile was 256 by 256 pixels). A per-pixel accuracy assessment led to 87.8 percent overall accuracy for the testing region, suggesting U-Net’s good generalization capabilities. For the VGG16 mapping, we proposed an object-based framing paradigm that retains spatial information and assists machine perception through Gaussian blurring. Gaussian blurring was used as a pre-processing step to enhance the contrast between objects of interest and background (contextual) information. Combined with the pre-trained VGG16 and transfer learning, this analytical approach generated a 77.3 percent overall accuracy for per-object assessment. The mapping accuracy could be further improved given more robust segmentation algorithms and better quantity/quality of training samples. Our study shows significant promise for DCNN implementation for urban mapping and our approach can transfer to a number of other remote sensing applications.},
DOI = {10.3390/rs13081523}
}



@Article{app11083547,
AUTHOR = {Hou, Xiaoyu and Zhang, Kunlin and Xu, Jihui and Huang, Wei and Yu, Xinmiao and Xu, Huaiyu},
TITLE = {Object Detection in Drone Imagery via Sample Balance Strategies and Local Feature Enhancement},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3547},
URL = {https://www.mdpi.com/2076-3417/11/8/3547},
ISSN = {2076-3417},
ABSTRACT = {With the advent of drones, new potential applications have emerged for the unconstrained analysis of images and videos from aerial view cameras. Despite the tremendous success of the generic object detection methods developed using ground-based photos, a considerable performance drop is observed when these same methods are directly applied to images captured by Unmanned Aerial Vehicles (UAVs). Usually, most of the work goes into improving the performance of the detector in aspects such as design loss, training sample selection, feature enhancement, and so forth. This paper proposes a detection framework based on an anchor-free detector with several modules, including a sample balance strategies module and super-resolved generated feature module, to improve performance. We proposed the sample balance strategies module to optimize the imbalance among training samples, especially the imbalance between positive and negative, and easy and hard samples. Due to the high frequencies and noisy representation of the small objects in images captured by drones, the detection task is extraordinarily challenging. However, when compared with other algorithms of this kind, our method achieves better results. We also propose a super-resolved generated GAN (Generative Adversarial Network) module with center-ness weights to effectively enhance the local feature map. Finally, we demonstrate our method’s effectiveness with the proposed modules by carrying out a state-of-the-art performance on Visdrone2020 benchmarks.},
DOI = {10.3390/app11083547}
}



@Article{rs13081529,
AUTHOR = {Jiang, Yufeng and Zhang, Li and Yan, Min and Qi, Jianguo and Fu, Tianmeng and Fan, Shunxiang and Chen, Bowei},
TITLE = {High-Resolution Mangrove Forests Classification with Machine Learning Using Worldview and UAV Hyperspectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1529},
URL = {https://www.mdpi.com/2072-4292/13/8/1529},
ISSN = {2072-4292},
ABSTRACT = {Mangrove forests, as important ecological and economic resources, have suffered a loss in the area due to natural and human activities. Monitoring the distribution of and obtaining accurate information on mangrove species is necessary for ameliorating the damage and protecting and restoring mangrove forests. In this study, we compared the performance of UAV Rikola hyperspectral images, WorldView-2 (WV-2) satellite-based multispectral images, and a fusion of data from both in the classification of mangrove species. We first used recursive feature elimination‒random forest (RFE-RF) to select the vegetation’s spectral and texture feature variables, and then implemented random forest (RF) and support vector machine (SVM) algorithms as classifiers. The results showed that the accuracy of the combined data was higher than that of UAV and WV-2 data; the vegetation index features of UAV hyperspectral data and texture index of WV-2 data played dominant roles; the overall accuracy of the RF algorithm was 95.89% with a Kappa coefficient of 0.95, which is more accurate and efficient than SVM. The use of combined data and RF methods for the classification of mangrove species could be useful in biomass estimation and breeding cultivation.},
DOI = {10.3390/rs13081529}
}



@Article{rs13081528,
AUTHOR = {Song, Yongze and Wu, Peng},
TITLE = {Earth Observation for Sustainable Infrastructure: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1528},
URL = {https://www.mdpi.com/2072-4292/13/8/1528},
ISSN = {2072-4292},
ABSTRACT = {Infrastructure is a fundamental sector for sustainable development and Earth observation has great potentials for sustainable infrastructure development (SID). However, implementations of the timely, large–scale and multi–source Earth observation are still limited in satisfying the huge global requirements of SID. This study presents a systematical literature review to identify trends of Earth observation for sustainable infrastructure (EOSI), investigate the relationship between EOSI and Sustainable Development Goals (SDGs), and explore challenges and future directions of EOSI. Results reveal the close associations of infrastructure, urban development, ecosystems, climate, Earth observation and GIS in EOSI, and indicate their relationships. In addition, from the perspective of EOSI–SDGs relationship, the huge potentials of EOSI are demonstrated from the 70% of the infrastructure influenced targets that can be directly or indirectly derived from Earth observation data, but have not been included in current SDG indicators. Finally, typical EOSI cases are presented to indicate challenges and future research directions. This review emphasizes the contributions and potentials of Earth observation to SID and EOSI is a powerful pathway to deliver on SDGs.},
DOI = {10.3390/rs13081528}
}



@Article{rs13081535,
AUTHOR = {Jiang, Fugen and Zhao, Feng and Ma, Kaisen and Li, Dongsheng and Sun, Hua},
TITLE = {Mapping the Forest Canopy Height in Northern China by Synergizing ICESat-2 with Sentinel-2 Using a Stacking Algorithm},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1535},
URL = {https://www.mdpi.com/2072-4292/13/8/1535},
ISSN = {2072-4292},
ABSTRACT = {The forest canopy height (FCH) plays a critical role in forest quality evaluation and resource management. The accurate and rapid estimation and mapping of the regional forest canopy height is crucial for understanding vegetation growth processes and the internal structure of the ecosystem. A stacking algorithm consisting of multiple linear regression (MLR), support vector machine (SVM), k-nearest neighbor (kNN), and random forest (RF) was used in this paper and demonstrated optimal performance in predicting the forest canopy height by synergizing Sentinel-2 images acquired from the cloud-based computation platform Google Earth Engine (GEE) with data from ICESat-2 (Ice, Cloud, and Land Elevation Satellite-2). This research was conducted to achieve continuous mapping of the canopy height of plantations in Saihanba Mechanical Forest Plantation, which is located in Chengde City, northern Hebei province, China. The results show that stacking achieved the best prediction accuracy for the forest canopy height, with an R2 of 0.77 and a root mean square error (RMSE) of 1.96 m. Compared with MLR, SVM, kNN, and RF, the RMSE obtained by stacking was reduced by 25.2%, 24.9%, 22.8%, and 18.7%, respectively. Since Sentinel-2 images and ICESat-2 data are publicly available, this opens the door for the accurate mapping of the continuous distribution of the forest canopy height globally in the future.},
DOI = {10.3390/rs13081535}
}



@Article{s21082803,
AUTHOR = {Jaffari, Rabeea and Hashmani, Manzoor Ahmed and Reyes-Aldasoro, Constantino Carlos},
TITLE = {A Novel Focal Phi Loss for Power Line Segmentation with Auxiliary Classifier U-Net},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2803},
URL = {https://www.mdpi.com/1424-8220/21/8/2803},
PubMedID = {33923472},
ISSN = {1424-8220},
ABSTRACT = {The segmentation of power lines (PLs) from aerial images is a crucial task for the safe navigation of unmanned aerial vehicles (UAVs) operating at low altitudes. Despite the advances in deep learning-based approaches for PL segmentation, these models are still vulnerable to the class imbalance present in the data. The PLs occupy only a minimal portion (1–5%) of the aerial images as compared to the background region (95–99%). Generally, this class imbalance problem is addressed via the use of PL-specific detectors in conjunction with the popular class balanced cross entropy (BBCE) loss function. However, these PL-specific detectors do not work outside their application areas and a BBCE loss requires hyperparameter tuning for class-wise weights, which is not trivial. Moreover, the BBCE loss results in low dice scores and precision values and thus, fails to achieve an optimal trade-off between dice scores, model accuracy, and precision–recall values. In this work, we propose a generalized focal loss function based on the Matthews correlation coefficient (MCC) or the Phi coefficient to address the class imbalance problem in PL segmentation while utilizing a generic deep segmentation architecture. We evaluate our loss function by improving the vanilla U-Net model with an additional convolutional auxiliary classifier head (ACU-Net) for better learning and faster model convergence. The evaluation of two PL datasets, namely the Mendeley Power Line Dataset and the Power Line Dataset of Urban Scenes (PLDU), where PLs occupy around 1% and 2% of the aerial images area, respectively, reveal that our proposed loss function outperforms the popular BBCE loss by 16% in PL dice scores on both the datasets, 19% in precision and false detection rate (FDR) values for the Mendeley PL dataset and 15% in precision and FDR values for the PLDU with a minor degradation in the accuracy and recall values. Moreover, our proposed ACU-Net outperforms the baseline vanilla U-Net for the characteristic evaluation parameters in the range of 1–10% for both the PL datasets. Thus, our proposed loss function with ACU-Net achieves an optimal trade-off for the characteristic evaluation parameters without any bells and whistles. Our code is available at Github.},
DOI = {10.3390/s21082803}
}



@Article{app11083586,
AUTHOR = {Xu, Gaofei and Guo, Wei and Zhao, Yang and Zhou, Yue and Zhang, Yinlong and Liu, Xinyu and Xu, Gaopeng and Li, Guangwei},
TITLE = {Online Learning Based Underwater Robotic Thruster Fault Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3586},
URL = {https://www.mdpi.com/2076-3417/11/8/3586},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a novel online learning-based fault detection designed for underwater robotic thruster health monitoring. In the fault detection algorithm, we build a mathematical model between the control variable and the propeller speed by fitting collected online work status data to the model. To improve the accuracy of online modeling, a multi-center PSO algorithm with memory ability is utilized to optimize the modeling parameters. Additionally, a model online update mechanism is designed to accommodate the model to the change of thruster work status and sea environment. During the operation, propeller speed of the underwater robot is predicted through the online learning-based model, and the model residuals are used for thruster health monitoring. To avoid false alarm, an adaptive fault detection strategy is established based on model online update mechanism. The proposed method has been extensively evaluated using different underwater robotics, through a sea trial data simulation, a pool test fault detection experiment and a sea trial fault detection experiment. Compared with fixed model-based method, speed prediction MAE of the online learning model is at least 37.9% lower than that of the fixed model. The online learning-based method show no misdiagnosis in experiments, while the fixed model-based method is misdiagnosed. Experimental results show that the proposed method is competitive in terms of accuracy, adaptability, and robustness.},
DOI = {10.3390/app11083586}
}



@Article{rs13081547,
AUTHOR = {He, Yixin and Zhai, Daosen and Huang, Fanghui and Wang, Dawei and Tang, Xiao and Zhang, Ruonan},
TITLE = {Joint Task Offloading, Resource Allocation, and Security Assurance for Mobile Edge Computing-Enabled UAV-Assisted VANETs},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1547},
URL = {https://www.mdpi.com/2072-4292/13/8/1547},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we propose a mobile edge computing (MEC)-enabled unmanned aerial vehicle (UAV)-assisted vehicular ad hoc network (VANET) architecture, based on which a number of vehicles are served by UAVs equipped with computation resource. Each vehicle has to offload its computing tasks to the proper MEC server on the UAV due to the limited computation ability. To counter the problems above, we first model and analyze the transmission model and the security assurance model from the vehicle to the MEC server on UAV, and the task computation model of the local vehicle and the edge UAV. Then, the vehicle offloading problem is formulated as a multi-objective optimization problem by jointly considering the task offloading, the resource allocation, and the security assurance. For tackling this hard problem, we decouple the multi-objective optimization problem as two subproblems and propose an efficient iterative algorithm to jointly make the MEC selection decision based on the criteria of load balancing and optimize the offloading ratio and the computation resource according to the Lagrangian dual decomposition. Finally, the simulation results demonstrate that our proposed scheme achieves significant performance superiority compared with other schemes in terms of the successful task processing ratio and the task processing delay.},
DOI = {10.3390/rs13081547}
}



@Article{s21082824,
AUTHOR = {Coluccia, Angelo and Fascista, Alessio and Schumann, Arne and Sommer, Lars and Dimou, Anastasios and Zarpalas, Dimitrios and Méndez, Miguel and de la Iglesia, David and González, Iago and Mercier, Jean-Philippe and Gagné, Guillaume and Mitra, Arka and Rajashekar, Shobha},
TITLE = {Drone vs. Bird Detection: Deep Learning Algorithms and Results from a Grand Challenge},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2824},
URL = {https://www.mdpi.com/1424-8220/21/8/2824},
PubMedID = {33923829},
ISSN = {1424-8220},
ABSTRACT = {Adopting effective techniques to automatically detect and identify small drones is a very compelling need for a number of different stakeholders in both the public and private sectors. This work presents three different original approaches that competed in a grand challenge on the “Drone vs. Bird” detection problem. The goal is to detect one or more drones appearing at some time point in video sequences where birds and other distractor objects may be also present, together with motion in background or foreground. Algorithms should raise an alarm and provide a position estimate only when a drone is present, while not issuing alarms on birds, nor being confused by the rest of the scene. In particular, three original approaches based on different deep learning strategies are proposed and compared on a real-world dataset provided by a consortium of universities and research centers, under the 2020 edition of the Drone vs. Bird Detection Challenge. Results show that there is a range in difficulty among different test sequences, depending on the size and the shape visibility of the drone in the sequence, while sequences recorded by a moving camera and very distant drones are the most challenging ones. The performance comparison reveals that the different approaches perform somewhat complementary, in terms of correct detection rate, false alarm rate, and average precision.},
DOI = {10.3390/s21082824}
}



@Article{drones5020028,
AUTHOR = {Li, Joan Y. Q. and Duce, Stephanie and Joyce, Karen E. and Xiang, Wei},
TITLE = {SeeCucumbers: Using Deep Learning and Drone Imagery to Detect Sea Cucumbers on Coral Reef Flats},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {28},
URL = {https://www.mdpi.com/2504-446X/5/2/28},
ISSN = {2504-446X},
ABSTRACT = {Sea cucumbers (Holothuroidea or holothurians) are a valuable fishery and are also crucial nutrient recyclers, bioturbation agents, and hosts for many biotic associates. Their ecological impacts could be substantial given their high abundance in some reef locations and thus monitoring their populations and spatial distribution is of research interest. Traditional in situ surveys are laborious and only cover small areas but drones offer an opportunity to scale observations more broadly, especially if the holothurians can be automatically detected in drone imagery using deep learning algorithms. We adapted the object detection algorithm YOLOv3 to detect holothurians from drone imagery at Hideaway Bay, Queensland, Australia. We successfully detected 11,462 of 12,956 individuals over 2.7ha with an average density of 0.5 individual/m2. We tested a range of hyperparameters to determine the optimal detector performance and achieved 0.855 mAP, 0.82 precision, 0.83 recall, and 0.82 F1 score. We found as few as ten labelled drone images was sufficient to train an acceptable detection model (0.799 mAP). Our results illustrate the potential of using small, affordable drones with direct implementation of open-source object detection models to survey holothurians and other shallow water sessile species.},
DOI = {10.3390/drones5020028}
}



@Article{rs13081557,
AUTHOR = {Balsi, Marco and Moroni, Monica and Chiarabini, Valter and Tanda, Giovanni},
TITLE = {High-Resolution Aerial Detection of Marine Plastic Litter by Hyperspectral Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1557},
URL = {https://www.mdpi.com/2072-4292/13/8/1557},
ISSN = {2072-4292},
ABSTRACT = {An automatic custom-made procedure is developed to identify macroplastic debris loads in coastal and marine environment, through hyperspectral imaging from unmanned aerial vehicles (UAVs). Results obtained during a remote-sensing field campaign carried out in the seashore of Sassari (Sardinia, Italy) are presented. A push-broom-sensor-based spectral device, carried onboard a DJI Matrice 600 drone, was employed for the acquisition of spectral data in the range 900−1700 nm. The hyperspectral platform was realized by assembling commercial devices, whereas algorithms for mosaicking, post-flight georeferencing, and orthorectification of the acquired images were developed in-house. Generation of the hyperspectral cube was based on mosaicking visible-spectrum images acquired synchronously with the hyperspectral lines, by performing correlation-based registration and applying the same translations, rotations, and scale changes to the hyperspectral data. Plastics detection was based on statistically relevant feature selection and Linear Discriminant Analysis, trained on a manually labeled sample. The results obtained from the inspection of either the beach site or the sea water facing the beach clearly show the successful separate identification of polyethylene (PE) and polyethylene terephthalate (PET) objects through the post-processing data treatment based on the developed classifier algorithm. As a further implementation of the procedure described, direct real-time processing, by an embedded computer carried onboard the drone, permitted the immediate plastics identification (and visual inspection in synchronized images) during the UAV survey, as documented by short video sequences provided in this research paper.},
DOI = {10.3390/rs13081557}
}



@Article{s21082834,
AUTHOR = {Kazaz, Billur and Poddar, Subhadipto and Arabi, Saeed and Perez, Michael A. and Sharma, Anuj and Whitman, J. Blake},
TITLE = {Deep Learning-Based Object Detection for Unmanned Aerial Systems (UASs)-Based Inspections of Construction Stormwater Practices},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2834},
URL = {https://www.mdpi.com/1424-8220/21/8/2834},
PubMedID = {33920610},
ISSN = {1424-8220},
ABSTRACT = {Construction activities typically create large amounts of ground disturbance, which can lead to increased rates of soil erosion. Construction stormwater practices are used on active jobsites to protect downstream waterbodies from offsite sediment transport. Federal and state regulations require routine pollution prevention inspections to ensure that temporary stormwater practices are in place and performing as intended. This study addresses the existing challenges and limitations in the construction stormwater inspections and presents a unique approach for performing unmanned aerial system (UAS)-based inspections. Deep learning-based object detection principles were applied to identify and locate practices installed on active construction sites. The system integrates a post-processing stage by clustering results. The developed framework consists of data preparation with aerial inspections, model training, validation of the model, and testing for accuracy. The developed model was created from 800 aerial images and was used to detect four different types of construction stormwater practices at 100% accuracy on the Mean Average Precision (MAP) with minimal false positive detections. Results indicate that object detection could be implemented on UAS-acquired imagery as a novel approach to construction stormwater inspections and provide accurate results for site plan comparisons by rapidly detecting the quantity and location of field-installed stormwater practices.},
DOI = {10.3390/s21082834}
}



@Article{s21082839,
AUTHOR = {Poudel, Sabitri and Moh, Sangman},
TITLE = {Hybrid Path Planning for Efficient Data Collection in UAV-Aided WSNs for Emergency Applications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2839},
URL = {https://www.mdpi.com/1424-8220/21/8/2839},
PubMedID = {33920627},
ISSN = {1424-8220},
ABSTRACT = {In unmanned aerial vehicle (UAV)-aided wireless sensor networks (UWSNs), a UAV is employed as a mobile sink to gather data from sensor nodes. Incorporating UAV helps prolong the network lifetime and avoid the energy-hole problem faced by sensor networks. In emergency applications, timely data collection from sensor nodes and transferal of the data to the base station (BS) is a prime requisite. The timely and safe path of UAV is one of the fundamental premises for effective UWSN operations. It is essential and challenging to identify a suitable path in an environment comprising various obstacles and to ensure that the path can efficiently reach the target point. This paper proposes a hybrid path planning (HPP) algorithm for efficient data collection by assuring the shortest collision-free path for UAV in emergency environments. In the proposed HPP scheme, the probabilistic roadmap (PRM) algorithm is used to design the shortest trajectory map and the optimized artificial bee colony (ABC) algorithm to improve different path constraints in a three-dimensional environment. Our simulation results show that the proposed HPP outperforms the PRM and conventional ABC schemes significantly in terms of flight time, energy consumption, convergence time, and flight path.},
DOI = {10.3390/s21082839}
}



@Article{rs13081562,
AUTHOR = {Ge, Xiangyu and Ding, Jianli and Jin, Xiuliang and Wang, Jingzhe and Chen, Xiangyue and Li, Xiaohang and Liu, Jie and Xie, Boqiang},
TITLE = {Estimating Agricultural Soil Moisture Content through UAV-Based Hyperspectral Images in the Arid Region},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1562},
URL = {https://www.mdpi.com/2072-4292/13/8/1562},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV)-based hyperspectral remote sensing is an important monitoring technology for the soil moisture content (SMC) of agroecological systems in arid regions. This technology develops precision farming and agricultural informatization. However, hyperspectral data are generally used in data mining. In this study, UAV-based hyperspectral imaging data with a resolution o 4 cm and totaling 70 soil samples (0–10 cm) were collected from farmland (2.5 × 104 m2) near Fukang City, Xinjiang Uygur Autonomous Region, China. Four estimation strategies were tested: the original image (strategy I), first- and second-order derivative methods (strategy II), the fractional-order derivative (FOD) technique (strategy III), and the optimal fractional order combined with the optimal multiband indices (strategy IV). These strategies were based on the eXtreme Gradient Boost (XGBoost) algorithm, with the aim of building the best estimation model for agricultural SMC in arid regions. The results demonstrated that FOD technology could effectively mine information (with an absolute maximum correlation coefficient of 0.768). By comparison, strategy IV yielded the best estimates out of the methods tested (R2val = 0.921, RMSEP = 1.943, and RPD = 2.736) for the SMC. The model derived from the order of 0.4 within strategy IV worked relatively well among the different derivative methods (strategy I, II, and III). In conclusion, the combination of FOD technology and the optimal multiband indices generated a highly accurate model within the XGBoost algorithm for SMC estimation. This research provided a promising data mining approach for UAV-based hyperspectral imaging data.},
DOI = {10.3390/rs13081562}
}



@Article{s21082835,
AUTHOR = {Hashima, Sherief and Hatano, Kohei and Kasban, Hany and Mahmoud Mohamed, Ehab},
TITLE = {Wi-Fi Assisted Contextual Multi-Armed Bandit for Neighbor Discovery and Selection in Millimeter Wave Device to Device Communications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2835},
URL = {https://www.mdpi.com/1424-8220/21/8/2835},
PubMedID = {33920717},
ISSN = {1424-8220},
ABSTRACT = {The unique features of millimeter waves (mmWaves) motivate its leveraging to future, beyond-fifth-generation/sixth-generation (B5G/6G)-based device-to-device (D2D) communications. However, the neighborhood discovery and selection (NDS) problem still needs intelligent solutions due to the trade-off of investigating adjacent devices for the optimum device choice against the crucial beamform training (BT) overhead. In this paper, by making use of multiband (μW/mmWave) standard devices, the mmWave NDS problem is addressed using machine-learning-based contextual multi-armed bandit (CMAB) algorithms. This is done by leveraging the context information of Wi-Fi signal characteristics, i.e., received signal strength (RSS), mean, and variance, to further improve the NDS method. In this setup, the transmitting device acts as the player, the arms are the candidate mmWave D2D links between that device and its neighbors, while the reward is the average throughput. We examine the NDS’s primary trade-off and the impacts of the contextual information on the total performance. Furthermore, modified energy-aware linear upper confidence bound (EA-LinUCB) and contextual Thomson sampling (EA-CTS) algorithms are proposed to handle the problem through reflecting the nearby devices’ withstanding battery levels, which simulate real scenarios. Simulation results ensure the superior efficiency of the proposed algorithms over the single band (mmWave) energy-aware noncontextual MAB algorithms (EA-UCB and EA-TS) and traditional schemes regarding energy efficiency and average throughput with a reasonable convergence rate.},
DOI = {10.3390/s21082835}
}



@Article{s21082848,
AUTHOR = {Zulkifley, Muhammad Aidiel and Behjati, Mehran and Nordin, Rosdiadee and Zakaria, Mohamad Shanudin},
TITLE = {Mobile Network Performance and Technical Feasibility of LTE-Powered Unmanned Aerial Vehicle},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2848},
URL = {https://www.mdpi.com/1424-8220/21/8/2848},
PubMedID = {33919486},
ISSN = {1424-8220},
ABSTRACT = {Conventional and license-free radio-controlled drone activities are limited to a line-of-sight (LoS) operational range. One of the alternatives to operate the drones beyond the visual line-of-sight (BVLoS) range is replacing the drone wireless communications system from the conventional industrial, scientific, and medical (ISM) radio band to a licensed cellular-connected system. The Long Term Evolution (LTE) technology that has been established for the terrestrial area allows command-and-control and payload communications between drone and ground station in real-time. However, with increasing height above the ground, the radio environment changes, and utilizing terrestrial cellular networks for drone communications may face new challenges. In this regard, this paper aims to develop an LTE-based control system prototype for low altitude small drones and investigate the feasibility and performance of drone cellular connectivity at different altitudes with measuring parameters such as latency, handover, and signal strength. The measurement results have shown that by increasing flight height from ground to 170 m the received signal power and the signal quality levels were reduced by 20 dBm and 10 dB respectively, the downlink data rate decreased to 70%, and latency increased up to 94 ms. It is concluded that although the existing LTE network can provide a minimum requirement for drone cellular connectivity, further improvements are still needed to enhance aerial coverage, eliminate interference, and reduce network latency.},
DOI = {10.3390/s21082848}
}



@Article{app11083642,
AUTHOR = {Bukin, Oleg and Proschenko, Dmitry and Korovetskiy, Denis and Chekhlenok, Alexey and Yurchik, Viktoria and Bukin, Ilya},
TITLE = {Development of the Artificial Intelligence and Optical Sensing Methods for Oil Pollution Monitoring of the Sea by Drones},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3642},
URL = {https://www.mdpi.com/2076-3417/11/8/3642},
ISSN = {2076-3417},
ABSTRACT = {The oil pollution of seas is increasing, especially in local areas, such as ports, roadsteads of the vessels, and bunkering zones. Today, methods of monitoring seawater are costly and applicable only in the case of big ecology disasters. The development of an operative and reasonable project for monitoring the sea surface for oil slick detection is described in this article using drones equipped with optical sensing and artificial intelligence. The monitoring system is implemented in the form of separate hard and soft frameworks (HSFWs) that combine monitoring methods, hardware, and software. Three frameworks are combined to fulfill the entire monitoring mission. HSFW1 performs the function of autonomous monitoring of thin oil slicks on the sea surface, using computer vision with AI elements for detection, segmentation, and classification of thin slicks. HSFW2 is based on the use of laser-induced fluorescence (LIF) to identify types of oil products that form a slick or that are in a dissolved state, as well as measure their concentration in solution. HSFW3 is designed for autonomous navigation and drone movement control. This article describes AI elements and hardware complexes of the three separate frameworks designed to solve the problems with monitoring slicks of oil products on the sea surface and oil products dissolved in seawater. The results of testing the HSFWs for the detection of pollution caused by marine fuel slicks are described.},
DOI = {10.3390/app11083642}
}



@Article{su13084511,
AUTHOR = {Haque, Amlan and Islam, Nahina and Samrat, Nahidul Hoque and Dey, Shuvashis and Ray, Biplob},
TITLE = {Smart Farming through Responsible Leadership in Bangladesh: Possibilities, Opportunities, and Beyond},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {4511},
URL = {https://www.mdpi.com/2071-1050/13/8/4511},
ISSN = {2071-1050},
ABSTRACT = {Smart farming has the potential to overcome the challenge of 2050 to feed 10 billion people. Both artificial intelligence (AI) and the internet of things (IoT) have become critical prerequisites to smart farming due to their high interoperability, sensors, and cutting-edge technologies. Extending the role of responsible leadership, this paper proposes an AI and IoT based smart farming system in Bangladesh. With a comprehensive literature review, this paper counsels the need to go beyond the simple application of traditional farming and irrigation practices and recommends implementing smart farming enabling responsible leadership to uphold sustainable agriculture. It contributes to the current literature of smart farming in several ways. First, this paper helps to understand the prospect and challenges of both AI and IoT and the requirement of smart farming in a nonwestern context. Second, it clarifies the interventions of responsible leadership into Bangladesh’s agriculture sector and justifies the demand for sustainable smart farming. Third, this paper is a step forward to explore future empirical studies for the effective and efficient use of AI and IoT to adopt smart farming. Finally, this paper will help policymakers to take responsible initiatives to plan and apply smart farming in a developing economy like Bangladesh.},
DOI = {10.3390/su13084511}
}



@Article{s21082862,
AUTHOR = {Yanes Luis, Samuel and Gutiérrez-Reina, Daniel and Toral Marín, Sergio},
TITLE = {A Dimensional Comparison between Evolutionary Algorithm and Deep Reinforcement Learning Methodologies for Autonomous Surface Vehicles with Water Quality Sensors},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2862},
URL = {https://www.mdpi.com/1424-8220/21/8/2862},
PubMedID = {33921649},
ISSN = {1424-8220},
ABSTRACT = {The monitoring of water resources using Autonomous Surface Vehicles with water-quality sensors has been a recent approach due to the advances in unmanned transportation technology. The Ypacaraí Lake, the biggest water resource in Paraguay, suffers from a major contamination problem because of cyanobacteria blooms. In order to supervise the blooms using these on-board sensor modules, a Non-Homogeneous Patrolling Problem (a NP-hard problem) must be solved in a feasible amount of time. A dimensionality study is addressed to compare the most common methodologies, Evolutionary Algorithm and Deep Reinforcement Learning, in different map scales and fleet sizes with changes in the environmental conditions. The results determined that Deep Q-Learning overcomes the evolutionary method in terms of sample-efficiency by 50–70% in higher resolutions. Furthermore, it reacts better than the Evolutionary Algorithm in high space-state actions. In contrast, the evolutionary approach shows a better efficiency in lower resolutions and needs fewer parameters to synthesize robust solutions. This study reveals that Deep Q-learning approaches exceed in efficiency for the Non-Homogeneous Patrolling Problem but with many hyper-parameters involved in the stability and convergence.},
DOI = {10.3390/s21082862}
}



@Article{electronics10080970,
AUTHOR = {Zhou, Liling and Wang, Yingzi and Liu, Yunfei and Zhang, Haifeng and Zheng, Shuaikang and Zou, Xudong and Li, Zhitian},
TITLE = {A Tightly-Coupled Positioning System of Online Calibrated RGB-D Camera and Wheel Odometry Based on SE(2) Plane Constraints},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {970},
URL = {https://www.mdpi.com/2079-9292/10/8/970},
ISSN = {2079-9292},
ABSTRACT = {The emergence of Automated Guided Vehicle (AGV) has greatly increased the efficiency of the transportation industry, which put forward the urgent requirement for the accuracy and ease of use of 2D planar motion robot positioning. Multi-sensor fusion positioning has gradually become an important technical route to improve overall efficiency when dealing with AGV positioning. As a sensor directly acquiring depth, the RGB-D camera has received extensive attention in indoor positioning in recent years, while wheel odometry is the sensor that comes with most two-dimensional planar motion robots, and its parameters will not change over time. Both the RGB-D camera and the wheel odometry are commonly used sensors for indoor robot positioning, but the existing research on the fusion of RGB-D and wheel odometry is limited based on classic filtering algorithms; few fusion solutions based on optimization algorithm of them are available at present. To ensure the practicability and greatly improve the accuracy of RGB-D and odometry fusion positioning scheme, this paper proposed a tightly-coupled positioning scheme of online calibrated RGB-D camera and wheel odometry based on SE(2) plane constraints. Experiments have proved that the angle accuracy of the extrinsic parameter in the calibration part is less than 0.5 degrees, and the displacement of the extrinsic parameter reaches the millimeter level. The field-test positioning accuracy of the positioning system we proposed having reached centimeter-level on the dataset without pre-calibration, which is better than ORB-SLAM2 relying solely on RGB-D cameras. The experimental results verify the excellent performance of the frame in positioning accuracy and ease of use and prove that it can be a potential promising technical solution in the field of two-dimensional AGV positioning.},
DOI = {10.3390/electronics10080970}
}



@Article{s21082864,
AUTHOR = {Zhang, Yuanping and Huang, Xiumei and Yang, Ming},
TITLE = {A Hybrid Visual Tracking Algorithm Based on SOM Network and Correlation Filter},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2864},
URL = {https://www.mdpi.com/1424-8220/21/8/2864},
PubMedID = {33921720},
ISSN = {1424-8220},
ABSTRACT = {To meet the challenge of video target tracking, based on a self-organization mapping network (SOM) and correlation filter, a long-term visual tracking algorithm is proposed. Objects in different videos or images often have completely different appearance, therefore, the self-organization mapping neural network with the characteristics of signal processing mechanism of human brain neurons is used to perform adaptive and unsupervised features learning. A reliable method of robust target tracking is proposed, based on multiple adaptive correlation filters with a memory function of target appearance at the same time. Filters in our method have different updating strategies and can carry out long-term tracking cooperatively. The first is the displacement filter, a kernelized correlation filter that combines contextual characteristics to precisely locate and track targets. Secondly, the scale filters are used to predict the changing scale of a target. Finally, the memory filter is used to maintain the appearance of the target in long-term memory and judge whether the target has failed to track. If the tracking fails, the incremental learning detector is used to recover the target tracking in the way of sliding window. Several experiments show that our method can effectively solve the tracking problems such as severe occlusion, target loss and scale change, and is superior to the state-of-the-art methods in the aspects of efficiency, accuracy and robustness.},
DOI = {10.3390/s21082864}
}



@Article{s21082868,
AUTHOR = {Cheng, Gong and Wei, Huangfu},
TITLE = {Virtual Angle Boundary-Aware Particle Swarm Optimization to Maximize the Coverage of Directional Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2868},
URL = {https://www.mdpi.com/1424-8220/21/8/2868},
PubMedID = {33921843},
ISSN = {1424-8220},
ABSTRACT = {With the transition of the mobile communication networks, the network goal of the Internet of everything further promotes the development of the Internet of Things (IoT) and Wireless Sensor Networks (WSNs). Since the directional sensor has the performance advantage of long-term regional monitoring, how to realize coverage optimization of Directional Sensor Networks (DSNs) becomes more important. The coverage optimization of DSNs is usually solved for one of the variables such as sensor azimuth, sensing radius, and time schedule. To reduce the computational complexity, we propose an optimization coverage scheme with a boundary constraint of eliminating redundancy for DSNs. Combined with Particle Swarm Optimization (PSO) algorithm, a Virtual Angle Boundary-aware Particle Swarm Optimization (VAB-PSO) is designed to reduce the computational burden of optimization problems effectively. The VAB-PSO algorithm generates the boundary constraint position between the sensors according to the relationship among the angles of different sensors, thus obtaining the boundary of particle search and restricting the search space of the algorithm. Meanwhile, different particles search in complementary space to improve the overall efficiency. Experimental results show that the proposed algorithm with a boundary constraint can effectively improve the coverage and convergence speed of the algorithm.},
DOI = {10.3390/s21082868}
}



@Article{rs13081599,
AUTHOR = {Seitsonen, Oula and Ikäheimo, Janne},
TITLE = {Detecting Archaeological Features with Airborne Laser Scanning in the Alpine Tundra of Sápmi, Northern Finland},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1599},
URL = {https://www.mdpi.com/2072-4292/13/8/1599},
ISSN = {2072-4292},
ABSTRACT = {Open access airborne laser scanning (ALS) data have been available in Finland for over a decade and have been actively applied by the Finnish archaeologists in that time. The low resolution of this laser scanning 2008–2019 dataset (0.5 points/m2), however, has hindered its usability for archaeological prospection. In the summer of 2020, the situation changed markedly, when the Finnish National Land Survey started a new countrywide ALS survey with a higher resolution of 5 points/m2. In this paper we present the first results of applying this newly available ALS material for archaeological studies. Finnish LIDARK consortium has initiated the development of semi-automated approaches for visualizing, detecting, and analyzing archaeological features with this new dataset. Our first case studies are situated in the Alpine tundra environment of Sápmi in northern Finland, and the assessed archaeological features range from prehistoric sites to indigenous Sámi reindeer herding features and Second Word War-era German military structures. Already the initial analyses of the new ALS-5p data show their huge potential for locating, mapping, and assessing archaeological material. These results also suggest an imminent burst in the number of known archaeological sites, especially in the poorly accessible and little studied northern wilderness areas, when more data become available.},
DOI = {10.3390/rs13081599}
}



@Article{rs13081602,
AUTHOR = {Sun, Qiaoqiao and Liu, Xuefeng and Bourennane, Salah},
TITLE = {Unsupervised Multi-Level Feature Extraction for Improvement of Hyperspectral Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1602},
URL = {https://www.mdpi.com/2072-4292/13/8/1602},
ISSN = {2072-4292},
ABSTRACT = {Deep learning models have strong abilities in learning features and they have been successfully applied in hyperspectral images (HSIs). However, the training of most deep learning models requires labeled samples and the collection of labeled samples are labor-consuming in HSI. In addition, single-level features from a single layer are usually considered, which may result in the loss of some important information. Using multiple networks to obtain multi-level features is a solution, but at the cost of longer training time and computational complexity. To solve these problems, a novel unsupervised multi-level feature extraction framework that is based on a three dimensional convolutional autoencoder (3D-CAE) is proposed in this paper. The designed 3D-CAE is stacked by fully 3D convolutional layers and 3D deconvolutional layers, which allows for the spectral-spatial information of targets to be mined simultaneously. Besides, the 3D-CAE can be trained in an unsupervised way without involving labeled samples. Moreover, the multi-level features are directly obtained from the encoded layers with different scales and resolutions, which is more efficient than using multiple networks to get them. The effectiveness of the proposed multi-level features is verified on two hyperspectral data sets. The results demonstrate that the proposed method has great promise in unsupervised feature learning and can help us to further improve the hyperspectral classification when compared with single-level features.},
DOI = {10.3390/rs13081602}
}



@Article{app11093737,
AUTHOR = {Hrúz, Michal and Bugaj, Martin and Novák, Andrej and Kandera, Branislav and Badánik, Benedikt},
TITLE = {The Use of UAV with Infrared Camera and RFID for Airframe Condition Monitoring},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3737},
URL = {https://www.mdpi.com/2076-3417/11/9/3737},
ISSN = {2076-3417},
ABSTRACT = {The new progressive smart technologies announced in the fourth industrial revolution in aviation—Aviation 4.0—represent new possibilities and big challenges in aircraft maintenance processes. The main benefit of these technologies is the possibility to monitor, transfer, store, and analyze huge datasets. Based on analysis outputs, there is a possibility to improve current preventive maintenance processes and implement predictive maintenance processes. These solutions lower the downtime, save manpower, and extend the components’ lifetime; thus, the maximum effectivity and safety is achieved. The article deals with the possible implementation of an unmanned aerial vehicle (UAV) with an infrared camera and Radio Frequency Identification (RFID) as two of the smart hangar technologies for airframe condition monitoring. The presented implementations of smart technologies follow up the specific results of a case study focused on trainer aircraft failure monitoring and its impact on maintenance strategy changes. The case study failure indexes show the critical parts of aircraft that are subjected to damage the most. The aim of the article was to justify the need for thorough monitoring of critical parts of the aircraft and then analyze and propose a more effective and the most suitable form of technical condition monitoring of aircraft critical parts. The article describes the whole process of visual inspection performed by an unmanned aerial vehicle (UAV) with an IR camera and its related processes; in addition, it covers the possible usage of RFID tags as a labeling tool supporting the visual inspection. The implementations criteria apply to the repair and overhaul small aircraft maintenance organization, and later, it can also increase operational efficiency. The final suggestions describe the possible usage of proposed solutions, their main benefits, and also the limitations of their implementations in maintenance of trainer aircraft.},
DOI = {10.3390/app11093737}
}



@Article{s21092912,
AUTHOR = {Carmona, Juan and Guindel, Carlos and Garcia, Fernando and de la Escalera, Arturo},
TITLE = {eHMI: Review and Guidelines for Deployment on Autonomous Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {2912},
URL = {https://www.mdpi.com/1424-8220/21/9/2912},
PubMedID = {33919209},
ISSN = {1424-8220},
ABSTRACT = {Human–machine interaction is an active area of research due to the rapid development of autonomous systems and the need for communication. This review provides further insight into the specific issue of the information flow between pedestrians and automated vehicles by evaluating recent advances in external human–machine interfaces (eHMI), which enable the transmission of state and intent information from the vehicle to the rest of the traffic participants. Recent developments will be explored and studies analyzing their effectiveness based on pedestrian feedback data will be presented and contextualized. As a result, we aim to draw a broad perspective on the current status and recent techniques for eHMI and some guidelines that will encourage future research and development of these systems.},
DOI = {10.3390/s21092912}
}



@Article{rs13091619,
AUTHOR = {Yan, Bin and Fan, Pan and Lei, Xiaoyan and Liu, Zhijie and Yang, Fuzeng},
TITLE = {A Real-Time Apple Targets Detection Method for Picking Robot Based on Improved YOLOv5},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1619},
URL = {https://www.mdpi.com/2072-4292/13/9/1619},
ISSN = {2072-4292},
ABSTRACT = {The apple target recognition algorithm is one of the core technologies of the apple picking robot. However, most of the existing apple detection algorithms cannot distinguish between the apples that are occluded by tree branches and occluded by other apples. The apples, grasping end-effector and mechanical picking arm of the robot are very likely to be damaged if the algorithm is directly applied to the picking robot. Based on this practical problem, in order to automatically recognize the graspable and ungraspable apples in an apple tree image, a light-weight apple targets detection method was proposed for picking robot using improved YOLOv5s. Firstly, BottleneckCSP module was improved designed to BottleneckCSP-2 module which was used to replace the BottleneckCSP module in backbone architecture of original YOLOv5s network. Secondly, SE module, which belonged to the visual attention mechanism network, was inserted to the proposed improved backbone network. Thirdly, the bonding fusion mode of feature maps, which were inputs to the target detection layer of medium size in the original YOLOv5s network, were improved. Finally, the initial anchor box size of the original network was improved. The experimental results indicated that the graspable apples, which were unoccluded or only occluded by tree leaves, and the ungraspable apples, which were occluded by tree branches or occluded by other fruits, could be identified effectively using the proposed improved network model in this study. Specifically, the recognition recall, precision, mAP and F1 were 91.48%, 83.83%, 86.75% and 87.49%, respectively. The average recognition time was 0.015 s per image. Contrasted with original YOLOv5s, YOLOv3, YOLOv4 and EfficientDet-D0 model, the mAP of the proposed improved YOLOv5s model increased by 5.05%, 14.95%, 4.74% and 6.75% respectively, the size of the model compressed by 9.29%, 94.6%, 94.8% and 15.3% respectively. The average recognition speeds per image of the proposed improved YOLOv5s model were 2.53, 1.13 and 3.53 times of EfficientDet-D0, YOLOv4 and YOLOv3 and model, respectively. The proposed method can provide technical support for the real-time accurate detection of multiple fruit targets for the apple picking robot.},
DOI = {10.3390/rs13091619}
}



@Article{rs13091629,
AUTHOR = {Kwak, Geun-Ho and Park, Chan-won and Lee, Kyung-do and Na, Sang-il and Ahn, Ho-yong and Park, No-Wook},
TITLE = {Potential of Hybrid CNN-RF Model for Early Crop Mapping with Limited Input Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1629},
URL = {https://www.mdpi.com/2072-4292/13/9/1629},
ISSN = {2072-4292},
ABSTRACT = {When sufficient time-series images and training data are unavailable for crop classification, features extracted from convolutional neural network (CNN)-based representative learning may not provide useful information to discriminate crops with similar spectral characteristics, leading to poor classification accuracy. In particular, limited input data are the main obstacles to obtain reliable classification results for early crop mapping. This study investigates the potential of a hybrid classification approach, i.e., CNN-random forest (CNN-RF), in the context of early crop mapping, that combines the automatic feature extraction capability of CNN with the superior discrimination capability of an RF classifier. Two experiments on incremental crop classification with unmanned aerial vehicle images were conducted to compare the performance of CNN-RF with that of CNN and RF with respect to the length of the time-series and training data sizes. When sufficient time-series images and training data were used for the classification, the accuracy of CNN-RF was slightly higher or comparable with that of CNN. In contrast, when fewer images and the smallest training data were used at the early crop growth stage, CNN-RF was substantially beneficial and the overall accuracy increased by maximum 6.7%p and 4.6%p in the two study areas, respectively, compared to CNN. This is attributed to its ability to discriminate crops from features with insufficient information using a more sophisticated classifier. The experimental results demonstrate that CNN-RF is an effective classifier for early crop mapping when only limited input images and training samples are available.},
DOI = {10.3390/rs13091629}
}



@Article{rs13091620,
AUTHOR = {Ge, Haixiao and Xiang, Haitao and Ma, Fei and Li, Zhenwang and Qiu, Zhengchao and Tan, Zhengzheng and Du, Changwen},
TITLE = {Estimating Plant Nitrogen Concentration of Rice through Fusing Vegetation Indices and Color Moments Derived from UAV-RGB Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1620},
URL = {https://www.mdpi.com/2072-4292/13/9/1620},
ISSN = {2072-4292},
ABSTRACT = {Estimating plant nitrogen concentration (PNC) has been conducted using vegetation indices (VIs) from UAV-based imagery, but color features have been rarely considered as additional variables. In this study, the VIs and color moments (color feature) were calculated from UAV-based RGB images, then partial least square regression (PLSR) and random forest regression (RF) models were established to estimate PNC through fusing VIs and color moments. The results demonstrated that the fusion of VIs and color moments as inputs yielded higher accuracies of PNC estimation compared to VIs or color moments as input; the RF models based on the combination of VIs and color moments (R2 ranging from 0.69 to 0.91 and NRMSE ranging from 0.07 to 0.13) showed similar performances to the PLSR models (R2 ranging from 0.68 to 0.87 and NRMSE ranging from 0.10 to 0.29); Among the top five important variables in the RF models, there was at least one variable which belonged to the color moments in different datasets, indicating the significant contribution of color moments in improving PNC estimation accuracy. This revealed the great potential of combination of RGB-VIs and color moments for the estimation of rice PNC.},
DOI = {10.3390/rs13091620}
}



@Article{jsan10020028,
AUTHOR = {Pourroostaei Ardakani, Saeid},
TITLE = {MINDS: Mobile Agent Itinerary Planning Using Named Data Networking in Wireless Sensor Networks},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {28},
URL = {https://www.mdpi.com/2224-2708/10/2/28},
ISSN = {2224-2708},
ABSTRACT = {Mobile agents have the potential to offer benefits, as they are able to either independently or cooperatively move throughout networks and collect/aggregate sensory data samples. They are programmed to autonomously move and visit sensory data stations through optimal paths, which are established according to the application requirements. However, mobile agent routing protocols still suffer heavy computation/communication overheads, lack of route planning accuracy and long-delay mobile agent migrations. For this, mobile agent route planning protocols aim to find the best-fitted paths for completing missions (e.g., data collection) with minimised delay, maximised performance and minimised transmitted traffic. This article proposes a mobile agent route planning protocol for sensory data collection called MINDS. The key goal of this MINDS is to reduce network traffic, maximise data robustness and minimise delay at the same time. This protocol utilises the Hamming distance technique to partition a sensor network into a number of data-centric clusters. In turn, a named data networking approach is used to form the cluster-heads as a data-centric, tree-based communication infrastructure. The mobile agents utilise a modified version of the Depth-First Search algorithm to move through the tree infrastructure according to a hop-count-aware fashion. As the simulation results show, MINDS reduces path length, reduces network traffic and increases data robustness as compared with two conventional benchmarks (ZMA and TBID) in dense and large wireless sensor networks.},
DOI = {10.3390/jsan10020028}
}



@Article{electronics10090999,
AUTHOR = {Azar, Ahmad Taher and Koubaa, Anis and Ali Mohamed, Nada and Ibrahim, Habiba A. and Ibrahim, Zahra Fathy and Kazim, Muhammad and Ammar, Adel and Benjdira, Bilel and Khamis, Alaa M. and Hameed, Ibrahim A. and Casalino, Gabriella},
TITLE = {Drone Deep Reinforcement Learning: A Review},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {999},
URL = {https://www.mdpi.com/2079-9292/10/9/999},
ISSN = {2079-9292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are increasingly being used in many challenging and diversified applications. These applications belong to the civilian and the military fields. To name a few; infrastructure inspection, traffic patrolling, remote sensing, mapping, surveillance, rescuing humans and animals, environment monitoring, and Intelligence, Surveillance, Target Acquisition, and Reconnaissance (ISTAR) operations. However, the use of UAVs in these applications needs a substantial level of autonomy. In other words, UAVs should have the ability to accomplish planned missions in unexpected situations without requiring human intervention. To ensure this level of autonomy, many artificial intelligence algorithms were designed. These algorithms targeted the guidance, navigation, and control (GNC) of UAVs. In this paper, we described the state of the art of one subset of these algorithms: the deep reinforcement learning (DRL) techniques. We made a detailed description of them, and we deduced the current limitations in this area. We noted that most of these DRL methods were designed to ensure stable and smooth UAV navigation by training computer-simulated environments. We realized that further research efforts are needed to address the challenges that restrain their deployment in real-life scenarios.},
DOI = {10.3390/electronics10090999}
}



@Article{app11093785,
AUTHOR = {Sariff, Nohaidda and Ismail, Zool Hilmi},
TITLE = {Broadcast Event-Triggered Control Scheme for Multi-Agent Rendezvous Problem in a Mixed Communication Environment},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3785},
URL = {https://www.mdpi.com/2076-3417/11/9/3785},
ISSN = {2076-3417},
ABSTRACT = {This paper addresses the communication issue encountered by a hybrid controller when finding consensus in terms of the rendezvous target point in a broadcast and communication environment. This issue may result in a high level of computation and the utilization of agent resources when a continuous communication is required by agents to meet convergence requirements. Thus, an event-triggered system was integrated into the design of a broadcast and distributed consensus linear controller using the simultaneous perturbation stochastic algorithm (SPSA). The agent’s movement towards the rendezvous point is based on the broadcast value, whereas the next agent’s state position depends on the distributed local controller output. The communication error obtained during communication between the agent and neighbors is only added to the gradient approximation error of the SPSA if the event-triggered function is violated. As a result, in our model, the number of channel utilizations was lower and the agents’ performances were preserved. The efficiencies and effectiveness of the proposed controller have been compared with the traditional sampling broadcast time-triggered (BTT) approach. The time and iterations required by the broadcast event-triggered (BET) system were less than 40.42% and 21% on average as compared to BTT. The trajectory was not the same—the BET showed scattered movements at the initial stage, whereas BTT showed a linear movement. In terms of the number of channels, 28.91% of channels were preserved during the few hundred iterations. Consequently, a variety of hybrid controllers with event-triggered mechanisms can be proposed for other multi-agent motion coordination tasks.},
DOI = {10.3390/app11093785}
}



@Article{rs13091642,
AUTHOR = {Wang, Peng and Niu, Yanxiong and Xiong, Rui and Ma, Fu and Zhang, Chunxi},
TITLE = {DGANet: Dynamic Gradient Adjustment Anchor-Free Object Detection in Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1642},
URL = {https://www.mdpi.com/2072-4292/13/9/1642},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing image object detection has been studied by many researchers in recent years using deep neural networks. However, optical remote sensing images contain many scenes with small and dense objects, resulting in a high rate of misrecognition. Firstly, in this work we selected a deep layer aggregation network with updated deformable convolution layers as the backbone to extract object features. The detection and classification of objects was based on the center-point network without non-maximum suppression. Secondly, the dynamic gradient adjustment embedded into the classification loss function was put forward to harmonize the quantity imbalance between easy and hard examples, as well as between positive and negative examples. Furthermore, the complete intersection over union (CIoU) loss function was selected as the objective function of bounding box regression, which achieves better convergence speed and accuracy. Finally, in order to validate the effectiveness and precision of the dynamic gradient adjustment network (DGANet), we conducted a series of experiments in remote sensing public datasets UCAS-AOD and LEVIR. The comparison experiments demonstrate that the DGANet achieves a more accurate detection result in optical remote sensing images.},
DOI = {10.3390/rs13091642}
}



@Article{inventions6020029,
AUTHOR = {Kashyap, Bhuwan and Kumar, Ratnesh},
TITLE = {Sensing Methodologies in Agriculture for Monitoring Biotic Stress in Plants Due to Pathogens and Pests},
JOURNAL = {Inventions},
VOLUME = {6},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {29},
URL = {https://www.mdpi.com/2411-5134/6/2/29},
ISSN = {2411-5134},
ABSTRACT = {Reducing agricultural losses is an effective way to sustainably increase agricultural output efficiency to meet our present and future needs for food, fiber, fodder, and fuel. Our ever-improving understanding of the ways in which plants respond to stress, biotic and abiotic, has led to the development of innovative sensing technologies for detecting crop stresses/stressors and deploying efficient measures. This article aims to present the current state of the methodologies applied in the field of agriculture towards the detection of biotic stress in crops. Key sensing methodologies for plant pathogen (or phytopathogen), as well as herbivorous insects/pests are presented, where the working principles are described, and key recent works discussed. The detection methods overviewed for phytopathogen-related stress identification include nucleic acid-based methods, immunological methods, imaging-based techniques, spectroscopic methods, phytohormone biosensing methods, monitoring methods for plant volatiles, and active remote sensing technologies. Whereas the pest-related sensing techniques include machine-vision-based methods, pest acoustic-emission sensors, and volatile organic compound-based stress monitoring methods. Additionally, Comparisons have been made between different sensing techniques as well as recently reported works, where the strengths and limitations are identified. Finally, the prospective future directions for monitoring biotic stress in crops are discussed.},
DOI = {10.3390/inventions6020029}
}



@Article{ijgi10050273,
AUTHOR = {Palander, Teijo and Kärhä, Kalle},
TITLE = {Utilization of Image, LiDAR and Gamma-Ray Information to Improve Environmental Sustainability of Cut-to-Length Wood Harvesting Operations in Peatlands: A Management Systems Perspective},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {273},
URL = {https://www.mdpi.com/2220-9964/10/5/273},
ISSN = {2220-9964},
ABSTRACT = {Forest industry corporations use quality management systems in their wood procurement operations. Spatial quality data are used to improve the quality of wood harvesting and to achieve environmental sustainability. Some studies have proposed new management systems based on LiDAR. The main aim of this study was to investigate how efficiently planning systems can select areas for wood harvesting a priori with respect to avoiding harvesting damage caused by forest machinery. A literature review revealed the possibility of using GISs, and case studies showed the criteria required to predict the required quality levels. Terrestrial LiDAR can be utilized in authorities’ quality control systems, but it is inefficient for preplanning without terrestrial gamma-ray data collection. Airborne LiDAR and gamma-ray information about forest soils can only be used for planning larger regions at the forest level because the information includes too much uncertainty to allow it to be used for planning in small-sized areas before wood harvesting operations involving wood procurement. In addition, airborne LiDAR is not accurate enough, even at the forest level, for the planning of wood procurement systems because wood harvesting remains challenging without field measurements. Therefore, there is a need for the use of manual ground-penetrating radar for determining the peat layer thickness and the depth to the groundwater table.},
DOI = {10.3390/ijgi10050273}
}



@Article{w13091171,
AUTHOR = {Wen, Chao and Zhan, Qingming and Zhan, De and Zhao, Huang and Yang, Chen},
TITLE = {Spatiotemporal Evolution of Lakes under Rapid Urbanization: A Case Study in Wuhan, China},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1171},
URL = {https://www.mdpi.com/2073-4441/13/9/1171},
ISSN = {2073-4441},
ABSTRACT = {The impact of urbanization on lakes in the urban context has aroused continuous attention from the public. However, the long-term evolution of lakes in a certain megacity and the heterogeneity of the spatial relationship between related influencing factors and lake changes are rarely discussed. The evolution of 58 lakes in Wuhan, China from 1990 to 2019 was analyzed from three aspects of lake area, lake landscape, and lakefront ecology, respectively. The Multi-Scale Geographic Weighted Regression model (MGWR) was then used to analyze the impact of related influencing factors on lake area change. The investigation found that the total area of 58 lakes decreased by 15.3%. A worsening trend was found regarding lake landscape with the five landscape indexes of lakes dropping; in contrast, lakefront ecology saw a gradual recovery with variations in the remote sensing ecological index (RSEI) in the lakefront area. The MGWR regression results showed that, on the whole, the increase in Gross Domestic Product (GDP), RSEI in the lakefront area, precipitation, and humidity contributed to lake restoration. The growth of population and the proportion of impervious surface (IS) in the lakefront area had different effects on different lakes. Specifically, the increase in GDP and population in all downtown districts and two suburb districts promoted lake restoration (e.g., Wu Lake), while the increase in population in Jiangxia led to lake loss. The growth of RSEI in lakefront area promoted the restoration of most lakes. A higher proportion of IS in lakefront area normally resulted in more lake loss. However, in some cases, the growth of IS was caused by lake conservation, which contributed to lake restoration (e.g., Tangxun Lake). The study reveals the spatiotemporal evolution of multiple lakes in Wuhan and provides a useful reference for the government to formulate differentiated protection policies.},
DOI = {10.3390/w13091171}
}



@Article{rs13091661,
AUTHOR = {Gargees, Rasha S. and Scott, Grant J.},
TITLE = {Large-Scale, Multiple Level-of-Detail Change Detection from Remote Sensing Imagery Using Deep Visual Feature Clustering},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1661},
URL = {https://www.mdpi.com/2072-4292/13/9/1661},
ISSN = {2072-4292},
ABSTRACT = {In the era of big data, where massive amounts of remotely sensed imagery can be obtained from various satellites accompanied by the rapid change in the surface of the Earth, new techniques for large-scale change detection are necessary to facilitate timely and effective human understanding of natural and human-made phenomena. In this research, we propose a chip-based change detection method that is enabled by using deep neural networks to extract visual features. These features are transformed into deep orthogonal visual features that are then clustered based on land cover characteristics. The resulting chip cluster memberships allow arbitrary level-of-detail change analysis that can also support irregular geospatial extent based agglomerations. The proposed methods naturally support cross-resolution temporal scenes without requiring normalization of the pixel resolution across scenes and without requiring pixel-level coregistration processes. This is achieved with configurable spatial locality comparisons between years, where the aperture of a unit of measure can be a single chip, a small neighborhood of chips, or a large irregular geospatial region. The performance of our proposed method has been validated using various quantitative and statistical metrics in addition to presenting the visual geo-maps and the percentage of the change. The results show that our proposed method efficiently detected the change from a large scale area.},
DOI = {10.3390/rs13091661}
}



@Article{app11093863,
AUTHOR = {Öztürk, Ali Emre and Erçelebi, Ergun},
TITLE = {Real UAV-Bird Image Classification Using CNN with a Synthetic Dataset},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3863},
URL = {https://www.mdpi.com/2076-3417/11/9/3863},
ISSN = {2076-3417},
ABSTRACT = {A large amount of training image data is required for solving image classification problems using deep learning (DL) networks. In this study, we aimed to train DL networks with synthetic images generated by using a game engine and determine the effects of the networks on performance when solving real-image classification problems. The study presents the results of using corner detection and nearest three-point selection (CDNTS) layers to classify bird and rotary-wing unmanned aerial vehicle (RW-UAV) images, provides a comprehensive comparison of two different experimental setups, and emphasizes the significant improvements in the performance in deep learning-based networks due to the inclusion of a CDNTS layer. Experiment 1 corresponds to training the commonly used deep learning-based networks with synthetic data and an image classification test on real data. Experiment 2 corresponds to training the CDNTS layer and commonly used deep learning-based networks with synthetic data and an image classification test on real data. In experiment 1, the best area under the curve (AUC) value for the image classification test accuracy was measured as 72%. In experiment 2, using the CDNTS layer, the AUC value for the image classification test accuracy was measured as 88.9%. A total of 432 different combinations of trainings were investigated in the experimental setups. The experiments were trained with various DL networks using four different optimizers by considering all combinations of batch size, learning rate, and dropout hyperparameters. The test accuracy AUC values for networks in experiment 1 ranged from 55% to 74%, whereas the test accuracy AUC values in experiment 2 networks with a CDNTS layer ranged from 76% to 89.9%. It was observed that the CDNTS layer has considerable effects on the image classification accuracy performance of deep learning-based networks. AUC, F-score, and test accuracy measures were used to validate the success of the networks.},
DOI = {10.3390/app11093863}
}



@Article{electronics10091021,
AUTHOR = {Chan, Teck Kai and Chin, Cheng Siong},
TITLE = {Review of Autonomous Intelligent Vehicles for Urban Driving and Parking},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1021},
URL = {https://www.mdpi.com/2079-9292/10/9/1021},
ISSN = {2079-9292},
ABSTRACT = {With the concept of Internet-of-Things, autonomous vehicles can provide higher driving efficiency, traffic safety, and freedom for the driver to perform other tasks. This paper first covers enabling technology involving a vehicle moving out of parking, traveling on the road, and parking at the destination. The development of autonomous vehicles relies on the data collected for deployment in actual road conditions. Research gaps and recommendations for autonomous intelligent vehicles are included. For example, a sudden obstacle while the autonomous vehicle executes the parking trajectory on the road is discussed. Several aspects of social problems, such as the liability of an accident affecting the autonomous vehicle, are described. A smart device to detect abnormal driving behaviors to prevent possible accidents is briefly discussed.},
DOI = {10.3390/electronics10091021}
}



@Article{rs13091669,
AUTHOR = {Chen, Zhiang and Wagner, Melissa and Das, Jnaneshwar and Doe, Robert K. and Cerveny, Randall S.},
TITLE = {Data-Driven Approaches for Tornado Damage Estimation with Unpiloted Aerial Systems},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1669},
URL = {https://www.mdpi.com/2072-4292/13/9/1669},
ISSN = {2072-4292},
ABSTRACT = {Tornado damage estimation is important for providing insights into tornado studies and assisting rapid disaster response. However, it is challenging to precisely estimate tornado damage because of the large volumes of perishable data. This study presents data-driven approaches to tornado damage estimation using imagery collected from Unpiloted Aerial Systems (UASs) following the 26 June 2018 Eureka Kansas tornado. High-resolution orthomosaics were generated from Structure from Motion (SfM). We applied deep neural networks (DNNs) on the orthomosaics to estimate tornado damage and assessed their performance in four scenarios: (1) object detection with binary categories, (2) object detection with multiple categories, (3) image classification with binary categories, and (4) image classification with multiple categories. Additionally, two types of tornado damage heatmaps were generated. By directly stitching the resulting image tiles from the DNN inference, we produced the first type of tornado damage heatmaps where damage estimates are accurately georeferenced. We also presented a Gaussian process (GP) regression model to build the second type of tornado damage heatmap (a spatially continuous tornado damage heatmap) by merging the first type of object detection and image classification heatmaps. The GP regression results were assessed with ground-truth annotations and National Weather Service (NWS) ground surveys. This detailed information can help NWS Weather Forecast Offices and emergency managers with their damage assessments and better inform disaster response and recovery.},
DOI = {10.3390/rs13091669}
}



@Article{s21093012,
AUTHOR = {Wang, Wenbo and Aguilar Sanchez, Ignacio and Caparra, Gianluca and McKeown, Andy and Whitworth, Tim and Lohan, Elena Simona},
TITLE = {A Survey of Spoofer Detection Techniques via Radio Frequency Fingerprinting with Focus on the GNSS Pre-Correlation Sampled Data},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3012},
URL = {https://www.mdpi.com/1424-8220/21/9/3012},
PubMedID = {33923015},
ISSN = {1424-8220},
ABSTRACT = {Radio frequency fingerprinting (RFF) methods are becoming more and more popular in the context of identifying genuine transmitters and distinguishing them from malicious or non-authorized transmitters, such as spoofers and jammers. RFF approaches have been studied to a moderate-to-great extent in the context of non-GNSS transmitters, such as WiFi, IoT, or cellular transmitters, but they have not yet been addressed much in the context of GNSS transmitters. In addition, the few RFF-related works in GNSS context are based on post-correlation or navigation data and no author has yet addressed the RFF problem in GNSS with pre-correlation data. Moreover, RFF methods in any of the three domains (pre-correlation, post-correlation, or navigation) are still hard to be found in the context of GNSS. The goal of this paper was two-fold: first, to provide a comprehensive survey of the RFF methods applicable in the GNSS context; and secondly, to propose a novel RFF methodology for spoofing detection, with a focus on GNSS pre-correlation data, but also applicable in a wider context. In order to support our proposed methodology, we qualitatively investigated the capability of different methods to be used in the context of pre-correlation sampled GNSS data, and we present a simulation-based example, under ideal noise conditions, of how the feature down selection can be done. We are also pointing out which of the transmitter features are likely to play the biggest roles in the RFF in GNSS, and which features are likely to fail in helping RFF-based spoofing detection.},
DOI = {10.3390/s21093012}
}



@Article{agriculture11050387,
AUTHOR = {Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu, Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A. and Moore, Steven and Rahman, Sk Mostafizur},
TITLE = {Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {387},
URL = {https://www.mdpi.com/2077-0472/11/5/387},
ISSN = {2077-0472},
ABSTRACT = {This paper explores the potential of machine learning algorithms for weed and crop classification from UAV images. The identification of weeds in crops is a challenging task that has been addressed through orthomosaicing of images, feature extraction and labelling of images to train machine learning algorithms. In this paper, the performances of several machine learning algorithms, random forest (RF), support vector machine (SVM) and k-nearest neighbours (KNN), are analysed to detect weeds using UAV images collected from a chilli crop field located in Australia. The evaluation metrics used in the comparison of performance were accuracy, precision, recall, false positive rate and kappa coefficient. MATLAB is used for simulating the machine learning algorithms; and the achieved weed detection accuracies are 96% using RF, 94% using SVM and 63% using KNN. Based on this study, RF and SVM algorithms are efficient and practical to use, and can be implemented easily for detecting weed from UAV images.},
DOI = {10.3390/agriculture11050387}
}



@Article{rs13091670,
AUTHOR = {Avola, Danilo and Cinque, Luigi and Diko, Anxhelo and Fagioli, Alessio and Foresti, Gian Luca and Mecca, Alessio and Pannone, Daniele and Piciarelli, Claudio},
TITLE = {MS-Faster R-CNN: Multi-Stream Backbone for Improved Faster R-CNN Object Detection and Aerial Tracking from UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1670},
URL = {https://www.mdpi.com/2072-4292/13/9/1670},
ISSN = {2072-4292},
ABSTRACT = {Tracking objects across multiple video frames is a challenging task due to several difficult issues such as occlusions, background clutter, lighting as well as object and camera view-point variations, which directly affect the object detection. These aspects are even more emphasized when analyzing unmanned aerial vehicles (UAV) based images, where the vehicle movement can also impact the image quality. A common strategy employed to address these issues is to analyze the input images at different scales to obtain as much information as possible to correctly detect and track the objects across video sequences. Following this rationale, in this paper, we introduce a simple yet effective novel multi-stream (MS) architecture, where different kernel sizes are applied to each stream to simulate a multi-scale image analysis. The proposed architecture is then used as backbone for the well-known Faster-R-CNN pipeline, defining a MS-Faster R-CNN object detector that consistently detects objects in video sequences. Subsequently, this detector is jointly used with the Simple Online and Real-time Tracking with a Deep Association Metric (Deep SORT) algorithm to achieve real-time tracking capabilities on UAV images. To assess the presented architecture, extensive experiments were performed on the UMCD, UAVDT, UAV20L, and UAV123 datasets. The presented pipeline achieved state-of-the-art performance, confirming that the proposed multi-stream method can correctly emulate the robust multi-scale image analysis paradigm.},
DOI = {10.3390/rs13091670}
}



@Article{rs13091672,
AUTHOR = {Pan, Erting and Ma, Yong and Fan, Fan and Mei, Xiaoguang and Huang, Jun},
TITLE = {Hyperspectral Image Classification across Different Datasets: A Generalization to Unseen Categories},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1672},
URL = {https://www.mdpi.com/2072-4292/13/9/1672},
ISSN = {2072-4292},
ABSTRACT = {With the rapid developments of hyperspectral imaging, the cost of collecting hyperspectral data has been lower, while the demand for reliable and detailed hyperspectral annotations has been much more substantial. However, limited by the difficulties of labelling annotations, most existing hyperspectral image (HSI) classification methods are trained and evaluated on a single hyperspectral data cube. It brings two significant challenges. On the one hand, many algorithms have reached a nearly perfect classification accuracy, but their trained models are hard to generalize to other datasets. On the other hand, since different hyperspectral datasets are usually not collected in the same scene, different datasets will contain different classes. To address these issues, in this paper, we propose a new paradigm for HSI classification, which is training and evaluating separately across different hyperspectral datasets. It is of great help to labelling hyperspectral data. However, it has rarely been studied in the hyperspectral community. In this work, we utilize a three-phase scheme, including feature embedding, feature mapping, and label reasoning. More specifically, we select a pair of datasets acquired by the same hyperspectral sensor, and the classifier learns from one dataset and then evaluated it on the other. Inspired by the latest advances in zero-shot learning, we introduce label semantic representation to establish associations between seen categories in the training set and unseen categories in the testing set. Extensive experiments on two pairs of datasets with different comparative methods have shown the effectiveness and potential of zero-shot learning in HSI classification.},
DOI = {10.3390/rs13091672}
}



@Article{drones5020031,
AUTHOR = {Song, Bonggeun and Park, Kyunghun},
TITLE = {Comparison of Outdoor Compost Pile Detection Using Unmanned Aerial Vehicle Images and Various Machine Learning Techniques},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {31},
URL = {https://www.mdpi.com/2504-446X/5/2/31},
ISSN = {2504-446X},
ABSTRACT = {Since outdoor compost piles (OCPs) contain large amounts of nitrogen and phosphorus, they act as a major pollutant that deteriorates water quality, such as eutrophication and green algae, when the OCPs enter the river during rainfall. In South Korea, OCPs are frequently used, but there is a limitation that a lot of manpower and budget are consumed to investigate the current situation, so it is necessary to efficiently investigate the OCPs. This study compared the accuracy of various machine learning techniques for the efficient detection and management of outdoor compost piles (OCPs), a non-point pollution source in agricultural areas in South Korea, using unmanned aerial vehicle (UAV) images. RGB, multispectral, and thermal infrared UAV images were taken in August and October 2019. Additionally, vegetation indices (NDVI, NDRE, ENDVI, and GNDVI) and surface temperature were also considered. Four machine learning techniques, including support vector machine (SVM), decision tree (DT), random forest (RF), and k-NN, were implemented, and the machine learning technique with the highest accuracy was identified by adjusting several variables. The accuracy of all machine learning techniques was very high, reaching values of up to 0.96. Particularly, the accuracy of the RF method with the number of estimators set to 10 was highest, reaching 0.989 in August and 0.987 in October. The proposed method allows for the prediction of OCP location and area over large regions, thereby foregoing the need for OCP field measurements. Therefore, our findings provide highly useful data for the improvement of OCP management strategies and water quality.},
DOI = {10.3390/drones5020031}
}



@Article{su13094883,
AUTHOR = {Khan, Nawab and Ray, Ram L. and Sargani, Ghulam Raza and Ihtisham, Muhammad and Khayyam, Muhammad and Ismail, Sohaib},
TITLE = {Current Progress and Future Prospects of Agriculture Technology: Gateway to Sustainable Agriculture},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4883},
URL = {https://www.mdpi.com/2071-1050/13/9/4883},
ISSN = {2071-1050},
ABSTRACT = {The agricultural industry is getting more data-centric and requires precise, more advanced data and technologies than before, despite being familiar with agricultural processes. The agriculture industry is being advanced by various information and advanced communication technologies, such as the Internet of Things (IoT). The rapid emergence of these advanced technologies has restructured almost all other industries, as well as advanced agriculture, which has shifted the industry from a statistical approach to a quantitative one. This radical change has shaken existing farming techniques and produced the latest prospects in a series of challenges. This comprehensive review article enlightens the potential of the IoT in the advancement of agriculture and the challenges faced when combining these advanced technologies with conventional agricultural systems. A brief analysis of these advanced technologies with sensors is presented in advanced agricultural applications. Numerous sensors that can be implemented for specific agricultural practices require best management practices (e.g., land preparation, irrigation systems, insect, and disease management). This review includes the integration of all suitable techniques, from sowing to harvesting, packaging, transportation, and advanced technologies available for farmers throughout the cropping system. Besides, this review article highlights the utilization of other tools such as unmanned aerial vehicles (UAVs) for crop monitoring and other beneficiary measures, such as optimizing crop yields. In addition, advanced programs based on the IoT are also discussed. Finally, based on our comprehensive review, we identified advanced prospects regarding the IoT, which are essential tools for sustainable agriculture.},
DOI = {10.3390/su13094883}
}



@Article{technologies9020031,
AUTHOR = {Ahsan, Md Manjurul and Li, Yueqing and Zhang, Jing and Ahad, Md Tanvir and Gupta, Kishor Datta},
TITLE = {Evaluating the Performance of Eigenface, Fisherface, and Local Binary Pattern Histogram-Based Facial Recognition Methods under Various Weather Conditions},
JOURNAL = {Technologies},
VOLUME = {9},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {31},
URL = {https://www.mdpi.com/2227-7080/9/2/31},
ISSN = {2227-7080},
ABSTRACT = {Facial recognition (FR) in unconstrained weather is still challenging and surprisingly ignored by many researchers and practitioners over the past few decades. Therefore, this paper aims to evaluate the performance of three existing popular facial recognition methods considering different weather conditions. As a result, a new face dataset (Lamar University database (LUDB)) was developed that contains face images captured under various weather conditions such as foggy, cloudy, rainy, and sunny. Three very popular FR methods—Eigenface (EF), Fisherface (FF), and Local binary pattern histogram (LBPH)—were evaluated considering two other face datasets, AT&amp;T and 5_Celebrity, along with LUDB in term of accuracy, precision, recall, and F1 score with 95% confidence interval (CI). Computational results show a significant difference among the three FR techniques in terms of overall time complexity and accuracy. LBPH outperforms the other two FR algorithms on both LUDB and 5_Celebrity datasets by achieving 40% and 95% accuracy, respectively. On the other hand, with minimum execution time of 1.37, 1.37, and 1.44 s per image on AT&amp;T,5_Celebrity, and LUDB, respectively, Fisherface achieved the best result.},
DOI = {10.3390/technologies9020031}
}



@Article{app11093938,
AUTHOR = {Bi, Shusheng and Yuan, Chang and Liu, Chang and Cheng, Jun and Wang, Wei and Cai, Yueri},
TITLE = {A Survey of Low-Cost 3D Laser Scanning Technology},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3938},
URL = {https://www.mdpi.com/2076-3417/11/9/3938},
ISSN = {2076-3417},
ABSTRACT = {By moving a commercial 2D LiDAR, 3D maps of the environment can be built, based on the data of a 2D LiDAR and its movements. Compared to a commercial 3D LiDAR, a moving 2D LiDAR is more economical. A series of problems need to be solved in order for a moving 2D LiDAR to perform better, among them, improving accuracy and real-time performance. In order to solve these problems, estimating the movements of a 2D LiDAR, and identifying and removing moving objects in the environment, are issues that should be studied. More specifically, calibrating the installation error between the 2D LiDAR and the moving unit, the movement estimation of the moving unit, and identifying moving objects at low scanning frequencies, are involved. As actual applications are mostly dynamic, and in these applications, a moving 2D LiDAR moves between multiple moving objects, we believe that, for a moving 2D LiDAR, how to accurately construct 3D maps in dynamic environments will be an important future research topic. Moreover, how to deal with moving objects in a dynamic environment via a moving 2D LiDAR has not been solved by previous research.},
DOI = {10.3390/app11093938}
}



@Article{robotics10020063,
AUTHOR = {Madokoro, Hirokazu and Yamamoto, Satoshi and Nishimura, Yo and Nix, Stephanie and Woo, Hanwool and Sato, Kazuhito},
TITLE = {Prototype Development of Small Mobile Robots for Mallard Navigation in Paddy Fields: Toward Realizing Remote Farming},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {63},
URL = {https://www.mdpi.com/2218-6581/10/2/63},
ISSN = {2218-6581},
ABSTRACT = {This study was conducted to develop robot prototypes of three models that navigate mallards to achieve high-efficiency rice-duck farming. We examined two robotics navigation approaches based on imprinting and feeding. As the first approach, we used imprinting applied to baby mallards. They exhibited follow behavior to our first prototype after imprinting. Experimentally obtained observation results revealed the importance of providing imprinting immediately up to one week after hatching. As another approach, we used feed placed on the top of our second prototype. Experimentally obtained results showed that adult mallards exhibited wariness not only against the robot, but also against the feeder. After relieving wariness with provision of more than one week time to become accustomed, adult mallards ate feed in the box on the robot. However, they ran away immediately at a slight movement. Based on this confirmation, we developed the third prototype as an autonomous mobile robot aimed for mallard navigation in a paddy field. The body width is less than the length between rice stalks. After checking the waterproof capability of a body waterproof box, we conducted an indoor driving test for manual operation. Moreover, we conducted outdoor evaluation tests to assess running on an actual paddy field. We developed indoor and outdoor image datasets using an onboard monocular camera. For the outdoor image datasets, our segmentation method based on SegNet achieved semantic segmentation for three semantic categories. For the indoor image datasets, our prediction method based on CNN and LSTM achieved visual prediction for three motion categories.},
DOI = {10.3390/robotics10020063}
}



@Article{app11093948,
AUTHOR = {Maw, Aye Aye and Tyan, Maxim and Nguyen, Tuan Anh and Lee, Jae-Woo},
TITLE = {iADA*-RL: Anytime Graph-Based Path Planning with Deep Reinforcement Learning for an Autonomous UAV},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3948},
URL = {https://www.mdpi.com/2076-3417/11/9/3948},
ISSN = {2076-3417},
ABSTRACT = {Path planning algorithms are of paramount importance in guidance and collision systems to provide trustworthiness and safety for operations of autonomous unmanned aerial vehicles (UAV). Previous works showed different approaches mostly focusing on shortest path discovery without a sufficient consideration on local planning and collision avoidance. In this paper, we propose a hybrid path planning algorithm that uses an anytime graph-based path planning algorithm for global planning and deep reinforcement learning for local planning which applied for a real-time mission planning system of an autonomous UAV. In particular, we aim to achieve a highly autonomous UAV mission planning system that is adaptive to real-world environments consisting of both static and moving obstacles for collision avoidance capabilities. To achieve adaptive behavior for real-world problems, a simulator is required that can imitate real environments for learning. For this reason, the simulator must be sufficiently flexible to allow the UAV to learn about the environment and to adapt to real-world conditions. In our scheme, the UAV first learns about the environment via a simulator, and only then is it applied to the real-world. The proposed system is divided into two main parts: optimal flight path generation and collision avoidance. A hybrid path planning approach is developed by combining a graph-based path planning algorithm with a learning-based algorithm for local planning to allow the UAV to avoid a collision in real time. The global path planning problem is solved in the first stage using a novel anytime incremental search algorithm called improved Anytime Dynamic A* (iADA*). A reinforcement learning method is used to carry out local planning between waypoints, to avoid any obstacles within the environment. The developed hybrid path planning system was investigated and validated in an AirSim environment. A number of different simulations and experiments were performed using AirSim platform in order to demonstrate the effectiveness of the proposed system for an autonomous UAV. This study helps expand the existing research area in designing efficient and safe path planning algorithms for UAVs.},
DOI = {10.3390/app11093948}
}



@Article{s21093049,
AUTHOR = {Allouch, Azza and Cheikhrouhou, Omar and Koubâa, Anis and Toumi, Khalifa and Khalgui, Mohamed and Nguyen Gia, Tuan},
TITLE = {UTM-Chain: Blockchain-Based Secure Unmanned Traffic Management for Internet of Drones},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3049},
URL = {https://www.mdpi.com/1424-8220/21/9/3049},
PubMedID = {33925489},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial systems (UAVs) are dramatically evolving and promoting several civil applications. However, they are still prone to many security issues that threaten public safety. Security becomes even more challenging when they are connected to the Internet as their data stream is exposed to attacks. Unmanned traffic management (UTM) represents one of the most important topics for small unmanned aerial systems for beyond-line-of-sight operations in controlled low-altitude airspace. However, without securing the flight path exchanges between drones and ground stations or control centers, serious security threats may lead to disastrous situations. For example, a predefined flight path could be easily altered to make the drone perform illegal operations. Motivated by these facts, this paper discusses the security issues for UTM’s components and addresses the security requirements for such systems. Moreover, we propose UTM-Chain, a lightweight blockchain-based security solution using hyperledger fabric for UTM of low-altitude UAVs which fits the computational and storage resources limitations of UAVs. Moreover, UTM-Chain provides secure and unalterable traffic data between the UAVs and their ground control stations. The performance of the proposed system related to transaction latency and resource utilization is analyzed by using cAdvisor. Finally, the analysis of security aspects demonstrates that the proposed UTM-Chain scheme is feasible and extensible for the secure sharing of UAV data.},
DOI = {10.3390/s21093049}
}



@Article{ani11051263,
AUTHOR = {Wang, Zhaojun and Wang, Jiangning and Lin, Congtian and Han, Yan and Wang, Zhaosheng and Ji, Liqiang},
TITLE = {Identifying Habitat Elements from Bird Images Using Deep Convolutional Neural Networks},
JOURNAL = {Animals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1263},
URL = {https://www.mdpi.com/2076-2615/11/5/1263},
PubMedID = {33925654},
ISSN = {2076-2615},
ABSTRACT = {With the rapid development of digital technology, bird images have become an important part of ornithology research data. However, due to the rapid growth of bird image data, it has become a major challenge to effectively process such a large amount of data. In recent years, deep convolutional neural networks (DCNNs) have shown great potential and effectiveness in a variety of tasks regarding the automatic processing of bird images. However, no research has been conducted on the recognition of habitat elements in bird images, which is of great help when extracting habitat information from bird images. Here, we demonstrate the recognition of habitat elements using four DCNN models trained end-to-end directly based on images. To carry out this research, an image database called Habitat Elements of Bird Images (HEOBs-10) and composed of 10 categories of habitat elements was built, making future benchmarks and evaluations possible. Experiments showed that good results can be obtained by all the tested models. ResNet-152-based models yielded the best test accuracy rate (95.52%); the AlexNet-based model yielded the lowest test accuracy rate (89.48%). We conclude that DCNNs could be efficient and useful for automatically identifying habitat elements from bird images, and we believe that the practical application of this technology will be helpful for studying the relationships between birds and habitat elements.},
DOI = {10.3390/ani11051263}
}



@Article{geomatics1020013,
AUTHOR = {Duarte, Lia and Teodoro, Ana Cláudia},
TITLE = {GIS Open-Source Plugins Development: A 10-Year Bibliometric Analysis on Scientific Literature},
JOURNAL = {Geomatics},
VOLUME = {1},
YEAR = {2021},
NUMBER = {2},
PAGES = {206--245},
URL = {https://www.mdpi.com/2673-7418/1/2/13},
ISSN = {2673-7418},
ABSTRACT = {The advent of Geographical Information Systems (GIS) has changed the way people think and interact with the world. The main objectives of this paper are: (i) to provide an overview of 10 years (2010–2020) regarding the creation/development of GIS open-source applications; and (ii) to evaluate the GIS open-source plugins for environmental science. In the first objective, we evaluate the publications regarding the development of GIS open-source geospatial software in the last 10 years, considering desktop, web GIS and mobile applications, so that we can analyze the impact of this type of application for different research areas. In the second objective, we analyze the development of GIS open-source applications in the field of environmental sciences (with more focus on QGIS plugins) in the last 10 years and discuss the applicability and usability of these GIS solutions in different environmental domains. A bibliometric analysis was performed using Web of Science database and VOSViewer software. We concluded that, in general, the development of GIS open-source applications has increased in the last 10 years, especially GIS mobile applications, since the big data and Internet of Things (IoT) era, which was expected given the new advanced technologies available in every area, especially in GIS.},
DOI = {10.3390/geomatics1020013}
}



@Article{rs13091704,
AUTHOR = {de Camargo, Tibor and Schirrmann, Michael and Landwehr, Niels and Dammer, Karl-Heinz and Pflanz, Michael},
TITLE = {Optimized Deep Learning Model as a Basis for Fast UAV Mapping of Weed Species in Winter Wheat Crops},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1704},
URL = {https://www.mdpi.com/2072-4292/13/9/1704},
ISSN = {2072-4292},
ABSTRACT = {Weed maps should be available quickly, reliably, and with high detail to be useful for site-specific management in crop protection and to promote more sustainable agriculture by reducing pesticide use. Here, the optimization of a deep residual convolutional neural network (ResNet-18) for the classification of weed and crop plants in UAV imagery is proposed. The target was to reach sufficient performance on an embedded system by maintaining the same features of the ResNet-18 model as a basis for fast UAV mapping. This would enable online recognition and subsequent mapping of weeds during UAV flying operation. Optimization was achieved mainly by avoiding redundant computations that arise when a classification model is applied on overlapping tiles in a larger input image. The model was trained and tested with imagery obtained from a UAV flight campaign at low altitude over a winter wheat field, and classification was performed on species level with the weed species Matricaria chamomilla L., Papaver rhoeas L., Veronica hederifolia L., and Viola arvensis ssp. arvensis observed in that field. The ResNet-18 model with the optimized image-level prediction pipeline reached a performance of 2.2 frames per second with an NVIDIA Jetson AGX Xavier on the full resolution UAV image, which would amount to about 1.78 ha h−1 area output for continuous field mapping. The overall accuracy for determining crop, soil, and weed species was 94%. There were some limitations in the detection of species unknown to the model. When shifting from 16-bit to 32-bit model precision, no improvement in classification accuracy was observed, but a strong decline in speed performance, especially when a higher number of filters was used in the ResNet-18 model. Future work should be directed towards the integration of the mapping process on UAV platforms, guiding UAVs autonomously for mapping purpose, and ensuring the transferability of the models to other crop fields.},
DOI = {10.3390/rs13091704}
}



@Article{f12050550,
AUTHOR = {Xu, Dandan and Wang, Haobin and Xu, Weixin and Luan, Zhaoqing and Xu, Xia},
TITLE = {LiDAR Applications to Estimate Forest Biomass at Individual Tree Scale: Opportunities, Challenges and Future Perspectives},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {550},
URL = {https://www.mdpi.com/1999-4907/12/5/550},
ISSN = {1999-4907},
ABSTRACT = {Accurate forest biomass estimation at the individual tree scale is the foundation of timber industry and forest management. It plays an important role in explaining ecological issues and small-scale processes. Remotely sensed images, across a range of spatial and temporal resolutions, with their advantages of non-destructive monitoring, are widely applied in forest biomass monitoring at global, ecoregion or community scales. However, the development of remote sensing applications for forest biomass at the individual tree scale has been relatively slow due to the constraints of spatial resolution and evaluation accuracy of remotely sensed data. With the improvements in platforms and spatial resolutions, as well as the development of remote sensing techniques, the potential for forest biomass estimation at the single tree level has been demonstrated. However, a comprehensive review of remote sensing of forest biomass scaled at individual trees has not been done. This review highlights the theoretical bases, challenges and future perspectives for Light Detection and Ranging (LiDAR) applications of individual trees scaled to whole forests. We summarize research on estimating individual tree volume and aboveground biomass (AGB) using Terrestrial Laser Scanning (TLS), Airborne Laser Scanning (ALS), Unmanned Aerial Vehicle Laser Scanning (UAV-LS) and Mobile Laser Scanning (MLS, including Vehicle-borne Laser Scanning (VLS) and Backpack Laser Scanning (BLS)) data.},
DOI = {10.3390/f12050550}
}



@Article{rs13091723,
AUTHOR = {Kuzmin, Anton and Korhonen, Lauri and Kivinen, Sonja and Hurskainen, Pekka and Korpelainen, Pasi and Tanhuanpää, Topi and Maltamo, Matti and Vihervaara, Petteri and Kumpula, Timo},
TITLE = {Detection of European Aspen (Populus tremula L.) Based on an Unmanned Aerial Vehicle Approach in Boreal Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1723},
URL = {https://www.mdpi.com/2072-4292/13/9/1723},
ISSN = {2072-4292},
ABSTRACT = {European aspen (Populus tremula L.) is a keystone species for biodiversity of boreal forests. Large-diameter aspens maintain the diversity of hundreds of species, many of which are threatened in Fennoscandia. Due to a low economic value and relatively sparse and scattered occurrence of aspen in boreal forests, there is a lack of information of the spatial and temporal distribution of aspen, which hampers efficient planning and implementation of sustainable forest management practices and conservation efforts. Our objective was to assess identification of European aspen at the individual tree level in a southern boreal forest using high-resolution photogrammetric point cloud (PPC) and multispectral (MSP) orthomosaics acquired with an unmanned aerial vehicle (UAV). The structure-from-motion approach was applied to generate RGB imagery-based PPC to be used for individual tree-crown delineation. Multispectral data were collected using two UAV cameras: Parrot Sequoia and MicaSense RedEdge-M. Tree-crown outlines were obtained from watershed segmentation of PPC data and intersected with multispectral mosaics to extract and calculate spectral metrics for individual trees. We assessed the role of spectral data features extracted from PPC and multispectral mosaics and a combination of it, using a machine learning classifier—Support Vector Machine (SVM) to perform two different classifications: discrimination of aspen from the other species combined into one class and classification of all four species (aspen, birch, pine, spruce) simultaneously. In the first scenario, the highest classification accuracy of 84% (F1-score) for aspen and overall accuracy of 90.1% was achieved using only RGB features from PPC, whereas in the second scenario, the highest classification accuracy of 86 % (F1-score) for aspen and overall accuracy of 83.3% was achieved using the combination of RGB and MSP features. The proposed method provides a new possibility for the rapid assessment of aspen occurrence to enable more efficient forest management as well as contribute to biodiversity monitoring and conservation efforts in boreal forests.},
DOI = {10.3390/rs13091723}
}



@Article{rs13091730,
AUTHOR = {Chen, Ang and Yang, Xiuchun and Xu, Bin and Jin, Yunxiang and Guo, Jian and Xing, Xiaoyu and Yang, Dong and Wang, Ping and Zhu, Libo},
TITLE = {Monitoring the Spatiotemporal Dynamics of Aeolian Desertification Using Google Earth Engine},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1730},
URL = {https://www.mdpi.com/2072-4292/13/9/1730},
ISSN = {2072-4292},
ABSTRACT = {Northern China has been long threatened by aeolian desertification. In recent years, all levels of the Chinese government have performed a series of ecological protection and sand control projects. To grasp the implementation effects of these projects and adjust policies in time, it is necessary to understand the process of aeolian desertification quickly and accurately. Remote sensing technologies play an irreplaceable role in aeolian desertification monitoring. In this study, the Zhenglan Banner, which is in the hinterland of the Hunshandake Sandy Land, was considered as the research area. Based on unmanned aerial vehicle (UAV) images, ground survey data, and Landsat images called in Google Earth Engine (GEE), the aeolian desertified land (ADL) in 2000, 2004, 2010, 2015, and 2019 was extracted using spectral mixture analysis. A desertification index (DI) was constructed to evaluate the spatial and temporal dynamics of the ADL in the Zhenglan Banner. Finally, a residual analysis explored the driving forces of aeolian desertification. The results showed that (1) the ADL area in the Zhenglan Banner has been trending downwards over the past 20 years but rebounded from 2004 to 2010; (2) over the past 20 years, the area of slightly, moderately, and severely desertified land has decreased at annual rates of 0.4%, 2.7%, and 3.4%, respectively; (3) human activities had significantly positive and negative impacts on the aeolian desertification trend for 20.0% and 21.0% of the study area, respectively, but not for the rest. This paper explored new techniques for rapid aeolian desertification monitoring and is of great significance for controlling and managing aeolian desertification in this region.},
DOI = {10.3390/rs13091730}
}



@Article{ijgi10050285,
AUTHOR = {Jiménez-Jiménez, Sergio Iván and Ojeda-Bustamante, Waldo and Marcial-Pablo, Mariana de Jesús and Enciso, Juan},
TITLE = {Digital Terrain Models Generated with Low-Cost UAV Photogrammetry: Methodology and Accuracy},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {285},
URL = {https://www.mdpi.com/2220-9964/10/5/285},
ISSN = {2220-9964},
ABSTRACT = {Digital terrain model (DTM) generation is essential to recreating terrain morphology once the external elements are removed. Traditional survey methods are still used to collect accurate geographic data on the land surface. Given the emergence of unmanned aerial vehicles (UAVs) equipped with low-cost digital cameras and better photogrammetric methods for digital mapping, efficient approaches are necessary to allow rapid land surveys with high accuracy. This paper provides a review, complemented with the authors’ experience, regarding the UAV photogrammetric process and field survey parameters for DTM generation using popular commercial photogrammetric software to process images obtained with fixed-wing or multicopter UAVs. We analyzed the quality and accuracy of the DTMs based on four categories: (i) the UAV system (UAV platforms and camera); (ii) flight planning and image acquisition (flight altitude, image overlap, UAV speed, orientation of the flight line, camera configuration, and georeferencing); (iii) photogrammetric DTM generation (software, image alignment, dense point cloud generation, and ground filtering); (iv) geomorphology and land use/cover. For flat terrain, UAV photogrammetry provided a horizontal root mean square error (RMSE) between 1 to 3 × the ground sample distance (GSD) and a vertical RMSE between 1 to 4.5 × GSD, and, for complex topography, a horizontal RMSE between 1 to 7 × GSD and a vertical RMSE between 1.5 to 5 × GSD. Finally, we stress that UAV photogrammetry can provide DTMs with high accuracy when the photogrammetric process variables are optimized.},
DOI = {10.3390/ijgi10050285}
}



@Article{rs13091741,
AUTHOR = {Hobley, Brandon and Arosio, Riccardo and French, Geoffrey and Bremner, Julie and Dolphin, Tony and Mackiewicz, Michal},
TITLE = {Semi-Supervised Segmentation for Coastal Monitoring Seagrass Using RPA Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1741},
URL = {https://www.mdpi.com/2072-4292/13/9/1741},
ISSN = {2072-4292},
ABSTRACT = {Intertidal seagrass plays a vital role in estimating the overall health and dynamics of coastal environments due to its interaction with tidal changes. However, most seagrass habitats around the globe have been in steady decline due to human impacts, disturbing the already delicate balance in the environmental conditions that sustain seagrass. Miniaturization of multi-spectral sensors has facilitated very high resolution mapping of seagrass meadows, which significantly improves the potential for ecologists to monitor changes. In this study, two analytical approaches used for classifying intertidal seagrass habitats are compared—Object-based Image Analysis (OBIA) and Fully Convolutional Neural Networks (FCNNs). Both methods produce pixel-wise classifications in order to create segmented maps. FCNNs are an emerging set of algorithms within Deep Learning. Conversely, OBIA has been a prominent solution within this field, with many studies leveraging in-situ data and multiresolution segmentation to create habitat maps. This work demonstrates the utility of FCNNs in a semi-supervised setting to map seagrass and other coastal features from an optical drone survey conducted at Budle Bay, Northumberland, England. Semi-supervision is also an emerging field within Deep Learning that has practical benefits of achieving state of the art results using only subsets of labelled data. This is especially beneficial for remote sensing applications where in-situ data is an expensive commodity. For our results, we show that FCNNs have comparable performance with the standard OBIA method used by ecologists.},
DOI = {10.3390/rs13091741}
}



@Article{rs13091749,
AUTHOR = {Wang, Zhe and Fan, Chao and Xian, Min},
TITLE = {Application and Evaluation of a Deep Learning Architecture to Urban Tree Canopy Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1749},
URL = {https://www.mdpi.com/2072-4292/13/9/1749},
ISSN = {2072-4292},
ABSTRACT = {Urban forest is a dynamic urban ecosystem that provides critical benefits to urban residents and the environment. Accurate mapping of urban forest plays an important role in greenspace management. In this study, we apply a deep learning model, the U-net, to urban tree canopy mapping using high-resolution aerial photographs. We evaluate the feasibility and effectiveness of the U-net in tree canopy mapping through experiments at four spatial scales—16 cm, 32 cm, 50 cm, and 100 cm. The overall performance of all approaches is validated on the ISPRS Vaihingen 2D Semantic Labeling dataset using four quantitative metrics, Dice, Intersection over Union, Overall Accuracy, and Kappa Coefficient. Two evaluations are performed to assess the model performance. Experimental results show that the U-net with the 32-cm input images perform the best with an overall accuracy of 0.9914 and an Intersection over Union of 0.9638. The U-net achieves the state-of-the-art overall performance in comparison with object-based image analysis approach and other deep learning frameworks. The outstanding performance of the U-net indicates a possibility of applying it to urban tree segmentation at a wide range of spatial scales. The U-net accurately recognizes and delineates tree canopy for different land cover features and has great potential to be adopted as an effective tool for high-resolution land cover mapping.},
DOI = {10.3390/rs13091749}
}



@Article{s21093152,
AUTHOR = {Liang, Peng and Shi, Wenzhong and Ding, Yixing and Liu, Zhiqiang and Shang, Haolv},
TITLE = {Road Extraction from High Resolution Remote Sensing Images Based on Vector Field Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3152},
URL = {https://www.mdpi.com/1424-8220/21/9/3152},
PubMedID = {34062917},
ISSN = {1424-8220},
ABSTRACT = {Accurate and up-to-date road network information is very important for the Geographic Information System (GIS) database, traffic management and planning, automatic vehicle navigation, emergency response and urban pollution sources investigation. In this paper, we use vector field learning to extract roads from high resolution remote sensing imaging. This method is usually used for skeleton extraction in nature image, but seldom used in road extraction. In order to improve the accuracy of road extraction, three vector fields are constructed and combined respectively with the normal road mask learning by a two-task network. The results show that all the vector fields are able to significantly improve the accuracy of road extraction, no matter the field is constructed in the road area or completely outside the road. The highest F1 score is 0.7618, increased by 0.053 compared with using only mask learning.},
DOI = {10.3390/s21093152}
}



@Article{s21093169,
AUTHOR = {Caldeira, Rafael Faria and Santiago, Wesley Esdras and Teruel, Barbara},
TITLE = {Identification of Cotton Leaf Lesions Using Deep Learning Techniques},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3169},
URL = {https://www.mdpi.com/1424-8220/21/9/3169},
PubMedID = {34063578},
ISSN = {1424-8220},
ABSTRACT = {The use of deep learning models to identify lesions on cotton leaves on the basis of images of the crop in the field is proposed in this article. Cultivated in most of the world, cotton is one of the economically most important agricultural crops. Its cultivation in tropical regions has made it the target of a wide spectrum of agricultural pests and diseases, and efficient solutions are required. Moreover, the symptoms of the main pests and diseases cannot be differentiated in the initial stages, and the correct identification of a lesion can be difficult for the producer. To help resolve the problem, the present research provides a solution based on deep learning in the screening of cotton leaves which makes it possible to monitor the health of the cotton crop and make better decisions for its management. With the learning models GoogleNet and Resnet50 using convolutional neural networks, a precision of 86.6% and 89.2%, respectively, was obtained. Compared with traditional approaches for the processing of images such as support vector machines (SVM), Closest k-neighbors (KNN), artificial neural networks (ANN) and neuro-fuzzy (NFC), the convolutional neural networks proved to be up to 25% more precise, suggesting that this method can contribute to a more rapid and reliable inspection of the plants growing in the field.},
DOI = {10.3390/s21093169}
}



@Article{ijgi10050293,
AUTHOR = {Maxwell, Aaron E. and Sharma, Maneesh and Kite, J. Steven and Donaldson, Kurt A. and Maynard, Shannon M. and Malay, Caleb M.},
TITLE = {Assessing the Generalization of Machine Learning-Based Slope Failure Prediction to New Geographic Extents},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {293},
URL = {https://www.mdpi.com/2220-9964/10/5/293},
ISSN = {2220-9964},
ABSTRACT = {Slope failure probabilistic models generated using random forest (RF) machine learning (ML), manually interpreted incident points, and light detection and ranging (LiDAR) digital terrain variables are assessed for predicting and generalizing to new geographic extents. Specifically, models for four Major Land Resource Areas (MLRAs) in the state of West Virginia in the United States (US) were created. All region-specific models were then used to predict withheld validation data within all four MLRAs. For all validation datasets, the model trained using data from the same MLRA provided the highest reported overall accuracy (OA), Kappa statistic, F1 Score, area under the receiver operating characteristic curve (AUC ROC), and area under the precision-recall curve (AUC PR). However, the model from the same MLRA as the validation dataset did not always provide the highest precision, recall, and/or specificity, suggesting that models extrapolated to new geographic extents tend to either overpredict or underpredict the land area of slope failure occurrence whereas they offer a better balance between omission and commission error within the region in which they were trained. This study highlights the value of developing region-specific inventories, models, and high resolution and detailed digital elevation data, since models may not generalize well to new geographic extents, potentially resulting from spatial heterogeneity in landscape and/or slope failure characteristics.},
DOI = {10.3390/ijgi10050293}
}



@Article{s21093204,
AUTHOR = {Shin, Sungtae and Yoon, Han Ul and Yoo, Byungseok},
TITLE = {Hand Gesture Recognition Using EGaIn-Silicone Soft Sensors},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3204},
URL = {https://www.mdpi.com/1424-8220/21/9/3204},
PubMedID = {34063055},
ISSN = {1424-8220},
ABSTRACT = {Exploiting hand gestures for non-verbal communication has extraordinary potential in HCI. A data glove is an apparatus widely used to recognize hand gestures. To improve the functionality of the data glove, a highly stretchable and reliable signal-to-noise ratio sensor is indispensable. To do this, the study focused on the development of soft silicone microchannel sensors using a Eutectic Gallium-Indium (EGaIn) liquid metal alloy and a hand gesture recognition system via the proposed data glove using the soft sensor. The EGaIn-silicone sensor was uniquely designed to include two sensing channels to monitor the finger joint movements and to facilitate the EGaIn alloy injection into the meander-type microchannels. We recruited 15 participants to collect hand gesture dataset investigating 12 static hand gestures. The dataset was exploited to estimate the performance of the proposed data glove in hand gesture recognition. Additionally, six traditional classification algorithms were studied. From the results, a random forest shows the highest classification accuracy of 97.3% and a linear discriminant analysis shows the lowest accuracy of 87.4%. The non-linearity of the proposed sensor deteriorated the accuracy of LDA, however, the other classifiers adequately overcame it and performed high accuracies (&gt;90%).},
DOI = {10.3390/s21093204}
}



@Article{ijgi10050300,
AUTHOR = {Yang, Dan and Mu, Kai and Yang, Hui and Luo, Mingliang and Lv, Wei and Zhang, Bin and Liu, Hui and Wang, Zhicheng},
TITLE = {A Study on Prediction Model of Gully Volume Based on Morphological Features in the JINSHA Dry-Hot Valley Region of Southwest China},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {300},
URL = {https://www.mdpi.com/2220-9964/10/5/300},
ISSN = {2220-9964},
ABSTRACT = {Gully erosion is well-developed in the Jinsha dry-hot valley region, which has caused serious soil losses. Gully volume is regarded as an effective indicator that can reflect the development intensity of gully erosion, and the evolutionary processes of gullies can be predicted based on the dynamic variation in gully volume. Establishing an effective prediction model of gully volume is essential to determine gully volume accurately and conveniently. Therefore, in this work, an empirical prediction model of gully volume was constructed and verified based on detailed morphological features acquired by elaborate field investigations and measurements in 134 gullies. The results showed the mean value of gully length, width, depth, cross-section area, volume, and vertical gradient decreased with the weakness of the activity degree of the gully, although the decrease in processes of these parameters had some differences. Moreover, a series of empirical prediction models of gully volume was constructed, and gully length was demonstrated to be a better predictor than other morphological features. Lastly, the effectiveness test showed the model of V = aL^b was the most effective in predicting gully volume among the different models established in this study. Our results provide a useful approach to predict gully volume in dry-hot valley regions.},
DOI = {10.3390/ijgi10050300}
}



@Article{electronics10091091,
AUTHOR = {Ayoub, Naeem and Schneider-Kamp, Peter},
TITLE = {Real-Time On-Board Deep Learning Fault Detection for Autonomous UAV Inspections},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1091},
URL = {https://www.mdpi.com/2079-9292/10/9/1091},
ISSN = {2079-9292},
ABSTRACT = {Inspection of high-voltage power lines using unmanned aerial vehicles is an emerging technological alternative to traditional methods. In the Drones4Energy project, we work toward building an autonomous vision-based beyond-visual-line-of-sight (BVLOS) power line inspection system. In this paper, we present a deep learning-based autonomous vision system to detect faults in power line components. We trained a YOLOv4-tiny architecture-based deep neural network, as it showed prominent results for detecting components with high accuracy. For running such deep learning models in a real-time environment, different single-board devices such as the Raspberry Pi 4, Nvidia Jetson Nano, Nvidia Jetson TX2, and Nvidia Jetson AGX Xavier were used for the experimental evaluation. Our experimental results demonstrated that the proposed approach can be effective and efficient for fully automatic real-time on-board visual power line inspection.},
DOI = {10.3390/electronics10091091}
}



@Article{rs13091809,
AUTHOR = {Feroz, Sainab and Abu Dabous, Saleh},
TITLE = {UAV-Based Remote Sensing Applications for Bridge Condition Assessment},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1809},
URL = {https://www.mdpi.com/2072-4292/13/9/1809},
ISSN = {2072-4292},
ABSTRACT = {Deterioration of bridge infrastructure is a serious concern to transport and government agencies as it declines serviceability and reliability of bridges and jeopardizes public safety. Maintenance and rehabilitation needs of bridge infrastructure are periodically monitored and assessed, typically every two years. Existing inspection techniques, such as visual inspection, are time-consuming, subjective, and often incomplete. Non-destructive testing (NDT) using Unmanned Aerial Vehicles (UAVs) have been gaining momentum for bridge monitoring in the recent years, particularly due to enhanced accessibility and cost efficiency, deterrence of traffic closure, and improved safety during inspection. The primary objective of this study is to conduct a comprehensive review of the application of UAVs in bridge condition monitoring, used in conjunction with remote sensing technologies. Remote sensing technologies such as visual imagery, infrared thermography, LiDAR, and other sensors, integrated with UAVs for data acquisition are analyzed in depth. This study compiled sixty-five journal and conference papers published in the last two decades scrutinizing NDT-based UAV systems. In addition to comparison of stand-alone and integrated NDT-UAV methods, the facilitation of bridge inspection using UAVs is thoroughly discussed in the present article in terms of ease of use, accuracy, cost-efficiency, employed data collection tools, and simulation platforms. Additionally, challenges and future perspectives of the reviewed UAV-NDT technologies are highlighted.},
DOI = {10.3390/rs13091809}
}



@Article{agronomy11050915,
AUTHOR = {Muharam, Farrah Melissa and Nurulhuda, Khairudin and Zulkafli, Zed and Tarmizi, Mohamad Arif and Abdullah, Asniyani Nur Haidar and Che Hashim, Muhamad Faiz and Mohd Zad, Siti Najja and Radhwane, Derraz and Ismail, Mohd Razi},
TITLE = {UAV- and Random-Forest-AdaBoost (RFA)-Based Estimation of Rice Plant Traits},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {915},
URL = {https://www.mdpi.com/2073-4395/11/5/915},
ISSN = {2073-4395},
ABSTRACT = {Rapid, accurate and inexpensive methods are required to analyze plant traits throughout all crop growth stages for plant phenotyping. Few studies have comprehensively evaluated plant traits from multispectral cameras onboard UAV platforms. Additionally, machine learning algorithms tend to over- or underfit data and limited attention has been paid to optimizing their performance through an ensemble learning approach. This study aims to (1) comprehensively evaluate twelve rice plant traits estimated from aerial unmanned vehicle (UAV)-based multispectral images and (2) introduce Random Forest AdaBoost (RFA) algorithms as an optimization approach for estimating plant traits. The approach was tested based on a farmer’s field in Terengganu, Malaysia, for the off-season from February to June 2018, involving five rice cultivars and three nitrogen (N) rates. Four bands, thirteen indices and Random Forest-AdaBoost (RFA) regression models were evaluated against the twelve plant traits according to the growth stages. Among the plant traits, plant height, green leaf and storage organ biomass, and foliar nitrogen (N) content were estimated well, with a coefficient of determination (R2) above 0.80. In comparing the bands and indices, red, Normalized Difference Vegetation Index (NDVI), Ratio Vegetation Index (RVI), Red-Edge Wide Dynamic Range Vegetation Index (REWDRVI) and Red-Edge Soil Adjusted Vegetation Index (RESAVI) were remarkable in estimating all plant traits at tillering, booting and milking stages with R2 values ranging from 0.80–0.99 and root mean square error (RMSE) values ranging from 0.04–0.22. Milking was found to be the best growth stage to conduct estimations of plant traits. In summary, our findings demonstrate that an ensemble learning approach can improve the accuracy as well as reduce under/overfitting in plant phenotyping algorithms.},
DOI = {10.3390/agronomy11050915}
}



@Article{agriculture11050420,
AUTHOR = {Chen, Shuo and Zhang, Kefei and Zhao, Yindi and Sun, Yaqin and Ban, Wei and Chen, Yu and Zhuang, Huifu and Zhang, Xuewei and Liu, Jinxiang and Yang, Tao},
TITLE = {An Approach for Rice Bacterial Leaf Streak Disease Segmentation and Disease Severity Estimation},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {420},
URL = {https://www.mdpi.com/2077-0472/11/5/420},
ISSN = {2077-0472},
ABSTRACT = {Rice bacterial leaf streak (BLS) is a serious disease in rice leaves and can seriously affect the quality and quantity of rice growth. Automatic estimation of disease severity is a crucial requirement in agricultural production. To address this, a new method (termed BLSNet) was proposed for rice and BLS leaf lesion recognition and segmentation based on a UNet network in semantic segmentation. An attention mechanism and multi-scale extraction integration were used in BLSNet to improve the accuracy of lesion segmentation. We compared the performance of the proposed network with that of DeepLabv3+ and UNet as benchmark models used in semantic segmentation. It was found that the proposed BLSNet model demonstrated higher segmentation and class accuracy. A preliminary investigation of BLS disease severity estimation was carried out based on our BLS segmentation results, and it was found that the proposed BLSNet method has strong potential to be a reliable automatic estimator of BLS disease severity.},
DOI = {10.3390/agriculture11050420}
}



@Article{rs13091819,
AUTHOR = {Qi, Tianjun and Zhao, Yan and Meng, Xingmin and Chen, Guan and Dijkstra, Tom},
TITLE = {AI-Based Susceptibility Analysis of Shallow Landslides Induced by Heavy Rainfall in Tianshui, China},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1819},
URL = {https://www.mdpi.com/2072-4292/13/9/1819},
ISSN = {2072-4292},
ABSTRACT = {Groups of landslides induced by heavy rainfall are widely distributed on a global basis and they usually result in major losses of human life and economic damage. However, compared with landslides induced by earthquakes, inventories of landslides induced by heavy rainfall are much less common. In this study we used high-precision remote sensing images before and after continuous heavy rainfall in southern Tianshui, China, from 20 June to 25 July 2013, to produce an inventory of 14,397 shallow landslides. Based on the results of landslide inventory, we utilized machine learning and the geographic information system (GIS) to map landslide susceptibility in this area and evaluated the relative weight of various factors affecting landslide development. First, 18 variables related to geomorphic conditions, slope material, geological conditions, and human activities were selected through collinearity analysis; second, 21 selected machine learning models were trained and optimized in the Python environment to evaluate the susceptibility of landslides. The results showed that the ExtraTrees model was the most effective for landslide susceptibility assessment, with an accuracy of 0.91. This predictive ability means that our landslide susceptibility results can be used in the implementation of landslide prevention and mitigation measures in the region. Analysis of the importance of the factors showed that the contribution of slope aspect (SA) was significantly higher than that of the other factors, followed by planar curvature (PLC), distance to river (DR), distance to fault (DTF), normalized difference vehicle index (NDVI), distance to road (DTR), and other factors. We conclude that factors related to geomorphic conditions are principally responsible for controlling landslide susceptibility in the study area.},
DOI = {10.3390/rs13091819}
}



@Article{rs13091828,
AUTHOR = {Wei, Hongjian and Huang, Yingping and Hu, Fuzhi and Zhao, Baigan and Guo, Zhiyang and Zhang, Rui},
TITLE = {Motion Estimation Using Region-Level Segmentation and Extended Kalman Filter for Autonomous Driving},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1828},
URL = {https://www.mdpi.com/2072-4292/13/9/1828},
ISSN = {2072-4292},
ABSTRACT = {Motion estimation is crucial to predict where other traffic participants will be at a certain period of time, and accordingly plan the route of the ego-vehicle. This paper presents a novel approach to estimate the motion state by using region-level instance segmentation and extended Kalman filter (EKF). Motion estimation involves three stages of object detection, tracking and parameter estimate. We first use a region-level segmentation to accurately locate the object region for the latter two stages. The region-level segmentation combines color, temporal (optical flow), and spatial (depth) information as the basis for segmentation by using super-pixels and Conditional Random Field. The optical flow is then employed to track the feature points within the object area. In the stage of parameter estimate, we develop a relative motion model of the ego-vehicle and the object, and accordingly establish an EKF model for point tracking and parameter estimate. The EKF model integrates the ego-motion, optical flow, and disparity to generate optimized motion parameters. During tracking and parameter estimate, we apply edge point constraint and consistency constraint to eliminate outliers of tracking points so that the feature points used for tracking are ensured within the object body and the parameter estimates are refined by inner points. Experiments have been conducted on the KITTI dataset, and the results demonstrate that our method presents excellent performance and outperforms the other state-of-the-art methods either in object segmentation and parameter estimate.},
DOI = {10.3390/rs13091828}
}



@Article{ijgi10050316,
AUTHOR = {Alshawabkeh, Yahya and Baik, Ahmad and Miky, Yehia},
TITLE = {Integration of Laser Scanner and Photogrammetry for Heritage BIM Enhancement},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {316},
URL = {https://www.mdpi.com/2220-9964/10/5/316},
ISSN = {2220-9964},
ABSTRACT = {Digital 3D capture and reliable reproduction of architectural features is the first and most difficult step towards defining a heritage BIM. Three-dimensional digital survey technologies, such as TLS and photogrammetry, enable experts to scan buildings with a new level of detail. Challenges in the tracing of parametric objects in a TLS point cloud include the reconstruction of occluded parts, measurement of uncertainties relevant to surface reflectivity, and edge detection and location. In addition to image-based techniques being considered cost effective, highly flexible, and efficient in producing a high-quality 3D textured model, they also provide a better interpretation of surface linear characteristics. This article addresses an architecture survey workflow using photogrammetry and TLS to optimize a point cloud that is sufficient for a reliable HBIM. Fusion-based workflows were proposed during the recording of two heritage sites—the Matbouli House Museum in Historic Jeddah, a UNESCO World Heritage Site; and Asfan Castle. In the Matbouli House Museum building, which is rich with complex architectural features, multi-sensor recording was implemented at different resolutions and levels of detail. The TLS data were used to reconstruct the basic shape of the main structural elements, while the imagery’s superior radiometric data and accessibility were effectively used to enhance the TLS point clouds for improving the geometry, data interpretation, and parametric tracing of irregular objects in the facade. Furthermore, in the workflow that is considered to be the ragged terrain of the Castle of Asfan, here, the TLS point cloud was supplemented with UAV data in the upper building zones where the shadow data originated. Both datasets were registered using an ICP algorithm to scale the photogrammetric data and define their actual position in the construction system. The hybrid scans were imported and processed in the BIM environment. The building components were segmented and classified into regular and irregular surfaces, in order to perform detailed building information modeling of the architectural elements. The proposed workflows demonstrated an appropriate performance in terms of reliable and complete BIM mapping in the complex structures.},
DOI = {10.3390/ijgi10050316}
}



@Article{aerospace8050133,
AUTHOR = {Yoon, Sugjoon and Shin, Dongcho and Choi, Younghoon and Park, Kyungtae},
TITLE = {Development of a Flexible and Expandable UTM Simulator Based on Open Sources and Platforms},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {133},
URL = {https://www.mdpi.com/2226-4310/8/5/133},
ISSN = {2226-4310},
ABSTRACT = {In order to study air traffic control of UAS’s (Unmanned Aerial Systems) in very low altitudes, the UTM (UAS Traffic Management) simulator has to be as flexible and expandable as other research simulators because relevant technologies and regulations are not matured enough at this stage. Available approaches using open sources and platforms are investigated to be used in the UTM simulator. The fundamental rationale for selection is availability of necessary resources to build a UTM simulator. Integration efforts to build a UTM simulator are elaborated, using Ardupilot, MavProxi, Cesium, and VWorld, which are selected from the thorough field study. Design requirements of a UTM simulator are determined by analyzing UTM services defined by NASA (National Aeronautics and Space Administration) and Eurocontrol. The UTM simulator, named eUTM, is composed of three components: UOS (UTM Operating System), UTM, and multiple GCSs (Ground Control Stations). GCSs are responsible for generation of flight paths of various UASs. UTM component copies functions of a real UTM such as monitoring and controlling air spaces. UOS provides simulation of environment such as weather, and controls the whole UTM simulator system. UOS also generates operation scenarios of UTM, and resides on the same UTM computer as an independent process. Two GCS simulators are connected to the UTM simulator in the present configuration, but the UTM simulator can be expanded to include up to 10 GCS simulators in the present design. In order to demonstrate the flexibility and expandability of eUTM simulator, several operation scenarios are realized and typical deconfliction scenarios among them are tested with a deconfliction algorithm. During the study, some limits are identified with applied open sources and platforms, which have to be resolved in order to obtain a flexible and expandable UTM simulator supporting relevant studies. Most of them are related to interfacing individual sources and platforms which use different program languages and communication drivers.},
DOI = {10.3390/aerospace8050133}
}



@Article{app11094263,
AUTHOR = {Ni, Minna and Sun, Zhihong and Luo, Yuhan and Yi, Qi and Zhang, Yiqing and Wang, Zhongyi},
TITLE = {Evaluation Model of Parking Equipment Planning and Design Based on Object-Oriented Technology},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4263},
URL = {https://www.mdpi.com/2076-3417/11/9/4263},
ISSN = {2076-3417},
ABSTRACT = {Stereo parking equipment has become an important means to solve the problem of parking difficulties, so it is necessary to study the planning of stereo parking equipment. This paper proposes an evaluation model for parking equipment planning and design, and verifies the feasibility of the model through an example. First, obtain the surface information of the planned area through object-oriented technology, and then complete the design layout of the area that can accommodate the most parking spaces according to the plan information map of the study area. Next, calculate the number of parking spaces required for each building in the area, and the number of available parking spaces within the maximum acceptable time for each building. Finally, compare the two to design the number and location of parking equipment. This method can quickly and accurately obtain the ground plane information map of the study area, while ensuring the capacity of parking spaces to meet the needs of users, it also improves the rationality and suitability of the planning and layout of stereo parking equipment, which can effectively guide the planning and construction of urban parking equipment.},
DOI = {10.3390/app11094263}
}



@Article{s21093262,
AUTHOR = {Mahmud, Md Sultan and Zahid, Azlan and He, Long and Martin, Phillip},
TITLE = {Opportunities and Possibilities of Developing an Advanced Precision Spraying System for Tree Fruits},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {3262},
URL = {https://www.mdpi.com/1424-8220/21/9/3262},
PubMedID = {34066785},
ISSN = {1424-8220},
ABSTRACT = {Reducing risk from pesticide applications has been gaining serious attention in the last few decades due to the significant damage to human health, environment, and ecosystems. Pesticide applications are an essential part of current agriculture, enhancing cultivated crop productivity and quality and preventing losses of up to 45% of the world food supply. However, inappropriate and excessive use of pesticides is a major rising concern. Precision spraying addresses these concerns by precisely and efficiently applying pesticides to the target area and substantially reducing pesticide usage while maintaining efficacy at preventing crop losses. This review provides a systematic summary of current technologies used for precision spraying in tree fruits and highlights their potential, briefly discusses factors affecting spraying parameters, and concludes with possible solutions to reduce excessive agrochemical uses. We conclude there is a critical need for appropriate sensing techniques that can accurately detect the target. In addition, air jet velocity, travel speed, wind speed and direction, droplet size, and canopy characteristics need to be considered for successful droplet deposition by the spraying system. Assessment of terrain is important when field elevation has significant variability. Control of airflow during spraying is another important parameter that needs to be considered. Incorporation of these variables in precision spraying systems will optimize spray decisions and help reduce excessive agrochemical applications.},
DOI = {10.3390/s21093262}
}



@Article{drones5020037,
AUTHOR = {Wei, Bingsheng and Barczyk, Martin},
TITLE = {Experimental Evaluation of Computer Vision and Machine Learning-Based UAV Detection and Ranging},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {37},
URL = {https://www.mdpi.com/2504-446X/5/2/37},
ISSN = {2504-446X},
ABSTRACT = {We consider the problem of vision-based detection and ranging of a target UAV using the video feed from a monocular camera onboard a pursuer UAV. Our previously published work in this area employed a cascade classifier algorithm to locate the target UAV, which was found to perform poorly in complex background scenes. We thus study the replacement of the cascade classifier algorithm with newer machine learning-based object detection algorithms. Five candidate algorithms are implemented and quantitatively tested in terms of their efficiency (measured as frames per second processing rate), accuracy (measured as the root mean squared error between ground truth and detected location), and consistency (measured as mean average precision) in a variety of flight patterns, backgrounds, and test conditions. Assigning relative weights of 20%, 40% and 40% to these three criteria, we find that when flying over a white background, the top three performers are YOLO v2 (76.73 out of 100), Faster RCNN v2 (63.65 out of 100), and Tiny YOLO (59.50 out of 100), while over a realistic background, the top three performers are Faster RCNN v2 (54.35 out of 100, SSD MobileNet v1 (51.68 out of 100) and SSD Inception v2 (50.72 out of 100), leading us to recommend Faster RCNN v2 as the recommended solution. We then provide a roadmap for further work in integrating the object detector into our vision-based UAV tracking system.},
DOI = {10.3390/drones5020037}
}



@Article{rs13091850,
AUTHOR = {Guffogg, Jenna A. and Soto-Berelov, Mariela and Jones, Simon D. and Bellman, Chris J. and Lavers, Jennifer L. and Skidmore, Andrew K.},
TITLE = {Towards the Spectral Mapping of Plastic Debris on Beaches},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1850},
URL = {https://www.mdpi.com/2072-4292/13/9/1850},
ISSN = {2072-4292},
ABSTRACT = {Floating and washed ashore marine plastic debris (MPD) is a growing environmental challenge. It has become evident that secluded locations including the Arctic, Antarctic, and remote islands are being impacted by plastic pollution generated thousands of kilometers away. Optical remote sensing of MPD is an emerging field that can aid in monitoring remote environments where in-person observation and data collection is not always feasible. Here we evaluate MPD spectral features in the visible to shortwave infrared regions for detecting varying quantities of MPD that have accumulated on beaches using a spectroradiometer. Measurements were taken from a range of in situ MPD accumulations ranging from 0.08% to 7.94% surface coverage. Our results suggest that spectral absorption features at 1215 nm and 1732 nm are useful for detecting varying abundance levels of MPD in a complex natural environment, however other absorption features at 931 nm, 1045 nm and 2046 nm could not detect in situ MPD. The reflectance of some in situ MPD accumulations was statistically different from samples that only contained organic debris and sand between 1.56% and 7.94% surface cover; however other samples with similar surface cover did not have reflectance that was statistically different from samples containing no MPD. Despite MPD being detectable against a background of sand and organic beach debris, a clear relationship between the surface cover of MPD and the strength of key absorption features could not be established. Additional research is needed to advance our understanding of the factors, such as type of MPD assemblage, that contribute to the bulk reflectance of MPD contaminated landscapes.},
DOI = {10.3390/rs13091850}
}



@Article{app11094292,
AUTHOR = {Moreno-Revelo, Mónica Y. and Guachi-Guachi, Lorena and Gómez-Mendoza, Juan Bernardo and Revelo-Fuelagán, Javier and Peluffo-Ordóñez, Diego H.},
TITLE = {Enhanced Convolutional-Neural-Network Architecture for Crop Classification},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4292},
URL = {https://www.mdpi.com/2076-3417/11/9/4292},
ISSN = {2076-3417},
ABSTRACT = {Automatic crop identification and monitoring is a key element in enhancing food production processes as well as diminishing the related environmental impact. Although several efficient deep learning techniques have emerged in the field of multispectral imagery analysis, the crop classification problem still needs more accurate solutions. This work introduces a competitive methodology for crop classification from multispectral satellite imagery mainly using an enhanced 2D convolutional neural network (2D-CNN) designed at a smaller-scale architecture, as well as a novel post-processing step. The proposed methodology contains four steps: image stacking, patch extraction, classification model design (based on a 2D-CNN architecture), and post-processing. First, the images are stacked to increase the number of features. Second, the input images are split into patches and fed into the 2D-CNN model. Then, the 2D-CNN model is constructed within a small-scale framework, and properly trained to recognize 10 different types of crops. Finally, a post-processing step is performed in order to reduce the classification error caused by lower-spatial-resolution images. Experiments were carried over the so-named Campo Verde database, which consists of a set of satellite images captured by Landsat and Sentinel satellites from the municipality of Campo Verde, Brazil. In contrast to the maximum accuracy values reached by remarkable works reported in the literature (amounting to an overall accuracy of about 81%, a f1 score of 75.89%, and average accuracy of 73.35%), the proposed methodology achieves a competitive overall accuracy of 81.20%, a f1 score of 75.89%, and an average accuracy of 88.72% when classifying 10 different crops, while ensuring an adequate trade-off between the number of multiply-accumulate operations (MACs) and accuracy. Furthermore, given its ability to effectively classify patches from two image sequences, this methodology may result appealing for other real-world applications, such as the classification of urban materials.},
DOI = {10.3390/app11094292}
}



@Article{app11094294,
AUTHOR = {Loss, Theresa and Bergmann, Alexander},
TITLE = {Vibration-Based Fingerprint Algorithm for Structural Health Monitoring of Wind Turbine Blades},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {4294},
URL = {https://www.mdpi.com/2076-3417/11/9/4294},
ISSN = {2076-3417},
ABSTRACT = {Monitoring the structural health of wind turbine blades is essential to increase energy capture and operational safety of turbines, and therewith enhance competitiveness of wind energy. With the current trends of designing blades ever longer, detailed knowledge of the vibrational characteristics at any point along the blade is desirable. In our approach, we monitor vibrations during operation of the turbine by wirelessly measuring accelerations on the outside of the blades. We propose an algorithm to extract so-called vibration-based fingerprints from those measurements, i.e., dominant vibrations such as eigenfrequencies and narrow-band noise. These fingerprints can then be used for subsequent analysis and visualisation, e.g., for comparing fingerprints across several sensor positions and for identifying vibrations as global or local properties. In this study, data were collected by sensors on two test turbines and fingerprints were successfully extracted for vibrations with both low and high operational variability. An analysis of sensors on the same blade indicates that fingerprints deviate for positions at large radial distance or at different blade sides and, hence, an evaluation with larger datasets of sensors at different positions is promising. In addition, the results show that distributed measurements on the blades are needed to gain a detailed understanding of blade vibrations and thereby reduce loads, increase energy harvesting and improve future blade design. In doing so, our method provides a tool for analysing vibrations with relation to environmental and operational variability in a comprehensive manner.},
DOI = {10.3390/app11094294}
}



@Article{rs13091853,
AUTHOR = {Jin, Xing and Tang, Ping and Zhang, Zheng},
TITLE = {Sequence Image Datasets Construction via Deep Convolution Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1853},
URL = {https://www.mdpi.com/2072-4292/13/9/1853},
ISSN = {2072-4292},
ABSTRACT = {Remote-sensing time-series datasets are significant for global change research and a better understanding of the Earth. However, remote-sensing acquisitions often provide sparse time series due to sensor resolution limitations and environmental factors such as cloud noise for optical data. Image transformation is the method that is often used to deal with this issue. This paper considers the deep convolution networks to learn the complex mapping between sequence images, called adaptive filter generation network (AdaFG), convolution long short-term memory network (CLSTM), and cycle-consistent generative adversarial network (CyGAN) for construction of sequence image datasets. AdaFG network uses a separable 1D convolution kernel instead of 2D kernels to capture the spatial characteristics of input sequence images and then is trained end-to-end using sequence images. CLSTM network can map between different images using the state information of multiple time-series images. CyGAN network can map an image from a source domain to a target domain without additional information. Our experiments, which were performed with unmanned aerial vehicle (UAV) and Landsat-8 datasets, show that the deep convolution networks are effective to produce high-quality time-series image datasets, and the data-driven deep convolution networks can better simulate complex and diverse nonlinear data information.},
DOI = {10.3390/rs13091853}
}



@Article{agriculture11050431,
AUTHOR = {Cheng, Zhenzhen and Qi, Lijun and Cheng, Yifan},
TITLE = {Cherry Tree Crown Extraction from Natural Orchard Images with Complex Backgrounds},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {431},
URL = {https://www.mdpi.com/2077-0472/11/5/431},
ISSN = {2077-0472},
ABSTRACT = {Highly effective pesticide applications require a continual adjustment of the pesticide spray flow rate that attends to different canopy characterizations. Real-time image processing with rapid target detection and data-processing technologies is vital for precision pesticide application. However, the extant studies do not provide an efficient and reliable method of extracting individual trees with irregular tree-crown shapes and complicated backgrounds. This paper on our study proposes a Mahalanobis distance and conditional random field (CRF)-based segmentation model to extract cherry trees accurately in a natural orchard environment. This study computed Mahalanobis distance from the image’s color, brightness and location features to acquire an initial classification of the canopy and background. A CRF was then created by using the Mahalanobis distance calculations as unary potential energy and the Gaussian kernel function based on the image color and pixels distance as binary potential energy. Finally, the study completed image segmentation using mean-field approximation. The results show that the proposed method displays a higher accuracy rate than the traditional algorithms K-means and GrabCut algorithms and lower labeling and training costs than the deep learning algorithm DeepLabv3+, with 92.1%, 94.5% and 93.3% of the average P, R and F1-score, respectively. Moreover, experiments on datasets with different overlap conditions and image acquisition times, as well as in different years and seasons, show that this method performs well under complex background conditions, with an average F1-score higher than 87.7%.},
DOI = {10.3390/agriculture11050431}
}



@Article{rs13091854,
AUTHOR = {Bashir, Syed Muhammad Arsalan and Wang, Yi},
TITLE = {Small Object Detection in Remote Sensing Images with Residual Feature Aggregation-Based Super-Resolution and Object Detector Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1854},
URL = {https://www.mdpi.com/2072-4292/13/9/1854},
ISSN = {2072-4292},
ABSTRACT = {This paper deals with detecting small objects in remote sensing images from satellites or any aerial vehicle by utilizing the concept of image super-resolution for image resolution enhancement using a deep-learning-based detection method. This paper provides a rationale for image super-resolution for small objects by improving the current super-resolution (SR) framework by incorporating a cyclic generative adversarial network (GAN) and residual feature aggregation (RFA) to improve detection performance. The novelty of the method is threefold: first, a framework is proposed, independent of the final object detector used in research, i.e., YOLOv3 could be replaced with Faster R-CNN or any object detector to perform object detection; second, a residual feature aggregation network was used in the generator, which significantly improved the detection performance as the RFA network detected complex features; and third, the whole network was transformed into a cyclic GAN. The image super-resolution cyclic GAN with RFA and YOLO as the detection network is termed as SRCGAN-RFA-YOLO, which is compared with the detection accuracies of other methods. Rigorous experiments on both satellite images and aerial images (ISPRS Potsdam, VAID, and Draper Satellite Image Chronology datasets) were performed, and the results showed that the detection performance increased by using super-resolution methods for spatial resolution enhancement; for an IoU of 0.10, AP of 0.7867 was achieved for a scale factor of 16.},
DOI = {10.3390/rs13091854}
}



@Article{app11104332,
AUTHOR = {Park, Gun and Lee, Jae Hyuk and Yoon, Hyungchul},
TITLE = {Semantic Structure from Motion for Railroad Bridges Using Deep Learning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4332},
URL = {https://www.mdpi.com/2076-3417/11/10/4332},
ISSN = {2076-3417},
ABSTRACT = {Current maintenance practices consume significant time, cost, and manpower. Thus, a new technique for maintenance is required. Construction information technologies, including building information modeling (BIM), have recently been applied to the field to carry out systematic and productive planning, design, construction, and maintenance. Although BIM is increasingly being applied to new structures, its application to existing structures has been limited. To apply BIM to an existing structure, a three-dimensional (3D) model of the structure that accurately represents the as-is status should be constructed and each structural component should be specified manually. This study proposes a method that constructs a 3D model and specifies the structural component automatically using photographic data with a camera installed on an unmanned aerial vehicle. This procedure is referred to as semantic structure from motion because it constructs a 3D point cloud model together with semantic information. A validation test was carried out on a railroad bridge to validate the performance of the proposed system. The average precision, intersection over union, and BF scores were 80.87%, 66.66%, and 56.33%, respectively. The proposed method could improve the current scan-to-BIM procedure by generating the as-is 3D point cloud model by specifying the structural component automatically.},
DOI = {10.3390/app11104332}
}



@Article{s21103313,
AUTHOR = {Sobczak, Łukasz and Filus, Katarzyna and Domański, Adam and Domańska, Joanna},
TITLE = {LiDAR Point Cloud Generation for SLAM Algorithm Evaluation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3313},
URL = {https://www.mdpi.com/1424-8220/21/10/3313},
PubMedID = {34064712},
ISSN = {1424-8220},
ABSTRACT = {With the emerging interest in the autonomous driving level at 4 and 5 comes a necessity to provide accurate and versatile frameworks to evaluate the algorithms used in autonomous vehicles. There is a clear gap in the field of autonomous driving simulators. It covers testing and parameter tuning of a key component of autonomous driving systems, SLAM, frameworks targeting off-road and safety-critical environments. It also includes taking into consideration the non-idealistic nature of the real-life sensors, associated phenomena and measurement errors. We created a LiDAR simulator that delivers accurate 3D point clouds in real time. The point clouds are generated based on the sensor placement and the LiDAR type that can be set using configurable parameters. We evaluate our solution based on comparison of the results using an actual device, Velodyne VLP-16, on real-life tracks and the corresponding simulations. We measure the error values obtained using Google Cartographer SLAM algorithm and the distance between the simulated and real point clouds to verify their accuracy. The results show that our simulation (which incorporates measurement errors and the rolling shutter effect) produces data that can successfully imitate the real-life point clouds. Due to dedicated mechanisms, it is compatible with the Robotic Operating System (ROS) and can be used interchangeably with data from actual sensors, which enables easy testing, SLAM algorithm parameter tuning and deployment.},
DOI = {10.3390/s21103313}
}



@Article{rs13101868,
AUTHOR = {Deur, Martina and Gašparović, Mateo and Balenović, Ivan},
TITLE = {An Evaluation of Pixel- and Object-Based Tree Species Classification in Mixed Deciduous Forests Using Pansharpened Very High Spatial Resolution Satellite Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1868},
URL = {https://www.mdpi.com/2072-4292/13/10/1868},
ISSN = {2072-4292},
ABSTRACT = {Quality tree species information gathering is the basis for making proper decisions in forest management. By applying new technologies and remote sensing methods, very high resolution (VHR) satellite imagery can give sufficient spatial detail to achieve accurate species-level classification. In this study, the influence of pansharpening of the WorldView-3 (WV-3) satellite imagery on classification results of three main tree species (Quercus robur L., Carpinus betulus L., and Alnus glutinosa (L.) Geartn.) has been evaluated. In order to increase tree species classification accuracy, three different pansharpening algorithms (Bayes, RCS, and LMVM) have been conducted. The LMVM algorithm proved the most effective pansharpening technique. The pixel- and object-based classification were applied to three pansharpened imageries using a random forest (RF) algorithm. The results showed a very high overall accuracy (OA) for LMVM pansharpened imagery: 92% and 96% for tree species classification based on pixel- and object-based approach, respectively. As expected, the object-based exceeded the pixel-based approach (OA increased by 4%). The influence of fusion on classification results was analyzed as well. Overall classification accuracy was improved by the spatial resolution of pansharpened images (OA increased by 7% for pixel-based approach). Also, regardless of pixel- or object-based classification approaches, the influence of the use of pansharpening is highly beneficial to classifying complex, natural, and mixed deciduous forest areas.},
DOI = {10.3390/rs13101868}
}



@Article{w13101333,
AUTHOR = {Lama, Giuseppe Francesco Cesare and Crimaldi, Mariano and Pasquino, Vittorio and Padulano, Roberta and Chirico, Giovanni Battista},
TITLE = {Bulk Drag Predictions of Riparian Arundo donax Stands through UAV-Acquired Multispectral Images},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1333},
URL = {https://www.mdpi.com/2073-4441/13/10/1333},
ISSN = {2073-4441},
ABSTRACT = {Estimating the main hydrodynamic features of real vegetated water bodies is crucial to assure a balance between their hydraulic conveyance and environmental quality. Riparian vegetation stands have a high impact on vegetated channels. The present work has the aim to integrate riparian vegetation’s reflectance indices and hydrodynamics of real vegetated water flows to assess the impact of riparian vegetation morphometry on bulk drag coefficients distribution along an abandoned vegetated drainage channel fully covered by 9–10 m high Arundo donax (commonly known as giant reed) stands, starting from flow average velocities measurements at 30 cross-sections identified along the channel. A map of riparian vegetation cover was obtained through digital processing of Unnamed Aerial Vehicle (UAV)-acquired multispectral images, which represent a fast way to observe riparian plants’ traits in hardly accessible areas such as vegetated water bodies in natural conditions. In this study, the portion of riparian plants effectively interacting with flow was expressed in terms of ground-based Leaf Area Index measurements (LAI), which easily related to UAV-based Normalized Difference Vegetation Index (NDVI). The comparative analysis between Arundo donax stands NDVI and LAI map enabled the analysis of the impact of UAV-acquired multispectral imagery on bulk drag predictions along the vegetated drainage channel.},
DOI = {10.3390/w13101333}
}



@Article{rs13101869,
AUTHOR = {Mattivi, Pietro and Pappalardo, Salvatore Eugenio and Nikolić, Nebojša and Mandolesi, Luca and Persichetti, Antonio and De Marchi, Massimo and Masin, Roberta},
TITLE = {Can Commercial Low-Cost Drones and Open-Source GIS Technologies Be Suitable for Semi-Automatic Weed Mapping for Smart Farming? A Case Study in NE Italy},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1869},
URL = {https://www.mdpi.com/2072-4292/13/10/1869},
ISSN = {2072-4292},
ABSTRACT = {Weed management is a crucial issue in agriculture, resulting in environmental in-field and off-field impacts. Within Agriculture 4.0, adoption of UASs combined with spatially explicit approaches may drastically reduce doses of herbicides, increasing sustainability in weed management. However, Agriculture 4.0 technologies are barely adopted in small-medium size farms. Recently, small and low-cost UASs, together with open-source software packages, may represent a low-cost spatially explicit system to map weed distribution in crop fields. The general aim is to map weed distribution by a low-cost UASs and a replicable workflow, completely based on open GIS software and algorithms: OpenDroneMap, QGIS, SAGA and OpenCV classification algorithms. Specific objectives are: (i) testing a low-cost UAS for weed mapping; (ii) assessing open-source packages for semi-automatic weed classification; (iii) performing a sustainable management scenario by prescription maps. Results showed high performances along the whole process: in orthomosaic generation at very high spatial resolution (0.01 m/pixel), in testing weed detection (Matthews Correlation Coefficient: 0.67–0.74), and in the production of prescription maps, reducing herbicide treatment to only 3.47% of the entire field. This study reveals the feasibility of low-cost UASs combined with open-source software, enabling a spatially explicit approach for weed management in small-medium size farmlands.},
DOI = {10.3390/rs13101869}
}



@Article{s21103317,
AUTHOR = {Wu, Xiaotian and Li, Jiongcheng and Zhou, Guanxing and Lü, Bo and Li, Qingqing and Yang, Hang},
TITLE = {RRG-GAN Restoring Network for Simple Lens Imaging System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3317},
URL = {https://www.mdpi.com/1424-8220/21/10/3317},
PubMedID = {34064779},
ISSN = {1424-8220},
ABSTRACT = {The simple lens computational imaging method provides an alternative way to achieve high-quality photography. It simplifies the design of the optical-front-end to a single-convex-lens and delivers the correction of optical aberration to a dedicated computational restoring algorithm. Traditional single-convex-lens image restoration is based on optimization theory, which has some shortcomings in efficiency and efficacy. In this paper, we propose a novel Recursive Residual Groups network under Generative Adversarial Network framework (RRG-GAN) to generate a clear image from the aberrations-degraded blurry image. The RRG-GAN network includes dual attention module, selective kernel network module, and residual resizing module to make it more suitable for the non-uniform deblurring task. To validate the evaluation algorithm, we collect sharp/aberration-degraded datasets by CODE V simulation. To test the practical application performance, we built a display-capture lab setup and reconstruct a manual registering dataset. Relevant experimental comparisons and actual tests verify the effectiveness of our proposed method.},
DOI = {10.3390/s21103317}
}



@Article{agronomy11050952,
AUTHOR = {Duarte, Lia and Teodoro, Ana Cláudia and Sousa, Joaquim J. and Pádua, Luís},
TITLE = {QVigourMap: A GIS Open Source Application for the Creation of Canopy Vigour Maps},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {952},
URL = {https://www.mdpi.com/2073-4395/11/5/952},
ISSN = {2073-4395},
ABSTRACT = {In a precision agriculture context, the amount of geospatial data available can be difficult to interpret in order to understand the crop variability within a given terrain parcel, raising the need for specific tools for data processing and analysis. This is the case for data acquired from Unmanned Aerial Vehicles (UAV), in which the high spatial resolution along with data from several spectral wavelengths makes data interpretation a complex process regarding vegetation monitoring. Vegetation Indices (VIs) are usually computed, helping in the vegetation monitoring process. However, a crop plot is generally composed of several non-crop elements, which can bias the data analysis and interpretation. By discarding non-crop data, it is possible to compute the vigour distribution for a specific crop within the area under analysis. This article presents QVigourMaps, a new open source application developed to generate useful outputs for precision agriculture purposes. The application was developed in the form of a QGIS plugin, allowing the creation of vigour maps, vegetation distribution maps and prescription maps based on the combination of different VIs and height information. Multi-temporal data from a vineyard plot and a maize field were used as case studies in order to demonstrate the potential and effectiveness of the QVigourMaps tool. The presented application can contribute to making the right management decisions by providing indicators of crop variability, and the outcomes can be used in the field to apply site-specific treatments according to the levels of vigour.},
DOI = {10.3390/agronomy11050952}
}



@Article{s21103374,
AUTHOR = {Liu, Hansen and Fan, Kuangang and Ouyang, Qinghua and Li, Na},
TITLE = {Real-Time Small Drones Detection Based on Pruned YOLOv4},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3374},
URL = {https://www.mdpi.com/1424-8220/21/10/3374},
PubMedID = {34066267},
ISSN = {1424-8220},
ABSTRACT = {To address the threat of drones intruding into high-security areas, the real-time detection of drones is urgently required to protect these areas. There are two main difficulties in real-time detection of drones. One of them is that the drones move quickly, which leads to requiring faster detectors. Another problem is that small drones are difficult to detect. In this paper, firstly, we achieve high detection accuracy by evaluating three state-of-the-art object detection methods: RetinaNet, FCOS, YOLOv3 and YOLOv4. Then, to address the first problem, we prune the convolutional channel and shortcut layer of YOLOv4 to develop thinner and shallower models. Furthermore, to improve the accuracy of small drone detection, we implement a special augmentation for small object detection by copying and pasting small drones. Experimental results verify that compared to YOLOv4, our pruned-YOLOv4 model, with 0.8 channel prune rate and 24 layers prune, achieves 90.5% mAP and its processing speed is increased by 60.4%. Additionally, after small object augmentation, the precision and recall of the pruned-YOLOv4 almost increases by 22.8% and 12.7%, respectively. Experiment results verify that our pruned-YOLOv4 is an effective and accurate approach for drone detection.},
DOI = {10.3390/s21103374}
}



@Article{s21103389,
AUTHOR = {Quan, Longzhe and Wu, Bing and Mao, Shouren and Yang, Chunjie and Li, Hengda},
TITLE = {An Instance Segmentation-Based Method to Obtain the Leaf Age and Plant Centre of Weeds in Complex Field Environments},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3389},
URL = {https://www.mdpi.com/1424-8220/21/10/3389},
PubMedID = {34068108},
ISSN = {1424-8220},
ABSTRACT = {Leaf age and plant centre are important phenotypic information of weeds, and accurate identification of them plays an important role in understanding the morphological structure of weeds, guiding precise targeted spraying and reducing the use of herbicides. In this work, a weed segmentation method based on BlendMask is proposed to obtain the phenotypic information of weeds under complex field conditions. This study collected images from different angles (front, side, and top views) of three kinds of weeds (Solanum nigrum, barnyard grass (Echinochloa crus-galli), and Abutilon theophrasti Medicus) in a maize field. Two datasets (with and without data enhancement) and two backbone networks (ResNet50 and ResNet101) were replaced to improve model performance. Finally, seven evaluation indicators are used to evaluate the segmentation results of the model under different angles. The results indicated that data enhancement and ResNet101 as the backbone network could enhance the model performance. The F1 value of the plant centre is 0.9330, and the recognition accuracy of leaf age can reach 0.957. The mIOU value of the top view is 0.642. Therefore, deep learning methods can effectively identify weed leaf age and plant centre, which is of great significance for variable spraying.},
DOI = {10.3390/s21103389}
}



@Article{s21103407,
AUTHOR = {He, Wei and Yang, Dehang and Peng, Haoqi and Liang, Songhong and Lin, Yingcheng},
TITLE = {An Efficient Ensemble Binarized Deep Neural Network on Chip with Perception-Control Integrated},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3407},
URL = {https://www.mdpi.com/1424-8220/21/10/3407},
PubMedID = {34068351},
ISSN = {1424-8220},
ABSTRACT = {Lightweight UAVs equipped with deep learning models have become a trend, which can be deployed for automatic navigation in a wide range of civilian and military missions. However, real-time applications usually need to process a large amount of image data, which leads to a very large computational complexity and storage consumption, and restricts its deployment on resource-constrained embedded edge devices. To reduce the computing requirements and storage occupancy of the neural network model, we proposed the ensemble binarized DroNet (EBDN) model, which implemented the reconstructed DroNet with the binarized and ensemble learning method, so that the model size of DroNet was effectively compressed, and ensemble learning method was used to overcome the defect of the poor performance of the low-precision network. Compared to the original DroNet, EBDN saves more than 7 times of memory footprint with similar model accuracy. Meanwhile, we also proposed a novel and high-efficiency hardware architecture to realize the EBDN on the chip (EBDNoC) system, which perfectly realizes the mapping of an algorithm model to hardware architecture. Compared to other solutions, the proposed architecture achieves about 10.21 GOP/s/kLUTs resource efficiency and 208.1 GOP/s/W energy efficiency, while also providing a good trade-off between model performance and resource utilization.},
DOI = {10.3390/s21103407}
}



@Article{su13105478,
AUTHOR = {Liu, Boda and Yang, Bin and Xiao, Jianzhuang and Zhu, Dayu and Zhang, Binghan and Wang, Zhichen and Dong, Miaosi},
TITLE = {Review of Optimization Dynamically Applied in the Construction and the Application Potential of ICT},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {5478},
URL = {https://www.mdpi.com/2071-1050/13/10/5478},
ISSN = {2071-1050},
ABSTRACT = {Currently, construction projects are getting more complex, applying more information and communication technologies (ICT), while few studies use real-time data to dynamically optimize construction. The purpose of this article is to study the current development status of the optimization applied dynamically in the construction phase and their potential for applying real data collected by ICT. This article reviews 72 relevant optimization methods and identified some of the ICT research studies that can provide them with dynamic data. The dynamic triggering mode of each research is first analyzed, then its dynamic way, dynamic data, data resource, optimization object, and method are identified and formulated. The results reveal the great value of dynamic optimization in dealing with the complicated and uncertain contextual conditions in construction. Different dynamic triggering modes have different affinities with real data. Then, through the analysis of ICT articles, the huge potential of these dynamic optimization methods in applying real data is shown. This paper points out the most practical dynamic mode for engineers or managers to continuously apply optimization methods to solve dynamic problems in construction, and put forward scientific questions for related researchers: How does one combine ICT with the event dynamics or uncertain parameters? Based on this, the research gap of this area is identified a conceptual solution is proposed.},
DOI = {10.3390/su13105478}
}



@Article{s21103411,
AUTHOR = {Ostrowski, Bartłomiej and Pióro, Michał and Tomaszewski, Artur},
TITLE = {Multicast Traffic Throughput Maximization through Joint Dynamic Modulation and Coding Schemes Assignment, and Transmission Power Control in Wireless Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3411},
URL = {https://www.mdpi.com/1424-8220/21/10/3411},
PubMedID = {34068410},
ISSN = {1424-8220},
ABSTRACT = {The paper concerns multicast packet traffic throughput maximization in multi-hop wireless sensor networks with time division multiple access to radio channel. We assume that the modulation and coding schemes (MCSs) that are used by the (broadcasting) nodes as well as the transmission power of the nodes are adjustable. This leads to the main research question studied in this paper: to what extent traffic throughput can be increased by proper MCSs assignment and transmission power control (TPC) at the nodes? To answer this question, we introduce mixed-integer programming formulations for joint MCSs assignment and TPC optimization, together with a solution algorithm. Finally, we present a numerical study illustrating the considerations of the paper. The numerical results show a significant gain being achieved by proper MCSs assignment, which is further increased by applying TPC.},
DOI = {10.3390/s21103411}
}



@Article{s21103416,
AUTHOR = {Burdziakowski, Pawel and Zakrzewska, Angelika},
TITLE = {A New Adaptive Method for the Extraction of Steel Design Structures from an Integrated Point Cloud},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3416},
URL = {https://www.mdpi.com/1424-8220/21/10/3416},
PubMedID = {34068991},
ISSN = {1424-8220},
ABSTRACT = {The continuous and intensive development of measurement technologies for reality modelling with appropriate data processing algorithms is currently being observed. The most popular methods include remote sensing techniques based on reflected-light digital cameras, and on active methods in which the device emits a beam. This research paper presents the process of data integration from terrestrial laser scanning (TLS) and image data from an unmanned aerial vehicle (UAV) that was aimed at the spatial mapping of a complicated steel structure, and a new automatic structure extraction method. We proposed an innovative method to minimize the data size and automatically extract a set of points (in the form of structural elements) that is vital from the perspective of engineering and comparative analyses. The outcome of the research was a complete technology for the acquisition of precise information with regard to complex and high steel structures. The developed technology includes such elements as a data integration method, a redundant data elimination method, integrated photogrammetric data filtration and a new adaptive method of structure edge extraction. In order to extract significant geometric structures, a new automatic and adaptive algorithm for edge extraction from a random point cloud was developed and presented herein. The proposed algorithm was tested using real measurement data. The developed algorithm is able to realistically reduce the amount of redundant data and correctly extract stable edges representing the geometric structures of a studied object without losing important data and information. The new algorithm automatically self-adapts to the received data. It does not require any pre-setting or initial parameters. The detection threshold is also adaptively selected based on the acquired data.},
DOI = {10.3390/s21103416}
}



@Article{app11104493,
AUTHOR = {Jo, Yongwon and Lee, Soobin and Lee, Youngjae and Kahng, Hyungu and Park, Seonghun and Bae, Seounghun and Kim, Minkwan and Han, Sungwon and Kim, Seoungbum},
TITLE = {Semantic Segmentation of Cabbage in the South Korea Highlands with Images by Unmanned Aerial Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4493},
URL = {https://www.mdpi.com/2076-3417/11/10/4493},
ISSN = {2076-3417},
ABSTRACT = {Identifying agricultural fields that grow cabbage in the highlands of South Korea is critical for accurate crop yield estimation. Only grown for a limited time during the summer, highland cabbage accounts for a significant proportion of South Korea’s annual cabbage production. Thus, it has a profound effect on the formation of cabbage prices. Traditionally, labor-extensive and time-consuming field surveys are manually carried out to derive agricultural field maps of the highlands. Recently, high-resolution overhead images of the highlands have become readily available with the rapid development of unmanned aerial vehicles (UAV) and remote sensing technology. In addition, deep learning-based semantic segmentation models have quickly advanced by recent improvements in algorithms and computational resources. In this study, we propose a semantic segmentation framework based on state-of-the-art deep learning techniques to automate the process of identifying cabbage cultivation fields. We operated UAVs and collected 2010 multispectral images under different spatiotemporal conditions to measure how well semantic segmentation models generalize. Next, we manually labeled these images at a pixel-level to obtain ground truth labels for training. Our results demonstrate that our framework performs well in detecting cabbage fields not only in areas included in the training data but also in unseen areas not included in the training data. Moreover, we analyzed the effects of infrared wavelengths on the performance of identifying cabbage fields. Based on the results of our framework, we expect agricultural officials to reduce time and manpower when identifying information about highlands cabbage fields by replacing field surveys.},
DOI = {10.3390/app11104493}
}



@Article{agriculture11050451,
AUTHOR = {Cui, Hongwei and Zhang, Qiang and Zhang, Jinsong and Wu, Zidan and Wu, Wenfu},
TITLE = {Classification of Grain Storage Inventory Modes Based on Temperature Contour Map of Grain Bulk Using Back Propagation Neural Network},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {451},
URL = {https://www.mdpi.com/2077-0472/11/5/451},
ISSN = {2077-0472},
ABSTRACT = {Inventory modes classification can reduce the workload of grain depot management and it is time-saving, not labor-intensive. This paper proposed a method of using a temperature contour map converted from digital temperature data to classify stored grain inventory modes in a large bulk grain warehouse, which mainly included detection of inventory changes and routine operations performed (aeration). The back propagation (BP) neural network was used in this method to identify and classify grain storage inventory modes based on the temperature contour map for helping grain depot management work. The method extracted and combined color coherence vector (CCV), texture feature vector (TFV) and smoothness feature vector (SFV) of temperature contour maps as the input vector of the BP neural network, and used inventory modes as the output vector. The experimental results indicated that the accuracy of the BP neural network with vector (CCV and TFV and SFV) as the input vector was about 93.9%, and its training time and prediction time were 320 and 0.12 s, respectively.},
DOI = {10.3390/agriculture11050451}
}



@Article{su13105548,
AUTHOR = {Awad, Mohamad M. and Lauteri, Marco},
TITLE = {Self-Organizing Deep Learning (SO-UNet)—A Novel Framework to Classify Urban and Peri-Urban Forests},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {5548},
URL = {https://www.mdpi.com/2071-1050/13/10/5548},
ISSN = {2071-1050},
ABSTRACT = {Forest-type classification is a very complex and difficult subject. The complexity increases with urban and peri-urban forests because of the variety of features that exist in remote sensing images. The success of forest management that includes forest preservation depends strongly on the accuracy of forest-type classification. Several classification methods are used to map urban and peri-urban forests and to identify healthy and non-healthy ones. Some of these methods have shown success in the classification of forests where others failed. The successful methods used specific remote sensing data technology, such as hyper-spectral and very high spatial resolution (VHR) images. However, both VHR and hyper-spectral sensors are very expensive, and hyper-spectral sensors are not widely available on satellite platforms, unlike multi-spectral sensors. Moreover, aerial images are limited in use, very expensive, and hard to arrange and manage. To solve the aforementioned problems, an advanced method, self-organizing–deep learning (SO-UNet), was created to classify forests in the urban and peri-urban environment using multi-spectral, multi-temporal, and medium spatial resolution Sentinel-2 images. SO-UNet is a combination of two different machine learning technologies: artificial neural network unsupervised self-organizing maps and deep learning UNet. Many experiments have been conducted, and the results showed that SO-UNet overwhelms UNet significantly. The experiments encompassed different settings for the parameters that control the algorithms.},
DOI = {10.3390/su13105548}
}



@Article{electronics10101193,
AUTHOR = {Saha, Subrata and Vasegaard, Alex Elkjær and Nielsen, Izabela and Hapka, Aneta and Budzisz, Henryk},
TITLE = {UAVs Path Planning under a Bi-Objective Optimization Framework for Smart Cities},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1193},
URL = {https://www.mdpi.com/2079-9292/10/10/1193},
ISSN = {2079-9292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have been used extensively for search and rescue operations, surveillance, disaster monitoring, attacking terrorists, etc. due to their growing advantages of low-cost, high maneuverability, and easy deployability. This study proposes a mixed-integer programming model under a multi-objective optimization framework to design trajectories that enable a set of UAVs to execute surveillance tasks. The first objective maximizes the cumulative probability of target detection to aim for mission planning success. The second objective ensures minimization of cumulative path length to provide a higher resource utilization goal. A two-step variable neighborhood search (VNS) algorithm is offered, which addresses the combinatorial optimization issue for determining the near-optimal sequence for cell visiting to reach the target. Numerical experiments and simulation results are evaluated in numerous benchmark instances. Results demonstrate that the proposed approach can favorably support practical deployability purposes.},
DOI = {10.3390/electronics10101193}
}



@Article{rs13101956,
AUTHOR = {Cong, Jingyu and Wang, Xianpeng and Lan, Xiang and Huang, Mengxing and Wan, Liangtian},
TITLE = {Fast Target Localization Method for FMCW MIMO Radar via VDSR Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1956},
URL = {https://www.mdpi.com/2072-4292/13/10/1956},
ISSN = {2072-4292},
ABSTRACT = {The traditional frequency-modulated continuous wave (FMCW) multiple-input multiple-output (MIMO) radar two-dimensional (2D) super-resolution (SR) estimation algorithm for target localization has high computational complexity, which runs counter to the increasing demand for real-time radar imaging. In this paper, a fast joint direction-of-arrival (DOA) and range estimation framework for target localization is proposed; it utilizes a very deep super-resolution (VDSR) neural network (NN) framework to accelerate the imaging process while ensuring estimation accuracy. Firstly, we propose a fast low-resolution imaging algorithm based on the Nystrom method. The approximate signal subspace matrix is obtained from partial data, and low-resolution imaging is performed on a low-density grid. Then, the bicubic interpolation algorithm is used to expand the low-resolution image to the desired dimensions. Next, the deep SR network is used to obtain the high-resolution image, and the final joint DOA and range estimation is achieved based on the reconstructed image. Simulations and experiments were carried out to validate the computational efficiency and effectiveness of the proposed framework.},
DOI = {10.3390/rs13101956}
}



@Article{ijgi10050345,
AUTHOR = {Chaidas, Konstantinos and Tataris, George and Soulakellis, Nikolaos},
TITLE = {Seismic Damage Semantics on Post-Earthquake LOD3 Building Models Generated by UAS},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {345},
URL = {https://www.mdpi.com/2220-9964/10/5/345},
ISSN = {2220-9964},
ABSTRACT = {In a post-earthquake scenario, the semantic enrichment of 3D building models with seismic damage is crucial from the perspective of disaster management. This paper aims to present the methodology and the results for the Level of Detail 3 (LOD3) building modelling (after an earthquake) with the enrichment of the semantics of the seismic damage based on the European Macroseismic Scale (EMS-98). The study area is the Vrisa traditional settlement on the island of Lesvos, Greece, which was affected by a devastating earthquake of Mw = 6.3 on 12 June 2017. The applied methodology consists of the following steps: (a) unmanned aircraft systems (UAS) nadir and oblique images are acquired and photogrammetrically processed for 3D point cloud generation, (b) 3D building models are created based on 3D point clouds and (c) 3D building models are transformed into a LOD3 City Geography Markup Language (CityGML) standard with enriched semantics of the related seismic damage of every part of the building (walls, roof, etc.). The results show that in following this methodology, CityGML LOD3 models can be generated and enriched with buildings’ seismic damage. These models can assist in the decision-making process during the recovery phase of a settlement as well as be the basis for its monitoring over time. Finally, these models can contribute to the estimation of the reconstruction cost of the buildings.},
DOI = {10.3390/ijgi10050345}
}



@Article{smartcities4020040,
AUTHOR = {Englund, Cristofer and Aksoy, Eren Erdal and Alonso-Fernandez, Fernando and Cooney, Martin Daniel and Pashami, Sepideh and Åstrand, Björn},
TITLE = {AI Perspectives in Smart Cities and Communities to Enable Road Vehicle Automation and Smart Traffic Control},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {2},
PAGES = {783--802},
URL = {https://www.mdpi.com/2624-6511/4/2/40},
ISSN = {2624-6511},
ABSTRACT = {Smart cities and communities (SCC) constitute a new paradigm in urban development. SCC ideate a data-centered society aimed at improving efficiency by automating and optimizing activities and utilities. Information and communication technology along with Internet of Things enables data collection and with the help of artificial intelligence (AI) situation awareness can be obtained to feed the SCC actors with enriched knowledge. This paper describes AI perspectives in SCC and gives an overview of AI-based technologies used in traffic to enable road vehicle automation and smart traffic control. Perception, smart traffic control and driver modeling are described along with open research challenges and standardization to help introduce advanced driver assistance systems and automated vehicle functionality in traffic. To fully realize the potential of SCC, to create a holistic view on a city level, availability of data from different stakeholders is necessary. Further, though AI technologies provide accurate predictions and classifications, there is an ambiguity regarding the correctness of their outputs. This can make it difficult for the human operator to trust the system. Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model. Another challenge related to trust is explainability: models can have difficulty explaining how they came to certain conclusions, so it is difficult for humans to trust them.},
DOI = {10.3390/smartcities4020040}
}



@Article{rs13101969,
AUTHOR = {Chen, Fang and Wang, Ning and Yu, Bo and Qin, Yuchu and Wang, Lei},
TITLE = {A Strategy of Parallel Seed-Based Image Segmentation Algorithms for Handling Massive Image Tiles over the Spark Platform},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1969},
URL = {https://www.mdpi.com/2072-4292/13/10/1969},
ISSN = {2072-4292},
ABSTRACT = {The volume of remote sensing images continues to grow as image sources become more diversified and with increasing spatial and spectral resolution. The handling of such large-volume datasets, which exceed available CPU memory, in a timely and efficient manner is becoming a challenge for single machines. The distributed cluster provides an effective solution with strong calculation power. There has been an increasing number of big data technologies that have been adopted to deal with large images using mature parallel technology. However, since most commercial big data platforms are not specifically developed for the remote sensing field, two main issues exist in processing large images with big data platforms using a distributed cluster. On the one hand, the quantities and categories of official algorithms used to process remote sensing images in big data platforms are limited compared to large amounts of sequential algorithms. On the other hand, the sequential algorithms employed directly to process large images in parallel over a distributed cluster may lead to incomplete objects in the tile edges and the generation of large communication volumes at the shuffle stage. It is, therefore, necessary to explore the distributed strategy and adapt the sequential algorithms over the distributed cluster. In this research, we employed two seed-based image segmentation algorithms to construct a distributed strategy based on the Spark platform. The proposed strategy focuses on modifying the incomplete objects by processing border areas and reducing the communication volume to a reasonable size by limiting the auxiliary bands and the buffer size to a small range during the shuffle stage. We calculated the F-measure and execution time to evaluate the accuracy and execution efficiency. The statistical data reveal that both segmentation algorithms maintained high accuracy, as achieved in the reference image segmented in the sequential way. Moreover, generally the strategy took less execution time compared to significantly larger auxiliary bands and buffer sizes. The proposed strategy can modify incomplete objects, with execution time being twice as fast as the strategies that do not employ communication volume reduction in the distributed cluster.},
DOI = {10.3390/rs13101969}
}



@Article{s21103523,
AUTHOR = {Abbas, Ziaul Haq and Ali, Zaiwar and Abbas, Ghulam and Jiao, Lei and Bilal, Muhammad and Suh, Doug-Young and Piran, Md. Jalil},
TITLE = {Computational Offloading in Mobile Edge with Comprehensive and Energy Efficient Cost Function: A Deep Learning Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3523},
URL = {https://www.mdpi.com/1424-8220/21/10/3523},
PubMedID = {34069364},
ISSN = {1424-8220},
ABSTRACT = {In mobile edge computing (MEC), partial computational offloading can be intelligently investigated to reduce the energy consumption and service delay of user equipment (UE) by dividing a single task into different components. Some of the components execute locally on the UE while the remaining are offloaded to a mobile edge server (MES). In this paper, we investigate the partial offloading technique in MEC using a supervised deep learning approach. The proposed technique, comprehensive and energy efficient deep learning-based offloading technique (CEDOT), intelligently selects the partial offloading policy and also the size of each component of a task to reduce the service delay and energy consumption of UEs. We use deep learning to find, simultaneously, the best partitioning of a single task with the best offloading policy. The deep neural network (DNN) is trained through a comprehensive dataset, generated from our mathematical model, which reduces the time delay and energy consumption of the overall process. Due to the complexity and computation of the mathematical model in the algorithm being high, due to trained DNN the complexity and computation are minimized in the proposed work. We propose a comprehensive cost function, which depends on various delays, energy consumption, radio resources, and computation resources. Furthermore, the cost function also depends on energy consumption and delay due to the task-division-process in partial offloading. None of the literature work considers the partitioning along with the computational offloading policy, and hence, the time and energy consumption due to task-division-process are ignored in the cost function. The proposed work considers all the important parameters in the cost function and generates a comprehensive training dataset with high computation and complexity. Once we get the training dataset, then the complexity is minimized through trained DNN which gives faster decision making with low energy consumptions. Simulation results demonstrate the superior performance of the proposed technique with high accuracy of the DNN in deciding offloading policy and partitioning of a task with minimum delay and energy consumption for UE. More than 70% accuracy of the trained DNN is achieved through a comprehensive training dataset. The simulation results also show the constant accuracy of the DNN when the UEs are moving which means the decision making of the offloading policy and partitioning are not affected by the mobility of UEs.},
DOI = {10.3390/s21103523}
}



@Article{rs13101975,
AUTHOR = {Wang, Lin and Zhou, Yuzhen and Hu, Qiao and Tang, Zhenghong and Ge, Yufeng and Smith, Adam and Awada, Tala and Shi, Yeyin},
TITLE = {Early Detection of Encroaching Woody Juniperus virginiana and Its Classification in Multi-Species Forest Using UAS Imagery and Semantic Segmentation Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1975},
URL = {https://www.mdpi.com/2072-4292/13/10/1975},
ISSN = {2072-4292},
ABSTRACT = {Woody plant encroachment into grasslands ecosystems causes significantly ecological destruction and economic losses. Effective and efficient management largely benefits from accurate and timely detection of encroaching species at an early development stage. Recent advances in unmanned aircraft systems (UAS) enabled easier access to ultra-high spatial resolution images at a centimeter level, together with the latest machine learning based image segmentation algorithms, making it possible to detect small-sized individuals of target species at early development stage and identify them when mixed with other species. However, few studies have investigated the optimal practical spatial resolution of early encroaching species detection. Hence, we investigated the performance of four popular semantic segmentation algorithms (decision tree, DT; random forest, RF; AlexNet; and ResNet) on a multi-species forest classification case with UAS-collected RGB images in original and down-sampled coarser spatial resolutions. The objective of this study was to explore the optimal segmentation algorithm and spatial resolution for eastern redcedar (Juniperus virginiana, ERC) early detection and its classification within a multi-species forest context. To be specific, firstly, we implemented and compared the performance of the four semantic segmentation algorithms with images in the original spatial resolution (0.694 cm). The highest overall accuracy was 0.918 achieved by ResNet with a mean interaction over union at 85.0%. Secondly, we evaluated the performance of ResNet algorithm with images in down-sampled spatial resolutions (1 cm to 5 cm with 0.5 cm interval). When applied on the down-sampled images, ERC segmentation performance decreased with decreasing spatial resolution, especially for those images coarser than 3 cm spatial resolution. The UAS together with the state-of-the-art semantic segmentation algorithms provides a promising tool for early-stage detection and localization of ERC and the development of effective management strategies for mixed-species forest management.},
DOI = {10.3390/rs13101975}
}



@Article{s21103533,
AUTHOR = {Ashraf, Imran and Din, Sadia and Hur, Soojung and Kim, Gunzung and Park, Yongwan},
TITLE = {Empirical Overview of Benchmark Datasets for Geomagnetic Field-Based Indoor Positioning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {3533},
URL = {https://www.mdpi.com/1424-8220/21/10/3533},
PubMedID = {34069507},
ISSN = {1424-8220},
ABSTRACT = {Indoor positioning and localization have been regarded as some of the most widely researched areas during the last decade. The wide proliferation of smartphones and the availability of fast-speed internet have initiated several location-based services. Concerning the importance of precise location information, many sensors are embedded into modern smartphones. Besides Wi-Fi positioning, a rich variety of technologies have been introduced or adopted for indoor positioning such as ultrawideband, infrared, radio frequency identification, Bluetooth beacons, pedestrian dead reckoning, and magnetic field, etc. However, special emphasis is put on infrastructureless approaches like Wi-Fi and magnetic field-based positioning, as they do not require additional infrastructure. Magnetic field positioning is an attractive solution for indoors; yet lack of public benchmarks and selection of suitable benchmarks are among the big challenges. While several benchmarks have been introduced over time, the selection criteria of a benchmark are not properly defined, which leads to positioning results that lack generalization. This study aims at analyzing various public benchmarks for magnetic field positioning and highlights their pros and cons for evaluation positioning algorithms. The concept of DUST (device, user, space, time) and DOWTS (dynamicity, orientation, walk, trajectory, and sensor fusion) is introduced which divides the characteristics of the magnetic field dataset into basic and advanced groups and discusses the publicly available datasets accordingly.},
DOI = {10.3390/s21103533}
}



@Article{jimaging7050090,
AUTHOR = {Hamdi, Slim and Bouindour, Samir and Snoussi, Hichem and Wang, Tian and Abid, Mohamed},
TITLE = {End-to-End Deep One-Class Learning for Anomaly Detection in UAV Video Stream},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {90},
URL = {https://www.mdpi.com/2313-433X/7/5/90},
ISSN = {2313-433X},
ABSTRACT = {In recent years, the use of drones for surveillance tasks has been on the rise worldwide. However, in the context of anomaly detection, only normal events are available for the learning process. Therefore, the implementation of a generative learning method in an unsupervised mode to solve this problem becomes fundamental. In this context, we propose a new end-to-end architecture capable of generating optical flow images from original UAV images and extracting compact spatio-temporal characteristics for anomaly detection purposes. It is designed with a custom loss function as a sum of three terms, the reconstruction loss (Rl), the generation loss (Gl) and the compactness loss (Cl) to ensure an efficient classification of the “deep-one” class. In addition, we propose to minimize the effect of UAV motion in video processing by applying background subtraction on optical flow images. We tested our method on very complex datasets called the mini-drone video dataset, and obtained results surpassing existing techniques’ performances with an AUC of 85.3.},
DOI = {10.3390/jimaging7050090}
}



@Article{drones5020041,
AUTHOR = {Krul, Sander and Pantos, Christos and Frangulea, Mihai and Valente, João},
TITLE = {Visual SLAM for Indoor Livestock and Farming Using a Small Drone with a Monocular Camera: A Feasibility Study},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {41},
URL = {https://www.mdpi.com/2504-446X/5/2/41},
ISSN = {2504-446X},
ABSTRACT = {Real-time data collection and decision making with drones will play an important role in precision livestock and farming. Drones are already being used in precision agriculture. Nevertheless, this is not the case for indoor livestock and farming environments due to several challenges and constraints. These indoor environments are limited in physical space and there is the localization problem, due to GPS unavailability. Therefore, this work aims to give a step toward the usage of drones for indoor farming and livestock management. To investigate on the drone positioning in these workspaces, two visual simultaneous localization and mapping (VSLAM)—LSD-SLAM and ORB-SLAM—algorithms were compared using a monocular camera onboard a small drone. Several experiments were carried out in a greenhouse and a dairy farm barn with the absolute trajectory and the relative pose error being analyzed. It was found that the approach that suits best these workspaces is ORB-SLAM. This algorithm was tested by performing waypoint navigation and generating maps from the clustered areas. It was shown that aerial VSLAM could be achieved within these workspaces and that plant and cattle monitoring could benefit from using affordable and off-the-shelf drone technology.},
DOI = {10.3390/drones5020041}
}



@Article{app11104647,
AUTHOR = {Liu, Chuanyang and Wu, Yiquan and Liu, Jingjing and Sun, Zuo and Xu, Huajie},
TITLE = {Insulator Faults Detection in Aerial Images from High-Voltage Transmission Lines Based on Deep Learning Model},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4647},
URL = {https://www.mdpi.com/2076-3417/11/10/4647},
ISSN = {2076-3417},
ABSTRACT = {Insulator fault detection is one of the essential tasks for high-voltage transmission lines’ intelligent inspection. In this study, a modified model based on You Only Look Once (YOLO) is proposed for detecting insulator faults in aerial images with a complex background. Firstly, aerial images with one fault or multiple faults are collected in diverse scenes, and then a novel dataset is established. Secondly, to increase feature reuse and propagation in the low-resolution feature layers, a Cross Stage Partial Dense YOLO (CSPD-YOLO) model is proposed based on YOLO-v3 and the Cross Stage Partial Network. The feature pyramid network and improved loss function are adopted to the CSPD-YOLO model, improving the accuracy of insulator fault detection. Finally, the proposed CSPD-YOLO model and compared models are trained and tested on the established dataset. The average precision of CSPD-YOLO model is 4.9% and 1.8% higher than that of YOLO-v3 and YOLO-v4, and the running time of CSPD-YOLO (0.011 s) model is slightly longer than that of YOLO-v3 (0.01 s) and YOLO-v4 (0.01 s). Compared with the excellent object detection models YOLO-v3 and YOLO-v4, the experimental results and analysis demonstrate that the proposed CSPD-YOLO model performs better in insulator fault detection from high-voltage transmission lines with a complex background.},
DOI = {10.3390/app11104647}
}



@Article{genes12050783,
AUTHOR = {Cortés, Andrés J. and López-Hernández, Felipe},
TITLE = {Harnessing Crop Wild Diversity for Climate Change Adaptation},
JOURNAL = {Genes},
VOLUME = {12},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {783},
URL = {https://www.mdpi.com/2073-4425/12/5/783},
PubMedID = {34065368},
ISSN = {2073-4425},
ABSTRACT = {Warming and drought are reducing global crop production with a potential to substantially worsen global malnutrition. As with the green revolution in the last century, plant genetics may offer concrete opportunities to increase yield and crop adaptability. However, the rate at which the threat is happening requires powering new strategies in order to meet the global food demand. In this review, we highlight major recent ‘big data’ developments from both empirical and theoretical genomics that may speed up the identification, conservation, and breeding of exotic and elite crop varieties with the potential to feed humans. We first emphasize the major bottlenecks to capture and utilize novel sources of variation in abiotic stress (i.e., heat and drought) tolerance. We argue that adaptation of crop wild relatives to dry environments could be informative on how plant phenotypes may react to a drier climate because natural selection has already tested more options than humans ever will. Because isolated pockets of cryptic diversity may still persist in remote semi-arid regions, we encourage new habitat-based population-guided collections for genebanks. We continue discussing how to systematically study abiotic stress tolerance in these crop collections of wild and landraces using geo-referencing and extensive environmental data. By uncovering the genes that underlie the tolerance adaptive trait, natural variation has the potential to be introgressed into elite cultivars. However, unlocking adaptive genetic variation hidden in related wild species and early landraces remains a major challenge for complex traits that, as abiotic stress tolerance, are polygenic (i.e., regulated by many low-effect genes). Therefore, we finish prospecting modern analytical approaches that will serve to overcome this issue. Concretely, genomic prediction, machine learning, and multi-trait gene editing, all offer innovative alternatives to speed up more accurate pre- and breeding efforts toward the increase in crop adaptability and yield, while matching future global food demands in the face of increased heat and drought. In order for these ‘big data’ approaches to succeed, we advocate for a trans-disciplinary approach with open-source data and long-term funding. The recent developments and perspectives discussed throughout this review ultimately aim to contribute to increased crop adaptability and yield in the face of heat waves and drought events.},
DOI = {10.3390/genes12050783}
}



@Article{rs13102017,
AUTHOR = {Liang, Anbang and Li, Qingquan and Chen, Zhipeng and Zhang, Dejin and Zhu, Jiasong and Yu, Jianwei and Fang, Xu},
TITLE = {Spherically Optimized RANSAC Aided by an IMU for Fisheye Image Matching},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {2017},
URL = {https://www.mdpi.com/2072-4292/13/10/2017},
ISSN = {2072-4292},
ABSTRACT = {Fisheye cameras are widely used in visual localization due to the advantage of the wide field of view. However, the severe distortion in fisheye images lead to feature matching difficulties. This paper proposes an IMU-assisted fisheye image matching method called spherically optimized random sample consensus (So-RANSAC). We converted the putative correspondences into fisheye spherical coordinates and then used an inertial measurement unit (IMU) to provide relative rotation angles to assist fisheye image epipolar constraints and improve the accuracy of pose estimation and mismatch removal. To verify the performance of So-RANSAC, experiments were performed on fisheye images of urban drainage pipes and public data sets. The experimental results showed that So-RANSAC can effectively improve the mismatch removal accuracy, and its performance was superior to the commonly used fisheye image matching methods in various experimental scenarios.},
DOI = {10.3390/rs13102017}
}



@Article{app11104706,
AUTHOR = {Tullu, Abera and Endale, Bedada and Wondosen, Assefinew and Hwang, Ho-Yon},
TITLE = {Machine Learning Approach to Real-Time 3D Path Planning for Autonomous Navigation of Unmanned Aerial Vehicle},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {4706},
URL = {https://www.mdpi.com/2076-3417/11/10/4706},
ISSN = {2076-3417},
ABSTRACT = {The need for civilian use of Unmanned Aerial Vehicles (UAVs) has drastically increased in recent years. Their potential applications for civilian use include door-to-door package delivery, law enforcement, first aid, and emergency services in urban areas, which put the UAVs into obstacle collision risk. Therefore, UAVs are required to be equipped with sensors so as to acquire Artificial Intelligence (AI) to avoid potential risks during mission execution. The AI comes with intensive training of an on-board machine that is responsible to autonomously navigate the UAV. The training enables the UAV to develop humanoid perception of the environment it is to be navigating in. During the mission, this perception detects and localizes objects in the environment. It is based on this AI that this work proposes a real-time three-dimensional (3D) path planner that maneuvers the UAV towards destination through obstacle-free path. The proposed path planner has a heuristic sense of A⋆ algorithm, but requires no frontier nodes to be stored in a memory unlike A⋆. The planner relies on relative locations of detected objects (obstacles) and determines collision-free paths. This path planner is light-weight and hence a fast guidance method for real-time purposes. Its performance efficiency is proved through rigorous Software-In-The-Loop (SITL) simulations in constrained-environment and preliminary real flight tests.},
DOI = {10.3390/app11104706}
}



@Article{agriengineering3020020,
AUTHOR = {Chowdhury, Muhammad E. H. and Rahman, Tawsifur and Khandakar, Amith and Ayari, Mohamed Arselene and Khan, Aftab Ullah and Khan, Muhammad Salman and Al-Emadi, Nasser and Reaz, Mamun Bin Ibne and Islam, Mohammad Tariqul and Ali, Sawal Hamid Md},
TITLE = {Automatic and Reliable Leaf Disease Detection Using Deep Learning Techniques},
JOURNAL = {AgriEngineering},
VOLUME = {3},
YEAR = {2021},
NUMBER = {2},
PAGES = {294--312},
URL = {https://www.mdpi.com/2624-7402/3/2/20},
ISSN = {2624-7402},
ABSTRACT = {Plants are a major source of food for the world population. Plant diseases contribute to production loss, which can be tackled with continuous monitoring. Manual plant disease monitoring is both laborious and error-prone. Early detection of plant diseases using computer vision and artificial intelligence (AI) can help to reduce the adverse effects of diseases and also overcome the shortcomings of continuous human monitoring. In this work, we propose the use of a deep learning architecture based on a recent convolutional neural network called EfficientNet on 18,161 plain and segmented tomato leaf images to classify tomato diseases. The performance of two segmentation models i.e., U-net and Modified U-net, for the segmentation of leaves is reported. The comparative performance of the models for binary classification (healthy and unhealthy leaves), six-class classification (healthy and various groups of diseased leaves), and ten-class classification (healthy and various types of unhealthy leaves) are also reported. The modified U-net segmentation model showed accuracy, IoU, and Dice score of 98.66%, 98.5%, and 98.73%, respectively, for the segmentation of leaf images. EfficientNet-B7 showed superior performance for the binary classification and six-class classification using segmented images with an accuracy of 99.95% and 99.12%, respectively. Finally, EfficientNet-B4 achieved an accuracy of 99.89% for ten-class classification using segmented images. It can be concluded that all the architectures performed better in classifying the diseases when trained with deeper networks on segmented images. The performance of each of the experimental studies reported in this work outperforms the existing literature.},
DOI = {10.3390/agriengineering3020020}
}



@Article{ijms22115423,
AUTHOR = {Mores, Antonia and Borrelli, Grazia Maria and Laidò, Giovanni and Petruzzino, Giuseppe and Pecchioni, Nicola and Amoroso, Luca Giuseppe Maria and Desiderio, Francesca and Mazzucotelli, Elisabetta and Mastrangelo, Anna Maria and Marone, Daniela},
TITLE = {Genomic Approaches to Identify Molecular Bases of Crop Resistance to Diseases and to Develop Future Breeding Strategies},
JOURNAL = {International Journal of Molecular Sciences},
VOLUME = {22},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {5423},
URL = {https://www.mdpi.com/1422-0067/22/11/5423},
PubMedID = {34063853},
ISSN = {1422-0067},
ABSTRACT = {Plant diseases are responsible for substantial crop losses each year and affect food security and agricultural sustainability. The improvement of crop resistance to pathogens through breeding represents an environmentally sound method for managing disease and minimizing these losses. The challenge is to breed varieties with a stable and broad-spectrum resistance. Different approaches, from markers to recent genomic and ‘post-genomic era’ technologies, will be reviewed in order to contribute to a better understanding of the complexity of host–pathogen interactions and genes, including those with small phenotypic effects and mechanisms that underlie resistance. An efficient combination of these approaches is herein proposed as the basis to develop a successful breeding strategy to obtain resistant crop varieties that yield higher in increasing disease scenarios.},
DOI = {10.3390/ijms22115423}
}



@Article{rs13112031,
AUTHOR = {Huerta, Roberto E. and Yépez, Fabiola D. and Lozano-García, Diego F. and Guerra Cobián, Víctor H. and Ferriño Fierro, Adrián L. and de León Gómez, Héctor and Cavazos González, Ricardo A. and Vargas-Martínez, Adriana},
TITLE = {Mapping Urban Green Spaces at the Metropolitan Level Using Very High Resolution Satellite Imagery and Deep Learning Techniques for Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2031},
URL = {https://www.mdpi.com/2072-4292/13/11/2031},
ISSN = {2072-4292},
ABSTRACT = {Urban green spaces (UGSs) provide essential environmental services for the well-being of ecosystems and society. Due to the constant environmental, social, and economic transformations of cities, UGSs pose new challenges for management, particularly in fast-growing metropolitan areas. With technological advancement and the evolution of deep learning, it is possible to optimize the acquisition of UGS inventories through the detection of geometric patterns present in satellite imagery. This research evaluates two deep learning model techniques for semantic segmentation of UGS polygons with the use of different convolutional neural network encoders on the U-Net architecture and very high resolution (VHR) imagery to obtain updated information on UGS polygons at the metropolitan area level. The best model yielded a Dice coefficient of 0.57, IoU of 0.75, recall of 0.80, and kappa coefficient of 0.94 with an overall accuracy of 0.97, which reflects a reliable performance of the network in detecting patterns that make up the varied geometry of UGSs. A complete database of UGS polygons was quantified and categorized by types with location and delimited by municipality, allowing for the standardization of the information at the metropolitan level, which will be useful for comparative analysis with a homogenized and updated database. This is of particular interest to urban planners and UGS decision-makers.},
DOI = {10.3390/rs13112031}
}



@Article{en14113004,
AUTHOR = {Alibabaei, Khadijeh and Gaspar, Pedro D. and Lima, Tânia M.},
TITLE = {Crop Yield Estimation Using Deep Learning Based on Climate Big Data and Irrigation Scheduling},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {3004},
URL = {https://www.mdpi.com/1996-1073/14/11/3004},
ISSN = {1996-1073},
ABSTRACT = {Deep learning has already been successfully used in the development of decision support systems in various domains. Therefore, there is an incentive to apply it in other important domains such as agriculture. Fertilizers, electricity, chemicals, human labor, and water are the components of total energy consumption in agriculture. Yield estimates are critical for food security, crop management, irrigation scheduling, and estimating labor requirements for harvesting and storage. Therefore, estimating product yield can reduce energy consumption. Two deep learning models, Long Short-Term Memory and Gated Recurrent Units, have been developed for the analysis of time-series data such as agricultural datasets. In this paper, the capabilities of these models and their extensions, called Bidirectional Long Short-Term Memory and Bidirectional Gated Recurrent Units, to predict end-of-season yields are investigated. The models use historical data, including climate data, irrigation scheduling, and soil water content, to estimate end-of-season yield. The application of this technique was tested for tomato and potato yields at a site in Portugal. The Bidirectional Long Short-Term memory outperformed the Gated Recurrent Units network, the Long Short-Term Memory, and the Bidirectional Gated Recurrent Units network on the validation dataset. The model was able to capture the nonlinear relationship between irrigation amount, climate data, and soil water content and predict yield with an MSE of 0.017 to 0.039. The performance of the Bidirectional Long Short-Term Memory in the test was compared with the most commonly used deep learning method, the Convolutional Neural Network, and machine learning methods including a Multi-Layer Perceptrons model and Random Forest Regression. The Bidirectional Long Short-Term Memory outperformed the other models with an R2 score between 0.97 and 0.99. The results show that analyzing agricultural data with the Long Short-Term Memory model improves the performance of the model in terms of accuracy. The Convolutional Neural Network model achieved the second-best performance. Therefore, the deep learning model has a remarkable ability to predict the yield at the end of the season.},
DOI = {10.3390/en14113004}
}



@Article{rs13112052,
AUTHOR = {Yan, Dongchuan and Li, Guoqing and Li, Xiangqiang and Zhang, Hao and Lei, Hua and Lu, Kaixuan and Cheng, Minghua and Zhu, Fuxiao},
TITLE = {An Improved Faster R-CNN Method to Detect Tailings Ponds from High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {2052},
URL = {https://www.mdpi.com/2072-4292/13/11/2052},
ISSN = {2072-4292},
ABSTRACT = {Dam failure of tailings ponds can result in serious casualties and environmental pollution. Therefore, timely and accurate monitoring is crucial for managing tailings ponds and preventing damage from tailings pond accidents. Remote sensing technology facilitates the regular extraction and monitoring of tailings pond information. However, traditional remote sensing techniques are inefficient and have low levels of automation, which hinders the large-scale, high-frequency, and high-precision extraction of tailings pond information. Moreover, research into the automatic and intelligent extraction of tailings pond information from high-resolution remote sensing images is relatively rare. However, the deep learning end-to-end model offers a solution to this problem. This study proposes an intelligent and high-precision method for extracting tailings pond information from high-resolution images, which improves deep learning target detection model: faster region-based convolutional neural network (Faster R-CNN). A comparison study is conducted and the model input size with the highest precision is selected. The feature pyramid network (FPN) is adopted to obtain multiscale feature maps with rich context information, the attention mechanism is used to improve the FPN, and the contribution degrees of feature channels are recalibrated. The model test results based on GoogleEarth high-resolution remote sensing images indicate a significant increase in the average precision (AP) and recall of tailings pond detection from that of Faster R-CNN by 5.6% and 10.9%, reaching 85.7% and 62.9%, respectively. Considering the current rapid increase in high-resolution remote sensing images, this method will be important for large-scale, high-precision, and intelligent monitoring of tailings ponds, which will greatly improve the decision-making efficiency in tailings pond management.},
DOI = {10.3390/rs13112052}
}



