
@Article{rs13152992,
AUTHOR = {Hajjar, Chantal and Ghattas, Ghassan and Sarkis, Maya Kharrat and Chamoun, Yolla Ghorra},
TITLE = {Vine Identification and Characterization in Goblet-Trained Vineyards Using Remotely Sensed Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2992},
URL = {https://www.mdpi.com/2072-4292/13/15/2992},
ISSN = {2072-4292},
ABSTRACT = {This paper proposes a novel approach for living and missing vine identification and vine characterization in goblet-trained vine plots using aerial images. Given the periodic structure of goblet vineyards, the RGB color coded parcel image is analyzed using proper processing techniques in order to determine the locations of living and missing vines. Vine characterization is achieved by implementing the marker-controlled watershed transform where the centers of the living vines serve as object markers. As a result, a precise mortality rate is calculated for each parcel. Moreover, all vines, even the overlapping ones, are fully recognized providing information about their size, shape, and green color intensity. The presented approach is fully automated and yields accuracy values exceeding 95% when the obtained results are assessed with ground-truth data. This unsupervised and automated approach can be applied to any type of plots presenting similar spatial patterns requiring only the image as input.},
DOI = {10.3390/rs13152992}
}



@Article{app11157013,
AUTHOR = {Junejo, Aisha Zahid and Hashmani, Manzoor Ahmed and Memon, Mehak Maqbool},
TITLE = {Empirical Evaluation of Privacy Efficiency in Blockchain Networks: Review and Open Challenges},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {7013},
URL = {https://www.mdpi.com/2076-3417/11/15/7013},
ISSN = {2076-3417},
ABSTRACT = {With the widespread of blockchain technology, preserving the anonymity and confidentiality of transactions have become crucial. An enormous portion of blockchain research is dedicated to the design and development of privacy protocols but not much has been achieved for proper assessment of these solutions. To mitigate the gap, we have first comprehensively classified the existing solutions based on blockchain fundamental building blocks (i.e., smart contracts, cryptography, and hashing). Next, we investigated the evaluation criteria used for validating these techniques. The findings depict that the majority of privacy solutions are validated based on computing resources i.e., memory, time, storage, throughput, etc., only, which is not sufficient. Hence, we have additionally identified and presented various other factors that strengthen or weaken blockchain privacy. Based on those factors, we have formulated an evaluation framework to analyze the efficiency of blockchain privacy solutions. Further, we have introduced a concept of privacy precision that is a quantifiable measure to empirically assess privacy efficiency in blockchains. The calculation of privacy precision will be based on the effectiveness and strength of various privacy protecting attributes of a solution and the associated risks. Finally, we conclude the paper with some open research challenges and future directions. Our study can serve as a benchmark for empirical assessment of blockchain privacy.},
DOI = {10.3390/app11157013}
}



@Article{coatings11080913,
AUTHOR = {Siang, Teng Wei and Firdaus Akbar, Muhammad and Nihad Jawad, Ghassan and Yee, Tan Shin and Mohd Sazali, Mohd Ilyas Sobirin},
TITLE = {A Past, Present, and Prospective Review on Microwave Nondestructive Evaluation of Composite Coatings},
JOURNAL = {Coatings},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {913},
URL = {https://www.mdpi.com/2079-6412/11/8/913},
ISSN = {2079-6412},
ABSTRACT = {Recent years have witnessed an increase in the use of composite coatings for numerous applications, including aerospace, aircraft, and maritime vessels. These materials owe this popularity surge to the superior strength, weight, stiffness, and electrical insulation they exhibit over conventional substances, such as metals. The growing demand for such materials is accompanied by the inevitable need for fast, accurate, and affordable nondestructive testing techniques to reveal any possible defects within the coatings or any defects under coating. However, typical nondestructive testing (NDT) techniques such as ultrasonic testing (UT), infrared thermography (IRT), eddy current testing (ECT), and laser shearography (LS) have failed to provide successful results when inspecting composite coatings. Consequently, microwave NDT techniques have emerged to compensate for the shortcomings of traditional NDT approaches. Numerous microwave NDT methods have been reported for composite coatings inspection. Although existing microwave NDT methods have shown successful inspection of composite coatings, they often face several challenges, such as low spatial image quality and extensive data interpretation. Nevertheless, many of these limitations can be addressed by utilizing microwave NDT techniques with modern technologies such as soft computing. Artificially intelligent techniques have greatly enhanced the reliability and accuracy of microwave NDT techniques. This paper reviews various traditional NDT techniques and their limitations in inspecting composite coatings. In addition, the article includes a detailed review of several microwave NDT techniques and their benefits in evaluating composite coatings. The paper also highlights the advantages of using the recently reported microwave NDT approaches employing artificial intelligence approaches. This review demonstrates that microwave NDT techniques in conjunction with artificial intelligence approaches have excellent prospects for further enhancing composite coatings inspection and assessment efficiency. The review aimed to provide the reader with a comprehensive overview of most NDT techniques used for composite materials alongside their most salient features.},
DOI = {10.3390/coatings11080913}
}



@Article{w13152080,
AUTHOR = {Wang, Yang and Tian, Yongzhong and Cao, Yan},
TITLE = {Dam Siting: A Review},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2080},
URL = {https://www.mdpi.com/2073-4441/13/15/2080},
ISSN = {2073-4441},
ABSTRACT = {Dams can effectively regulate the spatial and temporal distribution of water resources, where the rationality of dam siting determines whether the role of dams can be effectively performed. This paper reviews the research literature on dam siting in the past 20 years, discusses the methods used for dam siting, focuses on the factors influencing dam siting, and assesses the impact of different dam functions on siting factors. The results show the following: (1) Existing siting methods can be categorized into three types—namely, GIS/RS-based siting, MCDM- and MCDM-GIS-based siting, and machine learning-based siting. GIS/RS emphasizes the ability to capture and analyze data, MCDM has the advantage of weighing the importance of the relationship between multiple factors, and machine learning methods have a strong ability to learn and process complex data. (2) Site selection factors vary greatly, depending on the function of the dam. For dams with irrigation and water supply as the main purpose, the site selection is more focused on the evaluation of water quality. For dams with power generation as the main purpose, the hydrological factors characterizing the power generation potential are the most important. For dams with flood control as the main purpose, the topography and geological conditions are more important. (3) The integration of different siting methods and the siting of new functional dams in the existing research is not sufficient. Future research should focus on the integration of different methods and disciplines, in order to explore the siting of new types of dams.},
DOI = {10.3390/w13152080}
}



@Article{agronomy11081542,
AUTHOR = {Wang, Hao and Lyu, Suxing and Ren, Yaxin},
TITLE = {Paddy Rice Imagery Dataset for Panicle Segmentation},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1542},
URL = {https://www.mdpi.com/2073-4395/11/8/1542},
ISSN = {2073-4395},
ABSTRACT = {Accurate panicle identification is a key step in rice-field phenotyping. Deep learning methods based on high-spatial-resolution images provide a high-throughput and accurate solution of panicle segmentation. Panicle segmentation tasks require costly annotations to train an accurate and robust deep learning model. However, few public datasets are available for rice-panicle phenotyping. We present a semi-supervised deep learning model training process, which greatly assists the annotation and refinement of training datasets. The model learns the panicle features with limited annotations and localizes more positive samples in the datasets, without further interaction. After the dataset refinement, the number of annotations increased by 40.6%. In addition, we trained and tested modern deep learning models to show how the dataset is beneficial to both detection and segmentation tasks. Results of our comparison experiments can inspire others in dataset preparation and model selection.},
DOI = {10.3390/agronomy11081542}
}



@Article{ijms22158266,
AUTHOR = {Kim, Minsu and Lee, Chaewon and Hong, Subin and Kim, Song Lim and Baek, Jeong-Ho and Kim, Kyung-Hwan},
TITLE = {High-Throughput Phenotyping Methods for Breeding Drought-Tolerant Crops},
JOURNAL = {International Journal of Molecular Sciences},
VOLUME = {22},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {8266},
URL = {https://www.mdpi.com/1422-0067/22/15/8266},
PubMedID = {34361030},
ISSN = {1422-0067},
ABSTRACT = {Drought is a main factor limiting crop yields. Modern agricultural technologies such as irrigation systems, ground mulching, and rainwater storage can prevent drought, but these are only temporary solutions. Understanding the physiological, biochemical, and molecular reactions of plants to drought stress is therefore urgent. The recent rapid development of genomics tools has led to an increasing interest in phenomics, i.e., the study of phenotypic plant traits. Among phenomic strategies, high-throughput phenotyping (HTP) is attracting increasing attention as a way to address the bottlenecks of genomic and phenomic studies. HTP provides researchers a non-destructive and non-invasive method yet accurate in analyzing large-scale phenotypic data. This review describes plant responses to drought stress and introduces HTP methods that can detect changes in plant phenotypes in response to drought.},
DOI = {10.3390/ijms22158266}
}



@Article{rs13153024,
AUTHOR = {Ma, Huiqin and Huang, Wenjiang and Dong, Yingying and Liu, Linyi and Guo, Anting},
TITLE = {Using UAV-Based Hyperspectral Imagery to Detect Winter Wheat Fusarium Head Blight},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {3024},
URL = {https://www.mdpi.com/2072-4292/13/15/3024},
ISSN = {2072-4292},
ABSTRACT = {Fusarium head blight (FHB) is a major winter wheat disease in China. The accurate and timely detection of wheat FHB is vital to scientific field management. By combining three types of spectral features, namely, spectral bands (SBs), vegetation indices (VIs), and wavelet features (WFs), in this study, we explore the potential of using hyperspectral imagery obtained from an unmanned aerial vehicle (UAV), to detect wheat FHB. First, during the wheat filling period, two UAV-based hyperspectral images were acquired. SBs, VIs, and WFs that were sensitive to wheat FHB were extracted and optimized from the two images. Subsequently, a field-scale wheat FHB detection model was formulated, based on the optimal spectral feature combination of SBs, VIs, and WFs (SBs + VIs + WFs), using a support vector machine. Two commonly used data normalization algorithms were utilized before the construction of the model. The single WFs, and the spectral feature combination of optimal SBs and VIs (SBs + VIs), were respectively used to formulate models for comparison and testing. The results showed that the detection model based on the normalized SBs + VIs + WFs, using min–max normalization algorithm, achieved the highest R2 of 0.88 and the lowest RMSE of 2.68% among the three models. Our results suggest that UAV-based hyperspectral imaging technology is promising for the field-scale detection of wheat FHB. Combining traditional SBs and VIs with WFs can improve the detection accuracy of wheat FHB effectively.},
DOI = {10.3390/rs13153024}
}



@Article{su13158600,
AUTHOR = {Sharma, Meenakshi and Kaushik, Prashant and Chawade, Aakash},
TITLE = {Frontiers in the Solicitation of Machine Learning Approaches in Vegetable Science Research},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {8600},
URL = {https://www.mdpi.com/2071-1050/13/15/8600},
ISSN = {2071-1050},
ABSTRACT = {Along with essential nutrients and trace elements, vegetables provide raw materials for the food processing industry. Despite this, plant diseases and unfavorable weather patterns continue to threaten the delicate balance between vegetable production and consumption. It is critical to utilize machine learning (ML) in this setting because it provides context for decision-making related to breeding goals. Cutting-edge technologies for crop genome sequencing and phenotyping, combined with advances in computer science, are currently fueling a revolution in vegetable science and technology. Additionally, various ML techniques such as prediction, classification, and clustering are frequently used to forecast vegetable crop production in the field. In the vegetable seed industry, machine learning algorithms are used to assess seed quality before germination and have the potential to improve vegetable production with desired features significantly; whereas, in plant disease detection and management, the ML approaches can improve decision-support systems that assist in converting massive amounts of data into valuable recommendations. On similar lines, in vegetable breeding, ML approaches are helpful in predicting treatment results, such as what will happen if a gene is silenced. Furthermore, ML approaches can be a saviour to insufficient coverage and noisy data generated using various omics platforms. This article examines ML models in the field of vegetable sciences, which encompasses breeding, biotechnology, and genome sequencing.},
DOI = {10.3390/su13158600}
}



@Article{rs13153034,
AUTHOR = {Zhao, Yujin and Sun, Yihan and Chen, Wenhe and Zhao, Yanping and Liu, Xiaoliang and Bai, Yongfei},
TITLE = {The Potential of Mapping Grassland Plant Diversity with the Links among Spectral Diversity, Functional Trait Diversity, and Species Diversity},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {3034},
URL = {https://www.mdpi.com/2072-4292/13/15/3034},
ISSN = {2072-4292},
ABSTRACT = {Mapping biodiversity is essential for assessing conservation and ecosystem services in global terrestrial ecosystems. Compared with remotely sensed mapping of forest biodiversity, that of grassland plant diversity has been less studied, because of the small size of individual grass species and the inherent difficulty in identifying these species. The technological advances in unmanned aerial vehicle (UAV)-based or proximal imaging spectroscopy with high spatial resolution provide new approaches for mapping and assessing grassland plant diversity based on spectral diversity and functional trait diversity. However, relatively few studies have explored the relationships among spectral diversity, remote-sensing-estimated functional trait diversity, and species diversity in grassland ecosystems. In this study, we examined the links among spectral diversity, functional trait diversity, and species diversity in a semi-arid grassland monoculture experimental site. The results showed that (1) different grassland plant species harbored different functional traits or trait combinations (functional trait diversity), leading to different spectral patterns (spectral diversity). (2) The spectral diversity of grassland plant species increased gradually from the visible (VIR, 400–700 nm) to the near-infrared (NIR, 700–1100 nm) region, and to the short-wave infrared (SWIR, 1100–2400 nm) region. (3) As the species richness increased, the functional traits and spectral diversity increased in a nonlinear manner, finally tending to saturate. (4) Grassland plant species diversity could be accurately predicted using hyperspectral data (R2 = 0.73, p &lt; 0.001) and remotely sensed functional traits (R2 = 0.66, p &lt; 0.001) using cluster algorithms. This will enhance our understanding of the effect of biodiversity on ecosystem functions and support regional grassland biodiversity conservation.},
DOI = {10.3390/rs13153034}
}



@Article{app11157148,
AUTHOR = {Endale, Bedada and Tullu, Abera and Shi, Hayoung and Kang, Beom-Soo},
TITLE = {Robust Approach to Supervised Deep Neural Network Training for Real-Time Object Classification in Cluttered Indoor Environment},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {7148},
URL = {https://www.mdpi.com/2076-3417/11/15/7148},
ISSN = {2076-3417},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are being widely utilized for various missions: in both civilian and military sectors. Many of these missions demand UAVs to acquire artificial intelligence about the environments they are navigating in. This perception can be realized by training a computing machine to classify objects in the environment. One of the well known machine training approaches is supervised deep learning, which enables a machine to classify objects. However, supervised deep learning comes with huge sacrifice in terms of time and computational resources. Collecting big input data, pre-training processes, such as labeling training data, and the need for a high performance computer for training are some of the challenges that supervised deep learning poses. To address these setbacks, this study proposes mission specific input data augmentation techniques and the design of light-weight deep neural network architecture that is capable of real-time object classification. Semi-direct visual odometry (SVO) data of augmented images are used to train the network for object classification. Ten classes of 10,000 different images in each class were used as input data where 80% were for training the network and the remaining 20% were used for network validation. For the optimization of the designed deep neural network, a sequential gradient descent algorithm was implemented. This algorithm has the advantage of handling redundancy in the data more efficiently than other algorithms.},
DOI = {10.3390/app11157148}
}



@Article{app11157151,
AUTHOR = {Hu, Yijun and Shen, Jingfang and Qi, Yonghao},
TITLE = {Estimation of Rice Biomass at Different Growth Stages by Using Fractal Dimension in Image Processing},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {7151},
URL = {https://www.mdpi.com/2076-3417/11/15/7151},
ISSN = {2076-3417},
ABSTRACT = {Rice has long served as the staple food in Asia, and the cultivation of high-yield rice crops draws increasing attention from academic researchers. The prediction of rice growth condition by image features realizes nondestructive prediction and it has great implications for smart agriculture. We found a special image parameter called the fractal dimension that can improve the effect of the prediction model. As an important geometric feature, the fractal dimension could be calculated from the image, but it is rarely used in the field of rice growth prediction. In this paper, we attempt to combine the fractal dimension with traditional rice image features to improve the effect of the model. The thresholding method is used to transform the cropped rice image into binary image, and the box-counting method is used to calculate the fractal dimension of the image. The correlation coefficients are calculated to select the characteristics with a strong correlation with biomass. The prediction models of dry weight, fresh weight and plant height of rice are established by using random forest, support vector regression and linear regression. By evaluating the prediction effect of the model, it can be concluded that the fractal dimension can improve the prediction effect of the model. Among the models obtained by the three methods, the multiple linear regression model has the best comprehensive effect, with the dry weight prediction model R2 reaching 0.8697, the fresh weight prediction model R2 reaching 0.8631 and the plant height prediction model R2 reaching 0.9196. The model established in this paper has a fine effect and has a certain guiding significance in rice research.},
DOI = {10.3390/app11157151}
}



@Article{drones5030073,
AUTHOR = {Doukari, Michaela and Batsaris, Marios and Topouzelis, Konstantinos},
TITLE = {UASea: A Data Acquisition Toolbox for Improving Marine Habitat Mapping},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {73},
URL = {https://www.mdpi.com/2504-446X/5/3/73},
ISSN = {2504-446X},
ABSTRACT = {Unmanned aerial systems (UAS) are widely used in the acquisition of high-resolution information in the marine environment. Although the potential applications of UAS in marine habitat mapping are constantly increasing, many limitations need to be overcome—most of which are related to the prevalent environmental conditions—to reach efficient UAS surveys. The knowledge of the UAS limitations in marine data acquisition and the examination of the optimal flight conditions led to the development of the UASea toolbox. This study presents the UASea, a data acquisition toolbox that is developed for efficient UAS surveys in the marine environment. The UASea uses weather forecast data (i.e., wind speed, cloud cover, precipitation probability, etc.) and adaptive thresholds in a ruleset that calculates the optimal flight times in a day for the acquisition of reliable marine imagery using UAS in a given day. The toolbox provides hourly positive and negative suggestions, based on optimal or non-optimal survey conditions in a day, calculated according to the ruleset calculations. We acquired UAS images in optimal and non-optimal conditions and estimated their quality using an image quality equation. The image quality estimates are based on the criteria of sunglint presence, sea surface texture, water turbidity, and image naturalness. The overall image quality estimates were highly correlated with the suggestions of the toolbox, with a correlation coefficient of −0.84. The validation showed that 40% of the toolbox suggestions were a positive match to the images with higher quality. Therefore, we propose the optimal flight times to acquire reliable and accurate UAS imagery in the coastal environment through the UASea. The UASea contributes to proper flight planning and efficient UAS surveys by providing valuable information for mapping, monitoring, and management of the marine environment, which can be used globally in research and marine applications.},
DOI = {10.3390/drones5030073}
}



@Article{electronics10151864,
AUTHOR = {Sheu, Ming-Hwa and Jhang, Yu-Syuan and Morsalin, S M Salahuddin and Huang, Yao-Fong and Sun, Chi-Chia and Lai, Shin-Chi},
TITLE = {UAV Object Tracking Application Based on Patch Color Group Feature on Embedded System},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {1864},
URL = {https://www.mdpi.com/2079-9292/10/15/1864},
ISSN = {2079-9292},
ABSTRACT = {The discriminative object tracking system for unmanned aerial vehicles (UAVs) is widely used in numerous applications. While an ample amount of research has been carried out in this domain, implementing a low computational cost algorithm on a UAV onboard embedded system is still challenging. To address this issue, we propose a low computational complexity discriminative object tracking system for UAVs approach using the patch color group feature (PCGF) framework in this work. The tracking object is separated into several non-overlapping local image patches then the features are extracted into the PCGFs, which consist of the Gaussian mixture model (GMM). The object location is calculated by the similar PCGFs comparison from the previous frame and current frame. The background PCGFs of the object are removed by four directions feature scanning and dynamic threshold comparison, which improve the performance accuracy. In the terms of speed execution, the proposed algorithm accomplished 32.5 frames per second (FPS) on the x64 CPU platform without a GPU accelerator and 17 FPS in Raspberry Pi 4. Therefore, this work could be considered as a good solution for achieving a low computational complexity PCGF algorithm on a UAV onboard embedded system to improve flight times.},
DOI = {10.3390/electronics10151864}
}



@Article{agronomy11081554,
AUTHOR = {Lee, Dong-Ho and Kim, Hyeon-Jin and Park, Jong-Hwa},
TITLE = {UAV, a Farm Map, and Machine Learning Technology Convergence Classification Method of a Corn Cultivation Area},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1554},
URL = {https://www.mdpi.com/2073-4395/11/8/1554},
ISSN = {2073-4395},
ABSTRACT = {South Korea’s agriculture is characterized by a mixture of various cultivated crops. In such an agricultural environment, convergence technology for ICT (information, communications, and technology) and AI (artificial intelligence) as well as agriculture is required to classify objects and predict yields. In general, the classification of paddy fields and field boundaries takes a lot of time and effort. The Farm Map was developed to clearly demarcate and classify the boundaries of paddy fields and fields in Korea. Therefore, this study tried to minimize the time and effort required to divide paddy fields and fields through the application of the Farm Map. To improve the fact that UAV image processing for a wide area requires a lot of time and effort to classify objects, we suggest a method for optimizing cultivated crop recognition. This study aimed to evaluate the applicability and effectiveness of machine learning classification techniques using a Farm Map in object-based mapping of agricultural land using unmanned aerial vehicles (UAVs). In this study, the advanced function selection method for object classification is to improve classification accuracy by using two types of classifiers, support vector machine (SVM) and random forest (RF). As a result of classification by applying a Farm Map-based SVM algorithm to wide-area UAV images, producer’s accuracy (PA) was 81.68%, user’s accuracy (UA) was 75.09%, the Kappa coefficient was 0.77, and the F-measure was 0.78. The results of classification by the Farm Map-based RF algorithm were as follows: PA of 96.58%, UA of 92.27%, a Kappa coefficient of 0.94, and the F-measure of 0.94. In the cultivation environment in which various crops were mixed, the corn cultivation area was estimated to be 96.54 ha by SVM, showing an accuracy of 90.27%. RF provided an estimate of 98.77 ha and showed an accuracy of 92.36%, which was higher than that of SVM. As a result of using the Farm Map for the object-based classification method, the agricultural land classification showed a higher efficiency in terms of time than the existing object classification method. Most importantly, it was confirmed that the efficiency of data processing can be increased by minimizing the possibility of misclassification in the obtained results. The obtained results confirmed that rapid and reliable analysis is possible when the cultivated area of crops is identified using UAV images, a Farm Map, and machine learning.},
DOI = {10.3390/agronomy11081554}
}



@Article{agriculture11080740,
AUTHOR = {Ghajar, Shayan and Tracy, Benjamin},
TITLE = {Proximal Sensing in Grasslands and Pastures},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {740},
URL = {https://www.mdpi.com/2077-0472/11/8/740},
ISSN = {2077-0472},
ABSTRACT = {Reliable measures of biomass, species composition, nitrogen status, and nutritive value provide important indicators of the status of pastures and rangelands, allowing managers to make informed decisions. Traditional methods of sample collection necessitate significant investments in time and labor. Proximal sensing technologies have the potential to collect more data with a smaller investment in time and labor. However, methods and protocols for conducting pasture assessments with proximal sensors are still in development, equipment and software vary considerably, and the accuracy and utility of these assessments differ between methods and sites. This review summarizes the methods currently being developed to assess pastures and rangelands worldwide and discusses these emerging technologies in the context of diffusion of innovation theory.},
DOI = {10.3390/agriculture11080740}
}



@Article{rs13163067,
AUTHOR = {Zhao, Licheng and Guo, Wei and Wang, Jian and Wang, Haozhou and Duan, Yulin and Wang, Cong and Wu, Wenbin and Shi, Yun},
TITLE = {An Efficient Method for Estimating Wheat Heading Dates Using UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3067},
URL = {https://www.mdpi.com/2072-4292/13/16/3067},
ISSN = {2072-4292},
ABSTRACT = {Convenient, efficient, and high-throughput estimation of wheat heading dates is of great significance in plant sciences and agricultural research. However, documenting heading dates is time-consuming, labor-intensive, and subjective on a large-scale field. To overcome these challenges, model- and image-based approaches are used to estimate heading dates. Phenology models usually require complicated parameters calibrations, making it difficult to model other varieties and different locations, while in situ field-image recognition usually requires the deployment of a large amount of observational equipment, which is expensive. Therefore, in this study, we proposed a growth curve-based method for estimating wheat heading dates. The method first generates a height-based continuous growth curve based on five time-series unmanned aerial vehicle (UAV) images captured over the entire wheat growth cycle (&gt;200 d). Then estimate the heading date by generated growth curve. As a result, the proposed method had a mean absolute error of 2.81 d and a root mean square error of 3.49 d for 72 wheat plots composed of different varieties and densities sown on different dates. Thus, the proposed method is straightforward, efficient, and affordable and meets the high-throughput estimation requirements of large-scale fields and underdeveloped areas.},
DOI = {10.3390/rs13163067}
}



@Article{f12081035,
AUTHOR = {Perroy, Ryan L. and Sullivan, Timo and Benitez, David and Hughes, R. Flint and Keith, Lisa M. and Brill, Eva and Kissinger, Karma and Duda, Daniel},
TITLE = {Spatial Patterns of ‘Ōhi‘a Mortality Associated with Rapid ‘Ōhi‘a Death and Ungulate Presence},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1035},
URL = {https://www.mdpi.com/1999-4907/12/8/1035},
ISSN = {1999-4907},
ABSTRACT = {Effective forest management, particularly during forest disturbance events, requires timely and accurate monitoring information at appropriate spatial scales. In Hawai‘i, widespread ‘ōhi‘a (Metrosideros polymorpha Gaud.) mortality associated with introduced fungal pathogens affects forest stands across the archipelago, further impacting native ecosystems already under threat from invasive species. Here, we share results from an integrated monitoring program based on high resolution (&lt;5 cm) aerial imagery, field sampling, and confirmatory laboratory testing to detect and monitor ‘ōhi‘a mortality at the individual tree level across four representative sites on Hawai‘i island. We developed a custom imaging system for helicopter operations to map thousands of hectares (ha) per flight, a more useful scale than the ten to hundreds of ha typically covered using small, unoccupied aerial systems. Based on collected imagery, we developed a rating system of canopy condition to identify ‘ōhi‘a trees suspected of infection by the fungal pathogens responsible for rapid ‘ōhi‘a death (ROD); we used this system to quickly generate and share suspect tree candidate locations with partner agencies to rapidly detect new mortality outbreaks and prioritize field sampling efforts. In three of the four sites, 98% of laboratory samples collected from suspect trees assigned a high confidence rating (n = 50) and 89% of those assigned a medium confidence rating (n = 117) returned positive detections for the fungal pathogens responsible for ROD. The fourth site, which has a history of unexplained ‘ōhi‘a mortality, exhibited much lower positive detection rates: only 6% of sampled trees assigned a high confidence rating (n = 16) and 0% of the sampled suspect trees assigned a medium confidence rating (n = 20) were found to be positive for the pathogen. The disparity in positive detection rates across study sites illustrates challenges to definitively determine the cause of ‘ōhi‘a mortality from aerial imagery alone. Spatial patterns of ROD-associated ‘ōhi‘a mortality were strongly affected by ungulate presence or absence as measured by the density of suspected ROD trees in fenced (i.e., ungulate-free) and unfenced (i.e., ungulate present) areas. Suspected ROD tree densities in neighboring areas containing ungulates were two to 69 times greater than those found in ungulate-free zones. In one study site, a fence line breach occurred during the study period, and feral ungulates entered an area that was previously ungulate-free. Following the breach, suspect ROD tree densities in this area rose from 0.02 to 2.78 suspect trees/ha, highlighting the need for ungulate control to protect ‘ōhi‘a stands from Ceratocystis-induced mortality and repeat monitoring to detect forest changes and resource threats.},
DOI = {10.3390/f12081035}
}



@Article{rs13163073,
AUTHOR = {Bai, Xueyuan and Li, Zhenhai and Li, Wei and Zhao, Yu and Li, Meixuan and Chen, Hongyan and Wei, Shaochong and Jiang, Yuanmao and Yang, Guijun and Zhu, Xicun},
TITLE = {Comparison of Machine-Learning and CASA Models for Predicting Apple Fruit Yields from Time-Series Planet Imageries},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3073},
URL = {https://www.mdpi.com/2072-4292/13/16/3073},
ISSN = {2072-4292},
ABSTRACT = {Apple (Malus domestica Borkh. cv. “Fuji”), an important cash crop, is widely consumed around the world. Accurately predicting preharvest apple fruit yields is critical for planting policy making and agricultural management. This study attempted to explore an effective approach for predicting apple fruit yields based on time-series remote sensing data. In this study, time-series vegetation indices (VIs) were derived from Planet images and analyzed to further construct an accumulated VI (∑VIs)-based random forest (RF∑VI) model and a Carnegie–Ames–Stanford approach (CASA) model for predicting apple fruit yields. The results showed that (1) ∑NDVI was the optimal predictor to construct an RF model for apple fruit yield, and the R2, RMSE, and RPD values of the RF∑NDVI model reached 0.71, 16.40 kg/tree, and 1.83, respectively. (2) The maximum light use efficiency was determined to be 0.499 g C/MJ, and the CASASR model (R2 = 0.57, RMSE = 19.61 kg/tree, and RPD = 1.53) performed better than the CASANDVI model and the CASAAverage model (R2, RMSE, and RPD = 0.56, 24.47 kg/tree, 1.22 and 0.57, 20.82 kg/tree, 1.44, respectively). (3) This study compared the yield prediction accuracies obtained by the models using the same dataset, and the RF∑NDVI model (RPD = 1.83) showed a better performance in predicting apple fruit yields than the CASASR model (RPD = 1.53). The results obtained from this study indicated the potential of the RF∑NDVI model based on time-series Planet images to accurately predict apple fruit yields. The models could provide spatial and quantitative information of apple fruit yield, which would be valuable for agronomists to predict regional apple production to inform and develop national planting policies, agricultural management, and export strategies.},
DOI = {10.3390/rs13163073}
}



@Article{s21165293,
AUTHOR = {Pikalov, Simon and Azaria, Elisha and Sonnenberg, Shaya and Ben-Moshe, Boaz and Azaria, Amos},
TITLE = {Vision-Less Sensing for Autonomous Micro-Drones},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5293},
URL = {https://www.mdpi.com/1424-8220/21/16/5293},
PubMedID = {34450742},
ISSN = {1424-8220},
ABSTRACT = {This work presents a concept of intelligent vision-less micro-drones, which are motivated by flying animals such as insects, birds, and bats. The presented micro-drone (named BAT: Blind Autonomous Tiny-drone) can perform bio-inspired complex tasks without the use of cameras. The BAT uses LIDARs and self-emitted optical-flow in order to perform obstacle avoiding and maze-solving. The controlling algorithms were implemented on an onboard micro-controller, allowing the BAT to be fully autonomous. We further present a method for using the information collected by the drone to generate a detailed mapping of the environment. A complete model of the BAT was implemented and tested using several scenarios both in simulation and field experiments, in which it was able to explore and map complex building autonomously even in total darkness.},
DOI = {10.3390/s21165293}
}



@Article{rs13163088,
AUTHOR = {Wolf, Stefan and Sommer, Lars and Schumann, Arne},
TITLE = {FastAER Det: Fast Aerial Embedded Real-Time Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3088},
URL = {https://www.mdpi.com/2072-4292/13/16/3088},
ISSN = {2072-4292},
ABSTRACT = {Automated detection of objects in aerial imagery is the basis for many applications, such as search and rescue operations, activity monitoring or mapping. However, in many cases it is beneficial to employ a detector on-board of the aerial platform in order to avoid latencies, make basic decisions within the platform and save transmission bandwidth. In this work, we address the task of designing such an on-board aerial object detector, which meets certain requirements in accuracy, inference speed and power consumption. For this, we first outline a generally applicable design process for such on-board methods and then follow this process to develop our own set of models for the task. Specifically, we first optimize a baseline model with regards to accuracy while not increasing runtime. We then propose a fast detection head to significantly improve runtime at little cost in accuracy. Finally, we discuss several aspects to consider during deployment and in the runtime environment. Our resulting four models that operate at 15, 30, 60 and 90 FPS on an embedded Jetson AGX device are published for future benchmarking and comparison by the community.},
DOI = {10.3390/rs13163088}
}



@Article{rs13163095,
AUTHOR = {Zhao, Jianqing and Zhang, Xiaohu and Yan, Jiawei and Qiu, Xiaolei and Yao, Xia and Tian, Yongchao and Zhu, Yan and Cao, Weixing},
TITLE = {A Wheat Spike Detection Method in UAV Images Based on Improved YOLOv5},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3095},
URL = {https://www.mdpi.com/2072-4292/13/16/3095},
ISSN = {2072-4292},
ABSTRACT = {Deep-learning-based object detection algorithms have significantly improved the performance of wheat spike detection. However, UAV images crowned with small-sized, highly dense, and overlapping spikes cause the accuracy to decrease for detection. This paper proposes an improved YOLOv5 (You Look Only Once)-based method to detect wheat spikes accurately in UAV images and solve spike error detection and miss detection caused by occlusion conditions. The proposed method introduces data cleaning and data augmentation to improve the generalization ability of the detection network. The network is rebuilt by adding a microscale detection layer, setting prior anchor boxes, and adapting the confidence loss function of the detection layer based on the IoU (Intersection over Union). These refinements improve the feature extraction for small-sized wheat spikes and lead to better detection accuracy. With the confidence weights, the detection boxes in multiresolution images are fused to increase the accuracy under occlusion conditions. The result shows that the proposed method is better than the existing object detection algorithms, such as Faster RCNN, Single Shot MultiBox Detector (SSD), RetinaNet, and standard YOLOv5. The average accuracy (AP) of wheat spike detection in UAV images is 94.1%, which is 10.8% higher than the standard YOLOv5. Thus, the proposed method is a practical way to handle the spike detection in complex field scenarios and provide technical references for field-level wheat phenotype monitoring.},
DOI = {10.3390/rs13163095}
}



@Article{rs13163100,
AUTHOR = {Qi, Guanghui and Chang, Chunyan and Yang, Wei and Gao, Peng and Zhao, Gengxing},
TITLE = {Soil Salinity Inversion in Coastal Corn Planting Areas by the Satellite-UAV-Ground Integration Approach},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3100},
URL = {https://www.mdpi.com/2072-4292/13/16/3100},
ISSN = {2072-4292},
ABSTRACT = {Soil salinization is a significant factor affecting corn growth in coastal areas. How to use multi-source remote sensing data to achieve the target of rapid, efficient and accurate soil salinity monitoring in a large area is worth further study. In this research, using Kenli District of the Yellow River Delta as study area, the inversion of soil salinity in a corn planting area was carried out based on the integration of ground imaging hyperspectral, unmanned aerial vehicles (UAV) multispectral and Sentinel-2A satellite multispectral images. The UAV and ground images were fused, and the partial least squares inversion model was constructed by the fused UAV image. Then, inversion model was scaled up to the satellite by the TsHARP method, and finally, the accuracy of the satellite-UAV-ground inversion model and results was verified. The results show that the band fusion of UAV and ground images effectively enrich the spectral information of the UAV image. The accuracy of the inversion model constructed based on the fused UAV images was improved. The inversion results of soil salinity based on the integration of satellite-UAV-ground were highly consistent with the measured soil salinity (R2 = 0.716 and RMSE = 0.727), and the inversion model had excellent universal applicability. This research integrated the advantages of multi-source data to establish a unified satellite-UAV-ground model, which improved the ability of large-scale remote sensing data to finely indicate soil salinity.},
DOI = {10.3390/rs13163100}
}



@Article{min11080846,
AUTHOR = {Sinaice, Brian Bino and Owada, Narihiro and Saadat, Mahdi and Toriya, Hisatoshi and Inagaki, Fumiaki and Bagai, Zibisani and Kawamura, Youhei},
TITLE = {Coupling NCA Dimensionality Reduction with Machine Learning in Multispectral Rock Classification Problems},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {846},
URL = {https://www.mdpi.com/2075-163X/11/8/846},
ISSN = {2075-163X},
ABSTRACT = {Though multitudes of industries depend on the mining industry for resources, this industry has taken hits in terms of declining mineral ore grades and its current use of traditional, time-consuming and computationally costly rock and mineral identification methods. Therefore, this paper proposes integrating Hyperspectral Imaging, Neighbourhood Component Analysis (NCA) and Machine Learning (ML) as a combined system that can identify rocks and minerals. Modestly put, hyperspectral imaging gathers electromagnetic signatures of the rocks in hundreds of spectral bands. However, this data suffers from what is termed the ‘dimensionality curse’, which led to our employment of NCA as a dimensionality reduction technique. NCA, in turn, highlights the most discriminant feature bands, number of which being dependent on the intended application(s) of this system. Our envisioned application is rock and mineral classification via unmanned aerial vehicle (UAV) drone technology. In this study, we performed a 204-hyperspectral to 5-band multispectral reduction, because current production drones are limited to five multispectral bands sensors. Based on these bands, we applied ML to identify and classify rocks, thereby proving our hypothesis, reducing computational costs, attaining an ML classification accuracy of 71%, and demonstrating the potential mining industry optimisations attainable through this integrated system.},
DOI = {10.3390/min11080846}
}



@Article{make3030033,
AUTHOR = {Sejr, Jonas Herskind and Schneider-Kamp, Peter and Ayoub, Naeem},
TITLE = {Surrogate Object Detection Explainer (SODEx) with YOLOv4 and LIME},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {3},
YEAR = {2021},
NUMBER = {3},
PAGES = {662--671},
URL = {https://www.mdpi.com/2504-4990/3/3/33},
ISSN = {2504-4990},
ABSTRACT = {Due to impressive performance, deep neural networks for object detection in images have become a prevalent choice. Given the complexity of the neural network models used, users of these algorithms are typically given no hint as to how the objects were found. It remains, for example, unclear whether an object is detected based on what it looks like or based on the context in which it is located. We have developed an algorithm, Surrogate Object Detection Explainer (SODEx), that can explain any object detection algorithm using any classification explainer. We evaluate SODEx qualitatively and quantitatively by detecting objects in the COCO dataset with YOLOv4 and explaining these detections with LIME. This empirical evaluation does not only demonstrate the value of explainable object detection, it also provides valuable insights into how YOLOv4 detects objects.},
DOI = {10.3390/make3030033}
}



@Article{app11167240,
AUTHOR = {Jembre, Yalew Zelalem and Nugroho, Yuniarto Wimbo and Khan, Muhammad Toaha Raza and Attique, Muhammad and Paul, Rajib and Shah, Syed Hassan Ahmed and Kim, Beomjoon},
TITLE = {Evaluation of Reinforcement and Deep Learning Algorithms in Controlling Unmanned Aerial Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7240},
URL = {https://www.mdpi.com/2076-3417/11/16/7240},
ISSN = {2076-3417},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are abundantly becoming a part of society, which is a trend that is expected to grow even further. The quadrotor is one of the drone technologies that is applicable in many sectors and in both military and civilian activities, with some applications requiring autonomous flight. However, stability, path planning, and control remain significant challenges in autonomous quadrotor flights. Traditional control algorithms, such as proportional-integral-derivative (PID), have deficiencies, especially in tuning. Recently, machine learning has received great attention in flying UAVs to desired positions autonomously. In this work, we configure the quadrotor to fly autonomously by using agents (the machine learning schemes being used to fly the quadrotor autonomously) to learn about the virtual physical environment. The quadrotor will fly from an initial to a desired position. When the agent brings the quadrotor closer to the desired position, it is rewarded; otherwise, it is punished. Two reinforcement learning models, Q-learning and SARSA, and a deep learning deep Q-network network are used as agents. The simulation is conducted by integrating the robot operating system (ROS) and Gazebo, which allowed for the implementation of the learning algorithms and the physical environment, respectively. The result has shown that the Deep Q-network network with Adadelta optimizer is the best setting to fly the quadrotor from the initial to desired position.},
DOI = {10.3390/app11167240}
}



@Article{math9161868,
AUTHOR = {Marchetti, Francesco and Minisci, Edmondo},
TITLE = {Genetic Programming Guidance Control System for a Reentry Vehicle under Uncertainties},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {1868},
URL = {https://www.mdpi.com/2227-7390/9/16/1868},
ISSN = {2227-7390},
ABSTRACT = {As technology improves, the complexity of controlled systems increases as well. Alongside it, these systems need to face new challenges, which are made available by this technology advancement. To overcome these challenges, the incorporation of AI into control systems is changing its status, from being just an experiment made in academia, towards a necessity. Several methods to perform this integration of AI into control systems have been considered in the past. In this work, an approach involving GP to produce, offline, a control law for a reentry vehicle in the presence of uncertainties on the environment and plant models is studied, implemented and tested. The results show the robustness of the proposed approach, which is capable of producing a control law of a complex nonlinear system in the presence of big uncertainties. This research aims to describe and analyze the effectiveness of a control approach to generate a nonlinear control law for a highly nonlinear system in an automated way. Such an approach would benefit the control practitioners by providing an alternative to classical control approaches, without having to rely on linearization techniques.},
DOI = {10.3390/math9161868}
}



@Article{s21165323,
AUTHOR = {Kim, Yongsu and Kang, Hyoeun and Suryanto, Naufal and Larasati, Harashta Tatimma and Mukaroh, Afifatul and Kim, Howon},
TITLE = {Extended Spatially Localized Perturbation GAN (eSLP-GAN) for Robust Adversarial Camouflage Patches},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5323},
URL = {https://www.mdpi.com/1424-8220/21/16/5323},
PubMedID = {34450763},
ISSN = {1424-8220},
ABSTRACT = {Deep neural networks (DNNs), especially those used in computer vision, are highly vulnerable to adversarial attacks, such as adversarial perturbations and adversarial patches. Adversarial patches, often considered more appropriate for a real-world attack, are attached to the target object or its surroundings to deceive the target system. However, most previous research employed adversarial patches that are conspicuous to human vision, making them easy to identify and counter. Previously, the spatially localized perturbation GAN (SLP-GAN) was proposed, in which the perturbation was only added to the most representative area of the input images, creating a spatially localized adversarial camouflage patch that excels in terms of visual fidelity and is, therefore, difficult to detect by human vision. In this study, the use of the method called eSLP-GAN was extended to deceive classifiers and object detection systems. Specifically, the loss function was modified for greater compatibility with an object-detection model attack and to increase robustness in the real world. Furthermore, the applicability of the proposed method was tested on the CARLA simulator for a more authentic real-world attack scenario.},
DOI = {10.3390/s21165323}
}



@Article{s21165326,
AUTHOR = {Ramalingam, Balakrishnan and Tun, Thein and Mohan, Rajesh Elara and Gómez, Braulio Félix and Cheng, Ruoxi and Balakrishnan, Selvasundari and Mohan Rayaguru, Madan and Hayat, Abdullah Aamir},
TITLE = {AI Enabled IoRT Framework for Rodent Activity Monitoring in a False Ceiling Environment},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5326},
URL = {https://www.mdpi.com/1424-8220/21/16/5326},
PubMedID = {34450767},
ISSN = {1424-8220},
ABSTRACT = {Routine rodent inspection is essential to curbing rat-borne diseases and infrastructure damages within the built environment. Rodents find false ceilings to be a perfect spot to seek shelter and construct their habitats. However, a manual false ceiling inspection for rodents is laborious and risky. This work presents an AI-enabled IoRT framework for rodent activity monitoring inside a false ceiling using an in-house developed robot called “Falcon”. The IoRT serves as a bridge between the users and the robots, through which seamless information sharing takes place. The shared images by the robots are inspected through a Faster RCNN ResNet 101 object detection algorithm, which is used to automatically detect the signs of rodent inside a false ceiling. The efficiency of the rodent activity detection algorithm was tested in a real-world false ceiling environment, and detection accuracy was evaluated with the standard performance metrics. The experimental results indicate that the algorithm detects rodent signs and 3D-printed rodents with a good confidence level.},
DOI = {10.3390/s21165326}
}



@Article{drones5030077,
AUTHOR = {Safonova, Anastasiia and Hamad, Yousif and Dmitriev, Egor and Georgiev, Georgi and Trenkin, Vladislav and Georgieva, Margarita and Dimitrov, Stelian and Iliev, Martin},
TITLE = {Individual Tree Crown Delineation for the Species Classification and Assessment of Vital Status of Forest Stands from UAV Images},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {77},
URL = {https://www.mdpi.com/2504-446X/5/3/77},
ISSN = {2504-446X},
ABSTRACT = {Monitoring the structure parameters and damage to trees plays an important role in forest management. Remote-sensing data collected by an unmanned aerial vehicle (UAV) provides valuable resources to improve the efficiency of decision making. In this work, we propose an approach to enhance algorithms for species classification and assessment of the vital status of forest stands by using automated individual tree crowns delineation (ITCD). The approach can be potentially used for inventory and identifying the health status of trees in regional-scale forest areas. The proposed ITCD algorithm goes through three stages: preprocessing (contrast enhancement), crown segmentation based on wavelet transformation and morphological operations, and boundaries detection. The performance of the ITCD algorithm was demonstrated for different test plots containing homogeneous and complex structured forest stands. For typical scenes, the crown contouring accuracy is about 95%. The pixel-by-pixel classification is based on the ensemble supervised classification method error correcting output codes with the Gaussian kernel support vector machine chosen as a binary learner. We demonstrated that pixel-by-pixel species classification of multi-spectral images can be performed with a total error of about 1%, which is significantly less than by processing RGB images. The advantage of the proposed approach lies in the combined processing of multispectral and RGB photo images.},
DOI = {10.3390/drones5030077}
}



@Article{ani11082345,
AUTHOR = {Monteiro, António and Santos, Sérgio and Gonçalves, Pedro},
TITLE = {Precision Agriculture for Crop and Livestock Farming—Brief Review},
JOURNAL = {Animals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2345},
URL = {https://www.mdpi.com/2076-2615/11/8/2345},
PubMedID = {34438802},
ISSN = {2076-2615},
ABSTRACT = {In the last few decades, agriculture has played an important role in the worldwide economy. The need to produce more food for a rapidly growing population is creating pressure on crop and animal production and a negative impact to the environment. On the other hand, smart farming technologies are becoming increasingly common in modern agriculture to assist in optimizing agricultural and livestock production and minimizing the wastes and costs. Precision agriculture (PA) is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. Precision livestock farming (PLF), relying on the automatic monitoring of individual animals, is used for animal growth, milk production, and the detection of diseases as well as to monitor animal behavior and their physical environment, among others. This study aims to briefly review recent scientific and technological trends in PA and their application in crop and livestock farming, serving as a simple research guide for the researcher and farmer in the application of technology to agriculture. The development and operation of PA applications involve several steps and techniques that need to be investigated further to make the developed systems accurate and implementable in commercial environments.},
DOI = {10.3390/ani11082345}
}



@Article{rs13163145,
AUTHOR = {Singh, Sarvesh Kumar and Banerjee, Bikram Pratap and Raval, Simit},
TITLE = {Three-Dimensional Unique-Identifier-Based Automated Georeferencing and Coregistration of Point Clouds in Underground Mines},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3145},
URL = {https://www.mdpi.com/2072-4292/13/16/3145},
ISSN = {2072-4292},
ABSTRACT = {Spatially referenced and geometrically accurate laser scans are essential for mapping and monitoring applications in underground mines to ensure safe and smooth operation. However, obtaining an absolute 3D map in an underground mine environment is challenging using laser scanning due to the unavailability of global navigation satellite system (GNSS) signals. Consequently, applications that require georeferenced point cloud or coregistered multitemporal point clouds such as detecting changes, monitoring deformations, tracking mine logistics, measuring roadway convergence rate and evaluating construction performance become challenging. Current mapping practices largely include a manual selection of discernable reference points in laser scans for georeferencing and coregistration which is often time-consuming, arduous and error-prone. Moreover, challenges in obtaining a sensor positioning framework, the presence of structurally symmetric layouts and highly repetitive features (such as roof bolts) makes the multitemporal scans difficult to georeference and coregister. This study aims at overcoming these practical challenges through development of three-dimensional unique identifiers (3DUIDs) and a 3D registration (3DReG) workflow. Field testing of the developed approach in an underground coal mine has been found effective with an accuracy of 1.76 m in georeferencing and 0.16 m in coregistration for a scan length of 850 m. Additionally, automatic extraction of mine roadway profile has been demonstrated using 3DUID which is often a compliant and operational requirement for mitigating roadway related hazards that includes roadway convergence rate, roof/rock falls, floor heaves and vehicle clearance for collision avoidance. Potential applications of 3DUID include roadway profile extraction, guided automation, sensor calibration, reference targets for a routine survey and deformation monitoring.},
DOI = {10.3390/rs13163145}
}



@Article{rs13163146,
AUTHOR = {Chen, Dong and Li, Jing and Di, Shaoning and Peethambaran, Jiju and Xiang, Guiqiu and Wan, Lincheng and Li, Xianghong},
TITLE = {Critical Points Extraction from Building Façades by Analyzing Gradient Structure Tensor},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3146},
URL = {https://www.mdpi.com/2072-4292/13/16/3146},
ISSN = {2072-4292},
ABSTRACT = {This paper proposes a building façade contouring method from LiDAR (Light Detection and Ranging) scans and photogrammetric point clouds. To this end, we calculate the confidence property at multiple scales for an individual point cloud to measure the point cloud’s quality. The confidence property is utilized in the definition of the gradient for each point. We encode the individual point gradient structure tensor, whose eigenvalues reflect the gradient variations in the local neighborhood areas. The critical point clouds representing the building façade and rooftop (if, of course, such rooftops exist) contours are then extracted by jointly analyzing dual-thresholds of the gradient and gradient structure tensor. Based on the requirements of compact representation, the initial obtained critical points are finally downsampled, thereby achieving a tradeoff between the accurate geometry and abstract representation at a reasonable level. Various experiments using representative buildings in Semantic3D benchmark and other ubiquitous point clouds from ALS DublinCity and Dutch AHN3 datasets, MLS TerraMobilita/iQmulus 3D urban analysis benchmark, UAV-based photogrammetric dataset, and GeoSLAM ZEB-HORIZON scans have shown that the proposed method generates building contours that are accurate, lightweight, and robust to ubiquitous point clouds. Two comparison experiments also prove the superiority of the proposed method in terms of topological correctness, geometric accuracy, and representation compactness.},
DOI = {10.3390/rs13163146}
}



@Article{rs13163150,
AUTHOR = {Jovanović, Dušan and Gavrilović, Milan and Sladić, Dubravka and Radulović, Aleksandra and Govedarica, Miro},
TITLE = {Building Change Detection Method to Support Register of Identified Changes on Buildings},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3150},
URL = {https://www.mdpi.com/2072-4292/13/16/3150},
ISSN = {2072-4292},
ABSTRACT = {Based on a newly adopted “Rulebook on the records of identified changes on buildings in Serbia” (2020) that regulates the content, establishment, maintenance and use of records on identified changes on buildings, it is expected that the geodetic-cadastral information system will be extended with these records. The records contain data on determined changes of buildings in relation to the reference epoch of aerial or satellite imagery, namely data on buildings: (1) that are not registered in the real estate cadastre; (2) which are registered in the real estate cadastre, and have been changed in terms of the dimensions in relation to the data registered in the real estate cadastre; (3) which are registered in the real estate cadastre, but are removed on the ground. For this purpose, the LADM-based cadastral data model for Serbia is extended to include records on identified changes on buildings. In the year 2020, Republic Geodetic Authority commenced a new satellite acquisition for the purpose of restoration of official buildings registry, as part of a World Bank project for improving land administration in Serbia. Using this satellite imagery and existing cadastral data, we propose a method based on comparison of object-based and pixel-based image analysis approaches to automatically detect newly built, changed or demolished buildings and import these data into extended cadastral records. Our results, using only VHR images containing only RGB and NIR bands, showed object identification accuracy ranging from 84% to 88%, with kappa statistic from 89% to 96%. The accuracy of obtained results is satisfactory for the purpose of developing a register of changes on buildings to keep cadastral records up to date and to support activities related to legalization of illegal buildings, etc.},
DOI = {10.3390/rs13163150}
}



@Article{ijgi10080536,
AUTHOR = {Castro Noblejas, Hugo and Sortino Barrionuevo, Juan Francisco and Gumiel Muñoz, Darío and Mérida Rodríguez, Matías Francisco},
TITLE = {A Methodological Proposal for the Analysis of Lighting the House Building Façades},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {536},
URL = {https://www.mdpi.com/2220-9964/10/8/536},
ISSN = {2220-9964},
ABSTRACT = {Natural lighting is a fundamental element in the habitability of dwellings. However, it is still difficult to calculate its effect on the façades of the buildings in detail, due to the morphological complexity of the property itself, as well as the environment that surrounds it. This study provides a methodological proposal that uses pre-existing open data to extrude buildings by using a GIS procedure. Based on three selected real estate properties with different characteristics in the city of Marbella (Spain), the hours of sunlight received by each building’s façade are calculated, taking into account the digital land model and the digital surface model of the area. The results confirm the usefulness of the method to measure and analyze differences in luminosity between buildings with similar urban characteristics and their surroundings, as well as to record the differences in luminosity between floors and the orientations of the same building at several heights. The methodological proposal opens a path for many applications related to energy efficiency, housing conditions, and property valuation.},
DOI = {10.3390/ijgi10080536}
}



@Article{s21165407,
AUTHOR = {Košťák, Milan and Slabý, Antonín},
TITLE = {Designing a Simple Fiducial Marker for Localization in Spatial Scenes Using Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5407},
URL = {https://www.mdpi.com/1424-8220/21/16/5407},
PubMedID = {34450848},
ISSN = {1424-8220},
ABSTRACT = {The paper describes the process of designing a simple fiducial marker. The marker is meant for use in augmented reality applications. Unlike other systems, it does not encode any information, but it can be used for obtaining the position, rotation, relative size, and projective transformation. Also, the system works well with motion blur and is resistant to the marker’s imperfections, which could theoretically be drawn only by hand. Previous systems put constraints on colors that need to be used to form the marker. The proposed system works with any saturated color, leading to better blending with the surrounding environment. The marker’s final shape is a rectangular area of a solid color with three lines of a different color going from the center to three corners of the rectangle. Precise detection can be achieved using neural networks, given that the training set is very varied and well designed. A detailed literature review was performed, and no such system was found. Therefore, the proposed design is novel for localization in the spatial scene. The testing proved that the system works well both indoor and outdoor, and the detections are precise.},
DOI = {10.3390/s21165407}
}



@Article{rs13163165,
AUTHOR = {Li, Wenning and Li, Yi and Gong, Jianhua and Feng, Quanlong and Zhou, Jieping and Sun, Jun and Shi, Chenhui and Hu, Weidong},
TITLE = {Urban Water Extraction with UAV High-Resolution Remote Sensing Data Based on an Improved U-Net Model},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3165},
URL = {https://www.mdpi.com/2072-4292/13/16/3165},
ISSN = {2072-4292},
ABSTRACT = {Obtaining water body images quickly and reliably is important to guide human production activities and study urban change. This paper presents a fast and accurate method to identify water bodies in complex environments based on UAV high-resolution images. First, an improved U-Net (SU-Net) model is proposed in this paper. By increasing the number of connections in the middle layer of the neural network, more image features can be retained through S-shaped circular connections. Second, aiming at the interference of mixed ground objects and dark ground objects on water detection, the fusion of a deep learning network and visual features is investigated. We analyse the influence of a wavelet transform and grey level cooccurrence matrix (GLCM) on water extraction. Using a confusion matrix to evaluate accuracy, the following conclusions are drawn: (1) Compared with existing methods, the SU-Net method achieves a significant improvement in accuracy, and the overall accuracy (OA) is 96.25%. The kappa coefficient (KC) is 0.952. (2) SU-Net combined with the GLCM has a higher accuracy (OA is 97.4%) and robustness in distinguishing mixed and dark objects. Based on this method, a distinct water boundary in urban areas, which provides data for urban water vector mapping, can be obtained.},
DOI = {10.3390/rs13163165}
}



@Article{electronics10161931,
AUTHOR = {Wang, Zi-Hao and Chen, Wen-Jie and Qin, Kai-Yu},
TITLE = {Dynamic Target Tracking and Ingressing of a Small UAV Using Monocular Sensor Based on the Geometric Constraints},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {1931},
URL = {https://www.mdpi.com/2079-9292/10/16/1931},
ISSN = {2079-9292},
ABSTRACT = {In many applications of airborne visual techniques for unmanned aerial vehicles (UAVs), lightweight sensors and efficient visual positioning and tracking algorithms are essential in a GNSS-denied environment. Meanwhile, many tasks require the ability of recognition, localization, avoiding, or flying pass through these dynamic obstacles. In this paper, for a small UAV equipped with a lightweight monocular sensor, a single-frame parallel-features positioning method (SPPM) is proposed and verified for a real-time dynamic target tracking and ingressing problem. The solution is featured with systematic modeling of the geometric characteristics of moving targets, and the introduction of numeric iteration algorithms to estimate the geometric center of moving targets. The geometric constraint relationships of the target feature points are modeled as non-linear equations for scale estimation. Experiments show that the root mean square error percentage of static target tracking is less than 1.03% and the root mean square error of dynamic target tracking is less than 7.92 cm. Comprehensive indoor flight experiments are conducted to show the real-time convergence of the algorithm, the effectiveness of the solution in locating and tracking a moving target, and the excellent robustness to measurement noises.},
DOI = {10.3390/electronics10161931}
}



@Article{rs13163188,
AUTHOR = {Takechi, Hitoshi and Aragaki, Shunsuke and Irie, Mitsuteru},
TITLE = {Differentiation of River Sediments Fractions in UAV Aerial Images by Convolution Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3188},
URL = {https://www.mdpi.com/2072-4292/13/16/3188},
ISSN = {2072-4292},
ABSTRACT = {Riverbed material has multiple functions in river ecosystems, such as habitats, feeding grounds, spawning grounds, and shelters for aquatic organisms, and particle size of riverbed material reflects the tractive force of the channel flow. Therefore, regular surveys of riverbed material are conducted for environmental protection and river flood control projects. The field method is the most conventional riverbed material survey. However, conventional surveys of particle size of riverbed material require much labor, time, and cost to collect material on site. Furthermore, its spatial representativeness is also a problem because of the limited survey area against a wide riverbank. As a further solution to these problems, in this study, we tried an automatic classification of riverbed conditions using aerial photography with an unmanned aerial vehicle (UAV) and image recognition with artificial intelligence (AI) to improve survey efficiency. Due to using AI for image processing, a large number of images can be handled regardless of whether they are of fine or coarse particles. We tried a classification of aerial riverbed images that have the difference of particle size characteristics with a convolutional neural network (CNN). GoogLeNet, Alexnet, VGG-16 and ResNet, the common pre-trained networks, were retrained to perform the new task with the 70 riverbed images using transfer learning. Among the networks tested, GoogleNet showed the best performance for this study. The overall accuracy of the image classification reached 95.4%. On the other hand, it was supposed that shadows of the gravels caused the error of the classification. The network retrained with the images taken in the uniform temporal period gives higher accuracy for classifying the images taken in the same period as the training data. The results suggest the potential of evaluating riverbed materials using aerial photography with UAV and image recognition with CNN.},
DOI = {10.3390/rs13163188}
}



@Article{rs13163190,
AUTHOR = {Li, Kai-Yun and Burnside, Niall G. and de Lima, Raul Sampaio and Peciña, Miguel Villoslada and Sepp, Karli and Cabral Pinheiro, Victor Henrique and de Lima, Bruno Rucy Carneiro Alves and Yang, Ming-Der and Vain, Ants and Sepp, Kalev},
TITLE = {An Automated Machine Learning Framework in Unmanned Aircraft Systems: New Insights into Agricultural Management Practices Recognition Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3190},
URL = {https://www.mdpi.com/2072-4292/13/16/3190},
ISSN = {2072-4292},
ABSTRACT = {The recent trend of automated machine learning (AutoML) has been driving further significant technological innovation in the application of artificial intelligence from its automated algorithm selection and hyperparameter optimization of the deployable pipeline model for unraveling substance problems. However, a current knowledge gap lies in the integration of AutoML technology and unmanned aircraft systems (UAS) within image-based data classification tasks. Therefore, we employed a state-of-the-art (SOTA) and completely open-source AutoML framework, Auto-sklearn, which was constructed based on one of the most widely used ML systems: Scikit-learn. It was combined with two novel AutoML visualization tools to focus particularly on the recognition and adoption of UAS-derived multispectral vegetation indices (VI) data across a diverse range of agricultural management practices (AMP). These include soil tillage methods (STM), cultivation methods (CM), and manure application (MA), and are under the four-crop combination fields (i.e., red clover-grass mixture, spring wheat, pea-oat mixture, and spring barley). Furthermore, they have currently not been efficiently examined and accessible parameters in UAS applications are absent for them. We conducted the comparison of AutoML performance using three other common machine learning classifiers, namely Random Forest (RF), support vector machine (SVM), and artificial neural network (ANN). The results showed AutoML achieved the highest overall classification accuracy numbers after 1200 s of calculation. RF yielded the second-best classification accuracy, and SVM and ANN were revealed to be less capable among some of the given datasets. Regarding the classification of AMPs, the best recognized period for data capture occurred in the crop vegetative growth stage (in May). The results demonstrated that CM yielded the best performance in terms of classification, followed by MA and STM. Our framework presents new insights into plant–environment interactions with capable classification capabilities. It further illustrated the automatic system would become an important tool in furthering the understanding for future sustainable smart farming and field-based crop phenotyping research across a diverse range of agricultural environmental assessment and management applications.},
DOI = {10.3390/rs13163190}
}



@Article{rs13163191,
AUTHOR = {Ezzy, Haitham and Charter, Motti and Bonfante, Antonello and Brook, Anna},
TITLE = {How the Small Object Detection via Machine Learning and UAS-Based Remote-Sensing Imagery Can Support the Achievement of SDG2: A Case Study of Vole Burrows},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3191},
URL = {https://www.mdpi.com/2072-4292/13/16/3191},
ISSN = {2072-4292},
ABSTRACT = {Small mammals, and particularly rodents, are common inhabitants of farmlands, where they play key roles in the ecosystem, but when overabundant, they can be major pests, able to reduce crop production and farmers’ incomes, with tangible effects on the achievement of Sustainable Development Goals no 2 (SDG2, Zero Hunger) of the United Nations. Farmers do not currently have a standardized, accurate method of detecting the presence, abundance, and locations of rodents in their fields, and hence do not have environmentally efficient methods of rodent control able to promote sustainable agriculture oriented to reduce the environmental impacts of cultivation. New developments in unmanned aerial system (UAS) platforms and sensor technology facilitate cost-effective data collection through simultaneous multimodal data collection approaches at very high spatial resolutions in environmental and agricultural contexts. Object detection from remote-sensing images has been an active research topic over the last decade. With recent increases in computational resources and data availability, deep learning-based object detection methods are beginning to play an important role in advancing remote-sensing commercial and scientific applications. However, the performance of current detectors on various UAS-based datasets, including multimodal spatial and physical datasets, remains limited in terms of small object detection. In particular, the ability to quickly detect small objects from a large observed scene (at field scale) is still an open question. In this paper, we compare the efficiencies of applying one- and two-stage detector models to a single UAS-based image and a processed (via Pix4D mapper photogrammetric program) UAS-based orthophoto product to detect rodent burrows, for agriculture/environmental applications as to support farmer activities in the achievements of SDG2. Our results indicate that the use of multimodal data from low-cost UASs within a self-training YOLOv3 model can provide relatively accurate and robust detection for small objects (mAP of 0.86 and an F1-score of 93.39%), and can deliver valuable insights for field management with high spatial precision able to reduce the environmental costs of crop production in the direction of precision agriculture management.},
DOI = {10.3390/rs13163191}
}



@Article{rs13163207,
AUTHOR = {Feng, Shuai and Cao, Yingli and Xu, Tongyu and Yu, Fenghua and Zhao, Dongxue and Zhang, Guosheng},
TITLE = {Rice Leaf Blast Classification Method Based on Fused Features and One-Dimensional Deep Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3207},
URL = {https://www.mdpi.com/2072-4292/13/16/3207},
ISSN = {2072-4292},
ABSTRACT = {Rice leaf blast, which is seriously affecting the yield and quality of rice around the world, is a fungal disease that easily develops under high temperature and humidity conditions. Therefore, the use of accurate and non-destructive diagnostic methods is important for rice production management. Hyperspectral imaging technology is a type of crop disease identification method with great potential. However, a large amount of redundant information mixed in hyperspectral data makes it more difficult to establish an efficient disease classification model. At the same time, the difficulty and small scale of agricultural hyperspectral imaging data acquisition has resulted in unrepresentative features being acquired. Therefore, the focus of this study was to determine the best classification features and classification models for the five disease classes of leaf blast in order to improve the accuracy of grading the disease. First, the hyperspectral imaging data were pre-processed in order to extract rice leaf samples of five disease classes, and the number of samples was increased by data augmentation methods. Secondly, spectral feature wavelengths, vegetation indices and texture features were obtained based on the amplified sample data. Thirdly, seven one-dimensional deep convolutional neural networks (DCNN) models were constructed based on spectral feature wavelengths, vegetation indices, texture features and their fusion features. Finally, the model in this paper was compared and analyzed with the Inception V3, ZF-Net, TextCNN and bidirectional gated recurrent unit (BiGRU); support vector machine (SVM); and extreme learning machine (ELM) models in order to determine the best classification features and classification models for different disease classes of leaf blast. The results showed that the classification model constructed using fused features was significantly better than the model constructed with a single feature in terms of accuracy in grading the degree of leaf blast disease. The best performance was achieved with the combination of the successive projections algorithm (SPA) selected feature wavelengths and texture features (TFs). The modeling results also show that the DCNN model provides better classification capability for disease classification than the Inception V3, ZF-Net, TextCNN, BiGRU, SVM and ELM classification models. The SPA + TFs-DCNN achieved the best classification accuracy with an overall accuracy (OA) and Kappa of 98.58% and 98.22%, respectively. In terms of the classification of the specific different disease classes, the F1-scores for diseases of classes 0, 1 and 2 were all 100%, while the F1-scores for diseases of classes 4 and 5 were 96.48% and 96.68%, respectively. This study provides a new method for the identification and classification of rice leaf blast and a research basis for assessing the extent of the disease in the field.},
DOI = {10.3390/rs13163207}
}



@Article{s21165460,
AUTHOR = {Lang, Lei and Xu, Ke and Zhang, Qian and Wang, Dong},
TITLE = {Fast and Accurate Object Detection in Remote Sensing Images Based on Lightweight Deep Neural Network},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5460},
URL = {https://www.mdpi.com/1424-8220/21/16/5460},
PubMedID = {34450908},
ISSN = {1424-8220},
ABSTRACT = {Deep learning-based object detection in remote sensing images is an important yet challenging task due to a series of difficulties, such as complex geometry scene, dense target quantity, and large variant in object distributions and scales. Moreover, algorithm designers also have to make a trade-off between model’s complexity and accuracy to meet the real-world deployment requirements. To deal with these challenges, we proposed a lightweight YOLO-like object detector with the ability to detect objects in remote sensing images with high speed and high accuracy. The detector is constructed with efficient channel attention layers to improve the channel information sensitivity. Differential evolution was also developed to automatically find the optimal anchor configurations to address issue of large variant in object scales. Comprehensive experiment results show that the proposed network outperforms state-of-the-art lightweight models by 5.13% and 3.58% in accuracy on the RSOD and DIOR dataset, respectively. The deployed model on an NVIDIA Jetson Xavier NX embedded board can achieve a detection speed of 58 FPS with less than 10W power consumption, which makes the proposed detector very suitable for low-cost low-power remote sensing application scenarios.},
DOI = {10.3390/s21165460}
}



@Article{s21165476,
AUTHOR = {Wang, Rui and Zou, Jialing and Wen, James Zhiqing},
TITLE = {SFA-MDEN: Semantic-Feature-Aided Monocular Depth Estimation Network Using Dual Branches},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5476},
URL = {https://www.mdpi.com/1424-8220/21/16/5476},
PubMedID = {34450917},
ISSN = {1424-8220},
ABSTRACT = {Monocular depth estimation based on unsupervised learning has attracted great attention due to the rising demand for lightweight monocular vision sensors. Inspired by multi-task learning, semantic information has been used to improve the monocular depth estimation models. However, multi-task learning is still limited by multi-type annotations. As far as we know, there are scarcely any large public datasets that provide all the necessary information. Therefore, we propose a novel network architecture Semantic-Feature-Aided Monocular Depth Estimation Network (SFA-MDEN) to extract multi-resolution depth features and semantic features, which are merged and fed into the decoder, with the goal of predicting depth with the support of semantics. Instead of using loss functions to relate the semantics and depth, the fusion of feature maps for semantics and depth is employed to predict the monocular depth. Therefore, two accessible datasets with similar topics for depth estimation and semantic segmentation can meet the requirements of SFA-MDEN for training sets. We explored the performance of the proposed SFA-MDEN with experiments on different datasets, including KITTI, Make3D, and our own dataset BHDE-v1. The experimental results demonstrate that SFA-MDEN achieves competitive accuracy and generalization capacity compared to state-of-the-art methods.},
DOI = {10.3390/s21165476}
}



@Article{infrastructures6080115,
AUTHOR = {Munawar, Hafiz Suliman and Hammad, Ahmed W. A. and Haddad, Assed and Soares, Carlos Alberto Pereira and Waller, S. Travis},
TITLE = {Image-Based Crack Detection Methods: A Review},
JOURNAL = {Infrastructures},
VOLUME = {6},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {115},
URL = {https://www.mdpi.com/2412-3811/6/8/115},
ISSN = {2412-3811},
ABSTRACT = {Annually, millions of dollars are spent to carry out defect detection in key infrastructure including roads, bridges, and buildings. The aftermath of natural disasters like floods and earthquakes leads to severe damage to the urban infrastructure. Maintenance operations that follow for the damaged infrastructure often involve a visual inspection and assessment of their state to ensure their functional and physical integrity. Such damage may appear in the form of minor or major cracks, which gradually spread, leading to ultimate collapse or destruction of the structure. Crack detection is a very laborious task if performed via manual visual inspection. Many infrastructure elements need to be checked regularly and it is therefore not feasible as it will require significant human resources. This may also result in cases where cracks go undetected. A need, therefore, exists for performing automatic defect detection in infrastructure to ensure its effectiveness and reliability. Using image processing techniques, the captured or scanned images of the infrastructure parts can be analyzed to identify any possible defects. Apart from image processing, machine learning methods are being increasingly applied to ensure better performance outcomes and robustness in crack detection. This paper provides a review of image-based crack detection techniques which implement image processing and/or machine learning. A total of 30 research articles have been collected for the review which is published in top tier journals and conferences in the past decade. A comprehensive analysis and comparison of these methods are performed to highlight the most promising automated approaches for crack detection.},
DOI = {10.3390/infrastructures6080115}
}



@Article{rs13163234,
AUTHOR = {Cao, Jingwei and Song, Chuanxue and Song, Shixin and Xiao, Feng and Zhang, Xu and Liu, Zhiyang and Ang, Marcelo H.},
TITLE = {Robust Object Tracking Algorithm for Autonomous Vehicles in Complex Scenes},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3234},
URL = {https://www.mdpi.com/2072-4292/13/16/3234},
ISSN = {2072-4292},
ABSTRACT = {Object tracking is an essential aspect of environmental perception technology for autonomous vehicles. The existing object tracking algorithms can only be applied well to simple scenes. When the scenes become complex, the algorithms have poor tracking performance and insufficient robustness, and the problems of tracking drift and object loss are prone to occur. Therefore, a robust object tracking algorithm for autonomous vehicles in complex scenes is proposed. Firstly, we study the Siam-FC network and related algorithms, and analyze the problems that need to be addressed in object tracking. Secondly, the construction of a double-template Siamese network model based on multi-feature fusion is described, as is the use of the improved MobileNet V2 as the feature extraction backbone network, and the attention mechanism and template online update mechanism are introduced. Finally, relevant experiments were carried out based on public datasets and actual driving videos, with the aim of fully testing the tracking performance of the proposed algorithm on different objects in a variety of complex scenes. The results showed that, compared with other algorithms, the proposed algorithm had high tracking accuracy and speed, demonstrated stronger robustness and anti-interference abilities, and could still accurately track the object in real time without the introduction of complex structures. This algorithm can be effectively applied in intelligent vehicle driving assistance, and it will help to promote the further development and improvement of computer vision technology in the field of environmental perception.},
DOI = {10.3390/rs13163234}
}



@Article{rs13163240,
AUTHOR = {Zhang, Guangzong and Wu, Mengquan and Wei, Juan and He, Yufang and Niu, Lifeng and Li, Hanyu and Xu, Guochang},
TITLE = {Adaptive Threshold Model in Google Earth Engine: A Case Study of Ulva prolifera Extraction in the South Yellow Sea, China},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3240},
URL = {https://www.mdpi.com/2072-4292/13/16/3240},
ISSN = {2072-4292},
ABSTRACT = {An outbreak of Ulva prolifera poses a massive threat to coastal ecology in the Southern Yellow Sea, China (SYS). It is a necessity to extract its area and monitor its development accurately. At present, Ulva prolifera monitoring by remote sensing imagery is mostly based on a fixed threshold or artificial visual interpretation for threshold selection, which has large errors. In this paper, an adaptive threshold model based on Google Earth Engine (GEE) is proposed and applied to extract U. prolifera in the SYS. The model first applies the Floating Algae Index (FAI) or Normalized Difference Vegetation Index (NDVI) algorithm on the preprocessed remote sensing images and then uses the Canny Edge Filter and Otsu threshold segmentation algorithm to extract the threshold automatically. The model is applied to Landsat8/OLI and Sentinel-2/MSI images, and the confusion matrix and cross-sensor comparison are used to evaluate the accuracy and applicability of the model. The verification results show that the model extraction of U. prolifera based on the FAI algorithm has higher accuracy (R2 = 0.99, RMSE = 5.64) and better robustness. However, when the average cloud cover is more than 70% in the image (based on the statistical results of multi-year cloud cover information), the model based on the NDVI algorithm has better applicability and can extract the algae distributed at the edge of the cloud. When the model uses the FAI algorithm, it is named FAI-COM (model based on FAI, the Canny Edge Filter, and Otsu thresholding). And when the model uses the NDVI algorithm, it is named NDVI-COM (model based on NDVI, the Canny Edge Filter, and Otsu thresholding). Therefore, the final extraction results are generated by supplementing NDVI-COM results on the basis of FAI-COM extraction results in this paper. The F1-score of U. prolifera extracted results is above 0.85. The spatiotemporal distribution of U. prolifera in the South Yellow Sea from 2016 to 2020 is obtained through the model calculation. Overall, the coverage area of U. prolifera shows a decreasing trend over the five years. It is found that the delay in recovery time of Porphyra yezoensis culture facilities in the Northern Jiangsu Shoal and the manual salvage and cleaning-up of U. prolifera in May are among the reasons for the smaller interannual scale of algae in 2017 and 2018.},
DOI = {10.3390/rs13163240}
}



@Article{rs13163241,
AUTHOR = {Hassanzadeh, Amirhossein and Zhang, Fei and van Aardt, Jan and Murphy, Sean P. and Pethybridge, Sarah J.},
TITLE = {Broadacre Crop Yield Estimation Using Imaging Spectroscopy from Unmanned Aerial Systems (UAS): A Field-Based Case Study with Snap Bean},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3241},
URL = {https://www.mdpi.com/2072-4292/13/16/3241},
ISSN = {2072-4292},
ABSTRACT = {Accurate, precise, and timely estimation of crop yield is key to a grower’s ability to proactively manage crop growth and predict harvest logistics. Such yield predictions typically are based on multi-parametric models and in-situ sampling. Here we investigate the extension of a greenhouse study, to low-altitude unmanned aerial systems (UAS). Our principal objective was to investigate snap bean crop (Phaseolus vulgaris) yield using imaging spectroscopy (hyperspectral imaging) in the visible to near-infrared (VNIR; 400–1000 nm) region via UAS. We aimed to solve the problem of crop yield modelling by identifying spectral features explaining yield and evaluating the best time period for accurate yield prediction, early in time. We introduced a Python library, named Jostar, for spectral feature selection. Embedded in Jostar, we proposed a new ranking method for selected features that reaches an agreement between multiple optimization models. Moreover, we implemented a well-known denoising algorithm for the spectral data used in this study. This study benefited from two years of remotely sensed data, captured at multiple instances over the summers of 2019 and 2020, with 24 plots and 18 plots, respectively. Two harvest stage models, early and late harvest, were assessed at two different locations in upstate New York, USA. Six varieties of snap bean were quantified using two components of yield, pod weight and seed length. We used two different vegetation detection algorithms. the Red-Edge Normalized Difference Vegetation Index (RENDVI) and Spectral Angle Mapper (SAM), to subset the fields into vegetation vs. non-vegetation pixels. Partial least squares regression (PLSR) was used as the regression model. Among nine different optimization models embedded in Jostar, we selected the Genetic Algorithm (GA), Ant Colony Optimization (ACO), Simulated Annealing (SA), and Particle Swarm Optimization (PSO) and their resulting joint ranking. The findings show that pod weight can be explained with a high coefficient of determination (R2 = 0.78–0.93) and low root-mean-square error (RMSE = 940–1369 kg/ha) for two years of data. Seed length yield assessment resulted in higher accuracies (R2 = 0.83–0.98) and lower errors (RMSE = 4.245–6.018 mm). Among optimization models used, ACO and SA outperformed others and the SAM vegetation detection approach showed improved results when compared to the RENDVI approach when dense canopies were being examined. Wavelengths at 450, 500, 520, 650, 700, and 760 nm, were identified in almost all data sets and harvest stage models used. The period between 44–55 days after planting (DAP) the optimal time period for yield assessment. Future work should involve transferring the learned concepts to a multispectral system, for eventual operational use; further attention should also be paid to seed length as a ground truth data collection technique, since this yield indicator is far more rapid and straightforward.},
DOI = {10.3390/rs13163241}
}



@Article{e23081055,
AUTHOR = {Zhang, Tianci and Chen, Shutong and Chen, Zhengchuan},
TITLE = {Internet of Things: The Optimal Generation Rates under Preemption Strategy in a Multi-Source Queuing System},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1055},
URL = {https://www.mdpi.com/1099-4300/23/8/1055},
PubMedID = {34441195},
ISSN = {1099-4300},
ABSTRACT = {With the rapid development and wide application of the Internet of Things (IoT), how to provide timely and fresh information for strategic analysis and decision-making has become a key issue. Recent studies have shown that preemption strategies are of great importance to the improvement of information freshness. In view of this, we focus on the multi-source preemptive queuing model and investigate how to control the generation rate of each source to achieve the optimal overall information freshness. Specifically, we consider two typical preemption strategies: self-preemption strategy and global-preemption strategy. Noting that the urgency requirements of the systems on the data of each source are different, we propose the weighted average age of information (AoI) to characterize the overall information freshness of the system. For the self-preemption strategy, we prove that the optimal generation rate allocation is a convex problem and present an efficient algorithm to find the optimal solution. Additionally, we also derive a closed-form approximate optimal solution under light load cases to meet the demands for rapid deployment. For the global-preemption strategy, we directly derive the closed-form optimal solution of the corresponding problem. By comparing the optimized weighted average AoIs, the performance achieved by the global-preemption system was better than that achieved by the self-preemption system in terms of the overall timeliness. The numerical analysis verified the correctness of the theoretical analysis and that the proposed approximate solution had high accuracy not only under light load cases but also under other cases.},
DOI = {10.3390/e23081055}
}



@Article{s21165516,
AUTHOR = {Schäfer, Matthias and Strohmeier, Martin and Leonardi, Mauro and Lenders, Vincent},
TITLE = {LocaRDS: A Localization Reference Data Set},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5516},
URL = {https://www.mdpi.com/1424-8220/21/16/5516},
PubMedID = {34450957},
ISSN = {1424-8220},
ABSTRACT = {The use of wireless signals for the purposes of localization enables a host of applications relating to the determination and verification of the positions of network participants ranging from radar to satellite navigation. Consequently, this has been a longstanding interest of theoretical and practical research in mobile networks and many solutions have been proposed in the scientific literature. However, it is hard to assess the performance of these in the real world and, more importantly, to compare their advantages and disadvantages in a controlled scientific manner. With this work, we attempt to improve the current state of art methodology in localization research and to place it on a solid scientific grounding for future investigations. Concretely, we developed LocaRDS, an open reference data set of real-world crowdsourced flight data featuring more than 222 million measurements from over 50 million transmissions recorded by 323 sensors. We demonstrate how we can verify the quality of LocaRDS measurements so that it can be used to test, analyze and directly compare different localization methods. Finally, we provide an example implementation for the aircraft localization problem and a discussion of possible metrics for use with LocaRDS.},
DOI = {10.3390/s21165516}
}



@Article{rs13163247,
AUTHOR = {Yao, Guobiao and Yilmaz, Alper and Meng, Fei and Zhang, Li},
TITLE = {Review of Wide-Baseline Stereo Image Matching Based on Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3247},
URL = {https://www.mdpi.com/2072-4292/13/16/3247},
ISSN = {2072-4292},
ABSTRACT = {Strong geometric and radiometric distortions often exist in optical wide-baseline stereo images, and some local regions can include surface discontinuities and occlusions. Digital photogrammetry and computer vision researchers have focused on automatic matching for such images. Deep convolutional neural networks, which can express high-level features and their correlation, have received increasing attention for the task of wide-baseline image matching, and learning-based methods have the potential to surpass methods based on handcrafted features. Therefore, we focus on the dynamic study of wide-baseline image matching and review the main approaches of learning-based feature detection, description, and end-to-end image matching. Moreover, we summarize the current representative research using stepwise inspection and dissection. We present the results of comprehensive experiments on actual wide-baseline stereo images, which we use to contrast and discuss the advantages and disadvantages of several state-of-the-art deep-learning algorithms. Finally, we conclude with a description of the state-of-the-art methods and forecast developing trends with unresolved challenges, providing a guide for future work.},
DOI = {10.3390/rs13163247}
}



@Article{ijgi10080556,
AUTHOR = {Shen, Shengyu and Chen, Jiasheng and Zhang, Shaoyi and Cheng, Dongbing and Wang, Zhigang and Zhang, Tong},
TITLE = {Deep Fusion of DOM and DSM Features for Benggang Discovery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {556},
URL = {https://www.mdpi.com/2220-9964/10/8/556},
ISSN = {2220-9964},
ABSTRACT = {Benggang is a typical erosional landform in southern and southeastern China. Since benggang poses significant risks to local ecological environments and economic infrastructure, it is vital to accurately detect benggang-eroded areas. Relying only on remote sensing imagery for benggang detection cannot produce satisfactory results. In this study, we propose integrating high-resolution Digital Orthophoto Map (DOM) and Digital Surface Model (DSM) data for efficient and automatic benggang discovery. The fusion of complementary rich information hidden in both DOM and DSM data is realized by a two-stream convolutional neural network (CNN), which integrates aggregated terrain and activation image features that are both extracted by supervised deep learning. We aggregate local low-level geomorphic features via a supervised diffusion-convolutional embedding branch for expressive representations of benggang terrain variations. Activation image features are obtained from an image-oriented convolutional neural network branch. The two sources of information (DOM and DSM) are fused via a gated neural network, which learns the most discriminative features for the detection of benggang. The evaluation of a challenging benggang dataset demonstrates that our method exceeds several baselines, even with limited training examples. The results show that the fusion of DOM and DSM data is beneficial for benggang detection via supervised convolutional and deep fusion networks.},
DOI = {10.3390/ijgi10080556}
}



@Article{app11167550,
AUTHOR = {Tina, Giuseppe Marco and Ventura, Cristina and Ferlito, Sergio and De Vito, Saverio},
TITLE = {A State-of-Art-Review on Machine-Learning Based Methods for PV},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7550},
URL = {https://www.mdpi.com/2076-3417/11/16/7550},
ISSN = {2076-3417},
ABSTRACT = {In the current era, Artificial Intelligence (AI) is becoming increasingly pervasive with applications in several applicative fields effectively changing our daily life. In this scenario, machine learning (ML), a subset of AI techniques, provides machines with the ability to programmatically learn from data to model a system while adapting to new situations as they learn more by data they are ingesting (on-line training). During the last several years, many papers have been published concerning ML applications in the field of solar systems. This paper presents the state of the art ML models applied in solar energy’s forecasting field i.e., for solar irradiance and power production forecasting (both point and interval or probabilistic forecasting), electricity price forecasting and energy demand forecasting. Other applications of ML into the photovoltaic (PV) field taken into account are the modelling of PV modules, PV design parameter extraction, tracking the maximum power point (MPP), PV systems efficiency optimization, PV/Thermal (PV/T) and Concentrating PV (CPV) system design parameters’ optimization and efficiency improvement, anomaly detection and energy management of PV’s storage systems. While many review papers already exist in this regard, they are usually focused only on one specific topic, while in this paper are gathered all the most relevant applications of ML for solar systems in many different fields. The paper gives an overview of the most recent and promising applications of machine learning used in the field of photovoltaic systems.},
DOI = {10.3390/app11167550}
}



@Article{rs13163255,
AUTHOR = {Malbéteau, Yoann and Johansen, Kasper and Aragon, Bruno and Al-Mashhawari, Samir K. and McCabe, Matthew F.},
TITLE = {Overcoming the Challenges of Thermal Infrared Orthomosaics Using a Swath-Based Approach to Correct for Dynamic Temperature and Wind Effects},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3255},
URL = {https://www.mdpi.com/2072-4292/13/16/3255},
ISSN = {2072-4292},
ABSTRACT = {The miniaturization of thermal infrared sensors suitable for integration with unmanned aerial vehicles (UAVs) has provided new opportunities to observe surface temperature at ultra-high spatial and temporal resolutions. In parallel, there has been a rapid development of software capable of streamlining the generation of orthomosaics. However, these approaches were developed to process optical and multi-spectral image data and were not designed to account for the often rapidly changing surface characteristics inherent in the collection and processing of thermal data. Although radiometric calibration and shutter correction of uncooled sensors have improved, the processing of thermal image data remains difficult due to (1) vignetting effects on the uncooled microbolometer focal plane array; (2) inconsistencies between images relative to in-flight effects (wind-speed and direction); (3) unsuitable methods for thermal infrared orthomosaic generation. Here, we use thermal infrared UAV data collected with a FLIR-based TeAx camera over an agricultural field at different times of the day to assess inconsistencies in orthophotos and their impact on UAV-based thermal infrared orthomosaics. Depending on the wind direction and speed, we found a significant difference in UAV-based surface temperature (up to 2 °C) within overlapping areas of neighboring flight lines, with orthophotos collected with tail wind being systematically cooler than those with head wind. To address these issues, we introduce a new swath-based mosaicking approach, which was compared to three standard blending modes for orthomosaic generation. The swath-based mosaicking approach improves the ability to identify rapid changes of surface temperature during data acquisition, corrects for the influence of flight direction relative to the wind orientation, and provides uncertainty (pixel-based standard deviation) maps to accompany the orthomosaic of surface temperature. It also produced more accurate temperature retrievals than the other three standard orthomosaicking methods, with a root mean square error of 1.2 °C when assessed against in situ measurements. As importantly, our findings demonstrate that thermal infrared data require appropriate processing to reduce inconsistencies between observations, and thus, improve the accuracy and utility of orthomosaics.},
DOI = {10.3390/rs13163255}
}



@Article{rs13163263,
AUTHOR = {Liu, Zhijie and Guo, Pengju and Liu, Heng and Fan, Pan and Zeng, Pengzong and Liu, Xiangyang and Feng, Ce and Wang, Wang and Yang, Fuzeng},
TITLE = {Gradient Boosting Estimation of the Leaf Area Index of Apple Orchards in UAV Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3263},
URL = {https://www.mdpi.com/2072-4292/13/16/3263},
ISSN = {2072-4292},
ABSTRACT = {The leaf area index (LAI) is a key parameter for describing the canopy structure of apple trees. This index is also employed in evaluating the amount of pesticide sprayed per unit volume of apple trees. Hence, numerous manual and automatic methods have been explored for LAI estimation. In this work, the leaf area indices for different types of apple trees are obtained in terms of multispectral remote-sensing data collected with an unmanned aerial vehicle (UAV), along with simultaneous measurements of apple orchards. The proposed approach was tested on apple trees of the “Fuji”, “Golden Delicious”, and “Ruixue” types, which were planted in the Apple Experimental Station of the Northwest Agriculture and Forestry University in Baishui County, Shaanxi Province, China. Five vegetation indices of strong correlation with the apple leaf area index were selected and used to train models of support vector regression (SVR) and gradient-boosting decision trees (GBDT) for predicting the leaf area index of apple trees. The best model was selected based on the metrics of the coefficient of determination (R2) and the root-mean-square error (RMSE). The experimental results showed that the gradient-boosting decision tree model achieved the best performance with an R2 of 0.846, an RMSE of 0.356, and a spatial efficiency (SPAEF) of 0.57. This demonstrates the feasibility of our approach for fast and accurate remote-sensing-based estimation of the leaf area index of apple trees.},
DOI = {10.3390/rs13163263}
}



@Article{s21165554,
AUTHOR = {Pal, Shantanu and Mukhopadhyay, Subhas and Suryadevara, Nagender},
TITLE = {Development and Progress in Sensors and Technologies for Human Emotion Recognition},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5554},
URL = {https://www.mdpi.com/1424-8220/21/16/5554},
PubMedID = {34451002},
ISSN = {1424-8220},
ABSTRACT = {With the advancement of human-computer interaction, robotics, and especially humanoid robots, there is an increasing trend for human-to-human communications over online platforms (e.g., zoom). This has become more significant in recent years due to the Covid-19 pandemic situation. The increased use of online platforms for communication signifies the need to build efficient and more interactive human emotion recognition systems. In a human emotion recognition system, the physiological signals of human beings are collected, analyzed, and processed with the help of dedicated learning techniques and algorithms. With the proliferation of emerging technologies, e.g., the Internet of Things (IoT), future Internet, and artificial intelligence, there is a high demand for building scalable, robust, efficient, and trustworthy human recognition systems. In this paper, we present the development and progress in sensors and technologies to detect human emotions. We review the state-of-the-art sensors used for human emotion recognition and different types of activity monitoring. We present the design challenges and provide practical references of such human emotion recognition systems in the real world. Finally, we discuss the current trends in applications and explore the future research directions to address issues, e.g., scalability, security, trust, privacy, transparency, and decentralization.},
DOI = {10.3390/s21165554}
}



@Article{rs13163272,
AUTHOR = {Ivošević, Bojana and Lugonja, Predrag and Brdar, Sanja and Radulović, Mirjana and Vujić, Ante and Valente, João},
TITLE = {UAV-Based Land Cover Classification for Hoverfly (Diptera: Syrphidae) Habitat Condition Assessment: A Case Study on Mt. Stara Planina (Serbia)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3272},
URL = {https://www.mdpi.com/2072-4292/13/16/3272},
ISSN = {2072-4292},
ABSTRACT = {Habitat degradation, mostly caused by human impact, is one of the key drivers of biodiversity loss. This is a global problem, causing a decline in the number of pollinators, such as hoverflies. In the process of digitalizing ecological studies in Serbia, remote-sensing-based land cover classification has become a key component for both current and future research. Object-based land cover classification, using machine learning algorithms of very high resolution (VHR) imagery acquired by an unmanned aerial vehicle (UAV) was carried out in three different study sites on Mt. Stara Planina, Eastern Serbia. UAV land cover classified maps with seven land cover classes (trees, shrubs, meadows, road, water, agricultural land, and forest patches) were studied. Moreover, three different classification algorithms—support vector machine (SVM), random forest (RF), and k-NN (k-nearest neighbors)—were compared. This study shows that the random forest classifier performs better with respect to the other classifiers in all three study sites, with overall accuracy values ranging from 0.87 to 0.96. The overall results are robust to changes in labeling ground truth subsets. The obtained UAV land cover classified maps were compared with the Map of the Natural Vegetation of Europe (EPNV) and used to quantify habitat degradation and assess hoverfly species richness. It was concluded that the percentage of habitat degradation is primarily caused by anthropogenic pressure, thus affecting the richness of hoverfly species in the study sites. In order to enable research reproducibility, the datasets used in this study are made available in a public repository.},
DOI = {10.3390/rs13163272}
}



@Article{rs13163276,
AUTHOR = {Ulhaq, Anwaar and Adams, Peter and Cox, Tarnya E. and Khan, Asim and Low, Tom and Paul, Manoranjan},
TITLE = {Automated Detection of Animals in Low-Resolution Airborne Thermal Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3276},
URL = {https://www.mdpi.com/2072-4292/13/16/3276},
ISSN = {2072-4292},
ABSTRACT = {Detecting animals to estimate abundance can be difficult, particularly when the habitat is dense or the target animals are fossorial. The recent surge in the use of thermal imagers in ecology and their use in animal detections can increase the accuracy of population estimates and improve the subsequent implementation of management programs. However, the use of thermal imagers results in many hours of captured flight videos which require manual review for confirmation of species detection and identification. Therefore, the perceived cost and efficiency trade-off often restricts the use of these systems. Additionally, for many off-the-shelf systems, the exported imagery can be quite low resolution (&lt;9 Hz), increasing the difficulty of using automated detections algorithms to streamline the review process. This paper presents an animal species detection system that utilises the cost-effectiveness of these lower resolution thermal imagers while harnessing the power of transfer learning and an enhanced small object detection algorithm. We have proposed a distant object detection algorithm named Distant-YOLO (D-YOLO) that utilises YOLO (You Only Look Once) and improves its training and structure for the automated detection of target objects in thermal imagery. We trained our system on thermal imaging data of rabbits, their active warrens, feral pigs, and kangaroos collected by thermal imaging researchers in New South Wales and Western Australia. This work will enhance the visual analysis of animal species while performing well on low, medium and high-resolution thermal imagery.},
DOI = {10.3390/rs13163276}
}



@Article{ai2030023,
AUTHOR = {Xue, Zhihan and Gonsalves, Tad},
TITLE = {Vision Based Drone Obstacle Avoidance by Deep Reinforcement Learning},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {366--380},
URL = {https://www.mdpi.com/2673-2688/2/3/23},
ISSN = {2673-2688},
ABSTRACT = {Research on autonomous obstacle avoidance of drones has recently received widespread attention from researchers. Among them, an increasing number of researchers are using machine learning to train drones. These studies typically adopt supervised learning or reinforcement learning to train the networks. Supervised learning has a disadvantage in that it takes a significant amount of time to build the datasets, because it is difficult to cover the complex and changeable drone flight environment in a single dataset. Reinforcement learning can overcome this problem by using drones to learn data in the environment. However, the current research results based on reinforcement learning are mainly focused on discrete action spaces. In this way, the movement of drones lacks precision and has somewhat unnatural flying behavior. This study aims to use the soft-actor-critic algorithm to train a drone to perform autonomous obstacle avoidance in continuous action space using only the image data. The algorithm is trained and tested in a simulation environment built by Airsim. The results show that our algorithm enables the UAV to avoid obstacles in the training environment only by inputting the depth map. Moreover, it also has a higher obstacle avoidance rate in the reconfigured environment without retraining.},
DOI = {10.3390/ai2030023}
}



@Article{en14165131,
AUTHOR = {Martins, Leandro do C. and Tordecilla, Rafael D. and Castaneda, Juliana and Juan, Angel A. and Faulin, Javier},
TITLE = {Electric Vehicle Routing, Arc Routing, and Team Orienteering Problems in Sustainable Transportation},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5131},
URL = {https://www.mdpi.com/1996-1073/14/16/5131},
ISSN = {1996-1073},
ABSTRACT = {The increasing use of electric vehicles in road and air transportation, especially in last-mile delivery and city mobility, raises new operational challenges due to the limited capacity of electric batteries. These limitations impose additional driving range constraints when optimizing the distribution and mobility plans. During the last years, several researchers from the Computer Science, Artificial Intelligence, and Operations Research communities have been developing optimization, simulation, and machine learning approaches that aim at generating efficient and sustainable routing plans for hybrid fleets, including both electric and internal combustion engine vehicles. After contextualizing the relevance of electric vehicles in promoting sustainable transportation practices, this paper reviews the existing work in the field of electric vehicle routing problems. In particular, we focus on articles related to the well-known vehicle routing, arc routing, and team orienteering problems. The review is followed by numerical examples that illustrate the gains that can be obtained by employing optimization methods in the aforementioned field. Finally, several research opportunities are highlighted.},
DOI = {10.3390/en14165131}
}



@Article{en14165129,
AUTHOR = {Junaid, Muhammad and Shaikh, Asadullah and Hassan, Mahmood Ul and Alghamdi, Abdullah and Rajab, Khairan and Al Reshan, Mana Saleh and Alkinani, Monagi},
TITLE = {Smart Agriculture Cloud Using AI Based Techniques},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5129},
URL = {https://www.mdpi.com/1996-1073/14/16/5129},
ISSN = {1996-1073},
ABSTRACT = {This research proposes a generic smart cloud-based system in order to accommodate multiple scenarios where agriculture farms using Internet of Things (IoTs) need to be monitored remotely. The real-time and stored data are analyzed by specialists and farmers. The cloud acts as a central digital data store where information is collected from diverse sources in huge volumes and variety, such as audio, video, image, text, and digital maps. Artificial Intelligence (AI) based machine learning models such as Support Vector Machine (SVM), which is one of many classification types, are used to accurately classify the data. The classified data are assigned to the virtual machines where these data are processed and finally available to the end-users via underlying datacenters. This processed form of digital information is then used by the farmers to improve their farming skills and to update them as pre-disaster recovery for smart agri-food. Furthermore, it will provide general and specific information about international markets relating to their crops. This proposed system discovers the feasibility of the developed digital agri-farm using IoT-based cloud and provides solutions to problems. Overall, the approach works well and achieved performance efficiency in terms of execution time by 14%, throughput time by 5%, overhead time by 9%, and energy efficiency by 13.2% in the presence of competing smart farming baselines.},
DOI = {10.3390/en14165129}
}



@Article{s21165620,
AUTHOR = {Shan, Donghui and Lei, Tian and Yin, Xiaohong and Luo, Qin and Gong, Lei},
TITLE = {Extracting Key Traffic Parameters from UAV Video with On-Board Vehicle Data Validation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5620},
URL = {https://www.mdpi.com/1424-8220/21/16/5620},
PubMedID = {34451061},
ISSN = {1424-8220},
ABSTRACT = {The advantages of UAV video in flexibility, traceability, easy-operation, and abundant information make it a popular and powerful aerial tool applied in traffic monitoring in recent years. This paper proposed a systematic approach to detect and track vehicles based on the YOLO v3 model and the deep SORT algorithm for further extracting key traffic parameters. A field experiment was implemented to provide data for model training and validation to ensure the accuracy of the proposed approach. In the experiment, 5400 frame images and 1192 speed points were collected from two test vehicles equipped with high-precision GNSS-RTK and onboard OBD after completion of seven experimental groups with a different height (150 m to 500 m) and operating speed (40 km/h to 90 km/h). The results indicate that the proposed approach exhibits strong robustness and reliability, due to the 90.88% accuracy of object detection and 98.9% precision of tracking vehicle. Moreover, the absolute and relative error of extracted speed falls within ±3 km/h and 2%, respectively. The overall accuracy of the extracted parameters reaches up to 98%.},
DOI = {10.3390/s21165620}
}



@Article{sym13081537,
AUTHOR = {Zhu, Zixiong and Xie, Nianhao and Zong, Kang and Chen, Lei},
TITLE = {Building a Connected Communication Network for UAV Clusters Using DE-MADDPG},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1537},
URL = {https://www.mdpi.com/2073-8994/13/8/1537},
ISSN = {2073-8994},
ABSTRACT = {Clusters of unmanned aerial vehicles (UAVs) are often used to perform complex tasks. In such clusters, the reliability of the communication network connecting the UAVs is an essential factor in their collective efficiency. Due to the complex wireless environment, however, communication malfunctions within the cluster are likely during the flight of UAVs. In such cases, it is important to control the cluster and rebuild the connected network. The asymmetry of the cluster topology also increases the complexity of the control mechanisms. The traditional control methods based on cluster consistency often rely on the motion information of the neighboring UAVs. The motion information, however, may become unavailable because of the interrupted communications. UAV control algorithms based on deep reinforcement learning have achieved outstanding results in many fields. Here, we propose a cluster control method based on the Decomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG) to rebuild a communication network for UAV clusters. The DE-MADDPG improves the framework of the traditional multi-agent deep deterministic policy gradient (MADDPG) algorithm by decomposing the reward function. We further introduce the reward reshaping function to facilitate the convergence of the algorithm in sparse reward environments. To address the instability of the state-space in the reinforcement learning framework, we also propose the notion of the virtual leader–follower model. Extensive simulations show that the success rate of the DE-MADDPG is higher than that of the MADDPG algorithm, confirming the effectiveness of the proposed method.},
DOI = {10.3390/sym13081537}
}



@Article{app11167716,
AUTHOR = {Maraveas, Chrysanthos and Loukatos, Dimitrios and Bartzanas, Thomas and Arvanitis, Konstantinos G.},
TITLE = {Applications of Artificial Intelligence in Fire Safety of Agricultural Structures},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7716},
URL = {https://www.mdpi.com/2076-3417/11/16/7716},
ISSN = {2076-3417},
ABSTRACT = {Artificial intelligence applications in fire safety of agricultural structures have practical economic and technological benefits on commercial agriculture. The FAO estimates that wildfires result in at least USD 1 billion in agriculture-related losses due to the destruction of livestock pasture, destruction of agricultural buildings, premature death of farm animals, and general disruption of agricultural activities. Even though artificial neural networks (ANNs), genetic algorithms (GAs), probabilistic neural networks (PNNs), and adaptive neurofuzzy inference systems (ANFISs), among others, have proven useful in fire prevention, their application is limited in real farm environments. Most farms rely on traditional/non-technology-based methods of fire prevention. The case for AI in agricultural fire prevention is grounded on the accuracy and reliability of computer simulations in smoke movement analysis, risk assessment, and postfire analysis. In addition, such technologies can be coupled with next-generation fire-retardant materials such as intumescent coatings with a polymer binder, blowing agent, carbon donor, and acid donor. Future prospects for AI in agriculture transcend basic fire safety to encompass Society 5.0, energy systems in smart cities, UAV monitoring, Agriculture 4.0, and decentralized energy. However, critical challenges must be overcome, including the health and safety aspects, cost, and reliability. In brief, AI offers unlimited potential in the prevention of fire hazards in farms, but the existing body of knowledge is inadequate.},
DOI = {10.3390/app11167716}
}



@Article{s21165656,
AUTHOR = {Li, Xuanye and Li, Hongguang and Jiang, Yalong and Wang, Meng},
TITLE = {Lightweight Detection Network Based on Sub-Pixel Convolution and Objectness-Aware Structure for UAV Images},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5656},
URL = {https://www.mdpi.com/1424-8220/21/16/5656},
PubMedID = {34451098},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) can serve as an ideal mobile platform in various situations. Real-time object detection with on-board apparatus provides drones with increased flexibility as well as a higher intelligence level. In order to achieve good detection results in UAV images with complex ground scenes, small object size and high object density, most of the previous work introduced models with higher computational burdens, making deployment on mobile platforms more difficult.This paper puts forward a lightweight object detection framework. Besides being anchor-free, the framework is based on a lightweight backbone and a simultaneous up-sampling and detection module to form a more efficient detection architecture. Meanwhile, we add an objectness branch to assist the multi-class center point prediction, which notably improves the detection accuracy and only takes up very little computing resources. The results of the experiment indicate that the computational cost of this paper is 92.78% lower than the CenterNet with ResNet18 backbone, and the mAP is 2.8 points higher on the Visdrone-2018-VID dataset. A frame rate of about 220 FPS is achieved. Additionally, we perform ablation experiments to check on the validity of each part, and the method we propose is compared with other representative lightweight object detection methods on UAV image datasets.},
DOI = {10.3390/s21165656}
}



@Article{electronics10162038,
AUTHOR = {Tao, Zhen and Ren, Shiwei and Shi, Yueting and Wang, Xiaohua and Wang, Weijiang},
TITLE = {Accurate and Lightweight RailNet for Real-Time Rail Line Detection},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {2038},
URL = {https://www.mdpi.com/2079-9292/10/16/2038},
ISSN = {2079-9292},
ABSTRACT = {Railway transportation has always occupied an important position in daily life and social progress. In recent years, computer vision has made promising breakthroughs in intelligent transportation, providing new ideas for detecting rail lines. Yet the majority of rail line detection algorithms use traditional image processing to extract features, and their detection accuracy and instantaneity remain to be improved. This paper goes beyond the aforementioned limitations and proposes a rail line detection algorithm based on deep learning. First, an accurate and lightweight RailNet is designed, which takes full advantage of the powerful advanced semantic information extraction capabilities of deep convolutional neural networks to obtain high-level features of rail lines. The Segmentation Soul (SS) module is creatively added to the RailNet structure, which improves segmentation performance without any additional inference time. The Depth Wise Convolution (DWconv) is introduced in the RailNet to reduce the number of network parameters and eventually ensure real-time detection. Afterward, according to the binary segmentation maps of RailNet output, we propose the rail line fitting algorithm based on sliding window detection and apply the inverse perspective transformation. Thus the polynomial functions and curvature of the rail lines are calculated, and rail lines are identified in the original images. Furthermore, we collect a real-world rail lines dataset, named RAWRail. The proposed algorithm has been fully validated on the RAWRail dataset, running at 74 FPS, and the accuracy reaches 98.6%, which is superior to the current rail line detection algorithms and shows powerful potential in real applications.},
DOI = {10.3390/electronics10162038}
}



@Article{electronics10172046,
AUTHOR = {Zhao, Baojun and Tang, Wei and Pan, Yu and Han, Yuqi and Wang, Wenzheng},
TITLE = {Aircraft Type Recognition in Remote Sensing Images: Bilinear Discriminative Extreme Learning Machine Framework},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2046},
URL = {https://www.mdpi.com/2079-9292/10/17/2046},
ISSN = {2079-9292},
ABSTRACT = {Small inter-class and massive intra-class changes are important challenges in aircraft model recognition in the field of remote sensing. Although the aircraft model recognition algorithm based on the convolutional neural network (CNN) has excellent recognition performance, it is limited by sample sets and computing resources. To solve the above problems, we propose the bilinear discriminative extreme learning machine (ELM) network (BD-ELMNet), which integrates the advantages of the CNN, autoencoder (AE), and ELM. Specifically, the BD-ELMNet first executes the convolution and pooling operations to form a convolutional ELM (ELMConvNet) to extract shallow features. Furthermore, the manifold regularized ELM-AE (MRELM-AE), which can simultaneously consider the geometrical structure and discriminative information of aircraft data, is developed to extract discriminative features. The bilinear pooling model uses the feature association information for feature fusion to enhance the substantial distinction of features. Compared with the backpropagation (BP) optimization method, BD-ELMNet adopts a layer-by-layer training method without repeated adjustments to effectively learn discriminant features. Experiments involving the application of several methods, including the proposed method, to the MTARSI benchmark demonstrate that the proposed aircraft type recognition method outperforms the state-of-the-art methods.},
DOI = {10.3390/electronics10172046}
}



@Article{electronics10172048,
AUTHOR = {Lin, Weison and Adetomi, Adewale and Arslan, Tughrul},
TITLE = {Low-Power Ultra-Small Edge AI Accelerators for Image Recognition with Convolution Neural Networks: Analysis and Future Directions},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2048},
URL = {https://www.mdpi.com/2079-9292/10/17/2048},
ISSN = {2079-9292},
ABSTRACT = {Edge AI accelerators have been emerging as a solution for near customers’ applications in areas such as unmanned aerial vehicles (UAVs), image recognition sensors, wearable devices, robotics, and remote sensing satellites. These applications require meeting performance targets and resilience constraints due to the limited device area and hostile environments for operation. Numerous research articles have proposed the edge AI accelerator for satisfying the applications, but not all include full specifications. Most of them tend to compare the architecture with other existing CPUs, GPUs, or other reference research, which implies that the performance exposé of the articles are not comprehensive. Thus, this work lists the essential specifications of prior art edge AI accelerators and the CGRA accelerators during the past few years to define and evaluate the low power ultra-small edge AI accelerators. The actual performance, implementation, and productized examples of edge AI accelerators are released in this paper. We introduce the evaluation results showing the edge AI accelerator design trend about key performance metrics to guide designers. Last but not least, we give out the prospect of developing edge AI’s existing and future directions and trends, which will involve other technologies for future challenging constraints.},
DOI = {10.3390/electronics10172048}
}



@Article{info12090343,
AUTHOR = {Hu, Chunyang and Li, Jingchen and Shi, Haobin and Ning, Bin and Gu, Qiong},
TITLE = {Decentralized Offloading Strategies Based on Reinforcement Learning for Multi-Access Edge Computing},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {343},
URL = {https://www.mdpi.com/2078-2489/12/9/343},
ISSN = {2078-2489},
ABSTRACT = {Using reinforcement learning technologies to learn offloading strategies for multi-access edge computing systems has been developed by researchers. However, large-scale systems are unsuitable for reinforcement learning, due to their huge state spaces and offloading behaviors. For this reason, this work introduces the centralized training and decentralized execution mechanism, designing a decentralized reinforcement learning model for multi-access edge computing systems. Considering a cloud server and several edge servers, we separate the training and execution in the reinforcement learning model. The execution happens in edge devices of the system, and edge servers need no communication. Conversely, the training process occurs at the cloud device, which causes a lower transmission latency. The developed method uses a deep deterministic policy gradient algorithm to optimize offloading strategies. The simulated experiment shows that our method can learn the offloading strategy for each edge device efficiently.},
DOI = {10.3390/info12090343}
}



@Article{rs13173364,
AUTHOR = {Huang, Xin and Dong, Xiaoya and Ma, Jing and Liu, Kuan and Ahmed, Shibbir and Lin, Jinlong and Qiu, Baijing},
TITLE = {The Improved A* Obstacle Avoidance Algorithm for the Plant Protection UAV with Millimeter Wave Radar and Monocular Camera Data Fusion},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3364},
URL = {https://www.mdpi.com/2072-4292/13/17/3364},
ISSN = {2072-4292},
ABSTRACT = {To enhance obstacle avoidance abilities of the plant protection UAV in unstructured farmland, this article improved the traditional A* algorithms through dynamic heuristic functions, search point optimization, and inflection point optimization based on millimeter wave radar and monocular camera data fusion. Obstacle information extraction experiments were carried out. The performance between the improved algorithm and traditional algorithm was compared. Additionally, obstacle avoidance experiments were also carried out. The results show that the maximum error in distance measurement of data fusion method was 8.2%. Additionally, the maximum error in obstacle width and height measurement were 27.3% and 18.5%, respectively. The improved algorithm is more useful in path planning, significantly reduces data processing time, search grid, and turning points. The algorithm at most increases path length by 2.0%, at least reduces data processing time by 68.4%, search grid by 74.9%, and turning points by 20.7%. The maximum trajectory offset error was proportional to the flight speed, with a maximum trajectory offset of 1.4 m. The distance between the UAV and obstacle was inversely proportional to flight speed, with a minimum distance of 1.6 m. This method can provide a new idea for obstacle avoidance of the plant protection UAV.},
DOI = {10.3390/rs13173364}
}



@Article{s21175713,
AUTHOR = {Zhao, Shenglin and Cai, Haoyuan and Li, Wenkuan and Liu, Yaqian and Liu, Chunxiu},
TITLE = {Hand Gesture Recognition on a Resource-Limited Interactive Wristband},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5713},
URL = {https://www.mdpi.com/1424-8220/21/17/5713},
PubMedID = {34502604},
ISSN = {1424-8220},
ABSTRACT = {Most of the reported hand gesture recognition algorithms require high computational resources, i.e., fast MCU frequency and significant memory, which are highly inapplicable to the cost-effectiveness of consumer electronics products. This paper proposes a hand gesture recognition algorithm running on an interactive wristband, with computational resource requirements as low as Flash &lt; 5 KB, RAM &lt; 1 KB. Firstly, we calculated the three-axis linear acceleration by fusing accelerometer and gyroscope data with a complementary filter. Then, by recording the order of acceleration vectors crossing axes in the world coordinate frame, we defined a new feature code named axis-crossing code. Finally, we set templates for eight hand gestures to recognize new samples. We compared this algorithm’s performance with the widely used dynamic time warping (DTW) algorithm and recurrent neural network (BiLSTM and GRU). The results show that the accuracies of the proposed algorithm and RNNs are higher than DTW and that the time cost of the proposed algorithm is much less than those of DTW and RNNs. The average recognition accuracy is 99.8% on the collected dataset and 97.1% in the actual user-independent case. In general, the proposed algorithm is suitable and competitive in consumer electronics. This work has been volume-produced and patent-granted.},
DOI = {10.3390/s21175713}
}



@Article{rs13173361,
AUTHOR = {Marzialetti, Flavio and Frate, Ludovico and De Simone, Walter and Frattaroli, Anna Rita and Acosta, Alicia Teresa Rosario and Carranza, Maria Laura},
TITLE = {Unmanned Aerial Vehicle (UAV)-Based Mapping of Acacia saligna Invasion in the Mediterranean Coast},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3361},
URL = {https://www.mdpi.com/2072-4292/13/17/3361},
ISSN = {2072-4292},
ABSTRACT = {Remote Sensing (RS) is a useful tool for detecting and mapping Invasive Alien Plants (IAPs). IAPs mapping on dynamic and heterogeneous landscapes, using satellite RS data, is not always feasible. Unmanned aerial vehicles (UAV) with ultra-high spatial resolution data represent a promising tool for IAPs detection and mapping. This work develops an operational workflow for detecting and mapping Acacia saligna invasion along Mediterranean coastal dunes. In particular, it explores and tests the potential of RGB (Red, Green, Blue) and multispectral (Green, Red, Red Edge, Near Infra—Red) UAV images collected in pre-flowering and flowering phenological stages for detecting and mapping A. saligna. After ortho—mosaics generation, we derived from RGB images the DSM (Digital Surface Model) and HIS (Hue, Intensity, Saturation) variables, and we calculated the NDVI (Normalized Difference Vegetation Index). For classifying images of the two phenological stages we built a set of raster stacks which include different combination of variables. For image classification, we used the Geographic Object-Based Image Analysis techniques (GEOBIA) in combination with Random Forest (RF) classifier. All classifications derived from RS information (collected on pre-flowering and flowering stages and using different combinations of variables) produced A. saligna maps with acceptable accuracy values, with higher performances on classification derived from flowering period images, especially using DSM + HIS combination. The adopted approach resulted an efficient method for mapping and early detection of IAPs, also in complex environments offering a sound support to the prioritization of conservation and management actions claimed by the EU IAS Regulation 1143/2014.},
DOI = {10.3390/rs13173361}
}



@Article{s21175718,
AUTHOR = {Abdelmaboud, Abdelzahir},
TITLE = {The Internet of Drones: Requirements, Taxonomy, Recent Advances, and Challenges of Research Trends},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5718},
URL = {https://www.mdpi.com/1424-8220/21/17/5718},
PubMedID = {34502608},
ISSN = {1424-8220},
ABSTRACT = {The use of unmanned aerial vehicles or drones are a valuable technique in coping with issues related to life in the general public’s daily routines. Given the growing number of drones in low-altitude airspace, linking drones to form the Internet of drones (IoD) is a highly desirable trend to improve the safety as well as the quality of flight. However, there remain security, privacy, and communication issues related to IoD. In this paper, we discuss the key requirements of security, privacy, and communication and we present a taxonomy of IoD based on the most relevant considerations. Furthermore, we present the most commonly used commercial case studies and address the latest advancements and solutions proposed for the IoD environments. Lastly, we discuss the challenges and future research directions of IoD.},
DOI = {10.3390/s21175718}
}



@Article{su13179568,
AUTHOR = {Ansari, Emaad and Akhtar, Mohammad Nishat and Abdullah, Mohamad Nazir and Othman, Wan Amir Fuad Wajdi and Bakar, Elmi Abu and Hawary, Ahmad Faizul and Alhady, Syed Sahal Nazli},
TITLE = {Image Processing of UAV Imagery for River Feature Recognition of Kerian River, Malaysia},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {9568},
URL = {https://www.mdpi.com/2071-1050/13/17/9568},
ISSN = {2071-1050},
ABSTRACT = {The impact of floods is the most severe among the natural calamities occurring in Malaysia. The knock of floods is consistent and annually forces thousands of Malaysians to relocate. The lack of information from the Ministry of Environment and Water, Malaysia is the foremost obstacle in upgrading the flood mapping. With the expeditious evolution of computer techniques, processing of satellite and unmanned aerial vehicle (UAV) images for river hydromorphological feature detection and flood management have gathered pace in the last two decades. Different image processing algorithms—structure from motion (SfM), multi-view stereo (MVS), gradient vector flow (GVF) snake algorithm, etc.—and artificial neural networks are implemented for the monitoring and classification of river features. This paper presents the application of the k-means algorithm along with image thresholding to quantify variation in river surface flow areas and vegetation growth along Kerian River, Malaysia. The river characteristic recognition directly or indirectly assists in studying river behavior and flood monitoring. Dice similarity coefficient and Jaccard index are numerated between thresholded images that are clustered using the k-means algorithm and manually segmented images. Based on quantitative evaluation, a dice similarity coefficient and Jaccard index of up to 97.86% and 94.36% were yielded for flow area and vegetation calculation. Thus, the present technique is functional in evaluating river characteristics with reduced errors. With minimum errors, the present technique can be utilized for quantifying agricultural areas and urban areas around the river basin.},
DOI = {10.3390/su13179568}
}



@Article{rs13173382,
AUTHOR = {Qader, Sarchil Hama and Dash, Jadu and Alegana, Victor A. and Khwarahm, Nabaz R. and Tatem, Andrew J. and Atkinson, Peter M.},
TITLE = {The Role of Earth Observation in Achieving Sustainable Agricultural Production in Arid and Semi-Arid Regions of the World},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3382},
URL = {https://www.mdpi.com/2072-4292/13/17/3382},
ISSN = {2072-4292},
ABSTRACT = {Crop production is a major source of food and livelihood for many people in arid and semi-arid (ASA) regions across the world. However, due to irregular climatic events, ASA regions are affected commonly by frequent droughts that can impact food production. In addition, ASA regions in the Middle East and Africa are often characterised by political instability, which can increase population vulnerability to hunger and ill health. Remote sensing (RS) provides a platform to improve the spatial prediction of crop production and food availability, with the potential to positively impact populations. This paper, firstly, describes some of the important characteristics of agriculture in ASA regions that require monitoring to improve their management. Secondly, it demonstrates how freely available RS data can support decision-making through a cost-effective monitoring system that complements traditional approaches for collecting agricultural data. Thirdly, it illustrates the challenges of employing freely available RS data for mapping and monitoring crop area, crop status and forecasting crop yield in these regions. Finally, existing approaches used in these applications are evaluated, and the challenges associated with their use and possible future improvements are discussed. We demonstrate that agricultural activities can be monitored effectively and both crop area and crop yield can be predicted in advance using RS data. We also discuss the future challenges associated with maintaining food security in ASA regions and explore some recent advances in RS that can be used to monitor cropland and forecast crop production and yield.},
DOI = {10.3390/rs13173382}
}



@Article{rs13173381,
AUTHOR = {Mikula, Karol and Šibíková, Mária and Ambroz, Martin and Kollár, Michal and Ožvat, Aneta A. and Urbán, Jozef and Jarolímek, Ivan and Šibík, Jozef},
TITLE = {NaturaSat—A Software Tool for Identification, Monitoring and Evaluation of Habitats by Remote Sensing Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3381},
URL = {https://www.mdpi.com/2072-4292/13/17/3381},
ISSN = {2072-4292},
ABSTRACT = {The NaturaSat software integrates various image processing techniques together with vegetation data, into one multipurpose tool that is designed for performing facilities for all requirements of habitat exploration, all in one place. It provides direct access to multispectral Sentinel-2 data provided by the European Space Agency. It supports using these data with various vegetation databases, in a user-friendly environment, for, e.g., vegetation scientists, fieldwork experts, and nature conservationists. The presented study introduces the NaturaSat software, describes new powerful tools, such as the semi-automatic and automatic segmentation methods, and natural numerical networks, together with validated examples comparing field surveys and software outputs. The software is robust enough for field work researchers and stakeholders to accurately extract target units’ borders, even on the habitat level. The deep learning algorithm, developed for habitat classification within the NaturaSat software, can also be used in various research tasks or in nature conservation practices, such as identifying ecosystem services and conservation value. The exact maps of the habitats obtained within the project can improve many further vegetation and landscape ecology studies.},
DOI = {10.3390/rs13173381}
}



@Article{rs13173383,
AUTHOR = {Qin, Shengwu and Guo, Xu and Sun, Jingbo and Qiao, Shuangshuang and Zhang, Lingshuai and Yao, Jingyu and Cheng, Qiushi and Zhang, Yanqing},
TITLE = {Landslide Detection from Open Satellite Imagery Using Distant Domain Transfer Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3383},
URL = {https://www.mdpi.com/2072-4292/13/17/3383},
ISSN = {2072-4292},
ABSTRACT = {Using convolutional neural network (CNN) methods and satellite images for landslide identification and classification is a very efficient and popular task in geological hazard investigations. However, traditional CNNs have two disadvantages: (1) insufficient training images from the study area and (2) uneven distribution of the training set and validation set. In this paper, we introduced distant domain transfer learning (DDTL) methods for landslide detection and classification. We first introduce scene classification satellite imagery into the landslide detection task. In addition, in order to more effectively extract information from satellite images, we innovatively add an attention mechanism to DDTL (AM-DDTL). In this paper, the Longgang study area, a district in Shenzhen City, Guangdong Province, has only 177 samples as the landslide target domain. We examine the effect of DDTL by comparing three methods: the convolutional CNN, pretrained model and DDTL. We compare different attention mechanisms based on the DDTL. The experimental results show that the DDTL method has better detection performance than the normal CNN, and the AM-DDTL models achieve 94% classification accuracy, which is 7% higher than the conventional DDTL method. The requirements for the detection and classification of potential landslides at different disaster zones can be met by applying the AM-DDTL algorithm, which outperforms traditional CNN methods.},
DOI = {10.3390/rs13173383}
}



@Article{rs13173385,
AUTHOR = {Chen, Dong and Loboda, Tatiana V. and Silva, Julie A. and Tonellato, Maria R.},
TITLE = {Characterizing Small-Town Development Using Very High Resolution Imagery within Remote Rural Settings of Mozambique},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3385},
URL = {https://www.mdpi.com/2072-4292/13/17/3385},
ISSN = {2072-4292},
ABSTRACT = {While remotely sensed images of various resolutions have been widely used in identifying changes in urban and peri-urban environments, only very high resolution (VHR) imagery is capable of providing the information needed for understanding the changes taking place in remote rural environments, due to the small footprints and low density of man-made structures in these settings. However, limited by data availability, mapping man-made structures and conducting subsequent change detections in remote areas are typically challenging and thus require a certain level of flexibility in algorithm design that takes into account the specific environmental and image conditions. In this study, we mapped all buildings and corrals for two remote villages in Mozambique based on two single-date VHR images that were taken in 2004 and 2012, respectively. Our algorithm takes advantage of the presence of shadows and, through a fusion of both spectra- and object-based analysis techniques, is able to differentiate buildings with metal and thatch roofs with high accuracy (overall accuracy of 86% and 94% for 2004 and 2012, respectively). The comparison of the mapping results between 2004 and 2012 reveals multiple lines of evidence suggesting that both villages, while differing in many aspects, have experienced substantial increases in the economic status. As a case study, our project demonstrates the capability of a coupling of VHR imagery with locally adjusted classification algorithms to infer the economic development of small, remote rural settlements.},
DOI = {10.3390/rs13173385}
}



@Article{s21175745,
AUTHOR = {Fraga-Lamas, Paula and Lopes, Sérgio Ivan and Fernández-Caramés, Tiago M.},
TITLE = {Green IoT and Edge AI as Key Technological Enablers for a Sustainable Digital Transition towards a Smart Circular Economy: An Industry 5.0 Use Case},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5745},
URL = {https://www.mdpi.com/1424-8220/21/17/5745},
PubMedID = {34502637},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things (IoT) can help to pave the way to the circular economy and to a more sustainable world by enabling the digitalization of many operations and processes, such as water distribution, preventive maintenance, or smart manufacturing. Paradoxically, IoT technologies and paradigms such as edge computing, although they have a huge potential for the digital transition towards sustainability, they are not yet contributing to the sustainable development of the IoT sector itself. In fact, such a sector has a significant carbon footprint due to the use of scarce raw materials and its energy consumption in manufacturing, operating, and recycling processes. To tackle these issues, the Green IoT (G-IoT) paradigm has emerged as a research area to reduce such carbon footprint; however, its sustainable vision collides directly with the advent of Edge Artificial Intelligence (Edge AI), which imposes the consumption of additional energy. This article deals with this problem by exploring the different aspects that impact the design and development of Edge-AI G-IoT systems. Moreover, it presents a practical Industry 5.0 use case that illustrates the different concepts analyzed throughout the article. Specifically, the proposed scenario consists in an Industry 5.0 smart workshop that looks for improving operator safety and operation tracking. Such an application case makes use of a mist computing architecture composed of AI-enabled IoT nodes. After describing the application case, it is evaluated its energy consumption and it is analyzed the impact on the carbon footprint that it may have on different countries. Overall, this article provides guidelines that will help future developers to face the challenges that will arise when creating the next generation of Edge-AI G-IoT systems.},
DOI = {10.3390/s21175745}
}



@Article{sym13091579,
AUTHOR = {Wang, Xinheng and Gao, Xiaojin and Wang, Zuoxun and Ma, Chunrui and Song, Zengxu},
TITLE = {A Combined Model Based on EOBL-CSSA-LSSVM for Power Load Forecasting},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1579},
URL = {https://www.mdpi.com/2073-8994/13/9/1579},
ISSN = {2073-8994},
ABSTRACT = {Inaccurate electricity load forecasting can lead to the power sector gaining asymmetric information in the supply and demand relationship. This asymmetric information can lead to incorrect production or generation plans for the power sector. In order to improve the accuracy of load forecasting, a combined power load forecasting model based on machine learning algorithms, swarm intelligence optimization algorithms, and data pre-processing is proposed. Firstly, the original signal is pre-processed by the VMD–singular spectrum analysis data pre-processing method. Secondly, the noise-reduced signals are predicted using the Elman prediction model optimized by the sparrow search algorithm, the ELM prediction model optimized by the chaotic adaptive whale algorithm (CAWOA-ELM), and the LSSVM prediction model optimized by the chaotic sparrow search algorithm based on elite opposition-based learning (EOBL-CSSA-LSSVM) for electricity load data, respectively. Finally, the weighting coefficients of the three prediction models are calculated using the simulated annealing algorithm and weighted to obtain the prediction results. Comparative simulation experiments show that the VMD–singular spectrum analysis method and two improved intelligent optimization algorithms proposed in this paper can effectively improve the prediction accuracy. Additionally, the combined forecasting model proposed in this paper has extremely high forecasting accuracy, which can help the power sector to develop a reasonable production plan and power generation plans.},
DOI = {10.3390/sym13091579}
}



@Article{agronomy11091711,
AUTHOR = {Anderson, Nicholas Todd and Walsh, Kerry Brian and Koirala, Anand and Wang, Zhenglin and Amaral, Marcelo Henrique and Dickinson, Geoff Robert and Sinha, Priyakant and Robson, Andrew James},
TITLE = {Estimation of Fruit Load in Australian Mango Orchards Using Machine Vision},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1711},
URL = {https://www.mdpi.com/2073-4395/11/9/1711},
ISSN = {2073-4395},
ABSTRACT = {The performance of a multi-view machine vision method was documented at an orchard level, relative to packhouse count. High repeatability was achieved in night-time imaging, with an absolute percentage error of 2% or less. Canopy architecture impacted performance, with reasonable estimates achieved on hedge, single leader and conventional systems (3.4, 5.0, and 8.2 average percentage error, respectively) while fruit load of trellised orchards was over-estimated (at 25.2 average percentage error). Yield estimations were made for multiple orchards via: (i) human count of fruit load on ~5% of trees (FARM), (ii) human count of 18 trees randomly selected within three NDVI stratifications (CAL), (iii) multi-view counts (MV-Raw) and (iv) multi-view corrected for occluded fruit using manual counts of CAL trees (MV-CAL). Across the nine orchards for which results for all methods were available, the FARM, CAL, MV-Raw and MV-CAL methods achieved an average percentage error on packhouse counts of 26, 13, 11 and 17%, with SD of 11, 8, 11 and 9%, respectively, in the 2019–2020 season. The absolute percentage error of the MV-Raw estimates was 10% or less in 15 of the 20 orchards assessed. Greater error in load estimation occurred in the 2020–2021 season due to the time-spread of flowering. Use cases for the tree level data on fruit load was explored in context of fruit load density maps to inform early harvesting and to interpret crop damage, and tree frequency distributions based on fruit load per tree.},
DOI = {10.3390/agronomy11091711}
}



@Article{app11177960,
AUTHOR = {Son, Chang-Hwan},
TITLE = {Leaf Spot Attention Networks Based on Spot Feature Encoding for Leaf Disease Identification and Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {7960},
URL = {https://www.mdpi.com/2076-3417/11/17/7960},
ISSN = {2076-3417},
ABSTRACT = {This study proposes a new attention-enhanced YOLO model that incorporates a leaf spot attention mechanism based on regions-of-interest (ROI) feature extraction into the YOLO framework for leaf disease detection. Inspired by a previous study, which revealed that leaf spot attention based on the ROI-aware feature extraction can improve leaf disease recognition accuracy significantly and outperform state-of-the-art deep learning models, this study extends the leaf spot attention model to leaf disease detection. The primary idea is that spot areas indicating leaf diseases appear only in leaves, whereas the background area does not contain useful information regarding leaf diseases. To increase the discriminative power of the feature extractor that is required in the object detection framework, it is essential to extract informative and discriminative features from the spot and leaf areas. To realize this, a new ROI-aware feature extractor, that is, a spot feature extractor was designed. To divide the leaf image into spot, leaf, and background areas, the leaf segmentation module was first pretrained, and then spot feature encoding was applied to encode spot information. Next, the ROI-aware feature extractor was connected to an ROI-aware feature fusion layer to model the leaf spot attention mechanism, and to be joined with the YOLO detection subnetwork. The experimental results confirm that the proposed ROI-aware feature extractor can improve leaf disease detection by boosting the discriminative power of the spot features. In addition, the proposed attention-enhanced YOLO model outperforms conventional state-of-the-art object detection models.},
DOI = {10.3390/app11177960}
}



@Article{rs13173428,
AUTHOR = {You, Hangkai and Li, Shihua and Xu, Yifan and He, Ze and Wang, Di},
TITLE = {Tree Extraction from Airborne Laser Scanning Data in Urban Areas},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3428},
URL = {https://www.mdpi.com/2072-4292/13/17/3428},
ISSN = {2072-4292},
ABSTRACT = {Tree information in urban areas plays a significant role in many fields of study, such as ecology and environmental management. Airborne LiDAR scanning (ALS) excels at the fast and efficient acquisition of spatial information in urban-scale areas. Tree extraction from ALS data is an essential part of tree structural studies. Current raster-based methods that use canopy height models (CHMs) suffer from the loss of 3D structure information, whereas the existing point-based methods are non-robust in complex environments. Aiming at making full use of the canopy’s 3D structure information that is provided by point cloud data, and ensuring the method’s suitability in complex scenes, this paper proposes a new point-based method for tree extraction that is based on 3D morphological features. Considering the elevation deviations of the ALS data, we propose a neighborhood search method to filter out the ground and flat-roof points. A coarse extraction method, combining planar projection with a point density-filtering algorithm is applied to filter out distracting objects, such as utility poles and cars. After that, a Euclidean cluster extraction (ECE) algorithm is used as an optimization strategy for coarse extraction. In order to verify the robustness and accuracy of the method, airborne LiDAR data from Zhangye, Gansu, China and unmanned aircraft vehicle (UAV) LiDAR data from Xinyang, Henan, China were tested in this study. The experimental results demonstrated that our method was suitable for extracting trees in complex urban scenes with either high or low point densities. The extraction accuracy obtained for the airborne LiDAR data and UAV LiDAR data were 99.4% and 99.2%, respectively. In addition, a further study found that the aberrant vertical structure of the artificially pruned canopy was the main cause of the error. Our method achieved desirable results in different scenes, with only one adjustable parameter, making it an easy-to-use method for urban area studies.},
DOI = {10.3390/rs13173428}
}



@Article{drones5030085,
AUTHOR = {Steup, Christoph and Beckhaus, Jonathan and Mostaghim, Sanaz},
TITLE = {A Single-Copter UWB-Ranging-Based Localization System Extendable to a Swarm of Drones},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {85},
URL = {https://www.mdpi.com/2504-446X/5/3/85},
ISSN = {2504-446X},
ABSTRACT = {This paper presents a single-copter localization system as a first step towards a scalable multihop drone swarm localization system. The drone was equipped with ultrawideband (UWB) transceiver modules, which can be used for communication, as well as distance measurement. The location of the drone was detected based on fixed anchor points using a single type of UWB transceiver. Our aim is to create a swarm localization system that enables drones to switch their role between an active swarm member and an anchor node to enhance the localization of the whole swarm. To this end, this paper presents our current baseline localization system and its performance regarding single-drone localization with fixed anchors and its integration into our current modular quadcopters, which was designed to be easily extendable to a swarm localization system. The distance between each drone and the anchors was measured periodically, and a specially tailored gradient descent algorithm was used to solve the resulting nonlinear optimization problem. Additional copter and wireless-specific adaptations were performed to enhance the robustness. The system was tested with a Vicon system as a position reference and showed a high precision of 0.2 m with an update rate of &lt;10 Hz. Additionally, the system was integrated into the FINken copters of the SwarmLab and evaluated in multiple outdoor scenarios. These scenarios showed the generic usability of the approach, even though no accurate precision measurement was possible.},
DOI = {10.3390/drones5030085}
}



@Article{rs13173437,
AUTHOR = {Qi, Yuan and Dong, Xuhua and Chen, Pengchao and Lee, Kyeong-Hwan and Lan, Yubin and Lu, Xiaoyang and Jia, Ruichang and Deng, Jizhong and Zhang, Yali},
TITLE = {Canopy Volume Extraction of Citrus reticulate Blanco cv. Shatangju Trees Using UAV Image-Based Point Cloud Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3437},
URL = {https://www.mdpi.com/2072-4292/13/17/3437},
ISSN = {2072-4292},
ABSTRACT = {Automatic acquisition of the canopy volume parameters of the Citrus reticulate Blanco cv. Shatangju tree is of great significance to precision management of the orchard. This research combined the point cloud deep learning algorithm with the volume calculation algorithm to segment the canopy of the Citrus reticulate Blanco cv. Shatangju trees. The 3D (Three-Dimensional) point cloud model of a Citrus reticulate Blanco cv. Shatangju orchard was generated using UAV tilt photogrammetry images. The segmentation effects of three deep learning models, PointNet++, MinkowskiNet and FPConv, on Shatangju trees and the ground were compared. The following three volume algorithms: convex hull by slices, voxel-based method and 3D convex hull were applied to calculate the volume of Shatangju trees. Model accuracy was evaluated using the coefficient of determination (R2) and Root Mean Square Error (RMSE). The results show that the overall accuracy of the MinkowskiNet model (94.57%) is higher than the other two models, which indicates the best segmentation effect. The 3D convex hull algorithm received the highest R2 (0.8215) and the lowest RMSE (0.3186 m3) for the canopy volume calculation, which best reflects the real volume of Citrus reticulate Blanco cv. Shatangju trees. The proposed method is capable of rapid and automatic acquisition for the canopy volume of Citrus reticulate Blanco cv. Shatangju trees.},
DOI = {10.3390/rs13173437}
}



@Article{rs13173446,
AUTHOR = {Tan, Junxiang and Zhao, Haojie and Yang, Ronghao and Liu, Hua and Li, Shaoda and Liu, Jianfei},
TITLE = {An Entropy-Weighting Method for Efficient Power-Line Feature Evaluation and Extraction from LiDAR Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3446},
URL = {https://www.mdpi.com/2072-4292/13/17/3446},
ISSN = {2072-4292},
ABSTRACT = {Power-line inspection is an important means to maintain the safety of power networks. Light detection and ranging (LiDAR) technology can provide high-precision 3D information about power corridors for automated power-line inspection, so there are more and more utility companies relying on LiDAR systems instead of traditional manual operation. However, it is still a challenge to automatically detect power lines with high precision. To achieve efficient and accurate power-line extraction, this paper proposes an algorithm using entropy-weighting feature evaluation (EWFE), which is different from the existing hierarchical-multiple-rule evaluation of many geometric features. Six significant features are selected (Height above Ground Surface (HGS), Vertical Range Ratio (VRR), Horizontal Angle (HA), Surface Variation (SV), Linearity (LI) and Curvature Change (CC)), and then the features are combined to construct a vector for quantitative evaluation. The feature weights are determined by an entropy-weighting method (EWM) to achieve optimal distribution. The point clouds are filtered out by the HGS feature, which possesses the highest entropy value, and a portion of non-power-line points can be removed without loss of power-line points. The power lines are extracted by evaluation of the other five features. To decrease the interference from pylon points, this paper analyzes performance in different pylon situations and performs an adaptive weight transformation. We evaluate the EWFE method using four datasets with different transmission voltage scales captured by a light unmanned aerial vehicle (UAV) LiDAR system and a mobile LiDAR system. Experimental results show that our method demonstrates efficient performance, while algorithm parameters remain consistent for the four datasets. The precision F value ranges from 98.4% to 99.7%, and the efficiency ranges from 0.9 million points/s to 5.2 million points/s.},
DOI = {10.3390/rs13173446}
}



@Article{jimaging7090171,
AUTHOR = {Loddo, Andrea and Di Ruberto, Cecilia},
TITLE = {On the Efficacy of Handcrafted and Deep Features for Seed Image Classification},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {171},
URL = {https://www.mdpi.com/2313-433X/7/9/171},
PubMedID = {34564097},
ISSN = {2313-433X},
ABSTRACT = {Computer vision techniques have become important in agriculture and plant sciences due to their wide variety of applications. In particular, the analysis of seeds can provide meaningful information on their evolution, the history of agriculture, the domestication of plants, and knowledge of diets in ancient times. This work aims to propose an exhaustive comparison of several different types of features in the context of multiclass seed classification, leveraging two public plant seeds data sets to classify their families or species. In detail, we studied possible optimisations of five traditional machine learning classifiers trained with seven different categories of handcrafted features. We also fine-tuned several well-known convolutional neural networks (CNNs) and the recently proposed SeedNet to determine whether and to what extent using their deep features may be advantageous over handcrafted features. The experimental results demonstrated that CNN features are appropriate to the task and representative of the multiclass scenario. In particular, SeedNet achieved a mean F-measure of 96%, at least. Nevertheless, several cases showed satisfactory performance from the handcrafted features to be considered a valid alternative. In detail, we found that the Ensemble strategy combined with all the handcrafted features can achieve 90.93% of mean F-measure, at least, with a considerably lower amount of times. We consider the obtained results an excellent preliminary step towards realising an automatic seeds recognition and classification framework.},
DOI = {10.3390/jimaging7090171}
}



@Article{drones5030087,
AUTHOR = {Kotecha, Ketan and Garg, Deepak and Mishra, Balmukund and Narang, Pratik and Mishra, Vipul Kumar},
TITLE = {Background Invariant Faster Motion Modeling for Drone Action Recognition},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {87},
URL = {https://www.mdpi.com/2504-446X/5/3/87},
ISSN = {2504-446X},
ABSTRACT = {Visual data collected from drones has opened a new direction for surveillance applications and has recently attracted considerable attention among computer vision researchers. Due to the availability and increasing use of the drone for both public and private sectors, it is a critical futuristic technology to solve multiple surveillance problems in remote areas. One of the fundamental challenges in recognizing crowd monitoring videos’ human action is the precise modeling of an individual’s motion feature. Most state-of-the-art methods heavily rely on optical flow for motion modeling and representation, and motion modeling through optical flow is a time-consuming process. This article underlines this issue and provides a novel architecture that eliminates the dependency on optical flow. The proposed architecture uses two sub-modules, FMFM (faster motion feature modeling) and AAR (accurate action recognition), to accurately classify the aerial surveillance action. Another critical issue in aerial surveillance is a deficiency of the dataset. Out of few datasets proposed recently, most of them have multiple humans performing different actions in the same scene, such as a crowd monitoring video, and hence not suitable for directly applying to the training of action recognition models. Given this, we have proposed a novel dataset captured from top view aerial surveillance that has a good variety in terms of actors, daytime, and environment. The proposed architecture has shown the capability to be applied in different terrain as it removes the background before using the action recognition model. The proposed architecture is validated through the experiment with varying investigation levels and achieves a remarkable performance of 0.90 validation accuracy in aerial action recognition.},
DOI = {10.3390/drones5030087}
}



@Article{agriculture11090837,
AUTHOR = {Bhoi, Priya Brata and Wali, Veeresh S. and Swain, Deepak Kumar and Sharma, Kalpana and Bhoi, Akash Kumar and Bacco, Manlio and Barsocchi, Paolo},
TITLE = {Input Use Efficiency Management for Paddy Production Systems in India: A Machine Learning Approach},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {837},
URL = {https://www.mdpi.com/2077-0472/11/9/837},
ISSN = {2077-0472},
ABSTRACT = {This research illustrates the technical efficiency of the pan-India paddy cultivation status obtained through a stochastic frontier approach. The results suggest that the mean technical efficiency varies from 0.64 in Gujarat to 0.95 in Odisha. Inputs like human labor, mechanical labor, fertilizer, irrigation and insecticide were found to determine the yield in paddy cultivation across India (except for Chhattisgarh). Inefficiency in the paddy production in Punjab, Bihar, West Bengal, Andhra Pradesh, Tamil Nadu, Kerala, Assam, Gujarat and Odisha in 2016–2017 was caused by technical inefficiency due to poor input management, as suggested by the significant σ2U and σ2v values of the stochastic frontier model. In addition, most of the farm groups in the study operated in the high-efficiency group (80–90% technical efficiency). No specific pattern of input use can be visualized through descriptive measures to give any specific policy implication. Thus, machine learning algorithms based on the input parameters were tested on the data in order to predict the farmers’ efficiency class for individual states. The highest mean accuracy of 0.80 for the models of all of the states was achieved in random forest models. Among the various states of India, the best random forest prediction model based on accuracy was fitted to the input data of Bihar (0.91), followed by Uttar Pradesh (0.89), Andhra Pradesh (0.88), Assam (0.88) and West Bengal (0.86). Thus, the study provides a technique for the classification and prediction of a farmer’s efficiency group from the levels of input use in paddy cultivation for each state in the study. The study uses the DES input dataset to classify and predict the efficiency group of the farmer, as other machine learning models in agriculture have used mostly satellite, spectral imaging and soil property data to detect disease, weeds and crops.},
DOI = {10.3390/agriculture11090837}
}



@Article{s21175875,
AUTHOR = {Yang, Ming-Der and Hsu, Yu-Chun and Tseng, Wei-Cheng and Lu, Chian-Yu and Yang, Chin-Ying and Lai, Ming-Hsin and Wu, Dong-Hong},
TITLE = {Assessment of Grain Harvest Moisture Content Using Machine Learning on Smartphone Images for Optimal Harvest Timing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5875},
URL = {https://www.mdpi.com/1424-8220/21/17/5875},
PubMedID = {34502765},
ISSN = {1424-8220},
ABSTRACT = {Grain moisture content (GMC) is a key indicator of the appropriate harvest period of rice. Conventional testing is time-consuming and laborious, thus not to be implemented over vast areas and to enable the estimation of future changes for revealing optimal harvesting. Images of single panicles were shot with smartphones and corrected using a spectral–geometric correction board. In total, 86 panicle samples were obtained each time and then dried at 80 °C for 7 days to acquire the wet-basis GMC. In total, 517 valid samples were obtained, in which 80% was randomly used for training and 20% was used for testing to construct the image-based GMC assessment model. In total, 17 GMC surveys from a total of 201 samples were also performed from an area of 1 m2 representing on-site GMC, which enabled a multi-day GMC prediction. Eight color indices were selected using principal component analysis for building four machine learning models, including random forest, multilayer perceptron, support vector regression (SVR), and multivariate linear regression. The SVR model with a MAE of 1.23% was the most suitable for GMC of less than 40%. This study provides a real-time and cost-effective non-destructive GMC measurement using smartphones that enables on-farm prediction of harvest dates and facilitates the harvesting scheduling of agricultural machinery.},
DOI = {10.3390/s21175875}
}



@Article{electronics10172125,
AUTHOR = {Upadhyay, Jatin and Rawat, Abhishek and Deb, Dipankar},
TITLE = {Multiple Drone Navigation and Formation Using Selective Target Tracking-Based Computer Vision},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2125},
URL = {https://www.mdpi.com/2079-9292/10/17/2125},
ISSN = {2079-9292},
ABSTRACT = {Autonomous unmanned aerial vehicles work seamlessly within the GPS signal range, but their performance deteriorates in GPS-denied regions. This paper presents a unique collaborative computer vision-based approach for target tracking as per the image’s specific location of interest. The proposed method tracks any object without considering its properties like shape, color, size, or pattern. It is required to keep the target visible and line of sight during the tracking. The method gives freedom of selection to a user to track any target from the image and form a formation around it. We calculate the parameters like distance and angle from the image center to the object for the individual drones. Among all the drones, the one with a significant GPS signal strength or nearer to the target is chosen as the master drone to calculate the relative angle and distance between an object and other drones considering approximate Geo-location. Compared to actual measurements, the results of tests done on a quadrotor UAV frame achieve 99% location accuracy in a robust environment inside the exact GPS longitude and latitude block as GPS-only navigation methods. The individual drones communicate to the ground station through a telemetry link. The master drone calculates the parameters using data collected at ground stations. Various formation flying methods help escort other drones to meet the desired objective with a single high-resolution first-person view (FPV) camera. The proposed method is tested for Airborne Object Target Tracking (AOT) aerial vehicle model and achieves higher tracking accuracy.},
DOI = {10.3390/electronics10172125}
}



@Article{agriculture11090843,
AUTHOR = {Liu, Chengqi and Zhou, Han and Cao, Jing and Guo, Xuchao and Su, Jie and Wang, Longhe and Lu, Shuhan and Li, Lin},
TITLE = {Behavior Trajectory Tracking of Piglets Based on DLC-KPCA},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {843},
URL = {https://www.mdpi.com/2077-0472/11/9/843},
ISSN = {2077-0472},
ABSTRACT = {Tracking the behavior trajectories in pigs in group is becoming increasingly important for welfare feeding. A novel method was proposed in this study to accurately track individual trajectories of pigs in group and analyze their behavior characteristics. First, a multi-pig trajectory tracking model was established based on DeepLabCut (DLC) to realize the daily trajectory tracking of piglets. Second, a high-dimensional spatiotemporal feature model was established based on kernel principal component analysis (KPCA) to achieve nonlinear trajectory optimal clustering. At the same time, the abnormal trajectory correction model was established from five dimensions (semantic, space, angle, time, and velocity) to avoid trajectory loss and drift. Finally, the thermal map of the track distribution was established to analyze the four activity areas of the piggery (resting, drinking, excretion, and feeding areas). Experimental results show that the trajectory tracking accuracy of our method reaches 96.88%, the tracking speed is 350 fps, and the loss value is 0.002. Thus, the method based on DLC–KPCA can meet the requirements of identification of piggery area and tracking of piglets’ behavior. This study is helpful for automatic monitoring of animal behavior and provides data support for breeding.},
DOI = {10.3390/agriculture11090843}
}



@Article{rs13173459,
AUTHOR = {Pranga, Joanna and Borra-Serrano, Irene and Aper, Jonas and De Swaef, Tom and Ghesquiere, An and Quataert, Paul and Roldán-Ruiz, Isabel and Janssens, Ivan A. and Ruysschaert, Greet and Lootens, Peter},
TITLE = {Improving Accuracy of Herbage Yield Predictions in Perennial Ryegrass with UAV-Based Structural and Spectral Data Fusion and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3459},
URL = {https://www.mdpi.com/2072-4292/13/17/3459},
ISSN = {2072-4292},
ABSTRACT = {High-throughput field phenotyping using close remote sensing platforms and sensors for non-destructive assessment of plant traits can support the objective evaluation of yield predictions of large breeding trials. The main objective of this study was to examine the potential of unmanned aerial vehicle (UAV)-based structural and spectral features and their combination in herbage yield predictions across diploid and tetraploid varieties and breeding populations of perennial ryegrass (Lolium perenne L.). Canopy structural (i.e., canopy height) and spectral (i.e., vegetation indices) information were derived from data gathered with two sensors: a consumer-grade RGB and a 10-band multispectral (MS) camera system, which were compared in the analysis. A total of 468 field plots comprising 115 diploid and 112 tetraploid varieties and populations were considered in this study. A modelling framework established to predict dry matter yield (DMY), was used to test three machine learning algorithms, including Partial Least Squares Regression (PLSR), Random Forest (RF), and Support Vector Machines (SVM). The results of the nested cross-validation revealed: (a) the fusion of structural and spectral features achieved better DMY estimates as compared to models fitted with structural or spectral data only, irrespective of the sensor, ploidy level or machine learning algorithm applied; (b) models built with MS-based predictor variables, despite their lower spatial resolution, slightly outperformed the RGB-based models, as lower mean relative root mean square error (rRMSE) values were delivered; and (c) on average, the RF technique reported the best model performances among tested algorithms, regardless of the dataset used. The approach introduced in this study can provide accurate yield estimates (up to an RMSE = 308 kg ha−1) and useful information for breeders and practical farm-scale applications.},
DOI = {10.3390/rs13173459}
}



@Article{hydrology8030131,
AUTHOR = {Alexandris, Stavros and Psomiadis, Emmanouil and Proutsos, Nikolaos and Philippopoulos, Panos and Charalampopoulos, Ioannis and Kakaletris, George and Papoutsi, Eleni-Magda and Vassilakis, Stylianos and Paraskevopoulos, Antoniοs},
TITLE = {Integrating Drone Technology into an Innovative Agrometeorological Methodology for the Precise and Real-Time Estimation of Crop Water Requirements},
JOURNAL = {Hydrology},
VOLUME = {8},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {131},
URL = {https://www.mdpi.com/2306-5338/8/3/131},
ISSN = {2306-5338},
ABSTRACT = {Precision agriculture has been at the cutting edge of research during the recent decade, aiming to reduce water consumption and ensure sustainability in agriculture. The proposed methodology was based on the crop water stress index (CWSI) and was applied in Greece within the ongoing research project GreenWaterDrone. The innovative approach combines real spatial data, such as infrared canopy temperature, air temperature, air relative humidity, and thermal infrared image data, taken above the crop field using an aerial micrometeorological station (AMMS) and a thermal (IR) camera installed on an unmanned aerial vehicle (UAV). Following an initial calibration phase, where the ground micrometeorological station (GMMS) was installed in the crop, no equipment needed to be maintained in the field. Aerial and ground measurements were transferred in real time to sophisticated databases and applications over existing mobile networks for further processing and estimation of the actual water requirements of a specific crop at the field level, dynamically alerting/informing local farmers/agronomists of the irrigation necessity and additionally for potential risks concerning their fields. The supported services address farmers’, agricultural scientists’, and local stakeholders’ needs to conform to regional water management and sustainable agriculture policies. As preliminary results of this study, we present indicative original illustrations and data from applying the methodology to assess UAV functionality while aiming to evaluate and standardize all system processes.},
DOI = {10.3390/hydrology8030131}
}



@Article{s21175893,
AUTHOR = {Yu, Xin and Sun, Yushan and Wang, Xiangbin and Zhang, Guocheng},
TITLE = {End-to-End AUV Motion Planning Method Based on Soft Actor-Critic},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5893},
URL = {https://www.mdpi.com/1424-8220/21/17/5893},
PubMedID = {34502781},
ISSN = {1424-8220},
ABSTRACT = {This study aims to solve the problems of poor exploration ability, single strategy, and high training cost in autonomous underwater vehicle (AUV) motion planning tasks and to overcome certain difficulties, such as multiple constraints and a sparse reward environment. In this research, an end-to-end motion planning system based on deep reinforcement learning is proposed to solve the motion planning problem of an underactuated AUV. The system directly maps the state information of the AUV and the environment into the control instructions of the AUV. The system is based on the soft actor–critic (SAC) algorithm, which enhances the exploration ability and robustness to the AUV environment. We also use the method of generative adversarial imitation learning (GAIL) to assist its training to overcome the problem that learning a policy for the first time is difficult and time-consuming in reinforcement learning. A comprehensive external reward function is then designed to help the AUV smoothly reach the target point, and the distance and time are optimized as much as possible. Finally, the end-to-end motion planning algorithm proposed in this research is tested and compared on the basis of the Unity simulation platform. Results show that the algorithm has an optimal decision-making ability during navigation, a shorter route, less time consumption, and a smoother trajectory. Moreover, GAIL can speed up the AUV training speed and minimize the training time without affecting the planning effect of the SAC algorithm.},
DOI = {10.3390/s21175893}
}



@Article{nitrogen2030026,
AUTHOR = {Sapkota, Arati and Sharma, Moha Dutta and Giri, Hom Nath and Shrestha, Bishal and Panday, Dinesh},
TITLE = {Effect of Organic and Inorganic Sources of Nitrogen on Growth, Yield, and Quality of Beetroot Varieties in Nepal},
JOURNAL = {Nitrogen},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {378--391},
URL = {https://www.mdpi.com/2504-3129/2/3/26},
ISSN = {2504-3129},
ABSTRACT = {Economic use of organic and inorganic fertilizers following their availability is necessary for livestock-based Nepalese farming systems. However, how best to integrate these fertilizers in an appropriate manner is not yet clear. Thus, this study was conducted in the horticulture farm of the Agriculture and Forestry University (AFU), Rampur, Chitwan, Nepal from November 2018 to February 2019 to evaluate the effect of organic and inorganic sources of nitrogen (N) on growth, yield, and quality of beetroot (Beta vulgaris L.) varieties. The experiment was laid out in a two factorial randomized complete block design with four replications consisting of two beetroot varieties, i.e., Madhur and Ruby Red, and five N source combinations, i.e., N1: 100% poultry manure (PM), N2: 50% PM + 50% urea, N3: 100% farmyard manure (FYM), N4: 50% FYM + 50% urea, and N5: 100% urea (120:80:40 kg NPK ha−1). Results of this study indicated a significant impact of N sources and varieties on the assessed parameters. During harvest, a significantly higher plant height (41.84 cm), number of leaves per plant (14.68), leaf length (34.56 cm), leaf width (11.38 cm), and beetroot diameter (72.15 mm) were observed in the N2 treatment. Likewise, higher economic (49.78 t ha−1) and biological yields (78.69 t ha−1) were also recorded in the N2 compared to other N sources. Out of the two varieties, the Madhur variety was significantly better in most growth and yield parameters. Similarly, the Madhur variety showed a significantly higher economic (44.49 t ha−1) and biological yields (69.79 t ha−1) compared to the Ruby Red variety. However, the physiological weight loss was higher in the Ruby Red variety. Therefore, the current study suggests that an integration of poultry manure along with the combination of N fertilizer and the Madhur variety is the best combination for quality beetroot production in the Terai region of Nepal.},
DOI = {10.3390/nitrogen2030026}
}



@Article{s21175922,
AUTHOR = {Kalyani, Yogeswaranathan and Collier, Rem},
TITLE = {A Systematic Survey on the Role of Cloud, Fog, and Edge Computing Combination in Smart Agriculture},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5922},
URL = {https://www.mdpi.com/1424-8220/21/17/5922},
PubMedID = {34502813},
ISSN = {1424-8220},
ABSTRACT = {Cloud Computing is a well-established paradigm for building service-centric systems. However, ultra-low latency, high bandwidth, security, and real-time analytics are limitations in Cloud Computing when analysing and providing results for a large amount of data. Fog and Edge Computing offer solutions to the limitations of Cloud Computing. The number of agricultural domain applications that use the combination of Cloud, Fog, and Edge is increasing in the last few decades. This article aims to provide a systematic literature review of current works that have been done in Cloud, Fog, and Edge Computing applications in the smart agriculture domain between 2015 and up-to-date. The key objective of this review is to identify all relevant research on new computing paradigms with smart agriculture and propose a new architecture model with the combinations of Cloud–Fog–Edge. Furthermore, it also analyses and examines the agricultural application domains, research approaches, and the application of used combinations. Moreover, this survey discusses the components used in the architecture models and briefly explores the communication protocols used to interact from one layer to another. Finally, the challenges of smart agriculture and future research directions are briefly pointed out in this article.},
DOI = {10.3390/s21175922}
}



@Article{rs13173495,
AUTHOR = {Cota, Gizelle and Sagan, Vasit and Maimaitijiang, Maitiniyazi and Freeman, Karen},
TITLE = {Forest Conservation with Deep Learning: A Deeper Understanding of Human Geography around the Betampona Nature Reserve, Madagascar},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3495},
URL = {https://www.mdpi.com/2072-4292/13/17/3495},
ISSN = {2072-4292},
ABSTRACT = {Documenting the impacts of climate change and human activities on tropical rainforests is imperative for protecting tropical biodiversity and for better implementation of REDD+ and UN Sustainable Development Goals. Recent advances in very high-resolution satellite sensor systems (i.e., WorldView-3), computing power, and machine learning (ML) have provided improved mapping of fine-scale changes in the tropics. However, approaches so far focused on feature extraction or the extensive tuning of ML parameters, hindering the potential of ML in forest conservation mapping by not using textural information, which is found to be powerful for many applications. Additionally, the contribution of shortwave infrared (SWIR) bands in forest cover mapping is unknown. The objectives were to develop end-to-end mapping of the tropical forest using fully convolution neural networks (FCNNs) with WorldView-3 (WV-3) imagery and to evaluate human impact on the environment using the Betampona Nature Reserve (BNR) in Madagascar as the test site. FCNN (U-Net) using spatial/textural information was implemented and compared with feature-fed pixel-based methods including Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Network (DNN). Results show that the FCNN model outperformed other models with an accuracy of 90.9%, while SVM, RF, and DNN provided accuracies of 88.6%, 84.8%, and 86.6%, respectively. When SWIR bands were excluded from the input data, FCNN provided superior performance over other methods with a 1.87% decrease in accuracy, while the accuracies of other models—SVM, RF, and DNN—decreased by 5.42%, 3.18%, and 8.55%, respectively. Spatial–temporal analysis showed a 0.7% increase in Evergreen Forest within the BNR and a 32% increase in tree cover within residential areas likely due to forest regeneration and conservation efforts. Other effects of conservation efforts are also discussed.},
DOI = {10.3390/rs13173495}
}



@Article{horticulturae7090282,
AUTHOR = {Vrochidou, Eleni and Bazinas, Christos and Manios, Michail and Papakostas, George A. and Pachidis, Theodore P. and Kaburlasos, Vassilis G.},
TITLE = {Machine Vision for Ripeness Estimation in Viticulture Automation},
JOURNAL = {Horticulturae},
VOLUME = {7},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {282},
URL = {https://www.mdpi.com/2311-7524/7/9/282},
ISSN = {2311-7524},
ABSTRACT = {Ripeness estimation of fruits and vegetables is a key factor for the optimization of field management and the harvesting of the desired product quality. Typical ripeness estimation involves multiple manual samplings before harvest followed by chemical analyses. Machine vision has paved the way for agricultural automation by introducing quicker, cost-effective, and non-destructive methods. This work comprehensively surveys the most recent applications of machine vision techniques for ripeness estimation. Due to the broad area of machine vision applications in agriculture, this review is limited only to the most recent techniques related to grapes. The aim of this work is to provide an overview of the state-of-the-art algorithms by covering a wide range of applications. The potential of current machine vision techniques for specific viticulture applications is also analyzed. Problems, limitations of each technique, and future trends are discussed. Moreover, the integration of machine vision algorithms in grape harvesting robots for real-time in-field maturity assessment is additionally examined.},
DOI = {10.3390/horticulturae7090282}
}



@Article{rs13173499,
AUTHOR = {Mohammadi, Masoud and Rashidi, Maria and Mousavi, Vahid and Karami, Ali and Yu, Yang and Samali, Bijan},
TITLE = {Quality Evaluation of Digital Twins Generated Based on UAV Photogrammetry and TLS: Bridge Case Study},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3499},
URL = {https://www.mdpi.com/2072-4292/13/17/3499},
ISSN = {2072-4292},
ABSTRACT = {In the current modern era of information and technology, emerging remote advancements have been widely established for detailed virtual inspections and assessments of infrastructure assets, especially bridges. These technologies are capable of creating an accurate digital representation of the existing assets, commonly known as the digital twins. Digital twins are suitable alternatives to in-person and on-site based assessments that can provide safer, cheaper, more reliable, and less distributive bridge inspections. In the case of bridge monitoring, Unmanned Aerial Vehicle (UAV) photogrammetry and Terrestrial Laser Scanning (TLS) are among the most common advanced technologies that hold the potential to provide qualitative digital models; however, the research is still lacking a reliable methodology to evaluate the generated point clouds in terms of quality and geometric accuracy for a bridge size case study. Therefore, this paper aims to provide a comprehensive methodology along with a thorough bridge case study to evaluate two digital point clouds developed from an existing Australian heritage bridge via both UAV-based photogrammetry and TLS. In this regard, a range of proposed approaches were employed to compare point clouds in terms of points’ distribution, level of outlier noise, data completeness, surface deviation, and geometric accuracy. The comparative results of this case study not only proved the capability and applicability of the proposed methodology and approaches in evaluating these two voluminous point clouds, but they also exhibited a higher level of point density and more acceptable agreements with as-is measurements in TLS-based point clouds subjected to the implementation of a precise data capture and a 3D reconstruction model.},
DOI = {10.3390/rs13173499}
}



@Article{rs13173503,
AUTHOR = {Rodriguez, Roberto and Perroy, Ryan L. and Leary, James and Jenkins, Daniel and Panoff, Max and Mandel, Travis and Perez, Patricia},
TITLE = {Comparing Interpretation of High-Resolution Aerial Imagery by Humans and Artificial Intelligence to Detect an Invasive Tree Species},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3503},
URL = {https://www.mdpi.com/2072-4292/13/17/3503},
ISSN = {2072-4292},
ABSTRACT = {Timely, accurate maps of invasive plant species are critical for making appropriate management decisions to eliminate emerging target populations or contain infestations. High-resolution aerial imagery is routinely used to map, monitor, and detect invasive plant populations. While conventional image interpretation involving human analysts is straightforward, it can require high demands for time and resources to produce useful intelligence. We compared the performance of human analysts with a custom Retinanet-based deep convolutional neural network (DNN) for detecting individual miconia (Miconia calvescens DC) plants, using high-resolution unmanned aerial system (UAS) imagery collected over lowland tropical forests in Hawai’i. Human analysts (n = 38) examined imagery at three linear scrolling speeds (100, 200 and 300 px/s), achieving miconia detection recalls of 74 ± 3%, 60 ± 3%, and 50 ± 3%, respectively. The DNN achieved 83 ± 3% recall and completed the image analysis in 1% of the time of the fastest scrolling speed tested. Human analysts could discriminate large miconia leaf clusters better than isolated individual leaves, while the DNN detection efficacy was independent of leaf cluster size. Optically, the contrast in the red and green color channels and all three (i.e., red, green, and blue) signal to clutter ratios (SCR) were significant factors for human detection, while only the red channel contrast, and the red and green SCRs were significant factors for the DNN. A linear cost analysis estimated the operational use of a DNN to be more cost effective than human photo interpretation when the cumulative search area exceeds a minimum area. For invasive species like miconia, which can stochastically spread propagules across thousands of ha, the DNN provides a more efficient option for detecting incipient, immature miconia across large expanses of forested canopy. Increasing operational capacity for large-scale surveillance with a DNN-based image analysis workflow can provide more rapid comprehension of invasive plant abundance and distribution in forested watersheds and may become strategically vital to containing these invasions.},
DOI = {10.3390/rs13173503}
}



@Article{s21175948,
AUTHOR = {Fuentes, Sigfredo and Tongson, Eden and Unnithan, Ranjith R. and Gonzalez Viejo, Claudia},
TITLE = {Early Detection of Aphid Infestation and Insect-Plant Interaction Assessment in Wheat Using a Low-Cost Electronic Nose (E-Nose), Near-Infrared Spectroscopy and Machine Learning Modeling},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5948},
URL = {https://www.mdpi.com/1424-8220/21/17/5948},
PubMedID = {34502839},
ISSN = {1424-8220},
ABSTRACT = {Advances in early insect detection have been reported using digital technologies through camera systems, sensor networks, and remote sensing coupled with machine learning (ML) modeling. However, up to date, there is no cost-effective system to monitor insect presence accurately and insect-plant interactions. This paper presents results on the implementation of near-infrared spectroscopy (NIR) and a low-cost electronic nose (e-nose) coupled with machine learning. Several artificial neural network (ANN) models were developed based on classification to detect the level of infestation and regression to predict insect numbers for both e-nose and NIR inputs, and plant physiological response based on e-nose to predict photosynthesis rate (A), transpiration (E) and stomatal conductance (gs). Results showed high accuracy for classification models ranging within 96.5–99.3% for NIR and between 94.2–99.2% using e-nose data as inputs. For regression models, high correlation coefficients were obtained for physiological parameters (gs, E and A) using e-nose data from all samples as inputs (R = 0.86) and R = 0.94 considering only control plants (no insect presence). Finally, R = 0.97 for NIR and R = 0.99 for e-nose data as inputs were obtained to predict number of insects. Performances for all models developed showed no signs of overfitting. In this paper, a field-based system using unmanned aerial vehicles with the e-nose as payload was proposed and described for deployment of ML models to aid growers in pest management practices.},
DOI = {10.3390/s21175948}
}



@Article{atmos12091146,
AUTHOR = {Ma, Lei and Zhu, Xiaoxiang and Qiu, Chunping and Blaschke, Thomas and Li, Manchun},
TITLE = {Advances of Local Climate Zone Mapping and Its Practice Using Object-Based Image Analysis},
JOURNAL = {Atmosphere},
VOLUME = {12},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1146},
URL = {https://www.mdpi.com/2073-4433/12/9/1146},
ISSN = {2073-4433},
ABSTRACT = {In the context of climate change and urban heat islands, the concept of local climate zones (LCZ) aims for consistent and comparable mapping of urban surface structure and cover across cities. This study provides a timely survey of remote sensing-based applications of LCZ mapping considering the recent increase in publications. We analyze and evaluate several aspects that affect the performance of LCZ mapping, including mapping units/scale, transferability, sample dataset, low accuracy, and classification schemes. Since current LCZ analysis and mapping are based on per-pixel approaches, this study implements an object-based image analysis (OBIA) method and tests it for two cities in Germany using Sentinel 2 data. A comparison with a per-pixel method yields promising results. This study shall serve as a blueprint for future object-based remotely sensed LCZ mapping approaches.},
DOI = {10.3390/atmos12091146}
}



@Article{drones5030089,
AUTHOR = {Hoseini, Sayed Amir and Hassan, Jahan and Bokani, Ayub and Kanhere, Salil S.},
TITLE = {In Situ MIMO-WPT Recharging of UAVs Using Intelligent Flying Energy Sources},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {89},
URL = {https://www.mdpi.com/2504-446X/5/3/89},
ISSN = {2504-446X},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), used in civilian applications such as emergency medical deliveries, precision agriculture, wireless communication provisioning, etc., face the challenge of limited flight time due to their reliance on the on-board battery. Therefore, developing efficient mechanisms for in situ power transfer to recharge UAV batteries holds potential to extend their mission time. In this paper, we study the use of the far-field wireless power transfer (WPT) technique from specialized, transmitter UAVs (tUAVs) carrying Multiple Input Multiple Output (MIMO) antennas for transferring wireless power to receiver UAVs (rUAVs) in a mission. The tUAVs can fly and adjust their distance to the rUAVs to maximize energy transfer gain. The use of MIMO antennas further boosts the energy reception by narrowing the energy beam toward the rUAVs. The complexity of their dynamic operating environment increases with the growing number of tUAVs and rUAVs with varying levels of energy consumption and residual power. We propose an intelligent trajectory selection algorithm for the tUAVs based on a deep reinforcement learning model called Proximal Policy Optimization (PPO) to optimize the energy transfer gain. The simulation results demonstrate that the PPO-based system achieves about a tenfold increase in flight time for a set of realistic transmit power, distance, sub-band number and antenna numbers. Further, PPO outperforms the benchmark movement strategies of “Traveling Salesman Problem” and “Low Battery First” when used by the tUAVs.},
DOI = {10.3390/drones5030089}
}



@Article{su13179965,
AUTHOR = {Arredondo-Méndez, Víctor Hugo and Para-González, Lorena and Mascaraque-Ramírez, Carlos and Domínguez, Manuel},
TITLE = {The 4.0 Industry Technologies and Their Impact in the Continuous Improvement and the Organizational Results: An Empirical Approach},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {9965},
URL = {https://www.mdpi.com/2071-1050/13/17/9965},
ISSN = {2071-1050},
ABSTRACT = {This study analyses the relationships between the technologies of Industry 4.0, continuous improvement, and the business results. To carry out this study, 109 questionnaires to companies of different sectors were collected, but an indispensable condition to take into account was the fact that these companies develop themselves their logistics management. The analysis of the results obtained through the Partial Least Squares (PLS) methodology argues that there is a positive relationship between 4.0 Industry and continuous improvement processes, as well as between continuous improvement processes and organizational results, although it cannot be concluded that a direct relationship between 4.0 Industry and organizational results exists, which means that there are other variables, such as continuous improvement, mediating between them. With this work, there is already an accredited reference of the relationship, which has been verified to exist, between the Industry 4.0, the continuous improvement, and the business results.},
DOI = {10.3390/su13179965}
}



@Article{s21175974,
AUTHOR = {Du, Chunyu and Fan, Wenyi and Ma, Ye and Jin, Hung-Il and Zhen, Zhen},
TITLE = {The Effect of Synergistic Approaches of Features and Ensemble Learning Algorithms on Aboveground Biomass Estimation of Natural Secondary Forests Based on ALS and Landsat 8},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5974},
URL = {https://www.mdpi.com/1424-8220/21/17/5974},
PubMedID = {34502867},
ISSN = {1424-8220},
ABSTRACT = {Although the combination of Airborne Laser Scanning (ALS) data and optical imagery and machine learning algorithms were proved to improve the estimation of aboveground biomass (AGB), the synergistic approaches of different data and ensemble learning algorithms have not been fully investigated, especially for natural secondary forests (NSFs) with complex structures. This study aimed to explore the effects of the two factors on AGB estimation of NSFs based on ALS data and Landsat 8 imagery. The synergistic method of extracting novel features (i.e., COLI1 and COLI2) using optimal Landsat 8 features and the best-performing ALS feature (i.e., elevation mean) yielded higher accuracy of AGB estimation than either optical-only or ALS-only features. However, both of them failed to improve the accuracy compared to the simple combination of the untransformed features that generated them. The convolutional neural networks (CNN) model was much superior to other classic machine learning algorithms no matter of features. The stacked generalization (SG) algorithms, a kind of ensemble learning algorithms, greatly improved the accuracies compared to the corresponding base model, and the SG with the CNN meta-model performed best. This study provides technical support for a wall-to-wall AGB mapping of NSFs of northeastern China using efficient features and algorithms.},
DOI = {10.3390/s21175974}
}



@Article{rs13183563,
AUTHOR = {Koeva, Mila and Gasuku, Oscar and Lengoiboni, Monica and Asiama, Kwabena and Bennett, Rohan Mark and Potel, Jossam and Zevenbergen, Jaap},
TITLE = {Remote Sensing for Property Valuation: A Data Source Comparison in Support of Fair Land Taxation in Rwanda},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3563},
URL = {https://www.mdpi.com/2072-4292/13/18/3563},
ISSN = {2072-4292},
ABSTRACT = {Remotely sensed data is increasingly applied across many domains, including fit-for-purpose land administration (FFPLA), where the focus is on fast, affordable, and accurate property information collection. Property valuation, as one of the main functions of land administration systems, is influenced by locational, physical, legal, and economic factors. Despite the importance of property valuation to economic development, there are often no standardized rules or strict data requirements for property valuation for taxation in developing contexts, such as Rwanda. This study aims at assessing different remote sensing data in support of developing a new approach for property valuation for taxation in Rwanda; one that aligns with the FFPLA philosophy. Three different remote sensing technologies, (i) aerial images acquired with a digital camera, (ii) WorldView2 satellite images, and (iii) unmanned aerial vehicle (UAV) images obtained with a DJI Phantom 2 Vision Plus quadcopter, are compared and analyzed in terms of their fitness to fulfil the requirements for valuation for taxation purposes. Quantitative and qualitative methods are applied for the comparative analysis. Prior to the field visit, the fundamental concepts of property valuation for taxation and remote sensing were reviewed. In the field, reference data using high precision GNSS (Leica) was collected and used for quantitative assessment. Primary data was further collected via semi-structured interviews and focus group discussions. The results show that UAVs have the highest potential for collecting data to support property valuation for taxation. The main reasons are the prime need for accurate-enough and up-to-date information. The comparison of the different remote sensing techniques and the provided new approach can support land valuers and professionals in the field in bottom-up activities following the FFPLA principles and maintaining the temporal quality of data needed for fair taxation.},
DOI = {10.3390/rs13183563}
}



@Article{agronomy11091809,
AUTHOR = {Roslim, Muhammad Huzaifah Mohd and Juraimi, Abdul Shukor and Che’Ya, Nik Norasma and Sulaiman, Nursyazyla and Manaf, Muhammad Noor Hazwan Abd and Ramli, Zaid and Motmainna, Mst.},
TITLE = {Using Remote Sensing and an Unmanned Aerial System for Weed Management in Agricultural Crops: A Review},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1809},
URL = {https://www.mdpi.com/2073-4395/11/9/1809},
ISSN = {2073-4395},
ABSTRACT = {Weeds are unwanted plants that can reduce crop yields by competing for water, nutrients, light, space, and carbon dioxide, which need to be controlled to meet future food production requirements. The integration of drones, artificial intelligence, and various sensors, which include hyperspectral, multi-spectral, and RGB (red-green-blue), ensure the possibility of a better outcome in managing weed problems. Most of the major or minor challenges caused by weed infestation can be faced by implementing remote sensing systems in various agricultural tasks. It is a multi-disciplinary science that includes spectroscopy, optics, computer, photography, satellite launching, electronics, communication, and several other fields. Future challenges, including food security, sustainability, supply and demand, climate change, and herbicide resistance, can also be overcome by those technologies based on machine learning approaches. This review provides an overview of the potential and practical use of unmanned aerial vehicle and remote sensing techniques in weed management practices and discusses how they overcome future challenges.},
DOI = {10.3390/agronomy11091809}
}



@Article{computers10090112,
AUTHOR = {Corso, Marcelo Picolotto and Perez, Fabio Luis and Stefenon, Stéfano Frizzo and Yow, Kin-Choong and García Ovejero, Raúl and Leithardt, Valderi Reis Quietinho},
TITLE = {Classification of Contaminated Insulators Using k-Nearest Neighbors Based on Computer Vision},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {112},
URL = {https://www.mdpi.com/2073-431X/10/9/112},
ISSN = {2073-431X},
ABSTRACT = {Contamination on insulators may increase the surface conductivity of the insulator, and as a consequence, electrical discharges occur more frequently, which can lead to interruptions in a power supply. To maintain reliability in an electrical distribution power system, components that have lost their insulating properties must be replaced. Identifying the components that need maintenance is a difficult task as there are several levels of contamination that are hard to notice during inspections. To improve the quality of inspections, this paper proposes using k-nearest neighbors (k-NN) to classify the levels of insulator contamination based on images of insulators at various levels of contamination simulated in the laboratory. Computer vision features such as mean, variance, asymmetry, kurtosis, energy, and entropy are used for training the k-NN. To assess the robustness of the proposed approach, a statistical analysis and a comparative assessment with well-consolidated algorithms such as decision tree, ensemble subspace, and support vector machine models are presented. The k-NN showed up to 85.17% accuracy using the k-fold cross-validation method, with an average accuracy higher than 82% for the multi-classification of contamination of insulators, being superior to the compared models.},
DOI = {10.3390/computers10090112}
}



@Article{s21186035,
AUTHOR = {Cheng, Min-Lung and Matsuoka, Masashi},
TITLE = {An Efficient and Precise Remote Sensing Optical Image Matching Technique Using Binary-Based Feature Points},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6035},
URL = {https://www.mdpi.com/1424-8220/21/18/6035},
PubMedID = {34577242},
ISSN = {1424-8220},
ABSTRACT = {Matching local feature points is an important but crucial step for various optical image processing applications, such as image registration, image mosaicking, and structure-from-motion (SfM). Three significant issues associated with this subject have been the focus for years, including the robustness of the image features detected, the number of matches obtained, and the efficiency of the data processing. This paper proposes a systematic algorithm that incorporates the synthetic-colored enhanced accelerated binary robust invariant scalar keypoints (SC-EABRISK) method and the affine transformation with bounding box (ATBB) procedure to address these three issues. The SC-EABRISK approach selects the most representative feature points from an image and rearranges their descriptors by adding color information for more precise image matching. The ATBB procedure, meanwhile, is an outreach that implements geometric mapping to retrieve more matches from the feature points ignored during SC-EABRISK processing. The experimental results obtained using benchmark imagery datasets, close-range photos (CRPs), and aerial and satellite images indicate that the developed algorithm can perform up to 20 times faster than the previous EABRISK method, achieve thousands of matches, and improve the matching precision by more than 90%. Consequently, SC-EABRISK with the ATBB algorithm can address image matching efficiently and precisely.},
DOI = {10.3390/s21186035}
}



@Article{rs13183594,
AUTHOR = {Xia, Lang and Zhang, Ruirui and Chen, Liping and Li, Longlong and Yi, Tongchuan and Wen, Yao and Ding, Chenchen and Xie, Chunchun},
TITLE = {Evaluation of Deep Learning Segmentation Models for Detection of Pine Wilt Disease in Unmanned Aerial Vehicle Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3594},
URL = {https://www.mdpi.com/2072-4292/13/18/3594},
ISSN = {2072-4292},
ABSTRACT = {Pine wilt disease (PWD) is a serious threat to pine forests. Combining unmanned aerial vehicle (UAV) images and deep learning (DL) techniques to identify infected pines is the most efficient method to determine the potential spread of PWD over a large area. In particular, image segmentation using DL obtains the detailed shape and size of infected pines to assess the disease’s degree of damage. However, the performance of such segmentation models has not been thoroughly studied. We used a fixed-wing UAV to collect images from a pine forest in Laoshan, Qingdao, China, and conducted a ground survey to collect samples of infected pines and construct prior knowledge to interpret the images. Then, training and test sets were annotated on selected images, and we obtained 2352 samples of infected pines annotated over different backgrounds. Finally, high-performance DL models (e.g., fully convolutional networks for semantic segmentation, DeepLabv3+, and PSPNet) were trained and evaluated. The results demonstrated that focal loss provided a higher accuracy and a finer boundary than Dice loss, with the average intersection over union (IoU) for all models increasing from 0.656 to 0.701. From the evaluated models, DeepLLabv3+ achieved the highest IoU and an F1 score of 0.720 and 0.832, respectively. Also, an atrous spatial pyramid pooling module encoded multiscale context information, and the encoder–decoder architecture recovered location/spatial information, being the best architecture for segmenting trees infected by the PWD. Furthermore, segmentation accuracy did not improve as the depth of the backbone network increased, and neither ResNet34 nor ResNet50 was the appropriate backbone for most segmentation models.},
DOI = {10.3390/rs13183594}
}



@Article{machines9090193,
AUTHOR = {Sahal, Radhya and Alsamhi, Saeed H. and Brown, Kenneth N. and O’Shea, Donna and McCarthy, Conor and Guizani, Mohsen},
TITLE = {Blockchain-Empowered Digital Twins Collaboration: Smart Transportation Use Case},
JOURNAL = {Machines},
VOLUME = {9},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {193},
URL = {https://www.mdpi.com/2075-1702/9/9/193},
ISSN = {2075-1702},
ABSTRACT = {Digital twins (DTs) is a promising technology in the revolution of the industry and essential for Industry 4.0. DTs play a vital role in improving distributed manufacturing, providing up-to-date operational data representation of physical assets, supporting decision-making, and avoiding the potential risks in distributed manufacturing systems. Furthermore, DTs need to collaborate within distributed manufacturing systems to predict the risks and reach consensus-based decision-making. However, DTs collaboration suffers from single failure due to attack and connection in a centralized manner, data interoperability, authentication, and scalability. To overcome the above challenges, we have discussed the major high-level requirements for the DTs collaboration. Then, we have proposed a conceptual framework to fulfill the DTs collaboration requirements by using the combination of blockchain, predictive analysis techniques, and DTs technologies. The proposed framework aims to empower more intelligence DTs based on blockchain technology. In particular, we propose a concrete ledger-based collaborative DTs framework that focuses on real-time operational data analytics and distributed consensus algorithms. Furthermore, we describe how the conceptual framework can be applied using smart transportation system use cases, i.e., smart logistics and railway predictive maintenance. Finally, we highlighted the future direction to guide interested researchers in this interesting area.},
DOI = {10.3390/machines9090193}
}



@Article{rs13183600,
AUTHOR = {Solórzano, Jonathan V. and Mas, Jean François and Gao, Yan and Gallardo-Cruz, José Alberto},
TITLE = {Land Use Land Cover Classification with U-Net: Advantages of Combining Sentinel-1 and Sentinel-2 Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3600},
URL = {https://www.mdpi.com/2072-4292/13/18/3600},
ISSN = {2072-4292},
ABSTRACT = {The U-net is nowadays among the most popular deep learning algorithms for land use/land cover (LULC) mapping; nevertheless, it has rarely been used with synthetic aperture radar (SAR) and multispectral (MS) imagery. On the other hand, the discrimination between plantations and forests in LULC maps has been emphasized, especially for tropical areas, due to their differences in biodiversity and ecosystem services provision. In this study, we trained a U-net using different imagery inputs from Sentinel-1 and Sentinel-2 satellites, MS, SAR and a combination of both (MS + SAR); while a random forests algorithm (RF) with the MS + SAR input was also trained to evaluate the difference in algorithm selection. The classification system included ten classes, including old-growth and secondary forests, as well as old-growth and young plantations. The most accurate results were obtained with the MS + SAR U-net, where the highest overall accuracy (0.76) and average F1-score (0.58) were achieved. Although MS + SAR and MS U-nets gave similar results for almost all of the classes, for old-growth plantations and secondary forest, the addition of the SAR band caused an F1-score increment of 0.08–0.11 (0.62 vs. 0.54 and 0.45 vs. 0.34, respectively). Consecutively, in comparison with the MS + SAR RF, the MS + SAR U-net obtained higher F1-scores for almost all the classes. Our results show that using the U-net with a combined input of SAR and MS images enabled a higher F1-score and accuracy for a detailed LULC map, in comparison with other evaluated methods.},
DOI = {10.3390/rs13183600}
}



@Article{s21186046,
AUTHOR = {Zdziebko, Paweł and Holak, Krzysztof},
TITLE = {Synthetic Image Generation Using the Finite Element Method and Blender Graphics Program for Modeling of Vision-Based Measurement Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6046},
URL = {https://www.mdpi.com/1424-8220/21/18/6046},
PubMedID = {34577253},
ISSN = {1424-8220},
ABSTRACT = {Computer vision is a frequently used approach in static and dynamic measurements of various mechanical structures. Sometimes, however, conducting a large number of experiments is time-consuming and may require significant financial and human resources. On the contrary, the authors propose a simulation approach for performing experiments to synthetically generate vision data. Synthetic images of mechanical structures subjected to loads are generated in the following way. The finite element method is adopted to compute deformations of the studied structure, and next, the Blender graphics program is used to render images presenting that structure. As a result of the proposed approach, it is possible to obtain synthetic images that reliably reflect static and dynamic experiments. This paper presents the results of the application of the proposed approach in the analysis of a complex-shaped structure for which experimental validation was carried out. In addition, the second example of the process of 3D reconstruction of the examined structure (in a multicamera system) is provided. The results for the structure with damage (cantilever beam) are also presented. The obtained results allow concluding that the proposed approach reliably imitates the images captured during real experiments. In addition, the method can become a tool supporting the vision system configuration process before conducting final experimental research.},
DOI = {10.3390/s21186046}
}



@Article{app11188388,
AUTHOR = {Kim, Bubryur and Serfa Juan, Ronnie O. and Lee, Dong-Eun and Chen, Zengshun},
TITLE = {Importance of Image Enhancement and CDF for Fault Assessment of Photovoltaic Module Using IR Thermal Image},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8388},
URL = {https://www.mdpi.com/2076-3417/11/18/8388},
ISSN = {2076-3417},
ABSTRACT = {Infrared thermography is the science of measuring the infrared energy emitted by an object, translating it to apparent temperature variance, and displaying the result as an infrared image. Significantly, acquiring thermal images delivers distinctive levels of temperature differences in solar panels that correspond to their health status, which is beneficial for the early detection of defects. The proposed algorithm aims to analyze the thermal solar panel images. The acquired thermal solar panel images were segmented into solar cell sizes to provide more detailed information by region or cell area instead of the entire solar panel. This paper uses both the image histogram information and its corresponding cumulative distribution function (CDF), useful for image analysis. The acquired thermal solar panel images are enhanced using grayscale, histogram equalization, and adaptive histogram equalization to represent a domain that is easier to analyze. The experimental results reveal that the extraction results of thermal images provide better histogram and CDF features. Furthermore, the proposed scheme includes the convolutional neural network (CNN) for classifying the enhanced images, which shows that a 97% accuracy of classification was achieved. The proposed scheme could promote different thermal image applications—for example, non-physical visual recognition and fault detection analysis.},
DOI = {10.3390/app11188388}
}



@Article{rs13183604,
AUTHOR = {Yin, Qian and Chen, Ziyi and Zheng, Xin and Xu, Yingjun and Liu, Tianxue},
TITLE = {Sliding Windows Method Based on Terrain Self-Similarity for Higher DEM Resolution in Flood Simulating Modeling},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3604},
URL = {https://www.mdpi.com/2072-4292/13/18/3604},
ISSN = {2072-4292},
ABSTRACT = {A digital elevation model (DEM) is a quantitative representation of terrain and an important tool for Earth science and hydrological applications. A high-resolution DEM provides accurate basic Geodata and plays a crucial role in related scientific research and practical applications. However, in reality, high-resolution DEMs are often difficult to obtain. Due to the self-similarity present within terrains, we proposed a method using the original DEM itself as a sample to expand the DEM using sliding windows method (SWM) and generate a higher resolution DEM. The main processes of SWM include downsampling the original DEM and constructing mapping sets, searching for the optimal matching, window replacement. Then, we repeat these processes with the small-scale expansion factor. In this paper, the grid resolution of the Taitou Basin was expanded from 30 to 10 m. Overall, the superresolution reconstruction results showed that the method could achieve better outcomes than other commonly used techniques and exhibited a slight deviation (root mean square error (RMSE) = 3.38) from the realistic DEM. The generated high-resolution DEM prove to be significant in the application of flood simulation modeling.},
DOI = {10.3390/rs13183604}
}



@Article{app11188404,
AUTHOR = {Caballero, Rafael and Parra, Jesús and Trujillo, Miguel Ángel and Pérez-Grau, Francisco J. and Viguria, Antidio and Ollero, Aníbal},
TITLE = {Aerial Robotic Solution for Detailed Inspection of Viaducts},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8404},
URL = {https://www.mdpi.com/2076-3417/11/18/8404},
ISSN = {2076-3417},
ABSTRACT = {The inspection of public infrastructure, such as viaducts and bridges, is crucial for their proper maintenance given the heavy use of many of them. Current inspection techniques are very costly and manual, requiring highly qualified personnel and involving many risks. This article presents a novel solution for the detailed inspection of viaducts using aerial robotic platforms. The system provides a highly automated visual inspection platform that does not rely on GPS and could even fly underneath the infrastructure. Unlike commercially available solutions, our system automatically references the inspection to a global coordinate system usable throughout the lifespan of the infrastructure. In addition, the system includes another aerial platform with a robotic arm to make contact inspections of detected defects, thus providing information that cannot be obtained only with images. Both aerial robotic platforms feature flexibility in the choice of camera or contact measurement sensors as the situation requires. The system was validated by performing inspection flights on real viaducts.},
DOI = {10.3390/app11188404}
}



@Article{app11188419,
AUTHOR = {Zhao, Jiang and Sun, Jiaming and Cai, Zhihao and Wang, Longhong and Wang, Yingxun},
TITLE = {End-to-End Deep Reinforcement Learning for Image-Based UAV Autonomous Control},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8419},
URL = {https://www.mdpi.com/2076-3417/11/18/8419},
ISSN = {2076-3417},
ABSTRACT = {To achieve the perception-based autonomous control of UAVs, schemes with onboard sensing and computing are popular in state-of-the-art work, which often consist of several separated modules with respective complicated algorithms. Most methods depend on handcrafted designs and prior models with little capacity for adaptation and generalization. Inspired by the research on deep reinforcement learning, this paper proposes a new end-to-end autonomous control method to simplify the separate modules in the traditional control pipeline into a single neural network. An image-based reinforcement learning framework is established, depending on the design of the network architecture and the reward function. Training is performed with model-free algorithms developed according to the specific mission, and the control policy network can map the input image directly to the continuous actuator control command. A simulation environment for the scenario of UAV landing was built. In addition, the results under different typical cases, including both the small and large initial lateral or heading angle offsets, show that the proposed end-to-end method is feasible for perception-based autonomous control.},
DOI = {10.3390/app11188419}
}



@Article{su131810164,
AUTHOR = {Maqsoom, Ahsen and Aslam, Bilal and Gul, Muhammad Ehtisham and Ullah, Fahim and Kouzani, Abbas Z. and Mahmud, M. A. Parvez and Nawaz, Adnan},
TITLE = {Using Multivariate Regression and ANN Models to Predict Properties of Concrete Cured under Hot Weather},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {10164},
URL = {https://www.mdpi.com/2071-1050/13/18/10164},
ISSN = {2071-1050},
ABSTRACT = {Concrete is an important construction material. Its characteristics depend on the environmental conditions, construction methods, and mix factors. Working with concrete is particularly tricky in a hot climate. This study predicts the properties of concrete in hot conditions using the case study of Rawalpindi, Pakistan. In this research, variable casting temperatures, design factors, and curing conditions are investigated for their effects on concrete characteristics. For this purpose, water–cement ratio (w/c), in-situ concrete temperature (T), and curing methods of the concrete are varied, and their effects on pulse velocity (PV), compressive strength (fc), depth of water penetration (WP), and split tensile strength (ft) were studied for up to 180 days. Quadratic regression and artificial neural network (ANN) models have been formulated to forecast the properties of concrete in the current study. The results show that T, curing period, and moist curing strongly influence fc, ft, and PV, while WP is adversely affected by T and moist curing. The ANN model shows better results compared to the quadratic regression model. Furthermore, a combined ANN model of fc, ft, and PV was also developed that displayed higher accuracy than the individual ANN models. These models can help construction site engineers select the appropriate concrete parameters when concreting under hot climates to produce durable and long-lasting concrete.},
DOI = {10.3390/su131810164}
}



@Article{drones5030095,
AUTHOR = {Singha, Subroto and Aydin, Burchan},
TITLE = {Automated Drone Detection Using YOLOv4},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {95},
URL = {https://www.mdpi.com/2504-446X/5/3/95},
ISSN = {2504-446X},
ABSTRACT = {Drones are increasing in popularity and are reaching the public faster than ever before. Consequently, the chances of a drone being misused are multiplying. Automated drone detection is necessary to prevent unauthorized and unwanted drone interventions. In this research, we designed an automated drone detection system using YOLOv4. The model was trained using drone and bird datasets. We then evaluated the trained YOLOv4 model on the testing dataset, using mean average precision (mAP), frames per second (FPS), precision, recall, and F1-score as evaluation parameters. We next collected our own two types of drone videos, performed drone detections, and calculated the FPS to identify the speed of detection at three altitudes. Our methodology showed better performance than what has been found in previous similar studies, achieving a mAP of 74.36%, precision of 0.95, recall of 0.68, and F1-score of 0.79. For video detection, we achieved an FPS of 20.5 on the DJI Phantom III and an FPS of 19.0 on the DJI Mavic Pro.},
DOI = {10.3390/drones5030095}
}



@Article{aerospace8090258,
AUTHOR = {Wada, Daichi and Araujo-Estrada, Sergio A. and Windsor, Shane},
TITLE = {Unmanned Aerial Vehicle Pitch Control under Delay Using Deep Reinforcement Learning with Continuous Action in Wind Tunnel Test},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {258},
URL = {https://www.mdpi.com/2226-4310/8/9/258},
ISSN = {2226-4310},
ABSTRACT = {Nonlinear flight controllers for fixed-wing unmanned aerial vehicles (UAVs) can potentially be developed using deep reinforcement learning. However, there is often a reality gap between the simulation models used to train these controllers and the real world. This study experimentally investigated the application of deep reinforcement learning to the pitch control of a UAV in wind tunnel tests, with a particular focus of investigating the effect of time delays on flight controller performance. Multiple neural networks were trained in simulation with different assumed time delays and then wind tunnel tested. The neural networks trained with shorter delays tended to be susceptible to delay in the real tests and produce fluctuating behaviour. The neural networks trained with longer delays behaved more conservatively and did not produce oscillations but suffered steady state errors under some conditions due to unmodeled frictional effects. These results highlight the importance of performing physical experiments to validate controller performance and how the training approach used with reinforcement learning needs to be robust to reality gaps between simulation and the real world.},
DOI = {10.3390/aerospace8090258}
}



@Article{rs13183649,
AUTHOR = {Morales, Giorgio and Sheppard, John W. and Logan, Riley D. and Shaw, Joseph A.},
TITLE = {Hyperspectral Dimensionality Reduction Based on Inter-Band Redundancy Analysis and Greedy Spectral Selection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3649},
URL = {https://www.mdpi.com/2072-4292/13/18/3649},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imaging systems are becoming widely used due to their increasing accessibility and their ability to provide detailed spectral responses based on hundreds of spectral bands. However, the resulting hyperspectral images (HSIs) come at the cost of increased storage requirements, increased computational time to process, and highly redundant data. Thus, dimensionality reduction techniques are necessary to decrease the number of spectral bands while retaining the most useful information. Our contribution is two-fold: First, we propose a filter-based method called interband redundancy analysis (IBRA) based on a collinearity analysis between a band and its neighbors. This analysis helps to remove redundant bands and dramatically reduces the search space. Second, we apply a wrapper-based approach called greedy spectral selection (GSS) to the results of IBRA to select bands based on their information entropy values and train a compact convolutional neural network to evaluate the performance of the current selection. We also propose a feature extraction framework that consists of two main steps: first, it reduces the total number of bands using IBRA; then, it can use any feature extraction method to obtain the desired number of feature channels. We present classification results obtained from our methods and compare them to other dimensionality reduction methods on three hyperspectral image datasets. Additionally, we used the original hyperspectral data cube to simulate the process of using actual filters in a multispectral imager.},
DOI = {10.3390/rs13183649}
}



@Article{machines9090197,
AUTHOR = {Fourlas, George K. and Karras, George C.},
TITLE = {A Survey on Fault Diagnosis and Fault-Tolerant Control Methods for Unmanned Aerial Vehicles},
JOURNAL = {Machines},
VOLUME = {9},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {197},
URL = {https://www.mdpi.com/2075-1702/9/9/197},
ISSN = {2075-1702},
ABSTRACT = {The continuous evolution of modern technology has led to the creation of increasingly complex and advanced systems. This has been also reflected in the technology of Unmanned Aerial Vehicles (UAVs), where the growing demand for more reliable performance necessitates the development of sophisticated techniques that provide fault diagnosis and fault tolerance in a timely and accurate manner. Typically, a UAV consists of three types of subsystems: actuators, main structure and sensors. Therefore, a fault-monitoring system must be specifically designed to supervise and debug each of these subsystems, so that any faults can be addressed before they lead to disastrous consequences. In this survey article, we provide a detailed overview of recent advances and studies regarding fault diagnosis, Fault-Tolerant Control (FTC) and anomaly detection for UAVs. Concerning fault diagnosis, our interest is mainly focused on sensors and actuators, as these subsystems are mostly prone to faults, while their healthy operation usually ensures the smooth and reliable performance of the aerial vehicle.},
DOI = {10.3390/machines9090197}
}



@Article{rs13183651,
AUTHOR = {Wang, Weiqi and You, Xiong and Zhang, Xin and Chen, Lingyu and Zhang, Lantian and Liu, Xu},
TITLE = {LiDAR-Based SLAM under Semantic Constraints in Dynamic Environments},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3651},
URL = {https://www.mdpi.com/2072-4292/13/18/3651},
ISSN = {2072-4292},
ABSTRACT = {Facing the realistic demands of the application environment of robots, the application of simultaneous localisation and mapping (SLAM) has gradually moved from static environments to complex dynamic environments, while traditional SLAM methods usually result in pose estimation deviations caused by errors in data association due to the interference of dynamic elements in the environment. This problem is effectively solved in the present study by proposing a SLAM approach based on light detection and ranging (LiDAR) under semantic constraints in dynamic environments. Four main modules are used for the projection of point cloud data, semantic segmentation, dynamic element screening, and semantic map construction. A LiDAR point cloud semantic segmentation network SANet based on a spatial attention mechanism is proposed, which significantly improves the real-time performance and accuracy of point cloud semantic segmentation. A dynamic element selection algorithm is designed and used with prior knowledge to significantly reduce the pose estimation deviations caused by SLAM dynamic elements. The results of experiments conducted on the public datasets SemanticKITTI, KITTI, and SemanticPOSS show that the accuracy and robustness of the proposed approach are significantly improved.},
DOI = {10.3390/rs13183651}
}



@Article{buildings11090409,
AUTHOR = {Liu, Wenyao and Meng, Qingfeng and Li, Zhen and Hu, Xin},
TITLE = {Applications of Computer Vision in Monitoring the Unsafe Behavior of Construction Workers: Current Status and Challenges},
JOURNAL = {Buildings},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {409},
URL = {https://www.mdpi.com/2075-5309/11/9/409},
ISSN = {2075-5309},
ABSTRACT = {The unsafe behavior of construction workers is one of the main causes of safety accidents at construction sites. To reduce the incidence of construction accidents and improve the safety performance of construction projects, there is a need to identify risky factors by monitoring the behavior of construction workers. Computer vision (CV) technology, which is a powerful and automated tool used for extracting images and video information from construction sites, has been recognized and adopted as an effective construction site monitoring technology for the identification of risky factors resulting from the unsafe behavior of construction workers. In this article, we introduce the research background of this field and conduct a systematic statistical analysis of the relevant literature in this field through the bibliometric analysis method. Thereafter, we adopt a content-based analysis method to depict the historical explorations in the field. On this basis, the limitations and challenges in this field are identified, and future research directions are proposed. It is found that CV technology can effectively monitor the unsafe behaviors of construction workers. The research findings can enhance people’s understanding of construction safety management.},
DOI = {10.3390/buildings11090409}
}



@Article{atmos12091182,
AUTHOR = {Zhang, Zhenduo and Zheng, Wenbo and Li, Ying and Cao, Kai and Xie, Ming and Wu, Peng},
TITLE = {Monitoring Sulfur Content in Marine Fuel Oil Using Ultraviolet Imaging Technology},
JOURNAL = {Atmosphere},
VOLUME = {12},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1182},
URL = {https://www.mdpi.com/2073-4433/12/9/1182},
ISSN = {2073-4433},
ABSTRACT = {The emission of SO2 from ships is an important source of atmospheric pollution. Therefore, the International Maritime Organization (IMO) has established strict requirements for the sulfur content of marine fuel oil. In this paper, a new optical noncontact detection technique for ship exhaust emissions analysis is studied. Firstly, the single-band simulation analysis model of the imaging detection technology for SO2 concentration in ship exhaust gas and the deep neural network model for the prediction of sulfur content were established. A bench test was designed to monitor the tail gas concentration simultaneously using online and imaging detection methods, so as to obtain the concentration data in the flue and the ultraviolet image data. The results showed that 300 nm had a higher inversion accuracy than the other two bands. Finally, a deep neural network model was trained with the SO2 concentration data from the inversion and the engine power, and the predictive model of sulfur content in marine fuel oil was thereby obtained. When the deep learning model was used to predict sulfur content, the prediction accuracy at 300, 310, and 330 nm was 73%, 94%, and 71%, respectively.},
DOI = {10.3390/atmos12091182}
}



@Article{rs13183669,
AUTHOR = {Martínez Prentice, Ricardo and Villoslada Peciña, Miguel and Ward, Raymond D. and Bergamo, Thaisa F. and Joyce, Chris B. and Sepp, Kalev},
TITLE = {Machine Learning Classification and Accuracy Assessment from High-Resolution Images of Coastal Wetlands},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3669},
URL = {https://www.mdpi.com/2072-4292/13/18/3669},
ISSN = {2072-4292},
ABSTRACT = {High-resolution images obtained by multispectral cameras mounted on Unmanned Aerial Vehicles (UAVs) are helping to capture the heterogeneity of the environment in images that can be discretized in categories during a classification process. Currently, there is an increasing use of supervised machine learning (ML) classifiers to retrieve accurate results using scarce datasets with samples with non-linear relationships. We compared the accuracies of two ML classifiers using a pixel and object analysis approach in six coastal wetland sites. The results show that the Random Forest (RF) performs better than K-Nearest Neighbors (KNN) algorithm in the classification of pixels and objects and the classification based on pixel analysis is slightly better than the object-based analysis. The agreement between the classifications of objects and pixels is higher in Random Forest. This is likely due to the heterogeneity of the study areas, where pixel-based classifications are most appropriate. In addition, from an ecological perspective, as these wetlands are heterogeneous, the pixel-based classification reflects a more realistic interpretation of plant community distribution.},
DOI = {10.3390/rs13183669}
}



@Article{rs13183670,
AUTHOR = {Li, Wangbin and Sun, Kaimin and Du, Zhuotong and Hu, Xiuqing and Li, Wenzhuo and Wei, Jinjiang and Gao, Song},
TITLE = {PCNet: Cloud Detection in FY-3D True-Color Imagery Using Multi-Scale Pyramid Contextual Information},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3670},
URL = {https://www.mdpi.com/2072-4292/13/18/3670},
ISSN = {2072-4292},
ABSTRACT = {Cloud, one of the poor atmospheric conditions, significantly reduces the usability of optical remote-sensing data and hampers follow-up applications. Thus, the identification of cloud remains a priority for various remote-sensing activities, such as product retrieval, land-use/cover classification, object detection, and especially for change detection. However, the complexity of clouds themselves make it difficult to detect thin clouds and small isolated clouds. To accurately detect clouds in satellite imagery, we propose a novel neural network named the Pyramid Contextual Network (PCNet). Considering the limited applicability of a regular convolution kernel, we employed a Dilated Residual Block (DRB) to extend the receptive field of the network, which contains a dilated convolution and residual connection. To improve the detection ability for thin clouds, the proposed new model, pyramid contextual block (PCB), was used to generate global information at different scales. FengYun-3D MERSI-II remote-sensing images covering China with 14,165 × 24,659 pixels, acquired on 17 July 2019, are processed to conduct cloud-detection experiments. Experimental results show that the overall precision rates of the trained network reach 97.1% and the overall recall rates reach 93.2%, which performs better both in quantity and quality than U-Net, UNet++, UNet3+, PSPNet and DeepLabV3+.},
DOI = {10.3390/rs13183670}
}



@Article{s21186165,
AUTHOR = {Shaukat, Nabil and Moinuddin, Muhammad and Otero, Pablo},
TITLE = {Underwater Vehicle Positioning by Correntropy-Based Fuzzy Multi-Sensor Fusion},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6165},
URL = {https://www.mdpi.com/1424-8220/21/18/6165},
PubMedID = {34577372},
ISSN = {1424-8220},
ABSTRACT = {The ability of the underwater vehicle to determine its precise position is vital to completing a mission successfully. Multi-sensor fusion methods for underwater vehicle positioning are commonly based on Kalman filtering, which requires the knowledge of process and measurement noise covariance. As the underwater conditions are continuously changing, incorrect process and measurement noise covariance affect the accuracy of position estimation and sometimes cause divergence. Furthermore, the underwater multi-path effect and nonlinearity cause outliers that have a significant impact on positional accuracy. These non-Gaussian outliers are difficult to handle with conventional Kalman-based methods and their fuzzy variants. To address these issues, this paper presents a new and improved adaptive multi-sensor fusion method by using information-theoretic, learning-based fuzzy rules for Kalman filter covariance adaptation in the presence of outliers. Two novel metrics are proposed by utilizing correntropy Gaussian and Versoria kernels for matching theoretical and actual covariance. Using correntropy-based metrics and fuzzy logic together makes the algorithm robust against outliers in nonlinear dynamic underwater conditions. The performance of the proposed sensor fusion technique is compared and evaluated using Monte-Carlo simulations, and substantial improvements in underwater position estimation are obtained.},
DOI = {10.3390/s21186165}
}



@Article{urbansci5030068,
AUTHOR = {Chaturvedi, Vineet and de Vries, Walter T.},
TITLE = {Machine Learning Algorithms for Urban Land Use Planning: A Review},
JOURNAL = {Urban Science},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {68},
URL = {https://www.mdpi.com/2413-8851/5/3/68},
ISSN = {2413-8851},
ABSTRACT = {Urbanization is persistent globally and has increasingly significant spatial and environmental consequences. It is especially challenging in developing countries due to the increasing pressure on the limited resources, and damage to the bio-physical environment. Traditional analytical methods of studying the urban land use dynamics associated with urbanization are static and tend to rely on top-down approaches, such as linear and mathematical modeling. These traditional approaches do not capture the nonlinear properties of land use change. New technologies, such as artificial intelligence (AI) and machine learning (ML) have made it possible to model and predict the nonlinear aspects of urban land dynamics. AI and ML are programmed to recognize patterns and carry out predictions, decision making and perform operations with speed and accuracy. Classification, analysis and modeling using earth observation-based data forms the basis for the geospatial support for land use planning. In the process of achieving higher accuracies in the classification of spatial data, ML algorithms are being developed and being improved to enhance the decision-making process. The purpose of the research is to bring out the various ML algorithms and statistical models that have been applied to study aspects of land use planning using earth observation-based data (EO). It intends to review their performance, functional requirements, interoperability requirements and for which research problems can they be applied best. The literature review revealed that random forest (RF), deep learning like convolutional neural network (CNN) and support vector machine (SVM) algorithms are best suited for classification and pattern analysis of earth observation-based data. GANs (generative adversarial networks) have been used to simulate urban patterns. Algorithms like cellular automata, spatial logistic regression and agent-based modeling have been used for studying urban growth, land use change and settlement pattern analysis. Most of the papers reviewed applied ML algorithms for classification of EO data and to study urban growth and land use change. It is observed that hybrid approaches have better performance in terms of accuracies, efficiency and computational cost.},
DOI = {10.3390/urbansci5030068}
}



@Article{ijgi10090606,
AUTHOR = {Daranagama, Samitha and Witayangkurn, Apichon},
TITLE = {Automatic Building Detection with Polygonizing and Attribute Extraction from High-Resolution Images},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {606},
URL = {https://www.mdpi.com/2220-9964/10/9/606},
ISSN = {2220-9964},
ABSTRACT = {Buildings can be introduced as a fundamental element for forming a city. Therefore, up-to-date building maps have become vital for many applications, including urban mapping and urban expansion analysis. With the development of deep learning, segmenting building footprints from high-resolution remote sensing imagery has become a subject of intense study. Here, a modified version of the U-Net architecture with a combination of pre- and post-processing techniques was developed to extract building footprints from high-resolution aerial imagery and unmanned aerial vehicle (UAV) imagery. Data pre-processing with the logarithmic correction image enhancing algorithm showed the most significant improvement in the building detection accuracy for aerial images; meanwhile, the CLAHE algorithm improved the most concerning UAV images. This study developed a post-processing technique using polygonizing and polygon smoothing called the Douglas–Peucker algorithm, which made the building output directly ready to use for different applications. The attribute information, land use data, and population count data were applied using two open datasets. In addition, the building area and perimeter of each building were calculated as geometric attributes.},
DOI = {10.3390/ijgi10090606}
}



@Article{rs13183671,
AUTHOR = {Wang, Andong and Zhou, Guoxu and Zhao, Qibin},
TITLE = {Guaranteed Robust Tensor Completion via ∗L-SVD with Applications to Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3671},
URL = {https://www.mdpi.com/2072-4292/13/18/3671},
ISSN = {2072-4292},
ABSTRACT = {This paper conducts a rigorous analysis for the problem of robust tensor completion, which aims at recovering an unknown three-way tensor from incomplete observations corrupted by gross sparse outliers and small dense noises simultaneously due to various reasons such as sensor dead pixels, communication loss, electromagnetic interferences, cloud shadows, etc. To estimate the underlying tensor, a new penalized least squares estimator is first formulated by exploiting the low rankness of the signal tensor within the framework of tensor ∗L-Singular Value Decomposition (∗L-SVD) and leveraging the sparse structure of the outlier tensor. Then, an algorithm based on the Alternating Direction Method of Multipliers (ADMM) is designed to compute the estimator in an efficient way. Statistically, the non-asymptotic upper bound on the estimation error is established and further proved to be optimal (up to a log factor) in a minimax sense. Simulation studies on synthetic data demonstrate that the proposed error bound can predict the scaling behavior of the estimation error with problem parameters (i.e., tubal rank of the underlying tensor, sparsity of the outliers, and the number of uncorrupted observations). Both the effectiveness and efficiency of the proposed algorithm are evaluated through experiments for robust completion on seven different types of remote sensing data.},
DOI = {10.3390/rs13183671}
}



@Article{rs13183682,
AUTHOR = {Oseland, Eric and Shannon, Kent and Zhou, Jianfeng and Fritschi, Felix and Bish, Mandy D. and Bradley, Kevin W.},
TITLE = {Evaluating the Spectral Response and Yield of Soybean Following Exposure to Sublethal Rates of 2,4-D and Dicamba at Vegetative and Reproductive Growth Stages},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3682},
URL = {https://www.mdpi.com/2072-4292/13/18/3682},
ISSN = {2072-4292},
ABSTRACT = {The commercialization of synthetic auxin-resistant crops and the commensurate increase in post-emergent auxin-mimic herbicide applications has resulted in millions of hectares of injury to sensitive soybeans in the United States since 2016. Visual yield loss estimations following auxin injury can be difficult. The goal of this research was to determine if spectral variations following auxin injury to soybean allow for more precise yield loss estimations. Identical field experiments were performed in 2018, 2019, and 2020 in Columbia, Missouri to compare the ability of established vegetative indices to differentiate between exposure levels of 2,4-D and dicamba in soybean and predict yield loss. Soybeans were planted at three timings for growth stage separation and were exposed to sublethal rates of 2,4-D and dicamba at the R2, R1, and V3 growth stages. A UAV-mounted multispectral sensor was flown over the trial 14 days after the herbicide treatments. The results of this research found that vegetative indices incorporating the red-edge wavelength were more consistent in estimating yield loss than indices comprised of only visible or NIR wavelengths. Yield loss estimations became difficult when soybean injury occurred during later reproductive stages when soybean biomass was increased. This research also determined that when injury occurs to soybean in vegetative growth stages late in the growing season there is a greater likelihood for yield loss to occur due to decreased time for recovery. The results of this research could provide direction for more objective and accurate evaluations of yield loss following synthetic auxin injury than what is currently available.},
DOI = {10.3390/rs13183682}
}



@Article{s21186199,
AUTHOR = {Yan, Yanjun and Xu, Huihui and Zhang, Ning and Han, Guangjie and Liu, Mingliu},
TITLE = {Dynamic Divide Grouping Non-Orthogonal Multiple Access in Terrestrial-Satellite Integrated Network},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6199},
URL = {https://www.mdpi.com/1424-8220/21/18/6199},
PubMedID = {34577407},
ISSN = {1424-8220},
ABSTRACT = {Non-orthogonal multiple access (NOMA) has been extensively studied to improve the performance of the Terrestrial-Satellite Integrated Network (TSIN) on account of the shortage of frequency band resources. In this paper, the terrestrial network and satellite network synergistically provide complete coverage for ground users, and based on the architecture, we first formulate a constrained optimization problem to maximize the sum rate of the TSIN under the limited spectrum resources. As the terrestrial networks and the satellite network will cause interference to each other, we first investigate the capacity performance of the terrestrial networks and the satellite networks separately, in which the optimal power control factor expression is derived. Then, by constructing the relationship model between user elevation angle, beam angle and distance, we develop a dynamic group pairing schemes to ensure the effective pairing of NOMA users. Based on the user pairing, to obtain the optimal resource allocation, a joint optimization algorithm of power allocation, beam channel and base station channel resource is proposed. Finally, simulation results are provided to evaluate the user paring scheme as well as the total system performance, in comparison with the existing works.},
DOI = {10.3390/s21186199}
}



@Article{photonics8090394,
AUTHOR = {Kwan, Chiman and Larkin, Jude},
TITLE = {Detection of Small Moving Objects in Long Range Infrared Videos from a Change Detection Perspective},
JOURNAL = {Photonics},
VOLUME = {8},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {394},
URL = {https://www.mdpi.com/2304-6732/8/9/394},
ISSN = {2304-6732},
ABSTRACT = {Detection of small moving objects in long range infrared (IR) videos is challenging due to background clutter, air turbulence, and small target size. In this paper, we present two unsupervised, modular, and flexible frameworks to detect small moving targets. The key idea was inspired by change detection (CD) algorithms where frame differences can help detect motions. Our frameworks consist of change detection, small target detection, and some post-processing algorithms such as image denoising and dilation. Extensive experiments using actual long range mid-wave infrared (MWIR) videos with target distances beyond 3500 m from the camera demonstrated that one approach, using Local Intensity Gradient (LIG) only once in the workflow, performed better than the other, which used LIG in two places, in a 3500 m video, but slightly worse in 4000 m and 5000 m videos. Moreover, we also investigated the use of synthetic bands for target detection and observed promising results for 4000 m and 5000 m videos. Finally, a comparative study with two conventional methods demonstrated that our proposed scheme has comparable performance.},
DOI = {10.3390/photonics8090394}
}



@Article{ijgi10090617,
AUTHOR = {Yang, Su and Hou, Miaole and Shaker, Ahmed and Li, Songnian},
TITLE = {Modeling and Processing of Smart Point Clouds of Cultural Relics with Complex Geometries},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {617},
URL = {https://www.mdpi.com/2220-9964/10/9/617},
ISSN = {2220-9964},
ABSTRACT = {The digital documentation of cultural relics plays an important role in archiving, protection, and management. In the field of cultural heritage, three-dimensional (3D) point cloud data is effective at expressing complex geometric structures and geometric details on the surface of cultural relics, but lacks semantic information. To elaborate the geometric information of cultural relics and add meaningful semantic information, we propose a modeling and processing method of smart point clouds of cultural relics with complex geometries. An information modeling framework for complex geometric cultural relics was designed based on the concept of smart point clouds, in which 3D point cloud data are organized through the time dimension and different spatial scales indicating different geometric details. The proposed model allows smart point clouds or a subset to be linked with semantic information or related documents. As such, this novel information modeling framework can be used to describe rich semantic information and high-level details of geometry. The proposed information model not only expresses the complex geometric structure of the cultural relics and the geometric details on the surface, but also has rich semantic information, and can even be associated with documents. A case study of the Dazu Thousand-Hand Bodhisattva Statue, which is characterized by a variety of complex geometries, reveals that our proposed framework is capable of modeling and processing the statue with excellent applicability and expansibility. This work provides insights into the sustainable development of cultural heritage protection globally.},
DOI = {10.3390/ijgi10090617}
}



@Article{jimaging7090187,
AUTHOR = {Joseph, Seena and Olugbara, Oludayo O.},
TITLE = {Detecting Salient Image Objects Using Color Histogram Clustering for Region Granularity},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {187},
URL = {https://www.mdpi.com/2313-433X/7/9/187},
PubMedID = {34564113},
ISSN = {2313-433X},
ABSTRACT = {Salient object detection represents a novel preprocessing stage of many practical image applications in the discipline of computer vision. Saliency detection is generally a complex process to copycat the human vision system in the processing of color images. It is a convoluted process because of the existence of countless properties inherent in color images that can hamper performance. Due to diversified color image properties, a method that is appropriate for one category of images may not necessarily be suitable for others. The selection of image abstraction is a decisive preprocessing step in saliency computation and region-based image abstraction has become popular because of its computational efficiency and robustness. However, the performances of the existing region-based salient object detection methods are extremely hooked on the selection of an optimal region granularity. The incorrect selection of region granularity is potentially prone to under- or over-segmentation of color images, which can lead to a non-uniform highlighting of salient objects. In this study, the method of color histogram clustering was utilized to automatically determine suitable homogenous regions in an image. Region saliency score was computed as a function of color contrast, contrast ratio, spatial feature, and center prior. Morphological operations were ultimately performed to eliminate the undesirable artifacts that may be present at the saliency detection stage. Thus, we have introduced a novel, simple, robust, and computationally efficient color histogram clustering method that agglutinates color contrast, contrast ratio, spatial feature, and center prior for detecting salient objects in color images. Experimental validation with different categories of images selected from eight benchmarked corpora has indicated that the proposed method outperforms 30 bottom-up non-deep learning and seven top-down deep learning salient object detection methods based on the standard performance metrics.},
DOI = {10.3390/jimaging7090187}
}



@Article{rs13183710,
AUTHOR = {Abdollahi, Abolfazl and Pradhan, Biswajeet and Shukla, Nagesh and Chakraborty, Subrata and Alamri, Abdullah},
TITLE = {Multi-Object Segmentation in Complex Urban Scenes from High-Resolution Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3710},
URL = {https://www.mdpi.com/2072-4292/13/18/3710},
ISSN = {2072-4292},
ABSTRACT = {Terrestrial features extraction, such as roads and buildings from aerial images using an automatic system, has many usages in an extensive range of fields, including disaster management, change detection, land cover assessment, and urban planning. This task is commonly tough because of complex scenes, such as urban scenes, where buildings and road objects are surrounded by shadows, vehicles, trees, etc., which appear in heterogeneous forms with lower inter-class and higher intra-class contrasts. Moreover, such extraction is time-consuming and expensive to perform by human specialists manually. Deep convolutional models have displayed considerable performance for feature segmentation from remote sensing data in the recent years. However, for the large and continuous area of obstructions, most of these techniques still cannot detect road and building well. Hence, this work’s principal goal is to introduce two novel deep convolutional models based on UNet family for multi-object segmentation, such as roads and buildings from aerial imagery. We focused on buildings and road networks because these objects constitute a huge part of the urban areas. The presented models are called multi-level context gating UNet (MCG-UNet) and bi-directional ConvLSTM UNet model (BCL-UNet). The proposed methods have the same advantages as the UNet model, the mechanism of densely connected convolutions, bi-directional ConvLSTM, and squeeze and excitation module to produce the segmentation maps with a high resolution and maintain the boundary information even under complicated backgrounds. Additionally, we implemented a basic efficient loss function called boundary-aware loss (BAL) that allowed a network to concentrate on hard semantic segmentation regions, such as overlapping areas, small objects, sophisticated objects, and boundaries of objects, and produce high-quality segmentation maps. The presented networks were tested on the Massachusetts building and road datasets. The MCG-UNet improved the average F1 accuracy by 1.85%, and 1.19% and 6.67% and 5.11% compared with UNet and BCL-UNet for road and building extraction, respectively. Additionally, the presented MCG-UNet and BCL-UNet networks were compared with other state-of-the-art deep learning-based networks, and the results proved the superiority of the networks in multi-object segmentation tasks.},
DOI = {10.3390/rs13183710}
}



@Article{ai2030028,
AUTHOR = {Weber, Daniel and Gühmann, Clemens and Seel, Thomas},
TITLE = {RIANN—A Robust Neural Network Outperforms Attitude Estimation Filters},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {444--463},
URL = {https://www.mdpi.com/2673-2688/2/3/28},
ISSN = {2673-2688},
ABSTRACT = {Inertial-sensor-based attitude estimation is a crucial technology in various applications, from human motion tracking to autonomous aerial and ground vehicles. Application scenarios differ in characteristics of the performed motion, presence of disturbances, and environmental conditions. Since state-of-the-art attitude estimators do not generalize well over these characteristics, their parameters must be tuned for the individual motion characteristics and circumstances. We propose RIANN, a ready-to-use, neural network-based, parameter-free, real-time-capable inertial attitude estimator, which generalizes well across different motion dynamics, environments, and sampling rates, without the need for application-specific adaptations. We gather six publicly available datasets of which we exploit two datasets for the method development and the training, and we use four datasets for evaluation of the trained estimator in three different test scenarios with varying practical relevance. Results show that RIANN outperforms state-of-the-art attitude estimation filters in the sense that it generalizes much better across a variety of motions and conditions in different applications, with different sensor hardware and different sampling frequencies. This is true even if the filters are tuned on each individual test dataset, whereas RIANN was trained on completely separate data and has never seen any of these test datasets. RIANN can be applied directly without adaptations or training and is therefore expected to enable plug-and-play solutions in numerous applications, especially when accuracy is crucial but no ground-truth data is available for tuning or when motion and disturbance characteristics are uncertain. We made RIANN publicly available.},
DOI = {10.3390/ai2030028}
}



@Article{drones5030099,
AUTHOR = {Richardson, Galen and Leblanc, Sylvain G. and Lovitt, Julie and Rajaratnam, Krishan and Chen, Wenjun},
TITLE = {Leveraging AI to Estimate Caribou Lichen in UAV Orthomosaics from Ground Photo Datasets},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {99},
URL = {https://www.mdpi.com/2504-446X/5/3/99},
ISSN = {2504-446X},
ABSTRACT = {Relating ground photographs to UAV orthomosaics is a key linkage required for accurate multi-scaled lichen mapping. Conventional methods of multi-scaled lichen mapping, such as random forest models and convolutional neural networks, heavily rely on pixel DN values for classification. However, the limited spectral range of ground photos requires additional characteristics to differentiate lichen from spectrally similar objects, such as bright logs. By applying a neural network to tiles of a UAV orthomosaics, additional characteristics, such as surface texture and spatial patterns, can be used for inferences. Our methodology used a neural network (UAV LiCNN) trained on ground photo mosaics to predict lichen in UAV orthomosaic tiles. The UAV LiCNN achieved mean user and producer accuracies of 85.84% and 92.93%, respectively, in the high lichen class across eight different orthomosaics. We compared the known lichen percentages found in 77 vegetation microplots with the predicted lichen percentage calculated from the UAV LiCNN, resulting in a R2 relationship of 0.6910. This research shows that AI models trained on ground photographs effectively classify lichen in UAV orthomosaics. Limiting factors include the misclassification of spectrally similar objects to lichen in the RGB bands and dark shadows cast by vegetation.},
DOI = {10.3390/drones5030099}
}



@Article{app11188694,
AUTHOR = {Memon, Mehak Maqbool and Hashmani, Manzoor Ahmed and Junejo, Aisha Zahid and Rizvi, Syed Sajjad and Arain, Adnan Ashraf},
TITLE = {A Novel Luminance-Based Algorithm for Classification of Semi-Dark Images},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8694},
URL = {https://www.mdpi.com/2076-3417/11/18/8694},
ISSN = {2076-3417},
ABSTRACT = {Image classification of a visual scene based on visibility is significant due to the rise in readily available automated solutions. Currently, there are only two known spectrums of image visibility i.e., dark, and bright. However, normal environments include semi-dark scenarios. Hence, visual extremes that will lead to the accurate extraction of image features should be duly discarded. Fundamentally speaking there are two broad methods to perform visual scene-based image classification, i.e., machine learning (ML) methods and computer vision methods. In ML, the issues of insufficient data, sophisticated hardware and inadequate image classifier training time remain significant problems to be handled. These techniques fail to classify the visual scene-based images with high accuracy. The other alternative is computer vision (CV) methods, which also have major issues. CV methods do provide some basic procedures which may assist in such classification but, to the best of our knowledge, no CV algorithm exists to perform such classification, i.e., these do not account for semi-dark images in the first place. Moreover, these methods do not provide a well-defined protocol to calculate images’ content visibility and thereby classify images. One of the key algorithms for calculation of images’ content visibility is backed by the HSL (hue, saturation, lightness) color model. The HSL color model allows the visibility calculation of a scene by calculating the lightness/luminance of a single pixel. Recognizing the high potential of the HSL color model, we propose a novel framework relying on the simple approach of the statistical manipulation of an entire image’s pixel intensities, represented by HSL color model. The proposed algorithm, namely, Relative Perceived Luminance Classification (RPLC) uses the HSL (hue, saturation, lightness) color model to correctly identify the luminosity values of the entire image. Our findings prove that the proposed method yields high classification accuracy (over 78%) with a small error rate. We show that the computational complexity of RPLC is much less than that of the state-of-the-art ML algorithms.},
DOI = {10.3390/app11188694}
}



@Article{app11188698,
AUTHOR = {Cao, Minghe and Wang, Jianzhong and Ming, Li},
TITLE = {Multi-Templates Based Robust Tracking for Robot Person-Following Tasks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8698},
URL = {https://www.mdpi.com/2076-3417/11/18/8698},
ISSN = {2076-3417},
ABSTRACT = {While the robotics techniques have not developed to full automation, robot following is common and crucial in robotic applications to reduce the need for dedicated teleoperation. To achieve this task, the target must first be robustly and consistently perceived. In this paper, a robust visual tracking approach is proposed. The approach adopts a scene analysis module (SAM) to identify the real target and similar distractors, leveraging statistical characteristics of cross-correlation responses. Positive templates are collected based on the tracking confidence constructed by the SAM, and negative templates are gathered by the recognized distractors. Based on the collected templates, response fusion is performed. As a result, the responses of the target are enhanced and the false responses are suppressed, leading to robust tracking results. The proposed approach is validated on an outdoor robot-person following dataset and a collection of public person tracking datasets. The results show that our approach achieved state-of-the-art tracking performance in terms of both the robustness and AUC score.},
DOI = {10.3390/app11188698}
}



@Article{agriculture11090897,
AUTHOR = {Sirikun, Chaiyan and Samseemoung, Grianggai and Soni, Peeyush and Langkapin, Jaturong and Srinonchat, Jakkree},
TITLE = {A Grain Yield Sensor for Yield Mapping with Local Rice Combine Harvester},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {897},
URL = {https://www.mdpi.com/2077-0472/11/9/897},
ISSN = {2077-0472},
ABSTRACT = {Rice grain yield was estimated from a locally made Thai combine harvester using a specially developed sensing and monitoring system. The yield monitoring and sensing system, mounted on the rice combine harvester, collected and logged grain mass flow rate and moisture content, as well as pertinent information related to field, position and navigation. The developed system comprised a yield meter, GNSS receiver and a computer installed with customized software, which, when assembled on a local rice combine, mapped real-time rice yield along with grain moisture content. The performance of the developed system was evaluated at three neighboring (identically managed) rice fields. ArcGIS® software was used to create grain yield map with geographical information of the fields. The average grain yield values recorded were 3.63, 3.84 and 3.60 t ha−1, and grain moisture contents (w.b.) were 22.42%, 23.50% and 24.71% from the three fields, respectively. Overall average grain yield was 3.84 t ha−1 (CV = 63.68%) with 578.10 and 7761.58 kg ha−1 as the minimum and maximum values, respectively. The coefficients of variation in grain yield of the three fields were 57.44%, 63.68% and 60.41%, respectively. The system performance was evaluated at four different cutter bar heights (0.18, 0.25, 0.35 and 0.40 m) during the test. As expected, the tallest cutter bar height (0.40 m) offered the least error of 12.50% in yield estimation. The results confirmed that the developed grain yield sensor could be successfully used with the local rice combine harvester; hence, offers and ‘up-gradation’ potential in Thai agricultural mechanization.},
DOI = {10.3390/agriculture11090897}
}



@Article{smartcities4030065,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Qayyum, Siddra and Heravi, Amirhossein},
TITLE = {Application of Deep Learning on UAV-Based Aerial Images for Flood Detection},
JOURNAL = {Smart Cities},
VOLUME = {4},
YEAR = {2021},
NUMBER = {3},
PAGES = {1220--1242},
URL = {https://www.mdpi.com/2624-6511/4/3/65},
ISSN = {2624-6511},
ABSTRACT = {Floods are one of the most fatal and devastating disasters, instigating an immense loss of human lives and damage to property, infrastructure, and agricultural lands. To cater to this, there is a need to develop and implement real-time flood management systems that could instantly detect flooded regions to initiate relief activities as early as possible. Current imaging systems, relying on satellites, have demonstrated low accuracy and delayed response, making them unreliable and impractical to be used in emergency responses to natural disasters such as flooding. This research employs Unmanned Aerial Vehicles (UAVs) to develop an automated imaging system that can identify inundated areas from aerial images. The Haar cascade classifier was explored in the case study to detect landmarks such as roads and buildings from the aerial images captured by UAVs and identify flooded areas. The extracted landmarks are added to the training dataset that is used to train a deep learning algorithm. Experimental results show that buildings and roads can be detected from the images with 91% and 94% accuracy, respectively. The overall accuracy of 91% is recorded in classifying flooded and non-flooded regions from the input case study images. The system has shown promising results on test images belonging to both pre- and post-flood classes. The flood relief and rescue workers can quickly locate flooded regions and rescue stranded people using this system. Such real-time flood inundation systems will help transform the disaster management systems in line with modern smart cities initiatives.},
DOI = {10.3390/smartcities4030065}
}



@Article{rs13183750,
AUTHOR = {Shao, Ruizhe and Du, Chun and Chen, Hao and Li, Jun},
TITLE = {SUNet: Change Detection for Heterogeneous Remote Sensing Images from Satellite and UAV Using a Dual-Channel Fully Convolution Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3750},
URL = {https://www.mdpi.com/2072-4292/13/18/3750},
ISSN = {2072-4292},
ABSTRACT = {Change Detection in heterogeneous remote sensing images plays an increasingly essential role in many real-world applications, e.g., urban growth tracking, land use monitoring, disaster evaluation and damage assessment. The objective of change detection is to identify changes of geo-graphical entities or phenomena through two or more bitemporal images. Researchers have invested a lot in the homologous change detection and yielded fruitful results. However, change detection between heterogenous remote sensing images is still a great challenge, especially for change detection of heterogenous remote sensing images obtained from satellites and Unmanned Aerial Vehicles (UAV). The main challenges in satellite-UAV change detection tasks lie in the intensive difference of color for the same ground objects, various resolutions, the parallax effect and image distortion caused by different shooting angles and platform altitudes. To address these issues, we propose a novel method based on dual-channel fully convolution network. First, in order to alleviate the influence of differences between heterogeneous images, we employ two different channels to map heterogeneous remote sensing images from satellite and UAV, respectively, to a mutual high dimension latent space for the downstream change detection task. Second, we adopt Hough method to extract the edge of ground objects as auxiliary information to help the change detection model to pay more attention to shapes and contours, instead of colors. Then, IoU-WCE loss is designed to deal with the problem of imbalanced samples in change detection task. Finally, we conduct extensive experiments to verify the proposed method using a new Satellite-UAV heterogeneous image data set, named HTCD, which is annotated by us and has been open to public. The experimental results show that our method significantly outperforms the state-of-the-art change detection methods.},
DOI = {10.3390/rs13183750}
}



@Article{en14185967,
AUTHOR = {Benbouzid, Mohamed and Berghout, Tarek and Sarma, Nur and Djurović, Siniša and Wu, Yueqi and Ma, Xiandong},
TITLE = {Intelligent Condition Monitoring of Wind Power Systems: State of the Art Review},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {5967},
URL = {https://www.mdpi.com/1996-1073/14/18/5967},
ISSN = {1996-1073},
ABSTRACT = {Modern wind turbines operate in continuously transient conditions, with varying speed, torque, and power based on the stochastic nature of the wind resource. This variability affects not only the operational performance of the wind power system, but can also affect its integrity under service conditions. Condition monitoring continues to play an important role in achieving reliable and economic operation of wind turbines. This paper reviews the current advances in wind turbine condition monitoring, ranging from conventional condition monitoring and signal processing tools to machine-learning-based condition monitoring and usage of big data mining for predictive maintenance. A systematic review is presented of signal-based and data-driven modeling methodologies using intelligent and machine learning approaches, with the view to providing a critical evaluation of the recent developments in this area, and their applications in diagnosis, prognosis, health assessment, and predictive maintenance of wind turbines and farms.},
DOI = {10.3390/en14185967}
}



@Article{land10100995,
AUTHOR = {Emeka, Okoli Jude and Nahazanan, Haslinda and Kalantar, Bahareh and Khuzaimah, Zailani and Sani, Ojogbane Success},
TITLE = {Evaluation of the Effect of Hydroseeded Vegetation for Slope Reinforcement},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {995},
URL = {https://www.mdpi.com/2073-445X/10/10/995},
ISSN = {2073-445X},
ABSTRACT = {A landslide is a significant environmental hazard that results in an enormous loss of lives and properties. Studies have revealed that rainfall, soil characteristics, and human errors, such as deforestation, are the leading causes of landslides, reducing soil water infiltration and increasing the water runoff of a slope. This paper introduces vegetation establishment as a low-cost, practical measure for slope reinforcement through the ground cover and the root of the vegetation. This study reveals the level of complexity of the terrain with regards to the evaluation of high and low stability areas and has produced a landslide susceptibility map. For this purpose, 12 conditioning factors, namely slope, aspect, elevation, curvature, hill shade, stream power index (SPI), topographic wetness index (TWI), terrain roughness index (TRI), distances to roads, distance to lakes, distance to trees, and build-up, were used through the analytic hierarchy process (AHP) model to produce landslide susceptibility map. Receiver operating characteristics (ROC) was used for validation of the results. The area under the curve (AUC) values obtained from the ROC method for the AHP model was 0.865. Four seed samples, namely ryegrass, rye corn, signal grass, and couch, were hydroseeded to determine the vegetation root and ground cover’s effectiveness on stabilization and reinforcement on a high-risk susceptible 65° slope between August and December 2020. The observed monthly vegetation root of couch grass gave the most acceptable result. With a spreading and creeping vegetation ground cover characteristic, ryegrass showed the most acceptable monthly result for vegetation ground cover effectiveness. The findings suggest that the selection of couch species over other species is justified based on landslide control benefits.},
DOI = {10.3390/land10100995}
}



@Article{plants10101977,
AUTHOR = {Yee-Rendon, Arturo and Torres-Pacheco, Irineo and Trujillo-Lopez, Angelica Sarahy and Romero-Bringas, Karen Paola and Millan-Almaraz, Jesus Roberto},
TITLE = {Analysis of New RGB Vegetation Indices for PHYVV and TMV Identification in Jalapeño Pepper (Capsicum annuum) Leaves Using CNNs-Based Model},
JOURNAL = {Plants},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1977},
URL = {https://www.mdpi.com/2223-7747/10/10/1977},
ISSN = {2223-7747},
ABSTRACT = {Recently, deep-learning techniques have become the foundations for many breakthroughs in the automated identification of plant diseases. In the agricultural sector, many recent visual-computer approaches use deep-learning models. In this approach, a novel predictive analytics methodology to identify Tobacco Mosaic Virus (TMV) and Pepper Huasteco Yellow Vein Virus (PHYVV) visual symptoms on Jalapeño pepper (Capsicum annuum L.) leaves by using image-processing and deep-learning classification models is presented. The proposed image-processing approach is based on the utilization of Normalized Red-Blue Vegetation Index (NRBVI) and Normalized Green-Blue Vegetation Index (NGBVI) as new RGB-based vegetation indices, and its subsequent Jet pallet colored version NRBVI-Jet NGBVI-Jet as pre-processing algorithms. Furthermore, four standard pre-trained deep-learning architectures, Visual Geometry Group-16 (VGG-16), Xception, Inception v3, and MobileNet v2, were implemented for classification purposes. The objective of this methodology was to find the most accurate combination of vegetation index pre-processing algorithms and pre-trained deep- learning classification models. Transfer learning was applied to fine tune the pre-trained deep- learning models and data augmentation was also applied to prevent the models from overfitting. The performance of the models was evaluated using Top-1 accuracy, precision, recall, and F1-score using test data. The results showed that the best model was an Xception-based model that uses the NGBVI dataset. This model reached an average Top-1 test accuracy of 98.3%. A complete analysis of the different vegetation index representations using models based on deep-learning architectures is presented along with the study of the learning curves of these deep-learning models during the training phase.},
DOI = {10.3390/plants10101977}
}



@Article{plants10101989,
AUTHOR = {Kaur, Balwinder and Sandhu, Karansher S. and Kamal, Roop and Kaur, Kawalpreet and Singh, Jagmohan and Röder, Marion S. and Muqaddasi, Quddoos H.},
TITLE = {Omics for the Improvement of Abiotic, Biotic, and Agronomic Traits in Major Cereal Crops: Applications, Challenges, and Prospects},
JOURNAL = {Plants},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1989},
URL = {https://www.mdpi.com/2223-7747/10/10/1989},
ISSN = {2223-7747},
ABSTRACT = {Omics technologies, namely genomics, transcriptomics, proteomics, metabolomics, and phenomics, are becoming an integral part of virtually every commercial cereal crop breeding program, as they provide substantial dividends per unit time in both pre-breeding and breeding phases. Continuous advances in omics assure time efficiency and cost benefits to improve cereal crops. This review provides a comprehensive overview of the established omics methods in five major cereals, namely rice, sorghum, maize, barley, and bread wheat. We cover the evolution of technologies in each omics section independently and concentrate on their use to improve economically important agronomic as well as biotic and abiotic stress-related traits. Advancements in the (1) identification, mapping, and sequencing of molecular/structural variants; (2) high-density transcriptomics data to study gene expression patterns; (3) global and targeted proteome profiling to study protein structure and interaction; (4) metabolomic profiling to quantify organ-level, small-density metabolites, and their composition; and (5) high-resolution, high-throughput, image-based phenomics approaches are surveyed in this review.},
DOI = {10.3390/plants10101989}
}



@Article{rs13193826,
AUTHOR = {Wen, Xiang and Li, Xing and Zhang, Ce and Han, Wenquan and Li, Erzhu and Liu, Wei and Zhang, Lianpeng},
TITLE = {ME-Net: A Multi-Scale Erosion Network for Crisp Building Edge Detection from Very High Resolution Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3826},
URL = {https://www.mdpi.com/2072-4292/13/19/3826},
ISSN = {2072-4292},
ABSTRACT = {The detection of building edges from very high resolution (VHR) remote sensing imagery is essential to various geo-related applications, including surveying and mapping, urban management, etc. Recently, the rapid development of deep convolutional neural networks (DCNNs) has achieved remarkable progress in edge detection; however, there has always been the problem of edge thickness due to the large receptive field of DCNNs. In this paper, we proposed a multi-scale erosion network (ME-Net) for building edge detection to crisp the building edge through two innovative approaches: (1) embedding an erosion module (EM) in the network to crisp the edge and (2) adding the Dice coefficient and local cross entropy of edge neighbors into the loss function to increase its sensitivity to the receptive field. In addition, a new metric, Ene, to measure the crispness of the predicted building edge was proposed. The experiment results show that ME-Net not only detects the clearest and crispest building edges, but also achieves the best OA of 98.75%, 95.00% and 95.51% on three building edge datasets, and exceeds other edge detection networks 3.17% and 0.44% at least in strict F1-score and Ene. In a word, the proposed ME-Net is an effective and practical approach for detecting crisp building edges from VHR remote sensing imagery.},
DOI = {10.3390/rs13193826}
}



@Article{electronics10192345,
AUTHOR = {Pérez-Adán, Darian and Fresnedo, Óscar and González-Coma, José P. and Castedo, Luis},
TITLE = {Intelligent Reflective Surfaces for Wireless Networks: An Overview of Applications, Approached Issues, and Open Problems},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {2345},
URL = {https://www.mdpi.com/2079-9292/10/19/2345},
ISSN = {2079-9292},
ABSTRACT = {An intelligent reflective surface (IRS) is a novel and revolutionizing communication technology destined to enable the control of the radio environment. An IRS is a real-time controllable reflectarray with a massive number of low-cost passive elements which introduce a phase shift to the incoming signals from the sources before the propagation towards the destination. This technology introduces the notion of a smart propagation environment with the aim of improving the system performance. In this paper, we provide a comprehensive literature overview on IRS technology, including its basic concepts and reconfiguration, as well as its design aspects and applications for wireless communication systems. We also study the performance metrics and the setups considered in recent publications related to IRS and provide suggestions of future research lines based on still unexplored use cases in the state-of-the-art.},
DOI = {10.3390/electronics10192345}
}



@Article{app11198936,
AUTHOR = {Kovačič, Boštjan and Štraus, Luka and Držečnik, Mateja and Pučko, Zoran},
TITLE = {Applicability and Analysis of the Results of Non-Contact Methods in Determining the Vertical Displacements of Timber Beams},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {8936},
URL = {https://www.mdpi.com/2076-3417/11/19/8936},
ISSN = {2076-3417},
ABSTRACT = {Determining the displacements and consequent deformations of structures is a demanding branch of engineering. Displacements are most often determined by geodetic methods, among which high-precision non-contact methods have recently taken the lead. Engineering geodesy is an indispensable part of construction projects. In the desire for efficient and fast measurements, the technology of terrestrial laser scanning (TLS) and the use of robotic total station (RTS) and other geodetic methods are becoming more and more useful for engineers. In the presented study, we focused on the measurement and comparison of vertical displacements with various mentioned equipment and the determination of the influence of meteorological conditions on the displacements of timber beams that we used to perform the experiment. Measurements were performed both in the laboratory and outdoors. A novelty in the work was the use of a TLS scanner to determine the evaluation of small value displacements and the analysis of the usability of geodetic measuring equipment. In the Materials and Methods section, we describe the equipment used and the characteristics of the beams. The Results section describes the experimental outcomes, which include the performance of experimental analysis of vertical displacements of timber beams under different meteorological conditions. Altogether, the results consist of geodetic measurements and the processing of measured data. The results of measurements of vertical displacements with a terrestrial laser scanner were compared with the results obtained with a robotic total station were evaluated and compared with the displacements calculated from static analysis and the results of other methods used.},
DOI = {10.3390/app11198936}
}



@Article{rs13193837,
AUTHOR = {Pu, Yihan and Xu, Dandan and Wang, Haobin and An, Deshuai and Xu, Xia},
TITLE = {Extracting Canopy Closure by the CHM-Based and SHP-Based Methods with a Hemispherical FOV from UAV-LiDAR Data in a Poplar Plantation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3837},
URL = {https://www.mdpi.com/2072-4292/13/19/3837},
ISSN = {2072-4292},
ABSTRACT = {Canopy closure (CC), a useful biophysical parameter for forest structure, is an important indicator of forest resource and biodiversity. Light Detection and Ranging (LiDAR) data has been widely studied recently for forest ecosystems to obtain the three-dimensional (3D) structure of the forests. The components of the Unmanned Aerial Vehicle LiDAR (UAV-LiDAR) are similar to those of the airborne LiDAR, but with higher pulse density, which reveals more detailed vertical structures. Hemispherical photography (HP) had proven to be an effective method for estimating CC, but it was still time-consuming and limited in large forests. Thus, we used UAV-LiDAR data with a canopy-height-model-based (CHM-based) method and a synthetic-hemispherical-photography-based (SHP-based) method to extract CC from a pure poplar plantation in this study. The performance of the CC extraction methods based on an angular viewpoint was validated by the results of HP. The results showed that the CHM-based method had a high accuracy in a 45° zenith angle range with a 0.5 m pixel size and a larger radius (i.e., k = 2; R2 = 0.751, RMSE = 0.053), and the accuracy declined rapidly in zenith angles of 60° and 75° (R2 = 0.707, 0.490; RMSE = 0.053, 0.066). In addition, the CHM-based method showed an underestimate for leaf-off deciduous trees with low CC. The SHP-based method also had a high accuracy in a 45° zenith angle range, and its accuracy was stable in three zenith angle ranges (R2: 0.688, 0.674, 0.601 and RMSE = 0.059, 0.056, 0.058 for a 45°, 60° and 75° zenith angle range, respectively). There was a similar trend of CC change in HP and SHP results with the zenith angle range increase, but there was no significant change with the zenith angle range increase in the CHM-based method, which revealed that it was insensitive to the changes of angular CC compared to the SHP-based method. However, the accuracy of both methods showed differences in plantations with different ages, which had a slight underestimate for 8-year-old plantations and an overestimate for plantations with 17 and 20 years. Our research provided a reference for CC estimation from a point-based angular viewpoint and for monitoring the understory light conditions of plantations.},
DOI = {10.3390/rs13193837}
}



@Article{rs13193841,
AUTHOR = {Neupane, Krishna and Baysal-Gurel, Fulya},
TITLE = {Automatic Identification and Monitoring of Plant Diseases Using Unmanned Aerial Vehicles: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3841},
URL = {https://www.mdpi.com/2072-4292/13/19/3841},
ISSN = {2072-4292},
ABSTRACT = {Disease diagnosis is one of the major tasks for increasing food production in agriculture. Although precision agriculture (PA) takes less time and provides a more precise application of agricultural activities, the detection of disease using an Unmanned Aerial System (UAS) is a challenging task. Several Unmanned Aerial Vehicles (UAVs) and sensors have been used for this purpose. The UAVs’ platforms and their peripherals have their own limitations in accurately diagnosing plant diseases. Several types of image processing software are available for vignetting and orthorectification. The training and validation of datasets are important characteristics of data analysis. Currently, different algorithms and architectures of machine learning models are used to classify and detect plant diseases. These models help in image segmentation and feature extractions to interpret results. Researchers also use the values of vegetative indices, such as Normalized Difference Vegetative Index (NDVI), Crop Water Stress Index (CWSI), etc., acquired from different multispectral and hyperspectral sensors to fit into the statistical models to deliver results. There are still various drifts in the automatic detection of plant diseases as imaging sensors are limited by their own spectral bandwidth, resolution, background noise of the image, etc. The future of crop health monitoring using UAVs should include a gimble consisting of multiple sensors, large datasets for training and validation, the development of site-specific irradiance systems, and so on. This review briefly highlights the advantages of automatic detection of plant diseases to the growers.},
DOI = {10.3390/rs13193841}
}



@Article{robotics10040110,
AUTHOR = {Dworakowski, Daniel and Thompson, Christopher and Pham-Hung, Michael and Nejat, Goldie},
TITLE = {A Robot Architecture Using ContextSLAM to Find Products in Unknown Crowded Retail Environments},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {110},
URL = {https://www.mdpi.com/2218-6581/10/4/110},
ISSN = {2218-6581},
ABSTRACT = {Grocery shoppers must negotiate cluttered, crowded, and complex store layouts containing a vast variety of products to make their intended purchases. This complexity may prevent even experienced shoppers from finding their grocery items, consuming a lot of their time and resulting in monetary loss for the store. To address these issues, we present a generic grocery robot architecture for the autonomous search and localization of products in crowded dynamic unknown grocery store environments using a unique context Simultaneous Localization and Mapping (contextSLAM) method. The contextSLAM method uniquely creates contextually rich maps through the online fusion of optical character recognition and occupancy grid information to locate products and aid in robot localization in an environment. The novelty of our robot architecture is in its ability to intelligently use geometric and contextual information within the context map to direct robot exploration in order to localize products in unknown environments in the presence of dynamic people. Extensive experiments were conducted with a mobile robot to validate the overall architecture and contextSLAM, including in a real grocery store. The results of the experiments showed that our architecture was capable of searching for and localizing all products in various grocery lists in different unknown environments.},
DOI = {10.3390/robotics10040110}
}



@Article{rs13193853,
AUTHOR = {Wu, Yiguang and Wang, Meizhen and Liu, Xuejun and Wang, Ziran and Ma, Tianwu and Lu, Zhimin and Liu, Dan and Xie, Yujia and Li, Xiuquan and Wang, Xing},
TITLE = {Monitoring the Work Cycles of Earthmoving Excavators in Earthmoving Projects Using UAV Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3853},
URL = {https://www.mdpi.com/2072-4292/13/19/3853},
ISSN = {2072-4292},
ABSTRACT = {Monitoring the work cycles of earthmoving excavators is an important aspect of construction productivity assessment. Currently, the most advanced method for the recognition of work cycles is the “Stretching-Bending” Sequential Pattern (SBSP), which is based on fixed-carrier video monitoring (FC-SBSP). However, the application of this method presupposes the availability of preconstructed installation carriers to act as a surveillance camera as well as installed and commissioned surveillance systems that work in tandem with them. Obviously, this method is difficult to apply to projects with no conditions for a monitoring camera installation or which have a short construction time. This highlights the potential application of Unmanned Aerial Vehicle (UAV) remote sensing, which is flexible and mobile. Unfortunately, few studies have been conducted on the application of UAV remote sensing for the work cycle monitoring of earthmoving excavators. This research is necessary because the use of UAV remote sensing for monitoring the work cycles of earthmoving excavators can improve construction productivity and save time and costs, especially in post-disaster reconstruction projects involving harsh construction environments, and emergency projects with short construction periods. In addition, the challenges posed by UAV shaking may have to be taken into account when using the SBSP for UAV remote sensing. To this end, this study used application experiments in which stabilization processing of UAV video data was performed for UAV shaking. The application experimental results show that the work cycle performance of UAV remote-sensing-based SBSP (UAV-SBSP) for UAV video data was 2.45% and 5.36% lower in terms of precision and recall, respectively, without stabilization processing than after stabilization processing. Comparative experiments were also designed to investigate the applicability of the SBSP oriented toward UAV remote sensing. Comparative experimental results show that the same level of performance was obtained for the recognition of work cycles with the UAV-SBSP as compared with the FC-SBSP, demonstrating the good applicability of this method. Therefore, the results of this study show that UAV remote sensing enables effective monitoring of earthmoving excavator work cycles in construction sites where monitoring cameras are not available for installation, and it can be used as an alternative technology to fixed-carrier video monitoring for onsite proximity monitoring.},
DOI = {10.3390/rs13193853}
}



@Article{rs13193856,
AUTHOR = {Chen, Xiaolong and Guan, Jian and Mu, Xiaoqian and Wang, Zhigao and Liu, Ningbo and Wang, Guoqing},
TITLE = {Multi-Dimensional Automatic Detection of Scanning Radar Images of Marine Targets Based on Radar PPInet},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3856},
URL = {https://www.mdpi.com/2072-4292/13/19/3856},
ISSN = {2072-4292},
ABSTRACT = {Traditional radar target detection algorithms are mostly based on statistical theory. They have weak generalization capabilities for complex sea clutter environments and diverse target characteristics, and their detection performance would be significantly reduced. In this paper, the range-azimuth-frame information obtained by scanning radar is converted into plain position indicator (PPI) images, and a novel Radar-PPInet is proposed and used for marine target detection. The model includes CSPDarknet53, SPP, PANet, power non-maximum suppression (P-NMS), and multi-frame fusion section. The prediction frame coordinates, target category, and corresponding confidence are directly given through the feature extraction network. The network structure strengthens the receptive field and attention distribution structure, and further improves the efficiency of network training. P-NMS can effectively improve the problem of missed detection of multi-targets. Moreover, the false alarms caused by strong sea clutter are reduced by the multi-frame fusion, which is also a benefit for weak target detection. The verification using the X-band navigation radar PPI image dataset shows that compared with the traditional cell-average constant false alarm rate detector (CA-CFAR) and the two-stage Faster R-CNN algorithm, the proposed method significantly improved the detection probability by 15% and 10% under certain false alarm probability conditions, which is more suitable for various environment and target characteristics. Moreover, the computational burden is discussed showing that the Radar-PPInet detection model is significantly lower than the Faster R-CNN in terms of parameters and calculations.},
DOI = {10.3390/rs13193856}
}



@Article{rs13193859,
AUTHOR = {Czarnecki, Joby M. Prince and Samiappan, Sathishkumar and Zhou, Meilun and McCraine, Cary Daniel and Wasson, Louis L.},
TITLE = {Real-Time Automated Classification of Sky Conditions Using Deep Learning and Edge Computing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3859},
URL = {https://www.mdpi.com/2072-4292/13/19/3859},
ISSN = {2072-4292},
ABSTRACT = {The radiometric quality of remotely sensed imagery is crucial for precision agriculture applications because estimations of plant health rely on the underlying quality. Sky conditions, and specifically shadowing from clouds, are critical determinants in the quality of images that can be obtained from low-altitude sensing platforms. In this work, we first compare common deep learning approaches to classify sky conditions with regard to cloud shadows in agricultural fields using a visible spectrum camera. We then develop an artificial-intelligence-based edge computing system to fully automate the classification process. Training data consisting of 100 oblique angle images of the sky were provided to a convolutional neural network and two deep residual neural networks (ResNet18 and ResNet34) to facilitate learning two classes, namely (1) good image quality expected, and (2) degraded image quality expected. The expectation of quality stemmed from the sky condition (i.e., density, coverage, and thickness of clouds) present at the time of the image capture. These networks were tested using a set of 13,000 images. Our results demonstrated that ResNet18 and ResNet34 classifiers produced better classification accuracy when compared to a convolutional neural network classifier. The best overall accuracy was obtained by ResNet34, which was 92% accurate, with a Kappa statistic of 0.77. These results demonstrate a low-cost solution to quality control for future autonomous farming systems that will operate without human intervention and supervision.},
DOI = {10.3390/rs13193859}
}



@Article{app11198996,
AUTHOR = {Cao, Yuwei and Scaioni, Marco},
TITLE = {3DLEB-Net: Label-Efficient Deep Learning-Based Semantic Segmentation of Building Point Clouds at LoD3 Level},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {8996},
URL = {https://www.mdpi.com/2076-3417/11/19/8996},
ISSN = {2076-3417},
ABSTRACT = {In current research, fully supervised Deep Learning (DL) techniques are employed to train a segmentation network to be applied to point clouds of buildings. However, training such networks requires large amounts of fine-labeled buildings’ point-cloud data, presenting a major challenge in practice because they are difficult to obtain. Consequently, the application of fully supervised DL for semantic segmentation of buildings’ point clouds at LoD3 level is severely limited. In order to reduce the number of required annotated labels, we proposed a novel label-efficient DL network that obtains per-point semantic labels of LoD3 buildings’ point clouds with limited supervision, named 3DLEB-Net. In general, it consists of two steps. The first step (Autoencoder, AE) is composed of a Dynamic Graph Convolutional Neural Network (DGCNN) encoder and a folding-based decoder. It is designed to extract discriminative global and local features from input point clouds by faithfully reconstructing them without any label. The second step is the semantic segmentation network. By supplying a small amount of task-specific supervision, a segmentation network is proposed for semantically segmenting the encoded features acquired from the pre-trained AE. Experimentally, we evaluated our approach based on the Architectural Cultural Heritage (ArCH) dataset. Compared to the fully supervised DL methods, we found that our model achieved state-of-the-art results on the unseen scenes, with only 10% of labeled training data from fully supervised methods as input. Moreover, we conducted a series of ablation studies to show the effectiveness of the design choices of our model.},
DOI = {10.3390/app11198996}
}



@Article{met11101537,
AUTHOR = {Sharma, Vinamra Bhushan and Tewari, Saurabh and Biswas, Susham and Lohani, Bharat and Dwivedi, Umakant Dhar and Dwivedi, Deepak and Sharma, Ashutosh and Jung, Jae Pil},
TITLE = {Recent Advancements in AI-Enabled Smart Electronics Packaging for Structural Health Monitoring},
JOURNAL = {Metals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1537},
URL = {https://www.mdpi.com/2075-4701/11/10/1537},
ISSN = {2075-4701},
ABSTRACT = {Real-time health monitoring of civil infrastructures is performed to maintain their structural integrity, sustainability, and serviceability for a longer time. With smart electronics and packaging technology, large amounts of complex monitoring data are generated, requiring sophisticated artificial intelligence (AI) techniques for their processing. With the advancement of technology, more complex AI models have been applied, from simple models to sophisticated deep learning (DL) models, for structural health monitoring (SHM). In this article, a comprehensive review is performed, primarily on the applications of AI models for SHM to maintain the sustainability of diverse civil infrastructures. Three smart data capturing methods of SHM, namely, camera-based, smartphone-based, and unmanned aerial vehicle (UAV)-based methods, are also discussed, having made the utilization of intelligent paradigms easier. UAV is found to be the most promising smart data acquisition technology, whereas convolution neural networks are the most impressive DL model reported for SHM. Furthermore, current challenges and future perspectives of AI-based SHM systems are also described separately. Moreover, the Internet of Things (IoT) and smart city concepts are explained to elaborate on the contributions of intelligent SHM systems. The integration of SHM with IoT and cloud-based computing is leading us towards the evolution of future smart cities.},
DOI = {10.3390/met11101537}
}



@Article{rs13193874,
AUTHOR = {Ma, Xu and Lu, Lei and Ding, Jianli and Zhang, Fei and He, Baozhong},
TITLE = {Estimating Fractional Vegetation Cover of Row Crops from High Spatial Resolution Image},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3874},
URL = {https://www.mdpi.com/2072-4292/13/19/3874},
ISSN = {2072-4292},
ABSTRACT = {With high spatial resolution remote sensing images being increasingly used in precision agriculture, more details of the row structure of row crops are captured in the corresponding images. This phenomenon is a challenge for the estimation of the fractional vegetation cover (FVC) of row crops. Previous studies have found that there is an overestimation of FVC for the early growth stage of vegetation in the current algorithms. When the row crops are a form in the early stage of vegetation, their FVC may also have overestimation. Therefore, developing an algorithm to address this problem is necessary. This study used World-View 3 images as data sources and attempted to use the canopy reflectance model of row crops, coupling backward propagation neural networks (BPNNs) to estimate the FVC of row crops. Compared to the prevailing algorithms, i.e., empirical method, spectral mixture analysis, and continuous crop model coupling BPNNs, the results showed that the calculated accuracy of the canopy reflectance model of row crops coupling with BPNNs is the highest performing (RMSE = 0.0305). Moreover, when the structure is obvious, we found that the FVC of row crops was about 0.5–0.6, and the relationship between estimated FVC of row crops and NDVI presented a strong exponential relationship. The results reinforced the conclusion that the canopy reflectance model of row crops coupled with BPNNs is more suitable for estimating the FVC of row crops in high-resolution images.},
DOI = {10.3390/rs13193874}
}



@Article{rs13193878,
AUTHOR = {Montgomery, Joshua and Mahoney, Craig and Brisco, Brian and Boychuk, Lyle and Cobbaert, Danielle and Hopkinson, Chris},
TITLE = {Remote Sensing of Wetlands in the Prairie Pothole Region of North America},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3878},
URL = {https://www.mdpi.com/2072-4292/13/19/3878},
ISSN = {2072-4292},
ABSTRACT = {The Prairie Pothole Region (PPR) of North America is an extremely important habitat for a diverse range of wetland ecosystems that provide a wealth of socio-economic value. This paper describes the ecological characteristics and importance of PPR wetlands and the use of remote sensing for mapping and monitoring applications. While there are comprehensive reviews for wetland remote sensing in recent publications, there is no comprehensive review about the use of remote sensing in the PPR. First, the PPR is described, including the wetland classification systems that have been used, the water regimes that control the surface water and water levels, and the soil and vegetation characteristics of the region. The tools and techniques that have been used in the PPR for analyses of geospatial data for wetland applications are described. Field observations for ground truth data are critical for good validation and accuracy assessment of the many products that are produced. Wetland classification approaches are reviewed, including Decision Trees, Machine Learning, and object versus pixel-based approaches. A comprehensive description of the remote sensing systems and data that have been employed by various studies in the PPR is provided. A wide range of data can be used for various applications, including passive optical data like aerial photographs or satellite-based, Earth-observation data. Both airborne and spaceborne lidar studies are described. A detailed description of Synthetic Aperture RADAR (SAR) data and research are provided. The state of the art is the use of multi-source data to achieve higher accuracies and hybrid approaches. Digital Surface Models are also being incorporated in geospatial analyses to separate forest and shrub and emergent systems based on vegetation height. Remote sensing provides a cost-effective mechanism for mapping and monitoring PPR wetlands, especially with the logistical difficulties and cost of field-based methods. The wetland characteristics of the PPR dictate the need for high resolution in both time and space, which is increasingly possible with the numerous and increasing remote sensing systems available and the trend to open-source data and tools. The fusion of multi-source remote sensing data via state-of-the-art machine learning is recommended for wetland applications in the PPR. The use of such data promotes flexibility for sensor addition, subtraction, or substitution as a function of application needs and potential cost restrictions. This is important in the PPR because of the challenges related to the highly dynamic nature of this unique region.},
DOI = {10.3390/rs13193878}
}



@Article{bdcc5040050,
AUTHOR = {Gouiaa, Rafik and Akhloufi, Moulay A. and Shahbazi, Mozhdeh},
TITLE = {Advances in Convolution Neural Networks Based Crowd Counting and Density Estimation},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2504-2289/5/4/50},
ISSN = {2504-2289},
ABSTRACT = {Automatically estimating the number of people in unconstrained scenes is a crucial yet challenging task in different real-world applications, including video surveillance, public safety, urban planning, and traffic monitoring. In addition, methods developed to estimate the number of people can be adapted and applied to related tasks in various fields, such as plant counting, vehicle counting, and cell microscopy. Many challenges and problems face crowd counting, including cluttered scenes, extreme occlusions, scale variation, and changes in camera perspective. Therefore, in the past few years, tremendous research efforts have been devoted to crowd counting, and numerous excellent techniques have been proposed. The significant progress in crowd counting methods in recent years is mostly attributed to advances in deep convolution neural networks (CNNs) as well as to public crowd counting datasets. In this work, we review the papers that have been published in the last decade and provide a comprehensive survey of the recent CNNs based crowd counting techniques. We briefly review detection-based, regression-based, and traditional density estimation based approaches. Then, we delve into detail regarding the deep learning based density estimation approaches and recently published datasets. In addition, we discuss the potential applications of crowd counting and in particular its applications using unmanned aerial vehicle (UAV) images.},
DOI = {10.3390/bdcc5040050}
}



@Article{rs13193892,
AUTHOR = {Zhang, Tianxiang and Xu, Zhiyong and Su, Jinya and Yang, Zhifang and Liu, Cunjia and Chen, Wen-Hua and Li, Jiangyun},
TITLE = {Ir-UNet: Irregular Segmentation U-Shape Network for Wheat Yellow Rust Detection by UAV Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3892},
URL = {https://www.mdpi.com/2072-4292/13/19/3892},
ISSN = {2072-4292},
ABSTRACT = {Crop disease is widely considered as one of the most pressing challenges for food crops, and therefore an accurate crop disease detection algorithm is highly desirable for its sustainable management. The recent use of remote sensing and deep learning is drawing increasing research interests in wheat yellow rust disease detection. However, current solutions on yellow rust detection are generally addressed by RGB images and the basic semantic segmentation algorithms (e.g., UNet), which do not consider the irregular and blurred boundary problems of yellow rust area therein, restricting the disease segmentation performance. Therefore, this work aims to develop an automatic yellow rust disease detection algorithm to cope with these boundary problems. An improved algorithm entitled Ir-UNet by embedding irregular encoder module (IEM), irregular decoder module (IDM) and content-aware channel re-weight module (CCRM) is proposed and compared against the basic UNet while with various input features. The recently collected dataset by DJI M100 UAV equipped with RedEdge multispectral camera is used to evaluate the algorithm performance. Comparative results show that the Ir-UNet with five raw bands outperforms the basic UNet, achieving the highest overall accuracy (OA) score (97.13%) among various inputs. Moreover, the use of three selected bands, Red-NIR-RE, in the proposed Ir-UNet can obtain a comparable result (OA: 96.83%) while with fewer spectral bands and less computation load. It is anticipated that this study by seamlessly integrating the Ir-UNet network and UAV multispectral images can pave the way for automated yellow rust detection at farmland scales.},
DOI = {10.3390/rs13193892}
}



@Article{electronics10192377,
AUTHOR = {Khan, Mohammad Zubair and Alhazmi, Omar H. and Javed, Muhammad Awais and Ghandorh, Hamza and Aloufi, Khalid S.},
TITLE = {Reliable Internet of Things: Challenges and Future Trends},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {2377},
URL = {https://www.mdpi.com/2079-9292/10/19/2377},
ISSN = {2079-9292},
ABSTRACT = {The Internet of Things (IoT) is a vital component of many future industries. By intelligent integration of sensors, wireless communications, computing techniques, and data analytics, IoT can increase productivity and efficiency of industries. Reliability of data transmission is key to realize several applications offered by IoT. In this paper, we present an overview of future IoT applications, and their major communication requirements. We provide a brief survey of recent work in four major areas of reliable IoT including resource allocation, latency management, security, and reliability metrics. Finally, we highlight some of the important challenges for reliable IoT related to machine learning techniques, 6G communications and blockchain based security that need further investigation and discuss related future directions.},
DOI = {10.3390/electronics10192377}
}



@Article{drones5040106,
AUTHOR = {Nooralishahi, Parham and Ibarra-Castanedo, Clemente and Deane, Shakeb and López, Fernando and Pant, Shashank and Genest, Marc and Avdelidis, Nicolas P. and Maldague, Xavier P. V.},
TITLE = {Drone-Based Non-Destructive Inspection of Industrial Sites: A Review and Case Studies},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {106},
URL = {https://www.mdpi.com/2504-446X/5/4/106},
ISSN = {2504-446X},
ABSTRACT = {Using aerial platforms for Non-Destructive Inspection (NDI) of large and complex structures is a growing field of interest in various industries. Infrastructures such as: buildings, bridges, oil and gas, etc. refineries require regular and extensive inspections. The inspection reports are used to plan and perform required maintenance, ensuring their structural health and the safety of the workers. However, performing these inspections can be challenging due to the size of the facility, the lack of easy access, the health risks for the inspectors, or several other reasons, which has convinced companies to invest more in drones as an alternative solution to overcome these challenges. The autonomous nature of drones can assist companies in reducing inspection time and cost. Moreover, the employment of drones can lower the number of required personnel for inspection and can increase personnel safety. Finally, drones can provide a safe and reliable solution for inspecting hard-to-reach or hazardous areas. Despite the recent developments in drone-based NDI to reliably detect defects, several limitations and challenges still need to be addressed. In this paper, a brief review of the history of unmanned aerial vehicles, along with a comprehensive review of studies focused on UAV-based NDI of industrial and commercial facilities, are provided. Moreover, the benefits of using drones in inspections as an alternative to conventional methods are discussed, along with the challenges and open problems of employing drones in industrial inspections, are explored. Finally, some of our case studies conducted in different industrial fields in the field of Non-Destructive Inspection are presented.},
DOI = {10.3390/drones5040106}
}



@Article{s21196499,
AUTHOR = {Li, Shuyang and Hu, Xiaohui and Du, Yongwen},
TITLE = {Deep Reinforcement Learning for Computation Offloading and Resource Allocation in Unmanned-Aerial-Vehicle Assisted Edge Computing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6499},
URL = {https://www.mdpi.com/1424-8220/21/19/6499},
PubMedID = {34640820},
ISSN = {1424-8220},
ABSTRACT = {Computation offloading technology extends cloud computing to the edge of the access network close to users, bringing many benefits to terminal devices with limited battery and computational resources. Nevertheless, the existing computation offloading approaches are challenging to apply to specific scenarios, such as the dense distribution of end-users and the sparse distribution of network infrastructure. The technological revolution in the unmanned aerial vehicle (UAV) and chip industry has granted UAVs more computing resources and promoted the emergence of UAV-assisted mobile edge computing (MEC) technology, which could be applied to those scenarios. However, in the MEC system with multiple users and multiple servers, making reasonable offloading decisions and allocating system resources is still a severe challenge. This paper studies the offloading decision and resource allocation problem in the UAV-assisted MEC environment with multiple users and servers. To ensure the quality of service for end-users, we set the weighted total cost of delay, energy consumption, and the size of discarded tasks as our optimization objective. We further formulate the joint optimization problem as a Markov decision process and apply the soft actor–critic (SAC) deep reinforcement learning algorithm to optimize the offloading policy. Numerical simulation results show that the offloading policy optimized by our proposed SAC-based dynamic computing offloading (SACDCO) algorithm effectively reduces the delay, energy consumption, and size of discarded tasks for the UAV-assisted MEC system. Compared with the fixed local-UAV scheme in the specific simulation setting, our proposed approach reduces system delay and energy consumption by approximately 50% and 200%, respectively.},
DOI = {10.3390/s21196499}
}



@Article{act10100255,
AUTHOR = {Xia, Shuang and Zhang, Xiangyin},
TITLE = {Constrained Path Planning for Unmanned Aerial Vehicle in 3D Terrain Using Modified Multi-Objective Particle Swarm Optimization},
JOURNAL = {Actuators},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {255},
URL = {https://www.mdpi.com/2076-0825/10/10/255},
ISSN = {2076-0825},
ABSTRACT = {This paper considered the constrained unmanned aerial vehicle (UAV) path planning problem as the multi-objective optimization problem, in which both costs and constraints are treated as the objective functions. A novel multi-objective particle swarm optimization algorithm based on the Gaussian distribution and the Q-Learning technique (GMOPSO-QL) is proposed and applied to determine the feasible and optimal path for UAV. In GMOPSO-QL, the Gaussian distribution based updating operator is adopted to generate new particles, and the exploration and exploitation modes are introduced to enhance population diversity and convergence speed, respectively. Moreover, the Q-Learning based mode selection logic is introduced to balance the global search with the local search in the evolution process. Simulation results indicate that our proposed GMOPSO-QL can deal with the constrained UAV path planning problem and is superior to existing optimization algorithms in terms of efficiency and robustness.},
DOI = {10.3390/act10100255}
}



@Article{s21196520,
AUTHOR = {Novkovic, Ivan and Markovic, Goran B. and Lukic, Djordje and Dragicevic, Slavoljub and Milosevic, Marko and Djurdjic, Snezana and Samardzic, Ivan and Lezaic, Tijana and Tadic, Marija},
TITLE = {GIS-Based Forest Fire Susceptibility Zonation with IoT Sensor Network Support, Case Study—Nature Park Golija, Serbia},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6520},
URL = {https://www.mdpi.com/1424-8220/21/19/6520},
PubMedID = {34640837},
ISSN = {1424-8220},
ABSTRACT = {The territory of the Republic of Serbia is vulnerable to various natural disasters, among which forest fires stand out. In relation with climate changes, the number of forest fires in Serbia has been increasing from year to year. Protected natural areas are especially endangered by wildfires. For Nature Park Golija, as the second largest in Serbia, with an area of 75,183 ha, and with MaB Reserve Golija-Studenica on part of its territory (53,804 ha), more attention should be paid in terms of forest fire mitigation. GIS and multi-criteria decision analysis are indispensable when it comes to spatial analysis for the purpose of natural disaster risk management. Index-based and fuzzy AHP methods were used, together with TOPSIS method for forest fire susceptibility zonation. Very high and high forest fire susceptibility zone were recorded on 26.85% (Forest Fire Susceptibility Index) and 25.75% (fuzzy AHP). The additional support for forest fire prevention is realized through an additional Internet of Thing (IoT)-based sensor network that enables the continuous collection of local meteorological and environmental data, which enables low-cost and reliable real-time fire risk assessment and detection and the improved long-term and short-term forest fire susceptibility assessment. Obtained results can be applied for adequate forest fire risk management, improvement of the monitoring, and early warning systems in the Republic of Serbia, but are also important for relevant authorities at national, regional, and local level, which will be able to coordinate and intervene in a case of emergency events.},
DOI = {10.3390/s21196520}
}



@Article{rs13193898,
AUTHOR = {Cao, Duanguang and Xing, Hanfa and Wong, Man Sing and Kwan, Mei-Po and Xing, Huaqiao and Meng, Yuan},
TITLE = {A Stacking Ensemble Deep Learning Model for Building Extraction from Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3898},
URL = {https://www.mdpi.com/2072-4292/13/19/3898},
ISSN = {2072-4292},
ABSTRACT = {Automatically extracting buildings from remote sensing images with deep learning is of great significance to urban planning, disaster prevention, change detection, and other applications. Various deep learning models have been proposed to extract building information, showing both strengths and weaknesses in capturing the complex spectral and spatial characteristics of buildings in remote sensing images. To integrate the strengths of individual models and obtain fine-scale spatial and spectral building information, this study proposed a stacking ensemble deep learning model. First, an optimization method for the prediction results of the basic model is proposed based on fully connected conditional random fields (CRFs). On this basis, a stacking ensemble model (SENet) based on a sparse autoencoder integrating U-NET, SegNet, and FCN-8s models is proposed to combine the features of the optimized basic model prediction results. Utilizing several cities in Hebei Province, China as a case study, a building dataset containing attribute labels is established to assess the performance of the proposed model. The proposed SENet is compared with three individual models (U-NET, SegNet and FCN-8s), and the results show that the accuracy of SENet is 0.954, approximately 6.7%, 6.1%, and 9.8% higher than U-NET, SegNet, and FCN-8s models, respectively. The identification of building features, including colors, sizes, shapes, and shadows, is also evaluated, showing that the accuracy, recall, F1 score, and intersection over union (IoU) of the SENet model are higher than those of the three individual models. This suggests that the proposed ensemble model can effectively depict the different features of buildings and provides an alternative approach to building extraction with higher accuracy.},
DOI = {10.3390/rs13193898}
}



@Article{su131910859,
AUTHOR = {Song, Huiqi and Chen, Pengwei and Zhang, Yongxun and Chen, Youcheng},
TITLE = {Study Progress of Important Agricultural Heritage Systems (IAHS): A Literature Analysis},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {10859},
URL = {https://www.mdpi.com/2071-1050/13/19/10859},
ISSN = {2071-1050},
ABSTRACT = {Important Agricultural Heritage Systems (IAHS), as a new type of heritage, has received extensive attention from the international scientific communities. With the increase of IAHS research, reviews on it have been conducted by many scholars. However, visualized research to show future research trends of IAHS are lacking. Therefore, using metrology analysis methods, this study aims at presenting the progress of research and the general development trends of Globally Important Agricultural Heritage Systems (GIAHS) in the world from 2006 to 2020 to provide ideas for the development of countries or regions in the future. This study mapped 292 literatures from Web of Science core collections from 2006 to 2020 by CiteSpace software. The results show that research on IAHS from 2006 to 2020 experienced two stages: the fluctuating increase stage, and the steady growth stage. Author groups from China, Italy, the USA, Japan, etc., contributed many papers on IAHS. Institutions including the Chinese Academy of Sciences, the University of Florence and the University of Padua in Italy, etc., have a relatively high influence on international IAHS research. Agriculture Ecosystems &amp; Environment is the most cited journal. Agricultural Heritage Systems, regeneration, agriculture, agroforestry, dry-stone wall, social capital, instability, and agricultural biodiversity have been hotspots in the past 15 years. The research themes mainly focus on GIAHS, tourism, livelihood assets, and direct georeferencing. Authors in different regions concern different research themes. In the future, the fields of applications and microscopic views, social sciences, applications of standardized quantitative research methods, and broadened international cooperation should be paid more attention.},
DOI = {10.3390/su131910859}
}



@Article{jlpea11040039,
AUTHOR = {Saddik, Amine and Latif, Rachid and El Ouardi, Abdelhafid},
TITLE = {Low-Power FPGA Architecture Based Monitoring Applications in Precision Agriculture},
JOURNAL = {Journal of Low Power Electronics and Applications},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2079-9268/11/4/39},
ISSN = {2079-9268},
ABSTRACT = {Today’s on-chip systems technology has grounded impressive advances in computing power and energy consumption. The choice of the right architecture depends on the application. In our case, we were studying vegetation monitoring algorithms in precision agriculture. This study presents a system based on a monitoring algorithm for agricultural fields, an electronic architecture based on a CPU-FPGA SoC system and the OpenCL parallel programming paradigm. We focused our study on our own dataset of agricultural fields to validate the results. The fields studied in our case are in the Guelmin-Oued noun region in the south of Morocco. These fields are divided into two areas, with a total surface of 3.44 Ha2 for the first field and 3.73 Ha2 for the second. The images were collected using a DJI-type unmanned aerial vehicle and an RGB camera. Performance evaluation showed that the system could process up to 86 fps versus 12 fps or 20 fps in C/C++ and OpenMP implementations, respectively. Software optimizations have increased the performance to 107 fps, which meets real-time constraints.},
DOI = {10.3390/jlpea11040039}
}



@Article{rs13193913,
AUTHOR = {Zawadzka, Joanna and Truckell, Ian and Khouakhi, Abdou and Rivas Casado, Mónica},
TITLE = {Detection of Flood Damage in Urban Residential Areas Using Object-Oriented UAV Image Analysis Coupled with Tree-Based Classifiers},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3913},
URL = {https://www.mdpi.com/2072-4292/13/19/3913},
ISSN = {2072-4292},
ABSTRACT = {Timely clearing-up interventions are essential for effective recovery of flood-damaged housing, however, time-consuming door-to-door inspections for insurance purposes need to take place before major repairs can be done to adequately assess the losses caused by flooding. With the increased probability of flooding, there is a heightened need for rapid flood damage assessment methods. High resolution imagery captured by unmanned aerial vehicles (UAVs) offers an opportunity for accelerating the time needed for inspections, either through visual interpretation or automated image classification. In this study, object-oriented image segmentation coupled with tree-based classifiers was implemented on a 10 cm resolution RGB orthoimage, captured over the English town of Cockermouth a week after a flood triggered by storm Desmond, to automatically detect debris associated with damages predominantly to residential housing. Random forests algorithm achieved a good level of overall accuracy of 74%, with debris being correctly classified at the rate of 58%, and performing well for small debris (67%) and skips (64%). The method was successful at depicting brightly-colored debris, however, was prone to misclassifications with brightly-colored vehicles. Consequently, in the current stage, the methodology could be used to facilitate visual interpretation of UAV images. Methods to improve accuracy have been identified and discussed.},
DOI = {10.3390/rs13193913}
}



@Article{rs13193919,
AUTHOR = {Mo, Jiawei and Lan, Yubin and Yang, Dongzi and Wen, Fei and Qiu, Hongbin and Chen, Xin and Deng, Xiaoling},
TITLE = {Deep Learning-Based Instance Segmentation Method of Litchi Canopy from UAV-Acquired Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3919},
URL = {https://www.mdpi.com/2072-4292/13/19/3919},
ISSN = {2072-4292},
ABSTRACT = {Instance segmentation of fruit tree canopies from images acquired by unmanned aerial vehicles (UAVs) is of significance for the precise management of orchards. Although deep learning methods have been widely used in the fields of feature extraction and classification, there are still phenomena of complex data and strong dependence on software performances. This paper proposes a deep learning-based instance segmentation method of litchi trees, which has a simple structure and lower requirements for data form. Considering that deep learning models require a large amount of training data, a labor-friendly semi-auto method for image annotation is introduced. The introduction of this method allows for a significant improvement in the efficiency of data pre-processing. Facing the high requirement of a deep learning method for computing resources, a partition-based method is presented for the segmentation of high-resolution digital orthophoto maps (DOMs). Citrus data is added to the training set to alleviate the lack of diversity of the original litchi dataset. The average precision (AP) is selected to evaluate the metric of the proposed model. The results show that with the help of training with the litchi-citrus datasets, the best AP on the test set reaches 96.25%.},
DOI = {10.3390/rs13193919}
}



@Article{s21196540,
AUTHOR = {Pan, Qian and Gao, Maofang and Wu, Pingbo and Yan, Jingwen and Li, Shilei},
TITLE = {A Deep-Learning-Based Approach for Wheat Yellow Rust Disease Recognition from Unmanned Aerial Vehicle Images},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6540},
URL = {https://www.mdpi.com/1424-8220/21/19/6540},
PubMedID = {34640873},
ISSN = {1424-8220},
ABSTRACT = {Yellow rust is a disease with a wide range that causes great damage to wheat. The traditional method of manually identifying wheat yellow rust is very inefficient. To improve this situation, this study proposed a deep-learning-based method for identifying wheat yellow rust from unmanned aerial vehicle (UAV) images. The method was based on the pyramid scene parsing network (PSPNet) semantic segmentation model to classify healthy wheat, yellow rust wheat, and bare soil in small-scale UAV images, and to investigate the spatial generalization of the model. In addition, it was proposed to use the high-accuracy classification results of traditional algorithms as weak samples for wheat yellow rust identification. The recognition accuracy of the PSPNet model in this study reached 98%. On this basis, this study used the trained semantic segmentation model to recognize another wheat field. The results showed that the method had certain generalization ability, and its accuracy reached 98%. In addition, the high-accuracy classification result of a support vector machine was used as a weak label by weak supervision, which better solved the labeling problem of large-size images, and the final recognition accuracy reached 94%. Therefore, the present study method facilitated timely control measures to reduce economic losses.},
DOI = {10.3390/s21196540}
}



@Article{jne2040029,
AUTHOR = {Proctor, Philippe and Teuscher, Christof and Hecht, Adam and Osiński, Marek},
TITLE = {Proximal Policy Optimization for Radiation Source Search},
JOURNAL = {Journal of Nuclear Engineering},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {368--397},
URL = {https://www.mdpi.com/2673-4362/2/4/29},
ISSN = {2673-4362},
ABSTRACT = {Rapid search and localization for nuclear sources can be an important aspect in preventing human harm from illicit material in dirty bombs or from contamination. In the case of a single mobile radiation detector, there are numerous challenges to overcome such as weak source intensity, multiple sources, background radiation, and the presence of obstructions, i.e., a non-convex environment. In this work, we investigate the sequential decision making capability of deep reinforcement learning in the nuclear source search context. A novel neural network architecture (RAD-A2C) based on the advantage actor critic (A2C) framework and a particle filter gated recurrent unit for localization is proposed. Performance is studied in a randomized 20×20 m convex and non-convex simulation environment across a range of signal-to-noise ratio (SNR)s for a single detector and single source. RAD-A2C performance is compared to both an information-driven controller that uses a bootstrap particle filter and to a gradient search (GS) algorithm. We find that the RAD-A2C has comparable performance to the information-driven controller across SNR in a convex environment. The RAD-A2C far outperforms the GS algorithm in the non-convex environment with greater than 95% median completion rate for up to seven obstructions.},
DOI = {10.3390/jne2040029}
}



@Article{s21196588,
AUTHOR = {Kamal, Muhammad Ayoub and Raza, Hafiz Wahab and Alam, Muhammad Mansoor and Su’ud, Mazliham Mohd and Sajak, Aznida binti Abu Bakar},
TITLE = {Resource Allocation Schemes for 5G Network: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6588},
URL = {https://www.mdpi.com/1424-8220/21/19/6588},
PubMedID = {34640908},
ISSN = {1424-8220},
ABSTRACT = {Fifth-generation (5G) communication technology is intended to offer higher data rates, outstanding user exposure, lower power consumption, and extremely short latency. Such cellular networks will implement a diverse multi-layer model comprising device-to-device networks, macro-cells, and different categories of small cells to assist customers with desired quality-of-service (QoS). This multi-layer model affects several studies that confront utilizing interference management and resource allocation in 5G networks. With the growing need for cellular service and the limited resources to provide it, capably handling network traffic and operation has become a problem of resource distribution. One of the utmost serious problems is to alleviate the jamming in the network in support of having a better QoS. However, although a limited number of review papers have been written on resource distribution, no review papers have been written specifically on 5G resource allocation. Hence, this article analyzes the issue of resource allocation by classifying the various resource allocation schemes in 5G that have been reported in the literature and assessing their ability to enhance service quality. This survey bases its discussion on the metrics that are used to evaluate network performance. After consideration of the current evidence on resource allocation methods in 5G, the review hopes to empower scholars by suggesting future research areas on which to focus.},
DOI = {10.3390/s21196588}
}



@Article{app11199173,
AUTHOR = {Rodriguez-Conde, Ivan and Campos, Celso and Fdez-Riverola, Florentino},
TITLE = {On-Device Object Detection for More Efficient and Privacy-Compliant Visual Perception in Context-Aware Systems},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {9173},
URL = {https://www.mdpi.com/2076-3417/11/19/9173},
ISSN = {2076-3417},
ABSTRACT = {Ambient Intelligence (AmI) encompasses technological infrastructures capable of sensing data from environments and extracting high-level knowledge to detect or recognize users’ features and actions, as well as entities or events in their surroundings. Visual perception, particularly object detection, has become one of the most relevant enabling factors for this context-aware user-centered intelligence, being the cornerstone of relevant but complex tasks, such as object tracking or human action recognition. In this context, convolutional neural networks have proven to achieve state-of-the-art accuracy levels. However, they typically result in large and highly complex models that typically demand computation offloading onto remote cloud platforms. Such an approach has security- and latency-related limitations and may not be appropriate for some AmI use cases where the system response time must be as short as possible, and data privacy must be guaranteed. In the last few years, the on-device paradigm has emerged in response to those limitations, yielding more compact and efficient neural networks able to address inference directly on client machines, thus providing users with a smoother and better-tailored experience, with no need of sharing their data with an outsourced service. Framed in that novel paradigm, this work presents a review of the recent advances made along those lines in object detection, providing a comprehensive study of the most relevant lightweight CNN-based detection frameworks, discussing the most paradigmatic AmI domains where such an approach has been successfully applied, the different challenges arisen, the key strategies and techniques adopted to create visual solutions for image-based object classification and localization, as well as the most relevant factors to bear in mind when assessing or comparing those techniques, such as the evaluation metrics or the hardware setups used.},
DOI = {10.3390/app11199173}
}



@Article{rs13193949,
AUTHOR = {Shen, Ying and Li, Jie and Lin, Wenfu and Chen, Liqiong and Huang, Feng and Wang, Shu},
TITLE = {Camouflaged Target Detection Based on Snapshot Multispectral Imaging},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3949},
URL = {https://www.mdpi.com/2072-4292/13/19/3949},
ISSN = {2072-4292},
ABSTRACT = {The spectral information contained in the hyperspectral images (HSI) distinguishes the intrinsic properties of a target from the background, which is widely used in remote sensing. However, the low imaging speed and high data redundancy caused by the high spectral resolution of imaging spectrometers limit their application in scenarios with the real-time requirement. In this work, we achieve the precise detection of camouflaged targets based on snapshot multispectral imaging technology and band selection methods in urban-related scenes. Specifically, the camouflaged target detection algorithm combines the constrained energy minimization (CEM) algorithm and the improved maximum between-class variance (OTSU) algorithm (t-OTSU), which is proposed to obtain the initial target detection results and adaptively segment the target region. Moreover, an object region extraction (ORE) algorithm is proposed to obtain a complete target contour that improves the target detection capability of multispectral images (MSI). The experimental results show that the proposed algorithm has the ability to detect different camouflaged targets by using only four bands. The detection accuracy is above 99%, and the false alarm rate is below 0.2%. The research achieves the effective detection of camouflaged targets and has the potential to provide a new means for real-time multispectral sensing in complex scenes.},
DOI = {10.3390/rs13193949}
}



@Article{rs13193952,
AUTHOR = {Liu, Changjiang and Duan, Pan and Zhang, Fei and Jim, Chi-Yung and Tan, Mou Leong and Chan, Ngai Weng},
TITLE = {Feasibility of the Spatiotemporal Fusion Model in Monitoring Ebinur Lake’s Suspended Particulate Matter under the Missing-Data Scenario},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3952},
URL = {https://www.mdpi.com/2072-4292/13/19/3952},
ISSN = {2072-4292},
ABSTRACT = {High-frequency monitoring of suspended particulate matter (SPM) concentration can improve water resource management. Missing high-resolution satellite images could hamper remote-sensing SPM monitoring. This study resolved the problem by applying spatiotemporal fusion technology to obtain high spatial resolution and dense time-series data to fill image-data gaps. Three data sources (MODIS, Landsat 8, and Sentinel 2) and two spatiotemporal fusion methods (the enhanced spatial and temporal adaptive reflectance fusion model (ESTARFM) and the flexible spatiotemporal data fusion (FSDAF)) were used to reconstruct missing satellite images. We compared their fusion accuracy and verified the consistency of fusion images between data sources. For the fusion images, we used random forest (RF) and XGBoost as inversion methods and set “fusion first” and “inversion first” strategies to test the method’s feasibility in Ebinur Lake, Xinjiang, arid northwestern China. Our results showed that (1) the blue, green, red, and NIR bands of ESTARFM fusion image were better than FSDAF, with a good consistency (R2 ≥ 0.54) between the fused Landsat 8, Sentinel 2 images, and their original images; (2) the original image and fusion image offered RF inversion effect better than XGBoost. The inversion accuracy based on Landsat 8 and Sentinel 2 were R2 0.67 and 0.73, respectively. The correlation of SPM distribution maps of the two data sources attained a good consistency of R2 0.51; (3) in retrieving SPM from fused images, the “fusion first” strategy had better accuracy. The optimal combination was ESTARFM (Landsat 8)_RF and ESTARFM (Sentinel 2)_RF, consistent with original SPM maps (R2 = 0.38, 0.41, respectively). Overall, the spatiotemporal fusion model provided effective SPM monitoring under the image-absence scenario, with good consistency in the inversion of SPM. The findings provided the research basis for long-term and high-frequency remote-sensing SPM monitoring and high-precision smart water resource management.},
DOI = {10.3390/rs13193952}
}



@Article{app11199199,
AUTHOR = {Andrushia, A. Diana and Sagayam, K. Martin and Dang, Hien and Pomplun, Marc and Quach, Lien},
TITLE = {Visual-Saliency-Based Abnormality Detection for MRI Brain Images—Alzheimer’s Disease Analysis},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {9199},
URL = {https://www.mdpi.com/2076-3417/11/19/9199},
ISSN = {2076-3417},
ABSTRACT = {In recent years, medical image analysis has played a vital role in detecting diseases in their early stages. Medical images are rapidly becoming available for various applications to solve human problems. Therefore, complex medical features are needed to develop a diagnostic system for physicians to provide better treatment. Traditional methods of abnormality detection suffer from misidentification of abnormal regions in the given data. Visual-saliency detection methods are used to locate abnormalities to improve the accuracy of the proposed work. This study explores the role of a visual saliency map in the classification of Alzheimer’s disease (AD). Bottom-up saliency corresponds to image features, whereas top-down saliency uses domain knowledge in magnetic resonance imaging (MRI) brain images. The novelty of the proposed method lies in the use of an elliptical local binary pattern descriptor for low-level MRI characterization. Ellipse-like topologies help to obtain feature information from different orientations. Extensively directional features at different orientations cover the micro patterns. The brain regions of the Alzheimer’s disease stages were classified from the saliency maps. Multiple-kernel learning (MKL) and simple and efficient MKL (SEMKL) were used to classify Alzheimer’s disease from normal controls. The proposed method used the OASIS dataset and experimental results were compared with eight state-of-the-art methods. The proposed visual saliency-based abnormality detection produces reliable results in terms of accuracy, sensitivity, specificity, and f-measure.},
DOI = {10.3390/app11199199}
}



@Article{en14196316,
AUTHOR = {Berghout, Tarek and Benbouzid, Mohamed and Bentrcia, Toufik and Ma, Xiandong and Djurović, Siniša and Mouss, Leïla-Hayet},
TITLE = {Machine Learning-Based Condition Monitoring for PV Systems: State of the Art and Future Prospects},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6316},
URL = {https://www.mdpi.com/1996-1073/14/19/6316},
ISSN = {1996-1073},
ABSTRACT = {To ensure the continuity of electric power generation for photovoltaic systems, condition monitoring frameworks are subject to major enhancements. The continuous uniform delivery of electric power depends entirely on a well-designed condition maintenance program. A just-in-time task to deal with several naturally occurring faults can be correctly undertaken via the cooperation of effective detection, diagnosis, and prognostic analyses. Therefore, the present review first outlines different failure modes to which all photovoltaic systems are subjected, in addition to the essential integrated detection methods and technologies. Then, data-driven paradigms, and their contribution to solving this prediction problem, are also explored. Accordingly, this review primarily investigates the different learning architectures used (i.e., ordinary, hybrid, and ensemble) in relation to their learning frameworks (i.e., traditional and deep learning). It also discusses the extension of machine learning to knowledge-driven approaches, including generative models such as adversarial networks and transfer learning. Finally, this review provides insights into different works to highlight various operating conditions and different numbers and types of failures, and provides links to some publicly available datasets in the field. The clear organization of the abundant information on this subject may result in rigorous guidelines for the trends adopted in the future.},
DOI = {10.3390/en14196316}
}



@Article{drones5040111,
AUTHOR = {Avola, Danilo and Pannone, Daniele},
TITLE = {MAGI: Multistream Aerial Segmentation of Ground Images with Small-Scale Drones},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {111},
URL = {https://www.mdpi.com/2504-446X/5/4/111},
ISSN = {2504-446X},
ABSTRACT = {In recent years, small-scale drones have been used in heterogeneous tasks, such as border control, precision agriculture, and search and rescue. This is mainly due to their small size that allows for easy deployment, their low cost, and their increasing computing capability. The latter aspect allows for researchers and industries to develop complex machine- and deep-learning algorithms for several challenging tasks, such as object classification, object detection, and segmentation. Focusing on segmentation, this paper proposes a novel deep-learning model for semantic segmentation. The model follows a fully convolutional multistream approach to perform segmentation on different image scales. Several streams perform convolutions by exploiting kernels of different sizes, making segmentation tasks robust to flight altitude changes. Extensive experiments were performed on the UAV Mosaicking and Change Detection (UMCD) dataset, highlighting the effectiveness of the proposed method.},
DOI = {10.3390/drones5040111}
}



@Article{rs13193979,
AUTHOR = {Zhuang, Jiedong and Dai, Ming and Chen, Xuruoyan and Zheng, Enhui},
TITLE = {A Faster and More Effective Cross-View Matching Method of UAV and Satellite Images for UAV Geolocalization},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {3979},
URL = {https://www.mdpi.com/2072-4292/13/19/3979},
ISSN = {2072-4292},
ABSTRACT = {Cross-view geolocalization matches the same target in different images from various views, such as views of unmanned aerial vehicles (UAVs) and satellites, which is a key technology for UAVs to autonomously locate and navigate without a positioning system (e.g., GPS and GNSS). The most challenging aspect in this area is the shifting of targets and nonuniform scales among different views. Published methods focus on extracting coarse features from parts of images, but neglect the relationship between different views, and the influence of scale and shifting. To bridge this gap, an effective network is proposed with well-designed structures, referred to as multiscale block attention (MSBA), based on a local pattern network. MSBA cuts images into several parts with different scales, among which self-attention is applied to make feature extraction more efficient. The features of different views are extracted by a multibranch structure, which was designed to make different branches learn from each other, leading to a more subtle relationship between views. The method was implemented with the newest UAV-based geolocalization dataset. Compared with the existing state-of-the-art (SOTA) method, MSBA accuracy improved by almost 10% when the inference time was equal to that of the SOTA method; when the accuracy of MSBA was the same as that of the SOTA method, inference time was shortened by 30%.},
DOI = {10.3390/rs13193979}
}



@Article{atmos12101309,
AUTHOR = {Sánchez-Balseca, Joseph and Pérez-Foguet, Agustií},
TITLE = {Compositional Spatio-Temporal PM2.5 Modelling in Wildfires},
JOURNAL = {Atmosphere},
VOLUME = {12},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1309},
URL = {https://www.mdpi.com/2073-4433/12/10/1309},
ISSN = {2073-4433},
ABSTRACT = {Wildfires are natural ecological processes that generate high levels of fine particulate matter (PM2.5) that are dispersed into the atmosphere. PM2.5 could be a potential health problem due to its size. Having adequate numerical models to predict the spatial and temporal distribution of PM2.5 helps to mitigate the impact on human health. The compositional data approach is widely used in the environmental sciences and concentration analyses (parts of a whole). This numerical approach in the modelling process avoids one common statistical problem: the spurious correlation. PM2.5 is a part of the atmospheric composition. In this way, this study developed an hourly spatio-temporal PM2.5 model based on the dynamic linear modelling framework (DLM) with a compositional approach. The results of the model are extended using a Gaussian–Mattern field. The modelling of PM2.5 using a compositional approach presented adequate quality model indices (NSE = 0.82, RMSE = 0.23, and a Pearson correlation coefficient of 0.91); however, the correlation range showed a slightly lower value than the conventional/traditional approach. The proposed method could be used in spatial prediction in places without monitoring stations.},
DOI = {10.3390/atmos12101309}
}



@Article{bdcc5040053,
AUTHOR = {Jamil, Sonain and Rahman, MuhibUr and Haider, Amir},
TITLE = {Bag of Features (BoF) Based Deep Learning Framework for Bleached Corals Detection},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {5},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {53},
URL = {https://www.mdpi.com/2504-2289/5/4/53},
ISSN = {2504-2289},
ABSTRACT = {Coral reefs are the sub-aqueous calcium carbonate structures collected by the invertebrates known as corals. The charm and beauty of coral reefs attract tourists, and they play a vital role in preserving biodiversity, ceasing coastal erosion, and promoting business trade. However, they are declining because of over-exploitation, damaging fishery, marine pollution, and global climate changes. Also, coral reefs help treat human immune-deficiency virus (HIV), heart disease, and coastal erosion. The corals of Australia’s great barrier reef have started bleaching due to the ocean acidification, and global warming, which is an alarming threat to the earth’s ecosystem. Many techniques have been developed to address such issues. However, each method has a limitation due to the low resolution of images, diverse weather conditions, etc. In this paper, we propose a bag of features (BoF) based approach that can detect and localize the bleached corals before the safety measures are applied. The dataset contains images of bleached and unbleached corals, and various kernels are used to support the vector machine so that extracted features can be classified. The accuracy of handcrafted descriptors and deep convolutional neural networks is analyzed and provided in detail with comparison to the current method. Various handcrafted descriptors like local binary pattern, a histogram of an oriented gradient, locally encoded transform feature histogram, gray level co-occurrence matrix, and completed joint scale local binary pattern are used for feature extraction. Specific deep convolutional neural networks such as AlexNet, GoogLeNet, VGG-19, ResNet-50, Inception v3, and CoralNet are being used for feature extraction. From experimental analysis and results, the proposed technique outperforms in comparison to the current state-of-the-art methods. The proposed technique achieves 99.08% accuracy with a classification error of 0.92%. A novel bleached coral positioning algorithm is also proposed to locate bleached corals in the coral reef images.},
DOI = {10.3390/bdcc5040053}
}



@Article{rs13194017,
AUTHOR = {Harvey, Winthrop and Rainwater, Chase and Cothren, Jackson},
TITLE = {Direct Aerial Visual Geolocalization Using Deep Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {4017},
URL = {https://www.mdpi.com/2072-4292/13/19/4017},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) must keep track of their location in order to maintain flight plans. Currently, this task is almost entirely performed by a combination of Inertial Measurement Units (IMUs) and reference to GNSS (Global Navigation Satellite System). Navigation by GNSS, however, is not always reliable, due to various causes both natural (reflection and blockage from objects, technical fault, inclement weather) and artificial (GPS spoofing and denial). In such GPS-denied situations, it is desirable to have additional methods for aerial geolocalization. One such method is visual geolocalization, where aircraft use their ground facing cameras to localize and navigate. The state of the art in many ground-level image processing tasks involve the use of Convolutional Neural Networks (CNNs). We present here a study of how effectively a modern CNN designed for visual classification can be applied to the problem of Absolute Visual Geolocalization (AVL, localization without a prior location estimate). An Xception based architecture is trained from scratch over a &gt;1000 km2 section of Washington County, Arkansas to directly regress latitude and longitude from images from different orthorectified high-altitude survey flights. It achieves average localization accuracy on unseen image sets over the same region from different years and seasons with as low as 115 m average error, which localizes to 0.004% of the training area, or about 8% of the width of the 1.5 × 1.5 km input image. This demonstrates that CNNs are expressive enough to encode robust landscape information for geolocalization over large geographic areas. Furthermore, discussed are methods of providing uncertainty for CNN regression outputs, and future areas of potential improvement for use of deep neural networks in visual geolocalization.},
DOI = {10.3390/rs13194017}
}



@Article{ijgi10100680,
AUTHOR = {Yang, Annan and Wang, Chunmei and Pang, Guowei and Long, Yongqing and Wang, Lei and Cruse, Richard M. and Yang, Qinke},
TITLE = {Gully Erosion Susceptibility Mapping in Highly Complex Terrain Using Machine Learning Models},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {680},
URL = {https://www.mdpi.com/2220-9964/10/10/680},
ISSN = {2220-9964},
ABSTRACT = {Gully erosion is the most severe type of water erosion and is a major land degradation process. Gully erosion susceptibility mapping (GESM)’s efficiency and interpretability remains a challenge, especially in complex terrain areas. In this study, a WoE-MLC model was used to solve the above problem, which combines machine learning classification algorithms and the statistical weight of evidence (WoE) model in the Loess Plateau. The three machine learning (ML) algorithms utilized in this research were random forest (RF), gradient boosted decision trees (GBDT), and extreme gradient boosting (XGBoost). The results showed that: (1) GESM were well predicted by combining both machine learning regression models and WoE-MLC models, with the area under the curve (AUC) values both greater than 0.92, and the latter was more computationally efficient and interpretable; (2) The XGBoost algorithm was more efficient in GESM than the other two algorithms, with the strongest generalization ability and best performance in avoiding overfitting (averaged AUC = 0.947), followed by the RF algorithm (averaged AUC = 0.944), and GBDT algorithm (averaged AUC = 0.938); and (3) slope gradient, land use, and altitude were the main factors for GESM. This study may provide a possible method for gully erosion susceptibility mapping at large scale.},
DOI = {10.3390/ijgi10100680}
}



@Article{rs13204027,
AUTHOR = {Byun, Sungwoo and Shin, In-Kyoung and Moon, Jucheol and Kang, Jiyoung and Choi, Sang-Il},
TITLE = {Road Traffic Monitoring from UAV Images Using Deep Learning Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4027},
URL = {https://www.mdpi.com/2072-4292/13/20/4027},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we propose a deep neural network-based method for estimating speed of vehicles on roads automatically from videos recorded using unmanned aerial vehicle (UAV). The proposed method includes the following; (1) detecting and tracking vehicles by analyzing the videos, (2) calculating the image scales using the distances between lanes on the roads, and (3) estimating the speeds of vehicles on the roads. Our method can automatically measure the speed of the vehicles from the only videos recorded using UAV without additional information in both directions on the roads simultaneously. In our experiments, we evaluate the performance of the proposed method with the visual data at four different locations. The proposed method shows 97.6% recall rate and 94.7% precision rate in detecting vehicles, and it shows error (root mean squared error) of 5.27 km/h in estimating the speeds of vehicles.},
DOI = {10.3390/rs13204027}
}



@Article{agriculture11100981,
AUTHOR = {Wu, Yang and Xu, Lihong},
TITLE = {Image Generation of Tomato Leaf Disease Identification Based on Adversarial-VAE},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {981},
URL = {https://www.mdpi.com/2077-0472/11/10/981},
ISSN = {2077-0472},
ABSTRACT = {The deep neural network-based method requires a lot of data for training. Aiming at the problem of a lack of training images in tomato leaf disease identification, an Adversarial-VAE network model for generating images of 10 tomato leaf diseases is proposed, which is used to expand the training set for training an identification model. First, an Adversarial-VAE model is designed to generate tomato leaf disease images. Then, a multi-scale residual learning module is used to replace single-size convolution kernels to enrich extracted features, and a dense connection strategy is integrated into the Adversarial-VAE networks to further enhance the image generation ability. The training set is expanded by the proposed model, which generates the same number of images by training 10,892 images of 10 leaves. The generated images are superior to those of InfoGAN, WAE, VAE, and VAE-GAN measured by the Frechet Inception Distance (FID). The experimental results show that using the extension dataset that is generated by the Adversarial-VAE model to train the Resnet identification model could improve the accuracy of identification effectively. The model proposed in this paper could generate enough images of tomato leaf diseases and provide a feasible solution for data expansion of tomato leaf disease images.},
DOI = {10.3390/agriculture11100981}
}



@Article{rs13204029,
AUTHOR = {Zhao, Jianghong and Wang, Yinrui and Cao, Yuee and Guo, Ming and Huang, Xianfeng and Zhang, Ruiju and Dou, Xintong and Niu, Xinyu and Cui, Yuanyuan and Wang, Jun},
TITLE = {The Fusion Strategy of 2D and 3D Information Based on Deep Learning: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4029},
URL = {https://www.mdpi.com/2072-4292/13/20/4029},
ISSN = {2072-4292},
ABSTRACT = {Recently, researchers have realized a number of achievements involving deep-learning-based neural networks for the tasks of segmentation and detection based on 2D images, 3D point clouds, etc. Using 2D and 3D information fusion for the advantages of compensation and accuracy improvement has become a hot research topic. However, there are no critical reviews focusing on the fusion strategies of 2D and 3D information integration based on various data for segmentation and detection, which are the basic tasks of computer vision. To boost the development of this research domain, the existing representative fusion strategies are collected, introduced, categorized, and summarized in this paper. In addition, the general structures of different kinds of fusion strategies were firstly abstracted and categorized, which may inspire researchers. Moreover, according to the methods included in this paper, the 2D information and 3D information of different methods come from various kinds of data. Furthermore, suitable datasets are introduced and comparatively summarized to support the relative research. Last but not least, we put forward some open challenges and promising directions for future research.},
DOI = {10.3390/rs13204029}
}



@Article{rs13204032,
AUTHOR = {Khun, Kosal and Tremblay, Nicolas and Panneton, Bernard and Vigneault, Philippe and Lord, Etienne and Cavayas, François and Codjia, Claude},
TITLE = {Use of Oblique RGB Imagery and Apparent Surface Area of Plants for Early Estimation of Above-Ground Corn Biomass},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4032},
URL = {https://www.mdpi.com/2072-4292/13/20/4032},
ISSN = {2072-4292},
ABSTRACT = {Estimating above-ground biomass in the context of fertilization management requires the monitoring of crops at early stages. Conventional remote sensing techniques make use of vegetation indices such as the normalized difference vegetation index (NDVI), but they do not exploit the high spatial resolution (ground sampling distance &lt; 5 mm) now achievable with the introduction of unmanned aerial vehicles (UAVs) in agriculture. The aim of this study was to compare image mosaics to single images for the estimation of corn biomass and the influence of viewing angles in this estimation. Nadir imagery was captured by a high spatial resolution camera mounted on a UAV to generate orthomosaics of corn plots at different growth stages (from V2 to V7). Nadir and oblique images (30° and 45° with respect to the vertical) were also acquired from a zip line platform and processed as single images. Image segmentation was performed using the difference color index Excess Green-Excess Red, allowing for the discrimination between vegetation and background pixels. The apparent surface area of plants was then extracted and compared to biomass measured in situ. An asymptotic total least squares regression was performed and showed a strong relationship between the apparent surface area of plants and both dry and fresh biomass. Mosaics tended to underestimate the apparent surface area in comparison to single images because of radiometric degradation. It is therefore conceivable to process only single images instead of investing time and effort in acquiring and processing data for orthomosaic generation. When comparing oblique photography, an angle of 30° yielded the best results in estimating corn biomass, with a low residual standard error of orthogonal distance (RSEOD = 0.031 for fresh biomass, RSEOD = 0.034 for dry biomass). Since oblique imagery provides more flexibility in data acquisition with fewer constraints on logistics, this approach might be an efficient way to monitor crop biomass at early stages.},
DOI = {10.3390/rs13204032}
}



@Article{s21206705,
AUTHOR = {Farkhani, Sadaf and Skovsen, Søren Kelstrup and Dyrmann, Mads and Jørgensen, Rasmus Nyholm and Karstoft, Henrik},
TITLE = {Weed Classification Using Explainable Multi-Resolution Slot Attention},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {6705},
URL = {https://www.mdpi.com/1424-8220/21/20/6705},
PubMedID = {34695919},
ISSN = {1424-8220},
ABSTRACT = {In agriculture, explainable deep neural networks (DNNs) can be used to pinpoint the discriminative part of weeds for an imagery classification task, albeit at a low resolution, to control the weed population. This paper proposes the use of a multi-layer attention procedure based on a transformer combined with a fusion rule to present an interpretation of the DNN decision through a high-resolution attention map. The fusion rule is a weighted average method that is used to combine attention maps from different layers based on saliency. Attention maps with an explanation for why a weed is or is not classified as a certain class help agronomists to shape the high-resolution weed identification keys (WIK) that the model perceives. The model is trained and evaluated on two agricultural datasets that contain plants grown under different conditions: the Plant Seedlings Dataset (PSD) and the Open Plant Phenotyping Dataset (OPPD). The model represents attention maps with highlighted requirements and information about misclassification to enable cross-dataset evaluations. State-of-the-art comparisons represent classification developments after applying attention maps. Average accuracies of 95.42% and 96% are gained for the negative and positive explanations of the PSD test sets, respectively. In OPPD evaluations, accuracies of 97.78% and 97.83% are obtained for negative and positive explanations, respectively. The visual comparison between attention maps also shows high-resolution information.},
DOI = {10.3390/s21206705}
}



@Article{rs13204025,
AUTHOR = {Mirmazloumi, S. Mohammad and Moghimi, Armin and Ranjgar, Babak and Mohseni, Farzane and Ghorbanian, Arsalan and Ahmadi, Seyed Ali and Amani, Meisam and Brisco, Brian},
TITLE = {Status and Trends of Wetland Studies in Canada Using Remote Sensing Technology with a Focus on Wetland Classification: A Bibliographic Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4025},
URL = {https://www.mdpi.com/2072-4292/13/20/4025},
ISSN = {2072-4292},
ABSTRACT = {A large portion of Canada is covered by wetlands; mapping and monitoring them is of great importance for various applications. In this regard, Remote Sensing (RS) technology has been widely employed for wetland studies in Canada over the past 45 years. This study evaluates meta-data to investigate the status and trends of wetland studies in Canada using RS technology by reviewing the scientific papers published between 1976 and the end of 2020 (300 papers in total). Initially, a meta-analysis was conducted to analyze the status of RS-based wetland studies in terms of the wetland classification systems, methods, classes, RS data usage, publication details (e.g., authors, keywords, citations, and publications time), geographic information, and level of classification accuracies. The deep systematic review of 128 peer-reviewed articles illustrated the rising trend in using multi-source RS datasets along with advanced machine learning algorithms for wetland mapping in Canada. It was also observed that most of the studies were implemented over the province of Ontario. Pixel-based supervised classifiers were the most popular wetland classification algorithms. This review summarizes different RS systems and methodologies for wetland mapping in Canada to outline how RS has been utilized for the generation of wetland inventories. The results of this review paper provide the current state-of-the-art methods and datasets for wetland studies in Canada and will provide direction for future wetland mapping research.},
DOI = {10.3390/rs13204025}
}



@Article{fi13100260,
AUTHOR = {Qiao, Wenxin and Lu, Hao and Lu, Yu and Meng, Lijie and Liu, Yicen},
TITLE = {A Dynamic Service Reconfiguration Method for Satellite–Terrestrial Integrated Networks},
JOURNAL = {Future Internet},
VOLUME = {13},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {260},
URL = {https://www.mdpi.com/1999-5903/13/10/260},
ISSN = {1999-5903},
ABSTRACT = {Satellite–terrestrial integrated networks (STINs) are regarded as a promising solution to meeting the demands of global high-speed seamless network access in the future. Software-defined networking and network function virtualization (SDN/NFV) are two complementary technologies that can be used to ensure that the heterogeneous resources in STINs can be easily managed and deployed. Considering the dual mobility of satellites and ubiquitous users, along with the dynamic requirements of user requests and network resource states, it is challenging to maintain service continuity and high QoE performance in STINs. Thus, we investigate the service migration and reconfiguration scheme, which are of great significance to the guarantee of continuous service provisioning. Specifically, this paper proposes a dynamic service reconfiguration method that can support flexible service configurations on integrated networks, including LEO satellites and ground nodes. We first model the migration cost as an extra delay incurred by service migration and reconfiguration and then formulate the selection processes of the location and migration paths of virtual network functions (VNFs) as an integer linear programming (ILP) optimization problem. Then, we propose a fuzzy logic and quantum genetic algorithm (FQGA) to obtain an approximate optimal solution that can accelerate the solving process efficiently with the benefits of the high-performance computing capacity of QGA. The simulation results validate the effectiveness and improved performance of the scheme proposed in this paper.},
DOI = {10.3390/fi13100260}
}



@Article{rs13204036,
AUTHOR = {Lin, Feng-Cheng and Chuang, Yung-Chung},
TITLE = {Interoperability Study of Data Preprocessing for Deep Learning and High-Resolution Aerial Photographs for Forest and Vegetation Type Identification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4036},
URL = {https://www.mdpi.com/2072-4292/13/20/4036},
ISSN = {2072-4292},
ABSTRACT = {When original aerial photographs are combined with deep learning to classify forest vegetation cover, these photographs are often hindered by the interlaced composition of complex backgrounds and vegetation types as well as the influence of different deep learning calculation processes, resulting in unpredictable training and test results. The purpose of this research is to evaluate (1) data preprocessing, (2) the number of classification targets, and (3) convolutional neural network (CNN) approaches combined with deep learning’s effects on high-resolution aerial photographs to identify forest and vegetation types. Data preprocessing is mainly composed of principal component analysis and content simplification (noise elimination). The number of classification targets is divided into 14 types of forest vegetation that are more complex and difficult to distinguish and seven types of forest vegetation that are simpler. We used CNN approaches to compare three CNN architectures: VGG19, ResNet50, and SegNet. This study found that the models had the best execution efficiency and classification accuracy after data preprocessing using principal component analysis. However, an increase in the number of classification targets significantly reduced the classification accuracy. The algorithm analysis showed that VGG19 achieved the best classification accuracy, but SegNet achieved the best performance and overall stability of relative convergence. This proves that data preprocessing helps identify forest and plant categories in aerial photographs with complex backgrounds. If combined with the appropriate CNN algorithm, these architectures will have great potential to replace high-cost on-site forestland surveys. At the end of this study, a user-friendly classification system for practical application is proposed, and its testing showed good results.},
DOI = {10.3390/rs13204036}
}



@Article{en14206507,
AUTHOR = {G. M. Abdolrasol, Maher and Hannan, Mahammad Abdul and Hussain, S. M. Suhail and Ustun, Taha Selim and Sarker, Mahidur R. and Ker, Pin Jern},
TITLE = {Energy Management Scheduling for Microgrids in the Virtual Power Plant System Using Artificial Neural Networks},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {6507},
URL = {https://www.mdpi.com/1996-1073/14/20/6507},
ISSN = {1996-1073},
ABSTRACT = {This study uses an artificial neural network (ANN) as an intelligent controller for the management and scheduling of a number of microgrids (MGs) in virtual power plants (VPP). Two ANN-based scheduling control approaches are presented: the ANN-based backtracking search algorithm (ANN-BBSA) and ANN-based binary practical swarm optimization (ANN-BPSO) algorithm. Both algorithms provide the optimal schedule for every distribution generation (DG) to limit fuel consumption, reduce CO2 emission, and increase the system efficiency towards smart and economic VPP operation as well as grid decarbonization. Different test scenarios are executed to evaluate the controllers’ robustness and performance under changing system conditions. The test cases are different load curves to evaluate the ANN’s performance on untrained data. The untrained and trained load models used are real-load parameter data recorders in northern parts of Malaysia. The test results are analyzed to investigate the performance of these controllers under varying power system conditions. Additionally, a comparative study is performed to compare their performances with other solutions available in the literature based on several parameters. Results show the superiority of the ANN-based controllers in terms of cost reduction and efficiency.},
DOI = {10.3390/en14206507}
}



@Article{rs13204057,
AUTHOR = {Zhao, Liya and Yang, Qi and Zhao, Qiang and Wu, Jingwei},
TITLE = {Assessing the Long-Term Evolution of Abandoned Salinized Farmland via Temporal Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4057},
URL = {https://www.mdpi.com/2072-4292/13/20/4057},
ISSN = {2072-4292},
ABSTRACT = {Salinization in arid or semiarid regions with water logging limits cropland yield, threatening food security. The highest level of farmland salinization, that is, abandoned salinized farmland, is a tradeoff between inadequate drainage facilities and sustainable farming. The evolution of abandoned salinized farmlands is closely related to the development of cropping systems. However, detecting abandoned salinized farmland using time-series remote sensing data has not been investigated well by previous studies. In this study, a novel approach was proposed to detect the dynamics of abandoned salinized farmland using time-series multispectral and thermal imagery. Thirty-two years of temporal Landsat imagery (from 1988 to 2019) was used to assess the evolution of salinization in Hetao, a two-thousand-year-old irrigation district in northern China. As intermediate variables of the proposed method, the crop-specific planting area was retrieved via its unique temporal vegetation index (VI) pattern, in which the shape-model-fitting technology and the K-means cluster algorithm were used. The desert area was stripped from the clustered non-vegetative area using its distinct features in the thermal band. Subsequently, the abandoned salinized farmland was distinguished from the urban area by the threshold-based saline index (SI). In addition, a regression model between electrical conductance (EC) and SI was established, and the spatial saline degree was evaluated by the SI map in uncropped and unfrozen seasons. The results show that the cropland has constantly been expanding in recent decades (from 4.7 × 105 ha to 7.1 × 105 ha), while the planting area of maize and sunflower has grown and the area of wheat has decreased. Significant desalinization progress was observed in Hetao, where both the area of salt-affected land (salt-free area increased approximately 4 × 105 ha) and the abandoned salinized farmland decreased (reduced from 0.45 × 105 ha to 0.19 × 105 ha). This could be mainly attributed to three reasons: the popularization of water-saving irrigation technology, the construction of artificial drainage facilities, and a shift in cropping patterns. The decrease in irrigation and the increase in drainage have deepened the groundwater table in Hetao, which weakens the salt collection capacity of the abandoned salinized farmland. The results demonstrate the promising possibility of reutilizing abandoned salinized farmland via a leaching campaign where the groundwater table is sufficiently deep to stop salinization.},
DOI = {10.3390/rs13204057}
}



@Article{rs13204065,
AUTHOR = {Yu, Run and Luo, Youqing and Li, Haonan and Yang, Liyuan and Huang, Huaguo and Yu, Linfeng and Ren, Lili},
TITLE = {Three-Dimensional Convolutional Neural Network Model for Early Detection of Pine Wilt Disease Using UAV-Based Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {4065},
URL = {https://www.mdpi.com/2072-4292/13/20/4065},
ISSN = {2072-4292},
ABSTRACT = {As one of the most devastating disasters to pine forests, pine wilt disease (PWD) has caused tremendous ecological and economic losses in China. An effective way to prevent large-scale PWD outbreaks is to detect and remove the damaged pine trees at the early stage of PWD infection. However, early infected pine trees do not show obvious changes in morphology or color in the visible wavelength range, making early detection of PWD tricky. Unmanned aerial vehicle (UAV)-based hyperspectral imagery (HI) has great potential for early detection of PWD. However, the commonly used methods, such as the two-dimensional convolutional neural network (2D-CNN), fail to simultaneously extract and fully utilize the spatial and spectral information, whereas the three-dimensional convolutional neural network (3D-CNN) is able to collect this information from raw hyperspectral data. In this paper, we applied the residual block to 3D-CNN and constructed a 3D-Res CNN model, the performance of which was then compared with that of 3D-CNN, 2D-CNN, and 2D-Res CNN in identifying PWD-infected pine trees from the hyperspectral images. The 3D-Res CNN model outperformed the other models, achieving an overall accuracy (OA) of 88.11% and an accuracy of 72.86% for detecting early infected pine trees (EIPs). Using only 20% of the training samples, the OA and EIP accuracy of 3D-Res CNN can still achieve 81.06% and 51.97%, which is superior to the state-of-the-art method in the early detection of PWD based on hyperspectral images. Collectively, 3D-Res CNN was more accurate and effective in early detection of PWD. In conclusion, 3D-Res CNN is proposed for early detection of PWD in this paper, making the prediction and control of PWD more accurate and effective. This model can also be applied to detect pine trees damaged by other diseases or insect pests in the forest.},
DOI = {10.3390/rs13204065}
}



@Article{electronics10202474,
AUTHOR = {Nguyen, Lanh Van and Phung, Manh Duong and Ha, Quang Phuc},
TITLE = {Iterative Learning Sliding Mode Control for UAV Trajectory Tracking},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {20},
ARTICLE-NUMBER = {2474},
URL = {https://www.mdpi.com/2079-9292/10/20/2474},
ISSN = {2079-9292},
ABSTRACT = {This paper presents a novel iterative learning sliding mode controller (ILSMC) that can be applied to the trajectory tracking of quadrotor unmanned aerial vehicles (UAVs) subject to model uncertainties and external disturbances. Here, the proposed ILSMC is integrated in the outer loop of a controlled system. The control development, conducted in the discrete-time domain, does not require a priori information of the disturbance bound as with conventional SMC techniques. It only involves an equivalent control term for the desired dynamics in the closed loop and an iterative learning term to drive the system state toward the sliding surface to maintain robust performance. By learning from previous iterations, the ILSMC can yield very accurate tracking performance when a sliding mode is induced without control chattering. The design is then applied to the attitude control of a 3DR Solo UAV with a built-in PID controller. The simulation results and experimental validation with real-time data demonstrate the advantages of the proposed control scheme over existing techniques.},
DOI = {10.3390/electronics10202474}
}



@Article{min11101118,
AUTHOR = {Mishra, Amit Kumar},
TITLE = {AI4R2R (AI for Rock to Revenue): A Review of the Applications of AI in Mineral Processing},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {10},
ARTICLE-NUMBER = {1118},
URL = {https://www.mdpi.com/2075-163X/11/10/1118},
ISSN = {2075-163X},
ABSTRACT = {In the last few years, jargon, such as machine learning (ML) and artificial intelligence (AI), have been ubiquitous in both popular science media as well as the academic literature. Many industries have tried the current suite of ML and AI algorithms with various degrees of success. Mineral processing, as an industry, is looking at AI for two reasons. First of all, as with other industries, it is pertinent to know if AI algorithms can be used to enhance productivity. The second reason is specific to the mining industry. Of late, the grade of ores is reducing, and the demand for ethical mining (with as little effect on ecology as possible) is increasing. Thus, mineral processing industries also want to explore the possible use of AI in solving these challenges. In this review paper, first, the challenges in mineral processing that can potentially be solved by AI are presented. Then, some of the most pertinent developments in the domain of ML and AI (applied in the domain of mineral processing) are discussed. Lastly, a top-level modus operandi is presented for a mineral processing industry that might want to explore the possibilities of using AI in its processes. Following are some of the new paradigms added by this review. This review presents a holistic view of the domain of mineral processing with an AI lens. It is also one of the first reviews in this domain to thoroughly discuss the use of AI in ethical, green, and sustainable mineral processing. The AI process proposed in this paper is a comprehensive one. To ensure the relevance to industry, the flow was made agile with the spiral system engineering flow. This is expected to drive rapid and agile investigation of the potential of applying ML and AI in different mineral processing industries.},
DOI = {10.3390/min11101118}
}



