@article{VANWEERDENBURG2019143,
title = {Where to go and what to do: Extracting leisure activity potentials from Web data on urban space},
journal = {Computers, Environment and Urban Systems},
volume = {73},
pages = {143-156},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518302138},
author = {Demi {van Weerdenburg} and Simon Scheider and Benjamin Adams and Bas Spierings and Egbert {van der Zee}},
keywords = {Place affordance, Urban space, Knowledge extraction, City planning, Latent semantics, Multi-label classification},
abstract = {Web data is the most prominent source of information for deciding where to go and what to do. Exploiting this source for geographic analysis, however, does not come without difficulties. First, in recent years, the amount and diversity of available Web information about urban space have exploded, and it is therefore increasingly difficult to overview and exploit. Second, the bulk of information is in an unstructured form which is difficult to process and interpret by computers. Third, semi-structured sources, such as Web rankings, geolocated tags, check-ins, or mobile sensor data, do not fully reflect the more subtle qualities of a place, including the particular functions that make it attractive. In this article, we explore a method to capture leisure activity potentials from Web data on urban space using semantic topic models. We test three supervised multi-label machine learning strategies exploiting geolocated webtexts and place tags to estimate whether a given type of leisure activity is afforded or not. We train and validate these models on a manually curated dataset labeled with leisure ontology classes for the city of Zwolle, and discuss their potential for urban leisure and tourism research and related city policies and planning. We found that multi-label affordance estimation is not straightforward but can be made to work using both official webtexts and user-generated content on a medium semantic level. This opens up new opportunities for data-driven approaches to urban leisure and tourism studies.}
}
@article{POMPIGNA2022100986,
title = {Smart roads: A state of the art of highways innovations in the Smart Age},
journal = {Engineering Science and Technology, an International Journal},
volume = {25},
pages = {100986},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621000872},
author = {Andrea Pompigna and Raffaele Mauro},
keywords = {Smart road, Smart highway, Intelligent transport systems, Connected, Autonomous vehicles},
abstract = {The years we are experiencing are often identified as those of the Age of Smart Technologies. Smart is now a very popular term, with the meaning of clever, intelligent, sharp, quick on the uptake. Its extensive meaning can be grasped if we consider it as an acronym for Self-Monitoring Analysis and Reporting Technology to indicate the essential features of the innovative technologies that characterize today's society in its daily life. Thus, the advent of the Smart Age, which is therefore the era of smart technologies, has heavily characterized and modified many aspects of today's society compared to the past. In this panorama, some arising questions regard transport infrastructure systems and, first of all, road transport. This research proposes a focus on one main issue: how roads fit into this smart revolution? Actually, the paper aims to offer an overview of the smart approach in road engineering by proposing a broad discussion about the current state of innovation in the smart roads field, i.e. the roads of the Smart Age. After defining the key functions of a smart road, the paper reviews some innovative technologies that make these items effective. These are studied in depth both with regard to motorway-type infrastructures and urban roads and intersections, with attention to the various technological aspects and to the benefits perceivable by management, users and the community. The paper, therefore, offers a bird's eye view of this extremely dynamic sector with innovative technologies for a new intelligent and connected mobility, and discusses some of their criticalities and strengths allowing for optimization and development of new transport functions and services, improving energy efficiency and promoting social, economic and environmental sustainability.}
}
@article{FORCEN2020103909,
title = {Co-occurrence of deep convolutional features for image search},
journal = {Image and Vision Computing},
volume = {97},
pages = {103909},
year = {2020},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2020.103909},
url = {https://www.sciencedirect.com/science/article/pii/S026288562030041X},
author = {J.I. Forcen and Miguel Pagola and Edurne Barrenechea and Humberto Bustince},
keywords = {, Image retrieval, Feature aggregation, Pooling},
abstract = {Image search can be tackled using deep features from pre-trained Convolutional Neural Networks (CNN). The feature map from the last convolutional layer of a CNN encodes descriptive information from which a discriminative global descriptor can be obtained. We propose a new representation of co-occurrences from deep convolutional features to extract additional relevant information from this last convolutional layer. Combining this co-occurrence map with the feature map, we achieve an improved image representation. We present two different methods to get the co-occurrence representation, the first one based on direct aggregation of activations, and the second one, based on a trainable co-occurrence representation. The image descriptors derived from our methodology improve the performance in very well-known image retrieval datasets as we prove in the experiments.}
}
@article{LOYBENITEZ2020101847,
title = {Sustainable subway indoor air quality monitoring and fault-tolerant ventilation control using a sparse autoencoder-driven sensor self-validation},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101847},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101847},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719317871},
author = {Jorge Loy-Benitez and Qian Li and KiJeon Nam and ChangKyoo Yoo},
keywords = {Fault-tolerant ventilation control, Sensor self-validation, Sparse autoencoder, Subway transportation systems, IAQ monitoring},
abstract = {Sensors providing measurements for monitoring and control of indoor air quality (IAQ) are key components of the ventilation systems in subway stations. However, faulty sensors due to harsh ambient conditions may deliver incorrect information triggering misinterpretation; causing energy waste or IAQ deterioration. This paper presents a holistic online framework for sensor self-validation in a subway station based on a sparse autoencoder (AE) architecture. The sensor self-validation procedure consists of sensor fault detection, faulty sensor identification, and faulty sensor reconstruction. First, the AE-based detection rate between 44% and 100%. Then, the faulty sensor identification was conducted through an AE-sensor validity index (SVIAE). The faulty sensor reconstruction was conducted by the AE structure and evaluated with several performance metrics. Finally, the sustainability and fault-tolerance aspects of this framework were verified through mathematical modeling of the ventilation system; showing the effects of the faulty and reconstructed sensors on energy consumption and public health.}
}
@article{RAHMAN2020102372,
title = {Data-driven dynamic clustering framework for mitigating the adverse economic impact of Covid-19 lockdown practices},
journal = {Sustainable Cities and Society},
volume = {62},
pages = {102372},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102372},
url = {https://www.sciencedirect.com/science/article/pii/S221067072030593X},
author = {Md Arafatur Rahman and Nafees Zaman and A. Taufiq Asyhari and Fadi Al-Turjman and Md. Zakirul {Alam Bhuiyan} and M.F. Zolkipli},
keywords = {Covid-19, Dynamic clustering, Lockdown, Pandemic},
abstract = {The COVID-19 disease has once again reiterated the impact of pandemics beyond a biomedical event with potential rapid, dramatic, sweeping disruptions to the management, and conduct of everyday life. Not only the rate and pattern of contagion that threaten our sense of healthy living but also the safety measures put in place for containing the spread of the virus may require social distancing. Three different measures to counteract this pandemic situation have emerged, namely: (i) vaccination, (ii) herd immunity development, and (iii) lockdown. As the first measure is not ready at this stage and the second measure is largely considered unreasonable on the account of the gigantic number of fatalities, a vast majority of countries have practiced the third option despite having a potentially immense adverse economic impact. To mitigate such an impact, this paper proposes a data-driven dynamic clustering framework for moderating the adverse economic impact of COVID-19 flare-up. Through an intelligent fusion of healthcare and simulated mobility data, we model lockdown as a clustering problem and design a dynamic clustering algorithm for localized lockdown by taking into account the pandemic, economic and mobility aspects. We then validate the proposed algorithms by conducting extensive simulations using the Malaysian context as a case study. The findings signify the promises of dynamic clustering for lockdown coverage reduction, reduced economic loss, and military unit deployment reduction, as well as assess potential impact of uncooperative civilians on the contamination rate. The outcome of this work is anticipated to pave a way for significantly reducing the severe economic impact of the COVID-19 spreading. Moreover, the idea can be exploited for potentially the next waves of corona virus-related diseases and other upcoming viral life-threatening calamities.}
}
@article{PASHAZADEH201847,
title = {Big data handling mechanisms in the healthcare applications: A comprehensive and systematic literature review},
journal = {Journal of Biomedical Informatics},
volume = {82},
pages = {47-62},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S153204641830056X},
author = {Asma Pashazadeh and Nima Jafari Navimipour},
keywords = {Systematic literature review, Big data, Healthcare, Cloud, Agent, Heuristic},
abstract = {Healthcare provides many services such as diagnosing, treatment, prevention of diseases, illnesses, injuries, and other physical and mental disorders. Large-scale distributed data processing applications in healthcare as a basic concept operates on large amounts of data. Therefore, big data application functions are the main part of healthcare operations, but there was not any comprehensive and systematic survey about studying and evaluating the important techniques in this field. Therefore, this paper aims at providing the comprehensive, detailed, and systematic study of the state-of-the-art mechanisms in the big data related to healthcare applications in five categories, including machine learning, cloud-based, heuristic-based, agent-based, and hybrid mechanisms. Also, this paper displayed a systematic literature review (SLR) of the big data applications in the healthcare literature up to the end of 2016. Initially, 205 papers were identified, but a paper selection process reduced the number of papers to 29 important studies.}
}
@article{FBALBIN20203009,
title = {Predictive analytics on open big data for supporting smart transportation services},
journal = {Procedia Computer Science},
volume = {176},
pages = {3009-3018},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321049},
author = {Paul Patrick {F. Balbin} and Jackson C.R. Barker and Carson K. Leung and Marvin Tran and Riley P. Wall and Alfredo Cuzzocrea},
keywords = {Predictive analytics, open data, Winnipeg open data, big data, transportation data, on-time performance, frequent patterns, software engineering, large-scale systems},
abstract = {In the current era of big data, huge quantities of valuable data, which may be of different levels of veracity, are being generated at a rapid rate. Embedded into these big data are implicit, previously unknown and potentially useful information and valuable knowledge that can be discovered by data science solutions, which apply techniques like data mining. There has been a trend that more and more collections of these big data have been made openly available in science, government and non-profit organizations so that people could collaboratively study and analysis these open big data. In this article, we focus on open big data for public transit because public transit (e.g., bus) as a means of transportation is a vital part of many people’s lives. As time is a precious resource, bus delays could negatively affect commuters’ plans. Unfortunately, they are inevitable. Hence, many existing works focused on predicting bus delays. However, predicting on-time or early buses is also important. For instance, commuters who come to a bus stop on time may still miss their buses if the buses leave early. So, in this article, we examine open big data about bus performance (e.g., early, on-time, and late stops). We analyze the data with frequent pattern mining and make predictions with decision-tree based classification. For illustration, we perform predictive analytics on real-life open big data available on Winnipeg Open Data Portal, about bus performance from Winnipeg Transit. It shows the benefits of predictive analytics on open big data for supporting smart transportation services.}
}
@article{ZIYAN2021105555,
title = {China's self-driving car legislation study},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105555},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105555},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000285},
author = {Chen Ziyan and Liu Shiguo},
keywords = {Self-driving car, Autonomous driving, Road test, Legislative process, Legislative mode, Comparative study},
abstract = {The ongoing developments of artificial intelligence have made the self-driving car industry enter the phase of extensive road testing in recent years. However, in China, research on self-driving car legislation is basically nonexistent. This paper focuses on comprehensively analyzing the self-driving car legislation and testing process based on the latest provisions in both China and the United States and conducts a comparative study. This study gives a detailed analysis of legislation trends in the legal field and provides practical suggestions for promoting the self-driving car industry in China as well as for that in other countries.}
}
@article{ZHANG2020102895,
title = {Estimation of surface water quality parameters based on hyper-spectral and 3D-EEM fluorescence technologies in the Ebinur Lake Watershed, China},
journal = {Physics and Chemistry of the Earth, Parts A/B/C},
volume = {118-119},
pages = {102895},
year = {2020},
note = {Integrated Water Resources Development and Management: Leaving No One Behind for Water Security in Eastern and Southern Africa.},
issn = {1474-7065},
doi = {https://doi.org/10.1016/j.pce.2020.102895},
url = {https://www.sciencedirect.com/science/article/pii/S147470651930138X},
author = {Fei Zhang and Xiaoping Wang and Yun Chen and Muhadaisi Airiken},
keywords = {Back propagation-artificial neural network (BPANN), Three-dimensional excitation-emission matrix (3D-EEM), Fluorescence spectral, Hyper-spectral, Water quality parameters (WQPs)},
abstract = {Water quality research relies on field sampling, which is often very difficult to obtain, especially in arid areas. This study chose the Ebinur Lake Watershed in arid region as a study area. It analyzed 12 water quality parameters (WQPs) and hyper-spectral derived from 48 field samples. Parallel factor analysis (PARAFAC) method was employed to extract four fluorescent components from the fluorescence excitation-emission matrix (EEM) data. Four fluorescence spectral indices (Fn (355), the fluorescence index (FI), the humification index (HIX)) were used to characterize the organic matter. Estimated WQPs were then coupled with hyper-spectral and three-dimensional fluorescence technologies using the Back Propagation-Artificial Neural Network (BPANN) method developed in this study. The main findings are: (1) Higher correlations exist among the reflectance peaks, spectral indices (DI, NDI, RI) and WQPs, which is helpful to improve the accuracy of water quality estimation. (2) There are also high correlations among fluorescent components (peak) (C1, W2, W3, W4, W5 and W7), fluorescence spectral index and some of WQPs. This indicated that fluorescent components and fluorescence indices can be used to accurately monitor WQPs in surface water. (3) The BPANN model has a great potential for estimating WQPs, because of residual predictive deviation (RPD) of estimation model and verify model more than 1.4. These preliminary results have proved that hyper-spectral and fluorescence technologies is a valuable tool for monitoring surface water quality.}
}
@article{ZHAO2021102475,
title = {Human tracking and identification through a millimeter wave radar},
journal = {Ad Hoc Networks},
volume = {116},
pages = {102475},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102475},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521000421},
author = {Peijun Zhao and Chris Xiaoxuan Lu and Jianan Wang and Changhao Chen and Wei Wang and Niki Trigoni and Andrew Markham},
keywords = {Millimeter wave radar, Tracking, Identification, Intruder detection},
abstract = {The key to offering personalized services in smart spaces is knowing where a particular person is with a high degree of accuracy. Visual tracking is one such solution, but concerns arise around the potential leakage of raw video information and many people are not comfortable accepting cameras in their homes or workplaces. We propose a human tracking and identification system (mID11Parts of this work have been previously published in 2019 IEEE 15th International Conference on Distributed Computing in Sensor Systems (DCOSS) Zhao et. al. (2019)[1].) based on millimeter wave radar which has a high tracking accuracy, without being visually compromising. Using a low-cost, commercial, off-the-shelf radar, we first obtain sparse point clouds and form temporally associated trajectories. With the aid of a deep recurrent network, we identify individual users and show how to detect intruders. We evaluate and demonstrate our system, showing median position errors of 0.16 m, identification accuracy of 89% and intruder detection accuracy of 73% for 12 insiders. By increasing observation time from 2 s to 7 s, identification accuracy rises to 99%.}
}
@article{GALICIA2018800,
title = {A novel spark-based multi-step forecasting algorithm for big data time series},
journal = {Information Sciences},
volume = {467},
pages = {800-818},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518304493},
author = {A. Galicia and J.F. Torres and F. Martínez-Álvarez and A. Troncoso},
keywords = {Big data, Scalable, Electricity time series, Forecasting},
abstract = {This paper presents different scalable methods for predicting big time series, namely time series with a high frequency measurement. Methods are also developed to deal with arbitrary prediction horizons. The Apache Spark framework is proposed for distributed computing in order to achieve the scalability of the methods. Prediction methods have been developed using Spark’s MLlib library for machine learning. Since the library does not support multivariate regression, the prediction problem is formulated as h prediction sub-problems, where h is the number of future values to predict, that is, the prediction horizon. Furthermore, different kinds of representative methods have been chosen, such as decision trees, two tree-based ensemble techniques (Gradient-Boosted and Random Forest) and a linear regression method as a reference method for comparisons. Finally, the methodology has been tested in a real time series of electrical demand in Spain, with a time interval of ten minutes between measurements.}
}
@article{LI2021107574,
title = {FeatFlow: Learning geometric features for 3D motion estimation},
journal = {Pattern Recognition},
volume = {111},
pages = {107574},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107574},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320303770},
author = {Qing Li and Cheng Wang and Xin Li and Chenglu Wen},
keywords = {Feature learning, Motion estimation, Point clouds, Scene flow, Scan-matching, Ego-motion},
abstract = {3D motion estimation is an important prerequisite for the autonomous operation of vehicles and robots in dynamic environments. This work presents FeatFlow, a novel neural network architecture to estimate 3D motions from unstructured point clouds. Specifically, we learn deep geometric features to estimate the dense scene flow and the ego-motion of the platform. We build a scene flow estimation pipeline by an encoder-decoder architecture which comprises three novel modules: feature extractor, motion embedder, and flow decoder. By using a point-score layer to assign scores to the extracted features in a learning procedure, the feature extractor effectively extracts keypoints and features that are most significant for estimating the relative transformation between two consecutive point clouds. The whole model adaptively learns the required robust descriptors to represent a variety of point motions at the object or scene level. We evaluated our approach on synthetic data from FlyingThings3D, and real-world LiDAR scans from KITTI and Oxford RobotCar. Our network successfully generalizes to datasets with different patterns, outperforming various baselines and achieving state-of-the-art performance.}
}
@article{NIFORATOS201754,
title = {Understanding the potential of human–machine crowdsourcing for weather data},
journal = {International Journal of Human-Computer Studies},
volume = {102},
pages = {54-68},
year = {2017},
note = {Special Issue on Mobile and Situated Crowdsourcing},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1071581916301343},
author = {Evangelos Niforatos and Athanasios Vourvopoulos and Marc Langheinrich},
keywords = {Sensor networks, Smart cities, Crowdsourcing, Mobile sensing},
abstract = {Reliable weather estimation traditionally requires a dense network of meteorological measurement stations. The concept of participatory sensing promises to alleviate this requirement by crowdsourcing weather data from an ideally very large set of participating users instead. Participation may involve nothing more than downloading a corresponding app to enable the collection of such data, given that modern smartphones contain a plethora of weather-related sensors. To understand the potential of participatory sensing for weather estimation, and how humans can be put “in the loop” to further improve such sensing, we created Atmos – a crowdsourcing weather app that not only periodically samples smartphones’ sensors for weather measurements, but also allows users to enter their own estimates of both current and future weather conditions. We present the results of a 32-month public deployment of Atmos on the Google Play Store, showing that a combination of both types of “sensing” results in accurate temperature estimates, featuring an average error rate of 2.7°C, whereas when using only user inputs, the average error rate drops to 1.86°C.}
}
@article{EREN2022103434,
title = {Fuzzy-based GIS approach with new MCDM method for bike-sharing station site selection according to land-use types},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103434},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103434},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007071},
author = {Ezgi Eren and Burak Yiğit Katanalp},
keywords = {Bike-Sharing, Site Selection, Geographical Information System, Fuzzy Logic, MCDM},
abstract = {Bike-Sharing Systems (BSSs) support urban mobility by strengthening the scope of public transport networks, which may be insufficient in smart cities. This paper aims to present a hybrid approach that includes a Fuzzy Logic (FL)-based Geographic Information System (GIS), the Analytic Hierarchy Process (AHP), the VIse Kriterijumska Optimizacija I Kompromisno Resenje (VIKOR) method, and the Psychometric-VIKOR method for the problem of the selection of BSS station sites depending on transportation and recreational land uses. The FL approach was included in the GIS analysis in order to characterize the uncertainty of a potential passenger's tendency to start a trip at a station within an accessible distance from the urban facilities. By combining fuzzy-based GIS analysis with the AHP method, a spatial analysis was performed for the deployment of BSS stations. The VIKOR and Psychometric-VIKOR methods were used to evaluate the performance of current and alternative locations of BSS station. This study is the first known application of the Psychometric-VIKOR method for the problem of the selection of BSS locations. The innovative hybrid approach can be used by authorities as a useful tool in solving decision-making problems that involve uncertainty in future urban investments.}
}
@article{QI20213,
title = {Enabling technologies and tools for digital twin},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {3-21},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S027861251930086X},
author = {Qinglin Qi and Fei Tao and Tianliang Hu and Nabil Anwer and Ang Liu and Yongli Wei and Lihui Wang and A.Y.C. Nee},
keywords = {Digital twin, Five-dimension model, Enabling technologies, Enabling tools},
abstract = {Digital twin is revolutionizing industry. Fired by sensor updates and history data, the sophisticated models can mirror almost every facet of a product, process or service. In the future, everything in the physical world would be replicated in the digital space through digital twin technology. As a cutting-edge technology, digital twin has received a lot of attention. However, digital twin is far from realizing their potential, which is a complex system and long-drawn process. Researchers must model all the different parts of the objects or systems. Varied types of data needed to be collected and merged. Many researchers and participators in engineering are not clear which technologies and tools should be used. 5-dimension digital twin model provides reference guidance for understanding and implementing digital twin. From the perspective of 5-dimension digital twin model, this paper tries to investigate and summarize the frequently-used enabling technologies and tools for digital twin to provide technologies and tools references for the applications of digital twin in the future.}
}
@article{ALDOGAN2017311,
title = {A comparison study on active learning integrated ensemble approaches in sentiment analysis},
journal = {Computers & Electrical Engineering},
volume = {57},
pages = {311-323},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S004579061630773X},
author = {Deniz Aldoğan and Yusuf Yaslan},
keywords = {Active learning, Ensemble learning, Sentiment analysis, Machine learning, Artificial intelligence},
abstract = {One of the most challenging problems of sentiment analysis on social media is that labelling huge amounts of instances can be very expensive. Active learning has been proposed to overcome this problem and to provide means for choosing the most useful training instances. In this study, we introduce active learning to a framework which is comprised of most popular base and ensemble approaches for sentiment analysis. In addition, the implemented framework contains two ensemble approaches, i.e. a probabilistic algorithm and a derived version of Behavior Knowledge Space (BKS) algorithm. The Shannon Entropy approach was utilized for choosing among training data during active learning process and it was compared with maximum disagreement method and random selection of instances. It was observed that the former method causes better accuracies in less number of iterations. The above methods were tested on Cornell movie review dataset and a popular multi-domain product review dataset.}
}
@article{SARDUY2016187,
title = {Linear and non-linear methods for prediction of peak load at University of São Paulo},
journal = {Measurement},
volume = {78},
pages = {187-201},
year = {2016},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2015.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0263224115005370},
author = {Julio R. Gómez Sarduy and Katia Gregio {Di Santo} and Marco Antonio Saidel},
keywords = {Peak load prediction, Modeling, Neural network, Neuro-fuzzy system},
abstract = {Predicting the peak load contributes to the enhancement of energy management. This paper presents some models to forecast the peak load of a campus of the University of São Paulo, aiming to choose the best one for generalization. The developed models were linear and non-linear, respectively, linear regression and Artificial Neural Networks and Adaptive Neural Networks Inference Systems. The data used was power demand, weather variables and calendar data, which were treated and normalized. For non-linear models, differentially of other researches, the goal of this work is the used of only one network for forecast model from the introduction of a new variable that differentiates the type of day, if is weekday or not. The results obtained reports a good coincidence between the predicted and real peak load for all developed models, however the accuracy is better for non-linear models, mainly for Adaptive Neural Networks Inference Systems.}
}
@article{KNAUSS201685,
title = {ACon: A learning-based approach to deal with uncertainty in contextual requirements at runtime},
journal = {Information and Software Technology},
volume = {70},
pages = {85-99},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001676},
author = {Alessia Knauss and Daniela Damian and Xavier Franch and Angela Rook and Hausi A. Müller and Alex Thomo},
keywords = {Requirements engineering, Self-adaptive systems, Contextual requirements, Operationalization, Machine learning},
abstract = {Context: Runtime uncertainty such as unpredictable operational environment and failure of sensors that gather environmental data is a well-known challenge for adaptive systems. Objective: To execute requirements that depend on context correctly, the system needs up-to-date knowledge about the context relevant to such requirements. Techniques to cope with uncertainty in contextual requirements are currently underrepresented. In this paper we present ACon (Adaptation of Contextual requirements), a data-mining approach to deal with runtime uncertainty affecting contextual requirements. Method: ACon uses feedback loops to maintain up-to-date knowledge about contextual requirements based on current context information in which contextual requirements are valid at runtime. Upon detecting that contextual requirements are affected by runtime uncertainty, ACon analyses and mines contextual data, to (re-)operationalize context and therefore update the information about contextual requirements. Results: We evaluate ACon in an empirical study of an activity scheduling system used by a crew of 4 rowers in a wild and unpredictable environment using a complex monitoring infrastructure. Our study focused on evaluating the data mining part of ACon and analysed the sensor data collected onboard from 46 sensors and 90,748 measurements per sensor. Conclusion: ACon is an important step in dealing with uncertainty affecting contextual requirements at runtime while considering end-user interaction. ACon supports systems in analysing the environment to adapt contextual requirements and complements existing requirements monitoring approaches by keeping the requirements monitoring specification up-to-date. Consequently, it avoids manual analysis that is usually costly in today’s complex system environments.}
}
@article{BOJE2020103179,
title = {Towards a semantic Construction Digital Twin: Directions for future research},
journal = {Automation in Construction},
volume = {114},
pages = {103179},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103179},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519314785},
author = {Calin Boje and Annie Guerriero and Sylvain Kubicki and Yacine Rezgui},
keywords = {Digital Twin, BIM, Review, Industry Foundation Classes (IFC), Internet of Things (IoT), Framework, Artificial Intelligence (AI), Big data, Construction safety, Construction site},
abstract = {As the Architecture, Engineering and Construction sector is embracing the digital age, the processes involved in the design, construction and operation of built assets are more and more influenced by technologies dealing with value-added monitoring of data from sensor networks, management of this data in secure and resilient storage systems underpinned by semantic models, as well as the simulation and optimisation of engineering systems. Aside from enhancing the efficiency of the value chain, such information-intensive models and associated technologies play a decisive role in minimising the lifecycle impacts of our buildings. While Building Information Modelling provides procedures, technologies and data schemas enabling a standardised semantic representation of building components and systems, the concept of a Digital Twin conveys a more holistic socio-technical and process-oriented characterisation of the complex artefacts involved by leveraging the synchronicity of the cyber-physical bi-directional data flows. Moreover, BIM lacks semantic completeness in areas such as control systems, including sensor networks, social systems, and urban artefacts beyond the scope of buildings, thus requiring a holistic, scalable semantic approach that factors in dynamic data at different levels. The paper reviews the multi-faceted applications of BIM during the construction stage and highlights limits and requirements, paving the way to the concept of a Construction Digital Twin. A definition of such a concept is then given, described in terms of underpinning research themes, while elaborating on areas for future research.}
}
@article{JIANG2021123706,
title = {GPS data in urban online ride-hailing: The technical potential analysis of demand prediction model},
journal = {Journal of Cleaner Production},
volume = {279},
pages = {123706},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123706},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620337513},
author = {Wenxiao Jiang and Haoran Zhang and Yin Long and Jinyu Chen and Yi Sui and Xuan Song and Ryosuke Shibasaki and Qing Yu},
keywords = {Online ride-hailing, Technical potential analysis, Dispatching system simulation, Convolutional long short-term memory, Data mining},
abstract = {Online ride-hailing as an innovative travel mode becomes increasingly popular in cities around the world. To improve the efficiency of dispatching system of ride-hailing, many ride-hailing demand prediction models based on deep learning architecture have been proposed to reduce the gross travel distance between drivers and passengers. However, in most of these prediction models, only the error metrics are used for performance evaluation. There is scarce evidence on how much the demand prediction model reduces the unnecessary travel distance in reality. In this study, a multi-scenario-based method is proposed to evaluate technical potentials of the ride-hailing demand prediction model in ride-hailing dispatching system simulation. The ride-hailing dispatching is simulated in three scenarios: traditional dispatching system, prediction model-based dispatching system, and perfect prediction model-based dispatching system. One-month data of Didi Express service provided by Didi Chuxing GAIA Initiative in Chengdu is employed to support the simulation. Two terms, empty distance and relative performance, are introduced as the criteria of prediction model performance measurement. Simulation results reveal that the total empty distance reduced 1,164 km per day by using the prediction model compared with the traditional dispatching system. The relative performance is only 58.9% compared with the perfect prediction model-based dispatching system.}
}
@article{LYU2021117615,
title = {Artificial Intelligence and emerging digital technologies in the energy sector},
journal = {Applied Energy},
volume = {303},
pages = {117615},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117615},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921009843},
author = {Wenjing Lyu and Jin Liu},
keywords = {Artificial Intelligence, Digital technologies, Digitalization, General Purpose Technology, Technology adoption, The wage premium},
abstract = {Digitalization is an increasingly important direction of energy innovation moving forward. Nevertheless, which emerging digital technology is more crucial during the energy sector transformation stays underexplored. Using a near-universe of online job postings data collected between 2010 and 2019, we show that among the emerging digital technologies (i.e., Artificial Intelligence, Big data, Internet of Things, Robotics, Blockchain technology, and Cloud computing), Artificial Intelligence is the most widely adopted in the energy sector. We further calculate a systematic measure of the emerging digital technology intensity in job skill requirements and show that Artificial Intelligence proves to be the most valuable in the energy sector, either from the employee’s or the employer’s perspective. Particularly, Artificial Intelligence brings the highest wage premium to the average wage of the adopted energy firm and the local labor market. Meanwhile, Artificial Intelligence contributes the most to energy firms’ performance. Our findings suggest that energy firms should intentionally increase the requirement for Artificial Intelligence in hiring new talents. Our findings also indicate that major energy firms should take the leading role in adopting the emerging digital technologies to enjoy the predominant advantage as early as possible.}
}
@article{AHMAD2020118477,
title = {Novel deep supervised ML models with feature selection approach for large-scale utilities and buildings short and medium-term load requirement forecasts},
journal = {Energy},
volume = {209},
pages = {118477},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.118477},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220315851},
author = {Tanveer Ahmad and Hongcai Zhang},
keywords = {Deep supervised learning models, Short & medium-term forecasting, Utilities and building load, Time series, Multiple feature selection},
abstract = {Accurate energy analyses and forecasts not only impact a nation’s energy stability/security and environment but also provide policymakers with a reliable framework for decision-making. The load forecast of buildings and electricity companies for the arrangement of risk/low-cost demand and supply resources that fulfill future government commitments, plans consumer targets, and respond appropriately for stockholders. This study introduces two novels deep supervised machine learning models, including: (i) fit Gaussian Kernel regression model with random feature expansion (RFEM-GKR); and (ii) non-parametric based k-NN (NPK-NNM) models for buildings and the utility companies load demand forecasts with a higher predictive potential, speed, and accuracy. Five-fold cross-validation is used to reduce prediction errors and to improve network generalization. Real-load consumption data from two different locations (utility company and office building) are used to analyze and validate the proposed models. Each location data is further divided into six different feature selection (MFS) states. Each state is composed of various (16, 19, 17, 09, 16, and 13) types of real-time energy consumption and climatic feature variables. The energy consumption behaviors are then analyzed in terms of the feature significance applied with 5 min, 30 min, and 1-h of time-based on short-, and medium-term intervals. Eleven distance metrics used to measure the number of the neighboring object and the number of objective functions of the model network for accuracy. With less computational time, higher precision, and high penetration levels of multiple input feature variables, the method RFEM-GKR is proven superior. Therefore, because of its high accuracy and stability, the proposed model can be a successful tool to predict energy consumption.}
}
@article{AMARAL201957,
title = {Type-1 and singleton fuzzy logic system trained by a fast scaled conjugate gradient methods for dealing with binary classification problems},
journal = {Neurocomputing},
volume = {355},
pages = {57-70},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219306083},
author = {Renan P. Finotti Amaral and Moisés V. Ribeiro and Eduardo P. {de Aguiar}},
keywords = {Fuzzy logic system, Classification, Scaled conjugate gradient, Hessian-free, Computational complexity},
abstract = {This work introduces the type-1 and singleton fuzzy logic system trained by scaled conjugate gradient method and its usage for binary classification problems. Aiming to improve the performance of the training procedure, we propose the multiplication of the Hessian matrix by the directional vector using the so-called differential operator R{·}, which results in another proposal for training the aforementioned fuzzy logic system. In order to evaluate the proposals, performance analyses based on well-known data sets provided by UCI Machine Learning Repository and Knowledge Extraction based on Evolutionary Learning Repository together with well-established metrics are detailed. The numerical results show that the proposals achieve improvements in comparison with others gradient based training methods applied to the type-1 fuzzy logic system present in the literature. These improvements regard to the fast convergence speed under the constraint over the number of epochs during the training phase.}
}
@article{STAVROPOULOS20211656,
title = {Quality Monitoring of Manufacturing Processes based on Full Data Utilization},
journal = {Procedia CIRP},
volume = {104},
pages = {1656-1661},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.279},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101177X},
author = {Panagiotis Stavropoulos and Alexios Papacharalampopoulos and Kyriakos Sabatakakis and Dimitris Mourtzis},
keywords = {Manufacturing Process, Quality Assessment, Big Data},
abstract = {Monitoring of manufacturing processes through dedicated sensors can generate large volumes of data due to high sampling rates and concurrent retrieval of multiple values. Consequently, proper function of engaged systems towards real-time identification of defects may be hindered. This study investigates feasibility of welding processes quality monitoring under the prism of full data utilization in parallel to maintaining the manufacturing efficiency of the whole process-monitoring system. This is performed through analyzing computational times, storage needs and the cost of the alternative scenarios defined by the policy-hardware-software combination. A roadmap is created towards selecting the most appropriate quality monitoring workflow.}
}
@article{SHIRAZI2019230,
title = {A multiagent design for self-healing in electric power distribution systems},
journal = {Electric Power Systems Research},
volume = {171},
pages = {230-239},
year = {2019},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2019.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0378779619300896},
author = {E. Shirazi and S. Jadid},
keywords = {Distributed control, Multi agent systems, Self-healing, Service restoration, Smart grid, Distributed generation, Neural network},
abstract = {The out-of-service area after fault should berestored by changing the distribution system configuration by means of switching actions on the feeders. This paper proposes an agent-based approach for service restoration in smart distribution systems with distributed generations (DG). The proposed multi agent has four different types of agents: feeder agents, zone agents, switch agents and DG agents. The agents can communicate and cooperate with each other in order to supply services to out-of-service customers. An artificial neural network has been considered within DG agent to predict the DG generation. The restoration plan is built based on local data considering system conditions, operational constraints and fault location. Different loading conditions have been considered under different scenarios and the result of proposed multi agent approach for each scenario with and without DG have been compared. Simulation results show the efficiency of proposed agent architecture.}
}
@article{SHI2022100228,
title = {mPose: Environment- and subject-agnostic 3D skeleton posture reconstruction leveraging a single mmWave device},
journal = {Smart Health},
volume = {23},
pages = {100228},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100228},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000489},
author = {Cong Shi and Li Lu and Jian Liu and Yan Wang and Yingying Chen and Jiadi Yu},
keywords = {3D skeleton posture reconstruction, MmWave},
abstract = {Human skeleton posture reconstruction is an essential component for human–computer interactions (HCI) in various application domains. Traditional approaches usually rely on either cameras or on-body sensors, which induce privacy concerns or inconvenient practical setups. To address these practical concerns, this paper proposes a low-cost contactless skeleton posture reconstruction system, mPose, which can reconstruct a user’s 3D skeleton postures using a single mmWave device. mPose does not require the user to wear any sensors and can enable a broad range of emerging mobile applications (e.g., VR gaming and pervasive user input) via mmWave-5G ready Internet of Things (IoT) devices. Particularly, the system extracts multi-dimensional spatial information from mmWave signals which characterizes the skeleton postures in a 3D space. To mitigate the impacts of environmental changes, mPose dynamically detects the user location and extracts spatial features from the mmWave signals reflected only from the user. Furthermore, we develop a deep regression method with a domain discriminator to learn a mapping between the spatial features and the joint coordinates of human body while removing subject-specific characteristics, realizing robust posture reconstruction across users. Extensive experiments, involving 17 representative body postures, 7 subjects, and 3 indoor environments, show that mPose outperforms contemporary state-of-the-art RF-based solutions with a lower average joint error of only ∼30 mm, while achieving transferability across environments and subjects at the same time.}
}
@article{BAIDYA2021102243,
title = {Reviewing the opportunities, challenges, and future directions for the digitalization of energy},
journal = {Energy Research & Social Science},
volume = {81},
pages = {102243},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102243},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621003364},
author = {Sanghita Baidya and Vidyasagar Potdar and Partha {Pratim Ray} and Champa Nandi},
keywords = {Blockchain, IoT, Smart grid, Smart energy, Energy monitoring},
abstract = {Smart grids are the backbone of existing energy production and supply scenario in today’s society. Gradual increasing demand of energy has certainly emphasized the significance and provenance of reliability and predictive aspects over smart grids. Conventional smart grids under perform in terms of self-aware behavior, especially in delay tolerance, energy requirement, and dissemination of monitoring notions. Highly distributed geographical distribution sometimes cause problems for smart grids to provide necessary services to both the consumers and prosumers. Internet of Things (IoT) is thus integrated with smart grids to facilitate distributed monitoring services for smooth running of the smart grid. Although, IoT has remarkably supported smart grids to perform smarter than ever, it lacks in security, decentralization, transparency, and trust-less approaches. Thus, blockchain is envisaged to leverage minimizing such gaps and to pave new horizon in the blockchain-IoT enabled smart grid monitoring. In this paper, we investigate how blockchain and IoT together can improve existing smart grid ecosystem toward facilitation of better monitoring services. We do this via a Systematic Literature Review. Firstly, we present preliminaries behind the study, followed by in-depth review of different domains of IoT-based smart grid monitoring. Next, we discuss various attributes of blockchain-IoT derived smart grid management schemes. Then, we discuss possible opportunities and benefits of using blockchain with respect to blockchain-IoT based smart grid monitoring. Finally, we illustrate open research challenges and future directions in the aforementioned aspects. The article concludes that with certain changes in current blockchain technology, it can surely encompass the direction of enhanced monitoring of IoT based smart grid.}
}
@article{IQBAL2021101796,
title = {From luxury to necessity: Progress of touchless interaction technology},
journal = {Technology in Society},
volume = {67},
pages = {101796},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101796},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21002712},
author = {Muhammad Zahid Iqbal and Abraham G. Campbell},
keywords = {Touchless technology, Gestures, Touchless interaction, Contactless, Human computer interaction, Zero touch, Zero UI},
abstract = {Touchless Technology is facilitating the move to Zero User Interface(UI) propelled by the COVID-19 pandemic which has accelerated the use of this technology due to hygiene requirements. Zero UI can be defined as a controlled interface that enables user interaction with technology through voice, gestures, hand interaction, eye tracking, and biometrics such as facial recognition and contactless fingerprints. Smart devices, IoT sensors, smart appliances, smart TVs, smart assistants and consumer robotics are predominant examples of devices in which Zero UI is becoming increasingly adopted. These control interfaces include natural interaction modes such as voice or gestures. Touchscreens and shared devices such as kiosks, self-service counters and interactive displays are present in our everyday lives. Each of these interactions however is a concern for consumers in a post-COVID-19 world where hygiene is of utmost importance. The one-stop solution to hygienic interactions includes touchless technology such as voice control, remote mobile screen take over, biometric, and gesture control as Zero User interfaces. With the breakthroughs in image recognition and natural language processing, powered by advanced computer vision and machine learning, “Zero UI” is becoming a new normal. This paper is focusing on the progress of the touchless interaction technology during the COVID-19 pandemic, which actually accelerated development in this concept and moved it from being a luxury to a life necessity.}
}
@article{MUALLA2022103573,
title = {The quest of parsimonious XAI: A human-agent architecture for explanation formulation},
journal = {Artificial Intelligence},
volume = {302},
pages = {103573},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103573},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001247},
author = {Yazan Mualla and Igor Tchappi and Timotheus Kampik and Amro Najjar and Davide Calvaresi and Abdeljalil Abbas-Turki and Stéphane Galland and Christophe Nicolle},
keywords = {Explainable artificial intelligence, Human-computer interaction, Multi-agent systems, Empirical user studies, Statistical testing},
abstract = {With the widespread use of Artificial Intelligence (AI), understanding the behavior of intelligent agents and robots is crucial to guarantee successful human-agent collaboration since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system. However, providing overwhelming or unnecessary information may also confuse the users and cause failure. For these reasons, parsimony has been outlined as one of the key features allowing successful human-agent interaction with parsimonious explanation defined as the simplest explanation (i.e. least complex) that describes the situation adequately (i.e. descriptive adequacy). While parsimony is receiving growing attention in the literature, most of the works are carried out on the conceptual front. This paper proposes a mechanism for parsimonious eXplainable AI (XAI). In particular, it introduces the process of explanation formulation and proposes HAExA, a human-agent explainability architecture allowing to make it operational for remote robots. To provide parsimonious explanations, HAExA relies on both contrastive explanations and explanation filtering. To evaluate the proposed architecture, several research hypotheses are investigated in an empirical user study that relies on well-established XAI metrics to estimate how trustworthy and satisfactory the explanations provided by HAExA are. The results are analyzed using parametric and non-parametric statistical testing.}
}
@article{JIAO2021106227,
title = {Emerging artificial intelligence in piezoelectric and triboelectric nanogenerators},
journal = {Nano Energy},
volume = {88},
pages = {106227},
year = {2021},
issn = {2211-2855},
doi = {https://doi.org/10.1016/j.nanoen.2021.106227},
url = {https://www.sciencedirect.com/science/article/pii/S2211285521004833},
author = {Pengcheng Jiao},
keywords = {Artificial intelligence (AI), Piezoelectric nanogenerators (PENG), Triboelectric nanogenerators (TENG)},
abstract = {Piezoelectric nanogenerators (PENG) and triboelectric nanogenerators (TENG) have opened an exciting venue to sustainably harvest electrical energy from the environments, which have led to multifunctional applications in different fields. More recently, a paradigm shift has directed to the emerging artificial intelligence (AI) in PENG and TENG, aiming to address the challenges of the nanogenerators in analysis, design, fabrication, and application. AI-PENG and AI-TENG are envisioned to enhance and optimize the mechanical-to-electrical performance of the nanogenerators to a favorable behavior. However, an overview on the topic of AI-PENG and AI-TENG has not yet been exploited in the literature. In this review article, we showcase the recent progress of PENG and TENG and discuss the future trends of AI-enhanced nanogenerators with desirable electrical performance, i.e., using AI-enabled design models as a viable tool to design, predict, and optimize the structures and materials of PENG and TENG. This topical review explains why the nanogenerators are extensively considered as one of the promising energy solutions and are especially suitable for certain applications in engineering and life science, how to surpass the limitations of PENG and TENG by AI-based structural design and material discovery, and what technological avenues that AI-PENG and AI-TENG may provide for green energy in future innovations.}
}
@article{HEWA2021102857,
title = {Survey on blockchain based smart contracts: Applications, opportunities and challenges},
journal = {Journal of Network and Computer Applications},
volume = {177},
pages = {102857},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102857},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303234},
author = {Tharaka Hewa and Mika Ylianttila and Madhusanka Liyanage},
keywords = {Blockchain, Smart contracts, Applications, DLT, Hyperledger Fabric, Ethereum, Corda, Stellar},
abstract = {Blockchain is one of the disruptive technical innovation in the recent computing paradigm. Many applications already notoriously hard and complex are fortunate to ameliorate the service with the blessings of blockchain and smart contracts. The decentralized and autonomous execution with in-built transparency of blockchain based smart contracts revolutionize most of the applications with optimum and effective functionality. The paper explores the significant applications which already benefited from the smart contracts. We also highlight the future potential of the blockchain based smart contracts in these applications perspective.}
}
@article{LI2019117,
title = {A price decision approach for multiple multi-energy-supply microgrids considering demand response},
journal = {Energy},
volume = {167},
pages = {117-135},
year = {2019},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2018.10.189},
url = {https://www.sciencedirect.com/science/article/pii/S0360544218321868},
author = {Bei Li and Robin Roche and Damien Paire and Abdellatif Miraoui},
keywords = {Price, Multi-energy-system, Hydrogen storage system, Demand response, Optimization, Neural network},
abstract = {Multi-energy-supply microgrids covering different types of demands are expected to play an important role in smart grids. Local generation, energy storage systems, and renewable energy sources can form load service entities, which can provide ancillary services to the utility grid and to consumers. On the other hand, microgrids can also sell energy to load service entities or the utility grid to obtain profits. But how the load service entities can decide the electricity selling price to multiple microgrids, and how the microgrids can decide the electricity selling price to load service entities are problems. In this paper, we present a price decision method for multiple microgrids considering demand response. Mixed integer linear programming is used to control the operation of each microgrid, and is also used to operate the load service entities. A genetic algorithm is used to search for the best price for each microgrid and the load service entities. The combined method is deployed in a decentralized way, namely, each microgrid runs its own operation problem. The simulation results show that the new searched price works better than a basic time of use price, which can reduce the operation cost of the whole system. The searching method is compared with a method based on the Cournot model. At last, a large system is tested, in which 4 load service entities, 16 microgrids and an IEEE30-node network are considered. In order to reduce the searching time, a neural network model is presented to estimate the operation of the whole system. Based on the neural network model, the prices are obtained, and the results show that the prices based on neural network model are better than with the time of use price.}
}
@article{ASADZADEH2020100475,
title = {Information technology in emergency management of COVID-19 outbreak},
journal = {Informatics in Medicine Unlocked},
volume = {21},
pages = {100475},
year = {2020},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100475},
url = {https://www.sciencedirect.com/science/article/pii/S2352914820306262},
author = {Afsoon Asadzadeh and Saba Pakkhoo and Mahsa Mirzaei Saeidabad and Hero Khezri and Reza Ferdousi},
keywords = {Outbreak, COVID-19, Emergency management, Information technology, Disaster, Epidemic},
abstract = {Emergency management of the emerging infectious disease outbreak is critical for public health threats. Currently, control of the COVID-19 outbreak is an international concern and has become a crucial challenge in many countries. This article reviews significant information technologyIT) applications in emergency management of COVID-19 by considering the prevention/mitigation, preparedness, response, and recovery phases of the crisis. This review was conducted using MEDLINE PubMed), Embase, IEEE, and Google Scholar. Expert opinions were collected to show existence gaps, useful technologies for each phase of emergency management, and future direction. Results indicated that various IT-based systems such as surveillance systems, artificial intelligence, computational methods, Internet of things, remote sensing sensor, online service, and GIS geographic information system) could have different outbreak management applications, especially in response phases. Information technology was applied in several aspects, such as increasing the accuracy of diagnosis, early detection, ensuring healthcare providers’ safety, decreasing workload, saving time and cost, and drug discovery. We categorized these applications into four core topics, including diagnosis and prediction, treatment, protection, and management goals, which were confirmed by five experts. Without applying IT, the control and management of the crisis could be difficult on a large scale. For reducing and improving the hazard effect of disaster situations, the role of IT is inevitable. In addition to the response phase, communities should be considered to use IT capabilities in prevention, preparedness, and recovery phases. It is expected that IT will have an influential role in the recovery phase of COVID-19. Providing IT infrastructure and financial support by the governments should be more considered in facilitating IT capabilities.}
}
@article{DAUD2021798,
title = {Finding rising stars through hot topics detection},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {798-813},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329903},
author = {Ali Daud and Faizan Abbas and Tehmina Amjad and Abdulrahman A. Alshdadi and Jalal S. Alowibdi},
keywords = {Hot topics, Rising stars, Bibliometric network, Semantics, Co-author networks, Citation networks},
abstract = {Topic modeling methods have usually been applied in the past to identify the research interests of researchers. Observing the scientific growth, the trending topics can be identified as Stable, Hot, or Cold. Finding rising stars (junior researchers, who are at the start of their career) from a bibliometric network is a challenging task, specifically if the researchers have an interest in multiple sub-domains or are working on diverse topics. Existing methods for finding rising stars explore the co-author networks or citation networks, and ignore the textual content, which may help in finding rising stars through hot topics detection over time. A publication contributing to a hot topic can be an indication that the author of that publication may be a rising star and can become an expert in that domain in the future. This study proposes the Hot Topics Rising Star Rank (HTRS-Rank) method for finding rising stars by detecting hot topics. HTRS-Rank finds the junior scholars, who contribute to hot topics at the start of their career and ranks them based on the presence of hot topics in their publications. AMiner five years dataset ranging from 2005–2009 is selected for experimentation. Top 10 researchers are considered to measure the association strength using rank correlation among HTRS-Rank and baseline methods. Experimental results show the efficiency of HTRS-Rank in comparison to the baseline methods. The proposed HTRS Rank (TF–IDF) provides low standard deviation for productivity, citations and sociality as compared to baseline methods for more social and highly cited authors. It is identified that HTRS-Rank (WordNet) emphasizes the semantic similarity of two sentences, whereas HTRS-Rank (TF–IDF) scheme emphasizes the uniqueness or importance of each term, therefore TF–IDF approach performs better than WordNet approach due to having higher correlation with StarRank and WMIRank.}
}
@article{ABOUTORAB2021102984,
title = {A survey on the suitability of risk identification techniques in the current networked environment},
journal = {Journal of Network and Computer Applications},
volume = {178},
pages = {102984},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.102984},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000114},
author = {Hamed Aboutorab and Omar K. Hussain and Morteza Saberi and Farookh Khadeer Hussain and Elizabeth Chang},
keywords = {Networked environment, External events, Techniques for risk identification},
abstract = {Risk management is a necessary consideration in any decision-making activity. In global supply chains that are increasingly becoming internetworked and data-centric, risk management assists risk managers in making smart decisions to achieve their desired outcomes. Because of its importance, there are many studies in the literature on supply chain risk management. However, most of these are from a managerial perspective, and very few provide a technical perspective by surveying the techniques that have been used for risk management and their effectiveness. In this paper, we focus on the activity of risk identification and assess the suitability of the techniques used for this process to be effective in the current networked supply chain environment. We do this by first surveying the domain-independent risk identification techniques proposed between 1980 and 2020 and study the type of technique they use, classifying them according to the proposed taxonomy and then evaluating them to determine whether they meet the requirements of identifying risks in the networked environment. We then survey the techniques used for supply chain risk identification between 1980 and 2020 and determine their suitability along with their limitations in meeting the requirements of identifying risks in the networked supply chain environment. Based on the identified gaps, we then present different solution considerations to inform supply chain risk managers of the improvements needed to be made in the process of risk identification to meet the requirements of networked global supply chains.}
}
@article{KIM2021120972,
title = {How to develop data-driven technology roadmaps:The integration of topic modeling and link prediction},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120972},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120972},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004042},
author = {Junhan Kim and Youngjung Geum},
keywords = {Technology roadmap, Topic model, Latent dirichlet allocation, LDA, Link prediction, Data-analytics},
abstract = {Technology roadmaps have been used as an important tool in strategic technology planning, due to their strong advantages in linking technologies and markets. With the rise of big data analytics, several studies have been suggested regarding data-driven technology roadmapping. However, literatures on providing a systematic method for developing data-driven technology roadmaps is surprisingly sparse. In response, this study suggests a systematic and concrete framework to develop data-driven technology roadmaps. The data-driven roadmapping is consist of three phase: layer mapping, contents mapping, and opportunity finding. The first phase, layer mapping, deals with identifying sub-layers for the technology roadmap using topic modeling. Then, contents mapping is conducted using the keyword network analysis. Third, opportunity finding is conducted to anticipate future possible innovation chances, with the help of link prediction. Our study contributes to the field by suggesting a systematic method for data-driven roadmapping, and provides data-driven evidence that helps experts to make more reasonable decision-making.}
}
@article{MHAMDI2020695,
title = {Job Recommendation based on Job Profile Clustering and Job Seeker Behavior},
journal = {Procedia Computer Science},
volume = {175},
pages = {695-699},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318020},
author = {D. Mhamdi and R. Moulouki and M.Y. {El Ghoumari} and M. Azzouazi and L. Moussaid},
keywords = {job recommendation, profile clustering, word2vec, k-means clustering},
abstract = {This article presents a recommender system that aims to help job seekers to find suitable jobs. First, job offers are collected from job search websites then they are prepared to extract meaningful attributes such as job titles and technical skills. Job offers with common features are grouped into clusters. As job seeker like one job belonging to a cluster, he will probably find other jobs in that cluster that he will like as well. A list of top n recommendations is suggested after matching data from job clusters and job seeker behavior, which consists on user interactions such as applications, likes and rating.}
}
@article{NOH2022106539,
title = {Analyzing vehicle–pedestrian interactions: Combining data cube structure and predictive collision risk estimation model},
journal = {Accident Analysis & Prevention},
volume = {165},
pages = {106539},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106539},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521005704},
author = {Byeongjoon Noh and Hansaem Park and Hwasoo Yeo},
keywords = {Pedestrian safety, Predictive collision risk, Trajectory prediction, Data cube model, Multi-dimensional analysis},
abstract = {Road traffic accidents are a severe threat to human lives, particularly to vulnerable road users (VRUs) such as pedestrians causing premature deaths. Therefore, it is necessary to devise systems to prevent accidents in advance and respond proactively, using potential risky situations as one of the surrogate safety measurements. This study introduces a new concept of a pedestrian safety system that combines the field and the centralized processes. The system can warn of upcoming risks immediately in the field and improve the safety of risk-frequent areas by assessing the safety levels of roads without actual collisions. In particular, this study focuses on the latter by introducing a new analytical framework for a crosswalk safety assessment with various behaviors of vehicles/pedestrians and environmental features. We obtain these behavioral features from actual traffic video footages in the city with complete automatic processing. The proposed framework mainly analyzes these behaviors in multi-dimensional perspectives by constructing a data cube structure, which combines the Long Short-Term Memory (LSTM)-based predictive collision risk (PCR) estimation model and the on-line analytical processing (OLAP) operations. From the PCR estimation model, we categorize the severity of risks as four levels; “relatively safe,” “caution,” “warning,” and “danger,” and apply the proposed framework to assess the crosswalk safety with behavioral features. With the proposed framework, the various descriptive results are harvested, but we aim at conducting analysis based on two scenarios in our analytic experiments; the movement patterns of vehicles and pedestrians by road environment and the relationships between risk levels and car speeds. Consequently, the proposed framework can support decision-makers (e.g., urban planners, safety administrators) by providing the valuable information to improve pedestrian safety for future accidents, and it can help us better understand cars’ and pedestrians’ proactive behavior near the crosswalks. In order to confirm the feasibility and applicability of the proposed framework, we implement and apply it to actual operating CCTVs in Osan City, Republic of Korea.}
}
@article{JIN2019708,
title = {Robust ℓ2−Hypergraph and its applications},
journal = {Information Sciences},
volume = {501},
pages = {708-723},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519302026},
author = {Taisong Jin and Zhengtao Yu and Yue Gao and Shengxiang Gao and Xiaoshuai Sun and Cuihua Li},
keywords = {Hypergraph, Hyperedge, Representation coefficients, Ridge regression},
abstract = {Hypergraph, an important learning tool to modulate high-order data correlations, has a wide range of applications in machine learning and computer vision. The key issue of the hypergraph-based applications is to construct an informative hypergraph, in which the hyperedges effectively represent the high-order data correlations. In practice, the real-world data is usually sampled from a union of non-linear manifolds. Due to the issues of noise and data corruptions, many data samples deviate from the underlying data manifolds. To construct an informative hypergraph that represents real-world data distribution well, we propose a hypergraph model (ℓ2-Hypergraph). Our model generates each hyperedge by solving an affine subspace ridge regression problem, where the samples with non-zero representation coefficients are used for hyperege generation. Specifically, to be robust to sparse noise and corruptions, a sparse constraint is imposed on data errors. We have conducted image clustering and classification experiments on real-world datasets. The experimental results demonstrate that our hypergraph model is superior to the existing hypergraph construction methods in both accuracy and robustness to sparse noise.}
}
@article{CHIARELLO2021121177,
title = {Towards ESCO 4.0 – Is the European classification of skills in line with Industry 4.0? A text mining approach},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121177},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121177},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006107},
author = {Filippo Chiarello and Gualtiero Fantoni and Terence Hogarth and Vito Giordano and Liga Baltina and Irene Spada},
keywords = {Industry 4.0, Technological change, Employment, Skill analysis, Text mining},
abstract = {ESCO is a multilingual classification of Skills, Competences, Qualifications, and Occupations created by the European Commission to improve the supply of information on skills demand in the labour market. It is designed to assist individuals, employers, universities and training providers by giving them up to date and standardized information on skills. Rapid technological change means that ESCO needs to be updated in a timely manner. Evidence is presented here of how text-mining techniques can be applied to the analysis of data on emerging skill needs arising from Industry 4.0 to ensure that ESCO provides information which is current. The alignment between ESCO and Industry 4.0 technological trends is analysed. Using text mining techniques, information is extracted on Industry 4.0 technologies from: (i) two versions of ESCO (v1.0 - v1.1.); and (ii) from the 4.0 related scientific literature. These are then compared to identify potential data gaps in ESCO. The findings demonstrate that text mining applied on scientific literature to extract technology trends, can help policy makers to provide more up-to-date labour market intelligence.}
}
@article{ZAMANI2022121456,
title = {Developing metrics for emerging technologies: identification and assessment},
journal = {Technological Forecasting and Social Change},
volume = {176},
pages = {121456},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121456},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100891X},
author = {Mehdi Zamani and Haydar Yalcin and Ali Bonyadi Naeini and Gordana Zeba and Tugrul U Daim},
keywords = {Bibliometric analysis, Emerging technologies, Strategic diagrams, Thematic evolution maps, Future studies, Structural hole analysis},
abstract = {We analyze the field of emerging technologies using bibliographic techniques. The Web of Science (WoS) citation database was used as an article retrieval tool in this study. To analyze the studies, first the files received from the mentioned database were integrated by Bibexcel software. Then, the process of "current technologies” was studied from 1969 to 2020 by the Bibliometrix package from Bibliometrics and SciMat software, as well as thematic evolution maps and strategic diagrams. To identify the main actors and resources in these areas, the results obtained from the analysis indicated highly cited articles, journals, and influential authors in this field. In addition, the results of evaluating strategic diagrams and thematic evolution maps revealed the topics of emerging technologies. In general, the topics and technologies such as "Internet of Things (IoT)", "Intelligent Transportation Systems (ITS)", "Ultrasound", are among the emerging areas and technologies being considered during the last decade as the most central topics. Structural hole analysis has been applied to determine the sub-technologies of these technologies that have begun to mature. Furthermore, the trend of research focus on these topics has been increasing. For this reason, to determine the sub-subject areas that are open to improvement, they are handled with low aggregate constraints values.}
}
@article{SADIQUE2018199,
title = {Towards Security on Internet of Things: Applications and Challenges in Technology},
journal = {Procedia Computer Science},
volume = {141},
pages = {199-206},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.168},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318180},
author = {Kazi Masum Sadique and Rahim Rahmani and Paul Johannesson},
keywords = {Internet-of-Things, IoT security challenges, Trust in IoT, IoT data privacy, Distributed intelligence, Blockchain, 5G wireless technology, Software defined network (SDN), Network function virtualization (NFV), Machine learning},
abstract = {The Internet of Things (IoT) paradigm refers to the network of physical objects or "things" embedded with electronics, software, sensors, and connectivity to enable objects to exchange data with servers, centralized systems, and/or other connected devices based on a variety of communication infrastructures. IoT data collected from different sensors, nodes and collectors are transferred to the cloud over the internet. IoT devices are used by consumers, healthcare, businesses as well as by the governments. It is being forecast that 31 billion IoT devices will be deployed all over the world by the year 2020. As the use of IoT devices is increasing every moment several IoT vulnerabilities are introduced. The results and analysis indicate that massive deployment of IoT with an integration of new technologies are introducing new security challenges in IoT paradigm. In this paper, IoT security challenges and open issues are discussed which provides a ground for future research.}
}
@article{YANG2021117,
title = {GraphLSHC: Towards large scale spectral hypergraph clustering},
journal = {Information Sciences},
volume = {544},
pages = {117-134},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306824},
author = {Yiyang Yang and Sucheng Deng and Juan Lu and Yuhong Li and Zhiguo Gong and Leong Hou U and Zhifeng Hao},
keywords = {Machine Learning, Unsupervised Learning, Clustering, Hypergraph},
abstract = {Hypergraph is popularly used for describing multi-relationships among objects in a unified manner, and spectral clustering is regarded as one of the most effective algorithms for partitioning those objects (vertices) into different communities. However, the traditional spectral clustering for hypergraph (HC) incurs expensive costs in terms of both time and space. In this paper, we propose a framework called GraphLSHC to tackle the scalability problem faced by the large scale hypergraph spectral clustering. In our solution, the hypergraph used in GraphLSHC is expanded into a general format to capture complicated higher-order relationships. Moreover, GraphLSHC is capable to simultaneously partition both vertices and hyperedges according to the “eigen-trick”, which provides an approach for reducing the computational complexity of the clustering. To improve the performance further, several hyperedge-based sampling techniques are proposed, which can supplement the sampled matrix with the whole graph information. We also give a theoretical guarantee for the error boundary of the supplement. Several experiments show the superiority of the proposed framework over the state-of-the-art algorithms.}
}
@article{SHAHID2020534,
title = {Energy and delay efficient fog computing using caching mechanism},
journal = {Computer Communications},
volume = {154},
pages = {534-541},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S014036641931446X},
author = {Muzammil Hussain Shahid and Ahmad Raza Hameed and Saif {ul Islam} and Hasan Ali Khattak and Ikram Ud Din and Joel J.P.C. Rodrigues},
keywords = {Energy efficiency, Fog computing, Caching, Load balancing},
abstract = {Fog computing has emerged as an extension to the existing cloud infrastructure for providing latency-aware and highly scalable services to geographically distributed end devices. The addition of the fog layer in the cloud computing paradigm helps to improve the quality of service (QoS) in time-critical and delay-sensitive applications. Due to the continuous increase in the deployment of fog networks at large scale, energy efficiency is a significant issue in the fog computing paradigm to reduce the service cost and to protect the environment. A plethora of research has been conducted to reduce energy consumption in fog computing, majorly, focusing on the scheduling of incoming jobs to improve energy efficiency. However, node-level mechanisms have largely been neglected. Cache placement is a critical issue in fog networks for efficient content distribution to clients, which requires simultaneous consideration of many factors including quality of network connection, the demand for contents, and users’ activities. In this paper, a popularity-based caching mechanism in content delivery fog networks is proposed. In this context, two energy-aware mechanisms, i.e., content filtration and load balancing, have been applied. In the proposed approach, popular contents are found using random distribution and these contents are categorized into three classes. After finding the file popularity, an active fog node is selected based on the number of neighbors, energy level, and operational power. Further, the popular content is cached on the active node using a filtration mechanism. Moreover, a load-balancing algorithm is proposed to increase the overall system efficiency in the cached fog network. The evaluation of the proposed approach exhibits promising results in terms of energy consumption and latency. The proposed scheme consumes 92.6% and 82.7% less energy in comparison to without caching and simple caching mechanisms, respectively. Similarly, an improvement of 85.29% and 67.4% in delay has also been noticed while using advance caching against the without caching and simple caching techniques, respectively.}
}
@article{MAI202242,
title = {Learning behaviours data in programming education: Community analysis and outcome prediction with cleaned data},
journal = {Future Generation Computer Systems},
volume = {127},
pages = {42-55},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003344},
author = {Tai Tan Mai and Marija Bezbradica and Martin Crane},
keywords = {Community detection, Learning analytics, Random matrix theory, Machine learning, Educational data mining},
abstract = {Due to the COVID19 pandemic, more higher-level education programmes have moved to online channels, raising issues in monitoring students’ learning progress. Thanks to advances in online learning systems, however, student data can be automatically collected and used for the investigation and prediction of the students’ learning performance. In this article, we present a novel approach to analyse students’ learning behaviour, as well as the relationship between these behaviours and learning assessment results, in the context of programming education. A bespoke method has been built based on a combination of Random Matrix Theory, a Community Detection algorithm and statistical hypothesis tests. The datasets contain fine-grained information about students’ learning behaviours in two programming courses over two academic years with about 400 first-year students in a Medium-sized Metropolitan University in Dublin. The proposed method is a noval approach to data preprocessing which can improve the analysis and prediction based on learning behavioural datasets. The proposed approach deals with the issues of noise and trend effect in the data and has shown its success in detecting groups of students who have similar learning behaviours and outcomes. The higher performing groups have been found to be more active in practical-related activities throughout the course. Conversely, we found that the lower performing groups engage more with lecture notes instead of doing programming tasks. The learning behaviours data can also be used to predict students’ outcomes (i.e. Pass or Fail the terminal exams) at the early stages of the study, using popular machine learning classification techniques.}
}
@article{SUN2021143033,
title = {A human-centred assessment framework to prioritise heat mitigation efforts for active travel at city scale},
journal = {Science of The Total Environment},
volume = {763},
pages = {143033},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.143033},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720365633},
author = {Qian (Chayn) Sun and Tania Macleod and Alan Both and Joe Hurley and Andrew Butt and Marco Amati},
keywords = {Tree shade, Sky view factor (SVF), Heat vulnerability index (HVI), Deep learning, Active travel, GIS},
abstract = {Hot weather not only impacts upon human physical comfort and health, but also impacts the way that people access and experience active travel options such as walking and cycling. By evaluating the street thermal environment of a city alongside an assessment of those communities that are the most vulnerable to the effects of heat, we can prioritise areas in which heat mitigation interventions are most needed. In this paper, we propose a new approach for policy makers to determine where to delegate limited resources for heat mitigation with most effective outcomes for the communities. We use eye-level street panorama images and community profiles to provide a bottom-up, human-centred perspective of the city scale assessment, highlighting the situation of urban tree shade provision throughout the streets in comparison with environmental and social-economic status. The approach leverages multiple sources of spatial data including satellite thermal images, Google street view (GSV) images, land use and demographic census data. A deep learning model was developed to automate the classification of streetscape types and percentages at the street- and eye-view level. The methodology is metrics based and scalable which provides a data driven assessment of heat-related vulnerability. The findings of this study first contribute to sustainable development by developing a method to identify geographical areas or neighbourhoods that require heat mitigation; and enforce policies improving tree shade on routes, as a heat adaptation strategy, which will lead to increasing active travel and produce significant health benefits for residents. The approach can be also used to guide post COVID-19 city planning and design.}
}
@article{LIANG2020105895,
title = {Single and simultaneous fault diagnosis of gearbox via a semi-supervised and high-accuracy adversarial learning framework},
journal = {Knowledge-Based Systems},
volume = {198},
pages = {105895},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105895},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120302410},
author = {Pengfei Liang and Chao Deng and Jun Wu and Zhixin Yang and Jinxuan Zhu and Zihan Zhang},
keywords = {Single, Simultaneous, Fault diagnosis, Gearbox, Adversarial learning, Continuous wavelet transform},
abstract = {Gearboxes are the most widely used elements for transferring speed and power in many industrial machines. High-accuracy gearbox fault diagnosis is quite significant for keeping the machine working reliably and safely. Owing to various unseen faults, it is pretty challenging to realize high-accuracy intelligent fault diagnosis of gearboxes using existing methods. In addition, existing intelligent fault diagnosis methods heavily rely on a huge number of labeled samples, and the features extraction and selection are mainly done manually. In this paper, a semi-supervised and high-accuracy adversarial learning framework for the single and simultaneous fault diagnosis of the gearbox based on Generative Adversarial Nets and time-frequency imaging is proposed. The proposed method involves two parts. In the first part, continuous wavelet transform is adopted to transform one-dimensional raw vibration signals into two-dimensional time-frequency images. In the second part, the labeled and unlabeled time-frequency images are inputted into the built adversarial learning model to realize single and simultaneous fault diagnosis of the gearbox. Finally, two case studies are implemented to verify the proposed method. The results indicate that it is higher in accuracy and fewer in training steps of achieving the highest accuracy rate than other existing intelligent fault diagnosis methods in literatures. Moreover, its performance in stability is pretty good as well.}
}
@article{YANG2018280,
title = {Vegetable Image Retrieval with Fine-tuning VGG Model and Image Hash},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {17},
pages = {280-285},
year = {2018},
note = {6th IFAC Conference on Bio-Robotics BIOROBOTICS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.175},
url = {https://www.sciencedirect.com/science/article/pii/S240589631831293X},
author = {Zhaolu Yang and Jun Yue and Zhenbo Li and Ling Zhu},
keywords = {Image retrieval, Specific domain, Fine-tune, VGG, CBIR, PCA Hashing},
abstract = {Image descriptors based on activations of Convolutional Neural Networks (CNN) have become dominant in image retrieval due to their discriminative power, compactness of the representation, and the efficiency of search. Fine-tune existing CNN models for image retrieval in specific domain is significant for content-based image retrieval tasks. Inspired by recent successes of CNN with hierarchical features, in this paper, we fine-tuning VGG model to learn features for special vegetable dataset with the classification task. Furthermore, we propose utilizing some PCA Hashing strategies combinate CNN features extracted by the fine-tuned model to improve the performance of special domain CBIR tasks. Our experimental results demonstrate that leveraging the method we proposed can improve the performance of CBIR and the mAP increased by 10 to 20 percent in seam Hash code bits, compared to the model before fine-tuning.}
}
@article{BATTISTI2022121392,
title = {Creating new tech entrepreneurs with digital platforms: Meta-organizations for shared value in data-driven retail ecosystems},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121392},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121392},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521008234},
author = {Sandro Battisti and Nivedita Agarwal and Alexander Brem},
keywords = {Meta-organization, Artificial intelligence, Decision-making, Business ecosystem, Social innovation, Retail business},
abstract = {Creating technologically focused entrepreneurs is a crucial endeavor worldwide, especially with the exponential growth of artificial intelligence (AI) innovation. This research explores meta-organizations that enable new business models in the retail domain by acting as a powerful mechanism to support entrepreneurs in extracting value from data. This study investigates how meta-organizations engage users and empower tech entrepreneurs to create shared value by developing social innovation. This research involves an in-depth and longitudinal unique case study of a meta-organization operating in Italy, Germany, and Finland. Results indicate that the flexible structure of meta-organizations can effectively guide stakeholders of different mindsets to offer support to high-tech startups. AI-based platforms are a reliable alternative to tackle critical social issues in order to improve economic growth and increase people's performance in a stressful, competitive environment, such as the retail sector. The findings affirm that AI-based innovation orchestrated by meta-organizations can enable new business models by creating shared value for society. Seven critical success factors with implications for theory and practice are discussed, and a new model for AI-driven entrepreneurship is proposed.}
}
@article{JIANG2020209,
title = {A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition},
journal = {Information Fusion},
volume = {53},
pages = {209-221},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301381},
author = {Yingying Jiang and Wei Li and M. Shamim Hossain and Min Chen and Abdulhameed Alelaiwi and Muneer Al-Hammadi},
keywords = {Artificial intelligence, Multimodal information fusion, Data-driven emotion recognition},
abstract = {With the rapid development of artificial intelligence and mobile Internet, the new requirements for human-computer interaction have been put forward. The personalized emotional interaction service is a new trend in the human-computer interaction field. As a basis of emotional interaction, emotion recognition has also introduced many new advances with the development of artificial intelligence. The current research on emotion recognition mostly focuses on single-modal recognition such as expression recognition, speech recognition, limb recognition, and physiological signal recognition. However, the lack of the single-modal emotional information and vulnerability to various external factors lead to lower accuracy of emotion recognition. Therefore, multimodal information fusion for data-driven emotion recognition has been attracting the attention of researchers in the affective computing filed. This paper reviews the development background and hot spots of the data-driven multimodal emotion information fusion. Considering the real-time mental health monitoring system, the current development of multimodal emotion data sets, the multimodal features extraction, including the EEG, speech, expression, text features, and multimodal fusion strategies and recognition methods are discussed and summarized in detail. The main objective of this work is to present a clear explanation of the scientific problems and future research directions in the multimodal information fusion for data-driven emotion recognition field.}
}
@article{YACCHIREMA201825,
title = {System for monitoring and supporting the treatment of sleep apnea using IoT and big data},
journal = {Pervasive and Mobile Computing},
volume = {50},
pages = {25-40},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1574119217306259},
author = {Diana Yacchirema and David Sarabia-Jácome and Carlos E. Palau and Manuel Esteve},
keywords = {IoT, Big data, Fog computing, Cloud computing, Sleep apnea},
abstract = {Sleep apnea has become in the sleep disorder that causes greater concern in recent years due to its morbidity and mortality, higher medical care costs and poor people quality of life. Some proposals have addressed sleep apnea disease in elderly people, but they have still some technical limitations. For these reasons, this paper presents an innovative system based on fog and cloud computing technologies which in combination with IoT and big data platforms offers new opportunities to build novel and innovative services for supporting the sleep apnea and to overcome the current limitations. Particularly, the system is built on several low-power wireless networks with heterogeneous smart devices (i.e, sensors and actuators). In the fog, an edge node (Smart IoT Gateway) provides IoT connection and interoperability and pre-processing IoT data to detect events in real-time that might endanger the elderly’s health and to act accordingly. In the cloud, a Generic Enabler Context Broker manages, stores and injects data into the big data analyzer for further processing and analyzing. The system’s performance and subjective applicability are evaluated using over 30 GB size datasets and a questionnaire fulfilled by medicals specialist, respectively. Results show that the system data analytics improve the health professionals’ decision making to monitor and guide sleep apnea treatment, as well as improving elderly people’s quality of life.}
}
@article{GAO2021103304,
title = {Social welfare maximizing fleet charging scheduling through voting-based negotiation},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {130},
pages = {103304},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103304},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003132},
author = {Jie Gao and Terrence Wong and Chun Wang},
keywords = {Smart city, Transportation network companies (TNCs), EV charging scheduling, Negotiation at a large scale, SA-inspired automated negotiation mechanism},
abstract = {As an alternative to traditional taxi services, Transportation Network Companies (TNCs) such as Uber and Lyft are playing an increasingly important role in the paradigm shifting from car ownership to mobility as a service. We consider an electric vehicle fleet charging scheduling problem in the TNC setting where taxi drivers, as freelancers, have their individual preferences regarding when and where to charge their vehicles. In this setting, obtaining social welfare maximizing schedules is particularly difficult as drivers may behave strategically in competing over shared charging resources to advance their own benefits rather than the system wide social welfare. We propose a negotiation mechanism which allows drivers to collectively evolve an incumbent schedule into a socially beneficial one through an iterative voting process. The proposed mechanism provides a platform which enables multilateral negotiation among a large number of drivers. We prove that, given the design of the proposed mechanism, drivers’ best response strategy is to truthfully vote their best valued candidate schedules according to the acceptance quota prescribed by the scheduler at each voting round. In addition, experiment results show that the mechanism achieves on average 93% efficiency compared with optimal solutions and scales well to larger problem instances.}
}
@article{WANG2018150,
title = {Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {143},
pages = {150-166},
year = {2018},
note = {ISPRS Journal of Photogrammetry and Remote Sensing Theme Issue “Point Cloud Processing”},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S092427161830090X},
author = {Cheng Wang and Shiwei Hou and Chenglu Wen and Zheng Gong and Qing Li and Xiaotian Sun and Jonathan Li},
keywords = {Point clouds, Indoor modeling, Mobile laser scanning, Line framework extraction, Semantic labeling},
abstract = {Indoor building models are essential in many indoor applications. These models are composed of the primitives of the buildings, such as the ceilings, floors, walls, windows, and doors, but not the movable objects in the indoor spaces, such as furniture. This paper presents, for indoor environments, a novel semantic line framework-based modeling building method using backpacked laser scanning point cloud data. The proposed method first semantically labels the raw point clouds into the walls, ceiling, floor, and other objects. Then line structures are extracted from the labeled points to achieve an initial description of the building line framework. To optimize the detected line structures caused by furniture occlusion, a conditional Generative Adversarial Nets (cGAN) deep learning model is constructed. The line framework optimization model includes structure completion, extrusion removal, and regularization. The result of optimization is also derived from a quality evaluation of the point cloud. Thus, the data collection and building model representation become a united task-driven loop. The proposed method eventually outputs a semantic line framework model and provides a layout for the interior of the building. Experiments show that the proposed method effectively extracts the line framework from different indoor scenes.}
}
@article{LI2020106287,
title = {A systematic review of unsupervised learning techniques for software defect prediction},
journal = {Information and Software Technology},
volume = {122},
pages = {106287},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106287},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300379},
author = {Ning Li and Martin Shepperd and Yuchen Guo},
keywords = {Unsupervised learning, Software defect prediction, Machine learning, Systematic review, Meta-analysis},
abstract = {Background
Unsupervised machine learners have been increasingly applied to software defect prediction. It is an approach that may be valuable for software practitioners because it reduces the need for labeled training data.
Objective
Investigate the use and performance of unsupervised learning techniques in software defect prediction.
Method
We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our inclusion criteria published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the confusion matrices and employed the Matthews Correlation Coefficient (MCC) as our main performance measure.
Results
Our meta-analysis shows that unsupervised models are comparable with supervised models for both within-project and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check.
Conclusion
Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.}
}
@article{ZEB2021114164,
title = {KGEL: A novel end-to-end embedding learning framework for knowledge graph completion},
journal = {Expert Systems with Applications},
volume = {167},
pages = {114164},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114164},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420309039},
author = {Adnan Zeb and Anwar {Ul Haq} and Defu Zhang and Junde Chen and Zhiguo Gong},
keywords = {Knowledge graph, Link prediction, Weighted graph convolutional network, Tensor train decomposition, Tensor factorization},
abstract = {Knowledge graphs (KGs) have recently become increasingly popular due to the broad range of essential applications in various downstream tasks including intelligent search, personalized recommendations, intelligent financial data analytics, etc. During an automated construction of a KG, the knowledge facts from multiple knowledge sources are automatically extracted in the form of triples, and these observed triples are used to derive new unobserved triples for KG completion (also known as link prediction). State-of-the-art link prediction methods are known to be primarily KG embedding models, among which tensor factorization models have recently drawn much attention due to their scalability and expressive feature embeddings, and hence, perform well for link prediction. However, these embedding models consider each KG triple individually and fail to capture the useful information present in the neighborhood of a node. To this end, we propose a novel end-to-end KG embedding learning framework that consists of an encoder of a dual weighted graph convolutional network, and a decoder of a novel fully expressive tensor factorization model. The proposed encoder extends weighted graph convolutional network to generate two rich and high quality embedding vectors for each node by aggregating information from the neighboring nodes. The proposed decoder has a flexible and powerful tensor representation form of the Tensor Train decomposition that takes benefit of the two representations of each node in its embedding space to accurately model the KG triples. We also derive a bound on the size of the embeddings for full expressivity and show that our proposed tensor factorization model is fully expressive. Additionally, we show the relationship of our tensor factorization model to previous tensor factorization models. The experimental results show the effectiveness of the proposed framework that consistently marks performance gains over several previous models on recent standard link prediction datasets.}
}
@article{JIANG2021106431,
title = {A comprehensive study of macro factors related to traffic fatality rates by XGBoost-based model and GIS techniques},
journal = {Accident Analysis & Prevention},
volume = {163},
pages = {106431},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106431},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521004620},
author = {Feifeng Jiang and Jun Ma},
keywords = {Traffic fatality rates, Macro factors, National scale, XGBoost, GIS, Feature importance},
abstract = {With the fast development of economics, road safety is becoming a serious problem. Exploring macro factors is effective to improve road safety. However, the existing studies have some limitations: (1) The existing studies only considered one aspect of macro factors and constructed models based on a few data samples. (2) The methods commonly used cannot address the non-linear relationship or calculate the feature importance. The findings obtained from such models may be limited and biased. To address the limitations, this study proposes a BO-CV-XGBoost framework to explore the macro factors related to traffic fatality rate classes based on a high-dimensional dataset that fully considers the impact of multi-factor interaction with adequate data samples. The proposed framework is applied to a dataset in the US. 453 county-level macro factors are collected from various data sources, covering ten macro aspects, including topography, transportation, etc. The optimized BO-CV-XGBoost model obtains the best classification performance with an AUC of 0.8977 and an accuracy of 85.02%. Compared with other methods, the proposed model has superiority on fatality rate classification. Ten macro factors are identified, including ‘Current-dollar GDP’, ‘highway miles per person’, etc. The ten factors contain four aspects of information, including economics, transportation, education, and medical condition. Geographic information system (GIS) techniques are further used for spatial analysis of the identified macro factors. Therefore, targeted and effective measures are accordingly proposed to prevent traffic fatalities and improve road safety}
}
@article{RAHOUMA2019460,
title = {Design of a New Automated Fault Detector based on artificial intelligence and Big Data Techniques},
journal = {Procedia Computer Science},
volume = {163},
pages = {460-471},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.129},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321684},
author = {Kamel H. Rahouma and Farag M. Afify and Hesham F.A. Hamed},
keywords = {AI, BMS, Energy consumption abnormalities, Automated Fault Detection},
abstract = {Concerns over energy and environmental issues around the world have led to the worldwide focus on energy use reduction. Studies in each area of energy have shown that residential and commercial construction sectors consume more power than other sectors such as industry, agriculture, services, and transportation. Studies of energy consumption in building sectors have reported that energy savings of 10% to 30% can be obtained by using artificial intelligence (AI), the system would be capable of detecting and analyzing anomalies in energy usage pattern assessing, diagnosing and suggesting the best solution in suitable time. This paper proposes to integrate and hybridize between AI techniques and big data algorithms which can enhance monitoring and controlling building systems, increasing comfort and decreasing efficiently the running costs. In addition, the authors suggest a tool which aims to automatically detect abnormal energy consumption by using AI and big data which are produced by the Building Management System (BMS). This happens by designing a software application that is called Fault Detection Tool (FDT) which automatically detects the abnormalities of energy consumption, optimizes the use of different resources and analyzes faults, complaints and time taken to terminate them. Experimental results show that with the proposed approach, it is possible to accurately detect anomalous patterns in building energy consumption. This tool will be a part of an artificial intelligent decision-making system.}
}
@article{JAIN2020448,
title = {Implications of emerging technologies on the future of work},
journal = {IIMB Management Review},
volume = {32},
number = {4},
pages = {448-454},
year = {2020},
issn = {0970-3896},
doi = {https://doi.org/10.1016/j.iimb.2020.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S097038962030077X},
author = {Ankur Jain and Sushant Ranjan},
keywords = {Future of work, Jobs, Technology, Automation, Creativity, AI, IoT},
abstract = {Technology pessimists believe that emerging technologies will take away jobs. However, technology optimists suggest that “non-routine” jobs may not only exist but also surge. To better understand the implications of emerging technologies on the future of work, we conducted a panel discussion with senior industry leaders. The discussion was focussed on understanding what opportunities get created, how jobs get transformed, what competencies become essential, and what challenges are posed by emerging technologies. The emerging themes from the discussion highlighted creative tensions in the way technology interacts with work, opening a plethora of opportunities for shaping a desired future.}
}
@article{VANINI201637,
title = {Using barometric pressure data to recognize vertical displacement activities on smartphones},
journal = {Computer Communications},
volume = {87},
pages = {37-48},
year = {2016},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2016.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S014036641630041X},
author = {Salvatore Vanini and Francesca Faraci and Alan Ferrari and Silvia Giordano},
keywords = {Smartphone computing and communication, Activity recognition, Barometer, Inference model, Energy consumption},
abstract = {We introduce a novel, efficient methodology for the automatic recognition of major vertical displacements in human activities. It is based exclusively on barometric pressure measured by sensors commonly available on smartphones and tablets. We evaluate various algorithms to distinguish dynamic activities, identifying four different categories: standing/walking on the same floor, climbing stairs, riding an elevator and riding a cable-car. Activities are classified using standard deviation and slope of barometric pressure. We leverage three different inference models to predict the action performed by a user, namely: Bayesian networks, decision trees, and recurrent neural networks. We find that the best results are achieved with a recurrent neural network (reaching an overall error rate of less than 1%). We also show that decision tree classifiers can achieve good accuracy and offer a better trade-off between computational overhead and energy consumption; therefore, they are good candidates for smartphone implementations. As a proof of concept, we integrate the decision tree classifier in an App that infers user activity and measures elevation differences. Test results with various users show an average recognition accuracy rate of about 95%. We further show the power consumption of running barometric pressure measurements and analyse the correlation of pressure with environmental factors. Finally, we compare our approach to other standard methodologies for activity detection based on accelerometer and/or on GPS data. Our results show that our technique achieves similar accuracy while offering superior energy efficiency, independence from the sensor location, and immunity to environmental factors (e.g., weather conditions, air handlers).}
}
@article{MAZUNGA2021e00819,
title = {Manhole intrusion detection system with notification stages},
journal = {Scientific African},
volume = {12},
pages = {e00819},
year = {2021},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2021.e00819},
url = {https://www.sciencedirect.com/science/article/pii/S246822762100123X},
author = {Felix Mazunga and Tawanda Romosi and Rosemary Guvhu},
keywords = {Intrusion detection, Manhole cover, Copper theft, Critical infrastructure protection, Arduino Uno, GSM},
abstract = {Appropriate design of intrusion detection systems is extremely important to safeguard critical and valuable infrastructure. Telecommunications companies (TELCOs) and electric power utilities are experiencing an increased rate of rampant underground copper cable theft and vandalism. This paper addresses the important subject of combating the physical intrusion of critical infrastructure by proposing a low-cost, effective and unique manhole intrusion detection system. Manholes are openings to confined underground spaces that are used to access critical underground infrastructure and utilities such as sewer systems, electricity and telecommunication networks for maintenance and inspection. Critical infrastructure protection is very essential to avoid losses and incapacitations that would have a debilitating effect on economic activities, security, public health and safety, and so on. The increasing rate of rampant copper theft and vandalism could be attributed to the increased demand and high prices for copper on the black market. Critical infrastructure sectors such as TELCOs, electricity supply, water and rail transport are utilizing intrusion detection systems that only trigger an alarm when the critical infrastructure has already been vandalized or stolen. In this paper, we propose a low-cost and unique manhole intrusion detection system with notification stages devoted to safeguard critical infrastructure. Simulations of the system were performed before hardware implementation. The proposed system utilizes an Arduino Uno microcontroller and multiple sensors to trigger the intrusion stages early before the copper cables are vandalized. Due to the inclusion of three different additional sensors, the proposed system has an advantage of timely or early-stage intrusion detection. The response time is improved since the first alert is sent early before the actual cutting of the cable. The GSM messaging system is utilized as the alert mechanism during the intrusion stages. Notifications are also displayed on a local LCD.}
}
@article{VALLECRUZ2021101644,
title = {From E-budgeting to smart budgeting: Exploring the potential of artificial intelligence in government decision-making for resource allocation},
journal = {Government Information Quarterly},
pages = {101644},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101644},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000800},
author = {David Valle-Cruz and Vanessa Fernandez-Cortez and J. Ramon Gil-Garcia},
keywords = {Artificial intelligence, Public budgeting, E-budgeting, Smart budgeting, Multilayer perceptron, Multiobjective genetic algorithm, Smartness, Budget intelligent machine},
abstract = {Artificial intelligence has become an important tool for governments around the world. However, it is not clear to what extent artificial intelligence can improve decision-making, and some policy domains have not been the focus of most recent studies, including the public budget process. More specifically, budget allocation is one of the areas in which AI may have greatest potential. Therefore, this study attempts to contribute to this gap in our existing knowledge by answering the following research question: To what extent can artificial intelligence techniques help distribute public spending to increase GDP, decrease inflation and reduce the Gini index? In order to respond to this question, this article proposes an algorithmic approach on how budget inputs (specific expenditures) are processed to generate certain outputs (economic, political, and social outcomes). The authors use the multilayer perceptron and a multiobjective genetic algorithm to analyze World Bank Open Data from 1960 to 2019, including 217 countries. The advantages of implementing this type of decision support system in public expenditures allocation arise from the ability to process large amounts of data and to find patterns that are not easy to detect, which include multiple non-linear relationships. Some technical aspects of the expenditure allocation process could be improved with the help of these kinds of techniques. In addition, the results of the AI-based approach are consistent with the findings of the scientific literature on public budgets, using traditional statistical techniques.}
}
@article{GOUDARZI201983,
title = {Predictive modelling of building energy consumption based on a hybrid nature-inspired optimization algorithm},
journal = {Energy and Buildings},
volume = {196},
pages = {83-93},
year = {2019},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819304177},
author = {Shidrokh Goudarzi and Mohammad Hossein Anisi and Nazri Kama and Faiyaz Doctor and Seyed Ahmad Soleymani and Arun Kumar Sangaiah},
keywords = {Energy consumption, Algorithm design, Prediction},
abstract = {Overall energy consumption has expanded over the previous decades because of rapid population, urbanization and industrial growth rates. The high demand for energy leads to higher cost per unit of energy, which, can impact on the running costs of commercial and residential dwellings. Hence, there is a need for more effective predictive techniques that can be used to measure and optimize energy usage of large arrays of connected Internet of Things (IoT) devices and control points that constitute modern built environments. In this paper, we propose a lightweight IoT framework for predicting energy usage at a localized level for optimal configuration of building-wide energy dissemination policies. Autoregressive Integrated Moving Average (ARIMA) as a statistical liner model could be used for this purpose; however, it is unable to model the dynamic nonlinear relationships in nonstationary fluctuating power consumption data. Therefore, we have developed an improved hybrid model based on the ARIMA, Support Vector Regression (SVRs) and Particle Swarm Optimization (PSO) to predict precision energy usage from supplied data. The proposed model is evaluated using power consumption data acquired from environmental actuator devices controlling a large functional space in a building. Results show that the proposed hybrid model out-performs other alternative techniques in forecasting power consumption. The approach is appropriate in building energy policy implementations due to its precise estimations of energy consumption and lightweight monitoring infrastructure which can lead to reducing the cost on energy consumption. Moreover, it provides an accurate tool to optimize the energy consumption strategies in wider built environments such as smart cities.}
}
@article{SHAMSHIRBAND2020102582,
title = {Computational intelligence intrusion detection techniques in mobile cloud computing environments: Review, taxonomy, and open research issues},
journal = {Journal of Information Security and Applications},
volume = {55},
pages = {102582},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102582},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620307523},
author = {Shahab Shamshirband and Mahdis Fathi and Anthony T. Chronopoulos and Antonio Montieri and Fabio Palumbo and Antonio Pescapè},
keywords = {Cloud computing, Computational intelligence, Intrusion detection system, Mobile cloud computing, Security},
abstract = {With the increasing utilization of the Internet and its provided services, an increase in cyber-attacks to exploit the information occurs. A technology to store and maintain user's information that is mostly used for its simplicity and low-cost services is cloud computing (CC). Also, a new model of computing that is noteworthy today is mobile cloud computing (MCC) that is used to reduce the limitations of mobile devices by allowing them to offload certain computations to the remote cloud. The cloud environment may consist of critical or essential information of an organization; therefore, to prevent this environment from possible attacks a security solution is needed. An intrusion detection system (IDS) is a solution to these security issues. An IDS is a hardware or software device that can examine all inside and outside network activities and recognize doubtful patterns that may demonstrate a network attack and automatically alert the network (or system) administrator. Because of the ability of an IDS to detect known/unknown (inside/outside) attacks, it is an excellent choice for securing cloud computing. Various methods are used in an intrusion detection system to recognize attacks more accurately. Unlike survey papers presented so far, this paper aims to present a comprehensive survey of intrusion detection systems that use computational intelligence (CI) methods in a (mobile) cloud environment. We firstly provide an overview of CC and MCC paradigms and service models, also reviewing security threats in these contexts. Previous literature is critically surveyed, highlighting the advantages and limitations of previous work. Then we define a taxonomy for IDS and classify CI-based techniques into single and hybrid methods. Finally, we highlight open issues and future directions for research on this topic.}
}
@article{SAHRAOUI2022102699,
title = {A cooperative crowdsensing system based on flying and ground vehicles to control respiratory viral disease outbreaks},
journal = {Ad Hoc Networks},
volume = {124},
pages = {102699},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102699},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521002006},
author = {Yesin Sahraoui and Chaker Abdelaziz Kerrache and Marica Amadeo and Anna Maria Vegni and Ahmed Korichi and Jamel Nebhen and Muhammad Imran},
keywords = {UAVs, Covid-19, Crowdsensing, Sensors, Internet of Things, Internet of vehicles},
abstract = {The massive increase in population density in cities has led to several urban problems, such as an increment of air pollution, traffic congestion, and a faster spread of infectious diseases. With the rapid innovation in the intelligent sensors technology, and its integration into smart vehicles and Unmanned Aerial Vehicles (UAVs), a novel sensing paradigm has been promoted, namely vehicular crowdsensing, which leverages on-board sensors to capture information from the surrounding environment. Collected data are then analyzed to take proper countermeasures. In this paper, we present a smart coordination mechanism between UAVs and ground vehicles (GVs), which sense information like body temperature and breathing rate of people, in order to support a variety of monitoring applications, including discovering the presence of infectious diseases. In our framework, namely GUAVA, aerial and ground vehicles are equipped with GPS devices and thermal cameras to monitor specific geographic areas, detect humans’ vital parameters and, at the same time, discover duplicate data by identifying matching faces in thermal video sequences with the GaussianFace algorithm. The sensing tasks in hard-to-reach places are assigned to UAVs, with the ability to power up wirelessly from the nearest GV and offload the collected monitoring images to it. Simulation results have assessed our proposed framework, showing good performance in terms of distinct Quality of Service (QoS) metrics.}
}
@article{LIGORIO2022121447,
title = {Tracing the boundaries between sustainable cities and cities for sustainable development. An LDA analysis of management studies},
journal = {Technological Forecasting and Social Change},
volume = {176},
pages = {121447},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121447},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521008787},
author = {Lorenzo Ligorio and Andrea Venturelli and Fabio Caputo},
keywords = {LDA, SDGs, Sustainable cities, Institutional work, Sustainable development},
abstract = {In recent years, an increasing number of researchers have focused their attention on the absence of a comprehensive definition of ‘sustainable cities’. The discussion has also interested the United Nations, which has contributed by providing a point of reference for local governments in the Agenda 2030. To shed light on the debate, the present study has adopted a latent Dirichlet allocation (LDA) analysis of the literature produced within the last five years. Furthermore, this study has adopted a bibliometric analysis to provide further insights into the published literature. Finally, the institutional work theory has been used to read the results of the analysis concerning the three main institutional works identified by scholars: political, technical, and cultural work. The study has revealed eight different trends concerning sustainable cities research, involving topics such as urban development, public management, environmental issues, and cultural impact on citizens. Additionally, topic analysis has resulted in a strong connection with the SDG 11. Conclusions of the analysis include the realisation that the sustainable city is supported by the political work of local governments and by the contributions of either the cultural or the technical work of city institutional actors.}
}
@article{ZHANG202097,
title = {Large-scale point cloud contour extraction via 3D guided multi-conditional generative adversarial network},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {164},
pages = {97-105},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300940},
author = {Weini Zhang and Linwei Chen and Zhangyue Xiong and Yu Zang and Jonathan Li and Lei Zhao},
keywords = {Contour extraction, Multi-conditional GAN, Large-scale point cloud},
abstract = {As one of the most important features for human perception, contours are widely used in many graphics and mapping applications. However, for large outdoor scale point clouds, contour extraction is considerably challenging due to the huge, unstructured and irregular point space, thus leading to massive failure for existing approaches. In this paper, to generate contours consistent with human perception for outdoor scenes, we propose, for the first time, 3D guided multi-conditional GAN (3D-GMcGAN), a deep neural network based contour extraction network for large scale point clouds. Specifically, two ideas are proposed to enable the network to learn the distributions of labeled samples. First, a parametric space based framework is proposed via a novel similarity measurement of two parametric models. Such a framework significantly compresses the huge point data space, thus making it much easier for the network to “remember” target distribution. Second, to prevent network loss in the huge solution space, a guided learning framework is designed to assist finding the target contour distribution via an initial guidance. To evaluate the effectiveness of the pro-posed network, we open-sourced the first, to our knowledge, dataset for large scale point cloud with contour annotation information. Experimental results demonstrate that 3D-GMcGAN efficiently generates contours for the data with more than ten million points (about several minutes), while avoiding ad hoc stages or parameters. Also, the proposed framework produces minimal outliers and pseudo-contours, as suggested by comparisons with the state-of-the-art approaches.}
}
@article{NIU2019405,
title = {Multi-view SVM Classification with Feature Selection},
journal = {Procedia Computer Science},
volume = {162},
pages = {405-412},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320149},
author = {Yuting Niu and Yuan Shang and Yingjie Tian},
keywords = {multi-view learning, classification, feature selection, sparsity regularization, linear programming},
abstract = {With the rapid development of data mining technology, multi-view learning (MVL) has become a new research field, which has attracted wide attention of scholars at home and abroad. Multi-view learning is to combine multiple view data of the same entity for data classification, thereby improving learning performance. Previous multi-view research methods mainly concentrate on the relationship between different data views for classification problems. However, when the data is in high dimensions, it is necessary to perform feature selection in the multi-view data classification process. In this paper, we proposed a Multi-view Support Vector Machine Classification with Feature Selection (MSVMCFS) algorithm, which can not only classify multi-view data, but also select features for each view data in the process of classification. In the model, feature selection is performed by the l1 norm sparsity regularization, and consistency and complementarity between the two views are maintained. To achieve the optimization goal, we adopt linear programming to solve the model. The experimental results on 30 binary datasets demonstrate the validity of the model.}
}
@article{SHANG2021102656,
title = {Newton-interpolation-based zk-SNARK for Artificial Internet of Things},
journal = {Ad Hoc Networks},
volume = {123},
pages = {102656},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102656},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521001712},
author = {Xinglin Shang and Liang Tan and Keping Yu and Jing Zhang and Kuljeet Kaur and Mohammad Mehedi Hassan},
keywords = {Blockchain, AIoT, ZK-SNARK, Zero knowledge proof, Newton interpolation},
abstract = {Artificial Internet of Things (AIoT) is that the system collects all kinds of information in real-time through various sensors, and intelligence analysis of the data through machine learning in the terminal equipment, edge domains, or cloud centers, including positioning, comparison, forecasting, scheduling, etc. which brings about the data security and privacy issues. The blockchain is a tamper-evident, unforgeable distributed ledger that protects security and privacy through the famous algorithm zk-SNARK, which is also widely used in virtual digital currencies such as Zcash. In addition, by using zk-SNARK technology in the Loopring DEX 3.0 in Ethereum, not only decentralization but also transaction performance can be guaranteed. However, there are three main problems of zk-SNARK, one is the need to guarantee calculation accuracy, two is the long time to generate evidence, especially when using Lagrangian interpolation to QAP the transaction data requires more computation; the last is the poor scalability, especially when nodes need to recalculate all data when adding new transactions. In this paper, we propose a modified zk-SNARK based on Newtonian interpolation, improve the QAP part of zk-SNARK by Newtonian interpolation, and verify the correctness of the scheme through instantiation. Finally, we analyze the computational efficiency of the two interpolation methods, and the results show that Newton interpolation solves the above two problems in the original zk-SNARK, and significantly reduces the time complexity of the algorithm, which can further promote the application of blockchain in data management of AIoT.}
}
@article{HU201760,
title = {Location-aware fine-grained vehicle type recognition using multi-task deep networks},
journal = {Neurocomputing},
volume = {243},
pages = {60-68},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.02.085},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217304691},
author = {Bin Hu and Jian-Huang Lai and Chun-Chao Guo},
keywords = {Vehicle type recognition, Fine-grained vehicle, Unconstrained-view vehicle, Vehicle localization, Multi-task CNNs},
abstract = {Distinct from conventional approaches to generic vehicle type recognition, this paper proposes an approach to fine-grained vehicle type recognition that leverages multiple cues jointly to convolutional neural networks (CNNs). Fine-grained vehicle recognition is inherently difficult with many challenges, including (i) how to incorporate multiple cues through joint optimization instead of separately handling, (ii) how to learn data-driven representation for vehicles instead of hand-crafted features and (iii) how to perceive and localize the vehicle region instead of using the whole image. Our multi-task CNNs localize vehicles in the first stage and recognize subclasses in the second stage, allowing us to handle samples with cluttered backgrounds and those where vehicles do not fill most of the image. Through collaborative feature learning from multiple tasks in each stage, our approach can handle subtle inter-class variations at the subordinate level. We also develop new vehicle-oriented data augmentation strategies in CNN training. To advance research on vehicle-centered tasks, we release a new vehicle dataset that provides both semantic labels and bounding boxes. This dataset can be used in vehicle localization, recognition, viewpoint estimation, and so on. Extensive experiments on two benchmark vehicle datasets demonstrate that our approach outperforms state-of-the-art algorithms.}
}
@article{XU2019159,
title = {Gait recognition based on capsule network},
journal = {Journal of Visual Communication and Image Representation},
volume = {59},
pages = {159-167},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300318},
author = {Zhaopeng Xu and Wei Lu and Qin Zhang and Yuileong Yeung and Xin Chen},
keywords = {Gait recognition, Capsule network, Deep learning},
abstract = {Gait as a biometric feature is widely used for human identification, and gait recognition has recently become a significant research problem. According to a small amount of labeled multi-view, multi-walking-condition and multi-clothes-condition human walking videos, we can find an effective model based on capsule network to capture more discriminative features and promote gait recognition performance. This paper works on gait recognition based on capsule network and we consider two different architectures, namely matching local features at the bottom layer based on capsule network and matching mid-level features at the middle layer based on capsule network, input images such as GEI, CGI, and resolution of input image. Empirical evaluations are conducted in the aspect of kinds scenarios, namely cross-walking-condition, cross-view and cross-clothes condition. The approaches are evaluated on the CASIA-B dataset and OU-ISIR Treadmill dataset B. These results show that the methods exceed the previous state-of-the-art outcomes.}
}
@article{HU2020324,
title = {Follow me Robot-Mind: Cloud brain based personalized robot service with migration},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {324-332},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.01.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1932953X},
author = {Long Hu and Yinging Jiang and Fangxin Wang and Kai Hwang and M. Shamim Hossain and Ghulam Muhammad},
keywords = {Depressive disorder, Emotion perception, Intelligent clothing, Game system},
abstract = {With the development of AI, cloud computing and 5G technology, new opportunities are opening up in the field of robotics. However, current robots are not closely tied to users. Because they provide homogeneous and popular services to all users, there is a lack of consideration for the individual diversity of users. Another defect of current robots is that they do not have the ability of migration and emotional perception for users. Therefore, an individualized follow me Robot-Mind-centered robot service system is proposed in this article. The follow me Robot-Mind can migrate at any time along with the user’s geospatial movement, which allows it to continuously accompany and grow with the user. In addition to the follow me Robot-Mind and physical robot, the system also includes components like multi-functional intelligent clothing, the intelligent cloud and the 5G network. A variety of AI algorithms are deployed in the intelligent cloud, which guarantees the intelligence of the follow me Robot-Mind. Then based on the proposed system, a testbed is established and the experiment results show the advantages and effectiveness of the system.}
}
@article{SIEMIENIUCH2015104,
title = {Global drivers, sustainable manufacturing and systems ergonomics},
journal = {Applied Ergonomics},
volume = {51},
pages = {104-119},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2015.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0003687015000794},
author = {C.E. Siemieniuch and M.A. Sinclair and M.J.deC. Henshaw},
keywords = {Sustainability, Manufacturing, Systems ergonomics},
abstract = {This paper briefly explores the expected impact of the ‘Global Drivers’ (such as population demographics, food security; energy security; community security and safety), and the role of sustainability engineering in mitigating the potential effects of these Global Drivers. The message of the paper is that sustainability requires a significant input from Ergonomics/Human Factors, but the profession needs some expansion in its thinking in order to make this contribution. Creating a future sustainable world in which people experience an acceptable way of life will not happen without a large input from manufacturing industry into all the Global Drivers, both in delivering products that meet sustainability criteria (such as durability, reliability, minimised material requirement and low energy consumption), and in developing sustainable processes to deliver products for sustainability (such as minimum waste, minimum emissions and low energy consumption). Appropriate changes are already being implemented in manufacturing industry, including new business models, new jobs and new skills. Considerable high-level planning around the world is in progress and is bringing about these changes; for example, there is the US ‘Advanced Manufacturing National Program’ (AMNP)’, the German ‘Industrie 4.0’ plan, the French plan ‘la nouvelle France industrielle’ and the UK Foresight publications on the ‘Future of Manufacturing’. All of these activities recognise the central part that humans will continue to play in the new manufacturing paradigms; however, they do not discuss many of the issues that systems ergonomics professionals acknowledge. This paper discusses a number of these issues, highlighting the need for some new thinking and knowledge capture by systems ergonomics professionals. Among these are ethical issues, job content and skills issues. Towards the end, there is a summary of knowledge extensions considered necessary in order that systems ergonomists can be fully effective in this new environment, together with suggestions for the means to acquire and disseminate the knowledge extensions.}
}
@article{DRISS20212385,
title = {Microservices in IoT Security: Current Solutions, Research Challenges, and Future Directions},
journal = {Procedia Computer Science},
volume = {192},
pages = {2385-2395},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921017440},
author = {Maha Driss and Daniah Hasan and Wadii Boulila and Jawad Ahmad},
keywords = {Internet of Things, Security, Microservices, Survey},
abstract = {In recent years, the Internet of Things (IoT) technology has led to the emergence of multiple smart applications in different vital sectors including healthcare, education, agriculture, energy management, etc. IoT aims to interconnect several intelligent devices over the Internet such as sensors, monitoring systems, and smart appliances to control, store, exchange, and analyze collected data. The main issue in IoT environments is that they can present potential vulnerabilities to be illegally accessed by malicious users, which threatens the safety and privacy of gathered data. To face this problem, several recent works have been conducted using microservices-based architecture to minimize the security threats and attacks related to IoT data. By employing microservices, these works offer extensible, reusable, and reconfigurable security features. In this paper, we aim to provide a survey about microservices-based approaches for securing IoT applications. This survey will help practitioners understand ongoing challenges and explore new and promising research opportunities in the IoT security field. To the best of our knowledge, this paper constitutes the first survey that investigates the use of microservices technology for securing IoT applications.}
}
@article{DHOOGE2021108613,
title = {Hierarchical feature block ranking for data-efficient intrusion detection modeling},
journal = {Computer Networks},
volume = {201},
pages = {108613},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108613},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621005107},
author = {Laurens D’hooge and Miel Verkerken and Tim Wauters and Bruno Volckaert and Filip {De Turck}},
keywords = {Network security, Intrusion detection, Hybrid feature selection},
abstract = {The intrusion detection field has been increasing the adoption of newer datasets after relying mainly on KDD99 and NSL-KDD. Both the height and the width of the newer datasets have increased substantially since they are geared towards evaluation by machine learning methods. The feature sets however are most often statistics, derived either from the packets, or more commonly from the (reconstructed) flows. The ease with which connected clusters of features can be extracted as well as the tendency to be overinclusive to provide researchers with as much data as possible has introduced significant bloat in the datasets. In order to improve the effective and efficient use of the datasets, this article proposes a hybrid feature selection mechanism based on a first-pass filter method and a second-pass embedded method with a central role for statistical testing to identify hierarchies of dominant feature sets. The non-destructive approach allows for the hierarchies to be inspected, interpreted and related to each other. The proposed approach is validated by constructing the feature hierarchies at three different resolutions for all recent datasets published by the Canadian Institute for Cybersecurity (IDS2017, DoS2017, IDS2018 and DDoS2019, millions of samples, 76 features). Three standard supervised learners were given increasing access to the feature (blocks) in terms of their hierarchical position. The results show that attack classes with a clear network component can be detected with cross-validated balanced accuracy, precision and recall above 99%, even when the classification model has been built from just 1 to 4 features, while additionally under a very restrictive sampling regimen: training (0.8%), validation (0.2%) and testing (99%). When selecting models only for classification performance more attack classes are detected more reliably, and while this increases feature use to an average of 12, this is still preferable over using the datasets’ standard set of 76 features.}
}
@article{HATEGEKIMANA2020101827,
title = {IoT Device security through dynamic hardware isolation with cloud-Based update},
journal = {Journal of Systems Architecture},
volume = {109},
pages = {101827},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101827},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301193},
author = {Festus Hategekimana and Taylor JL Whitaker and Md Jubaer {Hossain Pantho} and Christophe Bobda},
keywords = {Internet of things security (IoT), Isolation and protection, Cloud-based learning, Network traffic classification, Field programmable gate arrays (FPGA)},
abstract = {This work proposes a novel approach to provide comprehensive security to IoT devices. Our approach is based on a reconfigurable hardware-based isolation and protection mechanism (IPM) that operates as a dynamic separation unit between devices and network, far from potential software manipulation. The IPM analyses communications for malicious activities and prevents damage to the IoT device. The IPM leverages a central cloud-based authority to broaden the scope of traffic analysis beyond that of a singular IoT device. The central server evaluates logs from all IPM-protected IoT devices to improve their defense mechanisms and periodically upgrade device IPMs through a remote secure provisioning mechanism. The IPM achieves a 98.68% detection rate when evaluated against a Neptune DoS attack.}
}
@article{NEELI202184,
title = {Insight to security paradigm , research trend & statistics in internet of things(IoT)},
journal = {Global Transitions Proceedings},
volume = {2},
number = {1},
pages = {84-90},
year = {2021},
note = {1st International Conference on Advances in Information, Computing and Trends in Data Engineering (AICDE - 2020)},
issn = {2666-285X},
doi = {https://doi.org/10.1016/j.gltp.2021.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S2666285X21000121},
author = {Jyoti Neeli and Shamshekhar Patil},
keywords = {Internet-of-things authentication, Actuators, Security, Sensors, Access control},
abstract = {The technology named Internet of Things (IoT) extends the ability for the man kind and computers to control billions of connectivity entities such as actuators, sensors, and other services. Realizing IoT as a system would permit integration of the cyber-world in an uninterrupted aspect to the distributive environment and will centrally make changes and authorize the human interaction to the outside world. This paper surveys on the major issues of concerns in IoT regarding security and privacy facing few enormous challenges. To accommodate the most promising technology, we briefly review the existing techniques analyzing the features of security architecture helpful in controlling the perquisites for it. Conventional measures of security countermeasures cannot be directly implied to the heterogeneous technology of IoT as there are number of standards and stacks for communication presumed.}
}
@article{CRONIN20191652,
title = {Flexible manufacturing systems using IIoT in the automotive sector},
journal = {Procedia Manufacturing},
volume = {38},
pages = {1652-1659},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.119},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301207},
author = {Con Cronin and Andrew Conway and Joseph Walsh},
keywords = {Flexible Autonomous Manufacturing, Cellular manufacturing, Industrial Internet of Things (IIoT), Robotics, Automotive},
abstract = {Flexible Manufacturing Systems (FMS) is a domain where progressive research has been compiled and is an integral part offering the ability to get all systems to communicate through IIoT. It is the ability of automotive manufacturers to change processes from ‘it’s the way we have always done it here’ to incorporating small but regular changes to achieve ultimate flexibility that will consolidate competitiveness in a now global emporium. The future of survival, in an increasingly congested industrial environment, can be achieved by making an innovative, technical, organizational and major financial effort to secure ultimate viability from low cost labour countries. This review paper will examine flexible manufacturing systems that utilize IIoT and the key elements to successful implementation of smart manufacturing for the automotive sector.}
}
@article{MARCODETCHART2021740,
title = {Neuro-inspired edge feature fusion using Choquet integrals},
journal = {Information Sciences},
volume = {581},
pages = {740-754},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521010240},
author = {Cedric Marco-Detchart and Giancarlo Lucca and Carlos Lopez-Molina and Laura {De Miguel} and Graçaliz {Pereira Dimuro} and Humberto Bustince},
keywords = {Image processing, Feature extraction, Edge detection, Choquet integral, -integral, Re-aggregation functions},
abstract = {It is known that the human visual system performs a hierarchical information process in which early vision cues (or primitives) are fused in the visual cortex to compose complex shapes and descriptors. While different aspects of the process have been extensively studied, such as lens adaptation or feature detection, some other aspects, such as feature fusion, have been mostly left aside. In this work, we elaborate on the fusion of early vision primitives using generalizations of the Choquet integral, and novel aggregation operators that have been extensively studied in recent years. We propose to use generalizations of the Choquet integral to sensibly fuse elementary edge cues, in an attempt to model the behaviour of neurons in the early visual cortex. Our proposal leads to a fully-framed edge detection algorithm whose performance is put to the test in state-of-the-art edge detection datasets.}
}
@article{DANSOAMOAKO2014409,
title = {ANN Model to Predict the Influence of Chemical and Biological Parameters on Iron and Manganese Accumulation},
journal = {Procedia Engineering},
volume = {70},
pages = {409-418},
year = {2014},
note = {12th International Conference on Computing and Control for the Water Industry, CCWI2013},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2014.02.046},
url = {https://www.sciencedirect.com/science/article/pii/S1877705814000484},
author = {E. Danso-Amoako and T.D. Prasad},
keywords = {Accumulation potential, water distribution network, artificial neural network, discolouration biological oxidation, oxidation},
abstract = {Water companies have adopted sophisticated risk-based management systems for managing water quality in water distribution networks (WDNs). Despite their efforts to comply with the standards for drinking water, they continue to receive customer complaints related to the water quality; discoloration is one such customer complaint. These complaints greatly undermine customers’ confidence in water companies. Discoloration is the result of release of accumulated material on pipe walls under stressed conditions. Therefore, understanding the causes of accumulation and the processes that influence accumulation of material is of paramount importance to water companies. In this paper, initially we identified various chemical and biological processes that highly influence the process of accumulation. Thereafter, using six years of water quality data, collected randomly, an artificial neural network (ANN) model was developed to predict Iron (Fe) and Manganese (Mn) accumulation potential. From the prediction profiler graph of the model, it was observed that increasing aluminium in the range 0 to 120μg/l resulted in an increase in Fe and Mn accumulation potential due to increased sorption capabilities. It was also observed that free chlorine residual FCR has a dual effect on Fe and Mn accumulation potential. A cross-validation coefficient of determination, R2 of 0.70 for the ANN model indicates that the model is likely to predict accumulation potential well on new datasets.}
}
@article{MACIEJEWSKI202114,
title = {The Visual Analytics and Data Exploration Research Lab at Arizona State University},
journal = {Visual Informatics},
volume = {5},
number = {1},
pages = {14-22},
year = {2021},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2020.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X20300929},
author = {Ross Maciejewski and Yuxin Ma and Jonas Lukasczyk},
keywords = {Visualization, Spatiotemporal, Explainable AI, Topology},
abstract = {This article describes the research agenda for the Visual Analytics and Data Exploration Research (VADER) Lab at Arizona State University. Over the past decade, the VADER Lab has focused on creating novel algorithms, tools and visualizations for spatiotemporal data. This article will highlight past success in spatiotemporal analysis, explainable AI, graph mining, and mathematical topology. While, at first, these topics seem largely disjoint, we will describe how the underpinnings of spatiotemporal analysis has informed the various research directions in the VADER Lab, and how this research agenda has served to form a network of strong international collaborations. Finally, we will outline a vision for the Lab’s future research.}
}
@article{WU2021101368,
title = {Unraveling the capabilities that enable digital transformation: A data-driven methodology and the case of artificial intelligence},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101368},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101368},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100121X},
author = {Mengjia Wu and Dilek Cetindamar Kozanoglu and Chao Min and Yi Zhang},
keywords = {Digital transformation, Digital capabilities, Bibliometrics, Topic analysis, Artificial intelligence},
abstract = {Digital transformation (DT) is prevalent in businesses today. However, current studies to guide DT are mostly qualitative, resulting in a strong call for quantitative evidence of exactly what DT is and the capabilities needed to enable it successfully. With the aim of filling the gaps, this paper presents a novel bibliometric framework that unearths clues from scientific articles and patents. The framework incorporates the scientific evolutionary pathways and hierarchical topic tree to quantitatively identify the DT research topics’ evolutionary patterns and hierarchies at play in DT research. Our results include a comprehensive definition of DT from the perspective of bibliometrics and a systematic categorization of the capabilities required to enable DT, distilled from over 10,179 academic papers on DT. To further yield practical insights on technological capabilities, the paper also includes a case study of 9,454 patents focusing on one of the emerging technologies - artificial intelligence (AI). We summarized the outcomes with a four-level AI capabilities model. The paper ends with a discussion on its contributions: presenting a quantitative account of the DT research, introducing a process-based understanding of DT, offering a list of major capabilities enabling DT, and drawing the attention of managers to be aware of capabilities needed when undertaking their DT journey.}
}
@article{YILMA2021243,
title = {Towards a Personalisation Framework for Cyber-Physical-Social System (CPSS)},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {243-248},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.028},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321007369},
author = {Bereket Abera Yilma and Yannick Naudet and Hervé Panetto},
keywords = {Cyber-Physical-Social system, Cyber-Physical System, Personalisation},
abstract = {A Cyber-Physical-Social System (CPSS) is an emerging paradigm often understood as a physical and virtual space of interaction which is cohabited by humans and sensor-enabled smart devices. In such settings, human interaction behaviour is often different from person to person and is guided by complex environmental and natural factors that are not yet fully explored. Thus, ensuring a seamless human-machine interaction in CPSS calls for efficient means of handling human dynamics and bringing interaction experience to a personal level. To this end in this paper, we propose a personalisation framework to support the design of CPSS in recognising and addressing human/social aspects.}
}
@article{SINGH2021176,
title = {Cross-domain secure data sharing using blockchain for industrial IoT},
journal = {Journal of Parallel and Distributed Computing},
volume = {156},
pages = {176-184},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S074373152100112X},
author = {Parminder Singh and Mehedi Masud and M. Shamim Hossain and Avinash Kaur},
keywords = {Blockchain, Industrial Internet of Things (IIoT), Cross-domain data sharing, Smart contract, Authentication},
abstract = {The Industrial Internet of Things (IIoT) enhances smart manufacturing process that escalates productivity through revolutionary techniques. The manufacturing process is sophisticated and complex because of various IoT domains (e.g. industries). A final product is an outcome of the efforts of several departments from different industries. However, this raises the cross-domain communication's privacy and security issues. Cross-domain data sharing for product manufacturing is a challenging research direction. This paper proposes a centralized cloud-based cross-domain data sharing platform using multiple security gateways. The security gateways use the blockchain to store the information into the centralized cloud. Once the application reported a malicious activity, the centralized cloud verifies the concern from the blockchain. Further, an action is taken against the party that performs malicious activity in the security gateways. The algorithms are designed for authentication and transaction of data. The proposed framework is able the secure data movement among different domains globally. The experiment result demonstrates that the proposed security and privacy framework helps to maintain trust among the industries that collaborate on manufacturing across the domains.}
}
@article{QIN2021,
title = {A robust framework combined saliency detection and image recognition for garbage classification},
journal = {Waste Management},
year = {2021},
issn = {0956-053X},
doi = {https://doi.org/10.1016/j.wasman.2021.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S0956053X21006152},
author = {Jiongming Qin and Cong Wang and Xu Ran and Shaohua Yang and Bin Chen},
keywords = {Saliency detection, Image segmentation, Garbage classification, Data fusion},
abstract = {Using deep learning to solve garbage classification has become a hot topic in computer version. The most widely used garbage dataset Trashnet only has garbage images with a white board as background. Previous studies based on Trashnet focus on using different networks to achieve a higher classification accuracy without considering the complex backgrounds which might encounter in practical applications. To solve this problem, we propose a framework that combines saliency detection and image classification to improve the generalization performance and robustness. A saliency network Salinet is adopted to obtain the garbage target area. Then, a smallest rectangle containing this area is created and used to segment the garbage. A classification network Inception V3 is used to identify the segmented garbage image. Images of the original Trashnet are fused with complex backgrounds of the other saliency detection datasets. The fused and original Trashnet are used together for training to improve the robustness to noises and complex backgrounds. Compared with the image classification networks and classic target detection algorithms, the proposed framework improves the accuracy of 0.50% − 15.79% on the testing sets fused with complex backgrounds. In addition, the proposed framework achieves the best performance with a gain of 4.80% in accuracy on the collected actual dataset. The comparisons prove that our framework is more robust to garbage classification in complex backgrounds. This method can be applied to smart trash cans to achieve automatic garbage classification.}
}
@article{BORKOWSKI2019220,
title = {Event-based failure prediction in distributed business processes},
journal = {Information Systems},
volume = {81},
pages = {220-235},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917300030},
author = {Michael Borkowski and Walid Fdhila and Matteo Nardelli and Stefanie Rinderle-Ma and Stefan Schulte},
keywords = {Failure prediction, Event-based systems, Business process management, Machine learning},
abstract = {Traditionally, research in Business Process Management has put a strong focus on centralized and intra-organizational processes. However, today’s business processes are increasingly distributed, deviating from a centralized layout, and therefore calling for novel methodologies of detecting and responding to unforeseen events, such as errors occurring during process runtime. In this article, we demonstrate how to employ event-based failure prediction in business processes. This approach allows to make use of the best of both traditional Business Process Management Systems and event-based systems. Our approach employs machine learning techniques and considers various types of events. We evaluate our solution using two business process data sets, including one from a real-world event log, and show that we are able to detect errors and predict failures with high accuracy.}
}
@article{MOHAMMED2022108493,
title = {An analysis of heuristic metrics for classifier ensemble pruning based on ordered aggregation},
journal = {Pattern Recognition},
volume = {124},
pages = {108493},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108493},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006695},
author = {Amgad M. Mohammed and Enrique Onieva and Michał Woźniak and Gonzalo Martínez-Muñoz},
keywords = {Heuristic optimization, Ensemble selection, Ensemble pruning, Classifier ensemble, Machine learning, Difficult samples, Ordering-based pruning, Classifier complementariness},
abstract = {Classifier ensemble pruning is a strategy through which a subensemble can be identified via optimizing a predefined performance criterion. Choosing the optimum or suboptimum subensemble decreases the initial ensemble size and increases its predictive performance. In this article, a set of heuristic metrics will be analyzed to guide the pruning process. The analyzed metrics are based on modifying the order of the classifiers in the bagging algorithm, with selecting the first set in the queue. Some of these criteria include general accuracy, the complementarity of decisions, ensemble diversity, the margin of samples, minimum redundancy, discriminant classifiers, and margin hybrid diversity. The efficacy of those metrics is affected by the original ensemble size, the required subensemble size, the kind of individual classifiers, and the number of classes. While the efficiency is measured in terms of the computational cost and the memory space requirements. The performance of those metrics is assessed over fifteen binary and fifteen multiclass benchmark classification tasks, respectively. In addition, the behavior of those metrics against randomness is measured in terms of the distribution of their accuracy around the median. Results show that ordered aggregation is an efficient strategy to generate subensembles that improve both predictive performance as well as computational and memory complexities of the whole bagging ensemble.}
}
@article{OGUNDOYIN2021100384,
title = {An efficient authentication scheme with strong privacy preservation for fog-assisted vehicular ad hoc networks based on blockchain and neuro-fuzzy},
journal = {Vehicular Communications},
volume = {31},
pages = {100384},
year = {2021},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2021.100384},
url = {https://www.sciencedirect.com/science/article/pii/S221420962100053X},
author = {Sunday Oyinlola Ogundoyin and Ismaila Adeniyi Kamil},
keywords = {Certificateless authentication, Blockchain, Neuro-fuzzy, Fog computing, SUMO, Denial-of-service (DoS)},
abstract = {Privacy, security and efficiency are important performance issues in vehicular ad hoc network (VANET), which researchers have tried to address in recent years. This research article proposes a lightweight and privacy-preserving certificateless authentication scheme in fog-assisted VANET using blockchain technology and neuro-fuzzy machine learning technique. A new authentication scheme is designed using certificateless signature based on elliptic curve cryptography (ECC) and hash function operation. The scheme utilizes two blockchains to achieve decentralized and transparent transactions and revocation process. In order to prevent denial-of-service attack, in which a roadside unit (RSU) is flooded with a massive amount of fake authentication requests so as to prevent legitimate nodes from being authenticated, a neuro-fuzzy algorithm is implemented to proactively detect and discard any anomalous requests prior to an authentication process. Moreover, the scheme is demonstrated to be semantically secure in the random oracle model (ROM) based on the intractability of the discrete logarithm problem (DLP). A panoptic analysis shows that the scheme possesses outstanding attributes required for a secure vehicular communication system. The experimental simulation is conducted using simulation of urban mobility (SUMO) and the broadly-accepted network simulator NS-3. The results indicate that the proposed scheme has a high efficiency in terms of transmission delay and message delivery rate. The comparative analysis with the state-of-the-art schemes reveals that the proposed scheme has an improvement of 50%–90.5% in computation cost and 38.46%–69.6% in communication overhead. The simulation result of the neuro-fuzzy gives an accuracy of 91.5% and further analysis shows that it significantly reduces the computation burden on the RSU proportionately with increase in the number of malicious messages.}
}
@article{COSTA2022116413,
title = {A smart sensor-data-driven optimization framework for improving the safety of excavation operations},
journal = {Expert Systems with Applications},
volume = {193},
pages = {116413},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116413},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421017000},
author = {Alberto Costa and Ze-Zhou Wang and Siang Huat Goh and Ian F.C. Smith},
keywords = {Sensor data, Surrogate model, Excavation, Systematic uncertainty, Derivative-free optimization, Physical-based behavior model},
abstract = {Excavation is a complex multistage problem, where field responses of soil properties such as deflections at one stage of the operation depend on responses at the preceding stage. In order to help asset managers make better decisions and thus improve safety, soil properties should be accurately identified using sensor-data collected at the current stage. This task is not easy to accomplish, mainly because of its intrinsic ambiguity. Sensors usually only measure effects (e.g., field responses) but not causes (e.g., soil parameter values). A strategy that helps meet this challenge is to perform inverse analysis to validate soil parameter values. Error-Domain Model Falsification (EDMF) is a methodology that achieves this goal. More precisely, EDMF helps identify good behavior models of excavation by falsifying soil parameter values for which the predictions of the corresponding behavior models cannot explain field-response measurements collected by sensors. However, a remaining challenge is the identification of soil parameter values that are not falsified by EDMF, especially when the computation of the predictions is time-consuming. This paper proposes a new framework that combines EDMF and an optimization algorithm for efficient identification of soil parameter values. Results on a full-scale excavation site in Singapore show that the new framework is robust and accurate, and it has the potential to improve current practice, which relies primarily on surrogate models without uncertainty.}
}
@article{BAGIES2022108688,
title = {Content delivery network for IoT-based Fog Computing environment},
journal = {Computer Networks},
volume = {205},
pages = {108688},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108688},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621005557},
author = {Enas Bagies and Ahmed Barnawi and Saoucene Mahfoudh and Neeraj Kumar},
keywords = {SDN, Fog Computing, CDN, IoT},
abstract = {With the continuous improvement and evolution of network technologies like IoT which demands the innovation and improvement of legacy networks, as well as, the requirements of end users’ Quality of Service (QoS) and Quality of Experience (QoE), new network paradigms have been invented, such as, Content Delivery Network (CDN), IoT and Fog Computing. In addition, the management and controlling of such a network may create an overhead to the network administrators. Therefore, Software-defined Network (SDN) has been introduced as a framework for managing and controlling the network devices where the network abstraction facilitates the introduction innovative solutions and improve the overall performance. In this paper, an SDN based architecture to implement fog-based CDN network was proposed aiming at further improvement on the routing functionalities essential in Fog-based CDN deployment. We have tested the proposed architecture and the results show significant gains. Moreover, we have identified some factors to be considered in such architectures for further performance improvement.}
}
@article{PAN2021103462,
title = {Digital interoperability and transformation in logistics and supply chain management: Editorial},
journal = {Computers in Industry},
volume = {129},
pages = {103462},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103462},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000695},
author = {Shenle Pan and Damien Trentesaux and Duncan McFarlane and Benoit Montreuil and Eric Ballot and George Q. Huang},
keywords = {Digital interoperability, Digital transformation, Logistics, Supply chain management, Editorial, Special issue},
abstract = {This editorial introduces this Special Issue on advances in research on digital interoperability and transformation in logistics and supply chain management. Eleven high-quality and original research works from both researchers and practitioners in the area have been selected to compose this Special Issue. This editorial first introduces the scientific context relevant to the Special Issue, then presents each of the eleven papers. From these papers, this editorial identifies several interesting prospective works, which are finally presented.}
}
@article{YU2021240,
title = {Deep regression for LiDAR-based localization in dense urban areas},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {172},
pages = {240-252},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620303518},
author = {Shangshu Yu and Cheng Wang and Zenglei Yu and Xin Li and Ming Cheng and Yu Zang},
keywords = {LiDAR-based localization, Deep regression, Multi-task learning, Residual connection, Inter-task constraint loss},
abstract = {LiDAR-based localization in a city-scale map is a fundamental question in autonomous driving research. As a reasonable localization scheme, the localization can be performed by global retrieval (that suggests potential candidates from the database) followed by geometric registration (that obtains an accurate relative pose). In this work, we develop a novel end-to-end, deep multi-task network that simultaneously performs global retrieval and geometric registration for LiDAR-based localization. Both retrieval and registration are formulated and solved as regression problems, and they can be deployed independently during inference time. We also design two mechanisms to enhance our multi-task regression network’s performance: residual connections for point clouds and a new loss function with learnable parameters. To alleviate the common phenomenon of vanishing gradients in neural networks, we employ residual connections to support constructing a deeper network effectively. At the same time, to solve the problem of huge differences in scale and units between different tasks, we propose a loss function that can automatically balance multi-tasks. Experiments on two public benchmarks validate the state-of-the-art performance of our algorithm in large-scale LiDAR-based localization.}
}
@article{HU2021102605,
title = {Scale-sets image classification with hierarchical sample enriching and automatic scale selection},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {105},
pages = {102605},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102605},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421003123},
author = {Zhongwen Hu and Tiezhu Shi and Chisheng Wang and Qingquan Li and Guofeng Wu},
keywords = {Object-based image classification, Scale-sets structure, Multiscale analysis, Scale parameter estimation, Sample enriching, Land useland cover},
abstract = {Object-based image analysis (OBIA) has been widely used for classifying high-spatial-resolution images, and the selection of scale parameter(s) is inevitable in previous OBIA tasks. However, selecting appropriate scale(s) for an application inherently depends on the objective of the application, which cannot be robustly solved at the segmentation stage. In this study, a novel framework without any scale parameter was proposed for object-based image classification. It consists of four major steps: (1) Scale-sets image representation: Multiscale segments from a region-merging algorithm are organized using a scale-sets structure, where each segment is represented as a node of the hierarchy. Note that the implementation of this step does not need any scale parameter. (2) Multiscale sample selection and enriching: Multiscale segmentation results are retrieved from the scale-sets structure and visualized, and then training samples are selected from multiple scales. The training samples are further enriched according to hierarchical relations. (3) Feature extraction and multiscale classification: Segments are described using spectral, textural and geometric features, and then classified using a Random Forest classifier. Through this step, all segments are labeled with a class label, and a classified scale-sets structure is obtained. (4) Automatic optimal classification map selection: A multiscale accuracy assessment is conducted to evaluate the performances at different scales, and the optimal classification map is selected. A QuickBird image and a Gaofen-2 image were used to demonstrate the effectiveness and advantages of the proposed approach. The experimental results demonstrated that the optimal scale of an object-based image classification work is a range, not a single value. Moreover, the multiscale accuracies obtained using training samples and ground truth maps showed the same tendency. Therefore, it is possible to automatically estimate the optimal classification scale using training samples. Besides, we have demonstrated the effectiveness of using hierarchical enriching strategy to improve the performance of object-image classification.}
}
@article{SHEN2022108221,
title = {Learning scale awareness in keypoint extraction and description},
journal = {Pattern Recognition},
volume = {121},
pages = {108221},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108221},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004027},
author = {Xuelun Shen and Cheng Wang and Xin Li and Yifan Peng and Zijian He and Chenglu Wen and Ming Cheng},
keywords = {Keypoint detection, Keypoint description, Image matching, Structure from motion, 3D reconstruction},
abstract = {To recover relative camera motion accurately and robustly, establishing a set of point-to-point correspondences in the pixel space is an essential yet challenging task in computer vision. Even though multi-scale design philosophy has been used with significant success in computer vision tasks, such as object detection and semantic segmentation, learning-based image matching has not been fully exploited. In this work, we explore a scale awareness learning approach in finding pixel-level correspondences based on the intuition that keypoints need to be extracted and described on an appropriate scale. With that insight, we propose a novel scale-aware network and then develop a new fusion scheme that derives high-consistency response maps and high-precision descriptions. We also revise the Second Order Similarity Regularization (SOSR) to make it more effective for the end-to-end image matching network, which leads to significant improvement in local feature descriptions. Experimental results run on multiple datasets demonstrate that our approach performs better than state-of-the-art methods under multiple criteria.}
}
@article{ABDULRAHMAN2021102986,
title = {A framework to simplify pre-processing location-based social media big data for sustainable urban planning and management},
journal = {Cities},
volume = {109},
pages = {102986},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102986},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120313342},
author = {Mohammed Abdul-Rahman and Edwin H.W. Chan and Man Sing Wong and Victor E. Irekponor and Maryam O. Abdul-Rahman},
keywords = {Big data, Community resilience, Internet of things, Natural language processing, Social media, Urban planning and management},
abstract = {Over the last decade, 90% of Big Data has been generated by people living in urban areas. With the advent of Internet of Things (IoT) and the increased use of the internet, Social Media has become an integral part of people's daily lives. Millions of unstructured data are being sent to the cloud every second, providing opinions practically on any discourse. This makes microblogs such as Twitter, Instagram, WeChat, and Facebook smart instruments for urban planners to harvest ‘big data’ on socioeconomics, urban dynamics, transportation, land uses, resilience, etc. This study proposed a framework for social media big data mining and data analytics using Twitter. It demonstrated the functionalities of the framework on a case study using Natural Language Processing and Machine Learning techniques like Latent Dirichlet Allocation and VADER Sentiment Analysis to mine, clean, process, and validate the data. The validated results from the case study showed high accuracy that Social Media Big Data can be used to study the spatiotemporal dynamism of community challenges.}
}
@article{FILHO202018,
title = {A fog-enabled smart home solution for decision-making using smart objects},
journal = {Future Generation Computer Systems},
volume = {103},
pages = {18-27},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.045},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18322301},
author = {Geraldo P. Rocha Filho and Rodolfo I. Meneguette and Guilherme Maia and Gustavo Pessin and Vinícius P. Gonçalves and Li Weigang and Jó Ueyama and Leandro A. Villas},
abstract = {The development of new smart objects for the sensing and actuation of a given place or environment led both the academia and industry to research and propose new protocols and intelligent systems to support such objects. One of the systems that has been gaining prominence is the smart residential environments. In this context, homes are equipped with smart objects to manage the living resources. However, managing such objects in residential environments requires data contextualization, i.e. collecting data from heterogeneous devices and actuate on the environment through context information generated from such data. To solve this problem, we propose an intelligent decision system based on the fog computing paradigm, which provides an efficient management of residential applications. The proposed solution is evaluated both in simulated and real environments. When compared with other studies from the literature in a simulated environment, the proposed solution shows a higher success rate with a lower delay in the decision-making process, higher efficiency in information dissemination with a lower overhead in the communication infrastructure, and increased robustness in processing with a lower power consumption. These results are also observed when considering a real environment evaluation.}
}
@article{APPEL2014161,
title = {Predictive analytics can facilitate proactive property vacancy policies for cities},
journal = {Technological Forecasting and Social Change},
volume = {89},
pages = {161-173},
year = {2014},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2013.08.028},
url = {https://www.sciencedirect.com/science/article/pii/S0040162513002138},
author = {Sheila U. Appel and Derek Botti and James Jamison and Leslie Plant and Jing Y. Shyr and Lav R. Varshney},
keywords = {Property vacancy, Systems of systems, Predictive analytics, Urban planning},
abstract = {Is it possible for a city to understand, analyze, predict, and therefore prevent vacant properties? In this paper, we demonstrate the feasibility of using techniques from machine learning and data mining to determine the future vacancy risks for individual properties and for neighborhoods using a variety of structural, demographic, socioeconomic, and city activity features with high accuracy. Within a larger systems-of-systems framework that we develop, these predictive analytics will allow a city to move from decision-making based on ‘educated anecdotes’ and reactive strategies aimed at the most urgent need, to policy development based on informed, holistic insight and proactive interventions that prevent and reverse decline. A demonstration of the use of predictive analytics within the sociotechnical system is provided using data from Syracuse, New York.}
}
@article{ZHANG2022126297,
title = {Multi-objective optimisation design for GFRP tendon reinforced cemented soil},
journal = {Construction and Building Materials},
volume = {320},
pages = {126297},
year = {2022},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2021.126297},
url = {https://www.sciencedirect.com/science/article/pii/S0950061821040277},
author = {Genbao Zhang and Changfu Chen and Kefei Li and Fan Xiao and Junbo Sun and Yufei Wang and Xiangyu Wang},
keywords = {Cemented soil, Element pullout test, Interface bond strength, Unconfined compressive strength, Glass fiber reinforced polymer reinforcement, Machine learning, Multi-objective optimisation},
abstract = {Rebar reinforced cemented soil is employed widely to solve the weak foundation problem led by sludge particularly. Nowadays, the glass fiber-reinforced polymer (GFRP) becomes a new tendon material instead of steel to avoid the performance degradation resulting from steel corrosion. The interface bond strength of GFRP tendon-reinforced cemented soils (GTRCS) displays its excellent mechanical capacity. Nevertheless, its application is obstructed by the deficient studies between the bond strength and influence factors. Therefore, this study investigates the effects of varying water contents (Cw: 50%-90%), cement proportions (Cc: 6%-30%), and curing periods (Tc: 28 days, 90 days) on both pullout strength (Tp) and unconfined compression strength (UCS) of GTRCS. The results showed that the pullout strength and compressive strength were positively related to Tc and Cc and negatively related to Cw. Besides, these experimental results were also utilised to develop support vector regression (SVR) models. The beetle antennae search (BAS) algorithm was used to adjust the SVR’s hyperparameters. The high correlation coefficients (0.988 for UCS and 0.972 for Tp) proved the reliability of the established BAS-SVR models. In addition, the multi-objective beetle antennae search algorithm (MOBAS-SVR) was developed for bi-objective optimisation designs (UCS-cost and Tp-cost). Finally, sensitivity analysis was conducted to range the significance of variables for Tp and UCS.}
}
@article{ZHAO2020226,
title = {Intelligent city intelligent medical sharing technology based on internet of things technology},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {226-233},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20300170},
author = {Xin Zhao and Wei Xiao and Lu Wu and Zhigang Zhao and Jidong Huo and Shi Wang and Zhenhua Guo and Dianmin Sun},
keywords = {Internet of things, Intelligent city, Intelligent medical sharing, Data encryption, Information security},
abstract = {Because of the imperfect domestic public medical management system, high medical costs, less channels and other problems troubling people’s livelihood. In particular, medical problems represented by ”inefficient medical system, poor quality of medical services, medical difficulties and high costs” are the focus of social concern. Therefore, the purpose of this paper is to establish a set of intelligent medical information network platform system, so that patients can enjoy safe, convenient and high-quality diagnosis and treatment services on the basis of short waiting time and basic medical expenses. Fundamentally solve the problem of ”difficult and expensive medical treatment”, and truly achieve ”everyone’s health, everyone’s health”. This paper designs and develops a medical distributed data sharing and integration system based on distributed heterogeneous data sets of medical data, and applies it to some local first-class hospitals. Finally, after questionnaire and interview survey, it is concluded that compared with the previous medical treatment, the sharing of intelligent medicine among the general public has increased by about 79%, and the medical environment has been greatly improved.}
}
@article{CANDELIERI2015844,
title = {Short-term forecasting of hourly water consumption by using automatic metering readers data},
journal = {Procedia Engineering},
volume = {119},
pages = {844-853},
year = {2015},
note = {Computing and Control for the Water Industry (CCWI2015) Sharing the best practice in water management},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.08.948},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815026181},
author = {Antonio Candelieri and Davide Soldi and Francesco Archetti},
keywords = {Short-term demand forecasting, Time-series mining, Time-series clustering, Support Vector Machines regression},
abstract = {A completely data-driven, fully adaptive self-learning algorithm for water demand forecasting in the short-term and with hourly periodicity is proposed, according to the renewed interest generated by the availability of new technological solutions such as Automatic Metering Readers (AMR), a key enabler of the “Smart Water” paradigm. The approach is based on two sequential stages: at the first stage (time-series clustering) the daily water demand patterns (i.e., time-series of hourly data) are analysed to identify a limited set of typical behaviours. At the second stage Support Vector Machine regression is used to obtain one specific forecasting model (consisting of a regression model for each hour) for each cluster identified at the first stage. The approach has been validated on real data acquired by AMRs deployed on the Italian pilot site of ICeWater, computing the widely adopted error measure MAPE (Mean Absolute Percentage Error).}
}
@article{LOUREIRO2021911,
title = {Artificial intelligence in business: State of the art and future research agenda},
journal = {Journal of Business Research},
volume = {129},
pages = {911-926},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320307451},
author = {Sandra Maria Correia Loureiro and João Guerreiro and Iis Tussyadiah},
keywords = {Artificial Intelligence, Intelligent agent, Business applications, Text mining, Research agenda, Future trends},
abstract = {This study provides an overview of state-of-the-art research on Artificial Intelligence in the business context and proposes an agenda for future research. First, by analyzing 404 relevant articles collected through Web of Science and Scopus, this article presents the evolution of research on AI in business over time, highlighting seminal works in the field, and the leading publication venues. Next, using a text-mining approach based on Latent Dirichlet Allocation, latent topics were extracted from the literature and comprehensively analyzed. The findings reveal 18 topics classified into four main clusters: societal impact of AI, organizational impact of AI, AI systems, and AI methodologies. This study then presents several main developmental trends and the resulting challenges, including robots and automated systems, Internet-of-Things and AI integration, law, and ethics, among others. Finally, a research agenda is proposed to guide the directions of future AI research in business addressing the identified trends and challenges.}
}