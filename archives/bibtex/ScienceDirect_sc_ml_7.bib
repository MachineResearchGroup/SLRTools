@article{VAZQUEZCANTELI2017415,
title = {Balancing comfort and energy consumption of a heat pump using batch reinforcement learning with fitted Q-iteration},
journal = {Energy Procedia},
volume = {122},
pages = {415-420},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.429},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217332629},
author = {José Vázquez-Canteli and Jérôme Kämpf and Zoltán Nagy},
keywords = {artificial intelligence, energy management, thermal comfort, building simulation, energy storage},
abstract = {In this study, a heat pump satisfies the heating and cooling needs of a building, and two water tanks store heat and cold respectively. Reinforcement learning (RL) is a model-free control approach that can learn from the behaviour of the occupants, weather conditions, and the thermal behaviour of the building in order to make near-optimal decisions. In this work we use of a specific RL technique called batch Q-learning, and integrate it into the urban building energy simulator CitySim. The goal of the controller is to reduce the energy consumption while maintaining adequate comfort temperatures.}
}
@article{ELBARACHI2021127820,
title = {A novel sentiment analysis framework for monitoring the evolving public opinion in real-time: Case study on climate change},
journal = {Journal of Cleaner Production},
volume = {312},
pages = {127820},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127820},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621020382},
author = {May {El Barachi} and Manar AlKhatib and Sujith Mathew and Farhad Oroumchian},
keywords = {Smart cities, Sentiment analysis, Opinion leaders, Social media analytics, Climate change},
abstract = {Smart city analytics involves tracking, interpreting, and evaluating the sentiments and emotions that are shared via online social media channels. Sentiment analysis of social media posts has become increasingly prominent in recent years as a means of gaining insights into how members of the public perceive current affairs. The ongoing research in this domain has leveraged multiple types of sentiment analysis. However, although the existing approaches enable researchers to acquire retrospective insights into public opinion, they do not enable a real-time evaluation. In addition, they are not readily scalable and necessitate the analysis of a significant amount of posts (in the millions) to facilitate a more in-depth evaluation. The study outlined in this paper was designed to address these shortcomings by presenting a framework that facilitates a real-time evaluation of the evolution of opinions shared by prominent public features and their respective followers; that is, high-impact posts. The developed solution encompasses a sophisticated Bi-directional LSTM classifier that was implemented and tested using a dataset consisting of 278,000 tweets related to the topic of climate change. The outcomes reveal that the proposed classifier achieved the following accuracies: 88.41% for discrimination; 89.66% for anger; 87.01% for inspiration; and 87.52% for joy - with negative emotions being more accurately classified than positive emotions. Similarly, the sentiment classification performance yielded accuracies of 89.32% for support and 89.80% for strong support, as well as 88.14% for opposition and 87.52% for strong opposition. In addition, the findings of the study indicated that geographic location, chosen topic, cultural sensitivities, and posting frequency all play a critical role in public reactions to posts and the ensuing perspectives they adopt. The solution stands out from existing retrospective analysis methods because it does not rely on the analysis of vast quantities of data records; rather, it can perform real-time, high-impact content analysis in a resource-efficient and sustainable manner. This framework can be used to generate insights into how public opinion is developing on a real-time basis. As such, it can have meaningful application within social media analysis efforts.}
}
@article{SARKER2020102762,
title = {ABC-RuleMiner: User behavioral rule-based machine learning method for context-aware intelligent services},
journal = {Journal of Network and Computer Applications},
volume = {168},
pages = {102762},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102762},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520302368},
author = {Iqbal H. Sarker and A.S.M. Kayes},
keywords = {Machine learning, Association rule mining, Non-redundancy, Context-aware computing, Decision making, Personalization, Mobile data analytics, Rule-based system, Intelligent services.},
abstract = {This paper formulates the problem of a rule-based machine learning method to discover the behavioral rules of individual smartphone users to provide context-aware intelligent services. Smartphones nowadays are considered as one of the most important Internet-of-Things (IoT) devices for providing various context-aware personalized services. These devices can record individuals' contextual data - for example, temporal, spatial, or social contexts, and their daily behavioral activity records. Association rule mining (ARM) is the most popular rule-based machine learning method for discovering rules for a particular constraint preference utilizing a given dataset. However, it generates numerous uninteresting contextual associations which lead to generate huge number of redundant rules that become useless in making context-aware decisions. This redundant generation makes not only the rule-set unnecessarily large but also makes the context-aware decision making process more complex and ineffective. To minimize these issues, in this paper, we propose a rule-based machine learning method “ABC-RuleMiner” that effectively identifies the redundancy in associations, and discovers a set of non-redundant behavioral rules (IF-THEN) for individual users by taking into account the precedence of relevant contexts. Our experiments on individuals’ contextual smartphone datasets show that this rule discovery approach is more effective while comparing with traditional rule-based methods.}
}
@article{KLAIB2021114037,
title = {Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies},
journal = {Expert Systems with Applications},
volume = {166},
pages = {114037},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114037},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308071},
author = {Ahmad F. Klaib and Nawaf O. Alsrehin and Wasen Y. Melhem and Haneen O. Bashtawi and Aws A. Magableh},
keywords = {Eye tracking techniques, Eye tracking applications, Electrooculography, Infrared oculography, Internet of Things, Machine learning, Scleral coil, Video oculography, Cloud computing, Fog computing, Choice modeling, Consumer psychology, Marketing},
abstract = {Eye tracking is the process of measuring where one is looking (point of gaze) or the motion of an eye relative to the head. Researchers have developed different algorithms and techniques to automatically track the gaze position and direction, which are helpful in different applications. Research on eye tracking is increasing owing to its ability to facilitate many different tasks, particularly for the elderly or users with special needs. This study aims to explore and review eye tracking concepts, methods, and techniques by further elaborating on efficient and effective modern approaches such as machine learning (ML), Internet of Things (IoT), and cloud computing. These approaches have been in use for more than two decades and are heavily used in the development of recent eye tracking applications. The results of this study indicate that ML and IoT are important aspects in evolving eye tracking applications owing to their ability to learn from existing data, make better decisions, be flexible, and eliminate the need to manually re-calibrate the tracker during the eye tracking process. In addition, they show that eye tracking techniques have more accurate detection results compared with traditional event-detection methods. In addition, various motives and factors in the use of a specific eye tracking technique or application are explored and recommended. Finally, some future directions related to the use of eye tracking in several developed applications are described.}
}
@article{WAMBUGU2021102515,
title = {A hybrid deep convolutional neural network for accurate land cover classification},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102515},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102515},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002221},
author = {Naftaly Wambugu and Yiping Chen and Zhenlong Xiao and Mingqiang Wei and Saifullahi {Aminu Bello} and José {Marcato Junior} and Jonathan Li},
keywords = {Deep learning, very high spatial resolution (VHSR), Remote sensing, Dilated convolution, Deep supervision, Spatial attention},
abstract = {Land cover classification provides updated information regarding the Earth's resources, which is vital for agricultural investigation, urban management, and disaster monitoring. Current advances in sensor technology on satellite and aerial remote sensing (RS) devices have improved the spatial-spectral, radiometric, and temporal resolutions of images over time. These improvements offer invaluable chances of understanding land cover information. However, land cover classification from RS images is an intricate task because of the high intra-class disparities, low inter-class similarities, and image variation types. We propose a cascaded residual dilated network (CRD-Net) for land cover classification using very high spatial resolution (VHSR) images to address these challenges. The proposed hybrid network follows the encoder-decoder concept with a spatial attention block to guide the network on learnable discriminate features coupled with an intermediary loss to enhance the training process. Moreover, a cascaded residual dilated module increases the network's receptive field to enrich multi-contextual features further, thus boosting the resultant feature descriptor. Extensive experimental results demonstrate that the proposed CRD-Net outperformed state-of-the-art methods, achieving an overall accuracy (OA) of 90.73% and 90.51% on the ISPRS Potsdam land cover dataset and ISPRS Vaihingen dataset, respectively.}
}
@article{SUNG2021102142,
title = {On the training of a neural network for online path planning with offline path planning algorithms},
journal = {International Journal of Information Management},
volume = {57},
pages = {102142},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102142},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219317918},
author = {Inkyung Sung and Bongjun Choi and Peter Nielsen},
keywords = {Path planning, Online path planning, Neural network, Offline path planning, Automated vehicle control, Data-driven control},
abstract = {One of the challenges in path planning for an automated vehicle is uncertainty in the operational environment of the vehicle, demanding a quick but sophisticated control of the vehicle online. To address this online path planning issue, neural networks, which can derive a heading for an operating vehicle in a given situation, have been actively studied, demonstrating their satisfactory performance. However, the study on the training path data, which specifies the desired output of a neural network and in turn influences the behavior of the neural network, has been neglected in the literature. Motivated by this fact, in this paper, we first generate different training path data sets applying two different offline path planning algorithms and evaluate the performance of a neural network as an online path planner depending on the training data under a simulation environment. We further investigate the properties of the training data that make a neural network more reliable for online path planning.}
}
@article{DEY2019205,
title = {A machine learning based intrusion detection scheme for data fusion in mobile clouds involving heterogeneous client networks},
journal = {Information Fusion},
volume = {49},
pages = {205-215},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518306110},
author = {Saurabh Dey and Qiang Ye and Srinivas Sampalli},
keywords = {Intrusion detection, Data fusion, Mobile cloud computing, Heterogeneous networks, Machine learning},
abstract = {The combination of traditional cloud computing and mobile computing leads to the novel paradigm of mobile cloud computing. Due to the mobility of network nodes in mobile cloud computing, security has been a challenging problem of paramount importance. When a mobile cloud involves heterogeneous client networks, such as Wireless Sensor Networks and Vehicular Networks, the security problem becomes more challenging because the client networks often have different security requirements in terms of computational complexity, power consumption, and security levels. To securely collect and fuse the data from heterogeneous client networks in complex systems of this kind, novel security schemes need to be devised. Intrusion detection is one of the key security functions in mobile clouds involving heterogeneous client networks. A variety of different rule-based intrusion detection methods could be employed in this type of systems. However, the existing intrusion detection schemes lead to high computation complexity or require frequent rule updates, which seriously harms their effectiveness. In this paper, we propose a machine learning based intrusion detection scheme for mobile clouds involving heterogeneous client networks. The proposed scheme does not require rule updates and its complexity can be customized to suit the requirements of the client networks. Technically, the proposed scheme includes two steps: multi-layer traffic screening and decision-based Virtual Machine (VM) selection. Our experimental results indicate that the proposed scheme is highly effective in terms of intrusion detection.}
}
@article{ARARAL2020102873,
title = {Why do cities adopt smart technologies? Contingency theory and evidence from the United States},
journal = {Cities},
volume = {106},
pages = {102873},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102873},
url = {https://www.sciencedirect.com/science/article/pii/S026427512031221X},
author = {Eduardo Araral},
keywords = {Smart cities, Managerial contingency theory, Institutions, United States},
abstract = {Little is known why cities adopt smart technologies. This study is the first to develop and test a managerial contingency theory to explain variations in the adoption of smart city technologies. The theory is tested using the Akaike Information Criterion Stepwise OLS regression model with data from 329 cities in the United States and 13 smart city technologies. The study finds that adoption of smart city technologies is indeed contingent with managerial incentives, constraints and context. Funding from state governments, availability of technical assistance and city branding have positive and statistically significant effects on smart technology adoption. Conversely, lack of leadership, legacy systems and lack of understanding of technology have negative and statistically significant effects. Demography, geography and form of government are also associated with technology adoption. Managerial contingency theory opens a new field of research on smart cities.}
}
@article{BOGAERTS202062,
title = {A graph CNN-LSTM neural network for short and long-term traffic forecasting based on trajectory data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {112},
pages = {62-77},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X19309349},
author = {Toon Bogaerts and Antonio D. Masegosa and Juan S. Angarita-Zapata and Enrique Onieva and Peter Hellinckx},
keywords = {Deep learning, Graph convolutional network, LSTM, Traffic forecasting, Trajectory data, GPS data, Long term, Short term, ITS},
abstract = {Traffic forecasting is an important research area in Intelligent Transportation Systems that is focused on anticipating traffic in order to mitigate congestion. In this work we propose a deep neural network that simultaneously extracts the spatial features of traffic, using graph convolution, and its temporal features by means of Long Short Term Memory (LSTM) cells to make both short-term and long-term predictions. The model is trained and tested using sparse trajectory (GPS) data coming from the ride-hailing service of DiDi in the cities of Xi'an and Chengdu in China. Besides, presenting the deep neural network, we also propose a data-reduction technique based on temporal correlation to select the most relevant road links to be used as input. Combining the suggested approaches, our model obtains better results compared to high-performance algorithms for traffic forecasting, such as LSTM or the algorithms presented in the TRANSFOR19 forecasting competition. The model is capable of maintaining its performance over different time-horizons from 5 min to up to 4 h with multi-step predictions.}
}
@article{DAI2021103621,
title = {Optimal logistics transportation and route planning based on fpga processor real-time system and machine learning},
journal = {Microprocessors and Microsystems},
volume = {80},
pages = {103621},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103621},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120307687},
author = {Xuezhen Dai and Meiqi Chen and Yanan Zhou},
keywords = {Logistics, Route planning, Transportation, Machine learning, Field programmable gate array},
abstract = {The shared bus's development requirements, relief of traffic congestion in urban areas, and improved utilization of the road resources providing the transport mode of the excellent user experience neotype are very urgent. To predict precise travel needs, the key for planning a dynamic routing lie a lie of the shared bus implementation. However, the shared bus data's sparse and high volatility will require a lot of resistance to predict travel accurately. Based on the user experience, very different from the traditional public transportation that is far more challenging to the relatively high number of optimization goals is because passengers of the shared bus route planning and shared bus route planning. This article, based on the shared bus data from different audiences sources, travel demand prediction and dynamic route planning in "the last mile", and a two-step process that consists of the shared bus dynamic routing (sub-bus), proposed and your scene. First of all, such traffic, time, week, location, and five of the prediction function such as a bus, to analyze residents' travel behavior to prepare the travel demand based on them precisely the machine learning model used to predict. Secondly, dynamically and predict the results of multiple operations bus optimal routing, designed to generate a fixed based on the shared bus destinations' operating characteristics, a dynamic programming algorithm wants below. Several experiments, based on the shared subway shuttle bus of evidence that people of the data and the reality has been purchased, the sub-bus is better than the method of dynamic route planning, etc. for the scene of such a "last mile".}
}
@article{PIKHART20201412,
title = {Intelligent information processing for language education: The use of artificial intelligence in language learning apps},
journal = {Procedia Computer Science},
volume = {176},
pages = {1412-1419},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.151},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320512},
author = {Marcel Pikhart},
keywords = {artificial intelligence, mobile apps, language education, applied linguistics, educational sustainability},
abstract = {This paper attempts to bring new ideas regarding the use of artificial intelligence in language learning apps. Artificial intelligence has seen a dramatic increase and its utilization in various aspects of data science, information science and various platforms is unprecedented. Its ubiquitousness is evident in smart cities, in online marketing, and data mining, however, its utilization in language learning apps is still somehow neglected. There are a few questions, such as why it is so, etc., this paper attempts to address. The research focuses on several most used language learning apps and the presence of artificial intelligence in them. The findings of the paper are as follows: basically none of the analysed apps uses any kind of machine learning, artificial intelligence or deep learning, and they are mostly based on predefined algorithms that do not utilize the full potential of the computational power we have currently available. The paper also suggests possible solutions and brings practical advice on how to implement artificial intelligence in these apps. The paper is important for any education innovation at the beginning of the 21 century. Without his innovative approach, education will lack sustainability and competitiveness, which will present a serious threat to the whole educational system.}
}
@article{HASEEB2021102779,
title = {Intelligent and secure edge-enabled computing model for sustainable cities using green internet of things},
journal = {Sustainable Cities and Society},
volume = {68},
pages = {102779},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102779},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721000718},
author = {Khalid Haseeb and Ikram {Ud Din} and Ahmad Almogren and Imran Ahmed and Mohsen Guizani},
keywords = {Data security, Edge computing, Green internet of things, Intelligent routing, Deep learning},
abstract = {Internet of Things (IoT) consists of a huge number of sensors along with physical things to gather and forward data intelligently. Green IoT applications based on Wireless Sensor Networks (WSNs) are developed in various domains, such as medical, engineering, industry, and smart cities to grow the production. To increase the performance of sustainable cities, communicating nodes are interconnected autonomously to observe the environment, where they need to be more energy-efficient. Edge computing operates in a distributed manner and improves the response time with the least latency through various edge servers. Although the integration of edge computing and Green IoT significantly improves the network performance in terms of computation and data storage, low powered sensors have constraints in terms of battery power, low transmission range, and security aspects. Therefore, adopting an emerging solution is needed to offer energy services with secure data delivery for sustainable cities. This paper presents an intelligent and secure edge-enabled computing (ISEC) model for sustainable cities using Green IoT, which aims to develop the communication strategy with decreasing the liability in terms of energy management and data security for data transportation. The proposed model generates optimal features using deep learning for data routing, which may help to train the sensors for predicting the finest routes toward edge servers. Moreover, the integration of distributed hashing with chaining strategy eases security solutions with efficient computing system. The experimental results reveal the improved performance of the proposed ISEC model against other solutions for energy consumption by 21 %, network throughput by 15 %, end-to-end delay by 12 %, route interruption by 36 %, and network overhead by 52 %.}
}
@article{SUN2019185,
title = {Region-of-interest undersampled MRI reconstruction: A deep convolutional neural network approach},
journal = {Magnetic Resonance Imaging},
volume = {63},
pages = {185-192},
year = {2019},
issn = {0730-725X},
doi = {https://doi.org/10.1016/j.mri.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0730725X19301870},
author = {Liyan Sun and Zhiwen Fan and Xinghao Ding and Yue Huang and John Paisley},
keywords = {Deep convolutional neural network, Magnetic resonance imaging, Image reconstruction, Region of interest},
abstract = {Compressive sensing enables fast magnetic resonance imaging (MRI) reconstruction with undersampled k-space data. However, in most existing MRI reconstruction models, the whole MR image is targeted and reconstructed without taking specific tissue regions into consideration. This may fails to emphasize the reconstruction accuracy on important and region-of-interest (ROI) tissues for diagnosis. In some ROI-based MRI reconstruction models, the ROI mask is extracted by human experts in advance, which is laborious when the MRI datasets are too large. In this paper, we propose a deep neural network architecture for ROI MRI reconstruction called ROIRecNet to improve reconstruction accuracy of the ROI regions in under-sampled MRI. In the model, we obtain the ROI masks by feeding an initially reconstructed MRI from a pre-trained MRI reconstruction network (RecNet) to a pre-trained MRI segmentation network (ROINet). Then we fine-tune the RecNet with a binary weighted ℓ2 loss function using the produced ROI mask. The resulting ROIRecNet can offer more focus on the ROI. We test the model on the MRBrainS13 dataset with different brain tissues being ROIs. The experiment shows the proposed ROIRecNet can significantly improve the reconstruction quality of the region of interest.}
}
@article{BANERJEE2020100380,
title = {RL-Sleep: Temperature Adaptive Sleep Scheduling using Reinforcement Learning for Sustainable Connectivity in Wireless Sensor Networks},
journal = {Sustainable Computing: Informatics and Systems},
volume = {26},
pages = {100380},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100380},
url = {https://www.sciencedirect.com/science/article/pii/S2210537919301155},
author = {Partha Sarathi Banerjee and Satyendra Nath Mandal and Debashis De and Biswajit Maiti},
keywords = {Temperature, Intelligent sleep scheduling, Heterogeneous wireless sensor network, Reinforcement learning, Entropy},
abstract = {Temperature variations have a significant effect on the sustainable operation of the power-constrained wireless sensor networks. The characteristics of wireless communication links deteriorates considerably with increase of temperature. Proactive measures may not always perform well in a dynamic environment where both the wireless links and sensor nodes are supposed to behave unexpectedly. Environment adaptive efficient sleep-schedule strategy can preserve the resources of the low power sensor nodes and thereby alleviate the adverse effects of temperature. In this paper, temperature adaptive intelligent sleep-scheduling strategy (RL-Sleep) for the wireless sensor nodes has been proposed. This algorithm is based on Reinforcement Learning which enables a node in the network to perceive the environment and decide autonomously about the action (transmit, listen or sleep) conducive for a stable operation of the network. Simulation results exhibit a good performance of the proposed approach in terms of sustainable operations of the network and connectivity.}
}
@article{MA2021177,
title = {Digital mapping of soil salinization based on Sentinel-1 and Sentinel-2 data combined with machine learning algorithms},
journal = {Regional Sustainability},
volume = {2},
number = {2},
pages = {177-188},
year = {2021},
issn = {2666-660X},
doi = {https://doi.org/10.1016/j.regsus.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666660X21000219},
author = {Guolin Ma and Jianli Ding and Lijng Han and Zipeng Zhang and Si Ran},
keywords = {Salinization, Digital soil mapping, XGBoost, Sentinel-1, Sentinel-2, Ogan-Kuqa River Oasis},
abstract = {Soil salinization is one of the most important causes of land degradation and desertification, especially in arid and semi-arid areas. The dynamic monitoring of soil salinization is of great significance to land management, agricultural activities, water quality, and sustainable development. The remote sensing images taken by the synthetic aperture radar (SAR) Sentinel-1 and the multispectral satellite Sentinel-2 with high resolution and short revisit period have the potential to monitor the spatial distribution of soil attribute information on a large area; however, there are limited studies on the combination of Sentinel-1 and Sentinel-2 for digital mapping of soil salinization. Therefore, in this study, we used topography indices derived from digital elevation model (DEM), SAR indices generated by Sentinel-1, and vegetation indices generated by Sentinel-2 to map soil salinization in the Ogan-Kuqa River Oasis located in the central and northern Tarim Basin in Xinjiang of China, and evaluated the potential of multi-source sensors to predict soil salinity. Using the soil electrical conductivity (EC) values of 70 ground sampling sites as the target variable and the optimal environmental factors as the predictive variable, we constructed three soil salinity inversion models based on classification and regression tree (CART), random forest (RF), and extreme gradient boosting (XGBoost). Then, we evaluated the prediction ability of different models through the five-fold cross validation. The prediction accuracy of XGBoost model is better than those of CART and RF, and soil salinity predicted by the three models has similar spatial distribution characteristics. Compared with the combination of topography indices and vegetation indices, the addition of SAR indices effectively improves the prediction accuracy of the model. In general, the method of soil salinity prediction based on multi-source sensor combination is better than that based on a single sensor. In addition, SAR indices, vegetation indices, and topography indices are all effective variables for soil salinity prediction. Weighted Difference Vegetation Index (WDVI) is designated as the most important variable in these variables, followed by DEM. The results showed that the high-resolution radar Sentinel-1 and multispectral Sentinel-2 have the potential to develop soil salinity prediction model.}
}
@article{RENUGADEVI2021,
title = {Revolution of Smart Healthcare Materials in Big Data Analytics},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.04.256},
url = {https://www.sciencedirect.com/science/article/pii/S221478532103176X},
author = {N. Renugadevi and S. Saravanan and C.M. {Naga Sudha}},
keywords = {Big data, Smart City, Smart health, COVID-19},
abstract = {In the present digital world, Information and Communication Technology along with Internet of Things are considered as backbone for the smart city evolution. IoT is concentrated more nowadays, due to the convergence of images on Information Technology and human behaviour known as Operation Technology. It is in the progressing stage with notion on cyber-physical world, where things are facilitated to satisfy feasible needs. Similarly, Internet of Medical Things is gaining attention due to the current COVID-19 situation. It has gifted numerous sensors and devices with the increased accuracy, productivity and reliability to the healthcare industry. Increasing the sensor devices in smart city, results in the handling of large amount of data. Therefore, big data is focused due to their characteristics such as volume, variety, velocity and veracity. Extraction of hidden correlations and insights from data is performed through big data analytics or big data value chain. Such concepts are integrated with Artificial Intelligence and Machine Learning technologies which are more important in collecting the real-time data collection and helps in gaining knowledge on how the smart cities and their applications in health sector evolves to adapt the conditions. The main advantage on Big Data Analytics is that it helps in commonality along with heterogeneity principles. Even though cities are digitalized, urban health is still limited to IoT devices which fetch sensor data, eliminating the inclusion of human anatomy and biological data. Integration of these two data forms will help in attaining the target of Big Data and leads to the contextualized, sustainable and resilient smart cities which renders more living factors. This chapter will focus on role of Big Data in smart health applications which plays a vital role in safeguarding the human lives.}
}
@article{GALIC2021105486,
title = {Data protection law beyond identifiability? Atmospheric profiles, nudging and the Stratumseind Living Lab},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105486},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105486},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920300911},
author = {Maša Galič and Raphaël Gellert},
keywords = {Data protection, Personal data, Smart city, Profiling, Nudging, Stratumseind},
abstract = {The deployment of pervasive information and communication technologies (ICTs) within smart city initiatives transforms cities into extraordinary apparatuses of data capture. ICTs such as smart cameras, sound sensors and lighting technology are trying to infer and affect persons’ interests, preferences, emotional states, and behaviour. It should be no surprise then that contemporary legal and policy debates on privacy in smart cities are dominated by a debate focused on data and, therefore, on data protection law. In other words, data protection law is the go-to legal framework to regulate data processing activities within smart cities and similar initiatives. While this may seem obvious, a number of important hurdles might prevent data protection law to be (successfully) applied to such initiatives. In this contribution, we examine one such hurdle: whether the data processed in the context of smart cities actually qualifies as personal data, thus falling within the scope of data protection law. This question is explored not only through a theoretical discussion but also by taking an illustrative example of a smart city-type initiative – the Stratumseind 2.0 project and its living lab in the Netherlands (the Stratumseind Living Lab; SLL). Our analysis shows that the requirement of ‘identifiability’ might be difficult to satisfy in the SLL and similar initiatives. This is so for two main reasons. First, a large amount of the data at stake do not qualify as personal data, at least at first blush. Most of it relates to the environment, such as, data about the weather, air quality, sound and crowding levels, rather than to identified or even likely identifiable individuals. This is connected to the second reason, according to which, the aim of many smart city initiatives (including the SLL) is not to identify and target specific individuals but to manage or nudge them as a multiplicity – a combination of the environment, persons and all of their interactions. This is done by trying to affect the ‘atmosphere’ on the street. We thus argue that a novel type of profiling operations is at stake; rather than relying on individual or group profiling, the SLL and similar initiatives rely upon what we have called ‘atmospheric profiling’. We conclude that it remains highly uncertain, whether smart city initiatives like the SLL actually process personal data. Yet, they still pose risks for a wide variety of rights and freedoms, which data protection law is meant to protect, and a need for regulation remains.}
}
@article{MUGHEES2021114844,
title = {Deep sequence to sequence Bi-LSTM neural networks for day-ahead peak load forecasting},
journal = {Expert Systems with Applications},
volume = {175},
pages = {114844},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114844},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002852},
author = {Neelam Mughees and Syed Ali Mohsin and Abdullah Mughees and Anam Mughees},
keywords = {Peak demand forecasting, Demand response programs, Bidirectional long-short term memory networks, Sequence to sequence regression, Deep neural networks},
abstract = {The power industry is currently facing the problem of an electricity supply–demand imbalance. The most inexpensive and efficient solution to alleviate this imbalance is to decrease electricity demand. Local electrical utilities should deploy demand response programs (DRP), and short-term peak demand forecasting (STPDF) plays a crucial role in their successful deployment. In residential sectors, peak demand forecasting is also critical because the energy policies, technological growth, and changing climate are further increasing the peak demand. Therefore, an accurate peak demand forecasting will help utility companies in avoiding blackouts and secure a continuous power supply by implementing subsidy schemes such as DRP. However, daily peak load is volatile, nonstationary, and nonlinear in nature, and hence it is hard to predict it accurately. This research work for the first time has attempted to design, implement, and test deep bidirectional long short-term memory based sequence to sequence (Bi-LSTM S2S) regression approach for “day-ahead” peak demand forecasting and has accomplished preliminary success. The day-ahead peak electricity demand forecasting model is designed and tested using the MATLAB software. For performance comparison, shallow Bi-LSTM S2S, shallow LSTM S2S, deep LSTM S2S, Levenberg-Marquardt backpropagation artificial neural networks (LMBP-ANN), and medium Gaussian support vector regression (MG-SVR) forecasting models are also developed and tested. Mean absolute percentage error (MAPE) and Root Mean Squared Error (RMSE) are used as performance metrics. It has been found out that in terms of both performance metrics, the proposed deep Bi-LSTM S2S day-ahead “peak” forecasting model has outperformed all the other models on both public holidays and normal days. The load pattern on public holidays is always different than on normal days, and there is always less data available in contrast to the normal days. Therefore, it is hard to accurately forecast their load.}
}
@article{QU2021247,
title = {DroneCOCoNet: Learning-based edge computation offloading and control networking for drone video analytics},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {247-262},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002351},
author = {Chengyi Qu and Prasad Calyam and Jeromy Yu and Aditya Vandanapu and Osunkoya Opeoluwa and Ke Gao and Songjie Wang and Raymond Chastain and Kannappan Palaniappan},
keywords = {Edge/fog computation offloading, Drone video analytics, Mobile edge computing, Learning-based scheme, Data processing in fog computing},
abstract = {Multi-Unmanned Aerial Vehicle (UAV) systems with high-resolution cameras have been found useful for operations such as smart city and disaster management. These systems feature Flying Ad-Hoc Networks (FANETs) that connect the computation edge with UAVs and a Ground Control Station (GCS) through air-to-ground wireless network links. Leveraging the edge/fog computation resources effectively with energy-latency-awareness, and handling intermittent failures of FANETs are the major challenges in supporting video processing applications. In this paper, we propose a novel “DroneCOCoNet” framework for drone video analytics that coordinates intelligent processing of large video datasets using edge computation offloading and performs network protocol selection based on resource-awareness. We present two edge computation offloading approaches, i.e., heuristic-based and reinforcement learning-based approaches. These approaches provide intelligent task sharing and co-ordination for dynamic offloading decision-making among UAVs. Our scheme handles the problem of computation offloading tasks in two separate ways: (i) heuristic decision-making process, and (ii) Markov decision process; wherein we aim to minimize the total computation costs as well as latency in the edge/fog resources while minimizing video processing times to meet application requirements. Our experimental results show that our heuristic-based offloading decision-making scheme enables lower scheduling time and energy consumption for low drone-to-ground server ratios. In comparison, our dynamic reinforcement learning-based decision-making approach increases the accuracy and saves overall time periodically. Notably, these results also hold in various other multi-UAV scenarios involving largely different numbers of detected objects in e.g., smart farming, transportation traffic flow monitoring and disaster response.}
}
@article{WANG202133,
title = {Energy management solutions in the Internet of Things applications: Technical analysis and new research directions},
journal = {Cognitive Systems Research},
volume = {67},
pages = {33-49},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720301157},
author = {Dayu Wang and Daojun Zhong and Alireza Souri},
keywords = {Internet of Things, Energy management, Smart devices, Green energy computing, Power generation},
abstract = {By advancement of Internet of Things (IoT) technology in smart life such as smart city, smart home, smart healthcare and smart transportation, interconnections between smart things are growing that complicate evaluation of efficiency factors on the intelligent systems. Energy consumption as one of the most challenging issues is increasing with the growing IoT devices and existing interconnections between cloud data centers, mobile applications and human activities. Managing energy efficiency and power consumption is one of the important issues in green IoT-enabled technologies. This paper presents an overview on the energy management solutions in the IoT based on Systematic Literature Review (SLR). The main goal of this SLR-based overview is to recognize significant research trends in the field of energy management and power consumption techniques which need additional consideration to highlight more efficient and effective methods in IoT. Also, a taxonomy is proposed to categorize the existing research studies on energy management solutions. A statistical and technical analysis of reviewed existing papers are provided, and evaluation factors and attributes are discussed. We observed that variety of published research papers in smart home have highest percentage to evaluate energy management in the IoT. Also, deep learning and clustering methods are must popular techniques that were applied to evaluate the energy management in IoT case studies. Finally, new challenges and forthcoming issues of the energy management and efficient power consumption methods are presented.}
}
@article{GUO2020128,
title = {A fast face detection method via convolutional neural network},
journal = {Neurocomputing},
volume = {395},
pages = {128-137},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.02.110},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219309075},
author = {Guanjun Guo and Hanzi Wang and Yan Yan and Jin Zheng and Bo Li},
keywords = {Fast face detection, Convolutional neural network, Discriminative complete feature maps},
abstract = {Current face or object detection methods via convolutional neural network (such as OverFeat, R-CNN and DenseNet) explicitly extract multi-scale features based on an image pyramid. However, such a strategy increases the computational burden for face detection. In this paper, we propose a fast face detection method based on discriminative complete features (DCFs) extracted by an elaborately designed convolutional neural network, where face detection is directly performed on the complete feature maps. DCFs have shown the ability of scale invariance, which is beneficial for face detection with high speed and promising performance. Therefore, extracting multi-scale features on an image pyramid employed in the conventional methods is not required in the proposed method, which can greatly improve its efficiency for face detection. Experimental results on several popular face detection datasets show the efficiency and the effectiveness of the proposed method for face detection.}
}
@article{RYCHENER2020648,
title = {Architecture Proposal for Machine Learning Based Industrial Process Monitoring},
journal = {Procedia Computer Science},
volume = {170},
pages = {648-655},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305925},
author = {Lorenz Rychener and Frédéric Montet and Jean Hennebert},
keywords = {System Architecture, Rule Engine, Anomaly Detection, Monitoring, Industry 4.0},
abstract = {In the context of Industry 4.0, an emerging trend is to increase the reliability of industrial process by using machine learning (ML) to detect anomalies of production machines. The main advantages of ML are in the ability to (1) capture non-linear phenomena, (2) adapt to many different processes without human intervention and (3) learn incrementally and improve over time. In this paper, we take the perspective of IT system architects and analyse the implications of the inclusion of ML components into a traditional anomaly detection systems. Through a prototype that we deployed for chemical reactors, our findings are that such ML components are impacting drastically the architecture of classical alarm systems. First, there is a need for long-term storage of the data that are used to train the models. Second, the training and usage of ML models can be CPU intensive and may request using specific resources. Third, there is no single algorithm that can detect machine errors. Fourth, human crafted alarm rules can now also include a learning process to improve these rules, for example by using active learning with a human-in-the-loop approach. These reasons are the motivations behind a microservice-based architecture for an alarm system in industrial machinery.}
}
@article{ELHAJJ2021108133,
title = {A taxonomy of PUF Schemes with a novel Arbiter-based PUF resisting machine learning attacks},
journal = {Computer Networks},
volume = {194},
pages = {108133},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108133},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002036},
author = {Mohammed El-Hajj and Ahmad Fadlallah and Maroun Chamoun and Ahmed Serhrouchni},
keywords = {Internet of Things, IoT, Security, Authentication, PUF, Arbiter, Machine Learning (ML)},
abstract = {As the Intenert of Things (IoT) continues to evolve in our daily personal lives and in future industrial systems (industry 4.0), one of the most significant issues is security. IoT systems must overcome a number of challenges, including deficiency of resources, low power consumption, and the need to protect devices against cyber-attacks. Regrettably, issues about energy use and the lack of computing resources limit the cryptographic methods that can be implemented on these devices. Moreover, the conventional use of non-volatile memory for storing secret keys are vulnerable to a number of attacks like reverse-engineering, cold-boot, side channel, device tampering, etc. Physical Unclonable Functions (PUFs) are one of the categories for enhancing physical device security and solving issues involving with the use of traditional cryptographic algorithms. PUFs are associated as lightweight one-way functions used to extract a unique identity for each end-device, based on physical factors introduced during manufacturing which are unforeseeable and unclonable. PUFs are promising hardware security primitive and have seen a lot of attention in the past few years. In this paper, we provide a survey of a large range of PUF schemes proposed in the literature. Our comparison analysis of the surveyed schemes ends with a number of observations. Then we propose a new scheme of Arbiter PUF to create an identity for each IoT device. The proposed scheme is resistant to Machine Learning (ML) attacks. It is proven to avoid the shortcoming of several previously proposed related PUF-based authentication protocols.}
}
@article{SIMPSON2021544,
title = {A fuzzy based Co-Operative Blackmailing Attack detection scheme for Edge Computing nodes in MANET-IOT environment},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {544-563},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002478},
author = {Serin V. Simpson and G. Nagarajan},
keywords = {Smart cities, IoT, Edge Computing, MEC, Secure SEAL, Co-Operative Blackmailing Attack},
abstract = {Smart cities have been developed with the help of large scale Internet of Things (IoT) device deployment. Under the combination of IoT and traditional Mobile ad-hoc network, the developers could design the communication model for smart city. The increased deployment of IoT devices in smart environment also raised the security and Quality of Service (QoS) related issues. The proposed, Secure approach for Smart city Environment by Accusation based List management (Secure SEAL) primarily explores the various chances of disruption in IoT network caused by co-operative attacks at the edge nodes. The internal co-operative attack is a type of attack which is jointly performed by a group of nodes in IoT network. There is no fine-grained technique to defend against co-operative attacks. To this end, a fuzzy based trustworthy environment is established for reducing the security threats in smart cities with the help of Edge Computing. The trustworthy environment ensures the timely identification of malicious entities and the detection of co-operative attacks. In Edge Computing, the computations required to perform at the end devices can be placed at Edge servers. This will reduce the expected latency and bandwidth utilization required for traditional cloud access. Secure SEAL uses a fuzzy based approach to avoid and isolate the presence of malicious nodes in IoT network. The suspected nodes are re-analyzed based on the reputation value obtained by reaction based trust evaluation. Eventually, the performance of the proposed trustworthy environment has been validated. The proposed work’s efficacy in minimizing the impact of co operative attacks (more than 90%) is comparatively high.}
}
@article{ABID2019292,
title = {Sentiment analysis through recurrent variants latterly on convolutional neural network of Twitter},
journal = {Future Generation Computer Systems},
volume = {95},
pages = {292-308},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18324944},
author = {Fazeel Abid and Muhammad Alam and Muhammad Yasir and Chen Li},
keywords = {Sentiment analysis, Word embeddings, Recurrent neural network (RNNs), Convolutional neural network (CNNs)},
abstract = {Sentiment analysis has been a hot area in the exploration field of language understanding, however, neural networks used in it are even lacking. Presently, the greater part of the work is proceeding on recognizing sentiments by concentrating on syntax and vocabulary. In addition, the task identified with natural language processing and for computing the exceptional and remarkable outcomes Recurrent neural networks (RNNs) and Convolutional neural networks (CNNs) have been utilized. Keeping in mind the end goal to capture the long-term dependencies CNNs, need to rely on assembling multiple layers. In this Paper for the improvement in understanding the sentiments, we constructed a joint architecture which places of RNN at first for capturing long-term dependencies with CNNs using global average pooling layer while on top a word embedding method using GloVe procured by unsupervised learning in the light of substantial twitter corpora to deal with this problem. Experimentations exhibit better execution when it is compared with the baseline model on the twitter’s corpora which tends to perform dependable results for the analysis of sentiment benchmarks by achieving 90.59% on Stanford Twitter Sentiment Corpus, 89.46% on Sentiment Strength Twitter Data and 88.72% on Health Care Reform Dataset respectively. Empirically, our work turned to be an efficient architecture with slight hyperparameter tuning which capable us to reduce the number of parameters with higher performance and not merely relying on convolutional multiple layers by constructing the RNN layer followed by convolutional layer to seizure long-term dependencies.}
}
@article{MOYSEN2018248,
title = {From 4G to 5G: Self-organized network management meets machine learning},
journal = {Computer Communications},
volume = {129},
pages = {248-268},
year = {2018},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2018.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0140366418300380},
author = {Jessica Moysen and Lorenza Giupponi},
keywords = {Network management, Self-organizing networks, Mobile networks, Machine learning, Big data},
abstract = {Self-organization as applied to cellular networks is usually referred to Selforganizing Networks (SONs), and it is a key driver for improving Operations,Administration, and Management (OAM) activities. SON aims at reducing the cost of installation and management of 4G and future 5G networks, by simplifying operational tasks through the capability to configure, optimize and heal itself. To satisfy 5G network management requirements, this autonomous management vision has to be extended to the end to end network. In literature and also in some instances of products available in the market, Machine Learning (ML) has been identified as the key tool to implement autonomous adaptability and take advantage of experience when making decisions. In this paper, we survey how 5G network management, with an end-to-end perspective of the network, can significantly benefit from ML solutions. We review and provide the basic concepts and taxonomy for SON, network management and ML. We analyze the available state of the art in the literature, standardization, and in the market. We pay special attention to 3rd Generation Partnership Project (3GPP) evolution in the area of network management and to the data that can be extracted from 3GPP networks, in order to gain knowledge and experience in how the network is working, and improve network performance in a proactive way. Finally, we go through the main challenges associated with this line of research, in both 4G and in what 5G is getting designed, while identifying new directions for research.}
}
@article{AHMAD2020117283,
title = {Smart energy forecasting strategy with four machine learning models for climate-sensitive and non-climate sensitive conditions},
journal = {Energy},
volume = {198},
pages = {117283},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.117283},
url = {https://www.sciencedirect.com/science/article/pii/S036054422030390X},
author = {Tanveer Ahmad and Chen Huanxin and Dongdong Zhang and Hongcai Zhang},
keywords = {Supervised learning, Utilities and building consumption, Energy forecasting, Imbalanced data handling, Performance correlation, Energy benchmark},
abstract = {Developing a reliable and robust algorithm for accurate energy demand prediction is indispensable for utility companies for various applications, e.g., power dispatching, market participation and infrastructure planning. However, this is challenging because the performance of a forecasting algorithm may be affected by various factors, such as data quality, geographic diversity, forecast horizon, customer segmentation and the forecast origin. Furthermore, an approach that performs well in one region may fail in other regions, and similarly, a model that forecasts accurately in one horizon may fail to produce an accurate prediction for other horizons. To overcome the above challenges such as rough data quality, different forecasting horizons, different kinds of loads and forecasting for different regions, this study proposes four machine learning/supervised learning models. These models are applied to improve the generalization of the network and reduce forecasting. These models are intended to simplify or demystify terms, complex concepts and data granularity used in energy forecasting. Two different data sites and four forecasting horizons are used to validate the proposed models. The coefficient of variation and mean absolute percentage error are 50% higher as compared with the existing model. The proposed supervised learning models ensure a generalization ability, robustness and high accuracy for building and utilities energy consumption forecasting. The forecasting results help to improve and automate the predictive modeling process while covering the knowledge-gaps between machine learning and conventional forecasting models.}
}
@article{ZHANG2019101086,
title = {An online reinforcement learning approach to quality-cost-aware task allocation for multi-attribute social sensing},
journal = {Pervasive and Mobile Computing},
volume = {60},
pages = {101086},
year = {2019},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2019.101086},
url = {https://www.sciencedirect.com/science/article/pii/S1574119219304511},
author = {Yang Zhang and Daniel (Yue) Zhang and Nathan Vance and Dong Wang},
keywords = {Social sensing, Multi-attribute optimization, Quality-cost-aware task allocation, Online reinforcement learning},
abstract = {Social sensing has emerged as a new sensing paradigm where humans (or devices on their behalf) collectively report measurements about the physical world. This paper focuses on a quality-cost-aware task allocation problem in multi-attribute social sensing applications. The goal is to identify a task allocation strategy (i.e., decide when and where to collect sensing data) to achieve an optimized tradeoff between the data quality and the sensing cost. While recent progress has been made to tackle similar problems, three important challenges have not been well addressed: (i) “online task allocation”: the task allocation schemes need to respond quickly to the potentially large dynamics of the measured variables in social sensing; (ii) “multi-attribute constrained optimization”: minimizing the overall sensing error given the dependencies and constraints of multiple attributes of the measured variables is a non-trivial problem to solve; (iii) “nonuniform task allocation cost”: the task allocation cost in social sensing often has a nonuniform distribution which adds additional complexity to the optimized task allocation problem. This paper develops a Quality-Cost-Aware Online Task Allocation (QCO-TA) scheme to address the above challenges using a principled online reinforcement learning framework. We evaluate the QCO-TA scheme through a real-world social sensing application and the results show that our scheme significantly outperforms the state-of-the-art baselines in terms of both sensing accuracy and cost.}
}
@article{ZHAI2019124,
title = {Optical flow estimation using channel attention mechanism and dilated convolutional neural networks},
journal = {Neurocomputing},
volume = {368},
pages = {124-132},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219311786},
author = {Mingliang Zhai and Xuezhi Xiang and Rongfang Zhang and Ning Lv and Abdulmotaleb El Saddik},
keywords = {Optical flow estimation, Channel attention, Dilated convolution, Deep learning, Convolutional neural networks},
abstract = {Learning optical flow based on convolutional neural networks has made great progress in recent years. These approaches usually design an encoder-decoder network that can be trained end-to-end. In encoder part, high-level feature information is extracted through a series of strided convolution, which is similar to most image classification networks. In contrast to classification task, spatial feature maps are then enlarged to full scale of input by conducting successive deconvolution layer in decoder part. However, optical flow estimation is a pixel-level task, and blurry flow fields are usually generated, which is caused by unrefined features and low-resolution. To address this problem, we propose a novel network, which combines attention mechanism and dilated convolutional neural network. In this network, the channel-wise features are adaptively weighted by building interdependencies among channels, which can weaken the weights of useless features and can enhance the directivity of feature extraction. Meanwhile, spatial precision is achieved by employing dilated convolution which improves the receptive field without large computational source and keeps the spatial resolution of feature map unchanged. Our network is trained on FlyingChairs and FlyingThings3D datasets in a supervised manner. Extensive experiments are conducted on MPI-Sintel and KITTI datasets to verify the effectiveness of the proposed method. The experimental results show that attention mechanism and dilated convolution are beneficial for optical flow estimation. Moreover, our method achieves better accuracy and visual improvements comparing to most of recent approaches.}
}
@article{BOUKERCHE2020107530,
title = {Machine Learning-based traffic prediction models for Intelligent Transportation Systems},
journal = {Computer Networks},
volume = {181},
pages = {107530},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107530},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620311877},
author = {Azzedine Boukerche and Jiahao Wang},
keywords = {Vehicular traffic flow prediction, Time-series, GCN, Parallel training, RNN, Sequence to sequence, Machine Learning, ITS},
abstract = {Intelligent Transportation Systems (ITS) have attracted an increasing amount of attention in recent years. Thanks to the fast development of vehicular computing hardware, vehicular sensors and citywide infrastructures, many impressive applications have been proposed under the topic of ITS, such as Vehicular Cloud (VC), intelligent traffic controls, etc. These applications can bring us a safer, more efficient, and also more enjoyable transportation environment. However, an accurate and efficient traffic flow prediction system is needed to achieve these applications, which creates an opportunity for applications under ITS to deal with the possible road situation in advance. To achieve better traffic flow prediction performance, many prediction methods have been proposed, such as mathematical modeling methods, parametric methods, and non-parametric methods. Among the non-parametric methods, the one of the most famous methods today is the Machine Learning-based (ML) method. It needs less prior knowledge about the relationship among different traffic patterns, less restriction on prediction tasks, and can better fit non-linear features in traffic data. There are several sub-classes under the ML method, such as regression model, kernel-based model, etc. For all these models, it is of vital importance that we choose an appropriate type of ML model before building up a prediction system. To do this, we should have a clear view of different ML methods; we investigate not only the accuracy of different models, but the applicable scenario and sometimes the specific type of problem the model was designed for. Therefore, in this paper, we are trying to build up a clear and thorough review of different ML models, and analyze the advantages and disadvantages of these ML models. In order to do this, different ML models will be categorized based on the ML theory they use. In each category, we will first give a short introduction of the ML theory they use, and we will focus on the specific changes made to the model when applied to different prediction problems. Meanwhile, we will also compare among different categories, which will help us to have a macro overview of what types of ML methods are good at what types of prediction tasks according to their unique model features. Furthermore, we review the useful add-ons used in traffic prediction, and last but not least, we discuss the open challenges in the traffic prediction field.}
}
@article{ALVARESSANCHES2021145600,
title = {Mobile surveys and machine learning can improve urban noise mapping: Beyond A-weighted measurements of exposure},
journal = {Science of The Total Environment},
volume = {775},
pages = {145600},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.145600},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721006689},
author = {Tatiana Alvares-Sanches and Patrick E. Osborne and Paul R. White},
keywords = {Noise pollution, Noise modelling, Spatial modelling, Personal exposure, Port city, Urban environment},
abstract = {Urban noise pollution is a major environmental issue, second only to fine particulate matter in its impacts on physical and mental health. To identify who is affected and where to prioritise actions, noise maps derived from traffic flows and propagation algorithms are widely used. These may not reflect true levels of exposure because they fail to consider noise from all sources and may leave gaps where roads or traffic data are absent. We present an improved approach to overcome these limitations. Using walking surveys, we recorded 52,366 audio clips of 10 s each along 733 km of routes throughout the port city of Southampton. We extracted power levels in low (11 to 177 Hz), mid (177 Hz to 5.68 kHz), high (5.68 to 22.72 kHz) and A-weighted frequencies and then built machine-learning (ML) models to predict noise levels at 30 m resolution across the entire city, driven by urban form. Model performance (r2) ranged from 0.41 (low frequencies) to 0.61 (mid frequencies) with mean absolute errors of 4.05 to 4.75 dB. The main predictors of noise were related to modes of transport (road, air, rail and water) but for low frequencies, port activities were also important. When mapped to the city scale, A-weighted frequencies produced a similar spatial pattern to mid-frequencies, but did not capture the major sources of low frequency noise from the port or scattered hotspots of high frequencies. We question whether A-weighted noise mapping is adequate for health and wellbeing impact assessments. We conclude that mobile surveys combined with ML offer an alternative way to map noise from all sources and at fine resolution across entire cities that may more accurately reflect true exposures. Our approach is suitable for noise data gathered by citizen scientists, or from a network of sensors, as well as from structured surveys.}
}
@article{WANG2021103510,
title = {Real-time monitoring for vibration quality of fresh concrete using convolutional neural networks and IoT technology},
journal = {Automation in Construction},
volume = {123},
pages = {103510},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103510},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520310906},
author = {Dong Wang and Bingyu Ren and Bo Cui and Jiajun Wang and Xiaoling Wang and Tao Guan},
keywords = {Vibration quality, Fresh concrete, Real-time monitoring, IoT technology, Convolutional neural network, Image classification},
abstract = {Vibration quality is critical to ensure the concrete strength, which directly affects the long-term safe operation of concrete structures. The vibration duration and vibration depth are key parameters to guarantee vibration quality. However, traditional manual inspection on concrete surface to judge the vibration duration and estimation of vibration depth is subjective and unreliable. Moreover, existing studies monitor the vibration duration based on the knowledge from prior experiments, ignoring the influence of concrete heterogeneity. Thus, a real-time monitoring method for vibration quality of fresh concrete based on ResNet with 50 layers (ResNet-50) and Internet of Things (IoT) technology is proposed. The IoT-based monitoring framework is proposed to measure vibration depth and capture concrete surface image (CSI). A three-category classification model of CSI is established based on fine-tuned ResNet-50 model using a self-constructed dataset with 15,006 images to determine proper vibration duration. A large-scale hydraulic engineering application verifies the performance of the proposed method.}
}
@article{KUSCUSIMSEK2021100914,
title = {Simulation of the climatic changes around the coastal land reclamation areas using artificial neural networks},
journal = {Urban Climate},
volume = {38},
pages = {100914},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100914},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521001449},
author = {Çağdaş {Kuşçu Şimşek} and Derya Arabacı},
keywords = {Artificial neural network, Urban climate, Thermal change detection, Land use planning, Land use/cover change, Coastal land reclamation},
abstract = {For the last 20 years, Istanbul has intensely experienced land use/cover change (LUCC) as a result of both the direction of investment strategies to the construction sector and urban transformation works. Within this process, the demand for the disposal of debris from building demolitions with minimum cost has made the creation of land reclamation areas a current issue. Land reclamation areas that pose a threat to the marine ecosystem also have effects on the local climate, depending on the LUCC experienced on the urban surface. In this study, two coastal reclamation areas of Istanbul (Yenikapı, Maltepe) were addressed, and the predictability of changes in the thermal environment after the landfill was examined using Artificial Neural Networks (ANN). When the relationship between the simulation data and the actual changes was statistically tested, correlations between the original and the simulated images of Maltepe and Yenikapı are 0.650 and 0.710 respectively were obtained. Also, it was determined that the simulations provided exact results in the range of 37–55%, and accurate results in the range of 66–87% with a sensitivity of 100 m. These results revealed that the simulations performed by the ANN have sufficient sensitivity for monitoring the thermal changes in urban areas.}
}
@article{MIR2022107014,
title = {Data driven smart policing: A novel road distance-based k-median model for optimal substation placement},
journal = {Computers in Human Behavior},
volume = {127},
pages = {107014},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107014},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100337X},
author = {Abinta Mehmood Mir and Ali Hassan and Asma Khalid and Zohair Raza Hassan and Faisal Kamiran and Agha Ali Raza and Saeed-Ul Hassan and Mudassir Shabbir},
keywords = {Crime analysis, Optimal location, Geometric clustering},
abstract = {In the context of smart city research, finding patterns in crime data to explore trends in crime and to locate the presence of crime has been an exciting research field. Our aim in this paper is optimizing the location of police substations within the jurisdiction of a police station so that law enforcement agencies could work efficiently. These substations are placed near by road in order to immediately respond to nearby crimes. We attempt to optimize the average minimum distance to a crime from its nearest substation. This distance is found by the underlying road network. We are locating these substations in an infinitely large set of points in the given region. Case study has been done on real-world crime data from Lahore, Pakistan to show the efficiency of these methods. We also explored what are trends in crime data with respect to years, seasons and day/night and where to place substations accordingly. We evaluated the placement of substation by our model through comparing the average time taken to report to a crime against random substations placed within the same area. Last but not the least, the analytical insights provided by this study be useful for policy making and smart city research for the inclusion of sustainable cities and communities.}
}
@article{XU2020107172,
title = {GR and BP neural network-based performance prediction of dual-antenna mobile communication networks},
journal = {Computer Networks},
volume = {172},
pages = {107172},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107172},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619309545},
author = {Lingwei Xu and Tianqi Quan and Jingjing Wang and T. {Aaron Gulliver} and Khoa N. Le},
keywords = {Mobile communication networks, Average symbol error probability, Channel capacity, Performance prediction, BP neural network, GR neural network},
abstract = {The performance of a dual-antenna mobile communication network in 2-Rayleigh fading is investigated in this paper. Exact average symbol error probability (SEP) expressions with selection combining (SC) are derived for q-ary phase-shift keying (PSK) and pulse-amplitude modulation (PAM). Exact expressions are also given for the channel capacity. It is important to predict the performance of mobile communication networks in complex wireless environments. Thus, we propose generalized regression (GR) and back-propagation (BP) neural network-based SEP prediction methods. The theoretical results are used to generate training data. The proposed prediction methods are compared to the extreme learning machine (ELM), locally weighted linear regression (LWLR), support vector machine (SVM), and radial basis function (RBF) neural network methods. The results obtained verify that the proposed methods provide better SEP predictions.}
}
@article{ZAFAR201986,
title = {QoS enhancement with deep learning-based interference prediction in mobile IoT},
journal = {Computer Communications},
volume = {148},
pages = {86-97},
year = {2019},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419306620},
author = {Saniya Zafar and Sobia Jangsher and Ouns Bouachir and Moayad Aloqaily and Jalel {Ben Othman}},
keywords = {Deep learning, Internet-of-Things (IoT), Mobile IoT (mIoT), Dependent Interference, Resource allocation, Moving small cells, Interference Graph},
abstract = {With the acceleration in mobile broadband, wireless infrastructure plays a significant role in Internet-of-Things (IoT) to ensure ubiquitous connectivity in mobile environment, making mobile IoT (mIoT) as center of attraction. Usually intelligent systems are accomplished through mIoT which demands for the increased data traffic. To meet the ever-increasing demands of mobile users, integration of small cells is a promising solution. For mIoT, small cells provide enhanced Quality-of-Service (QoS) with improved data rates. In this paper, mIoT-small cell based network in vehicular environment focusing city bus transit system is presented. However, integrating small cells in vehicles for mIoT makes resource allocation challenging because of the dynamic interference present between small cells which may impact cellular coverage and capacity negatively. This article proposes Threshold Percentage Dependent Interference Graph (TPDIG) using Deep Learning-based resource allocation algorithm for city buses mounted with moving small cells (mSCs). Long–Short Term Memory (LSTM) based neural networks are considered to predict city buses locations for interference determination between mSCs. Comparative analysis of resource allocation using TPDIG, Time Interval Dependent Interference Graph (TIDIG), and Global Positioning System Dependent Interference Graph (GPSDIG) is presented in terms of Resource Block (RB) usage and average achievable data rate of mIoT-mSC network.}
}
@article{YAMINSIDDIQUI2020,
title = {Smart occupancy detection for road traffic parking using deep extreme learning machine},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819313928},
author = {Shahan {Yamin Siddiqui} and Muhammad {Adnan Khan} and Sagheer Abbas and Farrukh Khan},
keywords = {Deep extreme machine learning, ANN, Feedforward propagation, Backward propagation},
abstract = {Predicting the location of parking is a long-lasting problem that has ultimate importance in our daily life. In this paper, artificial neural networks are used to predict the parking location that will be helpful for drivers to settle on a reasonable area for stopping. This approach eventually adds to the familiarity and wellbeing of traffic on the roads which results in a decrease in turbulence. By using the approach of Deep Extreme Learning Machine (DELM), reliability can be achieved with a marginal error rate thus reducing the skeptical inclination. In this article, the Proposed Car Parking Space Prediction (CPSP) to elaborate on the dilemma of parking space for vehicles, we have used deep learning neural networks in contrast with feedforward and backward propagation. When the results were taken into consideration, it was unveiled that extreme deep machine learning neural network bears the highest accuracy rate with 60% of training (21431 samples), 40% of test and validation (14287 examples). It has been observed that the proposed DELM has the highest precision rate of 91.25%. Simulation results validate the prediction effectiveness of the proposed DELM strategy.}
}
@article{SHAKARAMI2022102362,
title = {Resource provisioning in edge/fog computing: A Comprehensive and Systematic Review},
journal = {Journal of Systems Architecture},
volume = {122},
pages = {102362},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102362},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121002526},
author = {Ali Shakarami and Hamid Shakarami and Mostafa Ghobaei-Arani and Elaheh Nikougoftar and Mohammad Faraji-Mehmandar},
keywords = {Mobile edge computing, Fog computing, machine learning, Game theory, Resource provisioning, Elasticity},
abstract = {Close computing paradigms such as fog and edge have become promising technologies for mobile applications running on pervasive mobile equipment utilized by a wide range of users to remove such types of equipment’ inherent limitations. In such environments, competition is a severe challenge to gain computation and communication resources’ capabilities. Therefore, resource allocation in the mentioned environments are becoming a requirement, which is an essential challenging issue addressed by different approaches, including resource provisioning. However, to the best of the authors’ knowledge, any systematic, comprehensive, and detailed survey related to resource provisioning has not been applied in computation environments despite its importance. This paper provides a review of the resource provisioning approaches in computation paradigms in the form of a standard classification to identify the existing approaches on this critical topic and offer open issues. The proposed classification can be organized into five main fields: framework-based, heuristic/meta-heuristic-based, model-based, machine learning-based, and game theoretic-based mechanisms. Next, these classes are compared based on some essential features such as performance metrics, case studies, utilized techniques, and evaluation tools. Finally, open issues and uncovered or insufficiently covered future research challenges, including resource performance, resource location, uncertainties, resource elasticity, and resource migration are discussed, and the survey is concluded.}
}
@article{RUSSO20181090,
title = {Energy Saving in a Wireless Sensor Network by Data Prediction by using Self-Organized Maps},
journal = {Procedia Computer Science},
volume = {130},
pages = {1090-1095},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.161},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918305271},
author = {Adrien Russo and François Verdier and Benoît Miramond},
keywords = {Smart-cities, Energy Saving, Self-Organizing-Map, Time Series Prediction},
abstract = {We present in this paper a predictive analysis method based on a unsupervised type of machine learning algorithm : the kohonen maps. This model will be exploited in a network of sensors to reduce the number of transmission on the network. The aim of this paper is to present a learning algorithm and the result obtained in the context of smart city.}
}
@article{GIMPEL2021128048,
title = {Designing smart and sustainable irrigation: A case study},
journal = {Journal of Cleaner Production},
volume = {315},
pages = {128048},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128048},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621022666},
author = {Henner Gimpel and Valerie Graf-Drasch and Florian Hawlitschek and Kathrin Neumeier},
keywords = {Water scarcity, Trees, Irrigation, IoT, Smart city, Sustainability},
abstract = {Water scarcity is becoming an increasingly important issue – also for cities in the Western world. As key-regulators of cities' micro-climate, urban trees suffer from this scarcity fueled by rising temperatures. With climate change on our doorsteps, cities turn to smart city concepts that harness advanced information technologies' potential to conserve scarce resources. Regarding withering trees, we suggest that a smart city should consider putting smart irrigation systems in place. We use a design science research approach to establish, demonstrate, and evaluate how a smart irrigation system for urban trees can be designed. In doing so, we describe how IoT and data analytics can be leveraged as the backbone of sustainable and smart irrigation. Our design is informed by theory and experts, including city administration representatives, IoT specialists, and botanists. A prototypical instantiation including 18 sensors at eight trees evaluates the design knowledge's viability in Frankfurt am Main (Germany). First insights into collected data in the pilot area indicate promising effects. Projected to the currently 5000 young trees in Frankfurt, we argue that water distribution within one round of irrigation in spring could be reduced by up to 1 million liters.}
}
@article{SUN202150,
title = {PBNet: Part-based convolutional neural network for complex composite object detection in remote sensing imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {173},
pages = {50-65},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620303555},
author = {Xian Sun and Peijin Wang and Cheng Wang and Yingfei Liu and Kun Fu},
keywords = {Object detection, Remote sensing imagery, Complex composite object, Part-based detection, Context information},
abstract = {In recent years, deep learning-based algorithms have brought great improvements to rigid object detection. In addition to rigid objects, remote sensing images also contain many complex composite objects, such as sewage treatment plants, golf courses, and airports, which have neither a fixed shape nor a fixed size. In this paper, we validate through experiments that the results of existing methods in detecting composite objects are not satisfying enough. Therefore, we propose a unified part-based convolutional neural network (PBNet), which is specifically designed for composite object detection in remote sensing imagery. PBNet treats a composite object as a group of parts and incorporates part information into context information to improve composite object detection. Correct part information can guide the prediction of a composite object, thus solving the problems caused by various shapes and sizes. To generate accurate part information, we design a part localization module to learn the classification and localization of part points using bounding box annotation only. A context refinement module is designed to generate more discriminative features by aggregating local context information and global context information, which enhances the learning of part information and improve the ability of feature representation. We selected three typical categories of composite objects from a public dataset to conduct experiments to verify the detection performance and generalization ability of our method. Meanwhile, we build a more challenging dataset about a typical kind of complex composite objects, i.e., sewage treatment plants. It refers to the relevant information from authorities and experts. This dataset contains sewage treatment plants in seven cities in the Yangtze valley, covering a wide range of regions. Comprehensive experiments on two datasets show that PBNet surpasses the existing detection algorithms and achieves state-of-the-art accuracy.}
}
@article{LI2021102544,
title = {Exploring multiple crowdsourced data to learn deep convolutional neural networks for road extraction},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {104},
pages = {102544},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102544},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002518},
author = {Panle Li and Xiaohui He and Mengjia Qiao and Disheng Miao and Xijie Cheng and Dingjun Song and Mingyang Chen and Jiamian Li and Tao Zhou and Xiaoyu Guo and Xinyu Yan and Zhihui Tian},
keywords = {Road extraction, Deep convolutional neural networks, Multiple crowdsourced data, Multi-map integration model, Refined labels},
abstract = {Road extraction from high-resolution remote sensing images (HRSIs) is essential for applications in various areas. Although deep convolutional neural networks (DCNNs) have exhibited remarkable success in road extraction, the performance relies on a large amount of training samples which are hard to obtain. To address this issue, multiple crowdsourced data are used in this study, including OpenStreetMap (OSM), Zmap and GPS. And a multi-map integration model (MMIM) is developed to improve the noise robustness of DCNNs for road extraction tasks. Specifically, rich geographical road information are obtained from multiple crowdsourced data, including main roads, new construction roads, midsize and small roads, which can generate complete road training samples and reduce the label noise. Meanwhile, by exploring the true road label information hidden in different crowdsourced data, the MMIM is used to generate high-quality refined labels for learning DCNNs. In this case, the DCNN-based road extraction methods have more opportunities to learn true road distribution and avoid the overfitting problems of label noise. Experiments based on real road extraction dataset indicate that the proposed method shows great performance, and road extraction results are smoother and more complete.}
}
@article{NASIR2021,
title = {Enabling automation and edge intelligence over resource constraint IoT devices for smart home},
journal = {Neurocomputing},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.138},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016301},
author = {Mansoor Nasir and Khan Muhammad and Amin Ullah and Jamil Ahmad and Sung {Wook Baik} and Muhammad Sajjad},
keywords = {Artificial intelligence, Edge intelligence, IoT, Smart home, Deep learning, Human fall detection},
abstract = {Smart home applications are pervasive and have gained popularity due to the overwhelming use of Internet of Things (IoT). The revolution in IoT technologies made homes more convenient, efficient and perhaps more secure. The need to advance smart home technology is necessary at this stage as IoT is abundantly used in automation industry. However, most of the proposed solutions are lacking in certain key areas of the system i.e., high interoperability, data independence, privacy, and optimization in general. The use of machine learning algorithms requires high-end hardware and are usually deployed on servers, where computation is convenient, but at the cost of bandwidth. However, more recently edge AI enabled systems are being proposed to shift the computation burden from server side to the client side enabling smart devices. In this paper, we take advantage of the edge AI enabled technology to propose a fully featured cohesive system for smart home based on IoT and edge computing. The proposed system makes use of industry standards adopted for fog computing as well as providing robust response from connected IoT sensors in a typical smart home. The proposed system employs edge devices as a computational platform in terms of reducing energy cost and provides security, while remotely controlling all appliances behind a secure gateway. A case study of human fall detection is evaluated by a custom lightweight deep neural network architecture implemented over edge device of the proposed framework. The case study was validated using Le2i dataset. During the training, the early stopping threshold was achieved with 98% accuracy for training set and 94% for validation set. The model size of the network was 6.4 MB which is significantly lower than other networks with similar performance.}
}
@article{CHINNASWAMY2021107130,
title = {Trust aggregation authentication protocol using machine learning for IoT wireless sensor networks},
journal = {Computers & Electrical Engineering},
volume = {91},
pages = {107130},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107130},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621001348},
author = {Santhanakrishnan Chinnaswamy and Annapurani K},
keywords = {Authentication, Trust value, IoT, Security, Machine learning, Nonrepudiation},
abstract = {Security is a huge concern in the Internet of Things (IoT)-based Wireless Sensor Networks (WSNs). Authentication becomes critical in a highly secured environment and reliable communication, specifically trustworthiness. In security, nonrepudiation refers to a service, which provides proof of the origin of data and the integrity of the data. Hence, this factor is to be considered in an IoT environment. This work focuses on the proposed Trust Aggregation Authentication Protocol based on the Machine Learning technique (TAAPML). The total trust value is derived for each device from behavior and data trust values by the internet gateways. In the authentication phase, if either the trust value is less than a standard threshold value or the authentication token is invalid, then gateways omit that node. The trust threshold value is adaptively calculated by using a technique called Support Vector Machine (SVM) over the collected traffic data. TAAPML technique performance is evaluated with respect to Packet Delivery Ratio, Delay, Residual Energy, and Computational Overhead.}
}
@article{ODWYER2020102412,
title = {Integration of an energy management tool and digital twin for coordination and control of multi-vector smart energy systems},
journal = {Sustainable Cities and Society},
volume = {62},
pages = {102412},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102412},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720306338},
author = {Edward O’Dwyer and Indranil Pan and Richard Charlesworth and Sarah Butler and Nilay Shah},
keywords = {Urban energy systems, Smart cities, Building energy, Transport energy, Machine learning},
abstract = {As Internet of Things (IoT) technologies enable greater communication between energy assets in smart cities, the operational coordination of various energy networks in a city or district becomes more viable. Suitable tools are needed that can harness advanced control and machine learning techniques to achieve environmental, economic and resilience objectives. In this paper, an energy management tool is presented that can offer optimal control, scheduling, forecasting and coordination services to energy assets across a district, enabling optimal decisions under user-defined objectives. The tool presented here can coordinate different sub-systems in a district to avoid the violation of high-level system constraints and is designed in a generic fashion to enable transferable use across different energy sectors. The work demonstrates the potential for a single open-source optimisation framework to be applied across multiple energy vectors, providing local government the opportunity to manage different assets in a coordinated fashion. This is shown through case studies that integrate low-carbon communal heating for social housing with electric vehicle charge-point management to achieve high-level system constraints and local government objectives in the borough of Greenwich, London. The paper illustrates the theoretical methodology, the software architecture and the digital twin-based testing environment underpinning the proposed approach.}
}
@article{TAYARANIN2021110338,
title = {Applications of artificial intelligence in battling against covid-19: A literature review},
journal = {Chaos, Solitons & Fractals},
volume = {142},
pages = {110338},
year = {2021},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2020.110338},
url = {https://www.sciencedirect.com/science/article/pii/S0960077920307335},
author = {Mohammad-H. {Tayarani N.}},
keywords = {Artificial intelligence, Machine learning, Covid-19, SARS-CoV-2, Coronavirus, Epidemiology, Drug discovery, Vaccine development, Artificial neural networks, Evolutionary algorithms, Deep learning, Deep neural networks, Convolutional neural networks},
abstract = {Colloquially known as coronavirus, the Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2), that causes CoronaVirus Disease 2019 (COVID-19), has become a matter of grave concern for every country around the world. The rapid growth of the pandemic has wreaked havoc and prompted the need for immediate reactions to curb the effects. To manage the problems, many research in a variety of area of science have started studying the issue. Artificial Intelligence is among the area of science that has found great applications in tackling the problem in many aspects. Here, we perform an overview on the applications of AI in a variety of fields including diagnosis of the disease via different types of tests and symptoms, monitoring patients, identifying severity of a patient, processing covid-19 related imaging tests, epidemiology, pharmaceutical studies, etc. The aim of this paper is to perform a comprehensive survey on the applications of AI in battling against the difficulties the outbreak has caused. Thus we cover every way that AI approaches have been employed and to cover all the research until the writing of this paper. We try organize the works in a way that overall picture is comprehensible. Such a picture, although full of details, is very helpful in understand where AI sits in current pandemonium. We also tried to conclude the paper with ideas on how the problems can be tackled in a better way and provide some suggestions for future works.}
}
@article{SURKOVA2019333,
title = {Neural network modelling of services and goods sales analysis},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {333-336},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.545},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319324644},
author = {Liudmila Surkova and Alexey Laptev},
keywords = {neural networks, Kohonen maps, computer programs, data processing, development},
abstract = {This article explores the benefits of using neural networks in order to process a large amount of data for different scientific research. The study of sales of goods and services of the regions of the Russian Federation for 2018 is presented as an example confirming the feasibility of using specified method. A brief review of used software, “Deductor”, and data mining tool, self-organizing Kohonen maps (TSO) is presented, and the methodology of neural network modelling of trading activity is also described. The research is limited by the reviewed specter of goods in sales, based on the data obtained from the state statistics website. As a result, the main directions of sales industries in different regions are established, and the main sales centers in Russia are identified. Successful analysis of the dynamics of sales of goods and services will help to establish the direction of development of different regions and to increase the volume of services for consumption.}
}
@article{ZHAO2019449,
title = {Machine learning based privacy-preserving fair data trading in big data market},
journal = {Information Sciences},
volume = {478},
pages = {449-460},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309174},
author = {Yanqi Zhao and Yong Yu and Yannan Li and Gang Han and Xiaojiang Du},
keywords = {Data trading, Privacy-preserving, Machine learning, Fairness},
abstract = {In the era of big data, the produced and collected data explode due to the emerging technologies and applications that pervade everywhere in our daily lives, including internet of things applications such as smart home, smart city, smart grid, e-commerce applications and social network. Big data market can carry out efficient data trading, which provides a way to share data and further enhances the utility of data. However, to realize effective data trading in big data market, several challenges need to be resolved. The first one is to verify the data availability for a data consumer. The second is privacy of a data provider who is unwilling to reveal his real identity to the data consumer. The third is the payment fairness between a data provider and a data consumer with atomic exchange. In this paper, we address these challenges by proposing a new blockchain-based fair data trading protocol in big data market. The proposed protocol integrates ring signature, double-authentication-preventing signature and similarity learning to guarantee the availability of trading data, privacy of data providers and fairness between data providers and data consumers. We show the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. The implementation results with Solidity smart contract demonstrate the validity of the proposed blockchain-based fair data trading protocol.}
}
@article{ZANNOU2021101311,
title = {Relevant node discovery and selection approach for the Internet of Things based on neural networks and ant colony optimization},
journal = {Pervasive and Mobile Computing},
volume = {70},
pages = {101311},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2020.101311},
url = {https://www.sciencedirect.com/science/article/pii/S1574119220301395},
author = {Abderrahim Zannou and Abdelhak Boulaalam and El Habib Nfaoui},
keywords = {IoT, Edge computing, Edge server, Service discovery, Service selection, Neural networks, Ant colony optimization},
abstract = {The Internet of Things (IoT) brings opportunities to create new services and change how services are sold and consumed. The IoT is overpopulated by a large number of networks, millions of objects and a huge number of services and interactions. Despite this, the nature of IoT networks, such as the heterogeneity of resources, the dynamic topology, and the large number of similar services, makes service discovery a complex task in terms of accuracy and the time required. Furthermore, the discovery task can offer a set of providers for a given request, so selection of the most relevant provider node must take into account the available resources, such as the power energy and the period of time. In this paper, to overcome these limitations, we propose an approach for service discovery and selection in the IoT. The discovery phase is performed by an edge server using a neural network. The selection phase is performed by nodes to select the most adequate node from the set of relevant nodes using Ant Colony Optimization (ACO). The experimental results show high performance in term of accuracy (96.5%) and a longer network lifetime for the discovery and selection phases respectively, as well as a short period of time for both phases.}
}
@article{AHMED2020714,
title = {Heart disease identification from patients’ social posts, machine learning solution on Spark},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {714-722},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.056},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19315523},
author = {Hager Ahmed and Eman M.G. Younis and Abdeltawab Hendawi and Abdelmgeid A. Ali},
keywords = {Heart disease prediction, Machine learning, Streaming data, Apache Spark, Apache Kafka},
abstract = {Heart disease is one of the first causes of death worldwide. This paper presents a real-time system for predicting heart disease from medical data streams that describe a patient’s current health status. The main goal of the proposed system is to find the optimal machine learning algorithm that achieves high accuracy for heart disease prediction. Two types of features selection algorithms, univariate feature selection and Relief, are used to select important features from the dataset. We compared four types of machine learning algorithms; Decision Tree, Support Vector Machine, Random Forest Classifier, and Logistic Regression Classifier with the selected features as well as full features. We apply hyperparameter tuning and cross-validation with machine learning to enhance accuracy. One core merit of the proposed system is able to handle Twitter data streams that contain patients’ data efficiently. This is done by integrating Apache Kafka with Apache Spark as the underlying infrastructure of the system. The results show the random forest classifier outperforms the other models by achieving the highest accuracy at 94.9%.}
}
@article{YATNALKAR2020626,
title = {An Enhanced Ride Sharing Model Based on Human Characteristics and Machine Learning Recommender System},
journal = {Procedia Computer Science},
volume = {170},
pages = {626-633},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305901},
author = {Govind Yatnalkar and Husnu S. Narman and Haroon Malik},
keywords = {Ride Sharing, Characteristics, Machine Learning, Recommender System},
abstract = {Ride Sharing provides benefits like reducing traffic and pollution, but currently, the usage is significantly low due to social barriers, long rider waiting time, and unfair pricing models. Considering the aforementioned issues, we present an Enhanced Ride Sharing Model (ERSM) in which riders are matched based on a specific set of human characteristics using Machine Learning. After trip completion, we record the user feedback and compute two main characteristics that are most important to riders. The registered and the computed characteristics are fed to a classification module, which later predicts the two main characteristics for new riders. We have carried an extensive simulation with Google Map APIs and real-time New York City Cab data to measure the model performance. Our proposed model and obtained results will help service providers to increase the usage of Ride Sharing, and implicitly preserve natural resources plus improve environmental conditions.}
}
@article{MANNION201760,
title = {Policy invariance under reward transformations for multi-objective reinforcement learning},
journal = {Neurocomputing},
volume = {263},
pages = {60-73},
year = {2017},
note = {Multiobjective Reinforcement Learning: Theory and Applications},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.05.090},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217311037},
author = {Patrick Mannion and Sam Devlin and Karl Mason and Jim Duggan and Enda Howley},
keywords = {Reinforcement learning, Multi-objective, Potential-based, Reward shaping, Multi-agent systems},
abstract = {Reinforcement Learning (RL) is a powerful and well-studied Machine Learning paradigm, where an agent learns to improve its performance in an environment by maximising a reward signal. In multi-objective Reinforcement Learning (MORL) the reward signal is a vector, where each component represents the performance on a different objective. Reward shaping is a well-established family of techniques that have been successfully used to improve the performance and learning speed of RL agents in single-objective problems. The basic premise of reward shaping is to add an additional shaping reward to the reward naturally received from the environment, to incorporate domain knowledge and guide an agent’s exploration. Potential-Based Reward Shaping (PBRS) is a specific form of reward shaping that offers additional guarantees. In this paper, we extend the theoretical guarantees of PBRS to MORL problems. Specifically, we provide theoretical proof that PBRS does not alter the true Pareto front in both single- and multi-agent MORL. We also contribute the first published empirical studies of the effect of PBRS in single- and multi-agent MORL problems.}
}
@article{SALEEM2019381,
title = {Deep Learning for Internet of Things Data Analytics},
journal = {Procedia Computer Science},
volume = {163},
pages = {381-390},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321593},
author = {Tausifa Jan Saleem and Mohammad Ahsan Chishti},
keywords = {Internet of Things, Deep Learning, Data Analytics},
abstract = {The recent technological innovations and brisk amalgamation of domains such as sensing and actuating technologies, embedded systems, wireless communication, and data analytics are accelerating the growth of Internet of Things (IoT). The massive number of sensors deployed in IoT generate humongous volumes of data for a broad range of applications such as smart home, smart healthcare, smart manufacturing, smart transportation, smart grid, smart agriculture etc. Analyzing such data in order to facilitate enhanced decision making, increase productivity and accuracy, ameliorate revenue is a critical process that makes IoT a precious idea for businesses and a standard of life improving paradigm. Although deriving concealed information and inferences out of IoT data is promising to improve the standard of our lives, it is a complicated task that cannot be accomplished by conventional paradigms. Deep Learning would play a vital role in creating smarter IoT as it has shown remarkable results in different fields including image recognition, information retrieval, speech recognition, natural language processing, indoor localization, physiological and psychological state detection etc. and these form the foundation services for IoT applications. In this regard, investigating the potential of Deep Learning for IoT data analytics becomes indispensable. Motivated to address this concern, this paper explores the flair of Deep Learning for analyzing data generated from IoT environments. A detailed discussion on various Deep Learning architectures, their role in IoT data analytics and potential use cases is also presented. Finally, open research challenges and future research directions are discussed in order to promote future research in this domain.}
}
@article{LI20218625,
title = {Neural-network-based consensus of multiple Euler-Lagrange systems with an event-triggered mechanism},
journal = {Journal of the Franklin Institute},
volume = {358},
number = {16},
pages = {8625-8638},
year = {2021},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2021.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0016003221005196},
author = {Sheng Li and Wencheng Zou and Zhengrong Xiang},
abstract = {This paper addresses the consensus problem for a class of multiple Euler-Lagrange systems, where agents communicate with neighbors under an event-triggered mechanism. Due to the more complex dynamical characteristics, the consensus problem of multiple Euler-Lagrange systems is more challenging than that of ordinary second-order multi-agent systems. In this study, we assume that the inertia matrix, the Coriolis and centrifugal term, and the gravitational torque are totally unknown, then a protocol is derived by integrating the Lyapunov functional method, neural network approximation and adaptive control techniques. In addition, the event-triggered mechanism effectively reduces the communication traffic, and the Zeno behavior is well excluded. By a demonstrative example, the effectiveness of the protocol is illustrated.}
}
@article{KIM2021104629,
title = {Pressure pattern recognition in buildings using an unsupervised machine-learning algorithm},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {214},
pages = {104629},
year = {2021},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2021.104629},
url = {https://www.sciencedirect.com/science/article/pii/S016761052100115X},
author = {Bubryur Kim and N. Yuvaraj and K.T. Tse and Dong-Eun Lee and Gang Hu},
keywords = {Unsupervised learning algorithm, Wind tunnel testing, Pressure pattern, Clustering algorithms, Pattern recognition, Wind pressure},
abstract = {Owing to its significance in ensuring structural safety and occupant comfort, wind pressure on buildings has attracted the attention of numerous scholars. However, the characteristics of wind pressures are usually complex. This study employs an unsupervised machine-learning algorithm, clustering algorithms, to study wind pressures on buildings. Wind pressures on a single building and two adjacent buildings with different gaps are measured in a wind tunnel, with clustering algorithms applied to cluster different wind pressure patterns. The results show that for the single-building model, the pressure patterns are symmetrical on the side surfaces of the building; for the two-building model with a small gap, a channeling effect can be identified; for the two-building model with a large gap, the pressure patterns shared symmetry with that of the single-building model. Clustering algorithms can recognize unidentified patterns of wind pressures on buildings. This study demonstrates that clustering algorithms are a powerful tool for recognizing patterns hidden in complex pressure fields and flow fields. Therefore, this study proposes a promising machine-learning technique that can perfectly complement traditional building methods using wind engineering.}
}
@article{KUMAR20181,
title = {Deep learning framework for recognition of cattle using muzzle point image pattern},
journal = {Measurement},
volume = {116},
pages = {1-17},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2017.10.064},
url = {https://www.sciencedirect.com/science/article/pii/S0263224117306991},
author = {Santosh Kumar and Amit Pandey and K. {Sai Ram Satwik} and Sunil Kumar and Sanjay Kumar Singh and Amit Kumar Singh and Anand Mohan},
keywords = {Cattle recognition, Muzzle point image, Deep learning, Convolution Neural Network, DBN, SDAE, Verification, Computer vision, LBP, SURF, PCA, VLAD, LDA},
abstract = {Animal biometrics is a frontier area of computer vision, pattern recognition and cognitive science to plays the vital role for the registration, unique identification, and verification of livestock (cattle). The existing handcrafted texture feature extraction and appearance based feature representation techniques are unable to perform the animal recognition in the unconstrained environment. Recently deep learning approaches have achieved more attention for recognition of species or individual animal using visual features. In this research, we propose the deep learning based approach for identification of individual cattle based on their primary muzzle point (nose pattern) image pattern characteristics to addressing the problem of missed or swapped animals and false insurance claims. The major contributions of the work as follows: (1) preparation of muzzle point image database, which are not publically available, (2) extraction of the salient set of texture features and representation of muzzle point image of cattle using the deep learning based convolution neural network, deep belief neural network proposed approaches. The stacked denoising auto-encoder technique is applied to encode the extracted feature of muzzle point images and (3) experimental results and analysis of proposed approach. Extensive experimental results illustrate that the proposed deep learning approach outperforms state-of-the-art methods for recognition of cattle on muzzle point image database. The efficacy of the proposed deep learning approach is computed under different identification settings. With multiple test galleries, rank-1 identification accuracy of 98.99% is achieved.}
}
@article{ARJONAMARTINEZ2021183,
title = {Characterizing parking systems from sensor data through a data-driven approach},
journal = {Transportation Letters},
volume = {13},
number = {3},
pages = {183-192},
year = {2021},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2020.1866331},
url = {https://www.sciencedirect.com/science/article/pii/S1942786722000790},
author = {Jamie {Arjona Martinez} and Maria Paz Linares and Josep Casanovas},
keywords = {Parking availability forecast, deep learning, smart cities, recurrent models, time series},
abstract = {Nowadays, urban traffic affects the quality of life in cities as the problem becomes even more exacerbated by parking issues: congestion increases due to drivers searching slots to park. An Internet of Things approach permits drivers to know the parking availability in real time and provides data that can be used to develop predictive models. This can be useful in improving the management of parking areas while having an important effect on traffic. This work begins by describing the state-of-the-art parking predictive models and, then, introduces the recurrent neural network methods that were used Long Short-Term Memory and Gated Recurrent Unit, as well as the models developed according to real scenarios in Wattens and Los Angeles. To improve the quality of the models, exogenous variables related to weather and calendar are considered. Finally, the results are described, followed by suggestions for future research.}
}
@article{MULAHUWAISH2020102006,
title = {Efficient classification model of web news documents using machine learning algorithms for accurate information},
journal = {Computers & Security},
volume = {98},
pages = {102006},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302790},
author = {Aos Mulahuwaish and Kevin Gyorick and Kayhan Zrar Ghafoor and Halgurd S. Maghdid and Danda B. Rawat},
keywords = {Web News Applications, Data mining},
abstract = {Web applications are regarded as a popular platform to exchange information with users. These applications have to be able to process Big-Data quickly and to serve users in a timely manner with accurate information posted in news portals which can be a huge challenge to overcome. Huge computation power is needed to crawl the web and process big-data and the methods are needed to be developed to reduce space and time complexity of this process. Data mining is considered to be a solution to mitigate the aforementioned challenges by extracting specific information based on explicit features. This paper proposes an efficient model for web that extracts news information and sorts news documents into four different categories business, technology & science, health and entertainment. Four different machine learning classifiers Support Vector Machine (SVM), K-Nearest Neighbors (kNN), Decision Tree (DT) and Long Short-Term Memory (LSTM) are compared. These classifiers are implemented separately and are then compared using accuracy and receiver operating characteristic curves. The attained results show that the accuracy of kNN was the worst at 88.72% and SVM was the best at 95.04%.}
}
@article{MIANI2022432,
title = {Young drivers’ pedestrian anti-collision braking operation data modelling for ADAS development},
journal = {Transportation Research Procedia},
volume = {60},
pages = {432-439},
year = {2022},
note = {New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.12.056},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521009571},
author = {Matteo Miani and Matteo Dunnhofer and Christian Micheloni and Andrea Marini and Nicola Baldo},
keywords = {ADAS, Traffic safety, Driver behaviour, Gated Recurrent Units, Driving simulator},
abstract = {Smart cities and smart mobility come from intelligent systems designed by humans. Artificial Intelligence (AI) is contributing significantly to the development of these systems, and the automotive industry is the most prominent example of "smart" technology entering the market: there are Advanced Driver Assistance System (ADAS), Radar/LIDAR detection units and camera-based Computer Vision systems that can assess driving conditions. Actually, these technologies have become consumer goods and services in mass-produced vehicles to provide human drivers with tools for a more comfortable and safer driving. Nevertheless, they need to be further improved for progress in the transition to fully automated driving or simply to increase vehicle automation levels. To this end, it becomes imperative to accurately predict driver’s decisions, model human driving behaviors, and introduce more accurate risk assessment metrics. This paper presents a system that can learn to predict the future braking behavior of a driver in a typically urban vehicle-pedestrian conflict, i.e., when a pedestrian enters a zebra crossing from the curb and a vehicle is approaching. The algorithm proposes a sequential prediction of relevant operational indicators that continuously describe the encounter process. A car driving simulator was used to collect reliable data on braking behaviours of a cohort of 68 licensed university students, who faced the same urban scenario. The vehicle speed, steering wheel angle, and pedal activity were recorded as the participants approached the crosswalk, along with the azimuth angle of the pedestrian and the relative longitudinal distance between the vehicle and the pedestrian: the proposed system employs the vehicle information as human driving decisions and the pedestrian information as explanatory variables of the environmental state. In fact, the pedestrian’s polar coordinates are usually calculated by an on-board millimeter-wave radar which is typically used to perceive the environment around a vehicle. All mentioned information is represented in the form of time series data and is used to train a recurrent neural network in a supervised machine learning process. The main purpose of this research is to define a system of behavioral profiles in non-collision conditions that could be used for enhancing the existing intelligent driving systems, e.g., to reduce the number of warnings when the driver is not on a collision course with a pedestrian. Preliminary experiments reveal the feasibility of the proposed system.}
}
@article{BABAR2020102370,
title = {Secure and resilient demand side management engine using machine learning for IoT-enabled smart grid},
journal = {Sustainable Cities and Society},
volume = {62},
pages = {102370},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102370},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720305904},
author = {Muhammad Babar and Muhammad Usman Tariq and Mian Ahmad Jan},
keywords = {Smart grid, Security, Machine learning, Internet of things, Demand-side management, Home area network},
abstract = {The national security, economy, and healthcare heavily rely on the reliable distribution of electricity. The incorporation of communication technologies and sensors in the power structures, recognized as the smart grid which revolutionizes the model of the production, distribution, monitoring, and control of the electricity. To realize the applicability of smart grid, several issues need to be addressed. Securing the smart grid is a very challenging task and a pressing issue. In this article, a secure demand-side management (DSM) engine is proposed using machine learning (ML) for the Internet of Things (IoT)-enabled grid. The proposed DSM engine is responsible to preserve the efficient utilization of energy based on priorities. A specific resilient model is proposed to control intrusions in the smart grid. The resilient agent predicts the dishonest entities using the ML classifier. Advanced energy management and interface controlling agents are proposed to process energy information to optimize energy utilization. The efficient simulation is executed to test the efficiency of the proposed scheme. The analysis results reveal that the projected DSM engine is less vulnerable to the intrusion and effective enough to reduce the power utilization of the smart grid.}
}
@article{DOMINGUESJUNIOR2021102290,
title = {A new WAF architecture with machine learning for resource-efficient use},
journal = {Computers & Security},
volume = {106},
pages = {102290},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102290},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821001140},
author = {Manoel {Domingues Junior} and Nelson F.F. Ebecken},
keywords = {Web application firewall, SVM, Perceptron, Logistic regression, Benchmark, Modsecurity, OWASP CRS},
abstract = {Web Application Firewalls penalizes everyone, including latency in all requests, whether they are malicious or not. Several studies have reported the benefits of using Machine Learning to extract new rules to detect malware and malicious web requests. However, comparing the metrics of the models with their use of computational resources remains to be accomplished. This work aims to show a distributed WAF architecture, using ML classifiers as one of its components. Instead of having an enforcement point that analyzes the complete HTTP protocol for violations in this architecture, we have a trained classifier to detect them. The first part of this work verifies the viability of using classifiers based on their metrics, such as accuracy and recall. We analyze two datasets and make comparisons about their use. The second part of this paper compares ML models’ prediction processing time and a rules-based engine’s processing time. The classifiers used in this paper had a processing time of about 18x less than a rule-based engine. We also show that a classifier can find errors in the classification of a dataset generated by a WAF based on rules. We present samples and experimental codes to show the difference in approaches.}
}
@article{AL2021102435,
title = {STL-HDL: A new hybrid network intrusion detection system for imbalanced dataset on big data environment},
journal = {Computers & Security},
volume = {110},
pages = {102435},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102435},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002595},
author = {Samed Al and Murat Dener},
keywords = {Big data, Intrusion detection, Deep learning, Machine learning, Classification, Apache spark, Imbalanced data, SMOTE, Tomek-Links},
abstract = {The ability to process large amounts of data in real time using big data analytics tools brings many advantages that can be used in intrusion detection systems. Deep learning approaches have also been increasingly used in big data analysis and intrusion detection systems in recent years. In this study, a new classification-based network attack detection system is proposed on network flow traffic generating big data. In the proposed system, a Hybrid Deep Learning (HDL) network consisting of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) is used for a better intrusion detection system. In addition, data imbalance processing consisting of Synthetic Minority Oversampling Technique (SMOTE) and Tomek-Links sampling methods called STL was used to reduce the effects of data imbalance on system performance. In the study, PySpark providing Python support on Apache Spark platform in Google Colab environment was used. The multiclass evaluation of the model was made on the CIDDS-001 data set, and the binary classification evaluation was made on the UNS-NB15 data set. Nine different machine learning and deep learning algorithms have been compared to the proposed method. The results obtained were evaluated using the parameters of Accuracy, F-Measure, Precision, Recall, ROC Curve and Precision-Recall Curve. As a result, the proposed method has reached 99.83% accuracy in multiclass classification and 99.17% accuracy in binary classification. According to the results, the proposed method has achieved quite successful results in detecting network attacks in imbalanced data sets compared to current methods.}
}
@article{RAHMAN2021355,
title = {A light-weight dynamic ontology for Internet of Things using machine learning technique},
journal = {ICT Express},
volume = {7},
number = {3},
pages = {355-360},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2020.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405959520304902},
author = {Hafizur Rahman and Md. Iftekhar Hussain},
keywords = {Internet of Things, Ontology, Semantic, Light-weight, Dynamic},
abstract = {Ensuring semantic interoperability in the future Internet of Things can be a challenging task due to their heterogeneous nature and increasing scale. Ontologies are widely used to achieve semantic interoperability among IoT applications and services. But, available ontologies are very complex, static or unable to fulfill the requirements of IoT. To address this concern, we proposed a light-weight dynamic ontology using only the most important concepts and clustering technique. It provides dynamic semantics automatically to include additional concepts using machine learning technique. Compared to the existing ontology, the proposed model reduces query response time and memory consumption to some extent.}
}
@article{HAMMAM2020331,
title = {Real-time multiple spatiotemporal action localization and prediction approach using deep learning},
journal = {Neural Networks},
volume = {128},
pages = {331-344},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020301878},
author = {Ahmed Ali Hammam and Mona M. Soliman and Aboul Ella Hassanien},
keywords = {Deep learning, Action localization, Action prediction, Spatiotemporal, YOLO network, Optical flow},
abstract = {Detecting the locations of multiple actions in videos and classifying them in real-time are challenging problems termed ”action localization and prediction” problem. Convolutional neural networks (ConvNets) have achieved great success for action localization and prediction in still images. A major advance occurred when the AlexNet architecture was introduced in the ImageNet competition. ConvNets have since achieved state-of-the-art performances across a wide variety of machine vision tasks, including object detection, image segmentation, image classification, facial recognition, human pose estimation, and tracking. However, few works exist that address action localization and prediction in videos. The current action localization research primarily focuses on the classification of temporally trimmed videos in which only one action occurs per frame. Moreover, nearly all the current approaches work only offline and are too slow to be useful in real-world environments. In this work, we propose a fast and accurate deep-learning approach to perform real-time action localization and prediction. The proposed approach uses convolutional neural networks to localize multiple actions and predict their classes in real time. This approach starts by using appearance and motion detection networks (known as ”you only look once” (YOLO) networks) to localize and classify actions from RGB frames and optical flow frames using a two-stream model. We then propose a fusion step that increases the localization accuracy of the proposed approach. Moreover, we generate an action tube based on frame level detection. The frame by frame processing introduces an early action detection and prediction with top performance in terms of detection speed and precision. The experimental results demonstrate this superiority of our proposed approach in terms of both processing time and accuracy compared to recent offline and online action localization and prediction approaches on the challenging UCF-101-24 and J-HMDB-21 benchmarks.}
}
@article{GIMENEZ2020315,
title = {Semantic-based padding in convolutional neural networks for improving the performance in natural language processing. A case of study in sentiment analysis},
journal = {Neurocomputing},
volume = {378},
pages = {315-323},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.096},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219314687},
author = {Maite Giménez and Javier Palanca and Vicent Botti},
keywords = {Natural language processing, Convolutional neural networks, Padding},
abstract = {In this work, a methodology for applying semantic-based padding in Convolutional Neural Networks for Natural Language Processing tasks is proposed. Semantic-based padding takes advantage of the unused space required for having a fixed-size input matrix in a Convolutional Network effectively, using words present in the sentence. The methodology proposed has been evaluated intensively in Sentiment Analysis tasks using a variety of word embeddings. In all the experimentation carried out the proposed semantic-based padding improved the results achieved when no padding strategy is applied. Moreover, when the model used a pre-trained word embeddings, the performance of the state of the art has been surpassed.}
}
@article{SCOTT2021103914,
title = {Exploratory literature review of blockchain in the construction industry},
journal = {Automation in Construction},
volume = {132},
pages = {103914},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103914},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003654},
author = {Denis J. Scott and Tim Broyd and Ling Ma},
keywords = {Blockchain, Smart contract, Decentralised applications, Construction industry, Built environment, Smart cities},
abstract = {First academic publications on blockchain in construction instantiated in 2017, with three documents. Over the course of several years, new literature emerged at an average annual growth rate of 184%, surmounting to 121 documents at time of writing this article in early 2021. All 121 publications were reviewed to investigate the expansion and progression of the topic. A mixed methods approach was implemented to assess the existing environment through a literature review and scientometric analysis. Altogether, 33 application categories of blockchain in construction were identified and organised into seven subject areas, these include (1) procurement and supply chain, (2) design and construction, (3) operations and life cycle, (4) smart cities, (5) intelligent systems, (6) energy and carbon footprint, and (7) decentralised organisations. Limitations included using only one scientific database (Scopus), this was due to format inconsistencies when downloading and merging various bibliographic data sets for use in visual mapping software.}
}
@article{KHAN2020196,
title = {Cascading handcrafted features and Convolutional Neural Network for IoT-enabled brain tumor segmentation},
journal = {Computer Communications},
volume = {153},
pages = {196-207},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419313040},
author = {Hikmat Khan and Pir Masoom Shah and Munam Ali Shah and Saif ul Islam and Joel J.P.C. Rodrigues},
keywords = {IoT, Gliomas, Glioblastoma, Brain tumor, Segmentation, Deep learning, Convolutional neural network, SVM, Handcrafted features},
abstract = {The Internet of Things (IoT) has revolutionized the medical world by facilitating data acquisition using various IoT devices. These devices generate the data in multiple forms including text, images, and videos. Given this, the extraction of accurate and useful information from the massive serge IoT generated data is a highly challenging task. Recently, the brain tumor segmentation from IoT generated images has emerged as a promising issue that requires sophisticated and efficient techniques. The accurate brain tumor segmentation is challenging due to large variations in tumor appearance. Existing methods either use handcrafted features based techniques or Convolutional Neural Network (CNN). In this paper, a novel cascading approach for fully automatic brain tumor segmentation has been proposed, which intelligently combines handcrafted features and CNN. First, three handcrafted features are computed namely mean intensity, Local Binary Pattern (LBP) and Histogram of Oriented Gradients (HOG) and then Support Vector Machine (SVM) is employed to perform pixel classification that results in Confidence Surface Modality (CSM). This CSM along with the given Magnetic Resonance Imaging (MRI) is fed to a novel three pathways CNN architecture. In the experiments on BRATS 2015 dataset, the proposed method achieves promising results with Dice similarity scores of 0.81, 0.76 and 0.73 on complete, core and enhancing tumor, respectively.}
}
@article{ANTONOPOULOS2021100071,
title = {Data-driven modelling of energy demand response behaviour based on a large-scale residential trial},
journal = {Energy and AI},
volume = {4},
pages = {100071},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000252},
author = {Ioannis Antonopoulos and Valentin Robu and Benoit Couraud and David Flynn},
keywords = {Artificial intelligence, Machine learning, Artificial neural networks, Ensemble methods, Demand response, Residential response behaviour, Power systems},
abstract = {Recent years have seen an increasing interest in Demand Response (DR), as a means to satisfy the growing flexibility needs of modern power grids. This increased flexibility is required due to the growing proportion of intermittent renewable energy generation into the energy mix, and increasing complexity in demand profiles from the electrification of transport networks. Currently, less than 2% of the global potential for demand-side flexibility is currently utilised, but a more widespread adoption of residential consumers as flexibility resources can lead to substantially higher utilisation of the demand-side flexibility potential. In order to achieve this target, acquiring a better understanding of how residential DR participants respond in DR events is essential – and recent advances in novel machine learning and statistical AI provide promising tools to address this challenge. This study provides an in-depth analysis of how residential customers have responded in incentive-based DR, utilising household-related data from a large-scale, real-world trial: the Smart Grid, Smart City (SGSC) project. Using a number of different machine learning approaches, we model the relationship between a household’s response and household-related features. Moreover, we examine the potential effects of households’ features on the residential response behaviour, and highlight a number of key insights which raise questions about the reported level of consumers’ engagement in DR schemes, and the motivation for different customers’ response level. Finally, we explore the temporal structure of the response – and although we found no supporting evidence of DR responders learning over time for the available data from this trial, the proposed methodologies could be used for longer-term longitudinal DR studies. Our study concludes with a broader discussion of our findings and potential paths for future research in this emerging area.}
}
@article{IP2018376,
title = {Big data and machine learning for crop protection},
journal = {Computers and Electronics in Agriculture},
volume = {151},
pages = {376-383},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917314588},
author = {Ryan H.L. Ip and Li-Minn Ang and Kah Phooi Seng and J.C. Broster and J.E. Pratley},
keywords = {Big data, Machine learning, Crop protection, Weed control, Herbicide resistance, Markov random field model},
abstract = {Crop protection is the science and practice of managing plant diseases, weeds and other pests. Weed management and control are important given that crop yield losses caused by pests and weeds are high. However, farmers face increased complexity of weed control due to evolved resistance to herbicides. This paper first presents a brief review of some significant research efforts in crop protection using Big data with the focus on weed control and management followed by some potential applications. Some machine learning techniques for Big data analytics are also reviewed. The outlook for Big data and machine learning in crop protection is very promising. The potential of using Markov random fields (MRF) which takes into account the spatial component among neighboring sites for herbicide resistance modeling of ryegrass is then explored. To the best of our knowledge, no similar work of modeling herbicide resistance using the MRF has been reported. Experiments and data analytics have been performed on data collected from farms in Australia. Results have revealed the good performance of our approach.}
}
@article{JOSHI2021,
title = {A Fuzzy Logic based feature engineering approach for Botnet detection using ANN},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001658},
author = {Chirag Joshi and Ranjeet Kumar Ranjan and Vishal Bharti},
keywords = {Artificial Neural Network (ANN), Botnet, Fuzzy Logic, Fuzzy rules, CTU-13, Cyber Security},
abstract = {In recent years, Botnet has become one of the most dreadful type of malicious entity. Because of the hidden and carrying capacity of Botnet, the detection task has become a real challenge. Different methodologies have been applied for finding the source of Botnet at an early stage. Machine Learning and Deep Learning have greatly impacted these Botnet detection methodologies. But still, it is a difficult task because of the lesser number of features available in the Botnet datasets. In this paper, we have proposed a Fuzzy Logic based feature engineering method. The proposed method first identifies the fuzzy elements in the dataset and then generates fuzzy sets. The features generated using this method is used by an Artificial Neural Network for classification of Botnet. To train and evaluate the ANN model, we have used the CTU-13 dataset. The proposed feature engineering method and Botnet classification method has performed well with an accuracy of 99.94%. Still this method needs to be tested on different datasets. In future, new fuzzy rules can also be made to generate a new set of features as well as these rules can be used to generate features from other datasets.}
}
@article{SU2020106298,
title = {Applying deep learning algorithms to enhance simulations of large-scale groundwater flow in IoTs},
journal = {Applied Soft Computing},
volume = {92},
pages = {106298},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106298},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620302386},
author = {Yu-Sen Su and Chuen-Fa Ni and Wei-Ci Li and I-Hsien Lee and Chi-Ping Lin},
keywords = {Deep learning, Numerical simulation, Groundwater, Internet of Things},
abstract = {Deep learning for enhancing simulation IoTs groundwater flow is a good solution for gaining insights into the behavior of aquifer systems. In previous studies, corresponding results give a basis for the rational management of groundwater resources. The users generally require special skills or knowledge and massive observations in representing the field reality to perform the deep learning algorithms and simulations. To simplify the procedures for performing the numerical and large-scale groundwater flow simulations, we apply the deep learning algorithms which combine both the numerical groundwater model and large-scale IoTs, groundwater flow measuring equipment and various complex groundwater numerical models. The mechanism has the capability to show spatial distributions of in-situ data, analyze the spatial relationships of observed data, generate meshes, update users’ databases with in-situ observed data, and create professional reports. According to the numerical simulation results, we revealed that the deep learning algorithms are high computational efficiency, and we can enhance precise variance estimations for large-scale groundwater flow problems. The findings help users to best apply the deep learning algorithms in an easier way, get more accurate simulation results, and manage the groundwater resources rationally.}
}
@article{TAMAYO2020220,
title = {Unsupervised machine learning to analyze City Logistics through Twitter},
journal = {Transportation Research Procedia},
volume = {46},
pages = {220-228},
year = {2020},
note = {The 11th International Conference on City Logistics, Dubrovnik, Croatia, 12th - 14th June 2019},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.03.184},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520303859},
author = {Simon Tamayo and François Combes and Arthur Gaudron},
keywords = {City Logistics, machine learning, natural language processing, social media mining, sentiment analysis},
abstract = {City Logistics is characterized by multiple stakeholders that often have different views on such a complex system. From a public policy perspective, identifying stakeholders, issues and trends is a daunting challenge, only partially addressed by traditional observation systems. Nowadays, social media is one of the biggest channels of public expression and is often used to communicate opinions and content related to City Logistics. The idea of this research is that analyzing social media content could help in understanding the public perception of City Logistics. This paper offers a methodology for collecting content from Twitter and implementing machine learning techniques (Unsupervised Learning and Natural Language Processing), to perform content and sentiment analysis. The proposed methodology is applied to more than 110 000 tweets containing City Logistics key-terms. Results allowed the building of an Interest Map of concepts and a Sentiment Analysis to determine if City Logistics entries are positive, negative or neutral.}
}
@article{ZHANG20191,
title = {Smart Chinese medicine for hypertension treatment with a deep learning model},
journal = {Journal of Network and Computer Applications},
volume = {129},
pages = {1-8},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518304004},
author = {Qingchen Zhang and Changchuan Bai and Zhikui Chen and Peng Li and Shuo Wang and He Gao},
keywords = {Smart world, Smart Chinese medicine, Hypertension, Deep learning, Classical prescriptions},
abstract = {As one important component of smart world, smart health is drawing more and more attention. Over the years, hypertension has become a high incident disease and it is continuing to threaten human health seriously. Unfortunately, no effective way has been found to cure hypertension at present. Many medical experiences have demonstrated the curative effect of traditional Chinese medicine in the hypertension treatment. In this paper, we explore smart Chinese medicine for hypertension treatment by combining deep learning and the traditional Chinese medicine theory. First, we present a potential idea to categorize the clinical cases into different groups based on symptom with the stacked auto-encoder model. Furthermore, we discuss and summarize the curative effect of classical prescriptions and herbs for each type of symptom in hypertension. Finally, we review the related work about deep learning models in medical application. The explored smart Chinese medicine is expected to guide doctors to treat hypertension diseases in clinical, which is helpful to build smart world, especially smart health.}
}
@article{MIN2019502,
title = {Machine Learning based Digital Twin Framework for Production Optimization in Petrochemical Industry},
journal = {International Journal of Information Management},
volume = {49},
pages = {502-519},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311484},
author = {Qingfei Min and Yangguang Lu and Zhiyong Liu and Chao Su and Bo Wang},
keywords = {digital twin, machine learning, internet of things, petrochemical industry, production control optimization},
abstract = {Digital twins, along with the internet of things (IoT), data mining, and machine learning technologies, offer great potential in the transformation of today’s manufacturing paradigm toward intelligent manufacturing. Production control in petrochemical industry involves complex circumstances and a high demand for timeliness; therefore, agile and smart controls are important components of intelligent manufacturing in the petrochemical industry. This paper proposes a framework and approaches for constructing a digital twin based on the petrochemical industrial IoT, machine learning and a practice loop for information exchange between the physical factory and a virtual digital twin model to realize production control optimization. Unlike traditional production control approaches, this novel approach integrates machine learning and real-time industrial big data to train and optimize digital twin models. It can support petrochemical and other process manufacturing industries to dynamically adapt to the changing environment, respond in a timely manner to changes in the market due to production optimization, and improve economic benefits. Accounting for environmental characteristics, this paper provides concrete solutions for machine learning difficulties in the petrochemical industry, e.g., high data dimensions, time lags and alignment between time series data, and high demand for immediacy. The approaches were evaluated by applying them in the production unit of a petrochemical factory, and a model was trained via industrial IoT data and used to realize intelligent production control based on real-time data. A case study shows the effectiveness of this approach in the petrochemical industry.}
}
@article{YANG2022100061,
title = {A survey on multisource heterogeneous urban sensor access and data management technologies},
journal = {Measurement: Sensors},
volume = {19},
pages = {100061},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421000246},
author = {Fei Yang and Yixin Hua and Xiang Li and Zhenkai Yang and Xinkai Yu and Teng Fei},
keywords = {Smart city, Urban sensor, Access, Data management, Computational burden, Energy consumption, Cybersecurity},
abstract = {Urban sensors are an important part of urban infrastructures and are usually heterogeneous. Urban sensors with different uses vary greatly in hardware structure, communication protocols, data formats, interaction modes, sampling frequencies, data accuracy and service quality, thus posing an enormous challenge to the unified integration and sharing of massive sensor information resources. Consequently, access and data management methods for these multisource heterogeneous urban sensors are extremely important. Additionally, multisource heterogeneous urban sensor access and data management technologies provide strong support for intelligent perception and scientific management at the city scale and can accelerate the construction of smart cities or digital twin cities with virtual reality features. We systematically summarize the related research on these technologies. First, we present a summary of the concepts and applications of urban sensors. Then, the research progress on multisource heterogeneous urban sensor access technologies is analysed in relation to communication protocols, data transmission formats, access standards, access technologies and data transmission technologies. Subsequently, the data management technologies for urban sensors are reviewed from the perspectives of data cleaning, data compression, data storage, data indexing and data querying. In addition, the challenges faced by the technologies above and corresponding feasible solutions are discussed from three aspects, namely, the integration of massive Internet of Things (IoT), computational burden and energy consumption and cybersecurity. Finally, a summary of this paper is given, and possible future development directions are analysed and discussed.}
}
@article{BILORIA20213,
title = {From smart to empathic cities},
journal = {Frontiers of Architectural Research},
volume = {10},
number = {1},
pages = {3-16},
year = {2021},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2095263520300698},
author = {Nimish Biloria},
keywords = {Empathic city, Smart city, Wellbeing, Neoliberalism, Regenerative model},
abstract = {This paper acknowledges the contemporary neoliberal mode of operation of Smart Cities. The pitfalls of Smart Cities concerning its propensity towards techno-centric and efficiency-focused governance are identified, with diminutive emphasis on social equity and human-centric urban growth. Thus, the paper elaborates upon an alternative mode of person-environment-interaction based approach towards placemaking: Empathic Cities. This approach implies embracing a shift from efficiency to sufficiency and wellbeing embedded regenerative perspective for conceiving the built environment. First, the variable dimensions of urban growth and governance, which gave rise to the smart city, are contextualized. The embedded neoliberal operational agenda of smart cities are established. On this basis, the underpinnings of an empathic city are established by acknowledging the shift from techno-centric to human-centric and from product-based to context-based smart city and wellbeing perspectives. Strategies toward urban development are proposed, such as embracing a regenerative perspective wherein the city and its constituents need to be understood as interdependent systemic elements while embracing a human-centric and ethical approach. Additionally, a transition from efficiency to sufficiency-oriented practices and a shift towards inclusive modes of participatory governance are proposed as fundamental principles for an empathic future of the built environment.}
}
@article{DEY2021117103,
title = {Automated terminal unit performance analysis employing x-RBF neural network and associated energy optimisation – A case study based approach},
journal = {Applied Energy},
volume = {298},
pages = {117103},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117103},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921005493},
author = {Maitreyee Dey and Soumya Prakash Rana and Sandra Dudley},
keywords = {Heating, Ventilation and Air-Conditioning, Terminal Unit, Artificial Neural Networks, Automatic Fault Detection and Diagnosis, Energy Optimisation},
abstract = {An artificial neural network based framework, to analyse and address key energy related performance issues inside building environment is proposed. The study engages thousands of heating, ventilation, and air conditioning terminal unit data from real commercial buildings employing a new feature engineering method augmenting terminal unit time-series data to create novel features. These features are subsequently fed into the proposed neural network, named x-RBF which is a combination of x-means clustering followed by radial basic function neural networks for automatic fault detection and diagnosis. The new model has been successfully employed and investigated on different types of heating–cooling patterns concerning power demand and control strategies from actual building historical terminal unit data. The proposed x-RBF model has been trained-tested on approximately three years of building data and achieves 99.7% sensitivity, a first for real building applications. Comparison has been made with other neural networks to verify the performance of the proposed x-RBF and it is further validated through statistical measurements. The proposed model demonstrates its ability to truly predict and anticipate fault scenarios in terminal unit operation. Consequently, energy and cost calculations have been executed to realise the potential financial impact (as a consequence of performance improvements) that can be achieved by the proposed framework in the building environment.}
}
@article{SARAVANAN2021,
title = {Detection of software intrusion based on machine learning techniques for IOT systems},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.03.138},
url = {https://www.sciencedirect.com/science/article/pii/S221478532102157X},
author = {L Saravanan and  {Himanshu Sharma} and K.N Sreenivasulu and M Deivakani},
keywords = {IoT, Network Intrusion, Machine Learning},
abstract = {The essence of the Internet of Things (IoT) is simple in the globe. Dyn's 2016 cyber-attack showed vital fault lines between smart networks. The security of the Internet of Things (IoT) was a major concern. Apart from the threat to IoT protection, internet-related infestations often impact the entire internet environment, which can access insecure (smart gadgets. Mirai malware has disabled video surveillance systems and crippled the Internet with a DDoS (Distributed Denial of Service) target. Security threat mechanisms have developed both in terms of sophistication and variety in recent years. It is thus important to investigate strategies in the sense of IoT in order to detect and monitor emerging risks. By analysing current defensive strategies, the report classifies IoT security risks and issues for IoT networks. This paper discusses current NIDS methods, data sets and sniffing applications for the open and free-source network. Our priority is on intrusion device network detection (NIDS). The study then examines measures and evaluates state of the art IoT-based NIDS technologies for architectures, methodologies for detection and evaluation, risk analysis, and the implementation of algorithms. This study explores conventional ML approaches and NIDS and looks at future pathways. Our analysis concentrates on IoT NIDS for Machine Learning, as learning algorithms are secretly secure and performing. In addition to other leading surveys dealing with typical frames, it provides a thorough study of NIDSs that use different facets of learning methods on the internet of topics. We assume that the paper will aid in studies and the engineering to recognise risks and difficulties of IoT first, then to introduce their own IoT NIDS and ultimately create modern, knowledgeable approaches, taking into account IoT vulnerabilities. The survey also defends people from regular IoT NIDS.}
}
@article{ABDELLATIF2022111692,
title = {A thermal control methodology based on a machine learning forecasting model for indoor heating},
journal = {Energy and Buildings},
volume = {255},
pages = {111692},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111692},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821009762},
author = {Makram Abdellatif and Julien Chamoin and Jean-Marie Nianga and Didier Defer},
keywords = {Thermal comfort, Heating control, Learning algorithms, Linear regression, Genetic algorithms, Forecasting model},
abstract = {To take advantage of the data generated in buildings, this document proposes a methodology based on a machine learning model to improve thermal comfort and energy efficiency. This methodology uses measured data (e.g., indoor/outdoor temperature, relative humidity, etc.) and forecast data (e.g., meteorological data) to train a multiple linear regression model to forecast the indoor temperature of the space under study. Using the genetic algorithm optimization method, this model is then used to evaluate the different heating strategies generated. For each strategy, a score is assigned according to user-defined criteria in order to prioritize them and select the best one. By studying an office building simulated under the TRNSYS software, a multiple linear regression model was implemented with errors less than 1% and an adjusted R2 coefficient close to 0.9. Compared to a conventional heating strategy, this methodology can improve thermal comfort by up to 43%.}
}
@article{TULI2020187,
title = {HealthFog: An ensemble deep learning based Smart Healthcare System for Automatic Diagnosis of Heart Diseases in integrated IoT and fog computing environments},
journal = {Future Generation Computer Systems},
volume = {104},
pages = {187-200},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.043},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19313391},
author = {Shreshth Tuli and Nipam Basumatary and Sukhpal Singh Gill and Mohsen Kahani and Rajesh Chand Arya and Gurpreet Singh Wander and Rajkumar Buyya},
keywords = {Fog computing, Internet of things, Healthcare, Deep learning, Ensemble learning, Heart patient analysis},
abstract = {Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes which provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements.}
}
@article{ALAM2016437,
title = {Analysis of Eight Data Mining Algorithms for Smarter Internet of Things (IoT)},
journal = {Procedia Computer Science},
volume = {98},
pages = {437-442},
year = {2016},
note = {The 7th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2016)/The 6th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2016)/Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S187705091632213X},
author = {Furqan Alam and Rashid Mehmood and Iyad Katib and Aiiad Albeshri},
keywords = {Internet of Things (IoT), Support Vector Machine (SVM), K-Nearest Neighbours (KNN), Linear Discriminant Analysis (LDA), Naïve Bayes (NB), C4.5, C5.0, Artificial Neural Networks (ANNs), Deep Learning ANNs (DLANNs), Big Data, Smart Cities ;},
abstract = {Internet of Things (IoT) is set to revolutionize all aspects of our lives. The number of objects connected to IoT is expected to reach 50 billion by 2020, giving rise to an enormous amounts of valuable data. The data collected from the IoT devices will be used to understand and control complex environments around us, enabling better decision making, greater automation, higher efficiencies, productivity, accuracy, and wealth generation. Data mining and other artificial intelligence methods would play a critical role in creating smarter IoTs, albeit with many challenges. In this paper, we examine the applicability of eight well-known data mining algorithms for IoT data. These include, among others, the deep learning artificial neural networks (DLANNs), which build a feed forward multi-layer artificial neural network (ANN) for modelling high-level data abstractions. Our preliminary results on three real IoT datasets show that C4.5 and C5.0 have better accuracy, are memory efficient and have relatively higher processing speeds. ANNs and DLANNs can provide highly accurate results but are computationally expensive.}
}
@article{KIM2021102150,
title = {Cybersecurity for autonomous vehicles: Review of attacks and defense},
journal = {Computers & Security},
volume = {103},
pages = {102150},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102150},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820304235},
author = {Kyounggon Kim and Jun Seok Kim and Seonghoon Jeong and Jo-Hee Park and Huy Kang Kim},
keywords = {Smart city, Smart mobility, Autonomous vehicle artificial intelligence security survey},
abstract = {As technology has evolved, cities have become increasingly smart. Smart mobility is a crucial element in smart cities, and autonomous vehicles are an essential part of smart mobility. However, vulnerabilities in autonomous vehicles can be damaging to quality of life and human safety. For this reason, many security researchers have studied attacks and defenses for autonomous vehicles. However, there has not been systematic research on attacks and defenses for autonomous vehicles. In this survey, we analyzed previously conducted attack and defense studies described in 151 papers from 2008 to 2019 for a systematic and comprehensive investigation of autonomous vehicles. We classified autonomous attacks into the three categories of autonomous control system, autonomous driving systems components, and vehicle-to-everything communications. Defense against such attacks was classified into security architecture, intrusion detection, and anomaly detection. Due to the development of big data and communication technologies, techniques for detecting abnormalities using artificial intelligence and machine learning are gradually being developed. Lastly, we provide implications based on our systemic survey that future research on autonomous attacks and defenses is strongly combined with artificial intelligence and major component of smart cities.}
}
@article{WANG2020120175,
title = {Tension in big data using machine learning: Analysis and applications},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120175},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120175},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520310015},
author = {Huamao Wang and Yumei Yao and Said Salhi},
keywords = {Big data, Machine learning, Data size, Prediction accuracy, Social media},
abstract = {The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.}
}
@article{REHMAN2021453,
title = {DIDDOS: An approach for detection and identification of Distributed Denial of Service (DDoS) cyberattacks using Gated Recurrent Units (GRU)},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {453-466},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000327},
author = {Saif ur Rehman and Mubashir Khaliq and Syed Ibrahim Imtiaz and Aamir Rasool and Muhammad Shafiq and Abdul Rehman Javed and Zunera Jalil and Ali Kashif Bashir},
keywords = {Cyberattack, Cybersecurity, DDoS, IDS, Deep learning, GRU, Malware, Network, RNN, Traffic},
abstract = {Distributed Denial of Service (DDoS) attacks can put the communication networks in instability by throwing malicious traffic and requests in bulk over the network. Computer networks form a complex chain of nodes resulting in a formation of vigorous structure. Thus, in this scenario, it becomes a challenging task to provide an efficient and secure environment for the user. Numerous approaches have been adopted in the past to detect and prevent DDoS attacks but lack in providing efficient and reliable attack detection. As a result, there is still notable room for improvement in providing security against DDoS attacks. In this paper, a novel high-efficient approach is proposed named DIDDOS to protect against real-world new type DDoS attacks using Gated Recurrent Unit (GRU) a type of Recurrent Neural Network (RNN). Different classification algorithms such as Gated Recurrent Units (GRU), Recurrent Neural Networks (RNN), Naive Bayes (NB), and Sequential Minimal Optimization (SMO) are utilized to detect and identify DDoS attacks. Performance evaluation metrics like accuracy, recall, f1-score, and precision are used to evaluate the efficiency of the machine and deep learning classifiers. Experimental results yield the highest accuracy of 99.69% for DDoS classification in case of reflection attacks and 99.94% for DDoS classification in case of exploitation attacks using GRU.}
}
@article{SALEHI2018170,
title = {Emerging artificial intelligence methods in structural engineering},
journal = {Engineering Structures},
volume = {171},
pages = {170-189},
year = {2018},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2018.05.084},
url = {https://www.sciencedirect.com/science/article/pii/S0141029617335526},
author = {Hadi Salehi and Rigoberto Burgueño},
keywords = {Structural engineering, Artificial intelligence, Machine learning, Pattern recognition, Deep learning, Soft computing},
abstract = {Artificial intelligence (AI) is proving to be an efficient alternative approach to classical modeling techniques. AI refers to the branch of computer science that develops machines and software with human-like intelligence. Compared to traditional methods, AI offers advantages to deal with problems associated with uncertainties and is an effective aid to solve such complex problems. In addition, AI-based solutions are good alternatives to determine engineering design parameters when testing is not possible, thus resulting in significant savings in terms of human time and effort spent in experiments. AI is also able to make the process of decision making faster, decrease error rates, and increase computational efficiency. Among the different AI techniques, machine learning (ML), pattern recognition (PR), and deep learning (DL) have recently acquired considerable attention and are establishing themselves as a new class of intelligent methods for use in structural engineering. The objective of this review paper is to summarize techniques concerning applications of the noted AI methods in structural engineering developed over the last decade. First, a general introduction to AI is presented and the importance of AI in structural engineering is described. Thereafter, a review of recent applications of ML, PR, and DL in the field is provided, and the capability of such methods to address the restrictions of conventional models are discussed. Further, the advantages of employing such algorithmic methods are discussed in detail. Finally, potential research avenues and emerging trends for employing ML, PR, and DL are presented, and their limitations are discussed.}
}
@article{CORREA2020101510,
title = {A deep search method to survey data portals in the whole web: toward a machine learning classification model},
journal = {Government Information Quarterly},
volume = {37},
number = {4},
pages = {101510},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101510},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302896},
author = {Andreiwid Sheffer Correa and Alencar {Melo Jr.} and Flavio Soares Correa da Silva},
keywords = {Artificial intelligence (AI), Machine learning, Data portals, Open data, Survey},
abstract = {The emergence of standardized open data software platforms has provided a similar set of features to sustain the lifecycle of open data practices, which includes storing, managing, publishing, and visualizing data, in addition to providing an out-of-the-box solution for data portals. Accordingly, the dissemination of data portals that implement such platforms has paved the way for automation, wherein (meta)data extraction supplies the demand for quantity-oriented metrics, mainly for benchmark purposes. This has given rise to an issue regarding how to survey data portals globally, especially reducing the manual efforts, while covering a wide variety of sources that may not implement standardized solutions. Thus, this study raises two main problems: searching for standardized open data software platforms and identifying specific developed web-based software operated as data portals. This study aims to develop a method that deeply searches each web page on the internet and formalizes a machine learning classification model to improve the identification of data portals, irrespective of how these data portals implement a standardized open data software platform and comply with the open data technical guidelines. The contributions of this work have been demonstrated through a list of 1,650 open data portals generalized in a training model that makes it feasible to distinguish between a data portal (that may or may not implement a standardized platform) and an ordinary web page. The results provide new insights on how machine-readable, publicly available data are affected by artificial intelligence, with special focus on how it can be used to understand data openness worldwide.}
}
@article{ARAUJO2021108474,
title = {A hybrid optimization-Machine Learning approach for the VNF placement and chaining problem},
journal = {Computer Networks},
volume = {199},
pages = {108474},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108474},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004229},
author = {Samuel M.A. Araújo and Fernanda S.H. {de Souza} and Geraldo R. Mateus},
keywords = {VNF, NFV, Optimization, Machine Learning, Placement, Chaining, Online},
abstract = {Virtual Network Function Placement and Chaining Problem focuses on allocation of customers demand’s on the Substrate Network. Among other factors, an optimal allocation of resources is hampered by nature of the online problem and by its complexity. Since we are dealing with an NP-hard combinatorial problem, while the number of network components grows linearly, the computational processing and runtime increase exponentially. Therefore, an application of Machine Learning techniques to reduce the number of components present in the SN is proposed in this work. In particular, we have developed clustering techniques that aim to find groups of promising components to map customer demands and disregard those that are less promising. Two different clustering models are proposed: (i) based on the Spatial Location of the SN components; and (ii) based on SN’s historical resource consumption data. An Integer Linear Programming model is proposed to evaluate the different simulation scenarios. Both approaches reduced the execution time (≈75%) and the end-to-end delay of virtual requests, in addition to keeping the acceptance rate and profit stable compared to the exact approach.}
}
@article{POSPIESZNY2018184,
title = {An effective approach for software project effort and duration estimation with machine learning algorithms},
journal = {Journal of Systems and Software},
volume = {137},
pages = {184-196},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302947},
author = {Przemyslaw Pospieszny and Beata Czarnacka-Chrobot and Andrzej Kobylinski},
keywords = {Software project estimation, Machine learning, Effort and duration estimation, Ensemble models, ISBSG},
abstract = {During the last two decades, there has been substantial research performed in the field of software estimation using machine learning algorithms that aimed to tackle deficiencies of traditional and parametric estimation techniques, increase project success rates and align with modern development and project management approaches. Nevertheless, mostly due to inconclusive results and vague model building approaches, there are few or none deployments in practice. The purpose of this article is to narrow the gap between up-to-date research results and implementations within organisations by proposing effective and practical machine learning deployment and maintenance approaches by utilization of research findings and industry best practices. This was achieved by applying ISBSG dataset, smart data preparation, an ensemble averaging of three machine learning algorithms (Support Vector Machines, Neural Networks and Generalized Linear Models) and cross validation. The obtained models for effort and duration estimation are intended to provide a decision support tool for organisations that develop or implement software systems.}
}
@article{ZHAO201930,
title = {Where ThermoMesh meets ThermoNet: A machine learning based sensor for heat source localization and peak temperature estimation},
journal = {Sensors and Actuators A: Physical},
volume = {292},
pages = {30-38},
year = {2019},
issn = {0924-4247},
doi = {https://doi.org/10.1016/j.sna.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924424718314110},
author = {Jingzhou Zhao and Feng Ye},
keywords = {Temperature sensor, Machine learning, Underdetermined linear system, Inverse problem, Analog-to-information conversion, Localization},
abstract = {In this paper, we propose a new thermal sensor (ThermoMesh) for experimental heat source localization and peak temperature estimation (HSL&PTE) enabled by Machine Learning algorithms (ThermoNet). The mathematical model of the ThermoMesh sensor is first derived and experimentally validated. Its use for HSL&PTE of a single heat source is then numerically demonstrated with location accuracy of 99% and RMS temperature error of 1.2%. Complementing existing thermal imaging techniques based on radiative heat transfer, the ThermoMesh plus ThermoNet framework sheds new light on high-speed high-resolution heat source sensing via conductive heat transfer.}
}
@article{ZHOU2021102912,
title = {Urban flow prediction with spatial–temporal neural ODEs},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {124},
pages = {102912},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102912},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X2030810X},
author = {Fan Zhou and Liang Li and Kunpeng Zhang and Goce Trajcevski},
keywords = {Urban flow, Ordinary differential equations, Spatial–temporal learning, Time series prediction, Intelligent transportation system},
abstract = {With the recent advances in deep learning, data-driven methods have shown compelling performance in various application domains enabling the Smart Cities paradigm. Leveraging spatial–temporal data from multiple sources for (citywide) traffic forecasting is a key to strengthen the smart city management in areas such as urban traffic control, abnormal event detection, etc. Existing approaches of traffic flow prediction mainly rely on the development of various deep neural networks –e.g., Convolutional Neural Networks such as ResNet are used for modeling spatial dependencies among different regions, whereas recurrent neural networks are increasingly implemented for temporal dynamics modeling. Despite their advantages, the existing approaches suffer from limitations of intensive computations, lack of capabilities to properly deal with missing values, and simplistic integration of heterogeneous data. In this paper, we propose a novel urban flow prediction framework by generalizing the hidden states of the model with continuous-time dynamics of the latent states using neural ordinary differential equations (ODE). Specifically, we introduce a discretize-then-optimize approach to improve and balance the prediction accuracy and computational efficiency. It not only guarantees the prediction error but also provides high flexibility for decision-makers. Furthermore, we investigate the factors, both intrinsic and extrinsic, that affect the city traffic volume and use separate neural networks to extract and disentangle the influencing factors, which avoids the brute-force data fusion in previous works. Extensive experiments conducted on the real-world large-scale datasets demonstrate that our method outperforms the state-of-the-art baselines, while requiring significantly less memory cost and fewer model parameters.}
}
@article{HAZARIKA2022795,
title = {Area and energy efficient shift and accumulator unit for object detection in IoT applications},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {1},
pages = {795-809},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.04.099},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821003410},
author = {Anakhi Hazarika and Soumyajit Poddar and Moustafa M. Nasralla and Hafizur Rahaman},
keywords = {Convolution operation, Object detection, MAC unit, Approximate computing, Embedded platform},
abstract = {Convolutional Neural Networks (CNNs) exhibit significant performance enhancements in several machine learning tasks such as surveillance, intelligent transportation, smart grids and healthcare systems. With the proliferation of physical things being connected to internet and enabled with sensory capabilities to form an Internet of Thing (IoT) network, it is increasingly important to run CNN inference, a computationally intensive application, on the resource constrained IoT devices. Object detection is a fundamental computer vision problem that provides information for image understanding in several artificial intelligence (AI) applications in smart cities. Among various object detection algorithms, CNN has emerged as a new paradigm to improve the overall performance. The Multiply-accumulate (MAC) operations, which are used repeatedly in the convolution layers of CNN, hold extreme computational complexity. Hence, the overall computational workloads and their respective energy consumption of any CNN applications are on the rise. To overcome these escalating challenges, approximate computing mechanism has played a vital role in reducing power and area of computation intensive CNN applications. In this paper, we have designed an approximate MAC architecture, termed Shift and Accumulator Unit (SAC), for the error-resilient CNN based object detection algorithm targeting embedded platforms. The proposed computing unit deliberately trades accuracy to reduce design complexity and power consumption, thus suiting the resource constrained IoT devices. The pipeline architecture of the SAC unit saves approximately 1.8× clock cycles than the non-pipeline SAC architecture. The performance evaluation shows that the proposed computing unit has better energy efficiency and resource utilization than the accurate multiplier and state-of-the-art approximate multipliers without noticeable deterioration in overall performance.}
}
@article{ZHUANG2018225,
title = {Multi-label learning based deep transfer neural network for facial attribute classification},
journal = {Pattern Recognition},
volume = {80},
pages = {225-240},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2018.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0031320318301080},
author = {Ni Zhuang and Yan Yan and Si Chen and Hanzi Wang and Chunhua Shen},
keywords = {Transfer learning, Facial attribute classification, Multi-label learning, Deep learning, Convolutional neural networks},
abstract = {Deep Neural Network (DNN) has recently achieved outstanding performance in a variety of computer vision tasks, including facial attribute classification. The great success of classifying facial attributes with DNN often relies on a massive amount of labelled data. However, in real-world applications, labelled data are only provided for some commonly used attributes (such as age, gender); whereas, unlabelled data are available for other attributes (such as attraction, hairline). To address the above problem, we propose a novel deep transfer neural network method based on multi-label learning for facial attribute classification, termed FMTNet, which consists of three sub-networks: the Face detection Network (FNet), the Multi-label learning Network (MNet) and the Transfer learning Network (TNet). Firstly, based on the Faster Region-based Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with labelled data, where an effective loss weight scheme is developed to explicitly exploit the correlation between facial attributes based on attribute grouping. Finally, based on MNet, TNet is trained by taking advantage of unsupervised domain adaptation for unlabelled facial attribute classification. The three sub-networks are tightly coupled to perform effective facial attribute classification. A distinguishing characteristic of the proposed FMTNet method is that the three sub-networks (FNet, MNet and TNet) are constructed in a similar network structure. Extensive experimental results on challenging face datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art methods.}
}
@article{CHENG2022103624,
title = {Impact of internet of things paradigm towards energy consumption prediction: A systematic literature review},
journal = {Sustainable Cities and Society},
volume = {78},
pages = {103624},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103624},
url = {https://www.sciencedirect.com/science/article/pii/S221067072100888X},
author = {Yew Leong Cheng and Meng Hee Lim and Kar Hoou Hui},
keywords = {Internet of things (IoT), Energy prediction, Smart grids, Smart cities, Smart industries, Load forecasting},
abstract = {The contribution of buildings to energy consumption (both residential and commercial) is expected to gradually increase by 2040 in developed countries globally. Energy demand is rising around the world because of population growth and increased access to power through rapid urbanisation. These increases significantly impact the environment due to the processing of electricity from fossil fuels in heavy-duty power generation plants to cater for demand. A large amount of research has been undertaken in the recent past to mitigate energy usage via the internet of things and energy consumption prediction (IoT-ECP). However, systematic reviews of IoT-ECP applications remain scarce. Therefore, this study aims to systematically review the existing literature to identify the latest trends and their technological advances, including the integration concept and solutions to problems encountered in IoT-ECP. It also highlights the advantages between cloud and edge computing in IoT-ECP integration for real-time data streaming. Additionally, IoT-ECP smart integration promotes close interaction and monitoring of energy usage. Battery storage is a major challenge to tackle energy losses along network bandwidth and live streaming data traffic. Finally, the studies indicates that many existing research in IoT-ECP are using short-term load predictions. These are domains where future research could further expand to cover medium- to long-term time frames, forecasting to better balance demand fluctuations, and provide operational reserves with renewable energies.}
}
@article{ANTONOV202142,
title = {Façade deterioration prediction with the use of machine learning methods, based on objective parameters and e-participation data},
journal = {Procedia Computer Science},
volume = {193},
pages = {42-51},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020469},
author = {Aleksandr Antonov and Ivan Khodnenko and Sergei Kudinov},
keywords = {machine learning, logistic regression, predictive data, e-participation data, quality of urban environment, infrastructure deterioration},
abstract = {Condition monitoring and timely repair of residential buildings is an important task when ensuring a comfortable life in cities. In the case of large metropolitan areas, it is a difficult task to perform continuous objective condition monitoring for tens of thousands of residential buildings by efforts of experts. However, residential infrastructure health can be predicted on the basis of indirect data. These can be objective building parameters or subjective data on citizens’ complaints about deterioration. In cities today, it is possible to collect such data in machine-readable form from various information systems. This article proposes a method to predict external deterioration of buildings on the basis of indirect data, using machine learning and SMILE Low-coding platform. Based on the results of method approbation, which used data of a metropolis, the significance of electronic participation data and objective parameters of objects for façade deterioration forecast was assessed. Options for further research are proposed to improve the quality of deterioration predicting by using data on citizens’ complaints about infrastructure damage.}
}
@article{MASON2018162,
title = {Predicting host CPU utilization in the cloud using evolutionary neural networks},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {162-173},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17322793},
author = {Karl Mason and Martin Duggan and Enda Barrett and Jim Duggan and Enda Howley},
keywords = {Cloud computing, CPU prediction, Neural networks, Optimization, Differential Evolution, Particle Swarm Optimization, Covariance Matrix Adaptation Evolutionary Strategy, Time series, Neuroevolution},
abstract = {The Infrastructure as a Service (IaaS) platform in cloud computing provides resources as a service from a pool of compute, network, and storage resources. One of the major challenges facing cloud computing is to predict the usage of these resources in real time. By knowing future demands, cloud data centres can dynamically scale resources to decrease energy consumption while maintaining a high quality of service. However cloud resource consumption is ever changing, making it difficult for accurate predictions to be produced. This motivates the research presented in this paper which aims to predict in advance the level of CPU consumption of a host. This research implements evolutionary Neural Networks (NN), a powerful machine learning method, to make these predictions. A number of state of the art swarm and evolutionary optimization algorithms are implemented to train the neural networks to predict host utilization: Particle Swarm Optimization (PSO), Differential Evolution (DE) and Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES). The results of this research demonstrate that CMA-ES converges faster to a better solution on the training data. However when evaluated on the test data, DE performs statistically equal to CMA-ES. The results also demonstrate that the trained networks are still accurate when applied to CPU utilization data from different hosts with no further training needed. When evaluated to predict multiple steps into the future, the accuracy of the network understandably decreases but still performs well on average.}
}
@article{WEN2019178,
title = {A deep learning framework for road marking extraction, classification and completion from mobile laser scanning point clouds},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {147},
pages = {178-192},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618302855},
author = {Chenglu Wen and Xiaotian Sun and Jonathan Li and Cheng Wang and Yan Guo and Ayman Habib},
keywords = {Point cloud, Road marking, Extraction, Classification, Completion, Deep learning},
abstract = {Road markings play a critical role in road traffic safety and are one of the most important elements for guiding autonomous vehicles (AVs). High-Definition (HD) maps with accurate road marking information are very useful for many applications ranging from road maintenance, improving navigation, and prediction of upcoming road situations within AVs. This paper presents a deep learning-based framework for road marking extraction, classification and completion from three-dimensional (3D) mobile laser scanning (MLS) point clouds. Compared with existing road marking extraction methods, which are mostly based on intensity thresholds, our method is less sensitive to data quality. We added the step of road marking completion to further optimize the results. At the extraction stage, a modified U-net model was used to segment road marking pixels to overcome the intensity variation, low contrast and other issues. At the classification stage, a hierarchical classification method by integrating multi-scale clustering with Convolutional Neural Networks (CNN) was developed to classify different types of road markings with considerable differences. At the completion stage, a method based on a Generative Adversarial Network (GAN) was developed to complete small-size road markings first, then followed by completing broken lane lines and adding missing markings using a context-based method. In addition, we built a point cloud road marking dataset to train the deep network model and evaluate our method. The dataset contains urban road and highway MLS data and underground parking lot data acquired by our own assembled backpacked laser scanning system. Our experimental results obtained using the point clouds of different scenes demonstrated that our method is very promising for road marking extraction, classification and completion.}
}
@article{ZHANG201772,
title = {Social behavior study under pervasive social networking based on decentralized deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {86},
pages = {72-81},
year = {2017},
note = {Special Issue on Pervasive Social Networking},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516302867},
author = {Yue Zhang and Bin Song and Peng Zhang},
keywords = {Decentralized deep reinforcement learning, Market models, Monopolistically competitive market, Pervasive social networking, Social behavior, Users' patterns},
abstract = {Pervasive social networking (PSN) provides instant social activities such as communications and gaming, which attracts a growing attention. Under this circumstance, the study and analysis of users’ behaviors has a profound meaning, which may be extended to other relevant fields. In this article, we define and quantify users’ patterns to study social behaviors, after discussing and reconsidering social characteristics in PSN. Meanwhile, we treat PSN as a market, based on the standpoint that data can be priced and tradable. After analyzing its market structure, we describe PSN as a monopolistically competitive market, which contains multiple providers selling specialized goods. Afterwards, we study the social behaviors in this market from an economic perspective, namely applying market models. Finally, decentralized deep reinforcement learning is proposed to estimate users’ patterns and to solve market models, the prisoner's dilemma and the Cournot model to be specific. Simulation results demonstrate the flexibility and efficiency of our algorithms.}
}
@article{GHONEIM2020643,
title = {Cervical cancer classification using convolutional neural networks and extreme learning machines},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {643-649},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19306065},
author = {Ahmed Ghoneim and Ghulam Muhammad and M. Shamim Hossain},
abstract = {Cervical cancer is one of the main reasons of death from cancer in women. The complication of this cancer can be limited if it is diagnosed and treated at an early stage. In this paper, we propose a cervical cancer cell detection and classification system based on convolutional neural networks (CNNs). The cell images are fed into a CNNs model to extract deep-learned features. Then, an extreme learning machine (ELM)-based classifier classifies the input images. CNNs model is used via transfer learning and fine tuning. Alternatives to the ELM, multi-layer perceptron (MLP) and autoencoder (AE)-based classifiers are also investigated. Experiments are performed using the Herlev database. The proposed CNN-ELM-based system achieved 99.5% accuracy in the detection problem (2-class) and 91.2% in the classification problem (7-class).}
}
@article{CHEN2021340,
title = {Stability analysis of delayed neural networks based on a relaxed delay-product-type Lyapunov functional},
journal = {Neurocomputing},
volume = {439},
pages = {340-347},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221001806},
author = {Yun Chen and Gang Chen},
keywords = {Delayed neural networks, Stability, Polynomial inequalities, Lyapunov–Krasovskii functional, Time-varying delay},
abstract = {This paper deals with the stability analysis of delayed neural networks. A necessary and sufficient condition for positivity or negativity of the high-order polynomial over a finite interval is derived. An appropriate Lyapunov–Krasovskii functional (LKF), including a relax delay-product-type Lyapunov functional, is constructed. The necessary and sufficient condition of the polynomial inequality and a relaxed delay-product-type Lyapunov functional are employed to derive less conservative stability criteria. Finally, three commonly used numerical examples are presented to demonstrate the effectiveness and less conservativeness of the proposed method.}
}
@article{KAUR2022100035,
title = {Face mask recognition system using CNN model},
journal = {Neuroscience Informatics},
volume = {2},
number = {3},
pages = {100035},
year = {2022},
note = {Multimedia-based Emerging Technologies and Data Analytics for Neuroscience as a Service (NaaS)},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2021.100035},
url = {https://www.sciencedirect.com/science/article/pii/S2772528621000352},
author = {Gagandeep Kaur and Ritesh Sinha and Puneet Kumar Tiwari and Srijan Kumar Yadav and Prabhash Pandey and Rohit Raj and Anshu Vashisth and Manik Rakhra},
keywords = {Artificial Intelligence (AL), Machine learning (ML), Deep neural learning (DL), Convolutional Neural Network Model (CNN), Artificial Neural Networks (ANN), Security},
abstract = {COVID-19 epidemic has swiftly disrupted our day-to-day lives affecting the international trade and movements. Wearing a face mask to protect one's face has become the new normal. In the near future, many public service providers will expect the clients to wear masks appropriately to partake of their services. Therefore, face mask detection has become a critical duty to aid worldwide civilization. This paper provides a simple way to achieve this objective utilising some fundamental Machine Learning tools as TensorFlow, Keras, OpenCV and Scikit-Learn. The suggested technique successfully recognises the face in the image or video and then determines whether or not it has a mask on it. As a surveillance job performer, it can also recognise a face together with a mask in motion as well as in a video. The technique attains excellent accuracy. We investigate optimal parameter values for the Convolutional Neural Network model (CNN) in order to identify the existence of masks accurately without generating over-fitting.}
}