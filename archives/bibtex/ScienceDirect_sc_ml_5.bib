@article{HAN2021109689,
title = {Reinforcement learning control of constrained dynamic systems with uniformly ultimate boundedness stability guarantee},
journal = {Automatica},
volume = {129},
pages = {109689},
year = {2021},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2021.109689},
url = {https://www.sciencedirect.com/science/article/pii/S0005109821002090},
author = {Minghao Han and Yuan Tian and Lixian Zhang and Jun Wang and Wei Pan},
keywords = {Data-based control, Reinforcement learning, Constrained dynamic system, Uniformly ultimate boundedness stability, Lyapunov’s method},
abstract = {Reinforcement learning (RL) is promising for complicated stochastic nonlinear control problems. Without using a mathematical model, an optimal controller can be learned from data evaluated by certain performance criteria through trial-and-error. However, the data-based learning approach is notorious for not guaranteeing stability, which is the most fundamental property for any control system. In this paper, the classic Lyapunov’s method is explored to analyze the uniformly ultimate boundedness stability (UUB) solely based on data without using a mathematical model. It is further shown how RL with UUB guarantee can be applied to control dynamic systems with safety constraints. Based on the theoretical results, both off-policy and on-policy learning algorithms are proposed respectively. As a result, optimal controllers can be learned to guarantee UUB of the closed-loop system both at convergence and during learning. The proposed algorithms are evaluated on a series of robotic continuous control tasks with safety constraints. In comparison with the existing RL algorithms, the proposed method can achieve superior performance in terms of maintaining safety. As a qualitative evaluation of stability, our method shows impressive resilience even in the presence of external disturbances.}
}
@article{ORHEAN2018292,
title = {New scheduling approach using reinforcement learning for heterogeneous distributed systems},
journal = {Journal of Parallel and Distributed Computing},
volume = {117},
pages = {292-302},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301521},
author = {Alexandru Iulian Orhean and Florin Pop and Ioan Raicu},
keywords = {Scheduling, Distributed systems, Machine learning, SARSA},
abstract = {Computer clusters, cloud computing and the exploitation of parallel architectures and algorithms have become the norm when dealing with scientific applications that work with large quantities of data and perform complex and time-consuming calculations. With the rise of social media applications and smart devices, the amount of digital data and the velocity at which it is produced have increased exponentially, determining the development of distributed system frameworks and platforms that increase productivity, consistency, fault-tolerance and security of parallel applications. The performance of such systems is mainly influenced by the architectural disposition and composition of the physical machines, the resource allocation and the scheduling of jobs and tasks. This paper proposes a reinforcement learning algorithm to solve the scheduling problem in distributed systems. The machine learning technique takes into consideration the heterogeneity of the nodes and their disposition within the grid, and the arrangement of tasks in a directed acyclic graph of dependencies, ultimately determining a scheduling policy for a better execution time. This paper also proposes a platform, in which the algorithm is implemented, that offers scheduling as a service to distributed systems.}
}
@article{MESSAOUD2020100314,
title = {A survey on machine learning in Internet of Things: Algorithms, strategies, and applications},
journal = {Internet of Things},
volume = {12},
pages = {100314},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100314},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520301451},
author = {Seifeddine Messaoud and Abbas Bradai and Syed Hashim Raza Bukhari and Pham Tran Anh Quang and Olfa Ben Ahmed and Mohamed Atri},
keywords = {Wireless sensor network, Internet of Things, Machine learning categories, Machine learning algorithms},
abstract = {In the IoT and WSN era, large number of connected objects and sensing devices are dedicated to collect, transfer, and generate a huge amount of data for a wide variety of fields and applications. To effectively run these complex networks of connected objects, there are several challenges like topology changes, link failures, memory constraints, interoperability, network congestion, coverage, scalability, network management, security, and privacy to name a few. Thus, to overcome these challenges and exploiting them to support this technological outbreak would be one of the most crucial tasks of modern world. In the recent years, the development of Artificial Intelligence (AI) led to the emergence of Machine Learning (ML) which has become the key enabler to figure out solutions and learning models in an attempt to enhance the QoS parameters of IoT and WSNs. By learning from past experiences, ML techniques aim to resolve issues in the WSN and IoT's fields by building algorithmic models. In this paper, we are going to highlight the most fundamental concepts of ML categories and Algorithms. We start by providing a thorough overview of the WSN and IoT's technologies. We also discuss the vital role of ML techniques in driving up the evolution of these technologies. Then, as the key contribution of this paper, a new taxonomy of ML algorithms is provided. We also summarize the major applications and research challenges that leveraged ML techniques in the WSN and IoT. Eventually, we analyze the critical issues and list some future research directions.}
}
@article{ZHANG201953,
title = {Deep learning based underwater acoustic OFDM communications},
journal = {Applied Acoustics},
volume = {154},
pages = {53-58},
year = {2019},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2019.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X18307400},
author = {Youwen Zhang and Junxuan Li and Yuriy Zakharov and Xiang Li and Jianghui Li},
keywords = {Acoustic propagation model, Channel estimation and equalization, DNN, OFDM, Underwater acoustic communication},
abstract = {In this paper, we present a deep learning based underwater acoustic (UWA) orthogonal frequency-division multiplexing (OFDM) communication system. Unlike the traditional receiver for UWA OFDM communication system that performs explicitly channel estimation and equalization for the detection of transmitted symbols, the deep learning based UWA OFDM communication receiver interpreted as a deep neural network (DNN) can recover the transmitted symbols directly after sufficient training. The estimation of transmitted symbols in the DNN based receiver is achieved in two stages: (1) training stage, when labeled data such as known transmitted data and signal received in the unknown channel are used to train the DNN, and (2) test stage, where the DNN receiver recovers transmitted symbols given the received signal. To demonstrate the performance of the deep learning based UWA OFDM communications, we generate a large number of labeled and unlabeled data by using an acoustic propagation model with a measured sound speed profile to train and test the DNN receiver. The performance of the deep learning based UWA OFDM communications is evaluated under various system parameters, such as the cyclic prefix length, number of pilot symbols, and others. Simulation results demonstrate that the deep leaning based receiver offers consistent improvement in performance compared to the traditional UWA OFDM receiver.}
}
@article{XU202014960,
title = {Vehicle emission control on road with temporal traffic information using deep reinforcement learning⁎⁎This work was supported in part by the National Key R&D Program of China under Grant (2018AAA0100800, 2018YFE0106800, 2018YFC0213104), National Natural Science Foundation of China (61725304, 61673361), Major Special Science and Technology Project of Anhui, China (912198698036), as well as the Fundamental Research Funds for the Central Universities under Grant WK2380000001.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {14960-14965},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1988},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320326197},
author = {Zhenyi Xu and Yang Cao and Yu Kang and Zhenyi Zhao},
keywords = {Urban air pollution, traffic emission control, deep reinforcement learning},
abstract = {The increased vehicle usage significantly aggravate the urban air pollution, which have great impact on the public health. Therefore, it is necessary to make proper traffic control policies and reduce traffic emissions. However, it is difficult to establish control strategies based on modeling methods, and carry out online control based on historical traffic information for the complex time-varying characteristics of emissions. In this paper, we present a deep reinforcement learning emission control strategy, which automatically learns the optimal traffic flow and speed limits to reduce traffic emission on the target road segment based on the temporal traffic information. The proposed approach is evaluated on real world vehicle emission data in Hefei. And the results demonstrate the effectiveness of the proposed approach against baseline methods.}
}
@article{VEMIREDDY2021108463,
title = {Fuzzy Reinforcement Learning for energy efficient task offloading in Vehicular Fog Computing},
journal = {Computer Networks},
volume = {199},
pages = {108463},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108463},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004163},
author = {Satish Vemireddy and Rashmi Ranjan Rout},
keywords = {Vehicular Fog Computing, Fuzzy logic, Reinforcement learning, Scheduling, Energy, Optimization},
abstract = {Vehicular Fog Computing (VFC) has been envisioned as a potential fog computing paradigm which aims to offload delay sensitive tasks to mobile fog vehicles instead of remote cloud in order to facilitate computational demands of smart villages close to rural highways. There exists challenges related to task offloading in VFC that need to be addressed. Most often, Road Side Units (RSUs) deployed along rural highways are energy constrained and they need to provide energy efficient scheduling services for the allocation of tasks to fog vehicles. On the other hand, energy consumption optimization is challenging, since scheduling decision of local processing of tasks incur computation cost while the allocation of tasks to fog vehicles incurs communication cost. Although the task offloading to VFC reduces response latency, it leads to higher RSU energy consumption contributed by the communication of task data to fog vehicles. Therefore, this paper presents an energy efficient vehicle scheduling problem for offloading of tasks to mobile fog nodes subject to satisfy constraints of task deadline and resource availability. To resolve high dimensionality issue caused by increased number of vehicles in RSU coverage, we propose an on-policy reinforcement leaning based scheduling algorithm combined with fuzzy logic based greedy heuristic, named as Fuzzy Reinforcement Learning (FRL). This greedy heuristic not only accelerates learning process, but also improves long term reward when compared to Q-learning algorithm. Extensive experiments have been performed to evaluate the proposed algorithm and the simulation results show that the proposed FRL algorithm outperforms other scheduling algorithms such as First Come First Serve (FCFS), Rate Monotonic Scheduling (RMS), Fuzzy and Distributed Task Allocation with Distributed Process (DTA_DP).}
}
@article{BEDI2021103814,
title = {Detection of attacks in IoT sensors networks using machine learning algorithm},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103814},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103814},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120309595},
author = {Pradeep Bedi and Shivlal Mewada and Rasmbabu Arjunarao Vatti and Chaitanya Singh and Kanwalvir Singh Dhindsa and Muruganantham Ponnusamy and Ranjana Sikarwar},
keywords = {Machine learning, Random forest, Internet of things, Support vector machine, Artificial intelligence},
abstract = {Assault and peculiar location on the Internet of Things (IoT) framework is an increasing worry in the IoT region. By the expanded IoT foundation utilization in every area, assaults, and dangers in these frameworks are likewise developing proportionately. Malicious control, Spying, Forswearing of Service, Scan, Data Type Probing, Wrong setup, and malicious operation are such assaults and irregularities that may source an IOT framework disappointment. This project proposes a few Machine learning (ML) module that is contrasted with foresee assault and abnormalities on the IoT frameworks precisely. The ML algorithms that have been utilized here are Artificial Neural Network (ANN), Logistic Regression (LR), Random Forest (RF), Support Vector Machine (SVM), Decision Tree (DT). The assessment measurements utilized in the examination of presentation are f1 score, exactness, area, recollect, and precision under the ROC Curve. Even though these strategies have similar accuracy, different measurements demonstrate that RF executes relatively preferable.}
}
@article{DONTA2021,
title = {Survey on recent advances in IoT application layer protocols and machine learning scope for research directions},
journal = {Digital Communications and Networks},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000845},
author = {Praveen Kumar Donta and Satish Narayana Srirama and Tarachand Amgoth and Chandra Sekhara {Rao Annavarapu}},
keywords = {Internet of things, Machine learning, Application layer, Request-response protocols, Publish-subscribe protocols},
abstract = {The Internet of Things (IoT) has been growing over the past few years due to its flexibility and ease of use in real-time applications. The IoT’s foremost task is ensuring that there is proper communication between different types of applications and devices, and that the application layer protocols fulfill this necessity. However, as the number of applications grows, it is necessary to modify or enhance the application layer protocols according to specific IoT applications, allowing specific issues to be addressed, such as dynamic adaption to network conditions and interoperability. Recently, several IoT application layer protocols have been enhanced and modified according to application requirements. However, no existing survey articles have focused on these protocols. In this article, we survey traditional and recent advances in IoT application layer protocols, as well as relevant real-time applications and their adapted application layer protocols for improving performance. As changing the nature of protocols for each application is unrealistic, machine learning offers means of making protocols intelligent and able to adapt dynamically. In this context, we focus on providing open challenges to drive IoT application layer protocols in such a direction.}
}
@article{AAZAM2021108019,
title = {Task offloading in edge computing for machine learning-based smart healthcare},
journal = {Computer Networks},
volume = {191},
pages = {108019},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108019},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001298},
author = {Mohammad Aazam and Sherali Zeadally and Eduardo Feo Flushing},
keywords = {Cloud computing, Edge computing, Fog computing, Healthcare, Internet of Things (IoT), Middleware, Machine learning, Offloading},
abstract = {Recent advances in networking and mobile technologies such as 5G, long-term evolution (LTE), LiFi, wireless broadband (WiBro), WiFi-Direct, Bluetooth Low Energy (BLE) have paved the way for intelligent and smart services. With an average of more than 6.5 devices per person, a plethora of applications are being developed especially related to healthcare. Although, current edge devices such as smartphone and smartwatch are becoming increasingly more powerful and more affordable, there are certain tasks such as those involving machine learning that require higher computational resources, thereby resulting in higher energy consumption in the case of edge devices. Offloading tasks to co-located edge nodes such as fog (a cloud-like localized, smaller resource pool), or a femto-cloud (integration of multiple edge nodes) is one viable solution to address the issues such as performing compute-intensive tasks, and managing energy consumption. The outbreak of coronavirus disease 2019 (COVID-19) and becoming a pandemic has also made a case for edge computing (involving smartphone, wearables, health sensors) for the detection of symptoms to quarantine potential carriers of the virus. We focus on how various forms of smart and opportunistic healthcare (oHealth) can be provided by leveraging edge computing that makes use of a machine learning-based approach. We apply k-nearest neighbors (kNN), naive Bayes (NB), and support vector classification (SVC) algorithms on real data trace for the healthcare and safety-related scenarios we considered. The empirical results obtained provide useful insights into machine learning-based task offloading in edge computing.}
}
@article{AMIN2019542,
title = {Deep Learning for EEG motor imagery classification based on multi-layer CNNs feature fusion},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {542-554},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19306077},
author = {Syed Umar Amin and Mansour Alsulaiman and Ghulam Muhammad and Mohamed Amine Mekhtiche and M. {Shamim Hossain}},
keywords = {EEG motor imagery classification, Deep learning, Convolution neural network, Multi-layer CNNs feature fusion},
abstract = {Electroencephalography (EEG) motor imagery (MI) signals have recently gained a lot of attention as these signals encode a person’s intent of performing an action. Researchers have used MI signals to help disabled persons, control devices such as wheelchairs and even for autonomous driving. Hence decoding these signals accurately is important for a Brain–Computer interface (BCI) system. But EEG decoding is a challenging task because of its complexity, dynamic nature and low signal to noise ratio. Convolution neural network (CNN) has shown that it can extract spatial and temporal features from EEG, but in order to learn the dynamic correlations present in MI signals, we need improved CNN models. CNN can extract good features with both shallow and deep models pointing to the fact that, at different levels relevant features can be extracted. Fusion of multiple CNN models has not been experimented for EEG data. In this work, we propose a multi-layer CNNs method for fusing CNNs with different characteristics and architectures to improve EEG MI classification accuracy. Our method utilizes different convolutional features to capture spatial and temporal features from raw EEG data. We demonstrate that our novel MCNN and CCNN fusion methods outperforms all the state-of-the-art machine learning and deep learning techniques for EEG classification. We have performed various experiments to evaluate the performance of the proposed CNN fusion method on public datasets. The proposed MCNN method achieves 75.7% and 95.4% on the BCI Competition IV-2a dataset and the High Gamma Dataset respectively. The proposed CCNN method based on autoencoder cross-encoding achieves more than 10% improvement for cross-subject EEG classification.}
}
@article{LI2020102258,
title = {A deep learning method based on an attention mechanism for wireless network traffic prediction},
journal = {Ad Hoc Networks},
volume = {107},
pages = {102258},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102258},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519310923},
author = {Ming Li and Yuewen Wang and Zhaowen Wang and Huiying Zheng},
keywords = {Residual network, Wireless network traffic prediction, Recurrent neural network, Attention},
abstract = {With the rapid development of wireless networks, the self-management and active adjustment capabilities of base stations have become crucial. The accurate prediction of wireless network traffic is an important prerequisite for intelligent base stations. Traffic data has a high degree of nonlinearity and complexity, which is characterized by temporal and spatial correlation. Most of the existing forecasting methods do not consider both the temporal and spatial situations in the process of modeling traffic data. In this paper, a spatio-temporal convolutional network (LA-ResNet) is presented that uses an attention mechanism to solve spatio-temporal modeling and predict wireless network traffic. LA-ResNet consists of three parts: the residual network, the recurrent neural network, and an attention mechanism. Using this method, the temporal and spatial characteristics of wireless network traffic data are modeled and its related features are strengthened. Thus, the spatio-temporal correlation of wireless network traffic data can be effectively captured. The residual network can capture spatial features in the data. The combination of the recurrent neural network and the attention mechanism can capture the temporal dependence of the data. Finally, experiments on a real data set show that the prediction effect of the LA-ResNet model is better than the other existing prediction methods, such as RNN and 3DCNN, and the accurate prediction of traffic can be realized.}
}
@article{YONG2022108335,
title = {Analysis and prediction of diaphragm wall deflection induced by deep braced excavations using finite element method and artificial neural network optimized by metaheuristic algorithms},
journal = {Reliability Engineering & System Safety},
volume = {221},
pages = {108335},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108335},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022000163},
author = {Weixun Yong and Wengang Zhang and Hoang Nguyen and Xuan-Nam Bui and Yosoon Choi and Trung Nguyen-Thoi and Jian Zhou and Trung Tin Tran},
keywords = {System safety, Diaphragm wall, Soft clay, Braced excavation, Artificial neural network, Metaheuristic algorithms},
abstract = {The construction of metropolises in smart cities is the trend of developed countries. However, it may cause damages to the surrounding structures. For this reason, the diaphragm wall has been applied to prevent the deformation or collapse of the surrounding structures. Diaphragm walls can be deflected due to the swelling pressure or other geotechnical properties during construction. Therefore, the accurate prediction of diaphragm wall deflection (DWD) is challenging in construction aiming to ensure the safety of the surrounding structures. This study is, therefore, to propose two intelligent models for predicting DWD induced by deep braced excavations based on finite element method (FEM) and metaheuristic algorithms. Accordingly, a total of 1120 finite elements were analyzed to investigate the behaviors of DWD. Finally, a neural network with multiple layer perceptron (MLP) was optimized by two metaheuristic algorithms for predicting DWD, including whale optimization (WO) and Harris hawks optimization (HHO), called MLP-HHO and MLP-WO, respectively. The results indicated that the proposed MLP-HHO and MLP-WO provided high accuracy in predicting DWD. A comparison of the proposed models in this study and previous studies was also discussed to highlight the superiority of the proposed MLP-HHO and MLP-WO models.}
}
@article{AYVAZ2021114598,
title = {Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114598},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114598},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000397},
author = {Serkan Ayvaz and Koray Alpay},
keywords = {Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data},
abstract = {In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.}
}
@article{ZHOU2020109889,
title = {Passive and active phase change materials integrated building energy systems with advanced machine-learning based climate-adaptive designs, intelligent operations, uncertainty-based analysis and optimisations: A state-of-the-art review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {130},
pages = {109889},
year = {2020},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.109889},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120301829},
author = {Yuekuan Zhou and Siqian Zheng and Zhengxuan Liu and Tao Wen and Zhixiong Ding and Jun Yan and Guoqiang Zhang},
keywords = {Phase change materials (PCMs), Combined active and passive energy systems, Exhaust heat recovery, Stochastic uncertainty-based prediction, Multivariable and multi-objective optimisations, Machine learning},
abstract = {Integrating phase change materials (PCMs) in buildings cannot only enhance the energy performance, but also improve the renewable utilization efficiency through considerable latent heat during charging/discharging cycles. However, system performances are dependent on PCMs’ integrated forms, heat transfer enhancement solutions, system operating modes, together with optimal geometrical and operating parameters. In this study, passive, active, and combined passive/active solutions in PCMs systems have been comprehensively reviewed, when being applied in heating, cooling and electrical systems, together with a dialectical analysis on advantages and disadvantages. In addition to novel system designs, interdisciplinary applications of machine learning have been reviewed and formulated, from perspectives of reliable structures, smart operational controls, and stochastic uncertainty-based performance prediction. Furthermore, a generic methodology with a systematic and hierarchical procedure has been proposed, with the implementation of machine-learning based technique for optimisations during both design and operation periods. The mechanisms of machine learning techniques were characterised as the simplifications of modelling and optimization processes, through the errors-driven update, the support vector regression and the backpropagation neural network. Several technical challenges were identified, such as the heat transfer enhancement, the novel structural configurations and the flexible switch on operating modes. Finally, identified challenges on machine learning include the development of advanced learning algorithms for efficient performance predictions, optimal structural configurations on neural networks, the trade-off between computational complexity and reliable optimal solutions, and so on. The formulated climate-adaptive designs, intelligent operations, uncertainty-based analysis and optimisations with interdisciplinary machine learning techniques can promote PCMs applications in sustainable buildings.}
}
@article{JANARTHANAN2021102720,
title = {A deep learning approach for prediction of air quality index in a metropolitan city},
journal = {Sustainable Cities and Society},
volume = {67},
pages = {102720},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102720},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721000159},
author = {R. Janarthanan and P. Partheeban and K. Somasundaram and P. {Navin Elamparithi}},
keywords = {Air quality index, Deep learning, LSTM model, National Air Monitoring Program},
abstract = {In India, the Central and State Pollution Control Boards have commissioned the National Air Monitoring Program (NAMP) which covers 240 cities with 342 monitoring stations. Air Quality Index (AQI) has been categorized into different groups. To predict the AQI in Chennai city, the Dataset was collected, then preprocessed to replace missing values and remove redundant data. The mean, mean square error and standard deviation are extracted using the Grey Level Co-occurrence Matrix (GLCM). The combination of Support Vector Regression (SVR) and Long Short-Term Memory (LSTM) based deep learning model is used to classify the AQI values. The proposed deep learning model gives an accurate and specific value for AQI on the city’s specified location compared to the existing techniques. The prediction accuracy is improved in the proposed deep learning method, which will caution the public to reduce to an acceptable level. The deep learning mechanism predicts the AQI values accurately and helps to plan the metropolitan city for sustainable development. The expected AQI value can control the pollution level by incorporating road traffic signal coordination, encouraging the people to use public transportation, and planting more trees on some locations.}
}
@article{SARABIAJACOME2020100185,
title = {Highly-efficient fog-based deep learning AAL fall detection system},
journal = {Internet of Things},
volume = {11},
pages = {100185},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100185},
url = {https://www.sciencedirect.com/science/article/pii/S2542660518300829},
author = {David Sarabia-Jácome and Regel Usach and Carlos E. Palau and Manuel Esteve},
keywords = {IoT, Big data, Fog computing, Cloud computing, Deep learning, AAL, Health, AHA, Fall},
abstract = {Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.}
}
@article{CAO2021102278,
title = {A deep reinforcement learning-based on-demand charging algorithm for wireless rechargeable sensor networks},
journal = {Ad Hoc Networks},
volume = {110},
pages = {102278},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102278},
url = {https://www.sciencedirect.com/science/article/pii/S1570870520306399},
author = {Xianbo Cao and Wenzheng Xu and Xuxun Liu and Jian Peng and Tang Liu},
keywords = {Wireless rechargeable sensor networks, Time window, Mobile charging, Deep reinforcement learning technique},
abstract = {Wireless rechargeable sensor networks are widely used in many fields. However, the limited battery capacity of sensor nodes hinders its development. With the help of wireless energy transfer technology, employing a mobile charger to charge sensor nodes wirelessly has become a promising technology for prolonging the lifetime of wireless sensor networks. Considering that the energy consumption rate varies significantly among sensors, we need a better way to model the charging demand of each sensor, such that the sensors are able to be charged multiple times in one charging tour. Therefore, time window is used to represent charging demand. In order to allow the mobile charger to respond to these charging demands in time and transfer more energy to the sensors, we introduce a new metric: charging reward. This new metric enables us to measure the quality of sensor charging. And then, we study the problem of how to schedule the mobile charger to replenish the energy supply of sensors, such that the sum of charging rewards collected by mobile charger on its charging tour is maximized. The sum of the collected charging reward is subject to the energy capacity constraint on the mobile charger and the charging time windows of all sensor nodes. We first prove that this problem is NP-hard. Due to the complexity of the problem, then deep reinforcement learning technique is exploited to obtain the moving path for mobile charger. Finally, experimental simulations are conducted to evaluate the performance of the proposed charging algorithm, and the results show that the proposed scheme is very promising.}
}
@article{MEI2020103018,
title = {Densely connected deep neural network considering connectivity of pixels for automatic crack detection},
journal = {Automation in Construction},
volume = {110},
pages = {103018},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103018},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519307502},
author = {Qipei Mei and Mustafa Gül and Md Riasat Azim},
keywords = {Crack detection, Deep learning, Transposed convolution layer, Densely connected layers, Connectivity of pixels},
abstract = {In order to develop smart cities, the demand for assessing the condition of existing infrastructure systems in an automated manner is burgeoning rapidly. Among all the early signs of potential damage in infrastructure systems, formation of cracks is a critical one because it is directly related to the structural capacity and could significantly affect the serviceability of the infrastructure. This paper proposes a novel deep learning-based method considering the connectivity of pixels for automatic pavement crack detection which has the potential to complement the current practice involving visual inspection which is costly, inefficient and time-consuming. In the proposed method, the convolutional layers are densely connected in a feed-forward fashion to reuse features from multiple layers, and transposed convolution layers are used for multiple level feature fusion. A novel loss function considering the connectivity of pixels is introduced to overcome the issues related to the output of transposed convolution layers. The proposed method is tested on two datasets, where the first one is collected from a handheld smartphone and the second one is collected from a high-speed camera mounted on the rear of a moving car. In both datasets, the proposed method shows superior performance than other available methods.}
}
@article{ATITALLAH2021573,
title = {An Enhanced Randomly Initialized Convolutional Neural Network for Columnar Cactus Recognition in Unmanned Aerial Vehicle imagery},
journal = {Procedia Computer Science},
volume = {192},
pages = {573-581},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015465},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Anis Koubaa and Nesrine Atitallah and Henda Ben Ghézala},
keywords = {Convolutional neural networks, Weight initialization, Randomization, Remote sensing images, Recognition, Columnar cactus},
abstract = {Recently, Convolutional Neural Networks (CNNs) have made a great performance for remote sensing image classification. Plant recognition using CNNs is one of the active deep learning research topics due to its added-value in different related fields, especially environmental conservation and natural areas preservation. Automatic recognition of plants in protected areas helps in the surveillance process of these zones and ensures the sustainability of their ecosystems. In this work, we propose an Enhanced Randomly Initialized Convolutional Neural Network (ERI-CNN) for the recognition of columnar cactus, which is an endemic plant that exists in the Tehuacán-Cuicatlán Valley in southeastern Mexico. We used a public dataset created by a group of researchers that consists of more than 20000 remote sensing images. The experimental results confirm the effectiveness of the proposed model compared to other models reported in the literature like InceptionV3 and the modified LeNet-5 CNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall, 97.5% as f1-score, and 0.056 loss.}
}
@article{HORTA2021109,
title = {Extracting knowledge from Deep Neural Networks through graph analysis},
journal = {Future Generation Computer Systems},
volume = {120},
pages = {109-118},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000613},
author = {Vitor A.C. Horta and Ilaria Tiddi and Suzanne Little and Alessandra Mileo},
keywords = {Explainable AI, Deep representation learning, Graph analysis},
abstract = {The popularity of deep learning has increased tremendously in recent years due to its ability to efficiently solve complex tasks in challenging areas such as computer vision and language processing. Despite this success, low-level neural activity reproduced by Deep Neural Networks (DNNs) generates extremely rich representations of the data. These representations are difficult to characterise and cannot be directly used to understand the decision process. In this paper we build upon our exploratory work where we introduced the concept of a co-activation graph and investigated the potential of graph analysis for explaining deep representations. The co-activation graph encodes statistical correlations between neurons’ activation values and therefore helps to characterise the relationship between pairs of neurons in the hidden layers and output classes. To confirm the validity of our findings, our experimental evaluation is extended to consider datasets and models with different levels of complexity. For each of the considered datasets we explore the co-activation graph and use graph analysis to detect similar classes, find central nodes and use graph visualisation to better interpret the outcomes of the analysis. Our results show that graph analysis can reveal important insights into how DNNs work and enable partial explainability of deep learning models.}
}
@article{LI2021134,
title = {Relative geometry-aware siamese neural network for 6DOF camera relocalization},
journal = {Neurocomputing},
volume = {426},
pages = {134-146},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.09.071},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220316040},
author = {Qing Li and Jiasong Zhu and Rui Cao and Ke Sun and Jonathan M. Garibaldi and Qingquan Li and Bozhi Liu and Guoping Qiu},
keywords = {Camera relocalization, Siamese neural network, Relative geometry constraints},
abstract = {6DOF camera relocalization is an important component of autonomous driving and navigation. Deep learning has recently emerged as a promising technique to tackle this problem. In this paper, we present a novel relative geometry-aware Siamese neural network to enhance the performance of deep learning-based methods through explicitly exploiting the relative geometry constraints between images. We perform multi-task learning and predict the absolute and relative poses simultaneously. We regularize the shared-weight twin networks in both the pose and feature domains to ensure that the estimated poses are globally as well as locally correct. We employ metric learning and design a novel adaptive metric distance loss to learn a feature that is capable of distinguishing poses of visually similar images from different locations.We evaluate the proposed method on public indoor and outdoor benchmarks and the experimental results demonstrate that our method can significantly improve localization performance. Furthermore, extensive ablation evaluations are conducted to demonstrate the effectiveness of different terms of the loss function.}
}
@article{SHARMA2021108979,
title = {Data-driven short-term natural gas demand forecasting with machine learning techniques},
journal = {Journal of Petroleum Science and Engineering},
volume = {206},
pages = {108979},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.108979},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521006380},
author = {Vinayak Sharma and Ümit Cali and Bhav Sardana and Murat Kuzlu and Dishant Banga and Manisa Pipattanasomporn},
keywords = {Natural gas forecasting, Artificial neural networks, Conjugate gradient, Gradient boosting, Machine learning, Natural gas supply chain},
abstract = {Natural gas demand forecasting is one of the most crucial steps in the proper planning and operation of natural gas supply systems. The demand and supply of natural gas must be balanced at all times.Large error in forecasts of natural gas demand can cost Local Distribution Companies (LDCs) millions of dollars. In this study, techniques for accurate forecasting of natural gas demand are examined. The models are tested and validated on real data from nPower forecasting competition 2018, which consists of historical natural gas consumption and the corresponding weather forecast at 6-h intervals. The methodology presents a holistic approach that includes data pre-processing, feature engineering, feature selection, model development, and post-processing. To capture the intra-day variability in natural gas demand a block-wise approach is used to develop the forecasting models. In this approach, a separate model is developed for each block of the day. Subsequently, four different forecasting models are developed using the block-wise technique, namely, a block-wise gradient boosting model using features from sensitivity analysis (GB), a block-wise gradient boosting model using features from PCA (GB-PCA), a block-wise ANN-CG model using features from sensitivity analysis (ANN-CG) and a block-wise ANN-CG model using features from PCA (ANN-CG-PCA). Three hybrid forecasts are also developed by combining the forecasts from the four individual models. The results show that the combined models outperform the individual models, with an improvement of around 15% in terms of Mean Absolute Percentage Error (MAPE).}
}
@article{SHEN2021102852,
title = {When RSSI encounters deep learning: An area localization scheme for pervasive sensing systems},
journal = {Journal of Network and Computer Applications},
volume = {173},
pages = {102852},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102852},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303192},
author = {Zhishu Shen and Tiehua Zhang and Atsushi Tagami and Jiong Jin},
keywords = {Wireless sensor network, RSSI-Based localization, Area localization, Machine learning},
abstract = {Localization has long been considered as a crucial research problem for pervasive sensing systems, especially with the arrival of big data era. Various techniques have been proposed to improve the localization accuracy by leveraging common wireless signals, such as radio signal strength indication (RSSI), collected from sensors placed in pervasive environments. However, the measured signal value can be easily affected by noise caused by physical obstacles in such sensing environment, which in turn compromises the localization performance. Hence, we present a novel RSSI-based area localization scheme using deep neural network (DNN) to explore the underlying correlation between the RSSI data and the respective sensor placement to achieve a superior localization performance. Moreover, to cope with the sensor data loss issue that commonly occurs during wireless sensor network (WSN) operation, an algorithm is designed to reconstruct the missing data for respective sensors in order to preserve the performance of DNN localization model. The effectiveness of the proposed scheme is verified with a real-world WSN testbed deployed inside an office building. The results demonstrate that the proposed scheme provides satisfactory prediction accuracy in area localization for pervasive sensing systems, regardless of the data loss issue that occurs with the respective sensors.}
}
@article{USAMA2020571,
title = {Attention-based sentiment analysis using convolutional and recurrent neural network},
journal = {Future Generation Computer Systems},
volume = {113},
pages = {571-578},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19334600},
author = {Mohd Usama and Belal Ahmad and Enmin Song and M. Shamim Hossain and Mubarak Alrashoud and Ghulam Muhammad},
keywords = {CNN, RNN, Attention mechanism, Sentiment analysis},
abstract = {Convolution and recurrent neural network have obtained remarkable performance in natural language processing(NLP). Moreover, from the attention mechanism perspective convolution neural network(CNN) is applied less than recurrent neural network(RNN). Because RNN can learn long-term dependencies and gives better results than CNN. But CNN has its own advantage, can extract high-level features by using its local fix size context at the input level. Thus, this paper proposed a new model based on RNN with CNN-based attention mechanism by using the merits of both architectures together in one model. In the proposed model, first, CNN learns the high-level features of sentence from input representation. Second, we used attention mechanism to get the attention of the model on the features which contribute much in the prediction task by calculating the attention score from features context generated from CNN filters. Finally, these features context from CNN with attention score are commonly used at the RNN to process them sequentially. To validate the model we experiment on three benchmark datasets. Experiment results and their analysis demonstrate the effectiveness of the model.}
}
@article{NG2020255,
title = {Anomaly detection framework for Internet of things traffic using vector convolutional deep learning approach in fog environment},
journal = {Future Generation Computer Systems},
volume = {113},
pages = {255-265},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19316954},
author = {Bhuvaneswari Amma N.G. and Selvakumar S.},
keywords = {Anomaly detection, Big data, Convolutional neural network, Deep learning, Fog computing, IoT},
abstract = {The proliferation of Internet of things (IoT) devices has lured hackers to launch attacks. Therefore, anomalies in IoT traffic must be detected to mitigate these attacks and protect services rendered by smart devices. The lacuna in the existing anomaly detection techniques is the nonscalable nature of anomaly detection systems, resulting in the mishandling of large-scale data generated from IoT devices. The issue of scalability is addressed and an anomaly detection framework in a fog environment is proposed herein using vector convolutional deep learning (VCDL) approach. The anomaly detection system could be scalable if the traffic can be distributed to the nodes in the fog layer for processing. This is effectively captured in the VCDL approach in which the training of IoT traffic is distributed and computations are performed in the fog nodes. The parameters required for training are shared by the master node in the fog layer. Further, the proposed anomaly detection algorithm classifies IoT traffic as either normal or attack and then passes it to the cloud for attack mitigation. Experiments were conducted on UNSW’s Bot-IoT dataset and the results indicate that the proposed distributed deep learning approach can efficiently handle scalable data compared with the existing centralized deep learning approaches. Experimental results show that the proposed approach is significantly better in terms of accuracy, precision, and recall compared with the state-of-the-art anomaly detection systems.}
}
@article{CELIS2022241,
title = {Design of an early alert system for PM2.5 through a stochastic method and machine learning models},
journal = {Environmental Science & Policy},
volume = {127},
pages = {241-252},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121003166},
author = {Nathalia Celis and Alejandro Casallas and Ellie Anne López-Barrera and Hermes Martínez and Carlos A. {Peña Rincón} and Ricardo Arenas and Camilo Ferro},
keywords = {Early alert system, Risk assessment, Machine learning, WRF-Chem, Air quality protocol},
abstract = {In Latin America, the levels of pollution have risen considerably in the last few years. 2019, for example, had one of the largest numbers of air quality alerts. These alerts signal an increase in respiratory diseases among the population. For this reason, this paper designs a preventive early alert system for air quality. This system compares three machine learning models and validates, through statistical and categorical parameters (9), that a stochastic model, combined with a convolution bidirectional recurrent neural network (1D-BDLM), has an accuracy of ≈93±4% when forecasting the risk for each population group in all the monitoring stations. Likewise, it is also able to capture high pollution events without producing false alarms (≈10 ± 5%). This model is utilized to design an alert protocol (24 h in advance) before a pollution event occurs. The protocol distinguishes the level of alert and the type of population at risk, focusing on two objectives: pollution mitigation and risk reduction for the population. To reduce pollutant concentrations, this paper proposes limiting vehicle traffic in the most polluted city zones or, if necessary, throughout the entire area. In relation to stationary sources, this article proposes the implementation of monitoring measures in order to identify the most polluting factories and restrict their operation during a specific period of time. In regards to population risk, the protocol aims to reduce exposure time by recommending the avoidance of outdoor activities (in specific zones) and the use of protective gear, taking into consideration relevant differences between population groups.}
}
@article{MASUD2020215,
title = {Deep learning-based intelligent face recognition in IoT-cloud environment},
journal = {Computer Communications},
volume = {152},
pages = {215-222},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.050},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419312988},
author = {Mehedi Masud and Ghulam Muhammad and Hesham Alhumyani and Sultan S Alshamrani and Omar Cheikhrouhou and Saleh Ibrahim and M. Shamim Hossain},
keywords = {Deep neural network, Intelligent face recognition, Healthcare-IoT, Cloud environment},
abstract = {In recent years, the Internet-of-Things (IoT) technology is being used in many application areas such as healthcare, video surveillance, transportation etc. The massive adoption and growth of IoT in these areas are generating a massive amount of data. For example, IoT devices such as cameras are generating a huge amount of images when used in hospital surveillance scenarios. Here, face recognition is an important element that can be used for securing hospital facilities, emotion detection and sentiment analysis of patients, detecting patient fraud, and hospital traffic pattern analysis. Automatic and intelligent face recognition systems have high accuracy in a controlled environment; however, they have low accuracy in an uncontrolled environment. Also, the systems need to operate in real-time in many applications such as smart healthcare. This paper suggests a tree-based deep model for automatic face recognition in a cloud environment. The proposed deep model is computationally less expensive without compromising the accuracy. In the model, an input volume is split into several volumes, where a tree is constructed for each volume. A tree is defined by its branching factor and height. Each branch is represented by a residual function, which is constituted by a convolutional layer, a batch normalization, and a non-linear function. The proposed model is evaluated in various publicly available databases. A comparison of performance is also done with state-of-the-art deep models for face recognition. The results of the experiments demonstrate that the proposed model achieved accuracies of 98.65%, 99.19%, 95.84% on FEI, ORL, and LFW databases, respectively.}
}
@article{BHATTACHARYA2021102589,
title = {Deep learning and medical image processing for coronavirus (COVID-19) pandemic: A survey},
journal = {Sustainable Cities and Society},
volume = {65},
pages = {102589},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102589},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720308076},
author = {Sweta Bhattacharya and Praveen Kumar {Reddy Maddikunta} and Quoc-Viet Pham and Thippa Reddy Gadekallu and Siva Rama {Krishnan S} and Chiranji Lal Chowdhary and Mamoun Alazab and Md. {Jalil Piran}},
keywords = {Artificial intelligence (AI), Big data, Coronavirus pandemic, COVID-19, Epidemic outbreak, Deep learning, Medical image processing},
abstract = {Since December 2019, the coronavirus disease (COVID-19) outbreak has caused many death cases and affected all sectors of human life. With gradual progression of time, COVID-19 was declared by the world health organization (WHO) as an outbreak, which has imposed a heavy burden on almost all countries, especially ones with weaker health systems and ones with slow responses. In the field of healthcare, deep learning has been implemented in many applications, e.g., diabetic retinopathy detection, lung nodule classification, fetal localization, and thyroid diagnosis. Numerous sources of medical images (e.g., X-ray, CT, and MRI) make deep learning a great technique to combat the COVID-19 outbreak. Motivated by this fact, a large number of research works have been proposed and developed for the initial months of 2020. In this paper, we first focus on summarizing the state-of-the-art research works related to deep learning applications for COVID-19 medical image processing. Then, we provide an overview of deep learning and its applications to healthcare found in the last decade. Next, three use cases in China, Korea, and Canada are also presented to show deep learning applications for COVID-19 medical image processing. Finally, we discuss several challenges and issues related to deep learning implementations for COVID-19 medical image processing, which are expected to drive further studies in controlling the outbreak and controlling the crisis, which results in smart healthy cities.}
}
@article{GUPTA2020406,
title = {Machine Learning Models for Secure Data Analytics: A taxonomy and threat model},
journal = {Computer Communications},
volume = {153},
pages = {406-440},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419318493},
author = {Rajesh Gupta and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar},
keywords = {Big data, Secure Data Analytics, Data reduction, Machine learning models, Threat model, Data security and privacy},
abstract = {In recent years, rapid technological advancements in smart devices and their usage in a wide range of applications exponentially increases the data generated from these devices. So, the traditional data analytics techniques may not be able to handle this extreme volume of data known as Big Data (BD) generated by different devices. However, this exponential increase of data opens the doors for the different type of attackers to launch various attacks by exploiting various vulnerabilities (SQL injection, OS fingerprinting, malicious code execution, etc.) during data analytics. Motivated from the aforementioned discussion, in this paper, we explored Machine Learning (ML) and Deep Learning (DL)-based models and techniques which are capable off to identify and mitigate both the known as well as unknown attacks. ML and DL-based techniques have the capabilities to learn from the traffic pattern using training and testing datasets in the extensive network domains to make intelligent decisions concerning attack identification and mitigation. We also proposed a DL and ML-based Secure Data Analytics (SDA) architecture to classify normal or attack input data. A detailed taxonomy of SDA is abstracted into a threat model. This threat model addresses various research challenges in SDA using multiple parameters such as-efficiency, latency, accuracy, reliability, and attacks launched by the attackers. Finally, a comparison of existing SDA proposals with respect to various parameters is presented, which allows the end users to select one of the SDA proposals in comparison to its merits over the others.}
}
@article{RAMAMURTHY2020103280,
title = {Auto encoder based dimensionality reduction and classification using convolutional neural networks for hyperspectral images},
journal = {Microprocessors and Microsystems},
volume = {79},
pages = {103280},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103280},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120304397},
author = {Madhumitha Ramamurthy and Y. Harold Robinson and S. Vimal and A. Suresh},
keywords = {Dimensionality reduction, Accuracy, Classification, Auto encoder, Hyperspectral image, Convolutional neural networks},
abstract = {Hyperspectral images (HSI) are adjacent band images commonly used in remote sensing environment; the deep learning methodologies have the important feature for classification process. Additionally, the highest dimensionality of HSI enhances the computational complexity which affects the overall performance. Hence, the dimensionality reduction plays a vital role to enhance the performance while processing the Hyperspectral images. The HSI is initially segmented into the pixels, it belongs to the similar correlation and it is optimized using the neural network framework. Auto Encoder based dimensionality reduction is proposed for performance enhancement that denoising removed. The reconstructed pixel using vectors and also identifying the reconstructing loss enhances the overall accuracy. The Convolutional Neural network framework implements the classification process for Hyperspectral images. The performance analysis results on the proposed technique have improved accuracy and performance compared to the related techniques.}
}
@article{ZHONG2022107605,
title = {Machine learning-based multimedia services for business model evaluation},
journal = {Computers & Electrical Engineering},
volume = {97},
pages = {107605},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107605},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005395},
author = {Xiaoying Zhong and Xuejiao Tian and K. Deepa Thilak and Anbarasan M},
keywords = {Decision support system, Machine learning, Multimedia, Business model evaluation},
abstract = {A business model evaluation refers to how a company uses its capital to provide value to consumers while optimizing income and growth. Entrepreneurs are facing important challenges in executing their market development based on resource-centered business models. Hence, the Multimedia-assisted Business Evaluation Model (MBEM) based on machine learning has been proposed to improve business efficiency. Multimedia has the right to prohibit third persons from infringing a commercial model that involves the selling and distributing sound files for digital content by sending patents for the online distribution of music and video. It can be useful to produce a broad range of design proposals and find the newest innovations to create modern solutions to challenging structures and become more competitive markets. A Decision Support System (DSSs) based on machine learning for information systems supports organizational and business decision-making. The experimental results show that the proposed MBEM model enhances business evaluation, decision making, performance, and profitability compared to other popular methods.}
}
@article{SINGH2022380,
title = {A framework for privacy-preservation of IoT healthcare data using Federated Learning and blockchain technology},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {380-388},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004726},
author = {Saurabh Singh and Shailendra Rathore and Osama Alfarraj and Amr Tolba and Byungun Yoon},
keywords = {Federated Learning, Privacy-preserving, Blockchain, Internet-of-Things},
abstract = {With the dramatically increasing deployment of IoT (Internet-of-Things) and communication, data has always been a major priority to achieve intelligent healthcare in a smart city. For the modern environment, valuable assets are user IoT data. The privacy policy is even the biggest necessity to secure user’s data in a deep-rooted fundamental infrastructure of network and advanced applications, including smart healthcare. Federated learning acts as a special machine learning technique for privacy-preserving and offers to contextualize data in a smart city. This article proposes Blockchain and Federated Learning-enabled Secure Architecture for Privacy-Preserving in Smart Healthcare, where Blockchain-based IoT cloud platforms are used for security and privacy. Federated Learning technology is adopted for scalable machine learning applications like healthcare. Furthermore, users can obtain a well-trained machine learning model without sending personal data to the cloud. Moreover, it also discussed the applications of federated learning for a distributed secure environment in a smart city.}
}
@article{AHMAD2021120911,
title = {Methodological framework for short-and medium-term energy, solar and wind power forecasting with stochastic-based machine learning approach to monetary and energy policy applications},
journal = {Energy},
volume = {231},
pages = {120911},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120911},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221011592},
author = {Tanveer Ahmad and Dongdong Zhang and Chao Huang},
keywords = {Renewable energy forecasting, Stochastic Gaussian process model, Machine learning, Artificial neural networks, Objective functions, Multi-objective optimization},
abstract = {Anomalous seasons such as low-wind summers and extremely cold winters can seriously disrupt energy reliability and productivity. Better short/medium-term forecasts that provide reliable and strategic planning insights will allow the energy industry to plan for these extremes. In order to efficiently quantify uncertainty, this study proposes a Gaussian stochastic-based machine learning process model (GPR) for short/medium-term energy, solar, and wind (ESW) power forecasts using two different temporal resolutions of data. Four experimental steps (EXMS) were designed. Each EXMS is designed with four distinct fitting and predicting methods, and the GPR model uses seven kernel covariance functions for hyperparameter optimization. Real-time data is used for the forecasting analysis at three different locations. The forecasting results are validated using three existing models. The percent coefficient of variation of CVGPR1 and CVGPR2 of EXMS-1 and EXMS-3 for ESW power forecasts is 0.017%, 0.057%, 0.025%, and 0.223%, 0.225%, 0.170%, respectively. Accuracy has shown that the proposed model can predict ESW power simultaneously at two different temporal resolution data. The GPR accuracy with four EXMS methodologies is promising by addressing ESW power forecasts under the GPR framework of significant utilities, independent power producers, and public interest.}
}
@article{ZHANG2021116452,
title = {A review of machine learning in building load prediction},
journal = {Applied Energy},
volume = {285},
pages = {116452},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116452},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921000209},
author = {Liang Zhang and Jin Wen and Yanfei Li and Jianli Chen and Yunyang Ye and Yangyang Fu and William Livingood},
keywords = {Building energy system, Building load prediction, Building energy forecasting, Machine learning, Feature engineering, Data engineering},
abstract = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.}
}
@article{DELATORREPARRA2020102662,
title = {Detecting Internet of Things attacks using distributed deep learning},
journal = {Journal of Network and Computer Applications},
volume = {163},
pages = {102662},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102662},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520301363},
author = {Gonzalo {De La Torre Parra} and Paul Rad and Kim-Kwang Raymond Choo and Nicole Beebe},
keywords = {Cyber security, Cloud computing, Machine learning, Deep learning, Recurrent neural network},
abstract = {The reliability of Internet of Things (IoT) connected devices is heavily dependent on the security model employed to protect user data and prevent devices from engaging in malicious activity. Existing approaches for detecting phishing, distributed denial of service (DDoS), and Botnet attacks often focus on either the device or the back-end. In this paper, we propose a cloud-based distributed deep learning framework for phishing and Botnet attack detection and mitigation. The model comprises two key security mechanisms working cooperatively, namely: (1) a Distributed Convolutional Neural Network (DCNN) model embedded as an IoT device micro-security add-on for detecting phishing and application layer DDoS attacks; and (2) a cloud-based temporal Long-Short Term Memory (LSTM) network model hosted on the back-end for detecting Botnet attacks, and ingest CNN embeddings to detect distributed phishing attacks across multiple IoT devices. The distributed CNN model, embedded into a ML engine in the client's IoT device, allows us to detect and defend the IoT device from phishing attacks at the point of origin. We create a dataset consisting of both phishing and non-phishing URLs to train the proposed CNN add-on security model, and select the N_BaIoT dataset for training the back-end LSTM model. The joint training method minimizes communication and resource requirements for attack detection, and maximizes the usefulness of extracted features. In addition, an aggregation of schemes allows the automatic fusion of multiple requests to improve the overall performance of the system. Our experiments show that the IoT micro-security add-on running the proposed CNN model is capable of detecting phishing attacks with an accuracy of 94.3% and a F-1 score of 93.58%. Using the back-end LSTM model, the model detects Botnet attacks with an accuracy of 94.80% using all malicious data points in the used dataset. Thus, the findings demonstrate that the proposed approach is capable of detecting attacks, both at device and at the back-end level, in a distributed fashion.}
}
@article{ALSAD201986,
title = {RF-based drone detection and identification using deep learning approaches: An initiative towards a large open source drone database},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {86-97},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18330760},
author = {Mohammad F. Al-Sa’d and Abdulla Al-Ali and Amr Mohamed and Tamer Khattab and Aiman Erbad},
keywords = {UAV detection, Drone identification, Deep learning, Neural networks, Machine learning},
abstract = {The omnipresence of unmanned aerial vehicles, or drones, among civilians can lead to technical, security, and public safety issues that need to be addressed, regulated and prevented. Security agencies are in continuous search for technologies and intelligent systems that are capable of detecting drones. Unfortunately, breakthroughs in relevant technologies are hindered by the lack of open source databases for drone’s Radio Frequency (RF) signals, which are remotely sensed and stored to enable developing the most effective way for detecting and identifying these drones. This paper presents a stepping stone initiative towards the goal of building a database for the RF signals of various drones under different flight modes. We systematically collect, analyze, and record raw RF signals of different drones under different flight modes such as: off, on and connected, hovering, flying, and video recording. In addition, we design intelligent algorithms to detect and identify intruding drones using the developed RF database. Three deep neural networks (DNN) are used to detect the presence of a drone, the presence of a drone and its type, and lastly, the presence of a drone, its type, and flight mode. Performance of each DNN is validated through a 10-fold cross-validation process and evaluated using various metrics. Classification results show a general decline in performance when increasing the number of classes. Averaged accuracy has decreased from 99.7% for the first DNN (2-classes), to 84.5% for the second DNN (4-classes), and lastly, to 46.8% for the third DNN (10-classes). Nevertheless, results of the designed methods confirm the feasibility of the developed drone RF database to be used for detection and identification. The developed drone RF database along with our implementations are made publicly available for students and researchers alike.}
}
@article{BOUKERCHE2020102224,
title = {A performance modeling and analysis of a novel vehicular traffic flow prediction system using a hybrid machine learning-based model},
journal = {Ad Hoc Networks},
volume = {106},
pages = {102224},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102224},
url = {https://www.sciencedirect.com/science/article/pii/S1570870520301803},
author = {Azzedine Boukerche and Jiahao Wang},
keywords = {Vehicular traffic flow prediction, Time-series, GCN, Parallel training, RNN, Sequence to sequence},
abstract = {Traffic prediction on the road, as a vital part of the Intelligent Transportation System (ITS) has attracted much attention recently. It is always one of the hot topics about how to implement an efficient, robust, and accurate vehicular traffic prediction system. With the help of Machine Learning-based (ML) methods, especially Deep Learning-based (DL) methods, the accuracy of the prediction model is increased. However, we also noticed that there are still many open challenges under ML-based vehicular traffic prediction model real-world implementation. Firstly, the time consumption for training DL model is relatively large when compared to parametric models, such as ARIMA, SARIMA. Second, it is still a hot topic for road traffic prediction that how to capture the spacial relationship between road detectors, which is affected by the geographic correlation, as well as the time change. The last but not the least, it is important for us to implement the prediction system into the real world; meanwhile, we should find a way to make use of the advanced technology applied in ITS to improve the prediction system itself. In this paper, we focus on improving the features of the prediction model, which can be helpful for implementing the model in the real world. We present a new hybrid deep learning model by using Graph Convolutional Network (GCN) and the deep aggregation structure (i.e., the sequence to sequence structure) of Gated Recurrent Unit (GRU). Meanwhile, in order to solve the real-world prediction problem, i.e., the online prediction task, we present a new online prediction strategy by using refinement learning. In order to further improve the model’s accuracy and efficiency when applied to ITS, we make use of an efficient parallel training strategy while taking advantage of the vehicular cloud structure.}
}
@article{YANG2021102862,
title = {A shock wave diagram based deep learning model for early alerting an upcoming public event},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {122},
pages = {102862},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102862},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20307622},
author = {Hanyi Yang and Lili Du and Jamshid Mohammadi},
keywords = {Public event prediction, Traffic shock wave, Deep learning, Long-short term memory},
abstract = {Even though extensive efforts worked on traffic anomaly detection, this study noticed that little attention is given to predict public events before they start, like celebrations, sports games, and so on. Given a public event produces complicated spatiotemporal traffic impacts, existing data-driven approaches without involving traffic flow analysis show limited effectiveness to predict a public event. Motivated by this view, this study seeks to develop a domain-knowledge-based learning approach, which integrates shock wave analysis into deep learning models to predict the occurring of an upcoming public event (i.e., the SW-DLM approach). This integration raises new research challenging and calls for new approaches. Specifically, we develop an efficient algorithm to generate the shock wave diagrams to present the evolvement of the traffic anomaly, which expands whenever new traffic data is collected. Next, the shock wave diagram itself is not a well-coded input for feature extraction and learning. This study thus developed an innovative encoding approach, which transforms a shock wave diagram into an optimal pixel grid. Considering the features extracted from the encoded shock wave diagrams are fed into the long-short term memory (LSTM) model for the event prediction. The numerical experiments based on the field data indicate that the SW-DLM is able to predict a public event with 87% accuracy around 84 min before it starts in a day. It outperforms all data-driven machine learning models using point traffic data as inputs. Thus, we claim that using the shock wave diagrams can significantly improve the accuracy and efficiency of the learning approaches for predicting a public event. The SW-DLM will help develop preventive traffic control or route plans to avoid traffic congestion induced by public events.}
}
@article{GATI2021298,
title = {Differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems: State-of-the-art and perspectives},
journal = {Information Fusion},
volume = {76},
pages = {298-314},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000890},
author = {Nicholaus J. Gati and Laurence T. Yang and Jun Feng and Xin Nie and Zhian Ren and Samwel K. Tarus},
keywords = {Differential privacy, Deep computation, Data fusion, CPSS},
abstract = {The modern technological advancement influences the growth of the cyber–physical system and cyber–social system to a more advanced computing system cyber–physical–social system (CPSS). Therefore, CPSS leads the data science revolution by promoting tri-space information resource from a single space. The establishment of CPSSs increases the related privacy concerns. To provide privacy on CPSSs data, various privacy-preserving schemes have been introduced in the recent past. However, technological advancement in CPSSs requires the modifications of previous techniques to suit its dynamics. Meanwhile, differential privacy has emerged as an effective method to safeguard CPSSs data privacy. To completely comprehend the state-of-the-art developments and learn the field’s research directions, this article provides a comprehensive review of differentially private data fusion and deep learning in CPSSs. Additionally, we present a novel differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems , and various future research directions for CPSSs.}
}
@article{ASLAM2021110992,
title = {A survey on deep learning methods for power load and renewable energy forecasting in smart microgrids},
journal = {Renewable and Sustainable Energy Reviews},
volume = {144},
pages = {110992},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.110992},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121002847},
author = {Sheraz Aslam and Herodotos Herodotou and Syed Muhammad Mohsin and Nadeem Javaid and Nouman Ashraf and Shahzad Aslam},
keywords = {Energy forecasting, Renewable energy, Deep learning, Artificial neural networks, Machine learning},
abstract = {Microgrids have recently emerged as a building block for smart grids combining distributed renewable energy sources (RESs), energy storage devices, and load management methodologies. The intermittent nature of RESs brings several challenges to the smart microgrids, such as reliability, power quality, and balance between supply and demand. Thus, forecasting power generation from RESs, such as wind turbines and solar panels, is becoming essential for the efficient and perpetual operations of the power grid and it also helps in attaining optimal utilization of RESs. Energy demand forecasting is also an integral part of smart microgrids that helps in planning the power generation and energy trading with commercial grid. Machine learning (ML) and deep learning (DL) based models are promising solutions for predicting consumers’ demands and energy generations from RESs. In this context, this manuscript provides a comprehensive survey of the existing DL-based approaches, which are developed for power forecasting of wind turbines and solar panels as well as electric power load forecasting. It also discusses the datasets used to train and test the different DL-based prediction models, enabling future researchers to identify appropriate datasets to use in their work. Even though there are a few related surveys regarding energy management in smart grid applications, they are focused on a specific production application such as either solar or wind. Moreover, none of the surveys review the forecasting schemes for production and load side simultaneously. Finally, previous surveys do not consider the datasets used for forecasting despite their significance in DL-based forecasting approaches. Hence, our survey work is intrinsically different due to its data-centered view, along with presenting DL-based applications for load and energy generation forecasting in both residential and commercial sectors. The comparison of different DL approaches discussed in this manuscript reveals that the efficiency of such forecasting methods is highly dependent on the amount of the historical data and thus a large number of data storage devices and high processing power devices are required to deal with big data. Finally, this study raises several open research problems and opportunities in the area of renewable energy forecasting for smart microgrids.}
}
@article{PEREZ2021891,
title = {Energy-conscious optimization of Edge Computing through Deep Reinforcement Learning and two-phase immersion cooling},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {891-907},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002934},
author = {Sergio Pérez and Patricia Arroba and José M. Moya},
keywords = {Energy-aware optimization, Deep Reinforcement Learning, Edge Computing, Two-phase immersion cooling, Advanced driver assistance systems},
abstract = {Until now, the reigning computing paradigm has been Cloud Computing, whose facilities concentrate in large and remote areas. Novel data-intensive services with critical latency and bandwidth constraints, such as autonomous driving and remote health, will suffer under an increasingly saturated network. On the contrary, Edge Computing brings computing facilities closer to end-users to offload workloads in Edge Data Centers (EDCs). Nevertheless, Edge Computing raises other concerns like EDC size, energy consumption, price, and user-centered design. This research addresses these challenges by optimizing Edge Computing scenarios in two ways, two-phase immersion cooling systems and smart resource allocation via Deep Reinforcement Learning. To this end, several Edge Computing scenarios have been modeled, simulated, and optimized with energy-aware strategies using real traces of user demand and hardware behavior. These scenarios include air-cooled and two-phase immersion-cooled EDCs devised using hardware prototypes and a resource allocation manager based on an Advantage Actor–Critic (A2C) agent. Our immersion-cooled EDC’s IT energy model achieved an NRMSD of 3.15% and an R2 of 97.97%. These EDCs yielded an average energy saving of 22.8% compared to air-cooled. Our DRL-based allocation manager further reduced energy consumption by up to 23.8% in comparison to the baseline.}
}
@article{CARNEIRO2021,
title = {A predictive and user-centric approach to Machine Learning in data streaming scenarios},
journal = {Neurocomputing},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.100},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016490},
author = {Davide Carneiro and Miguel Guimarães and Fábio Silva and Paulo Novais},
keywords = {Meta-learning, Explainability, Streaming data, Big data},
abstract = {Machine Learning has emerged in the last years as the main solution to many of nowadays’ data-based decision problems. However, while new and more powerful algorithms and the increasing availability of computational resources contributed to a widespread use of Machine Learning, significant challenges still remain. Two of the most significant nowadays are the need to explain a model’s predictions, and the significant costs of training and re-training models, especially with large datasets or in streaming scenarios. In this paper we address both issues by proposing an approach we deem predictive and user-centric. It is predictive in the sense that it estimates the benefit of re-training a model with new data, and it is user-centric in the sense that it implements an explainable interface that produces interpretable explanations that accompany predictions. The former allows to reduce necessary resources (e.g. time, costs) spent on re-training models when no improvements are expected, while the latter allows for human users to have additional information to support decision-making. We validate the proposed approach with a group of public datasets and present a real application scenario.}
}
@article{DAMAJ2021,
title = {Intelligent transportation systems: A survey on modern hardware devices for the era of machine learning},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001877},
author = {Issam Damaj and Salwa K. {Al Khatib} and Tarek Naous and Wafic Lawand and Zainab Z. Abdelrazzak and Hussein T. Mouftah},
keywords = {Intelligent transportation systems, Machine learning, Hardware devices, Performance evaluation, Taxonomy},
abstract = {The increasing complexity of Intelligent Transportation Systems (ITS), that comprise a wide variety of applications and services, has imposed a necessity for high-performance Modern Hardware Devices (MHDs). The performance challenge has become more noticeable with the integration of Machine Learning (ML) techniques deployed in large-scale settings. ML has effectively supported the field of ITS by providing efficient and optimized solutions to problems that were otherwise tackled using traditional statistical and analytical approaches. Addressing the hardware deployment needs of ITS in the era of ML is a challenging problem that involves temporal, spatial, environmental, and economical factors. This survey reviews the recent literature of ML-driven ITS, in which MHDs were utilized, with a focus on performance indicators. A taxonomy is then synthesized, giving a complete representation of what the current capabilities of the surveyed ITS rely on in terms of ML techniques and technological infrastructure. To alleviate the difficulties faced in the non-trivial task of selecting suitable ML techniques and MHDs for an ITS with a specific complexity level, a performance evaluation framework is proposed. The presented survey sets the basis for developing suitable hardware, facilitating the integration of ML within ITS, and bridging the gap between research and real-world deployments.}
}
@article{SALMAN2021106357,
title = {A review on utilizing machine learning technology in the fields of electronic emergency triage and patient priority systems in telemedicine: Coherent taxonomy, motivations, open research challenges and recommendations for intelligent future work},
journal = {Computer Methods and Programs in Biomedicine},
volume = {209},
pages = {106357},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106357},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721004314},
author = {Omar H. Salman and Zahraa Taha and Muntadher Q. Alsabah and Yaseein S. Hussein and Ahmed S. Mohammed and Mohammed Aal-Nouman},
keywords = {Artificial Intelligence, Healthcare Monitoring, mHealth, Triage, Sensor, Data Mining, Big Data},
abstract = {Background
With the remarkable increasing in the numbers of patients, the triaging and prioritizing patients into multi-emergency level is required to accommodate all the patients, save more lives, and manage the medical resources effectively. Triaging and prioritizing patients becomes particularly challenging especially for the patients who are far from hospital and use telemedicine system. To this end, the researchers exploiting the useful tool of machine learning to address this challenge. Hence, carrying out an intensive investigation and in-depth study in the field of using machine learning in E-triage and patient priority are essential and required.
Objectives
This research aims to (1) provide a literature review and an in-depth study on the roles of machine learning in the fields of electronic emergency triage (E-triage) and prioritize patients for fast healthcare services in telemedicine applications. (2) highlight the effectiveness of machine learning methods in terms of algorithms, medical input data, output results, and machine learning goals in remote healthcare telemedicine systems. (3) present the relationship between machine learning goals and the electronic triage processes specifically on the: triage levels, medical features for input, outcome results as outputs, and the relevant diseases. (4), the outcomes of our analyses are subjected to organize and propose a cross-over taxonomy between machine learning algorithms and telemedicine structure. (5) present lists of motivations, open research challenges and recommendations for future intelligent work for both academic and industrial sectors in telemedicine and remote healthcare applications.
Methods
An intensive research is carried out by reviewing all articles related to the field of E-triage and remote priority systems that utilise machine learning algorithms and sensors. We have searched all related keywords to investigate the databases of Science Direct, IEEE Xplore, Web of Science, PubMed, and Medline for the articles, which have been published from January 2012 up to date.
Results
A new crossover matching between machine learning methods and telemedicine taxonomy is proposed. The crossover-taxonomy is developed in this study to identify the relationship between machine learning algorithm and the equivalent telemedicine categories whereas the machine learning algorithm has been utilized. The impact of utilizing machine learning is composed in proposing the telemedicine architecture based on synchronous (real-time/ online) and asynchronous (store-and-forward / offline) structure. In addition to that, list of machine learning algorithms, list of the performance metrics, list of inputs data and outputs results are presented. Moreover, open research challenges, the benefits of utilizing machine learning and the recommendations for new research opportunities that need to be addressed for the synergistic integration of multidisciplinary works are organized and presented accordingly.
Discussion
The state-of-the-art studies on the E-triage and priority systems that utilise machine learning algorithms in telemedicine architecture are discussed. This approach allows the researchers to understand the modernisation of healthcare systems and the efficient use of artificial intelligence and machine learning. In particular, the growing worldwide population and various chronic diseases such as heart chronic diseases, blood pressure and diabetes, require smart health monitoring systems in E-triage and priority systems, in which machine learning algorithms could be greatly beneficial.
Conclusions
Although research directions on E-triage and priority systems that use machine learning algorithms in telemedicine vary, they are equally essential and should be considered. Hence, we provide a comprehensive review to emphasise the advantages of the existing research in multidisciplinary works of artificial intelligence, machine learning and healthcare services.}
}
@article{KANJO201946,
title = {Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection},
journal = {Information Fusion},
volume = {49},
pages = {46-56},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518300460},
author = {Eiman Kanjo and Eman M.G. Younis and Chee Siang Ang},
keywords = {Deep learning, Emotion recognition, Convoltutional neural network, Long short-term memory mobile sensing},
abstract = {The detection and monitoring of emotions are important in various applications, e.g., to enable naturalistic and personalised human-robot interaction. Emotion detection often require modelling of various data inputs from multiple modalities, including physiological signals (e.g., EEG and GSR), environmental data (e.g., audio and weather), videos (e.g., for capturing facial expressions and gestures) and more recently motion and location data. Many traditional machine learning algorithms have been utilised to capture the diversity of multimodal data at the sensors and features levels for human emotion classification. While the feature engineering processes often embedded in these algorithms are beneficial for emotion modelling, they inherit some critical limitations which may hinder the development of reliable and accurate models. In this work, we adopt a deep learning approach for emotion classification through an iterative process by adding and removing large number of sensor signals from different modalities. Our dataset was collected in a real-world study from smart-phones and wearable devices. It merges local interaction of three sensor modalities: on-body, environmental and location into global model that represents signal dynamics along with the temporal relationships of each modality. Our approach employs a series of learning algorithms including a hybrid approach using Convolutional Neural Network and Long Short-term Memory Recurrent Neural Network (CNN-LSTM) on the raw sensor data, eliminating the needs for manual feature extraction and engineering. The results show that the adoption of deep-learning approaches is effective in human emotion classification when large number of sensors input is utilised (average accuracy 95% and F-Measure=%95) and the hybrid models outperform traditional fully connected deep neural network (average accuracy 73% and F-Measure=73%). Furthermore, the hybrid models outperform previously developed Ensemble algorithms that utilise feature engineering to train the model average accuracy 83% and F-Measure=82%)}
}
@article{NAUMAN202113,
title = {Reinforcement learning-enabled Intelligent Device-to-Device (I-D2D) communication in Narrowband Internet of Things (NB-IoT)},
journal = {Computer Communications},
volume = {176},
pages = {13-22},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001912},
author = {Ali Nauman and Muhammad Ali Jamshed and Rashid Ali and Korhan Cengiz and  Zulqarnain and Sung Won Kim},
keywords = {Reinforcement Learning (RL), Intelligent communication, Device-to-Device (D2D) communication, Narrowband Internet of Things (NB-IoT), th Generation (5G) networks},
abstract = {The 5th Generation (5G) and Beyond 5G (B5G) are expected to be the enabling technologies for Internet-of-Everything (IoE). The quality-of-service (QoS) for IoE in the context of uplink data delivery of the content is of prime importance. The 3rd Generation Partnership Project (3GPP) standardizes the Narrowband Internet-of-Things (NB-IoT) in 5G, which is Low Power Wide Area (LPWA) technology to enhance the coverage and to optimize the power consumption for the IoT devices. Repetitions of control and data signals between NB-IoT User Equipment (UE) and the evolved NodeB/Base Station (eNB/BS), is one of the most prominent characteristics in NB-IoT. These repetitions ensure high reliability in the context of data delivery of time-sensitive applications, e.g., healthcare applications. However, these repetitions degrade the performance of the resource-constrained IoT network in terms of energy consumption. Device-to-Device (D2D) communication standardized in Long Term Evolution-Advanced (LTE-A) offers a key solution for NB-IoT UE to transmit in two hops route instead of direct uplink, which augments the efficiency of the system. In an effort to improve the data packet delivery, this study investigates D2D communication for NB-IoT delay-sensitive applications, such as healthcare-IoT services. This study formulates the selection of D2D communication relay as Multi-Armed Bandit (MAB) problem and incorporates Upper Confidence Bound (UCB) based Reinforcement Learning (RL) to solve MAB problem. The proposed Intelligent-D2D (I-D2D) communication methodology selects the optimum relay with a maximum Packet Delivery Ratio (PDR) with minimum End-to-End Delay (EED), which ultimately augments energy efficiency.}
}
@article{ALLAHBUKHSH2020103202,
title = {Maintenance intervention predictions using entity-embedding neural networks},
journal = {Automation in Construction},
volume = {116},
pages = {103202},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103202},
url = {https://www.sciencedirect.com/science/article/pii/S092658051931132X},
author = {Zaharah {Allah Bukhsh} and Irina Stipanovic and Aaqib Saeed and Andre G. Doree},
keywords = {Maintenance decisions, Bridges, Maintenance prediction, Machine learning, Deep neural networks, Decision-support, Multi-task learning, Entity embedding},
abstract = {Data-driven decision support can substantially aid in smart and efficient maintenance planning of road bridges. However, many infrastructure managers primary rely on information obtained during visual inspection to subjectively decide on the follow-up maintenance actions. The subjective approach is likely to lack the appropriate use of inspection data and does not promise cost-effective maintenance plans. In this paper, we show that the historical and operational data, readily available at the agencies, is of vital importance and can be used effectively for the recommendations of maintenance advises for bridges. This is achieved by developing a machine learning system that is trained on the past asset management data and provide support to the decision-makers in the condition assessment, risk analysis, and maintenance planning tasks. We have evaluated several traditional learning algorithms as well as the deep neural networks with entity embedding to find the optimal predictive models in terms of predictive capability. Additionally, we have explored the multi-task learning framework that has a shared representation of related prediction tasks to develop a powerful unified model. The analysis of results shows that a unified multi-task learning model performed best for the considered problems followed by task-specific neural networks with entity embedding and class weights. The results of models are further evaluated by instance-level explanations, which provide insights about essential features and explain the importance of data attributes for a particular task.}
}
@article{ALMEIDA2022429,
title = {Vehicular traffic flow prediction using deployed traffic counters in a city},
journal = {Future Generation Computer Systems},
volume = {128},
pages = {429-442},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004180},
author = {Ana Almeida and Susana Brás and Ilídio Oliveira and Susana Sargento},
keywords = {Smart urban mobility, Traffic flow, Forecasting, Deep learning},
abstract = {The sustainable growth of cities created the need for better informed decisions based on information and communication technologies to sense the city and quantify its pulse. An important part of this concept of “smart cities” is the characterization of vehicular traffic flows and the prediction of urban mobility. Although there are several sensors that are able to infer the traffic flows in the city, road-mounted traffic counters can measure the number of vehicles in different parts of the roads. However, they are not usually used in traffic city prediction; therefore, we can provide a first step for the usefulness of these sensors in the city management. In this paper we study both statistical and deep learning methods to describe, understand and predict the city traffic profile. Although traffic presents seasonal patterns, in occasional situations these may not be verified. Considering the proposed approaches, statistical algorithms, such as SARIMA, and neural network algorithms, such as FFNN, LSTM, CNN and hybrid LSTM-CNN, we found that statistical models are significantly good to predict the traffic counters data in the short-term, even when anomalous traffic conditions are observed. For long-term predictions, CNNs have shown to be efficient and robust. Long-term and short-term forecasting, in the context of traffic flow prediction, may be a strategy to accomplish different goals. Long-term forecasting can be chosen for traffic flow description, and short-term forecasting can be used to identify and mitigate anomalies.}
}
@article{JIN2022315,
title = {Adaptive Dual-View WaveNet for urban spatial–temporal event prediction},
journal = {Information Sciences},
volume = {588},
pages = {315-330},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.12.085},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521013062},
author = {Guangyin Jin and Chenxi Liu and Zhexu Xi and Hengyu Sha and Yanyun Liu and Jincai Huang},
keywords = {Spatial–temporal prediction, Representation learning, WaveNet, Graph convolutional neural network},
abstract = {Spatial–temporal event prediction is a particular task for multivariate time series forecasting. Therefore, the complex entangled dynamics of space and time need to be considered. This task is an essential but crucial loop in future smart cities construction, which can be widely applied in urban traffic management, disaster monitoring and mobility analysis. In recent years, video-like spatial–temporal modelling has been the most common approach in many deep learning models. However, the video-like modelling approach cannot consider some latent region-wise correlations other than geographic spatial distance information. To overcome the limitation, we propose a novel neural network framework, Adaptive Dual-View WaveNet (ADVW-Net), for the urban spatial–temporal event prediction. By integrating the spatial representations from Convolutional Neural Network (CNN) and that from adaptive Graph convolutional neural network (GCN), our proposed model can capture not only the geographic correlations but also some latent region-wise dependencies from the input data. In addition, the effective architecture, WaveNet, can be transferred to region-wise spatial–temporal prediction scenarios for long-range temporal dependencies learning. Experimental results on three urban datasets demonstrate the superior performance of our proposed model.}
}
@article{WANG201887,
title = {Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud},
journal = {Decision Support Systems},
volume = {105},
pages = {87-95},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617302130},
author = {Yibo Wang and Wei Xu},
keywords = {Insurance fraud, Fraud detection, Text analytics, Topic modeling, Deep learning},
abstract = {Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection.}
}
@article{DIEZ2021173,
title = {Evaluation of Transport Events with the use of Big Data, Artificial Intelligence and Augmented Reality techniques},
journal = {Transportation Research Procedia},
volume = {58},
pages = {173-180},
year = {2021},
note = {XIV Conference on Transport Engineering, CIT2021},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521007833},
author = {Fernando Pérez Diez and Julià Cabrerizo Sinca and David Roche Vallès and José Magín {Campos Cacheda}},
keywords = {Smart cities, Big Data, Artificial Intelligence, Augmented Reality},
abstract = {The phenomenon of "smart cities" generalizes the use of Information and Communication Technologies. The generation and use of data to manage mobility is a challenge that many cities are betting on and investing in. Through the Internet of all things (IoT) and the use of sensors and mechanisms for capturing information, the number of data analysis tools such as Big Data, Artificial Intelligence (AI), and Augmented Reality (AR) has increased. With the constant use of assisted process learning (Machine Learning), it’s possible to improve event interpretation through the customization of learning protocols. Repetitively trained software can identify relevant events and report changes in critical scenarios that can trigger a series of protocols. The use of artificial intelligence techniques makes it possible to automate monotonous processes and improve transport management. This article analyzes different technologies used to generate transport information and data validation. It is intended to experiment with the use of technologies in the detection of relevant facts, changes of state, and identification of events. It also measures the reliability level when detecting events, and studies the implementation of possible solutions into the transport management system, in order to assist in decision making processes.}
}
@article{MOHANTA2020100227,
title = {Survey on IoT security: Challenges and solution using machine learning, artificial intelligence and blockchain technology},
journal = {Internet of Things},
volume = {11},
pages = {100227},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100227},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520300603},
author = {Bhabendu Kumar Mohanta and Debasish Jena and Utkalika Satapathy and Srikanta Patnaik},
keywords = {IoT, Security, Machine learning, Artificial intelligence, Blockchain technology},
abstract = {Internet of Things (IoT) is one of the most rapidly used technologies in the last decade in various applications. The smart things are connected in wireless or wired for communication, processing, computing, and monitoring different real-time scenarios. The things are heterogeneous and have low memory, less processing power. The implementation of the IoT system comes with security and privacy challenges because traditional based existing security protocols do not suitable for IoT devices. In this survey, the authors initially described an overview of the IoT technology and the area of its application. The primary security issue CIA (confidentially, Integrity, Availability) and layer-wise issues are identified. Then the authors systematically study the three primary technology Machine learning(ML), Artificial intelligence (AI), and Blockchain for addressing the security issue in IoT. In the end, an analysis of this survey, security issues solved by the ML, AI, and Blockchain with research challenges are mention.}
}
@article{WEI2021786,
title = {Reinforcement learning-based QoE-oriented dynamic adaptive streaming framework},
journal = {Information Sciences},
volume = {569},
pages = {786-803},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521004588},
author = {Xuekai Wei and Mingliang Zhou and Sam Kwong and Hui Yuan and Shiqi Wang and Guopu Zhu and Jingchao Cao},
keywords = {MPEG-DASH, Quality of experience, Machine learning, Reinforcement learning},
abstract = {Dynamic adaptive streaming over the HTTP (DASH) standard has been widely adopted by many content providers for online video transmission and greatly improve the performance. Designing an efficient DASH system is challenging because of the inherent large fluctuations characterizing both encoded video sequences and network traces. In this paper, a reinforcement learning (RL)-based DASH technique that addresses user quality of experience (QoE) is constructed. The DASH adaptive bitrate (ABR) selection problem is formulated as a Markov decision process (MDP) problem. Accordingly, an RL-based solution is proposed to solve the MDP problem, in which the DASH clients act as the RL agent, and the network variation constitutes the environment. The proposed user QoE is used as the reward by jointly considering the video quality and buffer status. The goal of the RL algorithm is to select a suitable video quality level for each video segment to maximize the total reward. Then, the proposed RL-based ABR algorithm is embedded in the QoE-oriented DASH framework. Experimental results show that the proposed RL-based ABR algorithm outperforms state-of-the-art schemes in terms of both temporal and visual QoE factors by a noticeable margin while guaranteeing application-level fairness when multiple clients share a bottlenecked network.}
}
@article{KEBANDE2020100122,
title = {Quantifying the need for supervised machine learning in conducting live forensic analysis of emergent configurations (ECO) in IoT environments},
journal = {Forensic Science International: Reports},
volume = {2},
pages = {100122},
year = {2020},
issn = {2665-9107},
doi = {https://doi.org/10.1016/j.fsir.2020.100122},
url = {https://www.sciencedirect.com/science/article/pii/S2665910720300712},
author = {Victor R. Kebande and Richard A. Ikuesan and Nickson M. Karie and Sadi Alawadi and Kim-Kwang Raymond Choo and Arafat Al-Dhaqm},
keywords = {Supervised machine, Learning, Live forensics, Emergent configurations, IoT},
abstract = {Machine learning has been shown as a promising approach to mine larger datasets, such as those that comprise data from a broad range of Internet of Things devices, across complex environment(s) to solve different problems. This paper surveys existing literature on the potential of using supervised classical machine learning techniques, such as K-Nearest Neigbour, Support Vector Machines, Naive Bayes and Random Forest algorithms, in performing live digital forensics for different IoT configurations. There are also a number of challenges associated with the use of machine learning techniques, as discussed in this paper.}
}
@article{USMANYASEEN2022108207,
title = {Cloud based scalable object recognition from video streams using orientation fusion and convolutional neural networks},
journal = {Pattern Recognition},
volume = {121},
pages = {108207},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003885},
author = {Muhammad {Usman Yaseen} and Ashiq Anjum and Giancarlo Fortino and Antonio Liotta and Amir Hussain},
keywords = {Scalable video anaytics, Feature fusion, Object orientation, Object recognition, Convolutional neural networks, Cloud-based video analytics},
abstract = {Object recognition from live video streams comes with numerous challenges such as the variation in illumination conditions and poses. Convolutional neural networks (CNNs) have been widely used to perform intelligent visual object recognition. Yet, CNNs still suffer from severe accuracy degradation, particularly on illumination-variant datasets. To address this problem, we propose a new CNN method based on orientation fusion for visual object recognition. The proposed cloud-based video analytics system pioneers the use of bi-dimensional empirical mode decomposition to split a video frame into intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz transform to produce monogenic object components, which are in turn used for the training of CNNs. Past works have demonstrated how the object orientation component may be used to pursue accuracy levels as high as 93%. Herein we demonstrate how a feature-fusion strategy of the orientation components leads to further improving visual recognition accuracy to 97%. We also assess the scalability of our method, looking at both the number and the size of the video streams under scrutiny. We carry out extensive experimentation on the publicly available Yale dataset, including also a self generated video datasets, finding significant improvements (both in accuracy and scale), in comparison to AlexNet, LeNet and SE-ResNeXt, which are three most commonly used deep learning models for visual object recognition and classification.}
}
@article{SALHAB2021107829,
title = {5G network slices resource orchestration using Machine Learning techniques},
journal = {Computer Networks},
volume = {188},
pages = {107829},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107829},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621000165},
author = {Nazih Salhab and Rami Langar and Rana Rahim},
keywords = {Network slicing, Machine-Learning, Resource orchestration, 5G and beyond, OpenAirInterface OAI},
abstract = {To efficiently serve heterogeneous demands in terms of data rate, reliability, latency and mobility, network operators must optimize the utilization of their infrastructure resources. In this context, we propose a framework to orchestrate resources for 5G networks by leveraging Machine Learning (ML) techniques. We start by classifying the demands for resources into groups in order to adequately serve them by dedicated logical virtual networks or Network Slices (NSs). To optimally implement these heterogeneous NSs that share the same infrastructure, we develop a new dynamic slicing approach of Physical Resource Blocks (PRBs). On first hand, we propose a predictive approach to achieve optimal slicing decisions of the PRBs from a limited resource pool. On second hand, we design an admission controller and a slice scheduler and formalize them as Knapsack problems. Finally, we design an adaptive resource manager by leveraging Deep Reinforcement Learning (DRL). Using our 5G experimental prototype based on OpenAirInterface (OAI), we generate a realistic dataset for evaluating ML based approaches as well as two baselines solutions (i.e. static slicing and uninformed random slicing-decisions). Simulation results show that using regression trees for both classification and prediction, coupled with the DRL-based adaptive resource manager, outperform alternative approaches in terms of prediction accuracy, resource smoothing, system utilization and network throughput.}
}
@article{LIANG2021168,
title = {Explaining the black-box model: A survey of local interpretation methods for deep neural networks},
journal = {Neurocomputing},
volume = {419},
pages = {168-182},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220312716},
author = {Yu Liang and Siguang Li and Chungang Yan and Maozhen Li and Changjun Jiang},
keywords = {Interpretable machine learning, Black-box models, Transparent models, Deep learning, Explainable artificial intelligence},
abstract = {Recently, a significant amount of research has been investigated on interpretation of deep neural networks (DNNs) which are normally processed as black box models. Among the methods that have been developed, local interpretation methods stand out which have the features of clear expression in interpretation and low computation complexity. Different from existing surveys which cover a broad range of methods on interpretation of DNNs, this survey focuses on local interpretation methods with an in-depth analysis of the representative works including the newly proposed approaches. From the perspective of principles, we first divide local interpretation methods into two main categories: model-driven methods and data-driven methods. Then we make a fine-grained distinction between the two types of these methods, and highlight the latest ideas and principles. We further demonstrate the effects of a number of interpretation methods by reproducing the results through open source software plugins. Finally, we point out research directions in this rapidly evolving field.}
}
@article{PINTO2021120725,
title = {Coordinated energy management for a cluster of buildings through deep reinforcement learning},
journal = {Energy},
volume = {229},
pages = {120725},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120725},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221009737},
author = {Giuseppe Pinto and Marco Savino Piscitelli and José Ramón Vázquez-Canteli and Zoltán Nagy and Alfonso Capozzoli},
keywords = {Coordinated energy management, Deep reinforcement learning, Building energy flexibility, Peak demand reduction, Grid interaction},
abstract = {Advanced control strategies can enable energy flexibility in buildings by enhancing on-site renewable energy exploitation and storage operation, significantly reducing both energy costs and emissions. However, when the energy management is faced shifting from a single building to a cluster of buildings, uncoordinated strategies may have negative effects on the grid reliability, causing undesirable new peaks. To overcome these limitations, the paper explores the opportunity to enhance energy flexibility of a cluster of buildings, taking advantage from the mutual collaboration between single buildings by pursuing a coordinated approach in energy management. This is achieved using Deep Reinforcement Learning (DRL), an adaptive model-free control algorithm, employed to manage the thermal storages of a cluster of four buildings equipped with different energy systems. The controller was designed to flatten the cluster load profile while optimizing energy consumption of each building. The coordinated energy management controller is tested and compared against a manually optimised rule-based one. Results shows a reduction of operational costs of about 4%, together with a decrease of peak demand up to 12%. Furthermore, the control strategy allows to reduce the average daily peak and average peak-to-average ratio by 10 and 6% respectively, highlighting the benefits of a coordinated approach.}
}
@article{JIANG2022338,
title = {Will you go where you search? A deep learning framework for estimating user search-and-go behavior},
journal = {Neurocomputing},
volume = {472},
pages = {338-348},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S092523122031496X},
author = {Renhe Jiang and Quanjun Chen and Zekun Cai and Zipei Fan and Xuan Song and Kota Tsubouchi and Ryosuke Shibasaki},
keywords = {User behavior modeling, Recommender system, Location-based services, Ubiquitous and mobile computing, Deep learning},
abstract = {Every day, people are using search engines for different purposes such as research, shopping, or entertainment. Among the behaviors of search engine users, we are particularly interested in search-and-go behavior, which intuitively corresponds to a simple but challenging question, i.e., will users go where they search? Accurately estimating such behavior can be of great importance for Internet companies to recommend point-of-interest (POI), advertisement, and route, as well as for governments and public service operators like metro companies to conduct traffic monitoring, crowd management, and transportation scheduling. Therefore, in this study, we first collect search log data and GPS log data with linked and consistent user ID from Yahoo! Japan portal application installed in millions of smart-phones and tablets. Then we propose a framework including a complete data-processing procedure and an end-to-end deep learning model to predict whether a user will check-in the searched place or not. Specifically, as users’ daily activities are considered to have high correlation with their travel, eating, and recreation decision in the future (i.e., go or not), Deep Spatial–Temporal Interaction Network (DeepSTIN) is elaborately designed to automatically learn the sophisticated spatiotemporal interactions between mobility data and search query data. Experimental results based on the standard metrics demonstrate that our proposed framework can achieve satisfactory performances on multiple real-world search scenarios.}
}
@article{CHEN2021107609,
title = {A generic shift-norm-activation approach for deep learning},
journal = {Pattern Recognition},
volume = {109},
pages = {107609},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107609},
url = {https://www.sciencedirect.com/science/article/pii/S003132032030412X},
author = {Zhi Chen and Pin-Han Ho},
keywords = {Activation, Normalization, CNN, Shifting, Deep learning},
abstract = {Deep learning has received increasing attention in the last decade. Its amazing success, is partly attributed to the evolution of normalization and activation techniques. However, less works have devoted to explore both modules together. This work, therefore, aims at pushing for a deeper understanding on the effect of normalization and activation together analytically. We design a generic method which integrates both normalization and activation together as a whole, named as the Generic Shift-Normalization-Activation Approach (GSNA), in reserving richer information propagation in neural networks. A rigorous mathematical analysis was performed to investigate the benefits of the designed method, such as its computation complexity, performance potential as well as optimization over trainable parameter initialization. Further, extensive experiments are conducted to demonstrate the superiority and generality of the designed method in many computer vision benchmarking tasks, such as CIFAR-10/100, SVHN, ImageNet32 × 32, etc. To explore its generality, we also conduct some experiments on natural language understanding tasks like text classification, natural language inference, and some variational generative task as well. More interestingly, GSNA can be naturally incorporated into the existing neural networks with arbitrary architectures, demonstrating its generic effectiveness in deep learning field.}
}
@article{XU20201,
title = {Deep learning based emotion analysis of microblog texts},
journal = {Information Fusion},
volume = {64},
pages = {1-11},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S156625352030302X},
author = {Dongliang Xu and Zhihong Tian and Rufeng Lai and Xiangtao Kong and Zhiyuan Tan and Wei Shi},
keywords = {Microblog short text, Emotional analysis, Convolutional neural network, Word2vec},
abstract = {Traditional text emotion analysis methods are primarily devoted to studying extended texts, such as news reports and full-length documents. Microblogs are considered short texts that are often characterized by large noises, new words, and abbreviations. Previous emotion classification methods usually fail to extract significant features and achieve poor classification effect when applied to processing of short texts or micro-texts. This study proposes a microblog emotion classification model, namely, CNN_Text_Word2vec, on the basis of convolutional neural network (CNN) to solve the above-mentioned problems. CNN_Text_Word2vec introduces a word2vec neural network model to train distributed word embeddings on every single word. The trained word vectors are used as input features for the model to learn microblog text features through parallel convolution layers with multiple convolution kernels of different sizes. Experiment results show that the overall accuracy rate of CNN_Text_Word2vec is 7.0% higher than that achieved by current mainstream methods, such as SVM, LSTM and RNN. Moreover, this study explores the impact of different semantic units on the accuracy of CNN_Text_Word2vec, specifically in processing of Chinese texts. The experimental results show that comparing to using feature vectors obtained from training words, feature vector obtained from training Chinese characters yields a better performance.}
}
@article{YU2021114663,
title = {Reinforcement learning approach for resource allocation in humanitarian logistics},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114663},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114663},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421001044},
author = {Lina Yu and Canrong Zhang and Jingyan Jiang and Huasheng Yang and Huayan Shang},
keywords = {Humanitarian logistics, Resource allocation, Reinforcement learning, Q-learning},
abstract = {When a disaster strikes, it is important to allocate limited disaster relief resources to those in need. This paper considers the allocation of resources in humanitarian logistics using three critical performance indicators: efficiency, effectiveness and equity. Three separate costs are considered to represent these metrics, namely, the accessibility-based delivery cost, the starting state-based deprivation cost, and the terminal penalty cost. A mixed-integer nonlinear programming model with multiple objectives and multiple periods is proposed. A Q-learning algorithm, a type of reinforcement learning method, is developed to address the complex optimization problem. The principles of the proposed algorithm, including the learning agent and its actions, the environment and its states, and reward functions, are presented in detail. The parameter settings of the proposed algorithm are also discussed in the experimental section. In addition, the solution quality of the proposed algorithm is compared with that of the exact dynamic programming method and a heuristic algorithm. The experimental results show that the efficiency of the algorithm is better than that of the dynamic programming method and the accuracy of the algorithm is higher than that of the heuristic algorithm. Moreover, the Q-learning algorithm provides close to or even optimal solutions to the resource allocation problem by adjusting the value of the training episode K in practical applications.}
}
@article{MICOLPOLICARPO2021100414,
title = {Machine learning through the lens of e-commerce initiatives: An up-to-date systematic literature review},
journal = {Computer Science Review},
volume = {41},
pages = {100414},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100414},
url = {https://www.sciencedirect.com/science/article/pii/S157401372100054X},
author = {Lucas {Micol Policarpo} and Diórgenes Eugênio {da Silveira} and Rodrigo {da Rosa Righi} and Rodolfo {Antunes Stoffel} and Cristiano André {da Costa} and Jorge Luis {Victória Barbosa} and Rodrigo Scorsatto and Tanuj Arcot},
keywords = {E-commerce, User behavior, Machine learning, Conversion rate},
abstract = {E-commerce platforms are a primary place for people to find, compare, and ultimately purchase products. They employ Machine Learning (ML), Business Intelligence (BI), mathematical formalism, and artificial intelligence (AI) to generate valuable knowledge about customer behavior, bringing benefits for both customers themselves and sellers. The state-of-the-art in this area does not include a comprehensive and up-to-date survey that explores the most common goals of e-commerce-related studies and the suitable ML techniques and frameworks for particular cases. In this context, we introduce a systematic literature review that revisits recent initiatives to employ ML techniques on different e-commerce scenarios. The contributions to the state-of-the-art are twofold: (i) a comprehensive review of ML methods and their relationship with the target goals of e-commerce platforms, including impact on profit growth; (ii) a novel taxonomy to reorganize ML-based e-commerce initiatives, which helps researchers to compare and classify efforts in this evolving area. This comprehensive literature review enables researchers and e-commerce administrators to conduct innovation projects better and redirect budget and human resource efforts.}
}
@article{PARK2021107744,
title = {Comparative analysis on predictability of natural ventilation rate based on machine learning algorithms},
journal = {Building and Environment},
volume = {195},
pages = {107744},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107744},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321001529},
author = {Hansaem Park and Dong Yoon Park},
keywords = {Natural ventilation rate, Predictive model, Machine learning, Shapley additive explanation (SHAP), Field measurement},
abstract = {The demand for efficient natural ventilation (NV) systems has increased for the development of sustainable buildings. However, the uncertainty of NV remains a challenging issue for appropriate utilization strategies of NV. For the successful implementation of NV systems in buildings, it is essential to clarify when and how to use NV systems in advance. In order to achieve the objectives, this study investigated the predictive models of NV rate (NVR) through eight machine learning (ML) algorithms, which are suitable for the interpretation of non-linear relationships between the measured indoor and outdoor environmental variables. Among all of the algorithms, deep neural network (DNN) ensured the best prediction performance for the NVR and it was shown that 40%, 46%, and 38% better predictive performance in terms of mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE) than multivariate linear regression (MLR), which had the highest error rate, respectively. Based on the Shapley additive explanation (SHAP), the most influential features that affected to results of predictive models were examined and most of the ML approaches, except for MLR, had similar features (the pressure difference, outdoor temperature, wind speed, indoor relative humidity, solar radiation, the difference of indoor/outdoor relative humidity, and wind direction). The results of this study can improve the prediction performance for NVR, and this would contribute to the development of an intelligent NV system. Future work needs to develop the optimal operating strategies for hybrid ventilation systems integrating NV and mechanical systems.}
}
@article{KORONIOTIS202091,
title = {A new network forensic framework based on deep learning for Internet of Things networks: A particle deep framework},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {91-106},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19325105},
author = {Nickolaos Koroniotis and Nour Moustafa and Elena Sitnikova},
keywords = {Network forensics, Threat detection, Attack tracing, Deep learning, Particle swarm optimization},
abstract = {With the prevalence of Internet of Things (IoT) systems, inconspicuous everyday household devices are connected to the Internet, providing automation and real-time services to their users. In spite of their light-weight design and low power, their vulnerabilities often give rise to cyber risks that harm their operations over network systems. One of the key challenges of securing IoT networks is tracing sources of cyber-attack events, along with obfuscating and encrypting network traffic. This study proposes a new network forensics framework , called a Particle Deep Framework (PDF), which describes the digital investigation phases for identifying and tracing attack behaviors in IoT networks. The proposed framework includes three new functions: (1) extracting network data flows and verifying their integrity to deal with encrypted networks; (2) utilizing a Particle Swarm Optimization (PSO) algorithm to automatically adapt parameters of deep learning; and (3) developing a Deep Neural Network (DNN) based on the PSO algorithm to discover and trace abnormal events from IoT network of smart homes. The proposed PDF is evaluated using the Bot-IoT and UNSW_NB15 datasets and compared with various deep learning techniques. Experimental results reveal a high performance of the proposed framework for discovering and tracing cyber-attack events compared with the other techniques.}
}
@article{KONSTANTAKOPOULOS2019810,
title = {A deep learning and gamification approach to improving human-building interaction and energy efficiency in smart infrastructure},
journal = {Applied Energy},
volume = {237},
pages = {810-821},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.12.065},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918318841},
author = {Ioannis C. Konstantakopoulos and Andrew R. Barkan and Shiying He and Tanya Veeravalli and Huihan Liu and Costas Spanos},
keywords = {Artificial intelligence for humans-in-the-loop cyber-physical systems, Human-building interaction, Deep learning, Discrete choice models, Game theory},
abstract = {In this paper, we propose a gamification approach as a novel framework for smart building infrastructure with the goal of motivating human occupants to consider personal energy usage and to have positive effects on their environment. Human interaction in the context of cyber-physical systems is a core component and consideration in the implementation of any smart building technology. Research has shown that the adoption of human-centric building services and amenities leads to improvements in the operational efficiency of these cyber-physical systems directed toward controlling building energy usage. We introduce a strategy that incorporates humans-in-the-loop modeling by creating an interface to allow building managers to interact with occupants and potentially incentivize energy efficient behavior. Game theoretic analysis typically relies on the assumption that the utility function of each individual agent is known a priori. Instead, we propose a novel benchmark utility learning framework that employs robust estimations of occupant actions toward energy efficiency. To improve forecasting performance, we extend the benchmark utility learning scheme by leveraging Deep Learning end-to-end training with deep bi-directional Recurrent Neural Networks. We apply the proposed methods to high-dimensional data from a social game experiment designed to encourage energy efficient behavior among smart building occupants. Using data gathered from occupant actions for resources such as room lighting, we forecast patterns of resource usage to demonstrate the performance of the proposed methods on ground truth data. The results of our study show that we can achieve a highly accurate representation of the ground truth for occupant resource usage. For demonstrations of our infrastructure and for downloading de-identified, high-dimensional data sets, please visit our website (smartNTU demo web portal: https://smartntu.eecs.berkeley.edu)}
}
@article{GHOSH2021103189,
title = {Artificial intelligence and internet of things in screening and management of autism spectrum disorder},
journal = {Sustainable Cities and Society},
volume = {74},
pages = {103189},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103189},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721004674},
author = {Tapotosh Ghosh and Md. Hasan Al Banna and Md. Sazzadur Rahman and M. Shamim Kaiser and Mufti Mahmud and A. S. M. Sanwar Hosen and Gi Hwan Cho},
keywords = {Autism, Smart city, Machine learning, Internet of things, Artificial intelligence, Monitoring},
abstract = {Autism is a disability that obstructs the process of a person’s development. Autistic individuals find it extremely difficult to cope with the world’s pace, can not communicate properly, and unable to express their feelings appropriately. Artificial Intelligence (AI), Machine Learning (ML), and Internet of Things (IoT) are used in several medical applications, and autistic individuals can be assisted using the proper use of automated systems. In this paper, some of the research works in the field of application of AI, ML, and IoT in autism were reviewed. State-of-the-art articles were collected and around 58 articles were selected which have significant contribution in this field. The selected research works were analyzed, represented, and compared. Finally, incorporation of the autism facilities in smart city environment is described, some research gaps and challenges were pointed out, and recommendations were provided for further research work.}
}
@article{ALWAN2022101951,
title = {Data quality challenges in large-scale cyber-physical systems: A systematic review},
journal = {Information Systems},
volume = {105},
pages = {101951},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101951},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001484},
author = {Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin},
keywords = {Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations},
abstract = {Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.}
}
@article{WU2021212,
title = {Distributed reinforcement learning algorithm of operator service slice competition prediction based on zero-sum markov game},
journal = {Neurocomputing},
volume = {439},
pages = {212-222},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.061},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221000898},
author = {Guomin Wu and Guoping Tan and Jinxin Deng and Defu Jiang},
keywords = {Network slicing, Operator competition, Multi-agent, Zero-sum markov game, Reinforcement learning},
abstract = {As a key enabling technology in the emerging network, network slicing can dynamically provide on-demand service with distinct logical slice instance. While most related studies have mainly focused on resource management, this study targets solving business competition between two operator slices using artificial intelligence. In this competition, each operator slice tries to maximize its own payoff, meanwhile its opponent strives to minimize it. Moreover, two operators update their marketing strategies over time. Therefore, predicting its result is a challenge. After the zero-sum Markov game is modeled for the research problem, we present the min–max Q learning algorithm. In each market state, each slice attains its temporary optimal strategy using the min–max algorithm. In the Markov decision process, Q value is dynamically modified under different market states, and the final Q value presents predictive result for this competition. Finally, a mass of numerical results prove that the min–max Q learning algorithm outperforms the repeated game, in which market state is invariable over time.}
}
@article{SHADROO2021107684,
title = {The two-phase scheduling based on deep learning in the Internet of Things},
journal = {Computer Networks},
volume = {185},
pages = {107684},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107684},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620312925},
author = {Shabnam Shadroo and Amir Masoud Rahmani and Ali Rezaee},
keywords = {Fog-cloud computing, Autoencoder, Task scheduling},
abstract = {With the growth of data generation speed and its huge volume, the cloud-based infrastructure alone does not meet the needs of the Internet of Things (IoT), thereby leading to inefficiency. By combining fog and cloud computing on the IoT, some processing occurs near the data generation location with a higher speed and without needing a large bandwidth. Cloud and fog computing requires proper management in scheduling, increasing resource efficiency, and reducing consumption costs. In this article, a two-phase scheduling algorithm is presented based on deep learning methods in the field of IoT. The first phase is to decide on the location of the task execution using the clustering method. The second phase involves scheduling the task according to the execution location. In the clustering section, three ideas based on the Self-Organizing Map (SOM) clustering method have been proposed. In the first and second ideas, the SOM and hierarchical SOM are used to cluster the features of the tasks received from the IoT layer. In the third idea, the feature is extracted, and its dimensions are reduced using the Autoencoder, which is one of the deep learning methods, after which clustering is done. After scheduling each cluster's tasks, comparing the methods presented in the article, it is shown that the feature extraction using deep learning can improve clustering in such a way to reduce the missed rate of tasks in the cloud and fog, as well as their costs.}
}
@article{WANG2021106883,
title = {Distributed machine learning for energy trading in electric distribution system of the future},
journal = {The Electricity Journal},
volume = {34},
number = {1},
pages = {106883},
year = {2021},
note = {Special Issue: Machine Learning Applications To Power System Planning And Operation},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2020.106883},
url = {https://www.sciencedirect.com/science/article/pii/S1040619020301755},
author = {Ning Wang and Jie Li and Shen-Shyang Ho and Chenxi Qiu},
keywords = {Smart grid, Machine learning, Energy trading, DC grid cell},
abstract = {Machine Learning (ML) has seen a great potential to solve many power system problems along with its transition into Smart Grid. Specifically, electric distribution systems have witnessed a rapid integration of distributed energy resources (DERs), including photovoltaic (PV) panels, electric vehicles (EV), and smart appliances, etc. Electricity consumers, equipped with such DERs and advanced metering/sensing/computing devices, are becoming self-interested prosumers who can behave more actively for their electric energy consumption. In this paper, the potential of distributed ML in solving the energy trading problem among prosumers of a future electric distribution system - building DC grid cell, is explored, while considering the limited computation, communication, and data privacy issues of the edge entities. A fully distributed energy trading framework based on ML is proposed to optimize the load and price prediction accuracy and energy trading efficiency. Computation resource allocation, communication schemes, ML task scheduling, as well as user sensitive data preserving issues in the distributed ML framework are addressed with consideration of all the economic and physical constraints of the electric distribution systems.}
}
@article{LIU2020102741,
title = {An improved deep learning model for predicting stock market price time series},
journal = {Digital Signal Processing},
volume = {102},
pages = {102741},
year = {2020},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2020.102741},
url = {https://www.sciencedirect.com/science/article/pii/S1051200420300865},
author = {Hui Liu and Zhihao Long},
keywords = {Stock market price, Deep learning network, Time series forecasting, Data processing, Error correction},
abstract = {As an important component of the economic market, the stock market has been concerned by many researchers. How to get the trend of the stock market and predict the stock price is a problem that many researchers are studying. In previous works, the prediction methods are mainly focused on statistical models and traditional neural network models which are relatively popular in recent years. Deep learning is not often used in the field of financial time series, but it has a strong learning ability and is suitable for complex time series such as financial time series. In particular, the LSTM network has the function of long-term memory because of its cyclic structure, so it is very suitable for financial time series prediction in theory. In the study, a novel stock closing price forecasting framework is proposed, which has a higher prediction than traditional models. The data processing part, the deep learning predictor part, and the predictor optimization method are the components of this deep hybrid framework. Data processing includes empirical wavelet transform (EWT) based preprocessing and outlier robust extreme learning machine (ORELM) model based post-processing. Long short-term memory (LSTM) network based deep learning network predictor, as the main part of the mixed frame, is jointly optimized by dropout strategy and particle swarm optimization (PSO) algorithm. Each algorithm in the hybrid framework can give full play to its own functions to achieve better prediction accuracy. In order to verify the performance of the model, three challenging datasets are selected for forecasting experiments. Some comparative models are also selected to prove the effectiveness of the proposed framework. Experimental results show that the hybrid framework proposed in the study has the best prediction accuracy and can be applied to stock market monitoring or financial data analysis and research.}
}
@article{YANG2020105817,
title = {Adaptive autonomous UAV scouting for rice lodging assessment using edge computing with deep learning EDANet},
journal = {Computers and Electronics in Agriculture},
volume = {179},
pages = {105817},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105817},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920319542},
author = {Ming-Der Yang and Jayson G. Boubin and Hui Ping Tsai and Hsin-Hung Tseng and Yu-Chun Hsu and Christopher C. Stewart},
keywords = {Autonomous UAV, Deep learning, Edge computing, Rice lodging, Adaptive},
abstract = {Rice is a globally important crop that will continue to play an essential role in feeding our world as we grapple with climate change and population growth. Lodging is a primary threat to rice production, decreasing rice yield, and quality. Lodging assessment is a tedious task and requires heavy labor and a long duration due to the vast land areas involved. Newly developed autonomous crop scouting techniques have shown promise in mapping crop fields without any human interaction. By combining autonomous scouting and lodged rice detection with edge computing, it is possible to estimate rice lodging faster and at a much lower cost than previous methods. This study presents an adaptive crop scouting mechanism for Autonomous Unmanned Aerial Vehicles (UAV). We simulate UAV crop scouting of rice fields at multiple levels using deep neural networks and real UAV energy profiles, focusing on areas with high lodging. Using the proposed method, we can scout rice fields 36% faster than conventional scouting methods at 99.25% accuracy.}
}
@article{LI2018127,
title = {Improving Resolution of 3D Surface With Convolutional Neural Networks},
journal = {Sustainable Cities and Society},
volume = {42},
pages = {127-138},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S2210670718303676},
author = {Zhen Li and Xiaomin Yang and Jianwen Song and Kai Liu and Zuping Wang and Wei Wu},
keywords = {3D surface imaging, Convolutional neural networks, Phase measuring profilometry, Super-Resolution},
abstract = {High-resolution (HR) 3D point cloud is always desired for smart city. Phase measuring profilometry (PMP) has widely used to generate 3D point cloud. However, due to the limitation of hardware, PMP is usually difficult to obtain HR 3D point cloud. This inspires us to exploit low-resolution (LR) pattern images or LR phase image to generate HR 3D point cloud. Specifically, we attempt to solve this problem using deep learning based super-resolution (SR) methods. We formulate a new deep learning based SR method for 3D point cloud. We show that the proposed SR surely improves the resolution of the reconstructed 3D point cloud. In experiments, we compare the proposed SR with other state-of-the-art SR methods, proving that the proposed SR can yield better quality of the reconstructed 3D point cloud and lower computational cost.}
}
@article{SHAPI2021100037,
title = {Energy consumption prediction by using machine learning for smart building: Case study in Malaysia},
journal = {Developments in the Built Environment},
volume = {5},
pages = {100037},
year = {2021},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2020.100037},
url = {https://www.sciencedirect.com/science/article/pii/S266616592030034X},
author = {Mel Keytingan M. Shapi and Nor Azuana Ramli and Lilik J. Awalin},
keywords = {Building energy management system, Machine learning, Microsoft Azure, Energy consumption, Prediction},
abstract = {Building Energy Management System (BEMS) has been a substantial topic nowadays due to its importance in reducing energy wastage. However, the performance of one of BEMS applications which is energy consumption prediction has been stagnant due to problems such as low prediction accuracy. Thus, this research aims to address the problems by developing a predictive model for energy consumption in Microsoft Azure cloud-based machine learning platform. Three methodologies which are Support Vector Machine, Artificial Neural Network, and k-Nearest Neighbour are proposed for the algorithm of the predictive model. Focusing on real-life application in Malaysia, two tenants from a commercial building are taken as a case study. The data collected is analysed and pre-processed before it is used for model training and testing. The performance of each of the methods is compared based on RMSE, NRMSE, and MAPE metrics. The experimentation shows that each tenant’s energy consumption has different distribution characteristics.}
}
@article{ZHANG2019112199,
title = {Deep reinforcement learning–based approach for optimizing energy conversion in integrated electrical and heating system with renewable energy},
journal = {Energy Conversion and Management},
volume = {202},
pages = {112199},
year = {2019},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2019.112199},
url = {https://www.sciencedirect.com/science/article/pii/S0196890419312051},
author = {Bin Zhang and Weihao Hu and Di Cao and Qi Huang and Zhe Chen and Frede Blaabjerg},
keywords = {Integrated energy system, Dynamic energy conversion, Artificial intelligence, Deep reinforcement learning, Proximal policy optimization},
abstract = {With advanced information technologies applied in integrated energy systems (IESs), controlling the energy conversion has become an effective method for improving grid flexibility and reducing the operating cost of IESs. This study proposes a dynamic energy conversion strategy for the energy management of an IES with renewable energy, which considers the system operator’s (SO) operating cost. Deep reinforcement learning (DRL) is used to illustrate the hierarchical decision-making process, in which the dynamic energy conversion problem is formulated as a discrete finite Markov decision process, and proximal policy optimization (PPO) is adopted to solve the decision-making problem. Using DRL, the SO can adaptively decide the wind power conversion ratio during the online learning process, where the uncertainties of customers’ load demand profiles, flexibility of spot electricity prices, and wind power generation are addressed. Simulations show that the proposed PPO-based renewable energy conversion algorithm can effectively reduce the SO’s operating cost.}
}
@article{ASSIS2021102942,
title = {A GRU deep learning system against attacks in software defined networks},
journal = {Journal of Network and Computer Applications},
volume = {177},
pages = {102942},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102942},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520304008},
author = {Marcos V.O. Assis and Luiz F. Carvalho and Jaime Lloret and Mario L. Proença},
keywords = {Gated recurrent units, SDN, Deep learning, DDoS, Intrusion detection},
abstract = {The management of modern network environments is becoming more and more complex due to new requirements of devices' heterogeneity regarding the popularization of the Internet of Things (IoT), as well as the dynamic traffic required by next-generation applications and services. To address this problem, Software-defined Networking (SDN) emerges as a management paradigm able to handle these problems through a centralized high-level network approach. However, this centralized characteristic also creates a critical failure spot since the central controller may be targeted by malicious users aiming to impair the network operation. This paper proposes an SDN defense system based on the analysis of single IP flow records, which uses the Gated Recurrent Units (GRU) deep learning method to detect DDoS and intrusion attacks. This direct flow inspection enables faster mitigation responses, minimizing the attack's impact over the SDN. The proposed model is tested against several different machine learning approaches over two public datasets, the CICDDoS 2019 and the CICIDS 2018. Furthermore, a lightweight mitigation approach is presented and evaluated through performance tests regarding each detection method. Finally, a feasibility test is performed regarding the throughput of flows per second that each detection method can analyze. This test is accomplished through the use of real IP Flow data collected at a large-scale network. The results point out promising detection rates and an elevated amount of analyzed flows per second, which makes GRU a feasible approach for the proposed system.}
}
@article{QIAN2022102680,
title = {Deep Roof Refiner: A detail-oriented deep learning network for refined delineation of roof structure lines using satellite imagery},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {107},
pages = {102680},
year = {2022},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2022.102680},
url = {https://www.sciencedirect.com/science/article/pii/S030324342200006X},
author = {Zhen Qian and Min Chen and Teng Zhong and Fan Zhang and Rui Zhu and Zhixin Zhang and Kai Zhang and Zhuo Sun and Guonian Lü},
keywords = {Deep learning, Roof Structure Lines, Satellite Imagery, Fine-grained Geospatial Data},
abstract = {Urban research is progressively moving towards fine-grained simulation and requires more granular and accurate geospatial data. In comparison to building footprints, roof structure lines (RSLs) are finer-grained elements of building roofs that provide a more sophisticated data reference. However, generating high-quality and up-to-date RSLs is arduous owing to the high expense of data sources (e.g., digital surface models and light detection and ranging data) and the low robustness of conventional image processing approaches. While the current combination of high-resolution satellite imagery and deep learning methods enables the automatic generation of RSLs, it also introduces two distinct challenges. First, the high diversity of roof sizes, forms, and spatial distribution complicates the extraction of essential RSL features from satellite imagery using general deep learning methods. Second, the significant class imbalance issue between foreground objects (i.e., RSLs) and background context in satellite imagery makes it difficult for deep learning methods to concentrate on RSL locations. To overcome these challenges and effectively delineate RSLs from satellite imagery, this study designs Deep Roof Refiner—an end-to-end and detail-oriented deep learning network and proposes a synthetic strategy to enhance the network’s performance. The effectiveness of the proposed network is verified by quantitative and qualitative experiments, with the optimal dataset scale F1-score and optimal image scale F1-score of 60.89% and 63.48%, respectively. The proposed network significantly outperforms state-of-the-art deep learning methods and associated conventional research. The results indicate that the delineated RSLs can serve as a reliable data source for some urban building-based studies.}
}
@article{MIGLANI2019100184,
title = {Deep learning models for traffic flow prediction in autonomous vehicles: A review, solutions, and challenges},
journal = {Vehicular Communications},
volume = {20},
pages = {100184},
year = {2019},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2019.100184},
url = {https://www.sciencedirect.com/science/article/pii/S2214209619302311},
author = {Arzoo Miglani and Neeraj Kumar},
keywords = {Cognitive Internet of Things, Traffic flow prediction, Machine learning, Deep learning, Autonomous vehicles},
abstract = {In the last few years, there has been an exponential increase in the usage of the autonomous vehicles across the globe. It is due to an exponential increase in the popularity and usage of the artificial intelligence techniques in various applications. Traffic flow predication is important for autonomous vehicles using which they decide their itinerary and take adaptive decisions (for example, turn let or right, move straight, lane change, stop, or accelerate) with respect to their surrounding objects. From the existing literature, it has been observed that research on autonomous vehicles has shifted from the traditional statistical models to adaptive machine learning techniques. However, existing machine learning models may not be directly applicable in this environment due to non-linear complex relationship between spatial and temporal data collected from the surroundings during the aforementioned adaptive decisions taken by the vehicles. So, with focus on these issues, in this article, we explore various deep learning models for traffic flow prediction in autonomous vehicles and compared these models with respect to their applicability in modern smart transportation systems. Various parameters are chosen to have a relative comparison among different deep learning models. Moreover, challenges and future research directions are also discussed in the article.}
}
@article{LU2021103349,
title = {Green energy harvesting strategies on edge-based urban computing in sustainable internet of things},
journal = {Sustainable Cities and Society},
volume = {75},
pages = {103349},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103349},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721006247},
author = {Man Lu and Guifang Fu and Nisreen Beshir Osman and Usama Konbr},
keywords = {Sustainable smart cities, Intelligent urban computing, Energy harvesting management, Internet of Things (IoT)},
abstract = {The development of intelligent urban computing has carried out different critical smart ecological and energy harvesting topics such as intelligent cities and societies. The main concept of the sustainable smart city is established for collecting and negotiation of the Internet of things (IoT) devices and smart applications to help and enhancement of human life. On the other hand, energy harvesting management as an important issue in sustainable urban computing has several challenges arising from the exponential growth of IoT devices, smart applications, and complex population to decrease the related factors on the energy consumption, power-saving, and environmental waste management. However, the notion of energy harvesting management for sustainable urban computing is still growing exponentially, and it should be attended to due to economic factors and governmental obstacles. This paper reviews different existing green energy harvesting strategies on the smart applications of sustainable and smart cities in edge-based intelligent urban computing. The existing energy harvesting strategies have been divided into five categories: smart home management, smart cities, smart grids, smart environmental systems, and smart transportation systems. This review aims to classify technical aspects of energy harvesting management methods in sustainable urban computing concerning applied algorithms, evaluation factors, and evaluation environments. As technical results, a general discussion is essential for the development of existing challenges and open research directions of energy harvesting and waste management for sustainable smart cities that contribute expressively to enhance the energy consumption of smart applications and human life in complex and metropolitan areas.}
}
@article{YU2021103795,
title = {Development of tourism resources based on fpga microprocessor and convolutional neural network},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103795},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103795},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120309406},
author = {Huixia Yu},
keywords = {Inbound tourism development, Fpga, Feature extraction, Machine learning, Real-time classification, tourism demand prediction, convolutional neural network (cnn)},
abstract = {Accuracy of inbound tourism demand, it is important for the development and implementation of inbound tourism strategy. Use the FPGA, as in the common traditional mechanical, convolution neural network, learn how they are wide, it is being used to traffic demand forecasting model. This over fitting, the difficulty of the parameters, as such convolution neural network, drawbacks that receive, and due to the minimum number of problem set. The proposed system has a combination of neural networks and FPGA convolution to establish tourism demand forecasting neural network. The most advanced methods for the country's tourism research for traditional statistical methods, predictive models and artificial intelligence soft computing techniques. Please improve tourism research and modelling method introduces significant prediction accuracy of artificial intelligence methods. The results showed that, compared with, including FPGA traditional neural network neural network convolution essence of traditional statistical methods and methods can improve the prediction accuracy. This approach provides a more accurate forecast travel demand model is a better choice. Develop and compare various classifiers based on convolutional neural networks (CNN) and long short-term memory networks (LSTM). These classifiers were trained and validated with data from hotels located on the island of Tenerife. An analysis of our findings shows that the most accurate and robust estimators are those based on LSTM recurrent neural networks.}
}
@article{RAHMAN2020,
title = {Intelligent waste management system using deep learning with IoT},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820304547},
author = {Md. Wahidur Rahman and Rahabul Islam and Arafat Hasan and Nasima Islam Bithi and Md. Mahmodul Hasan and Mohammad Motiur Rahman},
keywords = {Deep Learning, Internet of things (IoT), System Usability Scale (SUS), Waste classification},
abstract = {Waste management leads to the demolition of waste conducted by recycling and landfilling. Deep learning and the Internet of things (IoT) confer an agile solution in classification and real-time data monitoring, respectively. This paper reflects a capable architecture of the waste management system based on deep learning and IoT. The proposed model renders an astute way to sort digestible and indigestible waste using a convolutional neural network (CNN), a popular deep learning paradigm. The scheme also introduces an architectural design of a smart trash bin that utilizes a microcontroller with multiple sensors. The proposed method employs IoT and Bluetooth connectivity for data monitoring. IoT enables control of real-time data from anywhere while Bluetooth aids short-range data monitoring through an android application. To examine the efficacy of the developed model, the accuracy of waste label classification, sensors data estimation, and system usability scale (SUS) are enumerated and interpreted. The classification accuracy of the proposed architecture based on the CNN model is 95.3125%, and the SUS score is 86%. However, this smart system will be adjustable to household activities with real-time waste monitoring.}
}
@article{ALI2021852,
title = {Exploiting dynamic spatio-temporal correlations for citywide traffic flow prediction using attention based neural networks},
journal = {Information Sciences},
volume = {577},
pages = {852-870},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521008483},
author = {Ahmad Ali and Yanmin Zhu and Muhammad Zakarya},
keywords = {Predictions of traffic flows, Citywide traffic flows, Neural networks, Attention, Spatio-temporal correlations},
abstract = {For intelligent transportation systems (ITS), predicting urban traffic crowd flows is of great importance. However, it is challenging to represent various complex spatial relationships across distinct regions, as well as dynamic temporal relations among various time periods. To overcome this challenge, we propose DHSTNet, a novel deep Spatio-temporal neural network for predicting traffic crowd flows. Our proposed approach consists of four components: (i) the closeness component considers spontaneous changes of the traffic crowd flows; (ii) the period influence component frequently characterizes variations of daily flows; (iii) the weekly influence component characterizes the weekly arrangements of crowd flows; and (iv) the external branch component identifies various external influences. Our model applies diverse weights to individual branches. Then, it fuses the outcomes of the four features. Extensive experiments on two real-world datasets demonstrate the advantage of the proposed model over the compared baseline methods. Moreover, to verify the generalization of the proposed model, we apply the proposed attention-based mechanism with a previously proposed model, resulting in a hybrid approach known as Att-DHSTNet, to forecast short-term crowd flows. Experimental results also confirm improved performance.}
}
@article{MAJIDIFARD2020118513,
title = {Deep machine learning approach to develop a new asphalt pavement condition index},
journal = {Construction and Building Materials},
volume = {247},
pages = {118513},
year = {2020},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2020.118513},
url = {https://www.sciencedirect.com/science/article/pii/S0950061820305183},
author = {Hamed Majidifard and Yaw Adu-Gyamfi and William G. Buttlar},
keywords = {Pavement monitoring, Pavement distresses detection, Deep learning, Google API, Machine learning, Pavement condition prediction, YOLO, Image processing},
abstract = {Pavement condition assessment provides information to make more cost-effective and consistent decisions regarding management of pavement network. Generally, pavement distress inspections are performed using sophisticated data collection vehicles and/or foot-on-ground surveys. In either approach, the process of distress detection is human-dependent, expensive, inefficient, and/or unsafe. Automated pavement distress detection via road images is still a challenging issue among pavement researchers and computer-vision community. In recent years, advancement in deep learning has enabled researchers to develop robust tools for analyzing pavement images at unprecedented accuracies. Nevertheless, deep learning models necessitate a big ground truth dataset, which is often not readily accessible for pavement field. In this study, we reviewed our previous study, which a labeled pavement dataset was presented as the first step towards a more robust, easy-to-deploy pavement condition assessment system. In total, 7237 google street-view images were extracted, manually annotated for classification (nine categories of distress classes). Afterward, YOLO (you look only once) deep learning framework was implemented to train the model using the labeled dataset. In the current study, a U-net based model is developed to quantify the severity of the distresses, and finally, a hybrid model is developed by integrating the YOLO and U-net model to classify the distresses and quantify their severity simultaneously. Various pavement condition indices are developed by implementing various machine learning algorithms using the YOLO deep learning framework for distress classification and U-net for segmentation and distress densification. The output of the distress classification and segmentation models are used to develop a comprehensive pavement condition tool which rates each pavement image according to the type and severity of distress extracted. As a result, we are able to avoid over-dependence on human judgement throughout the pavement condition evaluation process. The outcome of this study could be conveniently employed to evaluate the pavement conditions during its service life and help to make valid decisions for rehabilitation or reconstruction of the roads at the right time.}
}
@article{WU2022108716,
title = {Online multimedia traffic classification from the QoS perspective using deep learning},
journal = {Computer Networks},
volume = {204},
pages = {108716},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108716},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621005715},
author = {Zheng Wu and Yu-ning Dong and Xiaohui Qiu and Jiong Jin},
keywords = {Multimedia traffic classification, Convolutional Neural Network, Class incremental learning, Sliding window},
abstract = {In the next generation of communication systems, data traffic is expected to increase dramatically and continuously. Particularly for multimedia traffic, it has a dominant share in the increasing traffic. Therefore, there is an urgent need to develop an effective and accurate scheme to achieve online and automatic traffic management. To this end, this paper proposes an online multimedia traffic classification framework based on a Convolutional Neural Network (CNN), capable of conducting fast and early classification as well as class incremental learning. First, the sliding window technique is applied to capture the flow slices for further feature extraction. Then, the 3-dimensional flow representation is extracted based on the probability distribution function. After that, according to the specific structure of features, a deeply adapted structure of CNN is devised to better learn the knowledge from the representation. Besides, to better support the addition of new services, a class incremental learning model is developed with the techniques of knowledge distillation and bias correction to achieve continuous learning without retraining from scratch. Our experimental results reveal that the proposed method achieves faster and more accurate traffic classification compared with the state-of-the-art. Additionally, the deployed scheme using incremental learning achieves drops by about 50% in both time and memory consumptions compared with existing methods, while guaranteeing the accurate classification after adding new classes.}
}
@article{CHEN2021102821,
title = {A novel Byzantine fault tolerance consensus for Green IoT with intelligence based on reinforcement},
journal = {Journal of Information Security and Applications},
volume = {59},
pages = {102821},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102821},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621000582},
author = {Peng Chen and Dezhi Han and Tien-Hsiung Weng and Kuan-Ching Li and Arcangelo Castiglione},
keywords = {Blockchain, Green IoT, Byzantine fault tolerance, Reinforcement learning, Consensus algorithm, Smart city},
abstract = {To enhance the consensus performance of Blockchain in the Green Internet of Things (G-IoT) and improve the static network structure and communication overheads in the Practical Byzantine Fault Tolerance (PBFT) consensus algorithm, in this paper, we propose a Credit Reinforce Byzantine Fault Tolerance (CRBFT) consensus algorithm by using reinforcement learning. The CRBFT algorithm divides the nodes into three types, each with different responsibilities: master node, sub-nodes, and candidate nodes, and sets the credit attribute to the node. The node’s credit can be adjusted adaptively through the reinforcement learning algorithm, which can dynamically change the state of nodes. CRBFT algorithm can automatically identify malicious nodes and invalid nodes, making them exit from the consensus network. Experimental results show that the CRBFT algorithm can effectively improve the consensus network’s security. Besides, compared with the PBFT algorithm, in CRBFT, the consensus delay is reduced by about 40%, and the traffic overhead is reduced by more than 45%. This reduction is conducive to save energy and reduce emissions.}
}
@article{HUANG2019283,
title = {Urban solar utilization potential mapping via deep learning technology: A case study of Wuhan, China},
journal = {Applied Energy},
volume = {250},
pages = {283-291},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.04.113},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919307780},
author = {Zhaojian Huang and Thushini Mendis and Shen Xu},
keywords = {Deep learning, Neural networks, Solar energy, Urban energy, Solar potential mapping},
abstract = {This study presents a novel approach to detect the city-wide solar potential which utilizes image segmentation with deep learning technology unlike traditional methods. In order to study the solar energy potential in the urban scale, there exists a requirement to quantify the roof area of buildings which are available to receive solar radiation, calculate the total solar radiation obtained within the region based on the meteorological conditions, and determine the total solar energy potential with carbon emissions savings and the economic recovery period. However, obtaining the overall roof area of a city is an existing difficulty when considering the quantification of solar potential in the urban scale. This study utilizes the U-Net of deep learning technology, and a large range of satellite maps to identify the building roof, in order to estimate the city's solar potential. This research established that the urban roofs of Wuhan have an annual photovoltaic electricity generation potential of 17292.30 × 106 kWh/year.}
}
@article{PANDYA2020555,
title = {Modeling and Prediction of Freight Delivery for Blocked and Unblocked Street Using Machine Learning Techniques},
journal = {Transportation Research Procedia},
volume = {48},
pages = {555-561},
year = {2020},
note = {Recent Advances and Emerging Issues in Transport Research – An Editorial Note for the Selected Proceedings of WCTR 2019 Mumbai},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.08.059},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520304750},
author = {Prachi Pandya and Rajesh Gujar and Vinay Vakharia},
keywords = {Freight delivery, Ahmedabad, Machine learning, Support vector machine, Artificial neural network},
abstract = {Freight deliveries on signalized city road are recognized as lane obstructions throughout delivery. Traffic jamming associated with urban freight deliveries has gained increasing attention recently. As traffic engineers and planners are tasked with findingsolutions to accomplishcomprehensive demand more sustainably with restrictedroad capacity. The goal of this research is to evaluatethe model for quantifying the capacity and delay effect of a freight delivery on a signalized city road in Ahmedabad. The all or nothing model similar to the procedure used in the highway capacity manual (HCM2010). The persistence is to provide an understandingof the use of these tools for analysis of urban freight delivery policy. The present study covers delay and vehicle capacity estimation that can account for the changeable locality of deliveries, duration, and different impact on different lane groups. To predict the vehicle capacity and delay estimation, machine learning models, Support vector machine and Artificial neural network was utilized. Result shows excellent agreement between experimental and predicted observations.}
}
@article{PAWAR2021,
title = {Deep learning based detection and localization of road accidents from traffic surveillance videos},
journal = {ICT Express},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S2405959521001478},
author = {Karishma Pawar and Vahida Attar},
keywords = {Accident detection, Deep learning, One-class classification, Video surveillance},
abstract = {Real-world traffic surveillance videos need continuous supervision to monitor and take appropriate actions in case of fatal accidents. However, continuously monitoring them with human supervision is error prone and tedious. Therefore, a deep learning approach for automatic detection and localization of road accidents has been proposed by formulating the problem as anomaly detection. The method follows one-class classification approach and applies spatio-temporal autoencoder and sequence-to-sequence long short-term memory autoencoder for modeling spatial and temporal representations in the video. The model is executed on a real-world video traffic surveillance datasets and significant results have been achieved both qualitatively and quantitatively.}
}
@article{P2021107362,
title = {Prediction of Risk Percentage in Software Projects by Training Machine Learning Classifiers},
journal = {Computers & Electrical Engineering},
volume = {94},
pages = {107362},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107362},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621003323},
author = {Gouthaman P and Suresh Sankaranarayanan},
keywords = {Software model, Agile, Waterfall, Evolutionary, Incremental, Machine learning, Risk prediction},
abstract = {Recently, software project failures have been increasing due to lack of planning and budget constraints. In this regard, identifying the suitable software model with the consideration of risk factors is imperative. Therefore, this study investigates the key software models utilized in the industry through an interaction with software development experts and literature survey. In this study, 15 standard indicators were chosen where a survey was conducted through a questionnaire. The major performance metrics which were taken into consideration are network, security, software, machine learning, internet of things and application programming interface. We proposed a novel framework for the received dataset through questionnaire in which the machine learning classifiers were applied and risk predictions for each of the identified software models were accomplished. Using this outcome, software product managers can identify the appropriate software model according to the software requirements along with risk prediction percentage.}
}
@article{LIU202015241,
title = {Platoon control of connected autonomous vehicles: A distributed reinforcement learning method by consensus},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {15241-15246},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2310},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329773},
author = {Bo Liu and Zhengtao Ding and Chen Lv},
keywords = {Distributed training, Reinforcement learning, Platoon, Consensus},
abstract = {This paper proposes a distributed reinforcement learning method based on deep Q-network and the consensus algorithm to deal with the multi-vehicle platoon control problem, which contains the two processes of local training and global consensus. The platooning problem is decomposed into many single-vehicle tasks based on deep Q-network, where each vehicle accumulates its experience data samples by interacting with its front and back vehicles. After initialization, all vehicles’ Q-networks are first locally optimized based on their own experience simultaneously. The consensus algorithm is then used to make all vehicles in a decentralized platoon approach each other, where the communication is only required among directly connected vehicles. At last, the simulation study shows that the Q-networks of all vehicles reach consensus first and then converge to the optimum in union using the proposed distributed deep Q-networks algorithm, and all vehicles learn to form the required platoon and move forward with a roughly equal separation.}
}
@article{MA2020352,
title = {Hybrid machine learning algorithm and statistical time series model for network-wide traffic forecast},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {111},
pages = {352-372},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X19303821},
author = {Tao Ma and Constantinos Antoniou and Tomer Toledo},
keywords = {Machine learning, Postprocess residuals, Network-wide traffic prediction, MSVR, Neural network, ARIMA},
abstract = {We propose a novel approach for network-wide traffic state prediction where the statistical time series model ARIMA is used to postprocess the residuals out of the fundamental machine learning algorithm MLP. This approach is named as NN-ARIMA. Neural Network MLP is employed to capture network-scale co-movement pattern of all traffic flows, and ARIMA is used to further extract location-specific traffic features in the residual time series out of Neural Network. The experiment results show that the postprocessing the residuals of Neural Network by the ARIMA analysis helps to significantly improve accuracy of traffic state prediction by 8.9–13.4% in term of mean squared error reduction. In order to verify the efficiency of the ARIMA analysis in the postprocessing, Multidimensional Support Vector Regression (MSVR) model is also employed to replace the role of Neural Network in the comparative experiment. Two streams of comparisons, (1) NN vs. NN-ARIMA and (2) MSVR vs. MSVR-ARIMA, are performed and show consistent results. The proposed approach not only can capture network-wide co-movement pattern of traffic flows, but also seize location-specific traffic characteristics as well as sharp nonlinearity of macroscopic traffic variables. The case study indicates that the accuracy of prediction can be significantly improved when both network-scale traffic features and location-specific characteristics are taken into account.}
}
@article{ZHANG2021102442,
title = {Modeling fine-scale residential land price distribution: An experimental study using open data and machine learning},
journal = {Applied Geography},
volume = {129},
pages = {102442},
year = {2021},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2021.102442},
url = {https://www.sciencedirect.com/science/article/pii/S0143622821000588},
author = {Peng Zhang and Shougeng Hu and Weidong Li and Chuanrong Zhang and Shengfu Yang and Shijin Qu},
keywords = {Land price distribution, Determinants, Spatiotemporal variation, Machine learning, Open data, Wuhan},
abstract = {Modeling the fine-scale spatiotemporal distribution of residential land prices (RLPs) is the basis for scientifically allocating land resources, managing the residential market and improving urban planning. The accurate mapping of the RLP dynamics require reliable land price prediction models and data with fine spatial and temporal resolution. With the aid of point of interest (POI) data and nighttime light (NTL) images, this paper attempts to explore the ability of machine learning algorithms (MLAs) to model grid-level RLPs using the case of Wuhan in China. Several land price prediction models were built using five MLAs and various geographic variables. The experimental results show that the extra-trees regression algorithm and the radial basis function-based support vector regression algorithm perform best in Period Ⅰ (2010–2014) and Period Ⅱ (2015–2019), respectively; therefore, they were selected to estimate the RLPs of the grids without observations in the corresponding period. Based on the estimated results, we found that the spatial pattern of the RLP in Wuhan transitioned from monocentric to polycentric between the two periods, and RLPs grew rapidly near newly formed urban subcenters and waterscapes. The relative importance of the predictor variables shows that commercial and educational facilities are important determinants of the RLP distribution in Wuhan; moreover, the relative importance of natural amenities and education facilities increased over time, while that of commercial facilities and public transportation decreased slightly. The case of Wuhan confirms the feasibility of MLAs and openly accessible urban data in modeling fine-scale RLP distributions. Our proposed framework provides a new approach to monitor the urban land price dynamics accurately and closely, which is beneficial for improving the infrastructure layout and achieve smart city growth.}
}
@article{SHUJA2021103005,
title = {Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey},
journal = {Journal of Network and Computer Applications},
volume = {181},
pages = {103005},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103005},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000321},
author = {Junaid Shuja and Kashif Bilal and Waleed Alasmary and Hassan Sinky and Eisa Alanazi},
keywords = {Caching, Edge networks, Machine learning, Popularity prediction, 5G},
abstract = {Edge networking is a complex and dynamic computing paradigm that aims to push cloud re-sources closer to the end user improving responsiveness and reducing backhaul traffic. User mobility, preferences, and content popularity are the dominant dynamic features of edge networks. Temporal and social features of content, such as the number of views and likes are leveraged to estimate the popularity of content from a global perspective. However, such estimates should not be mapped to an edge network with particular social and geographic characteristics. In next generation edge networks, i.e., 5G and beyond 5G, machine learning techniques can be applied to predict content popularity based on user preferences, cluster users based on similar content interests, and optimize cache placement and replacement strategies provided a set of constraints and predictions about the state of the network. These applications of machine learning can help identify relevant content for an edge network. This article investigates the application of machine learning techniques for in-network caching in edge networks. We survey recent state-of-the-art literature and formulate a comprehensive taxonomy based on (a) machine learning technique (method, objective, and features), (b) caching strategy (policy, location, and replacement), and (c) edge network (type and delivery strategy). A comparative analysis of the state-of-the-art literature is presented with respect to the parameters identified in the taxonomy. Moreover, we debate research challenges and future directions for optimal caching decisions and the application of machine learning in edge networks.}
}
@article{RENUGADEVI2021,
title = {IoT based smart energy grid for sustainable cites},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.02.270},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321013572},
author = {N. Renugadevi and S. Saravanan and C.M. {Naga Sudha}},
keywords = {Smart city, IoT, Smart grid, Big data analytics, Blockchain},
abstract = {In present scenario, each and every activity in basic lifestyle of people is becoming smarter with the day-to-day flourishing development of technology such as Internet of Things, Artificial Intelligence, Machine Learning and Blockchain. Among the smart improvements, smart city holds the highest rank due to the initiatives provided by the administration. Smart city mission has influenced the citizen’s life with respect to the automation on fundamental needs. Energy resources are considered as one of the important requirements for the survival of people. With the advent of Internet of Things, smart grid systems are designed in order to meet the sustainable demands. It also provides information about the energy resources consumed and alert the consumers about the demand prevails on the resources. Technological improvement is more essential for a country, only if it can be considered and adaptable for sustainable development. Therefore these papers outline the Internet of Things based smart grid which helps in the energy conservation process to meet the future demands. Energy management processes has to be well-versed among the citizens as it helps in understanding the possible ways on storage of energy resources. However, when energy storage is concerned, security issues are the counterpart which has to be focused at higher level. Therefore, blockchain techniques applied for energy storage systems are also outlined along with the applications and future directions of smart grid.}
}
@article{JAGANNATH2019101913,
title = {Machine learning for wireless communications in the Internet of Things: A comprehensive survey},
journal = {Ad Hoc Networks},
volume = {93},
pages = {101913},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.101913},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519300812},
author = {Jithin Jagannath and Nicholas Polosky and Anu Jagannath and Francesco Restuccia and Tommaso Melodia},
keywords = {Machine learning, Deep learning, Reinforcement learning, Internet of Things, Wireless ad hoc network, Spectrum sensing, Medium access control, Routing protocol},
abstract = {The Internet of Things (IoT) is expected to require more effective and efficient wireless communications than ever before. For this reason, techniques such as spectrum sharing, dynamic spectrum access, extraction of signal intelligence and optimized routing will soon become essential components of the IoT wireless communication paradigm. In this vision, IoT devices must be able to not only learn to autonomously extract spectrum knowledge on-the-fly from the network but also leverage such knowledge to dynamically change appropriate wireless parameters (e.g., frequency band, symbol modulation, coding rate, route selection, etc.) to reach the network’s optimal operating point. Given that the majority of the IoT will be composed of tiny, mobile, and energy-constrained devices, traditional techniques based on a priori network optimization may not be suitable, since (i) an accurate model of the environment may not be readily available in practical scenarios; (ii) the computational requirements of traditional optimization techniques may prove unbearable for IoT devices. To address the above challenges, much research has been devoted to exploring the use of machine learning to address problems in the IoT wireless communications domain. The reason behind machine learning’s popularity is that it provides a general framework to solve very complex problems where a model of the phenomenon being learned is too complex to derive or too dynamic to be summarized in mathematical terms. This work provides a comprehensive survey of the state of the art in the application of machine learning techniques to address key problems in IoT wireless communications with an emphasis on its ad hoc networking aspect. First, we present extensive background notions of machine learning techniques. Then, by adopting a bottom-up approach, we examine existing work on machine learning for the IoT at the physical, data-link and network layer of the protocol stack. Thereafter, we discuss directions taken by the community towards hardware implementation to ensure the feasibility of these techniques. Additionally, before concluding, we also provide a brief discussion of the application of machine learning in IoT beyond wireless communication. Finally, each of these discussions is accompanied by a detailed analysis of the related open problems and challenges.}
}
@article{WAMBUGU2021102603,
title = {Hyperspectral image classification on insufficient-sample and feature learning using deep neural networks: A review},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {105},
pages = {102603},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102603},
url = {https://www.sciencedirect.com/science/article/pii/S030324342100310X},
author = {Naftaly Wambugu and Yiping Chen and Zhenlong Xiao and Kun Tan and Mingqiang Wei and Xiaoxue Liu and Jonathan Li},
keywords = {Deep learning, Image classification, Remote sensing, Hyperspectral imagery, Supervised learning},
abstract = {Over the years, advances in sensor technologies have enhanced spatial, temporal, spectral, and radiometric resolutions, thus significantly improving the size, resolution, and quality of imagery. These vast developments have inspired improvement in various hyperspectral images (HSI) classification applications such as land cover mapping, vegetation classification, urban monitoring, and understanding which are essential for better utilization of Earth’s resources. HSI classification requires superior algorithms with greater accuracy, less computational complexity, and robustness to extract rich, spectral-spatial information. Deep convolution neural networks (DCCNs) have revolutionized image classification experience, with robust architectures being proposed from time to time. However, insufficient training samples have been earmarked as a significant bottleneck for supervised HSI classification and have not been fully explored in literature. To stimulate further research, this paper reviews current methods that handle labeled data insufficiency and the current feature learning methods for HSI classification using DCNNs. It also presents various methods’ results on the three most popular public HSI datasets, together with intuitive observations motivating future research by the hyperspectral community.}
}
@article{XU2020228,
title = {Improved Long Short-Term Memory based anomaly detection with concept drift adaptive method for supporting IoT services},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {228-242},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20302235},
author = {Rongbin Xu and Yongliang Cheng and Zhiqiang Liu and Ying Xie and Yun Yang},
keywords = {Smart city, Quality of service, Anomaly detection, Concept drift adaptive method, Long Short-Term Memory},
abstract = {The rapid rising of artificial intelligence (AI) and Internet of Things technologies leads to the accumulation of abundant communication data without being processed on time, which causes potential threat for smart city. How to effectively leverage these data for anomaly detection has become an increasingly popular research field as it is a fundamental aspect of cyber security for smart city services. The existing methods often focus on either static data for anomaly detection or streaming data without considering the influence of poor detection accuracy caused by concept drift phenomenon. In this paper, we concentrate on the anomaly detection problem of smart city services and distinguish different anomalies of communication in an effective way, which is aimed to protect data privacy of users. Then we propose an innovative concept drift adaptive method to improve the accuracy of anomaly detection, which fully considers time influence to change the sample distribution along timeline. Furthermore, we present an AI based Improved Long Short-Term Memory (I-LSTM) neural network that adds time factor and employs a novel smooth activation function, which can enhance the performance of multi-classification for anomaly detection. Finally, our proposed methods are evaluated with a real communication dataset. Extensive experimental results indicate that I-LSTM achieves the highest values on all indicators. This demonstrates the effectiveness of our proposed methods that can offer excellent quality of service for smart city, which is a perfect fusion of artificial intelligence and communication security.}
}
@article{ALI2022233,
title = {Exploiting dynamic spatio-temporal graph convolutional neural networks for citywide traffic flows prediction},
journal = {Neural Networks},
volume = {145},
pages = {233-247},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004123},
author = {Ahmad Ali and Yanmin Zhu and Muhammad Zakarya},
keywords = {Traffic flow prediction, Spatial and temporal dependencies, GCN, LSTM, Road safety},
abstract = {The prediction of crowd flows is an important urban computing issue whose purpose is to predict the future number of incoming and outgoing people in regions. Measuring the complicated spatial–temporal dependencies with external factors, such as weather conditions and surrounding point-of-interest (POI) distribution is the most difficult aspect of predicting crowd flows movement. To overcome the above issue, this paper advises a unified dynamic deep spatio-temporal neural network model based on convolutional neural networks and long short-term memory, termed as (DHSTNet) to simultaneously predict crowd flows in every region of a city. The DHSTNet model is made up of four separate components: a recent, daily, weekly, and an external branch component. Our proposed approach simultaneously assigns various weights to different branches and integrates the four properties’ outputs to generate final predictions. Moreover, to verify the generalization and scalability of the proposed model, we apply a Graph Convolutional Network (GCN) based on Long Short Term Memory (LSTM) with the previously published model, termed as GCN-DHSTNet; to capture the spatial patterns and short-term temporal features; and to illustrate its exceptional accomplishment in predicting the traffic crowd flows. The GCN-DHSTNet model not only depicts the spatio-temporal dependencies but also reveals the influence of different time granularity, which are recent, daily, weekly periodicity and external properties, respectively. Finally, a fully connected neural network is utilized to fuse the spatio-temporal features and external properties together. Using two different real-world traffic datasets, our evaluation suggests that the proposed GCN-DHSTNet method is approximately 7.9%–27.2% and 11.2%–11.9% better than the AAtt-DHSTNet method in terms of RMSE and MAPE metrics, respectively. Furthermore, AAtt-DHSTNet outperforms other state-of-the-art methods.}
}
@article{ZHU2019190,
title = {Context-based prediction for road traffic state using trajectory pattern mining and recurrent convolutional neural networks},
journal = {Information Sciences},
volume = {473},
pages = {190-201},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518307357},
author = {Jia Zhu and Changqin Huang and Min Yang and Gabriel Pui {Cheong Fung}},
keywords = {Recurrent convolutional neural networks, Traffic state prediction, Trajectory pattern mining, Context-based concept, Deep learning},
abstract = {With the broad adoption of the global positioning system-enabled devices, the massive amount of trajectory data is generated every day, which results in meaningful traffic patterns. This study aims to predict the future state of road traffic instead of merely showing the current traffic condition. First, we use a clustering method to group similar trajectories of a particular period together into a cluster for each road. Second, for each cluster, we average the lengths and angles of the entire trajectories in the group as the representative trajectory, which is regarded as the road pattern. Third, we create a feature vector for each road based on its historical traffic conditions and neighbor road patterns. Finally, we design a recurrent convolutional neural network for modeling the complex nonlinear relationship among features to predict road traffic conditions. Experimental results show that our approach performs more favorably compared with several traditional machine learning and state-of-the-art algorithms.}
}