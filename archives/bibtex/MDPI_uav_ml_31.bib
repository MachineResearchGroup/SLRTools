
@Article{s21134560,
AUTHOR = {Pi, Chen-Huan and Dai, Yi-Wei and Hu, Kai-Chun and Cheng, Stone},
TITLE = {General Purpose Low-Level Reinforcement Learning Control for Multi-Axis Rotor Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4560},
URL = {https://www.mdpi.com/1424-8220/21/13/4560},
PubMedID = {34283119},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a multipurpose reinforcement learning based low-level multirotor unmanned aerial vehicles control structure constructed using neural networks with model-free training. Other low-level reinforcement learning controllers developed in studies have only been applicable to a model-specific and physical-parameter-specific multirotor, and time-consuming training is required when switching to a different vehicle. We use a 6-degree-of-freedom dynamic model combining acceleration-based control from the policy neural network to overcome these problems. The UAV automatically learns the maneuver by an end-to-end neural network from fusion states to acceleration command. The state estimation is performed using the data from on-board sensors and motion capture. The motion capture system provides spatial position information and a multisensory fusion framework fuses the measurement from the onboard inertia measurement units for compensating the time delay and low update frequency of the capture system. Without requiring expert demonstration, the trained control policy implemented using an improved algorithm can be applied to various multirotors with the output directly mapped to actuators. The algorithm’s ability to control multirotors in the hovering and the tracking task is evaluated. Through simulation and actual experiments, we demonstrate the flight control with a quadrotor and hexrotor by using the trained policy. With the same policy, we verify that we can stabilize the quadrotor and hexrotor in the air under random initial states.},
DOI = {10.3390/s21134560}
}



@Article{agronomy11071363,
AUTHOR = {Bahrami, Hazhir and Homayouni, Saeid and Safari, Abdolreza and Mirzaei, Sayeh and Mahdianpari, Masoud and Reisi-Gahrouei, Omid},
TITLE = {Deep Learning-Based Estimation of Crop Biophysical Parameters Using Multi-Source and Multi-Temporal Remote Sensing Observations},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1363},
URL = {https://www.mdpi.com/2073-4395/11/7/1363},
ISSN = {2073-4395},
ABSTRACT = {Remote sensing data are considered as one of the primary data sources for precise agriculture. Several studies have demonstrated the excellent capability of radar and optical imagery for crop mapping and biophysical parameter estimation. This paper aims at modeling the crop biophysical parameters, e.g., Leaf Area Index (LAI) and biomass, using a combination of radar and optical Earth observations. We extracted several radar features from polarimetric Synthetic Aperture Radar (SAR) data and Vegetation Indices (VIs) from optical images to model crops’ LAI and dry biomass. Then, the mutual correlations between these features and Random Forest feature importance were calculated. We considered two scenarios to estimate crop parameters. First, Machine Learning (ML) algorithms, e.g., Support Vector Regression (SVR), Random Forest (RF), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB), were utilized to estimate two crop biophysical parameters. To this end, crops’ dry biomass and LAI were estimated using three input data; (1) SAR polarimetric features; (2) spectral VIs; (3) integrating both SAR and optical features. Second, a deep artificial neural network was created. These input data were fed to the mentioned algorithms and evaluated using the in-situ measurements. These observations of three cash crops, including soybean, corn, and canola, have been collected over Manitoba, Canada, during the Soil Moisture Active Validation Experimental 2012 (SMAPVEX-12) campaign. The results showed that GB and XGB have great potential in parameter estimation and remarkably improved accuracy. Our results also demonstrated a significant improvement in the dry biomass and LAI estimation compared to the previous studies. For LAI, the validation Root Mean Square Error (RMSE) was reported as 0.557 m2/m2 for canola using GB, and 0.298 m2/m2 for corn using GB, 0.233 m2/m2 for soybean using XGB. RMSE was reported for dry biomass as 26.29 g/m2 for canola utilizing SVR, 57.97 g/m2 for corn using RF, and 5.00 g/m2 for soybean using GB. The results revealed that the deep artificial neural network had a better potential to estimate crop parameters than the ML algorithms.},
DOI = {10.3390/agronomy11071363}
}



@Article{s21134587,
AUTHOR = {Jang, Hyoseon and Kim, Sangkyun and Yoo, Suhong and Han, Soohee and Sohn, Hong-Gyoo},
TITLE = {Feature Matching Combining Radiometric and Geometric Characteristics of Images, Applied to Oblique- and Nadir-Looking Visible and TIR Sensors of UAV Imagery},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4587},
URL = {https://www.mdpi.com/1424-8220/21/13/4587},
PubMedID = {34283114},
ISSN = {1424-8220},
ABSTRACT = {A large amount of information needs to be identified and produced during the process of promoting projects of interest. Thermal infrared (TIR) images are extensively used because they can provide information that cannot be extracted from visible images. In particular, TIR oblique images facilitate the acquisition of information of a building’s facade that is challenging to obtain from a nadir image. When a TIR oblique image and the 3D information acquired from conventional visible nadir imagery are combined, a great synergy for identifying surface information can be created. However, it is an onerous task to match common points in the images. In this study, a robust matching method of image pairs combined with different wavelengths and geometries (i.e., visible nadir-looking vs. TIR oblique, and visible oblique vs. TIR nadir-looking) is proposed. Three main processes of phase congruency, histogram matching, and Image Matching by Affine Simulation (IMAS) were adjusted to accommodate the radiometric and geometric differences of matched image pairs. The method was applied to Unmanned Aerial Vehicle (UAV) images of building and non-building areas. The results were compared with frequently used matching techniques, such as scale-invariant feature transform (SIFT), speeded-up robust features (SURF), synthetic aperture radar–SIFT (SAR–SIFT), and Affine SIFT (ASIFT). The method outperforms other matching methods in root mean square error (RMSE) and matching performance (matched and not matched). The proposed method is believed to be a reliable solution for pinpointing surface information through image matching with different geometries obtained via TIR and visible sensors.},
DOI = {10.3390/s21134587}
}



@Article{rs13132627,
AUTHOR = {Moura, Marks Melo and de Oliveira, Luiz Eduardo Soares and Sanquetta, Carlos Roberto and Bastos, Alexis and Mohan, Midhun and Corte, Ana Paula Dalla},
TITLE = {Towards Amazon Forest Restoration: Automatic Detection of Species from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2627},
URL = {https://www.mdpi.com/2072-4292/13/13/2627},
ISSN = {2072-4292},
ABSTRACT = {Precise assessments of forest species’ composition help analyze biodiversity patterns, estimate wood stocks, and improve carbon stock estimates. Therefore, the objective of this work was to evaluate the use of high-resolution images obtained from Unmanned Aerial Vehicle (UAV) for the identification of forest species in areas of forest regeneration in the Amazon. For this purpose, convolutional neural networks (CNN) were trained using the Keras–Tensorflow package with the faster_rcnn_inception_v2_pets model. Samples of six forest species were used to train CNN. From these, attempts were made with the number of thresholds, which is the cutoff value of the function; any value below this output is considered 0, and values above are treated as an output 1; that is, values above the value stipulated in the Threshold are considered as identified species. The results showed that the reduction in the threshold decreases the accuracy of identification, as well as the overlap of the polygons of species identification. However, in comparison with the data collected in the field, it was observed that there exists a high correlation between the trees identified by the CNN and those observed in the plots. The statistical metrics used to validate the classification results showed that CNN are able to identify species with accuracy above 90%. Based on our results, which demonstrate good accuracy and precision in the identification of species, we conclude that convolutional neural networks are an effective tool in classifying objects from UAV images.},
DOI = {10.3390/rs13132627}
}



@Article{rs13132631,
AUTHOR = {Grybas, Heather and Congalton, Russell G.},
TITLE = {A Comparison of Multi-Temporal RGB and Multispectral UAS Imagery for Tree Species Classification in Heterogeneous New Hampshire Forests},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {2631},
URL = {https://www.mdpi.com/2072-4292/13/13/2631},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial systems (UASs) have recently become an affordable means to map forests at the species level, but research into the performance of different classification methodologies and sensors is necessary so users can make informed choices that maximize accuracy. This study investigated whether multi-temporal UAS data improved the classified accuracy of 14 species examined the optimal time-window for data collection, and compared the performance of a consumer-grade RGB sensor to that of a multispectral sensor. A time series of UAS data was collected from early spring to mid-summer and a sequence of mono-temporal and multi-temporal classifications were carried out. Kappa comparisons were conducted to ascertain whether the multi-temporal classifications significantly improved accuracy and whether there were significant differences between the RGB and multispectral classifications. The multi-temporal classification approach significantly improved accuracy; however, there was no significant benefit when more than three dates were used. Mid- to late spring imagery produced the highest accuracies, potentially due to high spectral heterogeneity between species and homogeneity within species during this time. The RGB sensor exhibited significantly higher accuracies, probably due to the blue band, which was found to be very important for classification accuracy and lacking in the multispectral sensor employed here.},
DOI = {10.3390/rs13132631}
}



@Article{electronics10131605,
AUTHOR = {Kathen, Micaela Jara Ten and Flores, Isabel Jurado and Reina, Daniel Gutiérrez},
TITLE = {An Informative Path Planner for a Swarm of ASVs Based on an Enhanced PSO with Gaussian Surrogate Model Components Intended for Water Monitoring Applications},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {1605},
URL = {https://www.mdpi.com/2079-9292/10/13/1605},
ISSN = {2079-9292},
ABSTRACT = {Controlling the water quality of water supplies has always been a critical challenge, and water resource monitoring has become a need in recent years. Manual monitoring is not recommended in the case of large water surfaces for a variety of reasons, including expense and time consumption. In the last few years, researchers have proposed the use of autonomous vehicles for monitoring tasks. Fleets or swarms of vehicles can be deployed to conduct water resource explorations by using path planning techniques to guide the movements of each vehicle. The main idea of this work is the development of a monitoring system for Ypacarai Lake, where a fleet of autonomous surface vehicles will be guided by an improved particle swarm optimization based on the Gaussian process as a surrogate model. The purpose of using the surrogate model is to model water quality parameter behavior and to guide the movements of the vehicles toward areas where samples have not yet been collected; these areas are considered areas with high uncertainty or unexplored areas and areas with high contamination levels of the lake. The results show that the proposed approach, namely the enhanced GP-based PSO, balances appropriately the exploration and exploitation of the surface of Ypacarai Lake. In addition, the proposed approach has been compared with other techniques like the original particle swarm optimization and the particle swarm optimization with Gaussian process uncertainty component in a simulated Ypacarai Lake environment. The obtained results demonstrate the superiority of the proposed enhanced GP-based PSO in terms of mean square error with respect to the other techniques.},
DOI = {10.3390/electronics10131605}
}



@Article{en14134049,
AUTHOR = {Azelhak, Younes and Benaaouinate, Loubna and Medromi, Hicham and Errami, Youssef and Bouragba, Tarik and Voyer, Damien},
TITLE = {Exhaustive Comparison between Linear and Nonlinear Approaches for Grid-Side Control of Wind Energy Conversion Systems},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {4049},
URL = {https://www.mdpi.com/1996-1073/14/13/4049},
ISSN = {1996-1073},
ABSTRACT = {In this paper, we propose a comparative study of linear and nonlinear algorithms designed for grid-side control of the power flow in a wind energy conversion system. We performed several simulations and experiments with step and variable power scenarios for different values of the DC-link capacity with the DC storage element being the key element of the grid-side converter. The linear control was designed on the basis of the internal model control theory where an active damping was added to avoid steady state errors. Nonlinear controls were built using first and second order sliding mode controls with theoretical considerations to ensure accuracy and stability. We observed that the first order sliding mode control was the most efficient algorithm for controlling the DC-link voltage but that the chattering degraded the quality of the energy injected into the grid as well as the efficiency of the grid-side converter. The linear control caused overshoots on the DC-link voltage; however, this algorithm had better performance on the grid side due to its smoother control. Finally, the second order sliding mode control did not prove to be more robust than the other two algorithms. This can be explained by the fact that this control is theoretically more sensitive to converter losses.},
DOI = {10.3390/en14134049}
}



@Article{su13137497,
AUTHOR = {Wang, Hao and Ren, Yaxin and Meng, Zhijun},
TITLE = {A Farm Management Information System for Semi-Supervised Path Planning and Autonomous Vehicle Control},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {13},
ARTICLE-NUMBER = {7497},
URL = {https://www.mdpi.com/2071-1050/13/13/7497},
ISSN = {2071-1050},
ABSTRACT = {This paper presents a farm management information system targeting improvements in the ease of use and sustainability of robot farming systems. The system integrates the functionalities of field survey, path planning, monitoring, and controlling agricultural vehicles in real time. Firstly, a Grabcut-based semi-supervised field registration method is proposed for arable field detection from the orthoimage taken by the drone with an RGB camera. It partitions a complex field into simple geometric entities with simple user interaction. The average Mean Intersection over Union is about 0.95 when the field size ranges from 2.74 ha to 5.06 ha. In addition, a desktop software and a web application are developed as the entity of an FMIS. Compared to existing FMISs, this system provides more advanced features in robot farming, while providing simpler user interaction and better results. It allows clients to invoke web services and receive responses independent of programming language and platforms. Moreover, the system is compatible with other services, users, and devices following the open-source access protocol. We have evaluated the system by controlling 5 robot tractors with a 2 Hz communication frequency. The communication protocols will be publicly available to protentional users.},
DOI = {10.3390/su13137497}
}



@Article{su13147547,
AUTHOR = {Munawar, Hafiz Suliman and Ullah, Fahim and Qayyum, Siddra and Khan, Sara Imran and Mojtahedi, Mohammad},
TITLE = {UAVs in Disaster Management: Application of Integrated Aerial Imagery and Convolutional Neural Network for Flood Detection},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {7547},
URL = {https://www.mdpi.com/2071-1050/13/14/7547},
ISSN = {2071-1050},
ABSTRACT = {Floods have been a major cause of destruction, instigating fatalities and massive damage to the infrastructure and overall economy of the affected country. Flood-related devastation results in the loss of homes, buildings, and critical infrastructure, leaving no means of communication or travel for the people stuck in such disasters. Thus, it is essential to develop systems that can detect floods in a region to provide timely aid and relief to stranded people, save their livelihoods, homes, and buildings, and protect key city infrastructure. Flood prediction and warning systems have been implemented in developed countries, but the manufacturing cost of such systems is too high for developing countries. Remote sensing, satellite imagery, global positioning system, and geographical information systems are currently used for flood detection to assess the flood-related damages. These techniques use neural networks, machine learning, or deep learning methods. However, unmanned aerial vehicles (UAVs) coupled with convolution neural networks have not been explored in these contexts to instigate a swift disaster management response to minimize damage to infrastructure. Accordingly, this paper uses UAV-based aerial imagery as a flood detection method based on Convolutional Neural Network (CNN) to extract flood-related features from the images of the disaster zone. This method is effective in assessing the damage to local infrastructures in the disaster zones. The study area is based on a flood-prone region of the Indus River in Pakistan, where both pre-and post-disaster images are collected through UAVs. For the training phase, 2150 image patches are created by resizing and cropping the source images. These patches in the training dataset train the CNN model to detect and extract the regions where a flood-related change has occurred. The model is tested against both pre-and post-disaster images to validate it, which has positive flood detection results with an accuracy of 91%. Disaster management organizations can use this model to assess the damages to critical city infrastructure and other assets worldwide to instigate proper disaster responses and minimize the damages. This can help with the smart governance of the cities where all emergent disasters are addressed promptly.},
DOI = {10.3390/su13147547}
}



@Article{rs13142658,
AUTHOR = {Jozdani, Shahab and Chen, Dongmei and Chen, Wenjun and Leblanc, Sylvain G. and Prévost, Christian and Lovitt, Julie and He, Liming and Johnson, Brian A.},
TITLE = {Leveraging Deep Neural Networks to Map Caribou Lichen in High-Resolution Satellite Images Based on a Small-Scale, Noisy UAV-Derived Map},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2658},
URL = {https://www.mdpi.com/2072-4292/13/14/2658},
ISSN = {2072-4292},
ABSTRACT = {Lichen is an important food source for caribou in Canada. Lichen mapping using remote sensing (RS) images could be a challenging task, however, as lichens generally appear in unevenly distributed, small patches, and could resemble surficial features. Moreover, collecting lichen labeled data (reference data) is expensive, which restricts the application of many robust supervised classification models that generally demand a large quantity of labeled data. The goal of this study was to investigate the potential of using a very-high-spatial resolution (1-cm) lichen map of a small sample site (e.g., generated based on a single UAV scene and using field data) to train a subsequent classifier to map caribou lichen over a much larger area (~0.04 km2 vs. ~195 km2) and a lower spatial resolution image (in this case, a 50-cm WorldView-2 image). The limited labeled data from the sample site were also partially noisy due to spatial and temporal mismatching issues. For this, we deployed a recently proposed Teacher-Student semi-supervised learning (SSL) approach (based on U-Net and U-Net++ networks) involving unlabeled data to assist with improving the model performance. Our experiments showed that it was possible to scale-up the UAV-derived lichen map to the WorldView-2 scale with reasonable accuracy (overall accuracy of 85.28% and F1-socre of 84.38%) without collecting any samples directly in the WorldView-2 scene. We also found that our noisy labels were partially beneficial to the SSL robustness because they improved the false positive rate compared to the use of a cleaner training set directly collected within the same area in the WorldView-2 image. As a result, this research opens new insights into how current very high-resolution, small-scale caribou lichen maps can be used for generating more accurate large-scale caribou lichen maps from high-resolution satellite imagery.},
DOI = {10.3390/rs13142658}
}



@Article{rs13142663,
AUTHOR = {Chen, Chuanfa and Guo, Jiaojiao and Wu, Huiming and Li, Yanyan and Shi, Bo},
TITLE = {Performance Comparison of Filtering Algorithms for High-Density Airborne LiDAR Point Clouds over Complex LandScapes},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2663},
URL = {https://www.mdpi.com/2072-4292/13/14/2663},
ISSN = {2072-4292},
ABSTRACT = {Airborne light detection and ranging (LiDAR) technology has become the mainstream data source in geosciences and environmental sciences. Point cloud filtering is a prerequisite for almost all LiDAR-based applications. However, it is challenging to select a suitable filtering algorithm for handling high-density point clouds over complex landscapes. Therefore, to determine an appropriate filter on a specific environment, this paper comparatively assessed the performance of five representative filtering algorithms on six study sites with different terrain characteristics, where three plots are located in urban areas and three in forest areas. The representative filtering methods include simple morphological filter (SMRF), multiresolution hierarchical filter (MHF), slope-based filter (SBF), progressive TIN densification (PTD) and segmentation-based filter (SegBF). Results demonstrate that SMRF performs the best in urban areas, and compared to MHF, SBF, PTD and SegBF, the total error of SMRF is reduced by 1.38%, 48.21%, 48.25% and 31.03%, respectively. MHF outperforms the others in forest areas, and compared to SMRF, SBF, PTD and SegBF, the total error of MHF is reduced by 1.98%, 35.87%, 45.11% and 9.42%, respectively. Moreover, both SMRF and MHF keep a good balance between type I and II errors, which makes the produced DEMs much similar to the references. Overall, SMRF and MHF are recommended for urban and forest areas, respectively, and MHF averagely performs slightly better than SMRF on all areas with respect to kappa coefficient.},
DOI = {10.3390/rs13142663}
}



@Article{rs13142665,
AUTHOR = {Mirzazade, Ali and Popescu, Cosmin and Blanksvärd, Thomas and Täljsten, Björn},
TITLE = {Workflow for Off-Site Bridge Inspection Using Automatic Damage Detection-Case Study of the Pahtajokk Bridge},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2665},
URL = {https://www.mdpi.com/2072-4292/13/14/2665},
ISSN = {2072-4292},
ABSTRACT = {For the inspection of structures, particularly bridges, it is becoming common to replace humans with autonomous systems that use unmanned aerial vehicles (UAV). In this paper, a framework for autonomous bridge inspection using a UAV is proposed with a four-step workflow: (a) data acquisition with an efficient UAV flight path, (b) computer vision comprising training, testing and validation of convolutional neural networks (ConvNets), (c) point cloud generation using intelligent hierarchical dense structure from motion (DSfM), and (d) damage quantification. This workflow starts with planning the most efficient flight path that allows for capturing of the minimum number of images required to achieve the maximum accuracy for the desired defect size, then followed by bridge and damage recognition. Three types of autonomous detection are used: masking the background of the images, detecting areas of potential damage, and pixel-wise damage segmentation. Detection of bridge components by masking extraneous parts of the image, such as vegetation, sky, roads or rivers, can improve the 3D reconstruction in the feature detection and matching stages. In addition, detecting damaged areas involves the UAV capturing close-range images of these critical regions, and damage segmentation facilitates damage quantification using 2D images. By application of DSfM, a denser and more accurate point cloud can be generated for these detected areas, and aligned to the overall point cloud to create a digital model of the bridge. Then, this generated point cloud is evaluated in terms of outlier noise, and surface deviation. Finally, damage that has been detected is quantified and verified, based on the point cloud generated using the Terrestrial Laser Scanning (TLS) method. The results indicate this workflow for autonomous bridge inspection has potential.},
DOI = {10.3390/rs13142665}
}



@Article{s21144654,
AUTHOR = {Łabędź, Piotr and Skabek, Krzysztof and Ozimek, Paweł and Nytko, Mateusz},
TITLE = {Histogram Adjustment of Images for Improving Photogrammetric Reconstruction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4654},
URL = {https://www.mdpi.com/1424-8220/21/14/4654},
PubMedID = {34300393},
ISSN = {1424-8220},
ABSTRACT = {The accuracy of photogrammetric reconstruction depends largely on the acquisition conditions and on the quality of input photographs. This paper proposes methods of improving raster images that increase photogrammetric reconstruction accuracy. These methods are based on modifying color image histograms. Special emphasis was placed on the selection of channels of the RGB and CIE L*a*b* color models for further improvement of the reconstruction process. A methodology was proposed for assessing the quality of reconstruction based on premade reference models using positional statistics. The analysis of the influence of image enhancement on reconstruction was carried out for various types of objects. The proposed methods can significantly improve the quality of reconstruction. The superiority of methods based on the luminance channel of the L*a*b* model was demonstrated. Our studies indicated high efficiency of the histogram equalization method (HE), although these results were not highly distinctive for all performed tests.},
DOI = {10.3390/s21144654}
}



@Article{rs13142678,
AUTHOR = {Ge, Haixiao and Ma, Fei and Li, Zhenwang and Tan, Zhengzheng and Du, Changwen},
TITLE = {Improved Accuracy of Phenological Detection in Rice Breeding by Using Ensemble Models of Machine Learning Based on UAV-RGB Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2678},
URL = {https://www.mdpi.com/2072-4292/13/14/2678},
ISSN = {2072-4292},
ABSTRACT = {Accurate and timely detection of phenology at plot scale in rice breeding trails is crucial for understanding the heterogeneity of varieties and guiding field management. Traditionally, remote sensing studies of phenology detection have heavily relied on the time-series vegetation index (VI) data. However, the methodology based on time-series VI data was often limited by the temporal resolution. In this study, three types of ensemble models including hard voting (majority voting), soft voting (weighted majority voting) and model stacking, were proposed to identify the principal phenological stages of rice based on unmanned aerial vehicle (UAV) RGB imagery. These ensemble models combined RGB-VIs, color space (e.g., RGB and HSV) and textures derived from UAV-RGB imagery, and five machine learning algorithms (random forest; k-nearest neighbors; Gaussian naïve Bayes; support vector machine and logistic regression) as base models to estimate phenological stages in rice breeding. The phenological estimation models were trained on the dataset of late-maturity cultivars and tested independently on the dataset of early-medium-maturity cultivars. The results indicated that all ensemble models outperform individual machine learning models in all datasets. The soft voting strategy provided the best performance for identifying phenology with the overall accuracy of 90% and 93%, and the mean F1-scores of 0.79 and 0.81, respectively, in calibration and validation datasets, which meant that the overall accuracy and mean F1-scores improved by 5% and 7%, respectively, in comparison with those of the best individual model (GNB), tested in this study. Therefore, the ensemble models demonstrated great potential in improving the accuracy of phenology detection in rice breeding.},
DOI = {10.3390/rs13142678}
}



@Article{rs13142670,
AUTHOR = {Herzig, Paul and Borrmann, Peter and Knauer, Uwe and Klück, Hans-Christian and Kilias, David and Seiffert, Udo and Pillen, Klaus and Maurer, Andreas},
TITLE = {Evaluation of RGB and Multispectral Unmanned Aerial Vehicle (UAV) Imagery for High-Throughput Phenotyping and Yield Prediction in Barley Breeding},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2670},
URL = {https://www.mdpi.com/2072-4292/13/14/2670},
ISSN = {2072-4292},
ABSTRACT = {With advances in plant genomics, plant phenotyping has become a new bottleneck in plant breeding and the need for reliable high-throughput plant phenotyping techniques has emerged. In the face of future climatic challenges, it does not seem appropriate to continue to solely select for grain yield and a few agronomically important traits. Therefore, new sensor-based high-throughput phenotyping has been increasingly used in plant breeding research, with the potential to provide non-destructive, objective and continuous plant characterization that reveals the formation of the final grain yield and provides insights into the physiology of the plant during the growth phase. In this context, we present the comparison of two sensor systems, Red-Green-Blue (RGB) and multispectral cameras, attached to unmanned aerial vehicles (UAV), and investigate their suitability for yield prediction using different modelling approaches in a segregating barley introgression population at three environments with weekly data collection during the entire vegetation period. In addition to vegetation indices, morphological traits such as canopy height, vegetation cover and growth dynamics traits were used for yield prediction. Repeatability analyses and genotype association studies of sensor-based traits were compared with reference values from ground-based phenotyping to test the use of conventional and new traits for barley breeding. The relative height estimation of the canopy by UAV achieved high precision (up to r = 0.93) and repeatability (up to R2 = 0.98). In addition, we found a great overlap of detected significant genotypes between the reference heights and sensor-based heights. The yield prediction accuracy of both sensor systems was at the same level and reached a maximum prediction accuracy of r2 = 0.82 with a continuous increase in precision throughout the entire vegetation period. Due to the lower costs and the consumer-friendly handling of image acquisition and processing, the RGB imagery seems to be more suitable for yield prediction in this study.},
DOI = {10.3390/rs13142670}
}



@Article{rs13142683,
AUTHOR = {Liu, Zhi and Yang, Shuyuan and Feng, Zhixi and Gao, Quanwei and Wang, Min},
TITLE = {Fast SAR Autofocus Based on Ensemble Convolutional Extreme Learning Machine},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2683},
URL = {https://www.mdpi.com/2072-4292/13/14/2683},
ISSN = {2072-4292},
ABSTRACT = {Inaccurate Synthetic Aperture Radar (SAR) navigation information will lead to unknown phase errors in SAR data. Uncompensated phase errors can blur the SAR images. Autofocus is a technique that can automatically estimate phase errors from data. However, existing autofocus algorithms either have poor focusing quality or a slow focusing speed. In this paper, an ensemble learning-based autofocus method is proposed. Convolutional Extreme Learning Machine (CELM) is constructed and utilized to estimate the phase error. However, the performance of a single CELM is poor. To overcome this, a novel, metric-based combination strategy is proposed, combining multiple CELMs to further improve the estimation accuracy. The proposed model is trained with the classical bagging-based ensemble learning method. The training and testing process is non-iterative and fast. Experimental results conducted on real SAR data show that the proposed method has a good trade-off between focusing quality and speed.},
DOI = {10.3390/rs13142683}
}



@Article{s21144682,
AUTHOR = {Dawid, Wojciech and Pokonieczny, Krzysztof},
TITLE = {Methodology of Using Terrain Passability Maps for Planning the Movement of Troops and Navigation of Unmanned Ground Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4682},
URL = {https://www.mdpi.com/1424-8220/21/14/4682},
PubMedID = {34300422},
ISSN = {1424-8220},
ABSTRACT = {The determination of the route of movement is a key factor which enables navigation. In this article, the authors present the methodology of using different resolution terrain passability maps to generate graphs, which allow for the determination of the optimal route between two points. The routes are generated with the use of two commonly used pathfinding algorithms: Dijkstra’s and A-star. The proposed methodology allows for the determination of routes in various variants—A more secure route that avoids all terrain obstacles with a wide curve, or a shorter route, which is, however, more difficult to pass. In order to achieve that, two functions that modify the value of the index of passability (IOP), which is assigned to the primary fields that the passability map consists of, have been used. These functions have a β parameter that augments or reduces the impact of the applied function on IOP values. The paper also shows the possibilities of implementation of the methodology for the movement of single vehicles or unmanned ground vehicles (UGVs) by using detailed maps as well as for determining routes for large military operational units moving in a 1 km wide corridor. The obtained results show that the change in β value causes the change of a course of the route as expected and that Dijkstra’s algorithm is more stable and slightly faster than A-star. The area of application of the presented methodology is very wide because, except for planning the movement of unmanned ground vehicles or military units of different sizes, it can be used in crisis management, where the possibility of reaching the area outside the road network can be of key importance for the success of the salvage operation.},
DOI = {10.3390/s21144682}
}



@Article{su13147662,
AUTHOR = {Zhang, Jingyi and Liu, Jiaxin and Chen, Yaqi and Feng, Xiaochun and Sun, Zilai},
TITLE = {Knowledge Mapping of Machine Learning Approaches Applied in Agricultural Management—A Scientometric Review with CiteSpace},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {7662},
URL = {https://www.mdpi.com/2071-1050/13/14/7662},
ISSN = {2071-1050},
ABSTRACT = {With the continuous development of the Internet of Things, artificial intelligence, big data technology, and intelligent agriculture have become hot topics in agricultural science and technology research. Machine learning is one of the core topics in artificial intelligence, and its application has penetrated every aspect of human social life. In modern agricultural intelligent management and decision making, machine learning plays an important role in crop classification, crop disease and insect pest prediction, agricultural product price prediction, and other aspects of management and decision-making processes in agriculture. To detect and recognize the latest research developing features in a quantitative and visual way, and based on machine learning methods in agricultural management, the authors of this paper used CiteSpace bibliometric methods to analyze relevant studies on the development process and hot spots. High-value references, productive authors, country and institution distributions, journal visualizations, research topics, and emerging trends were reviewed and analyzed. According to the keyword visualization and high-value references, machine learning approaches focus on sustainable agriculture, water resources, remote sensing, and machine learning methods. The research mainly focuses on six topics: learning technology, land environment, reference evapotranspiration, decision support systems for river geography, soil management, and winter wheat, while learning technology has been the most popular in recent years.},
DOI = {10.3390/su13147662}
}



@Article{rs13142706,
AUTHOR = {Huang, Shenjin and Han, Wenting and Chen, Haipeng and Li, Guang and Tang, Jiandong},
TITLE = {Recognizing Zucchinis Intercropped with Sunflowers in UAV Visible Images Using an Improved Method Based on OCRNet},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2706},
URL = {https://www.mdpi.com/2072-4292/13/14/2706},
ISSN = {2072-4292},
ABSTRACT = {An improved semantic segmentation method based on object contextual representations network (OCRNet) is proposed to accurately identify zucchinis intercropped with sunflowers from unmanned aerial vehicle (UAV) visible images taken over Hetao Irrigation District, Inner Mongolia, China. The proposed method improves on the performance of OCRNet in two respects. First, based on the object region context extraction structure of the OCRNet, a branch that uses the channel attention module was added in parallel to rationally use channel feature maps with different weights and reduce the noise of invalid channel features. Secondly, Lovász-Softmax loss was introduced to improve the accuracy of the object region representation in the OCRNet and optimize the final segmentation result at the object level. We compared the proposed method with extant advanced semantic segmentation methods (PSPNet, DeepLabV3+, DNLNet, and OCRNet) in two test areas to test its effectiveness. The results showed that the proposed method achieved the best semantic segmentation effect in the two test areas. More specifically, our method performed better in processing image details, segmenting field edges, and identifying intercropping fields. The proposed method has significant advantages for crop classification and intercropping recognition based on UAV visible images, and these advantages are more substantive in object-level evaluation metrics (mIoU and intercropping IoU).},
DOI = {10.3390/rs13142706}
}



@Article{land10070723,
AUTHOR = {Kelm, Kathrine and Antos, Sarah and McLaren, Robin},
TITLE = {Applying the FFP Approach to Wider Land Management Functions},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {723},
URL = {https://www.mdpi.com/2073-445X/10/7/723},
ISSN = {2073-445X},
ABSTRACT = {The initial focus of implementing the Fit-for-Purpose Land Administration (FFPLA) methodology was to address the significant, global security of tenure divide. We argue that this land tenure methodology is proving successful in scaling up the provision of security of tenure for developing countries. The increasing adoption of the FFPLA methodology has also opened opportunities and provided flexibility for the innovative use of emerging technologies to accelerate the global roll out of security of tenure, such as the use of autonomous drones and machine learning techniques applied to image analysis. Despite wider adoption of participatory approaches to the recording of land tenure, similar FFP solutions for the other components of land administration services (land value, land use and land development) and land management functions are still evolving. This article therefore explores how the FFP approach can be applied to this wider set of land administration services and land management functions. A case study methodology, using three case studies, is used to determine if the case study approaches meet the FFP criteria. The focus is on the urban environment, drawing mostly from experiences and case studies in the Urban, Disaster Risk Management, Resilience &amp; Land Global Practice of the World Bank. These opportunities for the wider application of the FFP approach and associated principles are being triggered by the innovative use of emerging new data capture technology developments. The paper examines the innovative use of these emerging technologies to identify a common set of data capture techniques and geospatial data that can be shared across a range of urban land administration and management activities. Finally, the paper discusses how individual land projects could be integrated into a more holistic land administration and management program approach and deliver a significant set of socio-economic benefits more quickly. It is found that the FFP approach can be more widely adopted across land administration and land management and in many cases can share a common set of geospatial data. The authors argue that the wider adoption and integration of these new, innovative FFP urban management approaches will require a significant cultural, professional, and institutional change from all stakeholders. Future work will explore more deeply these institutional weaknesses, which will provide a basis for guidance to the World Bank and similar institutions.},
DOI = {10.3390/land10070723}
}



@Article{s21144726,
AUTHOR = {Pytka, Jarosław Alexander and Budzyński, Piotr and Tomiło, Paweł and Michałowska, Joanna and Gnapowski, Ernest and Błażejczak, Dariusz and Łukaszewicz, Andrzej},
TITLE = {IMUMETER—A Convolution Neural Network-Based Sensor for Measurement of Aircraft Ground Performance},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4726},
URL = {https://www.mdpi.com/1424-8220/21/14/4726},
PubMedID = {34300466},
ISSN = {1424-8220},
ABSTRACT = {The paper presents the development of the IMUMETER sensor, designed to study the dynamics of aircraft movement, in particular, to measure the ground performance of the aircraft. A motivation of this study was to develop a sensor capable of airplane motion measurement, especially for airfield performance, takeoff and landing. The IMUMETER sensor was designed on the basis of the method of artificial neural networks. The use of a neural network is justified by the fact that the automation of the measurement of the airplane’s ground distance during landing based on acceleration data is possible thanks to the recognition of the touchdown and stopping points, using artificial intelligence. The hardware is based on a single-board computer that works with the inertial navigation platform and a satellite navigation sensor. In the development of the IMUMETER device, original software solutions were developed and tested. The paper describes the development of the Convolution Neural Network, including the learning process based on the measurement results during flight tests of the PZL 104 Wilga 35A aircraft. The ground distance of the test airplane during landing on a grass runway was calculated using the developed neural network model. Additionally included are exemplary measurements of the landing distance of the test airplane during landing on a grass runway. The results obtained in this study can be useful in the development of artificial intelligence-based sensors, especially those for the measurement and analysis of aircraft flight dynamics.},
DOI = {10.3390/s21144726}
}



@Article{s21144738,
AUTHOR = {Abdollahi, Abolfazl and Pradhan, Biswajeet},
TITLE = {Urban Vegetation Mapping from Aerial Imagery Using Explainable AI (XAI)},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4738},
URL = {https://www.mdpi.com/1424-8220/21/14/4738},
PubMedID = {34300478},
ISSN = {1424-8220},
ABSTRACT = {Urban vegetation mapping is critical in many applications, i.e., preserving biodiversity, maintaining ecological balance, and minimizing the urban heat island effect. It is still challenging to extract accurate vegetation covers from aerial imagery using traditional classification approaches, because urban vegetation categories have complex spatial structures and similar spectral properties. Deep neural networks (DNNs) have shown a significant improvement in remote sensing image classification outcomes during the last few years. These methods are promising in this domain, yet unreliable for various reasons, such as the use of irrelevant descriptor features in the building of the models and lack of quality in the labeled image. Explainable AI (XAI) can help us gain insight into these limits and, as a result, adjust the training dataset and model as needed. Thus, in this work, we explain how an explanation model called Shapley additive explanations (SHAP) can be utilized for interpreting the output of the DNN model that is designed for classifying vegetation covers. We want to not only produce high-quality vegetation maps, but also rank the input parameters and select appropriate features for classification. Therefore, we test our method on vegetation mapping from aerial imagery based on spectral and textural features. Texture features can help overcome the limitations of poor spectral resolution in aerial imagery for vegetation mapping. The model was capable of obtaining an overall accuracy (OA) of 94.44% for vegetation cover mapping. The conclusions derived from SHAP plots demonstrate the high contribution of features, such as Hue, Brightness, GLCM_Dissimilarity, GLCM_Homogeneity, and GLCM_Mean to the output of the proposed model for vegetation mapping. Therefore, the study indicates that existing vegetation mapping strategies based only on spectral characteristics are insufficient to appropriately classify vegetation covers.},
DOI = {10.3390/s21144738}
}



@Article{ijgi10070482,
AUTHOR = {Xing, Zhizhong and Zhao, Shuanfeng and Guo, Wei and Guo, Xiaojun and Wang, Yuan},
TITLE = {Processing Laser Point Cloud in Fully Mechanized Mining Face Based on DGCNN},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {482},
URL = {https://www.mdpi.com/2220-9964/10/7/482},
ISSN = {2220-9964},
ABSTRACT = {Point cloud data can accurately and intuitively reflect the spatial relationship between the coal wall and underground fully mechanized mining equipment. However, the indirect method of point cloud feature extraction based on deep neural networks will lose some of the spatial information of the point cloud, while the direct method will lose some of the local information of the point cloud. Therefore, we propose the use of dynamic graph convolution neural network (DGCNN) to extract the geometric features of the sphere in the point cloud of the fully mechanized mining face (FMMF) in order to obtain the position of the sphere (marker) in the point cloud of the FMMF, thus providing a direct basis for the subsequent transformation of the FMMF coordinates to the national geodetic coordinates with the sphere as the intermediate medium. Firstly, we completed the production of a diversity sphere point cloud (training set) and an FMMF point cloud (test set). Secondly, we further improved the DGCNN to enhance the effect of extracting the geometric features of the sphere in the FMMF. Finally, we compared the effect of the improved DGCNN with that of PointNet and PointNet++. The results show the correctness and feasibility of using DGCNN to extract the geometric features of point clouds in the FMMF and provide a new method for the feature extraction of point clouds in the FMMF. At the same time, the results provide a direct early guarantee for analyzing the point cloud data of the FMMF under the national geodetic coordinate system in the future. This can provide an effective basis for the straightening and inclining adjustment of scraper conveyors, and it is of great significance for the transparent, unmanned, and intelligent mining of the FMMF.},
DOI = {10.3390/ijgi10070482}
}



@Article{drones5030060,
AUTHOR = {Jumaah, Huda Jamal and Kalantar, Bahareh and Halin, Alfian Abdul and Mansor, Shattri and Ueda, Naonori and Jumaah, Sarah Jamal},
TITLE = {Development of UAV-Based PM2.5 Monitoring System},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {60},
URL = {https://www.mdpi.com/2504-446X/5/3/60},
ISSN = {2504-446X},
ABSTRACT = {This paper proposes a UAV-based PM2.5 air quality and temperature-humidity monitoring system. The system includes an air quality detector comprising four Arduino sensor modules. Specifically, it includes a dust (DSM501A) sensor and a temperature and humidity (DHT11) sensor. The NEO-6M GPS module and DS3231 real-time module are also included for input visualization. A DIY SD card logging shield and memory module is also available for data recording purposes. The Arduino-based board houses multiple sensors and all are programmable using the Arduino integrated development environment (IDE) coding tool. Measurements conducted in a vertical flight path show promise where comparisons with ground truth references data showed good similarity. Overall, the results point to the idea that a light-weight and portable system can be used for accurate and reliable remote sensing data collection (in this case, PM2.5 concentration data and environmental data).},
DOI = {10.3390/drones5030060}
}



@Article{rs13142751,
AUTHOR = {Wengert, Matthias and Piepho, Hans-Peter and Astor, Thomas and Graß, Rüdiger and Wijesingha, Jayan and Wachendorf, Michael},
TITLE = {Assessing Spatial Variability of Barley Whole Crop Biomass Yield and Leaf Area Index in Silvoarable Agroforestry Systems Using UAV-Borne Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2751},
URL = {https://www.mdpi.com/2072-4292/13/14/2751},
ISSN = {2072-4292},
ABSTRACT = {Agroforestry systems (AFS) can provide positive ecosystem services while at the same time stabilizing yields under increasingly common drought conditions. The effect of distance to trees in alley cropping AFS on yield-related crop parameters has predominantly been studied using point data from transects. Unmanned aerial vehicles (UAVs) offer a novel possibility to map plant traits with high spatial resolution and coverage. In the present study, UAV-borne red, green, blue (RGB) and multispectral imagery was utilized for the prediction of whole crop dry biomass yield (DM) and leaf area index (LAI) of barley at three different conventionally managed silvoarable alley cropping agroforestry sites located in Germany. DM and LAI were modelled using random forest regression models with good accuracies (DM: R² 0.62, nRMSEp 14.9%, LAI: R² 0.92, nRMSEp 7.1%). Important variables for prediction included normalized reflectance, vegetation indices, texture and plant height. Maps were produced from model predictions for spatial analysis, showing significant effects of distance to trees on DM and LAI. Spatial patterns differed greatly between the sampled sites and suggested management and soil effects overriding tree effects across large portions of 96 m wide crop alleys, thus questioning alleged impacts of AFS tree rows on yield distribution in intensively managed barley populations. Models based on UAV-borne imagery proved to be a valuable novel tool for prediction of DM and LAI at high accuracies, revealing spatial variability in AFS with high spatial resolution and coverage.},
DOI = {10.3390/rs13142751}
}



@Article{agronomy11071409,
AUTHOR = {Anderson, Nicholas Todd and Walsh, Kerry Brian and Wulfsohn, Dvoralai},
TITLE = {Technologies for Forecasting Tree Fruit Load and Harvest Timing—From Ground, Sky and Time},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1409},
URL = {https://www.mdpi.com/2073-4395/11/7/1409},
ISSN = {2073-4395},
ABSTRACT = {The management and marketing of fruit requires data on expected numbers, size, quality and timing. Current practice estimates orchard fruit load based on the qualitative assessment of fruit number per tree and historical orchard yield, or manually counting a subsample of trees. This review considers technological aids assisting these estimates, in terms of: (i) improving sampling strategies by the number of units to be counted and their selection; (ii) machine vision for the direct measurement of fruit number and size on the canopy; (iii) aerial or satellite imagery for the acquisition of information on tree structural parameters and spectral indices, with the indirect assessment of fruit load; (iv) models extrapolating historical yield data with knowledge of tree management and climate parameters, and (v) technologies relevant to the estimation of harvest timing such as heat units and the proximal sensing of fruit maturity attributes. Machine vision is currently dominating research outputs on fruit load estimation, while the improvement of sampling strategies has potential for a widespread impact. Techniques based on tree parameters and modeling offer scalability, but tree crops are complicated (perennialism). The use of machine vision for flowering estimates, fruit sizing, external quality evaluation is also considered. The potential synergies between technologies are highlighted.},
DOI = {10.3390/agronomy11071409}
}



@Article{rs13142774,
AUTHOR = {Chandler, Chris J. and van der Heijden, Geertje M. F. and Boyd, Doreen S. and Foody, Giles M.},
TITLE = {Detection of Spatial and Temporal Patterns of Liana Infestation Using Satellite-Derived Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2774},
URL = {https://www.mdpi.com/2072-4292/13/14/2774},
ISSN = {2072-4292},
ABSTRACT = {Lianas (woody vines) play a key role in tropical forest dynamics because of their strong influence on tree growth, mortality and regeneration. Assessing liana infestation over large areas is critical to understand the factors that drive their spatial distribution and to monitor change over time. However, it currently remains unclear whether satellite-based imagery can be used to detect liana infestation across closed-canopy forests and therefore if satellite-observed changes in liana infestation can be detected over time and in response to climatic conditions. Here, we aim to determine the efficacy of satellite-based remote sensing for the detection of spatial and temporal patterns of liana infestation across a primary and selectively logged aseasonal forest in Sabah, Borneo. We used predicted liana infestation derived from airborne hyperspectral data to train a neural network classification for prediction across four Sentinel-2 satellite-based images from 2016 to 2019. Our results showed that liana infestation was positively related to an increase in Greenness Index (GI), a simple metric relating to the amount of photosynthetically active green leaves. Furthermore, this relationship was observed in different forest types and during (2016), as well as after (2017–2019), an El Niño-induced drought. Using a neural network classification, we assessed liana infestation over time and showed an increase in the percentage of severely (&gt;75%) liana infested pixels from 12.9% ± 0.63 (95% CI) in 2016 to 17.3% ± 2 in 2019. This implies that reports of increasing liana abundance may be more wide-spread than currently assumed. This is the first study to show that liana infestation can be accurately detected across closed-canopy tropical forests using satellite-based imagery. Furthermore, the detection of liana infestation during both dry and wet years and across forest types suggests this method should be broadly applicable across tropical forests. This work therefore advances our ability to explore the drivers responsible for patterns of liana infestation at multiple spatial and temporal scales and to quantify liana-induced impacts on carbon dynamics in tropical forests globally.},
DOI = {10.3390/rs13142774}
}



@Article{rs13142776,
AUTHOR = {Li, Yong and Shao, Zhenfeng and Huang, Xiao and Cai, Bowen and Peng, Song},
TITLE = {Meta-FSEO: A Meta-Learning Fast Adaptation with Self-Supervised Embedding Optimization for Few-Shot Remote Sensing Scene Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2776},
URL = {https://www.mdpi.com/2072-4292/13/14/2776},
ISSN = {2072-4292},
ABSTRACT = {The performance of deep learning is heavily influenced by the size of the learning samples, whose labeling process is time consuming and laborious. Deep learning algorithms typically assume that the training and prediction data are independent and uniformly distributed, which is rarely the case given the attributes and properties of different data sources. In remote sensing images, representations of urban land surfaces can vary across regions and by season, demanding rapid generalization of these surfaces in remote sensing data. In this study, we propose Meta-FSEO, a novel model for improving the performance of few-shot remote sensing scene classification in varying urban scenes. The proposed Meta-FSEO model deploys self-supervised embedding optimization for adaptive generalization in new tasks such as classifying features in new urban regions that have never been encountered during the training phase, thus balancing the requirements for feature classification tasks between multiple images collected at different times and places. We also created a loss function by weighting the contrast losses and cross-entropy losses. The proposed Meta-FSEO demonstrates a great generalization capability in remote sensing scene classification among different cities. In a five-way one-shot classification experiment with the Sentinel-1/2 Multi-Spectral (SEN12MS) dataset, the accuracy reached 63.08%. In a five-way five-shot experiment on the same dataset, the accuracy reached 74.29%. These results indicated that the proposed Meta-FSEO model outperformed both the transfer learning-based algorithm and two popular meta-learning-based methods, i.e., MAML and Meta-SGD.},
DOI = {10.3390/rs13142776}
}



@Article{rs13142780,
AUTHOR = {Shukla, Shivang and Tiddeman, Bernard and Miles, Helen C.},
TITLE = {A Wide Area Multiview Static Crowd Estimation System Using UAV and 3D Training Simulator},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2780},
URL = {https://www.mdpi.com/2072-4292/13/14/2780},
ISSN = {2072-4292},
ABSTRACT = {Crowd size estimation is a challenging problem, especially when the crowd is spread over a significant geographical area. It has applications in monitoring of rallies and demonstrations and in calculating the assistance requirements in humanitarian disasters. Therefore, accomplishing a crowd surveillance system for large crowds constitutes a significant issue. UAV-based techniques are an appealing choice for crowd estimation over a large region, but they present a variety of interesting challenges, such as integrating per-frame estimates through a video without counting individuals twice. Large quantities of annotated training data are required to design, train, and test such a system. In this paper, we have first reviewed several crowd estimation techniques, existing crowd simulators and data sets available for crowd analysis. Later, we have described a simulation system to provide such data, avoiding the need for tedious and error-prone manual annotation. Then, we have evaluated synthetic video from the simulator using various existing single-frame crowd estimation techniques. Our findings show that the simulated data can be used to train and test crowd estimation, thereby providing a suitable platform to develop such techniques. We also propose an automated UAV-based 3D crowd estimation system that can be used for approximately static or slow-moving crowds, such as public events, political rallies, and natural or man-made disasters. We evaluate the results by applying our new framework to a variety of scenarios with varying crowd sizes. The proposed system gives promising results using widely accepted metrics including MAE, RMSE, Precision, Recall, and F1 score to validate the results.},
DOI = {10.3390/rs13142780}
}



@Article{s21144845,
AUTHOR = {Li, Jingbo and Li, Changchun and Fei, Shuaipeng and Ma, Chunyan and Chen, Weinan and Ding, Fan and Wang, Yilin and Li, Yacong and Shi, Jinjin and Xiao, Zhen},
TITLE = {Wheat Ear Recognition Based on RetinaNet and Transfer Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4845},
URL = {https://www.mdpi.com/1424-8220/21/14/4845},
PubMedID = {34300585},
ISSN = {1424-8220},
ABSTRACT = {The number of wheat ears is an essential indicator for wheat production and yield estimation, but accurately obtaining wheat ears requires expensive manual cost and labor time. Meanwhile, the characteristics of wheat ears provide less information, and the color is consistent with the background, which can be challenging to obtain the number of wheat ears required. In this paper, the performance of Faster regions with convolutional neural networks (Faster R-CNN) and RetinaNet to predict the number of wheat ears for wheat at different growth stages under different conditions is investigated. The results show that using the Global WHEAT dataset for recognition, the RetinaNet method, and the Faster R-CNN method achieve an average accuracy of 0.82 and 0.72, with the RetinaNet method obtaining the highest recognition accuracy. Secondly, using the collected image data for recognition, the R2 of RetinaNet and Faster R-CNN after transfer learning is 0.9722 and 0.8702, respectively, indicating that the recognition accuracy of the RetinaNet method is higher on different data sets. We also tested wheat ears at both the filling and maturity stages; our proposed method has proven to be very robust (the R2 is above 90). This study provides technical support and a reference for automatic wheat ear recognition and yield estimation.},
DOI = {10.3390/s21144845}
}



@Article{rs13142792,
AUTHOR = {Jiang, Fugen and Chen, Chuanshi and Li, Chengjie and Kutia, Mykola and Sun, Hua},
TITLE = {A Novel Spatial Simulation Method for Mapping the Urban Forest Carbon Density in Southern China by the Google Earth Engine},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2792},
URL = {https://www.mdpi.com/2072-4292/13/14/2792},
ISSN = {2072-4292},
ABSTRACT = {Urban forest is an important component of terrestrial ecosystems and is highly related to global climate change. However, because of complex city landscapes, deriving the spatial distribution of urban forest carbon density and conducting accuracy assessments are difficult. This study proposes a novel spatial simulation method, optimized geographically weighted logarithm regression (OGWLR), using Landsat 8 data acquired by the Google Earth Engine (GEE) and field survey data to map the forest carbon density of Shenzhen city in southern China. To verify the effectiveness of the novel method, multiple linear regression (MLR), k-nearest neighbors (kNN), random forest (RF) and geographically weighted regression (GWR) models were established for comparison. The results showed that OGWLR achieved the highest coefficient of determination (R2 = 0.54) and the lowest root mean square error (RMSE = 13.28 Mg/ha) among all estimation models. In addition, OGWLR achieved a more consistent spatial distribution of carbon density with the actual situation. The carbon density of the forests in the study area was large in the central and western regions and coastal areas and small in the building and road areas. Therefore, this method can provide a new reference for urban forest carbon density estimation and mapping.},
DOI = {10.3390/rs13142792}
}



@Article{rs13142794,
AUTHOR = {Ran, Shuhao and Gao, Xianjun and Yang, Yuanwei and Li, Shaohua and Zhang, Guangbin and Wang, Ping},
TITLE = {Building Multi-Feature Fusion Refined Network for Building Extraction from High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2794},
URL = {https://www.mdpi.com/2072-4292/13/14/2794},
ISSN = {2072-4292},
ABSTRACT = {Deep learning approaches have been widely used in building automatic extraction tasks and have made great progress in recent years. However, the missing detection and wrong detection causing by spectrum confusion is still a great challenge. The existing fully convolutional networks (FCNs) cannot effectively distinguish whether the feature differences are from one building or the building and its adjacent non-building objects. In order to overcome the limitations, a building multi-feature fusion refined network (BMFR-Net) was presented in this paper to extract buildings accurately and completely. BMFR-Net is based on an encoding and decoding structure, mainly consisting of two parts: the continuous atrous convolution pyramid (CACP) module and the multiscale output fusion constraint (MOFC) structure. The CACP module is positioned at the end of the contracting path and it effectively minimizes the loss of effective information in multiscale feature extraction and fusion by using parallel continuous small-scale atrous convolution. To improve the ability to aggregate semantic information from the context, the MOFC structure performs predictive output at each stage of the expanding path and integrates the results into the network. Furthermore, the multilevel joint weighted loss function effectively updates parameters well away from the output layer, enhancing the learning capacity of the network for low-level abstract features. The experimental results demonstrate that the proposed BMFR-Net outperforms the other five state-of-the-art approaches in both visual interpretation and quantitative evaluation.},
DOI = {10.3390/rs13142794}
}



@Article{f12070943,
AUTHOR = {Vásquez, Felipe and Cravero, Ania and Castro, Manuel and Acevedo, Patricio},
TITLE = {Decision Support System Development of Wildland Fire: A Systematic Mapping},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {943},
URL = {https://www.mdpi.com/1999-4907/12/7/943},
ISSN = {1999-4907},
ABSTRACT = {Wildland fires have been a rising problem on the worldwide level, generating ecological and economic losses. Specifically, between wildland fire types, uncontrolled fires are critical due to the potential damage to the ecosystem and their effects on the soil, and, in the last decade, different technologies have been applied to fight them. Selecting a specific technology and Decision Support Systems (DSS) is fundamental, since the results and validity of this could drastically oscillate according to the different environmental and geographic factors of the terrain to be studied. Given the above, a systematic mapping was realized, with the purpose of recognizing the most-used DSS and context where they have been applied. One hundred and eighty-three studies were found that used different types of DSS to solve problems of detection, prediction, prevention, monitoring, simulation, administration, and access to routes. The concepts key to the type of solution are related to the use or development of systems or Information and Communication Technologies (ICT) in the computer science area. Although the use of BA and Big Data has increased in recent years, there are still many challenges to face, such as staff training, the friendly environment of DSS, and real-time decision-making.},
DOI = {10.3390/f12070943}
}



@Article{rs13142822,
AUTHOR = {Lin, Zhe and Guo, Wenxuan},
TITLE = {Cotton Stand Counting from Unmanned Aerial System Imagery Using MobileNet and CenterNet Deep Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2822},
URL = {https://www.mdpi.com/2072-4292/13/14/2822},
ISSN = {2072-4292},
ABSTRACT = {An accurate stand count is a prerequisite to determining the emergence rate, assessing seedling vigor, and facilitating site-specific management for optimal crop production. Traditional manual counting methods in stand assessment are labor intensive and time consuming for large-scale breeding programs or production field operations. This study aimed to apply two deep learning models, the MobileNet and CenterNet, to detect and count cotton plants at the seedling stage with unmanned aerial system (UAS) images. These models were trained with two datasets containing 400 and 900 images with variations in plant size and soil background brightness. The performance of these models was assessed with two testing datasets of different dimensions, testing dataset 1 with 300 by 400 pixels and testing dataset 2 with 250 by 1200 pixels. The model validation results showed that the mean average precision (mAP) and average recall (AR) were 79% and 73% for the CenterNet model, and 86% and 72% for the MobileNet model with 900 training images. The accuracy of cotton plant detection and counting was higher with testing dataset 1 for both CenterNet and MobileNet models. The results showed that the CenterNet model had a better overall performance for cotton plant detection and counting with 900 training images. The results also indicated that more training images are required when applying object detection models on images with different dimensions from training datasets. The mean absolute percentage error (MAPE), coefficient of determination (R2), and the root mean squared error (RMSE) values of the cotton plant counting were 0.07%, 0.98 and 0.37, respectively, with testing dataset 1 for the CenterNet model with 900 training images. Both MobileNet and CenterNet models have the potential to accurately and timely detect and count cotton plants based on high-resolution UAS images at the seedling stage. This study provides valuable information for selecting the right deep learning tools and the appropriate number of training images for object detection projects in agricultural applications.},
DOI = {10.3390/rs13142822}
}



@Article{agronomy11071435,
AUTHOR = {Che’Ya, Nik Norasma and Dunwoody, Ernest and Gupta, Madan},
TITLE = {Assessment of Weed Classification Using Hyperspectral Reflectance and Optimal Multispectral UAV Imagery},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1435},
URL = {https://www.mdpi.com/2073-4395/11/7/1435},
ISSN = {2073-4395},
ABSTRACT = {Weeds compete with crops and are hard to differentiate and identify due to their similarities in color, shape, and size. In this study, the weed species present in sorghum (sorghum bicolor (L.) Moench) fields, such as amaranth (Amaranthus macrocarpus), pigweed (Portulaca oleracea), mallow weed (Malva sp.), nutgrass (Cyperus rotundus), liver seed grass (Urochoa panicoides), and Bellive (Ipomea plebeian), were discriminated using hyperspectral data and were detected and analyzed using multispectral images. Discriminant analysis (DA) was used to identify the most significant spectral bands in order to discriminate weeds from sorghum using hyperspectral data. The results demonstrated good separation accuracy for Amaranthus macrocarpus, Urochoa panicoides, Malva sp., Cyperus rotundus, and Sorghum bicolor (L.) Moench at 440, 560, 680, 710, 720, and 850 nm. Later, the multispectral images of these six bands were collected to detect weeds in the sorghum crop fields using object-based image analysis (OBIA). The results showed that the differences between sorghum and weed species were detectable using the six selected bands, with data collected using an unmanned aerial vehicle. Here, the highest spatial resolution had the highest accuracy for weed detection. It was concluded that each weed was successfully discriminated using hyperspectral data and was detectable using multispectral data with higher spatial resolution.},
DOI = {10.3390/agronomy11071435}
}



@Article{electronics10141737,
AUTHOR = {Lee, Wooseop and Kang, Min-Hee and Song, Jaein and Hwang, Keeyeon},
TITLE = {The Design of Preventive Automated Driving Systems Based on Convolutional Neural Network},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1737},
URL = {https://www.mdpi.com/2079-9292/10/14/1737},
ISSN = {2079-9292},
ABSTRACT = {As automated vehicles have been considered one of the important trends in intelligent transportation systems, various research is being conducted to enhance their safety. In particular, the importance of technologies for the design of preventive automated driving systems, such as detection of surrounding objects and estimation of distance between vehicles. Object detection is mainly performed through cameras and LiDAR, but due to the cost and limits of LiDAR’s recognition distance, the need to improve Camera recognition technique, which is relatively convenient for commercialization, is increasing. This study learned convolutional neural network (CNN)-based faster regions with CNN (Faster R-CNN) and You Only Look Once (YOLO) V2 to improve the recognition techniques of vehicle-mounted monocular cameras for the design of preventive automated driving systems, recognizing surrounding vehicles in black box highway driving videos and estimating distances from surrounding vehicles through more suitable models for automated driving systems. Moreover, we learned the PASCAL visual object classes (VOC) dataset for model comparison. Faster R-CNN showed similar accuracy, with a mean average precision (mAP) of 76.4 to YOLO with a mAP of 78.6, but with a Frame Per Second (FPS) of 5, showing slower processing speed than YOLO V2 with an FPS of 40, and a Faster R-CNN, which we had difficulty detecting. As a result, YOLO V2, which shows better performance in accuracy and processing speed, was determined to be a more suitable model for automated driving systems, further progressing in estimating the distance between vehicles. For distance estimation, we conducted coordinate value conversion through camera calibration and perspective transform, set the threshold to 0.7, and performed object detection and distance estimation, showing more than 80% accuracy for near-distance vehicles. Through this study, it is believed that it will be able to help prevent accidents in automated vehicles, and it is expected that additional research will provide various accident prevention alternatives such as calculating and securing appropriate safety distances, depending on the vehicle types.},
DOI = {10.3390/electronics10141737}
}



@Article{rs13142833,
AUTHOR = {Wei, Xing and Johnson, Marcela A. and Langston, David B. and Mehl, Hillary L. and Li, Song},
TITLE = {Identifying Optimal Wavelengths as Disease Signatures Using Hyperspectral Sensor and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {2833},
URL = {https://www.mdpi.com/2072-4292/13/14/2833},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral sensors combined with machine learning are increasingly utilized in agricultural crop systems for diverse applications, including plant disease detection. This study was designed to identify the most important wavelengths to discriminate between healthy and diseased peanut (Arachis hypogaea L.) plants infected with Athelia rolfsii, the causal agent of peanut stem rot, using in-situ spectroscopy and machine learning. In greenhouse experiments, daily measurements were conducted to inspect disease symptoms visually and to collect spectral reflectance of peanut leaves on lateral stems of plants mock-inoculated and inoculated with A. rolfsii. Spectrum files were categorized into five classes based on foliar wilting symptoms. Five feature selection methods were compared to select the top 10 ranked wavelengths with and without a custom minimum distance of 20 nm. Recursive feature elimination methods outperformed the chi-square and SelectFromModel methods. Adding the minimum distance of 20 nm into the top selected wavelengths improved classification performance. Wavelengths of 501–505, 690–694, 763 and 884 nm were repeatedly selected by two or more feature selection methods. These selected wavelengths can be applied in designing optical sensors for automated stem rot detection in peanut fields. The machine-learning-based methodology can be adapted to identify spectral signatures of disease in other plant-pathogen systems.},
DOI = {10.3390/rs13142833}
}



@Article{en14144365,
AUTHOR = {Liu, Jingjing and Liu, Chuanyang and Wu, Yiquan and Xu, Huajie and Sun, Zuo},
TITLE = {An Improved Method Based on Deep Learning for Insulator Fault Detection in Diverse Aerial Images},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4365},
URL = {https://www.mdpi.com/1996-1073/14/14/4365},
ISSN = {1996-1073},
ABSTRACT = {Insulators play a significant role in high-voltage transmission lines, and detecting insulator faults timely and accurately is important for the safe and stable operation of power grids. Since insulator faults are extremely small and the backgrounds of aerial images are complex, insulator fault detection is a challenging task for automatically inspecting transmission lines. In this paper, a method based on deep learning is proposed for insulator fault detection in diverse aerial images. Firstly, to provide sufficient insulator fault images for training, a novel insulator fault dataset named “InSF-detection” is constructed. Secondly, an improved YOLOv3 model is proposed to reuse features and prevent feature loss. To improve the accuracy of insulator fault detection, SPP-networks and a multi-scale prediction network are employed for the improved YOLOv3 model. Finally, the improved YOLOv3 model and the compared models are trained and tested on the “InSF-detection”. The average precision (AP) of the improved YOLOv3 model is superior to YOLOv3 and YOLOv3-dense models, and just a little (1.2%) lower than that of CSPD-YOLO model; more importantly, the memory usage of the improved YOLOv3 model is 225 MB, which is the smallest between the four compared models. The experimental results and analysis validate that the improved YOLOv3 model achieves good performance for insulator fault detection in aerial images with diverse backgrounds.},
DOI = {10.3390/en14144365}
}



@Article{s21144929,
AUTHOR = {Hallee, Mitchell J. and Napolitano, Rebecca K. and Reinhart, Wesley F. and Glisic, Branko},
TITLE = {Crack Detection in Images of Masonry Using CNNs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {4929},
URL = {https://www.mdpi.com/1424-8220/21/14/4929},
PubMedID = {34300668},
ISSN = {1424-8220},
ABSTRACT = {While there is a significant body of research on crack detection by computer vision methods in concrete and asphalt, less attention has been given to masonry. We train a convolutional neural network (CNN) on images of brick walls built in a laboratory environment and test its ability to detect cracks in images of brick-and-mortar structures both in the laboratory and on real-world images taken from the internet. We also compare the performance of the CNN to a variety of simpler classifiers operating on handcrafted features. We find that the CNN performed better on the domain adaptation from laboratory to real-world images than these simple models. However, we also find that performance is significantly better in performing the reverse domain adaptation task, where the simple classifiers are trained on real-world images and tested on the laboratory images. This work demonstrates the ability to detect cracks in images of masonry using a variety of machine learning methods and provides guidance for improving the reliability of such models when performing domain adaptation for crack detection in masonry.},
DOI = {10.3390/s21144929}
}



@Article{electronics10141744,
AUTHOR = {Wazirali, Raniyah and Ahmad, Rami and Al-Amayreh, Ahmed and Al-Madi, Mohammad and Khalifeh, Ala’},
TITLE = {Secure Watermarking Schemes and Their Approaches in the IoT Technology: An Overview},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {14},
ARTICLE-NUMBER = {1744},
URL = {https://www.mdpi.com/2079-9292/10/14/1744},
ISSN = {2079-9292},
ABSTRACT = {Information security is considered one of the most important issues in various infrastructures related to the field of data communication where most of the modern studies focus on finding effective and low-weight secure approaches. Digital watermarking is a trend in security techniques that hides data by using data embedding and data extraction processes. Watermarking technology is integrated into different frames without adding an overheard as in the conventional encryption. Therefore, it is efficient to be used in data encryption for applications that run over limited resources such as the Internet of Things (IoT). In this paper, different digital watermarking algorithms and approaches are presented. Additionally, watermarking requirements and challenges are illustrated in detail. Moreover, the common architecture of the watermarking system is described. Furthermore, IoT technology and its challenges are highlighted. Finally, the paper provides the motivations, objectives and applications of the recent secure watermarking techniques in IoT and summarises them into one table. In addition, the paper highlights the potential to apply the modified watermark algorithms to secure IoT networks.},
DOI = {10.3390/electronics10141744}
}



@Article{electronics10151748,
AUTHOR = {Wei, Baoquan and Zuo, Yong and Liu, Yande and Luo, Wei and Wen, Kaiyun and Deng, Fangming},
TITLE = {Novel MOA Fault Detection Technology Based on Small Sample Infrared Image},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {1748},
URL = {https://www.mdpi.com/2079-9292/10/15/1748},
ISSN = {2079-9292},
ABSTRACT = {This paper proposes a novel metal oxide arrester (MOA) fault detection technology based on a small sample infrared image. The research is carried out from the detection process and data enhancement. A lightweight MOA identification and location algorithm is designed at the edge, which can not only reduce the amount of data uploaded, but also reduce the search space of cloud algorithm. In order to improve the accuracy and generalization ability of the defect detection model under the condition of small samples, a multi-model fusion detection algorithm is proposed. Different features of the image are extracted by multiple convolutional neural networks, and then multiple classifiers are trained. Finally, the weighted voting strategy is used for fault diagnosis. In addition, the extended model of fault samples is constructed by transfer learning and deep convolutional generative adversarial networks (DCGAN) to solve the problem of unbalanced training data sets. The experimental results show that the proposed method can realize the accurate location of arrester under the condition of small samples, and after the data expansion, the recognition rate of arrester anomalies can be improved from 83% to 85%, showing high effectiveness and reliability.},
DOI = {10.3390/electronics10151748}
}



@Article{insects12080663,
AUTHOR = {Valdez-Delgado, Kenia Mayela and Moo-Llanes, David A. and Danis-Lozano, Rogelio and Cisneros-Vázquez, Luis Alberto and Flores-Suarez, Adriana E. and Ponce-García, Gustavo and Medina-De la Garza, Carlos E. and Díaz-González, Esteban E. and Fernández-Salas, Ildefonso},
TITLE = {Field Effectiveness of Drones to Identify Potential Aedes aegypti Breeding Sites in Household Environments from Tapachula, a Dengue-Endemic City in Southern Mexico},
JOURNAL = {Insects},
VOLUME = {12},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {663},
URL = {https://www.mdpi.com/2075-4450/12/8/663},
ISSN = {2075-4450},
ABSTRACT = {Aedes aegypti control programs require more sensitive tools in order to survey domestic and peridomestic larval habitats for dengue and other arbovirus prevention areas. As a consequence of the COVID-19 pandemic, field technicians have faced a new occupational hazard during their work activities in dengue surveillance and control. Safer strategies to monitor larval populations, in addition to minimum householder contact, are undoubtedly urgently needed. Drones can be part of the solution in urban and rural areas that are dengue-endemic. Throughout this study, the proportion of larvae breeding sites found in the roofs and backyards of houses were assessed using drone images. Concurrently, the traditional ground field technician’s surveillance was utilized to sample the same house groups. The results were analyzed in order to compare the effectiveness of both field surveillance approaches. Aerial images of 216 houses from El Vergel village in Tapachula, Chiapas, Mexico, at a height of 30 m, were obtained using a drone. Each household was sampled indoors and outdoors by vector control personnel targeting all the containers that potentially served as Aedes aegypti breeding sites. The main results were that the drone could find 1 container per 2.8 found by ground surveillance; however, containers that were inaccessible by technicians in roofs and backyards, such as plastic buckets and tubs, disposable plastic containers and flowerpots were more often detected by drones than traditional ground surveillance. This new technological approach would undoubtedly improve the surveillance of Aedes aegypti in household environments, and better vector control activities would therefore be achieved in dengue-endemic countries.},
DOI = {10.3390/insects12080663}
}



@Article{s21154961,
AUTHOR = {Chen, Xiao and Chen, Zhuang and Liu, Guoxiang and Chen, Kun and Wang, Lu and Xiang, Wei and Zhang, Rui},
TITLE = {Railway Overhead Contact System Point Cloud Classification},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {4961},
URL = {https://www.mdpi.com/1424-8220/21/15/4961},
PubMedID = {34372197},
ISSN = {1424-8220},
ABSTRACT = {As the railway overhead contact system (OCS) is the key component along the high-speed railway, it is crucial to detect the quality of the OCS. Compared with conventional manual OCS detection, the vehicle-mounted Light Detection and Ranging (LiDAR) technology has advantages such as high efficiency and precision, which can solve the problems of OCS detection difficulty, low efficiency, and high risk. Aiming at the contact cables, return current cables, and catenary cables in the railway vehicle-mounted LiDAR OCS point cloud, this paper used a scale adaptive feature classification algorithm and the DBSCAN (density-based spatial clustering of applications with noise) algorithm considering OCS characteristics to classify the OCS point cloud. Finally, the return current cables, catenary cables, and contact cables in the OCS were accurately classified and extracted. To verify the accuracy of the method presented in this paper, we compared the experimental results of this article with the classification results of TerraSolid, and the classification results were evaluated in terms of four accuracy indicators. According to statistics, the average accuracy of using this method to extract two sets of OCS point clouds is 99.83% and 99.89%, respectively; the average precision is 100% and 99.97%, respectively; the average recall is 99.16% and 99.42%, respectively; and the average overall accuracy is 99.58% and 99.69% respectively, which is overall better than TerraSolid. The experimental results showed that this approach could accurately and quickly extract the complete OCS from the point cloud. It provides a new method for processing railway OCS point clouds and has high engineering application value in railway component detection.},
DOI = {10.3390/s21154961}
}



@Article{agriculture11080687,
AUTHOR = {Mesa, Armacheska Rivero and Chiang, John Y.},
TITLE = {Multi-Input Deep Learning Model with RGB and Hyperspectral Imaging for Banana Grading},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {687},
URL = {https://www.mdpi.com/2077-0472/11/8/687},
ISSN = {2077-0472},
ABSTRACT = {Grading is a vital process during the postharvest of horticultural products as it dramatically affects consumer preference and satisfaction when goods reach the market. Manual grading is time-consuming, uneconomical, and potentially destructive. A non-invasive automated system for export-quality banana tiers was developed, which utilized RGB, hyperspectral imaging, and deep learning techniques. A real dataset of pre-classified banana tiers based on quality and size (Class 1 for export quality bananas, Class 2 for the local market, and Class 3 for defective fruits) was utilized using international standards. The multi-input model achieved an excellent overall accuracy of 98.45% using only a minimal number of samples compared to other methods in the literature. The model was able to incorporate both external and internal properties of the fruit. The size of the banana was used as a feature for grade classification as well as other morphological features using RGB imaging, while reflectance values that offer valuable information and have shown a high correlation with the internal features of fruits were obtained through hyperspectral imaging. This study highlighted the combined strengths of RGB and hyperspectral imaging in grading bananas, and this can serve as a paradigm for grading other horticultural crops. The fast-processing time of the multi-input model developed can be advantageous when it comes to actual farm postharvest processes.},
DOI = {10.3390/agriculture11080687}
}



@Article{electronics10151758,
AUTHOR = {Yang, Shangyi and Sun, Chao and Kim, Youngok},
TITLE = {Indoor 3D Localization Scheme Based on BLE Signal Fingerprinting and 1D Convolutional Neural Network},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {1758},
URL = {https://www.mdpi.com/2079-9292/10/15/1758},
ISSN = {2079-9292},
ABSTRACT = {Indoor localization schemes have significant potential for use in location-based services in areas such as smart factories, mixed reality, and indoor navigation. In particular, received signal strength (RSS)-based fingerprinting is used widely, given its simplicity and low hardware requirements. However, most studies tend to focus on estimating the 2D position of the target. Moreover, it is known that the fingerprinting scheme is computationally costly, and its positioning accuracy is readily affected by random fluctuations in the RSS values caused by fading and the multipath effect. We propose an indoor 3D localization scheme based on both fingerprinting and a 1D convolutional neural network (CNN). Instead of using the conventional fingerprint matching method, we transform the 3D positioning problem into a classification problem and use the 1D CNN model with the RSS time-series data from Bluetooth low-energy beacons for classification. By using the 1D CNN with the time-series data from multiple beacons, the inherent drawback of RSS-based fingerprinting, namely, its susceptibility to noise and randomness, is overcome, resulting in enhanced positioning accuracy. To evaluate the proposed scheme, we developed a 3D positioning system and performed comprehensive tests, whose results confirmed that the scheme significantly outperforms the conventional common spatial pattern classification algorithm.},
DOI = {10.3390/electronics10151758}
}



@Article{agronomy11081458,
AUTHOR = {Ammar, Adel and Koubaa, Anis and Benjdira, Bilel},
TITLE = {Deep-Learning-Based Automated Palm Tree Counting and Geolocation in Large Farms from Aerial Geotagged Images},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1458},
URL = {https://www.mdpi.com/2073-4395/11/8/1458},
ISSN = {2073-4395},
ABSTRACT = {In this paper, we propose an original deep learning framework for the automated counting and geolocation of palm trees from aerial images using convolutional neural networks. For this purpose, we collected aerial images from two different regions in Saudi Arabia, using two DJI drones, and we built a dataset of around 11,000 instances of palm trees. Then, we applied several recent convolutional neural network models (Faster R-CNN, YOLOv3, YOLOv4, and EfficientDet) to detect palms and other trees, and we conducted a complete comparative evaluation in terms of average precision and inference speed. YOLOv4 and EfficientDet-D5 yielded the best trade-off between accuracy and speed (up to 99% mean average precision and 7.4 FPS). Furthermore, using the geotagged metadata of aerial images, we used photogrammetry concepts and distance corrections to automatically detect the geographical location of detected palm trees. This geolocation technique was tested on two different types of drones (DJI Mavic Pro and Phantom 4 pro) and was assessed to provide an average geolocation accuracy that attains 1.6 m. This GPS tagging allows us to uniquely identify palm trees and count their number from a series of drone images, while correctly dealing with the issue of image overlapping. Moreover, this innovative combination between deep learning object detection and geolocalization can be generalized to any other objects in UAV images.},
DOI = {10.3390/agronomy11081458}
}



@Article{app11156745,
AUTHOR = {Díez-Pastor, José Francisco and Latorre-Carmona, Pedro and Garrido-Labrador, José Luis and Ramírez-Sanz, José Miguel and Rodríguez, Juan J.},
TITLE = {Experimental Assessment of Feature Extraction Techniques Applied to the Identification of Properties of Common Objects, Using a Radar System},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {6745},
URL = {https://www.mdpi.com/2076-3417/11/15/6745},
ISSN = {2076-3417},
ABSTRACT = {Radar technology has evolved considerably in the last few decades. There are many areas where radar systems are applied, including air traffic control in airports, ocean surveillance, and research systems, to cite a few. Other types of sensors have recently appeared, which allow tracking sub-millimeter motion with high speed and accuracy rates. These millimeter-wave radars are giving rise to myriad new applications, from the recognition of the material close objects are made, to the recognition of hand gestures. They have also been recently used to identify how a person interacts with digital devices through the physical environment (Tangible User Interfaces, TUIs). In this case, the radar is used to detect the orientation, movement, or distance from the objects to the user’s hands or the digital device. This paper presents a thoughtful comparative analysis of different feature extraction techniques and classification strategies applied on a series of datasets that cover problems such as the identification of materials, element counting, or determining the orientation and distance of objects to the sensor. The results outperform previous works using these datasets, especially when the accuracy was lowest, showing the benefits feature extraction techniques have on classification performance.},
DOI = {10.3390/app11156745}
}



@Article{min11080798,
AUTHOR = {Isheyskiy, Valentin and Martinyskin, Evgeny and Smirnov, Sergey and Vasilyev, Anton and Knyazev, Kirill and Fatyanov, Timur},
TITLE = {Specifics of MWD Data Collection and Verification during Formation of Training Datasets},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {798},
URL = {https://www.mdpi.com/2075-163X/11/8/798},
ISSN = {2075-163X},
ABSTRACT = {This paper presents a structured analysis in the area of measurement while drilling (MWD) data processing and verification methods, as well as describes the main nuances and certain specifics of “clean” data selection in order to build a “parent” training database for subsequent use in machine learning algorithms. The main purpose of the authors is to create a trainable machine learning algorithm, which, based on the available “clean” input data associated with specific conditions, could correlate, process and select parameters obtained from the drilling rig and use them for further estimation of various rock characteristics, prediction of optimal drilling and blasting parameters, and blasting results. The paper is a continuation of a series of publications devoted to the prospects of using MWD technology for the quality management of drilling and blasting operations at mining enterprises.},
DOI = {10.3390/min11080798}
}



@Article{rs13152881,
AUTHOR = {Karami, Azam and Quijano, Karoll and Crawford, Melba},
TITLE = {Advancing Tassel Detection and Counting: Annotation and Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2881},
URL = {https://www.mdpi.com/2072-4292/13/15/2881},
ISSN = {2072-4292},
ABSTRACT = {Tassel counts provide valuable information related to flowering and yield prediction in maize, but are expensive and time-consuming to acquire via traditional manual approaches. High-resolution RGB imagery acquired by unmanned aerial vehicles (UAVs), coupled with advanced machine learning approaches, including deep learning (DL), provides a new capability for monitoring flowering. In this article, three state-of-the-art DL techniques, CenterNet based on point annotation, task-aware spatial disentanglement (TSD), and detecting objects with recursive feature pyramids and switchable atrous convolution (DetectoRS) based on bounding box annotation, are modified to improve their performance for this application and evaluated for tassel detection relative to Tasselnetv2+. The dataset for the experiments is comprised of RGB images of maize tassels from plant breeding experiments, which vary in size, complexity, and overlap. Results show that the point annotations are more accurate and simpler to acquire than the bounding boxes, and bounding box-based approaches are more sensitive to the size of the bounding boxes and background than point-based approaches. Overall, CenterNet has high accuracy in comparison to the other techniques, but DetectoRS can better detect early-stage tassels. The results for these experiments were more robust than Tasselnetv2+, which is sensitive to the number of tassels in the image.},
DOI = {10.3390/rs13152881}
}



@Article{drones5030066,
AUTHOR = {Walambe, Rahee and Marathe, Aboli and Kotecha, Ketan},
TITLE = {Multiscale Object Detection from Drone Imagery Using Ensemble Transfer Learning},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/2504-446X/5/3/66},
ISSN = {2504-446X},
ABSTRACT = {Object detection in uncrewed aerial vehicle (UAV) images has been a longstanding challenge in the field of computer vision. Specifically, object detection in drone images is a complex task due to objects of various scales such as humans, buildings, water bodies, and hills. In this paper, we present an implementation of ensemble transfer learning to enhance the performance of the base models for multiscale object detection in drone imagery. Combined with a test-time augmentation pipeline, the algorithm combines different models and applies voting strategies to detect objects of various scales in UAV images. The data augmentation also presents a solution to the deficiency of drone image datasets. We experimented with two specific datasets in the open domain: the VisDrone dataset and the AU-AIR Dataset. Our approach is more practical and efficient due to the use of transfer learning and two-level voting strategy ensemble instead of training custom models on entire datasets. The experimentation shows significant improvement in the mAP for both VisDrone and AU-AIR datasets by employing the ensemble transfer learning method. Furthermore, the utilization of voting strategies further increases the 3reliability of the ensemble as the end-user can select and trace the effects of the mechanism for bounding box predictions.},
DOI = {10.3390/drones5030066}
}



@Article{rs13152912,
AUTHOR = {Wang, Jingrui and Wang, Shuqing and Zou, Dongxiao and Chen, Huimin and Zhong, Run and Li, Hanliang and Zhou, Wei and Yan, Kai},
TITLE = {Social Network and Bibliometric Analysis of Unmanned Aerial Vehicle Remote Sensing Applications from 2010 to 2021},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2912},
URL = {https://www.mdpi.com/2072-4292/13/15/2912},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) Remote sensing (RS) has unique advantages over traditional satellite RS, including convenience, high resolution, affordability and fast acquisition speed, making it widely used in many fields. To provide an overview of the development of UAV RS applications during the past decade, we screened related publications from the Web of Science core database from 2010 to 2021, built co-author networks, a discipline interaction network, a keywords timeline view, a co-citation cluster, and detected burst citations using bibliometrics and social network analysis. Our results show that: (1) The number of UAV RS publications had an increasing trend, with explosive growth in the past five years. The number of papers published by China and the United States (US) is far ahead in this field; (2) The US has currently the greatest influence in this field through the largest number of international cooperations. Cooperation is mainly concentrated in countries and institutions with a large number of publications but is not widely distributed. (3) The application of UAV RS involves multiple interdisciplinary subjects, among which “Environmental Science and Ecology” ranks first; (4) Future research trends of UAV RS are expected to be related to artificial intelligence (e.g., artificial neural networks-based research). This paper provides a scientific basis and guidance for future developments of UAV RS applications, which can help the research community to better grasp the developments of this field.},
DOI = {10.3390/rs13152912}
}



@Article{en14154477,
AUTHOR = {Vodovozov, Valery and Raud, Zoja and Petlenkov, Eduard},
TITLE = {Review on Braking Energy Management in Electric Vehicles},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {4477},
URL = {https://www.mdpi.com/1996-1073/14/15/4477},
ISSN = {1996-1073},
ABSTRACT = {The adoption of electric vehicles promises numerous benefits for modern society. At the same time, there remain significant hurdles to their wide distribution, primarily related to battery-based energy sources. This review concerns the systematization of knowledge in one of the areas of the electric vehicle control, namely, the energy management issues when using braking controllers. The braking process optimization is summarized from two aspects. First, the advantageous solutions are presented that were identified in the field of gradual and urgent braking. Second, several findings discovered in adjacent fields of automation are debated as prospects for their possible application in braking control. Following the specific classification of braking methods, a generalized braking system composition is offered, and all publications are evaluated primarily in terms of their energy recovery abilities as a global target. Then, conventional and intelligent classes of braking controllers are compared. In the first category, classic PID, threshold, and sliding-mode controllers are reviewed in terms of their energy management restrictions. The second group relates to the issues of the tire friction-slip identification and braking torque allocation between the hydraulic and electrical brakes. From this perspective, several intelligent systems are analyzed in detail, especially fuzzy logic, neural network, and their numerous associations.},
DOI = {10.3390/en14154477}
}



@Article{rs13152914,
AUTHOR = {Cruz-Ramos, Clara and Garcia-Salgado, Beatriz P. and Reyes-Reyes, Rogelio and Ponomaryov, Volodymyr and Sadovnychiy, Sergiy},
TITLE = {Gabor Features Extraction and Land-Cover Classification of Urban Hyperspectral Images for Remote Sensing Applications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2914},
URL = {https://www.mdpi.com/2072-4292/13/15/2914},
ISSN = {2072-4292},
ABSTRACT = {The principles of the transform stage of the extract, transform and load (ETL) process can be applied to index the data in functional structures for the decision-making inherent in an urban remote sensing application. This work proposes a method that can be utilised as an organisation stage by reducing the data dimension with Gabor texture features extracted from grey-scale representations of the Hue, Saturation and Value (HSV) colour space and the Normalised Difference Vegetation Index (NDVI). Additionally, the texture features are reduced using the Linear Discriminant Analysis (LDA) method. Afterwards, an Artificial Neural Network (ANN) is employed to classify the data and build a tick data matrix indexed by the belonging class of the observations, which could be retrieved for further analysis according to the class selected to explore. The proposed method is compared in terms of classification rates, reduction efficiency and training time against the utilisation of other grey-scale representations and classifiers. This method compresses up to 87% of the original features and achieves similar classification results to non-reduced features but at a higher training time.},
DOI = {10.3390/rs13152914}
}



@Article{rs13152917,
AUTHOR = {Wei, Lifei and Wang, Kun and Lu, Qikai and Liang, Yajing and Li, Haibo and Wang, Zhengxiang and Wang, Run and Cao, Liqin},
TITLE = {Crops Fine Classification in Airborne Hyperspectral Imagery Based on Multi-Feature Fusion and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2917},
URL = {https://www.mdpi.com/2072-4292/13/15/2917},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imagery has been widely used in precision agriculture due to its rich spectral characteristics. With the rapid development of remote sensing technology, the airborne hyperspectral imagery shows detailed spatial information and temporal flexibility, which open a new way to accurate agricultural monitoring. To extract crop types from the airborne hyperspectral images, we propose a fine classification method based on multi-feature fusion and deep learning. In this research, the morphological profiles, GLCM texture and endmember abundance features are leveraged to exploit the spatial information of the hyperspectral imagery. Then, the multiple spatial information is fused with the original spectral information to generate classification result by using the deep neural network with conditional random field (DNN+CRF) model. Specifically, the deep neural network (DNN) is a deep recognition model which can extract depth features and mine the potential information of data. As a discriminant model, conditional random field (CRF) considers both spatial and contextual information to reduce the misclassification noises while keeping the object boundaries. Moreover, three multiple feature fusion approaches, namely feature stacking, decision fusion and probability fusion, are taken into account. In the experiments, two airborne hyperspectral remote sensing datasets (Honghu dataset and Xiong’an dataset) are used. The experimental results show that the classification performance of the proposed method is satisfactory, where the salt and pepper noise is decreased, and the boundary of the ground object is preserved.},
DOI = {10.3390/rs13152917}
}



@Article{rs13152918,
AUTHOR = {Banerjee, Bikram P. and Sharma, Vikas and Spangenberg, German and Kant, Surya},
TITLE = {Machine Learning Regression Analysis for Estimation of Crop Emergence Using Multispectral UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2918},
URL = {https://www.mdpi.com/2072-4292/13/15/2918},
ISSN = {2072-4292},
ABSTRACT = {Optimal crop emergence is an important trait in crop breeding for genotypic screening and for achieving potential growth and yield. Emergence is conventionally quantified manually by counting the sub-sections of field plots or scoring; these are less reliable, laborious and inefficient. Remote sensing technology is being increasingly used for high-throughput estimation of agronomic traits in field crops. This study developed a method for estimating wheat seedlings using multispectral images captured from an unmanned aerial vehicle. A machine learning regression (MLR) analysis was used by combining spectral and morphological information extracted from the multispectral images. The approach was tested on diverse wheat genotypes varying in seedling emergence. In this study, three supervised MLR models including regression trees, support vector regression and Gaussian process regression (GPR) were evaluated for estimating wheat seedling emergence. The GPR model was the most effective compared to the other methods, with R2 = 0.86, RMSE = 4.07 and MAE = 3.21 when correlated to the manual seedling count. In addition, imagery data collected at multiple flight altitudes and different wheat growth stages suggested that 10 m altitude and 20 days after sowing were desirable for optimal spatial resolution and image analysis. The method is deployable on larger field trials and other crops for effective and reliable seedling emergence estimates.},
DOI = {10.3390/rs13152918}
}



@Article{agronomy11081480,
AUTHOR = {Liu, Jizhan and Abbas, Irfan and Noor, Rana Shahzad},
TITLE = {Development of Deep Learning-Based Variable Rate Agrochemical Spraying System for Targeted Weeds Control in Strawberry Crop},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1480},
URL = {https://www.mdpi.com/2073-4395/11/8/1480},
ISSN = {2073-4395},
ABSTRACT = {Agrochemical application is an important tool in the agricultural industry for the protection of crops. Agrochemical application with conventional sprayers results in the waste of applied agrochemicals, which not only increases financial losses but also contaminates the environment. Targeted agrochemical sprayers using smart control systems can substantially decrease the chemical input, weed control cost, and destructive environmental contamination. A variable rate spraying system was developed using deep learning methods for the development of new models to classify weeds and to accurately spray on desired weeds target. Laboratory and field experiments were conducted to assess the sprayer performance for weed classification and precise spraying of the target weeds using three classification CNNs (Convolutional Neural Networks) models. The DCNNs models (AlexNet, VGG-16, and GoogleNet) were trained using a dataset containing a total of 12,443 images captured from the strawberry field (4200 images with spotted spurge, 4265 images with Shepherd’s purse, and 4178 strawberry plants). The VGG-16 model attained higher values of precision, recall and F1-score as compared to AlexNet and GoogleNet. Additionally VGG-16 model recorded higher percentage of completely sprayed weeds target (CS = 93%) values. Overall in all experiments, VGG-16 performed better than AlexNet and GoogleNet for real-time weeds target classification and precision spraying. The experiments results revealed that the Sprayer performance decreased with the increase of sprayer traveling speed above 3 km/h. Experimental results recommended that the sprayer with the VGG-16 model can achieve high performance that makes it more ideal for a real-time spraying application. It is concluded that the advanced variable rate spraying system has the potential for spot application of agrochemicals to control weeds in a strawberry field. It can reduce the crop input costs and the environmental pollution risks.},
DOI = {10.3390/agronomy11081480}
}



@Article{s21155044,
AUTHOR = {Behjati, Mehran and Mohd Noh, Aishah Binti and Alobaidy, Haider A. H. and Zulkifley, Muhammad Aidiel and Nordin, Rosdiadee and Abdullah, Nor Fadzilah},
TITLE = {LoRa Communications as an Enabler for Internet of Drones towards Large-Scale Livestock Monitoring in Rural Farms},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5044},
URL = {https://www.mdpi.com/1424-8220/21/15/5044},
PubMedID = {34372281},
ISSN = {1424-8220},
ABSTRACT = {Currently, smart farming is considered an effective solution to enhance the productivity of farms; thereby, it has recently received broad interest from service providers to offer a wide range of applications, from pest identification to asset monitoring. Although the emergence of digital technologies, such as the Internet of Things (IoT) and low-power wide-area networks (LPWANs), has led to significant advances in the smart farming industry, farming operations still need more efficient solutions. On the other hand, the utilization of unmanned aerial vehicles (UAVs), also known as drones, is growing rapidly across many civil application domains. This paper aims to develop a farm monitoring system that incorporates UAV, LPWAN, and IoT technologies to transform the current farm management approach and aid farmers in obtaining actionable data from their farm operations. In this regard, an IoT-based water quality monitoring system was developed because water is an essential aspect in livestock development. Then, based on the Long-Range Wide-Area Network (LoRaWAN®) technology, a multi-channel LoRaWAN® gateway was developed and integrated into a vertical takeoff and landing drone to convey collected data from the sensors to the cloud for further analysis. In addition, to develop LoRaWAN®-based aerial communication, a series of measurements and simulations were performed under different configurations and scenarios. Finally, to enhance the efficiency of aerial-based data collection, the UAV path planning was optimized. Measurement results showed that the maximum achievable LoRa coverage when operating on-air via the drone is about 10 km, and the Longley–Rice irregular terrain model provides the most suitable path loss model for the scenario of large-scale farms, and a multi-channel gateway with a spreading factor of 12 provides the most reliable communication link at a high drone speed (up to 95 km/h). Simulation results showed that the developed system can overcome the coverage limitation of LoRaWAN® and it can establish a reliable communication link over large-scale wireless sensor networks. In addition, it was shown that by optimizing flight paths, aerial data collection could be performed in a much shorter time than industrial mission planning (up to four times in our case).},
DOI = {10.3390/s21155044}
}



@Article{rs13152937,
AUTHOR = {Zeng, Linglin and Peng, Guozhang and Meng, Ran and Man, Jianguo and Li, Weibo and Xu, Binyuan and Lv, Zhengang and Sun, Rui},
TITLE = {Wheat Yield Prediction Based on Unmanned Aerial Vehicles-Collected Red–Green–Blue Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2937},
URL = {https://www.mdpi.com/2072-4292/13/15/2937},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles-collected (UAVs) digital red–green–blue (RGB) images provided a cost-effective method for precision agriculture applications regarding yield prediction. This study aims to fully explore the potential of UAV-collected RGB images in yield prediction of winter wheat by comparing it to multi-source observations, including thermal, structure, volumetric metrics, and ground-observed leaf area index (LAI) and chlorophyll content under the same level or across different levels of nitrogen fertilization. Color indices are vegetation indices calculated by the vegetation reflectance at visible bands (i.e., red, green, and blue) derived from RGB images. The results showed that some of the color indices collected at the jointing, flowering, and early maturity stages had high correlation (R2 = 0.76–0.93) with wheat grain yield. They gave the highest prediction power (R2 = 0.92–0.93) under four levels of nitrogen fertilization at the flowering stage. In contrast, the other measurements including canopy temperature, volumetric metrics, and ground-observed chlorophyll content showed lower correlation (R2 = 0.52–0.85) to grain yield. In addition, thermal information as well as volumetric metrics generally had little contribution to the improvement of grain yield prediction when combining them with color indices derived from digital images. Especially, LAI had inferior performance to color indices in grain yield prediction within the same level of nitrogen fertilization at the flowering stage (R2 = 0.00–0.40 and R2 = 0.55–0.68), and color indices provided slightly better prediction of yield than LAI at the flowering stage (R2 = 0.93, RMSE = 32.18 g/m2 and R2 = 0.89, RMSE = 39.82 g/m2) under all levels of nitrogen fertilization. This study highlights the capabilities of color indices in wheat yield prediction across genotypes, which also indicates the potential of precision agriculture application using many other flexible, affordable, and easy-to-handle devices such as mobile phones and near surface digital cameras in the future.},
DOI = {10.3390/rs13152937}
}



@Article{agriculture11080707,
AUTHOR = {Lu, Jinzhu and Tan, Lijuan and Jiang, Huanyu},
TITLE = {Review on Convolutional Neural Network (CNN) Applied to Plant Leaf Disease Classification},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {707},
URL = {https://www.mdpi.com/2077-0472/11/8/707},
ISSN = {2077-0472},
ABSTRACT = {Crop production can be greatly reduced due to various diseases, which seriously endangers food security. Thus, detecting plant diseases accurately is necessary and urgent. Traditional classification methods, such as naked-eye observation and laboratory tests, have many limitations, such as being time consuming and subjective. Currently, deep learning (DL) methods, especially those based on convolutional neural network (CNN), have gained widespread application in plant disease classification. They have solved or partially solved the problems of traditional classification methods and represent state-of-the-art technology in this field. In this work, we reviewed the latest CNN networks pertinent to plant leaf disease classification. We summarized DL principles involved in plant disease classification. Additionally, we summarized the main problems and corresponding solutions of CNN used for plant disease classification. Furthermore, we discussed the future development direction in plant disease classification.},
DOI = {10.3390/agriculture11080707}
}



@Article{rs13152948,
AUTHOR = {Fernández, Claudio I. and Leblon, Brigitte and Wang, Jinfei and Haddadi, Ata and Wang, Keri},
TITLE = {Detecting Infected Cucumber Plants with Close-Range Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2948},
URL = {https://www.mdpi.com/2072-4292/13/15/2948},
ISSN = {2072-4292},
ABSTRACT = {This study used close-range multispectral imagery over cucumber plants inside a commercial greenhouse to detect powdery mildew due to Podosphaera xanthii. It was collected using a MicaSense® RedEdge camera at 1.5 m over the top of the plant. Image registration was performed using Speeded-Up Robust Features (SURF) with an affine geometric transformation. The image background was removed using a binary mask created with the aligned NIR band of each image, and the illumination was corrected using Cheng et al.’s algorithm. Different features were computed, including RGB, image reflectance values, and several vegetation indices. For each feature, a fine Gaussian Support Vector Machines algorithm was trained and validated to classify healthy and infected pixels. The data set to train and validate the SVM was composed of 1000 healthy and 1000 infected pixels, split 70–30% into training and validation datasets, respectively. The overall validation accuracy was 89, 73, 82, 51, and 48%, respectively, for blue, green, red, red-edge, and NIR band image. With the RGB images, we obtained an overall validation accuracy of 89%, while the best vegetation index image was the PMVI-2 image which produced an overall accuracy of 81%. Using the five bands together, overall accuracy dropped from 99% in the training to 57% in the validation dataset. While the results of this work are promising, further research should be considered to increase the number of images to achieve better training and validation datasets.},
DOI = {10.3390/rs13152948}
}



@Article{rs13152965,
AUTHOR = {Ghaffarian, Saman and Valente, João and van der Voort, Mariska and Tekinerdogan, Bedir},
TITLE = {Effect of Attention Mechanism in Deep Learning-Based Remote Sensing Image Processing: A Systematic Literature Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2965},
URL = {https://www.mdpi.com/2072-4292/13/15/2965},
ISSN = {2072-4292},
ABSTRACT = {Machine learning, particularly deep learning (DL), has become a central and state-of-the-art method for several computer vision applications and remote sensing (RS) image processing. Researchers are continually trying to improve the performance of the DL methods by developing new architectural designs of the networks and/or developing new techniques, such as attention mechanisms. Since the attention mechanism has been proposed, regardless of its type, it has been increasingly used for diverse RS applications to improve the performances of the existing DL methods. However, these methods are scattered over different studies impeding the selection and application of the feasible approaches. This study provides an overview of the developed attention mechanisms and how to integrate them with different deep learning neural network architectures. In addition, it aims to investigate the effect of the attention mechanism on deep learning-based RS image processing. We identified and analyzed the advances in the corresponding attention mechanism-based deep learning (At-DL) methods. A systematic literature review was performed to identify the trends in publications, publishers, improved DL methods, data types used, attention types used, overall accuracies achieved using At-DL methods, and extracted the current research directions, weaknesses, and open problems to provide insights and recommendations for future studies. For this, five main research questions were formulated to extract the required data and information from the literature. Furthermore, we categorized the papers regarding the addressed RS image processing tasks (e.g., image classification, object detection, and change detection) and discussed the results within each group. In total, 270 papers were retrieved, of which 176 papers were selected according to the defined exclusion criteria for further analysis and detailed review. The results reveal that most of the papers reported an increase in overall accuracy when using the attention mechanism within the DL methods for image classification, image segmentation, change detection, and object detection using remote sensing images.},
DOI = {10.3390/rs13152965}
}



@Article{ijgi10080511,
AUTHOR = {Xulu, Sifiso and Mbatha, Nkanyiso and Peerbhay, Kabir},
TITLE = {Burned Area Mapping over the Southern Cape Forestry Region, South Africa Using Sentinel Data within GEE Cloud Platform},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {511},
URL = {https://www.mdpi.com/2220-9964/10/8/511},
ISSN = {2220-9964},
ABSTRACT = {Planted forests in South Africa have been affected by an increasing number of economically damaging fires over the past four decades. They constitute a major threat to the forestry industry and account for over 80% of the country’s commercial timber losses. Forest fires are more frequent and severe during the drier drought conditions that are typical in South Africa. For proper forest management, accurate detection and mapping of burned areas are required, yet the exercise is difficult to perform in the field because of time and expense. Now that ready-to-use satellite data are freely accessible in the cloud-based Google Earth Engine (GEE), in this study, we exploit the Sentinel-2-derived differenced normalized burned ratio (dNBR) to characterize burn severity areas, and also track carbon monoxide (CO) plumes using Sentinel-5 following a wildfire that broke over the southeastern coast of the Western Cape province in late October 2018. The results showed that 37.4% of the area was severely burned, and much of it occurred in forested land in the studied area. This was followed by 24.7% of the area that was burned at a moderate-high level. About 15.9% had moderate-low burned severity, whereas 21.9% was slightly burned. Random forests classifier was adopted to separate burned class from unburned and achieved an overall accuracy of over 97%. The most important variables in the classification included texture, NBR, and the NIR bands. The CO signal sharply increased during fire outbreaks and marked the intensity of black carbon over the affected area. Our study contributes to the understanding of forest fire in the dynamics over the Southern Cape forestry landscape. Furthermore, it also demonstrates the usefulness of Sentinel-5 for monitoring CO. Taken together, the Sentinel satellites and GEE offer an effective tool for mapping fires, even in data-poor countries.},
DOI = {10.3390/ijgi10080511}
}



@Article{s21155110,
AUTHOR = {Placidi, Pisana and Morbidelli, Renato and Fortunati, Diego and Papini, Nicola and Gobbi, Francesco and Scorzoni, Andrea},
TITLE = {Monitoring Soil and Ambient Parameters in the IoT Precision Agriculture Scenario: An Original Modeling Approach Dedicated to Low-Cost Soil Water Content Sensors},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5110},
URL = {https://www.mdpi.com/1424-8220/21/15/5110},
PubMedID = {34372355},
ISSN = {1424-8220},
ABSTRACT = {A low power wireless sensor network based on LoRaWAN protocol was designed with a focus on the IoT low-cost Precision Agriculture applications, such as greenhouse sensing and actuation. All subsystems used in this research are designed by using commercial components and free or open-source software libraries. The whole system was implemented to demonstrate the feasibility of a modular system built with cheap off-the-shelf components, including sensors. The experimental outputs were collected and stored in a database managed by a virtual machine running in a cloud service. The collected data can be visualized in real time by the user with a graphical interface. The reliability of the whole system was proven during a continued experiment with two natural soils, Loamy Sand and Silty Loam. Regarding soil parameters, the system performance has been compared with that of a reference sensor from Sentek. Measurements highlighted a good agreement for the temperature within the supposed accuracy of the adopted sensors and a non-constant sensitivity for the low-cost volumetric water contents (VWC) sensor. Finally, for the low-cost VWC sensor we implemented a novel procedure to optimize the parameters of the non-linear fitting equation correlating its analog voltage output with the reference VWC.},
DOI = {10.3390/s21155110}
}



@Article{s21155132,
AUTHOR = {Kuo, Ping-Huan and Lin, Ssu-Ting and Hu, Jun and Huang, Chiou-Jye},
TITLE = {Multi-Sensor Context-Aware Based Chatbot Model: An Application of Humanoid Companion Robot},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5132},
URL = {https://www.mdpi.com/1424-8220/21/15/5132},
PubMedID = {34372368},
ISSN = {1424-8220},
ABSTRACT = {In aspect of the natural language processing field, previous studies have generally analyzed sound signals and provided related responses. However, in various conversation scenarios, image information is still vital. Without the image information, misunderstanding may occur, and lead to wrong responses. In order to address this problem, this study proposes a recurrent neural network (RNNs) based multi-sensor context-aware chatbot technology. The proposed chatbot model incorporates image information with sound signals and gives appropriate responses to the user. In order to improve the performance of the proposed model, the long short-term memory (LSTM) structure is replaced by gated recurrent unit (GRU). Moreover, a VGG16 model is also chosen for a feature extractor for the image information. The experimental results demonstrate that the integrative technology of sound and image information, which are obtained by the image sensor and sound sensor in a companion robot, is helpful for the chatbot model proposed in this study. The feasibility of the proposed technology was also confirmed in the experiment.},
DOI = {10.3390/s21155132}
}



@Article{rs13152986,
AUTHOR = {Li, Xin and Xu, Feng and Xia, Runliang and Lyu, Xin and Gao, Hongmin and Tong, Yao},
TITLE = {Hybridizing Cross-Level Contextual and Attentive Representations for Remote Sensing Imagery Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2986},
URL = {https://www.mdpi.com/2072-4292/13/15/2986},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation of remote sensing imagery is a fundamental task in intelligent interpretation. Since deep convolutional neural networks (DCNNs) performed considerable insight in learning implicit representations from data, numerous works in recent years have transferred the DCNN-based model to remote sensing data analysis. However, the wide-range observation areas, complex and diverse objects and illumination and imaging angle influence the pixels easily confused, leading to undesirable results. Therefore, a remote sensing imagery semantic segmentation neural network, named HCANet, is proposed to generate representative and discriminative representations for dense predictions. HCANet hybridizes cross-level contextual and attentive representations to emphasize the distinguishability of learned features. First of all, a cross-level contextual representation module (CCRM) is devised to exploit and harness the superpixel contextual information. Moreover, a hybrid representation enhancement module (HREM) is designed to fuse cross-level contextual and self-attentive representations flexibly. Furthermore, the decoder incorporates DUpsampling operation to boost the efficiency losslessly. The extensive experiments are implemented on the Vaihingen and Potsdam benchmarks. In addition, the results indicate that HCANet achieves excellent performance on overall accuracy and mean intersection over union. In addition, the ablation study further verifies the superiority of CCRM.},
DOI = {10.3390/rs13152986}
}



@Article{rs13152992,
AUTHOR = {Hajjar, Chantal and Ghattas, Ghassan and Sarkis, Maya Kharrat and Chamoun, Yolla Ghorra},
TITLE = {Vine Identification and Characterization in Goblet-Trained Vineyards Using Remotely Sensed Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2992},
URL = {https://www.mdpi.com/2072-4292/13/15/2992},
ISSN = {2072-4292},
ABSTRACT = {This paper proposes a novel approach for living and missing vine identification and vine characterization in goblet-trained vine plots using aerial images. Given the periodic structure of goblet vineyards, the RGB color coded parcel image is analyzed using proper processing techniques in order to determine the locations of living and missing vines. Vine characterization is achieved by implementing the marker-controlled watershed transform where the centers of the living vines serve as object markers. As a result, a precise mortality rate is calculated for each parcel. Moreover, all vines, even the overlapping ones, are fully recognized providing information about their size, shape, and green color intensity. The presented approach is fully automated and yields accuracy values exceeding 95% when the obtained results are assessed with ground-truth data. This unsupervised and automated approach can be applied to any type of plots presenting similar spatial patterns requiring only the image as input.},
DOI = {10.3390/rs13152992}
}



@Article{ijgi10080513,
AUTHOR = {Zare Naghadehi, Saeid and Asadi, Milad and Maleki, Mohammad and Tavakkoli-Sabour, Seyed-Mohammad and Van Genderen, John Lodewijk and Saleh, Samira-Sadat},
TITLE = {Prediction of Urban Area Expansion with Implementation of MLC, SAM and SVMs’ Classifiers Incorporating Artificial Neural Network Using Landsat Data},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {513},
URL = {https://www.mdpi.com/2220-9964/10/8/513},
ISSN = {2220-9964},
ABSTRACT = {A reliable land cover (LC) map is essential for planners, as missing proper land cover maps may deviate a project. This study is focusing on land cover classification and prediction using three well known classifiers and remote sensing data. Maximum Likelihood classifier (MLC), Spectral Angle Mapper (SAM), and Support Vector Machines (SVMs) algorithms are used as the representatives for parametric, non-parametric and subpixel capable methods for change detection and change prediction of Urmia City (Iran) and its suburbs. Landsat images of 2000, 2010, and 2020 have been used to provide land cover information. The results demonstrated 0.93–0.94 overall accuracies for MLC and SVMs’ algorithms, but it was around 0.79 for the SAM algorithm. The MLC performed slightly better than SVMs’ classifier. Cellular Automata Artificial neural network method was used to predict land cover changes. Overall accuracy of MLC was higher than others at about 0.94 accuracy, although, SVMs were slightly more accurate for large area segments. Land cover maps were predicted for 2030, which demonstrate the city’s expansion from 5500 ha in 2000 to more than 9000 ha in 2030.},
DOI = {10.3390/ijgi10080513}
}



@Article{coatings11080913,
AUTHOR = {Siang, Teng Wei and Firdaus Akbar, Muhammad and Nihad Jawad, Ghassan and Yee, Tan Shin and Mohd Sazali, Mohd Ilyas Sobirin},
TITLE = {A Past, Present, and Prospective Review on Microwave Nondestructive Evaluation of Composite Coatings},
JOURNAL = {Coatings},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {913},
URL = {https://www.mdpi.com/2079-6412/11/8/913},
ISSN = {2079-6412},
ABSTRACT = {Recent years have witnessed an increase in the use of composite coatings for numerous applications, including aerospace, aircraft, and maritime vessels. These materials owe this popularity surge to the superior strength, weight, stiffness, and electrical insulation they exhibit over conventional substances, such as metals. The growing demand for such materials is accompanied by the inevitable need for fast, accurate, and affordable nondestructive testing techniques to reveal any possible defects within the coatings or any defects under coating. However, typical nondestructive testing (NDT) techniques such as ultrasonic testing (UT), infrared thermography (IRT), eddy current testing (ECT), and laser shearography (LS) have failed to provide successful results when inspecting composite coatings. Consequently, microwave NDT techniques have emerged to compensate for the shortcomings of traditional NDT approaches. Numerous microwave NDT methods have been reported for composite coatings inspection. Although existing microwave NDT methods have shown successful inspection of composite coatings, they often face several challenges, such as low spatial image quality and extensive data interpretation. Nevertheless, many of these limitations can be addressed by utilizing microwave NDT techniques with modern technologies such as soft computing. Artificially intelligent techniques have greatly enhanced the reliability and accuracy of microwave NDT techniques. This paper reviews various traditional NDT techniques and their limitations in inspecting composite coatings. In addition, the article includes a detailed review of several microwave NDT techniques and their benefits in evaluating composite coatings. The paper also highlights the advantages of using the recently reported microwave NDT approaches employing artificial intelligence approaches. This review demonstrates that microwave NDT techniques in conjunction with artificial intelligence approaches have excellent prospects for further enhancing composite coatings inspection and assessment efficiency. The review aimed to provide the reader with a comprehensive overview of most NDT techniques used for composite materials alongside their most salient features.},
DOI = {10.3390/coatings11080913}
}



@Article{w13152080,
AUTHOR = {Wang, Yang and Tian, Yongzhong and Cao, Yan},
TITLE = {Dam Siting: A Review},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {2080},
URL = {https://www.mdpi.com/2073-4441/13/15/2080},
ISSN = {2073-4441},
ABSTRACT = {Dams can effectively regulate the spatial and temporal distribution of water resources, where the rationality of dam siting determines whether the role of dams can be effectively performed. This paper reviews the research literature on dam siting in the past 20 years, discusses the methods used for dam siting, focuses on the factors influencing dam siting, and assesses the impact of different dam functions on siting factors. The results show the following: (1) Existing siting methods can be categorized into three types—namely, GIS/RS-based siting, MCDM- and MCDM-GIS-based siting, and machine learning-based siting. GIS/RS emphasizes the ability to capture and analyze data, MCDM has the advantage of weighing the importance of the relationship between multiple factors, and machine learning methods have a strong ability to learn and process complex data. (2) Site selection factors vary greatly, depending on the function of the dam. For dams with irrigation and water supply as the main purpose, the site selection is more focused on the evaluation of water quality. For dams with power generation as the main purpose, the hydrological factors characterizing the power generation potential are the most important. For dams with flood control as the main purpose, the topography and geological conditions are more important. (3) The integration of different siting methods and the siting of new functional dams in the existing research is not sufficient. Future research should focus on the integration of different methods and disciplines, in order to explore the siting of new types of dams.},
DOI = {10.3390/w13152080}
}



@Article{s21155183,
AUTHOR = {Motroni, Andrea and Buffi, Alice and Nepa, Paolo and Pesi, Mario and Congi, Antonio},
TITLE = {An Action Classification Method for Forklift Monitoring in Industry 4.0 Scenarios},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5183},
URL = {https://www.mdpi.com/1424-8220/21/15/5183},
PubMedID = {34372420},
ISSN = {1424-8220},
ABSTRACT = {The I-READ 4.0 project is aimed at developing an integrated and autonomous Cyber-Physical System for automatic management of very large warehouses with a high-stock rotation index. Thanks to a network of Radio Frequency Identification (RFID) readers operating in the Ultra-High-Frequency (UHF) band, both fixed and mobile, it is possible to implement an efficient management of assets and forklifts operating in an indoor scenario. A key component to accomplish this goal is the UHF-RFID Smart Gate, which consists of a checkpoint infrastructure based on RFID technology to identify forklifts and their direction of transit. This paper presents the implementation of a UHF-RFID Smart Gate with a single reader antenna with asymmetrical deployment, thus allowing the correct action classification with reduced infrastructure complexity and cost. The action classification method exploits the signal phase backscattered by RFID tags placed on the forklifts. The performance and the method capabilities are demonstrated through an on-site demonstrator in a real warehouse.},
DOI = {10.3390/s21155183}
}



@Article{agronomy11081542,
AUTHOR = {Wang, Hao and Lyu, Suxing and Ren, Yaxin},
TITLE = {Paddy Rice Imagery Dataset for Panicle Segmentation},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1542},
URL = {https://www.mdpi.com/2073-4395/11/8/1542},
ISSN = {2073-4395},
ABSTRACT = {Accurate panicle identification is a key step in rice-field phenotyping. Deep learning methods based on high-spatial-resolution images provide a high-throughput and accurate solution of panicle segmentation. Panicle segmentation tasks require costly annotations to train an accurate and robust deep learning model. However, few public datasets are available for rice-panicle phenotyping. We present a semi-supervised deep learning model training process, which greatly assists the annotation and refinement of training datasets. The model learns the panicle features with limited annotations and localizes more positive samples in the datasets, without further interaction. After the dataset refinement, the number of annotations increased by 40.6%. In addition, we trained and tested modern deep learning models to show how the dataset is beneficial to both detection and segmentation tasks. Results of our comparison experiments can inspire others in dataset preparation and model selection.},
DOI = {10.3390/agronomy11081542}
}



@Article{app11157102,
AUTHOR = {Xiao, Guoquan and Tong, Chao and Wang, Yue and Guan, Shuaishuai and Hong, Xiaobin and Shang, Bin},
TITLE = {CFD Simulation of the Safety of Unmanned Ship Berthing under the Influence of Various Factors},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {7102},
URL = {https://www.mdpi.com/2076-3417/11/15/7102},
ISSN = {2076-3417},
ABSTRACT = {The safety of unmanned ship berthing is of paramount importance. In order to explore the influence of wind and wave coupling, a berthing computational fluid dynamics (CFD) model was established, and the characteristics of speed field, pressure field, and vortex have been obtained under different speed, wind direction, and the quay wall distances. The results show that the total resistance of the hull against the current can be about 1.60 times higher compared to the downstream resistance, water flow resistance is the dominant factor, accounting for more than 80% of the total resistance. When changing the distance between ship and shore at fixed speed, the results found that the torque is small, but the growth rate is very large when driving below 2 m/s, and the torque growth rate is stable above 2 m/s. Based on the established coupling model, a multi-factor berthing safety study is carried out on an actual unmanned ship. The results show that when the speed increases from 4 m/s to 12 m/s, the curve slope is small, the resistance increases from 3666 N to 18,056 N, and the rear slope increases. The pressure increases with the speed, and when the speed is 24 m/s, the maximum pressure is up to 238,869 Pa. When the wind speed is fixed, the vertical force of the unmanned ship increases first and then decreases to zero and then reverses the same law change, and the maximum resistance is about 425 N at the wind angle of about 45 degrees; At 90 degrees, the maximum lateral force on an unmanned boat is about 638 N. The above results can provide control strategy for unmanned ship berthing safety, and provide theoretical basis for unmanned ship route planning and obstacle avoidance, safety design, etc.},
DOI = {10.3390/app11157102}
}



@Article{drones5030071,
AUTHOR = {Saitoh, Tomoko and Kobayashi, Moyu},
TITLE = {Appropriate Drone Flight Altitude for Horse Behavioral Observation},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {71},
URL = {https://www.mdpi.com/2504-446X/5/3/71},
ISSN = {2504-446X},
ABSTRACT = {Recently, drone technology advanced, and its safety and operability markedly improved, leading to its increased application in animal research. This study demonstrated drone application in livestock management, using its technology to observe horse behavior and verify the appropriate horse–drone distance for aerial behavioral observations. Recordings were conducted from September to October 2017 on 11 horses using the Phantom 4 Pro drone. Four flight altitudes were tested (60, 50, 40, and 30 m) to investigate the reactions of the horses to the drones and observe their behavior; the recording time at each altitude was 5 min. None of the horses displayed avoidance behavior at any flight altitude, and the observer was able to distinguish between any two horses. Recorded behaviors were foraging, moving, standing, recumbency, avoidance, and others. Foraging was the most common behavior observed both directly and in the drone videos. The correlation coefficients of all behavioral data from direct and drone video observations at all altitudes were significant (p &lt; 0.01). These results indicate that horse behavior can be discerned with equal accuracy by both direct and recorded drone video observations. In conclusion, drones can be useful for recording and analyzing horse behavior.},
DOI = {10.3390/drones5030071}
}



@Article{rs13153024,
AUTHOR = {Ma, Huiqin and Huang, Wenjiang and Dong, Yingying and Liu, Linyi and Guo, Anting},
TITLE = {Using UAV-Based Hyperspectral Imagery to Detect Winter Wheat Fusarium Head Blight},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {3024},
URL = {https://www.mdpi.com/2072-4292/13/15/3024},
ISSN = {2072-4292},
ABSTRACT = {Fusarium head blight (FHB) is a major winter wheat disease in China. The accurate and timely detection of wheat FHB is vital to scientific field management. By combining three types of spectral features, namely, spectral bands (SBs), vegetation indices (VIs), and wavelet features (WFs), in this study, we explore the potential of using hyperspectral imagery obtained from an unmanned aerial vehicle (UAV), to detect wheat FHB. First, during the wheat filling period, two UAV-based hyperspectral images were acquired. SBs, VIs, and WFs that were sensitive to wheat FHB were extracted and optimized from the two images. Subsequently, a field-scale wheat FHB detection model was formulated, based on the optimal spectral feature combination of SBs, VIs, and WFs (SBs + VIs + WFs), using a support vector machine. Two commonly used data normalization algorithms were utilized before the construction of the model. The single WFs, and the spectral feature combination of optimal SBs and VIs (SBs + VIs), were respectively used to formulate models for comparison and testing. The results showed that the detection model based on the normalized SBs + VIs + WFs, using min–max normalization algorithm, achieved the highest R2 of 0.88 and the lowest RMSE of 2.68% among the three models. Our results suggest that UAV-based hyperspectral imaging technology is promising for the field-scale detection of wheat FHB. Combining traditional SBs and VIs with WFs can improve the detection accuracy of wheat FHB effectively.},
DOI = {10.3390/rs13153024}
}



@Article{su13158600,
AUTHOR = {Sharma, Meenakshi and Kaushik, Prashant and Chawade, Aakash},
TITLE = {Frontiers in the Solicitation of Machine Learning Approaches in Vegetable Science Research},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {8600},
URL = {https://www.mdpi.com/2071-1050/13/15/8600},
ISSN = {2071-1050},
ABSTRACT = {Along with essential nutrients and trace elements, vegetables provide raw materials for the food processing industry. Despite this, plant diseases and unfavorable weather patterns continue to threaten the delicate balance between vegetable production and consumption. It is critical to utilize machine learning (ML) in this setting because it provides context for decision-making related to breeding goals. Cutting-edge technologies for crop genome sequencing and phenotyping, combined with advances in computer science, are currently fueling a revolution in vegetable science and technology. Additionally, various ML techniques such as prediction, classification, and clustering are frequently used to forecast vegetable crop production in the field. In the vegetable seed industry, machine learning algorithms are used to assess seed quality before germination and have the potential to improve vegetable production with desired features significantly; whereas, in plant disease detection and management, the ML approaches can improve decision-support systems that assist in converting massive amounts of data into valuable recommendations. On similar lines, in vegetable breeding, ML approaches are helpful in predicting treatment results, such as what will happen if a gene is silenced. Furthermore, ML approaches can be a saviour to insufficient coverage and noisy data generated using various omics platforms. This article examines ML models in the field of vegetable sciences, which encompasses breeding, biotechnology, and genome sequencing.},
DOI = {10.3390/su13158600}
}



@Article{app11157148,
AUTHOR = {Endale, Bedada and Tullu, Abera and Shi, Hayoung and Kang, Beom-Soo},
TITLE = {Robust Approach to Supervised Deep Neural Network Training for Real-Time Object Classification in Cluttered Indoor Environment},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {7148},
URL = {https://www.mdpi.com/2076-3417/11/15/7148},
ISSN = {2076-3417},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are being widely utilized for various missions: in both civilian and military sectors. Many of these missions demand UAVs to acquire artificial intelligence about the environments they are navigating in. This perception can be realized by training a computing machine to classify objects in the environment. One of the well known machine training approaches is supervised deep learning, which enables a machine to classify objects. However, supervised deep learning comes with huge sacrifice in terms of time and computational resources. Collecting big input data, pre-training processes, such as labeling training data, and the need for a high performance computer for training are some of the challenges that supervised deep learning poses. To address these setbacks, this study proposes mission specific input data augmentation techniques and the design of light-weight deep neural network architecture that is capable of real-time object classification. Semi-direct visual odometry (SVO) data of augmented images are used to train the network for object classification. Ten classes of 10,000 different images in each class were used as input data where 80% were for training the network and the remaining 20% were used for network validation. For the optimization of the designed deep neural network, a sequential gradient descent algorithm was implemented. This algorithm has the advantage of handling redundancy in the data more efficiently than other algorithms.},
DOI = {10.3390/app11157148}
}



@Article{electronics10151864,
AUTHOR = {Sheu, Ming-Hwa and Jhang, Yu-Syuan and Morsalin, S M Salahuddin and Huang, Yao-Fong and Sun, Chi-Chia and Lai, Shin-Chi},
TITLE = {UAV Object Tracking Application Based on Patch Color Group Feature on Embedded System},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {1864},
URL = {https://www.mdpi.com/2079-9292/10/15/1864},
ISSN = {2079-9292},
ABSTRACT = {The discriminative object tracking system for unmanned aerial vehicles (UAVs) is widely used in numerous applications. While an ample amount of research has been carried out in this domain, implementing a low computational cost algorithm on a UAV onboard embedded system is still challenging. To address this issue, we propose a low computational complexity discriminative object tracking system for UAVs approach using the patch color group feature (PCGF) framework in this work. The tracking object is separated into several non-overlapping local image patches then the features are extracted into the PCGFs, which consist of the Gaussian mixture model (GMM). The object location is calculated by the similar PCGFs comparison from the previous frame and current frame. The background PCGFs of the object are removed by four directions feature scanning and dynamic threshold comparison, which improve the performance accuracy. In the terms of speed execution, the proposed algorithm accomplished 32.5 frames per second (FPS) on the x64 CPU platform without a GPU accelerator and 17 FPS in Raspberry Pi 4. Therefore, this work could be considered as a good solution for achieving a low computational complexity PCGF algorithm on a UAV onboard embedded system to improve flight times.},
DOI = {10.3390/electronics10151864}
}



@Article{s21155250,
AUTHOR = {Zhang, Jing and Li, Jiwu and Yang, Hongwei and Feng, Xin and Sun, Geng},
TITLE = {Complex Environment Path Planning for Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {15},
ARTICLE-NUMBER = {5250},
URL = {https://www.mdpi.com/1424-8220/21/15/5250},
PubMedID = {34372486},
ISSN = {1424-8220},
ABSTRACT = {Flying safely in complex urban environments is a challenge for unmanned aerial vehicles because path planning in urban environments with many narrow passages and few dynamic flight obstacles is difficult. The path planning problem is decomposed into global path planning and local path adjustment in this paper. First, a branch-selected rapidly-exploring random tree (BS-RRT) algorithm is proposed to solve the global path planning problem in environments with narrow passages. A cyclic pruning algorithm is proposed to shorten the length of the planned path. Second, the GM(1,1) model is improved with optimized background value named RMGM(1,1) to predict the flight path of dynamic obstacles. Herein, the local path adjustment is made by analyzing the prediction results. BS-RRT demonstrated a faster convergence speed and higher stability in narrow passage environments when compared with RRT, RRT-Connect, P-RRT, 1-0 Bg-RRT, and RRT*. In addition, the path planned by BS-RRT through the use of the cyclic pruning algorithm was the shortest. The prediction error of RMGM(1,1) was compared with those of ECGM(1,1), PCGM(1,1), GM(1,1), MGM(1,1), and GDF. The trajectory predicted by RMGM(1,1) was closer to the actual trajectory. Finally, we use the two methods to realize path planning in urban environments.},
DOI = {10.3390/s21155250}
}



@Article{agronomy11081554,
AUTHOR = {Lee, Dong-Ho and Kim, Hyeon-Jin and Park, Jong-Hwa},
TITLE = {UAV, a Farm Map, and Machine Learning Technology Convergence Classification Method of a Corn Cultivation Area},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1554},
URL = {https://www.mdpi.com/2073-4395/11/8/1554},
ISSN = {2073-4395},
ABSTRACT = {South Korea’s agriculture is characterized by a mixture of various cultivated crops. In such an agricultural environment, convergence technology for ICT (information, communications, and technology) and AI (artificial intelligence) as well as agriculture is required to classify objects and predict yields. In general, the classification of paddy fields and field boundaries takes a lot of time and effort. The Farm Map was developed to clearly demarcate and classify the boundaries of paddy fields and fields in Korea. Therefore, this study tried to minimize the time and effort required to divide paddy fields and fields through the application of the Farm Map. To improve the fact that UAV image processing for a wide area requires a lot of time and effort to classify objects, we suggest a method for optimizing cultivated crop recognition. This study aimed to evaluate the applicability and effectiveness of machine learning classification techniques using a Farm Map in object-based mapping of agricultural land using unmanned aerial vehicles (UAVs). In this study, the advanced function selection method for object classification is to improve classification accuracy by using two types of classifiers, support vector machine (SVM) and random forest (RF). As a result of classification by applying a Farm Map-based SVM algorithm to wide-area UAV images, producer’s accuracy (PA) was 81.68%, user’s accuracy (UA) was 75.09%, the Kappa coefficient was 0.77, and the F-measure was 0.78. The results of classification by the Farm Map-based RF algorithm were as follows: PA of 96.58%, UA of 92.27%, a Kappa coefficient of 0.94, and the F-measure of 0.94. In the cultivation environment in which various crops were mixed, the corn cultivation area was estimated to be 96.54 ha by SVM, showing an accuracy of 90.27%. RF provided an estimate of 98.77 ha and showed an accuracy of 92.36%, which was higher than that of SVM. As a result of using the Farm Map for the object-based classification method, the agricultural land classification showed a higher efficiency in terms of time than the existing object classification method. Most importantly, it was confirmed that the efficiency of data processing can be increased by minimizing the possibility of misclassification in the obtained results. The obtained results confirmed that rapid and reliable analysis is possible when the cultivated area of crops is identified using UAV images, a Farm Map, and machine learning.},
DOI = {10.3390/agronomy11081554}
}



@Article{s21165263,
AUTHOR = {Rosado-Sanz, Javier and Jarabo-Amores, María-Pilar and Dauvignac, Jean-Yves and Mata-Moya, David and Lanteri, Jérôme and Migliaccio, Claire},
TITLE = {Design and Validation of a Reflectarray Antenna with Optimized Beam for Ground Target Monitoring with a DVB-S-Based Passive Radar},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5263},
URL = {https://www.mdpi.com/1424-8220/21/16/5263},
PubMedID = {34450720},
ISSN = {1424-8220},
ABSTRACT = {A reflectarray antenna with an optimized sectorial beam is designed for the surveillance channel of a DVB-S-based passive radar (PR). The employment of satellite illuminators requires a high gain antenna to counteract the losses due to the great distance from the transmitter, but without forgetting a beamwidth wide enough to provide angular coverage. A method based on optimizing the position of several contiguous beams is proposed to achieve the required sectorial pattern. Different reflectarray elements are designed to achieve S-curves with smooth slopes and covering all the required phases (the S-curve represents the reflection phase of a single element, as a function of size, rotation and incidence angle). The real phase and modulus of the reflection coefficient of each element are considered in the optimization process to achieve the best real prototype. Geometry has been studied and adapted to employ commercial elements for the feed, feed-arm and the structure that holds the aperture. The designed prototype has been characterized in an anechoic chamber achieving a stable gain greater than 19 dBi in almost the complete DVB-S band, from 10.5 GHz to 12 GHz with a sectorial beam of 8.7∘×5.2∘. The prototype has also been validated in PR trials in terrestrial scenarios allowing the detection of cars at distances up to 600 m away from the PR, improving the performance achieved with commercial parabolic dish antennas.},
DOI = {10.3390/s21165263}
}



@Article{f12081035,
AUTHOR = {Perroy, Ryan L. and Sullivan, Timo and Benitez, David and Hughes, R. Flint and Keith, Lisa M. and Brill, Eva and Kissinger, Karma and Duda, Daniel},
TITLE = {Spatial Patterns of ‘Ōhi‘a Mortality Associated with Rapid ‘Ōhi‘a Death and Ungulate Presence},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1035},
URL = {https://www.mdpi.com/1999-4907/12/8/1035},
ISSN = {1999-4907},
ABSTRACT = {Effective forest management, particularly during forest disturbance events, requires timely and accurate monitoring information at appropriate spatial scales. In Hawai‘i, widespread ‘ōhi‘a (Metrosideros polymorpha Gaud.) mortality associated with introduced fungal pathogens affects forest stands across the archipelago, further impacting native ecosystems already under threat from invasive species. Here, we share results from an integrated monitoring program based on high resolution (&lt;5 cm) aerial imagery, field sampling, and confirmatory laboratory testing to detect and monitor ‘ōhi‘a mortality at the individual tree level across four representative sites on Hawai‘i island. We developed a custom imaging system for helicopter operations to map thousands of hectares (ha) per flight, a more useful scale than the ten to hundreds of ha typically covered using small, unoccupied aerial systems. Based on collected imagery, we developed a rating system of canopy condition to identify ‘ōhi‘a trees suspected of infection by the fungal pathogens responsible for rapid ‘ōhi‘a death (ROD); we used this system to quickly generate and share suspect tree candidate locations with partner agencies to rapidly detect new mortality outbreaks and prioritize field sampling efforts. In three of the four sites, 98% of laboratory samples collected from suspect trees assigned a high confidence rating (n = 50) and 89% of those assigned a medium confidence rating (n = 117) returned positive detections for the fungal pathogens responsible for ROD. The fourth site, which has a history of unexplained ‘ōhi‘a mortality, exhibited much lower positive detection rates: only 6% of sampled trees assigned a high confidence rating (n = 16) and 0% of the sampled suspect trees assigned a medium confidence rating (n = 20) were found to be positive for the pathogen. The disparity in positive detection rates across study sites illustrates challenges to definitively determine the cause of ‘ōhi‘a mortality from aerial imagery alone. Spatial patterns of ROD-associated ‘ōhi‘a mortality were strongly affected by ungulate presence or absence as measured by the density of suspected ROD trees in fenced (i.e., ungulate-free) and unfenced (i.e., ungulate present) areas. Suspected ROD tree densities in neighboring areas containing ungulates were two to 69 times greater than those found in ungulate-free zones. In one study site, a fence line breach occurred during the study period, and feral ungulates entered an area that was previously ungulate-free. Following the breach, suspect ROD tree densities in this area rose from 0.02 to 2.78 suspect trees/ha, highlighting the need for ungulate control to protect ‘ōhi‘a stands from Ceratocystis-induced mortality and repeat monitoring to detect forest changes and resource threats.},
DOI = {10.3390/f12081035}
}



@Article{rs13163073,
AUTHOR = {Bai, Xueyuan and Li, Zhenhai and Li, Wei and Zhao, Yu and Li, Meixuan and Chen, Hongyan and Wei, Shaochong and Jiang, Yuanmao and Yang, Guijun and Zhu, Xicun},
TITLE = {Comparison of Machine-Learning and CASA Models for Predicting Apple Fruit Yields from Time-Series Planet Imageries},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3073},
URL = {https://www.mdpi.com/2072-4292/13/16/3073},
ISSN = {2072-4292},
ABSTRACT = {Apple (Malus domestica Borkh. cv. “Fuji”), an important cash crop, is widely consumed around the world. Accurately predicting preharvest apple fruit yields is critical for planting policy making and agricultural management. This study attempted to explore an effective approach for predicting apple fruit yields based on time-series remote sensing data. In this study, time-series vegetation indices (VIs) were derived from Planet images and analyzed to further construct an accumulated VI (∑VIs)-based random forest (RF∑VI) model and a Carnegie–Ames–Stanford approach (CASA) model for predicting apple fruit yields. The results showed that (1) ∑NDVI was the optimal predictor to construct an RF model for apple fruit yield, and the R2, RMSE, and RPD values of the RF∑NDVI model reached 0.71, 16.40 kg/tree, and 1.83, respectively. (2) The maximum light use efficiency was determined to be 0.499 g C/MJ, and the CASASR model (R2 = 0.57, RMSE = 19.61 kg/tree, and RPD = 1.53) performed better than the CASANDVI model and the CASAAverage model (R2, RMSE, and RPD = 0.56, 24.47 kg/tree, 1.22 and 0.57, 20.82 kg/tree, 1.44, respectively). (3) This study compared the yield prediction accuracies obtained by the models using the same dataset, and the RF∑NDVI model (RPD = 1.83) showed a better performance in predicting apple fruit yields than the CASASR model (RPD = 1.53). The results obtained from this study indicated the potential of the RF∑NDVI model based on time-series Planet images to accurately predict apple fruit yields. The models could provide spatial and quantitative information of apple fruit yield, which would be valuable for agronomists to predict regional apple production to inform and develop national planting policies, agricultural management, and export strategies.},
DOI = {10.3390/rs13163073}
}



@Article{s21165293,
AUTHOR = {Pikalov, Simon and Azaria, Elisha and Sonnenberg, Shaya and Ben-Moshe, Boaz and Azaria, Amos},
TITLE = {Vision-Less Sensing for Autonomous Micro-Drones},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5293},
URL = {https://www.mdpi.com/1424-8220/21/16/5293},
PubMedID = {34450742},
ISSN = {1424-8220},
ABSTRACT = {This work presents a concept of intelligent vision-less micro-drones, which are motivated by flying animals such as insects, birds, and bats. The presented micro-drone (named BAT: Blind Autonomous Tiny-drone) can perform bio-inspired complex tasks without the use of cameras. The BAT uses LIDARs and self-emitted optical-flow in order to perform obstacle avoiding and maze-solving. The controlling algorithms were implemented on an onboard micro-controller, allowing the BAT to be fully autonomous. We further present a method for using the information collected by the drone to generate a detailed mapping of the environment. A complete model of the BAT was implemented and tested using several scenarios both in simulation and field experiments, in which it was able to explore and map complex building autonomously even in total darkness.},
DOI = {10.3390/s21165293}
}



@Article{rs13163088,
AUTHOR = {Wolf, Stefan and Sommer, Lars and Schumann, Arne},
TITLE = {FastAER Det: Fast Aerial Embedded Real-Time Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3088},
URL = {https://www.mdpi.com/2072-4292/13/16/3088},
ISSN = {2072-4292},
ABSTRACT = {Automated detection of objects in aerial imagery is the basis for many applications, such as search and rescue operations, activity monitoring or mapping. However, in many cases it is beneficial to employ a detector on-board of the aerial platform in order to avoid latencies, make basic decisions within the platform and save transmission bandwidth. In this work, we address the task of designing such an on-board aerial object detector, which meets certain requirements in accuracy, inference speed and power consumption. For this, we first outline a generally applicable design process for such on-board methods and then follow this process to develop our own set of models for the task. Specifically, we first optimize a baseline model with regards to accuracy while not increasing runtime. We then propose a fast detection head to significantly improve runtime at little cost in accuracy. Finally, we discuss several aspects to consider during deployment and in the runtime environment. Our resulting four models that operate at 15, 30, 60 and 90 FPS on an embedded Jetson AGX device are published for future benchmarking and comparison by the community.},
DOI = {10.3390/rs13163088}
}



@Article{rs13163095,
AUTHOR = {Zhao, Jianqing and Zhang, Xiaohu and Yan, Jiawei and Qiu, Xiaolei and Yao, Xia and Tian, Yongchao and Zhu, Yan and Cao, Weixing},
TITLE = {A Wheat Spike Detection Method in UAV Images Based on Improved YOLOv5},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3095},
URL = {https://www.mdpi.com/2072-4292/13/16/3095},
ISSN = {2072-4292},
ABSTRACT = {Deep-learning-based object detection algorithms have significantly improved the performance of wheat spike detection. However, UAV images crowned with small-sized, highly dense, and overlapping spikes cause the accuracy to decrease for detection. This paper proposes an improved YOLOv5 (You Look Only Once)-based method to detect wheat spikes accurately in UAV images and solve spike error detection and miss detection caused by occlusion conditions. The proposed method introduces data cleaning and data augmentation to improve the generalization ability of the detection network. The network is rebuilt by adding a microscale detection layer, setting prior anchor boxes, and adapting the confidence loss function of the detection layer based on the IoU (Intersection over Union). These refinements improve the feature extraction for small-sized wheat spikes and lead to better detection accuracy. With the confidence weights, the detection boxes in multiresolution images are fused to increase the accuracy under occlusion conditions. The result shows that the proposed method is better than the existing object detection algorithms, such as Faster RCNN, Single Shot MultiBox Detector (SSD), RetinaNet, and standard YOLOv5. The average accuracy (AP) of wheat spike detection in UAV images is 94.1%, which is 10.8% higher than the standard YOLOv5. Thus, the proposed method is a practical way to handle the spike detection in complex field scenarios and provide technical references for field-level wheat phenotype monitoring.},
DOI = {10.3390/rs13163095}
}



@Article{rs13163100,
AUTHOR = {Qi, Guanghui and Chang, Chunyan and Yang, Wei and Gao, Peng and Zhao, Gengxing},
TITLE = {Soil Salinity Inversion in Coastal Corn Planting Areas by the Satellite-UAV-Ground Integration Approach},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3100},
URL = {https://www.mdpi.com/2072-4292/13/16/3100},
ISSN = {2072-4292},
ABSTRACT = {Soil salinization is a significant factor affecting corn growth in coastal areas. How to use multi-source remote sensing data to achieve the target of rapid, efficient and accurate soil salinity monitoring in a large area is worth further study. In this research, using Kenli District of the Yellow River Delta as study area, the inversion of soil salinity in a corn planting area was carried out based on the integration of ground imaging hyperspectral, unmanned aerial vehicles (UAV) multispectral and Sentinel-2A satellite multispectral images. The UAV and ground images were fused, and the partial least squares inversion model was constructed by the fused UAV image. Then, inversion model was scaled up to the satellite by the TsHARP method, and finally, the accuracy of the satellite-UAV-ground inversion model and results was verified. The results show that the band fusion of UAV and ground images effectively enrich the spectral information of the UAV image. The accuracy of the inversion model constructed based on the fused UAV images was improved. The inversion results of soil salinity based on the integration of satellite-UAV-ground were highly consistent with the measured soil salinity (R2 = 0.716 and RMSE = 0.727), and the inversion model had excellent universal applicability. This research integrated the advantages of multi-source data to establish a unified satellite-UAV-ground model, which improved the ability of large-scale remote sensing data to finely indicate soil salinity.},
DOI = {10.3390/rs13163100}
}



@Article{min11080846,
AUTHOR = {Sinaice, Brian Bino and Owada, Narihiro and Saadat, Mahdi and Toriya, Hisatoshi and Inagaki, Fumiaki and Bagai, Zibisani and Kawamura, Youhei},
TITLE = {Coupling NCA Dimensionality Reduction with Machine Learning in Multispectral Rock Classification Problems},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {846},
URL = {https://www.mdpi.com/2075-163X/11/8/846},
ISSN = {2075-163X},
ABSTRACT = {Though multitudes of industries depend on the mining industry for resources, this industry has taken hits in terms of declining mineral ore grades and its current use of traditional, time-consuming and computationally costly rock and mineral identification methods. Therefore, this paper proposes integrating Hyperspectral Imaging, Neighbourhood Component Analysis (NCA) and Machine Learning (ML) as a combined system that can identify rocks and minerals. Modestly put, hyperspectral imaging gathers electromagnetic signatures of the rocks in hundreds of spectral bands. However, this data suffers from what is termed the ‘dimensionality curse’, which led to our employment of NCA as a dimensionality reduction technique. NCA, in turn, highlights the most discriminant feature bands, number of which being dependent on the intended application(s) of this system. Our envisioned application is rock and mineral classification via unmanned aerial vehicle (UAV) drone technology. In this study, we performed a 204-hyperspectral to 5-band multispectral reduction, because current production drones are limited to five multispectral bands sensors. Based on these bands, we applied ML to identify and classify rocks, thereby proving our hypothesis, reducing computational costs, attaining an ML classification accuracy of 71%, and demonstrating the potential mining industry optimisations attainable through this integrated system.},
DOI = {10.3390/min11080846}
}



@Article{make3030033,
AUTHOR = {Sejr, Jonas Herskind and Schneider-Kamp, Peter and Ayoub, Naeem},
TITLE = {Surrogate Object Detection Explainer (SODEx) with YOLOv4 and LIME},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {3},
YEAR = {2021},
NUMBER = {3},
PAGES = {662--671},
URL = {https://www.mdpi.com/2504-4990/3/3/33},
ISSN = {2504-4990},
ABSTRACT = {Due to impressive performance, deep neural networks for object detection in images have become a prevalent choice. Given the complexity of the neural network models used, users of these algorithms are typically given no hint as to how the objects were found. It remains, for example, unclear whether an object is detected based on what it looks like or based on the context in which it is located. We have developed an algorithm, Surrogate Object Detection Explainer (SODEx), that can explain any object detection algorithm using any classification explainer. We evaluate SODEx qualitatively and quantitatively by detecting objects in the COCO dataset with YOLOv4 and explaining these detections with LIME. This empirical evaluation does not only demonstrate the value of explainable object detection, it also provides valuable insights into how YOLOv4 detects objects.},
DOI = {10.3390/make3030033}
}



@Article{app11167240,
AUTHOR = {Jembre, Yalew Zelalem and Nugroho, Yuniarto Wimbo and Khan, Muhammad Toaha Raza and Attique, Muhammad and Paul, Rajib and Shah, Syed Hassan Ahmed and Kim, Beomjoon},
TITLE = {Evaluation of Reinforcement and Deep Learning Algorithms in Controlling Unmanned Aerial Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7240},
URL = {https://www.mdpi.com/2076-3417/11/16/7240},
ISSN = {2076-3417},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are abundantly becoming a part of society, which is a trend that is expected to grow even further. The quadrotor is one of the drone technologies that is applicable in many sectors and in both military and civilian activities, with some applications requiring autonomous flight. However, stability, path planning, and control remain significant challenges in autonomous quadrotor flights. Traditional control algorithms, such as proportional-integral-derivative (PID), have deficiencies, especially in tuning. Recently, machine learning has received great attention in flying UAVs to desired positions autonomously. In this work, we configure the quadrotor to fly autonomously by using agents (the machine learning schemes being used to fly the quadrotor autonomously) to learn about the virtual physical environment. The quadrotor will fly from an initial to a desired position. When the agent brings the quadrotor closer to the desired position, it is rewarded; otherwise, it is punished. Two reinforcement learning models, Q-learning and SARSA, and a deep learning deep Q-network network are used as agents. The simulation is conducted by integrating the robot operating system (ROS) and Gazebo, which allowed for the implementation of the learning algorithms and the physical environment, respectively. The result has shown that the Deep Q-network network with Adadelta optimizer is the best setting to fly the quadrotor from the initial to desired position.},
DOI = {10.3390/app11167240}
}



@Article{math9161868,
AUTHOR = {Marchetti, Francesco and Minisci, Edmondo},
TITLE = {Genetic Programming Guidance Control System for a Reentry Vehicle under Uncertainties},
JOURNAL = {Mathematics},
VOLUME = {9},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {1868},
URL = {https://www.mdpi.com/2227-7390/9/16/1868},
ISSN = {2227-7390},
ABSTRACT = {As technology improves, the complexity of controlled systems increases as well. Alongside it, these systems need to face new challenges, which are made available by this technology advancement. To overcome these challenges, the incorporation of AI into control systems is changing its status, from being just an experiment made in academia, towards a necessity. Several methods to perform this integration of AI into control systems have been considered in the past. In this work, an approach involving GP to produce, offline, a control law for a reentry vehicle in the presence of uncertainties on the environment and plant models is studied, implemented and tested. The results show the robustness of the proposed approach, which is capable of producing a control law of a complex nonlinear system in the presence of big uncertainties. This research aims to describe and analyze the effectiveness of a control approach to generate a nonlinear control law for a highly nonlinear system in an automated way. Such an approach would benefit the control practitioners by providing an alternative to classical control approaches, without having to rely on linearization techniques.},
DOI = {10.3390/math9161868}
}



@Article{s21165323,
AUTHOR = {Kim, Yongsu and Kang, Hyoeun and Suryanto, Naufal and Larasati, Harashta Tatimma and Mukaroh, Afifatul and Kim, Howon},
TITLE = {Extended Spatially Localized Perturbation GAN (eSLP-GAN) for Robust Adversarial Camouflage Patches},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5323},
URL = {https://www.mdpi.com/1424-8220/21/16/5323},
PubMedID = {34450763},
ISSN = {1424-8220},
ABSTRACT = {Deep neural networks (DNNs), especially those used in computer vision, are highly vulnerable to adversarial attacks, such as adversarial perturbations and adversarial patches. Adversarial patches, often considered more appropriate for a real-world attack, are attached to the target object or its surroundings to deceive the target system. However, most previous research employed adversarial patches that are conspicuous to human vision, making them easy to identify and counter. Previously, the spatially localized perturbation GAN (SLP-GAN) was proposed, in which the perturbation was only added to the most representative area of the input images, creating a spatially localized adversarial camouflage patch that excels in terms of visual fidelity and is, therefore, difficult to detect by human vision. In this study, the use of the method called eSLP-GAN was extended to deceive classifiers and object detection systems. Specifically, the loss function was modified for greater compatibility with an object-detection model attack and to increase robustness in the real world. Furthermore, the applicability of the proposed method was tested on the CARLA simulator for a more authentic real-world attack scenario.},
DOI = {10.3390/s21165323}
}



@Article{s21165326,
AUTHOR = {Ramalingam, Balakrishnan and Tun, Thein and Mohan, Rajesh Elara and Gómez, Braulio Félix and Cheng, Ruoxi and Balakrishnan, Selvasundari and Mohan Rayaguru, Madan and Hayat, Abdullah Aamir},
TITLE = {AI Enabled IoRT Framework for Rodent Activity Monitoring in a False Ceiling Environment},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5326},
URL = {https://www.mdpi.com/1424-8220/21/16/5326},
PubMedID = {34450767},
ISSN = {1424-8220},
ABSTRACT = {Routine rodent inspection is essential to curbing rat-borne diseases and infrastructure damages within the built environment. Rodents find false ceilings to be a perfect spot to seek shelter and construct their habitats. However, a manual false ceiling inspection for rodents is laborious and risky. This work presents an AI-enabled IoRT framework for rodent activity monitoring inside a false ceiling using an in-house developed robot called “Falcon”. The IoRT serves as a bridge between the users and the robots, through which seamless information sharing takes place. The shared images by the robots are inspected through a Faster RCNN ResNet 101 object detection algorithm, which is used to automatically detect the signs of rodent inside a false ceiling. The efficiency of the rodent activity detection algorithm was tested in a real-world false ceiling environment, and detection accuracy was evaluated with the standard performance metrics. The experimental results indicate that the algorithm detects rodent signs and 3D-printed rodents with a good confidence level.},
DOI = {10.3390/s21165326}
}



@Article{w13162163,
AUTHOR = {Chao, Zhenhua and Fang, Xuan and Na, Jiaming and Che, Mingliang},
TITLE = {A Collaborative Sensing System for Farmland Water Conservancy Project Maintenance through Integrating Satellite, Aerial, and Ground Observations},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {2163},
URL = {https://www.mdpi.com/2073-4441/13/16/2163},
ISSN = {2073-4441},
ABSTRACT = {More and more attention has been paid to farmland water conservancy project (FWCP) maintenance in China, which can reallocate water resources in a more rational and efficient manner. Compared with the traditional survey such as field survey, FWCP maintenance can be improved efficiently with geospatial technology. To improve the level of FWCP maintenance in China, a collaborative sensing system framework by integrating satellite, aerial, and ground remote sensing is put forward. The structure of the system framework includes three sections, namely the data acquisition, the operational work, and the application and service. Through the construction and operation of such collaborative sensing system, it will break through the limitation of any single remote sensing platform and provide all-around and real-time information on FWCP. The collaborative monitoring schemes for the designed FWCP maintenance can engage ditch riders to maintain more effectively, which will enable them to communicate more specifically with smallholders in the process of irrigation. Only when ditch riders and farmers are fully involved, irrigation efficiency will be improved. Furthermore, the collaborative sensing system needs feasible standards for multi-source remote sensing data processing and intelligent information extraction such as data fusion, data assimilation, and data mining. In a way, this will promote the application of remote sensing in the field of agricultural irrigation and water saving. On the whole, it will be helpful to improve the traditional maintenance problems and is also the guarantee for establishing a long-term scientific management mechanism of FWCP maintenance in developing countries, especially in China.},
DOI = {10.3390/w13162163}
}



@Article{ani11082345,
AUTHOR = {Monteiro, António and Santos, Sérgio and Gonçalves, Pedro},
TITLE = {Precision Agriculture for Crop and Livestock Farming—Brief Review},
JOURNAL = {Animals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2345},
URL = {https://www.mdpi.com/2076-2615/11/8/2345},
PubMedID = {34438802},
ISSN = {2076-2615},
ABSTRACT = {In the last few decades, agriculture has played an important role in the worldwide economy. The need to produce more food for a rapidly growing population is creating pressure on crop and animal production and a negative impact to the environment. On the other hand, smart farming technologies are becoming increasingly common in modern agriculture to assist in optimizing agricultural and livestock production and minimizing the wastes and costs. Precision agriculture (PA) is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. Precision livestock farming (PLF), relying on the automatic monitoring of individual animals, is used for animal growth, milk production, and the detection of diseases as well as to monitor animal behavior and their physical environment, among others. This study aims to briefly review recent scientific and technological trends in PA and their application in crop and livestock farming, serving as a simple research guide for the researcher and farmer in the application of technology to agriculture. The development and operation of PA applications involve several steps and techniques that need to be investigated further to make the developed systems accurate and implementable in commercial environments.},
DOI = {10.3390/ani11082345}
}



@Article{rs13163145,
AUTHOR = {Singh, Sarvesh Kumar and Banerjee, Bikram Pratap and Raval, Simit},
TITLE = {Three-Dimensional Unique-Identifier-Based Automated Georeferencing and Coregistration of Point Clouds in Underground Mines},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3145},
URL = {https://www.mdpi.com/2072-4292/13/16/3145},
ISSN = {2072-4292},
ABSTRACT = {Spatially referenced and geometrically accurate laser scans are essential for mapping and monitoring applications in underground mines to ensure safe and smooth operation. However, obtaining an absolute 3D map in an underground mine environment is challenging using laser scanning due to the unavailability of global navigation satellite system (GNSS) signals. Consequently, applications that require georeferenced point cloud or coregistered multitemporal point clouds such as detecting changes, monitoring deformations, tracking mine logistics, measuring roadway convergence rate and evaluating construction performance become challenging. Current mapping practices largely include a manual selection of discernable reference points in laser scans for georeferencing and coregistration which is often time-consuming, arduous and error-prone. Moreover, challenges in obtaining a sensor positioning framework, the presence of structurally symmetric layouts and highly repetitive features (such as roof bolts) makes the multitemporal scans difficult to georeference and coregister. This study aims at overcoming these practical challenges through development of three-dimensional unique identifiers (3DUIDs) and a 3D registration (3DReG) workflow. Field testing of the developed approach in an underground coal mine has been found effective with an accuracy of 1.76 m in georeferencing and 0.16 m in coregistration for a scan length of 850 m. Additionally, automatic extraction of mine roadway profile has been demonstrated using 3DUID which is often a compliant and operational requirement for mitigating roadway related hazards that includes roadway convergence rate, roof/rock falls, floor heaves and vehicle clearance for collision avoidance. Potential applications of 3DUID include roadway profile extraction, guided automation, sensor calibration, reference targets for a routine survey and deformation monitoring.},
DOI = {10.3390/rs13163145}
}



@Article{rs13163150,
AUTHOR = {Jovanović, Dušan and Gavrilović, Milan and Sladić, Dubravka and Radulović, Aleksandra and Govedarica, Miro},
TITLE = {Building Change Detection Method to Support Register of Identified Changes on Buildings},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3150},
URL = {https://www.mdpi.com/2072-4292/13/16/3150},
ISSN = {2072-4292},
ABSTRACT = {Based on a newly adopted “Rulebook on the records of identified changes on buildings in Serbia” (2020) that regulates the content, establishment, maintenance and use of records on identified changes on buildings, it is expected that the geodetic-cadastral information system will be extended with these records. The records contain data on determined changes of buildings in relation to the reference epoch of aerial or satellite imagery, namely data on buildings: (1) that are not registered in the real estate cadastre; (2) which are registered in the real estate cadastre, and have been changed in terms of the dimensions in relation to the data registered in the real estate cadastre; (3) which are registered in the real estate cadastre, but are removed on the ground. For this purpose, the LADM-based cadastral data model for Serbia is extended to include records on identified changes on buildings. In the year 2020, Republic Geodetic Authority commenced a new satellite acquisition for the purpose of restoration of official buildings registry, as part of a World Bank project for improving land administration in Serbia. Using this satellite imagery and existing cadastral data, we propose a method based on comparison of object-based and pixel-based image analysis approaches to automatically detect newly built, changed or demolished buildings and import these data into extended cadastral records. Our results, using only VHR images containing only RGB and NIR bands, showed object identification accuracy ranging from 84% to 88%, with kappa statistic from 89% to 96%. The accuracy of obtained results is satisfactory for the purpose of developing a register of changes on buildings to keep cadastral records up to date and to support activities related to legalization of illegal buildings, etc.},
DOI = {10.3390/rs13163150}
}



@Article{act10080191,
AUTHOR = {Abro, Ghulam E Mustafa and Zulkifli, Saiful Azrin B. M. and Asirvadam, Vijanth Sagayan and Ali, Zain Anwar},
TITLE = {Model-Free-Based Single-Dimension Fuzzy SMC Design for Underactuated Quadrotor UAV},
JOURNAL = {Actuators},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {191},
URL = {https://www.mdpi.com/2076-0825/10/8/191},
ISSN = {2076-0825},
ABSTRACT = {The underactuated quadrotor unmanned aerial vehicle (UAV) is one of the nonlinear systems that have few actuators as compared to the degree of freedom (DOF); thus, it is a strenuous task to stabilize its attitude and positions. Moreover, an induction of unmodelled dynamic factors and uncertainties make it more difficult to control its maneuverability. In this paper, a model-free based single-dimension fuzzy sliding mode control (MFSDF-SMC) is proposed to control the attitude and positions of underactuated quadrotor UAV. The paper discusses the kinematic and dynamic models with unmodelled dynamic factors and unknown external disturbances. These unmodelled factors and disturbances may lead the quadrotor towards failure in tracking specific trajectory and may also generate some serious transient and steady-state issues. Furthermore, to avoid the problem of gimbal lock, the model is amalgamated with hyperbolic function to resolve the singularity issues dully developed due to Newton Euler’s dynamic modeling. The simulation results performed for MFSDF-SMC using MATLAB software R2020a are compared with conventional sliding mode control, fuzzy-based sliding control and single-dimension fuzzy-based sliding mode control without a model-free approach. The design and implementation of the model-free single dimension-based fuzzy sliding mode control (MFSDF-SMC) with an updated Lyapunov stability theorem is presented in this work. It is observed that MFSDF-SMC produces robust trajectory performance therefore, and the manuscript suggests the experimental setup to test the proposed algorithm in a noisy environment keeping the same conditions. The verification of the equipment used and its effective demonstration is also available for the reader within the manuscript.},
DOI = {10.3390/act10080191}
}



@Article{s21165397,
AUTHOR = {Vargas, Jorge and Alsweiss, Suleiman and Toker, Onur and Razdan, Rahul and Santos, Joshua},
TITLE = {An Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5397},
URL = {https://www.mdpi.com/1424-8220/21/16/5397},
PubMedID = {34450839},
ISSN = {1424-8220},
ABSTRACT = {Autonomous vehicles (AVs) rely on various types of sensor technologies to perceive the environment and to make logical decisions based on the gathered information similar to humans. Under ideal operating conditions, the perception systems (sensors onboard AVs) provide enough information to enable autonomous transportation and mobility. In practice, there are still several challenges that can impede the AV sensors’ operability and, in turn, degrade their performance under more realistic conditions that actually occur in the physical world. This paper specifically addresses the effects of different weather conditions (precipitation, fog, lightning, etc.) on the perception systems of AVs. In this work, the most common types of AV sensors and communication modules are included, namely: RADAR, LiDAR, ultrasonic, camera, and global navigation satellite system (GNSS). A comprehensive overview of their physical fundamentals, electromagnetic spectrum, and principle of operation is used to quantify the effects of various weather conditions on the performance of the selected AV sensors. This quantification will lead to several advantages in the simulation world by creating more realistic scenarios and by properly fusing responses from AV sensors in any object identification model used in AVs in the physical world. Moreover, it will assist in selecting the appropriate fading or attenuation models to be used in any X-in-the-loop (XIL, e.g., hardware-in-the-loop, software-in-the-loop, etc.) type of experiments to test and validate the manner AVs perceive the surrounding environment under certain conditions.},
DOI = {10.3390/s21165397}
}



@Article{s21165407,
AUTHOR = {Košťák, Milan and Slabý, Antonín},
TITLE = {Designing a Simple Fiducial Marker for Localization in Spatial Scenes Using Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5407},
URL = {https://www.mdpi.com/1424-8220/21/16/5407},
PubMedID = {34450848},
ISSN = {1424-8220},
ABSTRACT = {The paper describes the process of designing a simple fiducial marker. The marker is meant for use in augmented reality applications. Unlike other systems, it does not encode any information, but it can be used for obtaining the position, rotation, relative size, and projective transformation. Also, the system works well with motion blur and is resistant to the marker’s imperfections, which could theoretically be drawn only by hand. Previous systems put constraints on colors that need to be used to form the marker. The proposed system works with any saturated color, leading to better blending with the surrounding environment. The marker’s final shape is a rectangular area of a solid color with three lines of a different color going from the center to three corners of the rectangle. Precise detection can be achieved using neural networks, given that the training set is very varied and well designed. A detailed literature review was performed, and no such system was found. Therefore, the proposed design is novel for localization in the spatial scene. The testing proved that the system works well both indoor and outdoor, and the detections are precise.},
DOI = {10.3390/s21165407}
}



@Article{app11167358,
AUTHOR = {Li, Linlin and Xu, Shufang and Nie, Hua and Mao, Yingchi and Yu, Shun},
TITLE = {Collaborative Target Search Algorithm for UAV Based on Chaotic Disturbance Pigeon-Inspired Optimization},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7358},
URL = {https://www.mdpi.com/2076-3417/11/16/7358},
ISSN = {2076-3417},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have shown their superiority in military and civilian missions. In the face of complex tasks, many UAVs are usually needed to cooperate with each other. Therefore, multi-UAV cooperative target search has attracted more and more scholars’ attention. At present, there are many bionic algorithms for solving the cooperative search problem of multi-UAVs, including particle swarm optimization algorithm (PSO) and differential evolution (DE). Pigeon-inspired optimization (PIO) is a new swarm intelligence optimization algorithm proposed in recent years. It has great advantages over other algorithms in convergence, robustness, and accuracy, and has few parameters to be adjusted. Aiming at the shortcomings of the standard pigeon colony algorithm, such as poor population diversity, slow convergence speed, and the ease of falling into local optimum, we have proposed chaotic disturbance pigeon-inspired optimization (CDPIO) algorithm. The improved tent chaotic map was used to initialize the population and increase the diversity of the population. The disturbance factor is introduced in the iterative update stage of the algorithm to generate new individuals, replace the individuals with poor performance, and carry out disturbance to increase the optimization accuracy. Benchmark functions and UAV target search model were used to test the algorithm performance. The results show that the CDPIO had faster convergence speed, better optimization precision, better robustness, and better performance than PIO.},
DOI = {10.3390/app11167358}
}



@Article{s21165418,
AUTHOR = {Passos, João and Lopes, Sérgio Ivan and Clemente, Filipe Manuel and Moreira, Pedro Miguel and Rico-González, Markel and Bezerra, Pedro and Rodrigues, Luís Paulo},
TITLE = {Wearables and Internet of Things (IoT) Technologies for Fitness Assessment: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5418},
URL = {https://www.mdpi.com/1424-8220/21/16/5418},
PubMedID = {34450860},
ISSN = {1424-8220},
ABSTRACT = {Wearable and Internet of Things (IoT) technologies in sports open a new era in athlete’s training, not only for performance monitoring and evaluation but also for fitness assessment. These technologies rely on sensor systems that collect, process and transmit relevant data, such as biomarkers and/or other performance indicators that are crucial to evaluate the evolution of the athlete’s condition, and therefore potentiate their performance. This work aims to identify and summarize recent studies that have used wearables and IoT technologies and discuss its applicability for fitness assessment. A systematic review of electronic databases (WOS, CCC, DIIDW, KJD, MEDLINE, RSCI, SCIELO, IEEEXplore, PubMed, SPORTDiscus, Cochrane and Web of Science) was undertaken according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. From the 280 studies initially identified, 20 were fully examined in terms of hardware and software and their applicability for fitness assessment. Results have shown that wearable and IoT technologies have been used in sports not only for fitness assessment but also for monitoring the athlete’s internal and external workloads, employing physiological status monitoring and activity recognition and tracking techniques. However, the maturity level of such technologies is still low, particularly with the need for the acquisition of more—and more effective—biomarkers regarding the athlete’s internal workload, which limits its wider adoption by the sports community.},
DOI = {10.3390/s21165418}
}



@Article{agriculture11080766,
AUTHOR = {Haider, Tazeem and Farid, Muhammad Shahid and Mahmood, Rashid and Ilyas, Areeba and Khan, Muhammad Hassan and Haider, Sakeena Tul-Ain and Chaudhry, Muhammad Hamid and Gul, Mehreen},
TITLE = {A Computer-Vision-Based Approach for Nitrogen Content Estimation in Plant Leaves},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {766},
URL = {https://www.mdpi.com/2077-0472/11/8/766},
ISSN = {2077-0472},
ABSTRACT = {Nitrogen is an essential nutrient element required for optimum crop growth and yield. If a specific amount of nitrogen is not applied to crops, their yield is affected. Estimation of nitrogen level in crops is momentous to decide the nitrogen fertilization in crops. The amount of nitrogen in crops is measured through different techniques, including visual inspection of leaf color and texture and by laboratory analysis of plant leaves. Laboratory analysis-based techniques are more accurate than visual inspection, but they are costly, time-consuming, and require skilled laboratorian and precise equipment. Therefore, computer-based systems are required to estimate the amount of nitrogen in field crops. In this paper, a computer vision-based solution is introduced to solve this problem as well as to help farmers by providing an easier, cheaper, and faster approach for measuring nitrogen deficiency in crops. The system takes an image of the crop leaf as input and estimates the amount of nitrogen in it. The image is captured by placing the leaf on a specially designed slate that contains the reference green and yellow colors for that crop. The proposed algorithm automatically extracts the leaf from the image and computes its color similarity with the reference colors. In particular, we define a green color value (GCV) index from this analysis, which serves as a nitrogen indicator. We also present an evaluation of different color distance models to find a model able to accurately capture the color differences. The performance of the proposed system is evaluated on a Spinacia oleracea dataset. The results of the proposed system and laboratory analysis are highly correlated, which shows the effectiveness of the proposed system.},
DOI = {10.3390/agriculture11080766}
}



@Article{rs13163188,
AUTHOR = {Takechi, Hitoshi and Aragaki, Shunsuke and Irie, Mitsuteru},
TITLE = {Differentiation of River Sediments Fractions in UAV Aerial Images by Convolution Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3188},
URL = {https://www.mdpi.com/2072-4292/13/16/3188},
ISSN = {2072-4292},
ABSTRACT = {Riverbed material has multiple functions in river ecosystems, such as habitats, feeding grounds, spawning grounds, and shelters for aquatic organisms, and particle size of riverbed material reflects the tractive force of the channel flow. Therefore, regular surveys of riverbed material are conducted for environmental protection and river flood control projects. The field method is the most conventional riverbed material survey. However, conventional surveys of particle size of riverbed material require much labor, time, and cost to collect material on site. Furthermore, its spatial representativeness is also a problem because of the limited survey area against a wide riverbank. As a further solution to these problems, in this study, we tried an automatic classification of riverbed conditions using aerial photography with an unmanned aerial vehicle (UAV) and image recognition with artificial intelligence (AI) to improve survey efficiency. Due to using AI for image processing, a large number of images can be handled regardless of whether they are of fine or coarse particles. We tried a classification of aerial riverbed images that have the difference of particle size characteristics with a convolutional neural network (CNN). GoogLeNet, Alexnet, VGG-16 and ResNet, the common pre-trained networks, were retrained to perform the new task with the 70 riverbed images using transfer learning. Among the networks tested, GoogleNet showed the best performance for this study. The overall accuracy of the image classification reached 95.4%. On the other hand, it was supposed that shadows of the gravels caused the error of the classification. The network retrained with the images taken in the uniform temporal period gives higher accuracy for classifying the images taken in the same period as the training data. The results suggest the potential of evaluating riverbed materials using aerial photography with UAV and image recognition with CNN.},
DOI = {10.3390/rs13163188}
}



@Article{rs13163190,
AUTHOR = {Li, Kai-Yun and Burnside, Niall G. and de Lima, Raul Sampaio and Peciña, Miguel Villoslada and Sepp, Karli and Cabral Pinheiro, Victor Henrique and de Lima, Bruno Rucy Carneiro Alves and Yang, Ming-Der and Vain, Ants and Sepp, Kalev},
TITLE = {An Automated Machine Learning Framework in Unmanned Aircraft Systems: New Insights into Agricultural Management Practices Recognition Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3190},
URL = {https://www.mdpi.com/2072-4292/13/16/3190},
ISSN = {2072-4292},
ABSTRACT = {The recent trend of automated machine learning (AutoML) has been driving further significant technological innovation in the application of artificial intelligence from its automated algorithm selection and hyperparameter optimization of the deployable pipeline model for unraveling substance problems. However, a current knowledge gap lies in the integration of AutoML technology and unmanned aircraft systems (UAS) within image-based data classification tasks. Therefore, we employed a state-of-the-art (SOTA) and completely open-source AutoML framework, Auto-sklearn, which was constructed based on one of the most widely used ML systems: Scikit-learn. It was combined with two novel AutoML visualization tools to focus particularly on the recognition and adoption of UAS-derived multispectral vegetation indices (VI) data across a diverse range of agricultural management practices (AMP). These include soil tillage methods (STM), cultivation methods (CM), and manure application (MA), and are under the four-crop combination fields (i.e., red clover-grass mixture, spring wheat, pea-oat mixture, and spring barley). Furthermore, they have currently not been efficiently examined and accessible parameters in UAS applications are absent for them. We conducted the comparison of AutoML performance using three other common machine learning classifiers, namely Random Forest (RF), support vector machine (SVM), and artificial neural network (ANN). The results showed AutoML achieved the highest overall classification accuracy numbers after 1200 s of calculation. RF yielded the second-best classification accuracy, and SVM and ANN were revealed to be less capable among some of the given datasets. Regarding the classification of AMPs, the best recognized period for data capture occurred in the crop vegetative growth stage (in May). The results demonstrated that CM yielded the best performance in terms of classification, followed by MA and STM. Our framework presents new insights into plant–environment interactions with capable classification capabilities. It further illustrated the automatic system would become an important tool in furthering the understanding for future sustainable smart farming and field-based crop phenotyping research across a diverse range of agricultural environmental assessment and management applications.},
DOI = {10.3390/rs13163190}
}



@Article{rs13163191,
AUTHOR = {Ezzy, Haitham and Charter, Motti and Bonfante, Antonello and Brook, Anna},
TITLE = {How the Small Object Detection via Machine Learning and UAS-Based Remote-Sensing Imagery Can Support the Achievement of SDG2: A Case Study of Vole Burrows},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3191},
URL = {https://www.mdpi.com/2072-4292/13/16/3191},
ISSN = {2072-4292},
ABSTRACT = {Small mammals, and particularly rodents, are common inhabitants of farmlands, where they play key roles in the ecosystem, but when overabundant, they can be major pests, able to reduce crop production and farmers’ incomes, with tangible effects on the achievement of Sustainable Development Goals no 2 (SDG2, Zero Hunger) of the United Nations. Farmers do not currently have a standardized, accurate method of detecting the presence, abundance, and locations of rodents in their fields, and hence do not have environmentally efficient methods of rodent control able to promote sustainable agriculture oriented to reduce the environmental impacts of cultivation. New developments in unmanned aerial system (UAS) platforms and sensor technology facilitate cost-effective data collection through simultaneous multimodal data collection approaches at very high spatial resolutions in environmental and agricultural contexts. Object detection from remote-sensing images has been an active research topic over the last decade. With recent increases in computational resources and data availability, deep learning-based object detection methods are beginning to play an important role in advancing remote-sensing commercial and scientific applications. However, the performance of current detectors on various UAS-based datasets, including multimodal spatial and physical datasets, remains limited in terms of small object detection. In particular, the ability to quickly detect small objects from a large observed scene (at field scale) is still an open question. In this paper, we compare the efficiencies of applying one- and two-stage detector models to a single UAS-based image and a processed (via Pix4D mapper photogrammetric program) UAS-based orthophoto product to detect rodent burrows, for agriculture/environmental applications as to support farmer activities in the achievements of SDG2. Our results indicate that the use of multimodal data from low-cost UASs within a self-training YOLOv3 model can provide relatively accurate and robust detection for small objects (mAP of 0.86 and an F1-score of 93.39%), and can deliver valuable insights for field management with high spatial precision able to reduce the environmental costs of crop production in the direction of precision agriculture management.},
DOI = {10.3390/rs13163191}
}



@Article{rs13163207,
AUTHOR = {Feng, Shuai and Cao, Yingli and Xu, Tongyu and Yu, Fenghua and Zhao, Dongxue and Zhang, Guosheng},
TITLE = {Rice Leaf Blast Classification Method Based on Fused Features and One-Dimensional Deep Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3207},
URL = {https://www.mdpi.com/2072-4292/13/16/3207},
ISSN = {2072-4292},
ABSTRACT = {Rice leaf blast, which is seriously affecting the yield and quality of rice around the world, is a fungal disease that easily develops under high temperature and humidity conditions. Therefore, the use of accurate and non-destructive diagnostic methods is important for rice production management. Hyperspectral imaging technology is a type of crop disease identification method with great potential. However, a large amount of redundant information mixed in hyperspectral data makes it more difficult to establish an efficient disease classification model. At the same time, the difficulty and small scale of agricultural hyperspectral imaging data acquisition has resulted in unrepresentative features being acquired. Therefore, the focus of this study was to determine the best classification features and classification models for the five disease classes of leaf blast in order to improve the accuracy of grading the disease. First, the hyperspectral imaging data were pre-processed in order to extract rice leaf samples of five disease classes, and the number of samples was increased by data augmentation methods. Secondly, spectral feature wavelengths, vegetation indices and texture features were obtained based on the amplified sample data. Thirdly, seven one-dimensional deep convolutional neural networks (DCNN) models were constructed based on spectral feature wavelengths, vegetation indices, texture features and their fusion features. Finally, the model in this paper was compared and analyzed with the Inception V3, ZF-Net, TextCNN and bidirectional gated recurrent unit (BiGRU); support vector machine (SVM); and extreme learning machine (ELM) models in order to determine the best classification features and classification models for different disease classes of leaf blast. The results showed that the classification model constructed using fused features was significantly better than the model constructed with a single feature in terms of accuracy in grading the degree of leaf blast disease. The best performance was achieved with the combination of the successive projections algorithm (SPA) selected feature wavelengths and texture features (TFs). The modeling results also show that the DCNN model provides better classification capability for disease classification than the Inception V3, ZF-Net, TextCNN, BiGRU, SVM and ELM classification models. The SPA + TFs-DCNN achieved the best classification accuracy with an overall accuracy (OA) and Kappa of 98.58% and 98.22%, respectively. In terms of the classification of the specific different disease classes, the F1-scores for diseases of classes 0, 1 and 2 were all 100%, while the F1-scores for diseases of classes 4 and 5 were 96.48% and 96.68%, respectively. This study provides a new method for the identification and classification of rice leaf blast and a research basis for assessing the extent of the disease in the field.},
DOI = {10.3390/rs13163207}
}



@Article{s21165460,
AUTHOR = {Lang, Lei and Xu, Ke and Zhang, Qian and Wang, Dong},
TITLE = {Fast and Accurate Object Detection in Remote Sensing Images Based on Lightweight Deep Neural Network},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5460},
URL = {https://www.mdpi.com/1424-8220/21/16/5460},
PubMedID = {34450908},
ISSN = {1424-8220},
ABSTRACT = {Deep learning-based object detection in remote sensing images is an important yet challenging task due to a series of difficulties, such as complex geometry scene, dense target quantity, and large variant in object distributions and scales. Moreover, algorithm designers also have to make a trade-off between model’s complexity and accuracy to meet the real-world deployment requirements. To deal with these challenges, we proposed a lightweight YOLO-like object detector with the ability to detect objects in remote sensing images with high speed and high accuracy. The detector is constructed with efficient channel attention layers to improve the channel information sensitivity. Differential evolution was also developed to automatically find the optimal anchor configurations to address issue of large variant in object scales. Comprehensive experiment results show that the proposed network outperforms state-of-the-art lightweight models by 5.13% and 3.58% in accuracy on the RSOD and DIOR dataset, respectively. The deployed model on an NVIDIA Jetson Xavier NX embedded board can achieve a detection speed of 58 FPS with less than 10W power consumption, which makes the proposed detector very suitable for low-cost low-power remote sensing application scenarios.},
DOI = {10.3390/s21165460}
}



@Article{a14080239,
AUTHOR = {Song, Zhenyu and Yan, Xuemei and Zhao, Lvxing and Fan, Luyi and Tang, Cheng and Ji, Junkai},
TITLE = {Adaptive Self-Scaling Brain-Storm Optimization via a Chaotic Search Mechanism},
JOURNAL = {Algorithms},
VOLUME = {14},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {239},
URL = {https://www.mdpi.com/1999-4893/14/8/239},
ISSN = {1999-4893},
ABSTRACT = {Brain-storm optimization (BSO), which is a population-based optimization algorithm, exhibits a poor search performance, premature convergence, and a high probability of falling into local optima. To address these problems, we developed the adaptive mechanism-based BSO (ABSO) algorithm based on the chaotic local search in this study. The adjustment of the search space using the local search method based on an adaptive self-scaling mechanism balances the global search and local development performance of the ABSO algorithm, effectively preventing the algorithm from falling into local optima and improving its convergence accuracy. To verify the stability and effectiveness of the proposed ABSO algorithm, the performance was tested using 29 benchmark test functions, and the mean and standard deviation were compared with those of five other optimization algorithms. The results showed that ABSO outperforms the other algorithms in terms of stability and convergence accuracy. In addition, the performance of ABSO was further verified through a nonparametric statistical test.},
DOI = {10.3390/a14080239}
}



@Article{s21165476,
AUTHOR = {Wang, Rui and Zou, Jialing and Wen, James Zhiqing},
TITLE = {SFA-MDEN: Semantic-Feature-Aided Monocular Depth Estimation Network Using Dual Branches},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5476},
URL = {https://www.mdpi.com/1424-8220/21/16/5476},
PubMedID = {34450917},
ISSN = {1424-8220},
ABSTRACT = {Monocular depth estimation based on unsupervised learning has attracted great attention due to the rising demand for lightweight monocular vision sensors. Inspired by multi-task learning, semantic information has been used to improve the monocular depth estimation models. However, multi-task learning is still limited by multi-type annotations. As far as we know, there are scarcely any large public datasets that provide all the necessary information. Therefore, we propose a novel network architecture Semantic-Feature-Aided Monocular Depth Estimation Network (SFA-MDEN) to extract multi-resolution depth features and semantic features, which are merged and fed into the decoder, with the goal of predicting depth with the support of semantics. Instead of using loss functions to relate the semantics and depth, the fusion of feature maps for semantics and depth is employed to predict the monocular depth. Therefore, two accessible datasets with similar topics for depth estimation and semantic segmentation can meet the requirements of SFA-MDEN for training sets. We explored the performance of the proposed SFA-MDEN with experiments on different datasets, including KITTI, Make3D, and our own dataset BHDE-v1. The experimental results demonstrate that SFA-MDEN achieves competitive accuracy and generalization capacity compared to state-of-the-art methods.},
DOI = {10.3390/s21165476}
}



@Article{infrastructures6080115,
AUTHOR = {Munawar, Hafiz Suliman and Hammad, Ahmed W. A. and Haddad, Assed and Soares, Carlos Alberto Pereira and Waller, S. Travis},
TITLE = {Image-Based Crack Detection Methods: A Review},
JOURNAL = {Infrastructures},
VOLUME = {6},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {115},
URL = {https://www.mdpi.com/2412-3811/6/8/115},
ISSN = {2412-3811},
ABSTRACT = {Annually, millions of dollars are spent to carry out defect detection in key infrastructure including roads, bridges, and buildings. The aftermath of natural disasters like floods and earthquakes leads to severe damage to the urban infrastructure. Maintenance operations that follow for the damaged infrastructure often involve a visual inspection and assessment of their state to ensure their functional and physical integrity. Such damage may appear in the form of minor or major cracks, which gradually spread, leading to ultimate collapse or destruction of the structure. Crack detection is a very laborious task if performed via manual visual inspection. Many infrastructure elements need to be checked regularly and it is therefore not feasible as it will require significant human resources. This may also result in cases where cracks go undetected. A need, therefore, exists for performing automatic defect detection in infrastructure to ensure its effectiveness and reliability. Using image processing techniques, the captured or scanned images of the infrastructure parts can be analyzed to identify any possible defects. Apart from image processing, machine learning methods are being increasingly applied to ensure better performance outcomes and robustness in crack detection. This paper provides a review of image-based crack detection techniques which implement image processing and/or machine learning. A total of 30 research articles have been collected for the review which is published in top tier journals and conferences in the past decade. A comprehensive analysis and comparison of these methods are performed to highlight the most promising automated approaches for crack detection.},
DOI = {10.3390/infrastructures6080115}
}



@Article{rs13163234,
AUTHOR = {Cao, Jingwei and Song, Chuanxue and Song, Shixin and Xiao, Feng and Zhang, Xu and Liu, Zhiyang and Ang, Marcelo H.},
TITLE = {Robust Object Tracking Algorithm for Autonomous Vehicles in Complex Scenes},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3234},
URL = {https://www.mdpi.com/2072-4292/13/16/3234},
ISSN = {2072-4292},
ABSTRACT = {Object tracking is an essential aspect of environmental perception technology for autonomous vehicles. The existing object tracking algorithms can only be applied well to simple scenes. When the scenes become complex, the algorithms have poor tracking performance and insufficient robustness, and the problems of tracking drift and object loss are prone to occur. Therefore, a robust object tracking algorithm for autonomous vehicles in complex scenes is proposed. Firstly, we study the Siam-FC network and related algorithms, and analyze the problems that need to be addressed in object tracking. Secondly, the construction of a double-template Siamese network model based on multi-feature fusion is described, as is the use of the improved MobileNet V2 as the feature extraction backbone network, and the attention mechanism and template online update mechanism are introduced. Finally, relevant experiments were carried out based on public datasets and actual driving videos, with the aim of fully testing the tracking performance of the proposed algorithm on different objects in a variety of complex scenes. The results showed that, compared with other algorithms, the proposed algorithm had high tracking accuracy and speed, demonstrated stronger robustness and anti-interference abilities, and could still accurately track the object in real time without the introduction of complex structures. This algorithm can be effectively applied in intelligent vehicle driving assistance, and it will help to promote the further development and improvement of computer vision technology in the field of environmental perception.},
DOI = {10.3390/rs13163234}
}



@Article{rs13163241,
AUTHOR = {Hassanzadeh, Amirhossein and Zhang, Fei and van Aardt, Jan and Murphy, Sean P. and Pethybridge, Sarah J.},
TITLE = {Broadacre Crop Yield Estimation Using Imaging Spectroscopy from Unmanned Aerial Systems (UAS): A Field-Based Case Study with Snap Bean},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3241},
URL = {https://www.mdpi.com/2072-4292/13/16/3241},
ISSN = {2072-4292},
ABSTRACT = {Accurate, precise, and timely estimation of crop yield is key to a grower’s ability to proactively manage crop growth and predict harvest logistics. Such yield predictions typically are based on multi-parametric models and in-situ sampling. Here we investigate the extension of a greenhouse study, to low-altitude unmanned aerial systems (UAS). Our principal objective was to investigate snap bean crop (Phaseolus vulgaris) yield using imaging spectroscopy (hyperspectral imaging) in the visible to near-infrared (VNIR; 400–1000 nm) region via UAS. We aimed to solve the problem of crop yield modelling by identifying spectral features explaining yield and evaluating the best time period for accurate yield prediction, early in time. We introduced a Python library, named Jostar, for spectral feature selection. Embedded in Jostar, we proposed a new ranking method for selected features that reaches an agreement between multiple optimization models. Moreover, we implemented a well-known denoising algorithm for the spectral data used in this study. This study benefited from two years of remotely sensed data, captured at multiple instances over the summers of 2019 and 2020, with 24 plots and 18 plots, respectively. Two harvest stage models, early and late harvest, were assessed at two different locations in upstate New York, USA. Six varieties of snap bean were quantified using two components of yield, pod weight and seed length. We used two different vegetation detection algorithms. the Red-Edge Normalized Difference Vegetation Index (RENDVI) and Spectral Angle Mapper (SAM), to subset the fields into vegetation vs. non-vegetation pixels. Partial least squares regression (PLSR) was used as the regression model. Among nine different optimization models embedded in Jostar, we selected the Genetic Algorithm (GA), Ant Colony Optimization (ACO), Simulated Annealing (SA), and Particle Swarm Optimization (PSO) and their resulting joint ranking. The findings show that pod weight can be explained with a high coefficient of determination (R2 = 0.78–0.93) and low root-mean-square error (RMSE = 940–1369 kg/ha) for two years of data. Seed length yield assessment resulted in higher accuracies (R2 = 0.83–0.98) and lower errors (RMSE = 4.245–6.018 mm). Among optimization models used, ACO and SA outperformed others and the SAM vegetation detection approach showed improved results when compared to the RENDVI approach when dense canopies were being examined. Wavelengths at 450, 500, 520, 650, 700, and 760 nm, were identified in almost all data sets and harvest stage models used. The period between 44–55 days after planting (DAP) the optimal time period for yield assessment. Future work should involve transferring the learned concepts to a multispectral system, for eventual operational use; further attention should also be paid to seed length as a ground truth data collection technique, since this yield indicator is far more rapid and straightforward.},
DOI = {10.3390/rs13163241}
}



@Article{s21165516,
AUTHOR = {Schäfer, Matthias and Strohmeier, Martin and Leonardi, Mauro and Lenders, Vincent},
TITLE = {LocaRDS: A Localization Reference Data Set},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5516},
URL = {https://www.mdpi.com/1424-8220/21/16/5516},
PubMedID = {34450957},
ISSN = {1424-8220},
ABSTRACT = {The use of wireless signals for the purposes of localization enables a host of applications relating to the determination and verification of the positions of network participants ranging from radar to satellite navigation. Consequently, this has been a longstanding interest of theoretical and practical research in mobile networks and many solutions have been proposed in the scientific literature. However, it is hard to assess the performance of these in the real world and, more importantly, to compare their advantages and disadvantages in a controlled scientific manner. With this work, we attempt to improve the current state of art methodology in localization research and to place it on a solid scientific grounding for future investigations. Concretely, we developed LocaRDS, an open reference data set of real-world crowdsourced flight data featuring more than 222 million measurements from over 50 million transmissions recorded by 323 sensors. We demonstrate how we can verify the quality of LocaRDS measurements so that it can be used to test, analyze and directly compare different localization methods. Finally, we provide an example implementation for the aircraft localization problem and a discussion of possible metrics for use with LocaRDS.},
DOI = {10.3390/s21165516}
}



@Article{rs13163247,
AUTHOR = {Yao, Guobiao and Yilmaz, Alper and Meng, Fei and Zhang, Li},
TITLE = {Review of Wide-Baseline Stereo Image Matching Based on Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3247},
URL = {https://www.mdpi.com/2072-4292/13/16/3247},
ISSN = {2072-4292},
ABSTRACT = {Strong geometric and radiometric distortions often exist in optical wide-baseline stereo images, and some local regions can include surface discontinuities and occlusions. Digital photogrammetry and computer vision researchers have focused on automatic matching for such images. Deep convolutional neural networks, which can express high-level features and their correlation, have received increasing attention for the task of wide-baseline image matching, and learning-based methods have the potential to surpass methods based on handcrafted features. Therefore, we focus on the dynamic study of wide-baseline image matching and review the main approaches of learning-based feature detection, description, and end-to-end image matching. Moreover, we summarize the current representative research using stepwise inspection and dissection. We present the results of comprehensive experiments on actual wide-baseline stereo images, which we use to contrast and discuss the advantages and disadvantages of several state-of-the-art deep-learning algorithms. Finally, we conclude with a description of the state-of-the-art methods and forecast developing trends with unresolved challenges, providing a guide for future work.},
DOI = {10.3390/rs13163247}
}



@Article{metrology1010003,
AUTHOR = {Jetti, Harsha Vardhana and Salicone, Simona},
TITLE = {A Possibilistic Kalman Filter for the Reduction of the Final Measurement Uncertainty, in Presence of Unknown Systematic Errors},
JOURNAL = {Metrology},
VOLUME = {1},
YEAR = {2021},
NUMBER = {1},
PAGES = {39--51},
URL = {https://www.mdpi.com/2673-8244/1/1/3},
ISSN = {2673-8244},
ABSTRACT = {A Kalman filter is a concept that has been in existence for decades now and it is widely used in numerous areas. It provides a prediction of the system states as well as the uncertainty associated to it. The original Kalman filter can not propagate uncertainty in a correct way when the variables are not distributed normally or when there is a correlation in the measurements or when there is a systematic error in the measurements. For these reasons, there have been numerous variations of the original Kalman filter, most of them mathematically based (like the original one) on the theory of probability. Some of the variations indeed introduce some improvements, but without being completely successful. To deal with these problems, more recently, Kalman filters have also been defined using random-fuzzy variables (RFVs). These filters are capable of also propagating distributions that are not normal and propagating systematic contributions to uncertainty, thus providing the overall measurement uncertainty associated to the state predictions. In this paper, the authors make another step forward, by defining a possibilistic Kalman filter using random-fuzzy variables which not only considers and propagates both random and systematic contributions to uncertainty, but also reduces the overall uncertainty associated to the state predictions by compensating for the unknown residual systematic contributions.},
DOI = {10.3390/metrology1010003}
}



@Article{ijgi10080556,
AUTHOR = {Shen, Shengyu and Chen, Jiasheng and Zhang, Shaoyi and Cheng, Dongbing and Wang, Zhigang and Zhang, Tong},
TITLE = {Deep Fusion of DOM and DSM Features for Benggang Discovery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {556},
URL = {https://www.mdpi.com/2220-9964/10/8/556},
ISSN = {2220-9964},
ABSTRACT = {Benggang is a typical erosional landform in southern and southeastern China. Since benggang poses significant risks to local ecological environments and economic infrastructure, it is vital to accurately detect benggang-eroded areas. Relying only on remote sensing imagery for benggang detection cannot produce satisfactory results. In this study, we propose integrating high-resolution Digital Orthophoto Map (DOM) and Digital Surface Model (DSM) data for efficient and automatic benggang discovery. The fusion of complementary rich information hidden in both DOM and DSM data is realized by a two-stream convolutional neural network (CNN), which integrates aggregated terrain and activation image features that are both extracted by supervised deep learning. We aggregate local low-level geomorphic features via a supervised diffusion-convolutional embedding branch for expressive representations of benggang terrain variations. Activation image features are obtained from an image-oriented convolutional neural network branch. The two sources of information (DOM and DSM) are fused via a gated neural network, which learns the most discriminative features for the detection of benggang. The evaluation of a challenging benggang dataset demonstrates that our method exceeds several baselines, even with limited training examples. The results show that the fusion of DOM and DSM data is beneficial for benggang detection via supervised convolutional and deep fusion networks.},
DOI = {10.3390/ijgi10080556}
}



@Article{app11167550,
AUTHOR = {Tina, Giuseppe Marco and Ventura, Cristina and Ferlito, Sergio and De Vito, Saverio},
TITLE = {A State-of-Art-Review on Machine-Learning Based Methods for PV},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7550},
URL = {https://www.mdpi.com/2076-3417/11/16/7550},
ISSN = {2076-3417},
ABSTRACT = {In the current era, Artificial Intelligence (AI) is becoming increasingly pervasive with applications in several applicative fields effectively changing our daily life. In this scenario, machine learning (ML), a subset of AI techniques, provides machines with the ability to programmatically learn from data to model a system while adapting to new situations as they learn more by data they are ingesting (on-line training). During the last several years, many papers have been published concerning ML applications in the field of solar systems. This paper presents the state of the art ML models applied in solar energy’s forecasting field i.e., for solar irradiance and power production forecasting (both point and interval or probabilistic forecasting), electricity price forecasting and energy demand forecasting. Other applications of ML into the photovoltaic (PV) field taken into account are the modelling of PV modules, PV design parameter extraction, tracking the maximum power point (MPP), PV systems efficiency optimization, PV/Thermal (PV/T) and Concentrating PV (CPV) system design parameters’ optimization and efficiency improvement, anomaly detection and energy management of PV’s storage systems. While many review papers already exist in this regard, they are usually focused only on one specific topic, while in this paper are gathered all the most relevant applications of ML for solar systems in many different fields. The paper gives an overview of the most recent and promising applications of machine learning used in the field of photovoltaic systems.},
DOI = {10.3390/app11167550}
}



@Article{rs13163263,
AUTHOR = {Liu, Zhijie and Guo, Pengju and Liu, Heng and Fan, Pan and Zeng, Pengzong and Liu, Xiangyang and Feng, Ce and Wang, Wang and Yang, Fuzeng},
TITLE = {Gradient Boosting Estimation of the Leaf Area Index of Apple Orchards in UAV Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3263},
URL = {https://www.mdpi.com/2072-4292/13/16/3263},
ISSN = {2072-4292},
ABSTRACT = {The leaf area index (LAI) is a key parameter for describing the canopy structure of apple trees. This index is also employed in evaluating the amount of pesticide sprayed per unit volume of apple trees. Hence, numerous manual and automatic methods have been explored for LAI estimation. In this work, the leaf area indices for different types of apple trees are obtained in terms of multispectral remote-sensing data collected with an unmanned aerial vehicle (UAV), along with simultaneous measurements of apple orchards. The proposed approach was tested on apple trees of the “Fuji”, “Golden Delicious”, and “Ruixue” types, which were planted in the Apple Experimental Station of the Northwest Agriculture and Forestry University in Baishui County, Shaanxi Province, China. Five vegetation indices of strong correlation with the apple leaf area index were selected and used to train models of support vector regression (SVR) and gradient-boosting decision trees (GBDT) for predicting the leaf area index of apple trees. The best model was selected based on the metrics of the coefficient of determination (R2) and the root-mean-square error (RMSE). The experimental results showed that the gradient-boosting decision tree model achieved the best performance with an R2 of 0.846, an RMSE of 0.356, and a spatial efficiency (SPAEF) of 0.57. This demonstrates the feasibility of our approach for fast and accurate remote-sensing-based estimation of the leaf area index of apple trees.},
DOI = {10.3390/rs13163263}
}



@Article{s21165554,
AUTHOR = {Pal, Shantanu and Mukhopadhyay, Subhas and Suryadevara, Nagender},
TITLE = {Development and Progress in Sensors and Technologies for Human Emotion Recognition},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5554},
URL = {https://www.mdpi.com/1424-8220/21/16/5554},
PubMedID = {34451002},
ISSN = {1424-8220},
ABSTRACT = {With the advancement of human-computer interaction, robotics, and especially humanoid robots, there is an increasing trend for human-to-human communications over online platforms (e.g., zoom). This has become more significant in recent years due to the Covid-19 pandemic situation. The increased use of online platforms for communication signifies the need to build efficient and more interactive human emotion recognition systems. In a human emotion recognition system, the physiological signals of human beings are collected, analyzed, and processed with the help of dedicated learning techniques and algorithms. With the proliferation of emerging technologies, e.g., the Internet of Things (IoT), future Internet, and artificial intelligence, there is a high demand for building scalable, robust, efficient, and trustworthy human recognition systems. In this paper, we present the development and progress in sensors and technologies to detect human emotions. We review the state-of-the-art sensors used for human emotion recognition and different types of activity monitoring. We present the design challenges and provide practical references of such human emotion recognition systems in the real world. Finally, we discuss the current trends in applications and explore the future research directions to address issues, e.g., scalability, security, trust, privacy, transparency, and decentralization.},
DOI = {10.3390/s21165554}
}



@Article{app11167582,
AUTHOR = {Žuraulis, Vidas and Sivilevičius, Henrikas and Šabanovič, Eldar and Ivanov, Valentin and Skrickij, Viktor},
TITLE = {Variability of Gravel Pavement Roughness: An Analysis of the Impact on Vehicle Dynamic Response and Driving Comfort},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7582},
URL = {https://www.mdpi.com/2076-3417/11/16/7582},
ISSN = {2076-3417},
ABSTRACT = {Gravel pavement has lower construction costs but poorer performance than asphalt surfaces on roads. It also emits dust and deforms under the impact of vehicle loads and ambient air factors; the resulting ripples and ruts constantly deepen, and therefore increase vehicle vibrations and fuel consumption, and reduce safe driving speed and comfort. In this study, existing pavement quality evaluation indexes are analysed, and a methodology for adapting them for roads with gravel pavement is proposed. We report the measured wave depth and length of gravel pavement profile using the straightedge method on a 160 m long road section at three stages of road utilization. The measured pavement elevation was processed according to ISO 8608, and the frequency response of a vehicle was investigated using simulations in MATLAB/Simulink. The international roughness index (IRI) analysis showed that a speed of 30–45 km/h instead of 80 km/h provided the objective results of the IRI calculation on the flexible pavement due to the decreasing velocity of a vehicle’s unsprung mass on a more deteriorated road pavement state. The influence of the corrugation phenomenon of gravel pavement was explored, identifying specific driving safety and comfort cases. Finally, an increase in the dynamic load coefficient (DLC) at a low speed of 30 km/h on the most deteriorated pavement and a high speed of 90 km/h on the middle-quality pavement demonstrated the demand for timely gravel pavement maintenance and the complicated prediction of a safe driving speed for drivers. The main relevant objectives of this study are the adaptation of a road roughness indicator to gravel pavement, including the evaluation of vehicle dynamic responses at different speeds and pavement deterioration states.},
DOI = {10.3390/app11167582}
}



@Article{rs13163272,
AUTHOR = {Ivošević, Bojana and Lugonja, Predrag and Brdar, Sanja and Radulović, Mirjana and Vujić, Ante and Valente, João},
TITLE = {UAV-Based Land Cover Classification for Hoverfly (Diptera: Syrphidae) Habitat Condition Assessment: A Case Study on Mt. Stara Planina (Serbia)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3272},
URL = {https://www.mdpi.com/2072-4292/13/16/3272},
ISSN = {2072-4292},
ABSTRACT = {Habitat degradation, mostly caused by human impact, is one of the key drivers of biodiversity loss. This is a global problem, causing a decline in the number of pollinators, such as hoverflies. In the process of digitalizing ecological studies in Serbia, remote-sensing-based land cover classification has become a key component for both current and future research. Object-based land cover classification, using machine learning algorithms of very high resolution (VHR) imagery acquired by an unmanned aerial vehicle (UAV) was carried out in three different study sites on Mt. Stara Planina, Eastern Serbia. UAV land cover classified maps with seven land cover classes (trees, shrubs, meadows, road, water, agricultural land, and forest patches) were studied. Moreover, three different classification algorithms—support vector machine (SVM), random forest (RF), and k-NN (k-nearest neighbors)—were compared. This study shows that the random forest classifier performs better with respect to the other classifiers in all three study sites, with overall accuracy values ranging from 0.87 to 0.96. The overall results are robust to changes in labeling ground truth subsets. The obtained UAV land cover classified maps were compared with the Map of the Natural Vegetation of Europe (EPNV) and used to quantify habitat degradation and assess hoverfly species richness. It was concluded that the percentage of habitat degradation is primarily caused by anthropogenic pressure, thus affecting the richness of hoverfly species in the study sites. In order to enable research reproducibility, the datasets used in this study are made available in a public repository.},
DOI = {10.3390/rs13163272}
}



@Article{rs13163276,
AUTHOR = {Ulhaq, Anwaar and Adams, Peter and Cox, Tarnya E. and Khan, Asim and Low, Tom and Paul, Manoranjan},
TITLE = {Automated Detection of Animals in Low-Resolution Airborne Thermal Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3276},
URL = {https://www.mdpi.com/2072-4292/13/16/3276},
ISSN = {2072-4292},
ABSTRACT = {Detecting animals to estimate abundance can be difficult, particularly when the habitat is dense or the target animals are fossorial. The recent surge in the use of thermal imagers in ecology and their use in animal detections can increase the accuracy of population estimates and improve the subsequent implementation of management programs. However, the use of thermal imagers results in many hours of captured flight videos which require manual review for confirmation of species detection and identification. Therefore, the perceived cost and efficiency trade-off often restricts the use of these systems. Additionally, for many off-the-shelf systems, the exported imagery can be quite low resolution (&lt;9 Hz), increasing the difficulty of using automated detections algorithms to streamline the review process. This paper presents an animal species detection system that utilises the cost-effectiveness of these lower resolution thermal imagers while harnessing the power of transfer learning and an enhanced small object detection algorithm. We have proposed a distant object detection algorithm named Distant-YOLO (D-YOLO) that utilises YOLO (You Only Look Once) and improves its training and structure for the automated detection of target objects in thermal imagery. We trained our system on thermal imaging data of rabbits, their active warrens, feral pigs, and kangaroos collected by thermal imaging researchers in New South Wales and Western Australia. This work will enhance the visual analysis of animal species while performing well on low, medium and high-resolution thermal imagery.},
DOI = {10.3390/rs13163276}
}



@Article{rs13163288,
AUTHOR = {Bai, Ling and Li, Yinguo and Cen, Ming and Hu, Fangchao},
TITLE = {3D Instance Segmentation and Object Detection Framework Based on the Fusion of Lidar Remote Sensing and Optical Image Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3288},
URL = {https://www.mdpi.com/2072-4292/13/16/3288},
ISSN = {2072-4292},
ABSTRACT = {Since single sensor and high-density point cloud data processing have certain direct processing limitations in urban traffic scenarios, this paper proposes a 3D instance segmentation and object detection framework for urban transportation scenes based on the fusion of Lidar remote sensing technology and optical image sensing technology. Firstly, multi-source and multi-mode data pre-fusion and alignment of Lidar and camera sensor data are effectively carried out, and then a unique and innovative network of stereo regional proposal selective search-driven DAGNN is constructed. Finally, using the multi-dimensional information interaction, three-dimensional point clouds with multi-features and unique concave-convex geometric characteristics are instance over-segmented and clustered by the hypervoxel storage in the remarkable octree and growing voxels. Finally, the positioning and semantic information of significant 3D object detection in this paper are visualized by multi-dimensional mapping of the boundary box. The experimental results validate the effectiveness of the proposed framework with excellent feedback for small objects, object stacking, and object occlusion. It can be a remediable or alternative plan to a single sensor and provide an essential theoretical and application basis for remote sensing, autonomous driving, environment modeling, autonomous navigation, and path planning under the V2X intelligent network space– ground integration in the future.},
DOI = {10.3390/rs13163288}
}



@Article{s21165620,
AUTHOR = {Shan, Donghui and Lei, Tian and Yin, Xiaohong and Luo, Qin and Gong, Lei},
TITLE = {Extracting Key Traffic Parameters from UAV Video with On-Board Vehicle Data Validation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5620},
URL = {https://www.mdpi.com/1424-8220/21/16/5620},
PubMedID = {34451061},
ISSN = {1424-8220},
ABSTRACT = {The advantages of UAV video in flexibility, traceability, easy-operation, and abundant information make it a popular and powerful aerial tool applied in traffic monitoring in recent years. This paper proposed a systematic approach to detect and track vehicles based on the YOLO v3 model and the deep SORT algorithm for further extracting key traffic parameters. A field experiment was implemented to provide data for model training and validation to ensure the accuracy of the proposed approach. In the experiment, 5400 frame images and 1192 speed points were collected from two test vehicles equipped with high-precision GNSS-RTK and onboard OBD after completion of seven experimental groups with a different height (150 m to 500 m) and operating speed (40 km/h to 90 km/h). The results indicate that the proposed approach exhibits strong robustness and reliability, due to the 90.88% accuracy of object detection and 98.9% precision of tracking vehicle. Moreover, the absolute and relative error of extracted speed falls within ±3 km/h and 2%, respectively. The overall accuracy of the extracted parameters reaches up to 98%.},
DOI = {10.3390/s21165620}
}



@Article{sym13081537,
AUTHOR = {Zhu, Zixiong and Xie, Nianhao and Zong, Kang and Chen, Lei},
TITLE = {Building a Connected Communication Network for UAV Clusters Using DE-MADDPG},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1537},
URL = {https://www.mdpi.com/2073-8994/13/8/1537},
ISSN = {2073-8994},
ABSTRACT = {Clusters of unmanned aerial vehicles (UAVs) are often used to perform complex tasks. In such clusters, the reliability of the communication network connecting the UAVs is an essential factor in their collective efficiency. Due to the complex wireless environment, however, communication malfunctions within the cluster are likely during the flight of UAVs. In such cases, it is important to control the cluster and rebuild the connected network. The asymmetry of the cluster topology also increases the complexity of the control mechanisms. The traditional control methods based on cluster consistency often rely on the motion information of the neighboring UAVs. The motion information, however, may become unavailable because of the interrupted communications. UAV control algorithms based on deep reinforcement learning have achieved outstanding results in many fields. Here, we propose a cluster control method based on the Decomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG) to rebuild a communication network for UAV clusters. The DE-MADDPG improves the framework of the traditional multi-agent deep deterministic policy gradient (MADDPG) algorithm by decomposing the reward function. We further introduce the reward reshaping function to facilitate the convergence of the algorithm in sparse reward environments. To address the instability of the state-space in the reinforcement learning framework, we also propose the notion of the virtual leader–follower model. Extensive simulations show that the success rate of the DE-MADDPG is higher than that of the MADDPG algorithm, confirming the effectiveness of the proposed method.},
DOI = {10.3390/sym13081537}
}



@Article{rs13163314,
AUTHOR = {Migas-Mazur, Robert and Kycko, Marlena and Zwijacz-Kozica, Tomasz and Zagajewski, Bogdan},
TITLE = {Assessment of Sentinel-2 Images, Support Vector Machines and Change Detection Algorithms for Bark Beetle Outbreaks Mapping in the Tatra Mountains},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {3314},
URL = {https://www.mdpi.com/2072-4292/13/16/3314},
ISSN = {2072-4292},
ABSTRACT = {Cambiophagous insects, fires and windthrow cause significant forest disturbances, generating ecological changes and economical losses. The bark beetle (Ips typographus L.), inhabiting coniferous forests and eliminating weakened trees, plays a key role in posing a threat to tree stands, which are dominated by Norway spruce (Picea abies) and covers a large part of mountain areas, as well as the lowlands of Northern, Central and Eastern Europe. Due to the dynamics of the phenomena taking place, the EU recommends constant monitoring of forests in terms of large-area disturbances and factors affecting tree stands’ susceptibility to destruction. The right tools for this are multispectral satellite images, which regularly and free of charge provide up-to-date information on changes in the environment. The aim of this study was to develop a method of identifying disturbances of spruce stands, including the identification of bark beetle outbreaks. Sentinel 2 images from 2015–2018 were used for this purpose; the reference data were high-resolution aerial images, satellite WorldView 2, as well as field verification data. Support Vector Machines (SVM) distinguished six classes: deciduous forests, coniferous forests, grasslands, rocks, snags (dieback of standing trees) and cuts/windthrow. Remote sensing vegetation indices, Multivariate Alteration Detection (MAD), Multivariate Alteration Detection/Maximum Autocorrelation Factor (MAD/MAF), iteratively re-weighted Multivariate Alteration Detection (iMAD) and trained SVM signatures from another year, stacked band rasters allowed us to identify: (1) no changes; (2) dieback of standing trees; (3) logging or falling down of trees. The overall accuracy of the SVM classification oscillated between 97–99%; it was observed that in 2015–2018, as a result of the windthrow and bark beetle outbreaks and the consequences of those natural disturbances (e.g., sanitary cuts), approximately 62.5 km2 of coniferous stands (29%) died in the studied area of the Tatra Mountains.},
DOI = {10.3390/rs13163314}
}



@Article{app11167716,
AUTHOR = {Maraveas, Chrysanthos and Loukatos, Dimitrios and Bartzanas, Thomas and Arvanitis, Konstantinos G.},
TITLE = {Applications of Artificial Intelligence in Fire Safety of Agricultural Structures},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {7716},
URL = {https://www.mdpi.com/2076-3417/11/16/7716},
ISSN = {2076-3417},
ABSTRACT = {Artificial intelligence applications in fire safety of agricultural structures have practical economic and technological benefits on commercial agriculture. The FAO estimates that wildfires result in at least USD 1 billion in agriculture-related losses due to the destruction of livestock pasture, destruction of agricultural buildings, premature death of farm animals, and general disruption of agricultural activities. Even though artificial neural networks (ANNs), genetic algorithms (GAs), probabilistic neural networks (PNNs), and adaptive neurofuzzy inference systems (ANFISs), among others, have proven useful in fire prevention, their application is limited in real farm environments. Most farms rely on traditional/non-technology-based methods of fire prevention. The case for AI in agricultural fire prevention is grounded on the accuracy and reliability of computer simulations in smoke movement analysis, risk assessment, and postfire analysis. In addition, such technologies can be coupled with next-generation fire-retardant materials such as intumescent coatings with a polymer binder, blowing agent, carbon donor, and acid donor. Future prospects for AI in agriculture transcend basic fire safety to encompass Society 5.0, energy systems in smart cities, UAV monitoring, Agriculture 4.0, and decentralized energy. However, critical challenges must be overcome, including the health and safety aspects, cost, and reliability. In brief, AI offers unlimited potential in the prevention of fire hazards in farms, but the existing body of knowledge is inadequate.},
DOI = {10.3390/app11167716}
}



@Article{s21165656,
AUTHOR = {Li, Xuanye and Li, Hongguang and Jiang, Yalong and Wang, Meng},
TITLE = {Lightweight Detection Network Based on Sub-Pixel Convolution and Objectness-Aware Structure for UAV Images},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5656},
URL = {https://www.mdpi.com/1424-8220/21/16/5656},
PubMedID = {34451098},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) can serve as an ideal mobile platform in various situations. Real-time object detection with on-board apparatus provides drones with increased flexibility as well as a higher intelligence level. In order to achieve good detection results in UAV images with complex ground scenes, small object size and high object density, most of the previous work introduced models with higher computational burdens, making deployment on mobile platforms more difficult.This paper puts forward a lightweight object detection framework. Besides being anchor-free, the framework is based on a lightweight backbone and a simultaneous up-sampling and detection module to form a more efficient detection architecture. Meanwhile, we add an objectness branch to assist the multi-class center point prediction, which notably improves the detection accuracy and only takes up very little computing resources. The results of the experiment indicate that the computational cost of this paper is 92.78% lower than the CenterNet with ResNet18 backbone, and the mAP is 2.8 points higher on the Visdrone-2018-VID dataset. A frame rate of about 220 FPS is achieved. Additionally, we perform ablation experiments to check on the validity of each part, and the method we propose is compared with other representative lightweight object detection methods on UAV image datasets.},
DOI = {10.3390/s21165656}
}



@Article{electronics10162038,
AUTHOR = {Tao, Zhen and Ren, Shiwei and Shi, Yueting and Wang, Xiaohua and Wang, Weijiang},
TITLE = {Accurate and Lightweight RailNet for Real-Time Rail Line Detection},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {2038},
URL = {https://www.mdpi.com/2079-9292/10/16/2038},
ISSN = {2079-9292},
ABSTRACT = {Railway transportation has always occupied an important position in daily life and social progress. In recent years, computer vision has made promising breakthroughs in intelligent transportation, providing new ideas for detecting rail lines. Yet the majority of rail line detection algorithms use traditional image processing to extract features, and their detection accuracy and instantaneity remain to be improved. This paper goes beyond the aforementioned limitations and proposes a rail line detection algorithm based on deep learning. First, an accurate and lightweight RailNet is designed, which takes full advantage of the powerful advanced semantic information extraction capabilities of deep convolutional neural networks to obtain high-level features of rail lines. The Segmentation Soul (SS) module is creatively added to the RailNet structure, which improves segmentation performance without any additional inference time. The Depth Wise Convolution (DWconv) is introduced in the RailNet to reduce the number of network parameters and eventually ensure real-time detection. Afterward, according to the binary segmentation maps of RailNet output, we propose the rail line fitting algorithm based on sliding window detection and apply the inverse perspective transformation. Thus the polynomial functions and curvature of the rail lines are calculated, and rail lines are identified in the original images. Furthermore, we collect a real-world rail lines dataset, named RAWRail. The proposed algorithm has been fully validated on the RAWRail dataset, running at 74 FPS, and the accuracy reaches 98.6%, which is superior to the current rail line detection algorithms and shows powerful potential in real applications.},
DOI = {10.3390/electronics10162038}
}



@Article{electronics10172046,
AUTHOR = {Zhao, Baojun and Tang, Wei and Pan, Yu and Han, Yuqi and Wang, Wenzheng},
TITLE = {Aircraft Type Recognition in Remote Sensing Images: Bilinear Discriminative Extreme Learning Machine Framework},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2046},
URL = {https://www.mdpi.com/2079-9292/10/17/2046},
ISSN = {2079-9292},
ABSTRACT = {Small inter-class and massive intra-class changes are important challenges in aircraft model recognition in the field of remote sensing. Although the aircraft model recognition algorithm based on the convolutional neural network (CNN) has excellent recognition performance, it is limited by sample sets and computing resources. To solve the above problems, we propose the bilinear discriminative extreme learning machine (ELM) network (BD-ELMNet), which integrates the advantages of the CNN, autoencoder (AE), and ELM. Specifically, the BD-ELMNet first executes the convolution and pooling operations to form a convolutional ELM (ELMConvNet) to extract shallow features. Furthermore, the manifold regularized ELM-AE (MRELM-AE), which can simultaneously consider the geometrical structure and discriminative information of aircraft data, is developed to extract discriminative features. The bilinear pooling model uses the feature association information for feature fusion to enhance the substantial distinction of features. Compared with the backpropagation (BP) optimization method, BD-ELMNet adopts a layer-by-layer training method without repeated adjustments to effectively learn discriminant features. Experiments involving the application of several methods, including the proposed method, to the MTARSI benchmark demonstrate that the proposed aircraft type recognition method outperforms the state-of-the-art methods.},
DOI = {10.3390/electronics10172046}
}



@Article{rs13173352,
AUTHOR = {Gara, Tawanda W. and Rahimzadeh-Bajgiran, Parinaz and Darvishzadeh, Roshanak},
TITLE = {Forest Leaf Mass per Area (LMA) through the Eye of Optical Remote Sensing: A Review and Future Outlook},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3352},
URL = {https://www.mdpi.com/2072-4292/13/17/3352},
ISSN = {2072-4292},
ABSTRACT = {Quantitative remote sensing of leaf traits offers an opportunity to track biodiversity changes from space. Augmenting field measurement of leaf traits with remote sensing provides a pathway for monitoring essential biodiversity variables (EBVs) over space and time. Detailed information on key leaf traits such as leaf mass per area (LMA) is critical for understanding ecosystem structure and functioning, and subsequently the provision of ecosystem services. Although studies on remote sensing of LMA and related constituents have been conducted for over three decades, a comprehensive review of remote sensing of LMA—a key driver of leaf and canopy reflectance—has been lacking. This paper reviews the current state and potential approaches, in addition to the challenges associated with LMA estimation/retrieval in forest ecosystems. The physiology and environmental factors that influence the spatial and temporal variation of LMA are presented. The scope of scaling LMA using remote sensing systems at various scales, i.e., near ground (in situ), airborne, and spaceborne platforms is reviewed and discussed. The review explores the advantages and disadvantages of LMA modelling techniques from these platforms. Finally, the research gaps and perspectives for future research are presented. Our review reveals that although progress has been made, scaling LMA to regional and global scales remains a challenge. In addition to seasonal tracking, three-dimensional modeling of LMA is still in its infancy. Over the past decade, the remote sensing scientific community has made efforts to separate LMA constituents in physical modelling at the leaf level. However, upscaling these leaf models to canopy level in forest ecosystems remains untested. We identified future opportunities involving the synergy of multiple sensors, and investigated the utility of hybrid models, particularly at the canopy and landscape levels.},
DOI = {10.3390/rs13173352}
}



@Article{electronics10172048,
AUTHOR = {Lin, Weison and Adetomi, Adewale and Arslan, Tughrul},
TITLE = {Low-Power Ultra-Small Edge AI Accelerators for Image Recognition with Convolution Neural Networks: Analysis and Future Directions},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2048},
URL = {https://www.mdpi.com/2079-9292/10/17/2048},
ISSN = {2079-9292},
ABSTRACT = {Edge AI accelerators have been emerging as a solution for near customers’ applications in areas such as unmanned aerial vehicles (UAVs), image recognition sensors, wearable devices, robotics, and remote sensing satellites. These applications require meeting performance targets and resilience constraints due to the limited device area and hostile environments for operation. Numerous research articles have proposed the edge AI accelerator for satisfying the applications, but not all include full specifications. Most of them tend to compare the architecture with other existing CPUs, GPUs, or other reference research, which implies that the performance exposé of the articles are not comprehensive. Thus, this work lists the essential specifications of prior art edge AI accelerators and the CGRA accelerators during the past few years to define and evaluate the low power ultra-small edge AI accelerators. The actual performance, implementation, and productized examples of edge AI accelerators are released in this paper. We introduce the evaluation results showing the edge AI accelerator design trend about key performance metrics to guide designers. Last but not least, we give out the prospect of developing edge AI’s existing and future directions and trends, which will involve other technologies for future challenging constraints.},
DOI = {10.3390/electronics10172048}
}



@Article{info12090343,
AUTHOR = {Hu, Chunyang and Li, Jingchen and Shi, Haobin and Ning, Bin and Gu, Qiong},
TITLE = {Decentralized Offloading Strategies Based on Reinforcement Learning for Multi-Access Edge Computing},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {343},
URL = {https://www.mdpi.com/2078-2489/12/9/343},
ISSN = {2078-2489},
ABSTRACT = {Using reinforcement learning technologies to learn offloading strategies for multi-access edge computing systems has been developed by researchers. However, large-scale systems are unsuitable for reinforcement learning, due to their huge state spaces and offloading behaviors. For this reason, this work introduces the centralized training and decentralized execution mechanism, designing a decentralized reinforcement learning model for multi-access edge computing systems. Considering a cloud server and several edge servers, we separate the training and execution in the reinforcement learning model. The execution happens in edge devices of the system, and edge servers need no communication. Conversely, the training process occurs at the cloud device, which causes a lower transmission latency. The developed method uses a deep deterministic policy gradient algorithm to optimize offloading strategies. The simulated experiment shows that our method can learn the offloading strategy for each edge device efficiently.},
DOI = {10.3390/info12090343}
}



@Article{rs13173364,
AUTHOR = {Huang, Xin and Dong, Xiaoya and Ma, Jing and Liu, Kuan and Ahmed, Shibbir and Lin, Jinlong and Qiu, Baijing},
TITLE = {The Improved A* Obstacle Avoidance Algorithm for the Plant Protection UAV with Millimeter Wave Radar and Monocular Camera Data Fusion},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3364},
URL = {https://www.mdpi.com/2072-4292/13/17/3364},
ISSN = {2072-4292},
ABSTRACT = {To enhance obstacle avoidance abilities of the plant protection UAV in unstructured farmland, this article improved the traditional A* algorithms through dynamic heuristic functions, search point optimization, and inflection point optimization based on millimeter wave radar and monocular camera data fusion. Obstacle information extraction experiments were carried out. The performance between the improved algorithm and traditional algorithm was compared. Additionally, obstacle avoidance experiments were also carried out. The results show that the maximum error in distance measurement of data fusion method was 8.2%. Additionally, the maximum error in obstacle width and height measurement were 27.3% and 18.5%, respectively. The improved algorithm is more useful in path planning, significantly reduces data processing time, search grid, and turning points. The algorithm at most increases path length by 2.0%, at least reduces data processing time by 68.4%, search grid by 74.9%, and turning points by 20.7%. The maximum trajectory offset error was proportional to the flight speed, with a maximum trajectory offset of 1.4 m. The distance between the UAV and obstacle was inversely proportional to flight speed, with a minimum distance of 1.6 m. This method can provide a new idea for obstacle avoidance of the plant protection UAV.},
DOI = {10.3390/rs13173364}
}



@Article{s21175713,
AUTHOR = {Zhao, Shenglin and Cai, Haoyuan and Li, Wenkuan and Liu, Yaqian and Liu, Chunxiu},
TITLE = {Hand Gesture Recognition on a Resource-Limited Interactive Wristband},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5713},
URL = {https://www.mdpi.com/1424-8220/21/17/5713},
PubMedID = {34502604},
ISSN = {1424-8220},
ABSTRACT = {Most of the reported hand gesture recognition algorithms require high computational resources, i.e., fast MCU frequency and significant memory, which are highly inapplicable to the cost-effectiveness of consumer electronics products. This paper proposes a hand gesture recognition algorithm running on an interactive wristband, with computational resource requirements as low as Flash &lt; 5 KB, RAM &lt; 1 KB. Firstly, we calculated the three-axis linear acceleration by fusing accelerometer and gyroscope data with a complementary filter. Then, by recording the order of acceleration vectors crossing axes in the world coordinate frame, we defined a new feature code named axis-crossing code. Finally, we set templates for eight hand gestures to recognize new samples. We compared this algorithm’s performance with the widely used dynamic time warping (DTW) algorithm and recurrent neural network (BiLSTM and GRU). The results show that the accuracies of the proposed algorithm and RNNs are higher than DTW and that the time cost of the proposed algorithm is much less than those of DTW and RNNs. The average recognition accuracy is 99.8% on the collected dataset and 97.1% in the actual user-independent case. In general, the proposed algorithm is suitable and competitive in consumer electronics. This work has been volume-produced and patent-granted.},
DOI = {10.3390/s21175713}
}



@Article{su13179568,
AUTHOR = {Ansari, Emaad and Akhtar, Mohammad Nishat and Abdullah, Mohamad Nazir and Othman, Wan Amir Fuad Wajdi and Bakar, Elmi Abu and Hawary, Ahmad Faizul and Alhady, Syed Sahal Nazli},
TITLE = {Image Processing of UAV Imagery for River Feature Recognition of Kerian River, Malaysia},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {9568},
URL = {https://www.mdpi.com/2071-1050/13/17/9568},
ISSN = {2071-1050},
ABSTRACT = {The impact of floods is the most severe among the natural calamities occurring in Malaysia. The knock of floods is consistent and annually forces thousands of Malaysians to relocate. The lack of information from the Ministry of Environment and Water, Malaysia is the foremost obstacle in upgrading the flood mapping. With the expeditious evolution of computer techniques, processing of satellite and unmanned aerial vehicle (UAV) images for river hydromorphological feature detection and flood management have gathered pace in the last two decades. Different image processing algorithms—structure from motion (SfM), multi-view stereo (MVS), gradient vector flow (GVF) snake algorithm, etc.—and artificial neural networks are implemented for the monitoring and classification of river features. This paper presents the application of the k-means algorithm along with image thresholding to quantify variation in river surface flow areas and vegetation growth along Kerian River, Malaysia. The river characteristic recognition directly or indirectly assists in studying river behavior and flood monitoring. Dice similarity coefficient and Jaccard index are numerated between thresholded images that are clustered using the k-means algorithm and manually segmented images. Based on quantitative evaluation, a dice similarity coefficient and Jaccard index of up to 97.86% and 94.36% were yielded for flow area and vegetation calculation. Thus, the present technique is functional in evaluating river characteristics with reduced errors. With minimum errors, the present technique can be utilized for quantifying agricultural areas and urban areas around the river basin.},
DOI = {10.3390/su13179568}
}



@Article{rs13173380,
AUTHOR = {Grau, Joan and Liang, Kang and Ogilvie, Jae and Arp, Paul and Li, Sheng and Robertson, Bonnie and Meng, Fan-Rui},
TITLE = {Using Unmanned Aerial Vehicle and LiDAR-Derived DEMs to Estimate Channels of Small Tributary Streams},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3380},
URL = {https://www.mdpi.com/2072-4292/13/17/3380},
ISSN = {2072-4292},
ABSTRACT = {Defining stream channels in a watershed is important for assessing freshwater habitat availability, complexity, and quality. However, mapping channels of small tributary streams becomes challenging due to frequent channel change and dense vegetation coverage. In this study, we used an Unmanned Aerial Vehicle (UAV) and photogrammetry method to obtain a 3D Digital Surface Model (DSM) to estimate the total in-stream channel and channel width within grazed riparian pastures. We used two methods to predict the stream channel boundary: the Slope Gradient (SG) and Vertical Slope Position (VSP). As a comparison, the same methods were also applied using low-resolution DEM, obtained with traditional photogrammetry (coarse resolution) and two more LiDAR-derived DEMs with different resolution. When using the SG method, the higher-resolution, UAV-derived DEM provided the best agreement with the field-validated area followed by the high-resolution LiDAR DEM, with Mean Squared Errors (MSE) of 1.81 m and 1.91 m, respectively. The LiDAR DEM collected at low resolution was able to predict the stream channel with a MSE of 3.33 m. Finally, the coarse DEM did not perform accurately and the MSE obtained was 26.76 m. On the other hand, when the VSP method was used we found that low-resolution LiDAR DEM performed the best followed by high-resolution LiDAR, with MSE values of 9.70 and 11.45 m, respectively. The MSE for the UAV-derived DEM was 15.12 m and for the coarse DEM was 20.78 m. We found that the UAV-derived DEM could be used to identify steep bank which could be used for mapping the hydrogeomorphology of lower order streams. Therefore, UAVs could be applied to efficiently map small stream channels in order to monitor the dynamic changes occurring in these ecosystems at a local scale. However, the VSP method should be used to map stream channels in small watersheds when high resolution DEM data is not available.},
DOI = {10.3390/rs13173380}
}



@Article{rs13173382,
AUTHOR = {Qader, Sarchil Hama and Dash, Jadu and Alegana, Victor A. and Khwarahm, Nabaz R. and Tatem, Andrew J. and Atkinson, Peter M.},
TITLE = {The Role of Earth Observation in Achieving Sustainable Agricultural Production in Arid and Semi-Arid Regions of the World},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3382},
URL = {https://www.mdpi.com/2072-4292/13/17/3382},
ISSN = {2072-4292},
ABSTRACT = {Crop production is a major source of food and livelihood for many people in arid and semi-arid (ASA) regions across the world. However, due to irregular climatic events, ASA regions are affected commonly by frequent droughts that can impact food production. In addition, ASA regions in the Middle East and Africa are often characterised by political instability, which can increase population vulnerability to hunger and ill health. Remote sensing (RS) provides a platform to improve the spatial prediction of crop production and food availability, with the potential to positively impact populations. This paper, firstly, describes some of the important characteristics of agriculture in ASA regions that require monitoring to improve their management. Secondly, it demonstrates how freely available RS data can support decision-making through a cost-effective monitoring system that complements traditional approaches for collecting agricultural data. Thirdly, it illustrates the challenges of employing freely available RS data for mapping and monitoring crop area, crop status and forecasting crop yield in these regions. Finally, existing approaches used in these applications are evaluated, and the challenges associated with their use and possible future improvements are discussed. We demonstrate that agricultural activities can be monitored effectively and both crop area and crop yield can be predicted in advance using RS data. We also discuss the future challenges associated with maintaining food security in ASA regions and explore some recent advances in RS that can be used to monitor cropland and forecast crop production and yield.},
DOI = {10.3390/rs13173382}
}



@Article{rs13173383,
AUTHOR = {Qin, Shengwu and Guo, Xu and Sun, Jingbo and Qiao, Shuangshuang and Zhang, Lingshuai and Yao, Jingyu and Cheng, Qiushi and Zhang, Yanqing},
TITLE = {Landslide Detection from Open Satellite Imagery Using Distant Domain Transfer Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3383},
URL = {https://www.mdpi.com/2072-4292/13/17/3383},
ISSN = {2072-4292},
ABSTRACT = {Using convolutional neural network (CNN) methods and satellite images for landslide identification and classification is a very efficient and popular task in geological hazard investigations. However, traditional CNNs have two disadvantages: (1) insufficient training images from the study area and (2) uneven distribution of the training set and validation set. In this paper, we introduced distant domain transfer learning (DDTL) methods for landslide detection and classification. We first introduce scene classification satellite imagery into the landslide detection task. In addition, in order to more effectively extract information from satellite images, we innovatively add an attention mechanism to DDTL (AM-DDTL). In this paper, the Longgang study area, a district in Shenzhen City, Guangdong Province, has only 177 samples as the landslide target domain. We examine the effect of DDTL by comparing three methods: the convolutional CNN, pretrained model and DDTL. We compare different attention mechanisms based on the DDTL. The experimental results show that the DDTL method has better detection performance than the normal CNN, and the AM-DDTL models achieve 94% classification accuracy, which is 7% higher than the conventional DDTL method. The requirements for the detection and classification of potential landslides at different disaster zones can be met by applying the AM-DDTL algorithm, which outperforms traditional CNN methods.},
DOI = {10.3390/rs13173383}
}



@Article{s21175745,
AUTHOR = {Fraga-Lamas, Paula and Lopes, Sérgio Ivan and Fernández-Caramés, Tiago M.},
TITLE = {Green IoT and Edge AI as Key Technological Enablers for a Sustainable Digital Transition towards a Smart Circular Economy: An Industry 5.0 Use Case},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5745},
URL = {https://www.mdpi.com/1424-8220/21/17/5745},
PubMedID = {34502637},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things (IoT) can help to pave the way to the circular economy and to a more sustainable world by enabling the digitalization of many operations and processes, such as water distribution, preventive maintenance, or smart manufacturing. Paradoxically, IoT technologies and paradigms such as edge computing, although they have a huge potential for the digital transition towards sustainability, they are not yet contributing to the sustainable development of the IoT sector itself. In fact, such a sector has a significant carbon footprint due to the use of scarce raw materials and its energy consumption in manufacturing, operating, and recycling processes. To tackle these issues, the Green IoT (G-IoT) paradigm has emerged as a research area to reduce such carbon footprint; however, its sustainable vision collides directly with the advent of Edge Artificial Intelligence (Edge AI), which imposes the consumption of additional energy. This article deals with this problem by exploring the different aspects that impact the design and development of Edge-AI G-IoT systems. Moreover, it presents a practical Industry 5.0 use case that illustrates the different concepts analyzed throughout the article. Specifically, the proposed scenario consists in an Industry 5.0 smart workshop that looks for improving operator safety and operation tracking. Such an application case makes use of a mist computing architecture composed of AI-enabled IoT nodes. After describing the application case, it is evaluated its energy consumption and it is analyzed the impact on the carbon footprint that it may have on different countries. Overall, this article provides guidelines that will help future developers to face the challenges that will arise when creating the next generation of Edge-AI G-IoT systems.},
DOI = {10.3390/s21175745}
}



@Article{drones5030083,
AUTHOR = {Miller, Alexander and Miller, Boris and Miller, Gregory},
TITLE = {Navigation of Underwater Drones and Integration of Acoustic Sensing with Onboard Inertial Navigation System},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {83},
URL = {https://www.mdpi.com/2504-446X/5/3/83},
ISSN = {2504-446X},
ABSTRACT = {The navigation of autonomous underwater vehicles is a major scientific and technological challenge. The principal difficulty is the opacity of the water media for usual types of radiation except for the acoustic waves. Thus, an acoustic transducer (array) composed of an acoustic sonar is the only tool for external measurements of the AUV attitude and position. Another difficulty is the inconstancy of the speed of propagation of acoustic waves, which depends on the temperature, salinity, and pressure. For this reason, only the data fusion of the acoustic measurements with data from other onboard inertial navigation system sensors can provide the necessary estimation quality and robustness. This review presents common approaches to underwater navigation and also one novel method of velocity measurement. The latter is an analog of the well-known Optical Flow method but based on a sequence of sonar array measurements.},
DOI = {10.3390/drones5030083}
}



@Article{sym13091579,
AUTHOR = {Wang, Xinheng and Gao, Xiaojin and Wang, Zuoxun and Ma, Chunrui and Song, Zengxu},
TITLE = {A Combined Model Based on EOBL-CSSA-LSSVM for Power Load Forecasting},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1579},
URL = {https://www.mdpi.com/2073-8994/13/9/1579},
ISSN = {2073-8994},
ABSTRACT = {Inaccurate electricity load forecasting can lead to the power sector gaining asymmetric information in the supply and demand relationship. This asymmetric information can lead to incorrect production or generation plans for the power sector. In order to improve the accuracy of load forecasting, a combined power load forecasting model based on machine learning algorithms, swarm intelligence optimization algorithms, and data pre-processing is proposed. Firstly, the original signal is pre-processed by the VMD–singular spectrum analysis data pre-processing method. Secondly, the noise-reduced signals are predicted using the Elman prediction model optimized by the sparrow search algorithm, the ELM prediction model optimized by the chaotic adaptive whale algorithm (CAWOA-ELM), and the LSSVM prediction model optimized by the chaotic sparrow search algorithm based on elite opposition-based learning (EOBL-CSSA-LSSVM) for electricity load data, respectively. Finally, the weighting coefficients of the three prediction models are calculated using the simulated annealing algorithm and weighted to obtain the prediction results. Comparative simulation experiments show that the VMD–singular spectrum analysis method and two improved intelligent optimization algorithms proposed in this paper can effectively improve the prediction accuracy. Additionally, the combined forecasting model proposed in this paper has extremely high forecasting accuracy, which can help the power sector to develop a reasonable production plan and power generation plans.},
DOI = {10.3390/sym13091579}
}



@Article{agronomy11091711,
AUTHOR = {Anderson, Nicholas Todd and Walsh, Kerry Brian and Koirala, Anand and Wang, Zhenglin and Amaral, Marcelo Henrique and Dickinson, Geoff Robert and Sinha, Priyakant and Robson, Andrew James},
TITLE = {Estimation of Fruit Load in Australian Mango Orchards Using Machine Vision},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1711},
URL = {https://www.mdpi.com/2073-4395/11/9/1711},
ISSN = {2073-4395},
ABSTRACT = {The performance of a multi-view machine vision method was documented at an orchard level, relative to packhouse count. High repeatability was achieved in night-time imaging, with an absolute percentage error of 2% or less. Canopy architecture impacted performance, with reasonable estimates achieved on hedge, single leader and conventional systems (3.4, 5.0, and 8.2 average percentage error, respectively) while fruit load of trellised orchards was over-estimated (at 25.2 average percentage error). Yield estimations were made for multiple orchards via: (i) human count of fruit load on ~5% of trees (FARM), (ii) human count of 18 trees randomly selected within three NDVI stratifications (CAL), (iii) multi-view counts (MV-Raw) and (iv) multi-view corrected for occluded fruit using manual counts of CAL trees (MV-CAL). Across the nine orchards for which results for all methods were available, the FARM, CAL, MV-Raw and MV-CAL methods achieved an average percentage error on packhouse counts of 26, 13, 11 and 17%, with SD of 11, 8, 11 and 9%, respectively, in the 2019–2020 season. The absolute percentage error of the MV-Raw estimates was 10% or less in 15 of the 20 orchards assessed. Greater error in load estimation occurred in the 2020–2021 season due to the time-spread of flowering. Use cases for the tree level data on fruit load was explored in context of fruit load density maps to inform early harvesting and to interpret crop damage, and tree frequency distributions based on fruit load per tree.},
DOI = {10.3390/agronomy11091711}
}



@Article{app11177960,
AUTHOR = {Son, Chang-Hwan},
TITLE = {Leaf Spot Attention Networks Based on Spot Feature Encoding for Leaf Disease Identification and Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {7960},
URL = {https://www.mdpi.com/2076-3417/11/17/7960},
ISSN = {2076-3417},
ABSTRACT = {This study proposes a new attention-enhanced YOLO model that incorporates a leaf spot attention mechanism based on regions-of-interest (ROI) feature extraction into the YOLO framework for leaf disease detection. Inspired by a previous study, which revealed that leaf spot attention based on the ROI-aware feature extraction can improve leaf disease recognition accuracy significantly and outperform state-of-the-art deep learning models, this study extends the leaf spot attention model to leaf disease detection. The primary idea is that spot areas indicating leaf diseases appear only in leaves, whereas the background area does not contain useful information regarding leaf diseases. To increase the discriminative power of the feature extractor that is required in the object detection framework, it is essential to extract informative and discriminative features from the spot and leaf areas. To realize this, a new ROI-aware feature extractor, that is, a spot feature extractor was designed. To divide the leaf image into spot, leaf, and background areas, the leaf segmentation module was first pretrained, and then spot feature encoding was applied to encode spot information. Next, the ROI-aware feature extractor was connected to an ROI-aware feature fusion layer to model the leaf spot attention mechanism, and to be joined with the YOLO detection subnetwork. The experimental results confirm that the proposed ROI-aware feature extractor can improve leaf disease detection by boosting the discriminative power of the spot features. In addition, the proposed attention-enhanced YOLO model outperforms conventional state-of-the-art object detection models.},
DOI = {10.3390/app11177960}
}



@Article{en14175370,
AUTHOR = {Specht, Mariusz and Stateczny, Andrzej and Specht, Cezary and Widźgowski, Szymon and Lewicka, Oktawia and Wiśniewska, Marta},
TITLE = {Concept of an Innovative Autonomous Unmanned System for Bathymetric Monitoring of Shallow Waterbodies (INNOBAT System)},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5370},
URL = {https://www.mdpi.com/1996-1073/14/17/5370},
ISSN = {1996-1073},
ABSTRACT = {Bathymetry is a subset of hydrography, aimed at measuring the depth of waterbodies and waterways. Measurements are taken inter alia to detect natural obstacles or other navigational obstacles that endanger the safety of navigation, to examine the navigability conditions, anchorages, waterways and other commercial waterbodies, and to determine the parameters of the safe depth of waterbodies in the vicinity of ports, etc. Therefore, it is necessary to produce precise and reliable seabed maps, so that any hazards that may occur, particularly in shallow waterbodies, can be prevented, including the high dynamics of hydromorphological changes. This publication is aimed at developing a concept of an innovative autonomous unmanned system for bathymetric monitoring of shallow waterbodies. A bathymetric and topographic system will use autonomous unmanned aerial and surface vehicles to study the seabed relief in the littoral zone (even at depths of less than 1 m), in line with the requirements set out for the most stringent International Hydrographic Organization (IHO) order—exclusive. Unlike other existing solutions, the INNOBAT system will enable the coverage of the entire surveyed area with measurements, which will allow a comprehensive assessment of the hydrographic and navigation situation in the waterbody to be conducted.},
DOI = {10.3390/en14175370}
}



@Article{rs13173437,
AUTHOR = {Qi, Yuan and Dong, Xuhua and Chen, Pengchao and Lee, Kyeong-Hwan and Lan, Yubin and Lu, Xiaoyang and Jia, Ruichang and Deng, Jizhong and Zhang, Yali},
TITLE = {Canopy Volume Extraction of Citrus reticulate Blanco cv. Shatangju Trees Using UAV Image-Based Point Cloud Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3437},
URL = {https://www.mdpi.com/2072-4292/13/17/3437},
ISSN = {2072-4292},
ABSTRACT = {Automatic acquisition of the canopy volume parameters of the Citrus reticulate Blanco cv. Shatangju tree is of great significance to precision management of the orchard. This research combined the point cloud deep learning algorithm with the volume calculation algorithm to segment the canopy of the Citrus reticulate Blanco cv. Shatangju trees. The 3D (Three-Dimensional) point cloud model of a Citrus reticulate Blanco cv. Shatangju orchard was generated using UAV tilt photogrammetry images. The segmentation effects of three deep learning models, PointNet++, MinkowskiNet and FPConv, on Shatangju trees and the ground were compared. The following three volume algorithms: convex hull by slices, voxel-based method and 3D convex hull were applied to calculate the volume of Shatangju trees. Model accuracy was evaluated using the coefficient of determination (R2) and Root Mean Square Error (RMSE). The results show that the overall accuracy of the MinkowskiNet model (94.57%) is higher than the other two models, which indicates the best segmentation effect. The 3D convex hull algorithm received the highest R2 (0.8215) and the lowest RMSE (0.3186 m3) for the canopy volume calculation, which best reflects the real volume of Citrus reticulate Blanco cv. Shatangju trees. The proposed method is capable of rapid and automatic acquisition for the canopy volume of Citrus reticulate Blanco cv. Shatangju trees.},
DOI = {10.3390/rs13173437}
}



@Article{drones5030086,
AUTHOR = {Grigusova, Paulina and Larsen, Annegret and Achilles, Sebastian and Klug, Alexander and Fischer, Robin and Kraus, Diana and Übernickel, Kirstin and Paulino, Leandro and Pliscoff, Patricio and Brandl, Roland and Farwig, Nina and Bendix, Jörg},
TITLE = {Area-Wide Prediction of Vertebrate and Invertebrate Hole Density and Depth across a Climate Gradient in Chile Based on UAV and Machine Learning},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {86},
URL = {https://www.mdpi.com/2504-446X/5/3/86},
ISSN = {2504-446X},
ABSTRACT = {Burrowing animals are important ecosystem engineers affecting soil properties, as their burrowing activity leads to the redistribution of nutrients and soil carbon sequestration. The magnitude of these effects depends on the spatial density and depth of such burrows, but a method to derive this type of spatially explicit data is still lacking. In this study, we test the potential of using consumer-oriented UAV RGB imagery to determine the density and depth of holes created by burrowing animals at four study sites along a climate gradient in Chile, by combining UAV data with empirical field plot observations and machine learning techniques. To enhance the limited spectral information in RGB imagery, we derived spatial layers representing vegetation type and height and used landscape textures and diversity to predict hole parameters. Across-site models for hole density generally performed better than those for depth, where the best-performing model was for the invertebrate hole density (R2 = 0.62). The best models at individual study sites were obtained for hole density in the arid climate zone (R2 = 0.75 and 0.68 for invertebrates and vertebrates, respectively). Hole depth models only showed good to fair performance. Regarding predictor importance, the models heavily relied on vegetation height, texture metrics, and diversity indices.},
DOI = {10.3390/drones5030086}
}



@Article{jimaging7090171,
AUTHOR = {Loddo, Andrea and Di Ruberto, Cecilia},
TITLE = {On the Efficacy of Handcrafted and Deep Features for Seed Image Classification},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {171},
URL = {https://www.mdpi.com/2313-433X/7/9/171},
PubMedID = {34564097},
ISSN = {2313-433X},
ABSTRACT = {Computer vision techniques have become important in agriculture and plant sciences due to their wide variety of applications. In particular, the analysis of seeds can provide meaningful information on their evolution, the history of agriculture, the domestication of plants, and knowledge of diets in ancient times. This work aims to propose an exhaustive comparison of several different types of features in the context of multiclass seed classification, leveraging two public plant seeds data sets to classify their families or species. In detail, we studied possible optimisations of five traditional machine learning classifiers trained with seven different categories of handcrafted features. We also fine-tuned several well-known convolutional neural networks (CNNs) and the recently proposed SeedNet to determine whether and to what extent using their deep features may be advantageous over handcrafted features. The experimental results demonstrated that CNN features are appropriate to the task and representative of the multiclass scenario. In particular, SeedNet achieved a mean F-measure of 96%, at least. Nevertheless, several cases showed satisfactory performance from the handcrafted features to be considered a valid alternative. In detail, we found that the Ensemble strategy combined with all the handcrafted features can achieve 90.93% of mean F-measure, at least, with a considerably lower amount of times. We consider the obtained results an excellent preliminary step towards realising an automatic seeds recognition and classification framework.},
DOI = {10.3390/jimaging7090171}
}



@Article{drones5030087,
AUTHOR = {Kotecha, Ketan and Garg, Deepak and Mishra, Balmukund and Narang, Pratik and Mishra, Vipul Kumar},
TITLE = {Background Invariant Faster Motion Modeling for Drone Action Recognition},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {87},
URL = {https://www.mdpi.com/2504-446X/5/3/87},
ISSN = {2504-446X},
ABSTRACT = {Visual data collected from drones has opened a new direction for surveillance applications and has recently attracted considerable attention among computer vision researchers. Due to the availability and increasing use of the drone for both public and private sectors, it is a critical futuristic technology to solve multiple surveillance problems in remote areas. One of the fundamental challenges in recognizing crowd monitoring videos’ human action is the precise modeling of an individual’s motion feature. Most state-of-the-art methods heavily rely on optical flow for motion modeling and representation, and motion modeling through optical flow is a time-consuming process. This article underlines this issue and provides a novel architecture that eliminates the dependency on optical flow. The proposed architecture uses two sub-modules, FMFM (faster motion feature modeling) and AAR (accurate action recognition), to accurately classify the aerial surveillance action. Another critical issue in aerial surveillance is a deficiency of the dataset. Out of few datasets proposed recently, most of them have multiple humans performing different actions in the same scene, such as a crowd monitoring video, and hence not suitable for directly applying to the training of action recognition models. Given this, we have proposed a novel dataset captured from top view aerial surveillance that has a good variety in terms of actors, daytime, and environment. The proposed architecture has shown the capability to be applied in different terrain as it removes the background before using the action recognition model. The proposed architecture is validated through the experiment with varying investigation levels and achieves a remarkable performance of 0.90 validation accuracy in aerial action recognition.},
DOI = {10.3390/drones5030087}
}



@Article{agriculture11090837,
AUTHOR = {Bhoi, Priya Brata and Wali, Veeresh S. and Swain, Deepak Kumar and Sharma, Kalpana and Bhoi, Akash Kumar and Bacco, Manlio and Barsocchi, Paolo},
TITLE = {Input Use Efficiency Management for Paddy Production Systems in India: A Machine Learning Approach},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {837},
URL = {https://www.mdpi.com/2077-0472/11/9/837},
ISSN = {2077-0472},
ABSTRACT = {This research illustrates the technical efficiency of the pan-India paddy cultivation status obtained through a stochastic frontier approach. The results suggest that the mean technical efficiency varies from 0.64 in Gujarat to 0.95 in Odisha. Inputs like human labor, mechanical labor, fertilizer, irrigation and insecticide were found to determine the yield in paddy cultivation across India (except for Chhattisgarh). Inefficiency in the paddy production in Punjab, Bihar, West Bengal, Andhra Pradesh, Tamil Nadu, Kerala, Assam, Gujarat and Odisha in 2016–2017 was caused by technical inefficiency due to poor input management, as suggested by the significant σ2U and σ2v values of the stochastic frontier model. In addition, most of the farm groups in the study operated in the high-efficiency group (80–90% technical efficiency). No specific pattern of input use can be visualized through descriptive measures to give any specific policy implication. Thus, machine learning algorithms based on the input parameters were tested on the data in order to predict the farmers’ efficiency class for individual states. The highest mean accuracy of 0.80 for the models of all of the states was achieved in random forest models. Among the various states of India, the best random forest prediction model based on accuracy was fitted to the input data of Bihar (0.91), followed by Uttar Pradesh (0.89), Andhra Pradesh (0.88), Assam (0.88) and West Bengal (0.86). Thus, the study provides a technique for the classification and prediction of a farmer’s efficiency group from the levels of input use in paddy cultivation for each state in the study. The study uses the DES input dataset to classify and predict the efficiency group of the farmer, as other machine learning models in agriculture have used mostly satellite, spectral imaging and soil property data to detect disease, weeds and crops.},
DOI = {10.3390/agriculture11090837}
}



@Article{s21175875,
AUTHOR = {Yang, Ming-Der and Hsu, Yu-Chun and Tseng, Wei-Cheng and Lu, Chian-Yu and Yang, Chin-Ying and Lai, Ming-Hsin and Wu, Dong-Hong},
TITLE = {Assessment of Grain Harvest Moisture Content Using Machine Learning on Smartphone Images for Optimal Harvest Timing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5875},
URL = {https://www.mdpi.com/1424-8220/21/17/5875},
PubMedID = {34502765},
ISSN = {1424-8220},
ABSTRACT = {Grain moisture content (GMC) is a key indicator of the appropriate harvest period of rice. Conventional testing is time-consuming and laborious, thus not to be implemented over vast areas and to enable the estimation of future changes for revealing optimal harvesting. Images of single panicles were shot with smartphones and corrected using a spectral–geometric correction board. In total, 86 panicle samples were obtained each time and then dried at 80 °C for 7 days to acquire the wet-basis GMC. In total, 517 valid samples were obtained, in which 80% was randomly used for training and 20% was used for testing to construct the image-based GMC assessment model. In total, 17 GMC surveys from a total of 201 samples were also performed from an area of 1 m2 representing on-site GMC, which enabled a multi-day GMC prediction. Eight color indices were selected using principal component analysis for building four machine learning models, including random forest, multilayer perceptron, support vector regression (SVR), and multivariate linear regression. The SVR model with a MAE of 1.23% was the most suitable for GMC of less than 40%. This study provides a real-time and cost-effective non-destructive GMC measurement using smartphones that enables on-farm prediction of harvest dates and facilitates the harvesting scheduling of agricultural machinery.},
DOI = {10.3390/s21175875}
}



@Article{electronics10172125,
AUTHOR = {Upadhyay, Jatin and Rawat, Abhishek and Deb, Dipankar},
TITLE = {Multiple Drone Navigation and Formation Using Selective Target Tracking-Based Computer Vision},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2125},
URL = {https://www.mdpi.com/2079-9292/10/17/2125},
ISSN = {2079-9292},
ABSTRACT = {Autonomous unmanned aerial vehicles work seamlessly within the GPS signal range, but their performance deteriorates in GPS-denied regions. This paper presents a unique collaborative computer vision-based approach for target tracking as per the image’s specific location of interest. The proposed method tracks any object without considering its properties like shape, color, size, or pattern. It is required to keep the target visible and line of sight during the tracking. The method gives freedom of selection to a user to track any target from the image and form a formation around it. We calculate the parameters like distance and angle from the image center to the object for the individual drones. Among all the drones, the one with a significant GPS signal strength or nearer to the target is chosen as the master drone to calculate the relative angle and distance between an object and other drones considering approximate Geo-location. Compared to actual measurements, the results of tests done on a quadrotor UAV frame achieve 99% location accuracy in a robust environment inside the exact GPS longitude and latitude block as GPS-only navigation methods. The individual drones communicate to the ground station through a telemetry link. The master drone calculates the parameters using data collected at ground stations. Various formation flying methods help escort other drones to meet the desired objective with a single high-resolution first-person view (FPV) camera. The proposed method is tested for Airborne Object Target Tracking (AOT) aerial vehicle model and achieves higher tracking accuracy.},
DOI = {10.3390/electronics10172125}
}



@Article{agriculture11090843,
AUTHOR = {Liu, Chengqi and Zhou, Han and Cao, Jing and Guo, Xuchao and Su, Jie and Wang, Longhe and Lu, Shuhan and Li, Lin},
TITLE = {Behavior Trajectory Tracking of Piglets Based on DLC-KPCA},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {843},
URL = {https://www.mdpi.com/2077-0472/11/9/843},
ISSN = {2077-0472},
ABSTRACT = {Tracking the behavior trajectories in pigs in group is becoming increasingly important for welfare feeding. A novel method was proposed in this study to accurately track individual trajectories of pigs in group and analyze their behavior characteristics. First, a multi-pig trajectory tracking model was established based on DeepLabCut (DLC) to realize the daily trajectory tracking of piglets. Second, a high-dimensional spatiotemporal feature model was established based on kernel principal component analysis (KPCA) to achieve nonlinear trajectory optimal clustering. At the same time, the abnormal trajectory correction model was established from five dimensions (semantic, space, angle, time, and velocity) to avoid trajectory loss and drift. Finally, the thermal map of the track distribution was established to analyze the four activity areas of the piggery (resting, drinking, excretion, and feeding areas). Experimental results show that the trajectory tracking accuracy of our method reaches 96.88%, the tracking speed is 350 fps, and the loss value is 0.002. Thus, the method based on DLC–KPCA can meet the requirements of identification of piggery area and tracking of piglets’ behavior. This study is helpful for automatic monitoring of animal behavior and provides data support for breeding.},
DOI = {10.3390/agriculture11090843}
}



@Article{s21175893,
AUTHOR = {Yu, Xin and Sun, Yushan and Wang, Xiangbin and Zhang, Guocheng},
TITLE = {End-to-End AUV Motion Planning Method Based on Soft Actor-Critic},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5893},
URL = {https://www.mdpi.com/1424-8220/21/17/5893},
PubMedID = {34502781},
ISSN = {1424-8220},
ABSTRACT = {This study aims to solve the problems of poor exploration ability, single strategy, and high training cost in autonomous underwater vehicle (AUV) motion planning tasks and to overcome certain difficulties, such as multiple constraints and a sparse reward environment. In this research, an end-to-end motion planning system based on deep reinforcement learning is proposed to solve the motion planning problem of an underactuated AUV. The system directly maps the state information of the AUV and the environment into the control instructions of the AUV. The system is based on the soft actor–critic (SAC) algorithm, which enhances the exploration ability and robustness to the AUV environment. We also use the method of generative adversarial imitation learning (GAIL) to assist its training to overcome the problem that learning a policy for the first time is difficult and time-consuming in reinforcement learning. A comprehensive external reward function is then designed to help the AUV smoothly reach the target point, and the distance and time are optimized as much as possible. Finally, the end-to-end motion planning algorithm proposed in this research is tested and compared on the basis of the Unity simulation platform. Results show that the algorithm has an optimal decision-making ability during navigation, a shorter route, less time consumption, and a smoother trajectory. Moreover, GAIL can speed up the AUV training speed and minimize the training time without affecting the planning effect of the SAC algorithm.},
DOI = {10.3390/s21175893}
}



@Article{rs13173482,
AUTHOR = {Roy Choudhury, Malini and Das, Sumanta and Christopher, Jack and Apan, Armando and Chapman, Scott and Menzies, Neal W. and Dang, Yash P.},
TITLE = {Improving Biomass and Grain Yield Prediction of Wheat Genotypes on Sodic Soil Using Integrated High-Resolution Multispectral, Hyperspectral, 3D Point Cloud, and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3482},
URL = {https://www.mdpi.com/2072-4292/13/17/3482},
ISSN = {2072-4292},
ABSTRACT = {Sodic soils adversely affect crop production over extensive areas of rain-fed cropping worldwide, with particularly large areas in Australia. Crop phenotyping may assist in identifying cultivars tolerant to soil sodicity. However, studies to identify the most appropriate traits and reliable tools to assist crop phenotyping on sodic soil are limited. Hence, this study evaluated the ability of multispectral, hyperspectral, 3D point cloud, and machine learning techniques to improve estimation of biomass and grain yield of wheat genotypes grown on a moderately sodic (MS) and highly sodic (HS) soil sites in northeastern Australia. While a number of studies have reported using different remote sensing approaches and crop traits to quantify crop growth, stress, and yield variation, studies are limited using the combination of these techniques including machine learning to improve estimation of genotypic biomass and yield, especially in constrained sodic soil environments. At close to flowering, unmanned aerial vehicle (UAV) and ground-based proximal sensing was used to obtain remote and/or proximal sensing data, while biomass yield and crop heights were also manually measured in the field. Grain yield was machine-harvested at maturity. UAV remote and/or proximal sensing-derived spectral vegetation indices (VIs), such as normalized difference vegetation index, optimized soil adjusted vegetation index, and enhanced vegetation index and crop height were closely corresponded to wheat genotypic biomass and grain yields. UAV multispectral VIs more closely associated with biomass and grain yields compared to proximal sensing data. The red-green-blue (RGB) 3D point cloud technique was effective in determining crop height, which was slightly better correlated with genotypic biomass and grain yield than ground-measured crop height data. These remote sensing-derived crop traits (VIs and crop height) and wheat biomass and grain yields were further simulated using machine learning algorithms (multitarget linear regression, support vector machine regression, Gaussian process regression, and artificial neural network) with different kernels to improve estimation of biomass and grain yield. The artificial neural network predicted biomass yield (R2 = 0.89; RMSE = 34.8 g/m2 for the MS and R2 = 0.82; RMSE = 26.4 g/m2 for the HS site) and grain yield (R2 = 0.88; RMSE = 11.8 g/m2 for the MS and R2 = 0.74; RMSE = 16.1 g/m2 for the HS site) with slightly less error than the others. Wheat genotypes Mitch, Corack, Mace, Trojan, Lancer, and Bremer were identified as more tolerant to sodic soil constraints than Emu Rock, Janz, Flanker, and Gladius. The study improves our ability to select appropriate traits and techniques in accurate estimation of wheat genotypic biomass and grain yields on sodic soils. This will also assist farmers in identifying cultivars tolerant to sodic soil constraints.},
DOI = {10.3390/rs13173482}
}



@Article{fire4030057,
AUTHOR = {Zhang, Zhen and Wang, Leilei and Xue, Naiting and Du, Zhiheng},
TITLE = {Spatiotemporal Analysis of Active Fires in the Arctic Region during 2001–2019 and a Fire Risk Assessment Model},
JOURNAL = {Fire},
VOLUME = {4},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {57},
URL = {https://www.mdpi.com/2571-6255/4/3/57},
ISSN = {2571-6255},
ABSTRACT = {The increasing frequency of active fires worldwide has caused significant impacts on terrestrial, aquatic, and atmospheric systems. Polar regions have received little attention due to their sparse populations, but active fires in the Arctic cause carbon losses from peatlands, which affects the global climate system. Therefore, it is necessary to focus on the spatiotemporal variations in active fires in the Arctic and to assess the fire risk. We used MODIS C6 data from 2001 to 2019 and VIIRS V1 data from 2012 to 2019 to analyse the spatiotemporal characteristics of active fires and establish a fire risk assessment model based on logistic regression. The trends in active fire frequency based on MODIS C6 and VIIRS V1 data are consistent. Throughout the Arctic, the fire frequency appears to be fluctuating and overall increasing. Fire occurrence has obvious seasonality, being concentrated in summer (June–August) and highest in July, when lightning is most frequent. The frequency of active fires is related to multiple factors, such as vegetation type, NDVI, elevation, slope, air temperature, precipitation, wind speed, and distances from roads and settlements. A risk assessment model was constructed based on logistic regression and found to be accurate. The results are helpful in understanding the risk of fires in the Arctic under climate change and provide a scientific basis for fire prediction and control and for reducing fire-related carbon emissions.},
DOI = {10.3390/fire4030057}
}



@Article{rs13173495,
AUTHOR = {Cota, Gizelle and Sagan, Vasit and Maimaitijiang, Maitiniyazi and Freeman, Karen},
TITLE = {Forest Conservation with Deep Learning: A Deeper Understanding of Human Geography around the Betampona Nature Reserve, Madagascar},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3495},
URL = {https://www.mdpi.com/2072-4292/13/17/3495},
ISSN = {2072-4292},
ABSTRACT = {Documenting the impacts of climate change and human activities on tropical rainforests is imperative for protecting tropical biodiversity and for better implementation of REDD+ and UN Sustainable Development Goals. Recent advances in very high-resolution satellite sensor systems (i.e., WorldView-3), computing power, and machine learning (ML) have provided improved mapping of fine-scale changes in the tropics. However, approaches so far focused on feature extraction or the extensive tuning of ML parameters, hindering the potential of ML in forest conservation mapping by not using textural information, which is found to be powerful for many applications. Additionally, the contribution of shortwave infrared (SWIR) bands in forest cover mapping is unknown. The objectives were to develop end-to-end mapping of the tropical forest using fully convolution neural networks (FCNNs) with WorldView-3 (WV-3) imagery and to evaluate human impact on the environment using the Betampona Nature Reserve (BNR) in Madagascar as the test site. FCNN (U-Net) using spatial/textural information was implemented and compared with feature-fed pixel-based methods including Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Network (DNN). Results show that the FCNN model outperformed other models with an accuracy of 90.9%, while SVM, RF, and DNN provided accuracies of 88.6%, 84.8%, and 86.6%, respectively. When SWIR bands were excluded from the input data, FCNN provided superior performance over other methods with a 1.87% decrease in accuracy, while the accuracies of other models—SVM, RF, and DNN—decreased by 5.42%, 3.18%, and 8.55%, respectively. Spatial–temporal analysis showed a 0.7% increase in Evergreen Forest within the BNR and a 32% increase in tree cover within residential areas likely due to forest regeneration and conservation efforts. Other effects of conservation efforts are also discussed.},
DOI = {10.3390/rs13173495}
}



@Article{horticulturae7090282,
AUTHOR = {Vrochidou, Eleni and Bazinas, Christos and Manios, Michail and Papakostas, George A. and Pachidis, Theodore P. and Kaburlasos, Vassilis G.},
TITLE = {Machine Vision for Ripeness Estimation in Viticulture Automation},
JOURNAL = {Horticulturae},
VOLUME = {7},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {282},
URL = {https://www.mdpi.com/2311-7524/7/9/282},
ISSN = {2311-7524},
ABSTRACT = {Ripeness estimation of fruits and vegetables is a key factor for the optimization of field management and the harvesting of the desired product quality. Typical ripeness estimation involves multiple manual samplings before harvest followed by chemical analyses. Machine vision has paved the way for agricultural automation by introducing quicker, cost-effective, and non-destructive methods. This work comprehensively surveys the most recent applications of machine vision techniques for ripeness estimation. Due to the broad area of machine vision applications in agriculture, this review is limited only to the most recent techniques related to grapes. The aim of this work is to provide an overview of the state-of-the-art algorithms by covering a wide range of applications. The potential of current machine vision techniques for specific viticulture applications is also analyzed. Problems, limitations of each technique, and future trends are discussed. Moreover, the integration of machine vision algorithms in grape harvesting robots for real-time in-field maturity assessment is additionally examined.},
DOI = {10.3390/horticulturae7090282}
}



@Article{rs13173503,
AUTHOR = {Rodriguez, Roberto and Perroy, Ryan L. and Leary, James and Jenkins, Daniel and Panoff, Max and Mandel, Travis and Perez, Patricia},
TITLE = {Comparing Interpretation of High-Resolution Aerial Imagery by Humans and Artificial Intelligence to Detect an Invasive Tree Species},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3503},
URL = {https://www.mdpi.com/2072-4292/13/17/3503},
ISSN = {2072-4292},
ABSTRACT = {Timely, accurate maps of invasive plant species are critical for making appropriate management decisions to eliminate emerging target populations or contain infestations. High-resolution aerial imagery is routinely used to map, monitor, and detect invasive plant populations. While conventional image interpretation involving human analysts is straightforward, it can require high demands for time and resources to produce useful intelligence. We compared the performance of human analysts with a custom Retinanet-based deep convolutional neural network (DNN) for detecting individual miconia (Miconia calvescens DC) plants, using high-resolution unmanned aerial system (UAS) imagery collected over lowland tropical forests in Hawai’i. Human analysts (n = 38) examined imagery at three linear scrolling speeds (100, 200 and 300 px/s), achieving miconia detection recalls of 74 ± 3%, 60 ± 3%, and 50 ± 3%, respectively. The DNN achieved 83 ± 3% recall and completed the image analysis in 1% of the time of the fastest scrolling speed tested. Human analysts could discriminate large miconia leaf clusters better than isolated individual leaves, while the DNN detection efficacy was independent of leaf cluster size. Optically, the contrast in the red and green color channels and all three (i.e., red, green, and blue) signal to clutter ratios (SCR) were significant factors for human detection, while only the red channel contrast, and the red and green SCRs were significant factors for the DNN. A linear cost analysis estimated the operational use of a DNN to be more cost effective than human photo interpretation when the cumulative search area exceeds a minimum area. For invasive species like miconia, which can stochastically spread propagules across thousands of ha, the DNN provides a more efficient option for detecting incipient, immature miconia across large expanses of forested canopy. Increasing operational capacity for large-scale surveillance with a DNN-based image analysis workflow can provide more rapid comprehension of invasive plant abundance and distribution in forested watersheds and may become strategically vital to containing these invasions.},
DOI = {10.3390/rs13173503}
}



@Article{en14175517,
AUTHOR = {Typiak, Rafał and Rykała, Łukasz and Typiak, Andrzej},
TITLE = {Configuring a UWB Based Location System for a UGV Operating in a Follow-Me Scenario},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5517},
URL = {https://www.mdpi.com/1996-1073/14/17/5517},
ISSN = {1996-1073},
ABSTRACT = {Unmanned Ground Vehicles (UGV) are devices capable of performing basic working movements without the operator being in their immediate working environment. Their capabilities include but are not limited to the perception of the environment with the use of sensors, determining the platform’s position, and planning and executing its movement. Ultra Wideband (UWB) is one of the wireless communication technologies which is increasingly used in location systems. This article presents the use of UWB technology in developing a guide localization system for a UGV (one of the stages of implementing a follow-me system). The article describes tests carried out on the developed testbed. Their aim was to determine the hardware configuration of the anchor arrangement characterized by the minimum number of lost data packets during operation. In order to determine the influence of the analysed variables on the output values, the method of global sensitivity analysis for neural networks was used.},
DOI = {10.3390/en14175517}
}



@Article{rs13173510,
AUTHOR = {Li, Siyuan and Jiao, Jiannan and Wang, Chi},
TITLE = {Research on Polarized Multi-Spectral System and Fusion Algorithm for Remote Sensing of Vegetation Status at Night},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3510},
URL = {https://www.mdpi.com/2072-4292/13/17/3510},
ISSN = {2072-4292},
ABSTRACT = {The monitoring of vegetation via remote sensing has been widely applied in various fields, such as crop diseases and pests, forest coverage and vegetation growth status, but such monitoring activities were mainly carried out in the daytime, resulting in limitations in sensing the status of vegetation at night. In this article, with the aim of monitoring the health status of outdoor plants at night by remote sensing, a polarized multispectral low-illumination-level imaging system (PMSIS) was established, and a fusion algorithm was proposed to detect vegetation by sensing the spectrum and polarization characteristics of the diffuse and specular reflection of vegetation. The normalized vegetation index (NDVI), degree of linear polarization (DoLP) and angle of polarization (AOP) are all calculated in the fusion algorithm to better detect the health status of plants in the night environment. Based on NDVI, DoLP and AOP fusion images (NDAI), a new index of night plant state detection (NPSDI) was proposed. A correlation analysis was made for the chlorophyll content (SPAD), nitrogen content (NC), NDVI and NPSDI to understand their capabilities to detect plants under stress. The scatter plot of NPSDI shows a good distinction between vegetation with different health levels, which can be seen from the high specificity and sensitivity values. It can be seen that NPSDI has a good correlation with NDVI (coefficient of determination R2 = 0.968), PSAD (R2 = 0.882) and NC (R2 = 0.916), which highlights the potential of NPSDI in the identification of plant health status. The results clearly show that the proposed fusion algorithm can enhance the contrast effect and the generated fusion image will carry richer vegetation information, thereby monitoring the health status of plants at night more effectively. This algorithm has a great potential in using remote sensing platform to monitor the health of vegetation and crops.},
DOI = {10.3390/rs13173510}
}



@Article{s21175948,
AUTHOR = {Fuentes, Sigfredo and Tongson, Eden and Unnithan, Ranjith R. and Gonzalez Viejo, Claudia},
TITLE = {Early Detection of Aphid Infestation and Insect-Plant Interaction Assessment in Wheat Using a Low-Cost Electronic Nose (E-Nose), Near-Infrared Spectroscopy and Machine Learning Modeling},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5948},
URL = {https://www.mdpi.com/1424-8220/21/17/5948},
PubMedID = {34502839},
ISSN = {1424-8220},
ABSTRACT = {Advances in early insect detection have been reported using digital technologies through camera systems, sensor networks, and remote sensing coupled with machine learning (ML) modeling. However, up to date, there is no cost-effective system to monitor insect presence accurately and insect-plant interactions. This paper presents results on the implementation of near-infrared spectroscopy (NIR) and a low-cost electronic nose (e-nose) coupled with machine learning. Several artificial neural network (ANN) models were developed based on classification to detect the level of infestation and regression to predict insect numbers for both e-nose and NIR inputs, and plant physiological response based on e-nose to predict photosynthesis rate (A), transpiration (E) and stomatal conductance (gs). Results showed high accuracy for classification models ranging within 96.5–99.3% for NIR and between 94.2–99.2% using e-nose data as inputs. For regression models, high correlation coefficients were obtained for physiological parameters (gs, E and A) using e-nose data from all samples as inputs (R = 0.86) and R = 0.94 considering only control plants (no insect presence). Finally, R = 0.97 for NIR and R = 0.99 for e-nose data as inputs were obtained to predict number of insects. Performances for all models developed showed no signs of overfitting. In this paper, a field-based system using unmanned aerial vehicles with the e-nose as payload was proposed and described for deployment of ML models to aid growers in pest management practices.},
DOI = {10.3390/s21175948}
}



@Article{electronics10172164,
AUTHOR = {Ammar, Anis and Fredj, Hana Ben and Souani, Chokri},
TITLE = {Accurate Realtime Motion Estimation Using Optical Flow on an Embedded System},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {2164},
URL = {https://www.mdpi.com/2079-9292/10/17/2164},
ISSN = {2079-9292},
ABSTRACT = {Motion estimation has become one of the most important techniques used in realtime computer vision application. There are several algorithms to estimate object motions. One of the most widespread techniques consists of calculating the apparent velocity field observed between two successive images of the same scene, known as the optical flow. However, the high accuracy of dense optical flow estimation is costly in run time. In this context, we designed an accurate motion estimation system based on the calculation of the optical flow of a moving object using the Lucas–Kanade algorithm. Our approach was applied on a local treatment region implemented into Raspberry Pi 4, with several improvements. The efficiency of our accurate realtime implementation was demonstrated by the experimental results, showing better performance than with the conventional calculation.},
DOI = {10.3390/electronics10172164}
}



@Article{atmos12091146,
AUTHOR = {Ma, Lei and Zhu, Xiaoxiang and Qiu, Chunping and Blaschke, Thomas and Li, Manchun},
TITLE = {Advances of Local Climate Zone Mapping and Its Practice Using Object-Based Image Analysis},
JOURNAL = {Atmosphere},
VOLUME = {12},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1146},
URL = {https://www.mdpi.com/2073-4433/12/9/1146},
ISSN = {2073-4433},
ABSTRACT = {In the context of climate change and urban heat islands, the concept of local climate zones (LCZ) aims for consistent and comparable mapping of urban surface structure and cover across cities. This study provides a timely survey of remote sensing-based applications of LCZ mapping considering the recent increase in publications. We analyze and evaluate several aspects that affect the performance of LCZ mapping, including mapping units/scale, transferability, sample dataset, low accuracy, and classification schemes. Since current LCZ analysis and mapping are based on per-pixel approaches, this study implements an object-based image analysis (OBIA) method and tests it for two cities in Germany using Sentinel 2 data. A comparison with a per-pixel method yields promising results. This study shall serve as a blueprint for future object-based remotely sensed LCZ mapping approaches.},
DOI = {10.3390/atmos12091146}
}



@Article{drones5030089,
AUTHOR = {Hoseini, Sayed Amir and Hassan, Jahan and Bokani, Ayub and Kanhere, Salil S.},
TITLE = {In Situ MIMO-WPT Recharging of UAVs Using Intelligent Flying Energy Sources},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {89},
URL = {https://www.mdpi.com/2504-446X/5/3/89},
ISSN = {2504-446X},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), used in civilian applications such as emergency medical deliveries, precision agriculture, wireless communication provisioning, etc., face the challenge of limited flight time due to their reliance on the on-board battery. Therefore, developing efficient mechanisms for in situ power transfer to recharge UAV batteries holds potential to extend their mission time. In this paper, we study the use of the far-field wireless power transfer (WPT) technique from specialized, transmitter UAVs (tUAVs) carrying Multiple Input Multiple Output (MIMO) antennas for transferring wireless power to receiver UAVs (rUAVs) in a mission. The tUAVs can fly and adjust their distance to the rUAVs to maximize energy transfer gain. The use of MIMO antennas further boosts the energy reception by narrowing the energy beam toward the rUAVs. The complexity of their dynamic operating environment increases with the growing number of tUAVs and rUAVs with varying levels of energy consumption and residual power. We propose an intelligent trajectory selection algorithm for the tUAVs based on a deep reinforcement learning model called Proximal Policy Optimization (PPO) to optimize the energy transfer gain. The simulation results demonstrate that the PPO-based system achieves about a tenfold increase in flight time for a set of realistic transmit power, distance, sub-band number and antenna numbers. Further, PPO outperforms the benchmark movement strategies of “Traveling Salesman Problem” and “Low Battery First” when used by the tUAVs.},
DOI = {10.3390/drones5030089}
}



@Article{s21175974,
AUTHOR = {Du, Chunyu and Fan, Wenyi and Ma, Ye and Jin, Hung-Il and Zhen, Zhen},
TITLE = {The Effect of Synergistic Approaches of Features and Ensemble Learning Algorithms on Aboveground Biomass Estimation of Natural Secondary Forests Based on ALS and Landsat 8},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {5974},
URL = {https://www.mdpi.com/1424-8220/21/17/5974},
PubMedID = {34502867},
ISSN = {1424-8220},
ABSTRACT = {Although the combination of Airborne Laser Scanning (ALS) data and optical imagery and machine learning algorithms were proved to improve the estimation of aboveground biomass (AGB), the synergistic approaches of different data and ensemble learning algorithms have not been fully investigated, especially for natural secondary forests (NSFs) with complex structures. This study aimed to explore the effects of the two factors on AGB estimation of NSFs based on ALS data and Landsat 8 imagery. The synergistic method of extracting novel features (i.e., COLI1 and COLI2) using optimal Landsat 8 features and the best-performing ALS feature (i.e., elevation mean) yielded higher accuracy of AGB estimation than either optical-only or ALS-only features. However, both of them failed to improve the accuracy compared to the simple combination of the untransformed features that generated them. The convolutional neural networks (CNN) model was much superior to other classic machine learning algorithms no matter of features. The stacked generalization (SG) algorithms, a kind of ensemble learning algorithms, greatly improved the accuracies compared to the corresponding base model, and the SG with the CNN meta-model performed best. This study provides technical support for a wall-to-wall AGB mapping of NSFs of northeastern China using efficient features and algorithms.},
DOI = {10.3390/s21175974}
}



@Article{rs13183563,
AUTHOR = {Koeva, Mila and Gasuku, Oscar and Lengoiboni, Monica and Asiama, Kwabena and Bennett, Rohan Mark and Potel, Jossam and Zevenbergen, Jaap},
TITLE = {Remote Sensing for Property Valuation: A Data Source Comparison in Support of Fair Land Taxation in Rwanda},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3563},
URL = {https://www.mdpi.com/2072-4292/13/18/3563},
ISSN = {2072-4292},
ABSTRACT = {Remotely sensed data is increasingly applied across many domains, including fit-for-purpose land administration (FFPLA), where the focus is on fast, affordable, and accurate property information collection. Property valuation, as one of the main functions of land administration systems, is influenced by locational, physical, legal, and economic factors. Despite the importance of property valuation to economic development, there are often no standardized rules or strict data requirements for property valuation for taxation in developing contexts, such as Rwanda. This study aims at assessing different remote sensing data in support of developing a new approach for property valuation for taxation in Rwanda; one that aligns with the FFPLA philosophy. Three different remote sensing technologies, (i) aerial images acquired with a digital camera, (ii) WorldView2 satellite images, and (iii) unmanned aerial vehicle (UAV) images obtained with a DJI Phantom 2 Vision Plus quadcopter, are compared and analyzed in terms of their fitness to fulfil the requirements for valuation for taxation purposes. Quantitative and qualitative methods are applied for the comparative analysis. Prior to the field visit, the fundamental concepts of property valuation for taxation and remote sensing were reviewed. In the field, reference data using high precision GNSS (Leica) was collected and used for quantitative assessment. Primary data was further collected via semi-structured interviews and focus group discussions. The results show that UAVs have the highest potential for collecting data to support property valuation for taxation. The main reasons are the prime need for accurate-enough and up-to-date information. The comparison of the different remote sensing techniques and the provided new approach can support land valuers and professionals in the field in bottom-up activities following the FFPLA principles and maintaining the temporal quality of data needed for fair taxation.},
DOI = {10.3390/rs13183563}
}



@Article{pr9091612,
AUTHOR = {Huang, Yan-Shu and Sheriff, M. Ziyan and Bachawala, Sunidhi and Gonzalez, Marcial and Nagy, Zoltan K. and Reklaitis, Gintaras V.},
TITLE = {Evaluation of a Combined MHE-NMPC Approach to Handle Plant-Model Mismatch in a Rotary Tablet Press},
JOURNAL = {Processes},
VOLUME = {9},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1612},
URL = {https://www.mdpi.com/2227-9717/9/9/1612},
ISSN = {2227-9717},
ABSTRACT = {The transition from batch to continuous processes in the pharmaceutical industry has been driven by the potential improvement in process controllability, product quality homogeneity, and reduction of material inventory. A quality-by-control (QbC) approach has been implemented in a variety of pharmaceutical product manufacturing modalities to increase product quality through a three-level hierarchical control structure. In the implementation of the QbC approach it is common practice to simplify control algorithms by utilizing linearized models with constant model parameters. Nonlinear model predictive control (NMPC) can effectively deliver control functionality for highly sensitive variations and nonlinear multiple-input-multiple-output (MIMO) systems, which is essential for the highly regulated pharmaceutical manufacturing industry. This work focuses on developing and implementing NMPC in continuous manufacturing of solid dosage forms. To mitigate control degradation caused by plant-model mismatch, careful monitoring and continuous improvement strategies are studied. When moving horizon estimation (MHE) is integrated with NMPC, historical data in the past time window together with real-time data from the sensor network enable state estimation and accurate tracking of the highly sensitive model parameters. The adaptive model used in the NMPC strategy can compensate for process uncertainties, further reducing plant-model mismatch effects. The nonlinear mechanistic model used in both MHE and NMPC can predict the essential but complex powder properties and provide physical interpretation of abnormal events. The adaptive NMPC implementation and its real-time control performance analysis and practical applicability are demonstrated through a series of illustrative examples that highlight the effectiveness of the proposed approach for different scenarios of plant-model mismatch, while also incorporating glidant effects.},
DOI = {10.3390/pr9091612}
}



@Article{agronomy11091809,
AUTHOR = {Roslim, Muhammad Huzaifah Mohd and Juraimi, Abdul Shukor and Che’Ya, Nik Norasma and Sulaiman, Nursyazyla and Manaf, Muhammad Noor Hazwan Abd and Ramli, Zaid and Motmainna, Mst.},
TITLE = {Using Remote Sensing and an Unmanned Aerial System for Weed Management in Agricultural Crops: A Review},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1809},
URL = {https://www.mdpi.com/2073-4395/11/9/1809},
ISSN = {2073-4395},
ABSTRACT = {Weeds are unwanted plants that can reduce crop yields by competing for water, nutrients, light, space, and carbon dioxide, which need to be controlled to meet future food production requirements. The integration of drones, artificial intelligence, and various sensors, which include hyperspectral, multi-spectral, and RGB (red-green-blue), ensure the possibility of a better outcome in managing weed problems. Most of the major or minor challenges caused by weed infestation can be faced by implementing remote sensing systems in various agricultural tasks. It is a multi-disciplinary science that includes spectroscopy, optics, computer, photography, satellite launching, electronics, communication, and several other fields. Future challenges, including food security, sustainability, supply and demand, climate change, and herbicide resistance, can also be overcome by those technologies based on machine learning approaches. This review provides an overview of the potential and practical use of unmanned aerial vehicle and remote sensing techniques in weed management practices and discusses how they overcome future challenges.},
DOI = {10.3390/agronomy11091809}
}



@Article{computers10090112,
AUTHOR = {Corso, Marcelo Picolotto and Perez, Fabio Luis and Stefenon, Stéfano Frizzo and Yow, Kin-Choong and García Ovejero, Raúl and Leithardt, Valderi Reis Quietinho},
TITLE = {Classification of Contaminated Insulators Using k-Nearest Neighbors Based on Computer Vision},
JOURNAL = {Computers},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {112},
URL = {https://www.mdpi.com/2073-431X/10/9/112},
ISSN = {2073-431X},
ABSTRACT = {Contamination on insulators may increase the surface conductivity of the insulator, and as a consequence, electrical discharges occur more frequently, which can lead to interruptions in a power supply. To maintain reliability in an electrical distribution power system, components that have lost their insulating properties must be replaced. Identifying the components that need maintenance is a difficult task as there are several levels of contamination that are hard to notice during inspections. To improve the quality of inspections, this paper proposes using k-nearest neighbors (k-NN) to classify the levels of insulator contamination based on images of insulators at various levels of contamination simulated in the laboratory. Computer vision features such as mean, variance, asymmetry, kurtosis, energy, and entropy are used for training the k-NN. To assess the robustness of the proposed approach, a statistical analysis and a comparative assessment with well-consolidated algorithms such as decision tree, ensemble subspace, and support vector machine models are presented. The k-NN showed up to 85.17% accuracy using the k-fold cross-validation method, with an average accuracy higher than 82% for the multi-classification of contamination of insulators, being superior to the compared models.},
DOI = {10.3390/computers10090112}
}



@Article{rs13183594,
AUTHOR = {Xia, Lang and Zhang, Ruirui and Chen, Liping and Li, Longlong and Yi, Tongchuan and Wen, Yao and Ding, Chenchen and Xie, Chunchun},
TITLE = {Evaluation of Deep Learning Segmentation Models for Detection of Pine Wilt Disease in Unmanned Aerial Vehicle Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3594},
URL = {https://www.mdpi.com/2072-4292/13/18/3594},
ISSN = {2072-4292},
ABSTRACT = {Pine wilt disease (PWD) is a serious threat to pine forests. Combining unmanned aerial vehicle (UAV) images and deep learning (DL) techniques to identify infected pines is the most efficient method to determine the potential spread of PWD over a large area. In particular, image segmentation using DL obtains the detailed shape and size of infected pines to assess the disease’s degree of damage. However, the performance of such segmentation models has not been thoroughly studied. We used a fixed-wing UAV to collect images from a pine forest in Laoshan, Qingdao, China, and conducted a ground survey to collect samples of infected pines and construct prior knowledge to interpret the images. Then, training and test sets were annotated on selected images, and we obtained 2352 samples of infected pines annotated over different backgrounds. Finally, high-performance DL models (e.g., fully convolutional networks for semantic segmentation, DeepLabv3+, and PSPNet) were trained and evaluated. The results demonstrated that focal loss provided a higher accuracy and a finer boundary than Dice loss, with the average intersection over union (IoU) for all models increasing from 0.656 to 0.701. From the evaluated models, DeepLLabv3+ achieved the highest IoU and an F1 score of 0.720 and 0.832, respectively. Also, an atrous spatial pyramid pooling module encoded multiscale context information, and the encoder–decoder architecture recovered location/spatial information, being the best architecture for segmenting trees infected by the PWD. Furthermore, segmentation accuracy did not improve as the depth of the backbone network increased, and neither ResNet34 nor ResNet50 was the appropriate backbone for most segmentation models.},
DOI = {10.3390/rs13183594}
}



@Article{rs13183600,
AUTHOR = {Solórzano, Jonathan V. and Mas, Jean François and Gao, Yan and Gallardo-Cruz, José Alberto},
TITLE = {Land Use Land Cover Classification with U-Net: Advantages of Combining Sentinel-1 and Sentinel-2 Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3600},
URL = {https://www.mdpi.com/2072-4292/13/18/3600},
ISSN = {2072-4292},
ABSTRACT = {The U-net is nowadays among the most popular deep learning algorithms for land use/land cover (LULC) mapping; nevertheless, it has rarely been used with synthetic aperture radar (SAR) and multispectral (MS) imagery. On the other hand, the discrimination between plantations and forests in LULC maps has been emphasized, especially for tropical areas, due to their differences in biodiversity and ecosystem services provision. In this study, we trained a U-net using different imagery inputs from Sentinel-1 and Sentinel-2 satellites, MS, SAR and a combination of both (MS + SAR); while a random forests algorithm (RF) with the MS + SAR input was also trained to evaluate the difference in algorithm selection. The classification system included ten classes, including old-growth and secondary forests, as well as old-growth and young plantations. The most accurate results were obtained with the MS + SAR U-net, where the highest overall accuracy (0.76) and average F1-score (0.58) were achieved. Although MS + SAR and MS U-nets gave similar results for almost all of the classes, for old-growth plantations and secondary forest, the addition of the SAR band caused an F1-score increment of 0.08–0.11 (0.62 vs. 0.54 and 0.45 vs. 0.34, respectively). Consecutively, in comparison with the MS + SAR RF, the MS + SAR U-net obtained higher F1-scores for almost all the classes. Our results show that using the U-net with a combined input of SAR and MS images enabled a higher F1-score and accuracy for a detailed LULC map, in comparison with other evaluated methods.},
DOI = {10.3390/rs13183600}
}



@Article{s21186046,
AUTHOR = {Zdziebko, Paweł and Holak, Krzysztof},
TITLE = {Synthetic Image Generation Using the Finite Element Method and Blender Graphics Program for Modeling of Vision-Based Measurement Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6046},
URL = {https://www.mdpi.com/1424-8220/21/18/6046},
PubMedID = {34577253},
ISSN = {1424-8220},
ABSTRACT = {Computer vision is a frequently used approach in static and dynamic measurements of various mechanical structures. Sometimes, however, conducting a large number of experiments is time-consuming and may require significant financial and human resources. On the contrary, the authors propose a simulation approach for performing experiments to synthetically generate vision data. Synthetic images of mechanical structures subjected to loads are generated in the following way. The finite element method is adopted to compute deformations of the studied structure, and next, the Blender graphics program is used to render images presenting that structure. As a result of the proposed approach, it is possible to obtain synthetic images that reliably reflect static and dynamic experiments. This paper presents the results of the application of the proposed approach in the analysis of a complex-shaped structure for which experimental validation was carried out. In addition, the second example of the process of 3D reconstruction of the examined structure (in a multicamera system) is provided. The results for the structure with damage (cantilever beam) are also presented. The obtained results allow concluding that the proposed approach reliably imitates the images captured during real experiments. In addition, the method can become a tool supporting the vision system configuration process before conducting final experimental research.},
DOI = {10.3390/s21186046}
}



@Article{app11188388,
AUTHOR = {Kim, Bubryur and Serfa Juan, Ronnie O. and Lee, Dong-Eun and Chen, Zengshun},
TITLE = {Importance of Image Enhancement and CDF for Fault Assessment of Photovoltaic Module Using IR Thermal Image},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8388},
URL = {https://www.mdpi.com/2076-3417/11/18/8388},
ISSN = {2076-3417},
ABSTRACT = {Infrared thermography is the science of measuring the infrared energy emitted by an object, translating it to apparent temperature variance, and displaying the result as an infrared image. Significantly, acquiring thermal images delivers distinctive levels of temperature differences in solar panels that correspond to their health status, which is beneficial for the early detection of defects. The proposed algorithm aims to analyze the thermal solar panel images. The acquired thermal solar panel images were segmented into solar cell sizes to provide more detailed information by region or cell area instead of the entire solar panel. This paper uses both the image histogram information and its corresponding cumulative distribution function (CDF), useful for image analysis. The acquired thermal solar panel images are enhanced using grayscale, histogram equalization, and adaptive histogram equalization to represent a domain that is easier to analyze. The experimental results reveal that the extraction results of thermal images provide better histogram and CDF features. Furthermore, the proposed scheme includes the convolutional neural network (CNN) for classifying the enhanced images, which shows that a 97% accuracy of classification was achieved. The proposed scheme could promote different thermal image applications—for example, non-physical visual recognition and fault detection analysis.},
DOI = {10.3390/app11188388}
}



@Article{app11188404,
AUTHOR = {Caballero, Rafael and Parra, Jesús and Trujillo, Miguel Ángel and Pérez-Grau, Francisco J. and Viguria, Antidio and Ollero, Aníbal},
TITLE = {Aerial Robotic Solution for Detailed Inspection of Viaducts},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8404},
URL = {https://www.mdpi.com/2076-3417/11/18/8404},
ISSN = {2076-3417},
ABSTRACT = {The inspection of public infrastructure, such as viaducts and bridges, is crucial for their proper maintenance given the heavy use of many of them. Current inspection techniques are very costly and manual, requiring highly qualified personnel and involving many risks. This article presents a novel solution for the detailed inspection of viaducts using aerial robotic platforms. The system provides a highly automated visual inspection platform that does not rely on GPS and could even fly underneath the infrastructure. Unlike commercially available solutions, our system automatically references the inspection to a global coordinate system usable throughout the lifespan of the infrastructure. In addition, the system includes another aerial platform with a robotic arm to make contact inspections of detected defects, thus providing information that cannot be obtained only with images. Both aerial robotic platforms feature flexibility in the choice of camera or contact measurement sensors as the situation requires. The system was validated by performing inspection flights on real viaducts.},
DOI = {10.3390/app11188404}
}



@Article{agronomy11091818,
AUTHOR = {Lytridis, Chris and Kaburlasos, Vassilis G. and Pachidis, Theodore and Manios, Michalis and Vrochidou, Eleni and Kalampokas, Theofanis and Chatzistamatis, Stamatis},
TITLE = {An Overview of Cooperative Robotics in Agriculture},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {1818},
URL = {https://www.mdpi.com/2073-4395/11/9/1818},
ISSN = {2073-4395},
ABSTRACT = {Agricultural robotics has been a popular subject in recent years from an academic as well as a commercial point of view. This is because agricultural robotics addresses critical issues such as seasonal shortages in manual labor, e.g., during harvest, as well as the increasing concern regarding environmentally friendly practices. On one hand, several individual agricultural robots have already been developed for specific tasks (e.g., for monitoring, spraying, harvesting, transport, etc.) with varying degrees of effectiveness. On the other hand, the use of cooperative teams of agricultural robots in farming tasks is not as widespread; yet, it is an emerging trend. This paper presents a comprehensive overview of the work carried out so far in the area of cooperative agricultural robotics and identifies the state-of-the-art. This paper also outlines challenges to be addressed in fully automating agricultural production; the latter is promising for sustaining an increasingly vast human population, especially in cases of pandemics such as the recent COVID-19 pandemic.},
DOI = {10.3390/agronomy11091818}
}



@Article{su131810164,
AUTHOR = {Maqsoom, Ahsen and Aslam, Bilal and Gul, Muhammad Ehtisham and Ullah, Fahim and Kouzani, Abbas Z. and Mahmud, M. A. Parvez and Nawaz, Adnan},
TITLE = {Using Multivariate Regression and ANN Models to Predict Properties of Concrete Cured under Hot Weather},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {10164},
URL = {https://www.mdpi.com/2071-1050/13/18/10164},
ISSN = {2071-1050},
ABSTRACT = {Concrete is an important construction material. Its characteristics depend on the environmental conditions, construction methods, and mix factors. Working with concrete is particularly tricky in a hot climate. This study predicts the properties of concrete in hot conditions using the case study of Rawalpindi, Pakistan. In this research, variable casting temperatures, design factors, and curing conditions are investigated for their effects on concrete characteristics. For this purpose, water–cement ratio (w/c), in-situ concrete temperature (T), and curing methods of the concrete are varied, and their effects on pulse velocity (PV), compressive strength (fc), depth of water penetration (WP), and split tensile strength (ft) were studied for up to 180 days. Quadratic regression and artificial neural network (ANN) models have been formulated to forecast the properties of concrete in the current study. The results show that T, curing period, and moist curing strongly influence fc, ft, and PV, while WP is adversely affected by T and moist curing. The ANN model shows better results compared to the quadratic regression model. Furthermore, a combined ANN model of fc, ft, and PV was also developed that displayed higher accuracy than the individual ANN models. These models can help construction site engineers select the appropriate concrete parameters when concreting under hot climates to produce durable and long-lasting concrete.},
DOI = {10.3390/su131810164}
}



@Article{drones5030095,
AUTHOR = {Singha, Subroto and Aydin, Burchan},
TITLE = {Automated Drone Detection Using YOLOv4},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {95},
URL = {https://www.mdpi.com/2504-446X/5/3/95},
ISSN = {2504-446X},
ABSTRACT = {Drones are increasing in popularity and are reaching the public faster than ever before. Consequently, the chances of a drone being misused are multiplying. Automated drone detection is necessary to prevent unauthorized and unwanted drone interventions. In this research, we designed an automated drone detection system using YOLOv4. The model was trained using drone and bird datasets. We then evaluated the trained YOLOv4 model on the testing dataset, using mean average precision (mAP), frames per second (FPS), precision, recall, and F1-score as evaluation parameters. We next collected our own two types of drone videos, performed drone detections, and calculated the FPS to identify the speed of detection at three altitudes. Our methodology showed better performance than what has been found in previous similar studies, achieving a mAP of 74.36%, precision of 0.95, recall of 0.68, and F1-score of 0.79. For video detection, we achieved an FPS of 20.5 on the DJI Phantom III and an FPS of 19.0 on the DJI Mavic Pro.},
DOI = {10.3390/drones5030095}
}



@Article{aerospace8090258,
AUTHOR = {Wada, Daichi and Araujo-Estrada, Sergio A. and Windsor, Shane},
TITLE = {Unmanned Aerial Vehicle Pitch Control under Delay Using Deep Reinforcement Learning with Continuous Action in Wind Tunnel Test},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {258},
URL = {https://www.mdpi.com/2226-4310/8/9/258},
ISSN = {2226-4310},
ABSTRACT = {Nonlinear flight controllers for fixed-wing unmanned aerial vehicles (UAVs) can potentially be developed using deep reinforcement learning. However, there is often a reality gap between the simulation models used to train these controllers and the real world. This study experimentally investigated the application of deep reinforcement learning to the pitch control of a UAV in wind tunnel tests, with a particular focus of investigating the effect of time delays on flight controller performance. Multiple neural networks were trained in simulation with different assumed time delays and then wind tunnel tested. The neural networks trained with shorter delays tended to be susceptible to delay in the real tests and produce fluctuating behaviour. The neural networks trained with longer delays behaved more conservatively and did not produce oscillations but suffered steady state errors under some conditions due to unmodeled frictional effects. These results highlight the importance of performing physical experiments to validate controller performance and how the training approach used with reinforcement learning needs to be robust to reality gaps between simulation and the real world.},
DOI = {10.3390/aerospace8090258}
}



@Article{rs13183649,
AUTHOR = {Morales, Giorgio and Sheppard, John W. and Logan, Riley D. and Shaw, Joseph A.},
TITLE = {Hyperspectral Dimensionality Reduction Based on Inter-Band Redundancy Analysis and Greedy Spectral Selection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3649},
URL = {https://www.mdpi.com/2072-4292/13/18/3649},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral imaging systems are becoming widely used due to their increasing accessibility and their ability to provide detailed spectral responses based on hundreds of spectral bands. However, the resulting hyperspectral images (HSIs) come at the cost of increased storage requirements, increased computational time to process, and highly redundant data. Thus, dimensionality reduction techniques are necessary to decrease the number of spectral bands while retaining the most useful information. Our contribution is two-fold: First, we propose a filter-based method called interband redundancy analysis (IBRA) based on a collinearity analysis between a band and its neighbors. This analysis helps to remove redundant bands and dramatically reduces the search space. Second, we apply a wrapper-based approach called greedy spectral selection (GSS) to the results of IBRA to select bands based on their information entropy values and train a compact convolutional neural network to evaluate the performance of the current selection. We also propose a feature extraction framework that consists of two main steps: first, it reduces the total number of bands using IBRA; then, it can use any feature extraction method to obtain the desired number of feature channels. We present classification results obtained from our methods and compare them to other dimensionality reduction methods on three hyperspectral image datasets. Additionally, we used the original hyperspectral data cube to simulate the process of using actual filters in a multispectral imager.},
DOI = {10.3390/rs13183649}
}



@Article{machines9090197,
AUTHOR = {Fourlas, George K. and Karras, George C.},
TITLE = {A Survey on Fault Diagnosis and Fault-Tolerant Control Methods for Unmanned Aerial Vehicles},
JOURNAL = {Machines},
VOLUME = {9},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {197},
URL = {https://www.mdpi.com/2075-1702/9/9/197},
ISSN = {2075-1702},
ABSTRACT = {The continuous evolution of modern technology has led to the creation of increasingly complex and advanced systems. This has been also reflected in the technology of Unmanned Aerial Vehicles (UAVs), where the growing demand for more reliable performance necessitates the development of sophisticated techniques that provide fault diagnosis and fault tolerance in a timely and accurate manner. Typically, a UAV consists of three types of subsystems: actuators, main structure and sensors. Therefore, a fault-monitoring system must be specifically designed to supervise and debug each of these subsystems, so that any faults can be addressed before they lead to disastrous consequences. In this survey article, we provide a detailed overview of recent advances and studies regarding fault diagnosis, Fault-Tolerant Control (FTC) and anomaly detection for UAVs. Concerning fault diagnosis, our interest is mainly focused on sensors and actuators, as these subsystems are mostly prone to faults, while their healthy operation usually ensures the smooth and reliable performance of the aerial vehicle.},
DOI = {10.3390/machines9090197}
}



@Article{rs13183651,
AUTHOR = {Wang, Weiqi and You, Xiong and Zhang, Xin and Chen, Lingyu and Zhang, Lantian and Liu, Xu},
TITLE = {LiDAR-Based SLAM under Semantic Constraints in Dynamic Environments},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3651},
URL = {https://www.mdpi.com/2072-4292/13/18/3651},
ISSN = {2072-4292},
ABSTRACT = {Facing the realistic demands of the application environment of robots, the application of simultaneous localisation and mapping (SLAM) has gradually moved from static environments to complex dynamic environments, while traditional SLAM methods usually result in pose estimation deviations caused by errors in data association due to the interference of dynamic elements in the environment. This problem is effectively solved in the present study by proposing a SLAM approach based on light detection and ranging (LiDAR) under semantic constraints in dynamic environments. Four main modules are used for the projection of point cloud data, semantic segmentation, dynamic element screening, and semantic map construction. A LiDAR point cloud semantic segmentation network SANet based on a spatial attention mechanism is proposed, which significantly improves the real-time performance and accuracy of point cloud semantic segmentation. A dynamic element selection algorithm is designed and used with prior knowledge to significantly reduce the pose estimation deviations caused by SLAM dynamic elements. The results of experiments conducted on the public datasets SemanticKITTI, KITTI, and SemanticPOSS show that the accuracy and robustness of the proposed approach are significantly improved.},
DOI = {10.3390/rs13183651}
}



@Article{rs13183659,
AUTHOR = {Tomíček, Jiří and Mišurec, Jan and Lukeš, Petr},
TITLE = {Prototyping a Generic Algorithm for Crop Parameter Retrieval across the Season Using Radiative Transfer Model Inversion and Sentinel-2 Satellite Observations},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3659},
URL = {https://www.mdpi.com/2072-4292/13/18/3659},
ISSN = {2072-4292},
ABSTRACT = {In this study, Sentinel-2 data were used for the retrieval of three key biophysical parameters of crops: leaf area index (LAI), leaf chlorophyll content (LCC), and leaf water content (LWC) for dominant crop types in the Czech Republic, including winter wheat (Triticum aestivum), spring barley (Hordeum vulgare), winter rapeseed (Brassica napus subsp. napus), alfalfa (Medicago sativa), sugar beet (Beta vulgaris), and corn (Zea mays subsp. Mays) in different stages of crop development. Artificial neural networks were applied in combination with an approach using look-up tables that is based on PROSAIL simulations to retrieve the biophysical properties tailored for each crop type. Crop-specific PROSAIL model optimization and validation were based upon a large dataset of in situ measurements collected in 2017 and 2018 in lowland of Central Bohemia region. For LCC and LAI, respectively, low relative root mean square error (rRMSE; 25%, 37%) was achieved. Additionally, a relatively strong correlation with in situ measurements (r = 0.80) was obtained for LAI. On the contrary, the results of the LWC parameter retrieval proved to be unsatisfactory. We have developed a generic tool for biophysical monitoring of agricultural crops based on the interpretation of Sentinel-2 satellite data by inversion of the radiation transfer model. The resulting crop condition maps can serve as precision agriculture inputs for selective fertilizer and irrigation application as well as for yield potential assessment.},
DOI = {10.3390/rs13183659}
}



@Article{buildings11090409,
AUTHOR = {Liu, Wenyao and Meng, Qingfeng and Li, Zhen and Hu, Xin},
TITLE = {Applications of Computer Vision in Monitoring the Unsafe Behavior of Construction Workers: Current Status and Challenges},
JOURNAL = {Buildings},
VOLUME = {11},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {409},
URL = {https://www.mdpi.com/2075-5309/11/9/409},
ISSN = {2075-5309},
ABSTRACT = {The unsafe behavior of construction workers is one of the main causes of safety accidents at construction sites. To reduce the incidence of construction accidents and improve the safety performance of construction projects, there is a need to identify risky factors by monitoring the behavior of construction workers. Computer vision (CV) technology, which is a powerful and automated tool used for extracting images and video information from construction sites, has been recognized and adopted as an effective construction site monitoring technology for the identification of risky factors resulting from the unsafe behavior of construction workers. In this article, we introduce the research background of this field and conduct a systematic statistical analysis of the relevant literature in this field through the bibliometric analysis method. Thereafter, we adopt a content-based analysis method to depict the historical explorations in the field. On this basis, the limitations and challenges in this field are identified, and future research directions are proposed. It is found that CV technology can effectively monitor the unsafe behaviors of construction workers. The research findings can enhance people’s understanding of construction safety management.},
DOI = {10.3390/buildings11090409}
}



@Article{rs13183663,
AUTHOR = {Liu, Shenzhou and Zeng, Wenzhi and Wu, Lifeng and Lei, Guoqing and Chen, Haorui and Gaiser, Thomas and Srivastava, Amit Kumar},
TITLE = {Simulating the Leaf Area Index of Rice from Multispectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3663},
URL = {https://www.mdpi.com/2072-4292/13/18/3663},
ISSN = {2072-4292},
ABSTRACT = {Accurate estimation of the leaf area index (LAI) is essential for crop growth simulations and agricultural management. This study conducted a field experiment with rice and measured the LAI in different rice growth periods. The multispectral bands (B) including red edge (RE, 730 nm ± 16 nm), near-infrared (NIR, 840 nm ± 26 nm), green (560 nm ± 16 nm), red (650 nm ± 16 nm), blue (450 nm ± 16 nm), and visible light (RGB) were also obtained by an unmanned aerial vehicle (UAV) with multispectral sensors (DJI-P4M, SZ DJI Technology Co., Ltd.). Based on the bands, five vegetation indexes (VI) including Green Normalized Difference Vegetation Index (GNDVI), Leaf Chlorophyll Index (LCI), Normalized Difference Red Edge Index (NDRE), Normalized Difference Vegetation Index (NDVI), and Optimization Soil-Adjusted Vegetation Index (OSAVI) were calculated. The semi-empirical model (SEM), the random forest model (RF), and the Extreme Gradient Boosting model (XGBoost) were used to estimate rice LAI based on multispectral bands, VIs, and their combinations, respectively. The results indicated that the GNDVI had the highest accuracy in the SEM (R2 = 0.78, RMSE = 0.77). For the single band, NIR had the highest accuracy in both RF (R2 = 0.73, RMSE = 0.98) and XGBoost (R2 = 0.77, RMSE = 0.88). Band combination of NIR + red improved the estimation accuracy in both RF (R2 = 0.87, RMSE = 0.65) and XGBoost (R2 = 0.88, RMSE = 0.63). NDRE and LCI were the first two single VIs for LAI estimation using both RF and XGBoost. However, putting more than one VI together could only increase the LAI estimation accuracy slightly. Meanwhile, the bands + VIs combinations could improve the accuracy in both RF and XGBoost. Our study recommended estimating rice LAI by a combination of red + NIR + OSAVI + NDVI + GNDVI + LCI + NDRE (2B + 5V) with XGBoost to obtain high accuracy and overcome the potential over-fitting issue (R2 = 0.91, RMSE = 0.54).},
DOI = {10.3390/rs13183663}
}



@Article{rs13183670,
AUTHOR = {Li, Wangbin and Sun, Kaimin and Du, Zhuotong and Hu, Xiuqing and Li, Wenzhuo and Wei, Jinjiang and Gao, Song},
TITLE = {PCNet: Cloud Detection in FY-3D True-Color Imagery Using Multi-Scale Pyramid Contextual Information},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3670},
URL = {https://www.mdpi.com/2072-4292/13/18/3670},
ISSN = {2072-4292},
ABSTRACT = {Cloud, one of the poor atmospheric conditions, significantly reduces the usability of optical remote-sensing data and hampers follow-up applications. Thus, the identification of cloud remains a priority for various remote-sensing activities, such as product retrieval, land-use/cover classification, object detection, and especially for change detection. However, the complexity of clouds themselves make it difficult to detect thin clouds and small isolated clouds. To accurately detect clouds in satellite imagery, we propose a novel neural network named the Pyramid Contextual Network (PCNet). Considering the limited applicability of a regular convolution kernel, we employed a Dilated Residual Block (DRB) to extend the receptive field of the network, which contains a dilated convolution and residual connection. To improve the detection ability for thin clouds, the proposed new model, pyramid contextual block (PCB), was used to generate global information at different scales. FengYun-3D MERSI-II remote-sensing images covering China with 14,165 × 24,659 pixels, acquired on 17 July 2019, are processed to conduct cloud-detection experiments. Experimental results show that the overall precision rates of the trained network reach 97.1% and the overall recall rates reach 93.2%, which performs better both in quantity and quality than U-Net, UNet++, UNet3+, PSPNet and DeepLabV3+.},
DOI = {10.3390/rs13183670}
}



@Article{s21186165,
AUTHOR = {Shaukat, Nabil and Moinuddin, Muhammad and Otero, Pablo},
TITLE = {Underwater Vehicle Positioning by Correntropy-Based Fuzzy Multi-Sensor Fusion},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {6165},
URL = {https://www.mdpi.com/1424-8220/21/18/6165},
PubMedID = {34577372},
ISSN = {1424-8220},
ABSTRACT = {The ability of the underwater vehicle to determine its precise position is vital to completing a mission successfully. Multi-sensor fusion methods for underwater vehicle positioning are commonly based on Kalman filtering, which requires the knowledge of process and measurement noise covariance. As the underwater conditions are continuously changing, incorrect process and measurement noise covariance affect the accuracy of position estimation and sometimes cause divergence. Furthermore, the underwater multi-path effect and nonlinearity cause outliers that have a significant impact on positional accuracy. These non-Gaussian outliers are difficult to handle with conventional Kalman-based methods and their fuzzy variants. To address these issues, this paper presents a new and improved adaptive multi-sensor fusion method by using information-theoretic, learning-based fuzzy rules for Kalman filter covariance adaptation in the presence of outliers. Two novel metrics are proposed by utilizing correntropy Gaussian and Versoria kernels for matching theoretical and actual covariance. Using correntropy-based metrics and fuzzy logic together makes the algorithm robust against outliers in nonlinear dynamic underwater conditions. The performance of the proposed sensor fusion technique is compared and evaluated using Monte-Carlo simulations, and substantial improvements in underwater position estimation are obtained.},
DOI = {10.3390/s21186165}
}



@Article{urbansci5030068,
AUTHOR = {Chaturvedi, Vineet and de Vries, Walter T.},
TITLE = {Machine Learning Algorithms for Urban Land Use Planning: A Review},
JOURNAL = {Urban Science},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {68},
URL = {https://www.mdpi.com/2413-8851/5/3/68},
ISSN = {2413-8851},
ABSTRACT = {Urbanization is persistent globally and has increasingly significant spatial and environmental consequences. It is especially challenging in developing countries due to the increasing pressure on the limited resources, and damage to the bio-physical environment. Traditional analytical methods of studying the urban land use dynamics associated with urbanization are static and tend to rely on top-down approaches, such as linear and mathematical modeling. These traditional approaches do not capture the nonlinear properties of land use change. New technologies, such as artificial intelligence (AI) and machine learning (ML) have made it possible to model and predict the nonlinear aspects of urban land dynamics. AI and ML are programmed to recognize patterns and carry out predictions, decision making and perform operations with speed and accuracy. Classification, analysis and modeling using earth observation-based data forms the basis for the geospatial support for land use planning. In the process of achieving higher accuracies in the classification of spatial data, ML algorithms are being developed and being improved to enhance the decision-making process. The purpose of the research is to bring out the various ML algorithms and statistical models that have been applied to study aspects of land use planning using earth observation-based data (EO). It intends to review their performance, functional requirements, interoperability requirements and for which research problems can they be applied best. The literature review revealed that random forest (RF), deep learning like convolutional neural network (CNN) and support vector machine (SVM) algorithms are best suited for classification and pattern analysis of earth observation-based data. GANs (generative adversarial networks) have been used to simulate urban patterns. Algorithms like cellular automata, spatial logistic regression and agent-based modeling have been used for studying urban growth, land use change and settlement pattern analysis. Most of the papers reviewed applied ML algorithms for classification of EO data and to study urban growth and land use change. It is observed that hybrid approaches have better performance in terms of accuracies, efficiency and computational cost.},
DOI = {10.3390/urbansci5030068}
}



@Article{ijgi10090606,
AUTHOR = {Daranagama, Samitha and Witayangkurn, Apichon},
TITLE = {Automatic Building Detection with Polygonizing and Attribute Extraction from High-Resolution Images},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {606},
URL = {https://www.mdpi.com/2220-9964/10/9/606},
ISSN = {2220-9964},
ABSTRACT = {Buildings can be introduced as a fundamental element for forming a city. Therefore, up-to-date building maps have become vital for many applications, including urban mapping and urban expansion analysis. With the development of deep learning, segmenting building footprints from high-resolution remote sensing imagery has become a subject of intense study. Here, a modified version of the U-Net architecture with a combination of pre- and post-processing techniques was developed to extract building footprints from high-resolution aerial imagery and unmanned aerial vehicle (UAV) imagery. Data pre-processing with the logarithmic correction image enhancing algorithm showed the most significant improvement in the building detection accuracy for aerial images; meanwhile, the CLAHE algorithm improved the most concerning UAV images. This study developed a post-processing technique using polygonizing and polygon smoothing called the Douglas–Peucker algorithm, which made the building output directly ready to use for different applications. The attribute information, land use data, and population count data were applied using two open datasets. In addition, the building area and perimeter of each building were calculated as geometric attributes.},
DOI = {10.3390/ijgi10090606}
}



@Article{rs13183671,
AUTHOR = {Wang, Andong and Zhou, Guoxu and Zhao, Qibin},
TITLE = {Guaranteed Robust Tensor Completion via ∗L-SVD with Applications to Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3671},
URL = {https://www.mdpi.com/2072-4292/13/18/3671},
ISSN = {2072-4292},
ABSTRACT = {This paper conducts a rigorous analysis for the problem of robust tensor completion, which aims at recovering an unknown three-way tensor from incomplete observations corrupted by gross sparse outliers and small dense noises simultaneously due to various reasons such as sensor dead pixels, communication loss, electromagnetic interferences, cloud shadows, etc. To estimate the underlying tensor, a new penalized least squares estimator is first formulated by exploiting the low rankness of the signal tensor within the framework of tensor ∗L-Singular Value Decomposition (∗L-SVD) and leveraging the sparse structure of the outlier tensor. Then, an algorithm based on the Alternating Direction Method of Multipliers (ADMM) is designed to compute the estimator in an efficient way. Statistically, the non-asymptotic upper bound on the estimation error is established and further proved to be optimal (up to a log factor) in a minimax sense. Simulation studies on synthetic data demonstrate that the proposed error bound can predict the scaling behavior of the estimation error with problem parameters (i.e., tubal rank of the underlying tensor, sparsity of the outliers, and the number of uncorrupted observations). Both the effectiveness and efficiency of the proposed algorithm are evaluated through experiments for robust completion on seven different types of remote sensing data.},
DOI = {10.3390/rs13183671}
}



@Article{app11188555,
AUTHOR = {Lee, Donghee and Park, Wooryong and Nam, Woochul},
TITLE = {Autonomous Landing of Micro Unmanned Aerial Vehicles with Landing-Assistive Platform and Robust Spherical Object Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8555},
URL = {https://www.mdpi.com/2076-3417/11/18/8555},
ISSN = {2076-3417},
ABSTRACT = {Autonomous unmanned aerial vehicle (UAV) landing can be useful in multiple applications. Precise landing is a difficult task because of the significant navigation errors of the global positioning system (GPS). To overcome these errors and to realize precise landing control, various sensors have been installed on UAVs. However, this approach can be challenging for micro UAVs (MAVs) because strong thrust forces are required to carry multiple sensors. In this study, a new autonomous MAV landing system is proposed, in which a landing platform actively assists vehicle landing. In addition to the vision system of the UAV, a camera was installed on the platform to precisely control the MAV near the landing area. The platform was also designed with various types of equipment to assist the MAV in searching, approaching, alignment, and landing. Furthermore, a novel algorithm was developed for robust spherical object detection under different illumination conditions. To validate the proposed landing system and detection algorithm, 80 flight experiments were conducted using a DJI TELLO drone, which successfully landed on the platform in every trial with a small landing position average error of 2.7 cm.},
DOI = {10.3390/app11188555}
}



@Article{rs13183681,
AUTHOR = {Krause, Johannes R. and Hinojosa-Corona, Alejandro and Gray, Andrew B. and Burke Watson, Elizabeth},
TITLE = {Emerging Sensor Platforms Allow for Seagrass Extent Mapping in a Turbid Estuary and from the Meadow to Ecosystem Scale},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3681},
URL = {https://www.mdpi.com/2072-4292/13/18/3681},
ISSN = {2072-4292},
ABSTRACT = {Seagrass meadows are globally important habitats, protecting shorelines, providing nursery areas for fish, and sequestering carbon. However, both anthropogenic and natural environmental stressors have led to a worldwide reduction seagrass habitats. For purposes of management and restoration, it is essential to produce accurate maps of seagrass meadows over a variety of spatial scales, resolutions, and at temporal frequencies ranging from months to years. Satellite remote sensing has been successfully employed to produce maps of seagrass in the past, but turbid waters and difficulty in obtaining low-tide scenes pose persistent challenges. This study builds on an increased availability of affordable high temporal frequency imaging platforms, using seasonal unmanned aerial vehicle (UAV) surveys of seagrass extent at the meadow scale, to inform machine learning classifications of satellite imagery of a 40 km2 bay. We find that object-based image analysis is suitable to detect seasonal trends in seagrass extent from UAV imagery and find that trends vary between individual meadows at our study site Bahía de San Quintín, Baja California, México, during our study period in 2019. We further suggest that compositing multiple satellite imagery classifications into a seagrass probability map allows for an estimation of seagrass extent in turbid waters and report that in 2019, seagrass covered 2324 ha of Bahía de San Quintín, indicating a recovery from losses reported for previous decades.},
DOI = {10.3390/rs13183681}
}



@Article{app11188571,
AUTHOR = {Espinoza-Fraire, Tadeo and Saenz, Armando and Salas, Francisco and Juarez, Raymundo and Giernacki, Wojciech},
TITLE = {Trajectory Tracking with Adaptive Robust Control for Quadrotor},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8571},
URL = {https://www.mdpi.com/2076-3417/11/18/8571},
ISSN = {2076-3417},
ABSTRACT = {This work proposes three robust mechanisms based on the MIT rule and the sliding-mode techniques. These robust mechanisms have to tune the gains of an adaptive Proportional-Derivative controller to steer a quadrotor in a predefined trajectory. The adaptive structure is a model reference adaptive control (MRAC). The robust mechanisms proposed to achieve the control objective (trajectory tracking) are MIT rule, MIT rule with sliding mode (MIT-SM), MIT rule with twisting (MIT-Twisting), and MIT rule with high order sliding mode (MIT-HOSM).},
DOI = {10.3390/app11188571}
}



@Article{photonics8090394,
AUTHOR = {Kwan, Chiman and Larkin, Jude},
TITLE = {Detection of Small Moving Objects in Long Range Infrared Videos from a Change Detection Perspective},
JOURNAL = {Photonics},
VOLUME = {8},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {394},
URL = {https://www.mdpi.com/2304-6732/8/9/394},
ISSN = {2304-6732},
ABSTRACT = {Detection of small moving objects in long range infrared (IR) videos is challenging due to background clutter, air turbulence, and small target size. In this paper, we present two unsupervised, modular, and flexible frameworks to detect small moving targets. The key idea was inspired by change detection (CD) algorithms where frame differences can help detect motions. Our frameworks consist of change detection, small target detection, and some post-processing algorithms such as image denoising and dilation. Extensive experiments using actual long range mid-wave infrared (MWIR) videos with target distances beyond 3500 m from the camera demonstrated that one approach, using Local Intensity Gradient (LIG) only once in the workflow, performed better than the other, which used LIG in two places, in a 3500 m video, but slightly worse in 4000 m and 5000 m videos. Moreover, we also investigated the use of synthetic bands for target detection and observed promising results for 4000 m and 5000 m videos. Finally, a comparative study with two conventional methods demonstrated that our proposed scheme has comparable performance.},
DOI = {10.3390/photonics8090394}
}



@Article{en14185861,
AUTHOR = {Benammar, Samir and Tee, Kong Fah},
TITLE = {Criticality Analysis and Maintenance of Solar Tower Power Plants by Integrating the Artificial Intelligence Approach},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {5861},
URL = {https://www.mdpi.com/1996-1073/14/18/5861},
ISSN = {1996-1073},
ABSTRACT = {Maintenance of solar tower power plants (STPP) is very important to ensure production continuity. However, random and non-optimal maintenance can increase the intervention cost. In this paper, a new procedure, based on the criticality analysis, was proposed to improve the maintenance of the STPP. This procedure is the combination of three methods, which are failure mode effects and criticality analysis (FMECA), Bayesian network and artificial intelligence. The FMECA is used to estimate the criticality index of the different elements of STPP. Moreover, corrections and improvements were introduced on the criticality index values based on the expert advice method. The modeling and the simulation of the FMECA estimations incorporating the expert advice method corrections were performed using the Bayesian network. The artificial neural network is used to predicate the criticality index of the STPP exploiting the database obtained from the Bayesian network simulations. The results showed a good agreement comparing predicted and actual criticality index values. In order to reduce the criticality index value of the critical elements of STPP, some maintenance recommendations were suggested.},
DOI = {10.3390/en14185861}
}



@Article{rs13183710,
AUTHOR = {Abdollahi, Abolfazl and Pradhan, Biswajeet and Shukla, Nagesh and Chakraborty, Subrata and Alamri, Abdullah},
TITLE = {Multi-Object Segmentation in Complex Urban Scenes from High-Resolution Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {3710},
URL = {https://www.mdpi.com/2072-4292/13/18/3710},
ISSN = {2072-4292},
ABSTRACT = {Terrestrial features extraction, such as roads and buildings from aerial images using an automatic system, has many usages in an extensive range of fields, including disaster management, change detection, land cover assessment, and urban planning. This task is commonly tough because of complex scenes, such as urban scenes, where buildings and road objects are surrounded by shadows, vehicles, trees, etc., which appear in heterogeneous forms with lower inter-class and higher intra-class contrasts. Moreover, such extraction is time-consuming and expensive to perform by human specialists manually. Deep convolutional models have displayed considerable performance for feature segmentation from remote sensing data in the recent years. However, for the large and continuous area of obstructions, most of these techniques still cannot detect road and building well. Hence, this work’s principal goal is to introduce two novel deep convolutional models based on UNet family for multi-object segmentation, such as roads and buildings from aerial imagery. We focused on buildings and road networks because these objects constitute a huge part of the urban areas. The presented models are called multi-level context gating UNet (MCG-UNet) and bi-directional ConvLSTM UNet model (BCL-UNet). The proposed methods have the same advantages as the UNet model, the mechanism of densely connected convolutions, bi-directional ConvLSTM, and squeeze and excitation module to produce the segmentation maps with a high resolution and maintain the boundary information even under complicated backgrounds. Additionally, we implemented a basic efficient loss function called boundary-aware loss (BAL) that allowed a network to concentrate on hard semantic segmentation regions, such as overlapping areas, small objects, sophisticated objects, and boundaries of objects, and produce high-quality segmentation maps. The presented networks were tested on the Massachusetts building and road datasets. The MCG-UNet improved the average F1 accuracy by 1.85%, and 1.19% and 6.67% and 5.11% compared with UNet and BCL-UNet for road and building extraction, respectively. Additionally, the presented MCG-UNet and BCL-UNet networks were compared with other state-of-the-art deep learning-based networks, and the results proved the superiority of the networks in multi-object segmentation tasks.},
DOI = {10.3390/rs13183710}
}



@Article{ai2030028,
AUTHOR = {Weber, Daniel and Gühmann, Clemens and Seel, Thomas},
TITLE = {RIANN—A Robust Neural Network Outperforms Attitude Estimation Filters},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {3},
PAGES = {444--463},
URL = {https://www.mdpi.com/2673-2688/2/3/28},
ISSN = {2673-2688},
ABSTRACT = {Inertial-sensor-based attitude estimation is a crucial technology in various applications, from human motion tracking to autonomous aerial and ground vehicles. Application scenarios differ in characteristics of the performed motion, presence of disturbances, and environmental conditions. Since state-of-the-art attitude estimators do not generalize well over these characteristics, their parameters must be tuned for the individual motion characteristics and circumstances. We propose RIANN, a ready-to-use, neural network-based, parameter-free, real-time-capable inertial attitude estimator, which generalizes well across different motion dynamics, environments, and sampling rates, without the need for application-specific adaptations. We gather six publicly available datasets of which we exploit two datasets for the method development and the training, and we use four datasets for evaluation of the trained estimator in three different test scenarios with varying practical relevance. Results show that RIANN outperforms state-of-the-art attitude estimation filters in the sense that it generalizes much better across a variety of motions and conditions in different applications, with different sensor hardware and different sampling frequencies. This is true even if the filters are tuned on each individual test dataset, whereas RIANN was trained on completely separate data and has never seen any of these test datasets. RIANN can be applied directly without adaptations or training and is therefore expected to enable plug-and-play solutions in numerous applications, especially when accuracy is crucial but no ground-truth data is available for tuning or when motion and disturbance characteristics are uncertain. We made RIANN publicly available.},
DOI = {10.3390/ai2030028}
}



@Article{drones5030099,
AUTHOR = {Richardson, Galen and Leblanc, Sylvain G. and Lovitt, Julie and Rajaratnam, Krishan and Chen, Wenjun},
TITLE = {Leveraging AI to Estimate Caribou Lichen in UAV Orthomosaics from Ground Photo Datasets},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {99},
URL = {https://www.mdpi.com/2504-446X/5/3/99},
ISSN = {2504-446X},
ABSTRACT = {Relating ground photographs to UAV orthomosaics is a key linkage required for accurate multi-scaled lichen mapping. Conventional methods of multi-scaled lichen mapping, such as random forest models and convolutional neural networks, heavily rely on pixel DN values for classification. However, the limited spectral range of ground photos requires additional characteristics to differentiate lichen from spectrally similar objects, such as bright logs. By applying a neural network to tiles of a UAV orthomosaics, additional characteristics, such as surface texture and spatial patterns, can be used for inferences. Our methodology used a neural network (UAV LiCNN) trained on ground photo mosaics to predict lichen in UAV orthomosaic tiles. The UAV LiCNN achieved mean user and producer accuracies of 85.84% and 92.93%, respectively, in the high lichen class across eight different orthomosaics. We compared the known lichen percentages found in 77 vegetation microplots with the predicted lichen percentage calculated from the UAV LiCNN, resulting in a R2 relationship of 0.6910. This research shows that AI models trained on ground photographs effectively classify lichen in UAV orthomosaics. Limiting factors include the misclassification of spectrally similar objects to lichen in the RGB bands and dark shadows cast by vegetation.},
DOI = {10.3390/drones5030099}
}



@Article{aerospace8090267,
AUTHOR = {Kim, Eric J. and Perez, Ruben E.},
TITLE = {Neuroevolutionary Control for Autonomous Soaring},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {267},
URL = {https://www.mdpi.com/2226-4310/8/9/267},
ISSN = {2226-4310},
ABSTRACT = {The energy efficiency and flight endurance of small unmanned aerial vehicles (SUAVs) can be improved through the implementation of autonomous soaring strategies. Biologically inspired flight techniques such as dynamic and thermal soaring offer significant energy savings through the exploitation of naturally occurring wind phenomena for thrustless flight. Recent interest in the application of artificial intelligence algorithms for autonomous soaring has been motivated by the pursuit of instilling generalized behavior in control systems, centered around the use of neural networks. However, the topology of such networks is usually predetermined, restricting the search space of potential solutions, while often resulting in complex neural networks that can pose implementation challenges for the limited hardware onboard small-scale autonomous vehicles. In exploring a novel method of generating neurocontrollers, this paper presents a neural network-based soaring strategy to extend flight times and advance the potential operational capability of SUAVs. In this study, the Neuroevolution of Augmenting Topologies (NEAT) algorithm is used to train efficient and effective neurocontrollers that can control a simulated aircraft along sustained dynamic and thermal soaring trajectories. The proposed approach evolves interpretable neural networks in a way that preserves simplicity while maximizing performance without requiring extensive training datasets. As a result, the combined trajectory planning and aircraft control strategy is suitable for real-time implementation on SUAV platforms.},
DOI = {10.3390/aerospace8090267}
}



@Article{app11188694,
AUTHOR = {Memon, Mehak Maqbool and Hashmani, Manzoor Ahmed and Junejo, Aisha Zahid and Rizvi, Syed Sajjad and Arain, Adnan Ashraf},
TITLE = {A Novel Luminance-Based Algorithm for Classification of Semi-Dark Images},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8694},
URL = {https://www.mdpi.com/2076-3417/11/18/8694},
ISSN = {2076-3417},
ABSTRACT = {Image classification of a visual scene based on visibility is significant due to the rise in readily available automated solutions. Currently, there are only two known spectrums of image visibility i.e., dark, and bright. However, normal environments include semi-dark scenarios. Hence, visual extremes that will lead to the accurate extraction of image features should be duly discarded. Fundamentally speaking there are two broad methods to perform visual scene-based image classification, i.e., machine learning (ML) methods and computer vision methods. In ML, the issues of insufficient data, sophisticated hardware and inadequate image classifier training time remain significant problems to be handled. These techniques fail to classify the visual scene-based images with high accuracy. The other alternative is computer vision (CV) methods, which also have major issues. CV methods do provide some basic procedures which may assist in such classification but, to the best of our knowledge, no CV algorithm exists to perform such classification, i.e., these do not account for semi-dark images in the first place. Moreover, these methods do not provide a well-defined protocol to calculate images’ content visibility and thereby classify images. One of the key algorithms for calculation of images’ content visibility is backed by the HSL (hue, saturation, lightness) color model. The HSL color model allows the visibility calculation of a scene by calculating the lightness/luminance of a single pixel. Recognizing the high potential of the HSL color model, we propose a novel framework relying on the simple approach of the statistical manipulation of an entire image’s pixel intensities, represented by HSL color model. The proposed algorithm, namely, Relative Perceived Luminance Classification (RPLC) uses the HSL (hue, saturation, lightness) color model to correctly identify the luminosity values of the entire image. Our findings prove that the proposed method yields high classification accuracy (over 78%) with a small error rate. We show that the computational complexity of RPLC is much less than that of the state-of-the-art ML algorithms.},
DOI = {10.3390/app11188694}
}



