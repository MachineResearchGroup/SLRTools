@inproceedings{10.1145/3451471.3451481,
author = {Joshi, Pramath and Liyange Don, Mindula and Guildford Chikodi, Sidney and Bitanga, Austin and Chowdhury, Mourita and Mu, Yixin and Farrell, Vivienne and Burton Watson, Richard and John Ryan, Peter},
title = {Web-Enabled Smart City Applications for Urban Transport and Parking Operations},
year = {2021},
isbn = {9781450388955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3451471.3451481},
doi = {10.1145/3451471.3451481},
abstract = {This paper describes the development of a prototype website for traffic, parking and transport in a smart city. Machine Learning (ML) tools are applied to open datasets from the City of Melbourne, Australia to develop a set of Application Programming Interfaces (APIs) that provide useful information for the city's managers and citizens. The APIs accessed from this website enable users to query the ML models and obtain answers to questions such as: which parts of the city have the greatest pedestrian traffic, or the availability and cost of parking spots. The freeware tool RStudio was used for Big Data analytics while Machine Learning with Plumber was used to wrap the R code into APIs and Swagger to specify and document them. Postman and Swagger were used for testing while Docker was employed to package the APIs into standard containers for cloud deployment. The prototype website was developed using Wix and deployed on the Nectar cloud. The resulting website provides predictive models for COM traffic, parking, and transport and demonstrates the application of online smart city services for city planners and managers.},
booktitle = {2021 The 4th International Conference on Software Engineering and Information Management},
pages = {56–62},
numpages = {7},
keywords = {Plumber, Machine Learning, Nectar, Docker, Wix, Big Data Analytics, Swagger, Smart Cities, RStudio, APIs},
location = {Yokohama, Japan},
series = {ICSIM 2021}
}

@inproceedings{10.1145/3469259.3470488,
author = {Bulusu, Nirupama and Aryafar, Ehsan and Liu, Feng},
title = {Towards Adaptive, Self-Configuring Networked Unmanned Aerial Vehicles},
year = {2021},
isbn = {9781450385992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3469259.3470488},
doi = {10.1145/3469259.3470488},
abstract = {Networked drones have the potential to transform various applications domains; yet their adoption particularly in indoor and forest environments has been stymied by the lack of accurate maps and autonomous navigation abilities in the absence of GPS, the lack of highly reliable, energy-efficient wireless communications, and the challenges of visually inferring and understanding an environment with resource-limited individual drones. We advocate a novel vision for the research community in the development of distributed, localized algorithms that enable the networked drones to dynamically coordinate to perform adaptive beam forming to achieve high capacity directional aerial communications, and collaborative machine learning to simultaneously localize, map and visually infer the challenging environment, even when individual drones are resource-limited in terms of computation and communication due to payload restrictions.},
booktitle = {Proceedings of the 7th Workshop on Micro Aerial Vehicle Networks, Systems, and Applications},
pages = {25–30},
numpages = {6},
keywords = {federated learning, adaptive beam forming, drones, localization, collaborative visual inference},
location = {Virtual, WI, USA},
series = {Dronet'21}
}

@inproceedings{10.1145/2910674.2910676,
author = {Nnennaya, M. Udochu and Akpaibor, Etse-Oghena and Borate, Akash P. and Deshpande, Aakash G. and Lewis, Frank},
title = {Joint Behavioural Control of Autonomous Multi-Robot Systems for Lead-Follower Formation to Improve Human-Robot Interaction},
year = {2016},
isbn = {9781450343374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2910674.2910676},
doi = {10.1145/2910674.2910676},
abstract = {The effective autonomous cooperation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has led to an increase in research and development for improving human robot interaction, due to their high potential for achieving successful missions in challenging environments. In this paper, a voice command system is developed to control a UAV and a vision-based approach system controls the UGV in a lead-follower formation to enhance human-robot teams for military defence and rescue missions in urban areas. The proposed voice control framework is executed using open source interface, Robot Operating System (ROS), for aerial vehicle communication supported by spoken dialogues using a wireless microphone, while the UGV detects and tracks a target image captured by the on board low-cost camera from the height of the UAV, using the proposed vision-based approach. The lead-follower formation is based on communication and visibility between the UAV and UGV in a given environment. In addition, a speech recognition module translates voice input to text and the published text is then mapped to the control input of the UAV in order to execute corresponding voice commands. The relative distance and position between the UAV and UGV is estimated from the image received via laptop. UGVs and UAVs have sophisticated design features such as network device communication, navigation, vision sensors and obstacle detection sensors. The UAV is a quad-copter that leads the UGV to accessible areas, while the UGV plays the role of the follower with wheel activators to track and locate the target image on the UAV, given a defined geographic path. The joint behavioural control system of the proposed voice command and vision-based approach is verified by experimental setups that show the UAV has a high voice recognition response and the UGV autonomously tracks the moving UAV in real time operation. As a consequence of the experimental tests, it is expected that the voice control system and vision-based approach developed play an important role in future combat fields.},
booktitle = {Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {7},
numpages = {8},
keywords = {voice command, Autonomous, human-robot teams, human-robot interaction, combat field, urban areas, behavioural control, UGV, UAV, quad-copter, military},
location = {Corfu, Island, Greece},
series = {PETRA '16}
}

@inproceedings{10.1145/3213526.3213532,
author = {Montanari, Alessandro and Kringberg, Fredrika and Valentini, Alice and Mascolo, Cecilia and Prorok, Amanda},
title = {Surveying Areas in Developing Regions Through Context Aware Drone Mobility},
year = {2018},
isbn = {9781450358392},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3213526.3213532},
doi = {10.1145/3213526.3213532},
abstract = {Developing regions are often characterized by large areas that are poorly reachable or explored. The mapping of these regions and the census of roaming populations in these areas are often difficult and sporadic.In this paper we put forward an approach to aid area surveying which relies on autonomous drone mobility. In particular we illustrate the two main components of the approach. An efficient on-device object detection component, built on Convolutional Neural Networks, capable of detecting human settlements and animals on the ground with acceptable performance (latency and accuracy) and a path planning component, informed by the object identification module, which exploits Artificial Potential Fields to dynamically adapt the flight in order to gather useful information of the environment, while keeping optimal flight paths. We report some initial performance results of the on board visual perception module and describe our experimental platform based on a fixed-wing aircraft.},
booktitle = {Proceedings of the 4th ACM Workshop on Micro Aerial Vehicle Networks, Systems, and Applications},
pages = {27–32},
numpages = {6},
keywords = {Artificial potential field, UAV, Area surveying, Unmanned aerial vehicles, Object detection, Autonomous vehicles, Convolutional neural network},
location = {Munich, Germany},
series = {DroNet'18}
}

@inproceedings{10.1145/3383972.3384071,
author = {Li, Zhengxin and Liu, Jia and Zhang, Xiaofeng},
title = {Similarity Measure of Multivariate Time Series Based on Segmentation},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3383972.3384071},
doi = {10.1145/3383972.3384071},
abstract = {Similarity measure of time series is a fundamental problem in data mining tasks. However, most of the similarity methods are mainly for univariate time series, rather than multivariate time series. Among the existing approaches for multivariate time series, dynamic time warping can obtain high accuracy, but the calculation cost is expensive. To solve this challenging problem, a similarity measure of multivariate time series is proposed. We first segment multivariate time series and extract the mean value and span of each sequence as its feature. Then, a similarity measure based on dynamic time warping is proposed. Finally, extensive experiments on real-world data sets are executed. The experimental results indicate the proposed method can improve the efficiency while keeping the accuracy of similarity measure.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {47–51},
numpages = {5},
keywords = {similarity measure, computational complexity, segmentation, Time series, dynamic time warping},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{10.1145/3220228.3220237,
author = {Ad\~{a}o, Telmo and P\'{a}dua, Lu\'{\i}s and Hruundefinedka, Jon\'{a}undefined and Marques, Pedro and Peres, Emanuel and Sousa, Joaquim Jo\~{a}o and Cunha, Ant\'{o}nio and de Sousa, Ant\'{o}nio M. R. and Morais, Raul},
title = {A Pilot Digital Image Processing Approach for Detecting Vineyard Parcels in Douro Region through High-Resolution Aerial Imagery},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3220228.3220237},
doi = {10.1145/3220228.3220237},
abstract = {Vineyard parcels delimitation is a preliminary but important task to support zoning activities, which can be burdensome and time-consuming when manually performed. In spite of being desirable to overcome such issue, the implementation of a semi-/fully automatic delimitation approach can meet serious development challenges when dealing with vineyards like the ones that prevail in Douro Region (north-east of Portugal), mainly due to the great diversity of parcel/row formats and several factors that can hamper detection as, for example, interrupted rows and inter-row vegetation. Thereby, with the aim of addressing vineyard parcels detection and delimitation in Douro Region, a preliminary method based on segmentation and morphological operations upon highresolution aerial imagery is proposed. This method was tested in a data set collected from vineyards located at the University of Tr'as-os-Montes and Alto Douro(Vila Real, Portugal). The presence of some of the previously mentioned challenging conditions - namely interrupted rows and inter-row grassing - in a few parcels contributed to lower the overall detection accuracy, pointing out the need for future improvements. Notwithstanding, encouraging preliminary results were achieved.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {67–71},
numpages = {5},
keywords = {vineyard parcel, RGB, automatic vine parcelling, digitalimage processing, UAV, zoning, UAS},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@article{10.1145/3478513.3480525,
author = {Li, Bosheng and Ka\l{}u\.{z}ny, Jacek and Klein, Jonathan and Michels, Dominik L. and Pa\l{}ubicki, Wojtek and Benes, Bedrich and Pirk, S\"{o}ren},
title = {Learning to Reconstruct Botanical Trees from Single Images},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3478513.3480525},
doi = {10.1145/3478513.3480525},
abstract = {We introduce a novel method for reconstructing the 3D geometry of botanical trees from single photographs. Faithfully reconstructing a tree from single-view sensor data is a challenging and open problem because many possible 3D trees exist that fit the tree's shape observed from a single view. We address this challenge by defining a reconstruction pipeline based on three neural networks. The networks simultaneously mask out trees in input photographs, identify a tree's species, and obtain its 3D radial bounding volume - our novel 3D representation for botanical trees. Radial bounding volumes (RBV) are used to orchestrate a procedural model primed on learned parameters to grow a tree that matches the main branching structure and the overall shape of the captured tree. While the RBV allows us to faithfully reconstruct the main branching structure, we use the procedural model's morphological constraints to generate realistic branching for the tree crown. This constraints the number of solutions of tree models for a given photograph of a tree. We show that our method reconstructs various tree species even when the trees are captured in front of complex backgrounds. Moreover, although our neural networks have been trained on synthetic data with data augmentation, we show that our pipeline performs well for real tree photographs. We evaluate the reconstructed geometries with several metrics, including leaf area index and maximum radial tree distances.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {231},
numpages = {15},
keywords = {tree reconstruction, botanical tree models, shape reconstruction, bounding volumes}
}

@article{10.1145/3440207,
author = {Wang, Sheng and Bao, Zhifeng and Culpepper, J. Shane and Cong, Gao},
title = {A Survey on Trajectory Data Management, Analytics, and Learning},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3440207},
doi = {10.1145/3440207},
abstract = {Recent advances in sensor and mobile devices have enabled an unprecedented increase in the availability and collection of urban trajectory data, thus increasing the demand for more efficient ways to manage and analyze the data being produced. In this survey, we comprehensively review recent research trends in trajectory data management, ranging from trajectory pre-processing, storage, common trajectory analytic tools, such as querying spatial-only and spatial-textual trajectory data, and trajectory clustering. We also explore four closely related analytical tasks commonly used with trajectory data in interactive or real-time processing. Deep trajectory learning is also reviewed for the first time. Finally, we outline the essential qualities that a trajectory data management system should possess to maximize flexibility.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {39},
numpages = {36},
keywords = {storage system, Trajectory, urban analytics, deep learning, similarity search}
}

@inproceedings{10.1145/3343031.3350853,
author = {Wang, Gaoang and Wang, Yizhou and Zhang, Haotian and Gu, Renshu and Hwang, Jenq-Neng},
title = {Exploit the Connectivity: Multi-Object Tracking with TrackletNet},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3343031.3350853},
doi = {10.1145/3343031.3350853},
abstract = {Multi-object tracking (MOT) is an important topic and critical task related to both static and moving camera applications, such as traffic flow analysis, autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works exploit spatial and temporal information for MOT, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by associating detection results frame by frame with the help of the appearance similarity and the spatial consistency. To compensate camera movement, epipolar constraints are taken into consideration in the association. Then, for every pair of two tracklets, the similarity, called the connectivity in the paper, is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups and each group represents a unique object ID. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieves promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {482–490},
numpages = {9},
keywords = {epipolar geometry, connectivity, tracklet, multi-object tracking, trackletnet},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.5555/3463952.3463981,
author = {Beal, Ryan and Chalkiadakis, Georgios and Norman, Timothy J. and Ramchurn, Sarvapali D.},
title = {Optimising Long-Term Outcomes Using Real-World Fluent Objectives: An Application to Football},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we present a novel approach for optimising long-term tactical and strategic decision-making in football (soccer) by encapsulating events in a league environment across a given time frame. We model the teams' objectives for a season and track how these evolve as games unfold to give a fluent objective that can aid in decision-making games. We develop Markov chain Monte Carlo and deep learning-based algorithms that make use of the fluent objectives in order to learn from prior games and other games in the environment and increase the teams' long-term performance. Simulations of our approach using real-world datasets from 760 matches shows that by using optimised tactics with our fluent objective and prior games, we can on average increase teams mean expected finishing distribution in the league by up to 35.6%.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {196–204},
numpages = {9},
keywords = {game theory, optimisation, fluent objectives, sports analytics, simulation},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inbook{10.1145/3483845.3483854,
author = {Yang, Xueying and Gang, Huang},
title = {Coordinated Path Planning for Multi-UAVs Based on Critical Track Points},
year = {2021},
isbn = {9781450390453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3483845.3483854},
abstract = {Multi-UAVs cooperative trajectory planning (MUCTP) refers to planning a number of safe, reliable and non-collision from each UAV starting point to the target point in known, partially known or unknown environment. In the planning process, it is necessary to consider the constraints of the UAV itself and the synergistic restriction relationship. Therefore, in order to improve the efficiency of collaborative path planning, a multi-UAVs collaborative path planning algorithm based on key path points is proposed in this paper. In this algorithm, the gene location representation method of individual population was defined, the feasible domain of three-dimensional space was set, and the objective function was constructed by combining the constraint conditions. The experimental results show that the algorithm proposed in this paper has fast convergence speed and strong synergistic ability in multi-UAVs cooperative path planning, which makes the planned track group more reasonable.},
booktitle = {2021 2nd International Conference on Control, Robotics and Intelligent System},
pages = {48–53},
numpages = {6}
}

@inbook{10.1145/3453688.3461740,
author = {Huang, Shaoyi and Chen, Shiyang and Peng, Hongwu and Manu, Daniel and Kong, Zhenglun and Yuan, Geng and Yang, Lei and Wang, Shusen and Liu, Hang and Ding, Caiwen},
title = {HMC-T<span class="smallcaps SmallerCapital">RAN</span>: A Tensor-Core Inspired Hierarchical Model Compression for Transformer-Based DNNs on GPU},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3453688.3461740},
abstract = {Although Transformer-based deep learning models have been widely used in many natural language processing (NLP) tasks as well as computer vision, they suffer from gigantic model size and long latency. Network pruning can reduce the computational cost and model size. However, existing works mainly focus on irregular(sparse) pruning, which often causes irregular computations and extra indices per remained weight. In this work, we propose a Tensor-core inspired hierarchical model compression method to push the performance limit on modern GPUs. We present two modes of the two-step process. In the first mode, we use the Tensor-core aware block-based weight pruning method to exploit model sparsity in a coarse-grained manner and then use low-rank [33] decomposition to further reduce the weight storage in a fine-grained manner.In the second mode, we first use irregular pruning to achieve a highly sparse model and then apply the Tensor-core aware weight constraint on the sparse model to decompose the sparse matrix to several smaller but Tensor-core friendly sub-matrices. Experiments on Transformer, BERTBASE models show the proposed method outperforms the state-of-the-art.},
booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
pages = {169–174},
numpages = {6}
}

@inproceedings{10.1145/3383972.3384033,
author = {Rao, Xiaohui and Lin, Shuisheng and Yuan, Zhengxi},
title = {Heterogeneous Network Selection Based on Bertrand-Matching Game},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3383972.3384033},
doi = {10.1145/3383972.3384033},
abstract = {In order to make the best network selection and improve network access satisfaction, this paper proposes a heterogeneous network selection algorithm based on game theory. This algorithm constructs a Bertrand-matching two-tier game model of network Service Provider(SP) and User Equipment(UE), taking multiple factors at network and user levels into account. It allows UEs to select the network more precisely and meets the profit needs of SPs. Simulations show that the proposed model can achieve convergent results and maximize the profits of each SP. Compared with the model of sequential access based on UE's request and the model of the random access, the total satisfaction indexes increase by 22.44% and 38.45% respectively.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {468–472},
numpages = {5},
keywords = {Network selection, game theory, matching game, Bertrand game},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{10.1145/3459212.3459215,
author = {Wang, Rui and Kang, Bin and Zhu, Wei-Ping},
title = {Meta-Learning Based Siamese Network with Channel-Wise Self-Attention for Visual Tracking},
year = {2021},
isbn = {9781450388917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3459212.3459215},
doi = {10.1145/3459212.3459215},
abstract = {A great deal of attention has been paid to Siamese networks in visual object tracking. However, the Siamese network often faces the problem of over-fitting when adapting to new scenes, where the training samples for scene adaptation are limited. In this paper, we propose a meta-learning based Siamese network with channel-wise self-attention (CSA) on the basis of SiamFC3s architecture to overcome this limitation, in which meta-learning mechanism is adopted for efficient scene adaptation and CSA is used for better representation of the target object. Specifically, in the meta-learning phase, only part of the neurons' parameters, called hyper parameters are updated, leading to lightweight adaptation, which not only reduces the computational burden of the network but also overcomes the over-fitting problem. Experimental results based on benchmark OTB-100 have shown that our meta-learning based SiamFC3s incorporating the proposed CSA outperforms the baseline method SiamFC3s by 3.4% in success rate and 5.6% in precision rate.},
booktitle = {2021 3rd International Conference on Image, Video and Signal Processing},
pages = {14–19},
numpages = {6},
keywords = {Siamese network, Meta-learning, Channel-wise self-attention},
location = {Singapore, Singapore},
series = {IVSP 2021}
}

@inproceedings{10.1109/IPSN.2018.00051,
author = {Mithun, Niluthpol Chowdhury and Munir, Sirajum and Guo, Karen and Shelton, Charles},
title = {ODDS: Real-Time Object Detection Using Depth Sensors on Embedded GPUs},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1109/IPSN.2018.00051},
doi = {10.1109/IPSN.2018.00051},
abstract = {Detecting objects that are carried when someone enters or exits a room is very useful for a wide range of smart building applications including safety, security, and energy efficiency. While there has been a significant amount of work on object recognition using large-scale RGB image datasets, RGB cameras are too privacy invasive in many smart building applications and they work poorly in the dark. Additionally, deep object detection networks require powerful and expensive GPUs. We propose a novel system that we call ODDS (Object Detector using a Depth Sensor) that can detect objects in real-time using only raw depth data on an embedded GPU, e.g., NVIDIA Jetson TX1. Hence, our solution is significantly less privacy invasive (even if the sensor is compromised) and less expensive, while maintaining a comparable accuracy with state of the art solutions. Specifically, we resort to training a deep convolutional neural network using raw depth images, with curriculum based learning to improve accuracy by considering the complexity and imbalance in object classes and developing a sparse coding based technique that speeds up the system ~2x with minimal loss of accuracy. Based on a complete implementation and real-world evaluation, we see ODDS achieve 80.14% mean average precision in object detection in real-time (5--6 FPS) on a Jetson TX1.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {230–241},
numpages = {12},
keywords = {deep learning, curriculum learning, network pruning, embedded systems, object detection},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1145/3377713.3377810,
author = {Shi, Linlin and Zhou, Zhenwei and Huang, Yun},
title = {Research and Verification of Fault Diagnosis Method for Avionics System Based on Data Mining},
year = {2019},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3377713.3377810},
doi = {10.1145/3377713.3377810},
abstract = {In this paper, a simplified fault simulation platform for avionics system is built in the laboratory, and the research and validation of fault classification methods for avionics system are carried out. According to the module composition and structure of typical avionics system, the module object is divided into several functional circuits. The fault diagnosis of SRU and LRU level of avionics system based on data mining is carried out by using the fault sensitive parameters collected under normal equipment and injection fault conditions. By analyzing the sensitivity parameters of the general processing module and extracting the fault-sensitive features, different levels of fault pattern recognition are carried out by using machine learning methods such as BP neural network, and the method is validated by using the fault simulation platform of avionics system.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {28–33},
numpages = {6},
keywords = {Avionics system, machine leaning, faultDiagnosis, data mining},
location = {Sanya, China},
series = {ACAI 2019}
}

@inproceedings{10.1145/3036331.3036354,
author = {Suprapto, Bhakti Yudho and Heryanto, M. Ary and Suprijono, Herwin and Kusumoputro, Benyamin},
title = {Altitude Control of Heavy-Lift Hexacopter Using Direct Inverse Control Based on Elman Recurrent Neural Network},
year = {2017},
isbn = {9781450348164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3036331.3036354},
doi = {10.1145/3036331.3036354},
abstract = {This paper proposes the use of Direct Inverse Control (DIC) with Elman Recurrent Neural Network (ERNN) learning algorithm for the altitude control of a heavy-lift hexacopter. The study was conducted analytically using the real flight data obtained from real plant experiment. The results showed that the ERNN can successfully control the altitude of the heavy-lift hexacopter, where the response generated by the DIC system was in good agreement with the test data with low error. Furthermore, the proposed DIC system can also control the attitude, e.g. roll, pitch and yaw of the hexacopter which are also crucial for the hexacopter movement control.},
booktitle = {Proceedings of the 8th International Conference on Computer Modeling and Simulation},
pages = {135–140},
numpages = {6},
keywords = {Heavy-Lift Hexacopter, DIC, Elman Recurrent Neural Networks, Neural Networks, Altitude Control},
location = {Canberra, Australia},
series = {ICCMS '17}
}

@inproceedings{10.1145/3383972.3384032,
author = {Fu, Chenqin and Bao, Tengfei and Lv, Liang and Sirajidin, Salayidin and Fang, Tao and Huo, Hong},
title = {Multi-Task Learning for Bi-Temporal Remote Sensing Scene Parsing via Patch-Pixel Representation},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3383972.3384032},
doi = {10.1145/3383972.3384032},
abstract = {Bi-temporal high resolution (BHR) remote sensing images assemble abundant information and ample ground details. Recently, scene parsing based on bi-temporal images has played an important role in many fields. Nevertheless, for the purpose of bi-temporal images understanding, conventional semantic segmentation can neither utilized feature relevance adequately nor tackle the problem of low-speed inference. In this paper, we propose a confused matrix unit (CMU) for scene parsing in BHR images. The designed unit is capable of representing scene transformation condition at patch level. Based on the proposed unit, a Siamese fully convolution network is developed for supervised scene parsing in BHR images. For the details of the network, we design Siamese architecture to obtain feature representation from BHR images. Then, feature aggregation protocol is implemented to yield feature fusion map. Next, CMU is calculated to reflect patch-level scene content. Subsequently, according to the results of CMU, a decoder part of semantic segmentation is designed to achieve binary pixel-level change detection. Finally, a post-processing operation is carried out to assign different dense labels for different scenes. The proposed method is evaluated both on aerial and satellite image data sets. The test result has illustrated that the proposed algorithm achieves better performance than other existing methods, especially in accuracy and inference time.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {360–367},
numpages = {8},
keywords = {post-processing, Scene parsing, BHR remote sensing images, confuse matrix, pixel-level detection},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{10.1145/3389189.3397998,
author = {Katsamenis, Iason and Protopapadakis, Eftychios and Voulodimos, Athanasios and Dres, Dimitris and Drakoulis, Dimitris},
title = {Man Overboard Event Detection from RGB and Thermal Imagery: Possibilities and Limitations},
year = {2020},
isbn = {9781450377737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3389189.3397998},
doi = {10.1145/3389189.3397998},
abstract = {A man overboard is an emergency incident, in which fast detection is the most crucial factor, for the quickest and most efficient recovery of the victim. As such, efficient monitoring methodologies should be employed. A variety of sensors is available today, supporting a continuous monitoring process, regardless of environmental conditions; RGB and thermal are two commonly used sensors. At the same time, several algorithms and techniques have been tested and proved to be efficient in human detection and situation recognition tasks. However, to this day a coherent methodology for fall detection over multiple sensors on a large-scale deployment, complying with related ISO standards on extremely low false positive alerts, has not been implemented. In this paper, we investigate the possibilities as well as the limitations of man overboard vision-based systems' development based on RGB and thermal imagery.},
booktitle = {Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {67},
numpages = {6},
keywords = {deep learning, computer vision, thermal imaging, human detection, man overboard},
location = {Corfu, Greece},
series = {PETRA '20}
}

@inproceedings{10.1145/2566468.2566483,
author = {Tiwari, Ashish and Dutertre, Bruno and Jovanovi\'{c}, Dejan and de Candia, Thomas and Lincoln, Patrick D. and Rushby, John and Sadigh, Dorsa and Seshia, Sanjit},
title = {Safety Envelope for Security},
year = {2014},
isbn = {9781450326520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2566468.2566483},
doi = {10.1145/2566468.2566483},
abstract = {We present an approach for detecting sensor spoofing attacks on a cyber-physical system. Our approach consists of two steps. In the first step, we construct a safety envelope of the system. Under nominal conditions (that is, when there are no attacks), the system always stays inside its safety envelope. In the second step, we build an attack detector: a monitor that executes synchronously with the system and raises an alarm whenever the system state falls outside the safety envelope. We synthesize safety envelopes using a modifed machine learning procedure applied on data collected from the system when it is not under attack. We present experimental results that show effectiveness of our approach, and also validate the several novel features that we introduced in our learning procedure.},
booktitle = {Proceedings of the 3rd International Conference on High Confidence Networked Systems},
pages = {85–94},
numpages = {10},
keywords = {invariants, security, hybrid systems, safety envelopes},
location = {Berlin, Germany},
series = {HiCoNS '14}
}

@inproceedings{10.5555/3283535.3283545,
author = {Abbas, Houssam and Saha, Indranil and Shoukry, Yasser and Ehlers, R\"{u}diger and Fainekos, Georgios and Gupta, Rajesh and Majumdar, Rupak and Ulus, Dogan},
title = {Embedded Software for Robotics: Challenges and Future Directions: Special Session},
year = {2018},
isbn = {9781538655641},
publisher = {IEEE Press},
abstract = {This paper surveys recent challenges and solutions in the design, implementation, and verification of embedded software for robotics. Emphasis is placed on mobile robots, like self-driving cars. In design, it addresses programming support for robotic systems, secure state estimation, and ROS-based monitor generation. In the implementation phase, it describes the synthesis of control software using finite precision arithmetic, real-time platforms and architectures for safety-critical robotics, efficient implementation of neural network based-controllers, and standards for computer vision applications. The issues in verification include verification of neural network-based robotic controllers, and falsification of closed-loop control systems. The paper also describes notable open-source robotic platforms. Along the way, we highlight important research problems for developing the next generation of high-performance, low-resource-usage, correct embedded software.},
booktitle = {Proceedings of the International Conference on Embedded Software},
articleno = {10},
numpages = {10},
keywords = {embedded software, secure state estimation, robot operating system, robotics, monitor synthesis, neural networks},
location = {Turin, Italy},
series = {EMSOFT '18}
}

@inproceedings{10.1145/3340997.3341011,
author = {Cai, Xuan and Li, Huayu and Wang, Li},
title = {Cascade Evolving Network for Vehicle Detection of Highway},
year = {2019},
isbn = {9781450363235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3340997.3341011},
doi = {10.1145/3340997.3341011},
abstract = {In this paper, a novel vehicle detection scheme via Cascade Evolving Network (CEN) is presented, which is designed for our highway vehicle detection dataset captured from super wide-angle lens. The highway images are in multi-scale, and almost all cars are dense and seriously obscured. To handle such obstacles, CEN makes better use of contextual information by proposing and refining the object boxes under different feature representations. Specifically, our framework is embedded as a light-weight cascade network. First a Light-weight Parallel Network (LPN) with a small Intersection Over Union (IOU) is applied for extracting multi-scale feature map. The parallel two networks, Coarse-grained Network (CgN) with a smallest IOU and Fine-grained Network (FgN) with a bit larger IOU produce multi-scale candidate boxes with various settings of prior anchors. The smallest IOU is designed for small objects whose IOU is smaller than large ones. Another two subnetworks refine the vague edges of proposals afterwards with gradual increasing IOU. For maximizing contextual information, three subnetworks connect together. Meanwhile, a new novel feature fusion method, named Grouped Region Proposal Network (GRPN) is adopted. CEN achieves the promising results on our highway vehicle detection dataset. To verify the robustness of the network, an evaluation on the DETRAC benchmark dataset is implemented, and obtain a significant improvement over the baseline model of Faster RCNN by 13.11% for mAP. This shows that the initial boxes can be better refined for both localization and recognition in CEN. Furthermore, Our network achieves 7-11 FPS detection speed on a moderate commercial GPU, which is much more effective than the baseline model.},
booktitle = {Proceedings of the 2019 4th International Conference on Machine Learning Technologies},
pages = {93–100},
numpages = {8},
keywords = {Cascade Evolving Network, feature fusion, cascade structure, Vehicle detection of highway},
location = {Nanchang, China},
series = {ICMLT 2019}
}

@inproceedings{10.1145/3209811.3209880,
author = {Bondi, Elizabeth and Dey, Debadeepta and Kapoor, Ashish and Piavis, Jim and Shah, Shital and Fang, Fei and Dilkina, Bistra and Hannaford, Robert and Iyer, Arvind and Joppa, Lucas and Tambe, Milind},
title = {AirSim-W: A Simulation Environment for Wildlife Conservation with UAVs},
year = {2018},
isbn = {9781450358163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3209811.3209880},
doi = {10.1145/3209811.3209880},
abstract = {Increases in poaching levels have led to the use of unmanned aerial vehicles (UAVs or drones) to count animals, locate animals in parks, and even find poachers. Finding poachers is often done at night through the use of long wave thermal infrared cameras mounted on these UAVs. Unfortunately, monitoring the live video stream from the conservation UAVs all night is an arduous task. In order to assist in this monitoring task, new techniques in computer vision have been developed. This work is based on a dataset which took approximately six months to label. However, further improvement in detection and future testing of autonomous flight require not only more labeled training data, but also an environment where algorithms can be safely tested. In order to meet both goals efficiently, we present AirSim-W, a simulation environment that has been designed specifically for the domain of wildlife conservation. This includes (i) creation of an African savanna environment in Unreal Engine, (ii) integration of a new thermal infrared model based on radiometry, (iii) API code expansions to follow objects of interest or fly in zig-zag patterns to generate simulated training data, and (iv) demonstrated detection improvement using simulated data generated by AirSim-W. With these additional simulation features, AirSim-W will be directly useful for wildlife conservation research.},
booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},
articleno = {40},
numpages = {12},
keywords = {unmanned aerial vehicles, simulation, drones, object detection, wildlife conservation},
location = {Menlo Park and San Jose, CA, USA},
series = {COMPASS '18}
}

@inproceedings{10.1145/3375998.3376043,
author = {Wen, Jie and Zhou, Dan and Feng, Haoran and Wang, Yongcai and Geng, Xiongfei and Ma, Hengzhe and Yang, Zongwei},
title = {LifeTag: Vital Sign Detection for Drowning People in Sea Accidents by Wearable Device},
year = {2019},
isbn = {9781450377027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3375998.3376043},
doi = {10.1145/3375998.3376043},
abstract = {For lifesaving in shipwreck accidents, a wearable device, called LifeTag is designed for marine travellers. The LifeTag integrates localization, communication and life-sign detection modules, which will be triggered on automatically when falling into water and broadcasts the location and life status of the drowning people, so that rescuing ships within 10 nautical miles can receive the signal. This will speed up the drowning people searching and rescue process to improve the lifesaving probability. This paper focuses on the design of data processing technique to accurately detect the life status of drowning people. Real experiments are conducted which show that the inertial sensor data can be processed by machine learning method to efficiently detect the drowning people's life sign. But a challenge problem is that LifeTag requires a very efficient implementation of the classifier, which needs to be embedded into the resource limited firmware of the LifeTag device. To accomplish this, we investigate key feature selection and seek for the efficient and effective classifier design. A simplified online classifier is therefore investigated. Finally, we implement the optimized classifier into the firm ware. Practical experiments verify nearly 100% prediction accuracy of the proposed solutions.},
booktitle = {Proceedings of the 2019 8th International Conference on Networks, Communication and Computing},
pages = {57–64},
numpages = {8},
keywords = {localization, signal processing, LifeTag, sea wave detection, vital sign, shipwreck rescue, machine learning, inertial sensor, wearable},
location = {Luoyang, China},
series = {ICNCC 2019}
}

@inproceedings{10.1145/3318299.3318348,
author = {Fu, Xingyu and Fang, Bin and Qian, Jiye and Wu, Zhenni and Zhu, Jiajie and Du, Tongxin},
title = {Roadside Traffic Sign Detection Based on Faster R-CNN},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3318299.3318348},
doi = {10.1145/3318299.3318348},
abstract = {This paper presents an improved traffic sign detection method based on Faster R-CNN with dataset augmentation and subcategory detection scheme. Firstly, we extract natural scene frames from given videos and determine 20 categories of traffic signs. Secondly, we extend the image dataset and extract regions of interest, then manually annotate all categories. Thirdly, we train the Faster R-CNN model based on TensorFlow, then test the model and obtain the following evaluation indexes: the mean average precision is 99.07%, the recall rate is 99.66%, and the precision rate is 97.54%. Finally, we add the subcategory detection scheme to determine traffic light states, and we get the following evaluation indexes: the mean average precision is 99.50%, the recall rate is 100%, and the precision rate is 94.40%. Our experiments prove the robustness and accuracy for both traffic sign detection and subcategory detection of traffic light.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {439–444},
numpages = {6},
keywords = {subcategory detection, Traffic sign detection, faster R-CNN},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1109/DS-RT52167.2021.9576133,
author = {Power, Joshua and Jacoby, Derek and Sun, Xi and Plaudis, Matt and Drouin, Marc-Antoine and Coady, Yvonne and Meng, Julian},
title = {Real-Time Mission Planning Simulations from Geospatial Data},
year = {2021},
isbn = {9781665433266},
publisher = {IEEE Press},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1109/DS-RT52167.2021.9576133},
doi = {10.1109/DS-RT52167.2021.9576133},
abstract = {Current mission planning simulations are typically carefully designed around a tradeoff between tight integration with accurate geospatial data or meeting real-time demands. Our proposed system is designed to achieve both. Deriving from work on cloud geospatial stores and mission planning systems, we are developing a platform for consuming new data, building it into models accessible from a real-time simulation server, and constructing machine learning systems over this data. This combination of large datasets with continuous pipelines feeding a real-time simulation forces us to bring together innovations on both the cloud infrastructure and user interface level.},
booktitle = {Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications},
articleno = {25},
numpages = {2},
keywords = {data driven mission planning, real-time simulation, internet of underwater things, marine megafauna, distributed system, machine learning, internet of flying things, cloud processing},
location = {Valencia, Spain},
series = {DS-RT '21}
}

@inproceedings{10.1145/3086439.3086448,
author = {Chakareski, Jacob},
title = {Drone Networks for Virtual Human Teleportation},
year = {2017},
isbn = {9781450349604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3086439.3086448},
doi = {10.1145/3086439.3086448},
abstract = {We consider a drone-based vision sensor network that captures collocated viewpoints of the scene underneath and sends them to a remote user for volumetric 360-degree navigable visual immersion on his virtual reality head-mounted display. The reconstruction quality of the immersive scene representation on the device and thus the quality of user experience will depend on the signal sampling rate and location of each drone. Moreover, there is a limit on the aggregate amount of data the network can sample and relay towards the user, stemming from transmission constraints. Finally, the user navigation actions will dynamically place different priorities on specific viewpoints of the captured scene. We make multiple contributions in this context. First, we formulate the viewpoint-priority-aware scene reconstruction error as a function of the assigned sampling rates and compute their optimal values that minimize the former, for given drone positions and system constraints. Second, we design an online view sampling policy that takes actions while exploring new drone locations to discover the best drone network configuration over the scene. We characterize its approximation versus convergence characteristics using novel spectral graph analysis and show considerable advances relative to the state-of-the-art. Finally, to enable the drone sensors to efficiently communicate their data back to the aggregation point, we formulate computationally efficient rate-distortion-power optimized transmission scheduling policies that meet the low-latency application requirements, while conserving the available energy. Our experimental results demonstrate the competitive advantages of our approach over multiple performance factors. This is a first-of-its-kind study of an emerging application of prospectively broad societal impact.},
booktitle = {Proceedings of the 3rd Workshop on Micro Aerial Vehicle Networks, Systems, and Applications},
pages = {21–26},
numpages = {6},
keywords = {volumetric 360-degree video, Rate-distortion optimized scheduling, Reinforcement learning, Networked sensing drones, Networked virtual and augment reality applications},
location = {Niagara Falls, New York, USA},
series = {DroNet '17}
}

@inproceedings{10.1145/2642937.2643011,
author = {Luckow, Kasper and P\u{a}s\u{a}reanu, Corina S. and Dwyer, Matthew B. and Filieri, Antonio and Visser, Willem},
title = {Exact and Approximate Probabilistic Symbolic Execution for Nondeterministic Programs},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2642937.2643011},
doi = {10.1145/2642937.2643011},
abstract = {Probabilistic software analysis seeks to quantify the likelihood of reaching a target event under uncertain environments. Recent approaches compute probabilities of execution paths using symbolic execution, but do not support nondeterminism. Nondeterminism arises naturally when no suitable probabilistic model can capture a program behavior, e.g., for multithreading or distributed systems.In this work, we propose a technique, based on symbolic execution, to synthesize schedulers that resolve nondeterminism to maximize the probability of reaching a target event. To scale to large systems, we also introduce approximate algorithms to search for good schedulers, speeding up established random sampling and reinforcement learning results through the quantification of path probabilities based on symbolic execution.We implemented the techniques in Symbolic PathFinder and evaluated them on nondeterministic Java programs. We show that our algorithms significantly improve upon a state-of-the-art statistical model checking algorithm, originally developed for Markov Decision Processes.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {575–586},
numpages = {12},
keywords = {symbolic execution, probabilistic software analysis, nondeterministic programs},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1145/3459745,
author = {Ajenaghughrure, Ighoyota Ben and Sousa, Sonia Cl\'{a}udia Da Costa and Lamas, David},
title = {Psychophysiological Modeling of Trust In Technology: Influence of Feature Selection Methods},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3459745},
doi = {10.1145/3459745},
abstract = {Trust as a precursor for users' acceptance of artificial intelligence (AI) technologies that operate as a conceptual extension of humans (e.g., autonomous vehicles (AVs)) is highly influenced by users' risk perception amongst other factors. Prior studies that investigated the interplay between risk and trust perception recommended the development of real-time tools for monitoring cognitive states (e.g., trust). The primary objective of this study was to investigate a feature selection method that yields feature sets that can help develop a highly optimized and stable ensemble trust classifier model. The secondary objective of this study was to investigate how varying levels of risk perception influence users' trust and overall reliance on technology. A within-subject four-condition experiment was implemented with an AV driving game. This experiment involved 25 participants, and their electroencephalogram, electrodermal activity, and facial electromyogram psychophysiological signals were acquired. We applied wrapper, filter, and hybrid feature selection methods on the 82 features extracted from the psychophysiological signals. We trained and tested five voting-based ensemble trust classifier models using training and testing datasets containing only the features identified by the feature selection methods. The results indicate the superiority of the hybrid feature selection method over other methods in terms of model performance. In addition, the self-reported trust measurement and overall reliance of participants on the technology (AV) measured with joystick movements throughout the game reveals that a reduction in risk results in an increase in trust and overall reliance on technology.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {may},
articleno = {203},
numpages = {25},
keywords = {autonomous vehicle, machine learning, classifier, psychophysiology, trust}
}

@inproceedings{10.1145/2556288.2557230,
author = {Afergan, Daniel and Peck, Evan M. and Solovey, Erin T. and Jenkins, Andrew and Hincks, Samuel W. and Brown, Eli T. and Chang, Remco and Jacob, Robert J.K.},
title = {Dynamic Difficulty Using Brain Metrics of Workload},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2556288.2557230},
doi = {10.1145/2556288.2557230},
abstract = {Dynamic difficulty adjustments can be used in human-computer systems in order to improve user engagement and performance. In this paper, we use functional near-infrared spectroscopy (fNIRS) to obtain passive brain sensing data and detect extended periods of boredom or overload. From these physiological signals, we can adapt a simulation in order to optimize workload in real-time, which allows the system to better fit the task to the user from moment to moment. To demonstrate this idea, we ran a laboratory study in which participants performed path planning for multiple unmanned aerial vehicles (UAVs) in a simulation. Based on their state, we varied the difficulty of the task by adding or removing UAVs and found that we were able to decrease error by 35% over a baseline condition. Our results show that we can use fNIRS brain sensing to detect task difficulty in real-time and construct an interface that improves user performance through dynamic difficulty adjustment.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3797–3806},
numpages = {10},
keywords = {uav, dynamic difficulty, workload, fnirs, near-infrared spectroscopy, passive brain-computer interface, bci},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inbook{10.1145/3471621.3471869,
author = {Ding, Aolin and Murthy, Praveen and Garcia, Luis and Sun, Pengfei and Chan, Matthew and Zonouz, Saman},
title = {Mini-Me, You Complete Me! Data-Driven Drone Security via DNN-Based Approximate Computing},
year = {2021},
isbn = {9781450390583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3471621.3471869},
abstract = {The safe operation of robotic aerial vehicles (RAV) requires effective security protection of their controllers against cyber-physical attacks. The frequency and sophistication of past attacks against such embedded platforms highlight the need for better defense mechanisms. Existing estimation-based control monitors have tradeoffs, with lightweight linear state estimators lacking sufficient coverage, and heavier data-driven learned models facing implementation and accuracy issues on a constrained real-time RAV. We present Mini-Me, a data-driven online monitoring framework that models the program-level control state dynamics to detect runtime data-oriented attacks against RAVs. Mini-Me leverages the internal dataflow information and control variable dependencies of RAV controller functions to train a neural network-based approximate model as the lightweight replica of the original controller programs. Mini-Me runs the minimal approximate model and detects malicious control state deviation by comparing the estimated outputs with those outputs calculated by the original controller program. We demonstrate Mini-Me on a widely adopted RAV physical model as well as popular RAV virtual models based on open-source firmware, ArduPilot and PX4, and show its effectiveness in detecting five types of attack cases with an average 0.34% space overhead and 2.6% runtime overhead. },
booktitle = {24th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {428–441},
numpages = {14}
}

@inproceedings{10.1145/3350546.3352515,
author = {Santos, Eugene and Nguyen, Hien and Kim, Keum Joo and Russell, Jacob and Veenhuis, Luke and De Guelle, Luke},
title = {Analysis of Computational Models to Describe Individual Decision-Making Process},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3350546.3352515},
doi = {10.1145/3350546.3352515},
abstract = {Understanding the human decision-making process and evaluating the quality of these decisions has been the focus of many researchers. Previously, we proposed a computational, cognitive framework called the Double Transition Model (DTM) to study human decision-making processes. We applied it to simulate a couple of scenarios developed through a naval warfare simulation game called Steel Ocean. This framework concentrated on the cognitive process of an individual's decision-making process and capturing his cognitive style. One of the key functionalities of this framework has been to provide a reward distribution indicating the quality of decisions made under certain conditions. In this paper, we present a rigorous investigation of our models capturing individual characteristics with respect to decision-making style and the reward distributions. In particular, our models explored the following questions: 1) whether individual models are different from each other like human beings are; 2) whether these models exhibit particular decision-making styles; and 3) whether these models can capture different situations as human beings do. We evaluated the capability of our models capturing these individuals' characteristics by comparing multiple DTMs against each other, each built from a couple of individuals under various circumstances. We confirmed that individual characteristics could be captured in the DTMs. Furthermore, we compared individuals' trajectories (i.e., a sequence of decisions) identified by multiple DTMs in addition to their associated neighbors to verify that decision-making process in various social conditions could be described with DTMs. Our empirical study was conducted on two sets of real-world data: Supervisory Control Operations User Testbed (SCOUT) and the naval warfare simulation game (Steel Ocean).},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {172–179},
numpages = {8},
keywords = {supervisory control operation, cognitive style, computational framework, naval warfare, Decision-making process},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.5555/3451906.3451933,
author = {Cheriguene, Youssra and Djellikh, Soumia and Bousbaa, Fatima Zohra and Lagraa, Nasreddine and Lakas, Abderrahmane and Kerrache, Chaker Abdelaziz and Tahari, Abdou El Karim},
title = {SEMRP: An Energy-Efficient Multicast Routing Protocol for UAV Swarms},
year = {2020},
publisher = {IEEE Press},
abstract = {The deployment of a swarm of cooperative UAVs applications for the execution of distributed tasks has increased attention from both academia and industry researchers. The use of a group of UAVs instead of one single UAV offers many advantages like extending the mission coverage, providing a reliable ad-hoc networks services, and enhancing the service performance, to name a few. However, due to the highly dynamic nature of the swarm topology, the coordination of a large number of UAVs poses new challenges to traditional inter-UAV communication protocols. Therefore, there is a need for the design of new networking protocols that can efficiently support the fast-pace and real-time requirements of a coordinated swarm navigation in various environments. In this paper, we propose SEMRP a Swarm energy-efficient multicast routing protocol for UAVs flying in group formations. The main purpose of SEMRP is to facilitate the control and information delivery between UAVs while minimizing inter-UAV packet loss, packet re-transmission, and end-to-end delay. In this study we show how SEMRP achieves these objectives by taking into account various Quality-of-Service parameters like the network throughput, the UAVs mobility, and energy efficiency to ensure a timely and accurate information delivery to all members of a UAV swarm. The results of the conducted simulation using NS-2 advocate for the efficiency of our proposal through its to two presented versions (SEMRP-v1 and SEMRP-v2) in term of reducing the total emission energy (at least by 10 dBm), optimizing the End-to-End Delay by 44%, and increasing the packet delivery ratio by more than to 22% compared to SP-GMRF protocol.},
booktitle = {Proceedings of the IEEE/ACM 24th International Symposium on Distributed Simulation and Real Time Applications},
pages = {182–189},
numpages = {8},
keywords = {SEMRP, UAVs, multicast routing protocol, swarm of UAVs},
location = {Prague, Czech Republic},
series = {DS-RT '20}
}

@inproceedings{10.1109/ICCPS.2018.00021,
author = {Schmittle, Matt and Lukina, Anna and Vacek, Lukas and Das, Jnaneshwar and Buskirk, Christopher P. and Rees, Stephen and Sztipanovits, Janos and Grosu, Radu and Kumar, Vijay},
title = {OpenUAV: A UAV Testbed for the CPS and Robotics Community},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1109/ICCPS.2018.00021},
doi = {10.1109/ICCPS.2018.00021},
abstract = {Multirotor Unmanned Aerial Vehicles (UAV) have grown in popularity for research and education, overcoming challenges associated with fixed wing and ground robots. Unfortunately, extensive physical testing can be expensive and time consuming because of short flight times due to battery constraints and safety precautions. Simulation tools offer a low barrier to entry and enable testing and validation before field trials. However, most of the well-known simulators today have a high barrier to entry due to the need for powerful computers and the time required for initial set up. In this paper, we present OpenUAV, an open source test bed for UAV education and research that overcomes these barriers. We leverage the Containers as a Service (CaaS) technology to enable students and researchers carry out simulations on the cloud. We have based our framework on open-source tools including ROS, Gazebo, Docker, PX4, and Ansible, we designed the simulation framework so that it has no special hardware requirements. Two use-cases are presented. First, we show how a UAV can navigate around obstacles, and second, we test a multi-UAV swarm formation algorithm. To our knowledge, this is the first open-source, cloud-enabled testbed for UAVs. The code is available on GitHub: https://github.com/Open-UAV.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {130–139},
numpages = {10},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@article{10.1145/3306202,
author = {Morcel, Raghid and Hajj, Hazem and Saghir, Mazen A. R. and Akkary, Haitham and Artail, Hassan and Khanna, Rahul and Keshavamurthy, Anil},
title = {FeatherNet: An Accelerated Convolutional Neural Network Design for Resource-Constrained FPGAs},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-7406},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3306202},
doi = {10.1145/3306202},
abstract = {Convolutional Neural Network (ConvNet or CNN) algorithms are characterized by a large number of model parameters and high computational complexity. These two requirements have made it challenging for implementations on resource-limited FPGAs. The challenges are magnified when considering designs for low-end FPGAs. While previous work has demonstrated successful ConvNet implementations with high-end FPGAs, this article presents a ConvNet accelerator design that enables the implementation of complex deep ConvNet architectures on resource-constrained FPGA platforms aimed at the IoT market. We call the design “FeatherNet” for its light resource utilization. The implementations are VHDL-based providing flexibility in design optimizations. As part of the design process, new methods are introduced to address several design challenges. The first method is a novel stride-aware graph-based method targeted at ConvNets that aims at achieving efficient signal processing with reduced resource utilization. The second method addresses the challenge of determining the minimal precision arithmetic needed while preserving high accuracy. For this challenge, we propose variable-width dynamic fixed-point representations combined with a layer-by-layer design-space pruning heuristic across the different layers of the deep ConvNet model. The third method aims at achieving a modular design that can support different types of ConvNet layers while ensuring low resource utilization. For this challenge, we propose the modules to be relatively small and composed of computational filters that can be interconnected to build an entire accelerator design. These model elements can be easily configured through HDL parameters (e.g., layer type, mask size, stride, etc.) to meet the needs of specific ConvNet implementations and thus they can be reused to implement a wide variety of ConvNet architectures. The fourth method addresses the challenge of design portability between two different FPGA vendor platforms, namely, Intel/Altera and Xilinx. For this challenge, we propose to instantiate the device-specific hardware blocks needed in each computational filter, rather than relying on the synthesis tools to infer these blocks, while keeping track of the similarities and differences between the two platforms. We believe that the solutions to these design challenges further advance knowledge as they can benefit designers and other researchers using similar devices or facing similar challenges. Our results demonstrated the success of addressing the design challenges and achieving low (30%) resource utilization for the low-end FPGA platforms: Zedboard and Cyclone V. The design overcame the limitation of designs targeted for high-end platforms and that cannot fit on low-end IoT platforms. Furthermore, our design showed superior performance results (measured in terms of [Frame/s/W] per Dollar) compared to high-end optimized designs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {mar},
articleno = {6},
numpages = {27},
keywords = {Convolutional neural networks, embedded-vision, resource-constrained FPGAs, IoT applications}
}

@inproceedings{10.5555/2030470.2030487,
author = {Khalastchi, Eliahu and Kaminka, Gal A. and Kalech, Meir and Lin, Raz},
title = {Online Anomaly Detection in Unmanned Vehicles},
year = {2011},
isbn = {0982657153},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Autonomy requires robustness. The use of unmanned (autonomous) vehicles is appealing for tasks which are dangerous or dull. However, increased reliance on autonomous robots increases reliance on their robustness. Even with validated software, physical faults can cause the controlling software to perceive the environment incorrectly, and thus to make decisions that lead to task failure. We present an online anomaly detection method for robots, that is light-weight, and is able to take into account a large number of monitored sensors and internal measurements, with high precision. We demonstrate a specialization of the familiar Mahalanobis Distance for robot use, and also show how it can be used even with very large dimensions, by online selection of correlated measurements for its use. We empirically evaluate these contributions in different domains: commercial Unmanned Aerial Vehicles (UAVs), a vacuum-cleaning robot, and a high-fidelity flight simulator. We find that the online Mahalanobis distance technique, presented here, is superior to previous methods.},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {115–122},
numpages = {8},
keywords = {machine learning, mahalanobis distance, anomaly detection, robotics, uncertainty},
location = {Taipei, Taiwan},
series = {AAMAS '11}
}

@inproceedings{10.5555/2023718.2023721,
author = {Hoffert, Joe and Schmidt, Douglas C. and Gokhale, Aniruddha},
title = {Adapting Distributed Real-Time and Embedded Pub/Sub Middleware for Cloud Computing Environments},
year = {2010},
isbn = {9783642169540},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Enterprise distributed real-time and embedded (DRE) publish/subscribe (pub/sub) systems manage resources and data that are vital to users. Cloud computing---where computing resources are provisioned elastically and leased as a service---is an increasingly popular deployment paradigm. Enterprise DRE pub/sub systems can leverage cloud computing provisioning services to execute needed functionality when on-site computing resources are not available. Although cloud computing provides flexible on-demand computing and networking resources, enterprise DRE pub/sub systems often cannot accurately characterize their behavior a priori for the variety of resource configurations cloud computing supplies (e.g., CPU and network bandwidth), which makes it hard for DRE systems to leverage conventional cloud computing platforms.This paper provides two contributions to the study of how autonomic configuration of DRE pub/sub middleware can provision and use on-demand cloud resources effectively. We first describe how supervised machine learning can configure DRE pub/sub middleware services and transport protocols autonomically to support end-to-end quality-of-service (QoS) requirements based on cloud computing resources. We then present results that empirically validate how computing and networking resources affect enterprise DRE pub/sub system QoS. These results show how supervised machine learning can configure DRE pub/sub middleware adaptively in &lt; 10 μsec with bounded time complexity to support key QoS reliability and latency requirements.},
booktitle = {Proceedings of the ACM/IFIP/USENIX 11th International Conference on Middleware},
pages = {21–41},
numpages = {21},
keywords = {DRE systems, autonomic configuration, pub/sub middleware, cloud computing},
location = {Bangalore, India},
series = {Middleware '10}
}

@inproceedings{10.1145/3450550.3465346,
author = {Karmanova, Ekaterina and Serpiva, Valerii and Perminov, Stepan and Ibrahimov, Roman and Fedoseev, Aleksey and Tsetserukou, Dzmitry},
title = {SwarmPlay: A Swarm of Nano-Quadcopters Playing Tic-Tac-Toe Board Game against a Human},
year = {2021},
isbn = {9781450383646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3450550.3465346},
doi = {10.1145/3450550.3465346},
abstract = { We present a new paradigm of games, i.e. SwarmPlay, where each playing component is presented by an individual drone that has its own mobility and swarm intelligence to win against a human player. The motivation behind the research is to make the games with machines tangible and interactive. Although some research on the robotic players for board games already exists, e.g., chess, the SwarmPlay technology has the potential to offer much more engagement and interaction with a human as it proposes a multi-agent swarm instead of a single interactive robot. The proposed system consists of a robotic swarm, a workstation, a computer vision (CV), and Game Theory-based algorithms. A novel game algorithm was developed to provide a natural game experience to the user. The preliminary user study revealed that participants were highly engaged in the game with drones (69% put a maximum score on the Likert scale) and found it less artificial compared to the regular computer-based systems (77% put maximum score). The affection of the user’s game perception from its outcome was analyzed and put under discussion. User study revealed that SwarmPlay has the potential to be implemented in a wider range of games, significantly improving human-drone interactivity.},
booktitle = {ACM SIGGRAPH 2021 Emerging Technologies},
articleno = {5},
numpages = {4},
keywords = {Multi-Agent Systems, Human-Drone Interaction (HDI), Game Theory, Computer Vision},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3318299.3318306,
author = {Xiao, Yu Xiang and Wu, Mei Min and Bi, Qian},
title = {Visual Optimization of Cluster Simulation Based on Multi Process Service and Load Balancing Agent},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3318299.3318306},
doi = {10.1145/3318299.3318306},
abstract = {This article introduces the OsgEarth open source project and the establishment of three-dimensional (3D) cluster situation. On account of multiple nodes and heavy task, the simulation visual effect in the 3D situation is not smooth. Aiming at the problems mentioned above, a multi process service architecture and a dynamic load balancing agent are proposed to deal with heavy task. Simultaneously, a visual optimization scheme based on callback and multithread interpolation is proposed to settle the caton phenomenon caused by the multi nodes in the 3D situation. On this basis, we verify the cluster simulation scene of 40 and 200 nodes. The experiments demonstrates a favourable visual impact with high performance.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {494–500},
numpages = {7},
keywords = {multithread interpolation, load balancing, Multi process service architecture},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3180496.3180635,
author = {Shuai, Du-Juan and Liu, Chang-Hua},
title = {Image Segmentation of Field Rape Based on Color Feature and Three: Dimensional Mapping},
year = {2017},
isbn = {9781450353441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3180496.3180635},
doi = {10.1145/3180496.3180635},
abstract = {Giving that the changing light in the natural condition has negative impacts on the image segmentation of rape fields, the image of rape was processed by color segmentation algorithm and three - dimensional spatial mapping to realize the effective segmentation of rape and soil, rape and rape flowers. In this paper, the components of the RGB image in green crop is extracted by the color feature, and then, the gray space of the image is transformed into the three-dimensional space by using the green super-green existence principle. The mainly steps is: the separation of crops and soil; the extraction of rape flowers; the eatraction of rape plants. The experimental results indicate that this method can not only achieve the goal of the separation of soil and crops, but also extract the plants as well as rape flowers, and the segmentation of image is effective.},
booktitle = {Proceedings of the 2017 International Conference on Wireless Communications, Networking and Applications},
pages = {215–219},
numpages = {5},
keywords = {Three -dimensional spatial mapping, Color segmentation algorithm, Cole flower, Image segmentation, Color feature},
location = {Shenzhen, China},
series = {WCNA 2017}
}

@inbook{10.1145/3431920.3439295,
author = {Huang, Qijing and Wang, Dequan and Dong, Zhen and Gao, Yizhao and Cai, Yaohui and Li, Tian and Wu, Bichen and Keutzer, Kurt and Wawrzynek, John},
title = {CoDeNet: Efficient Deployment of Input-Adaptive Object Detection on Embedded FPGAs},
year = {2021},
isbn = {9781450382182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3431920.3439295},
abstract = {Deploying deep learning models on embedded systems for computer vision tasks has been challenging due to limited compute resources and strict energy budgets. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, such as object detection, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this need, recent work introduces dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolution may access arbitrary pixels in the image with the access pattern being input-dependent and varying with spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware.In this work, we harness the flexibility of FPGAs to develop a novel object detection pipeline with deformable convolutions. We show the speed-accuracy tradeoffs for a set of algorithm modifications including irregular-access versus limited-range and fixed-shape on a flexible hardware accelerator. We evaluate these algorithmic changes with corresponding hardware optimizations and show a 1.36x and 9.76x speedup respectively for the full and depthwise deformable convolution on hardware with minor accuracy loss. We then co-design a network called CoDeNet with the modified deformable convolution for object detection and quantize the network to 4-bit weights and 8-bit activations. With our high-efficiency implementation, our solution reaches 26.9 frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50 on the standard object detection dataset, Pascal VOC. With our higher-accuracy implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of parameters--20.9x smaller but 10% more accurate than Tiny-YOLO.},
booktitle = {The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {206–216},
numpages = {11}
}

@inproceedings{10.1145/3357000.3366142,
author = {Li, Site and Gu, Yang and Song, Zhichao and Xing, Tengfei and Meng, Yiping and Xu, Pengfei and Hu, Runbo and Zhang, Tiancheng and Yu, Ge and Chai, Hua},
title = {Small Traffic Sign Detection Through Selective Feature Fusion Based Faster R-CNN With Arc-Softmax Loss},
year = {2019},
isbn = {9781450369671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3357000.3366142},
doi = {10.1145/3357000.3366142},
abstract = {Traffic signs are basic and important elements in maps. They are related to traffic regulations, profoundly affecting/managing the travel mode of human beings and efficiency of vehicle running. Traffic sign mining technology is applied in many research fields such as traditional map update, high-precision map establishment and automatic driving. Image based traffic sign identification technology has the advantages of low cost and high efficiency over manual processing mode, and traffic sign detection has thus become a significant task with the pacing advancement of autonomous driving. However, many common object detection methods cannot be directly applied to this task, as the size of traffic signs are very small yet they vary considerably. Due to such characteristics, features of traffic signs are difficult to capture, and are harder to discriminate between classes. To address this problem, we proposed a selective feature fusion based Faster R-CNN with Arc-Softmax loss, which optimizes the detection performance from the two following ways: network structure and loss function. We discover that each Faster R-CNN layer is only capable of detecting targets within a certain size range. By carefully selecting and combining different layers' feature maps, we can extract features that effectively represent traffic signs of various sizes. Then, Arc-Softmax loss penalizes the angular distances between the feature vectors of different signs, and their corresponding weight vectors of the last fully connected layers, thereby encouraging intra-class compactness and inter-class separability between learned features. Extensive analysis and experiments on the challenging Tsinghua-Tencent 100K benchmark demonstrate the superiority and implementation simplicity of our proposed method. Code will be made publicly available.},
booktitle = {Proceedings of the 12th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
articleno = {4},
numpages = {9},
keywords = {map updates, traffic sign, object detection},
location = {Chicago, IL, USA},
series = {IWCTS'19}
}

@article{10.1145/3464942,
author = {Petrolo, Riccardo and Shaikhanov, Zhambyl and Lin, Yingyan and Knightly, Edward},
title = {ASTRO: A System for Off-Grid Networked Drone Sensing Missions},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1914},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3464942},
doi = {10.1145/3464942},
abstract = {We present the design, implementation, and experimental evaluation of ASTRO, a modular end-to-end system for distributed sensing missions with autonomous networked drones. We introduce the fundamental system architecture features that enable agnostic sensing missions on top of the ASTRO drones. We demonstrate the key principles of ASTRO by using on-board software-defined radios to find and track a mobile radio target. We show how simple distributed on-board machine learning methods can be used to find and track a mobile target, even if all drones lose contact with a ground control. Also, we show that ASTRO is able to find the target even if it is hiding under a three-ton concrete slab, representing a highly irregular propagation environment. Our findings reveal that, despite no prior training and noisy sensory measurements, ASTRO drones are able to learn the propagation environment in the scale of seconds and localize a target with a mean accuracy of 8 m. Moreover, ASTRO drones are able to track the target with relatively constant error over time, even as it moves at a speed close to the maximum drone speed.},
journal = {ACM Trans. Internet Things},
month = {jul},
articleno = {24},
numpages = {22},
keywords = {drones, sensing drones, Cyber-physical systems}
}

@inproceedings{10.1145/2783449.2783516,
author = {Raut, Sushil and Gu, Qingyi and Aoyama, Tadayoshi and Takaki, Takeshi and Ishii, Idaku},
title = {Real-Time Optical Image Stabilization Using High Frame Rate Video Sequence at 500 Fps},
year = {2015},
isbn = {9781450333566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2783449.2783516},
doi = {10.1145/2783449.2783516},
abstract = {In this paper, we have developed Real-Time optical image stabilization system which uses high frame rate (HFR) video sequence captured by high speed camera head in order to stabilize jitter due to undesired camera motion while handling camera head by handheld system or mounted on mobile robots or vehicles. Optical Image Stabilization (OIS) is conducted using 512\texttimes{}512 color images captured at 500 fps by generating counter motion against high displacement motion. An improved feature extraction algorithm is implemented on high speed vision platform which is facilitated with Field Programmable Gate Array (FPGA) in order to implement user defined hardware logic to reduce the computation cost and to realize real-time image processing. Extracted features are matched with consecutive frames to obtain frame by frame displacement and direction of motion. Based on frame displacement camera fluctuation is calculated. Counter motion is determined by filtering high displacement using Infinite Impulse Response (IIR) high pass filter. High speed camera head is mounted on 2-DOF Pan-Tilt-compensation system which is used to respond to the calculated counter motion. Since the whole process is realized in real-time, high speed OIS is proved as computationally efficient.},
booktitle = {Proceedings of the 2015 Conference on Advances In Robotics},
articleno = {66},
numpages = {6},
keywords = {HFR-camera, smoothing images, motion estimation, real-time image processing, image-stabilization, deblurring},
location = {Goa, India},
series = {AIR '15}
}

@inproceedings{10.1145/3465481.3470084,
author = {Sotelo Monge, Marco Antonio and Maestre Vidal, Jorge and Medenou Choumanof, Roumen Daton},
title = {Adaptive Mitigation of Tactical Denial of Sustainability},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3465481.3470084},
doi = {10.1145/3465481.3470084},
abstract = { As the military sector digitalizes, there is a broader need for distributed computation and storage solutions deployable at the operational edge, which pose the potential of minimizing the conventional information exchanges by among others, proved enhancements in terms of latency and communication cost. The state of the art refers to multiple applications of edge computing with a vast value on tactical clouds, with may range from enabling optimized transmissions, to federated machine learning and data fusion services. However, very few of them explore the risks they entail in terms of sustainability, were an inappropriate orchestration of the edge capabilities may for example, mistakenly or maliciously deplete the energy sources that feed their supportive infrastructure, or cause electromagnetic alterations exploitable by electronic warfare actuations. In order to facilitate their understanding and contribute to their mitigation, this paper delves into the problematic inherent in Tactical Denial of Sustainability (TDoS) against tactical clouds. The conducted research introduces a framework to prevent TDoS and if required, identify their symptoms and mitigate their damage. The proposal is settled on the Self-Organizing Network (SON) paradigm and its Self-Protection (SP) applications, which benefits of their synergy with the Observe-Orient-Decide-Act (OODA) loop towards dynamically and adaptively triggering proportional reactions.},
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {104},
numpages = {9},
keywords = {Denial of Sustainability, Network Function Virtualization, Self-Organizing Networks, Cloud Computing, Cyber Defence},
location = {Vienna, Austria},
series = {ARES 2021}
}

@article{10.1145/3314387,
author = {Heard, Jamison and Heald, Rachel and Harriott, Caroline E. and Adams, Julie A.},
title = {A Diagnostic Human Workload Assessment Algorithm for Collaborative and Supervisory Human--Robot Teams},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3314387},
doi = {10.1145/3314387},
abstract = {High-stress environments, such as first-response or a NASA control room, require optimal task performance, as a single mistake may cause monetary loss or even the loss of human life. Robots can partner with humans in a collaborative or supervisory paradigm to augment the human’s abilities and increase task performance. Such teaming paradigms require the robot to appropriately interact with the human without decreasing either’s task performance. Workload is related to task performance; thus, a robot may use a human’s workload state to modify its interactions with the human. Assessing the human’s workload state may also allow for dynamic task (re-)allocation, as a robot can predict whether a task may overload the human and, if so, allocate it elsewhere. A diagnostic workload assessment algorithm that accurately estimates workload using results from two evaluations, one peer based and one supervisory based, is presented. The algorithm correctly classified workload at least 90% of the time when trained on data from the same human--robot teaming paradigm. This algorithm is an initial step toward robots that can adapt their interactions and intelligently (re-)allocate tasks.},
journal = {J. Hum.-Robot Interact.},
month = {jun},
articleno = {7},
numpages = {30},
keywords = {Human-robotic interaction, workload assessment, human--robot teaming}
}

@inproceedings{10.1145/2656045.2656071,
author = {Zadeh, Mohammad Mehdi Zeinali and Salem, Mahmoud and Kumar, Neeraj and Cutulenco, Greta and Fischmeister, Sebastian},
title = {SiPTA: Signal Processing for Trace-Based Anomaly Detection},
year = {2014},
isbn = {9781450330527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2656045.2656071},
doi = {10.1145/2656045.2656071},
abstract = {Given a set of historic good traces, trace-based anomaly detection deals with the problem of determining whether or not a specific trace represents a normal execution scenario. Most current approaches mainly focus on application areas outside of the embedded systems domain and thus do not take advantage of the intrinsic properties of this domain.This work introduces SiPTA, a novel technique for offline trace-based anomaly detection that utilizes the intrinsic feature of periodicity found in embedded systems. SiPTA uses signal processing as the underlying processing algorithm. The paper describes a generic framework for mapping execution traces to channels and signals for further processing. The classification stage of SiPTA uses a comprehensive set of metrics adapted from standard signal processing. The system is particularly useful for embedded systems, and the paper demonstrates this by comparing SiPTA with state-of-the-art approaches based on Markov Model and Neural Networks. The paper shows the technical feasibility and viability of SiPTA through multiple case studies using traces from a field-tested hexacopter, a mobile phone platform, and a car infotainment unit. In the experiments, our approach outperformed every other tested method.},
booktitle = {Proceedings of the 14th International Conference on Embedded Software},
articleno = {6},
numpages = {10},
location = {New Delhi, India},
series = {EMSOFT '14}
}

@inproceedings{10.5555/3201607.3201621,
author = {Hu, Biao and Huang, Kai},
title = {Scheduling and Shaping of Complex Task Activations for Mixed-Criticality Systems},
year = {2018},
publisher = {IEEE Press},
abstract = {In this paper, we present a new schedulability test that can cope with complex activation patterns for mixed-criticality systems. Under this analysis we proceed to present a shaping approach that can adaptively make use of system slack to improve the quality of service to less critical tasks. Compared with the state-of-the-art scheduling analysis, our scheduling analysis is more effective in handling the case that activation events can be backlogged and task deadlines can be arbitrary; and the shaping approach furthermore reduces the dropped jobs of less critical tasks without jeopardizing the guarantee to critical tasks. Extensive simulations and real-life deployment in Raspberry Pi 3 board confirm the effectiveness of our proposed schedulability test and shaping approach.},
booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
pages = {58–63},
numpages = {6},
location = {Jeju, Republic of Korea},
series = {ASPDAC '18}
}

@inproceedings{10.1145/2735960.2735971,
author = {Zhang, Xiaodong and Clark, Matthew and Rattan, Kudip and Muse, Jonathan},
title = {Controller Verification in Adaptive Learning Systems towards Trusted Autonomy},
year = {2015},
isbn = {9781450334556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2735960.2735971},
doi = {10.1145/2735960.2735971},
abstract = {With the increasing levels of adaptation and autonomy in complex cyber-physical systems (CPS), the traditional notion that such systems can be fully tested and validated offline is becoming an impossible task. It is virtually impossible to analyze or test ahead of time all the possible parameter values resulting from the uncertainty in system operational and environmental conditions. This paper considers the problem of online controller verification in a class of first-order nonlinear uncertain systems incorporating neural network based learning algorithms. Based on several critical assumptions, an on-line neural network model is employed to ensure robustness and fault-tolerance to certain modeling uncertainty and physical faults under consideration. However, these assumptions may be violated in the presence of software faults or unanticipated physical faults in the closed-loop system, leading to unstable learning behaviors and controller malfunctions. Based on Lyapunov stability theory, a online controller verification scheme is developed to detect such unstable learning behaviors by continuously monitoring the decrease of Lyapunov functions. Adaptive thresholds for detecting malfunctions of the adaptive learning controller are derived, ensuring the robustness with respect to modeling uncertainty and neural network approximation error. Additionally, the detectability conditions are investigated, characterizing the class of detectable software faults and unanticipated hardware faults. An upper bound on the detection time of controller malfunction is also derived. Some simulation results using a two-tank system are shown to illustrate the effectiveness of the controller verification method.},
booktitle = {Proceedings of the ACM/IEEE Sixth International Conference on Cyber-Physical Systems},
pages = {31–40},
numpages = {10},
keywords = {verification and validation of control systems, fault detection, neural networks, adaptive learning systems},
location = {Seattle, Washington},
series = {ICCPS '15}
}

@inproceedings{10.1145/3469877.3497698,
author = {Wang, Rui and Zheng, Chengyu and Jiang, Yanru and Wang, Zhaoxin and Ye, Min and Wang, Chenglong and Song, Ning and Nie, Jie},
title = {A Fine-Grained River Ice Semantic Segmentation Based on Attentive Features and Enhancing Feature Fusion},
year = {2021},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3469877.3497698},
doi = {10.1145/3469877.3497698},
abstract = {The semantic segmentation of frazil ice and anchor ice is of great significance for river management, ship navigation, and ice hazard forecasting in cold regions. Especially, distinguishing frazil ice from sediment-carrying anchor ice can increase the estimation accuracy of the sediment transportation capacity of the river. Although the river ice semantic segmentation methods based on deep learning has achieved great prediction accuracy, there is still the problem of insufficient feature extraction. To address this problem, we proposed a Fine-Grained River Ice Semantic Segmentation (FGRIS) based on attentive features and enhancing feature fusion to deal with these challenges. First, we propose a Dual-Attention Mechanism (DAM) method, which uses a combination of channel attention features and position attention features to extract more comprehensive semantic features. Then, we proposed a novel Branch Feature Fusion (BFF) module to bridge the semantic feature gap between high-level feature semantic features and low-level semantic features, which is robust to different scales. Experimental results conducted on Alberta River Ice Segmentation Dataset demonstrate the superiority of the proposed method.},
booktitle = {ACM Multimedia Asia},
articleno = {83},
numpages = {8},
keywords = {Attention mechanism, Feature fusion, River ice, Semantic segmentation},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

