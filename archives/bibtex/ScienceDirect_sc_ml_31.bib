@article{SCIULLO2020100164,
title = {WoT Store: Managing resources and applications on the web of things},
journal = {Internet of Things},
volume = {9},
pages = {100164},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100164},
url = {https://www.sciencedirect.com/science/article/pii/S254266052030007X},
author = {Luca Sciullo and Lorenzo Gigli and Angelo Trotta and Marco Di Felice},
keywords = {Internet of Things (IoT), Web of Things (WoT), Interoperability, Software platform, Resource management, Performance Evaluation},
abstract = {The chaotic growth of the Internet of Things (IoT) determined a fragmented landscape with a huge number of devices, technologies and platforms available on the market, and the consequential issues of interoperability on many system deployments. The Web of Things (WoT) architecture recently proposed by the W3C consortium constitutes a novel solution to enable interoperability across IoT platforms and application domains. At the same time, in order to see an effective improvement, a wide adoption of the W3C WoT solutions from the academic and industrial communities is required; this translates into the need of well-defined and complete support tools easing the deployment of W3C WoT applications. In this paper, we meet such requirement by proposing the WoT Store, a novel platform for managing and easing the deployment of Things and applications on the W3C WoT. The WoT Store allows the dynamic discovery of the resources available in the environment, i.e. the Things, and to interact with each of them through a dashboard, by visualizing their properties, executing commands or observing the notifications produced. In addition, similar to popular app stores, the WoT Store allows the search and execution of third-party WoT applications that interact with the available Things again in a seamless way. We validate the operations of our framework with two evaluation studies. First, through a small-case testbed, we demonstrate the Thing discovery and the possibility to run WoT applications that orchestrate the operations of multiple, heterogeneous Wireless Sensor Networks (WSNs). Second, through a mixed real/simulated large-scale crowdsensing scenario, we demonstrate the scalability of the platform, and the possibility to aggregate and visualize the data-streams produced by the WoT applications with minimal efforts for the users.}
}
@article{LIU201862,
title = {Curve fitting based efficient parameter selection for robust provable data possession},
journal = {Journal of Parallel and Distributed Computing},
volume = {120},
pages = {62-76},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518303514},
author = {Meng Liu and Xuyun Zhang and Chi Yang and Qiang He and Jianbing Zhang},
keywords = {Provable data possession, Parameter selection, Integrity verification, Cloud computing, Cloud storage},
abstract = {Cyberspace faces a series of threats that can spread rapidly across distributed systems. Many such transmissible cyber threats aim to damage users’ data. In recent years, the popularity of cloud computing has driven a lot of users to save their data in the cloud. The centralization of users’ data in the cloud has created new opportunities and incentives for transmissible cyber threats targeting users’ data. In this context, in addition to cloud vendor’s security mechanisms, it is important to allow users to efficiently verify the integrity of their data saved in the cloud. The seminal sampling based PDP (Provable Data Possession) scheme can attain effective probabilistic verification with high computational efficiency for the integrity of users’ data saved in the cloud by use of a set of randomly sampled data blocks. By integrating with the forward error correcting (FEC) technique, a recently-proposed robust PDP scheme can protect against arbitrarily small amounts of data corruption and has therefore been widely adopted in practice. It is a core task to determine the number of sample blocks in this scheme because this parameter plays a fundamental role in balancing the security of the scheme and the computational cost. A smaller value can comprise the security while a larger one can incur extra high computational cost. Existing work mainly leverages the Monte Carlo methods to estimate this parameter. However, these methods suffer from heavy computational cost. In this paper, we propose a method to determine this parameter based on the curve fitting technique. Specifically, we formally analyze the parameter selection process of the robust PDP scheme, and leverage the curve fitting technique to improve the efficiency of parameter selection while ensuring the optimality of the number of samples for the robustness of the scheme. Extensive experimental results demonstrate the effectiveness and efficiency of our approach. Specifically, it can be 25 times faster than the existing solution for 1, 000, 000 times simulated attacks.}
}
@article{LIU2021346,
title = {Review of digital twin about concepts, technologies, and industrial applications},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {346-361},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301072},
author = {Mengnan Liu and Shuiliang Fang and Huiyue Dong and Cunzhi Xu},
keywords = {Digital twin, Product lifecycle, Simulation, Industrial application, Literature review},
abstract = {Various kinds of engineering software and digitalized equipment are widely applied through the lifecycle of industrial products. As a result, massive data of different types are being produced. However, these data are hysteretic and isolated from each other, leading to low efficiency and low utilization of these valuable data. Simulation based on theoretical and static model has been a conventional and powerful tool for the verification, validation, and optimization of a system in its early planning stage, but no attention is paid to the simulation application during system run-time. With the development of new-generation information and digitalization technologies, more data can be collected, and it is time to find a way for the deep application of all these data. As a result, the concept of digital twin has aroused much concern and is developing rapidly. Dispute and discussions around concepts, paradigms, frameworks, applications, and technologies of digital twin are on the rise both in academic and industrial communities. After a complete search of several databases and careful selection according to the proposed criteria, 240 academic publications about digital twin are identified and classified. This paper conducts a comprehensive and in-depth review of these literatures to analyze digital twin from the perspective of concepts, technologies, and industrial applications. Research status, evolution of the concept, key enabling technologies of three aspects, and fifteen kinds of industrial applications in respective lifecycle phase are demonstrated in detail. Based on this, observations and future work recommendations for digital twin research are presented in the form of different lifecycle phases.}
}
@article{FURSZYFERDELRIO2020111631,
title = {Critically reviewing smart home technology applications and business models in Europe},
journal = {Energy Policy},
volume = {144},
pages = {111631},
year = {2020},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2020.111631},
url = {https://www.sciencedirect.com/science/article/pii/S0301421520303669},
author = {Dylan D. {Furszyfer Del Rio} and Benjamin K. Sovacool and Noam Bergman and Karen E. Makuch},
keywords = {Smart home technologies, Smart energy, ‘Prosumers’, Decarbonisation},
abstract = {Smart home technologies refer to devices that provide some degree of digitally connected or enhanced services to occupants. Smart homes have become central in recent technology and policy discussions about energy efficiency, climate change, and innovation. However, many studies are speculative, lacking empirical data, and focus on costs and benefits, but not business models and emerging markets. To address these gaps, our study presents data from semi-structured expert interviews and a review of the recent literature. Although we draw from empirical data collected in the United Kingdom, we place our findings in the context of Europe because the UK has access to European markets for smart home technologies and platforms. Our sampling strategy included experts from Amazon, Microsoft, the International Energy Agency, government, academic, and civil society stakeholders. We identify a diversity of definitions associated with smart home technologies and draw from our data to discuss applications centred on digital connections, enhanced control, automation, and learning. We analyse fifteen distinct business models for smart home technologies, ranging from energy services and household data monitoring to assisted living, security and safety, and new advertising channels (among others). Our assessment ought to guide future innovation patterns, technology deployment, and policy activity relating to smart homes, especially insofar as they can deliver energy services more affordably or help meeting carbon mitigation priorities.}
}
@article{ORTEGA2020101744,
title = {Dynamic facial presentation attack detection for automated border control systems},
journal = {Computers & Security},
volume = {92},
pages = {101744},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101744},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820300298},
author = {David Ortega and Alberto Fernández-Isabel and Isaac {Martín de Diego} and Cristina Conde and Enrique Cabello},
keywords = {Border access control, Dynamic detection, Presentation attack detection, Face recognition, Face anti-spoofing},
abstract = {Millions of passengers travel every day, border crossing being one of their most common activities. At these points it is extremely important that security is completely guaranteed. However, the maintaining adequate security levels is a very demanding issue. This has promoted the development of systems capable of providing support to border authorities by automatising some of their tasks. Thus, Automated Border Control (ABC) systems have become a key tool. These systems increase the flow of travellers as they can achieve fast evaluations of individuals through their machine-readable travel documents. However, this has motivated the appearance of attacks that try to avoid the identity detection of individuals by these systems. Presentation Attack Detection (PAD) algorithms have emerged to mitigate such a problem. This paper presents the On-the-Fly Presentation Attack Detection (FlyPAD) framework that implements a set of dynamic PAD techniques. It allows multiple types of attacks to be detected as the traveller approaches the ABC system, rather than being static in front of the cameras. Several experiments have been carried out, both in the laboratory and in real environments, obtaining promising results.}
}
@article{KANANI20202694,
title = {Exploring and Optimizing the Fog Computing in Different Dimensions},
journal = {Procedia Computer Science},
volume = {171},
pages = {2694-2703},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.292},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920312850},
author = {Pratik Kanani and Mamta Padole},
keywords = {Fog Computing, Purpose, Characteristics, Security, Optimization, Nice value, Process Scheduling in Fog},
abstract = {Internet of Things (IoT) and Cloud Computing have enabled real time decision making in systems like irrigation system, weather monitoring systems, smart home, driver-less vehicles etc. The cloud infrastructure has great computation power but involves high network latency. Thus, is unsuitable for critical health care applications like ECG monitoring, where the delay, in decision making, communicating emergency and giving timely treatment may become fatal. In such situations, Fog Computing facilitates very fast and secured response by saving network transmission to cloud and hence delays. In this paper, Fog Computing is explored in terms of its architecture, characteristics, functioning and purpose. Several fog nodes which are available in the market are studied and represented which can be used for Fog Computing developments. Existing applications and different Fog based applications are discussed along with the pros and cons of Fog Computing. Different optimization techniques to improve the Fog computing are discussed in detail with their impact on performance time. Such optimization techniques help to make health care application more efficient in terms of response time. Here, Fog Computing domain is explored in possible dimensions and relevant enhancements are discussed.}
}
@article{CURRY2019405,
title = {A Real-time Linked Dataspace for the Internet of Things: Enabling “Pay-As-You-Go” Data Management in Smart Environments},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {405-422},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1732887X},
author = {Edward Curry and Wassim Derguech and Souleiman Hasan and Christos Kouroupetroglou and Umair {ul Hassan}},
keywords = {Smart environments, Data management, Internet of Things, Water management, Energy management, Dataspace, Linked data, Semantic web, Event processing, Distributed systems},
abstract = {As smart environments move from a research vision to concrete manifestations in real-world enabled by the Internet of Things, they are encountering a number of very practical challenges in data management in terms of the flexibility needed to bring together contextual and real-time data, the interface between new digital infrastructures and existing information systems, and how to easily share data between stakeholders in the environment. Therefore, data management approaches for smart environments need to support flexibility, dynamicity, incremental change, while keeping costs to a minimum. A Dataspace is an emerging approach to data management that has proved fruitful for personal information and scientific data management. However, their use within smart environments and for real-time data remains largely unexplored. This paper introduces a Real-time Linked Dataspace (RLD) as an enabling platform for data management within smart environments. This paper identifies common data management requirements for smart energy and water environments, details the RLD architecture and the key support services and their tiered support levels, and a principled approach to “Pay-As-You-Go” data management. The paper presents a dataspace query service for real-time data streams and entities to enable unified entity-centric queries across live and historical stream data. The RLD was validated in 5 real-world pilot smart environments following the OODA (Observe, Orient, Decide, and Act) Loop to build real-time analytics, decisions support, and smart apps for energy and water management. The pilots demonstrate that the RLD enables incremental pay-as-you-go data management with support services that simplify the development of applications and analytics for smart environments. Finally, the paper discusses experiences, lessons learnt, and future directions.}
}
@article{ASSIMAKIS2021104391,
title = {Steady state Kalman filter design for cases and deaths prediction of Covid-19 in Greece},
journal = {Results in Physics},
volume = {26},
pages = {104391},
year = {2021},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2021.104391},
url = {https://www.sciencedirect.com/science/article/pii/S2211379721005131},
author = {N. Assimakis and M. Adam and A. Ktena and C. Manasis},
keywords = {Prediction, Kalman filters, Steady state, Covid-19},
abstract = {In this work we study the applicability of the steady state Kalman filter in order to predict new cases and deaths of Covid-19. We use the actual observations of new cases and deaths. First, we deal with short term prediction, namely daily prediction. We propose the use of the golden steady state Kalman Filter, which is designed to have parameters related to the golden section. It was found that the proposed golden steady state Kalman Filter has a satisfactory behavior compared with the classical mean or average filter. Secondly, we deal with long term prediction, for example average prediction per quarantine period (14 days). We propose to process blocks of measurements of time window corresponding for example to the quarantine period in order to predict the average of cases and deaths using steady state Kalman Filter. It was found that the proposed golden steady state Kalman Filter produces more reliable predictions than the classical mean or average filter does. The use of steady state Kalman Filter for cases and deaths prediction of Covid-19 can be effective for resources and prevention measures planning.}
}
@article{WANG2021105414,
title = {Big data in safety management: An overview},
journal = {Safety Science},
volume = {143},
pages = {105414},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105414},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521002587},
author = {Bing Wang and Yuanjie Wang},
keywords = {Safety, Big data, Safety management, Big-data-driven, Industrial safety},
abstract = {Big data has an important influence on safety management in various fields where its applications are becoming more prevalent. The analysis results of big data have become an important reference influencing safety decision-making. Realizing the promising benefits of big data in safety management has motivated us to write a review on the influence of big data and its applications in safety management. This study also investigates the challenges faced by big data in safety management and provides insights to future directions for research and practice. We first briefly introduce the development history of big data and its influence on safety management. We then review the general theories and technologies of big data in safety management. Finally, we summarize the typical applications of big data in safety management according to different fields. Additional findings from the review process are also presented.}
}
@article{PAGAN2018587,
title = {Power transmission and workload balancing policies in eHealth mobile cloud computing scenarios},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {587-601},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17302133},
author = {Josué Pagán and Marina Zapater and José L. Ayala},
keywords = {WBSN, Migraine prediction, Energy optimization, Data Center, Balancing workload, Economic savings},
abstract = {The Internet of Things (IoT) holds big promises for healthcare, especially in proactive personal eHealth. Prediction of symptomatic crises in chronic diseases in the IoT scenario leads to the deployment of ambulatory monitoring systems. These systems place a major concern in the amount of data to be processed and the intelligent management of the energy consumption. The huge amount of data generated for these systems require high computing capabilities only available in Data Centers. This paper presents a real case of prediction in the eHealth scenario, devoted to neurological disorders. The presented case study focuses on the migraine headache, a disease that affects around 15% of the European population. This paper extrapolates results from real data and simulations in a study where migraine patients are monitored using an unobtrusive Wireless Body Sensor Network. Low-power techniques are applied in monitorization nodes. Techniques such us: on-node signal processing and radio policies to make node’s autonomy longer and save energy, have been applied. Workload balancing policies are carried out in the coordinator nodes and Data Centers to reduce the computational burden in these facilities and minimize its energy consumption. Our results draw average savings of € 288 million in this eHealth scenario applied only to 2% of European migraine sufferers; in addition to savings of € 1272 million due to the benefits of the migraine prediction.}
}
@article{ADNANKHAN2019116,
title = {Energy Harvesting—Technical Analysis of Evolution, Control Strategies, and Future Aspectsa},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {2},
pages = {116-125},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80314201},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300187},
author = {MD. Shahrukh {Adnan Khan} and Md. Tanbhir Hoq and A.H.M. {Zadidul Karim} and Md. Khairul Alam and Masum Howlader and Rajprasad Kumar Rajkumar},
keywords = {Energy harvesting (EH), piezoelectric, radio frequency (RF), solar, triboelectric, wind},
abstract = {This paper provides a technical analysis of energy harvesting (EH) in the field of power and energy sector, including different aspects of harvesting energy, individual case history, control strategies of harvesting in the field of power and energy sector together with the current trend and future aspects of it. EH is comparatively a new concept which is growing very fast since the 20th century and catching new generation research approaches. This paper not only describes the past and current scenarios of harvesting energy with radio frequency (RF) and renewables but also gives author’s own anticipation of the upcoming future trends of it by comparing the case histories.}
}
@article{ZHANG20151,
title = {Regenerative sustainability for the built environment – from vision to reality: an introductory chapter},
journal = {Journal of Cleaner Production},
volume = {109},
pages = {1-10},
year = {2015},
note = {Special Issue: Toward a Regenerative Sustainability Paradigm for the Built Environment: from vision to reality},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959652615013815},
author = {Xiaoling Zhang and Martin Skitmore and Martin {De Jong} and Donald Huisingh and Matthew Gray},
keywords = {Regenerative sustainability, Theory, Framework, Built environment, Pathways, Obstacles and enablers},
abstract = {Regenerative sustainability is emerging as an alternative discourse around the transition from a ‘mechanistic’ to an ‘ecological’ or living systems worldview. This view helps us to re-conceptualize relationships among humans' technological, ecological, economic, social and political systems. Through exploration of ‘net positive’ or ‘regenerative’ development lenses and the traditional sustainability literature, the conceptualization and approaches to achieve sustainable development and ecological modernization are expanded to articulate and to explore the evolving sustainability discourse, ‘regenerative sustainability’. This Special Volume of Journal of Cleaner Production (SV) is focused upon various dimensions of regenerative sustainability (e.g. regenerative design, regenerative development, and positive development) applied to the urban built environment at scales, which range from individual buildings, neighbourhoods, and urban developments to integrated regional sustainable development. The main focus is on how these approaches and developments are evolving, how they can help us to prevent or adapt to climate change and how these approaches are likely to evolve in the next two to three decades. These approaches are addressed in four themes: (1) reviewing the theoretical development of the discourse of regenerative sustainability, its emerging principles and practices, (2) explaining how it can be measured and monitored, (3) providing encouraging practical pathways and examples of its implementation in multiple cultural and climatic contexts, and (4) mapping obstacles and enablers that must be addressed to help to ensure that more rapid progress is made in implementing the transitions towards an urban built environment that supports genuinely sustainable societies.}
}
@article{BENNETT2019435,
title = {Transformations, transitions, or tall tales? A global review of the uptake and impact of NoSQL, blockchain, and big data analytics on the land administration sector},
journal = {Land Use Policy},
volume = {83},
pages = {435-448},
year = {2019},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2019.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0264837718309402},
author = {Rohan Mark Bennett and M. Pickering and J. Sargent},
keywords = {Unstructured data, Distributed databases, Land administration, Land Registration, Cadastre},
abstract = {Unstructured data, non-relational databases, distributed databases, and big data analytics potentially change the landscape for digital land data creation, use, management, and dissemination. The way land data is governed and the resultant impact on land tenure security delivery is open to substantial re-thinking. Drawing from international cases, this paper provides a state-of-the-art examination of prototypes and demonstrators from the global land sector. The aim is to deliver an insight into the opportunities, challenges, impacts, and future scenarios regarding the application of the emerging technologies. The paper finds that whilst uptake of non-relational and distributed databases is occurring, it remains largely at the level of proof-of-concept, demonstrator or pilot. Scaled uptake is occurring slower than anticipated, and to a lesser degree than in other sectors. However, the broader and longer-term impacts on the land sector remain difficult to judge. Meanwhile, emerging distributed analytical databases appear to be under-exploited or at least underexplored within the land sector. Overall, the examined technologies are seen to enable new operational approaches for conventional land sector transactions, and also entirely new land related services, digital products, and actors: the ways and means of holding, transferring, and securing land may become more nuanced, fractionalized, and/or automated. Whether these observed embryonic developments lead to sector transformation or orderly transition, is seen to be heavily reliant on the prevailing political, socio-cultural and institutional settings within a jurisdiction.}
}
@article{MANAVALAN2019925,
title = {A review of Internet of Things (IoT) embedded sustainable supply chain for industry 4.0 requirements},
journal = {Computers & Industrial Engineering},
volume = {127},
pages = {925-953},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218305709},
author = {E. Manavalan and K. Jayakrishna},
keywords = {Sustainable supply chain (SSC), Closed loop supply chain (CLSC), Enterprise Resource Planning (ERP), Industry 4.0, IoT, IIoT},
abstract = {Supply Chain organizations in the present global environment operate in market that is increasingly complex and dynamic in nature. Sustainable supply chain becomes inevitable to meet the aggressive change in the customer requirements. Based on the reviews, it is revealed that manufacturing companies need to speed up in shifting the focus towards sustainability and make use of technology like ‘Internet of Things’ (IoT) to meet the organization’s goal. The objective of this research paper is to review the various aspects of SCM, ERP, IoT and Industry 4.0 and explore the potential opportunities available in IoT embedded sustainable supply chain for Industry 4.0 transformation. In this review, a comprehensive study on various factors, that affects the sustainable supply chain were analyzed and the results recorded. Based on the review, a framework for assessing the readiness of supply chain organization from various perspectives has been proposed to meet the requirements of the fourth Industrial Revolution. The conceptual framework model has been formulated from five important perspectives of supply chain management namely Business, Technology, Sustainable Development, Collaboration and Management Strategy. This study furnishes the criteria that can be assessed by companies to realize the readiness for industry 4.0 transformation.}
}
@article{KHARGONEKAR202017008,
title = {A Framework for Ethics in Cyber-Physical-Human Systems⁎⁎Supported by the University of California, Irvine and the State University of New York, Albany.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17008-17015},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1251},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320316530},
author = {Pramod P. Khargonekar and Meera Sampath},
keywords = {Cyber-physical-human systems, ethical issues, social impact of automation},
abstract = {This paper proposes a conceptual framework for consideration of ethical issues in the emerging category of smart cyber-physical systems. Cyber-physical systems (CPS) that bring together controls, communications, computing, and physical systems are being developed in a wide variety of application domains ranging from transportation, energy, and manufacturing, to biomedical and agriculture. Smart CPS are already being and will increasingly be deployed to work with humans, in workplaces, homes, or public spaces, resulting in the creation of cyber-physical human systems (CPHS). Ethical issues in smart CPS and CPHS can be examined within the larger frameworks of ethics of technology and ethics of artificial intelligence. We begin with a description of trends and visions for the future development of smart CPS. We next outline fundamental theories of ethics that offer foundations for thinking about ethical issues in smart CPHS. We argue that it is necessary to fight the tendency toward technological determinism. We argue that in analyzing ethics of smart CPHS, we need to anticipate increasing capabilities and the future deployment of such systems. Ultimately, if these systems are widely deployed in society, they will have a very significant impact, including possible negative consequences, on individuals, communities, nations, and the world. Our framework has two main dimensions: (i) stage of development of CPHS domain from early stage research to mature technologies; and (ii) locus of decision making: individual, corporate, and government settings. We illustrate the framework with some specific examples.}
}
@article{SJODIN2021574,
title = {How AI capabilities enable business model innovation: Scaling AI through co-evolutionary processes and feedback loops},
journal = {Journal of Business Research},
volume = {134},
pages = {574-587},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321003386},
author = {David Sjödin and Vinit Parida and Maximilian Palmié and Joakim Wincent},
keywords = {Artificial intelligence, Digital servitization, Digital transformation, Digitalization, Business model innovation, Platform},
abstract = {Artificial intelligence (AI) is predicted to radically transform the ways manufacturing firms create, deliver, and capture value. However, many manufacturers struggle to successfully assimilate AI capabilities into their business models and operations at scale. In this paper, we explore how manufacturing firms can develop AI capabilities and innovate their business models to scale AI in digital servitization. We present empirical insights from a case study of six leading manufacturers engaged in AI. The findings reveal three sets of critical AI capabilities: data pipeline, algorithm development, and AI democratization. To scale these capabilities, firms need to innovate their business models by focusing on agile customer co-creation, data-driven delivery operations, and scalable ecosystem integration. We combine these insights into a co-evolutionary framework for scaling AI through business model innovation underscoring the mechanisms and feedback loops. We offer insights into how manufacturers can scale AI, with important implications for management.}
}
@article{JAIN2016156,
title = {A GPU Based Implementation of Robust Face Detection System},
journal = {Procedia Computer Science},
volume = {87},
pages = {156-163},
year = {2016},
note = {Fourth International Conference on Recent Trends in Computer Science & Engineering (ICRTCSE 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.142},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916304811},
author = {Vaibhav Jain and Dinesh Patel},
keywords = {Face Detection, GPU, CUDA, Integral Image},
abstract = {Face detection is the active research area in the field of computer vision because it is the first step in various applications like face recognition, military intelligence and surveillance, human computer interaction etc. Face detection algorithms are computationally intensive, which makes it is difficult to perform face detection task in real-time. We can overcome the processing limitations of the face detection algorithms by offloading computation to the graphics processing unit (GPU) using NVIDIAs Compute Unified Device Architecture (CUDA). In this paper, we have developed a GPU based implementation of robust face detection based on Viola Jones face detection algorithm. To verify our work, we compared our implementation with traditional CPU implementation for same algorithm.}
}
@article{COSTIN2018257,
title = {Building Information Modeling (BIM) for transportation infrastructure – Literature review, applications, challenges, and recommendations},
journal = {Automation in Construction},
volume = {94},
pages = {257-281},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309470},
author = {Aaron Costin and Alireza Adibfar and Hanjin Hu and Stuart S. Chen},
keywords = {Building Information Modeling (BIM), Bridge Information Modeling (BrIM), Civil Integrated Management (CIM), Civil information Modeling (CiM), Industry Foundation Classes (IFC), Transportation infrastructure, Emerging technologies, Literature review},
abstract = {Transportation infrastructure is a critical component to a nation’s economy, security, and wellbeing. In order to keep up with the rising population, there is a great need for more efficient and cost-effective technologies and techniques to not only repair the infrastructure, but also to advance and expand the transportation infrastructure to sustain the growing population. Building Information Modeling (BIM) has been widely adopted in the building industry, and its established methods and technologies show enormous potential in benefiting the transportation industry. The purpose of this paper is to present a literature review and critical analysis of BIM for transportation infrastructure. A total of 189 publications in the area of BIM for transportation infrastructure were reviewed, including journal articles, conference proceedings, and published reports. Additionally, schemas and file formats from 9 main categories and 34 areas related to transportation infrastructure were reviewed. An application was developed to collect, store, and analyze the publications. Various algorithms were developed and implemented to help in the automation and analysis of the review. The goal of this paper is to provide a comprehensive, up-to-date literature review and critical analysis of research areas regarding BIM for transportation infrastructure to further facilitate research and applications in this domain. Based on the results of the analysis, current topics and trends, applications and uses, emerging technologies, benefits, challenges and limitations, research gaps, and future needs are discussed. Significantly, the contribution of this paper is providing the foundation of current research, gaps, and emerging technologies needed to facilitate further research and applications for both academia and industry stakeholders to develop more efficient and cost-effective techniques necessary to repair, advance, and expand the transportation infrastructure. Furthermore, the results show that the use of BIM for transportation infrastructure has been increasing, although the research has mainly been focusing on roads, highways, and bridges. The results also reveal a major need for a standard neutral exchange format and schema to promote interoperability. Most importantly, the continuing collaboration between academia and industry is required to mitigate most challenges and to realize the full potential of BIM for transportation infrastructure.}
}
@article{NAJIB20191300,
title = {Survey on Trust Calculation Methods in Internet of Things},
journal = {Procedia Computer Science},
volume = {161},
pages = {1300-1307},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.245},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931957X},
author = {Warsun Najib and Selo Sulistyo and  Widyawan},
keywords = {trust calculation, trust evaluation, trust classification, internet of things, IoT},
abstract = {The Internet of Things (IoT) is widely affecting our daily lives in many different activities and applications. IoT devices are ranging from a tiny wearable device to large industrial applications. Wide variety of IoT applications have been developed and deployed involving many devices and produced huge data. In IoT applications, privacy, security, and trust play an important role in the success of IoT implementation. Trust can be defined as a key property to establish trustworthy and connectivity among devices to ensure secure services and applications. This paper addresses a survey of trust calculation models for IoT systems, i.e., what are available models or methods used by researcher to compute trust in IoT system. In addition, classification is also developed to categorize trust calculation model using five parameters including trust metric, trust source, trust algorithm, trust architecture, and trust propagation. Furthermore, some research challenges and directions are identified within the topic of trust-based security in IoT.}
}
@article{TA2020100283,
title = {A secure road traffic congestion detection and notification concept based on V2I communications},
journal = {Vehicular Communications},
volume = {25},
pages = {100283},
year = {2020},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2020.100283},
url = {https://www.sciencedirect.com/science/article/pii/S2214209620300541},
author = {Vinh-Thong Ta and Amit Dvir},
keywords = {Security, V2I communications, Traffic congestion detection},
abstract = {Applying vehicular (V2X) communications in detecting traffic congestion is a promising approach, as smart and self-driving vehicles are equipped with sensors that can be used to detect an incident anywhere in real-time. Unfortunately, without appropriate security measures and careful design the communication can be vulnerable to malicious attacks, causing even more damage on the roads. Addressing these problems, we propose a high-level system architecture and a security protocol specifically designed for congestion detection based on vehicle-to-infrastructure (V2I) type communication. The security properties of our proposed approach are formally verified using the ProVerif tool, and its efficiency compared to the traditional traffic light systems is demonstrated through simulations with the Veins framework. Results show that our system is secure against a large set of attacks, and can have lower total travelling time compared to three traditional traffic light approaches based on induction loop, lane area detector/camera (installed near the junctions), and static lights.}
}
@article{FEINBERG2019579,
title = {Examining spatiotemporal variability of urban particulate matter and application of high-time resolution data from a network of low-cost air pollution sensors},
journal = {Atmospheric Environment},
volume = {213},
pages = {579-584},
year = {2019},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2019.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S1352231019304170},
author = {Stephen Neil Feinberg and Ron Williams and Gayle Hagler and Judy Low and Larry Smith and Ryan Brown and Daniel Garver and Michael Davis and Michael Morton and Joe Schaefer and John Campbell},
keywords = {Low cost sensor, Fine particulate matter, Sensor network},
abstract = {Traditional air monitoring approaches using regulatory monitors have historically been used to assess regional-scale trends in air pollutants across large geographical areas. Recent advances in air pollution sensor technologies could provide additional information about nearby sources, support the siting of regulatory monitoring stations, and improve our knowledge of finer-scale spatiotemporal variation of ambient air pollutants and their associated health effects. Sensors are now being developed that are much smaller and lower cost than traditional ambient air monitoring systems and are capable of being deployed as a network to provide greater coverage of a given area. The CitySpace project conducted by the US EPA and the Shelby County Health Department included the deployment of a network of 17 sensor pods using Alphasense OPC-N2 particulate matter (PM) sensors integrated with meteorological sensors in Memphis, TN for six months. Sensor pods were collocated with a federal equivalent method (FEM) tapered element oscillating microbalance (TEOM) monitor both before and after the primary study period. Six of the sensor pods were found to meet the data quality objective (DQO) of coefficient of determination (R2) greater than 0.5 when collocated with the TEOM. Seven pods were decommissioned before the end of the study due to mechanical failure. The six pods meeting the DQO were used to examine the spatiotemporal variability of fine PM (PM2.5) across the Memphis area. One site was found to have higher relative PM2.5 concentrations when compared to the other sites in the network. The 1-min data from this sensor pod were evaluated to quantify the regional urban background and local-scale contributions to PM2.5 at that monitoring location. This method found that approximately 20% of the PM2.5 was attributed to local sources at this location, compared to 9% at a local regulatory monitoring site. Additionally, the 1-min data were combined with 1-min wind speed and wind direction data to examine potential sources in the area using the nonparametric trajectory analysis (NTA) technique. This method geographically identified local source areas that contributed to the measured concentrations at the high reading sensor location throughout the course of the study.}
}
@article{XU2021111642,
title = {On the resilience of modern power systems: A comprehensive review from the cyber-physical perspective},
journal = {Renewable and Sustainable Energy Reviews},
volume = {152},
pages = {111642},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111642},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121009175},
author = {Luo Xu and Qinglai Guo and Yujie Sheng and S.M. Muyeen and Hongbin Sun},
keywords = {Cyber-physical system, Cyberattack, Human-in-the-loop, Natural hazards, Resilience, Smart grid},
abstract = {The digital transformation of power systems into cyber-physical systems (CPSs) is the inevitable trend of modern power systems with the integration of large-scale renewable energy. The in-depth interdependence of cyber and physical spaces leads to more complicated external environments for such cyber-physical power systems (CPPSs) and brings great challenges to the resilience of CPPSs. A resilient CPS imposes strict requirements for its ability to cope with high-impact, low-probability cyber-physical disturbances. To better study the vulnerability and resilience of CPPSs, several representative blackouts from the past two decades are reviewed from the cyber-physical perspective. Inspired by general system theory, this study offers a framework with three key features of a CPPS and presents the three-layer interdependences from facilities to functions. The differences between CPPS resilience and conventional power system resilience are also emphasized. Thereafter, the study discusses the influence of cyber-physical disturbances from natural hazards, cyberattacks, and human-in-the-loop on the resilience of CPPSs. Accordingly, a survey of the state-of-the-art resilience-oriented techniques for CPPS in the face of natural hazards is organized based on quantitative metrics as well as planning and operation attributes. Regarding the resilience against cyberattacks, relevant cutting-edge research is reviewed in terms of prevention, detection, and mitigation strategies. Furthermore, from the cyber-physical-social perspective, the exploitation of social behaviors to inform the design of the physical system and the cyber system to ultimately enhance the resilience of CPPSs is also studied. Based on the findings from this research, the remaining challenges and the broad prospects of cyber-physical resilience enhancement techniques are also discussed.}
}
@article{ROBERTS2019242,
title = {Characterisation of Australian apartment electricity demand and its implications for low-carbon cities},
journal = {Energy},
volume = {180},
pages = {242-257},
year = {2019},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.04.222},
url = {https://www.sciencedirect.com/science/article/pii/S036054421930862X},
author = {Mike B. Roberts and Navid Haghdadi and Anna Bruce and Iain MacGill},
keywords = {Apartments, Residential electricity demand, Load profiles, Cluster analysis, Load aggregation, Low-carbon cities},
abstract = {Understanding of residential electricity demand has application in efficient building design, network planning and broader policy and regulation, as well as in planning the deployment of energy efficiency technologies and distributed energy resources with potential emissions reduction benefits and societal and household cost savings. Very few studies have explored the specific demand characteristics of apartments, which house a growing proportion of the global urban population. We present a study of apartment electricity loads, using a dataset containing a year of half-hourly electricity data for 6,000 Australian households, to examine the relationship between dwelling type, demographic characteristics and load profile. The focus on apartments, combined with the size of the data set, and the representative seasonal load profiles obtained through clustering full annual profiles, is unique in the literature. We find that median per-occupant household electricity use is 21% lower for apartments than for houses and that, on average, apartments have lower load factor and higher daily load variability, and show greater diversity in their daily peak times, resulting in a lower coincidence factor for aggregations of apartment loads. Using cluster analysis and classification, we also show the impact of dwelling type on the shape of household electricity load profiles.}
}
@article{PERSIA2020931,
title = {Improving orienteering-based tourist trip planning with social sensing},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {931-945},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19303929},
author = {Fabio Persia and Giovanni Pilato and Mouzhi Ge and Paolo Bolzoni and Daniela D’Auria and Sven Helmer},
keywords = {Social sensing, Orienteering, Semantic mapping, Semantic similarity},
abstract = {We enhance a tourist trip planning framework based on orienteering with category constraints by adding social sensing. This allows us to customize a user’s experience without putting the burden of preference elicitation on the user. We identify the interests of a user by analyzing their Tweets and then match these interests to descriptions of points of interests. For this analysis we adapt different schemes for social sensing to the needs of our orienteering context and compare them to find the most suitable approach. We show that our technique is fast enough for use in real-time dynamic settings and also has a higher accuracy compared to previous approaches. Additionally, we integrate a more efficient algorithm for solving the orienteering problem, boosting the overall performance and utility of our framework further, as demonstrated by the positive user satisfaction received by real users.}
}
@article{CIAVOTTA2017931,
title = {A Microservice-based Middleware for the Digital Factory},
journal = {Procedia Manufacturing},
volume = {11},
pages = {931-938},
year = {2017},
note = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.197},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917304055},
author = {Michele Ciavotta and Marino Alge and Silvia Menato and Diego Rovere and Paolo Pedrazzoli},
keywords = {Microservices, Real-digital synchronization, Cyber Physical Systems, Data Stream Analysis, Smart Factory, Industrie 4.0},
abstract = {In recent years a considerable effort has been spent by research and industrial communities in the digitalization of production environments with the main objective of achieving a new automation paradigm, more flexible, responsive to changes, and safe. This paper presents the architecture, and discusses the benefits, of a distributed middleware prototype supporting a new generation of smart-factory-enabled applications with special attention paid to simulation tools. Devised within the scope of MAYA EU project, the proposed platform aims at being the first solution capable of empowering shop-floor Cyber-Physical-Systems (CPSs), providing an environment for their Digital Twin along the whole plant life-cycle. The platform implements a microservice IoT-Big Data architecture supporting the distributed publication of multidisciplinary simulation models, managing in an optimized way streams of data coming from the shop-floor for real-digital synchronization, ensuring security and confidentiality of sensible data.}
}
@article{LI2020101004,
title = {How should we understand the digital economy in Asia? Critical assessment and research agenda},
journal = {Electronic Commerce Research and Applications},
volume = {44},
pages = {101004},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2020.101004},
url = {https://www.sciencedirect.com/science/article/pii/S1567422320300818},
author = {Kai Li and Dan J. Kim and Karl R. Lang and Robert J. Kauffman and Maurizio Naldi},
keywords = {Asian digital economy, COVID-19 pandemic, Digitalization, Digital trade, Digital transformation, Economic growth, Emerging technologies, Firm-level technological sophistication, Global value chains, Industrial infrastructure, Information and communication technologies (ICTs), Information technology (IT), Informatization, IT innovation, Smiling curve, Technological change, Technology impacts, Transaction costs, Value chain participation},
abstract = {By Asian digital economy, we refer to high-tech developments, business and social transformations, and information-driven changes in the region’s growth. We discuss its background and foundations, significance in Asia and contribution to removal of historical barriers in traditional business. We assess how new value chains are transforming country-level involvement in worldwide manufacturing and note “smiling curve theory” predictions about the global value chain in Asia for high-tech firms and their economies. The takeaway is that the digital economy in Asian nations involves revamping business processes through technology innovation, government policies for growth, and digital entrepreneurship. We analyze the “digital economy and society index”, and attributes of nations, societies and economies, as a basis for framing our ideas. We consider research directions prompted by data analytics and AI, the platform economy, digital trade, fintech innovation, and societal and economic sustainability. We further highlight new issues in light of the COVID-19 pandemic.}
}
@article{TAREK2020231,
title = {Survey on spectrum sharing/allocation for cognitive radio networks Internet of Things},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {4},
pages = {231-239},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2020.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519302622},
author = {Dina Tarek and Abderrahim Benslimane and M. Darwish and Amira M. Kotb},
abstract = {Many IoT applications have been developed recently, this leads to increasing the network load. To avoid the lack of spectrum, IoT can use cognitive radio network concept. This paper presents most of the researches addressing channel allocation and packets scheduling, when merging the cognitive radio networks with the IoT technology. Surveying the researches done in this area shows the work done is still in its early stage, and needs a lot of developments and investigations.}
}
@article{VICTORELLI202013,
title = {Understanding human-data interaction: Literature review and recommendations for design},
journal = {International Journal of Human-Computer Studies},
volume = {134},
pages = {13-32},
year = {2020},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919301193},
author = {Eliane Zambon Victorelli and Julio Cesar {Dos Reis} and Heiko Hornung and Alysson Bolognesi Prado},
keywords = {Human-Data Interaction, Literature review, Research challenges, Data deluge},
abstract = {The trend of collecting information about human activities to inform and influence actions and decisions poses a series of challenges to analyze this data deluge. The lack of ability to understand and interact with this amount of data prevents people and organizations from taking the best of this information. To investigate how people interact with data, a new area of study called “Human-Data Interaction” (HDI) is emerging. In this article, we conduct a thorough literature review to create the big picture about the subject. We carry out a variety of analyses and visual examinations to understand the characteristics of existing publications, detecting the most frequently addressed research topics and consolidating the research challenges. Based on the needs of HDI we found in the analyzed publications, we organize a set of recommendations and evaluate online systems that demand intensive human-data interaction. The obtained results indicate there are still many open questions for this interesting area, which is maturing with an increase number of publications in the last years, and that systems with large amount of data openly available poorly meet the proposed recommendations.}
}
@article{MOKARRAM20213362,
title = {Risk-based multi-criteria decision analysis of gas power plants placement in semi-arid regions},
journal = {Energy Reports},
volume = {7},
pages = {3362-3372},
year = {2021},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.05.071},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721003553},
author = {Marzieh Mokarram and Miadreza Shafie-khah and Jamshid Aghaei},
keywords = {Gas power plant (GPP), Ordered weighted averaging (OWA), Analytic hierarchy process (AHP), Fuzzy method, Self-organizing map (SOM)},
abstract = {The purpose of this research is to determine the optimum location to construct gas power plants (GPPs) in semi-arid regions. A combination of ordered weight averaging (OWA) and analytic hierarchy process (AHP) method named OWA-AHP is utilized to prepare land suitability maps with different risk scenarios when fuzzy method is to homogenize inputs. In the proposed method, AHP is to weigh each different parameter while OWA considers risk levels. In order to validate the accuracy of the proposed method, the receiver operating characteristics (ROC) curve is investigated. Besides, the self-organizing map (SOM) algorithm and Pearson correlation are used to determine the most important parameters for constructing GPP. According to obtained results, the areas located in the north and parts of the east and south of the selected case study (about 15%) at all risk levels are the optimum areas to be hosted for GPPs. Furthermore, ROC curve shows that the Area Under the Curve (AUC) values are high for both AHP and OWA-AHP methods (AUC Fuzzy-AHP = 94.0%, AUC OWA = 89.0%). The results of the SOM algorithm and Pearson correlation with high accuracy (RModel1: 0.853 and RModel2: 0.940) also depict that distance to the pipeline and road are the most important parameters to identify suitable locations for GPP.}
}
@article{KANKANAMGE2020101360,
title = {Determining disaster severity through social media analysis: Testing the methodology with South East Queensland Flood tweets},
journal = {International Journal of Disaster Risk Reduction},
volume = {42},
pages = {101360},
year = {2020},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2019.101360},
url = {https://www.sciencedirect.com/science/article/pii/S2212420919307940},
author = {Nayomi Kankanamge and Tan Yigitcanlar and Ashantha Goonetilleke and Md. Kamruzzaman},
keywords = {Social media, Data analytics, Big data, Crowdsourcing, Volunteered geographic information, South East Queensland Floods},
abstract = {Social media was underutilised in disaster management practices, as it was not seen as a real-time ground level information harvesting tool during a disaster. In recent years, with the increasing popularity and use of social media, people have started to express their views, experiences, images, and video evidences through different social media platforms. Consequently, harnessing such crowdsourced information has become an opportunity for authorities to obtain enhanced situation awareness data for efficient disaster management practices. Nonetheless, the current disaster-related Twitter analytics methods are not versatile enough to define disaster impacts levels as interpreted by the local communities. This paper contributes to the existing knowledge by applying and extending a well-established data analysis framework, and identifying highly impacted disaster areas as perceived by the local communities. For this, the study used real-time Twitter data posted during the 2010–2011 South East Queensland Floods. The findings reveal that: (a) Utilising Twitter is a promising approach to reflect citizen knowledge; (b) Tweets could be used to identify the fluctuations of disaster severity over time; (c) The spatial analysis of tweets validates the applicability of geo-located messages to demarcate highly impacted disaster zones.}
}
@article{CAIZA2020e03706,
title = {Fog computing at industrial level, architecture, latency, energy, and security: A review},
journal = {Heliyon},
volume = {6},
number = {4},
pages = {e03706},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e03706},
url = {https://www.sciencedirect.com/science/article/pii/S240584402030551X},
author = {Gustavo Caiza and Morelva Saeteros and William Oñate and Marcelo V. Garcia},
keywords = {Computer science, Industry 4.0, Cloud computing, Fog nodes, Fog computing, Smart factories},
abstract = {The industrial applications in the cloud do not meet the requirements of low latency and reliability since variables must be continuously monitored. For this reason, industrial internet of things (IIoT) is a challenge for the current infrastructure because it generates a large amount of data making cloud computing reach the edge and become fog computing (FC). FC can be considered as a new component of Industry 4.0, which aims to solve the problem of big data, reduce energy consumption in industrial sensor networks, improve the security, processing and storage real-time data. It is a promising growing paradigm that offers new opportunities and challenges, beside the ones inherited from cloud computing, which requires a new heterogeneous architecture to improve the network capacity for delivering edge services, that is, providing computing resources closer to the end user. The purpose of this research is to show a systematic review of the most recent studies about the architecture, security, latency, and energy consumption that FC presents at industrial level and thus provide an overview of the current characteristics and challenges of this new technology.}
}
@article{TUBALLA2016710,
title = {A review of the development of Smart Grid technologies},
journal = {Renewable and Sustainable Energy Reviews},
volume = {59},
pages = {710-725},
year = {2016},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2016.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364032116000393},
author = {Maria Lorena Tuballa and Michael Lochinvar Abundo},
keywords = {Smart Grid, Smart Grid technologies, Intelligent grid, Electricity grids, Power systems},
abstract = {Energy sustainability and environmental preservation have become worldwide concerns with the many manifestations of climate change and the continually increasing demand for energy. As cities and nations become more technologically advanced, electricity consumption rises to levels that may no longer be manageable if left unattended. The Smart Grid offers an answer to the shift to more sustainable technologies such as distributed generation and microgrids. A general public awareness and adequate attention from potential researchers and policy makers is crucial. This paper presents an overview of the Smart Grid with its general features, functionalities and characteristics. It presents the Smart Grid fundamental and related technologies and have identified the research activities, challenges and issues. It demonstrates how these technologies have shaped the modern electricity grid and continued to evolve and strengthen its role in the better alignment of energy demand and supply. Smart Grid implementation and practices in various locations are also unveiled. Concrete energy policies facilitate Smart Grid initiatives across the nations. Interestingly, Smart Grid practices in different regions barely indicate competition but rather an unbordered community of similar aspirations and shared lessons.}
}
@article{WANG2021270,
title = {New Paradigm of Data-Driven Smart Customisation through Digital Twin},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {270-280},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301333},
author = {Xingzhi Wang and Yuchen Wang and Fei Tao and Ang Liu},
keywords = {Digital twin, Customisation, Smart manufacturing, Personalisation, Product Service system},
abstract = {Big data is one of the most important resources for the promotion of smart customisation. With access to data from multiple sources, manufacturers can provide on-demand and customised products. However, existing research of smart customisation has focused on data generated from the physical world, not virtual models. As physical data is constrained by what has already occurred, it is limited in the identification of new areas to improve customer satisfaction. A new technology called digital twin aims to achieve this integration of physical and virtual entities. Incorporation of digital twin into the paradigm of existing data-driven smart customisation will make the process more responsive, adaptable and predictive. This paper presents a new framework of data-driven smart customisation augmented by digital twin. The new framework aims to facilitate improved collaboration of all stakeholders in the customisation process. A case study of the elevator industry illustrates the efficacy of the proposed framework.}
}
@article{ZHUANG2021102075,
title = {The connotation of digital twin, and the construction and application method of shop-floor digital twin},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {68},
pages = {102075},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102075},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302854},
author = {Cunbo Zhuang and Tian Miao and Jianhua Liu and Hui Xiong},
keywords = {Digital twin, Digital twin shop-floor, Shop-floor digital twin, Cyber-physical systems, Smart manufacturing, Markov chain},
abstract = {Digital twin (DT) technology provides a novel, feasible, and clear implementation path for the realization of smart manufacturing and cyber-physical systems (CPS). Currently, DT is applied to all stages of the product lifecycle, including design, production, and service, although its application in the production stage is not yet extensive. Shop-floor digital twin (SDT) is a digital mapping model of the corresponding physical shop-floor. How to build and apply SDT has always been challenging when applying DT technology in the production phase. To address the existing problems, this paper first reviews the origin and evolution of DT, including its application status in the production stage. Then, an implementation framework for the construction and application of SDT is proposed. Three key implementation techniques are explained in detail: the five-dimensional modeling of SDT; DT-based 3D visual and real-time monitoring of shop-floor operating status; and prediction of shop-floor operating status based on SDT using Markov chain. A DT-based visual monitoring and prediction system (DT-VMPS) for shop-floor operating status is developed, and the feasibility and effectiveness of the proposed method are demonstrated through the use of an engineering case study. Finally, a summary of the contributions of the paper is given, and future research issues are discussed.}
}
@article{RANA2021100294,
title = {Vibration based pavement roughness monitoring system using vehicle dynamics and smartphone with estimated vehicle parameters},
journal = {Results in Engineering},
volume = {12},
pages = {100294},
year = {2021},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2021.100294},
url = {https://www.sciencedirect.com/science/article/pii/S2590123021000955},
author = {Shohel Rana and  Asaduzzaman},
keywords = {Vibration, Pavement condition monitoring, Vehicle dynamics, Vehicle system identification, Grey-box model, Smartphone},
abstract = {Due to the high cost associated with pavement roughness monitoring, researches are going on to develop new techniques of pavement monitoring which are cost effective, frequent and easily implementable. Conventionally, pavement monitoring is performed using road profilers consisting of laser and inertial sensors. Though these profilers provide measurements of high accuracy, the high operating cost of these devices makes it difficult to ensure regular pavement monitoring. With the development of technology, new researches are being performed for the development of low-cost pavement monitoring systems using ordinary vehicle and smartphones. However, in most of the studies, the effect of vehicle suspension system on pavement monitoring was not considered and also direct reconstruction of pavement profile was not performed to compute the International Roughness Index (IRI), widely used for pavement roughness monitoring system. Instead, some correlation based procedures have been followed for measurement of pavement roughness. In this paper, a vibration based method for monitoring of pavement condition is introduced using ordinary vehicle and smartphone as vibration sensors which includes calibration for vehicle suspension system. The whole process can be divided into two stages. The unknown dynamic parameters of the ordinary vehicle are identified first. The vehicle is driven over a speed bump of known dimension and vehicle vertical acceleration is recorded. State-space representation of the vehicle model with unknown parameters is performed and the parameters are estimated using Grey-Box model system identification technique. Secondly, a new pavement profile reconstruction technique is proposed based on inverse formulation of vehicle dynamics. The vertical acceleration of the vehicle with known parameters are collected by driving it over the pavement, the condition of which needs to be monitored. A new mathematical formulation of vehicle dynamics is developed and using the collected acceleration data, pavement profile is reconstructed. Further, International Roughness Index of the reconstructed pavement is calculated. Numerical simulations and field testing of the proposed method are performed for various pavement roughness conditions and vehicle speeds performed for determining its viability for pavement condition monitoring.}
}
@article{ALARIFI2021101036,
title = {Automated control scheduling to improve the operative performance of smart renewable energy systems},
journal = {Sustainable Energy Technologies and Assessments},
volume = {45},
pages = {101036},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101036},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821000461},
author = {Abdulaziz Alarifi and Ahmad {Ali AlZubi} and Osama Alfarraj and Ayed Alwadain},
keywords = {Automation, Control scheduling, Q-Learning, Renewable energy resources},
abstract = {Smart renewable energy systems are designed as an alternative to conventional power generation methods that utilize natural resources. The development of automation and its allied technologies has improved the performance and operations of smart energy systems in recent years. In this article, automated control scheduling (ACS) for balancing power generation and deficiency in renewable energy systems is introduced. This scheduling control is supported by deep Q-learning to define the operation and shortage state of energy systems. Based on the definitive control schedule, the functions of the systems can be modeled without requiring additional time for power generation and dissemination. In addition, the power generation process relies on environmental conditions to gain maximum profitable power for the operating cost of the power systems. The energy generation system analyzes the environmental conditions suitable for achieving maximum profit, and power dissemination is also modeled. The performance of the proposed system was analyzed using the following metrics: operational profit, imbalance factor, successful prediction of operations, and time lag.}
}
@article{NING2019101,
title = {A novel ontology consistent with acknowledged standards in smart homes},
journal = {Computer Networks},
volume = {148},
pages = {101-107},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618311964},
author = {Huansheng Ning and Feifei Shi and Tao Zhu and Qingjuan Li and Liming Chen},
keywords = {Semantic, Ontology, Standards, Smart homes, Internet of Things},
abstract = {With the development of Internet of Things, the Smart Home equipped with various sensors and devices has become a hot area attracting global attention and concern. In order to get a better understanding of ambient environments, adding semantics to sensor data plays a significant role. Researchers are attempting to build semantic models in order to satisfy their own requirements, which leads to little reusability between different models. This paper aims to provide a novel ontology which follows publicly acknowledged standards for achieving sensor data semantization in smart homes, including modeling sensors, context and activities with semantics. For keeping consistent with current accepted standards, the proposed ontology is based on the Semantic Sensor Network Ontology. In addition, we enrich the ontologies by incorporating spatiotemporal information and user profiles. The ontology is designed using Protégé and a use case is demonstrated to show the great potentiality in daily activity recognition in smart homes.}
}
@article{CARDENAS201749,
title = {ProximiThings: Implementing Proxemic Interactions in the Internet of Things},
journal = {Procedia Computer Science},
volume = {113},
pages = {49-56},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.286},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316952},
author = {Carlos Cardenas and J. Antonio Garcia-Macias},
keywords = {Internet of Things, proxemics, human-centric systems},
abstract = {Proxemic dimensions can help mediate interactions between entities (people, objects) in Internet of Things environments and constitute an important element in the construction of proactive, context-aware and human-centric systems. However, as evidenced by the literature, little attention has been paid to implementing proxemic interactions in IoT platforms. We present ProximiThings, a framework for facilitating the incorporation of proxemic capabilities in IoT-based systems. We describe the construction of various prototypes to show how ProximiThings can be used by developers in real settings.}
}
@article{BRUNS2019186,
title = {Learning of complex event processing rules with genetic programming},
journal = {Expert Systems with Applications},
volume = {129},
pages = {186-199},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302386},
author = {Ralf Bruns and Jürgen Dunkel and Norman Offel},
keywords = {Genetic programming, Complex event processing, Rule learning, Pattern mining},
abstract = {Complex Event Processing (CEP) is an established software technology to extract relevant information from massive data streams. Currently, domain experts have to determine manually CEP rules that define a situation of interest. However, often CEP rules cannot be formulated by experts, because the relevant interdependencies and relations between the data are not explicitly known, but inherently hidden in the data streams. To cope with this problem, we present a new learning approach for CEP rules, which is based on Genetic Programming. We discuss in detail the different building blocks of Genetic Programming and how to adjust them to CEP rule learning. Extensive evaluations with synthetic and real world data demonstrate the high potential of the approach and give some hints about the choice of suitable process parameters.}
}
@article{KAANICHE2020102807,
title = {Privacy enhancing technologies for solving the privacy-personalization paradox: Taxonomy and survey},
journal = {Journal of Network and Computer Applications},
volume = {171},
pages = {102807},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102807},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520302794},
author = {Nesrine Kaaniche and Maryline Laurent and Sana Belguith},
keywords = {Privacy enhancing technologies, Recommendation services, Web-search engines, Pervasive applications, Location-based services, Profile-based services, Cryptographic trends, Secure communications, Anonymous certification, Private information retrieval, Secure multi-party computation, Homomorphic encryption, Trust models},
abstract = {Personal data are often collected and processed in a decentralized fashion, within different contexts. For instance, with the emergence of distributed applications, several providers are usually correlating their records, and providing personalized services to their clients. Collected data include geographical and indoor positions of users, their movement patterns as well as sensor-acquired data that may reveal users' physical conditions, habits and interests. Consequently, this may lead to undesired consequences such as unsolicited advertisement and even to discrimination and stalking. To mitigate privacy threats, several techniques emerged, referred to as Privacy Enhancing Technologies, PETs for short. On one hand, the increasing pressure on service providers to protect users' privacy resulted in PETs being adopted. One the other hand, service providers have built their business model on personalized services, e.g. targeted ads and news. The objective of the paper is then to identify which of the PETs have the potential to satisfy both usually divergent - economical and ethical - purposes. This paper identifies a taxonomy classifying eight categories of PETs into three groups, and for better clarity, it considers three categories of personalized services. After defining and presenting the main features of PETs with illustrative examples, the paper points out which PETs best fit each personalized service category. Then, it discusses some of the inter-disciplinary privacy challenges that may slow down the adoption of these techniques, namely: technical, social, legal and economic concerns. Finally, it provides recommendations and highlights several research directions.}
}
@article{SCHATTEN201631,
title = {A roadmap for scalable agent organizations in the Internet of Everything},
journal = {Journal of Systems and Software},
volume = {115},
pages = {31-41},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216000170},
author = {Markus Schatten and Jurica Ševa and Igor Tomičić},
keywords = {Organizational design, Large-scale multiagent system, Internet of Things},
abstract = {Computing is increasingly ubiquitous, with everyday items including smartphones, cars, clothes and household appliances gaining increasingly sophisticated computing and communication capacities. With the development of the Internet of Things, it is just a matter of time before devices have to collaborate and compete with each other, in order to provide better services to mankind. These embedded software systems are increasingly autonomous and connected, and can thus be modeled as multiagent systems (MAS). Only 30 years ago it was science fiction that over a billion people will exchange billions of e-mails on a daily basis. Today a scenario of millions of collaborating agents sometimes embedded in gadgets and appliances, sometimes in form of networked and big data services, may also sound futuristic. However given the current rate of development in electronics, we will soon have to manage large scale MAS where millions of agents exist, collaborate and compete. Organization theory provides the necessary methodology to approach complex systems in order to design, implement and strategically manage them towards success. In this paper a state-of-the-art on organizational design techniques for large-scale MAS will be presented, missing advancements will be identified and a roadmap for future developments and application scenarios will be provided.}
}
@article{DAO2020106655,
title = {Reliable broadcasting for safety services in dense infrastructureless peer-aware communications},
journal = {Reliability Engineering & System Safety},
volume = {193},
pages = {106655},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106655},
url = {https://www.sciencedirect.com/science/article/pii/S095183201830471X},
author = {Nhu-Ngoc Dao and Duc-Nghia Vu and Arooj Masood and Woongsoo Na and Sungrae Cho},
keywords = {Reliable broadcast, Safety service, Infrastructureless peer-aware communication, IEEE 802.15.8},
abstract = {The IEEE 802.15.8 project has introduced peer-aware communication (PAC) as a promising technology enabling high-quality proximity services in dense infrastrutureless ad hoc environments. PAC applications cover a variety of high-assurance services, especially those related to safety such as hazard alerts, emergency exit guidance, and relative position in cases where networking facilities have been destroyed or unavailable. To support these services effectively, PAC must overcome the massive device density to provide a reliable broadcast protocol for rapidly disseminating urgent information across the entire network. As such, we propose a reliable rumor broadcast (RRB) scheme for safety services in a dense infrastructureless PAC network. The proposed RRB scheme takes advantage of neighboring relations among PAC devices (PDs) to broadcast rumor abstract information instead of transmitting heavy broadcast frames. The broadcast frames are only forwarded based on requests from neighboring PDs. In the RRB scheme, rumor frames aim to reduce the broadcasting overhead, while the use of neighboring relations ensures reliable communications across the entire network. Simulation analysis demonstrates that the proposed RRB scheme achieves outstanding performance compared with that of existing algorithms in terms of overhead reduction and energy efficiency while maintaining a better transmission reliability improvement.}
}
@article{MUKHERJEE2018688,
title = {Flexible IoT security middleware for end-to-end cloud–fog communication},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {688-703},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311470},
author = {Bidyut Mukherjee and Songjie Wang and Wenyi Lu and Roshan Lal Neupane and Daniel Dunn and Yijie Ren and Qi Su and Prasad Calyam},
keywords = {IoT security middleware, Mobile edge cloud, Cloud–fog communication, Secure IoT applications},
abstract = {IoT (Internet of Things) based smart devices such as sensors have been actively used in edge clouds i.e., ‘fogs’ along with public clouds. They provide critical data during scenarios ranging from e.g., disaster response to in-home healthcare. However, for these devices to work effectively, end-to-end security schemes for the device communication protocols have to be flexible and should depend upon the application requirements as well as the resource constraints at the network-edge. In this paper, we present the design and implementation of a flexible IoT security middleware for end-to-end cloud–fog communications involving smart devices and cloud-hosted applications. The novel features of our middleware are in its ability to cope with intermittent network connectivity as well as device constraints in terms of computational power, memory, energy, and network bandwidth. To provide security during intermittent network conditions, we use a ‘Session Resumption’ algorithm in order for our middleware to reuse encrypted sessions from the recent past, if a recently disconnected device wants to resume a prior connection that was interrupted. In addition, we describe an ‘Optimal Scheme Decider’ algorithm that enables our middleware to select the best possible end-to-end security scheme option that matches with a given set of device constraints. Experiment results show how our middleware implementation also provides fast and resource-aware security by leveraging static properties i.e., static pre-shared keys (PSKs) for a variety of IoT-based application requirements that have trade-offs in higher security or faster data transfer rates.}
}
@article{LEE2021421,
title = {MPdist-based missing data imputation for supporting big data analyses in IoT-based applications},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {421-432},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002375},
author = {Gyeong Ho Lee and Jaeseob Han and Jun Kyun Choi},
keywords = {Internet of Things, Big data analyses, Data management, Missing data, Imputation, Univariate time series, MPdist},
abstract = {Recent years have witnessed an enormous growth in the number of wireless IoT devices and thereby Internet of Things (IoT) is classified as one of the novel cutting edge technologies that has redesigned the traditional industry into a smart industry, given its applicability for a wide range of applications in providing data-driven decision-making. Meanwhile, the data with missing values is still emerging as one of the longstanding challenges in an IoT architecture due to the common-mode failures, potentially leading to both bias and loss of precision. In spite of the fact that numerous techniques have been developed for imputing missing values, the major issue in terms of imputation precision or computational complexity for large missing subsequences is still a matter under debate. To address this issue, this paper proposes a newly developed algorithm called MP-BMDI that ensures high imputation performance for supporting big data analyses in IoT-based applications, where the absence of large missing subsequences is fully required to offer unbiased results. In our approach, we initially seek a finite number of subsequences that are mostly similar to the subsequence before the missing values, then adjust the height of these following subsequences to optimal locations. Once the most proper subsequence for replacing is chosen among them based on the pattern score function PSF(r) introduced in this paper, the missing gap is completely filled by the corresponding subsequence. Numerical results are here presented to validate the merits of the proposed algorithm compared to the alternative benchmark approaches by leveraging sensor data collected from real-time environmental monitoring and deliver significant insights on the effectiveness of the proposed algorithm from various perspectives.}
}
@article{LE2020111689,
title = {Heart rate extraction based on eigenvalues using UWB impulse radar remote sensing},
journal = {Sensors and Actuators A: Physical},
volume = {303},
pages = {111689},
year = {2020},
issn = {0924-4247},
doi = {https://doi.org/10.1016/j.sna.2019.111689},
url = {https://www.sciencedirect.com/science/article/pii/S0924424719311318},
author = {Minhhuy Le},
keywords = {UWB radar, Vital sign, Remote sensing, PCA, Eigenvalues},
abstract = {Ultra wideband impulse (UWB) radar for vital signs measurement applications is getting much attention because it is a contactless method. However, the heartbeat signal is usually be perturbed in strong noisy signals from stationary and non-stationary objects. In addition, the respiration signal, which has a much higher intensity than the heartbeat signal, makes it challenging to extract the heartbeat signal. Thus, this paper proposes an extraction method of heartbeat signal and frequency using eigenvalues. Eigenvalues of the vital signal were found to be strongly correlated with the fundamental and harmonic frequencies of respiration and heartbeat. Experimental results show that the proposed method could improve the signal-to-noise ratio of over 18 dB for different distances from 0.5 m to 3.0 m and different orientations of the person. Furthermore, the original signal of respiration and heartbeat can be reconstructed by the selection of the eigenvalues and the eigenvectors.}
}
@article{JING2021102518,
title = {A blockchain-based code copyright management system},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102518},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102518},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000273},
author = {Nan Jing and Qi Liu and Vijayan Sugumaran},
keywords = {Blockchain, Copyright management application, Node cooperation},
abstract = {With the increasing number of open-source software projects, code plagiarism has become one of the threats to the software industry. However, current research on code copyright protection mostly focuses on the approach for code plagiarism detection, failing to fundamentally solve the problem of copyright confirmation and protection. This paper proposes a blockchain-based code copyright management system. Firstly, an Abstract Syntax Tree-based code originality verification model is constructed. The originality of the uploaded code is determined through its similarity to other original codes. Secondly, the Peer-to-Peer blockchain network is designed to store the copyright information of the original code. The nodes in the blockchain network can verify the originality of the code based on the code originality verification model. Then, through the construction of blocks and legitimacy validation and linking of blocks, the blockchain-based code copyright management structure is built. The whole process guarantees that the copyright information is traceable and will not be tampered with. According to the experiments, the accuracy and processing time of the code originality verification model are shown to meet code originality verification requirements. The experiment also shows that the best storage type of the code copyright information is the code fingerprint which is a 256bits hash value converted from code eigenvalues. It performs better in both response speed and storage efficiency. Moreover, because of the uniqueness and irreversibility of the result from the SHA256 algorithm, the code fingerprint storage yields a better level of storage security. In summary, this paper proposes a blockchain-based code copyright management system which provides better response speed and storage efficiency.}
}
@article{TANG201970,
title = {Dynamic resource allocation strategy for latency-critical and computation-intensive applications in cloud–edge environment},
journal = {Computer Communications},
volume = {134},
pages = {70-82},
year = {2019},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2018.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0140366418304687},
author = {Hengliang Tang and Chunlin Li and Jingpan Bai and JianHang Tang and Youlong Luo},
keywords = {Resource allocation, cloud–edge environment, Resource scheduling, Resource matching},
abstract = {Edge computing is more and more popular due to its low latency and bandwidth-efficient services. Edge computing is mainly applied to the latency-critical and computation-intensive application. However, there are several challenges to the improvement on the quality of service in edge computing environment. For instance, the reduction of server latency, the network transmission efficiency, etc. In this paper, we propose the dynamic resource allocation algorithm for cloud–edge environment. The dynamic resource allocation algorithm consists of the resource scheduling algorithm and the resource matching algorithm. In the resource scheduling algorithm, a resource scheduling problem can be obtained according to the stored penalty of scheduling contents, the value of scheduling contents and the transmission cost of scheduling contents. Then, tabu search algorithm is applied to find the optimal solution to the resource scheduling problem. Furthermore, the resources are scheduled into the edge servers from cloud datacenter with the optimal solution. In the resource matching algorithm, an optimization problem of the resource matching is built with respect to the resource location, the task priorities and the network transmission cost. For addressing this problem, the optimal problem is converted to an optimal matching problem of the weighted bipartite graph. Moreover, an optimal matching problem of the weighted complete bipartite graph is created by adding the spurious containers. Then, the optimal strategy of the resource matching for tasks on the edge servers is achieved. Finally, the performance of the proposed algorithms and some typical resource allocation algorithms is evaluated via extensive experiments. The results indicate that proposed algorithms can effectively reduce network delay and enhance QoS.}
}
@article{RAMAKRISHNAN2014207,
title = {Enabling Self-learning in Dynamic and Open IoT Environments},
journal = {Procedia Computer Science},
volume = {32},
pages = {207-214},
year = {2014},
note = {The 5th International Conference on Ambient Systems, Networks and Technologies (ANT-2014), the 4th International Conference on Sustainable Energy Information Technology (SEIT-2014)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.416},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914006164},
author = {Arun kishore Ramakrishnan and Davy Preuveneers and Yolande Berbers},
keywords = {Internet of Things (IoT), Self-learning, correlation mining, Kullback-Leibler divergence, apriori frequent set mining},
abstract = {The next big thing in computing is the Internet of Things (IoT), referring to dynamic and ever-evolving environments, generating high-volume streams of heterogeneous yet correlated contextual information of varying quality and complexity. Moreover, the increase in user mobility and unreliable sensor availability in IoT, necessitates the context-aware applications to dynamically adapt their behavior at run time. In this paper, we elicit the need for different self-learning techniques to tackle the openness of the IoT environments and propose enabling algorithms to achieve them. First, we present possible application scenarios which can benefit from both supervised and unsupervised self-learning. Later, we propose correlation mining algorithms based on Kullback-Leibler (KL) divergence and frequent set mining that exploits correlated contexts to enable unsupervised self-learning. These algorithms help to identify alternate sources for a context and semantically describe the previously unseen contexts in terms of already known contexts. We have realized the proposed algorithms on top of a Bayesian framework (HARD-BN) which supports autonomous learning. Our experiments demonstrate the applicability of the proposed correlation mining algorithms and their feasibility to enable self-learning in open and ever-evolving IoT environments.}
}
@article{PATEL2021,
title = {Operating system support, protocol stack with key concerns and testbed facilities for IoT: A case study perspective},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821000021},
author = {Bimal Patel and Parth Shah},
keywords = {Internet of Things (IoT), Operating system, Design characteristics, Protocol stack, Testbed implementation, Developer concern, IoT integration},
abstract = {In terms of heterogeneous devices and sensors, man and machine collaborate seamlessly, giving birth to the Internet of People, Internet of Things, and Internet of the Future. Within a short period, 30 billion intelligent devices in the form of smart applications will get connected to make an individual's life smoother, more comfortable, faster and accessible from anywhere at any time. IoT combines the power of IPv6 for network connectivity, sensing and nextgen communication technologies to meet future demands. Due to constraint environment in terms of memory, computation, and energy, this review paper will provide in-depth analysis of different OS'es. Candidate OS and Simulator is selected, keeping in mind various barriers and critical design characteristics. By doing this survey, it is possible for other research communities to make an appropriate choice for OS and making IoT a reality. Finally, the case study is also discussed which provides protocol stack implementation details along with deployable testbed, its size in terms of nodes supported and other security challenges. Critical preference level in terms of programming language and application areas from a developer perspective is also analysed and discussed to provide further research direction. IoT integration with future domains, along with various challenges, is explored at the end.}
}
@article{GILANI2020107099,
title = {A review of ontologies within the domain of smart and ongoing commissioning},
journal = {Building and Environment},
volume = {182},
pages = {107099},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.107099},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320304741},
author = {Sara Gilani and Caroline Quinn and J.J. McArthur},
keywords = {Building management system, Building information model, Data structure, Key performance indicator, Performance improvement, Fault detection and diagnosis},
abstract = {Smart and ongoing commissioning (SOCx) of buildings can result in a significant reduction in the gap between design and operational performance. Ontologies play an important role in SOCx as they facilitate data readability and reasoning by machines. A better understanding of ontologies is required in order to develop and incorporate them in SOCx. This paper critically reviews the state-of-the-art research on building data ontologies since 2014 within the SOCx domain through sorting them based on building data types, general approaches, and applications. The data types of two main domains of building information modeling and building management system have been considered in the majority of existing ontologies. Three main applications are evident from a critical analysis of existing ontologies: (1) key performance indicator calculation, (2) building performance improvement, and (3) fault detection and diagnosis. The key gaps found in the literature review are a holistic ontology for SOCx and insight on how such approaches should be evaluated. Based on these findings, this study provides recommendations for future necessary research including: identification of SOCx-related data types, assessment of ontology performance, and creation of open-source approaches.}
}
@article{TAN2021101546,
title = {Adaptive governance of autonomous vehicles: Accelerating the adoption of disruptive technologies in Singapore},
journal = {Government Information Quarterly},
volume = {38},
number = {2},
pages = {101546},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101546},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20303257},
author = {Si Ying Tan and Araz Taeihagh},
keywords = {Autonomous vehicles, Driverless cars, Adaptive, Governance, Singapore, Case study},
abstract = {Despite their promise, there have been discussions surrounding the technological risks of autonomous vehicles (AVs) and the extent to which AVs are ready for large-scale deployment. Using a case study approach, this article examines the development and implementation of AVs in Singapore. Our findings reveal that AV regulatory sandboxes, the formalisation of safety assessments and the release of technical guidelines are some of the most adaptive and innovative instruments that have been adopted to govern AVs in Singapore. Furthermore, Singapore's approach to AVs has applied an adaptive strategy that is both pre-emptive and responsive. The accelerated expansion of trials and regulatory provisions for AVs demonstrates Singapore's aspiration to be nimble, and showcases the simultaneous adoption of two contrasting implementation approaches – prescriptive and experimentalist – to guide AV adoption. The regulatory lessons derived from the governance of AVs in Singapore could provide useful policy guidance, and could inform policy discussions of AVs as well as other autonomous systems.}
}
@article{FROEHLICH2021,
title = {How space can support African civil societies: Security, peace, and development through Efficient Governance Supported by space applications},
journal = {Acta Astronautica},
year = {2021},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0094576521003039},
author = {Annette Froehlich and Nicolas Ringas and James Wilson},
keywords = {Africa, Sustainable development goals, Outer space, Agenda 2063, Primary needs, E-governance},
abstract = {This study investigates the vast potential for space-based technologies and applications in supporting governance to strengthen civil society, bolster democratic processes, and allow for socio-economic development throughout Africa. It offers a comprehensive analysis of African governance levels, existing e-governance systems, African challenges relating to e-governance adoption, and how e-governance solutions can assist in realising the continent's development goals. First, the study explores the integral role of good governance in achieving the UN Agenda 2030 Sustainable Development Goals (SDGs) and the AU's Agenda 2063 aspirations. Subsequently, the existing African governance structures established at a continental level are described to provide an overview of the frameworks informing governance within Africa. Specific challenges that need to be overcome to ensure good governance are then addressed and the current governance levels in Africa are evaluated. An assessment of e-government readiness levels both globally and within Africa is offered through an examination of the United Nations E-Government Survey from 2018. Specific barriers to e-government, both abroad and within Africa, are then analysed along with the associated risks relating to e-government systems. It then investigates how digital divides and lack of internet connectivity within Africa can be addressed by new technologies and commitments from both terrestrial and space actors. The opportunities afforded by rapidly growing satellite constellations being deployed to increase connectivity levels and reduce the costs thereof are then analysed in detail and the various planned constellations are assessed on a technical level. Lastly, the potential for Earth Observation and remote sensing data to assist with e-governance activities across sectors including healthcare, education, agriculture, water and sanitation, and emergency responses is investigated.}
}
@article{WANG20197444,
title = {Adaptive synchronization of complex networks with general distributed update laws for coupling weights},
journal = {Journal of the Franklin Institute},
volume = {356},
number = {13},
pages = {7444-7465},
year = {2019},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2019.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S0016003219304971},
author = {Liliang Wang and Zhiyong Sun and Yue Cao},
abstract = {This paper discusses adaptive synchronization control for complex networks interacted in an undirected weighted graph, and aims to provide a novel and general approach for the design of distributed update laws for adaptively adjusting coupling weights. The proposed updating laws are very general in the sense that they encompass most weight update laws reported in the literature as special cases, and also provide new insights in the analysis of network system evolution and graph weight convergence. We show a rigorous proof for the synchronization stability of the overall complex network to a synchronized state, and demonstrate the convergence of adaptive weights for each edge to some bounded constants. A detailed comparison with available results is provided to elaborate the new features and advantages of the proposed adaptive strategies as compared with conventional adaptive laws. The effectiveness of the proposed approach is also validated by several typical simulations.}
}
@article{BLAIR2019104521,
title = {Models of everywhere revisited: A technological perspective},
journal = {Environmental Modelling & Software},
volume = {122},
pages = {104521},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.104521},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218303773},
author = {Gordon S. Blair and Keith Beven and Rob Lamb and Richard Bassett and Kris Cauwenberghs and Barry Hankin and Graham Dean and Neil Hunter and Liz Edwards and Vatsala Nundloll and Faiza Samreen and Will Simm and Ross Towe},
keywords = {Environmental modelling, Models of everywhere, Grid computing, Cloud computing, Data science},
abstract = {The concept ‘models of everywhere’ was first introduced in the mid 2000s as a means of reasoning about the environmental science of a place, changing the nature of the underlying modelling process, from one in which general model structures are used to one in which modelling becomes a learning process about specific places, in particular capturing the idiosyncrasies of that place. At one level, this is a straightforward concept, but at another it is a rich multi-dimensional conceptual framework involving the following key dimensions: models of everywhere, models of everything and models at all times, being constantly re-evaluated against the most current evidence. This is a compelling approach with the potential to deal with epistemic uncertainties and non-linearities. However, the approach has, as yet, not been fully utilised or explored. This paper examines the concept of models of everywhere in the light of recent advances in technology. The paper argues that, when first proposed, technology was a limiting factor but now, with advances in areas such as Internet of Things, cloud computing and data analytics, many of the barriers have been alleviated. Consequently, it is timely to look again at the concept of models of everywhere in practical conditions as part of a trans-disciplinary effort to tackle the remaining research questions. The paper concludes by identifying the key elements of a research agenda that should underpin such experimentation and deployment.}
}
@article{CASALE201634,
title = {Current and Future Challenges of Software Engineering for Services and Applications},
journal = {Procedia Computer Science},
volume = {97},
pages = {34-42},
year = {2016},
note = {2nd International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.278},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916320944},
author = {Giuliano Casale and Cristina Chesta and Peter Deussen and Elisabetta {Di Nitto} and Panagiotis Gouvas and Sotiris Koussouris and Vlado Stankovski and Andreas Symeonidis and Vlassis Vlassiou and Anastasios Zafeiropoulos and Zhiming Zhao},
keywords = {Software, Services, Research Challenges, Collaboration, Software Development},
abstract = {ICT (Information and Communication Technology) and, in particular, software is more and more pervasive and it cannot be considered anymore as a minor element of a complex systems. In domains like cloud, big data, IoT (Internet of Things), CPS (Cyber-Physical Systems) it is the core element. We need to consolidate the software engineering discipline, which, despite the impressive achievements in the area of software technology, is probably one of the youngest scientific and technological disciplines with about 60 years of history. This paper summarizes the challenges that the Software Engineering for Services and Applications (SE4SA) cluster is considering as relevant.}
}
@article{ELAGGOUNE2020465,
title = {A fuzzy agent approach for smart data extraction in big data environments},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {4},
pages = {465-478},
year = {2020},
note = {Emerging Software Systems},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819302010},
author = {Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough},
keywords = {Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data},
abstract = {The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.}
}
@article{ZHU2022103671,
title = {A method of estimating the spatiotemporal distribution of reflected sunlight from glass curtain walls in high-rise business districts using street-view panoramas},
journal = {Sustainable Cities and Society},
volume = {79},
pages = {103671},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103671},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722000051},
author = {Haipeng Zhu and Zongchao Gu},
keywords = {Sunlight reflection, Panoramic projection transformation, Spatiotemporal distribution estimation, Street-view data, Thermal comfort, Cooling city},
abstract = {Obtaining the time and location of sunlight reflection within a street environment is important to improve the comfort and safety of public spaces. This paper proposes a method to estimate the spatiotemporal distribution of sunlight reflection within a street based on street-view data. First, the solar path on the panorama is plotted to obtain the potential orientation of the radiation source and the reflection on building facades. Next, the continuous time–date distribution of direct and reflected sunlight at a certain site is obtained by the panoramic projection transformation approach. The accumulative duration and time-series parameters of the reflected scenario are estimated by a matrix operation. By mapping the time-specific duration of the sunlight reflection scene, we can obtain the spatiotemporal distribution within the street network. We applied the proposed method to a case study in a high-rise business district and quantified the range, duration, and intensity of reflected sunlight in the surrounding area. Seasonal changes, city morphology, and facade orientation were found to be the factors affecting the distribution tendency of reflected sunlight. The risk detection, visualization, and quantification analysis results on sunlight reflection can provide a reference for city management, urban planning, and environmental evaluation.}
}
@article{DENG2021103289,
title = {Street-level solar radiation mapping and patterns profiling using Baidu Street View images},
journal = {Sustainable Cities and Society},
volume = {75},
pages = {103289},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103289},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721005655},
author = {Mingyu Deng and Wei Yang and Chao Chen and Zhou Wu and Yong Liu and Chaocan Xiang},
keywords = {Solar radiation, Baidu Street View image, Solar radiation mapping, Radiation patterns profiling, Clustering},
abstract = {The variability of urban landscapes and weather conditions makes it challenging to quantify the solar radiation of urban streets. In this study, we propose a framework to map and profile the street-level solar radiation using Baidu Street View (BSV) images. In our framework, a BSV-based radiation estimation method considering weather conditions is established to map the hourly street-level solar radiation. Then, a two-stage clustering method is proposed to profile the radiation patterns, including daily radiation patterns (DRP) and multifaceted radiation patterns (MRP), from the estimated radiation data. Taking Chongqing as a case, results show that (1) the estimated solar radiation of the proposed method is closer to the real situation than the two existing methods. (2) The solar radiation of streets varies greatly among different road grades and streetscapes; the street environment and weather conditions have a significant impact on solar radiation in mountain cities. (3) Eight types of DRP and six types of MRP are discovered by the two-stage clustering method. Moreover, the features, geographical distribution and applications of the radiation patterns are investigated in detail to support urban planning. This work can greatly promote studies relating the solar radiation at the street-level in the future.}
}
@article{MOSKVITCH201336,
title = {Penal code: the coming world of trial by algorithm},
journal = {New Scientist},
volume = {219},
number = {2933},
pages = {36-39},
year = {2013},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(13)62195-8},
url = {https://www.sciencedirect.com/science/article/pii/S0262407913621958},
author = {Katia Moskvitch},
abstract = {We're creating a society where all-seeing intelligent algorithms can punish every wrongdoingdoer – but do we want it?}
}
@article{PANTANO2017430,
title = {‘You will like it!’ using open data to predict tourists' response to a tourist attraction},
journal = {Tourism Management},
volume = {60},
pages = {430-438},
year = {2017},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2016.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0261517716302680},
author = {Eleonora Pantano and Constantinos-Vasilios Priporas and Nikolaos Stylos},
keywords = {Open data, Online reviews, Tourism, Travel propositions},
abstract = {The increasing amount of user-generated content spread via social networking services such as reviews, comments, and past experiences, has made a great deal of information available. Tourists can access this information to support their decision making process. This information is freely accessible online and generates so-called “open data”. While many studies have investigated the effect of online reviews on tourists' decisions, none have directly investigated the extent to which open data analyses might predict tourists' response to a certain destination. To this end, our study contributes to the process of predicting tourists' future preferences via MathematicaTM, software that analyzes a large set of the open data (i.e. tourists’ reviews) that is freely available on tripadvisor. This is devised by generating the classification function and the best model for predicting the destination tourists would potentially select. The implications for the tourist industry are discussed in terms of research and practice.}
}
@article{SHABESTARI2019162,
title = {A taxonomy of software-based and hardware-based approaches for energy efficiency management in the Hadoop},
journal = {Journal of Network and Computer Applications},
volume = {126},
pages = {162-177},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303667},
author = {Fatemeh Shabestari and Amir Masoud Rahmani and Nima Jafari Navimipour and Sam Jabbehdari},
keywords = {Hadoop, Energy efficiency, MapReduce, HDFS, YARN, Big data},
abstract = {Apache Hadoop framework supports the storing and processing of big data datasets using simple programming models. Energy management has been recognized as one of the major issues in Hadoop, and many types of research have been conducted in this scope. However, despite the importance of this issue, there is no inclusive study about energy efficiency in Hadoop. In this paper, the techniques of energy efficiency in Hadoop are classified into two main categories. Moreover, the benefits and drawbacks of these methods and a systematic study of the conducted research are provided and examined in this paper. Another aim is to provide the visions for the descriptions of open issues and recommendations for future research.}
}
@article{PILLAI2020102207,
title = {Shopping intention at AI-powered automated retail stores (AIPARS)},
journal = {Journal of Retailing and Consumer Services},
volume = {57},
pages = {102207},
year = {2020},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102207},
url = {https://www.sciencedirect.com/science/article/pii/S0969698919302887},
author = {Rajasshrie Pillai and Brijesh Sivathanu and Yogesh K. Dwivedi},
keywords = {TRAM, PLS-SEM, Perceived enjoyment, Customization, Interactivity, Artificial intelligence-powered automated retail stores},
abstract = {Artificial Intelligence (AI) is transforming the way retail stores operate. AI-Powered Automated Retail Stores are the next revolution in physical retail. Consumers are facing fully automated technology in these retail stores. Therefore, it is necessary to scrutinize the antecedents of consumers' intention to shop at AI-Powered Automated Retail Stores. This study delves into this area to find the predictors of consumers’ intention to shop at AI-Powered Automated Retail Stores. It extends the technology readiness and acceptance model by the addition of AI context-specific constructs such as Perceived Enjoyment, Customization and Interactivity from the present literature. The proposed model is tested by surveying 1250 consumers & the data is analyzed using the PLS-SEM technique and empirically validated. The outcome of the study reveals that Innovativeness and Optimism of consumers affect the perceived ease and perceived usefulness. Insecurity negatively affects the perceived usefulness of AI-powered automated retail stores. Perceived ease of use, perceived usefulness, perceived enjoyment, customization and interactivity are significant predictors of shopping intention of consumers in AI-powered automated stores. This research presents insightful academic and managerial implications in the domain of retailing and technology in retail.}
}
@article{GUNDUZ2021107676,
title = {Matching functions of supply chain management with smart and sustainable Tools: A novel hybrid BWM-QFD based method},
journal = {Computers & Industrial Engineering},
volume = {162},
pages = {107676},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107676},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221005805},
author = {Mehmet Akif Gunduz and Sercan Demir and Turan Paksoy},
keywords = {Best-Worst Method, Quality Function Deployment, Supply Chain Management, Smartness, Sustainability},
abstract = {In recent years, there is a noticeable increase in interest in supply chain smartness and sustainability since a growing number of companies are adopting smart technologies and sustainable practices in the functions of supply chain management. Therefore, scholars and practitioners seek to make sense of how this phenomenon can be addressed concerning companies’ maturity level of supply chain smartness and sustainability. This paper proposes a novel hybrid methodology combining the Best-Worst Method (BWM) and Quality Function Deployment (QFD) to assess the level of maturity for supply chain smartness and sustainability by weighting the functions of supply chain management. A twin-QFD technique is used to obtain a conceptual design to determine the relationship between the functions of supply chain smartness tools and sustainability indicators to assess the level of maturity, whereas the BWM is used to determine the weights of the functions of supply chain management. A case study in the automotive manufacturing industry is applied to demonstrate the applicability of the proposed approach. The findings disclose the prominent smart technologies (simulation, big data analytics, cloud computing) and sustainability indicators (costs, lead time, and damage and loss) in integrating Industry 4.0 technologies and sustainable supply chain practices. Findings also suggest a guideline to compare the current and targeted levels of smartness and sustainability maturity. This study provides insights for scholars and practitioners and contributes to the body of knowledge by evaluating companies’ maturity of digital transformation and sustainable practices in the supply chain functions.}
}
@article{SYAMIMI2020409,
title = {VR industrial applications―A singapore perspective},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {409-420},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S209657962030070X},
author = {Athirah Syamimi and Yiwei Gong and Ryan Liew},
keywords = {Virtual reality, VR applications, Building information modeling},
abstract = {Virtual Reality (VR) has been around for a long time but has come into the spotlight only recently. From an industrial perspective, this article serves as a proverbial scalpel to dissect the different use cases and commercial applications of VR in Singapore. Before researching the Singapore market, we examine how VR has evolved. At the moment, the global annual budget for VR (and augmented reality) is at an upward trend with a leading growth in market value for the training sector. VR in Singapore has also seen a rapid development in recent years. We discuss some of the Singapore government's initiatives to promote the commercial adoption of VR for the digital economy of the nation. To address the mass adoption of VR, we present VRcollab's business solutions for the construction and building industry. 2020 is one of the most important years for VR in history.}
}
@article{SUHAIBKAMRAN2021,
title = {Role of smart materials and digital twin (DT) for the adoption of electric vehicles in India},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.09.249},
url = {https://www.sciencedirect.com/science/article/pii/S221478532106096X},
author = {Sayed {Suhaib Kamran} and Abid Haleem and Shashi Bahl and Mohd Javaid and Devaki Nandan and Ajay {Singh Verma}},
keywords = {Digital Twin (DT), Industry 4.0, Electric Vehicle (EV) industry, Electric Vehicles (EVs), Repair and maintenance, Smart Materials},
abstract = {This paper aims to integrate Industry 4.0 technologies, precisely the Digital Twin concept and smart materials, with the Electric Vehicle (EV) industry, to complement the adoption of Electric Vehicles (EVs) in the Indian market. In the current scenario, the adoption of EVs in India faces challenges ranging from insufficient charging facilities to the difficulties related to the complicated connections inside an EV, making the repair and maintenance of these vehicles quite arduous and costly. This paper specifically deals with the complexities regarding the repair and maintenance of EVs and aims to ease the process to ensure a bright future for the EV industry in the Indian markets. Throughout this paper, different challenges regarding the repair and maintenance of EVs are discussed. The solutions to overcome these challenges with the help of various Industry 4.0 technologies and smart materials will pave the way for the smooth and efficient adoption of EVs in the Indian markets. The paper also discusses the significant research challenges associated with the study and its future scope in the industry. The findings of this study enabled us to recognize a variety of smart materials that the EV industry can use in their cars which will make the maintenance of EVs much more affordable by cutting down the heavy costs incurred in getting the dents and scratches repaired if the vehicle meets an accident. Also, the use of Digital Twin in the servicing and repair of EVs will make the process a lot more convenient by reducing the service time and related costs drastically. Further in our study, we have described in detail the working process of DT and smart materials in the EV industry to achieve the goals of this study.}
}
@article{SHI2020105379,
title = {Automatic generation of meteorological briefing by event knowledge guided summarization model},
journal = {Knowledge-Based Systems},
volume = {192},
pages = {105379},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105379},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119306276},
author = {Kaize Shi and Hao Lu and Yifan Zhu and Zhendong Niu},
keywords = {Meteorological domain, Fine-tuned BERT model, Event knowledge guided summarization, EKGS model, Briefing generation framework, Meteorological decision support platform},
abstract = {In recent years, frequent meteorological disasters have brought great suffering to people. The meteorological briefing is an effective way to realize the real-time perception of extreme meteorological events, which is of great significance for decision-makers to formulate plans and provide timely assistance. Traditional meteorological briefings primarily rely on physical sensors for data collection and are organized manually. However, such an approach has the disadvantages of rigid content, high cost, and poor real-time performance. As an emerging lightweight social sensor, social networks can respond to real world events in a timely and comprehensive manner, which also makes up for the shortcomings of the traditional methods. In this paper, we present an event knowledge guided summarization (EKGS) model to automatically summarize weibo posts in the meteorological domain. Our model consists of two modules: a summary generation module and an event knowledge guidance module. The event knowledge guidance module is used to guide and constrain the content generated by the summary generation module, so that it can generate the content with core knowledge of specific events, which are 14 types of extreme meteorological events defined by the China Meteorological Administration (CMA). Compared to other baseline models, our EKGS model achieves the best test results on all metrics. In addition, we construct an automatic meteorological briefing generation framework based on the EKGS model, which has been applied as an online service to the meteorological briefing overview module of the CMA Public Meteorological Service Center.}
}
@article{DAS2021339,
title = {Generation of overlapping clusters constructing suitable graph for crime report analysis},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {339-357},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000376},
author = {Ankur Das and Janmenjoy Nayak and Bighnaraj Naik and Uttam Ghosh},
keywords = {Cybercrime, Crime report analysis, Overlapping clustering, Graph centrality measures, Unsupervised learning, Natural Language Processing},
abstract = {Cybercrime is a kind of criminal activity generally committed by cybercriminals or hackers. Crime activities are growing explosively all over the world which motivates the law enforcement agencies for systematic analysis of crimes. In many cases, crime information is stored as online text reports in an unstructured way and one report describes several different criminal activities. Analysis of these crime reports for identifying patterns and trends in crime and devising solutions to crime detection and prevention strategies are very challenging tasks. In this paper, the crime reports are preprocessed and relations among named entity pairs are extracted to give the structured form to the reports. Each extracted relation is converted to an n-dimensional real-valued vector based on the concept of Word2Vec model of Natural Language Processing. Then a novel agglomerative graph partitioning algorithm using various graph centrality measures is applied to partition the extracted relations. All the extracted relations of a report which are in a single partition are replaced by the representative of that partition and thus each report is described by a set of distinct types of relations. Next, a graph for the set of reports is constructed in such a way that nodes are corresponding to the tuple of relations that describes the reports, and an edge between a pair of nodes is drawn only if the corresponding pair of relations are of a similar type of two different reports. The constructed graph is a disconnected graph with each connected component is a clique. These cliques are easily identified in linear time of the number of edges in the graph and each clique provides a cluster of reports. As each report is described by a set of relations of different types, so obtained clusters are overlapping clusters. The degree of membership of a report in a cluster is also identified in the paper. The proposed method is experimented, and compared with some state-of-the-art partition-based and overlapping clustering algorithms to demonstrate its effectiveness in the domain of crime corpora.}
}
@article{HARIRCHI20179527,
title = {Active Model Discrimination with Applications to Fraud Detection in Smart Buildings**This work is supported in part by an Early Career Faculty grant from NASA’s Space Technology Research Grants Program and DARPA grant N66001-14-1-4045.},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {9527-9534},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1616},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317322140},
author = {Farshad Harirchi and Sze Zheng Yong and Emil Jacobsen and Necmiye Ozay},
keywords = {Input design, Model discrimination, Fault, attack detection, Smart building},
abstract = {In this paper, we consider the problem of active model discrimination amongst a finite number of affine models with uncontrolled and noise inputs, each representing a different system operating mode that corresponds to a fault type or an attack strategy, or to an unobserved intent of another robot, etc. The active model discrimination problem aims to find optimal separating inputs that guarantee that the outputs of all the affine models cannot be identical over a finite horizon. This will enable a system operator to detect and uniquely identify potential faults or attacks, despite the presence of process and measurement noise. Since the resulting model discrimination problem is a nonlinear non-convex mixed-integer program, we propose to solve this in a computationally tractable manner, albeit only approximately, by proposing a sequence of restrictions that guarantee that the obtained input is separating. Finally, we apply our approach to attack detection in the area of cyber-physical systems security.}
}
@article{GARCIAJIMENEZ2017834,
title = {Forest fire detection: A fuzzy system approach based on overlap indices},
journal = {Applied Soft Computing},
volume = {52},
pages = {834-842},
year = {2017},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616305002},
author = {Santiago Garcia-Jimenez and Aranzazu Jurio and Miguel Pagola and Laura {De Miguel} and Edurne Barrenechea and Humberto Bustince},
keywords = {Overlap index, Overlap function, Forest fire detection},
abstract = {It is well known that a powerful method to tackle diverse problems with lack of knowledge and/or uncertainty are Fuzzy Logic Systems (FLSs). In the literature, there exist different fuzzy inference mechanisms based on fuzzy variables and fuzzy rules to obtain a solution. In this work we introduce a generalization of the inference algorithm proposed by Mamdani, by using overlap functions and overlap indices. A challenging issue is the selection of most suitable overlap expressions for each problem. For this aim, we propose to use the convex combination of several ones. In this way, the conclusions obtained by our FLSs avoid the bad results obtained by an inadequate overlap expression. We test our proposal on a real problem of forest fire detection using a wireless sensor network.}
}
@article{BOKKISAM2021107312,
title = {Effective community energy management through transactive energy marketplace},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107312},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107312},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002846},
author = {Hanumantha Rao Bokkisam and Selvan M.P.},
keywords = {Demand response, Demand-side management, Energy communities, Electricity markets, Peer-to-peer energy trading, Renewable integration, Smart grid, Transactive energy},
abstract = {This paper presents a framework of transactive energy marketplace for effective community energy management with demand response and peer-to-peer energy trading. Further, this paper focuses on improving the monetary benefits to the participants by trading their excess generation/demand with the neighborhoods in addition to the upstream utility. Participants trade their net energy at cost-effective internal market prices determined by a transactive energy market operator. The supply-to-demand ratio based market pricing mechanism is employed to determine the day-ahead internal market prices. A case study is conducted using the data collected from ten participants in a residential community of Trichy, India. The case study results validate the effectiveness of the proposed framework in improving the monetary benefits of the market participants and reducing the grid dependency. The reduction in community electricity bill is found to be between 28% and 68% in different scenarios with the proposed market compared to the traditional market.}
}
@article{RADHAKRISHNA2018582,
title = {A novel fuzzy similarity measure and prevalence estimation approach for similarity profiled temporal association pattern mining},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {582-595},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17303795},
author = {Vangipuram Radhakrishna and Shadi A. Aljawarneh and P.V. Kumar and V. Janaki},
keywords = {Prevalence value, Spatial, Temporal, Fuzzy dissimilarity, Association pattern},
abstract = {Data generated from Sensors, IoT environment and many real time applications is mainly spatial, temporal, or spatio-temporal. Some of them include data generated from geospatial, geographical, medical, weather, finance and environmental applications. Such data objects changes over time. Conventional knowledge discovery techniques available do not address the need for analyzing such complex datasets and hence data analysis has become increasingly complex and challenging. Soft computing principles such as fuzzy logic, evolutionary and nature inspired computations may be applied to analyze dynamically varying data. Analyzing temporal trends of association patterns requires handling the temporal data, as prevalence values of temporal patterns are implicitly vectors. Finding Prevalence values of temporal association patterns and validating them for similarity using conventional approach increases the computational complexity. This makes it challenging as the conventional data mining algorithms do not address this need. In this research, we propose a novel approach for estimation of temporal association pattern prevalence values and a novel temporal fuzzy similarity measure which holds monotonicity to find similarity between any two temporal patterns. Experiments are performed considering naive, sequential, spamine and proposed approach. The results obtained show the proposed approach is promising and reduces computational complexity in terms of computing true prevalence and optimizing execution times.}
}
@article{BOUCETTA2021108275,
title = {A Latin rectangles-based TSCH scheduling and interference mitigation design},
journal = {Computer Networks},
volume = {197},
pages = {108275},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108275},
url = {https://www.sciencedirect.com/science/article/pii/S138912862100298X},
author = {Chérifa Boucetta and Boubakr Nour and Michel Sortais and Hassine Moungla},
keywords = {Latin rectangles, TSCH, IIoT, Scheduling, Interface mitigation},
abstract = {The Industrial Internet of Things (IIoT) connects a large number of industrial objects to the Internet that requires a higher level of control in terms of reliability, low power, and delay. IEEE 802.15.4e is the standard of the IIoT and includes time-synchronized channel hopping mechanisms to allow multiple communications. It controls the medium access operations using a time–frequency schedule. However, TSCH (Time Slotted Channel Hopping) specification does not specify how to build an optimized schedule. In this paper, we propose a distributed channel hopping scheme by providing an analytical model for the exploitation of Latin rectangles to avoid interference and collisions. Indeed, Latin rectangles are used to perform the scheduling process, where rows present the channel offsets and columns for slot offsets. Thus, the frequency of communication is derived using Latin rectangles, which prevents the scheduling function of nodes from considering channels already allocated in their neighborhood. Consequently, interference and multi-path fading are mitigated with more reliability and robustness. Markov chain model for the queue on every node is introduced and takes the bulk arrivals and the slot distribution into account. We analyze the efficiency of this algorithm by analytical techniques and extensive simulations for three bulk arrivals: Poisson, Bernoulli, and Geometric.}
}
@article{FOTOUHI2021103409,
title = {Optimal time-differentiated pricing for a competitive mixed traditional and crowdsourced event parking market},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {132},
pages = {103409},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103409},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21004046},
author = {Hossein Fotouhi and Elise Miller-Hooks},
keywords = {Crowdsourcing, Emerging markets, Multi-player game, Event management, Pricing, Equilibrium},
abstract = {An event-based parking pricing problem, the Crowdsourced Event Parking Market Pricing Problem, is proposed wherein parking lot owners and others who are willing to rent out privately owned spaces compete to attract drivers who are looking for available parking spaces. Each parking location owner’s problem is modeled as a bi-level program, where the upper-level parking garages, individuals and consolidators (the players) compete for customers, setting their prices to maximize revenue given the response of the lower-level followers. The lower-level followers choose their parking locations based on the utilities they derive from the spaces, which is a function of the proximity of the spaces to their destinations, parking fees and crowdedness. Prices are set as a function of the time of reservation. Reservation-time based pricing enables price differentiation for late comers either in the form of lower prices to attract last-minute customers when excess spaces are anticipated or higher-pricing if few spaces are expected to remain empty. A multi-period, stochastic Equilibrium Problem with Equilibrium Constraints (EPEC) formulation of the Crowdsourced Event Parking Market Pricing Problem is presented, and a diagonalization method with embedded gradient ascent approach for solution of individual player Mathematical Programs with Equilibrium Constraints (MPECs) is proposed for its solution. Both supply- and demand-side uncertainties are explicitly modeled. Solutions provide competitive parking prices set by reservation time for each parking facility, whether the facility involves a large parking garage or single parking space owner. Numerical experiments were conducted to illustrate the proposed concepts and assess the potential impact of crowdsourced parking spaces on the parking market. The results show that social welfare increases by more than 5% when crowdsourced parking locations account for 7% of the parking market. Results from additional numerical experiments show that ignoring stochasticity results in revenue loss for all parking owners. The developed techniques aim to facilitate existing and new parking facility owners to participate in crowdsourced event-parking markets.}
}
@article{ELIDRISSI2020458,
title = {Modular design for an urban signalized intersections network using synchronized timed Petri nets and responsive control},
journal = {Procedia Computer Science},
volume = {170},
pages = {458-465},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305263},
author = {Hajar Lamghari Elidrissi and Ahmed Nait-Sidi-Moh and Abdelouahed Tajer},
keywords = {Urban intersections, Traffic lights, Petri nets, SUMO, Modelling, Simulation, Control},
abstract = {Traffic flow at urban intersections fluctuates randomly throughout the day. It depends on several dynamic factors and requires adequate regulation and adaptable control strategies in particular for traffic signals regulation. This paper deals with this problem, and our proposal focuses on the adaptive management of traffic lights within urban intersections. To do so, Timed Synchronized Petri Nets (TSPNs) models are first developed to model and study the system behaviour. Then a real-time adaptive control strategy is proposed for traffic regulation within urban signalized intersections. The control logic is shared between two communicating components of the system. The slaves (TSPNs sub-models) control the traffic signals displays, phases transitions, and traffic flow fluctuations. The master (controller) decides and selects the next phase that should be serviced and determines its green light duration. Due to the used modularity approach, the developed models decrease the modelling complexity, and more importantly, they can be adapted easily to a cluster of intersections. Furthermore, various traffic signal control strategies could be implemented using these models. Moreover, some simulations are performed, and the obtained results are analysed and discussed. Our control strategy is validated through these simulation results.}
}
@article{YANG2020102015,
title = {Beyond beaconing: Emerging applications and challenges of BLE},
journal = {Ad Hoc Networks},
volume = {97},
pages = {102015},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.102015},
url = {https://www.sciencedirect.com/science/article/pii/S1570870518307170},
author = {Jian Yang and Christian Poellabauer and Pramita Mitra and Cynthia Neubecker},
keywords = {Bluetooth low energy, BLE, Communication, Applications, Low power, Low latency},
abstract = {As an emerging technology with exceptional low energy consumption and low-latency data transmissions, Bluetooth Low Energy (BLE) has gained significant momentum in various application domains, such as Indoor Positioning, Home Automation, and Wireless Personal Area Network (WPAN) communications. With various novel protocol stack features, BLE is finding use on resource-constrained sensor nodes as well as more powerful gateway devices. Particularly proximity detection using BLE beacons has been a popular usage scenario ever since the release of Bluetooth 4.0, primarily due to the beacons’ energy efficiency and ease of deployment. However, with the rapid rise of the Internet of Things (IoT), BLE is likely to be a significant component in many other applications with widely varying performance and Quality-of-Service (QoS) requirements and there is a need for a consolidated view of the role that BLE will play in applications beyond beaconing. This paper comprehensively surveys state-of-the-art applications built with BLE, obstacles to adoption of BLE in new application areas, and current solutions from academia and industry that further expand the capabilities of BLE.}
}
@article{SHIROWZHAN2020101033,
title = {Data mining for recognition of spatial distribution patterns of building heights using airborne lidar data},
journal = {Advanced Engineering Informatics},
volume = {43},
pages = {101033},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101033},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300021},
author = {S. Shirowzhan and S. Lim and J. Trinder and H. Li and S.M.E. Sepasgozar},
keywords = {GIS, Digital Building Model, Spatial statistics, Spatial patterns, Spatial data mining, Kernel density, Spatial big data},
abstract = {There is an increasing demand for spatial big data visualisation in Geographic Information Systems (GIS) in building construction and urban development. Exploring building height patterns is required to obtain and visualize essential information about spatio-temporal vertical urban developments due to the trends towards increasing building heights in different urban fabrics. While metrics to characterize horizontal patterns of urban fabric using spectral information exist, theoritical-based metrics identifying the patterns of vertical urban developments using height values are still scarce. In addition, there is a lack of reliable methods to analyze height information for modeling the distribution of building heights and to automatically detect three-dimensional urban patterns. In this paper, we propose to apply the spatial statistics of Local Moran’s I (LMI), Gi∗ and Kernel Density Estimation (KDE) on building heights to explore vertical urban patterns through detecting the concentration of relatively higher buildings. The proposed methods were applied on two different airborne lidar point cloud data sets. The results show overall good performance of LMI and Gi* methods compared to KDE. It is also found that there is a higher level of agreement between clusters of relatively higher buildings derived by the autocorrelation statistics of LMI and Gi*, compared with the patterns derived from the Kernel density. For the lower accuracies obtained from the KDE, the authors suggest to use either LMI or Gi* for this kind of study. The spatial closeness of clusters of higher buildings to major roads, defined by the mean distances of the clusters to major roads, were investigated and based on the Analysis of Variance (ANOVA) and Tukey’s tests, the mean distances were found to be shorter than for all other buildings. Lastly, an analysis of clusters of the relatively higher buildings showed varying land uses for the two case studies.}
}
@article{KARIMIZIARANI2022103577,
title = {Hazard risk awareness and disaster management: Extracting the information content of twitter data},
journal = {Sustainable Cities and Society},
volume = {77},
pages = {103577},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103577},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721008428},
author = {Mohammadsepehr Karimiziarani and Keighobad Jafarzadegan and Peyman Abbaszadeh and Wanyun Shao and Hamid Moradkhani},
keywords = {Twitter, Natural language processing, Hurricane Harvey, Disaster management},
abstract = {The use of social media platforms such as Twitter significantly increases during natural hazards. With the emergence of several social media platforms over the past decade, many studies have investigated the applications of these platforms during calamities. This study presents a comprehensive spatiotemporal analysis of textual content from millions of tweets shared on Twitter during Hurricane Harvey (2017) across several affected counties in southeast Texas. We propose a new Hazard Risk Awareness (HRA) Index, which considers multiple factors, including the number of tweets, population, internet use rate, and natural hazard characteristics per geographic location. We then map the HRA Index across southeast Texas. Utilizing a dataset of 18 million tweets, we employ Natural Language Processing (NLP) along with a set of statistical techniques to perform analysis on the textual data generated by Twitter users during Hurricane Harvey. This enables us to subdivide the tweet contents into several categories per county that would inform crisis management during the event. In all, our study provides valuable information at the county level before, during, and after Harvey that could significantly help disaster managers and responders to minimize the consequences of the event and improve the preparedness of the residents for it. Since HRA is derived based on the meteorological observations and some demographic information, depending on the availability of such dataset and the nature of the hazard (i.e., flood, wildfire, hurricane, and earthquake), this index can be calculated and employed for assessing the risk awareness of a community exposed to either of these natural hazards.}
}
@article{KIM2017171,
title = {Adaptive data rate control in low power wide area networks for long range IoT services},
journal = {Journal of Computational Science},
volume = {22},
pages = {171-178},
year = {2017},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1877750317304428},
author = {Dae-Young Kim and Seokhoon Kim and Houcine Hassan and Jong Hyuk Park},
keywords = {LPWAN, IoT, Adaptive data rate control, Congestion identification, Data transmission},
abstract = {Internet of Things (IoT) technologies can provide various intelligent services by collecting information from objects. To collect information, Wireless Sensor Networks (WSNs) are exploited. The Low Power Wide Area Network (LPWAN), one type of WSN, has been designed for long-range IoT services. It consumes low power and uses a low data rate for data transmission. The LPWAN includes several communication standards, and Long Range Wide Area Network (LoRaWAN) is the representative standard of the LPWAN. LoRaWAN provides several data rates for transmission and enables adaptive data rate control in order to maintain network connectivity. In the LoRaWAN, the wireless condition is considered by the reception status of the acknowledgement (ACK) message, and adaptive data rate control is performed according to the wireless condition. Because the judgment of the wireless condition by the reception status of ACK messages does not reflect congestion, adaptive data rate control can lead to inefficiency in data transmission. For efficient data transmission in long-range IoT services, this paper proposes a congestion classifier using logistic regression and modified adaptive data rate control. The proposed scheme controls the data rate according to the congestion estimation. Through extensive analysis, we show the proposed scheme’s efficiency in data transmission.}
}
@article{BOVET2014985,
title = {Will Web Technologies Impact on Building Automation Systems Architecture?},
journal = {Procedia Computer Science},
volume = {32},
pages = {985-990},
year = {2014},
note = {The 5th International Conference on Ambient Systems, Networks and Technologies (ANT-2014), the 4th International Conference on Sustainable Energy Information Technology (SEIT-2014)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.522},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914007224},
author = {Gérôme Bovet and Jean Hennebert},
keywords = {Building Management System, Internet-of-Things, Web-of-Things, Architecture},
abstract = {Offices, factories and even private housings are more and more endowed with building management systems (BMS) targeting an increase of comfort as well as lowering energy costs. This expansion is made possible by the progress realized in pervasive computing, providing small sized and affordable sensing devices. However, current BMS are often based on proprietary tech- nologies, making their interoperability and evolution more difficult. For example, we observe the emergence of new applications based on intelligent data analysis able to compute more complex models about the use of the building. Such applications rely on heterogeneous sets of sensors, web data, user feedback and self-learning algorithms. In this position paper, we discuss the role of Web technologies for standardizing the application layer, and thus providing a framework for developing advanced building applications. We present our vision of TASSo, a layered Web model facing actual and future challenges for building management systems.}
}
@article{AGYEMAN2020123292,
title = {Modeling the long-run drivers of total renewable energy consumption: Evidence from top five heavily polluted countries},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123292},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123292},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620333370},
author = {Joy Korang Agyeman and Bismark Ameyaw and Yao Li and Jamal Appiah-Kubi and Augustine Annan and Amos Oppong and Martinson Ankrah Twumasi},
keywords = {Renewable energy consumption, Economic growth, Bidirectional long short-term memory (bi-LSTM), Forecasting, Fully modified ordinary least square (FMOLS)},
abstract = {A boom in renewable energy consumption as a percentage of final energy consumption (REC) has a significant impact on cleaner production and environmental sustainability. However, studies on REC as for an individual country-case and panel case approach for top emitters are rare. Against this backdrop, this study investigates the long-run drivers of REC by using a time series data from 1990 to 2015. The FMOLS-grouped results indicate that economic growth and trade openness increase REC whereas population growth has a negative but significant impact on REC. To check for causal links, the innovating accounting approach using variance decomposition analysis is applied. The results show that there is a unidirectional causality running from economic growth, trade openness, and population growth to REC. Further, the predictive accuracy of the FMOLS based econometric output and the bi-directional long short-term memory (Bi-LSTM) is analyzed. The Bi-LSTM formulated algorithm outperformed the econometric output. The Bi-LSTM is utilized to forecast REC to the year 2030. The output from the Bi-LSTM shows that China, the US, India, the Russian Federation’s, and Japan’s REC will hit ∼11.3395, ∼11.1245, ∼34.6969, ∼2.9097, and ∼7.4859, respectively. The US and Japan’s REC levels will increase while that of China, India, and Russia Federation will decrease. As a policy implication, new policy directives for China, India, and the Russian Federation are required to boost REC levels.}
}
@article{ZHU2022104032,
title = {Principal component analysis based data collection for sustainable internet of things enabled Cyber–Physical Systems},
journal = {Microprocessors and Microsystems},
volume = {88},
pages = {104032},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104032},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121002040},
author = {Tongxin Zhu and Xiuzhen Cheng and Wei Cheng and Zhi Tian and Yingshu Li},
keywords = {Principal component analysis, Data collection, Internet of Things (IoT), Cyber–Physical System (CPS)},
abstract = {The Internet of Things (IoT) enabled Cyber–Physical System (CPS) is a promising technology applying in smart home, industrial manufacturing, intelligent transportation, etc. The IoT enabled CPS consists of two main components, i.e., IoT devices and cybers, which interact with each other. The IoT devices collect sensory data from physical environments and transmit them to the cybers, and the cybers make decisions to respond to the collected data and issue commands to control the IoT devices. It is generally known that energy is an important but limited resource in IoT devices. Data compression is an efficient way to reduce the energy consumption of data collection in sustainable IoT enabled CPSs, especially the Principal Component Analysis (PCA) based data compression. The trade-off between data compression ratio and data reconstruction error is one of the biggest challenges for PCA based data compression. In this paper, we investigate PCA based data compression to maximize the compression ratio with bounded reconstruction error for data collection in IoT enabled CPSs. Firstly, a similarity based clustering algorithm is proposed to cluster IoT devices in an IoT enabled CPS. Then, a PCA based data compression algorithm is proposed to compress the collected data to the greatest extent in each cluster with a bounded reconstruction error. Extensive simulations are conducted to verify the efficiency and effectiveness of the proposed algorithms.}
}
@article{CHANG2018998,
title = {Applying intelligent data traffic adaptation to high-performance multiple big data analytics platforms},
journal = {Computers & Electrical Engineering},
volume = {70},
pages = {998-1018},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617327106},
author = {Bao Rong Chang and Hsiu-Fen Tsai and Po-Hao Liao},
keywords = {Big data platform, Intelligent data traffic adaptation, R programming, Scheduling optimization algorithm, Performance index},
abstract = {This paper introduces the integration of two data processing platforms, RHhadoop and SparkR, to carry out rapid big data retrieval and analytics using R programming, which can serve as part of business intelligence. Besides, it has developed the job scheduling optimization called Memory-Sensitive Heterogeneous Earliest Finish Time algorithm to enhance system throughput. However, the bottleneck of system throughput is definitely relevant to data traffic problem over network, especially a large amount of data exchange between distributed computing nodes within a cluster. The objective of this paper is to propose an intelligence approach to tackle the crucial problems of inefficient data traffic flow. Adaptive network-based fuzzy inference system along with particle swarm optimization has employed to tune the network-related parameters at computing nodes for improving network QoS and speed up data transportation significantly. In order to examine the computing efficiency, performance index has been evaluated for all of treatments in the experiment.}
}
@article{MIASAYEDAVA2022114283,
title = {Automated environmental compliance monitoring of rivers with IoT and open government data},
journal = {Journal of Environmental Management},
volume = {303},
pages = {114283},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114283},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721023458},
author = {Lizaveta Miasayedava and Keegan McBride and Jeffrey Andrew Tuhtan},
keywords = {Environmental compliance monitoring, Environmental flows, Internet of things, Open government data},
abstract = {Environmental monitoring of rivers is a cornerstone of the European Union's Water Framework Directive. It requires the estimation and reporting of environmental flows in rivers whose characteristics vary widely across the EU member states. This variability has resulted in a fragmentation of estimation and reporting methods for environmental flows and is exhibited by the myriad of regulatory guidelines and estimation procedures. To standardise and systematically evaluate environmental flows at the pan-European scale, we propose to formalise the estimation procedures through automation by reusing existing river monitoring resources. In this work, we explore how sensor-generated hydrological open government data can be repurposed to automate the estimation and monitoring of river environmental flows. In contrast to existing environmental flows estimation methods, we propose a scalable IoT-based architecture and implement its cloud-layer web service. The major contribution of this work is the demonstration of an automated environmental flows system based on open river monitoring data routinely collected by national authorities. Moreover, the proposed system adds value to existing environmental monitoring data, reduces development and operational costs, facilitates streamlining of environmental compliance and allows for any authority with similar data to reuse or scale it with new data and methods. We critically discuss the opportunities and challenges associated with open government data, including its quality. Finally, we demonstrate the proposed system using the Estonian national river monitoring network and define further research directions.}
}
@article{RAMOS2021475,
title = {Data mining techniques for electricity customer characterization},
journal = {Procedia Computer Science},
volume = {186},
pages = {475-488},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.168},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921010048},
author = {Sérgio Ramos and João Soares and Samuel S. Cembranel and Inês Tavares and Z. Foroozandeh and Zita Vale and Rubipiara Fernandes},
keywords = {Knowledge discovery in Databases, data mining, clustering, classification, typical load profiles},
abstract = {The liberalization of electricity markets has been resulted in the emergence of new players, increasing the competitiveness in the markets, standing those can provide better services for better prices. The knowledge of energy consumers’ profile has been an important tool to help players to make decisions in the electrical sectors. In this paper, a characterization model of typical load curves for Low Voltage (LV) customers is proposed and evaluated. The identification of consumption patterns is based on clustering analysis. The clustering methodology is based on seven algorithms, partitional and hierarchical. Also, five clustering validity indices are used to identify the best data partition. With the knowledge obtained in clustering analysis, a classification model is used to classify new customers according to their consumption data. The classification model is used to select the correct class for each customer. To make the model simple, each load curve is represented by three indices which represent load curves shape. The methodology used in this work demonstrates to be an effective tool and can be used in most diverse sectors, highlighting the use of knowledge in the optimization of the energy contracting for low voltage customers. The energy consumption data can be constantly updated to improve the model precision, finding estimates that can better represent consumers and their consumption habits.}
}
@article{ALSHAER2019792,
title = {IBRIDIA: A hybrid solution for processing big logistics data},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {792-804},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1830606X},
author = {Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk},
keywords = {Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm},
abstract = {Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.}
}
@article{SPENCER2018728,
title = {A Refinement of Lasso Regression Applied to Temperature Forecasting},
journal = {Procedia Computer Science},
volume = {130},
pages = {728-735},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304897},
author = {Bruce Spencer and Omar Alfandi and Feras Al-Obeidat},
keywords = {Home Sensor Network, Temperature Forecasting, Lasso regression, Feature Selection, Model Predictive Control, Energy Efficiency, Internet of Things},
abstract = {Model predictive controllers use accurate temperature forecasts to save energy by optimally controlling heating, ventilation and air conditioning equipment while achieving comfort for occupants. In a “smart” building, i.e. one that is outfitted with sensors, temperature forecasts are computed from data gathered by these sensors. Recently, accurate temperature forecasts have been generated using relatively few observations from each sensor. However, long sensor histories are available in smart houses. In this paper we consider improving forecast accuracy by using up to 24 hours of quarter-hourly readings. In particular, we overcome forecast inaccuracy that arises from the “one standard error” heuristic (1SE) in lasso regression. When there are many historical observations, low variance in the error estimations can result in excessively high values for the lasso hyperparameter λ. We propose the midfel refinement of lasso regression, which adjusts λ based on the shape of the error curve, resulting in improved forecast accuracy. We illustrate its effect in a setting where lasso regression is used to select sensors based on forecast accuracy. In this setting, midfel lasso regression using many historical observations has two effects: its improves accuracy and uses fewer sensors. Thus it potentially reduces costs arising both from energy usage and from sensor installation.}
}
@article{LANTSEVA2018463,
title = {Assessment of pedestrian flow volumes through public transport modelling},
journal = {Procedia Computer Science},
volume = {136},
pages = {463-471},
year = {2018},
note = {7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.265},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918315709},
author = {Anastasia A. Lantseva and Sergey V. Ivanov},
keywords = {Type your keywords here, separated by semicolons},
abstract = {One of the priority urban tasks is the identification and assessment of pedestrian flows in the city. Since the patterns of residents’ behaviour are the basis for planning and developing the urban environment, in this study, we propose the extended approach to modelling of pedestrian flows, which relies on public transport passenger choice rules. The study aims to assess pedestrian flows within the city as a whole and incoming flow at each public transport stops. The results are shown on the example of St. Petersburg and verified by the real data from the smart cards.}
}
@article{KUMAR2018691,
title = {Exploring Data Security Issues and Solutions in Cloud Computing},
journal = {Procedia Computer Science},
volume = {125},
pages = {691-697},
year = {2018},
note = {The 6th International Conference on Smart Computing and Communications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917328570},
author = {P. Ravi Kumar and P. Herbert Raj and P. Jelciana},
keywords = {Cloud Computing, Data Security, Cloud Services, Confidentiality, Integrity, Availability, Authentication, Access Control},
abstract = {Cloud computing is one of the fastest emerging technologies in computing. There are many advantages as well few security issues in cloud computing. This paper explores the different data security issues in cloud computing in a multi-tenant environment and proposes methods to overcome the security issues. This paper also describes Cloud computing models such as the deployment models and the service delivery models. In any business or Cloud Computing data are exceptionally important, data leaking or corruption can shatter the confidence of the people and can lead to the collapse of that business. Currently cloud computing is used directly or indirectly in many businesses and if any data breaching has happened in cloud computing, that will affect the cloud computing as well as the company’s business. This is one of the main reasons for cloud computing companies to give more attention to data security.}
}
@article{SPYRIDIS2021102232,
title = {Modelling and simulation of a new cooperative algorithm for UAV swarm coordination in mobile RF target tracking},
journal = {Simulation Modelling Practice and Theory},
volume = {107},
pages = {102232},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102232},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X20301672},
author = {Yannis Spyridis and Thomas Lagkas and Panagiotis Sarigiannidis and Jie Zhang},
keywords = {Simulation, Modelling, Unmanned aerial vehicles, UAV swarm, Mobile target tracking, RSSI, Wireless sensor networks},
abstract = {Recent advancements in sensor technology have allowed unmanned aerial vehicles (UAVs) to function as sensing devices in cooperative aerial communication networks, offering novel solutions in applications of environment inspection, disaster detection and search and rescue operations. Towards this trend, the efficient deployment and coordination of UAV networks is of vital importance. Generating controlled experimental conditions to implement and evaluate different approaches in this context can be impractical and costly and thus the solution of modelling is often preferred. This paper introduces a tracking model in which multirotor UAVs, equipped with received signal strength indicator (RSSI) sensors, are organized in a swarm and cooperate to approximate and trail a moving target. The proposed algorithm is able to offer autonomous tracking in large scale environments, by utilising just the strength of the communication signal emitted by a radio frequency transmitter carried by the target. A model of the proposed algorithm is created, and its performance is thoroughly evaluated in a specialized simulator developed in the Processing IDE. Results demonstrate the increased tracking efficiency of the proposed solution compared to a trilateration method.}
}
@article{VANDERWOUDEN2022105729,
title = {Are Chinese cities getting smarter in terms of knowledge and technology they produce?},
journal = {World Development},
volume = {150},
pages = {105729},
year = {2022},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2021.105729},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X21003442},
author = {Frank {van der Wouden}},
keywords = {China, Inequality, Technology, Development, Patents, Publications},
abstract = {Are Chinese cities becoming smarter in terms of the knowledge and technologies they produce? For decades, the widely held believe in the global North is that China merely copies, imitates and only incrementally improves ideas. Recently, this believe is increasingly being challenged. In this paper I quantify this process. Using data from about 6,1 million patents and 60 million academic publications I examine the complexity of technologies and knowledge developed in major Chinese cities and compare these to those produced in other global cities. The results show that (1) knowledge and technologies produced in Chinese cities has increasingly taken a greater share of total output of global cities; (2) the quality of this output has increased over time; (3) Chinese cities improve the quality of their knowledge output before increasing the quality of technology; and (4) only Beijing, Shanghai and Shenzhen compete with other global cities in terms of the quality of output, suggesting emerging regional inequality within China linked to knowledge and technology production. Not only is the size of the Chinese economy increasing, so is the quantity and quality of its knowledge and technology output.}
}
@article{OGUNDOYIN2021100937,
title = {Optimization techniques and applications in fog computing: An exhaustive survey},
journal = {Swarm and Evolutionary Computation},
volume = {66},
pages = {100937},
year = {2021},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2021.100937},
url = {https://www.sciencedirect.com/science/article/pii/S2210650221000985},
author = {Sunday Oyinlola Ogundoyin and Ismaila Adeniyi Kamil},
keywords = {Fog computing, Optimization, Metaheuristic, Swarm intelligence, Multi-objective, Objective function},
abstract = {This paper presents an all-inclusive review of the optimization methods and their applications in fog computing. We put forward a taxonomy of the optimization techniques and then discuss their applications to solving various optimization problems in this field. Moreover, we develop a glossary of different optimization metrics in the literature and classify various evaluation environments used to test the solutions of different algorithms. The distribution of the relevant publications and the threats to the validity of the study are also discussed. Finally, we present the challenges in the existing literature and the future trends for research in fog computing optimization.}
}
@article{XIAO2020498,
title = {EdgeABC: An architecture for task offloading and resource allocation in the Internet of Things},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {498-508},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19323738},
author = {Kaile Xiao and Zhipeng Gao and Weisong Shi and Xuesong Qiu and Yang Yang and Lanlan Rui},
keywords = {Internet of Things, Edge computing, Blockchain, Resources allocation, Fairness, Profits of service providers},
abstract = {The evolving Internet of Things (IoT) faces considerable challenges in terms of sensitive delay requirements for tasks, reasonable allocation requirements of resources, and reliability requirements of resource transactions. In the paper, considering these problems, we propose an emerging IoT architecture EdgeABC introduced the blockchain to ensure the integrity of resource transaction data and the profits of service provider, and we also propose a Task Offloading and Resource Allocation (TO-RA) algorithm, where the TO-RA algorithm is implemented on the blockchain in the form of smart contracts. In other words, the architecture proposed optimizes the resource allocation in IoT based on the advantages of the blockchain. Specifically, we first propose a subtask-virtual machine mapping strategy to complete the Task Offloading (TO) and the first allocation of resources; then aiming at the possible load imbalance problem of the system, we propose a stack cache supplement mechanism to complete the Resource Allocation (RA) based on the TO strategy. Finally, simulation experiments verify that the fairness, user satisfaction, and system utility of the TO-RA algorithm are superior to traditional algorithms.}
}
@article{KORTOCI2022100241,
title = {Air pollution exposure monitoring using portable low-cost air quality sensors},
journal = {Smart Health},
volume = {23},
pages = {100241},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100241},
url = {https://www.sciencedirect.com/science/article/pii/S235264832100057X},
author = {Pranvera Kortoçi and Naser Hossein Motlagh and Martha Arbayani Zaidan and Pak Lun Fung and Samu Varjonen and Andrew Rebeiro-Hargrave and Jarkko V. Niemi and Petteri Nurmi and Tareq Hussein and Tuukka Petäjä and Markku Kulmala and Sasu Tarkoma},
keywords = {Air quality, Air pollution, Internet of things, Low-cost sensor, Data classification, Wood smoke},
abstract = {Urban environments with a high degree of industrialization are infested with hazardous chemicals and airborne pollutants. These pollutants can have devastating effects on human health, causing both acute and chronic diseases such as respiratory infections, lung cancer, and heart disease. Air pollution monitoring is vital not only to citizens, warning them on the health risks of air pollutants, but also to policy-makers, assisting them on drafting regulations and laws that aim at minimizing those health risks. Currently, air pollution monitoring predominantly relies on expensive high-end static sensor stations. These stations produce only aggregated information about air pollutants, and are unable to capture variations in individual's air pollution exposure. As an alternative, this article develops a citizen-based air pollution monitoring system that captures individual exposure levels to air pollutants during daily indoor and outdoor activities. We present a low-cost portable sensor and carry out a measurement campaign using the sensors to demonstrate the validity and benefits of citizen-based pollution measurements. Specifically, we (i) successfully classify the data into indoor and outdoor, and (ii) validate the consistency and accuracy of our outdoor-classified data to the measurements of a high-end reference monitoring station. Our experimental results further prove the effectiveness of our campaign by (i) providing fine-grained air pollution insights over a wide geographical area, (ii) identifying probable causes of air pollution dependent on the area, and (iii) providing citizens with personalized insights about air pollutants in their daily commute.}
}
@article{WANG2020139148,
title = {Improving population mapping using Luojia 1-01 nighttime light image and location-based social media data},
journal = {Science of The Total Environment},
volume = {730},
pages = {139148},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.139148},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720326656},
author = {Luyao Wang and Hong Fan and Yankun Wang},
keywords = {Population mapping, Luojia 1-01 image, Point of interest, Check-in data, Attraction degree},
abstract = {Fine-resolution population mapping, which is vital to urban planning, public health, and disaster management, has gained considerable attention in socioeconomic and environmental studies. Although population distribution has been considered highly correlated with urban facilities, the quantitative relationship between the two has yet to be revealed when considering huge heterogeneity. To address this problem, the present study proposed a novel population mapping method by adopting Luojia 1-01 nighttime light imagery, points of interest (POI), and social media check-in data. A grid-based attraction degree (AD) model was built to quantify the possibility of population concentration in each geographic unit with a comprehensive consideration of the distribution and the popularity of facilities. On the basis of kernel density estimation, 16 attraction indexes were extracted by matching POI and check-in data. Multiple variables were used to train a random forest model, through which fine-scale population mapping was conducted in Zhejiang, China. The comparison between demographic and WorldPop data proved the high accuracy of our approach (R2 = 0.75 and 0.58). To explore the characteristics of the model further, the most appropriate search distance (650 m) and acquisition time (19:00–08:00) of the check-in data were discussed. The contrast experiment revealed that the model could outperform those from previous studies on rural and suburban areas with a few check-in points and low AD; thus, the mapping error caused by heterogeneity considerably decreased. The results indicated the proposed method has great potential in fine-scale population mapping.}
}
@article{WU20161,
title = {Quantification of knee vibroarthrographic signal irregularity associated with patellofemoral joint cartilage pathology based on entropy and envelope amplitude measures},
journal = {Computer Methods and Programs in Biomedicine},
volume = {130},
pages = {1-12},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715301139},
author = {Yunfeng Wu and Pinnan Chen and Xin Luo and Hui Huang and Lifang Liao and Yuchen Yao and Meihong Wu and Rangaraj M. Rangayyan},
keywords = {Approximate entropy, Articular cartilage, Fuzzy entropy, Knee joint, Symbolic entropy, Vibroarthrography},
abstract = {Background and objective
Injury of knee joint cartilage may result in pathological vibrations between the articular surfaces during extension and flexion motions. The aim of this paper is to analyze and quantify vibroarthrographic (VAG) signal irregularity associated with articular cartilage degeneration and injury in the patellofemoral joint.
Methods
The symbolic entropy (SyEn), approximate entropy (ApEn), fuzzy entropy (FuzzyEn), and the mean, standard deviation, and root-mean-squared (RMS) values of the envelope amplitude, were utilized to quantify the signal fluctuations associated with articular cartilage pathology of the patellofemoral joint. The quadratic discriminant analysis (QDA), generalized logistic regression analysis (GLRA), and support vector machine (SVM) methods were used to perform signal pattern classifications.
Results
The experimental results showed that the patients with cartilage pathology (CP) possess larger SyEn and ApEn, but smaller FuzzyEn, over the statistical significance level of the Wilcoxon rank-sum test (p<0.01), than the healthy subjects (HS). The mean, standard deviation, and RMS values computed from the amplitude difference between the upper and lower signal envelopes are also consistently and significantly larger (p<0.01) for the group of CP patients than for the HS group. The SVM based on the entropy and envelope amplitude features can provide superior classification performance as compared with QDA and GLRA, with an overall accuracy of 0.8356, sensitivity of 0.9444, specificity of 0.8, Matthews correlation coefficient of 0.6599, and an area of 0.9212 under the receiver operating characteristic curve.
Conclusions
The SyEn, ApEn, and FuzzyEn features can provide useful information about pathological VAG signal irregularity based on different entropy metrics. The statistical parameters of signal envelope amplitude can be used to characterize the temporal fluctuations related to the cartilage pathology.}
}
@article{HUANG2018557,
title = {Building edge intelligence for online activity recognition in service-oriented IoT systems},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {557-567},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17306520},
author = {Zhenqiu Huang and Kwei-Jay Lin and Bo-Lung Tsai and Surong Yan and Chi-Sheng Shih},
abstract = {This paper presents the edge intelligence support for smart Internet of Things (IoT) using the service-oriented architecture. We propose an edge intelligence framework for building smart IoT applications. The proposed edge intelligence framework pushes the streaming processing capability from cloud core to edge devices, in order to better support timely and reliable streaming data analytics in smart IoT applications. We have designed annotation based programming primitives for developers to build online learning capabilities on edge devices. We have also implemented a user activity recognition engine, and compared its performances between running on either an edge device or cloud servers. Using our edge intelligence framework can improve the real-time and fault-tolerance performance significantly without degrading the activity recognition accuracy in a smart home.}
}
@article{CHIMUKA2019101926,
title = {Impact of artificial intelligence on patent law. Towards a new analytical framework – [ the Multi-Level Model]},
journal = {World Patent Information},
volume = {59},
pages = {101926},
year = {2019},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2019.101926},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018300814},
author = {Garikai Chimuka},
keywords = {Patent law, Multi-level model, Patentability, Inventorship, Industry 4.0, 4th industrial revolution},
abstract = {Inventions and new ideas are at the center of societal transformation. Inventions have been historically protected by a system of intellectual property law of which patents are at the heart. Whilst patent law is still deeply moored in its roots in the industrial revolution, to a greater extend it has been able to adapt to the successive revolutions like the computing albeit with some challenges. The world is now at an unprecedented threshold of the most far reaching revolution whose consequences to patent law in particular are so far reaching that its impact is still unknown. This is the AI revolution. This paper begins with an analysis of the AI revolution from available literature and research. It notes that there are divergent views among scholars about the impact of AI on various elements of patent law. It posits that this dissonance might be a result of most scholars treating AI as one homogenous block without distinguishing the various phases in the evolution of AI. It thus proposes a new sui generis conceptual framework – [ the Multi-Level Model] as a suitable basis for insightful conceptual analysis of AI impact on patent law with focus on two key questions; patentability and inventorship.}
}
@article{BALLI2022114984,
title = {Various thermoeconomic assessments of a heat and power system with a micro gas turbine engine used for industry},
journal = {Energy Conversion and Management},
volume = {252},
pages = {114984},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.114984},
url = {https://www.sciencedirect.com/science/article/pii/S0196890421011602},
author = {Ozgur Balli and Hakan Caliskan},
keywords = {Economic analysis, Thermoeconomic analysis, EXCEM, SPECO, Micro gas turbine, Combined heat and power (CHP)},
abstract = {The natural gas fired combined heat and power system driven by a micro gas turbine engine (M−CHP) of a dairy products company is investigated basing on the actual test data. The system is investigated along with economic analysis and specific cost (SPECO), exergy-cost-energy-mass (EXCEM), modified EXCEM (m-EXCEM) thermoeconomic analyses. The present worth of M−CHP system is estimated to be 241,190.20 US$. According to SPECO method, the cost formation of waste exergy in the M−CHP system is found as 6.634 US$/h, while the combustion chamber component of the system has the maximum (worst) waste exergy cost rate to be 4.366 GJ/h. According to EXCEM method, the waste exergy cost formation of M−CHP system is determined to be 0.563x10-6 GJ/hUS$, while it is estimated as 0.078 GJ/US$ with m-EXCEM method. Among the components of the system, the combustion chamber has the worst waste exergy cost rate.}
}
@article{WANG2021142030,
title = {Characterizing soil salinity at multiple depth using electromagnetic induction and remote sensing data with random forests: A case study in Tarim River Basin of southern Xinjiang, China},
journal = {Science of The Total Environment},
volume = {754},
pages = {142030},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.142030},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720355595},
author = {Fei Wang and Shengtian Yang and Yang Wei and Qian Shi and Jianli Ding},
keywords = {Soil salinity, Electromagnetic induction (EMI), Digital soil mapping, Random forest, Harmonized World Soil Database, Tarim River Basin},
abstract = {Tarim River Basin is experiencing heavy soil degeneration in a long term because of the extreme natural conditions, added with improper human activities such as reclamation and rejected field repeatedly, which hindered the soil health. One of the mainly form is soil salinization. Spatial distribution and variation of soil salinity is essential both for agricultural resource management and local economic development. However, knowledge of the spatial distribution of soil salinization in this region has not been updated since 1980s while land use and climate have undergone major changed. Electromagnetic induction (EMI) has been successfully used to directly measurement the spatial distribution of targeting soil property at field- scale, and apparent electrical conductivity (ECa, mS m−1) has become a surrogate of soil salinity (EC, dS m−1) studied by many researchers at local scale. However, the effectiveness of this equipment has not been verified in the typical soil salinization areas in southern Xinjiang, especially on a large scale. This study was aimed to test the performance of ECa jointed with Random Forest (RF) for soil salinity regional–scale mapping at a typical arid area, taking Tarim River Basin as an example. The result showed that ECa together with environmental derivative variables and with RF were suited for regional–scale soil salinity mapping. Predicted accuracy of EC was higher at surface (0–20 cm, R2 = 0.65, RMSE = 5.59) and deeper soil depth (60–80 cm, R2 = 0.63, RMSE = 2.00, and 80–100 cm, R2 = 0.61, RMSE = 1.73), lower at transitional zone (20–40 cm, R2 = 0.55, RMSE = 2.66, and 40–60 cm, R2 = 0.51, RMSE = 2.49). When ECa is involved in modeling, the prediction accuracy of multiple depths of EC is improved by 13.33%–61.54%, of which the most obvious depths are 60–80 cm and 0–20 cm. The results of variable importance show that SoilGrids were also favored the power EC model. Hence, we strongly recommended to joint EMI reads with remote sensing imagery for soil salinity monitoring at large scale in southern Xinjiang. These EC and ECa map can provide a data source for environmental modeling, a benchmark against which to evaluate and monitor water and salt dynamics, and a guide for the design of future soil surveys.}
}
@article{MOURTZIS20211960,
title = {Smart Manufacturing and Tactile Internet Powered by 5G: Investigation of Current Developments, Challenges, and Future Trends},
journal = {Procedia CIRP},
volume = {104},
pages = {1960-1969},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.331},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012294},
author = {Dimitris Mourtzis},
keywords = {Tactile Internet, Smart Manufacturing, 5G, Industry 4.0},
abstract = {Communication latency has been a significant barrier for many applications deployed in manufacturing networks. Despite the constant development of improved communication protocols and standards during Industry 4.0, the latency problem still exists, decreasing the Quality of Services (QoS) and Quality of experience (QoE). Therefore, high availability, security and ultra-low latency offered by Tactile Internet (TI), will create a new dimension to human-to-machine interaction (HMI) by enabling haptic and tactile sensations. The 5G mobile communication systems will support this emerging Internet at the wireless edge. Consequently, TI can be used as backbone for delay mitigation in cooperation with 5G networks, for ultra-reliable low-latency applications such as Smart Manufacturing, Virtual and Augmented Reality. Therefore, the aim of this paper is to present the state-of-the-art of 5G and TI, the challenges, the trends for 5G networks beyond 2020 and to provide a conceptual framework integrating 5G and TI to existing industrial case studies.}
}