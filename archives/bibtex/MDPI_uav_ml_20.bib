
@Article{rs12233926,
AUTHOR = {Deur, Martina and Gašparović, Mateo and Balenović, Ivan},
TITLE = {Tree Species Classification in Mixed Deciduous Forests Using Very High Spatial Resolution Satellite Imagery and Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3926},
URL = {https://www.mdpi.com/2072-4292/12/23/3926},
ISSN = {2072-4292},
ABSTRACT = {Spatially explicit information on tree species composition is important for both the forest management and conservation sectors. In combination with machine learning algorithms, very high-resolution satellite imagery may provide an effective solution to reduce the need for labor-intensive and time-consuming field-based surveys. In this study, we evaluated the possibility of using multispectral WorldView-3 (WV-3) satellite imagery for the classification of three main tree species (Quercus robur L., Carpinus betulus L., and Alnus glutinosa (L.) Geartn.) in a lowland, mixed deciduous forest in central Croatia. The pixel-based supervised classification was performed using two machine learning algorithms: random forest (RF) and support vector machine (SVM). Additionally, the contribution of gray level cooccurrence matrix (GLCM) texture features from WV-3 imagery in tree species classification was evaluated. Principal component analysis confirmed GLCM variance to be the most significant texture feature. Of the 373 visually interpreted reference polygons, 237 were used as training polygons and 136 were used as validation polygons. The validation results show relatively high overall accuracy (85%) for tree species classification based solely on WV-3 spectral characteristics and the RF classification approach. As expected, an improvement in classification accuracy was achieved by a combination of spectral and textural features. With the additional use of GLCM variance, the overall accuracy improved by 10% and 7% for RF and SVM classification approaches, respectively.},
DOI = {10.3390/rs12233926}
}



@Article{e22121358,
AUTHOR = {Jia, Jinlu and Lai, Zhenyi and Qian, Yurong and Yao, Ziqiang},
TITLE = {Aerial Video Trackers Review},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1358},
URL = {https://www.mdpi.com/1099-4300/22/12/1358},
PubMedID = {33266268},
ISSN = {1099-4300},
ABSTRACT = {Target tracking technology that is based on aerial videos is widely used in many fields; however, this technology has challenges, such as image jitter, target blur, high data dimensionality, and large changes in the target scale. In this paper, the research status of aerial video tracking and the characteristics, background complexity and tracking diversity of aerial video targets are summarized. Based on the findings, the key technologies that are related to tracking are elaborated according to the target type, number of targets and applicable scene system. The tracking algorithms are classified according to the type of target, and the target tracking algorithms that are based on deep learning are classified according to the network structure. Commonly used aerial photography datasets are described, and the accuracies of commonly used target tracking methods are evaluated in an aerial photography dataset, namely, UAV123, and a long-video dataset, namely, UAV20L. Potential problems are discussed, and possible future research directions and corresponding development trends in this field are analyzed and summarized.},
DOI = {10.3390/e22121358}
}



@Article{math8122140,
AUTHOR = {Kupervasser, Oleg and Kutomanov, Hennadii and Levi, Ori and Pukshansky, Vladislav and Yavich, Roman},
TITLE = {Using Deep Learning for Visual Navigation of Drone with Respect to 3D Ground Objects},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2140},
URL = {https://www.mdpi.com/2227-7390/8/12/2140},
ISSN = {2227-7390},
ABSTRACT = {In the paper, visual navigation of a drone is considered. The drone navigation problem consists of two parts. The first part is finding the real position and orientation of the drone. The second part is finding the difference between desirable and real position and orientation of the drone and creation of the correspondent control signal for decreasing the difference. For the first part of the drone navigation problem, the paper presents a method for determining the coordinates of the drone camera with respect to known three-dimensional (3D) ground objects using deep learning. The algorithm has two stages. It causes the algorithm to be easy for interpretation by artificial neural network (ANN) and consequently increases its accuracy. At the first stage, we use the first ANN to find coordinates of the object origin projection. At the second stage, we use the second ANN to find the drone camera position and orientation. The algorithm has high accuracy (these errors were found for the validation set of images as differences between positions and orientations, obtained from a pretrained artificial neural network, and known positions and orientations), it is not sensitive to interference associated with changes in lighting, the appearance of external moving objects and the other phenomena where other methods of visual navigation are not effective. For the second part of the drone navigation problem, the paper presents a method for stabilization of drone flight controlled by autopilot with time delay. Indeed, image processing for navigation demands a lot of time and results in a time delay. However, the proposed method allows to get stable control in the presence of this time delay.},
DOI = {10.3390/math8122140}
}



@Article{s20236896,
AUTHOR = {Buzzy, Michael and Thesma, Vaishnavi and Davoodi, Mohammadreza and Mohammadpour Velni, Javad},
TITLE = {Real-Time Plant Leaf Counting Using Deep Object Detection Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6896},
URL = {https://www.mdpi.com/1424-8220/20/23/6896},
ISSN = {1424-8220},
ABSTRACT = {The use of deep neural networks (DNNs) in plant phenotyping has recently received considerable attention. By using DNNs, valuable insights into plant traits can be readily achieved. While these networks have made considerable advances in plant phenotyping, the results are processed too slowly to allow for real-time decision-making. Therefore, being able to perform plant phenotyping computations in real-time has become a critical part of precision agriculture and agricultural informatics. In this work, we utilize state-of-the-art object detection networks to accurately detect, count, and localize plant leaves in real-time. Our work includes the creation of an annotated dataset of Arabidopsis plants captured using Cannon Rebel XS camera. These images and annotations have been complied and made publicly available. This dataset is then fed into a Tiny-YOLOv3 network for training. The Tiny-YOLOv3 network is then able to converge and accurately perform real-time localization and counting of the leaves. We also create a simple robotics platform based on an Android phone and iRobot create2 to demonstrate the real-time capabilities of the network in the greenhouse. Additionally, a performance comparison is conducted between Tiny-YOLOv3 and Faster R-CNN. Unlike Tiny-YOLOv3, which is a single network that does localization and identification in a single pass, the Faster R-CNN network requires two steps to do localization and identification. While with Tiny-YOLOv3, inference time, F1 Score, and false positive rate (FPR) are improved compared to Faster R-CNN, other measures such as difference in count (DiC) and AP are worsened. Specifically, for our implementation of Tiny-YOLOv3, the inference time is under 0.01 s, the F1 Score is over 0.94, and the FPR is around 24%. Last, transfer learning using Tiny-YOLOv3 to detect larger leaves on a model trained only on smaller leaves is implemented. The main contributions of the paper are in creating dataset (shared with the research community), as well as the trained Tiny-YOLOv3 network for leaf localization and counting.},
DOI = {10.3390/s20236896}
}



@Article{s20236936,
AUTHOR = {Balaniuk, Remis and Isupova, Olga and Reece, Steven},
TITLE = {Mining and Tailings Dam Detection in Satellite Imagery Using Deep Learning},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6936},
URL = {https://www.mdpi.com/1424-8220/20/23/6936},
ISSN = {1424-8220},
ABSTRACT = {This work explores the combination of free cloud computing, free open-source software, and deep learning methods to analyze a real, large-scale problem: the automatic country-wide identification and classification of surface mines and mining tailings dams in Brazil. Locations of officially registered mines and dams were obtained from the Brazilian government open data resource. Multispectral Sentinel-2 satellite imagery, obtained and processed at the Google Earth Engine platform, was used to train and test deep neural networks using the TensorFlow 2 application programming interface (API) and Google Colaboratory (Colab) platform. Fully convolutional neural networks were used in an innovative way to search for unregistered ore mines and tailing dams in large areas of the Brazilian territory. The efficacy of the approach is demonstrated by the discovery of 263 mines that do not have an official mining concession. This exploratory work highlights the potential of a set of new technologies, freely available, for the construction of low cost data science tools that have high social impact. At the same time, it discusses and seeks to suggest practical solutions for the complex and serious problem of illegal mining and the proliferation of tailings dams, which pose high risks to the population and the environment, especially in developing countries.},
DOI = {10.3390/s20236936}
}



@Article{rs12233974,
AUTHOR = {Mangeruga, Marino and Casavola, Alessandro and Pupo, Francesco and Bruno, Fabio},
TITLE = {An Underwater Pathfinding Algorithm for Optimised Planning of Survey Dives},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3974},
URL = {https://www.mdpi.com/2072-4292/12/23/3974},
ISSN = {2072-4292},
ABSTRACT = {In scientific and technical diving, the survey of unknown or partially unexplored areas is a common task that requires an accurate planning for ensuring the optimal use of resources and the divers&rsquo; safety. In particular, in any kind of diving activity, it is essential to foresee the &ldquo;dive profile&rdquo; that represents the diver&rsquo;s exposure to pressure over time, ensuring that the dive plan complies with the specific safety rules that have to be applied in accordance with the diver&rsquo;s qualification and the environmental conditions. This paper presents a novel approach to dive planning based on an original underwater pathfinding algorithm that computes the best 3D path to follow during the dive in order to be able to maximise the number of points of interest (POIs) visited, while taking into account the safety limitations. The proposed approach, for the first time, considers the morphology of the 3D space in which the dive takes place to compute the best path, taking into account the decompression limits and avoiding the obstacles through the analysis of a 3D map of the site. Moreover, three different cost functions are proposed and evaluated to identify the one that could suit the divers&rsquo; needs better.},
DOI = {10.3390/rs12233974}
}



@Article{su122310150,
AUTHOR = {Zhu, Yongyan and Jeon, Seongwoo and Sung, Hyunchan and Kim, Yoonji and Park, Chiyoung and Cha, Sungeun and Jo, Hyun-woo and Lee, Woo-kyun},
TITLE = {Developing UAV-Based Forest Spatial Information and Evaluation Technology for Efficient Forest Management},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {10150},
URL = {https://www.mdpi.com/2071-1050/12/23/10150},
ISSN = {2071-1050},
ABSTRACT = {Forest spatial information is regularly established and managed as basic data for national forest planning and forest policy establishment. Among them, the grade of vegetation conservation shall be investigated and evaluated according to the value of vegetation conservation. As the collection of field data over large or remote areas is difficult, unmanned aerial vehicles (UAVs) are increasingly being used for this purpose. Consequently, there is a need for research on UAV-monitoring and three-dimensional (3D) image generation techniques. In this study, a new method that can efficiently collect and analyze UAV spatial data to survey and assess forests was developed. Both UAV-based and LiDAR imaging methods were evaluated in conjunction with the ground control point measurement method for forest surveys. In addition, by fusing the field survey database of each target site and the UAV optical and LiDAR images, the Gongju, Samcheok, and Seogwipo regions were analyzed based on deep learning. The kappa value showed 0.59, 0.47, and 0.78 accuracy for each of the sites in terms of vegetation type (artificial or natural), and 0.68, 0.53, and 0.62 accuracy in terms of vegetation layer structure. The results of comparative analysis with ecological natural maps by establishing vegetation conservation levels show that about 83.9% of the areas are consistent. The findings verified the applicability of this UAV-based approach for the construction of geospatial information on forests. The proposed method can be useful for improving the efficiency of the Vegetation Conservation Classification system and for conducting high-resolution monitoring in forests worldwide.},
DOI = {10.3390/su122310150}
}



@Article{ma13235549,
AUTHOR = {Shin, Hyun Kyu and Ahn, Yong Han and Lee, Sang Hyo and Kim, Ha Young},
TITLE = {Automatic Concrete Damage Recognition Using Multi-Level Attention Convolutional Neural Network},
JOURNAL = {Materials},
VOLUME = {13},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {5549},
URL = {https://www.mdpi.com/1996-1944/13/23/5549},
PubMedID = {33291411},
ISSN = {1996-1944},
ABSTRACT = {There has been an increase in the deterioration of buildings and infrastructure in dense urban regions, and several defects in the structures are being exposed. To ensure the effective diagnosis of building conditions, vision-based automatic damage recognition techniques have been developed. However, conventional image processing techniques have some limitations in real-world situations owing to their manual feature extraction approach. To overcome these limitations, a convolutional neural network-based image recognition technique was adopted in this study, and a convolution-based concrete multi-damage recognition neural network (CMDnet) was developed. The image datasets consisted of 1981 types of concrete surface damages, including surface cracks, rebar exposure and delamination, as well as intact. Furthermore, it was experimentally demonstrated that the proposed model could accurately classify the damage types. The results obtained in this study reveal that the proposed model can recognize the different damage types from digital images of the surfaces of concrete structures. The trained CMDnet demonstrated a damage-detection accuracy of 98.9%. Moreover, the proposed model could be applied in automatic damage detection networks to achieve superior performance with regard to concrete surface damage detection and recognition, as well as accelerating efficient damage identification during the diagnosis of deteriorating structures used in civil engineering applications.},
DOI = {10.3390/ma13235549}
}



@Article{electronics9122076,
AUTHOR = {Mariscal-Harana, Jorge and Alarcón, Víctor and González, Fidel and Calvente, Juan José and Pérez-Grau, Francisco Javier and Viguria, Antidio and Ollero, Aníbal},
TITLE = {Audio-Based Aircraft Detection System for Safe RPAS BVLOS Operations},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2076},
URL = {https://www.mdpi.com/2079-9292/9/12/2076},
ISSN = {2079-9292},
ABSTRACT = {For the Remotely Piloted Aircraft Systems (RPAS) market to continue its current growth rate, cost-effective &lsquo;Detect and Avoid&rsquo; systems that enable safe beyond visual line of sight (BVLOS) operations are critical. We propose an audio-based &lsquo;Detect and Avoid&rsquo; system, composed of microphones and an embedded computer, which performs real-time inferences using a sound event detection (SED) deep learning model. Two state-of-the-art SED models, YAMNet and VGGish, are fine-tuned using our dataset of aircraft sounds and their performances are compared for a wide range of configurations. YAMNet, whose MobileNet architecture is designed for embedded applications, outperformed VGGish both in terms of aircraft detection and computational performance. YAMNet&rsquo;s optimal configuration, with &gt;70% true positive rate and precision, results from combining data augmentation and undersampling with the highest available inference frequency (i.e., 10 Hz). While our proposed &lsquo;Detect and Avoid&rsquo; system already allows the detection of small aircraft from sound in real time, additional testing using multiple aircraft types is required. Finally, a larger training dataset, sensor fusion, or remote computations on cloud-based services could further improve system performance.},
DOI = {10.3390/electronics9122076}
}



@Article{ijgi9120728,
AUTHOR = {Zhou, Dongbo and Liu, Shuangjian and Yu, Jie and Li, Hao},
TITLE = {A High-Resolution Spatial and Time-Series Labeled Unmanned Aerial Vehicle Image Dataset for Middle-Season Rice},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {728},
URL = {https://www.mdpi.com/2220-9964/9/12/728},
ISSN = {2220-9964},
ABSTRACT = {The existing remote sensing image datasets target the identification of objects, features, or man-made targets but lack the ability to provide the date and spatial information for the same feature in the time-series images. The spatial and temporal information is important for machine learning methods so that networks can be trained to support precision classification, particularly for agricultural applications of specific crops with distinct phenological growth stages. In this paper, we built a high-resolution unmanned aerial vehicle (UAV) image dataset for middle-season rice. We scheduled the UAV data acquisition in five villages of Hubei Province for three years, including 11 or 13 growing stages in each year that were accompanied by the annual agricultural surveying business. We investigated the accuracy of the vector maps for each field block and the precise information regarding the crops in the field by surveying each village and periodically arranging the UAV flight tasks on a weekly basis during the phenological stages. Subsequently, we developed a method to generate the samples automatically. Finally, we built a high-resolution UAV image dataset, including over 500,000 samples with the location and phenological growth stage information, and employed the imagery dataset in several machine learning algorithms for classification. We performed two exams to test our dataset. First, we used four classical deep learning networks for the fine classification of spatial and temporal information. Second, we used typical models to test the land cover on our dataset and compared this with the UCMerced Land Use Dataset and RSSCN7 Dataset. The results showed that the proposed image dataset supported typical deep learning networks in the classification task to identify the location and time of middle-season rice and achieved high accuracy with the public image dataset.},
DOI = {10.3390/ijgi9120728}
}



@Article{logistics4040033,
AUTHOR = {Haji, Mona and Kerbache, Laoucine and Muhammad, Mahaboob and Al-Ansari, Tareq},
TITLE = {Roles of Technology in Improving Perishable Food Supply Chains},
JOURNAL = {Logistics},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {33},
URL = {https://www.mdpi.com/2305-6290/4/4/33},
ISSN = {2305-6290},
ABSTRACT = {Food supply chains are considered to be more complex systems than other types of supply chains. This complexity is due to the continuous changes taking place, particularly in ensuring the quality of food products throughout the entire supply chain, from growing, procurement of resources, production, and management of stock, to distribution to the final consumers. For that, food supply chain markets have become more highly developed in the use of modern technologies, and have begun to implement them in their logistical systems to satisfy their customers&rsquo; needs. The main objectives of this review are to identify the different technological implementations in different phases of the food supply chain processes and point out the key factors for using technologies to improve the characteristics of the perishable food supply chain. A total number of 137 articles were analyzed in this research to achieve these review objectives. Some of the various technologies found in different phases of the food supply chain were radio frequency identification (RFID), the Internet of Things (IoT), blockchain, three-dimensional printing (3DP), autonomous vehicles, and unmanned aerial vehicles (UAVs). These technologies were found in different phases of the food supply chain and improved the efficiency of supplying perishable foods. The review identified different characteristics of the perishable food supply chain. The main finding indicated that technological implementation enhances the efficiency and sustainability of the food supply chains and helps to retain perishable food characteristics.},
DOI = {10.3390/logistics4040033}
}



@Article{rs12234000,
AUTHOR = {Nevavuori, Petteri and Narra, Nathaniel and Linna, Petri and Lipping, Tarmo},
TITLE = {Crop Yield Prediction Using Multitemporal UAV Data and Spatio-Temporal Deep Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {4000},
URL = {https://www.mdpi.com/2072-4292/12/23/4000},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51% mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17% MAPE with four weekly frames from the beginning of the season.},
DOI = {10.3390/rs12234000}
}



@Article{rs12234003,
AUTHOR = {Li, Yansheng and Chen, Ruixian and Zhang, Yongjun and Zhang, Mi and Chen, Ling},
TITLE = {Multi-Label Remote Sensing Image Scene Classification by Combining a Convolutional Neural Network and a Graph Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {4003},
URL = {https://www.mdpi.com/2072-4292/12/23/4003},
ISSN = {2072-4292},
ABSTRACT = {As one of the fundamental tasks in remote sensing (RS) image understanding, multi-label remote sensing image scene classification (MLRSSC) is attracting increasing research interest. Human beings can easily perform MLRSSC by examining the visual elements contained in the scene and the spatio-topological relationships of these visual elements. However, most of existing methods are limited by only perceiving visual elements but disregarding the spatio-topological relationships of visual elements. With this consideration, this paper proposes a novel deep learning-based MLRSSC framework by combining convolutional neural network (CNN) and graph neural network (GNN), which is termed the MLRSSC-CNN-GNN. Specifically, the CNN is employed to learn the perception ability of visual elements in the scene and generate the high-level appearance features. Based on the trained CNN, one scene graph for each scene is further constructed, where nodes of the graph are represented by superpixel regions of the scene. To fully mine the spatio-topological relationships of the scene graph, the multi-layer-integration graph attention network (GAT) model is proposed to address MLRSSC, where the GAT is one of the latest developments in GNN. Extensive experiments on two public MLRSSC datasets show that the proposed MLRSSC-CNN-GNN can obtain superior performance compared with the state-of-the-art methods.},
DOI = {10.3390/rs12234003}
}



@Article{app10238754,
AUTHOR = {Sultan, Wajeeha and Anjum, Nadeem and Stansfield, Mark and Ramzan, Naeem},
TITLE = {Hybrid Local and Global Deep-Learning Architecture for Salient-Object Detection},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {8754},
URL = {https://www.mdpi.com/2076-3417/10/23/8754},
ISSN = {2076-3417},
ABSTRACT = {Salient-object detection is a fundamental and the most challenging problem in computer vision. This paper focuses on the detection of salient objects, especially in low-contrast images. To this end, a hybrid deep-learning architecture is proposed where features are extracted on both the local and global level. These features are then integrated to extract the exact boundary of the object of interest in an image. Experimentation was performed on five standard datasets, and results were compared with state-of-the-art approaches. Both qualitative and quantitative analyses showed the robustness of the proposed architecture.},
DOI = {10.3390/app10238754}
}



@Article{agronomy10121926,
AUTHOR = {Lyu, Hong-Kun and Yun, Sanghun and Choi, Byeongdae},
TITLE = {Machine Learning Feature Extraction Based on Binary Pixel Quantification Using Low-Resolution Images for Application of Unmanned Ground Vehicles in Apple Orchards},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1926},
URL = {https://www.mdpi.com/2073-4395/10/12/1926},
ISSN = {2073-4395},
ABSTRACT = {Deep learning and machine learning (ML) technologies have been implemented in various applications, and various agriculture technologies are being developed based on image-based object recognition technology. We propose an orchard environment free space recognition technology suitable for developing small-scale agricultural unmanned ground vehicle (UGV) autonomous mobile equipment using a low-cost lightweight processor. We designed an algorithm to minimize the amount of input data to be processed by the ML algorithm through low-resolution grayscale images and image binarization. In addition, we propose an ML feature extraction method based on binary pixel quantification that can be applied to an ML classifier to detect free space for autonomous movement of UGVs from binary images. Here, the ML feature is extracted by detecting the local-lowest points in segments of a binarized image and by defining 33 variables, including local-lowest points, to detect the bottom of a tree trunk. We trained six ML models to select a suitable ML model for trunk bottom detection among various ML models, and we analyzed and compared the performance of the trained models. The ensemble model demonstrated the best performance, and a test was performed using this ML model to detect apple tree trunks from 100 new images. Experimental results indicate that it is possible to recognize free space in an apple orchard environment by learning using approximately 100 low-resolution grayscale images.},
DOI = {10.3390/agronomy10121926}
}



@Article{rs12244010,
AUTHOR = {Liu, Xiang and Liu, Huiyu and Datta, Pawanjeet and Frey, Julian and Koch, Barbara},
TITLE = {Mapping an Invasive Plant Spartina alterniflora by Combining an Ensemble One-Class Classification Algorithm with a Phenological NDVI Time-Series Analysis Approach in Middle Coast of Jiangsu, China},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4010},
URL = {https://www.mdpi.com/2072-4292/12/24/4010},
ISSN = {2072-4292},
ABSTRACT = {Spartina alterniflora (S. alterniflora) is one of the worst plant invaders in the coastal wetlands of China. Accurate and repeatable mapping of S. alterniflora invasion is essential to develop cost-effective management strategies for conserving native biodiversity. Traditional remote-sensing-based mapping methods require a lot of fieldwork for sample collection. Moreover, our ability to detect this invasive species is still limited because of poor spectral separability between S. alterniflora and its co-dominant native plants. Therefore, we proposed a novel scheme that uses an ensemble one-class classifier (EOCC) in combination with phenological Normalized Difference Vegetation Index (NDVI) time-series analysis (TSA) to detect S. alterniflora. We evaluated the performance of the EOCC algorithm in two scenarios, i.e., single-scene analysis (SSA) and NDVI-TSA in the core zones of Yancheng National Natural Reserve (YNNR). Meanwhile, a fully supervised classifier support vector machine (SVM) was tested in the two scenarios for comparison. With these scenarios, the crucial phenological stages and the advantage of phenological NDVI-TSA in S. alterniflora recognition were also investigated. Results indicated the EOCC using only positive training data performed similarly well with the SVM trained on complete training data in the YNNR. Moreover, the EOCC algorithm presented a more robust transferability with notably higher classification accuracy than the SVM when being transferred to a second site, without a second training. Furthermore, when combined with the phenological NDVI-TSA, the EOCC algorithm presented more balanced sensitivity&ndash;specificity result, showing slightly better transferability than it performed in the best phenological stage (i.e., senescence stage of November). The achieved results (overall accuracy (OA), Kappa, and true skill statistic (TSS) were 92.92%, 0.843, and 0.834 for the YNNR, and OA, Kappa, and TSS were 90.94%, 0.815, and 0.825 for transferability to the non-training site) suggest that our detection scheme has a high potential for the mapping of S. alterniflora across different areas, and the EOCC algorithm can be a viable alternative to traditional supervised classification method for invasive plant detection.},
DOI = {10.3390/rs12244010}
}



@Article{en13246496,
AUTHOR = {Pierdicca, Roberto and Paolanti, Marina and Felicetti, Andrea and Piccinini, Fabio and Zingaretti, Primo},
TITLE = {Automatic Faults Detection of Photovoltaic Farms: solAIr, a Deep Learning-Based System for Thermal Images},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {6496},
URL = {https://www.mdpi.com/1996-1073/13/24/6496},
ISSN = {1996-1073},
ABSTRACT = {Renewable energy sources will represent the only alternative to limit fossil fuel usage and pollution. For this reason, photovoltaic (PV) power plants represent one of the main systems adopted to produce clean energy. Monitoring the state of health of a system is fundamental. However, these techniques are time demanding, cause stops to the energy generation, and often require laboratory instrumentation, thus being not cost-effective for frequent inspections. Moreover, PV plants are often located in inaccessible places, making any intervention dangerous. In this paper, we propose solAIr, an artificial intelligence system based on deep learning for anomaly cells detection in photovoltaic images obtained from unmanned aerial vehicles equipped with a thermal infrared sensor. The proposed anomaly cells detection system is based on the mask region-based convolutional neural network (Mask R-CNN) architecture, adopted because it simultaneously performs object detection and instance segmentation, making it useful for the automated inspection task. The proposed system is trained and evaluated on the photovoltaic thermal images dataset, a publicly available dataset collected for this work. Furthermore, the performances of three state-of-art deep neural networks, (DNNs) including UNet, FPNet and LinkNet, are compared and evaluated. Results show the effectiveness and the suitability of the proposed approach in terms of intersection over union (IoU) and the Dice coefficient.},
DOI = {10.3390/en13246496}
}



@Article{rs12244028,
AUTHOR = {Solla, Mercedes and Gonçalves, Luisa M. S. and Gonçalves, Gil and Francisco, Carina and Puente, Iván and Providência, Paulo and Gaspar, Florindo and Rodrigues, Hugo},
TITLE = {A Building Information Modeling Approach to Integrate Geomatic Data for the Documentation and Preservation of Cultural Heritage},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4028},
URL = {https://www.mdpi.com/2072-4292/12/24/4028},
ISSN = {2072-4292},
ABSTRACT = {Non-destructive testing (NDT) techniques play an important role in the characterization and diagnosis of historic buildings, keeping in mind their conservation and possible rehabilitation. This paper presents a new approach that merges building information modeling (BIM) with environment geospatial data obtained by several non-destructive techniques, namely terrestrial laser scanning, ground-penetrating radar, infrared thermography, and the automatic classification of pathologies based on RGB (red, green, blue) imaging acquired with an unmanned aircraft system (UAS). This approach was applied to the inspection of the Monastery of Batalha in Leiria, Portugal, a UNESCO World Heritage Site. To assess the capabilities of each technique, different parts of the monastery were examined, namely (i) part of its west fa&ccedil;ade, including a few protruding buttresses, and (ii) the masonry vaults of the Church (nave, right-hand aisle, and transept) and the Founder&rsquo;s Chapel. After describing the employed techniques, a discussion of the optimization, treatment and integration of the acquired data through the BIM approach is presented. This work intends to contribute to the application of BIM in the field of cultural heritage, aiming at its future use in different activities such as facility management, support in the restoration and rehabilitation process, and research.},
DOI = {10.3390/rs12244028}
}



@Article{s20247061,
AUTHOR = {Yang, Zhao and Tang, Rong and Bao, Jie and Lu, Jiahuan and Zhang, Zhijie},
TITLE = {A Real-Time Trajectory Prediction Method of Small-Scale Quadrotors Based on GPS Data and Neural Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7061},
URL = {https://www.mdpi.com/1424-8220/20/24/7061},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a real-time trajectory prediction method for quadrotors based on a bidirectional gated recurrent unit model. Historical trajectory data of ten types of quadrotors were obtained. The bidirectional gated recurrent units were constructed and utilized to learn the historic data. The prediction results were compared with the traditional gated recurrent unit method to test its prediction performance. The efficiency of the proposed algorithm was investigated by comparing the training loss and training time. The results over the testing datasets showed that the proposed model produced better prediction results than the baseline models for all scenarios of the testing datasets. It was also found that the proposed model can converge to a stable state faster than the traditional gated recurrent unit model. Moreover, various types of training samples were applied and compared. With the same randomly selected test datasets, the performance of the prediction model can be improved by selecting the historical trajectory samples of the quadrotors close to the weight or volume of the target quadrotor for training. In addition, the performance of stable trajectory samples is significantly better than that with unstable trajectory segments with a frequent change of speed and direction with large angles.},
DOI = {10.3390/s20247061}
}



@Article{s20247071,
AUTHOR = {Kashiyama, Takehiro and Sobue, Hideaki and Sekimoto, Yoshihide},
TITLE = {Sky Monitoring System for Flying Object Detection Using 4K Resolution Camera},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7071},
URL = {https://www.mdpi.com/1424-8220/20/24/7071},
ISSN = {1424-8220},
ABSTRACT = {The use of drones and other unmanned aerial vehicles has expanded rapidly in recent years. These devices are expected to enter practical use in various fields, such as taking measurements through aerial photography and transporting small and lightweight objects. Simultaneously, concerns over these devices being misused for terrorism or other criminal activities have increased. In response, several sensor systems have been developed to monitor drone flights. In particular, with the recent progress of deep neural network technology, the monitoring of systems using image processing has been proposed. This study developed a monitoring system for flying objects using a 4K camera and a state-of-the-art convolutional neural network model to achieve real-time processing. We installed a monitoring system in a high-rise building in an urban area during this study and evaluated the precision with which it could detect flying objects at different distances under different weather conditions. The results obtained provide important information for determining the accuracy of monitoring systems with image processing in practice.},
DOI = {10.3390/s20247071}
}



@Article{app10248833,
AUTHOR = {Acción, Álvaro and Argüello, Francisco and Heras, Dora B.},
TITLE = {Dual-Window Superpixel Data Augmentation for Hyperspectral Image Classification},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {8833},
URL = {https://www.mdpi.com/2076-3417/10/24/8833},
ISSN = {2076-3417},
ABSTRACT = {Deep learning (DL) has been shown to obtain superior results for classification tasks in the field of remote sensing hyperspectral imaging. Superpixel-based techniques can be applied to DL, significantly decreasing training and prediction times, but the results are usually far from satisfactory due to overfitting. Data augmentation techniques alleviate the problem by synthetically generating new samples from an existing dataset in order to improve the generalization capabilities of the classification model. In this paper we propose a novel data augmentation framework in the context of superpixel-based DL called dual-window superpixel (DWS). With DWS, data augmentation is performed over patches centered on the superpixels obtained by the application of simple linear iterative clustering (SLIC) superpixel segmentation. DWS is based on dividing the input patches extracted from the superpixels into two regions and independently applying transformations over them. As a result, four different data augmentation techniques are proposed that can be applied to a superpixel-based CNN classification scheme. An extensive comparison in terms of classification accuracy with other data augmentation techniques from the literature using two datasets is also shown. One of the datasets consists of small hyperspectral small scenes commonly found in the literature. The other consists of large multispectral vegetation scenes of river basins. The experimental results show that the proposed approach increases the overall classification accuracy for the selected datasets. In particular, two of the data augmentation techniques introduced, namely, dual-flip and dual-rotate, obtained the best results.},
DOI = {10.3390/app10248833}
}



@Article{a13120333,
AUTHOR = {Celis, Raúl de and Solano, Pablo and Cadarso, Luis},
TITLE = {Applying Neural Networks in Aerial Vehicle Guidance to Simplify Navigation Systems},
JOURNAL = {Algorithms},
VOLUME = {13},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {333},
URL = {https://www.mdpi.com/1999-4893/13/12/333},
ISSN = {1999-4893},
ABSTRACT = {The Guidance, Navigation and Control (GNC) of air and space vehicles has been one of the spearheads of research in the aerospace field in recent times. Using Global Navigation Satellite Systems (GNSS) and inertial navigation systems, accuracy may be detached from range. However, these sensor-based GNC systems may cause significant errors in determining attitude and position. These effects can be ameliorated using additional sensors, independent of cumulative errors. The quadrant photodetector semiactive laser is a good candidate for such a purpose. However, GNC systems&rsquo; development and construction costs are high. Reducing costs, while maintaining safety and accuracy standards, is key for development in aerospace engineering. Advanced algorithms for getting such standards while eliminating sensors are cornerstone. The development and application of machine learning techniques to GNC poses an innovative path for reducing complexity and costs. Here, a new nonlinear hybridization algorithm, which is based on neural networks, to estimate the gravity vector is presented. Using a neural network means that once it is trained, the physical-mathematical foundations of flight are not relevant; it is the network that returns dynamics to be fed to the GNC algorithm. The gravity vector, which can be accurately predicted, is used to determine vehicle attitude without calling for gyroscopes. Nonlinear simulations based on real flight dynamics are used to train the neural networks. Then, the approach is tested and simulated together with a GNC system. Monte Carlo analysis is conducted to determine performance when uncertainty arises. Simulation results prove that the performance of the presented approach is robust and precise in a six-degree-of-freedom simulation environment.},
DOI = {10.3390/a13120333}
}



@Article{jimaging6120137,
AUTHOR = {Bhuiyan, Md Abul Ehsan and Witharana, Chandi and Liljedahl, Anna K.},
TITLE = {Use of Very High Spatial Resolution Commercial Satellite Imagery and Deep Learning to Automatically Map Ice-Wedge Polygons across Tundra Vegetation Types},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {137},
URL = {https://www.mdpi.com/2313-433X/6/12/137},
ISSN = {2313-433X},
ABSTRACT = {We developed a high-throughput mapping workflow, which centers on deep learning (DL) convolutional neural network (CNN) algorithms on high-performance distributed computing resources, to automatically characterize ice-wedge polygons (IWPs) from sub-meter resolution commercial satellite imagery. We applied a region-based CNN object instance segmentation algorithm, namely the Mask R-CNN, to automatically detect and classify IWPs in North Slope of Alaska. The central goal of our study was to systematically expound the DLCNN model interoperability across varying tundra types (sedge, tussock sedge, and non-tussock sedge) and image scene complexities to refine the understanding of opportunities and challenges for regional-scale mapping applications. We corroborated quantitative error statistics along with detailed visual inspections to gauge the IWP detection accuracies. We found promising model performances (detection accuracies: 89% to 96% and classification accuracies: 94% to 97%) for all candidate image scenes with varying tundra types. The mapping workflow discerned the IWPs by exhibiting low absolute mean relative error (AMRE) values (0.17&ndash;0.23). Results further suggest the importance of increasing the variability of training samples when practicing transfer-learning strategy to map IWPs across heterogeneous tundra cover types. Overall, our findings demonstrate the robust performances of IWPs mapping workflow in multiple tundra landscapes.},
DOI = {10.3390/jimaging6120137}
}



@Article{ijgi9120743,
AUTHOR = {Murtiyoso, Arnadi and Veriandi, Mirza and Suwardhi, Deni and Soeksmantono, Budhy and Harto, Agung Budi},
TITLE = {Automatic Workflow for Roof Extraction and Generation of 3D CityGML Models from Low-Cost UAV Image-Derived Point Clouds},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {743},
URL = {https://www.mdpi.com/2220-9964/9/12/743},
ISSN = {2220-9964},
ABSTRACT = {Developments in UAV sensors and platforms in recent decades have stimulated an upsurge in its application for 3D mapping. The relatively low-cost nature of UAVs combined with the use of revolutionary photogrammetric algorithms, such as dense image matching, has made it a strong competitor to aerial lidar mapping. However, in the context of 3D city mapping, further 3D modeling is required to generate 3D city models which is often performed manually using, e.g., photogrammetric stereoplotting. The aim of the paper was to try to implement an algorithmic approach to building point cloud segmentation, from which an automated workflow for the generation of roof planes will also be presented. 3D models of buildings are then created using the roofs’ planes as a base, therefore satisfying the requirements for a Level of Detail (LoD) 2 in the CityGML paradigm. Consequently, the paper attempts to create an automated workflow starting from UAV-derived point clouds to LoD 2-compatible 3D model. Results show that the rule-based segmentation approach presented in this paper works well with the additional advantage of instance segmentation and automatic semantic attribute annotation, while the 3D modeling algorithm performs well for low to medium complexity roofs. The proposed workflow can therefore be implemented for simple roofs with a relatively low number of planar surfaces. Furthermore, the automated approach to the 3D modeling process also helps to maintain the geometric requirements of CityGML such as 3D polygon coplanarity vis-à-vis manual stereoplotting.},
DOI = {10.3390/ijgi9120743}
}



@Article{rs12244070,
AUTHOR = {Ellsäßer, Florian and Röll, Alexander and Ahongshangbam, Joyson and Waite, Pierre-André and Hendrayanto and Schuldt, Bernhard and Hölscher, Dirk},
TITLE = {Predicting Tree Sap Flux and Stomatal Conductance from Drone-Recorded Surface Temperatures in a Mixed Agroforestry System—A Machine Learning Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4070},
URL = {https://www.mdpi.com/2072-4292/12/24/4070},
ISSN = {2072-4292},
ABSTRACT = {Plant transpiration is a key element in the hydrological cycle. Widely used methods for its assessment comprise sap flux techniques for whole-plant transpiration and porometry for leaf stomatal conductance. Recently emerging approaches based on surface temperatures and a wide range of machine learning techniques offer new possibilities to quantify transpiration. The focus of this study was to predict sap flux and leaf stomatal conductance based on drone-recorded and meteorological data and compare these predictions with in-situ measured transpiration. To build the prediction models, we applied classical statistical approaches and machine learning algorithms. The field work was conducted in an oil palm agroforest in lowland Sumatra. Random forest predictions yielded the highest congruence with measured sap flux (r2 = 0.87 for trees and r2 = 0.58 for palms) and confidence intervals for intercept and slope of a Passing-Bablok regression suggest interchangeability of the methods. Differences in model performance are indicated when predicting different tree species. Predictions for stomatal conductance were less congruent for all prediction methods, likely due to spatial and temporal offsets of the measurements. Overall, the applied drone and modelling scheme predicts whole-plant transpiration with high accuracy. We conclude that there is large potential in machine learning approaches for ecological applications such as predicting transpiration.},
DOI = {10.3390/rs12244070}
}



@Article{rs12244080,
AUTHOR = {Kavats, Olena and Khramov, Dmitriy and Sergieieva, Kateryna and Vasyliev, Volodymyr},
TITLE = {Monitoring of Sugarcane Harvest in Brazil Based on Optical and SAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4080},
URL = {https://www.mdpi.com/2072-4292/12/24/4080},
ISSN = {2072-4292},
ABSTRACT = {The algorithms for determining sugarcane harvest dates are proposed; the algorithms allow the ability to monitor large areas and are based on the publicly available Synthetic Aperture Radar (SAR) and optical satellite data. Algorithm 1 uses the NDVI (Normalized Difference Vegetation Index) time series derived from Sentinel-2 data. Sharp and continuous decrease in the NDVI values is the main sign of sugarcane harvest. The NDVI time series allows the ability to determine most harvest dates. The best estimates of the sugarcane areas harvested per month have been obtained from March to August 2018 when cloudy pixel percentage is less than 45% of the image area. Algorithm 2 of the harvest monitoring uses the coherence time series derived from Sentinel-1 Single Look Complex (SLC) images and optical satellite data. Low coherence, demonstrating sharp growth upon the harvest completion, corresponds to the harvest period. The NDVI time series trends were used to refine the algorithm. It is supposed that the descending NDVI trend corresponds to harvest. The algorithms were used to identify the harvest dates and calculate the harvested areas of the reference sample of 574 sugarcane parcels with a total area of 3745 ha in the state of S&atilde;o Paulo, Brazil. The harvested areas identified by visual interpretation coincide with the optical-data algorithm (algorithm 1) by 97%; the coincidence with the algorithm based on SAR and optical data (algorithm 2) is 90%. The main practical applications of the algorithms are harvest monitoring and identification of the harvested fields to estimate the harvested area.},
DOI = {10.3390/rs12244080}
}



@Article{ani10122387,
AUTHOR = {Hyun, Chang-Uk and Park, Mijin and Lee, Won Young},
TITLE = {Remotely Piloted Aircraft System (RPAS)-Based Wildlife Detection: A Review and Case Studies in Maritime Antarctica},
JOURNAL = {Animals},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2387},
URL = {https://www.mdpi.com/2076-2615/10/12/2387},
PubMedID = {33327472},
ISSN = {2076-2615},
ABSTRACT = {In wildlife biology, it is important to conduct efficient observations and quantitative monitoring of wild animals. Conventional wildlife monitoring mainly relies on direct field observations by the naked eyes or through binoculars, on-site image acquisition at fixed spots, and sampling or capturing under severe areal constraints. Recently, remotely piloted aircraft systems (RPAS), also called drones or unmanned aerial vehicles (UAV), were successfully applied to detect wildlife with imaging sensors, such as RGB and thermal-imaging sensors, with superior detection capabilities to those of human observation. Here, we review studies with RPAS which has been increasingly used in wildlife detection and explain how an RPAS-based high-resolution RGB image can be applied to wild animal studies from the perspective of individual detection and population surveys as well as behavioral studies. The applicability of thermal-imaging sensors was also assessed with further information extractable from image analyses. In addition, RPAS-based case studies of acquisition of high-resolution RGB images for the purpose of detecting southern elephant seals (Mirounga leonina) and shape property extraction using thermal-imaging sensor in King George Island, maritime Antarctica is presented as applications in an extreme environment. The case studies suggest that currently available cost-effective small-sized RPAS, which are capable of flexible operation and mounting miniaturized imaging sensors, and are easily maneuverable even from an inflatable boat, can be an effective and supportive technique for both the visual interpretation and quantitative analysis of wild animals in low-accessible extreme or maritime environments.},
DOI = {10.3390/ani10122387}
}



@Article{electronics9122144,
AUTHOR = {Fuentes, Jose Eduardo and Moya, Francisco David and Montoya, Oscar Danilo},
TITLE = {Method for Estimating Solar Energy Potential Based on Photogrammetry from Unmanned Aerial Vehicles},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2144},
URL = {https://www.mdpi.com/2079-9292/9/12/2144},
ISSN = {2079-9292},
ABSTRACT = {This study presents a method to estimate the solar energy potential based on 3D data taken from unmanned aerial devices. The solar energy potential on the roof of a building was estimated before the placement of solar panels using photogrammetric data analyzed in a geographic information system, and the predictions were compared with the data recorded after installation. The areas of the roofs were chosen using digital surface models and the hemispherical viewshed algorithm, considering how the solar radiation on the roof surface would be affected by the orientation of the surface with respect to the sun, the shade of trees, surrounding objects, topography, and the atmospheric conditions. The results show that the efficiency percentages of the panels and the data modeled by the proposed method from surface models are very similar to the theoretical efficiency of the panels. Radiation potential can be estimated from photogrammetric data and a 3D model in great detail and at low cost. This method allows the estimation of solar potential as well as the optimization of the location and orientation of solar panels.},
DOI = {10.3390/electronics9122144}
}



@Article{s20247176,
AUTHOR = {Gou, Huabei and Guo, Xiao and Lou, Wenjie and Ou, Jiajun and Yuan, Jiace},
TITLE = {Path Following Control for Underactuated Airships with Magnitude and Rate Saturation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7176},
URL = {https://www.mdpi.com/1424-8220/20/24/7176},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a reinforcement learning (RL) based path following strategy for underactuated airships with magnitude and rate saturation. The Markov decision process (MDP) model for the control problem is established. Then an error bounded line-of-sight (LOS) guidance law is investigated to restrain the state space. Subsequently, a proximal policy optimization (PPO) algorithm is employed to approximate the optimal action policy through trial and error. Since the optimal action policy is generated from the action space, the magnitude and rate saturation can be avoided. The simulation results, involving circular, general, broken-line, and anti-wind path following tasks, demonstrate that the proposed control scheme can transfer to new tasks without adaptation, and possesses satisfying real-time performance and robustness.},
DOI = {10.3390/s20247176}
}



@Article{rs12244091,
AUTHOR = {Hussain, Nazar and Farooque, Aitazaz A. and Schumann, Arnold W. and McKenzie-Gopsill, Andrew and Esau, Travis and Abbas, Farhat and Acharya, Bishnu and Zaman, Qamar},
TITLE = {Design and Development of a Smart Variable Rate Sprayer Using Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4091},
URL = {https://www.mdpi.com/2072-4292/12/24/4091},
ISSN = {2072-4292},
ABSTRACT = {The uniform application (UA) of agrochemicals results in the over-application of harmful chemicals, increases crop input costs, and deteriorates the environment when compared with variable rate application (VA). A smart variable rate sprayer (SVRS) was designed, developed, and tested using deep learning (DL) for VA application of agrochemicals. Real-time testing of the SVRS took place for detecting and spraying and/or skipping lambsquarters weed and early blight infected and healthy potato plants. About 24,000 images were collected from potato fields in Prince Edward Island and New Brunswick under varying sunny, cloudy, and partly cloudy conditions and processed/trained using YOLOv3 and tiny-YOLOv3 models. Due to faster performance, the tiny-YOLOv3 was chosen to deploy in SVRS. A laboratory experiment was designed under factorial arrangements, where the two spraying techniques (UA and VA) and the three weather conditions (cloudy, partly cloudy, and sunny) were the two independent variables with spray volume consumption as a response variable. The experimental treatments had six repetitions in a 2 × 3 factorial design. Results of the two-way ANOVA showed a significant effect of spraying application techniques on volume consumption of spraying liquid (p-value &lt; 0.05). There was no significant effect of weather conditions and interactions between the two independent variables on volume consumption during weeds and simulated diseased plant detection experiments (p-value &gt; 0.05). The SVRS was able to save 42 and 43% spraying liquid during weeds and simulated diseased plant detection experiments, respectively. Water sensitive papers’ analysis showed the applicability of SVRS for VA with &gt;40% savings of spraying liquid by SVRS when compared with UA. Field applications of this technique would reduce the crop input costs and the environmental risks in conditions (weed and disease) like experimental testing.},
DOI = {10.3390/rs12244091}
}



@Article{rs12244104,
AUTHOR = {Chadwick, Andrew J. and Goodbody, Tristan R. H. and Coops, Nicholas C. and Hervieux, Anne and Bater, Christopher W. and Martens, Lee A. and White, Barry and Röeser, Dominik},
TITLE = {Automatic Delineation and Height Measurement of Regenerating Conifer Crowns under Leaf-Off Conditions Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4104},
URL = {https://www.mdpi.com/2072-4292/12/24/4104},
ISSN = {2072-4292},
ABSTRACT = {The increasing use of unmanned aerial vehicles (UAV) and high spatial resolution imagery from associated sensors necessitates the continued advancement of efficient means of image processing to ensure these tools are utilized effectively. This is exemplified in the field of forest management, where the extraction of individual tree crown information stands to benefit operational budgets. We explored training a region-based convolutional neural network (Mask R-CNN) to automatically delineate individual tree crown (ITC) polygons in regenerating forests (14 years after harvest) using true colour red-green-blue (RGB) imagery with an average ground sampling distance (GSD) of 3 cm. We predicted ITC polygons to extract height information using canopy height models generated from digital aerial photogrammetric (DAP) point clouds. Our approach yielded an average precision of 0.98, an average recall of 0.85, and an average F1 score of 0.91 for the delineation of ITC. Remote height measurements were strongly correlated with field height measurements (r2 = 0.93, RMSE = 0.34 m). The mean difference between DAP-derived and field-collected height measurements was &minus;0.37 m and &minus;0.24 m for white spruce (Picea glauca) and lodgepole pine (Pinus contorta), respectively. Our results show that accurate ITC delineation in young, regenerating stands is possible with fine-spatial resolution RGB imagery and that predicted ITC can be used in combination with DAP to estimate tree height.},
DOI = {10.3390/rs12244104}
}



@Article{rs12244118,
AUTHOR = {Wang, Nan and Xue, Jie and Peng, Jie and Biswas, Asim and He, Yong and Shi, Zhou},
TITLE = {Integrating Remote Sensing and Landscape Characteristics to Estimate Soil Salinity Using Machine Learning Methods: A Case Study from Southern Xinjiang, China},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4118},
URL = {https://www.mdpi.com/2072-4292/12/24/4118},
ISSN = {2072-4292},
ABSTRACT = {Soil salinization, one of the most severe global land degradation problems, leads to the loss of arable land and declines in crop yields. Monitoring the distribution of salinized soil and degree of salinization is critical for management, remediation, and utilization of salinized soil; however, there is a lack of thorough assessment of various data sources including remote sensing and landscape characteristics for estimating soil salinity in arid and semi-arid areas. The overall goal of this study was to develop a framework for estimating soil salinity in diverse landscapes by fusing information from satellite images, landscape characteristics, and appropriate machine learning models. To explore the spatial distribution of soil salinity in southern Xinjiang, China, as a case study, we obtained 151 soil samples in a field campaign, which were analyzed in laboratory for soil electrical conductivity. A total of 35 indices including remote sensing classifiers (11), terrain attributes (3), vegetation spectral indices (8), and salinity spectral indices (13) were calculated or derived and correlated with soil salinity. Nine were used to model and estimate soil salinity using four predictive modelling approaches: partial least squares regression (PLSR), convolutional neural network (CNN), support vector machine (SVM) learning, and random forest (RF). Testing datasets were divided into vegetation-covered and bare soil samples and were used for accuracy assessment. The RF model was the best regression model in this study, with R2 = 0.75, and was most effective in revealing the spatial characteristics of salt distribution. Importance analysis and path modeling of independent variables indicated that environmental factors and soil salinity indices including digital elevation model (DEM), B10, and green atmospherically resistant vegetation index (GARI) showed the strongest contribution in soil salinity estimation. This showed a great promise in the measurement and monitoring of soil salinity in arid and semi-arid areas from the integration of remote sensing, landscape characteristics, and using machine learning model.},
DOI = {10.3390/rs12244118}
}



@Article{app10249013,
AUTHOR = {Manrique Escobar, Camilo Andrés and Pappalardo, Carmine Maria and Guida, Domenico},
TITLE = {A Parametric Study of a Deep Reinforcement Learning Control System Applied to the Swing-Up Problem of the Cart-Pole},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {9013},
URL = {https://www.mdpi.com/2076-3417/10/24/9013},
ISSN = {2076-3417},
ABSTRACT = {In this investigation, the nonlinear swing-up problem associated with the cart-pole system modeled as a multibody dynamical system is solved by developing a deep Reinforcement Learning (RL) controller. Furthermore, the sensitivity analysis of the deep RL controller applied to the cart-pole swing-up problem is carried out. To this end, the influence of modifying the physical properties of the system and the presence of dry friction forces are analyzed employing the cumulative reward during the task. Extreme limits for the modifications of the parameters are determined to prove that the neural network architecture employed in this work features enough learning capability to handle the task under modifications as high as 90% on the pendulum mass, as well as a 100% increment on the cart mass. As expected, the presence of dry friction greatly affects the performance of the controller. However, a post-training of the agent in the modified environment takes only thirty-nine episodes to find the optimal control policy, resulting in a promising path for further developments of robust controllers.},
DOI = {10.3390/app10249013}
}



@Article{electronics9122163,
AUTHOR = {Cañas, José M. and Martín-Martín, Diego and Arias, Pedro and Vega, Julio and Roldán-Álvarez, David and García-Pérez, Lía and Fernández-Conde, Jesús},
TITLE = {Open-Source Drone Programming Course for Distance Engineering Education},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2163},
URL = {https://www.mdpi.com/2079-9292/9/12/2163},
ISSN = {2079-9292},
ABSTRACT = {This article presents a full course for autonomous aerial robotics inside the RoboticsAcademy framework. This &ldquo;drone programming&rdquo; course is open-access and ready-to-use for any teacher/student to teach/learn drone programming with it for free. The students may program diverse drones on their computers without a physical presence in this course. Unmanned aerial vehicles (UAV) applications are essentially practical, as their intelligence resides in the software part. Therefore, the proposed course emphasizes drone programming through practical learning. It comprises a collection of exercises resembling drone applications in real life, such as following a road, visual landing, and people search and rescue, including their corresponding background theory. The course has been successfully taught for five years to students from several university engineering degrees. Some exercises from the course have also been validated in three aerial robotics competitions, including an international one. RoboticsAcademy is also briefly presented in the paper. It is an open framework for distance robotics learning in engineering degrees. It has been designed as a practical complement to the typical online videos of massive open online courses (MOOCs). Its educational contents are built upon robot operating system (ROS) middleware (de facto standard in robot programming), the powerful 3D Gazebo simulator, and the widely used Python programming language. Additionally, RoboticsAcademy is a suitable tool for gamified learning and online robotics competitions, as it includes several competitive exercises and automatic assessment tools.},
DOI = {10.3390/electronics9122163}
}



@Article{s20247245,
AUTHOR = {Shi, Chenqi and Niu, Xinyv and Li, Tao and Li, Sen and Huang, Chanjuan and Niu, Qiang},
TITLE = {Exploring Fast Fingerprint Construction Algorithm for Unmodulated Visible Light Indoor Localization},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7245},
URL = {https://www.mdpi.com/1424-8220/20/24/7245},
ISSN = {1424-8220},
ABSTRACT = {The study of visible light indoor position has received considerable attention. The visible light indoor position has problems such as deployment difficulty and high cost. In our system, we propose a new fingerprint construction algorithm to simplify visible light indoor position. This method can realize the rapid construction of a visible fingerprint database and prove that the fingerprint database can be used repeatedly in different environments. We proved the theoretical feasibility of this method through theoretical derivation. We carried out extensive experiments in two classic real indoor environments. Experimental results show that reverse fingerprinting can be achieved. In 95% of cases, the positioning accuracy can be guaranteed to be less than 10 cm.},
DOI = {10.3390/s20247245}
}



@Article{rs12244135,
AUTHOR = {Rajendran, Ganesh B. and Kumarasamy, Uma M. and Zarro, Chiara and Divakarachari, Parameshachari B. and Ullo, Silvia L.},
TITLE = {Land-Use and Land-Cover Classification Using a Human Group-Based Particle Swarm Optimization Algorithm with an LSTM Classifier on Hybrid Pre-Processing Remote-Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4135},
URL = {https://www.mdpi.com/2072-4292/12/24/4135},
ISSN = {2072-4292},
ABSTRACT = {Land-use and land-cover (LULC) classification using remote sensing imagery plays a vital role in many environment modeling and land-use inventories. In this study, a hybrid feature optimization algorithm along with a deep learning classifier is proposed to improve the performance of LULC classification, helping to predict wildlife habitat, deteriorating environmental quality, haphazard elements, etc. LULC classification is assessed using Sat 4, Sat 6 and Eurosat datasets. After the selection of remote-sensing images, normalization and histogram equalization methods are used to improve the quality of the images. Then, a hybrid optimization is accomplished by using the local Gabor binary pattern histogram sequence (LGBPHS), the histogram of oriented gradient (HOG) and Haralick texture features, for the feature extraction from the selected images. The benefits of this hybrid optimization are a high discriminative power and invariance to color and grayscale images. Next, a human group-based particle swarm optimization (PSO) algorithm is applied to select the optimal features, whose benefits are a fast convergence rate and ease of implementation. After selecting the optimal feature values, a long short-term memory (LSTM) network is utilized to classify the LULC classes. Experimental results showed that the human group-based PSO algorithm with a LSTM classifier effectively well differentiates the LULC classes in terms of classification accuracy, recall and precision. A maximum improvement of 6.03% on Sat 4 and 7.17% on Sat 6 in LULC classification is reached when the proposed human group-based PSO with LSTM is compared to individual LSTM, PSO with LSTM, and Human Group Optimization (HGO) with LSTM. Moreover, an improvement of 2.56% in accuracy is achieved, compared to the existing models, GoogleNet, Visual Geometric Group (VGG), AlexNet, ConvNet, when the proposed method is applied.},
DOI = {10.3390/rs12244135}
}



@Article{agronomy10121989,
AUTHOR = {Armenta-Medina, Dagoberto and Ramirez-delReal, Tania A. and Villanueva-Vásquez, Daniel and Mejia-Aguirre, Cristian},
TITLE = {Trends on Advanced Information and Communication Technologies for Improving Agricultural Productivities: A Bibliometric Analysis},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1989},
URL = {https://www.mdpi.com/2073-4395/10/12/1989},
ISSN = {2073-4395},
ABSTRACT = {In this work, an exhaustive revision is given of the literature associated with advanced information and communication technologies in agriculture within a window of 25 years using bibliometric tools enabled to detect of the main actors, structure, and dynamics in the scientific papers. The main findings are a trend of growth in the dynamics of publications associated with advanced information and communication technologies in agriculture productivity. Another assertion is that countries, like the USA, China, and Brazil, stand out in many publications due to allocating more resources to research, development, and agricultural productivity. In addition, the collaboration networks between countries are frequently in regions with closer cultural and idiomatic ties; additionally, terms&rsquo; occurrence are obtained with Louvain algorithm predominating four clusters: precision agriculture, smart agriculture, remote sensing, and climate smart agriculture. Finally, the thematic-map characterization with Callon&rsquo;s density and centrality is applied in three periods. The first period of thematic analysis shows a transition in detecting the variability of a nutrient, such as nitrogen, through the help of immature georeferenced techniques, towards greater remote sensing involvement. In the transition from the second to the third stage, the maturation of technologies, such as unmanned aerial vehicles, wireless sensor networks, and the machine learning area, is observed.},
DOI = {10.3390/agronomy10121989}
}



@Article{app10249051,
AUTHOR = {Wonsick, Murphy and Padir, Taskin},
TITLE = {A Systematic Review of Virtual Reality Interfaces for Controlling and Interacting with Robots},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {9051},
URL = {https://www.mdpi.com/2076-3417/10/24/9051},
ISSN = {2076-3417},
ABSTRACT = {There is a significant amount of synergy between virtual reality (VR) and the field of robotics. However, it has only been in approximately the past five years that commercial immersive VR devices have been available to developers. This new availability has led to a rapid increase in research using VR devices in the field of robotics, especially in the development of VR interfaces for operating robots. In this paper, we present a systematic review on VR interfaces for robot operation that utilize commercially available immersive VR devices. A total of 41 papers published between 2016&ndash;2020 were collected for review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Papers are discussed and categorized into five categories: (1) Visualization, which focuses on displaying data or information to operators; (2) Robot Control and Planning, which focuses on connecting human input or movement to robot movement; (3) Interaction, which focuses on the development of new interaction techniques and/or identifying best interaction practices; (4) Usability, which focuses on user experiences of VR interfaces; and (5) Infrastructure, which focuses on system architectures or software to support connecting VR and robots for interface development. Additionally, we provide future directions to continue development in VR interfaces for operating robots.},
DOI = {10.3390/app10249051}
}



@Article{jsan9040059,
AUTHOR = {De Vita, Fabrizio and Bruneo, Dario},
TITLE = {Leveraging Stack4Things for Federated Learning in Intelligent Cyber Physical Systems},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2224-2708/9/4/59},
ISSN = {2224-2708},
ABSTRACT = {During the last decade, the Internet of Things acted as catalyst for the big data phenomenon. As result, modern edge devices can access a huge amount of data that can be exploited to build useful services. In such a context, artificial intelligence has a key role to develop intelligent systems (e.g., intelligent cyber physical systems) that create a connecting bridge with the physical world. However, as time goes by, machine and deep learning applications are becoming more complex, requiring increasing amounts of data and training time, which makes the use of centralized approaches unsuitable. Federated learning is an emerging paradigm which enables the cooperation of edge devices to learn a shared model (while keeping private their training data), thereby abating the training time. Although federated learning is a promising technique, its implementation is difficult and brings a lot of challenges. In this paper, we present an extension of Stack4Things, a cloud platform developed in our department; leveraging its functionalities, we enabled the deployment of federated learning on edge devices without caring their heterogeneity. Experimental results show a comparison with a centralized approach and demonstrate the effectiveness of the proposed approach in terms of both training time and model accuracy.},
DOI = {10.3390/jsan9040059}
}



@Article{electronics9122178,
AUTHOR = {Lee, Hojun and Kang, Minhee and Song, Jaein and Hwang, Keeyeon},
TITLE = {The Detection of Black Ice Accidents for Preventative Automated Vehicles Using Convolutional Neural Networks},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2178},
URL = {https://www.mdpi.com/2079-9292/9/12/2178},
ISSN = {2079-9292},
ABSTRACT = {Automated Vehicles (AVs) are expected to dramatically reduce traffic accidents that have occurred when using human driving vehicles (HVs). However, despite the rapid development of AVs, accidents involving AVs can occur even in ideal situations. Therefore, in order to enhance their safety, &ldquo;preventive design&rdquo; for accidents is continuously required. Accordingly, the &ldquo;preventive design&rdquo; that prevents accidents in advance is continuously required to enhance the safety of AVs. Specially, black ice with characteristics that are difficult to identify with the naked eye&mdash;the main cause of major accidents in winter vehicles&mdash;is expected to cause serious injuries in the era of AVs, and measures are needed to prevent them. Therefore, this study presents a Convolutional Neural Network (CNN)-based black ice detection plan to prevent traffic accidents of AVs caused by black ice. Due to the characteristic of black ice that is formed only in a certain environment, we augmented image data and learned road environment images. Tests showed that the proposed CNN model detected black ice with 96% accuracy and reproducibility. It is expected that the CNN model for black ice detection proposed in this study will contribute to improving the safety of AVs and prevent black ice accidents in advance.},
DOI = {10.3390/electronics9122178}
}



@Article{rs12244149,
AUTHOR = {Samarin, Maxim and Zweifel, Lauren and Roth, Volker and Alewell, Christine},
TITLE = {Identifying Soil Erosion Processes in Alpine Grasslands on Aerial Imagery with a U-Net Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4149},
URL = {https://www.mdpi.com/2072-4292/12/24/4149},
ISSN = {2072-4292},
ABSTRACT = {Erosion in alpine grasslands is a major threat to ecosystem services of alpine soils. Natural causes for the occurrence of soil erosion are steep topography and prevailing climate conditions in combination with soil fragility. To increase our understanding of ongoing erosion processes and support sustainable land-use management, there is a need to acquire detailed information on spatial occurrence and temporal trends. Existing approaches to identify these trends are typically laborious, have lack of transferability to other regions, and are consequently only applicable to smaller regions. In order to overcome these limitations and create a sophisticated erosion monitoring tool capable of large-scale analysis, we developed a model based on U-Net, a fully convolutional neural network, to map different erosion processes on high-resolution aerial images (RGB, 0.25&ndash;0.5 m). U-Net was trained on a high-quality data set consisting of labeled erosion sites mapped with object-based image analysis (OBIA) for the Urseren Valley (Central Swiss Alps) for five aerial images (16 year period). We used the U-Net model to map the same study area and conduct quality assessments based on a held-out test region and a temporal transferability test on new images. Erosion classes are assigned according to their type (shallow landslide and sites with reduced vegetation affected by sheet erosion) or land-use impacts (livestock trails and larger management affected areas). We show that results obtained by OBIA and U-Net follow similar linear trends for the 16 year study period, exhibiting increases in total degraded area of 167% and 201%, respectively. Segmentations of eroded sites are generally in good agreement, but also display method-specific differences, which lead to an overall precision of 73%, a recall of 84%, and a F1-score of 78%. Our results show that U-Net is transferable to spatially (within our study area) and temporally unseen data (data from new years) and is therefore a method suitable to efficiently and successfully capture the temporal trends and spatial heterogeneity of degradation in alpine grasslands. Additionally, U-Net is a powerful and robust tool to map erosion sites in a predictive manner utilising large amounts of new aerial imagery.},
DOI = {10.3390/rs12244149}
}



@Article{rs12244143,
AUTHOR = {Hassanijalilian, Oveis and Igathinathane, C. and Bajwa, Sreekala and Nowatzki, John},
TITLE = {Rating Iron Deficiency in Soybean Using Image Processing and Decision-Tree Based Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4143},
URL = {https://www.mdpi.com/2072-4292/12/24/4143},
ISSN = {2072-4292},
ABSTRACT = {The most efficient way of soybean (Glycine max (L.) Merrill) iron deficiency chlorosis (IDC) management is to select a tolerant cultivar suitable for the specific growing condition. These cultivars are selected by field experts based on IDC visual ratings. However, this visual rating method is laborious, expensive, time-consuming, subjective, and impractical on larger scales. Therefore, a modern digital image-based method using tree-based machine learning classifier models for rating soybean IDC at plot-scale was developed. Data were collected from soybean IDC cultivar trial plots. Images were processed with MATLAB and corrected for light intensity by using a standard color board in the image. The three machine learning models used in this study were decision tree (DT), random forest (RF), and adaptive boosting (AdaBoost). Calculated indices from images, such as dark green color index (DGCI), canopy size, and pixel counts into DGCI ranges and IDC visual scoring were used as input and target variables to train these models. Metrics such as precision, recall, and f1-score were used to assess the performance of the classifier models. Among all three models, AdaBoost had the best performance (average f1-score = 0.75) followed by RF and DT the least. Therefore, a ready-to-use methodology of image processing with AdaBoost model for soybean IDC rating was recommended. The developed method can be easily adapted to smartphone applications or scaled-up using images from aerial platforms.},
DOI = {10.3390/rs12244143}
}



@Article{rs12244169,
AUTHOR = {Tran, Dai Quoc and Park, Minsoo and Jung, Daekyo and Park, Seunghee},
TITLE = {Damage-Map Estimation Using UAV Images and Deep Learning Algorithms for Disaster Management System},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4169},
URL = {https://www.mdpi.com/2072-4292/12/24/4169},
ISSN = {2072-4292},
ABSTRACT = {Estimating the damaged area after a forest fire is important for responding to this natural catastrophe. With the support of aerial remote sensing, typically with unmanned aerial vehicles (UAVs), the aerial imagery of forest-fire areas can be easily obtained; however, retrieving the burnt area from the image is still a challenge. We implemented a new approach for segmenting burnt areas from UAV images using deep learning algorithms. First, the data were collected from a forest fire in Andong, the Republic of Korea, in April 2020. Then, the proposed two-patch-level deep-learning models were implemented. A patch-level 1 network was trained using the UNet++ architecture. The output prediction of this network was used as a position input for the second network, which used UNet. It took the reference position from the first network as its input and refined the results. Finally, the final performance of our proposed method was compared with a state-of-the-art image-segmentation algorithm to prove its robustness. Comparative research on the loss functions was also performed. Our proposed approach demonstrated its effectiveness in extracting burnt areas from UAV images and can contribute to estimating maps showing the areas damaged by forest fires.},
DOI = {10.3390/rs12244169}
}



@Article{ijgi9120759,
AUTHOR = {Zang, Yufu and Li, Bijun and Xiao, Xiongwu and Zhu, Jianfeng and Meng, Fancong},
TITLE = {An Efficient Probabilistic Registration Based on Shape Descriptor for Heritage Field Inspection},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {759},
URL = {https://www.mdpi.com/2220-9964/9/12/759},
ISSN = {2220-9964},
ABSTRACT = {Heritage documentation is implemented by digitally recording historical artifacts for the conservation and protection of these cultural heritage objects. As efficient spatial data acquisition tools, laser scanners have been widely used to collect highly accurate three-dimensional (3D) point clouds without damaging the original structure and the environment. To ensure the integrity and quality of the collected data, field inspection (i.e., on-spot checking the data quality) should be carried out to determine the need for additional measurements (i.e., extra laser scanning for areas with quality issues such as data missing and quality degradation). To facilitate inspection of all collected point clouds, especially checking the quality issues in overlaps between adjacent scans, all scans should be registered together. Thus, a point cloud registration method that is able to register scans fast and robustly is required. To fulfill the aim, this study proposes an efficient probabilistic registration for free-form cultural heritage objects by integrating the proposed principal direction descriptor and curve constraints. We developed a novel shape descriptor based on a local frame of principal directions. Within the frame, its density and distance feature images were generated to describe the shape of the local surface. We then embedded the descriptor into a probabilistic framework to reject ambiguous matches. Spatial curves were integrated as constraints to delimit the solution space. Finally, a multi-view registration was used to refine the position and orientation of each scan for the field inspection. Comprehensive experiments show that the proposed method was able to perform well in terms of rotation error, translation error, robustness, and runtime and outperformed some commonly used approaches.},
DOI = {10.3390/ijgi9120759}
}



@Article{s20247321,
AUTHOR = {Oh, Donggeun and Han, Junghee},
TITLE = {Fisheye-Based Smart Control System for Autonomous UAV Operation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7321},
URL = {https://www.mdpi.com/1424-8220/20/24/7321},
ISSN = {1424-8220},
ABSTRACT = {Recently, as UAVs (unmanned aerial vehicles) have become smaller and higher-performance, they play a very important role in the Internet of Things (IoT). Especially, UAVs are currently used not only in military fields but also in various private sectors such as IT, agriculture, logistics, construction, etc. The range is further expected to increase. Drone-related techniques need to evolve along with this change. In particular, there is a need for the development of an autonomous system in which a drone can determine and accomplish its mission even in the absence of remote control from a GCS (Ground Control Station). Responding to such requirements, there have been various studies and algorithms developed for autonomous flight systems. Especially, many ML-based (Machine-Learning-based) methods have been proposed for autonomous path finding. Unlike other studies, the proposed mechanism could enable autonomous drone path finding over a large target area without size limitations, one of the challenges of ML-based autonomous flight or driving in the real world. Specifically, we devised Multi-Layer HVIN (Hierarchical VIN) methods that increase the area applicable to autonomous flight by overlaying multiple layers. To further improve this, we developed Fisheye HVIN, which applied an adaptive map compression ratio according to the drone’s location. We also built an autonomous flight training and verification platform. Through the proposed simulation platform, it is possible to train ML-based path planning algorithms in a realistic environment that takes into account the physical characteristics of UAV movements.},
DOI = {10.3390/s20247321}
}



@Article{agriculture10120653,
AUTHOR = {Bolfe, Édson Luis and Jorge, Lúcio André de Castro and Sanches, Ieda Del’Arco and Luchiari Júnior, Ariovaldo and da Costa, Cinthia Cabral and Victoria, Daniel de Castro and Inamasu, Ricardo Yassushi and Grego, Célia Regina and Ferreira, Victor Rodrigues and Ramirez, Andrea Restrepo},
TITLE = {Precision and Digital Agriculture: Adoption of Technologies and Perception of Brazilian Farmers},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {653},
URL = {https://www.mdpi.com/2077-0472/10/12/653},
ISSN = {2077-0472},
ABSTRACT = {The rapid population growth has driven the demand for more food, fiber, energy, and water, which is associated to an increase in the need to use natural resources in a more sustainable way. The use of precision agriculture machinery and equipment since the 1990s has provided important productive gains and maximized the use of agricultural inputs. The growing connectivity in the rural environment, in addition to its greater integration with data from sensor systems, remote sensors, equipment, and smartphones have paved the way for new concepts from the so-called Agriculture 4.0 or Digital Agriculture. This article presents the results of a survey carried out with 504 Brazilian farmers about the digital technologies in use, as well as current and future applications, perceived benefits, and challenges. The questionnaire was prepared, organized, and made available to the public through the online platform LimeSurvey and was available from 17 April to 2 June 2020. The primary data obtained for each question previously defined were consolidated and analyzed statistically. The results indicate that 84% of the interviewed farmers use at least one digital technology in their production system that differs according to technological complexity level. The main perceived benefit refers to the perception of increased productivity and the main challenges are the acquisition costs of machines, equipment, software, and connectivity. It is also noteworthy that 95% of farmers would like to learn more about new technologies to strengthen the agricultural development in their properties.},
DOI = {10.3390/agriculture10120653}
}



@Article{rs12244193,
AUTHOR = {Tilon, Sofia and Nex, Francesco and Kerle, Norman and Vosselman, George},
TITLE = {Post-Disaster Building Damage Detection from Earth Observation Imagery Using Unsupervised and Transferable Anomaly Detecting Generative Adversarial Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4193},
URL = {https://www.mdpi.com/2072-4292/12/24/4193},
ISSN = {2072-4292},
ABSTRACT = {We present an unsupervised deep learning approach for post-disaster building damage detection that can transfer to different typologies of damage or geographical locations. Previous advances in this direction were limited by insufficient qualitative training data. We propose to use a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN) because it only requires pre-event imagery of buildings in their undamaged state. This approach aids the post-disaster response phase because the model can be developed in the pre-event phase and rapidly deployed in the post-event phase. We used the xBD dataset, containing pre- and post- event satellite imagery of several disaster-types, and a custom made Unmanned Aerial Vehicle (UAV) dataset, containing post-earthquake imagery. Results showed that models trained on UAV-imagery were capable of detecting earthquake-induced damage. The best performing model for European locations obtained a recall, precision and F1-score of 0.59, 0.97 and 0.74, respectively. Models trained on satellite imagery were capable of detecting damage on the condition that the training dataset was void of vegetation and shadows. In this manner, the best performing model for (wild)fire events yielded a recall, precision and F1-score of 0.78, 0.99 and 0.87, respectively. Compared to other supervised and/or multi-epoch approaches, our results are encouraging. Moreover, in addition to image classifications, we show how contextual information can be used to create detailed damage maps without the need of a dedicated multi-task deep learning framework. Finally, we formulate practical guidelines to apply this single-epoch and unsupervised method to real-world applications.},
DOI = {10.3390/rs12244193}
}



@Article{s21010014,
AUTHOR = {Dong, Mei and Wu, Hongyu and Hu, Hui and Azzam, Rafig and Zhang, Liang and Zheng, Zengrong and Gong, Xiaonan},
TITLE = {Deformation Prediction of Unstable Slopes Based on Real-Time Monitoring and DeepAR Model},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {14},
URL = {https://www.mdpi.com/1424-8220/21/1/14},
ISSN = {1424-8220},
ABSTRACT = {With increased urbanization, accidents related to slope instability are frequently encountered in construction sites. The deformation and failure mechanism of a landslide is a complex dynamic process, which seriously threatens people&rsquo;s lives and property. Currently, prediction and early warning of a landslide can be effectively performed by using Internet of Things (IoT) technology to monitor the landslide deformation in real time and an artificial intelligence algorithm to predict the deformation trend. However, if a slope failure occurs during the construction period, the builders and decision-makers find it challenging to effectively apply IoT technology to monitor the emergency and assist in proposing treatment measures. Moreover, for projects during operation (e.g., a motorway in a mountainous area), no recognized artificial intelligence algorithm exists that can forecast the deformation of steep slopes using the huge data obtained from monitoring devices. In this context, this paper introduces a real-time wireless monitoring system with multiple sensors for retrieving high-frequency overall data that can describe the deformation feature of steep slopes. The system was installed in the Qili connecting line of a motorway in Zhejiang Province, China, to provide a technical support for the design and implementation of safety solutions for the steep slopes. Most of the devices were retained to monitor the slopes even after construction. The machine learning Probabilistic Forecasting with Autoregressive Recurrent Networks (DeepAR) model based on time series and probabilistic forecasting was introduced into the project to predict the slope displacement. The predictive accuracy of the DeepAR model was verified by the mean absolute error, the root mean square error and the goodness of fit. This study demonstrates that the presented monitoring system and the introduced predictive model had good safety control ability during construction and good prediction accuracy during operation. The proposed approach will be helpful to assess the safety of excavated slopes before constructing new infrastructures.},
DOI = {10.3390/s21010014}
}



@Article{rs13010023,
AUTHOR = {Zhao, Wei and Yamada, William and Li, Tianxin and Digman, Matthew and Runge, Troy},
TITLE = {Augmenting Crop Detection for Precision Agriculture with Deep Visual Transfer Learning—A Case Study of Bale Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2072-4292/13/1/23},
ISSN = {2072-4292},
ABSTRACT = {In recent years, precision agriculture has been researched to increase crop production with less inputs, as a promising means to meet the growing demand of agriculture products. Computer vision-based crop detection with unmanned aerial vehicle (UAV)-acquired images is a critical tool for precision agriculture. However, object detection using deep learning algorithms rely on a significant amount of manually prelabeled training datasets as ground truths. Field object detection, such as bales, is especially difficult because of (1) long-period image acquisitions under different illumination conditions and seasons; (2) limited existing prelabeled data; and (3) few pretrained models and research as references. This work increases the bale detection accuracy based on limited data collection and labeling, by building an innovative algorithms pipeline. First, an object detection model is trained using 243 images captured with good illimitation conditions in fall from the crop lands. In addition, domain adaptation (DA), a kind of transfer learning, is applied for synthesizing the training data under diverse environmental conditions with automatic labels. Finally, the object detection model is optimized with the synthesized datasets. The case study shows the proposed method improves the bale detecting performance, including the recall, mean average precision (mAP), and F measure (F1 score), from averages of 0.59, 0.7, and 0.7 (the object detection) to averages of 0.93, 0.94, and 0.89 (the object detection + DA), respectively. This approach could be easily scaled to many other crop field objects and will significantly contribute to precision agriculture.},
DOI = {10.3390/rs13010023}
}



@Article{agronomy11010011,
AUTHOR = {Cruz Ulloa, Christyan and Krus, Anne and Barrientos, Antonio and Del Cerro, Jaime and Valero, Constantino},
TITLE = {Robotic Fertilisation Using Localisation Systems Based on Point Clouds in Strip-Cropping Fields},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {11},
URL = {https://www.mdpi.com/2073-4395/11/1/11},
ISSN = {2073-4395},
ABSTRACT = {The use of robotic systems in organic farming has taken on a leading role in recent years; the Sureveg CORE Organic Cofund ERA-Net project seeks to evaluate the benefits of strip-cropping to produce organic vegetables. This includes, among other objectives, the development of a robotic tool that facilitates the automation of the fertilisation process, allowing the individual treatment (at the plant level). In organic production, the slower nutrient release of the used fertilisers poses additional difficulties, as a tardy detection of deficiencies can no longer be corrected. To improve the detection, as well as counter the additional labour stemming from the strip-cropping configuration, an integrated robotic tool is proposed to detect individual crop deficiencies and react on a single-crop basis. For the development of this proof-of-concept, one of the main objectives of this work is implementing a robust localisation method within the vegetative environment based on point clouds, through the generation of general point cloud maps (G-PC) and local point cloud maps (L-PC) of a crop row. The plants&rsquo; geometric characteristics were extracted from the G-PC as a framework in which the robot&rsquo;s positioning is defined. Through the processing of real-time lidar data, the L-PC is then defined and compared to the predefined reference system previously deduced. Both subsystems are integrated with ROS (Robot Operating System), alongside motion planning, and an inverse kinematics CCD (Cyclic Coordinate Descent) solver, among others. Tests were performed using a simulated environment of the crop row developed in Gazebo, followed by actual measurements in a strip-cropping field. During real-time data-acquisition, the localisation error is reduced from 13 mm to 11 mm within the first 120 cm of measurement. The encountered real-time geometric characteristics were found to coincide with those in the G-PC to an extend of 98.6%.},
DOI = {10.3390/agronomy11010011}
}



@Article{rs13010039,
AUTHOR = {Carvalho, Osmar Luiz Ferreira de and de Carvalho Júnior, Osmar Abílio and Albuquerque, Anesmar Olino de and Bem, Pablo Pozzobon de and Silva, Cristiano Rosa and Ferreira, Pedro Henrique Guimarães and Moura, Rebeca dos Santos de and Gomes, Roberto Arnaldo Trancoso and Guimarães, Renato Fontes and Borges, Díbio Leandro},
TITLE = {Instance Segmentation for Large, Multi-Channel Remote Sensing Imagery Using Mask-RCNN and a Mosaicking Approach},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2072-4292/13/1/39},
ISSN = {2072-4292},
ABSTRACT = {Instance segmentation is the state-of-the-art in object detection, and there are numerous applications in remote sensing data where these algorithms can produce significant results. Nevertheless, one of the main problems is that most algorithms use Red, Green, and Blue (RGB) images, whereas Satellite images often present more channels that can be crucial to improve performance. Therefore, the present work brings three contributions: (a) conversion system from ground truth polygon data into the Creating Common Object in Context (COCO) annotation format; (b) Detectron2 software source code adaptation and application on multi-channel imagery; and (c) large scene image mosaicking. We applied the procedure in a Center Pivot Irrigation System (CPIS) dataset with ground truth produced by the Brazilian National Water Agency (ANA) and Landsat-8 Operational Land Imager (OLI) imagery (7 channels with 30-m resolution). Center pivots are a modern irrigation system technique with massive growth potential in Brazil and other world areas. The round shapes with different textures, colors, and spectral behaviors make it appropriate to use Deep Learning instance segmentation. We trained the model using 512 &times; 512-pixel sized patches using seven different backbone structures (ResNet50- Feature Pyramid Network (FPN), Resnet50-DC5, ResNet50-C4, Resnet101-FPN, Resnet101-DC5, ResNet101-FPN, and ResNeXt101-FPN). The model evaluation used standard COCO metrics (Average Precision (AP), AP50, AP75, APsmall, APmedium, and AR100). ResNeXt101-FPN had the best results, with a 3% advantage over the second-best model (ResNet101-FPN). We also compared the ResNeXt101-FPN model in the seven-channel and RGB imagery, where the multi-channel model had a 3% advantage, demonstrating great improvement using a larger number of channels. This research is also the first with a mosaicking algorithm using instance segmentation models, where we tested in a 1536 &times; 1536-pixel image using a non-max suppression sorted by area method. The proposed methodology is innovative and suitable for many other remote sensing problems and medical imagery that often present more channels.},
DOI = {10.3390/rs13010039}
}



@Article{robotics10010002,
AUTHOR = {Follini, Camilla and Magnago, Valerio and Freitag, Kilian and Terzer, Michael and Marcher, Carmen and Riedl, Michael and Giusti, Andrea and Matt, Dominik Tobias},
TITLE = {BIM-Integrated Collaborative Robotics for Application in Building Construction and Maintenance},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/2218-6581/10/1/2},
ISSN = {2218-6581},
ABSTRACT = {The application of robotics in construction is hindered by the site environment, which is unstructured and subject to change. At the same time, however, buildings and corresponding sites can be accurately described by Building Information Modeling (BIM). Such a model contains geometric and semantic data about the construction and operation phases of the building and it is already available at the design phase. We propose a method to leverage BIM for simple yet efficient deployment of robotic systems for construction and operation of buildings. With our proposed approach, BIM is used to provide the robot with a priori geometric and semantic information on the environment and to store information on the operation progress. We present two applications that verify the effectiveness of our proposed method. This system represents a step forward towards an easier application of robots in construction.},
DOI = {10.3390/robotics10010002}
}



@Article{rs13010052,
AUTHOR = {Maung, Win Sithu and Sasaki, Jun},
TITLE = {Assessing the Natural Recovery of Mangroves after Human Disturbance Using Neural Network Classification and Sentinel-2 Imagery in Wunbaik Mangrove Forest, Myanmar},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2072-4292/13/1/52},
ISSN = {2072-4292},
ABSTRACT = {In this study, we examined the natural recovery of mangroves in abandoned shrimp ponds located in the Wunbaik Mangrove Forest (WMF) in Myanmar using artificial neural network (ANN) classification and a change detection approach with Sentinel-2 satellite images. In 2020, we conducted various experiments related to mangrove classification by tuning input features and hyper-parameters. The selected ANN model was used with a transfer learning approach to predict the mangrove distribution in 2015. Changes were detected using classification results from 2015 and 2020. Naturally recovering mangroves were identified by extracting the change detection results of three abandoned shrimp ponds selected during field investigation. The proposed method yielded an overall accuracy of 95.98%, a kappa coefficient of 0.92, mangrove and non-mangrove precisions of 0.95 and 0.98, respectively, recalls of 0.96, and F1 scores of 0.96 for the 2020 classification. For the 2015 prediction, transfer learning improved model performance, resulting in an overall accuracy of 97.20%, a kappa coefficient of 0.94, mangrove and non-mangrove precisions of 0.98 and 0.96, respectively, recalls of 0.98 and 0.97, and F1 scores of 0.96. The change detection results showed that mangrove forests in the WMF slightly decreased between 2015 and 2020. Naturally recovering mangroves were detected at approximately 50% of each abandoned site within a short abandonment period. This study demonstrates that the ANN method using Sentinel-2 imagery and topographic and canopy height data can produce reliable results for mangrove classification. The natural recovery of mangroves presents a valuable opportunity for mangrove rehabilitation at human-disturbed sites in the WMF.},
DOI = {10.3390/rs13010052}
}



@Article{rs13010054,
AUTHOR = {Biffi, Leonardo Josoé and Mitishita, Edson and Liesenberg, Veraldo and Santos, Anderson Aparecido dos and Gonçalves, Diogo Nunes and Estrabis, Nayara Vasconcelos and Silva, Jonathan de Andrade and Osco, Lucas Prado and Ramos, Ana Paula Marques and Centeno, Jorge Antonio Silva and Schimalski, Marcos Benedito and Rufato, Leo and Neto, Sílvio Luís Rafaeli and Marcato Junior, José and Gonçalves, Wesley Nunes},
TITLE = {ATSS Deep Learning-Based Approach to Detect Apple Fruits},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2072-4292/13/1/54},
ISSN = {2072-4292},
ABSTRACT = {In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. Specifically, in fruit detection problems, several recent works were developed using Deep Learning (DL) methods applied in images acquired in different acquisition levels. However, the increasing use of anti-hail plastic net cover in commercial orchards highlights the importance of terrestrial remote sensing systems. Apples are one of the most highly-challenging fruits to be detected in images, mainly because of the target occlusion problem occurrence. Additionally, the introduction of high-density apple tree orchards makes the identification of single fruits a real challenge. To support farmers to detect apple fruits efficiently, this paper presents an approach based on the Adaptive Training Sample Selection (ATSS) deep learning method applied to close-range and low-cost terrestrial RGB images. The correct identification supports apple production forecasting and gives local producers a better idea of forthcoming management practices. The main advantage of the ATSS method is that only the center point of the objects is labeled, which is much more practicable and realistic than bounding-box annotations in heavily dense fruit orchards. Additionally, we evaluated other object detection methods such as RetinaNet, Libra Regions with Convolutional Neural Network (R-CNN), Cascade R-CNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), and High-Resolution Network (HRNet). The study area is a highly-dense apple orchard consisting of Fuji Suprema apple fruits (Malus domestica Borkh) located in a smallholder farm in the state of Santa Catarina (southern Brazil). A total of 398 terrestrial images were taken nearly perpendicularly in front of the trees by a professional camera, assuring both a good vertical coverage of the apple trees in terms of heights and overlapping between picture frames. After, the high-resolution RGB images were divided into several patches for helping the detection of small and/or occluded apples. A total of 3119, 840, and 2010 patches were used for training, validation, and testing, respectively. Moreover, the proposed method&rsquo;s generalization capability was assessed by applying simulated image corruptions to the test set images with different severity levels, including noise, blurs, weather, and digital processing. Experiments were also conducted by varying the bounding box size (80, 100, 120, 140, 160, and 180 pixels) in the image original for the proposed approach. Our results showed that the ATSS-based method slightly outperformed all other deep learning methods, between 2.4% and 0.3%. Also, we verified that the best result was obtained with a bounding box size of 160 &times; 160 pixels. The proposed method was robust regarding most of the corruption, except for snow, frost, and fog weather conditions. Finally, a benchmark of the reported dataset is also generated and publicly available.},
DOI = {10.3390/rs13010054}
}



@Article{rs13010068,
AUTHOR = {Pineda, Mónica and Barón, Matilde and Pérez-Bueno, María-Luisa},
TITLE = {Thermal Imaging for Plant Stress Detection and Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {68},
URL = {https://www.mdpi.com/2072-4292/13/1/68},
ISSN = {2072-4292},
ABSTRACT = {In the last few years, large efforts have been made to develop new methods to optimize stress detection in crop fields. Thus, plant phenotyping based on imaging techniques has become an essential tool in agriculture. In particular, leaf temperature is a valuable indicator of the physiological status of plants, responding to both biotic and abiotic stressors. Often combined with other imaging sensors and data-mining techniques, thermography is crucial in the implementation of a more automatized, precise and sustainable agriculture. However, thermal data need some corrections related to the environmental and measuring conditions in order to achieve a correct interpretation of the data. This review focuses on the state of the art of thermography applied to the detection of biotic stress. The work will also revise the most important abiotic stress factors affecting the measurements as well as practical issues that need to be considered in order to implement this technique, particularly at the field scale.},
DOI = {10.3390/rs13010068}
}



@Article{rs13010071,
AUTHOR = {Xu, Zhiyong and Zhang, Weicun and Zhang, Tianxiang and Li, Jiangyun},
TITLE = {HRCNet: High-Resolution Context Extraction Network for Semantic Segmentation of Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {71},
URL = {https://www.mdpi.com/2072-4292/13/1/71},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation is a significant method in remote sensing image (RSIs) processing and has been widely used in various applications. Conventional convolutional neural network (CNN)-based semantic segmentation methods are likely to lose the spatial information in the feature extraction stage and usually pay little attention to global context information. Moreover, the imbalance of category scale and uncertain boundary information meanwhile exists in RSIs, which also brings a challenging problem to the semantic segmentation task. To overcome these problems, a high-resolution context extraction network (HRCNet) based on a high-resolution network (HRNet) is proposed in this paper. In this approach, the HRNet structure is adopted to keep the spatial information. Moreover, the light-weight dual attention (LDA) module is designed to obtain global context information in the feature extraction stage and the feature enhancement feature pyramid (FEFP) structure is promoted and employed to fuse the contextual information of different scales. In addition, to achieve the boundary information, we design the boundary aware (BA) module combined with the boundary aware loss (BAloss) function. The experimental results evaluated on Potsdam and Vaihingen datasets show that the proposed approach can significantly improve the boundary and segmentation performance up to 92.0% and 92.3% on overall accuracy scores, respectively. As a consequence, it is envisaged that the proposed HRCNet model will be an advantage in remote sensing images segmentation.},
DOI = {10.3390/rs13010071}
}



@Article{s21010118,
AUTHOR = {Wittstruck, Lucas and Kühling, Insa and Trautz, Dieter and Kohlbrecher, Maik and Jarmer, Thomas},
TITLE = {UAV-Based RGB Imagery for Hokkaido Pumpkin (Cucurbita max.) Detection and Yield Estimation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {118},
URL = {https://www.mdpi.com/1424-8220/21/1/118},
ISSN = {1424-8220},
ABSTRACT = {Pumpkins are economically and nutritionally valuable vegetables with increasing popularity and acreage across Europe. Successful commercialization, however, require detailed pre-harvest information about number and weight of the fruits. To get a non-destructive and cost-effective yield estimation, we developed an image processing methodology for high-resolution RGB data from Unmanned aerial vehicle (UAV) and applied this on a Hokkaido pumpkin farmer&rsquo;s field in North-western Germany. The methodology was implemented in the programming language Python and comprised several steps, including image pre-processing, pixel-based image classification, classification post-processing for single fruit detection, and fruit size and weight quantification. To derive the weight from two-dimensional imagery, we calculated elliptical spheroids from lengths of diameters and heights. The performance of this processes was evaluated by comparison with manually harvested ground-truth samples and cross-checked for misclassification from randomly selected test objects. Errors in classification and fruit geometry could be successfully reduced based on the described processing steps. Additionally, different lighting conditions, as well as shadows, in the image data could be compensated by the proposed methodology. The results revealed a satisfactory detection of 95% (error rate of 5%) from the field sample, as well as a reliable volume and weight estimation with Pearson&rsquo;s correlation coefficients of 0.83 and 0.84, respectively, from the described ellipsoid approach. The yield was estimated with 1.51 kg m&minus;2 corresponding to an average individual fruit weight of 1100 g and an average number of 1.37 pumpkins per m2. Moreover, spatial distribution of aggregated fruit densities and weights were calculated to assess in-field optimization potential for agronomic management as demonstrated between a shaded edge compared to the rest of the field. The proposed approach provides the Hokkaido producer useful information for more targeted pre-harvest marketing strategies, since most food retailers request homogeneous lots within prescribed size or weight classes.},
DOI = {10.3390/s21010118}
}



@Article{electronics10010027,
AUTHOR = {Mun, Hyunsu and Lee, Youngseok},
TITLE = {Internet Traffic Classification with Federated Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {27},
URL = {https://www.mdpi.com/2079-9292/10/1/27},
ISSN = {2079-9292},
ABSTRACT = {As Internet traffic classification is a typical problem for ISPs or mobile carriers, there have been a lot of studies based on statistical packet header information, deep packet inspection, or machine learning. Due to recent advances in end-to-end encryption and dynamic port policies, machine or deep learning has been an essential key to improve the accuracy of packet classification. In addition, ISPs or mobile carriers should carefully deal with the privacy issue while collecting user packets for accounting or security. The recent development of distributed machine learning, called federated learning, collaboratively carries out machine learning jobs on the clients without uploading data to a central server. Although federated learning provides an on-device learning framework towards user privacy protection, its feasibility and performance of Internet traffic classification have not been fully examined. In this paper, we propose a federated-learning traffic classification protocol (FLIC), which can achieve an accuracy comparable to centralized deep learning for Internet application identification without privacy leakage. FLIC can classify new applications on-the-fly when a participant joins in learning with a new application, which has not been done in previous works. By implementing the prototype of FLIC clients and a server with TensorFlow, the clients gather packets, perform the on-device training job and exchange the training results with the FLIC server. In addition, we demonstrate that federated learning-based packet classification achieves an accuracy of 88% under non-independent and identically distributed (non-IID) traffic across clients. When a new application that can be classified dynamically as a client participates in learning was added, an accuracy of 92% was achieved.},
DOI = {10.3390/electronics10010027}
}



@Article{rs13010084,
AUTHOR = {Yamaguchi, Tomoaki and Tanaka, Yukie and Imachi, Yuto and Yamashita, Megumi and Katsura, Keisuke},
TITLE = {Feasibility of Combining Deep Learning and RGB Images Obtained by Unmanned Aerial Vehicle for Leaf Area Index Estimation in Rice},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {84},
URL = {https://www.mdpi.com/2072-4292/13/1/84},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is a vital parameter for predicting rice yield. Unmanned aerial vehicle (UAV) surveillance with an RGB camera has been shown to have potential as a low-cost and efficient tool for monitoring crop growth. Simultaneously, deep learning (DL) algorithms have attracted attention as a promising tool for the task of image recognition. The principal aim of this research was to evaluate the feasibility of combining DL and RGB images obtained by a UAV for rice LAI estimation. In the present study, an LAI estimation model developed by DL with RGB images was compared to three other practical methods: a plant canopy analyzer (PCA); regression models based on color indices (CIs) obtained from an RGB camera; and vegetation indices (VIs) obtained from a multispectral camera. The results showed that the estimation accuracy of the model developed by DL with RGB images (R2 = 0.963 and RMSE = 0.334) was higher than those of the PCA (R2 = 0.934 and RMSE = 0.555) and the regression models based on CIs (R2 = 0.802-0.947 and RMSE = 0.401&ndash;1.13), and comparable to that of the regression models based on VIs (R2 = 0.917&ndash;0.976 and RMSE = 0.332&ndash;0.644). Therefore, our results demonstrated that the estimation model using DL with an RGB camera on a UAV could be an alternative to the methods using PCA and a multispectral camera for rice LAI estimation.},
DOI = {10.3390/rs13010084}
}



@Article{s21010210,
AUTHOR = {Park, Dongsuk and Lee, Seungeui and Park, SeongUk and Kwak, Nojun},
TITLE = {Radar-Spectrogram-Based UAV Classification Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {210},
URL = {https://www.mdpi.com/1424-8220/21/1/210},
ISSN = {1424-8220},
ABSTRACT = {With the upsurge in the use of Unmanned Aerial Vehicles (UAVs) in various fields, detecting and identifying them in real-time are becoming important topics. However, the identification of UAVs is difficult due to their characteristics such as low altitude, slow speed, and small radar cross-section (LSS). With the existing deterministic approach, the algorithm becomes complex and requires a large number of computations, making it unsuitable for real-time systems. Hence, effective alternatives enabling real-time identification of these new threats are needed. Deep learning-based classification models learn features from data by themselves and have shown outstanding performance in computer vision tasks. In this paper, we propose a deep learning-based classification model that learns the micro-Doppler signatures (MDS) of targets represented on radar spectrogram images. To enable this, first, we recorded five LSS targets (three types of UAVs and two different types of human activities) with a frequency modulated continuous wave (FMCW) radar in various scenarios. Then, we converted signals into spectrograms in the form of images by Short time Fourier transform (STFT). After the data refinement and augmentation, we made our own radar spectrogram dataset. Secondly, we analyzed characteristics of the radar spectrogram dataset with the ResNet-18 model and designed the ResNet-SP model with less computation, higher accuracy and stability based on the ResNet-18 model. The results show that the proposed ResNet-SP has a training time of 242 s and an accuracy of 83.39%, which is superior to the ResNet-18 that takes 640 s for training with an accuracy of 79.88%.},
DOI = {10.3390/s21010210}
}



@Article{agriculture11010022,
AUTHOR = {Rahman, Mohammad Fatin Fatihur and Fan, Shurui and Zhang, Yan and Chen, Lei},
TITLE = {A Comparative Study on Application of Unmanned Aerial Vehicle Systems in Agriculture},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {22},
URL = {https://www.mdpi.com/2077-0472/11/1/22},
ISSN = {2077-0472},
ABSTRACT = {Presently in agriculture, there is much ample scope for drone and UAS (Unmanned Aircraft System) development. Because of their low cost and small size, these devices have the ability to help many developing countries with economic prosperity. The entire aggregation of financial investments in the agricultural area has increased appreciably in recent years. Sooth to say, agriculture remains a massive part of the world&rsquo;s commercial growth, and due to some complications, the agriculture fields withstand massive losses. Pets and destructive insects seem to be the primary reasons for certain degenerative diseases. It minimizes the potential productivity of the crops. For increasing the quality of the plants, fertilizers and pesticides are appropriately applied. Using UAVs (Unmanned Aerial Vehicles) for spraying pesticides and fertilizing materials is an exuberant contraption. It adequately reduces the rate of health dilemma and the number of workers, which is quite an impressive landmark. Willing producers are also adopting UAVs in agriculture to soil and field analysis, seed sowing, lessen the time and costs correlated with crop scouting, and field mapping. It is rapid, and it can sensibly diminish a farmer&rsquo;s workload, which is significantly a part of the agricultural revolution. This article aims to proportionally represent the concept of agricultural purposed UAV clear to the neophytes. First, this paper outlines the harmonic framework of the agricultural UAV, and then it abundantly illustrates the methods and materials. Finally, the article portrays the outcome.},
DOI = {10.3390/agriculture11010022}
}



@Article{rs13010123,
AUTHOR = {Guo, Anting and Huang, Wenjiang and Dong, Yingying and Ye, Huichun and Ma, Huiqin and Liu, Bo and Wu, Wenbin and Ren, Yu and Ruan, Chao and Geng, Yun},
TITLE = {Wheat Yellow Rust Detection Using UAV-Based Hyperspectral Technology},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {123},
URL = {https://www.mdpi.com/2072-4292/13/1/123},
ISSN = {2072-4292},
ABSTRACT = {Yellow rust is a worldwide disease that poses a serious threat to the safety of wheat production. Numerous studies on near-surface hyperspectral remote sensing at the leaf scale have achieved good results for disease monitoring. The next step is to monitor the disease at the field scale, which is of great significance for disease control. In our study, an unmanned aerial vehicle (UAV) equipped with a hyperspectral sensor was used to obtain hyperspectral images at the field scale. Vegetation indices (VIs) and texture features (TFs) extracted from the UAV-based hyperspectral images and their combination were used to establish partial least-squares regression (PLSR)-based disease monitoring models in different infection periods. In addition, we resampled the original images with 1.2 cm spatial resolution to images with different spatial resolutions (3 cm, 5 cm, 7 cm, 10 cm, 15 cm, and 20 cm) to evaluate the effect of spatial resolution on disease monitoring accuracy. The findings showed that the VI-based model had the highest monitoring accuracy (R2 = 0.75) in the mid-infection period. The TF-based model could be used to monitor yellow rust at the field scale and obtained the highest R2 in the mid- and late-infection periods (0.65 and 0.82, respectively). The VI-TF-based models had the highest accuracy in each infection period and outperformed the VI-based or TF-based models. The spatial resolution had a negligible influence on the VI-based monitoring accuracy, but significantly influenced the TF-based monitoring accuracy. Furthermore, the optimal spatial resolution for monitoring yellow rust using the VI-TF-based model in each infection period was 10 cm. The findings provide a reference for accurate disease monitoring using UAV hyperspectral images.},
DOI = {10.3390/rs13010123}
}



@Article{geosciences11010021,
AUTHOR = {Zolkos, Scott and Fiske, Greg and Windholz, Tiffany and Duran, Gabriel and Yang, Zhiqiang and Olenchenko, Vladimir and Faguet, Alexey and Natali, Susan M.},
TITLE = {Detecting and Mapping Gas Emission Craters on the Yamal and Gydan Peninsulas, Western Siberia},
JOURNAL = {Geosciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {21},
URL = {https://www.mdpi.com/2076-3263/11/1/21},
ISSN = {2076-3263},
ABSTRACT = {Rapid climate warming at northern high latitudes is driving geomorphic changes across the permafrost zone. In the Yamal and Gydan peninsulas in western Siberia, subterranean accumulation of methane beneath or within ice-rich permafrost can create mounds at the land surface. Once over-pressurized by methane, these mounds can explode and eject frozen ground, forming a gas emission crater (GEC). While GECs pose a hazard to human populations and infrastructure, only a small number have been identified in the Yamal and Gydan peninsulas, where the regional distribution and frequency of GECs and other types of land surface change are relatively unconstrained. To understand the distribution of landscape change within 327,000 km2 of the Yamal-Gydan region, we developed a semi-automated multivariate change detection algorithm using satellite-derived surface reflectance, elevation, and water extent in the Google Earth Engine cloud computing platform. We found that 5% of the landscape changed from 1984 to 2017. The algorithm detected all seven GECs reported in the scientific literature and three new GEC-like features, and further revealed that retrogressive thaw slumps were more abundant than GECs. Our methodology can be refined to detect and better understand diverse types of land surface change and potentially mitigate risks across the northern permafrost zone.},
DOI = {10.3390/geosciences11010021}
}



@Article{s21010231,
AUTHOR = {Jiang, Weiheng and Wu, Xiaogang and Wang, Yimou and Chen, Bolin and Feng, Wenjiang and Jin, Yi},
TITLE = {Time–Frequency-Analysis-Based Blind Modulation Classification for Multiple-Antenna Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {231},
URL = {https://www.mdpi.com/1424-8220/21/1/231},
PubMedID = {33401416},
ISSN = {1424-8220},
ABSTRACT = {Blind modulation classification is an important step in implementing cognitive radio networks. The multiple-input multiple-output (MIMO) technique is widely used in military and civil communication systems. Due to the lack of prior information about channel parameters and the overlapping of signals in MIMO systems, the traditional likelihood-based and feature-based approaches cannot be applied in these scenarios directly. Hence, in this paper, to resolve the problem of blind modulation classification in MIMO systems, the time&ndash;frequency analysis method based on the windowed short-time Fourier transform was used to analyze the time&ndash;frequency characteristics of time-domain modulated signals. Then, the extracted time&ndash;frequency characteristics are converted into red&ndash;green&ndash;blue (RGB) spectrogram images, and the convolutional neural network based on transfer learning was applied to classify the modulation types according to the RGB spectrogram images. Finally, a decision fusion module was used to fuse the classification results of all the receiving antennas. Through simulations, we analyzed the classification performance at different signal-to-noise ratios (SNRs); the results indicate that, for the single-input single-output (SISO) network, our proposed scheme can achieve 92.37% and 99.12% average classification accuracy at SNRs of &minus;4 and 10 dB, respectively. For the MIMO network, our scheme achieves 80.42% and 87.92% average classification accuracy at &minus;4 and 10 dB, respectively. The proposed method greatly improves the accuracy of modulation classification in MIMO networks.},
DOI = {10.3390/s21010231}
}



@Article{land10010029,
AUTHOR = {Papp, Levente and van Leeuwen, Boudewijn and Szilassi, Péter and Tobak, Zalán and Szatmári, József and Árvai, Mátyás and Mészáros, János and Pásztor, László},
TITLE = {Monitoring Invasive Plant Species Using Hyperspectral Remote Sensing Data},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {29},
URL = {https://www.mdpi.com/2073-445X/10/1/29},
ISSN = {2073-445X},
ABSTRACT = {The species richness and biodiversity of vegetation in Hungary are increasingly threatened by invasive plant species brought in from other continents and foreign ecosystems. These invasive plant species have spread aggressively in the natural and semi-natural habitats of Europe. Common milkweed (Asclepias syriaca) is one of the species that pose the greatest ecological menace. Therefore, the primary purpose of the present study is to map and monitor the spread of common milkweed, the most common invasive plant species in Europe. Furthermore, the possibilities to detect and validate this special invasive plant by analyzing hyperspectral remote sensing data were investigated. In combination with field reference data, high-resolution hyperspectral aerial images acquired by an unmanned aerial vehicle (UAV) platform in 138 spectral bands in areas infected by common milkweed were examined. Then, support vector machine (SVM) and artificial neural network (ANN) classification algorithms were applied to the highly accurate field reference data. As a result, common milkweed individuals were distinguished in hyperspectral images, achieving an overall accuracy of 92.95% in the case of supervised SVM classification. Using the ANN model, an overall accuracy of 99.61% was achieved. To evaluate the proposed approach, two experimental tests were conducted, and in both cases, we managed to distinguish the individual specimens within the large variety of spreading invasive species in a study area of 2 ha, based on centimeter spatial resolution hyperspectral UAV imagery.},
DOI = {10.3390/land10010029}
}



@Article{app11010363,
AUTHOR = {Roldán-Gómez, Juan Jesús and González-Gironda, Eduardo and Barrientos, Antonio},
TITLE = {A Survey on Robotic Technologies for Forest Firefighting: Applying Drone Swarms to Improve Firefighters’ Efficiency and Safety},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {363},
URL = {https://www.mdpi.com/2076-3417/11/1/363},
ISSN = {2076-3417},
ABSTRACT = {Forest firefighting missions encompass multiple tasks related to prevention, surveillance, and extinguishing. This work presents a complete survey of firefighters on the current problems in their work and the potential technological solutions. Additionally, it reviews the efforts performed by the academy and industry to apply different types of robots in the context of firefighting missions. Finally, all this information is used to propose a concept of operation for the comprehensive application of drone swarms in firefighting. The proposed system is a fleet of quadcopters that individually are only able to visit waypoints and use payloads, but collectively can perform tasks of surveillance, mapping, monitoring, etc. Three operator roles are defined, each one with different access to information and functions in the mission: mission commander, team leaders, and team members. These operators take advantage of virtual and augmented reality interfaces to intuitively get the information of the scenario and, in the case of the mission commander, control the drone swarm.},
DOI = {10.3390/app11010363}
}



@Article{drones5010004,
AUTHOR = {Flores, Donovan and González-Hernández, Iván and Lozano, Rogelio and Vazquez-Nicolas, Jesus Manuel and Hernandez Toral, Jorge Luis},
TITLE = {Automated Agave Detection and Counting Using a Convolutional Neural Network and Unmanned Aerial Systems},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {4},
URL = {https://www.mdpi.com/2504-446X/5/1/4},
ISSN = {2504-446X},
ABSTRACT = {We present an automatic agave detection method for counting plants based on aerial data from a UAV (Unmanned Aerial Vehicle). Our objective is to autonomously count the number of agave plants in an area to aid management of the yield. An orthomosaic is obtained from agave plantations, which is then used to create a database. This database is in turn used to train a Convolutional Neural Network (CNN). The proposed method is based on computer image processing, and the CNN increases the detection performance of the approach. The main contribution of the present paper is to propose a method for agave plant detection with a high level of precision. In order to test the proposed method in a real agave plantation, we develop a UAV platform, which is equipped with several sensors to reach accurate counting. Therefore, our prototype can safely track a desired path to detect and count agave plants. For comparison purposes, we perform the same application using a simpler algorithm. The result shows that our proposed algorithm has better performance reaching an F1 score of 0.96 as opposed to 0.57 for the Haar algorithm. The obtained experimental results suggest that the proposed algorithm is robust and has considerable potential to help farmers manage agave agroecosystems.},
DOI = {10.3390/drones5010004}
}



@Article{rs13010127,
AUTHOR = {Yeh, Chia-Cheng and Chang, Yang-Lang and Alkhaleefah, Mohammad and Hsu, Pai-Hui and Eng, Weiyong and Koo, Voon-Chet and Huang, Bormin and Chang, Lena},
TITLE = {YOLOv3-Based Matching Approach for Roof Region Detection from Drone Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {127},
URL = {https://www.mdpi.com/2072-4292/13/1/127},
ISSN = {2072-4292},
ABSTRACT = {Due to the large data volume, the UAV image stitching and matching suffers from high computational cost. The traditional feature extraction algorithms&mdash;such as Scale-Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), and Oriented FAST Rotated BRIEF (ORB)&mdash;require heavy computation to extract and describe features in high-resolution UAV images. To overcome this issue, You Only Look Once version 3 (YOLOv3) combined with the traditional feature point matching algorithms is utilized to extract descriptive features from the drone dataset of residential areas for roof detection. Unlike the traditional feature extraction algorithms, YOLOv3 performs the feature extraction solely on the proposed candidate regions instead of the entire image, thus the complexity of the image matching is reduced significantly. Then, all the extracted features are fed into Structural Similarity Index Measure (SSIM) to identify the corresponding roof region pair between consecutive image sequences. In addition, the candidate corresponding roof pair by our architecture serves as the coarse matching region pair and limits the search range of features matching to only the detected roof region. This further improves the feature matching consistency and reduces the chances of wrong feature matching. Analytical results show that the proposed method is 13&times; faster than the traditional image matching methods with comparable performance.},
DOI = {10.3390/rs13010127}
}



@Article{s21010256,
AUTHOR = {Han, Pengfei and Mei, Han and Liu, Di and Zeng, Ning and Tang, Xiao and Wang, Yinghong and Pan, Yuepeng},
TITLE = {Calibrations of Low-Cost Air Pollution Monitoring Sensors for CO, NO2, O3, and SO2},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {256},
URL = {https://www.mdpi.com/1424-8220/21/1/256},
PubMedID = {33401737},
ISSN = {1424-8220},
ABSTRACT = {Pollutant gases, such as CO, NO2, O3, and SO2 affect human health, and low-cost sensors are an important complement to regulatory-grade instruments in pollutant monitoring. Previous studies focused on one or several species, while comprehensive assessments of multiple sensors remain limited. We conducted a 12-month field evaluation of four Alphasense sensors in Beijing and used single linear regression (SLR), multiple linear regression (MLR), random forest regressor (RFR), and neural network (long short-term memory (LSTM)) methods to calibrate and validate the measurements with nearby reference measurements from national monitoring stations. For performances, CO &gt; O3 &gt; NO2 &gt; SO2 for the coefficient of determination (R2) and root mean square error (RMSE). The MLR did not increase the R2 after considering the temperature and relative humidity influences compared with the SLR (with R2 remaining at approximately 0.6 for O3 and 0.4 for NO2). However, the RFR and LSTM models significantly increased the O3, NO2, and SO2 performances, with the R2 increasing from 0.3&ndash;0.5 to &gt;0.7 for O3 and NO2, and the RMSE decreasing from 20.4 to 13.2 ppb for NO2. For the SLR, there were relatively larger biases, while the LSTMs maintained a close mean relative bias of approximately zero (e.g., &lt;5% for O3 and NO2), indicating that these sensors combined with the LSTMs are suitable for hot spot detection. We highlight that the performance of LSTM is better than that of random forest and linear methods. This study assessed four electrochemical air quality sensors and different calibration models, and the methodology and results can benefit assessments of other low-cost sensors.},
DOI = {10.3390/s21010256}
}



@Article{rs13010147,
AUTHOR = {De Swaef, Tom and Maes, Wouter H. and Aper, Jonas and Baert, Joost and Cougnon, Mathias and Reheul, Dirk and Steppe, Kathy and Roldán-Ruiz, Isabel and Lootens, Peter},
TITLE = {Applying RGB- and Thermal-Based Vegetation Indices from UAVs for High-Throughput Field Phenotyping of Drought Tolerance in Forage Grasses},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {147},
URL = {https://www.mdpi.com/2072-4292/13/1/147},
ISSN = {2072-4292},
ABSTRACT = {The persistence and productivity of forage grasses, important sources for feed production, are threatened by climate change-induced drought. Breeding programs are in search of new drought tolerant forage grass varieties, but those programs still rely on time-consuming and less consistent visual scoring by breeders. In this study, we evaluate whether Unmanned Aerial Vehicle (UAV) based remote sensing can complement or replace this visual breeder score. A field experiment was set up to test the drought tolerance of genotypes from three common forage types of two different species: Festuca arundinacea, diploid Lolium perenne and tetraploid Lolium perenne. Drought stress was imposed by using mobile rainout shelters. UAV flights with RGB and thermal sensors were conducted at five time points during the experiment. Visual-based indices from different colour spaces were selected that were closely correlated to the breeder score. Furthermore, several indices, in particular H and NDLab, from the HSV (Hue Saturation Value) and CIELab (Commission Internationale de l&rsquo;&eacute;clairage) colour space, respectively, displayed a broad-sense heritability that was as high or higher than the visual breeder score, making these indices highly suited for high-throughput field phenotyping applications that can complement or even replace the breeder score. The thermal-based Crop Water Stress Index CWSI provided complementary information to visual-based indices, enabling the analysis of differences in ecophysiological mechanisms for coping with reduced water availability between species and ploidy levels. All species/types displayed variation in drought stress tolerance, which confirms that there is sufficient variation for selection within these groups of grasses. Our results confirmed the better drought tolerance potential of Festuca arundinacea, but also showed which Lolium perenne genotypes are more tolerant.},
DOI = {10.3390/rs13010147}
}



@Article{rs13020162,
AUTHOR = {Qin, Jun and Wang, Biao and Wu, Yanlan and Lu, Qi and Zhu, Haochen},
TITLE = {Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {162},
URL = {https://www.mdpi.com/2072-4292/13/2/162},
ISSN = {2072-4292},
ABSTRACT = {Pine nematode is a highly contagious disease that causes great damage to the world&rsquo;s pine forest resources. Timely and accurate identification of pine nematode disease can help to control it. At present, there are few research on pine nematode disease identification, and it is difficult to accurately identify and locate nematode disease in a single pine by existing methods. This paper proposes a new network, SCANet (spatial-context-attention network), to identify pine nematode disease based on unmanned aerial vehicle (UAV) multi-spectral remote sensing images. In this method, a spatial information retention module is designed to reduce the loss of spatial information; it preserves the shallow features of pine nematode disease and expands the receptive field to enhance the extraction of deep features through a context information module. SCANet reached an overall accuracy of 79% and a precision and recall of around 0.86, and 0.91, respectively. In addition, 55 disease points among 59 known disease points were identified, which is better than other methods (DeepLab V3+, DenseNet, and HRNet). This paper presents a fast, precise, and practical method for identifying nematode disease and provides reliable technical support for the surveillance and control of pine wood nematode disease.},
DOI = {10.3390/rs13020162}
}



@Article{s21020343,
AUTHOR = {Bjerge, Kim and Nielsen, Jakob Bonde and Sepstrup, Martin Videbæk and Helsing-Nielsen, Flemming and Høye, Toke Thomas},
TITLE = {An Automated Light Trap to Monitor Moths (Lepidoptera) Using Computer Vision-Based Tracking and Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {343},
URL = {https://www.mdpi.com/1424-8220/21/2/343},
PubMedID = {33419136},
ISSN = {1424-8220},
ABSTRACT = {Insect monitoring methods are typically very time-consuming and involve substantial investment in species identification following manual trapping in the field. Insect traps are often only serviced weekly, resulting in low temporal resolution of the monitoring data, which hampers the ecological interpretation. This paper presents a portable computer vision system capable of attracting and detecting live insects. More specifically, the paper proposes detection and classification of species by recording images of live individuals attracted to a light trap. An Automated Moth Trap (AMT) with multiple light sources and a camera was designed to attract and monitor live insects during twilight and night hours. A computer vision algorithm referred to as Moth Classification and Counting (MCC), based on deep learning analysis of the captured images, tracked and counted the number of insects and identified moth species. Observations over 48 nights resulted in the capture of more than 250,000 images with an average of 5675 images per night. A customized convolutional neural network was trained on 2000 labeled images of live moths represented by eight different classes, achieving a high validation F1-score of 0.93. The algorithm measured an average classification and tracking F1-score of 0.71 and a tracking detection rate of 0.79. Overall, the proposed computer vision system and algorithm showed promising results as a low-cost solution for non-destructive and automatic monitoring of moths.},
DOI = {10.3390/s21020343}
}



@Article{su13020503,
AUTHOR = {Zhao, Rongkun and Li, Yuechen and Ma, Mingguo},
TITLE = {Mapping Paddy Rice with Satellite Remote Sensing: A Review},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {503},
URL = {https://www.mdpi.com/2071-1050/13/2/503},
ISSN = {2071-1050},
ABSTRACT = {Paddy rice is a staple food of three billion people in the world. Timely and accurate estimation of the paddy rice planting area and paddy rice yield can provide valuable information for the government, planners and decision makers to formulate policies. This article reviews the existing paddy rice mapping methods presented in the literature since 2010, classifies these methods, and analyzes and summarizes the basic principles, advantages and disadvantages of these methods. According to the data sources used, the methods are divided into three categories: (I) Optical mapping methods based on remote sensing; (II) Mapping methods based on microwave remote sensing; and (III) Mapping methods based on the integration of optical and microwave remote sensing. We found that the optical remote sensing data sources are mainly MODIS, Landsat, and Sentinel-2, and the emergence of Sentinel-1 data has promoted research on radar mapping methods for paddy rice. Multisource data integration further enhances the accuracy of paddy rice mapping. The best methods are phenology algorithms, paddy rice mapping combined with machine learning, and multisource data integration. Innovative methods include the time series similarity method, threshold method combined with mathematical models, and object-oriented image classification. With the development of computer technology and the establishment of cloud computing platforms, opportunities are provided for obtaining large-scale high-resolution rice maps. Multisource data integration, paddy rice mapping under different planting systems and the connection with global changes are the focus of future development priorities.},
DOI = {10.3390/su13020503}
}



@Article{rs13020183,
AUTHOR = {Avtar, Ram and Singh, Deepak and Umarhadi, Deha Agus and Yunus, Ali P. and Misra, Prakhar and Desai, Pranav N. and Kouser, Asma and Kurniawan, Tonni Agustiono and Phanindra, KBVN},
TITLE = {Impact of COVID-19 Lockdown on the Fisheries Sector: A Case Study from Three Harbors in Western India},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {183},
URL = {https://www.mdpi.com/2072-4292/13/2/183},
ISSN = {2072-4292},
ABSTRACT = {The COVID-19 related lockdowns have brought the planet to a standstill. It has severely shrunk the global economy in the year 2020, including India. The blue economy and especially the small-scale fisheries sector in India have dwindled due to disruptions in the fish catch, market, and supply chain. This research presents the applicability of satellite data to monitor the impact of COVID-19 related lockdown on the Indian fisheries sector. Three harbors namely Mangrol, Veraval, and Vankbara situated on the north-western coast of India were selected in this study based on characteristics like harbor&rsquo;s age, administrative control, and availability of cloud-free satellite images. To analyze the impact of COVID in the fisheries sector, we utilized high-resolution PlanetScope data for monitoring and comparison of &ldquo;area under fishing boats&rdquo; during the pre-lockdown, lockdown, and post-lockdown phases. A support vector machine (SVM) classification algorithm was used to identify the area under the boats. The classification results were complemented with socio-economic data and ground-level information for understanding the impact of the pandemic on the three sites. During the peak of the lockdown, it was found that the &ldquo;area under fishing boats&rdquo; near the docks and those parked on the land area increased by 483%, 189%, and 826% at Mangrol, Veraval, and Vanakbara harbor, respectively. After phase-I of lockdown, the number of parked vessels decreased, yet those already moved out to the land area were not returned until the south-west monsoon was over. A quarter of the annual production is estimated to be lost at the three harbors due to lockdown. Our last observation (September 2020) result shows that regular fishing activity has already been re-established in all three locations. PlanetScope data with daily revisit time has a higher potential to be used in the future and can help policymakers in making informed decisions vis-&agrave;-vis the fishing industry during an emergency situation like COVID-19.},
DOI = {10.3390/rs13020183}
}



@Article{en14020294,
AUTHOR = {Kulsinskas, Andrius and Durdevic, Petar and Ortiz-Arroyo, Daniel},
TITLE = {Internal Wind Turbine Blade Inspections Using UAVs: Analysis and Design Issues},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {294},
URL = {https://www.mdpi.com/1996-1073/14/2/294},
ISSN = {1996-1073},
ABSTRACT = {Interior and exterior wind turbine blade inspections are necessary to extend the lifetime of wind turbine generators. The use of unmanned vehicles is an alternative to exterior wind turbine blade inspections performed by technicians that require the use of cranes and ropes. Interior wind turbine blade inspections are even more challenging due to the confined spaces, lack of illumination, and the presence of potentially harmful internal structural components. Additionally, the cost of manned interior wind turbine blade inspections is a major limiting factor. This paper analyses all aspects of the viability of using manually controlled or autonomous aerial vehicles for interior wind turbine blade inspections. We discuss why the size, weight, and flight time of a vehicle, in addition to the structure of the wind turbine blade, are the main limiting factors in performing internal blade inspections. We also describe the design issues that must be considered to provide autonomy to unmanned vehicles and the control system, the sensors that can be used, and introduce some of the algorithms for localization, obstacle avoidance and path planning that are best suited for the task. Lastly, we briefly describe which non-destructive test instrumentation can be used for the purpose.},
DOI = {10.3390/en14020294}
}



@Article{app11020543,
AUTHOR = {Zhang, Tianxiang and Su, Jinya and Xu, Zhiyong and Luo, Yulin and Li, Jiangyun},
TITLE = {Sentinel-2 Satellite Imagery for Urban Land Cover Classification by Optimized Random Forest Classifier},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {543},
URL = {https://www.mdpi.com/2076-3417/11/2/543},
ISSN = {2076-3417},
ABSTRACT = {Land cover classification is able to reflect the potential natural and social process in urban development, providing vital information to stakeholders. Recent solutions on land cover classification are generally addressed by remotely sensed imagery and supervised classification methods. However, a high-performance classifier is desirable but challenging due to the existence of model hyperparameters. Conventional approaches generally rely on manual tuning, which is time-consuming and far from satisfying. Therefore, this work aims to propose a systematic method to automatically tune the hyperparameters by Bayesian parameter optimization for the random forest classifier. The recently launched Sentinel-2A/B satellites are drawn to provide the remote sensing imageries for land cover classification case study in Beijing, China, which have the best spectral/spatial resolutions among the freely available satellites. The improved random forest with Bayesian parameter optimization is compared against the support vector machine (SVM) and random forest (RF) with default hyperparameters by discriminating five land cover classes including building, tree, road, water, and crop field. Comparative experimental results show that the optimized RF classifier outperforms the conventional SVM and the RF with default hyperparameters in terms of accuracy, precision, and recall. The effects of band/feature number and the band usefulness are also assessed. It is envisaged that the improved classifier for Sentinel-2 satellite image processing can find a wide range of applications where high-resolution satellite imagery classification is applicable.},
DOI = {10.3390/app11020543}
}



@Article{app11020546,
AUTHOR = {Xie, Jiajia and Zhou, Rui and Liu, Yuan and Luo, Jun and Xie, Shaorong and Peng, Yan and Pu, Huayan},
TITLE = {Reinforcement-Learning-Based Asynchronous Formation Control Scheme for Multiple Unmanned Surface Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {546},
URL = {https://www.mdpi.com/2076-3417/11/2/546},
ISSN = {2076-3417},
ABSTRACT = {The high performance and efficiency of multiple unmanned surface vehicles (multi-USV) promote the further civilian and military applications of coordinated USV. As the basis of multiple USVs&rsquo; cooperative work, considerable attention has been spent on developing the decentralized formation control of the USV swarm. Formation control of multiple USV belongs to the geometric problems of a multi-robot system. The main challenge is the way to generate and maintain the formation of a multi-robot system. The rapid development of reinforcement learning provides us with a new solution to deal with these problems. In this paper, we introduce a decentralized structure of the multi-USV system and employ reinforcement learning to deal with the formation control of a multi-USV system in a leader&ndash;follower topology. Therefore, we propose an asynchronous decentralized formation control scheme based on reinforcement learning for multiple USVs. First, a simplified USV model is established. Simultaneously, the formation shape model is built to provide formation parameters and to describe the physical relationship between USVs. Second, the advantage deep deterministic policy gradient algorithm (ADDPG) is proposed. Third, formation generation policies and formation maintenance policies based on the ADDPG are proposed to form and maintain the given geometry structure of the team of USVs during movement. Moreover, three new reward functions are designed and utilized to promote policy learning. Finally, various experiments are conducted to validate the performance of the proposed formation control scheme. Simulation results and contrast experiments demonstrate the efficiency and stability of the formation control scheme.},
DOI = {10.3390/app11020546}
}



@Article{s21020391,
AUTHOR = {Bigazzi, Luca and Gherardini, Stefano and Innocenti, Giacomo and Basso, Michele},
TITLE = {Development of Non Expensive Technologies for Precise Maneuvering of Completely Autonomous Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {391},
URL = {https://www.mdpi.com/1424-8220/21/2/391},
PubMedID = {33429920},
ISSN = {1424-8220},
ABSTRACT = {In this paper, solutions for precise maneuvering of an autonomous small (e.g., 350-class) Unmanned Aerial Vehicles (UAVs) are designed and implemented from smart modifications of non expensive mass market technologies. The considered class of vehicles suffers from light load, and, therefore, only a limited amount of sensors and computing devices can be installed on-board. Then, to make the prototype capable of moving autonomously along a fixed trajectory, a &ldquo;cyber-pilot&rdquo;, able on demand to replace the human operator, has been implemented on an embedded control board. This cyber-pilot overrides the commands thanks to a custom hardware signal mixer. The drone is able to localize itself in the environment without ground assistance by using a camera possibly mounted on a 3 Degrees Of Freedom (DOF) gimbal suspension. A computer vision system elaborates the video stream pointing out land markers with known absolute position and orientation. This information is fused with accelerations from a 6-DOF Inertial Measurement Unit (IMU) to generate a &ldquo;virtual sensor&rdquo; which provides refined estimates of the pose, the absolute position, the speed and the angular velocities of the drone. Due to the importance of this sensor, several fusion strategies have been investigated. The resulting data are, finally, fed to a control algorithm featuring a number of uncoupled digital PID controllers which work to bring to zero the displacement from the desired trajectory.},
DOI = {10.3390/s21020391}
}



@Article{s21020395,
AUTHOR = {Wei, Ziang and Fernandes, Henrique and Herrmann, Hans-Georg and Tarpani, Jose Ricardo and Osman, Ahmad},
TITLE = {A Deep Learning Method for the Impact Damage Segmentation of Curve-Shaped CFRP Specimens Inspected by Infrared Thermography},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {395},
URL = {https://www.mdpi.com/1424-8220/21/2/395},
PubMedID = {33429939},
ISSN = {1424-8220},
ABSTRACT = {Advanced materials such as continuous carbon fiber-reinforced thermoplastic (CFRP) laminates are commonly used in many industries, mainly because of their strength, stiffness to weight ratio, toughness, weldability, and repairability. Structural components working in harsh environments such as satellites are permanently exposed to some sort of damage during their lifetimes. To detect and characterize these damages, non-destructive testing and evaluation techniques are essential tools, especially for composite materials. In this study, artificial intelligence was applied in combination with infrared thermography to detected and segment impact damage on curved laminates that were previously submitted to a severe thermal stress cycles and subsequent ballistic impacts. Segmentation was performed on both mid-wave and long-wave infrared sequences obtained simultaneously during pulsed thermography experiments by means of a deep neural network. A deep neural network was trained for each wavelength. Both networks generated satisfactory results. The model trained with mid-wave images achieved an F1-score of 92.74% and the model trained with long-wave images achieved an F1-score of 87.39%.},
DOI = {10.3390/s21020395}
}



@Article{s21020396,
AUTHOR = {Galyaev, Andrey A. and Lysenko, Pavel V. and Yakhno, Victor P.},
TITLE = {2D Optimal Trajectory Planning Problem in Threat Environment for UUV with Non-Uniform Radiation Pattern},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/1424-8220/21/2/396},
PubMedID = {33429963},
ISSN = {1424-8220},
ABSTRACT = {Path planning is necessary in many applications using unmanned underwater vehicles (UUVs). The main class of tasks is the planning of safe routes with minimal energy costs and/or minimal levels of emitted physical and information signals. Since the action planner is on board the UUV, the main focus is on methods and algorithms that allow it to build reference trajectories while minimizing the number of calculations. The study is devoted to the problem of the optimal route planning for a UUV with a non-uniform radiation pattern. The problem is stated in the form of two point variational problem for which necessary and sufficient optimality conditions are proved. Particular attention is paid to cases where optimality conditions are not met. These cases are directly related to found specific forms of a radiation pattern. Sufficient optimality conditions are extended on the class of two-link and multi-link motion paths. Software tools have been developed and computer simulations have been performed for various types of radiation patterns.},
DOI = {10.3390/s21020396}
}



@Article{rs13020197,
AUTHOR = {Dirscherl, Mariel and Dietz, Andreas J. and Kneisel, Christof and Kuenzer, Claudia},
TITLE = {A Novel Method for Automated Supraglacial Lake Mapping in Antarctica Using Sentinel-1 SAR Imagery and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {197},
URL = {https://www.mdpi.com/2072-4292/13/2/197},
ISSN = {2072-4292},
ABSTRACT = {Supraglacial meltwater accumulation on ice sheets can be a main driver for accelerated ice discharge, mass loss, and global sea-level-rise. With further increasing surface air temperatures, meltwater-induced hydrofracturing, basal sliding, or surface thinning will cumulate and most likely trigger unprecedented ice mass loss on the Greenland and Antarctic ice sheets. While the Greenland surface hydrological network as well as its impacts on ice dynamics and mass balance has been studied in much detail, Antarctic supraglacial lakes remain understudied with a circum-Antarctic record of their spatio-temporal development entirely lacking. This study provides the first automated supraglacial lake extent mapping method using Sentinel-1 synthetic aperture radar (SAR) imagery over Antarctica and complements the developed optical Sentinel-2 supraglacial lake detection algorithm presented in our companion paper. In detail, we propose the use of a modified U-Net for semantic segmentation of supraglacial lakes in single-polarized Sentinel-1 imagery. The convolutional neural network (CNN) is implemented with residual connections for optimized performance as well as an Atrous Spatial Pyramid Pooling (ASPP) module for multiscale feature extraction. The algorithm is trained on 21,200 Sentinel-1 image patches and evaluated in ten spatially or temporally independent test acquisitions. In addition, George VI Ice Shelf is analyzed for intra-annual lake dynamics throughout austral summer 2019/2020 and a decision-level fused Sentinel-1 and Sentinel-2 maximum lake extent mapping product is presented for January 2020 revealing a more complete supraglacial lake coverage (~770 km2) than the individual single-sensor products. Classification results confirm the reliability of the proposed workflow with an average Kappa coefficient of 0.925 and a F1-score of 93.0% for the supraglacial water class across all test regions. Furthermore, the algorithm is applied in an additional test region covering supraglacial lakes on the Greenland ice sheet which further highlights the potential for spatio-temporal transferability. Future work involves the integration of more training data as well as intra-annual analyses of supraglacial lake occurrence across the whole continent and with focus on supraglacial lake development throughout a summer melt season and into Antarctic winter.},
DOI = {10.3390/rs13020197}
}



@Article{f12010066,
AUTHOR = {Korznikov, Kirill A. and Kislov, Dmitry E. and Altman, Jan and Doležal, Jiří and Vozmishcheva, Anna S. and Krestov, Pavel V.},
TITLE = {Using U-Net-Like Deep Convolutional Neural Networks for Precise Tree Recognition in Very High Resolution RGB (Red, Green, Blue) Satellite Images},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/1999-4907/12/1/66},
ISSN = {1999-4907},
ABSTRACT = {Very high resolution satellite imageries provide an excellent foundation for precise mapping of plant communities and even single plants. We aim to perform individual tree recognition on the basis of very high resolution RGB (red, green, blue) satellite images using deep learning approaches for northern temperate mixed forests in the Primorsky Region of the Russian Far East. We used a pansharpened satellite RGB image by GeoEye-1 with a spatial resolution of 0.46 m/pixel, obtained in late April 2019. We parametrized the standard U-Net convolutional neural network (CNN) and trained it in manually delineated satellite images to solve the satellite image segmentation problem. For comparison purposes, we also applied standard pixel-based classification algorithms, such as random forest, k-nearest neighbor classifier, naive Bayes classifier, and quadratic discrimination. Pattern-specific features based on grey level co-occurrence matrices (GLCM) were computed to improve the recognition ability of standard machine learning methods. The U-Net-like CNN allowed us to obtain precise recognition of Mongolian poplar (Populus suaveolens Fisch. ex Loudon s.l.) and evergreen coniferous trees (Abies holophylla Maxim., Pinus koraiensis Siebold &amp; Zucc.). We were able to distinguish species belonging to either poplar or coniferous groups but were unable to separate species within the same group (i.e. A. holophylla and P. koraiensis were not distinguishable). The accuracy of recognition was estimated by several metrics and exceeded values obtained for standard machine learning approaches. In contrast to pixel-based recognition algorithms, the U-Net-like CNN does not lead to an increase in false-positive decisions when facing green-colored objects that are similar to trees. By means of U-Net-like CNN, we obtained a mean accuracy score of up to 0.96 in our computational experiments. The U-Net-like CNN recognizes tree crowns not as a set of pixels with known RGB intensities but as spatial objects with a specific geometry and pattern. This CNN&rsquo;s specific feature excludes misclassifications related to objects of similar colors as objects of interest. We highlight that utilization of satellite images obtained within the suitable phenological season is of high importance for successful tree recognition. The suitability of the phenological season is conceptualized as a group of conditions providing highlighting objects of interest over other components of vegetation cover. In our case, the use of satellite images captured in mid-spring allowed us to recognize evergreen fir and pine trees as the first class of objects (&ldquo;conifers&rdquo;) and poplars as the second class, which were in a leafless state among other deciduous tree species.},
DOI = {10.3390/f12010066}
}



@Article{s21020409,
AUTHOR = {Ince, Omer Faruk and Kim, Jun-Sik},
TITLE = {TIMA SLAM: Tracking Independently and Mapping Altogether for an Uncalibrated Multi-Camera System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {409},
URL = {https://www.mdpi.com/1424-8220/21/2/409},
PubMedID = {33435556},
ISSN = {1424-8220},
ABSTRACT = {We present a novel simultaneous localization and mapping (SLAM) system that extends the state-of-the-art ORB-SLAM2 for multi-camera usage without precalibration. In this system, each camera is tracked independently on a shared map, and the extrinsic parameters of each camera in the fixed multi-camera system are estimated online up to a scalar ambiguity (for RGB cameras). Thus, the laborious precalibration of extrinsic parameters between cameras becomes needless. By optimizing the map, the keyframe poses, and the relative poses of the multi-camera system simultaneously, observations from the multiple cameras are utilized robustly, and the accuracy of the shared map is improved. The system is not only compatible with RGB sensors but also works on RGB-D cameras. For RGB cameras, the performance of the system tested on the well-known EuRoC/ASL and KITTI datasets that are in the stereo configuration for indoor and outdoor environments, respectively, as well as our dataset that consists of three cameras with small overlapping regions. For the RGB-D tests, we created a dataset that consists of two cameras for an indoor environment. The experimental results showed that the proposed method successfully provides an accurate multi-camera SLAM system without precalibration of the multi-cameras.},
DOI = {10.3390/s21020409}
}



@Article{rs13020216,
AUTHOR = {Wang, Yutang and Wang, Jia and Chang, Shuping and Sun, Lu and An, Likun and Chen, Yuhan and Xu, Jiangqi},
TITLE = {Classification of Street Tree Species Using UAV Tilt Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {216},
URL = {https://www.mdpi.com/2072-4292/13/2/216},
ISSN = {2072-4292},
ABSTRACT = {As an important component of the urban ecosystem, street trees have made an outstanding contribution to alleviating urban environmental pollution. Accurately extracting tree characteristics and species information can facilitate the monitoring and management of street trees, as well as aiding landscaping and studies of urban ecology. In this study, we selected the suburban areas of Beijing and Zhangjiakou and investigated six representative street tree species using unmanned aerial vehicle (UAV) tilt photogrammetry. We extracted five tree attributes and four combined attribute parameters and used four types of commonly-used machine learning classification algorithms as classifiers for tree species classification. The results show that random forest (RF), support vector machine (SVM), and back propagation (BP) neural network provide better classification results when using combined parameters for tree species classification, compared with those using individual tree attributes alone; however, the K-nearest neighbor (KNN) algorithm produced the opposite results. The best combination for classification is the BP neural network using combined attributes, with a classification precision of 89.1% and F-measure of 0.872, and we conclude that this approach best meets the requirements of street tree surveys. The results also demonstrate that optical UAV tilt photogrammetry combined with a machine learning classification algorithm is a low-cost, high-efficiency, and high-precision method for tree species classification.},
DOI = {10.3390/rs13020216}
}



@Article{jmse9010065,
AUTHOR = {Xu, Jin and Pan, Xinxiang and Jia, Baozhu and Wu, Xuerui and Liu, Peng and Li, Bo},
TITLE = {Oil Spill Detection Using LBP Feature and K-Means Clustering in Shipborne Radar Image},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {9},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {65},
URL = {https://www.mdpi.com/2077-1312/9/1/65},
ISSN = {2077-1312},
ABSTRACT = {Oil spill accidents have seriously harmed the marine environment. Effective oil spill monitoring can provide strong scientific and technological support for emergency response of law enforcement departments. Shipborne radar can be used to monitor oil spills immediately after the accident. In this paper, the original shipborne radar image collected by the teaching-practice ship Yukun of Dalian Maritime University during the oil spill accident of Dalian on 16 July 2010 was taken as the research data, and an oil spill detection method was proposed by using LBP texture feature and K-means algorithm. First, Laplacian operator, Otsu algorithm, and mean filter were used to suppress the co-frequency interference noises and high brightness pixels. Then the gray intensity correction matrix was used to reduce image nonuniformity. Next, using LBP texture feature and K-means clustering algorithm, the effective oil spill regions were extracted. Finally, the adaptive threshold was applied to identify the oil films. This method can automatically detect oil spills in shipborne radar image. It can provide a guarantee for real-time monitoring of oil spill accidents.},
DOI = {10.3390/jmse9010065}
}



@Article{ijgi10010022,
AUTHOR = {Yu, Tong and Wu, Wenjin and Gong, Chen and Li, Xinwu},
TITLE = {Residual Multi-Attention Classification Network for A Forest Dominated Tropical Landscape Using High-Resolution Remote Sensing Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {22},
URL = {https://www.mdpi.com/2220-9964/10/1/22},
ISSN = {2220-9964},
ABSTRACT = {Tropical forests are of vital importance for maintaining biodiversity, regulating climate and material cycles while facing deforestation, agricultural reclamation, and managing various pressures. Remote sensing (RS) can support effective monitoring and mapping approaches for tropical forests, and to facilitate this we propose a deep neural network with an encoder&ndash;decoder architecture here to classify tropical forests and their environment. To deal with the complexity of tropical landscapes, this method utilizes a multi-scale convolution neural network (CNN) to expand the receptive field and extract multi-scale features. The model refines the features with several attention modules and fuses them through an upsampling module. A two-stage training strategy is proposed to alleviate misclassifications caused by sample imbalances. A joint loss function based on cross-entropy loss and the generalized Dice loss is applied in the first stage, and the second stage used the focal loss to fine-tune the weights. As a case study, we use Hainan tropical reserves to test the performance of this model. Compared with four state-of-the-art (SOTA) semantic segmentation networks, our network achieves the best performance with two Hainan datasets (mean intersection over union (MIoU) percentages of 85.78% and 82.85%). We also apply the new model to classify a public true color dataset which has 17 semantic classes and obtain results with an 83.75% MIoU. This further demonstrates the applicability and potential of this model in complex classification tasks.},
DOI = {10.3390/ijgi10010022}
}



@Article{s21020471,
AUTHOR = {Kentsch, Sarah and Cabezas, Mariano and Tomhave, Luca and Groß, Jens and Burkhard, Benjamin and Lopez Caceres, Maximo Larry and Waki, Katsushi and Diez, Yago},
TITLE = {Analysis of UAV-Acquired Wetland Orthomosaics Using GIS, Computer Vision, Computational Topology and Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {471},
URL = {https://www.mdpi.com/1424-8220/21/2/471},
PubMedID = {33440797},
ISSN = {1424-8220},
ABSTRACT = {Invasive blueberry species endanger the sensitive environment of wetlands and protection laws call for management measures. Therefore, methods are needed to identify blueberry bushes, locate them, and characterise their distribution and properties with a minimum of disturbance. UAVs (Unmanned Aerial Vehicles) and image analysis have become important tools for classification and detection approaches. In this study, techniques, such as GIS (Geographical Information Systems) and deep learning, were combined in order to detect invasive blueberry species in wetland environments. Images that were collected by UAV were used to produce orthomosaics, which were analysed to produce maps of blueberry location, distribution, and spread in each study site, as well as bush height and area information. Deep learning networks were used with transfer learning and unfrozen weights in order to automatically detect blueberry bushes reaching True Positive Values (TPV) of 93.83% and an Overall Accuracy (OA) of 98.83%. A refinement of the result masks reached a Dice of 0.624. This study provides an efficient and effective methodology to study wetlands while using different techniques.},
DOI = {10.3390/s21020471}
}



@Article{bdcc5010002,
AUTHOR = {Fenu, Gianni and Malloci, Francesca Maridina},
TITLE = {Forecasting Plant and Crop Disease: An Explorative Study on Current Algorithms},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/2504-2289/5/1/2},
ISSN = {2504-2289},
ABSTRACT = {Every year, plant diseases cause a significant loss of valuable food crops around the world. The plant and crop disease management practice implemented in order to mitigate damages have changed considerably. Today, through the application of new information and communication technologies, it is possible to predict the onset or change in the severity of diseases using modern big data analysis techniques. In this paper, we present an analysis and classification of research studies conducted over the past decade that forecast the onset of disease at a pre-symptomatic stage (i.e., symptoms not visible to the naked eye) or at an early stage. We examine the specific approaches and methods adopted, pre-processing techniques and data used, performance metrics, and expected results, highlighting the issues encountered. The results of the study reveal that this practice is still in its infancy and that many barriers need to be overcome.},
DOI = {10.3390/bdcc5010002}
}



@Article{rs13020239,
AUTHOR = {Shao, Zhenfeng and Zhou, Zifan and Huang, Xiao and Zhang, Ya},
TITLE = {MRENet: Simultaneous Extraction of Road Surface and Road Centerline in Complex Urban Scenes from Very High-Resolution Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {239},
URL = {https://www.mdpi.com/2072-4292/13/2/239},
ISSN = {2072-4292},
ABSTRACT = {Automatic extraction of the road surface and road centerline from very high-resolution (VHR) remote sensing images has always been a challenging task in the field of feature extraction. Most existing road datasets are based on data with simple and clear backgrounds under ideal conditions, such as images derived from Google Earth. Therefore, the studies on road surface extraction and road centerline extraction under complex scenes are insufficient. Meanwhile, most existing efforts addressed these two tasks separately, without considering the possible joint extraction of road surface and centerline. With the introduction of multitask convolutional neural network models, it is possible to carry out these two tasks simultaneously by facilitating information sharing within a multitask deep learning model. In this study, we first design a challenging dataset using remote sensing images from the GF-2 satellite. The dataset contains complex road scenes with manually annotated images. We then propose a two-task and end-to-end convolution neural network, termed Multitask Road-related Extraction Network (MRENet), for road surface extraction and road centerline extraction. We take features extracted from the road as the condition of centerline extraction, and the information transmission and parameter sharing between the two tasks compensate for the potential problem of insufficient road centerline samples. In the network design, we use atrous convolutions and a pyramid scene parsing pooling module (PSP pooling), aiming to expand the network receptive field, integrate multilevel features, and obtain more abundant information. In addition, we use a weighted binary cross-entropy function to alleviate the background imbalance problem. Experimental results show that the proposed algorithm outperforms several comparative methods in the aspects of classification precision and visual interpretation.},
DOI = {10.3390/rs13020239}
}



@Article{s21020507,
AUTHOR = {Wang, Le and Xiang, Lirong and Tang, Lie and Jiang, Huanyu},
TITLE = {A Convolutional Neural Network-Based Method for Corn Stand Counting in the Field},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {507},
URL = {https://www.mdpi.com/1424-8220/21/2/507},
PubMedID = {33450839},
ISSN = {1424-8220},
ABSTRACT = {Accurate corn stand count in the field at early season is of great interest to corn breeders and plant geneticists. However, the commonly used manual counting method is time consuming, laborious, and prone to error. Nowadays, unmanned aerial vehicles (UAV) tend to be a popular base for plant-image-collecting platforms. However, detecting corn stands in the field is a challenging task, primarily because of camera motion, leaf fluttering caused by wind, shadows of plants caused by direct sunlight, and the complex soil background. As for the UAV system, there are mainly two limitations for early seedling detection and counting. First, flying height cannot ensure a high resolution for small objects. It is especially difficult to detect early corn seedlings at around one week after planting, because the plants are small and difficult to differentiate from the background. Second, the battery life and payload of UAV systems cannot support long-duration online counting work. In this research project, we developed an automated, robust, and high-throughput method for corn stand counting based on color images extracted from video clips. A pipeline developed based on the YoloV3 network and Kalman filter was used to count corn seedlings online. The results demonstrate that our method is accurate and reliable for stand counting, achieving an accuracy of over 98% at growth stages V2 and V3 (vegetative stages with two and three visible collars) with an average frame rate of 47 frames per second (FPS). This pipeline can also be mounted easily on manned cart, tractor, or field robotic systems for online corn counting.},
DOI = {10.3390/s21020507}
}



@Article{s21020513,
AUTHOR = {Lemaire, Pierre and Crispim-Junior, Carlos Fernando and Robinault, Lionel and Tougne, Laure},
TITLE = {Registering Unmanned Aerial Vehicle Videos in the Long Term},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {513},
URL = {https://www.mdpi.com/1424-8220/21/2/513},
PubMedID = {33450881},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have become a very popular way of acquiring video within contexts such as remote data acquisition or surveillance. Unfortunately, their viewpoint is often unstable, which tends to impact the automatic processing of their video flux negatively. To counteract the effects of an inconsistent viewpoint, two video processing strategies are classically adopted, namely registration and stabilization, which tend to be affected by distinct issues, namely jitter and drifting. Following our prior work, we suggest that the motion estimators used in both situations can be modeled to take into account their inherent errors. By acknowledging that drifting and jittery errors are of a different nature, we propose a combination that is able to limit their influence and build a hybrid solution for jitter-free video registration. In this work, however, our modeling was restricted to 2D-rigid transforms, which are rather limited in the case of airborne videos. In the present paper, we extend and refine the theoretical ground of our previous work. This addition allows us to show how to practically adapt our previous work to perspective transforms, which our study shows to be much more accurate for this problem. A lightweight implementation enables us to automatically register stationary UAV videos in real time. Our evaluation includes traffic surveillance recordings of up to 2 h and shows the potential of the proposed approach when paired with background subtraction tasks.},
DOI = {10.3390/s21020513}
}



@Article{app11020716,
AUTHOR = {Chen, Ruibiao and Shu, Fangxing and Lei, Kai and Wang, Jianping and Zhang, Liangjie},
TITLE = {User Clustering and Power Allocation for Energy Efficiency Maximization in Downlink Non-Orthogonal Multiple Access Systems},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {716},
URL = {https://www.mdpi.com/2076-3417/11/2/716},
ISSN = {2076-3417},
ABSTRACT = {Non-orthogonal multiple access (NOMA) has been considered a promising technique for the fifth generation (5G) mobile communication networks because of its high spectrum efficiency. In NOMA, by using successive interference cancellation (SIC) techniques at the receivers, multiple users with different channel gain can be multiplexed together in the same subchannel for concurrent transmission in the same spectrum. The simultaneously multiple transmission achieves high system throughput in NOMA. However, it also leads to more energy consumption, limiting its application in many energy-constrained scenarios. As a result, the enhancement of energy efficiency becomes a critical issue in NOMA systems. This paper focuses on efficient user clustering strategy and power allocation design of downlink NOMA systems. The energy efficiency maximization of downlink NOMA systems is formulated as an NP-hard optimization problem under maximum transmission power, minimum data transmission rate requirement, and SIC requirement. For the approximate solution with much lower complexity, we first exploit a quick suboptimal clustering method to assign each user to a subchannel. Given the user clustering result, the optimal power allocation problem is solved in two steps. By employing the Lagrangian multiplier method with Karush&ndash;Kuhn&ndash;Tucker optimality conditions, the optimal power allocation is calculated for each subchannel. In addition, then, an inter-cluster dynamic programming model is further developed to achieve the overall maximum energy efficiency. The theoretical analysis and simulations show that the proposed schemes achieve a significant energy efficiency gain compared with existing methods.},
DOI = {10.3390/app11020716}
}



@Article{sym13010128,
AUTHOR = {Ma, Yong and Liu, Yangguo and Zhang, Lin and Cao, Yuanlong and Guo, Shihui and Li, Hanxi},
TITLE = {Research Review on Parking Space Detection Method},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {128},
URL = {https://www.mdpi.com/2073-8994/13/1/128},
ISSN = {2073-8994},
ABSTRACT = {The parking assist system is an essential application of the car&rsquo;s active collision avoidance system in low-speed and complex urban environments, which has been a hot research topic in recent years. Parking space detection is an important step of the parking assistance system, and its research object is parking spaces with symmetrical structures in parking lots. By analyzing and investigating parking space information measured by the sensors, reliable detection of sufficient parking spaces can be realized. First, this article discusses the main problems in the process of detecting parking spaces, illustrating the research significance and current research status of parking space detection methods. In addition, it further introduces some parking space detection methods, including free-space-based methods, parking-space-marking-based methods, user-interface-based methods, and infrastructure-based methods, which are all under methods of parking space selection. Lastly, this article summarizes the parking space detection methods, which gives a clear direction for future research.},
DOI = {10.3390/sym13010128}
}



@Article{rs13020260,
AUTHOR = {Nguyen, Ha Trang and Lopez Caceres, Maximo Larry and Moritake, Koma and Kentsch, Sarah and Shu, Hase and Diez, Yago},
TITLE = {Individual Sick Fir Tree (Abies mariesii) Identification in Insect Infested Forests by Means of UAV Images and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {260},
URL = {https://www.mdpi.com/2072-4292/13/2/260},
ISSN = {2072-4292},
ABSTRACT = {Insect outbreaks are a recurrent natural phenomenon in forest ecosystems expected to increase due to climate change. Recent advances in Unmanned Aerial Vehicles (UAV) and Deep Learning (DL) Networks provide us with tools to monitor them. In this study we used nine orthomosaics and normalized Digital Surface Models (nDSM) to detect and classify healthy and sick Maries fir trees as well as deciduous trees. This study aims at automatically classifying treetops by means of a novel computer vision treetops detection algorithm and the adaptation of existing DL architectures. Considering detection alone, the accuracy results showed 85.70% success. In terms of detection and classification, we were able to detect/classify correctly 78.59% of all tree classes (39.64% for sick fir). However, with data augmentation, detection/classification percentage of the sick fir class rose to 73.01% at the cost of the result accuracy of all tree classes that dropped 63.57%. The implementation of UAV, computer vision and DL techniques contribute to the development of a new approach to evaluate the impact of insect outbreaks in forest.},
DOI = {10.3390/rs13020260}
}



@Article{electronics10020169,
AUTHOR = {Hashima, Sherief and ElHalawany, Basem M. and Hatano, Kohei and Wu, Kaishun and Mohamed, Ehab Mahmoud},
TITLE = {Leveraging Machine-Learning for D2D Communications in 5G/Beyond 5G Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {169},
URL = {https://www.mdpi.com/2079-9292/10/2/169},
ISSN = {2079-9292},
ABSTRACT = {Device-to-device (D2D) communication is a promising paradigm for the fifth generation (5G) and beyond 5G (B5G) networks. Although D2D communication provides several benefits, including limited interference, energy efficiency, reduced delay, and network overhead, it faces a lot of technical challenges such as network architecture, and neighbor discovery, etc. The complexity of configuring D2D links and managing their interference, especially when using millimeter-wave (mmWave), inspire researchers to leverage different machine-learning (ML) techniques to address these problems towards boosting the performance of D2D networks. In this paper, a comprehensive survey about recent research activities on D2D networks will be explored with putting more emphasis on utilizing mmWave and ML methods. After exploring existing D2D research directions accompanied with their existing conventional solutions, we will show how different ML techniques can be applied to enhance the D2D networks performance over using conventional ways. Then, still open research directions in ML applications on D2D networks will be investigated including their essential needs. A case study of applying multi-armed bandit (MAB) as an efficient online ML tool to enhance the performance of neighbor discovery and selection (NDS) in mmWave D2D networks will be presented. This case study will put emphasis on the high potency of using ML solutions over using the conventional non-ML based methods for highly improving the average throughput performance of mmWave NDS.},
DOI = {10.3390/electronics10020169}
}



@Article{rs13020274,
AUTHOR = {Yao, Guobiao and Yilmaz, Alper and Zhang, Li and Meng, Fei and Ai, Haibin and Jin, Fengxiang},
TITLE = {Matching Large Baseline Oblique Stereo Images Using an End-to-End Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {274},
URL = {https://www.mdpi.com/2072-4292/13/2/274},
ISSN = {2072-4292},
ABSTRACT = {The available stereo matching algorithms produce large number of false positive matches or only produce a few true-positives across oblique stereo images with large baseline. This undesired result happens due to the complex perspective deformation and radiometric distortion across the images. To address this problem, we propose a novel affine invariant feature matching algorithm with subpixel accuracy based on an end-to-end convolutional neural network (CNN). In our method, we adopt and modify a Hessian affine network, which we refer to as IHesAffNet, to obtain affine invariant Hessian regions using deep learning framework. To improve the correlation between corresponding features, we introduce an empirical weighted loss function (EWLF) based on the negative samples using K nearest neighbors, and then generate deep learning-based descriptors with high discrimination that is realized with our multiple hard network structure (MTHardNets). Following this step, the conjugate features are produced by using the Euclidean distance ratio as the matching metric, and the accuracy of matches are optimized through the deep learning transform based least square matching (DLT-LSM). Finally, experiments on Large baseline oblique stereo images acquired by ground close-range and unmanned aerial vehicle (UAV) verify the effectiveness of the proposed approach, and comprehensive comparisons demonstrate that our matching algorithm outperforms the state-of-art methods in terms of accuracy, distribution and correct ratio. The main contributions of this article are: (i) our proposed MTHardNets can generate high quality descriptors; and (ii) the IHesAffNet can produce substantial affine invariant corresponding features with reliable transform parameters.},
DOI = {10.3390/rs13020274}
}



@Article{aerospace8010018,
AUTHOR = {Wada, Daichi and Araujo-Estrada, Sergio A. and Windsor, Shane},
TITLE = {Unmanned Aerial Vehicle Pitch Control Using Deep Reinforcement Learning with Discrete Actions in Wind Tunnel Test},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2226-4310/8/1/18},
ISSN = {2226-4310},
ABSTRACT = {Deep reinforcement learning is a promising method for training a nonlinear attitude controller for fixed-wing unmanned aerial vehicles. Until now, proof-of-concept studies have demonstrated successful attitude control in simulation. However, detailed experimental investigations have not yet been conducted. This study applied deep reinforcement learning for one-degree-of-freedom pitch control in wind tunnel tests with the aim of gaining practical understandings of attitude control application. Three controllers with different discrete action choices, that is, elevator angles, were designed. The controllers with larger action rates exhibited better performance in terms of following angle-of-attack commands. The root mean square errors for tracking angle-of-attack commands decreased from 3.42&deg; to 1.99&deg; as the maximum action rate increased from 10&deg;/s to 50&deg;/s. The comparison between experimental and simulation results showed that the controller with a smaller action rate experienced the friction effect, and the controllers with larger action rates experienced fluctuating behaviors in elevator maneuvers owing to delay. The investigation of the effect of friction and delay on pitch control highlighted the importance of conducting experiments to understand actual control performances, specifically when the controllers were trained with a low-fidelity model.},
DOI = {10.3390/aerospace8010018}
}



@Article{brainsci11010106,
AUTHOR = {Andreu-Perez, Ana R. and Kiani, Mehrin and Andreu-Perez, Javier and Reddy, Pratusha and Andreu-Abela, Jaime and Pinto, Maria and Izzetoglu, Kurtulus},
TITLE = {Single-Trial Recognition of Video Gamer’s Expertise from Brain Haemodynamic and Facial Emotion Responses},
JOURNAL = {Brain Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {106},
URL = {https://www.mdpi.com/2076-3425/11/1/106},
PubMedID = {33466787},
ISSN = {2076-3425},
ABSTRACT = {With an increase in consumer demand of video gaming entertainment, the game industry is exploring novel ways of game interaction such as providing direct interfaces between the game and the gamers&rsquo; cognitive or affective responses. In this work, gamer&rsquo;s brain activity has been imaged using functional near infrared spectroscopy (fNIRS) whilst they watch video of a video game (League of Legends) they play. A video of the face of the participants is also recorded for each of a total of 15 trials where a trial is defined as watching a gameplay video. From the data collected, i.e., gamer&rsquo;s fNIRS data in combination with emotional state estimation from gamer&rsquo;s facial expressions, the expertise level of the gamers has been decoded per trial in a multi-modal framework comprising of unsupervised deep feature learning and classification by state-of-the-art models. The best tri-class classification accuracy is obtained using a cascade of random convolutional kernel transform (ROCKET) feature extraction method and deep classifier at 91.44%. This is the first work that aims at decoding expertise level of gamers using non-restrictive and portable technologies for brain imaging, and emotional state recognition derived from gamers&rsquo; facial expressions. This work has profound implications for novel designs of future human interactions with video games and brain-controlled games.},
DOI = {10.3390/brainsci11010106}
}



@Article{rs13020276,
AUTHOR = {Manish, Raja and Lin, Yi-Chun and Ravi, Radhika and Hasheminasab, Seyyed Meghdad and Zhou, Tian and Habib, Ayman},
TITLE = {Development of a Miniaturized Mobile Mapping System for In-Row, Under-Canopy Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {276},
URL = {https://www.mdpi.com/2072-4292/13/2/276},
ISSN = {2072-4292},
ABSTRACT = {This paper focuses on the development of a miniaturized mobile mapping platform with advantages over current agricultural phenotyping systems in terms of acquiring data that facilitate under-canopy plant trait extraction. The system is based on an unmanned ground vehicle (UGV) for in-row, under-canopy data acquisition to deliver accurately georeferenced 2D and 3D products. The paper addresses three main aspects pertaining to the UGV development: (a) architecture of the UGV mobile mapping system (MMS), (b) quality assessment of acquired data in terms of georeferencing information as well as derived 3D point cloud, and (c) ability to derive phenotypic plant traits using data acquired by the UGV MMS. The experimental results from this study demonstrate the ability of the UGV MMS to acquire dense and accurate data over agricultural fields that would facilitate highly accurate plant phenotyping (better than above-canopy platforms such as unmanned aerial systems and high-clearance tractors). Plant centers and plant count with an accuracy in the 90% range have been achieved.},
DOI = {10.3390/rs13020276}
}



@Article{rs13020278,
AUTHOR = {Zheng, Qiong and Ye, Huichun and Huang, Wenjiang and Dong, Yingying and Jiang, Hao and Wang, Chongyang and Li, Dan and Wang, Li and Chen, Shuisen},
TITLE = {Integrating Spectral Information and Meteorological Data to Monitor Wheat Yellow Rust at a Regional Scale: A Case Study},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {278},
URL = {https://www.mdpi.com/2072-4292/13/2/278},
ISSN = {2072-4292},
ABSTRACT = {Wheat yellow rust has a severe impact on wheat production and threatens food security in China; as such, an effective monitoring method is necessary at the regional scale. We propose a model for yellow rust monitoring based on Sentinel-2 multispectral images and a series of two-stage vegetation indices and meteorological data. Sensitive spectral vegetation indices (single- and two-stage indices) and meteorological features for wheat yellow rust discrimination were selected using the random forest method. Wheat yellow rust monitoring models were established using three different classification methods: linear discriminant analysis (LDA), support vector machine (SVM), and artificial neural network (ANN). The results show that models based on two-stage indices (i.e., those calculated using images from two different days) significantly outperform single-stage index models (i.e., those calculated using an image from a single day), the overall accuracy improved from 63.2% to 78.9%. The classification accuracies of models combining a vegetation index with meteorological feature are higher than those of pure vegetation index models. Among them, the model based on two-stage vegetation indices and meteorological features performs best, with a classification accuracy exceeding 73.7%. The SVM algorithm performed best for wheat yellow rust monitoring among the three algorithms; its classification accuracy (84.2%) was ~10.5% and 5.3% greater than those of LDA and ANN, respectively. Combined with crop growth and environmental information, our model has great potential for monitoring wheat yellow rust at a regional scale. Future work will focus on regional-scale monitoring and forecasting of crop disease.},
DOI = {10.3390/rs13020278}
}



@Article{s21020570,
AUTHOR = {Biundini, Iago Z. and Pinto, Milena F. and Melo, Aurelio G. and Marcato, Andre L. M. and Honório, Leonardo M. and Aguiar, Maria J. R.},
TITLE = {A Framework for Coverage Path Planning Optimization Based on Point Cloud for Structural Inspection},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {570},
URL = {https://www.mdpi.com/1424-8220/21/2/570},
PubMedID = {33467417},
ISSN = {1424-8220},
ABSTRACT = {Different practical applications have emerged in the last few years, requiring periodic and detailed inspections to verify possible structural changes. Inspections using Unmanned Aerial Vehicles (UAVs) should minimize flight time due to battery time restrictions and identify the terrain&rsquo;s topographic features. In this sense, Coverage Path Planning (CPP) aims at finding the best path to coverage of a determined area respecting the operation&rsquo;s restrictions. Photometric information from the terrain is used to create routes or even refine paths already created. Therefore, this research&rsquo;s main contribution is developing a methodology that uses a metaheuristic algorithm based on point cloud data to inspect slope and dams structures. The technique was applied in a simulated and real scenario to verify its effectiveness. The results showed an increasing 3D reconstructions&rsquo; quality observing optimizing photometric and mission time criteria.},
DOI = {10.3390/s21020570}
}



@Article{s21020581,
AUTHOR = {Zhang, Xiaomin and Zhao, Zhiyao and Wang, Zhaoyang and Wang, Xiaoyi},
TITLE = {Fault Detection and Identification Method for Quadcopter Based on Airframe Vibration Signals},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {581},
URL = {https://www.mdpi.com/1424-8220/21/2/581},
PubMedID = {33467463},
ISSN = {1424-8220},
ABSTRACT = {Quadcopters are widely used in a variety of military and civilian mission scenarios. Real-time online detection of the abnormal state of the quadcopter is vital to the safety of aircraft. Existing data-driven fault detection methods generally usually require numerous sensors to collect data. However, quadcopter airframe space is limited. A large number of sensors cannot be loaded, meaning that it is difficult to use additional sensors to capture fault signals for quadcopters. In this paper, without additional sensors, a Fault Detection and Identification (FDI) method for quadcopter blades based on airframe vibration signals is proposed using the airborne acceleration sensor. This method integrates multi-axis data information and effectively detects and identifies quadcopter blade faults through Long and Short-Term Memory (LSTM) network models. Through flight experiments, the quadcopter triaxial accelerometer data are collected for airframe vibration signals at first. Then, the wavelet packet decomposition method is employed to extract data features, and the standard deviations of the wavelet packet coefficients are employed to form the feature vector. Finally, the LSTM-based FDI model is constructed for quadcopter blade FDI. The results show that the method can effectively detect and identify quadcopter blade faults with a better FDI performance and a higher model accuracy compared with the Back Propagation (BP) neural network-based FDI model.},
DOI = {10.3390/s21020581}
}



@Article{rs13020289,
AUTHOR = {Debella-Gilo, Misganu and Gjertsen, Arnt Kristian},
TITLE = {Mapping Seasonal Agricultural Land Use Types Using Deep Learning on Sentinel-2 Image Time Series},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {289},
URL = {https://www.mdpi.com/2072-4292/13/2/289},
ISSN = {2072-4292},
ABSTRACT = {The size and location of agricultural fields that are in active use and the type of use during the growing season are among the vital information that is needed for the careful planning and forecasting of agricultural production at national and regional scales. In areas where such data are not readily available, an independent seasonal monitoring method is needed. Remote sensing is a widely used tool to map land use types, although there are some limitations that can partly be circumvented by using, among others, multiple observations, careful feature selection and appropriate analysis methods. Here, we used Sentinel-2 satellite image time series (SITS) over the land area of Norway to map three agricultural land use classes: cereal crops, fodder crops (grass) and unused areas. The Multilayer Perceptron (MLP) and two variants of the Convolutional Neural Network (CNN), are implemented on SITS data of four different temporal resolutions. These enabled us to compare twelve model-dataset combinations to identify the model-dataset combination that results in the most accurate predictions. The CNN is implemented in the spectral and temporal dimensions instead of the conventional spatial dimension. Rather than using existing deep learning architectures, an autotuning procedure is implemented so that the model hyperparameters are empirically optimized during the training. The results obtained on held-out test data show that up to 94% overall accuracy and 90% Cohen&rsquo;s Kappa can be obtained when the 2D CNN is applied on the SITS data with a temporal resolution of 7 days. This is closely followed by the 1D CNN on the same dataset. However, the latter performs better than the former in predicting data outside the training set. It is further observed that cereal is predicted with the highest accuracy, followed by grass. Predicting the unused areas has been found to be difficult as there is no distinct surface condition that is common for all unused areas.},
DOI = {10.3390/rs13020289}
}



@Article{geomatics1010004,
AUTHOR = {Moreni, Mael and Theau, Jerome and Foucher, Samuel},
TITLE = {Train Fast While Reducing False Positives: Improving Animal Classification Performance Using Convolutional Neural Networks},
JOURNAL = {Geomatics},
VOLUME = {1},
YEAR = {2021},
NUMBER = {1},
PAGES = {34--49},
URL = {https://www.mdpi.com/2673-7418/1/1/4},
ISSN = {2673-7418},
ABSTRACT = {The combination of unmanned aerial vehicles (UAV) with deep learning models has the capacity to replace manned aircrafts for wildlife surveys. However, the scarcity of animals in the wild often leads to highly unbalanced, large datasets for which even a good detection method can return a large amount of false detections. Our objectives in this paper were to design a training method that would reduce training time, decrease the number of false positives and alleviate the fine-tuning effort of an image classifier in a context of animal surveys. We acquired two highly unbalanced datasets of deer images with a UAV and trained a Resnet-18 classifier using hard-negative mining and a series of recent techniques. Our method achieved sub-decimal false positive rates on two test sets (1 false positive per 19,162 and 213,312 negatives respectively), while training on small but relevant fractions of the data. The resulting training times were therefore significantly shorter than they would have been using the whole datasets. This high level of efficiency was achieved with little tuning effort and using simple techniques. We believe this parsimonious approach to dealing with highly unbalanced, large datasets could be particularly useful to projects with either limited resources or extremely large datasets.},
DOI = {10.3390/geomatics1010004}
}



@Article{rs13020296,
AUTHOR = {Jin, Xing and Tang, Ping and Houet, Thomas and Corpetti, Thomas and Alvarez-Vanhard, Emilien Gence and Zhang, Zheng},
TITLE = {Sequence Image Interpolation via Separable Convolution Network},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {296},
URL = {https://www.mdpi.com/2072-4292/13/2/296},
ISSN = {2072-4292},
ABSTRACT = {Remote-sensing time-series data are significant for global environmental change research and a better understanding of the Earth. However, remote-sensing acquisitions often provide sparse time series due to sensor resolution limitations and environmental factors, such as cloud noise for optical data. Image interpolation is the method that is often used to deal with this issue. This paper considers the deep learning method to learn the complex mapping of an interpolated intermediate image from predecessor and successor images, called separable convolution network for sequence image interpolation. The separable convolution network uses a separable 1D convolution kernel instead of 2D kernels to capture the spatial characteristics of input sequence images and then is trained end-to-end using sequence images. Our experiments, which were performed with unmanned aerial vehicle (UAV) and Landsat-8 datasets, show that the method is effective to produce high-quality time-series interpolated images, and the data-driven deep model can better simulate complex and diverse nonlinear image data information.},
DOI = {10.3390/rs13020296}
}



@Article{agronomy11010174,
AUTHOR = {Li, Haolu and Wang, Guojie and Dong, Zhen and Wei, Xikun and Wu, Mengjuan and Song, Huihui and Amankwah, Solomon Obiri Yeboah},
TITLE = {Identifying Cotton Fields from Remote Sensing Images Using Multiple Deep Learning Networks},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {174},
URL = {https://www.mdpi.com/2073-4395/11/1/174},
ISSN = {2073-4395},
ABSTRACT = {Remote sensing imageries processed through empirical and deterministic approaches help predict multiple agronomic traits throughout the growing season. Accurate identification of cotton crop from remotely sensed imageries is a significant task in precision agriculture. This study aims to utilize a deep learning-based framework for cotton crop field identification with Gaofen-1 (GF-1) high-resolution (16 m) imageries in Wei-Ku region, China. An optimized model for the pixel-wise multidimensional densely connected convolutional neural network (DenseNet) was used. Four widely-used classic convolutional neural networks (CNNs), including ResNet, VGG, SegNet, and DeepLab v3+, were also used for accuracy assessment. The results infer that DenseNet can identify cotton crop features within a relatively shorter time about 5 h for training convergence. The model performance was examined by multiple indicators (P, F1, R, and mIou) produced through the confusion matrix, and the derived cotton fields were then visualized. The DenseNet model has illustrated considerable improvements in comparison with the preceding mainstream models. The results showed that the retrieval precision was 0.948, F1 score was 0.953, and mIou was 0.911. Furthermore, its performance is relatively better in discriminating cotton crop fields&rsquo; fine structures when clouds, mountain shadows, and urban built up.},
DOI = {10.3390/agronomy11010174}
}



@Article{rs13020310,
AUTHOR = {Zou, Kunlin and Chen, Xin and Zhang, Fan and Zhou, Hang and Zhang, Chunlong},
TITLE = {A Field Weed Density Evaluation Method Based on UAV Imaging and Modified U-Net},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {310},
URL = {https://www.mdpi.com/2072-4292/13/2/310},
ISSN = {2072-4292},
ABSTRACT = {Weeds are one of the main factors affecting the yield and quality of agricultural products. Accurate evaluation of weed density is of great significance for field management, especially precision weeding. In this paper, a weed density calculating and mapping method in the field is proposed. An unmanned aerial vehicle (UAV) was used to capture field images. The excess green minus excess red index, combined with the minimum error threshold segmentation method, was used to segment green plants and bare land. A modified U-net was used to segment crops from images. After removing the bare land and crops from the field, images of weeds were obtained. The weed density was evaluated by the ratio of weed area to total area on the segmented image. The accuracy of the green plant segmentation was 93.5%. In terms of crop segmentation, the intersection over union (IoU) was 93.40%, and the segmentation time of a single image was 35.90 ms. Finally, the determination coefficient of the UAV evaluated weed density and the manually observed weed density was 0.94, and the root mean square error was 0.03. With the proposed method, the weed density of a field can be effectively evaluated from UAV images, hence providing critical information for precision weeding.},
DOI = {10.3390/rs13020310}
}



@Article{ijgi10010039,
AUTHOR = {Zhou, Kai and Xie, Yan and Gao, Zhan and Miao, Fang and Zhang, Lei},
TITLE = {FuNet: A Novel Road Extraction Network with Fusion of Location Data and Remote Sensing Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2220-9964/10/1/39},
ISSN = {2220-9964},
ABSTRACT = {Road semantic segmentation is unique and difficult. Road extraction from remote sensing imagery often produce fragmented road segments leading to road network disconnection due to the occlusion of trees, buildings, shadows, cloud, etc. In this paper, we propose a novel fusion network (FuNet) with fusion of remote sensing imagery and location data, which plays an important role of location data in road connectivity reasoning. A universal iteration reinforcement (IteR) module is embedded into FuNet to enhance the ability of network learning. We designed the IteR formula to repeatedly integrate original information and prediction information and designed the reinforcement loss function to control the accuracy of road prediction output. Another contribution of this paper is the use of histogram equalization data pre-processing to enhance image contrast and improve the accuracy by nearly 1%. We take the excellent D-LinkNet as the backbone network, designing experiments based on the open dataset. The experiment result shows that our method improves over the compared advanced road extraction methods, which not only increases the accuracy of road extraction, but also improves the road topological connectivity.},
DOI = {10.3390/ijgi10010039}
}



@Article{su13031017,
AUTHOR = {Zhu, Kuanxing and Xu, Peihua and Cao, Chen and Zheng, Lianjing and Liu, Yue and Dong, Xiujun},
TITLE = {Preliminary Identification of Geological Hazards from Songpinggou to Feihong in Mao County along the Minjiang River Using SBAS-InSAR Technique Integrated Multiple Spatial Analysis Methods},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1017},
URL = {https://www.mdpi.com/2071-1050/13/3/1017},
ISSN = {2071-1050},
ABSTRACT = {Landslides and collapses are common geological hazards in mountainous areas, posing significant threats to the lives and property of residents. Therefore, early identification of disasters is of great significance for disaster prevention. In this study, we used Small Baseline Subset Interferometric Synthetic Aperture Radar (SBAS-InSAR) technology to process C-band Sentinel-1A images to monitor the surface deformation from Songpinggou to Feihong in Maoxian County, Sichuan Province. Visibility analysis was used to remove the influence of geometric distortion on the SAR images and retain deformation information in the visible area. Hot spot and kernel density analyses were performed on the deformation data, and 18 deformation clusters were obtained. Velocity and slope data were integrated, and 26 disaster areas were interpreted from the 18 deformation clusters, including 20 potential landslides and 6 potential collapses. A detailed field investigation indicated that potential landslides No. 6 and No. 8 had developed cracks and were severely damaged, with a high probability of occurrence. Potential collapse No. 22 had developed fissures, exposing a dangerous rock mass and posing significant threats to the lives and property of residents. This study shows that the proposed method that combines visibility analysis, InSAR deformation rates, and spatial analysis can quickly and accurately identify potential geological disasters and provide guidance for local disaster prevention and mitigation.},
DOI = {10.3390/su13031017}
}



@Article{electronics10030222,
AUTHOR = {Zhao, Baigan and Huang, Yingping and Wei, Hongjian and Hu, Xing},
TITLE = {Ego-Motion Estimation Using Recurrent Convolutional Neural Networks through Optical Flow Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {222},
URL = {https://www.mdpi.com/2079-9292/10/3/222},
ISSN = {2079-9292},
ABSTRACT = {Visual odometry (VO) refers to incremental estimation of the motion state of an agent (e.g., vehicle and robot) by using image information, and is a key component of modern localization and navigation systems. Addressing the monocular VO problem, this paper presents a novel end-to-end network for estimation of camera ego-motion. The network learns the latent subspace of optical flow (OF) and models sequential dynamics so that the motion estimation is constrained by the relations between sequential images. We compute the OF field of consecutive images and extract the latent OF representation in a self-encoding manner. A Recurrent Neural Network is then followed to examine the OF changes, i.e., to conduct sequential learning. The extracted sequential OF subspace is used to compute the regression of the 6-dimensional pose vector. We derive three models with different network structures and different training schemes: LS-CNN-VO, LS-AE-VO, and LS-RCNN-VO. Particularly, we separately train the encoder in an unsupervised manner. By this means, we avoid non-convergence during the training of the whole network and allow more generalized and effective feature representation. Substantial experiments have been conducted on KITTI and Malaga datasets, and the results demonstrate that our LS-RCNN-VO outperforms the existing learning-based VO approaches.},
DOI = {10.3390/electronics10030222}
}



@Article{s21030694,
AUTHOR = {Sahal, Radhya and Alsamhi, Saeed H. and Breslin, John G. and Ali, Muhammad Intizar},
TITLE = {Industry 4.0 towards Forestry 4.0: Fire Detection Use Case},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {694},
URL = {https://www.mdpi.com/1424-8220/21/3/694},
PubMedID = {33498450},
ISSN = {1424-8220},
ABSTRACT = {Forestry 4.0 is inspired by the Industry 4.0 concept, which plays a vital role in the next industrial generation revolution. It is ushering in a new era for efficient and sustainable forest management. Environmental sustainability and climate change are related challenges to promote sustainable forest management of natural resources. Internet of Forest Things (IoFT) is an emerging technology that helps manage forest sustainability and protect forest from hazards via distributing smart devices for gathering data stream during monitoring and detecting fire. Stream processing is a well-known research area, and recently, it has gained a further significance due to the emergence of IoFT devices. Distributed stream processing platforms have emerged, e.g., Apache Flink, Storm, and Spark, etc. Querying windowing is the heart of any stream-processing platform which splits infinite data stream into chunks of finite data to execute a query. Dynamic query window-based processing can reduce the reporting time in case of missing and delayed events caused by data drift.In this paper, we present a novel dynamic mechanism to recommend the optimal window size and type based on the dynamic context of IoFT application. In particular, we designed a dynamic window selector for stream queries considering input stream data characteristics, application workload and resource constraints to recommend the optimal stream query window configuration. A research gap on the likelihood of adopting smart IoFT devices in environmental sustainability indicates a lack of empirical studies to pursue forest sustainability, i.e., sustainable forestry applications. So, we focus on forest fire management and detection as a use case of Forestry 4.0, one of the dynamic environmental management challenges, i.e., climate change, to deliver sustainable forestry goals. According to the dynamic window selector&rsquo;s experimental results, end-to-end latency time for the reported fire alerts has been reduced by dynamical adaptation of window size with IoFT stream rate changes.},
DOI = {10.3390/s21030694}
}



@Article{app11030953,
AUTHOR = {Hong, Jin and Kwon, Junseok},
TITLE = {Visual Tracking of Small Unmanned Aerial Vehicles Based on Object Proposal Voting},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {953},
URL = {https://www.mdpi.com/2076-3417/11/3/953},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we propose a novel visual tracking method for unmanned aerial vehicles (UAVs) in aerial scenery. To track the UAVs robustly, we present a new object proposal method that can accurately determine the object regions that are likely to exist. The proposed object proposal method is robust to small objects and severe background clutter. For this, we vote on candidate areas of the object and increase or decrease the weight of the area accordingly. Thus, the method can accurately propose the object areas that can be used to track small-sized UAVs with the assumption that their motion is smooth over time. Experimental results verify that UAVs are accurately tracked even when they are very small and the background is complex. The proposed method qualitatively and quantitatively delivers state-of-the-art performance in comparison with conventional object proposal-based methods.},
DOI = {10.3390/app11030953}
}



@Article{drones5010008,
AUTHOR = {Butcher, Paul A. and Colefax, Andrew P. and Gorkin, Robert A. and Kajiura, Stephen M. and López, Naima A. and Mourier, Johann and Purcell, Cormac R. and Skomal, Gregory B. and Tucker, James P. and Walsh, Andrew J. and Williamson, Jane E. and Raoult, Vincent},
TITLE = {The Drone Revolution of Shark Science: A Review},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {8},
URL = {https://www.mdpi.com/2504-446X/5/1/8},
ISSN = {2504-446X},
ABSTRACT = {Over the past decade, drones have become a popular tool for wildlife management and research. Drones have shown significant value for animals that were often difficult or dangerous to study using traditional survey methods. In the past five years drone technology has become commonplace for shark research with their use above, and more recently, below the water helping to minimise knowledge gaps about these cryptic species. Drones have enhanced our understanding of shark behaviour and are critically important tools, not only due to the importance and conservation of the animals in the ecosystem, but to also help minimise dangerous encounters with humans. To provide some guidance for their future use in relation to sharks, this review provides an overview of how drones are currently used with critical context for shark monitoring. We show how drones have been used to fill knowledge gaps around fundamental shark behaviours or movements, social interactions, and predation across multiple species and scenarios. We further detail the advancement in technology across sensors, automation, and artificial intelligence that are improving our abilities in data collection and analysis and opening opportunities for shark-related beach safety. An investigation of the shark-based research potential for underwater drones (ROV/AUV) is also provided. Finally, this review provides baseline observations that have been pioneered for shark research and recommendations for how drones might be used to enhance our knowledge in the future.},
DOI = {10.3390/drones5010008}
}



@Article{s21030742,
AUTHOR = {Nguyen, Canh and Sagan, Vasit and Maimaitiyiming, Matthew and Maimaitijiang, Maitiniyazi and Bhadra, Sourav and Kwasniewski, Misha T.},
TITLE = {Early Detection of Plant Viral Disease Using Hyperspectral Imaging and Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {742},
URL = {https://www.mdpi.com/1424-8220/21/3/742},
PubMedID = {33499335},
ISSN = {1424-8220},
ABSTRACT = {Early detection of grapevine viral diseases is critical for early interventions in order to prevent the disease from spreading to the entire vineyard. Hyperspectral remote sensing can potentially detect and quantify viral diseases in a nondestructive manner. This study utilized hyperspectral imagery at the plant level to identify and classify grapevines inoculated with the newly discovered DNA virus grapevine vein-clearing virus (GVCV) at the early asymptomatic stages. An experiment was set up at a test site at South Farm Research Center, Columbia, MO, USA (38.92 N, &minus;92.28 W), with two grapevine groups, namely healthy and GVCV-infected, while other conditions were controlled. Images of each vine were captured by a SPECIM IQ 400&ndash;1000 nm hyperspectral sensor (Oulu, Finland). Hyperspectral images were calibrated and preprocessed to retain only grapevine pixels. A statistical approach was employed to discriminate two reflectance spectra patterns between healthy and GVCV vines. Disease-centric vegetation indices (VIs) were established and explored in terms of their importance to the classification power. Pixel-wise (spectral features) classification was performed in parallel with image-wise (joint spatial&ndash;spectral features) classification within a framework involving deep learning architectures and traditional machine learning. The results showed that: (1) the discriminative wavelength regions included the 900&ndash;940 nm range in the near-infrared (NIR) region in vines 30 days after sowing (DAS) and the entire visual (VIS) region of 400&ndash;700 nm in vines 90 DAS; (2) the normalized pheophytization index (NPQI), fluorescence ratio index 1 (FRI1), plant senescence reflectance index (PSRI), anthocyanin index (AntGitelson), and water stress and canopy temperature (WSCT) measures were the most discriminative indices; (3) the support vector machine (SVM) was effective in VI-wise classification with smaller feature spaces, while the RF classifier performed better in pixel-wise and image-wise classification with larger feature spaces; and (4) the automated 3D convolutional neural network (3D-CNN) feature extractor provided promising results over the 2D convolutional neural network (2D-CNN) in learning features from hyperspectral data cubes with a limited number of samples.},
DOI = {10.3390/s21030742}
}



@Article{s21030750,
AUTHOR = {Garrido, Iván and Erazo-Aux, Jorge and Lagüela, Susana and Sfarra, Stefano and Ibarra-Castanedo, Clemente and Pivarčiová, Elena and Gargiulo, Gianfranco and Maldague, Xavier and Arias, Pedro},
TITLE = {Introduction of Deep Learning in Thermographic Monitoring of Cultural Heritage and Improvement by Automatic Thermogram Pre-Processing Algorithms},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {750},
URL = {https://www.mdpi.com/1424-8220/21/3/750},
PubMedID = {33499344},
ISSN = {1424-8220},
ABSTRACT = {The monitoring of heritage objects is necessary due to their continuous deterioration over time. Therefore, the joint use of the most up-to-date inspection techniques with the most innovative data processing algorithms plays an important role to apply the required prevention and conservation tasks in each case study. InfraRed Thermography (IRT) is one of the most used Non-Destructive Testing (NDT) techniques in the cultural heritage field due to its advantages in the analysis of delicate objects (i.e., undisturbed, non-contact and fast inspection of large surfaces) and its continuous evolution in both the acquisition and the processing of the data acquired. Despite the good qualitative and quantitative results obtained so far, the lack of automation in the IRT data interpretation predominates, with few automatic analyses that are limited to specific conditions and the technology of the thermographic camera. Deep Learning (DL) is a data processor with a versatile solution for highly automated analysis. Then, this paper introduces the latest state-of-the-art DL model for instance segmentation, Mask Region-Convolution Neural Network (Mask R-CNN), for the automatic detection and segmentation of the position and area of different surface and subsurface defects, respectively, in two different artistic objects belonging to the same family: Marquetry. For that, active IRT experiments are applied to each marquetry. The thermal image sequences acquired are used as input dataset in the Mask R-CNN learning process. Previously, two automatic thermal image pre-processing algorithms based on thermal fundamentals are applied to the acquired data in order to improve the contrast between defective and sound areas. Good detection and segmentation results are obtained regarding state-of-the-art IRT data processing algorithms, which experience difficulty in identifying the deepest defects in the tests. In addition, the performance of the Mask R-CNN is improved by the prior application of the proposed pre-processing algorithms.},
DOI = {10.3390/s21030750}
}



@Article{rs13030396,
AUTHOR = {Fernández, Claudio Ignacio and Haddadi, Ata and Leblon, Brigitte and Wang, Jinfei and Wang, Keri},
TITLE = {Comparison between Three Registration Methods in the Case of Non-Georeferenced Close Range of Multispectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/2072-4292/13/3/396},
ISSN = {2072-4292},
ABSTRACT = {Cucumber powdery mildew, which is caused by Podosphaera xanthii, is a major disease that has a significant economic impact in cucumber greenhouse production. It is necessary to develop a non-invasive fast detection system for that disease. Such a system will use multispectral imagery acquired at a close range with a camera attached to a mobile cart&rsquo;s mechanic extension. This study evaluated three image registration methods applied to non-georeferenced multispectral images acquired at close range over greenhouse cucumber plants with a MicaSense&reg; RedEdge camera. The detection of matching points was performed using Speeded-Up Robust Features (SURF), and outliers matching points were removed using the M-estimator Sample Consensus (MSAC) algorithm. Three geometric transformations (affine, similarity, and projective) were considered in the registration process. For each transformation, we mapped the matching points of the blue, green, red, and NIR band images into the red-edge band space and computed the root mean square error (RMSE in pixel) to estimate the accuracy of each image registration. Our results achieved an RMSE of less than 1 pixel with the similarity and affine transformations and of less than 2 pixels with the projective transformation, whatever the band image. We determined that the best image registration method corresponded to the affine transformation because the RMSE is less than 1 pixel and the RMSEs have a Gaussian distribution for all of the bands, but the blue band.},
DOI = {10.3390/rs13030396}
}



@Article{f12020131,
AUTHOR = {Chen, Xinxin and Jiang, Kang and Zhu, Yushi and Wang, Xiangjun and Yun, Ting},
TITLE = {Individual Tree Crown Segmentation Directly from UAV-Borne LiDAR Data Using the PointNet of Deep Learning},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {131},
URL = {https://www.mdpi.com/1999-4907/12/2/131},
ISSN = {1999-4907},
ABSTRACT = {Accurate individual tree crown (ITC) segmentation from scanned point clouds is a fundamental task in forest biomass monitoring and forest ecology management. Light detection and ranging (LiDAR) as a mainstream tool for forest survey is advancing the pattern of forest data acquisition. In this study, we performed a novel deep learning framework directly processing the forest point clouds belonging to the four forest types (i.e., the nursery base, the monastery garden, the mixed forest, and the defoliated forest) to realize the ITC segmentation. The specific steps of our approach were as follows: first, a voxelization strategy was conducted to subdivide the collected point clouds with various tree species from various forest types into many voxels. These voxels containing point clouds were taken as training samples for the PointNet deep learning framework to identify the tree crowns at the voxel scale. Second, based on the initial segmentation results, we used the height-related gradient information to accurately depict the boundaries of each tree crown. Meanwhile, the retrieved tree crown breadths of individual trees were compared with field measurements to verify the effectiveness of our approach. Among the four forest types, our results revealed the best performance for the nursery base (tree crown detection rate r = 0.90; crown breadth estimation R2 &gt; 0.94 and root mean squared error (RMSE) &lt; 0.2m). A sound performance was also achieved for the monastery garden and mixed forest, which had complex forest structures, complicated intersections of branches and different building types, with r = 0.85, R2 &gt; 0.88 and RMSE &lt; 0.6 m for the monastery garden and r = 0.80, R2 &gt; 0.85 and RMSE &lt; 0.8 m for the mixed forest. For the fourth forest plot type with the distribution of crown defoliation across the woodland, we achieved the performance with r = 0.82, R2 &gt; 0.79 and RMSE &lt; 0.7 m. Our method presents a robust framework inspired by the deep learning technology and computer graphics theory that solves the ITC segmentation problem and retrieves forest parameters under various forest conditions.},
DOI = {10.3390/f12020131}
}



@Article{jlpea11010007,
AUTHOR = {Sahoo, Siva Satyendra and Ranjbar, Behnaz and Kumar, Akash},
TITLE = {Reliability-Aware Resource Management in Multi-/Many-Core Systems: A Perspective Paper},
JOURNAL = {Journal of Low Power Electronics and Applications},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {7},
URL = {https://www.mdpi.com/2079-9268/11/1/7},
ISSN = {2079-9268},
ABSTRACT = {With the advancement of technology scaling, multi/many-core platforms are getting more attention in embedded systems due to the ever-increasing performance requirements and power efficiency. This feature size scaling, along with architectural innovations, has dramatically exacerbated the rate of manufacturing defects and physical fault-rates. As a result, in addition to providing high parallelism, such hardware platforms have introduced increasing unreliability into the system. Such systems need to be well designed to ensure long-term and application-specific reliability, especially in mixed-criticality systems, where incorrect execution of applications may cause catastrophic consequences. However, the optimal allocation of applications/tasks on multi/many-core platforms is an increasingly complex problem. Therefore, reliability-aware resource management is crucial while ensuring the application-specific Quality-of-Service (QoS) requirements and optimizing other system-level performance goals. This article presents a survey of recent works that focus on reliability-aware resource management in multi-/many-core systems. We first present an overview of reliability in electronic systems, associated fault models and the various system models used in related research. Then, we present recent published articles primarily focusing on aspects such as application-specific reliability optimization, mixed-criticality awareness, and hardware resource heterogeneity. To underscore the techniques’ differences, we classify them based on the design space exploration. In the end, we briefly discuss the upcoming trends and open challenges within the domain of reliability-aware resource management for future research.},
DOI = {10.3390/jlpea11010007}
}



@Article{info12020051,
AUTHOR = {Godio, Simone and Primatesta, Stefano and Guglieri, Giorgio and Dovis, Fabio},
TITLE = {A Bioinspired Neural Network-Based Approach for Cooperative Coverage Planning of UAVs},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {51},
URL = {https://www.mdpi.com/2078-2489/12/2/51},
ISSN = {2078-2489},
ABSTRACT = {This paper describes a bioinspired neural-network-based approach to solve a coverage planning problem for a fleet of unmanned aerial vehicles exploring critical areas. The main goal is to fully cover the map, maintaining a uniform distribution of the fleet on the map, and avoiding collisions between vehicles and other obstacles. This specific task is suitable for surveillance applications, where the uniform distribution of the fleet in the map permits them to reach any position on the map as fast as possible in emergency scenarios. To solve this problem, a bioinspired neural network structure is adopted. Specifically, the neural network consists of a grid of neurons, where each neuron has a local cost and has a local connection only with neighbor neurons. The cost of each neuron influences the cost of its neighbors, generating an attractive contribution to unvisited neurons. We introduce several controls and precautions to minimize the risk of collisions and optimize coverage planning. Then, preliminary simulations are performed in different scenarios by testing the algorithm in four maps and with fleets consisting of 3 to 10 vehicles. Results confirm the ability of the proposed approach to manage and coordinate the fleet providing the full coverage of the map in every tested scenario, avoiding collisions between vehicles, and uniformly distributing the fleet on the map.},
DOI = {10.3390/info12020051}
}



@Article{f12020147,
AUTHOR = {Li, Hao and Shi, Qingdong and Wan, Yanbo and Shi, Haobo and Imin, Bilal},
TITLE = {Using Sentinel-2 Images to Map the Populus euphratica Distribution Based on the Spectral Difference Acquired at the Key Phenological Stage},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {147},
URL = {https://www.mdpi.com/1999-4907/12/2/147},
ISSN = {1999-4907},
ABSTRACT = {Populus euphratica is an important tree species in desert ecosystems. The protection and restoration of natural Populus euphratica forests requires accurate positioning information. The use of Sentinel-2 images to map the Populus euphratica distribution at a large scale faces challenges associated with discriminating between Populus euphratica and Tamarix chinensis. To address this problem, this study selected the Daliyabuyi Oasis in the hinterland of the Taklimakan Desert as the study site and sought to distinguish Populus euphratica from Tamarix chinensis. First, we determined the peak spectral difference period (optimal time window) between Populus euphratica and Tamarix chinensis within monthly Sentinel-2 time-series images. Then, an appropriate vegetation index was selected to represent the spectral difference between Populus euphratica and Tamarix chinensis within the key phenological stage. Finally, the maximum entropy method was used to automatically determine the threshold to map the Populus euphratica distribution. The results indicated that the period from 22 April to 1 May was the optimal time window for mapping the Populus euphratica distribution in the Daliyabuyi Oasis. The combination of the inverted red-edge chlorophyll index (IRECI) and the maximum entropy method can effectively distinguish Populus euphratica from Tamarix chinensis. The user&rsquo;s accuracy of the Populus euphratica distribution extraction from single-data Sentinel-2 images acquired within the optimal time window was 0.83, the producer&rsquo;s accuracy was 0.72, and the F1-score was 0.77. This study verified the feasibility of mapping Populus euphratica distribution based on Sentinel-2 images, and analyzed the validity of exploiting spectral differences within the key phenological stage from a single-data image to distinguish between the two species. The results can be used to extract the distribution of Populus euphratica and serve as an auxiliary variable for other plant classification methods, providing a reference for the extraction and classification of desert plants.},
DOI = {10.3390/f12020147}
}



@Article{rs13030440,
AUTHOR = {Zhang, Haiming and Wang, Mingchang and Wang, Fengyan and Yang, Guodong and Zhang, Ying and Jia, Junqian and Wang, Siqi},
TITLE = {A Novel Squeeze-and-Excitation W-Net for 2D and 3D Building Change Detection with Multi-Source and Multi-Feature Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {440},
URL = {https://www.mdpi.com/2072-4292/13/3/440},
ISSN = {2072-4292},
ABSTRACT = {Building Change Detection (BCD) is one of the core issues in earth observation and has received extensive attention in recent years. With the rapid development of earth observation technology, the data source of remote sensing change detection is continuously enriched, which provides the possibility to describe the spatial details of the ground objects more finely and to characterize the ground objects with multiple perspectives and levels. However, due to the different physical mechanisms of multi-source remote sensing data, BCD based on heterogeneous data is a challenge. Previous studies mostly focused on the BCD of homogeneous remote sensing data, while the use of multi-source remote sensing data and considering multiple features to conduct 2D and 3D BCD research is sporadic. In this article, we propose a novel and general squeeze-and-excitation W-Net, which is developed from U-Net and SE-Net. Its unique advantage is that it can not only be used for BCD of homogeneous and heterogeneous remote sensing data respectively but also can input both homogeneous and heterogeneous remote sensing data for 2D or 3D BCD by relying on its bidirectional symmetric end-to-end network architecture. Moreover, from a unique perspective, we use image features that are stable in performance and less affected by radiation differences and temporal changes. We innovatively introduced the squeeze-and-excitation module to explicitly model the interdependence between feature channels so that the response between the feature channels is adaptively recalibrated to improve the information mining ability and detection accuracy of the model. As far as we know, this is the first proposed network architecture that can simultaneously use multi-source and multi-feature remote sensing data for 2D and 3D BCD. The experimental results in two 2D data sets and two challenging 3D data sets demonstrate that the promising performances of the squeeze-and-excitation W-Net outperform several traditional and state-of-the-art approaches. Moreover, both visual and quantitative analyses of the experimental results demonstrate competitive performance in the proposed network. This demonstrates that the proposed network and method are practical, physically justified, and have great potential application value in large-scale 2D and 3D BCD and qualitative and quantitative research.},
DOI = {10.3390/rs13030440}
}



@Article{rs13030439,
AUTHOR = {Avtar, Ram and Kouser, Asma and Kumar, Ashwani and Singh, Deepak and Misra, Prakhar and Gupta, Ankita and Yunus, Ali P. and Kumar, Pankaj and Johnson, Brian Alan and Dasgupta, Rajarshi and Sahu, Netrananda and Besse Rimba, Andi},
TITLE = {Remote Sensing for International Peace and Security: Its Role and Implications},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {439},
URL = {https://www.mdpi.com/2072-4292/13/3/439},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing technology has seen a massive rise in popularity over the last two decades, becoming an integral part of our lives. Space-based satellite technologies facilitated access to the inaccessible terrains, helped humanitarian teams, support complex emergencies, and contributed to monitoring and verifying conflict zones. The scoping phase of this review investigated the utility of the role of remote sensing application to complement international peace and security activities owing to their ability to provide objective near real-time insights at the ground level. The first part of this review looks into the major research concepts and implementation of remote sensing-based techniques for international peace and security applications and presented a meta-analysis on how advanced sensor capabilities can support various aspects of peace and security. With key examples, we demonstrated how this technology assemblage enacts multiple versions of peace and security: for refugee relief operations, in armed conflicts monitoring, tracking acts of genocide, providing evidence in courts of law, and assessing contravention in human rights. The second part of this review anticipates future challenges that can hinder the applicative capabilities of remote sensing in peace and security. Varying types of sensors pose discrepancies in image classifications and issues like cost, resolution, and difficulty of ground-truth in conflict areas. With emerging technologies and sufficient secondary resources available, remote sensing plays a vital operational tool in conflict-affected areas by supporting an extensive diversity in public policy actions for peacekeeping processes.},
DOI = {10.3390/rs13030439}
}



@Article{rs13030441,
AUTHOR = {Fu, Han and Fu, Bihong and Shi, Pilong},
TITLE = {An Improved Segmentation Method for Automatic Mapping of Cone Karst from Remote Sensing Data Based on DeepLab V3+ Model},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {441},
URL = {https://www.mdpi.com/2072-4292/13/3/441},
ISSN = {2072-4292},
ABSTRACT = {The South China Karst, a United Nations Educational, Scientific and Cultural Organization (UNESCO) natural heritage site, is one of the world&rsquo;s most spectacular examples of humid tropical to subtropical karst landscapes. The Libo cone karst in the southern Guizhou Province is considered as the world reference site for these types of karst, forming a distinctive and beautiful landscape. Geomorphic information and spatial distribution of cone karst is essential for conservation and management for Libo heritage site. In this study, a deep learning (DL) method based on DeepLab V3+ network was proposed to document the cone karst landscape in Libo by multi-source data, including optical remote sensing images and digital elevation model (DEM) data. The training samples were generated by using Landsat remote sensing images and their combination with satellite derived DEM data. Each group of training dataset contains 898 samples. The input module of DeepLab V3+ network was improved to accept four-channel input data, i.e., combination of Landsat RGB images and DEM data. Our results suggest that the mean intersection over union (MIoU) using the four-channel data as training samples by a new DL-based pixel-level image segmentation approach is the highest, which can reach 95.5%. The proposed method can accomplish automatic extraction of cone karst landscape by self-learning of deep neural network, and therefore it can also provide a powerful and automatic tool for documenting other type of geological landscapes worldwide.},
DOI = {10.3390/rs13030441}
}



@Article{jsan10010007,
AUTHOR = {Benbarrad, Tajeddine and Salhaoui, Marouane and Kenitar, Soukaina Bakhat and Arioua, Mounir},
TITLE = {Intelligent Machine Vision Model for Defective Product Inspection Based on Machine Learning},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {7},
URL = {https://www.mdpi.com/2224-2708/10/1/7},
ISSN = {2224-2708},
ABSTRACT = {Quality control is one of the industrial tasks most susceptible to be improved by implementing technological innovations. As an innovative technology, machine vision enables reliable and fast 24/7 inspections and helps producers to improve the efficiency of manufacturing operations. The accessible data by vision equipment will be used to identify and report defective products, understand the causes of deficiencies and allow rapid and efficient intervention in smart factories. From this perspective, the proposed machine vision model in this paper combines the identification of defective products and the continuous improvement of manufacturing processes by predicting the most suitable parameters of production processes to obtain a defect-free item. The suggested model exploits all generated data by various integrated technologies in the manufacturing chain, thus meeting the requirements of quality management in the context of Industry 4.0, based on predictive analysis to identify patterns in data and suggest corrective actions to ensure product quality. In addition, a comparative study between several machine learning algorithms, both for product classification and process improvement models, is performed in order to evaluate the designed system. The results of this study show that the proposed model largely meets the requirements for the proper implementation of these techniques.},
DOI = {10.3390/jsan10010007}
}



@Article{drones5010010,
AUTHOR = {Guirado, Robert and Padró, Joan-Cristian and Zoroa, Albert and Olivert, José and Bukva, Anica and Cavestany, Pedro},
TITLE = {StratoTrans: Unmanned Aerial System (UAS) 4G Communication Framework Applied on the Monitoring of Road Traffic and Linear Infrastructure},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {10},
URL = {https://www.mdpi.com/2504-446X/5/1/10},
ISSN = {2504-446X},
ABSTRACT = {This study provides an operational solution to directly connect drones to internet by means of 4G telecommunications and exploit drone acquired data, including telemetry and imagery but focusing on video transmission. The novelty of this work is the application of 4G connection to link the drone directly to a data server where video (in this case to monitor road traffic) and imagery (in the case of linear infrastructures) are processed. However, this framework is appliable to any other monitoring purpose where the goal is to send real-time video or imagery to the headquarters where the drone data is processed, analyzed, and exploited. We describe a general framework and analyze some key points, such as the hardware to use, the data stream, and the network coverage, but also the complete resulting implementation of the applied unmanned aerial system (UAS) communication system through a Virtual Private Network (VPN) featuring a long-range telemetry high-capacity video link (up to 15 Mbps, 720 p video at 30 fps with 250 ms of latency). The application results in the real-time exploitation of the video, obtaining key information for traffic managers such as vehicle tracking, vehicle classification, speed estimation, and roundabout in-out matrices. The imagery downloads and storage is also performed thorough internet, although the Structure from Motion postprocessing is not real-time due to photogrammetric workflows. In conclusion, we describe a real-case application of drone connection to internet thorough 4G network, but it can be adapted to other applications. Although 5G will -in time- surpass 4G capacities, the described framework can enhance drone performance and facilitate paths for upgrading the connection of on-board devices to the 5G network.},
DOI = {10.3390/drones5010010}
}



@Article{rs13030457,
AUTHOR = {Zhou, Xixuan and Yang, Liao and Wang, Weisheng and Chen, Baili},
TITLE = {UAV Data as an Alternative to Field Sampling to Monitor Vineyards Using Machine Learning Based on UAV/Sentinel-2 Data Fusion},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {457},
URL = {https://www.mdpi.com/2072-4292/13/3/457},
ISSN = {2072-4292},
ABSTRACT = {Pests and diseases affect the yield and quality of grapes directly and engender noteworthy economic losses. Diagnosing &ldquo;lesions&rdquo; on vines as soon as possible and dynamically monitoring symptoms caused by pests and diseases at a larger scale are essential to pest control. This study has appraised the capabilities of high-resolution unmanned aerial vehicle (UAV) data as an alternative to manual field sampling to obtain sampling canopy sets and to supplement satellite-based monitoring using machine learning models including partial least squared regression (PLSR), support vector regression (SVR), random forest regression (RFR), and extreme learning regression (ELR) with a new activation function. UAV data were acquired from two flights in Turpan to determine disease severity (DS) and disease incidence (DI) and compared with field visual assessments. The UAV-derived canopy structure including canopy height (CH) and vegetation fraction cover (VFC), as well as satellite-based spectral features calculated from Sentinel-2A/B data were analyzed to evaluate the potential of UAV data to replace manual sampling data and predict DI. It was found that SVR slightly outperformed the other methods with a root mean square error (RMSE) of 1.89%. Moreover, the combination of canopy structure (CS) and vegetation index (VIs) improved prediction accuracy compared with single-type features (RMSEcs of 2.86% and RMSEVIs of 1.93%). This study tested the ability of UAV sampling to replace manual sampling on a large scale and introduced opportunities and challenges of fusing different features to monitor vineyards using machine learning. Within this framework, disease incidence can be estimated efficiently and accurately for larger area monitoring operation.},
DOI = {10.3390/rs13030457}
}



@Article{s21030877,
AUTHOR = {Liu, Jian and Xu, Youshuan and Li, Henghui and Guo, Jiao},
TITLE = {Soil Moisture Retrieval in Farmland Areas with Sentinel Multi-Source Data Based on Regression Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {877},
URL = {https://www.mdpi.com/1424-8220/21/3/877},
PubMedID = {33525486},
ISSN = {1424-8220},
ABSTRACT = {As an important component of the earth ecosystem, soil moisture monitoring is of great significance in the fields of crop growth monitoring, crop yield estimation, variable irrigation, and other related applications. In order to mitigate or eliminate the impacts of sparse vegetation covers in farmland areas, this study combines multi-source remote sensing data from Sentinel-1 radar and Sentinel-2 optical satellites to quantitatively retrieve soil moisture content. Firstly, a traditional Oh model was applied to estimate soil moisture content after removing vegetation influence by a water cloud model. Secondly, support vector regression (SVR) and generalized regression neural network (GRNN) models were used to establish the relationships between various remote sensing features and real soil moisture. Finally, a regression convolutional neural network (CNNR) model is constructed to extract deep-level features of remote sensing data to increase soil moisture retrieval accuracy. In addition, polarimetric decomposition features for real Sentinel-1 PolSAR data are also included in the construction of inversion models. Based on the established soil moisture retrieval models, this study analyzes the influence of each input feature on the inversion accuracy in detail. The experimental results show that the optimal combination of R2 and root mean square error (RMSE) for SVR is 0.7619 and 0.0257 cm3/cm3, respectively. The optimal combination of R2 and RMSE for GRNN is 0.7098 and 0.0264 cm3/cm3, respectively. Especially, the CNNR model with optimal feature combination can generate inversion results with the highest accuracy, whose R2 and RMSE reach up to 0.8947 and 0.0208 cm3/cm3, respectively. Compared to other methods, the proposed algorithm improves the accuracy of soil moisture retrieval from synthetic aperture radar (SAR) and optical data. Furthermore, after adding polarization decomposition features, the R2 of CNNR is raised by 0.1524 and the RMSE of CNNR decreased by 0.0019 cm3/cm3 on average, which means that the addition of polarimetric decomposition features effectively improves the accuracy of soil moisture retrieval results.},
DOI = {10.3390/s21030877}
}



@Article{rs13030461,
AUTHOR = {Croce, Valeria and Caroti, Gabriella and De Luca, Livio and Jacquot, Kévin and Piemonte, Andrea and Véron, Philippe},
TITLE = {From the Semantic Point Cloud to Heritage-Building Information Modeling: A Semiautomatic Approach Exploiting Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {461},
URL = {https://www.mdpi.com/2072-4292/13/3/461},
ISSN = {2072-4292},
ABSTRACT = {This work presents a semi-automatic approach to the 3D reconstruction of Heritage-Building Information Models from point clouds based on machine learning techniques. The use of digital information systems leveraging on three-dimensional (3D) representations in architectural heritage documentation and analysis is ever increasing. For the creation of such repositories, reality-based surveying techniques, such as photogrammetry and laser scanning, allow the fast collection of reliable digital replicas of the study objects in the form of point clouds. Besides, their output is raw and unstructured, and the transition to intelligible and semantic 3D representations is still a scarcely automated and time-consuming process requiring considerable human intervention. More refined methods for 3D data interpretation of heritage point clouds are therefore sought after. In tackling these issues, the proposed approach relies on (i) the application of machine learning techniques to semantically label 3D heritage data by identification of relevant geometric, radiometric and intensity features, and (ii) the use of the annotated data to streamline the construction of Heritage-Building Information Modeling (H-BIM) systems, where purely geometric information derived from surveying is associated with semantic descriptors on heritage documentation and management. The &ldquo;Grand-Ducal Cloister&rdquo; dataset, related to the emblematic case study of the Pisa Charterhouse, is discussed.},
DOI = {10.3390/rs13030461}
}



@Article{agronomy11020249,
AUTHOR = {Frossard, Emmanuel and Liebisch, Frank and Hgaza, Valérie Kouamé and Kiba, Delwendé Innocent and Kirchgessner, Norbert and Müller, Laurin and Müller, Patrick and Pouya, Nestor and Ringger, Cecil and Walter, Achim},
TITLE = {Evaluation of Image-Based Phenotyping Methods for Measuring Water Yam (Dioscorea alata L.) Growth and Nitrogen Nutritional Status under Greenhouse and Field Conditions},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {249},
URL = {https://www.mdpi.com/2073-4395/11/2/249},
ISSN = {2073-4395},
ABSTRACT = {New management practices must be developed to improve yam productivity. By allowing non-destructive analyses of important plant traits, image-based phenotyping techniques could help developing such practices. Our objective was to determine the potential of image-based phenotyping methods to assess traits relevant for tuber yield formation in yam grown in the glasshouse and in the field. We took plant and leaf pictures with consumer cameras. We used the numbers of image pixels to derive the shoot biomass and the total leaf surface and calculated the ‘triangular greenness index’ (TGI) which is an indicator of the leaf chlorophyll content. Under glasshouse conditions, the number of pixels obtained from nadir view (view from the top) was positively correlated to shoot biomass, and total leaf surface, while the TGI was negatively correlated to the SPAD values and nitrogen (N) content of diagnostic leaves. Pictures taken from nadir view in the field showed an increase in soil surface cover and a decrease in TGI with time. TGI was negatively correlated to SPAD values measured on diagnostic leaves but was not correlated to leaf N content. In conclusion, these phenotyping techniques deliver relevant results but need to be further developed and validated for application in yam.},
DOI = {10.3390/agronomy11020249}
}



@Article{rs13030472,
AUTHOR = {Chen, Yang and Liu, Guanlan and Xu, Yaming and Pan, Pai and Xing, Yin},
TITLE = {PointNet++ Network Architecture with Individual Point Level and Global Features on Centroid for ALS Point Cloud Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {472},
URL = {https://www.mdpi.com/2072-4292/13/3/472},
ISSN = {2072-4292},
ABSTRACT = {Airborne laser scanning (ALS) point cloud has been widely used in the fields of ground powerline surveying, forest monitoring, urban modeling, and so on because of the great convenience it brings to people&rsquo;s daily life. However, the sparsity and uneven distribution of point clouds increases the difficulty of setting uniform parameters for semantic classification. The PointNet++ network is an end-to-end learning network for irregular point data and highly robust to small perturbations of input points along with corruption. It eliminates the need to calculate costly handcrafted features and provides a new paradigm for 3D understanding. However, each local region in the output is abstracted by its centroid and local feature that encodes the centroid&rsquo;s neighborhood. The feature learned on the centroid point may not contain relevant information of itself for random sampling, especially in large-scale neighborhood balls. Moreover, the centroid point&rsquo;s global-level information in each sample layer is also not marked. Therefore, this study proposed a modified PointNet++ network architecture which concentrates the point-level and global features on the centroid point towards the local features to facilitate classification. The proposed approach also utilizes a modified Focal Loss function to solve the extremely uneven category distribution on ALS point clouds. An elevation- and distance-based interpolation method is also proposed for the objects in ALS point clouds which exhibit discrepancies in elevation distributions. The experiments on the Vaihingen dataset of the International Society for Photogrammetry and Remote Sensing and the GML(B) 3D dataset demonstrate that the proposed method which provides additional contextual information to support classification achieves high accuracy with simple discriminative models and new state-of-the-art performance in power line categories.},
DOI = {10.3390/rs13030472}
}



@Article{app11031258,
AUTHOR = {Madridano, Ángel and Al-Kaff, Abdulla and Flores, Pablo and Martín, David and de la Escalera, Arturo},
TITLE = {Software Architecture for Autonomous and Coordinated Navigation of UAV Swarms in Forest and Urban Firefighting},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1258},
URL = {https://www.mdpi.com/2076-3417/11/3/1258},
ISSN = {2076-3417},
ABSTRACT = {Advances in the field of unmanned aerial vehicles (UAVs) have led to an exponential increase in their market, thanks to the development of innovative technological solutions aimed at a wide range of applications and services, such as emergencies and those related to fires. In addition, the expansion of this market has been accompanied by the birth and growth of the so-called UAV swarms. Currently, the expansion of these systems is due to their properties in terms of robustness, versatility, and efficiency. Along with these properties there is an aspect, which is still a field of study, such as autonomous and cooperative navigation of these swarms. In this paper we present an architecture that includes a set of complementary methods that allow the establishment of different control layers to enable the autonomous and cooperative navigation of a swarm of UAVs. Among the different layers, there are a global trajectory planner based on sampling, algorithms for obstacle detection and avoidance, and methods for autonomous decision making based on deep reinforcement learning. The paper shows satisfactory results for a line-of-sight based algorithm for global path planner trajectory smoothing in 2D and 3D. In addition, a novel method for autonomous navigation of UAVs based on deep reinforcement learning is shown, which has been tested in 2 different simulation environments with promising results about the use of these techniques to achieve autonomous navigation of UAVs.},
DOI = {10.3390/app11031258}
}



@Article{s21030927,
AUTHOR = {Carramiñana, David and Campaña, Iván and Bergesio, Luca and Bernardos, Ana M. and Besada, Juan A.},
TITLE = {Sensors and Communication Simulation for Unmanned Traffic Management},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {927},
URL = {https://www.mdpi.com/1424-8220/21/3/927},
PubMedID = {33573192},
ISSN = {1424-8220},
ABSTRACT = {Unmanned traffic management (UTM) systems will become a key enabler to the future drone market ecosystem, enabling the safe concurrent operation of both manned and unmanned aircrafts. Currently, these systems are usually tested by performing real scenarios that are costly, limited, hardly scalable, and poorly repeatable. As a solution, in this paper we propose an agent-based simulation platform, implemented through a micro service architecture, which may simulate UTM information sources, such as flight plans, telemetry messages, or tracks from a surveillance network. The final objective of this simulator is to use these information streams to perform a system-level evaluation of UTM systems both in the pre-flight and in-flight stages. The proposed platform, with a focus on simulation of communications and sensors, allows to model UTM actors&rsquo; behaviors and their interactions. In addition, it also considers the manual definition of events to simulate unexpected behaviors/events (contingencies), such as communications failures or pilots&rsquo; actions. In order to validate our architecture, we implemented a simulator that considers the following actors: drones, pilots, ground control stations, surveillance networks, and communications networks. This platform enables the simulation of the drone trajectory and control, the C2 (command and control) link, drone detection by surveillance sensors, and the communication of all agents by means of a mobile communications network. Our results show that it is possible to truthfully recreate complex scenarios using this simulator, mitigating the disadvantages of real testbeds.},
DOI = {10.3390/s21030927}
}



@Article{min11020148,
AUTHOR = {Jung, Dahee and Choi, Yosoon},
TITLE = {Systematic Review of Machine Learning Applications in Mining: Exploration, Exploitation, and Reclamation},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {148},
URL = {https://www.mdpi.com/2075-163X/11/2/148},
ISSN = {2075-163X},
ABSTRACT = {Recent developments in smart mining technology have enabled the production, collection, and sharing of a large amount of data in real time. Therefore, research employing machine learning (ML) that utilizes these data is being actively conducted in the mining industry. In this study, we reviewed 109 research papers, published over the past decade, that discuss ML techniques for mineral exploration, exploitation, and mine reclamation. Research trends, ML models, and evaluation methods primarily discussed in the 109 papers were systematically analyzed. The results demonstrated that ML studies have been actively conducted in the mining industry since 2018, mostly for mineral exploration. Among the ML models, support vector machine was utilized the most, followed by deep learning models. The ML models were evaluated mostly in terms of their root mean square error and coefficient of determination.},
DOI = {10.3390/min11020148}
}



@Article{rs13030504,
AUTHOR = {Yang, Wanting and Zhang, Xianfeng and Luo, Peng},
TITLE = {Transferability of Convolutional Neural Network Models for Identifying Damaged Buildings Due to Earthquake},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {504},
URL = {https://www.mdpi.com/2072-4292/13/3/504},
ISSN = {2072-4292},
ABSTRACT = {The collapse of buildings caused by earthquakes can lead to a large loss of life and property. Rapid assessment of building damage with remote sensing image data can support emergency rescues. However, current studies indicate that only a limited sample set can usually be obtained from remote sensing images immediately following an earthquake. Consequently, the difficulty in preparing sufficient training samples constrains the generalization of the model in the identification of earthquake-damaged buildings. To produce a deep learning network model with strong generalization, this study adjusted four Convolutional Neural Network (CNN) models for extracting damaged building information and compared their performance. A sample dataset of damaged buildings was constructed by using multiple disaster images retrieved from the xBD dataset. Using satellite and aerial remote sensing data obtained after the 2008 Wenchuan earthquake, we examined the geographic and data transferability of the deep network model pre-trained on the xBD dataset. The result shows that the network model pre-trained with samples generated from multiple disaster remote sensing images can extract accurately collapsed building information from satellite remote sensing data. Among the adjusted CNN models tested in the study, the adjusted DenseNet121 was the most robust. Transfer learning solved the problem of poor adaptability of the network model to remote sensing images acquired by different platforms and could identify disaster-damaged buildings properly. These results provide a solution to the rapid extraction of earthquake-damaged building information based on a deep learning network model.},
DOI = {10.3390/rs13030504}
}



@Article{s21030974,
AUTHOR = {Rahman, Ehab Ur and Zhang, Yihong and Ahmad, Sohail and Ahmad, Hafiz Ishfaq and Jobaer, Sayed},
TITLE = {Autonomous Vision-Based Primary Distribution Systems Porcelain Insulators Inspection Using UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {974},
URL = {https://www.mdpi.com/1424-8220/21/3/974},
PubMedID = {33540500},
ISSN = {1424-8220},
ABSTRACT = {The early detection of damaged (partially broken) outdoor insulators in primary distribution systems is of paramount importance for continuous electricity supply and public safety. Unmanned aerial vehicles (UAVs) present a safer, autonomous, and efficient way to examine the power system components without closing the power distribution system. In this work, a novel dataset is designed by capturing real images using UAVs and manually generated images collected to overcome the data insufficiency problem. A deep Laplacian pyramid-based super-resolution network is implemented to reconstruct high-resolution training images. To improve the visibility of low-light images, a low-light image enhancement technique is used for the robust exposure correction of the training images. A different fine-tuning strategy is implemented for fine-tuning the object detection model to increase detection accuracy for the specific faulty insulators. Several flight path strategies are proposed to overcome the shuttering effect of insulators, along with providing a less complex and time- and energy-efficient approach for capturing a video stream of the power system components. The performance of different object detection models is presented for selecting the most suitable one for fine-tuning on the specific faulty insulator dataset. For the detection of damaged insulators, our proposed method achieved an F1-score of 0.81 and 0.77 on two different datasets and presents a simple and more efficient flight strategy. Our approach is based on real aerial inspection of in-service porcelain insulators by extensive evaluation of several video sequences showing robust fault recognition and diagnostic capabilities. Our approach is demonstrated on data acquired by a drone in Swat, Pakistan.},
DOI = {10.3390/s21030974}
}



@Article{drones5010012,
AUTHOR = {Oleksyn, Semonn and Tosetto, Louise and Raoult, Vincent and Joyce, Karen E. and Williamson, Jane E.},
TITLE = {Going Batty: The Challenges and Opportunities of Using Drones to Monitor the Behaviour and Habitat Use of Rays},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2504-446X/5/1/12},
ISSN = {2504-446X},
ABSTRACT = {The way an animal behaves in its habitat provides insight into its ecological role. As such, collecting robust, accurate datasets in a time-efficient manner is an ever-present pressure for the field of behavioural ecology. Faced with the shortcomings and physical limitations of traditional ground-based data collection techniques, particularly in marine studies, drones offer a low-cost and efficient approach for collecting data in a range of coastal environments. Despite drones being widely used to monitor a range of marine animals, they currently remain underutilised in ray research. The innovative application of drones in environmental and ecological studies has presented novel opportunities in animal observation and habitat assessment, although this emerging field faces substantial challenges. As we consider the possibility to monitor rays using drones, we face challenges related to local aviation regulations, the weather and environment, as well as sensor and platform limitations. Promising solutions continue to be developed, however, growing the potential for drone-based monitoring of behaviour and habitat use of rays. While the barriers to enter this field may appear daunting for researchers with little experience with drones, the technology is becoming increasingly accessible, helping ray researchers obtain a wide range of highly useful data.},
DOI = {10.3390/drones5010012}
}



@Article{s21041033,
AUTHOR = {Wen, Qiaodi and Luo, Ziqi and Chen, Ruitao and Yang, Yifan and Li, Guofa},
TITLE = {Deep Learning Approaches on Defect Detection in High Resolution Aerial Images of Insulators},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1033},
URL = {https://www.mdpi.com/1424-8220/21/4/1033},
PubMedID = {33546245},
ISSN = {1424-8220},
ABSTRACT = {By detecting the defect location in high-resolution insulator images collected by unmanned aerial vehicle (UAV) in various environments, the occurrence of power failure can be timely detected and the caused economic loss can be reduced. However, the accuracies of existing detection methods are greatly limited by the complex background interference and small target detection. To solve this problem, two deep learning methods based on Faster R-CNN (faster region-based convolutional neural network) are proposed in this paper, namely Exact R-CNN (exact region-based convolutional neural network) and CME-CNN (cascade the mask extraction and exact region-based convolutional neural network). Firstly, we proposed an Exact R-CNN based on a series of advanced techniques including FPN (feature pyramid network), cascade regression, and GIoU (generalized intersection over union). RoI Align (region of interest align) is introduced to replace RoI pooling (region of interest pooling) to address the misalignment problem, and the depthwise separable convolution and linear bottleneck are introduced to reduce the computational burden. Secondly, a new pipeline is innovatively proposed to improve the performance of insulator defect detection, namely CME-CNN. In our proposed CME-CNN, an insulator mask image is firstly generated to eliminate the complex background by using an encoder-decoder mask extraction network, and then the Exact R-CNN is used to detect the insulator defects. The experimental results show that our proposed method can effectively detect insulator defects, and its accuracy is better than the examined mainstream target detection algorithms.},
DOI = {10.3390/s21041033}
}



@Article{app11041403,
AUTHOR = {Kamarudin, Mohd Hider and Ismail, Zool Hilmi and Saidi, Noor Baity},
TITLE = {Deep Learning Sensor Fusion in Plant Water Stress Assessment: A Comprehensive Review},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1403},
URL = {https://www.mdpi.com/2076-3417/11/4/1403},
ISSN = {2076-3417},
ABSTRACT = {Water stress is one of the major challenges to food security, causing a significant economic loss for the nation as well for growers. Accurate assessment of water stress will enhance agricultural productivity through optimization of plant water usage, maximizing plant breeding strategies, and preventing forest wildfire for better ecosystem management. Recent advancements in sensor technologies have enabled high-throughput, non-contact, and cost-efficient plant water stress assessment through intelligence system modeling. The advanced deep learning sensor fusion technique has been reported to improve the performance of the machine learning application for processing the collected sensory data. This paper extensively reviews the state-of-the-art methods for plant water stress assessment that utilized the deep learning sensor fusion approach in their application, together with future prospects and challenges of the application domain. Notably, 37 deep learning solutions fell under six main areas, namely soil moisture estimation, soil water modelling, evapotranspiration estimation, evapotranspiration forecasting, plant water status estimation and plant water stress identification. Basically, there are eight deep learning solutions compiled for the 3D-dimensional data and plant varieties challenge, including unbalanced data that occurred due to isohydric plants, and the effect of variations that occur within the same species but cultivated from different locations.},
DOI = {10.3390/app11041403}
}



@Article{s21041076,
AUTHOR = {Yan, Peng and Jia, Tao and Bai, Chengchao},
TITLE = {Searching and Tracking an Unknown Number of Targets: A Learning-Based Method Enhanced with Maps Merging},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1076},
URL = {https://www.mdpi.com/1424-8220/21/4/1076},
PubMedID = {33557359},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have been widely used in search and rescue (SAR) missions due to their high flexibility. A key problem in SAR missions is to search and track moving targets in an area of interest. In this paper, we focus on the problem of Cooperative Multi-UAV Observation of Multiple Moving Targets (CMUOMMT). In contrast to the existing literature, we not only optimize the average observation rate of the discovered targets, but we also emphasize the fairness of the observation of the discovered targets and the continuous exploration of the undiscovered targets, under the assumption that the total number of targets is unknown. To achieve this objective, a deep reinforcement learning (DRL)-based method is proposed under the Partially Observable Markov Decision Process (POMDP) framework, where each UAV maintains four observation history maps, and maps from different UAVs within a communication range can be merged to enhance UAVs’ awareness of the environment. A deep convolutional neural network (CNN) is used to process the merged maps and generate the control commands to UAVs. The simulation results show that our policy can enable UAVs to balance between giving the discovered targets a fair observation and exploring the search region compared with other methods.},
DOI = {10.3390/s21041076}
}



@Article{electronics10040387,
AUTHOR = {Chamran, Mohammad Kazem and Yau, Kok-Lim Alvin and Noor, Rafidah Md. and Wu, Celimuge},
TITLE = {An Experimental Study on D2D Route Selection Mechanism in 5G Scenarios},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {387},
URL = {https://www.mdpi.com/2079-9292/10/4/387},
ISSN = {2079-9292},
ABSTRACT = {This paper demonstrates a route selection mechanism on a testbed with heterogeneous device-to-device (D2D) wireless communication for a 5G network scenario. The source node receives information about the primary users’ (PUs’) (or licensed users’) activities and available routes from the macrocell base station (or a central controller) and makes a decision to select a multihop route to the destination node. The source node from small cells can either choose: (a) a route with direct communication with the macrocell base station to improve the route performance; or (b) a route with D2D communication among nodes in the small cells to offload traffic from the macrocell to improve spectrum efficiency. The selected D2D route has the least PUs’ activities. The route selection mechanism is investigated on our testbed that helps to improve the accuracy of network performance measurement. In traditional testbeds, each node (e.g., Universal Software Radio Peripheral (USRP) that serves as the front-end communication block) is connected to a single processing unit (e.g., a personal computer) via a switch using cables. In our testbed, each USRP node is connected to a separate processing unit, i.e., raspberry Pi3 B+ (or RP3), which offers three main advantages: (a) control messages and data packets are exchanged via the wireless medium; (b) separate processing units make decisions in a distributed and heterogeneous manner; and (c) the nodes are placed further apart from one another. Therefore, in the investigation of our route selection scheme, the response delay of control message exchange and the packet loss caused by the operating environment (e.g., ambient noise) are implied in our end-to-end delay and packet delivery ratio measurement. Our results show an increase of end-to-end delay and a decrease of packet delivery ratio due to the transmission of control messages and data packets in the wireless medium in the presence of the dynamic PUs’ activities. Furthermore, D2D communication can offload 25% to 75% traffic from macrocell base station to small cells.},
DOI = {10.3390/electronics10040387}
}



@Article{s21041108,
AUTHOR = {Melo, Aurelio G. and Pinto, Milena F. and Marcato, Andre L. M. and Honório, Leonardo M. and Coelho, Fabrício O.},
TITLE = {Dynamic Optimization and Heuristics Based Online Coverage Path Planning in 3D Environment for UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1108},
URL = {https://www.mdpi.com/1424-8220/21/4/1108},
PubMedID = {33562647},
ISSN = {1424-8220},
ABSTRACT = {Path planning is one of the most important issues in the robotics field, being applied in many domains ranging from aerospace technology and military tasks to manufacturing and agriculture. Path planning is a branch of autonomous navigation. In autonomous navigation, dynamic decisions about the path have to be taken while the robot moves towards its goal. Among the navigation area, an important class of problems is Coverage Path Planning (CPP). The CPP technique is associated with determining a collision-free path that passes through all viewpoints in a specific area. This paper presents a method to perform CPP in 3D environment for Unmanned Aerial Vehicles (UAVs) applications, namely 3D dynamic for CPP applications (3DD-CPP). The proposed method can be deployed in an unknown environment through a combination of linear optimization and heuristics. A model to estimate cost matrices accounting for UAV power usage is proposed and evaluated for a few different flight speeds. As linear optimization methods can be computationally demanding to be used on-board a UAV, this work also proposes a distributed execution of the algorithm through fog-edge computing. Results showed that 3DD-CPP had a good performance in both local execution and fog-edge for different simulated scenarios. The proposed heuristic is capable of re-optimization, enabling execution in environments with local knowledge of the environments.},
DOI = {10.3390/s21041108}
}



@Article{s21041151,
AUTHOR = {Horla, Dariusz and Giernacki, Wojciech and Cieślak, Jacek and Campoy, Pascual},
TITLE = {Altitude Measurement-Based Optimization of the Landing Process of UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1151},
URL = {https://www.mdpi.com/1424-8220/21/4/1151},
PubMedID = {33562147},
ISSN = {1424-8220},
ABSTRACT = {The paper addresses the loop shaping problem in the altitude control of an unmanned aerial vehicle to land the flying robot with a specific landing scenario adopted. The proposed solution is optimal, in the sense of the selected performance indices, namely minimum-time, minimum-energy, and velocity-penalized related functions, achieving their minimal values, with numerous experiments conducted throughout the development and preparation to the Mohamed Bin Zayed International Robotics Challenge (MBZIRC 2020). A novel approach to generation of a reference altitude trajectory is presented, which is then tracked in a standard, though optimized, control loop. Three landing scenarios are considered, namely: minimum-time, minimum-energy, and velocity-penalized landing scenarios. The experimental results obtained with the use of the Simulink Support Package for Parrot Minidrones, and the OptiTrack motion capture system proved the effectiveness of the proposed approach.},
DOI = {10.3390/s21041151}
}



@Article{rs13040584,
AUTHOR = {Zhu, Linglong and Zhang, Yonghong and Wang, Jiangeng and Tian, Wei and Liu, Qi and Ma, Guangyi and Kan, Xi and Chu, Ya},
TITLE = {Downscaling Snow Depth Mapping by Fusion of Microwave and Optical Remote-Sensing Data Based on Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {584},
URL = {https://www.mdpi.com/2072-4292/13/4/584},
ISSN = {2072-4292},
ABSTRACT = {Accurate high spatial resolution snow depth mapping in arid and semi-arid regions is of great importance for snow disaster assessment and hydrological modeling. However, due to the complex topography and low spatial-resolution microwave remote-sensing data, the existing snow depth datasets have large errors and uncertainty, and actual spatiotemporal heterogeneity of snow depth cannot be effectively detected. This paper proposed a deep learning approach based on downscaling snow depth retrieval by fusion of satellite remote-sensing data with multiple spatial scales and diverse characteristics. The (Fengyun-3 Microwave Radiation Imager) FY-3 MWRI data were downscaled to 500 m resolution to match Moderate-resolution Imaging Spectroradiometer (MODIS) snow cover, meteorological and geographic data. A deep neural network was constructed to capture detailed spectral and radiation signals and trained to retrieve the higher spatial resolution snow depth from the aforementioned input data and ground observation. Verified by in situ measurements, downscaled snow depth has the lowest root mean square error (RMSE) and mean absolute error (MAE) (8.16 cm, 4.73 cm respectively) among Environmental and Ecological Science Data Center for West China Snow Depth (WESTDC_SD, 9.38 cm and 5.36 cm), the Microwave Radiation Imager (MWRI) Ascend Snow Depth (MWRI_A_SD, 9.45 cm and 5.49 cm) and MWRI Descend Snow Depth (MWRI_D_SD, 10.55 cm and 6.13 cm) in the study area. Meanwhile, downscaled snow depth could provide more detailed information in spatial distribution, which has been used to analyze the decrease of retrieval accuracy by various topography factors.},
DOI = {10.3390/rs13040584}
}



@Article{aerospace8020044,
AUTHOR = {Uzun, Mevlut and Demirezen, Mustafa Umut and Inalhan, Gokhan},
TITLE = {Physics Guided Deep Learning for Data-Driven Aircraft Fuel Consumption Modeling},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {44},
URL = {https://www.mdpi.com/2226-4310/8/2/44},
ISSN = {2226-4310},
ABSTRACT = {This paper presents a physics-guided deep neural network framework to estimate fuel consumption of an aircraft. The framework aims to improve data-driven models’ consistency in flight regimes that are not covered by data. In particular, we guide the neural network with the equations that represent fuel flow dynamics. In addition to the empirical error, we embed this physical knowledge as several extra loss terms. Results show that our proposed model accomplishes correct predictions on the labeled test set, as well as assuring physical consistency in unseen flight regimes. The results indicate that our model, while being applicable to the aircraft’s complete flight envelope, yields lower fuel consumption error measures compared to the model-based approaches and other supervised learning techniques utilizing the same training data sets. In addition, our deep learning model produces fuel consumption trends similar to the BADA4 aircraft performance model, which is widely utilized in real-world operations, in unseen and untrained flight regimes. In contrast, the other supervised learning techniques fail to produce meaningful results. Overall, the proposed methodology enhances the explainability of data-driven models without deteriorating accuracy.},
DOI = {10.3390/aerospace8020044}
}



@Article{ai2010004,
AUTHOR = {Espejo-Garcia, Borja and Malounas, Ioannis and Vali, Eleanna and Fountas, Spyros},
TITLE = {Testing the Suitability of Automated Machine Learning for Weeds Identification},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {1},
PAGES = {34--47},
URL = {https://www.mdpi.com/2673-2688/2/1/4},
ISSN = {2673-2688},
ABSTRACT = {In the past years, several machine-learning-based techniques have arisen for providing effective crop protection. For instance, deep neural networks have been used to identify different types of weeds under different real-world conditions. However, these techniques usually require extensive involvement of experts working iteratively in the development of the most suitable machine learning system. To support this task and save resources, a new technique called Automated Machine Learning has started being studied. In this work, a complete open-source Automated Machine Learning system was evaluated with two different datasets, (i) The Early Crop Weeds dataset and (ii) the Plant Seedlings dataset, covering the weeds identification problem. Different configurations, such as the use of plant segmentation, the use of classifier ensembles instead of Softmax and training with noisy data, have been compared. The results showed promising performances of 93.8% and 90.74% F1 score depending on the dataset used. These performances were aligned with other related works in AutoML, but they are far from machine-learning-based systems manually fine-tuned by human experts. From these results, it can be concluded that finding a balance between manual expert work and Automated Machine Learning will be an interesting path to work in order to increase the efficiency in plant protection.},
DOI = {10.3390/ai2010004}
}



@Article{rs13040633,
AUTHOR = {Zhang, Xiuwei and Zhou, Yang and Jin, Jiaojiao and Wang, Yafei and Fan, Minhao and Wang, Ning and Zhang, Yanning},
TITLE = {ICENETv2: A Fine-Grained River Ice Semantic Segmentation Network Based on UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {633},
URL = {https://www.mdpi.com/2072-4292/13/4/633},
ISSN = {2072-4292},
ABSTRACT = {Accurate ice segmentation is one of the most crucial techniques for intelligent ice monitoring. Compared with ice segmentation, it can provide more information for ice situation analysis, change trend prediction, and so on. Therefore, the study of ice segmentation has important practical significance. In this study, we focused on fine-grained river ice segmentation using unmanned aerial vehicle (UAV) images. This has the following difficulties: (1) The scale of river ice varies greatly in different images and even in the same image; (2) the same kind of river ice differs greatly in color, shape, texture, size, and so on; and (3) the appearances of different kinds of river ice sometimes appear similar due to the complex formation and change procedure. Therefore, to perform this study, the NWPU_YRCC2 dataset was built, in which all UAV images were collected in the Ningxia–Inner Mongolia reach of the Yellow River. Then, a novel semantic segmentation method based on deep convolution neural network, named ICENETv2, is proposed. To achieve multiscale accurate prediction, we design a multilevel features fusion framework, in which multi-scale high-level semantic features and lower-level finer features are effectively fused. Additionally, a dual attention module is adopted to highlight distinguishable characteristics, and a learnable up-sampling strategy is further used to improve the segmentation accuracy of the details. Experiments show that ICENETv2 achieves the state-of-the-art on the NWPU_YRCC2 dataset. Finally, our ICENETv2 is also applied to solve a realistic problem, calculating drift ice cover density, which is one of the most important factors to predict the freeze-up data of the river. The results demonstrate that the performance of ICENETv2 meets the actual application demand.},
DOI = {10.3390/rs13040633}
}



@Article{rs13040659,
AUTHOR = {Yuval, Matan and Alonso, Iñigo and Eyal, Gal and Tchernov, Dan and Loya, Yossi and Murillo, Ana C. and Treibitz, Tali},
TITLE = {Repeatable Semantic Reef-Mapping through Photogrammetry and Label-Augmentation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {659},
URL = {https://www.mdpi.com/2072-4292/13/4/659},
ISSN = {2072-4292},
ABSTRACT = {In an endeavor to study natural systems at multiple spatial and taxonomic resolutions, there is an urgent need for automated, high-throughput frameworks that can handle plethora of information. The coalescence of remote-sensing, computer-vision, and deep-learning elicits a new era in ecological research. However, in complex systems, such as marine-benthic habitats, key ecological processes still remain enigmatic due to the lack of cross-scale automated approaches (mms to kms) for community structure analysis. We address this gap by working towards scalable and comprehensive photogrammetric surveys, tackling the profound challenges of full semantic segmentation and 3D grid definition. Full semantic segmentation (where every pixel is classified) is extremely labour-intensive and difficult to achieve using manual labeling. We propose using label-augmentation, i.e., propagation of sparse manual labels, to accelerate the task of full segmentation of photomosaics. Photomosaics are synthetic images generated from a projected point-of-view of a 3D model. In the lack of navigation sensors (e.g., a diver-held camera), it is difficult to repeatably determine the slope-angle of a 3D map. We show this is especially important in complex topographical settings, prevalent in coral-reefs. Specifically, we evaluate our approach on benthic habitats, in three different environments in the challenging underwater domain. Our approach for label-augmentation shows human-level accuracy in full segmentation of photomosaics using labeling as sparse as 0.1%, evaluated on several ecological measures. Moreover, we found that grid definition using a leveler improves the consistency in community-metrics obtained due to occlusions and topology (angle and distance between objects), and that we were able to standardise the 3D transformation with two percent error in size measurements. By significantly easing the annotation process for full segmentation and standardizing the 3D grid definition we present a semantic mapping methodology enabling change-detection, which is practical, swift, and cost-effective. Our workflow enables repeatable surveys without permanent markers and specialized mapping gear, useful for research and monitoring, and our code is available online. Additionally, we release the Benthos data-set, fully manually labeled photomosaics from three oceanic environments with over 4500 segmented objects useful for research in computer-vision and marine ecology.},
DOI = {10.3390/rs13040659}
}



@Article{rs13040663,
AUTHOR = {Fan, Runze and Xu, Ting-Bing and Wei, Zhenzhong},
TITLE = {Estimating 6D Aircraft Pose from Keypoints and Structures},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {663},
URL = {https://www.mdpi.com/2072-4292/13/4/663},
ISSN = {2072-4292},
ABSTRACT = {This article addresses the challenge of 6D aircraft pose estimation from a single RGB image during the flight. Many recent works have shown that keypoints-based approaches, which first detect keypoints and then estimate the 6D pose, achieve remarkable performance. However, it is hard to locate the keypoints precisely in complex weather scenes. In this article, we propose a novel approach, called Pose Estimation with Keypoints and Structures (PEKS), which leverages multiple intermediate representations to estimate the 6D pose. Unlike previous works, our approach simultaneously locates keypoints and structures to recover the pose parameter of aircraft through a Perspective-n-Point Structure (PnPS) algorithm. These representations integrate the local geometric information of the object and the topological relationship between components of the target, which effectively improve the accuracy and robustness of 6D pose estimation. In addition, we contribute a dataset for aircraft pose estimation which consists of 3681 real images and 216,000 rendered images. Extensive experiments on our own aircraft pose dataset and multiple open-access pose datasets (e.g., ObjectNet3D, LineMOD) demonstrate that our proposed method can accurately estimate 6D aircraft pose in various complex weather scenes while achieving the comparative performance with the state-of-the-art pose estimation methods.},
DOI = {10.3390/rs13040663}
}



@Article{rs13040681,
AUTHOR = {Morell-Monzó, Sergio and Sebastiá-Frasquet, María-Teresa and Estornell, Javier},
TITLE = {Land Use Classification of VHR Images for Mapping Small-Sized Abandoned Citrus Plots by Using Spectral and Textural Information},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {681},
URL = {https://www.mdpi.com/2072-4292/13/4/681},
ISSN = {2072-4292},
ABSTRACT = {Agricultural land abandonment is an increasing problem in Europe. The Comunitat Valenciana Region (Spain) is one of the most important citrus producers in Europe suffering this problem. This region characterizes by small sized citrus plots and high spatial fragmentation which makes necessary to use Very High-Resolution images to detect abandoned plots. In this paper spectral and Gray Level Co-Occurrence Matrix (GLCM)-based textural information derived from the Normalized Difference Vegetation Index (NDVI) are used to map abandoned citrus plots in Oliva municipality (eastern Spain). The proposed methodology is based on three general steps: (a) extraction of spectral and textural features from the image, (b) pixel-based classification of the image using the Random Forest algorithm, and (c) assignment of a single value per plot by majority voting. The best results were obtained when extracting the texture features with a 9 × 9 window size and the Random Forest model showed convergence around 100 decision trees. Cross-validation of the model showed an overall accuracy of the pixel-based classification of 87% and an overall accuracy of the plot-based classification of 95%. All the variables used are statistically significant for the classification, however the most important were contrast, dissimilarity, NIR band (720 nm), and blue band (620 nm). According to our results, 31% of the plots classified as citrus in Oliva by current methodology are abandoned. This is very important to avoid overestimating crop yield calculations by public administrations. The model was applied successfully outside the main study area (Oliva municipality); with a slightly lower accuracy (92%). This research provides a new approach to map small agricultural plots, especially to detect land abandonment in woody evergreen crops that have been little studied until now.},
DOI = {10.3390/rs13040681}
}



@Article{rs13040705,
AUTHOR = {Kopačková-Strnadová, Veronika and Koucká, Lucie and Jelének, Jan and Lhotáková, Zuzana and Oulehle, Filip},
TITLE = {Canopy Top, Height and Photosynthetic Pigment Estimation Using Parrot Sequoia Multispectral Imagery and the Unmanned Aerial Vehicle (UAV)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {705},
URL = {https://www.mdpi.com/2072-4292/13/4/705},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing is one of the modern methods that have significantly developed over the last two decades and, nowadays, it provides a new means for forest monitoring. High spatial and temporal resolutions are demanded for the accurate and timely monitoring of forests. In this study, multi-spectral Unmanned Aerial Vehicle (UAV) images were used to estimate canopy parameters (definition of crown extent, top, and height, as well as photosynthetic pigment contents). The UAV images in Green, Red, Red-Edge, and Near infrared (NIR) bands were acquired by Parrot Sequoia camera over selected sites in two small catchments (Czech Republic) covered dominantly by Norway spruce monocultures. Individual tree extents, together with tree tops and heights, were derived from the Canopy Height Model (CHM). In addition, the following were tested: (i) to what extent can the linear relationship be established between selected vegetation indexes (Normalized Difference Vegetation Index (NDVI) and NDVIred edge) derived for individual trees and the corresponding ground truth (e.g., biochemically assessed needle photosynthetic pigment contents) and (ii) whether needle age selection as a ground truth and crown light conditions affect the validity of linear models. The results of the conducted statistical analysis show that the two vegetation indexes (NDVI and NDVIred edge) tested here have the potential to assess photosynthetic pigments in Norway spruce forests at a semi-quantitative level; however, the needle-age selection as a ground truth was revealed to be a very important factor. The only usable results were obtained for linear models when using the second year needle pigment contents as a ground truth. On the other hand, the illumination conditions of the crown proved to have very little effect on the model’s validity. No study was found to directly compare these results conducted on coniferous forest stands. This shows that there is a further need for studies dealing with a quantitative estimation of the biochemical variables of nature coniferous forests when employing spectral data that were acquired by the UAV platform at a very high spatial resolution.},
DOI = {10.3390/rs13040705}
}



@Article{s21041386,
AUTHOR = {Liu, Feng and Dai, Shuling and Zhao, Yongjia},
TITLE = {Learning to Have a Civil Aircraft Take Off under Crosswind Conditions by Reinforcement Learning with Multimodal Data and Preprocessing Data},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1386},
URL = {https://www.mdpi.com/1424-8220/21/4/1386},
PubMedID = {33669479},
ISSN = {1424-8220},
ABSTRACT = {Autopilot technology in the field of aviation has developed over many years. However, it is difficult for an autopilot system to autonomously operate a civil aircraft under bad weather conditions. In this paper, we present a reinforcement learning (RL) algorithm using multimodal data and preprocessing data to have a civil aircraft take off autonomously under crosswind conditions. The multimodal data include the common flight status and visual information. The preprocessing is a new design that maps some flight data by nonlinear functions based on the general flight dynamics before these data are fed into the RL model. Extensive experiments under different crosswind conditions with a professional flight simulator demonstrate that the proposed method can effectively control a civil aircraft to take off under various crosswind conditions and achieve better performance than trials without visual information or preprocessing data.},
DOI = {10.3390/s21041386}
}



@Article{rs13040732,
AUTHOR = {Nomura, Ryota and Oki, Kazuo},
TITLE = {Downscaling of MODIS NDVI by Using a Convolutional Neural Network-Based Model with Higher Resolution SAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {732},
URL = {https://www.mdpi.com/2072-4292/13/4/732},
ISSN = {2072-4292},
ABSTRACT = {The normalized difference vegetation index (NDVI) is a simple but powerful indicator, that can be used to observe green live vegetation efficiently. Since its introduction in the 1970s, NDVI has been used widely for land management, food security, and physical models. For these applications, acquiring NDVI in both high spatial resolution and high temporal resolution is preferable. However, there is generally a trade-off between temporal and spatial resolution when using satellite images. To relieve this problem, a convolutional neural network (CNN) based downscaling model was proposed in this research. This model is capable of estimating 10-m high resolution NDVI from MODIS (Moderate Resolution Imaging Spectroradiometer) 250-m resolution NDVI by using Sentinel-1 10-m resolution synthetic aperture radar (SAR) data. First, this downscaling model was trained to estimate Sentinel-2 10-m resolution NDVI from a combination of upscaled 250-m resolution Sentinel-2 NDVI and 10-m resolution Sentinel-1 SAR data, by using data acquired in 2019 in the target area. Then, the generality of this model was validated by applying it to test data acquired in 2020, with the result that the model predicted the NDVI with reasonable accuracy (MAE = 0.090, ρ = 0.734 on average). Next, 250-m NDVI from MODIS data was used as input to confirm this model under conditions replicating an actual application case. Although there were mismatch in the original MODIS and Sentinel-2 NDVI data, the model predicted NDVI with acceptable accuracy (MAE = 0.108, ρ = 0.650 on average). Finally, this model was applied to predict high spatial resolution NDVI using MODIS and Sentinel-1 data acquired in target area from 1 January 2020~31 December 2020. In this experiment, double cropping of cabbage, which was not observable at the original MODIS resolution, was observed by enhanced temporal resolution of high spatial resolution NDVI images (approximately ×2.5). The proposed method enables the production of 10-m resolution NDVI data with acceptable accuracy when cloudless MODIS NDVI and Sentinel-1 SAR data is available, and can enhance the temporal resolution of high resolution 10-m NDVI data.},
DOI = {10.3390/rs13040732}
}



@Article{rs13040733,
AUTHOR = {Gao, Bowen and Chen, Ninghua and Blaschke, Thomas and Wu, Chase Q. and Chen, Jianyu and Xu, Yaochen and Yang, Xiaoping and Du, Zhenhong},
TITLE = {Automated Characterization of Yardangs Using Deep Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/13/4/733},
ISSN = {2072-4292},
ABSTRACT = {The morphological characteristics of yardangs are the direct evidence that reveals the wind and fluvial erosion for lacustrine sediments in arid areas. These features can be critical indicators in reconstructing local wind directions and environment conditions. Thus, the fast and accurate extraction of yardangs is key to studying their regional distribution and evolution process. However, the existing automated methods to characterize yardangs are of limited generalization that may only be feasible for specific types of yardangs in certain areas. Deep learning methods, which are superior in representation learning, provide potential solutions for mapping yardangs with complex and variable features. In this study, we apply Mask region-based convolutional neural networks (Mask R-CNN) to automatically delineate and classify yardangs using very high spatial resolution images from Google Earth. The yardang field in the Qaidam Basin, northwestern China is selected to conduct the experiments and the method yields mean average precisions of 0.869 and 0.671 for intersection of union (IoU) thresholds of 0.5 and 0.75, respectively. The manual validation results on images of additional study sites show an overall detection accuracy of 74%, while more than 90% of the detected yardangs can be correctly classified and delineated. We then conclude that Mask R-CNN is a robust model to characterize multi-scale yardangs of various types and allows for the research of the morphological and evolutionary aspects of aeolian landform.},
DOI = {10.3390/rs13040733}
}



@Article{su13042161,
AUTHOR = {Nieto-Julián, Juan E. and Lara, Lenin and Moyano, Juan},
TITLE = {Implementation of a TeamWork-HBIM for the Management and Sustainability of Architectural Heritage},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {2161},
URL = {https://www.mdpi.com/2071-1050/13/4/2161},
ISSN = {2071-1050},
ABSTRACT = {The benefits of Building Information Modelling (BIM) accrue from the needs of the interoperability of applied technologies. This scope is strongly related to heritage buildings. Protection plans encompassing phases of heritage conservation, interpretation, intervention and dissemination could lead to a sustainable model through a TeamWork-HBIM project. This work develops a step by step semantically enriched 3D model, from accurate data acquisition to the creation of a container of artistic assets. TeamWork-HBIM acts as a database for movable assets, i.e., parametric objects (GDL) with graphical and semantic information, which are valid for recording, inventory and cataloguing processes. Thus, heritage properties were created and used to create recording and inventory sheets related to movable assets. Consequently, a parametric object was edited in the HBIM project, so a new category called “Heritage Furniture” was available. Data from the monitoring of the artistic asset were included in that category. In addition, the specialist technicians from the TeamWork-HBIM team catalogued a dataset related to artistic, historical and conservation properties. Another advantage of the system was the reliability of the structure of the HBIM project, which was based on the actual geometry of the building provided by the point clouds. The information was valid for both modelling works and specialists in virtual monitoring. Moreover, the reliability of metadata was collected in a common data environment (CDE), which was available for everyone. As a result, the Teamwork-HBIM-CDE project meets the needs of private institutions, such as the Foundation of the Church of the Company of Jesus in Quito, related to the sustainability of the historic site. This sustainability is shown by the implementation of a methodology that strengthens the interdisciplinary information flow by including all disciplines of historical heritage.},
DOI = {10.3390/su13042161}
}



@Article{jsan10010015,
AUTHOR = {Blekos, Kostas and Tsakas, Anastasios and Xouris, Christos and Evdokidis, Ioannis and Alexandropoulos, Dimitris and Alexakos, Christos and Katakis, Sofoklis and Makedonas, Andreas and Theoharatos, Christos and Lalos, Aris},
TITLE = {Analysis, Modeling and Multi-Spectral Sensing for the Predictive Management of Verticillium Wilt in Olive Groves},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2224-2708/10/1/15},
ISSN = {2224-2708},
ABSTRACT = {The intensification and expansion in the cultivation of olives have contributed to the significant spread of Verticillium wilt, which is the most important fungal problem affecting olive trees. Recent studies confirm that practices such as the use of innovative natural minerals (Zeoshell ZF1) and the application of beneficial microorganisms (Micosat F BS WP) restore health in infected trees. However, for their efficient implementation the above methodologies require the marking of trees in the early stages of infestation—a task that is impractical with traditional means (manual labor) but also very difficult, as early stages are difficult to perceive with the naked eye. In this paper, we present the results of the My Olive Grove Coach (MyOGC) project, which used multispectral imaging from unmanned aerial vehicles to develop an olive grove monitoring system based on the autonomous and automatic processing of the multispectral images using computer vision and machine learning techniques. The goal of the system is to monitor and assess the health of olive groves, help in the prediction of Verticillium wilt spread and implement a decision support system that guides the farmer/agronomist.},
DOI = {10.3390/jsan10010015}
}



@Article{rs13040767,
AUTHOR = {Przewoźna, Patrycja and Hawryło, Paweł and Zięba-Kulawik, Karolina and Inglot, Adam and Mączka, Krzysztof and Wężyk, Piotr and Matczak, Piotr},
TITLE = {Use of Bi-Temporal ALS Point Clouds for Tree Removal Detection on Private Property in Racibórz, Poland},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {767},
URL = {https://www.mdpi.com/2072-4292/13/4/767},
ISSN = {2072-4292},
ABSTRACT = {Trees growing on private property have become an essential part of urban green policies. In many places, restrictions are imposed on tree removal on private property. However, monitoring compliance of these regulations appears difficult due to a lack of reference data and public administration capacity. We assessed the impact of the temporary suspension of mandatory permits on tree removal, which was in force in 2017 in Poland, on the change in urban tree cover (UTC) in the case of the municipality of Racibórz. The bi-temporal airborne laser scanning (ALS) point clouds (2011 and 2017) and administrative records on tree removal permits were used for analyzing the changes of UTC in the period of 2011–2017. The results show increased tree removal at a time when the mandatory permit was suspended. Moreover, it appeared that most trees on private properties were removed without obtaining permission when it was obligatory. The method based on LiDAR we proposed allows for monitoring green areas, including private properties.},
DOI = {10.3390/rs13040767}
}



@Article{app11041835,
AUTHOR = {Liao, Kuo-Chien and Lu, Jau-Huai},
TITLE = {Using UAV to Detect Solar Module Fault Conditions of a Solar Power Farm with IR and Visual Image Analysis},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1835},
URL = {https://www.mdpi.com/2076-3417/11/4/1835},
ISSN = {2076-3417},
ABSTRACT = {In recent years, solar energy has been regarded as one of the most important sustainable energy sources. Under the rapid and large-scale construction of solar farms, the maintenance and inspection of the health conditions of solar modules in a large solar farm become an important issue. This article proposes a method for detecting solar cell faults with unmanned aerial vehicle (UAV) equipped with a thermal imager and a visible light camera, and providing a fast and reliable detection method. The detection process includes a new concept of real-time monitoring of the detected area and analysis of the health of solar panels. An image process is proposed that may quickly and accurately detect the abnormality of a solar module. The whole process includes grayscale conversion, filtering, 3-D temperature representation, probability density function, and cumulative density function analysis. Ten cases in real fields have been studied with this process, including large scale solar farms and small size solar modules installed on buildings. Results show that the cumulative density function is a convenient way to determine the health status of the solar panel and may provide maintenance personnel a basis for determining whether replacement of solar cells is necessary for improving the overall power generation efficiency and simplify the maintenance process. It is worth noting that image recognition can increase the clarity of IR images and the cumulative chart can judge the defect rate of the cell. These two methods were combined to provide an instant, fast and accurate defect judgment.},
DOI = {10.3390/app11041835}
}



@Article{s21041456,
AUTHOR = {Ribeiro, Roberto and Ramos, João and Safadinho, David and Reis, Arsénio and Rabadão, Carlos and Barroso, João and Pereira, António},
TITLE = {Web AR Solution for UAV Pilot Training and Usability Testing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1456},
URL = {https://www.mdpi.com/1424-8220/21/4/1456},
PubMedID = {33669733},
ISSN = {1424-8220},
ABSTRACT = {Data and services are available anywhere at any time thanks to the Internet and mobile devices. Nowadays, there are new ways of representing data through trendy technologies such as augmented reality (AR), which extends our perception of reality through the addition of a virtual layer on top of real-time images. The great potential of unmanned aerial vehicles (UAVs) for carrying out routine and professional tasks has encouraged their use in the creation of several services, such as package delivery or industrial maintenance. Unfortunately, drone piloting is difficult to learn and requires specific training. Since regular training is performed with virtual simulations, we decided to propose a multiplatform cloud-hosted solution based in Web AR for drone training and usability testing. This solution defines a configurable trajectory through virtual elements represented over barcode markers placed on a real environment. The main goal is to provide an inclusive and accessible training solution which could be used by anyone who wants to learn how to pilot or test research related to UAV control. For this paper, we reviewed drones, AR, and human–drone interaction (HDI) to propose an architecture and implement a prototype, which was built using a Raspberry Pi 3, a camera, and barcode markers. The validation was conducted using several test scenarios. The results show that a real-time AR experience for drone pilot training and usability testing is achievable through web technologies. Some of the advantages of this approach, compared to traditional methods, are its high availability by using the web and other ubiquitous devices; the minimization of technophobia related to crashes; and the development of cost-effective alternatives to train pilots and make the testing phase easier for drone researchers and developers through trendy technologies.},
DOI = {10.3390/s21041456}
}



@Article{mi12020214,
AUTHOR = {Han, Shipeng and Meng, Zhen and Zhang, Xingcheng and Yan, Yuepeng},
TITLE = {Hybrid Deep Recurrent Neural Networks for Noise Reduction of MEMS-IMU with Static and Dynamic Conditions},
JOURNAL = {Micromachines},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {214},
URL = {https://www.mdpi.com/2072-666X/12/2/214},
PubMedID = {33672478},
ISSN = {2072-666X},
ABSTRACT = {Micro-electro-mechanical system inertial measurement unit (MEMS-IMU), a core component in many navigation systems, directly determines the accuracy of inertial navigation system; however, MEMS-IMU system is often affected by various factors such as environmental noise, electronic noise, mechanical noise and manufacturing error. These can seriously affect the application of MEMS-IMU used in different fields. Focus has been on MEMS gyro since it is an essential and, yet, complex sensor in MEMS-IMU which is very sensitive to noises and errors from the random sources. In this study, recurrent neural networks are hybridized in four different ways for noise reduction and accuracy improvement in MEMS gyro. These are two-layer homogenous recurrent networks built on long short term memory (LSTM-LSTM) and gated recurrent unit (GRU-GRU), respectively; and another two-layer but heterogeneous deep networks built on long short term memory-gated recurrent unit (LSTM-GRU) and a gated recurrent unit-long short term memory (GRU-LSTM). Practical implementation with static and dynamic experiments was carried out for a custom MEMS-IMU to validate the proposed networks, and the results show that GRU-LSTM seems to be overfitting large amount data testing for three-dimensional axis gyro in the static test. However, for X-axis and Y-axis gyro, LSTM-GRU had the best noise reduction effect with over 90% improvement in the three axes. For Z-axis gyroscope, LSTM-GRU performed better than LSTM-LSTM and GRU-GRU in quantization noise and angular random walk, while LSTM-LSTM shows better improvement than both GRU-GRU and LSTM-GRU networks in terms of zero bias stability. In the dynamic experiments, the Hilbert spectrum carried out revealed that time-frequency energy of the LSTM-LSTM, GRU-GRU, and GRU-LSTM denoising are higher compared to LSTM-GRU in terms of the whole frequency domain. Similarly, Allan variance analysis also shows that LSTM-GRU has a better denoising effect than the other networks in the dynamic experiments. Overall, the experimental results demonstrate the effectiveness of deep learning algorithms in MEMS gyro noise reduction, among which LSTM-GRU network shows the best noise reduction effect and great potential for application in the MEMS gyroscope area.},
DOI = {10.3390/mi12020214}
}



@Article{app11041861,
AUTHOR = {Rong, Zihao and Wang, Shaofan and Kong, Dehui and Yin, Baocai},
TITLE = {A Cascaded Ensemble of Sparse-and-Dense Dictionaries for Vehicle Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1861},
URL = {https://www.mdpi.com/2076-3417/11/4/1861},
ISSN = {2076-3417},
ABSTRACT = {Vehicle detection as a special case of object detection has practical meaning but faces challenges, such as the difficulty of detecting vehicles of various orientations, the serious influence from occlusion, the clutter of background, etc. In addition, existing effective approaches, like deep-learning-based ones, demand a large amount of training time and data, which causes trouble for their application. In this work, we propose a dictionary-learning-based vehicle detection approach which explicitly addresses these problems. Specifically, an ensemble of sparse-and-dense dictionaries (ESDD) are learned through supervised low-rank decomposition; each pair of sparse-and-dense dictionaries (SDD) in the ensemble is trained to represent either a subcategory of vehicle (corresponding to certain orientation range or occlusion level) or a subcategory of background (corresponding to a cluster of background patterns) and only gives good reconstructions to samples of the corresponding subcategory, making the ESDD capable of classifying vehicles from background even though they exhibit various appearances. We further organize ESDD into a two-level cascade (CESDD) to perform coarse-to-fine two-stage classification for better performance and computation reduction. The CESDD is then coupled with a downstream AdaBoost process to generate robust classifications. The proposed CESDD model is used as a window classifier in a sliding-window scan process over image pyramids to produce multi-scale detections, and an adapted mean-shift-like non-maximum suppression process is adopted to remove duplicate detections. Our CESDD vehicle detection approach is evaluated on KITTI dataset and compared with other strong counterparts; the experimental results exhibit the effectiveness of CESDD-based classification and detection, and the training of CESDD only demands small amount of time and data.},
DOI = {10.3390/app11041861}
}



@Article{s21041492,
AUTHOR = {Li, Guoming and Huang, Yanbo and Chen, Zhiqian and Chesser, Gary D. and Purswell, Joseph L. and Linhoss, John and Zhao, Yang},
TITLE = {Practices and Applications of Convolutional Neural Network-Based Computer Vision Systems in Animal Farming: A Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1492},
URL = {https://www.mdpi.com/1424-8220/21/4/1492},
PubMedID = {33670030},
ISSN = {1424-8220},
ABSTRACT = {Convolutional neural network (CNN)-based computer vision systems have been increasingly applied in animal farming to improve animal management, but current knowledge, practices, limitations, and solutions of the applications remain to be expanded and explored. The objective of this study is to systematically review applications of CNN-based computer vision systems on animal farming in terms of the five deep learning computer vision tasks: image classification, object detection, semantic/instance segmentation, pose estimation, and tracking. Cattle, sheep/goats, pigs, and poultry were the major farm animal species of concern. In this research, preparations for system development, including camera settings, inclusion of variations for data recordings, choices of graphics processing units, image preprocessing, and data labeling were summarized. CNN architectures were reviewed based on the computer vision tasks in animal farming. Strategies of algorithm development included distribution of development data, data augmentation, hyperparameter tuning, and selection of evaluation metrics. Judgment of model performance and performance based on architectures were discussed. Besides practices in optimizing CNN-based computer vision systems, system applications were also organized based on year, country, animal species, and purposes. Finally, recommendations on future research were provided to develop and improve CNN-based computer vision systems for improved welfare, environment, engineering, genetics, and management of farm animals.},
DOI = {10.3390/s21041492}
}



@Article{rs13040808,
AUTHOR = {Neupane, Bipul and Horanont, Teerayut and Aryal, Jagannath},
TITLE = {Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {808},
URL = {https://www.mdpi.com/2072-4292/13/4/808},
ISSN = {2072-4292},
ABSTRACT = {Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.},
DOI = {10.3390/rs13040808}
}



@Article{app11041950,
AUTHOR = {Qi, Haixia and Liang, Yu and Ding, Quanchen and Zou, Jun},
TITLE = {Automatic Identification of Peanut-Leaf Diseases Based on Stack Ensemble},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1950},
URL = {https://www.mdpi.com/2076-3417/11/4/1950},
ISSN = {2076-3417},
ABSTRACT = {Peanut is an important food crop, and diseases of its leaves can directly reduce its yield and quality. In order to solve the problem of automatic identification of peanut-leaf diseases, this paper uses a traditional machine-learning method to ensemble the output of a deep learning model to identify diseases of peanut leaves. The identification of peanut-leaf diseases included healthy leaves, rust disease on a single leaf, leaf-spot disease on a single leaf, scorch disease on a single leaf, and both rust disease and scorch disease on a single leaf. Three types of data-augmentation methods were used: image flipping, rotation, and scaling. In this experiment, the deep-learning model had a higher accuracy than the traditional machine-learning methods. Moreover, the deep-learning model achieved better performance when using data augmentation and a stacking ensemble. After ensemble by logistic regression, the accuracy of residual network with 50 layers (ResNet50) was as high as 97.59%, and the F1 score of dense convolutional network with 121 layers (DenseNet121) was as high as 90.50. The deep-learning model used in this experiment had the greatest improvement in F1 score after the logistic regression ensemble. Deep-learning networks with deeper network layers like ResNet50 and DenseNet121 performed better in this experiment. This study can provide a reference for the identification of peanut-leaf diseases.},
DOI = {10.3390/app11041950}
}



@Article{land10020223,
AUTHOR = {Binte Mostafiz, Rubaiya and Noguchi, Ryozo and Ahamed, Tofael},
TITLE = {Agricultural Land Suitability Assessment Using Satellite Remote Sensing-Derived Soil-Vegetation Indices},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {223},
URL = {https://www.mdpi.com/2073-445X/10/2/223},
ISSN = {2073-445X},
ABSTRACT = {Satellite remote sensing technologies have a high potential in applications for evaluating land conditions and can facilitate optimized planning for agricultural sectors. However, misinformed land selection decisions limit crop yields and increase production-related costs to farmers. Therefore, the purpose of this research was to develop a land suitability assessment system using satellite remote sensing-derived soil-vegetation indicators. A multicriteria decision analysis was conducted by integrating weighted linear combinations and fuzzy multicriteria analyses in a GIS platform for suitability assessment using the following eight criteria: elevation, slope, and LST vegetation indices (SAVI, ARVI, SARVI, MSAVI, and OSAVI). The relative priorities of the indicators were identified using a fuzzy expert system. Furthermore, the results of the land suitability assessment were evaluated by ground truthed yield data. In addition, a yield estimation method was developed using indices representing influential factors. The analysis utilizing equal weights showed that 43% of the land (1832 km2) was highly suitable, 41% of the land (1747 km2) was moderately suitable, and 10% of the land (426 km2) was marginally suitable for improved yield productions. Alternatively, expert knowledge was also considered, along with references, when using the fuzzy membership function; as a result, 48% of the land (2045 km2) was identified as being highly suitable; 39% of the land (2045 km2) was identified as being moderately suitable, and 7% of the land (298 km2) was identified as being marginally suitable. Additionally, 6% (256 km2) of the land was described as not suitable by both methods. Moreover, the yield estimation using SAVI (R2 = 77.3%), ARVI (R2 = 68.9%), SARVI (R2 = 71.1%), MSAVI (R2 = 74.5%) and OSAVI (R2 = 81.2%) showed a good predictive ability. Furthermore, the combined model using these five indices reported the highest accuracy (R2 = 0.839); this model was then applied to develop yield prediction maps for the corresponding years (2017–2020). This research suggests that satellite remote sensing methods in GIS platforms are an effective and convenient way for agricultural land-use planners and land policy makers to select suitable cultivable land areas with potential for increased agricultural production.},
DOI = {10.3390/land10020223}
}



@Article{rs13040814,
AUTHOR = {Megahed, Yasmine and Shaker, Ahmed and Yan, Wai Yeung},
TITLE = {Fusion of Airborne LiDAR Point Clouds and Aerial Images for Heterogeneous Land-Use Urban Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {814},
URL = {https://www.mdpi.com/2072-4292/13/4/814},
ISSN = {2072-4292},
ABSTRACT = {The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors.},
DOI = {10.3390/rs13040814}
}



@Article{s21051571,
AUTHOR = {Bonci, Andrea and Cen Cheng, Pangcheng  David and Indri, Marina and Nabissi, Giacomo and Sibona, Fiorella},
TITLE = {Human-Robot Perception in Industrial Environments: A Survey},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1571},
URL = {https://www.mdpi.com/1424-8220/21/5/1571},
PubMedID = {33668162},
ISSN = {1424-8220},
ABSTRACT = {Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.},
DOI = {10.3390/s21051571}
}



@Article{rs13050837,
AUTHOR = {Bajić, Milan and Bajić, Milan},
TITLE = {Modeling and Simulation of Very High Spatial Resolution UXOs and Landmines in a Hyperspectral Scene for UAV Survey},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {837},
URL = {https://www.mdpi.com/2072-4292/13/5/837},
ISSN = {2072-4292},
ABSTRACT = {This paper presents methods for the modeling and simulation of explosive target placement in terrain spectral images (i.e., real hyperspectral 90-channel VNIR data), considering unexploded ordnances, landmines, and improvised explosive devices. The models used for landmine detection operate at sub-pixel levels. The presented research uses very fine spatial resolutions, 0.945 × 0.945 mm for targets and 1.868 × 1.868 cm for the scene, where the number of target pixels ranges from 52 to 116. While previous research has used the mean spectral value of the target, it is omitted in this paper. The model considers the probability of detection and its confidence intervals, which are derived and used in the analysis of the considered explosive targets. The detection results are better when decreased target endmembers are used to match the scene resolution, rather than using endmembers at the full resolution of the target. Unmanned aerial vehicles, as carriers of snapshot hyperspectral cameras, enable flexible target resolution selection and good area coverage.},
DOI = {10.3390/rs13050837}
}



@Article{electronics10050543,
AUTHOR = {Jung, Soyi and Yun, Won Joon and Kim, Joongheon and Kim, Jae-Hyun},
TITLE = {Coordinated Multi-Agent Deep Reinforcement Learning for Energy-Aware UAV-Based Big-Data Platforms},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {543},
URL = {https://www.mdpi.com/2079-9292/10/5/543},
ISSN = {2079-9292},
ABSTRACT = {This paper proposes a novel coordinated multi-agent deep reinforcement learning (MADRL) algorithm for energy sharing among multiple unmanned aerial vehicles (UAVs) in order to conduct big-data processing in a distributed manner. For realizing UAV-assisted aerial surveillance or flexible mobile cellular services, robust wireless charging mechanisms are essential for delivering energy sources from charging towers (i.e., charging infrastructure) to their associated UAVs for seamless operations of autonomous UAVs in the sky. In order to actively and intelligently manage the energy resources in charging towers, a MADRL-based coordinated energy management system is desired and proposed for energy resource sharing among charging towers. When the required energy for charging UAVs is not enough in charging towers, the energy purchase from utility company (i.e., energy source provider in local energy market) is desired, which takes high costs. Therefore, the main objective of our proposed coordinated MADRL-based energy sharing learning algorithm is minimizing energy purchase from external utility companies to minimize system-operational costs. Finally, our performance evaluation results verify that the proposed coordinated MADRL-based algorithm achieves desired performance improvements.},
DOI = {10.3390/electronics10050543}
}



@Article{s21051617,
AUTHOR = {Safonova, Anastasiia and Guirado, Emilio and Maglinets, Yuriy and Alcaraz-Segura, Domingo and Tabik, Siham},
TITLE = {Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask R-CNN},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1617},
URL = {https://www.mdpi.com/1424-8220/21/5/1617},
PubMedID = {33668984},
ISSN = {1424-8220},
ABSTRACT = {Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.},
DOI = {10.3390/s21051617}
}



@Article{s21051631,
AUTHOR = {Martini, Bruno Guilherme and Helfer, Gilson Augusto and Barbosa, Jorge Luis Victória and Espinosa Modolo, Regina Célia and da Silva, Marcio Rosa and de Figueiredo, Rodrigo Marques and Mendes, André Sales and Silva, Luís Augusto and Leithardt, Valderi Reis Quietinho},
TITLE = {IndoorPlant: A Model for Intelligent Services in Indoor Agriculture Based on Context Histories},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1631},
URL = {https://www.mdpi.com/1424-8220/21/5/1631},
PubMedID = {33652603},
ISSN = {1424-8220},
ABSTRACT = {The application of ubiquitous computing has increased in recent years, especially due to the development of technologies such as mobile computing, more accurate sensors, and specific protocols for the Internet of Things (IoT). One of the trends in this area of research is the use of context awareness. In agriculture, the context involves the environment, for example, the conditions found inside a greenhouse. Recently, a series of studies have proposed the use of sensors to monitor production and/or the use of cameras to obtain information about cultivation, providing data, reminders, and alerts to farmers. This article proposes a computational model for indoor agriculture called IndoorPlant. The model uses the analysis of context histories to provide intelligent generic services, such as predicting productivity, indicating problems that cultivation may suffer, and giving suggestions for improvements in greenhouse parameters. IndoorPlant was tested in three scenarios of the daily life of farmers with hydroponic production data that were obtained during seven months of cultivation of radicchio, lettuce, and arugula. Finally, the article presents the results obtained through intelligent services that use context histories. The scenarios used services to recommend improvements in cultivation, profiles and, finally, prediction of the cultivation time of radicchio, lettuce, and arugula using the partial least squares (PLS) regression technique. The prediction results were relevant since the following values were obtained: 0.96 (R2, coefficient of determination), 1.06 (RMSEC, square root of the mean square error of calibration), and 1.94 (RMSECV, square root of the mean square error of cross validation) for radicchio; 0.95 (R2), 1.37 (RMSEC), and 3.31 (RMSECV) for lettuce; 0.93 (R2), 1.10 (RMSEC), and 1.89 (RMSECV) for arugula. Eight farmers with different functions on the farm filled out a survey based on the technology acceptance model (TAM). The results showed 92% acceptance regarding utility and 98% acceptance for ease of use.},
DOI = {10.3390/s21051631}
}



@Article{app11052105,
AUTHOR = {Papić, Vladan and Šolić, Petar and Milan, Ante and Gotovac, Sven and Polić, Miljenko},
TITLE = {High-Resolution Image Transmission from UAV to Ground Station for Search and Rescue Missions Planning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2105},
URL = {https://www.mdpi.com/2076-3417/11/5/2105},
ISSN = {2076-3417},
ABSTRACT = {Search and rescue (SAR) missions comprise search for, and provision of aid to people who are in distress or imminent danger. Providing the best possible input for the planners and search teams, up-to-date information about the terrain is of essential importance because every additional hour needed to search a person decreases probability of success. Therefore, availability of aerial images and updated terrain maps as a basis for planning and monitoring SAR missions in real-time is very important for rescuers. In this paper, we present a system for transmission of high-resolution images from an unmanned aerial vehicle (UAV) to the ground station (GS). We define and calculate data rate and transmission distance requirements between the UAV and GS in a mission scenario. Five tests were designed and carried out to confirm the viability of the proposed system architecture and modules. Test results present throughput measurements for various UAV and GS distances, antenna heights and UAV antenna yaw angles. Experimental results from the series of conducted outdoor tests show that the proposed solution using two pMDDL2450 datalinks at 2.4 GHz and a directional antenna on the receiving side can be used for a real-time transmission of high-resolution images acquired with a camera on a UAV. Achieved throughput at a UAV-GS distance of 5 km was 1.4 MB/s (11.2 Mbps). The limitations and possible improvements of the proposed system as well as future work are also discussed.},
DOI = {10.3390/app11052105}
}



@Article{rs13050898,
AUTHOR = {Sadeghi-Tehran, Pouria and Virlet, Nicolas and Hawkesford, Malcolm J.},
TITLE = {A Neural Network Method for Classification of Sunlit and Shaded Components of Wheat Canopies in the Field Using High-Resolution Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {898},
URL = {https://www.mdpi.com/2072-4292/13/5/898},
ISSN = {2072-4292},
ABSTRACT = {(1) Background: Information rich hyperspectral sensing, together with robust image analysis, is providing new research pathways in plant phenotyping. This combination facilitates the acquisition of spectral signatures of individual plant organs as well as providing detailed information about the physiological status of plants. Despite the advances in hyperspectral technology in field-based plant phenotyping, little is known about the characteristic spectral signatures of shaded and sunlit components in wheat canopies. Non-imaging hyperspectral sensors cannot provide spatial information; thus, they are not able to distinguish the spectral reflectance differences between canopy components. On the other hand, the rapid development of high-resolution imaging spectroscopy sensors opens new opportunities to investigate the reflectance spectra of individual plant organs which lead to the understanding of canopy biophysical and chemical characteristics. (2) Method: This study reports the development of a computer vision pipeline to analyze ground-acquired imaging spectrometry with high spatial and spectral resolutions for plant phenotyping. The work focuses on the critical steps in the image analysis pipeline from pre-processing to the classification of hyperspectral images. In this paper, two convolutional neural networks (CNN) are employed to automatically map wheat canopy components in shaded and sunlit regions and to determine their specific spectral signatures. The first method uses pixel vectors of the full spectral features as inputs to the CNN model and the second method integrates the dimension reduction technique known as linear discriminate analysis (LDA) along with the CNN to increase the feature discrimination and improves computational efficiency. (3) Results: The proposed technique alleviates the limitations and lack of separability inherent in existing pre-defined hyperspectral classification methods. It optimizes the use of hyperspectral imaging and ensures that the data provide information about the spectral characteristics of the targeted plant organs, rather than the background. We demonstrated that high-resolution hyperspectral imagery along with the proposed CNN model can be powerful tools for characterizing sunlit and shaded components of wheat canopies in the field. The presented method will provide significant advances in the determination and relevance of spectral properties of shaded and sunlit canopy components under natural light conditions.},
DOI = {10.3390/rs13050898}
}



@Article{s21051682,
AUTHOR = {Tai, Kuan-Chen and Tang, Chih-Wei},
TITLE = {Siamese Networks-Based People Tracking Using Template Update for 360-Degree Videos Using EAC Format},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1682},
URL = {https://www.mdpi.com/1424-8220/21/5/1682},
PubMedID = {33804396},
ISSN = {1424-8220},
ABSTRACT = {Rich information is provided by 360-degree videos. However, non-uniform geometric deformation caused by sphere-to-plane projection significantly decreases tracking accuracy of existing trackers, and the huge amount of data makes it difficult to achieve real-time tracking. Thus, this paper proposes a Siamese networks-based people tracker using template update for 360-degree equi-angular cubemap (EAC) format videos. Face stitching overcomes the problem of content discontinuity of the EAC format and avoids raising new geometric deformation in stitched images. Fully convolutional Siamese networks enable tracking at high speed. Mostly important, to be robust against combination of non-uniform geometric deformation of the EAC format and partial occlusions caused by zero padding in stitched images, this paper proposes a novel Bayes classifier-based timing detector of template update by referring to the linear discriminant feature and statistics of a score map generated by Siamese networks. Experimental results show that the proposed scheme significantly improves tracking accuracy of the fully convolutional Siamese networks SiamFC on the EAC format with operation beyond the frame acquisition rate. Moreover, the proposed score map-based timing detector of template update outperforms state-of-the-art score map-based timing detectors.},
DOI = {10.3390/s21051682}
}



@Article{app11052163,
AUTHOR = {Munaye, Yirga Yayeh and Juang, Rong-Terng and Lin, Hsin-Piao and Tarekegn, Getaneh Berie and Lin, Ding-Bing},
TITLE = {Deep Reinforcement Learning Based Resource Management in UAV-Assisted IoT Networks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2163},
URL = {https://www.mdpi.com/2076-3417/11/5/2163},
ISSN = {2076-3417},
ABSTRACT = {The resource management in wireless networks with massive Internet of Things (IoT) users is one of the most crucial issues for the advancement of fifth-generation networks. The main objective of this study is to optimize the usage of resources for IoT networks. Firstly, the unmanned aerial vehicle is considered to be a base station for air-to-ground communications. Secondly, according to the distribution and fluctuation of signals; the IoT devices are categorized into urban and suburban clusters. This clustering helps to manage the environment easily. Thirdly, real data collection and preprocessing tasks are carried out. Fourthly, the deep reinforcement learning approach is proposed as a main system development scheme for resource management. Fifthly, K-means and round-robin scheduling algorithms are applied for clustering and managing the users’ resource requests, respectively. Then, the TensorFlow (python) programming tool is used to test the overall capability of the proposed method. Finally, this paper evaluates the proposed approach with related works based on different scenarios. According to the experimental findings, our proposed scheme shows promising outcomes. Moreover, on the evaluation tasks, the outcomes show rapid convergence, suitable for heterogeneous IoT networks, and low complexity.},
DOI = {10.3390/app11052163}
}



@Article{s21051688,
AUTHOR = {Ali, Luqman and Alnajjar, Fady and Jassmi, Hamad Al and Gocho, Munkhjargal and Khan, Wasif and Serhani, M. Adel},
TITLE = {Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1688},
URL = {https://www.mdpi.com/1424-8220/21/5/1688},
PubMedID = {33804490},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.},
DOI = {10.3390/s21051688}
}



@Article{app11052185,
AUTHOR = {Nakama, Justin and Parada, Ricky and Matos-Carvalho, João P. and Azevedo, Fábio and Pedro, Dário and Campos, Luís},
TITLE = {Autonomous Environment Generator for UAV-Based Simulation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2185},
URL = {https://www.mdpi.com/2076-3417/11/5/2185},
ISSN = {2076-3417},
ABSTRACT = {The increased demand for Unmanned Aerial Vehicles (UAV) has also led to higher demand for realistic and efficient UAV testing environments. The current use of simulated environments has been shown to be a relatively inexpensive, safe, and repeatable way to evaluate UAVs before real-world use. However, the use of generic environments and manually-created custom scenarios leaves more to be desired. In this paper, we propose a new testbed that utilizes machine learning algorithms to procedurally generate, scale, and place 3D models to create a realistic environment. These environments are additionally based on satellite images, thus providing users with a more robust example of real-world UAV deployment. Although certain graphical improvements could be made, this paper serves as a proof of concept for an novel autonomous and relatively-large scale environment generator. Such a testbed could allow for preliminary operational planning and testing worldwide, without the need for on-site evaluation or data collection in the future.},
DOI = {10.3390/app11052185}
}



@Article{rs13050937,
AUTHOR = {Najafi, Payam and Feizizadeh, Bakhtiar and Navid, Hossein},
TITLE = {A Comparative Approach of Fuzzy Object Based Image Analysis and Machine Learning Techniques Which Are Applied to Crop Residue Cover Mapping by Using Sentinel-2 Satellite and UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {937},
URL = {https://www.mdpi.com/2072-4292/13/5/937},
ISSN = {2072-4292},
ABSTRACT = {Conservation tillage methods through leaving the crop residue cover (CRC) on the soil surface protect it from water and wind erosions. Hence, the percentage of the CRC on the soil surface is very critical for the evaluation of tillage intensity. The objective of this study was to develop a new methodology based on the semiautomated fuzzy object based image analysis (fuzzy OBIA) and compare its efficiency with two machine learning algorithms which include: support vector machine (SVM) and artificial neural network (ANN) for the evaluation of the previous CRC and tillage intensity. We also considered the spectral images from two remotely sensed platforms of the unmanned aerial vehicle (UAV) and Sentinel-2 satellite, respectively. The results indicated that fuzzy OBIA for multispectral Sentinel-2 image based on Gaussian membership function with overall accuracy and Cohen’s kappa of 0.920 and 0.874, respectively, surpassed machine learning algorithms and represented the useful results for the classification of tillage intensity. The results also indicated that overall accuracy and Cohen’s kappa for the classification of RGB images from the UAV using fuzzy OBIA method were 0.860 and 0.779, respectively. The semiautomated fuzzy OBIA clearly outperformed machine learning approaches in estimating the CRC and the classification of the tillage methods and also it has the potential to substitute or complement field techniques.},
DOI = {10.3390/rs13050937}
}



@Article{rs13050939,
AUTHOR = {Xue, Yongan and Zhao, Jinling and Zhang, Mingmei},
TITLE = {A Watershed-Segmentation-Based Improved Algorithm for Extracting Cultivated Land Boundaries},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {939},
URL = {https://www.mdpi.com/2072-4292/13/5/939},
ISSN = {2072-4292},
ABSTRACT = {To accurately extract cultivated land boundaries based on high-resolution remote sensing imagery, an improved watershed segmentation algorithm was proposed herein based on a combination of pre- and post-improvement procedures. Image contrast enhancement was used as the pre-improvement, while the color distance of the Commission Internationale de l´Eclairage (CIE) color space, including the Lab and Luv, was used as the regional similarity measure for region merging as the post-improvement. Furthermore, the area relative error criterion (δA), the pixel quantity error criterion (δP), and the consistency criterion (Khat) were used for evaluating the image segmentation accuracy. The region merging in Red–Green–Blue (RGB) color space was selected to compare the proposed algorithm by extracting cultivated land boundaries. The validation experiments were performed using a subset of Chinese Gaofen-2 (GF-2) remote sensing image with a coverage area of 0.12 km2. The results showed the following: (1) The contrast-enhanced image exhibited an obvious gain in terms of improving the image segmentation effect and time efficiency using the improved algorithm. The time efficiency increased by 10.31%, 60.00%, and 40.28%, respectively, in the RGB, Lab, and Luv color spaces. (2) The optimal segmentation and merging scale parameters in the RGB, Lab, and Luv color spaces were C for minimum areas of 2000, 1900, and 2000, and D for a color difference of 1000, 40, and 40. (3) The algorithm improved the time efficiency of cultivated land boundary extraction in the Lab and Luv color spaces by 35.16% and 29.58%, respectively, compared to the RGB color space. The extraction accuracy was compared to the RGB color space using the δA, δP, and Khat, that were improved by 76.92%, 62.01%, and 16.83%, respectively, in the Lab color space, while they were 55.79%, 49.67%, and 13.42% in the Luv color space. (4) Through the visual comparison, time efficiency, and segmentation accuracy, the comprehensive extraction effect using the proposed algorithm was obviously better than that of RGB color-based space algorithm. The established accuracy evaluation indicators were also proven to be consistent with the visual evaluation. (5) The proposed method has a satisfying transferability by a wider test area with a coverage area of 1 km2. In addition, the proposed method, based on the image contrast enhancement, was to perform the region merging in the CIE color space according to the simulated immersion watershed segmentation results. It is a useful attempt for the watershed segmentation algorithm to extract cultivated land boundaries, which provides a reference for enhancing the watershed algorithm.},
DOI = {10.3390/rs13050939}
}



@Article{s21051766,
AUTHOR = {Müezzinoğlu, Taha and Karaköse, Mehmet},
TITLE = {An Intelligent Human–Unmanned Aerial Vehicle Interaction Approach in Real Time Based on Machine Learning Using Wearable Gloves},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1766},
URL = {https://www.mdpi.com/1424-8220/21/5/1766},
PubMedID = {33806388},
ISSN = {1424-8220},
ABSTRACT = {The interactions between humans and unmanned aerial vehicles (UAVs), whose applications are increasing in the civilian field rather than for military purposes, are a popular future research area. Human–UAV interactions are a challenging problem because UAVs move in a three-dimensional space. In this paper, we present an intelligent human–UAV interaction approach in real time based on machine learning using wearable gloves. The proposed approach offers scientific contributions such as a multi-mode command structure, machine-learning-based recognition, task scheduling algorithms, real-time usage, robust and effective use, and high accuracy rates. For this purpose, two wearable smart gloves working in real time were designed. The signal data obtained from the gloves were processed with machine-learning-based methods and classified multi-mode commands were included in the human–UAV interaction process via the interface according to the task scheduling algorithm to facilitate sequential and fast operation. The performance of the proposed approach was verified on a data set created using 25 different hand gestures from 20 different people. In a test using the proposed approach on 49,000 datapoints, process time performance of a few milliseconds was achieved with approximately 98 percent accuracy.},
DOI = {10.3390/s21051766}
}



@Article{rs13050977,
AUTHOR = {Crusiol, Luís Guilherme Teixeira and Nanni, Marcos Rafael and Furlanetto, Renato Herrig and Sibaldelli, Rubson Natal Ribeiro and Cezar, Everson and Sun, Liang and Foloni, José Salvador Simonetto and Mertz-Henning, Liliane Marcia and Nepomuceno, Alexandre Lima and Neumaier, Norman and Farias, José Renato Bouças},
TITLE = {Yield Prediction in Soybean Crop Grown under Different Levels of Water Availability Using Reflectance Spectroscopy and Partial Least Squares Regression},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {977},
URL = {https://www.mdpi.com/2072-4292/13/5/977},
ISSN = {2072-4292},
ABSTRACT = {Soybean grain yield has regularly been impaired by drought periods, and the future climatic scenarios for soybean production might drastically impact yields worldwide. In this context, the knowledge of soybean yield is extremely important to subsidize government and corporative decisions over technical issues. This paper aimed to predict grain yield in soybean crop grown under different levels of water availability using reflectance spectroscopy and partial least square regression (PLSR). Field experiments were undertaken at Embrapa Soja (Brazilian Agricultural Research Corporation) in the 2016/2017, 2017/2018 and 2018/2019 cropping seasons. The data collected were analyzed following a split plot model in a randomized complete block design, with four blocks. The following water conditions were distributed in the field plots: irrigated (IRR), non-irrigated (NIRR) and water deficit induced at the vegetative (WDV) and reproductive stages (WDR) using rainout shelters. Soybean genotypes with different responses to water deficit were distributed in the subplots. Soil moisture and weather data were monitored daily. A total of 7216 leaf reflectance (from 400 to 2500 nm, measured by the FieldSpec 3 Jr spectroradiometer) was collected at 24 days in the three cropping seasons. The PLSR (p ≤ 0.05) was performed to predict soybean grain yield by its leaf-based reflectance spectroscopy. The results demonstrated the highest accuracy in soybean grain yield prediction at the R5 phenological stage, corresponding to the period when grains are being formed (R2 ranging from 0.731 to 0.924 and the RMSE from 334 to 403 kg ha−1—7.77 to 11.33%). Analyzing the three cropping seasons into a single PLSR model at R5 stage, R2 equal to 0.775, 0.730 and 0.688 were obtained at the calibration, cross-validation and external validation stages, with RMSE lower than 634 kg ha−1 (13.34%). The PLSR demonstrated higher accuracy in plants submitted to water deficit both at the vegetative and reproductive periods in comparison to plants under natural rainfall or irrigation.},
DOI = {10.3390/rs13050977}
}



@Article{s21051809,
AUTHOR = {Malhotra, Parushi and Singh, Yashwant and Anand, Pooja and Bangotra, Deep Kumar and Singh, Pradeep Kumar and Hong, Wei-Chiang},
TITLE = {Internet of Things: Evolution, Concerns and Security Challenges},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1809},
URL = {https://www.mdpi.com/1424-8220/21/5/1809},
PubMedID = {33807724},
ISSN = {1424-8220},
ABSTRACT = {The escalated growth of the Internet of Things (IoT) has started to reform and reshape our lives. The deployment of a large number of objects adhered to the internet has unlocked the vision of the smart world around us, thereby paving a road towards automation and humongous data generation and collection. This automation and continuous explosion of personal and professional information to the digital world provides a potent ground to the adversaries to perform numerous cyber-attacks, thus making security in IoT a sizeable concern. Hence, timely detection and prevention of such threats are pre-requisites to prevent serious consequences. The survey conducted provides a brief insight into the technology with prime attention towards the various attacks and anomalies and their detection based on the intelligent intrusion detection system (IDS). The comprehensive look-over presented in this paper provides an in-depth analysis and assessment of diverse machine learning and deep learning-based network intrusion detection system (NIDS). Additionally, a case study of healthcare in IoT is presented. The study depicts the architecture, security, and privacy issues and application of learning paradigms in this sector. The research assessment is finally concluded by listing the results derived from the literature. Additionally, the paper discusses numerous research challenges to allow further rectifications in the approaches to deal with unusual complications.},
DOI = {10.3390/s21051809}
}



@Article{app11052347,
AUTHOR = {Solis, Jorge and Karlsson, Christoffer and Johansson, Simon and Richardsson, Kristoffer},
TITLE = {Towards the Development of an Automatic UAV-Based Indoor Environmental Monitoring System: Distributed Off-Board Control System for a Micro Aerial Vehicle},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2347},
URL = {https://www.mdpi.com/2076-3417/11/5/2347},
ISSN = {2076-3417},
ABSTRACT = {This research aims to develop an automatic unmanned aerial vehicle (UAV)-based indoor environmental monitoring system for the acquisition of data at a very fine scale to detect rapid changes in environmental features of plants growing in greenhouses. Due to the complexity of the proposed research, in this paper we proposed an off-board distributed control system based on visual input for a micro aerial vehicle (MAV) able to hover, navigate, and fly to a desired target location without considerably affecting the effective flight time. Based on the experimental results, the MAV was able to land on the desired location within a radius of about 10 cm from the center point of the landing pad, with a reduction in the effective flight time of about 28%.},
DOI = {10.3390/app11052347}
}



@Article{ijgi10030144,
AUTHOR = {Gebrehiwot, Asmamaw A and Hashemi-Beni, Leila},
TITLE = {Three-Dimensional Inundation Mapping Using UAV Image Segmentation and Digital Surface Model},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {144},
URL = {https://www.mdpi.com/2220-9964/10/3/144},
ISSN = {2220-9964},
ABSTRACT = {Flood occurrence is increasing due to the expansion of urbanization and extreme weather like hurricanes; hence, research on methods of inundation monitoring and mapping has increased to reduce the severe impacts of flood disasters. This research studies and compares two methods for inundation depth estimation using UAV images and topographic data. The methods consist of three main stages: (1) extracting flooded areas and create 2D inundation polygons using deep learning; (2) reconstructing 3D water surface using the polygons and topographic data; and (3) deriving a water depth map using the 3D reconstructed water surface and a pre-flood DEM. The two methods are different at reconstructing the 3D water surface (stage 2). The first method uses structure from motion (SfM) for creating a point cloud of the area from overlapping UAV images, and the water polygons resulted from stage 1 is applied for water point cloud classification. While the second method reconstructs the water surface by intersecting the water polygons and a pre-flood DEM created using the pre-flood LiDAR data. We evaluate the proposed methods for inundation depth mapping over the Town of Princeville during a flooding event during Hurricane Matthew. The methods are compared and validated using the USGS gauge water level data acquired during the flood event. The RMSEs for water depth using the SfM method and integrated method based on deep learning and DEM were 0.34m and 0.26m, respectively.},
DOI = {10.3390/ijgi10030144}
}



@Article{su13052905,
AUTHOR = {Zhao, Wei and Li, Tianxin and Qi, Bozhao and Nie, Qifan and Runge, Troy},
TITLE = {Terrain Analytics for Precision Agriculture with Automated Vehicle Sensors and Data Fusion},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2905},
URL = {https://www.mdpi.com/2071-1050/13/5/2905},
ISSN = {2071-1050},
ABSTRACT = {Precision agriculture aims to use minimal inputs to generate maximal yields by managing the plant and its environment at a discrete instead of a field level. This new farming methodology requires localized field data including topological terrain attributes, which influence irrigation, field moisture, nutrient runoff, soil compaction, and traction and stability for traversing agriculture machines. Existing research studies have used different sensors, such as distance sensors and cameras, to collect topological information, which may be constrained by energy cost, performance, price, etc. This study proposed a low-cost method to perform farmland topological analytics using sensor implementation and data processing. Inertial measurement unit sensors, which are widely used in automated vehicle study, and a camera are set up on a robot vehicle. Then experiments are conducted under indoor simulated environments that include five common topographies that would be encountered on farms, combined with validation experiments in a real-world field. A data fusion approach was developed and implemented to track robot vehicle movements, monitor the surrounding environment, and finally recognize the topography type in real time. The resulting method was able to clearly recognize topography changes. This low-cost and easy-mount method will be able to augment and calibrate existing mapping algorithms with multidimensional information. Practically, it can also achieve immediate improvement for the operation and path planning of large agricultural machines.},
DOI = {10.3390/su13052905}
}



@Article{app11052408,
AUTHOR = {Oñate-López, José and Navarro, Loraine and Quintero M., Christian G. and Pardo, Mauricio},
TITLE = {Intelligent Exploration Approaches Based on Utility Functions Optimization for Multi-Agent Environment Applications},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2408},
URL = {https://www.mdpi.com/2076-3417/11/5/2408},
ISSN = {2076-3417},
ABSTRACT = {In this work, the problem of exploring an unknown environment with a team of agents and search different targets on it is considered. The key problem to be solved in multiple agents is choosing appropriate target points for the individual agents to simultaneously explore different regions of the environment. An intelligent approach is presented to coordinate several agents using a market-based model to identify the appropriate task for each agent. It is proposed to compare the fitting of the market utility function using neural networks and optimize this function using genetic algorithms to avoid heavy computation in the Non-Polynomial (NP: nondeterministic polynomial time) path-planning problem. An indoor environment inspires the proposed approach with homogeneous physical agents, and its performance is tested in simulations. The results show that the proposed approach allocates agents effectively to the environment and enables them to carry out their mission quickly.},
DOI = {10.3390/app11052408}
}



@Article{app11062458,
AUTHOR = {Roberts, Ronald and Inzerillo, Laura and Di Mino, Gaetano},
TITLE = {Exploiting Data Analytics and Deep Learning Systems to Support Pavement Maintenance Decisions},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2458},
URL = {https://www.mdpi.com/2076-3417/11/6/2458},
ISSN = {2076-3417},
ABSTRACT = {Road networks are critical infrastructures within any region and it is imperative to maintain their conditions for safe and effective movement of goods and services. Road Management, therefore, plays a key role to ensure consistent efficient operation. However, significant resources are required to perform necessary maintenance activities to achieve and maintain high levels of service. Pavement maintenance can typically be very expensive and decisions are needed concerning planning and prioritizing interventions. Data are key towards enabling adequate maintenance planning but in many instances, there is limited available information especially in small or under-resourced urban road authorities. This study develops a roadmap to help these authorities by using flexible data analysis and deep learning computational systems to highlight important factors within road networks, which are used to construct models that can help predict future intervention timelines. A case study in Palermo, Italy was successfully developed to demonstrate how the techniques could be applied to perform appropriate feature selection and prediction models based on limited data sources. The workflow provides a pathway towards more effective pavement maintenance management practices using techniques that can be readily adapted based on different environments. This takes another step towards automating these practices within the pavement management system.},
DOI = {10.3390/app11062458}
}



@Article{rs13061053,
AUTHOR = {Stathopoulou, Elisavet Konstantina and Battisti, Roberto and Cernea, Dan and Remondino, Fabio and Georgopoulos, Andreas},
TITLE = {Semantically Derived Geometric Constraints for MVS Reconstruction of Textureless Areas},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1053},
URL = {https://www.mdpi.com/2072-4292/13/6/1053},
ISSN = {2072-4292},
ABSTRACT = {Conventional multi-view stereo (MVS) approaches based on photo-consistency measures are generally robust, yet often fail in calculating valid depth pixel estimates in low textured areas of the scene. In this study, a novel approach is proposed to tackle this challenge by leveraging semantic priors into a PatchMatch-based MVS in order to increase confidence and support depth and normal map estimation. Semantic class labels on image pixels are used to impose class-specific geometric constraints during multiview stereo, optimising the depth estimation on weakly supported, textureless areas, commonly present in urban scenarios of building facades, indoor scenes, or aerial datasets. Detecting dominant shapes, e.g., planes, with RANSAC, an adjusted cost function is introduced that combines and weighs both photometric and semantic scores propagating, thus, more accurate depth estimates. Being adaptive, it fills in apparent information gaps and smoothing local roughness in problematic regions while at the same time preserves important details. Experiments on benchmark and custom datasets demonstrate the effectiveness of the presented approach.},
DOI = {10.3390/rs13061053}
}



@Article{s21061947,
AUTHOR = {Nemer, Ibrahim and Sheltami, Tarek and Ahmad, Irfan and Yasar, Ansar Ul-Haque and Abdeen, Mohammad A. R.},
TITLE = {RF-Based UAV Detection and Identification Using Hierarchical Learning Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1947},
URL = {https://www.mdpi.com/1424-8220/21/6/1947},
PubMedID = {33802189},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are widely available in the current market to be used either for recreation as a hobby or to serve specific industrial requirements, such as agriculture and construction. However, illegitimate and criminal usage of UAVs is also on the rise which introduces their effective identification and detection as a research challenge. This paper proposes a novel machine learning-based for efficient identification and detection of UAVs. Specifically, an improved UAV identification and detection approach is presented using an ensemble learning based on the hierarchical concept, along with pre-processing and feature extraction stages for the Radio Frequency (RF) data. Filtering is applied on the RF signals in the detection approach to improve the output. This approach consists of four classifiers and they are working in a hierarchical way. The sample will pass the first classifier to check the availability of the UAV, and then it will specify the type of the detected UAV using the second classifier. The last two classifiers will handle the sample that is related to Bebop and AR to specify their mode. Evaluation of the proposed approach with publicly available dataset demonstrates better efficiency compared to existing detection systems in the literature. It has the ability to investigate whether a UAV is flying within the area or not, and it can directly identify the type of UAV and then the flight mode of the detected UAV with accuracy around 99%.},
DOI = {10.3390/s21061947}
}



@Article{s21061960,
AUTHOR = {Fotouhi, Azade and Ding, Ming and Hassan, Mahbub},
TITLE = {Deep Q-Learning for Two-Hop Communications of Drone Base Stations},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1960},
URL = {https://www.mdpi.com/1424-8220/21/6/1960},
PubMedID = {33799546},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we address the application of the flying Drone Base Stations (DBS) in order to improve the network performance. Given the high degrees of freedom of a DBS, it can change its position and adapt its trajectory according to the users movements and the target environment. A two-hop communication model, between an end-user and a macrocell through a DBS, is studied in this work. We propose Q-learning and Deep Q-learning based solutions to optimize the drone’s trajectory. Simulation results show that, by employing our proposed models, the drone can autonomously fly and adapts its mobility according to the users’ movements. Additionally, the Deep Q-learning model outperforms the Q-learning model and can be applied in more complex environments.},
DOI = {10.3390/s21061960}
}



@Article{f12030327,
AUTHOR = {Dainelli, Riccardo and Toscano, Piero and Di Gennaro, Salvatore Filippo and Matese, Alessandro},
TITLE = {Recent Advances in Unmanned Aerial Vehicle Forest Remote Sensing—A Systematic Review. Part I: A General Framework},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {327},
URL = {https://www.mdpi.com/1999-4907/12/3/327},
ISSN = {1999-4907},
ABSTRACT = {Natural, semi-natural, and planted forests are a key asset worldwide, providing a broad range of positive externalities. For sustainable forest planning and management, remote sensing (RS) platforms are rapidly going mainstream. In a framework where scientific production is growing exponentially, a systematic analysis of unmanned aerial vehicle (UAV)-based forestry research papers is of paramount importance to understand trends, overlaps and gaps. The present review is organized into two parts (Part I and Part II). Part II inspects specific technical issues regarding the application of UAV-RS in forestry, together with the pros and cons of different UAV solutions and activities where additional effort is needed, such as the technology transfer. Part I systematically analyzes and discusses general aspects of applying UAV in natural, semi-natural and artificial forestry ecosystems in the recent peer-reviewed literature (2018–mid-2020). The specific goals are threefold: (i) create a carefully selected bibliographic dataset that other researchers can draw on for their scientific works; (ii) analyze general and recent trends in RS forest monitoring (iii) reveal gaps in the general research framework where an additional activity is needed. Through double-step filtering of research items found in the Web of Science search engine, the study gathers and analyzes a comprehensive dataset (226 articles). Papers have been categorized into six main topics, and the relevant information has been subsequently extracted. The strong points emerging from this study concern the wide range of topics in the forestry sector and in particular the retrieval of tree inventory parameters often through Digital Aerial Photogrammetry (DAP), RGB sensors, and machine learning techniques. Nevertheless, challenges still exist regarding the promotion of UAV-RS in specific parts of the world, mostly in the tropical and equatorial forests. Much additional research is required for the full exploitation of hyperspectral sensors and for planning long-term monitoring.},
DOI = {10.3390/f12030327}
}



@Article{s21061994,
AUTHOR = {Ma, Qian and Han, Wenting and Huang, Shenjin and Dong, Shide and Li, Guang and Chen, Haipeng},
TITLE = {Distinguishing Planting Structures of Different Complexity from UAV Multispectral Images},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1994},
URL = {https://www.mdpi.com/1424-8220/21/6/1994},
PubMedID = {33808967},
ISSN = {1424-8220},
ABSTRACT = {This study explores the classification potential of a multispectral classification model for farmland with planting structures of different complexity. Unmanned aerial vehicle (UAV) remote sensing technology is used to obtain multispectral images of three study areas with low-, medium-, and high-complexity planting structures, containing three, five, and eight types of crops, respectively. The feature subsets of three study areas are selected by recursive feature elimination (RFE). Object-oriented random forest (OB-RF) and object-oriented support vector machine (OB-SVM) classification models are established for the three study areas. After training the models with the feature subsets, the classification results are evaluated using a confusion matrix. The OB-RF and OB-SVM models’ classification accuracies are 97.09% and 99.13%, respectively, for the low-complexity planting structure. The equivalent values are 92.61% and 99.08% for the medium-complexity planting structure and 88.99% and 97.21% for the high-complexity planting structure. For farmland with fragmentary plots and a high-complexity planting structure, as the planting structure complexity changed from low to high, both models’ overall accuracy levels decreased. The overall accuracy of the OB-RF model decreased by 8.1%, and that of the OB-SVM model only decreased by 1.92%. OB-SVM achieves an overall classification accuracy of 97.21%, and a single-crop extraction accuracy of at least 85.65%. Therefore, UAV multispectral remote sensing can be used for classification applications in highly complex planting structures.},
DOI = {10.3390/s21061994}
}



@Article{rs13061081,
AUTHOR = {Liu, Zhen and Wu, Wenxiu and Gu, Xingyu and Li, Shuwei and Wang, Lutai and Zhang, Tianjie},
TITLE = {Application of Combining YOLO Models and 3D GPR Images in Road Detection and Maintenance},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1081},
URL = {https://www.mdpi.com/2072-4292/13/6/1081},
ISSN = {2072-4292},
ABSTRACT = {Improving the detection efficiency and maintenance benefits is one of the greatest challenges in road testing and maintenance. To address this problem, this paper presents a method for combining the you only look once (YOLO) series with 3D ground-penetrating radar (GPR) images to recognize the internal defects in asphalt pavement and compares the effectiveness of traditional detection and GPR detection by evaluating the maintenance benefits. First, traditional detection is conducted to survey and summarize the surface conditions of tested roads, which are missing the internal information. Therefore, GPR detection is implemented to acquire the images of concealed defects. Then, the YOLOv5 model with the most even performance of the six selected models is applied to achieve the rapid identification of road defects. Finally, the benefits evaluation of maintenance programs based on these two detection methods is conducted from economic and environmental perspectives. The results demonstrate that the economic scores are improved and the maintenance cost is reduced by $49,398/km based on GPR detection; the energy consumption and carbon emissions are reduced by 792,106 MJ/km (16.94%) and 56,289 kg/km (16.91%), respectively, all of which indicates the effectiveness of 3D GPR in pavement detection and maintenance.},
DOI = {10.3390/rs13061081}
}



@Article{rs13061094,
AUTHOR = {Peng, Xingshuo and Han, Wenting and Ao, Jianyi and Wang, Yi},
TITLE = {Assimilation of LAI Derived from UAV Multispectral Data into the SAFY Model to Estimate Maize Yield},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1094},
URL = {https://www.mdpi.com/2072-4292/13/6/1094},
ISSN = {2072-4292},
ABSTRACT = {In this study, we develop a method to estimate corn yield based on remote sensing data and ground monitoring data under different water treatments. Spatially explicit information on crop yields is essential for farmers and agricultural agencies to make well-informed decisions. One approach to estimate crop yield with remote sensing is data assimilation, which integrates sequential observations of canopy development from remote sensing into model simulations of crop growth processes. We found that leaf area index (LAI) inversion based on unmanned aerial vehicle (UAV) vegetation index has a high accuracy, with R2 and root mean square error (RMSE) values of 0.877 and 0.609, respectively. Maize yield estimation based on UAV remote sensing data and simple algorithm for yield (SAFY) crop model data assimilation has different yield estimation accuracy under different water treatments. This method can be used to estimate corn yield, where R2 is 0.855 and RMSE is 692.8kg/ha. Generally, the higher the water stress, the lower the estimation accuracy. Furthermore, we perform the yield estimate mapping at 2 m spatial resolution, which has a higher spatial resolution and accuracy than satellite remote sensing. The great potential of incorporating UAV observations with crop data to monitor crop yield, and improve agricultural management is therefore indicated.},
DOI = {10.3390/rs13061094}
}



@Article{w13060791,
AUTHOR = {Liang, Zhongwei and Liu, Xiaochu and Zou, Tao and Xiao, Jinrui},
TITLE = {Adaptive Prediction of Water Droplet Infiltration Effectiveness of Sprinkler Irrigation Using Regularized Sparse Autoencoder–Adaptive Network-Based Fuzzy Inference System (RSAE–ANFIS)},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {791},
URL = {https://www.mdpi.com/2073-4441/13/6/791},
ISSN = {2073-4441},
ABSTRACT = {As the high productive efficiency of sprinkler irrigation is largely based on balanced soil moisture distribution, it is essential to study the exact effectiveness of water droplet infiltration, which provides a theoretical basis for rationally scheduling the circulation efficiency of groundwater in agricultural irrigation performance. This research carried out adaptive prediction of the droplet infiltration effectiveness of sprinkler irrigation by using a novel approach of a regularized sparse autoencoder–adaptive network-based fuzzy inference system (RSAE–ANFIS), for the purpose of quantifying actual water droplet infiltration and effectiveness results of precision irrigation in various environmental conditions. The intelligent prediction experiment we implemented could be phased as: the demonstration of governing equations of droplet infiltration for sprinkler irrigation modeling; the measurement and computation of probability densities in water droplet infiltration; innovative establishment and working analysis of RSAE–ANFIS; and the adaptive prediction of infiltration effectiveness indexes, such as average soil moisture depth increment (θ, mm), irrigation infiltration efficiency (ea, %), irrigation turn duration efficiency (et, mm/min), and the uniformity coefficient of soil moisture infiltration (Cu, %), which were implemented to provide a comprehensive illustration for the effective scheduling of sprinkler irrigation. Result comparisons indicated that when jetting pressure (Pw) was 255.2 kPa, the impinge angle (Wa) was 42.5°, the water flow rate (Fa) was 0.67 kg/min, and continuous irrigation time (Tc) was 32.4 min (error tolerance = ±5%, the same as follows), thereby an optimum and stable effectiveness quality of sprinkler irrigation could be achieved, whereas average soil moisture depth increment (θ) was 57.6 mm, irrigation infiltration efficiency (ea) was 62.5%, irrigation turn duration efficiency (et) was 34.5 mm/min, and the uniformity coefficient of soil moisture infiltration (Cu) was 53.6%, accordingly. It could be concluded that the proposed approach of the regularized sparse autoencoder–adaptive network-based fuzzy inference system has outstanding predictive capability and possesses much better working superiority for infiltration effectiveness in accuracy and efficiency; meanwhile, a high agreement between the adaptive predicted and actual measured values of infiltration effectiveness could be obtained. This novel intelligent prediction system has been promoted constructively to improve the quality uniformity of sprinkler irrigation and, consequently, to facilitate the productive management of sprinkler irrigated agriculture.},
DOI = {10.3390/w13060791}
}



@Article{s21062052,
AUTHOR = {Yang, Xinghai and Wang, Fengjiao and Bai, Zhiquan and Xun, Feifei and Zhang, Yulin and Zhao, Xiuyang},
TITLE = {Deep Learning-Based Congestion Detection at Urban Intersections},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2052},
URL = {https://www.mdpi.com/1424-8220/21/6/2052},
PubMedID = {33803952},
ISSN = {1424-8220},
ABSTRACT = {In this paper, a deep learning-based traffic state discrimination method is proposed to detect traffic congestion at urban intersections. The detection algorithm includes two parts, global speed detection and a traffic state discrimination algorithm. Firstly, the region of interest (ROI) is selected as the road intersection from the input image of the You Only Look Once (YOLO) v3 object detection algorithm for vehicle target detection. The Lucas-Kanade (LK) optical flow method is employed to calculate the vehicle speed. Then, the corresponding intersection state can be obtained based on the vehicle speed and the discrimination algorithm. The detection of the vehicle takes the position information obtained by YOLOv3 as the input of the LK optical flow algorithm and forms an optical flow vector to complete the vehicle speed detection. Experimental results show that the detection algorithm can detect the vehicle speed and traffic state discrimination method can judge the traffic state accurately, which has a strong anti-interference ability and meets the practical application requirements.},
DOI = {10.3390/s21062052}
}



@Article{s21062062,
AUTHOR = {Dias, Pollyanna G. Faria and Silva, Mateus C. and Rocha Filho, Geraldo P. and Vargas, Patrícia A. and Cota, Luciano P. and Pessin, Gustavo},
TITLE = {Swarm Robotics: A Perspective on the Latest Reviewed Concepts and Applications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2062},
URL = {https://www.mdpi.com/1424-8220/21/6/2062},
PubMedID = {33804187},
ISSN = {1424-8220},
ABSTRACT = {Known as an artificial intelligence subarea, Swarm Robotics is a developing study field investigating bio-inspired collaborative control approaches and integrates a huge collection of agents, reasonably plain robots, in a distributed and decentralized manner. It offers an inspiring essential platform for new researchers to be engaged and share new knowledge to examine their concepts in analytical and heuristic strategies. This paper introduces an overview of current activities in Swarm Robotics and examines the present literature in this area to establish to approach between a realistic swarm robotic system and real-world enforcements. First, we review several Swarm Intelligence concepts to define Swarm Robotics systems, reporting their essential qualities and features and contrast them to generic multi-robotic systems. Second, we report a review of the principal projects that allow realistic study of Swarm Robotics. We demonstrate knowledge regarding current hardware platforms and multi-robot simulators. Finally, the forthcoming promissory applications and the troubles to surpass with a view to achieving them have been described and analyzed.},
DOI = {10.3390/s21062062}
}



@Article{asi4010023,
AUTHOR = {Naseem, Usman and Khushi, Matloob and Khan, Shah Khalid and Shaukat, Kamran and Moni, Mohammad Ali},
TITLE = {A Comparative Analysis of Active Learning for Biomedical Text Mining},
JOURNAL = {Applied System Innovation},
VOLUME = {4},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2571-5577/4/1/23},
ISSN = {2571-5577},
ABSTRACT = {An enormous amount of clinical free-text information, such as pathology reports, progress reports, clinical notes and discharge summaries have been collected at hospitals and medical care clinics. These data provide an opportunity of developing many useful machine learning applications if the data could be transferred into a learn-able structure with appropriate labels for supervised learning. The annotation of this data has to be performed by qualified clinical experts, hence, limiting the use of this data due to the high cost of annotation. An underutilised technique of machine learning that can label new data called active learning (AL) is a promising candidate to address the high cost of the label the data. AL has been successfully applied to labelling speech recognition and text classification, however, there is a lack of literature investigating its use for clinical purposes. We performed a comparative investigation of various AL techniques using ML and deep learning (DL)-based strategies on three unique biomedical datasets. We investigated random sampling (RS), least confidence (LC), informative diversity and density (IDD), margin and maximum representativeness-diversity (MRD) AL query strategies. Our experiments show that AL has the potential to significantly reducing the cost of manual labelling. Furthermore, pre-labelling performed using AL expediates the labelling process by reducing the time required for labelling.},
DOI = {10.3390/asi4010023}
}



@Article{ani11030829,
AUTHOR = {Herlin, Anders and Brunberg, Emma and Hultgren, Jan and Högberg, Niclas and Rydberg, Anna and Skarin, Anna},
TITLE = {Animal Welfare Implications of Digital Tools for Monitoring and Management of Cattle and Sheep on Pasture},
JOURNAL = {Animals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {829},
URL = {https://www.mdpi.com/2076-2615/11/3/829},
PubMedID = {33804235},
ISSN = {2076-2615},
ABSTRACT = {The opportunities for natural animal behaviours in pastures imply animal welfare benefits. Nevertheless, monitoring the animals can be challenging. The use of sensors, cameras, positioning equipment and unmanned aerial vehicles in large pastures has the potential to improve animal welfare surveillance. Directly or indirectly, sensors measure environmental factors together with the behaviour and physiological state of the animal, and deviations can trigger alarms for, e.g., disease, heat stress and imminent calving. Electronic positioning includes Radio Frequency Identification (RFID) for the recording of animals at fixed points. Positioning units (GPS) mounted on collars can determine animal movements over large areas, determine their habitat and, somewhat, health and welfare. In combination with other sensors, such units can give information that helps to evaluate the welfare of free-ranging animals. Drones equipped with cameras can also locate and count the animals, as well as herd them. Digitally defined virtual fences can keep animals within a predefined area without the use of physical barriers, relying on acoustic signals and weak electric shocks. Due to individual variations in learning ability, some individuals may be exposed to numerous electric shocks, which might compromise their welfare. More research and development are required, especially regarding the use of drones and virtual fences.},
DOI = {10.3390/ani11030829}
}



@Article{rs13061117,
AUTHOR = {Li, Jing and Xie, Yuguang and Li, Congcong and Dai, Yanran and Ma, Jiaxin and Dong, Zheng and Yang, Tao},
TITLE = {UAV-Assisted Wide Area Multi-Camera Space Alignment Based on Spatiotemporal Feature Map},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1117},
URL = {https://www.mdpi.com/2072-4292/13/6/1117},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we investigate the problem of aligning multiple deployed camera into one united coordinate system for cross-camera information sharing and intercommunication. However, the difficulty is greatly increased when faced with large-scale scene under chaotic camera deployment. To address this problem, we propose a UAV-assisted wide area multi-camera space alignment approach based on spatiotemporal feature map. It employs the great global perception of Unmanned Aerial Vehicles (UAVs) to meet the challenge from wide-range environment. Concretely, we first present a novel spatiotemporal feature map construction approach to represent the input aerial and ground monitoring data. In this way, the motion consistency across view is well mined to overcome the great perspective gap between the UAV and ground cameras. To obtain the corresponding relationship between their pixels, we propose a cross-view spatiotemporal matching strategy. Through solving relative relationship with the above air-to-ground point correspondences, all ground cameras can be aligned into one surveillance space. The proposed approach was evaluated in both simulation and real environments qualitatively and quantitatively. Extensive experimental results demonstrate that our system can successfully align all ground cameras with very small pixel error. Additionally, the comparisons with other works on different test situations also verify its superior performance.},
DOI = {10.3390/rs13061117}
}



@Article{aerospace8030079,
AUTHOR = {Swinney, Carolyn J. and Woods, John C.},
TITLE = {Unmanned Aerial Vehicle Operating Mode Classification Using Deep Residual Learning Feature Extraction},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {79},
URL = {https://www.mdpi.com/2226-4310/8/3/79},
ISSN = {2226-4310},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) undoubtedly pose many security challenges. We need only look to the December 2018 Gatwick Airport incident for an example of the disruption UAVs can cause. In total, 1000 flights were grounded for 36 h over the Christmas period which was estimated to cost over 50 million pounds. In this paper, we introduce a novel approach which considers UAV detection as an imagery classification problem. We consider signal representations Power Spectral Density (PSD); Spectrogram, Histogram and raw IQ constellation as graphical images presented to a deep Convolution Neural Network (CNN) ResNet50 for feature extraction. Pre-trained on ImageNet, transfer learning is utilised to mitigate the requirement for a large signal dataset. We evaluate performance through machine learning classifier Logistic Regression. Three popular UAVs are classified in different modes; switched on; hovering; flying; flying with video; and no UAV present, creating a total of 10 classes. Our results, validated with 5-fold cross validation and an independent dataset, show PSD representation to produce over 91% accuracy for 10 classifications. Our paper treats UAV detection as an imagery classification problem by presenting signal representations as images to a ResNet50, utilising the benefits of transfer learning and outperforming previous work in the field.},
DOI = {10.3390/aerospace8030079}
}



