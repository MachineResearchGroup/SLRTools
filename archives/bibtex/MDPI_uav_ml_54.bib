
@Article{drones5010012,
AUTHOR = {Oleksyn, Semonn and Tosetto, Louise and Raoult, Vincent and Joyce, Karen E. and Williamson, Jane E.},
TITLE = {Going Batty: The Challenges and Opportunities of Using Drones to Monitor the Behaviour and Habitat Use of Rays},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2504-446X/5/1/12},
ISSN = {2504-446X},
ABSTRACT = {The way an animal behaves in its habitat provides insight into its ecological role. As such, collecting robust, accurate datasets in a time-efficient manner is an ever-present pressure for the field of behavioural ecology. Faced with the shortcomings and physical limitations of traditional ground-based data collection techniques, particularly in marine studies, drones offer a low-cost and efficient approach for collecting data in a range of coastal environments. Despite drones being widely used to monitor a range of marine animals, they currently remain underutilised in ray research. The innovative application of drones in environmental and ecological studies has presented novel opportunities in animal observation and habitat assessment, although this emerging field faces substantial challenges. As we consider the possibility to monitor rays using drones, we face challenges related to local aviation regulations, the weather and environment, as well as sensor and platform limitations. Promising solutions continue to be developed, however, growing the potential for drone-based monitoring of behaviour and habitat use of rays. While the barriers to enter this field may appear daunting for researchers with little experience with drones, the technology is becoming increasingly accessible, helping ray researchers obtain a wide range of highly useful data.},
DOI = {10.3390/drones5010012}
}



@Article{s21041033,
AUTHOR = {Wen, Qiaodi and Luo, Ziqi and Chen, Ruitao and Yang, Yifan and Li, Guofa},
TITLE = {Deep Learning Approaches on Defect Detection in High Resolution Aerial Images of Insulators},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1033},
URL = {https://www.mdpi.com/1424-8220/21/4/1033},
PubMedID = {33546245},
ISSN = {1424-8220},
ABSTRACT = {By detecting the defect location in high-resolution insulator images collected by unmanned aerial vehicle (UAV) in various environments, the occurrence of power failure can be timely detected and the caused economic loss can be reduced. However, the accuracies of existing detection methods are greatly limited by the complex background interference and small target detection. To solve this problem, two deep learning methods based on Faster R-CNN (faster region-based convolutional neural network) are proposed in this paper, namely Exact R-CNN (exact region-based convolutional neural network) and CME-CNN (cascade the mask extraction and exact region-based convolutional neural network). Firstly, we proposed an Exact R-CNN based on a series of advanced techniques including FPN (feature pyramid network), cascade regression, and GIoU (generalized intersection over union). RoI Align (region of interest align) is introduced to replace RoI pooling (region of interest pooling) to address the misalignment problem, and the depthwise separable convolution and linear bottleneck are introduced to reduce the computational burden. Secondly, a new pipeline is innovatively proposed to improve the performance of insulator defect detection, namely CME-CNN. In our proposed CME-CNN, an insulator mask image is firstly generated to eliminate the complex background by using an encoder-decoder mask extraction network, and then the Exact R-CNN is used to detect the insulator defects. The experimental results show that our proposed method can effectively detect insulator defects, and its accuracy is better than the examined mainstream target detection algorithms.},
DOI = {10.3390/s21041033}
}



@Article{app11041403,
AUTHOR = {Kamarudin, Mohd Hider and Ismail, Zool Hilmi and Saidi, Noor Baity},
TITLE = {Deep Learning Sensor Fusion in Plant Water Stress Assessment: A Comprehensive Review},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1403},
URL = {https://www.mdpi.com/2076-3417/11/4/1403},
ISSN = {2076-3417},
ABSTRACT = {Water stress is one of the major challenges to food security, causing a significant economic loss for the nation as well for growers. Accurate assessment of water stress will enhance agricultural productivity through optimization of plant water usage, maximizing plant breeding strategies, and preventing forest wildfire for better ecosystem management. Recent advancements in sensor technologies have enabled high-throughput, non-contact, and cost-efficient plant water stress assessment through intelligence system modeling. The advanced deep learning sensor fusion technique has been reported to improve the performance of the machine learning application for processing the collected sensory data. This paper extensively reviews the state-of-the-art methods for plant water stress assessment that utilized the deep learning sensor fusion approach in their application, together with future prospects and challenges of the application domain. Notably, 37 deep learning solutions fell under six main areas, namely soil moisture estimation, soil water modelling, evapotranspiration estimation, evapotranspiration forecasting, plant water status estimation and plant water stress identification. Basically, there are eight deep learning solutions compiled for the 3D-dimensional data and plant varieties challenge, including unbalanced data that occurred due to isohydric plants, and the effect of variations that occur within the same species but cultivated from different locations.},
DOI = {10.3390/app11041403}
}



@Article{electronics10040387,
AUTHOR = {Chamran, Mohammad Kazem and Yau, Kok-Lim Alvin and Noor, Rafidah Md. and Wu, Celimuge},
TITLE = {An Experimental Study on D2D Route Selection Mechanism in 5G Scenarios},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {387},
URL = {https://www.mdpi.com/2079-9292/10/4/387},
ISSN = {2079-9292},
ABSTRACT = {This paper demonstrates a route selection mechanism on a testbed with heterogeneous device-to-device (D2D) wireless communication for a 5G network scenario. The source node receives information about the primary users’ (PUs’) (or licensed users’) activities and available routes from the macrocell base station (or a central controller) and makes a decision to select a multihop route to the destination node. The source node from small cells can either choose: (a) a route with direct communication with the macrocell base station to improve the route performance; or (b) a route with D2D communication among nodes in the small cells to offload traffic from the macrocell to improve spectrum efficiency. The selected D2D route has the least PUs’ activities. The route selection mechanism is investigated on our testbed that helps to improve the accuracy of network performance measurement. In traditional testbeds, each node (e.g., Universal Software Radio Peripheral (USRP) that serves as the front-end communication block) is connected to a single processing unit (e.g., a personal computer) via a switch using cables. In our testbed, each USRP node is connected to a separate processing unit, i.e., raspberry Pi3 B+ (or RP3), which offers three main advantages: (a) control messages and data packets are exchanged via the wireless medium; (b) separate processing units make decisions in a distributed and heterogeneous manner; and (c) the nodes are placed further apart from one another. Therefore, in the investigation of our route selection scheme, the response delay of control message exchange and the packet loss caused by the operating environment (e.g., ambient noise) are implied in our end-to-end delay and packet delivery ratio measurement. Our results show an increase of end-to-end delay and a decrease of packet delivery ratio due to the transmission of control messages and data packets in the wireless medium in the presence of the dynamic PUs’ activities. Furthermore, D2D communication can offload 25% to 75% traffic from macrocell base station to small cells.},
DOI = {10.3390/electronics10040387}
}



@Article{s21041108,
AUTHOR = {Melo, Aurelio G. and Pinto, Milena F. and Marcato, Andre L. M. and Honório, Leonardo M. and Coelho, Fabrício O.},
TITLE = {Dynamic Optimization and Heuristics Based Online Coverage Path Planning in 3D Environment for UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1108},
URL = {https://www.mdpi.com/1424-8220/21/4/1108},
PubMedID = {33562647},
ISSN = {1424-8220},
ABSTRACT = {Path planning is one of the most important issues in the robotics field, being applied in many domains ranging from aerospace technology and military tasks to manufacturing and agriculture. Path planning is a branch of autonomous navigation. In autonomous navigation, dynamic decisions about the path have to be taken while the robot moves towards its goal. Among the navigation area, an important class of problems is Coverage Path Planning (CPP). The CPP technique is associated with determining a collision-free path that passes through all viewpoints in a specific area. This paper presents a method to perform CPP in 3D environment for Unmanned Aerial Vehicles (UAVs) applications, namely 3D dynamic for CPP applications (3DD-CPP). The proposed method can be deployed in an unknown environment through a combination of linear optimization and heuristics. A model to estimate cost matrices accounting for UAV power usage is proposed and evaluated for a few different flight speeds. As linear optimization methods can be computationally demanding to be used on-board a UAV, this work also proposes a distributed execution of the algorithm through fog-edge computing. Results showed that 3DD-CPP had a good performance in both local execution and fog-edge for different simulated scenarios. The proposed heuristic is capable of re-optimization, enabling execution in environments with local knowledge of the environments.},
DOI = {10.3390/s21041108}
}



@Article{s21041151,
AUTHOR = {Horla, Dariusz and Giernacki, Wojciech and Cieślak, Jacek and Campoy, Pascual},
TITLE = {Altitude Measurement-Based Optimization of the Landing Process of UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1151},
URL = {https://www.mdpi.com/1424-8220/21/4/1151},
PubMedID = {33562147},
ISSN = {1424-8220},
ABSTRACT = {The paper addresses the loop shaping problem in the altitude control of an unmanned aerial vehicle to land the flying robot with a specific landing scenario adopted. The proposed solution is optimal, in the sense of the selected performance indices, namely minimum-time, minimum-energy, and velocity-penalized related functions, achieving their minimal values, with numerous experiments conducted throughout the development and preparation to the Mohamed Bin Zayed International Robotics Challenge (MBZIRC 2020). A novel approach to generation of a reference altitude trajectory is presented, which is then tracked in a standard, though optimized, control loop. Three landing scenarios are considered, namely: minimum-time, minimum-energy, and velocity-penalized landing scenarios. The experimental results obtained with the use of the Simulink Support Package for Parrot Minidrones, and the OptiTrack motion capture system proved the effectiveness of the proposed approach.},
DOI = {10.3390/s21041151}
}



@Article{s21041182,
AUTHOR = {Vangi, Elia and D’Amico, Giovanni and Francini, Saverio and Giannetti, Francesca and Lasserre, Bruno and Marchetti, Marco and Chirici, Gherardo},
TITLE = {The New Hyperspectral Satellite PRISMA: Imagery for Forest Types Discrimination},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1182},
URL = {https://www.mdpi.com/1424-8220/21/4/1182},
PubMedID = {33567591},
ISSN = {1424-8220},
ABSTRACT = {Different forest types based on different tree species composition may have similar spectral signatures if observed with traditional multispectral satellite sensors. Hyperspectral imagery, with a more continuous representation of their spectral behavior may instead be used for their classification. The new hyperspectral Precursore IperSpettrale della Missione Applicativa (PRISMA) sensor, developed by the Italian Space Agency, is able to capture images in a continuum of 240 spectral bands ranging between 400 and 2500 nm, with a spectral resolution smaller than 12 nm. The new sensor can be employed for a large number of remote sensing applications, including forest types discrimination. In this study, we compared the capabilities of the new PRISMA sensor against the well-known Sentinel-2 Multi-Spectral Instrument (MSI) in recognition of different forest types through a pairwise separability analysis carried out in two study areas in Italy, using two different nomenclature systems and four separability metrics. The PRISMA hyperspectral sensor, compared to Sentinel-2 MSI, allowed for a better discrimination in all forest types, increasing the performance when the complexity of the nomenclature system also increased. PRISMA achieved an average improvement of 40% for the discrimination between two forest categories (coniferous vs. broadleaves) and of 102% in the discrimination between five forest types based on main tree species groups.},
DOI = {10.3390/s21041182}
}



@Article{rs13040633,
AUTHOR = {Zhang, Xiuwei and Zhou, Yang and Jin, Jiaojiao and Wang, Yafei and Fan, Minhao and Wang, Ning and Zhang, Yanning},
TITLE = {ICENETv2: A Fine-Grained River Ice Semantic Segmentation Network Based on UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {633},
URL = {https://www.mdpi.com/2072-4292/13/4/633},
ISSN = {2072-4292},
ABSTRACT = {Accurate ice segmentation is one of the most crucial techniques for intelligent ice monitoring. Compared with ice segmentation, it can provide more information for ice situation analysis, change trend prediction, and so on. Therefore, the study of ice segmentation has important practical significance. In this study, we focused on fine-grained river ice segmentation using unmanned aerial vehicle (UAV) images. This has the following difficulties: (1) The scale of river ice varies greatly in different images and even in the same image; (2) the same kind of river ice differs greatly in color, shape, texture, size, and so on; and (3) the appearances of different kinds of river ice sometimes appear similar due to the complex formation and change procedure. Therefore, to perform this study, the NWPU_YRCC2 dataset was built, in which all UAV images were collected in the Ningxia–Inner Mongolia reach of the Yellow River. Then, a novel semantic segmentation method based on deep convolution neural network, named ICENETv2, is proposed. To achieve multiscale accurate prediction, we design a multilevel features fusion framework, in which multi-scale high-level semantic features and lower-level finer features are effectively fused. Additionally, a dual attention module is adopted to highlight distinguishable characteristics, and a learnable up-sampling strategy is further used to improve the segmentation accuracy of the details. Experiments show that ICENETv2 achieves the state-of-the-art on the NWPU_YRCC2 dataset. Finally, our ICENETv2 is also applied to solve a realistic problem, calculating drift ice cover density, which is one of the most important factors to predict the freeze-up data of the river. The results demonstrate that the performance of ICENETv2 meets the actual application demand.},
DOI = {10.3390/rs13040633}
}



@Article{rs13040705,
AUTHOR = {Kopačková-Strnadová, Veronika and Koucká, Lucie and Jelének, Jan and Lhotáková, Zuzana and Oulehle, Filip},
TITLE = {Canopy Top, Height and Photosynthetic Pigment Estimation Using Parrot Sequoia Multispectral Imagery and the Unmanned Aerial Vehicle (UAV)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {705},
URL = {https://www.mdpi.com/2072-4292/13/4/705},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing is one of the modern methods that have significantly developed over the last two decades and, nowadays, it provides a new means for forest monitoring. High spatial and temporal resolutions are demanded for the accurate and timely monitoring of forests. In this study, multi-spectral Unmanned Aerial Vehicle (UAV) images were used to estimate canopy parameters (definition of crown extent, top, and height, as well as photosynthetic pigment contents). The UAV images in Green, Red, Red-Edge, and Near infrared (NIR) bands were acquired by Parrot Sequoia camera over selected sites in two small catchments (Czech Republic) covered dominantly by Norway spruce monocultures. Individual tree extents, together with tree tops and heights, were derived from the Canopy Height Model (CHM). In addition, the following were tested: (i) to what extent can the linear relationship be established between selected vegetation indexes (Normalized Difference Vegetation Index (NDVI) and NDVIred edge) derived for individual trees and the corresponding ground truth (e.g., biochemically assessed needle photosynthetic pigment contents) and (ii) whether needle age selection as a ground truth and crown light conditions affect the validity of linear models. The results of the conducted statistical analysis show that the two vegetation indexes (NDVI and NDVIred edge) tested here have the potential to assess photosynthetic pigments in Norway spruce forests at a semi-quantitative level; however, the needle-age selection as a ground truth was revealed to be a very important factor. The only usable results were obtained for linear models when using the second year needle pigment contents as a ground truth. On the other hand, the illumination conditions of the crown proved to have very little effect on the model’s validity. No study was found to directly compare these results conducted on coniferous forest stands. This shows that there is a further need for studies dealing with a quantitative estimation of the biochemical variables of nature coniferous forests when employing spectral data that were acquired by the UAV platform at a very high spatial resolution.},
DOI = {10.3390/rs13040705}
}



@Article{rs13040720,
AUTHOR = {Bohlin, Jonas and Wallerman, Jörgen and Fransson, Johan E. S.},
TITLE = {Extraction of Spectral Information from Airborne 3D Data for Assessment of Tree Species Proportions},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {720},
URL = {https://www.mdpi.com/2072-4292/13/4/720},
ISSN = {2072-4292},
ABSTRACT = {With the rapid development of photogrammetric software and accessible camera technology, land surveys and other mapping organizations now provide various point cloud and digital surface model products from aerial images, often including spectral information. In this study, methods for colouring the point cloud and the importance of different metrics were compared for tree species-specific estimates at a coniferous hemi-boreal test site in southern Sweden. A total of three different data sets of aerial image-based products and one multi-spectral lidar data set were used to estimate tree species-specific proportion and stem volume using an area-based approach. Metrics were calculated for 156 field plots (10 m radius) from point cloud data and used in a Random Forest analysis. Plot level accuracy was evaluated using leave-one-out cross-validation. The results showed small differences in estimation accuracy of species-specific variables between the colouring methods. Simple averages of the spectral metrics had the highest importance and using spectral data from two seasons improved species prediction, especially deciduous proportion. Best tree species-specific proportion was estimated using multi-spectral lidar with 0.22 root mean square error (RMSE) for pine, 0.22 for spruce and 0.16 for deciduous. Corresponding RMSE for aerial images was 0.24, 0.23 and 0.20 for pine, spruce and deciduous, respectively. For the species-specific stem volume at plot level using image data, the RMSE in percent of surveyed mean was 129% for pine, 60% for spruce and 118% for deciduous.},
DOI = {10.3390/rs13040720}
}



@Article{s21041386,
AUTHOR = {Liu, Feng and Dai, Shuling and Zhao, Yongjia},
TITLE = {Learning to Have a Civil Aircraft Take Off under Crosswind Conditions by Reinforcement Learning with Multimodal Data and Preprocessing Data},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1386},
URL = {https://www.mdpi.com/1424-8220/21/4/1386},
PubMedID = {33669479},
ISSN = {1424-8220},
ABSTRACT = {Autopilot technology in the field of aviation has developed over many years. However, it is difficult for an autopilot system to autonomously operate a civil aircraft under bad weather conditions. In this paper, we present a reinforcement learning (RL) algorithm using multimodal data and preprocessing data to have a civil aircraft take off autonomously under crosswind conditions. The multimodal data include the common flight status and visual information. The preprocessing is a new design that maps some flight data by nonlinear functions based on the general flight dynamics before these data are fed into the RL model. Extensive experiments under different crosswind conditions with a professional flight simulator demonstrate that the proposed method can effectively control a civil aircraft to take off under various crosswind conditions and achieve better performance than trials without visual information or preprocessing data.},
DOI = {10.3390/s21041386}
}



@Article{rs13040733,
AUTHOR = {Gao, Bowen and Chen, Ninghua and Blaschke, Thomas and Wu, Chase Q. and Chen, Jianyu and Xu, Yaochen and Yang, Xiaoping and Du, Zhenhong},
TITLE = {Automated Characterization of Yardangs Using Deep Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/13/4/733},
ISSN = {2072-4292},
ABSTRACT = {The morphological characteristics of yardangs are the direct evidence that reveals the wind and fluvial erosion for lacustrine sediments in arid areas. These features can be critical indicators in reconstructing local wind directions and environment conditions. Thus, the fast and accurate extraction of yardangs is key to studying their regional distribution and evolution process. However, the existing automated methods to characterize yardangs are of limited generalization that may only be feasible for specific types of yardangs in certain areas. Deep learning methods, which are superior in representation learning, provide potential solutions for mapping yardangs with complex and variable features. In this study, we apply Mask region-based convolutional neural networks (Mask R-CNN) to automatically delineate and classify yardangs using very high spatial resolution images from Google Earth. The yardang field in the Qaidam Basin, northwestern China is selected to conduct the experiments and the method yields mean average precisions of 0.869 and 0.671 for intersection of union (IoU) thresholds of 0.5 and 0.75, respectively. The manual validation results on images of additional study sites show an overall detection accuracy of 74%, while more than 90% of the detected yardangs can be correctly classified and delineated. We then conclude that Mask R-CNN is a robust model to characterize multi-scale yardangs of various types and allows for the research of the morphological and evolutionary aspects of aeolian landform.},
DOI = {10.3390/rs13040733}
}



@Article{su13042161,
AUTHOR = {Nieto-Julián, Juan E. and Lara, Lenin and Moyano, Juan},
TITLE = {Implementation of a TeamWork-HBIM for the Management and Sustainability of Architectural Heritage},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {2161},
URL = {https://www.mdpi.com/2071-1050/13/4/2161},
ISSN = {2071-1050},
ABSTRACT = {The benefits of Building Information Modelling (BIM) accrue from the needs of the interoperability of applied technologies. This scope is strongly related to heritage buildings. Protection plans encompassing phases of heritage conservation, interpretation, intervention and dissemination could lead to a sustainable model through a TeamWork-HBIM project. This work develops a step by step semantically enriched 3D model, from accurate data acquisition to the creation of a container of artistic assets. TeamWork-HBIM acts as a database for movable assets, i.e., parametric objects (GDL) with graphical and semantic information, which are valid for recording, inventory and cataloguing processes. Thus, heritage properties were created and used to create recording and inventory sheets related to movable assets. Consequently, a parametric object was edited in the HBIM project, so a new category called “Heritage Furniture” was available. Data from the monitoring of the artistic asset were included in that category. In addition, the specialist technicians from the TeamWork-HBIM team catalogued a dataset related to artistic, historical and conservation properties. Another advantage of the system was the reliability of the structure of the HBIM project, which was based on the actual geometry of the building provided by the point clouds. The information was valid for both modelling works and specialists in virtual monitoring. Moreover, the reliability of metadata was collected in a common data environment (CDE), which was available for everyone. As a result, the Teamwork-HBIM-CDE project meets the needs of private institutions, such as the Foundation of the Church of the Company of Jesus in Quito, related to the sustainability of the historic site. This sustainability is shown by the implementation of a methodology that strengthens the interdisciplinary information flow by including all disciplines of historical heritage.},
DOI = {10.3390/su13042161}
}



@Article{jsan10010015,
AUTHOR = {Blekos, Kostas and Tsakas, Anastasios and Xouris, Christos and Evdokidis, Ioannis and Alexandropoulos, Dimitris and Alexakos, Christos and Katakis, Sofoklis and Makedonas, Andreas and Theoharatos, Christos and Lalos, Aris},
TITLE = {Analysis, Modeling and Multi-Spectral Sensing for the Predictive Management of Verticillium Wilt in Olive Groves},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2224-2708/10/1/15},
ISSN = {2224-2708},
ABSTRACT = {The intensification and expansion in the cultivation of olives have contributed to the significant spread of Verticillium wilt, which is the most important fungal problem affecting olive trees. Recent studies confirm that practices such as the use of innovative natural minerals (Zeoshell ZF1) and the application of beneficial microorganisms (Micosat F BS WP) restore health in infected trees. However, for their efficient implementation the above methodologies require the marking of trees in the early stages of infestation—a task that is impractical with traditional means (manual labor) but also very difficult, as early stages are difficult to perceive with the naked eye. In this paper, we present the results of the My Olive Grove Coach (MyOGC) project, which used multispectral imaging from unmanned aerial vehicles to develop an olive grove monitoring system based on the autonomous and automatic processing of the multispectral images using computer vision and machine learning techniques. The goal of the system is to monitor and assess the health of olive groves, help in the prediction of Verticillium wilt spread and implement a decision support system that guides the farmer/agronomist.},
DOI = {10.3390/jsan10010015}
}



@Article{app11041835,
AUTHOR = {Liao, Kuo-Chien and Lu, Jau-Huai},
TITLE = {Using UAV to Detect Solar Module Fault Conditions of a Solar Power Farm with IR and Visual Image Analysis},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1835},
URL = {https://www.mdpi.com/2076-3417/11/4/1835},
ISSN = {2076-3417},
ABSTRACT = {In recent years, solar energy has been regarded as one of the most important sustainable energy sources. Under the rapid and large-scale construction of solar farms, the maintenance and inspection of the health conditions of solar modules in a large solar farm become an important issue. This article proposes a method for detecting solar cell faults with unmanned aerial vehicle (UAV) equipped with a thermal imager and a visible light camera, and providing a fast and reliable detection method. The detection process includes a new concept of real-time monitoring of the detected area and analysis of the health of solar panels. An image process is proposed that may quickly and accurately detect the abnormality of a solar module. The whole process includes grayscale conversion, filtering, 3-D temperature representation, probability density function, and cumulative density function analysis. Ten cases in real fields have been studied with this process, including large scale solar farms and small size solar modules installed on buildings. Results show that the cumulative density function is a convenient way to determine the health status of the solar panel and may provide maintenance personnel a basis for determining whether replacement of solar cells is necessary for improving the overall power generation efficiency and simplify the maintenance process. It is worth noting that image recognition can increase the clarity of IR images and the cumulative chart can judge the defect rate of the cell. These two methods were combined to provide an instant, fast and accurate defect judgment.},
DOI = {10.3390/app11041835}
}



@Article{s21041456,
AUTHOR = {Ribeiro, Roberto and Ramos, João and Safadinho, David and Reis, Arsénio and Rabadão, Carlos and Barroso, João and Pereira, António},
TITLE = {Web AR Solution for UAV Pilot Training and Usability Testing},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1456},
URL = {https://www.mdpi.com/1424-8220/21/4/1456},
PubMedID = {33669733},
ISSN = {1424-8220},
ABSTRACT = {Data and services are available anywhere at any time thanks to the Internet and mobile devices. Nowadays, there are new ways of representing data through trendy technologies such as augmented reality (AR), which extends our perception of reality through the addition of a virtual layer on top of real-time images. The great potential of unmanned aerial vehicles (UAVs) for carrying out routine and professional tasks has encouraged their use in the creation of several services, such as package delivery or industrial maintenance. Unfortunately, drone piloting is difficult to learn and requires specific training. Since regular training is performed with virtual simulations, we decided to propose a multiplatform cloud-hosted solution based in Web AR for drone training and usability testing. This solution defines a configurable trajectory through virtual elements represented over barcode markers placed on a real environment. The main goal is to provide an inclusive and accessible training solution which could be used by anyone who wants to learn how to pilot or test research related to UAV control. For this paper, we reviewed drones, AR, and human–drone interaction (HDI) to propose an architecture and implement a prototype, which was built using a Raspberry Pi 3, a camera, and barcode markers. The validation was conducted using several test scenarios. The results show that a real-time AR experience for drone pilot training and usability testing is achievable through web technologies. Some of the advantages of this approach, compared to traditional methods, are its high availability by using the web and other ubiquitous devices; the minimization of technophobia related to crashes; and the development of cost-effective alternatives to train pilots and make the testing phase easier for drone researchers and developers through trendy technologies.},
DOI = {10.3390/s21041456}
}



@Article{s21041492,
AUTHOR = {Li, Guoming and Huang, Yanbo and Chen, Zhiqian and Chesser, Gary D. and Purswell, Joseph L. and Linhoss, John and Zhao, Yang},
TITLE = {Practices and Applications of Convolutional Neural Network-Based Computer Vision Systems in Animal Farming: A Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1492},
URL = {https://www.mdpi.com/1424-8220/21/4/1492},
PubMedID = {33670030},
ISSN = {1424-8220},
ABSTRACT = {Convolutional neural network (CNN)-based computer vision systems have been increasingly applied in animal farming to improve animal management, but current knowledge, practices, limitations, and solutions of the applications remain to be expanded and explored. The objective of this study is to systematically review applications of CNN-based computer vision systems on animal farming in terms of the five deep learning computer vision tasks: image classification, object detection, semantic/instance segmentation, pose estimation, and tracking. Cattle, sheep/goats, pigs, and poultry were the major farm animal species of concern. In this research, preparations for system development, including camera settings, inclusion of variations for data recordings, choices of graphics processing units, image preprocessing, and data labeling were summarized. CNN architectures were reviewed based on the computer vision tasks in animal farming. Strategies of algorithm development included distribution of development data, data augmentation, hyperparameter tuning, and selection of evaluation metrics. Judgment of model performance and performance based on architectures were discussed. Besides practices in optimizing CNN-based computer vision systems, system applications were also organized based on year, country, animal species, and purposes. Finally, recommendations on future research were provided to develop and improve CNN-based computer vision systems for improved welfare, environment, engineering, genetics, and management of farm animals.},
DOI = {10.3390/s21041492}
}



@Article{rs13040808,
AUTHOR = {Neupane, Bipul and Horanont, Teerayut and Aryal, Jagannath},
TITLE = {Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {808},
URL = {https://www.mdpi.com/2072-4292/13/4/808},
ISSN = {2072-4292},
ABSTRACT = {Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.},
DOI = {10.3390/rs13040808}
}



@Article{app11041950,
AUTHOR = {Qi, Haixia and Liang, Yu and Ding, Quanchen and Zou, Jun},
TITLE = {Automatic Identification of Peanut-Leaf Diseases Based on Stack Ensemble},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1950},
URL = {https://www.mdpi.com/2076-3417/11/4/1950},
ISSN = {2076-3417},
ABSTRACT = {Peanut is an important food crop, and diseases of its leaves can directly reduce its yield and quality. In order to solve the problem of automatic identification of peanut-leaf diseases, this paper uses a traditional machine-learning method to ensemble the output of a deep learning model to identify diseases of peanut leaves. The identification of peanut-leaf diseases included healthy leaves, rust disease on a single leaf, leaf-spot disease on a single leaf, scorch disease on a single leaf, and both rust disease and scorch disease on a single leaf. Three types of data-augmentation methods were used: image flipping, rotation, and scaling. In this experiment, the deep-learning model had a higher accuracy than the traditional machine-learning methods. Moreover, the deep-learning model achieved better performance when using data augmentation and a stacking ensemble. After ensemble by logistic regression, the accuracy of residual network with 50 layers (ResNet50) was as high as 97.59%, and the F1 score of dense convolutional network with 121 layers (DenseNet121) was as high as 90.50. The deep-learning model used in this experiment had the greatest improvement in F1 score after the logistic regression ensemble. Deep-learning networks with deeper network layers like ResNet50 and DenseNet121 performed better in this experiment. This study can provide a reference for the identification of peanut-leaf diseases.},
DOI = {10.3390/app11041950}
}



@Article{rs13040814,
AUTHOR = {Megahed, Yasmine and Shaker, Ahmed and Yan, Wai Yeung},
TITLE = {Fusion of Airborne LiDAR Point Clouds and Aerial Images for Heterogeneous Land-Use Urban Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {814},
URL = {https://www.mdpi.com/2072-4292/13/4/814},
ISSN = {2072-4292},
ABSTRACT = {The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors.},
DOI = {10.3390/rs13040814}
}



@Article{s21051571,
AUTHOR = {Bonci, Andrea and Cen Cheng, Pangcheng  David and Indri, Marina and Nabissi, Giacomo and Sibona, Fiorella},
TITLE = {Human-Robot Perception in Industrial Environments: A Survey},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1571},
URL = {https://www.mdpi.com/1424-8220/21/5/1571},
PubMedID = {33668162},
ISSN = {1424-8220},
ABSTRACT = {Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.},
DOI = {10.3390/s21051571}
}



@Article{rs13050837,
AUTHOR = {Bajić, Milan and Bajić, Milan},
TITLE = {Modeling and Simulation of Very High Spatial Resolution UXOs and Landmines in a Hyperspectral Scene for UAV Survey},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {837},
URL = {https://www.mdpi.com/2072-4292/13/5/837},
ISSN = {2072-4292},
ABSTRACT = {This paper presents methods for the modeling and simulation of explosive target placement in terrain spectral images (i.e., real hyperspectral 90-channel VNIR data), considering unexploded ordnances, landmines, and improvised explosive devices. The models used for landmine detection operate at sub-pixel levels. The presented research uses very fine spatial resolutions, 0.945 × 0.945 mm for targets and 1.868 × 1.868 cm for the scene, where the number of target pixels ranges from 52 to 116. While previous research has used the mean spectral value of the target, it is omitted in this paper. The model considers the probability of detection and its confidence intervals, which are derived and used in the analysis of the considered explosive targets. The detection results are better when decreased target endmembers are used to match the scene resolution, rather than using endmembers at the full resolution of the target. Unmanned aerial vehicles, as carriers of snapshot hyperspectral cameras, enable flexible target resolution selection and good area coverage.},
DOI = {10.3390/rs13050837}
}



@Article{s21051617,
AUTHOR = {Safonova, Anastasiia and Guirado, Emilio and Maglinets, Yuriy and Alcaraz-Segura, Domingo and Tabik, Siham},
TITLE = {Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask R-CNN},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1617},
URL = {https://www.mdpi.com/1424-8220/21/5/1617},
PubMedID = {33668984},
ISSN = {1424-8220},
ABSTRACT = {Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.},
DOI = {10.3390/s21051617}
}



@Article{app11052105,
AUTHOR = {Papić, Vladan and Šolić, Petar and Milan, Ante and Gotovac, Sven and Polić, Miljenko},
TITLE = {High-Resolution Image Transmission from UAV to Ground Station for Search and Rescue Missions Planning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2105},
URL = {https://www.mdpi.com/2076-3417/11/5/2105},
ISSN = {2076-3417},
ABSTRACT = {Search and rescue (SAR) missions comprise search for, and provision of aid to people who are in distress or imminent danger. Providing the best possible input for the planners and search teams, up-to-date information about the terrain is of essential importance because every additional hour needed to search a person decreases probability of success. Therefore, availability of aerial images and updated terrain maps as a basis for planning and monitoring SAR missions in real-time is very important for rescuers. In this paper, we present a system for transmission of high-resolution images from an unmanned aerial vehicle (UAV) to the ground station (GS). We define and calculate data rate and transmission distance requirements between the UAV and GS in a mission scenario. Five tests were designed and carried out to confirm the viability of the proposed system architecture and modules. Test results present throughput measurements for various UAV and GS distances, antenna heights and UAV antenna yaw angles. Experimental results from the series of conducted outdoor tests show that the proposed solution using two pMDDL2450 datalinks at 2.4 GHz and a directional antenna on the receiving side can be used for a real-time transmission of high-resolution images acquired with a camera on a UAV. Achieved throughput at a UAV-GS distance of 5 km was 1.4 MB/s (11.2 Mbps). The limitations and possible improvements of the proposed system as well as future work are also discussed.},
DOI = {10.3390/app11052105}
}



@Article{rs13050898,
AUTHOR = {Sadeghi-Tehran, Pouria and Virlet, Nicolas and Hawkesford, Malcolm J.},
TITLE = {A Neural Network Method for Classification of Sunlit and Shaded Components of Wheat Canopies in the Field Using High-Resolution Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {898},
URL = {https://www.mdpi.com/2072-4292/13/5/898},
ISSN = {2072-4292},
ABSTRACT = {(1) Background: Information rich hyperspectral sensing, together with robust image analysis, is providing new research pathways in plant phenotyping. This combination facilitates the acquisition of spectral signatures of individual plant organs as well as providing detailed information about the physiological status of plants. Despite the advances in hyperspectral technology in field-based plant phenotyping, little is known about the characteristic spectral signatures of shaded and sunlit components in wheat canopies. Non-imaging hyperspectral sensors cannot provide spatial information; thus, they are not able to distinguish the spectral reflectance differences between canopy components. On the other hand, the rapid development of high-resolution imaging spectroscopy sensors opens new opportunities to investigate the reflectance spectra of individual plant organs which lead to the understanding of canopy biophysical and chemical characteristics. (2) Method: This study reports the development of a computer vision pipeline to analyze ground-acquired imaging spectrometry with high spatial and spectral resolutions for plant phenotyping. The work focuses on the critical steps in the image analysis pipeline from pre-processing to the classification of hyperspectral images. In this paper, two convolutional neural networks (CNN) are employed to automatically map wheat canopy components in shaded and sunlit regions and to determine their specific spectral signatures. The first method uses pixel vectors of the full spectral features as inputs to the CNN model and the second method integrates the dimension reduction technique known as linear discriminate analysis (LDA) along with the CNN to increase the feature discrimination and improves computational efficiency. (3) Results: The proposed technique alleviates the limitations and lack of separability inherent in existing pre-defined hyperspectral classification methods. It optimizes the use of hyperspectral imaging and ensures that the data provide information about the spectral characteristics of the targeted plant organs, rather than the background. We demonstrated that high-resolution hyperspectral imagery along with the proposed CNN model can be powerful tools for characterizing sunlit and shaded components of wheat canopies in the field. The presented method will provide significant advances in the determination and relevance of spectral properties of shaded and sunlit canopy components under natural light conditions.},
DOI = {10.3390/rs13050898}
}



@Article{app11052163,
AUTHOR = {Munaye, Yirga Yayeh and Juang, Rong-Terng and Lin, Hsin-Piao and Tarekegn, Getaneh Berie and Lin, Ding-Bing},
TITLE = {Deep Reinforcement Learning Based Resource Management in UAV-Assisted IoT Networks},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2163},
URL = {https://www.mdpi.com/2076-3417/11/5/2163},
ISSN = {2076-3417},
ABSTRACT = {The resource management in wireless networks with massive Internet of Things (IoT) users is one of the most crucial issues for the advancement of fifth-generation networks. The main objective of this study is to optimize the usage of resources for IoT networks. Firstly, the unmanned aerial vehicle is considered to be a base station for air-to-ground communications. Secondly, according to the distribution and fluctuation of signals; the IoT devices are categorized into urban and suburban clusters. This clustering helps to manage the environment easily. Thirdly, real data collection and preprocessing tasks are carried out. Fourthly, the deep reinforcement learning approach is proposed as a main system development scheme for resource management. Fifthly, K-means and round-robin scheduling algorithms are applied for clustering and managing the users’ resource requests, respectively. Then, the TensorFlow (python) programming tool is used to test the overall capability of the proposed method. Finally, this paper evaluates the proposed approach with related works based on different scenarios. According to the experimental findings, our proposed scheme shows promising outcomes. Moreover, on the evaluation tasks, the outcomes show rapid convergence, suitable for heterogeneous IoT networks, and low complexity.},
DOI = {10.3390/app11052163}
}



@Article{s21051688,
AUTHOR = {Ali, Luqman and Alnajjar, Fady and Jassmi, Hamad Al and Gocho, Munkhjargal and Khan, Wasif and Serhani, M. Adel},
TITLE = {Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1688},
URL = {https://www.mdpi.com/1424-8220/21/5/1688},
PubMedID = {33804490},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.},
DOI = {10.3390/s21051688}
}



@Article{rs13050937,
AUTHOR = {Najafi, Payam and Feizizadeh, Bakhtiar and Navid, Hossein},
TITLE = {A Comparative Approach of Fuzzy Object Based Image Analysis and Machine Learning Techniques Which Are Applied to Crop Residue Cover Mapping by Using Sentinel-2 Satellite and UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {937},
URL = {https://www.mdpi.com/2072-4292/13/5/937},
ISSN = {2072-4292},
ABSTRACT = {Conservation tillage methods through leaving the crop residue cover (CRC) on the soil surface protect it from water and wind erosions. Hence, the percentage of the CRC on the soil surface is very critical for the evaluation of tillage intensity. The objective of this study was to develop a new methodology based on the semiautomated fuzzy object based image analysis (fuzzy OBIA) and compare its efficiency with two machine learning algorithms which include: support vector machine (SVM) and artificial neural network (ANN) for the evaluation of the previous CRC and tillage intensity. We also considered the spectral images from two remotely sensed platforms of the unmanned aerial vehicle (UAV) and Sentinel-2 satellite, respectively. The results indicated that fuzzy OBIA for multispectral Sentinel-2 image based on Gaussian membership function with overall accuracy and Cohen’s kappa of 0.920 and 0.874, respectively, surpassed machine learning algorithms and represented the useful results for the classification of tillage intensity. The results also indicated that overall accuracy and Cohen’s kappa for the classification of RGB images from the UAV using fuzzy OBIA method were 0.860 and 0.779, respectively. The semiautomated fuzzy OBIA clearly outperformed machine learning approaches in estimating the CRC and the classification of the tillage methods and also it has the potential to substitute or complement field techniques.},
DOI = {10.3390/rs13050937}
}



@Article{rs13050939,
AUTHOR = {Xue, Yongan and Zhao, Jinling and Zhang, Mingmei},
TITLE = {A Watershed-Segmentation-Based Improved Algorithm for Extracting Cultivated Land Boundaries},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {939},
URL = {https://www.mdpi.com/2072-4292/13/5/939},
ISSN = {2072-4292},
ABSTRACT = {To accurately extract cultivated land boundaries based on high-resolution remote sensing imagery, an improved watershed segmentation algorithm was proposed herein based on a combination of pre- and post-improvement procedures. Image contrast enhancement was used as the pre-improvement, while the color distance of the Commission Internationale de l´Eclairage (CIE) color space, including the Lab and Luv, was used as the regional similarity measure for region merging as the post-improvement. Furthermore, the area relative error criterion (δA), the pixel quantity error criterion (δP), and the consistency criterion (Khat) were used for evaluating the image segmentation accuracy. The region merging in Red–Green–Blue (RGB) color space was selected to compare the proposed algorithm by extracting cultivated land boundaries. The validation experiments were performed using a subset of Chinese Gaofen-2 (GF-2) remote sensing image with a coverage area of 0.12 km2. The results showed the following: (1) The contrast-enhanced image exhibited an obvious gain in terms of improving the image segmentation effect and time efficiency using the improved algorithm. The time efficiency increased by 10.31%, 60.00%, and 40.28%, respectively, in the RGB, Lab, and Luv color spaces. (2) The optimal segmentation and merging scale parameters in the RGB, Lab, and Luv color spaces were C for minimum areas of 2000, 1900, and 2000, and D for a color difference of 1000, 40, and 40. (3) The algorithm improved the time efficiency of cultivated land boundary extraction in the Lab and Luv color spaces by 35.16% and 29.58%, respectively, compared to the RGB color space. The extraction accuracy was compared to the RGB color space using the δA, δP, and Khat, that were improved by 76.92%, 62.01%, and 16.83%, respectively, in the Lab color space, while they were 55.79%, 49.67%, and 13.42% in the Luv color space. (4) Through the visual comparison, time efficiency, and segmentation accuracy, the comprehensive extraction effect using the proposed algorithm was obviously better than that of RGB color-based space algorithm. The established accuracy evaluation indicators were also proven to be consistent with the visual evaluation. (5) The proposed method has a satisfying transferability by a wider test area with a coverage area of 1 km2. In addition, the proposed method, based on the image contrast enhancement, was to perform the region merging in the CIE color space according to the simulated immersion watershed segmentation results. It is a useful attempt for the watershed segmentation algorithm to extract cultivated land boundaries, which provides a reference for enhancing the watershed algorithm.},
DOI = {10.3390/rs13050939}
}



@Article{s21051766,
AUTHOR = {Müezzinoğlu, Taha and Karaköse, Mehmet},
TITLE = {An Intelligent Human–Unmanned Aerial Vehicle Interaction Approach in Real Time Based on Machine Learning Using Wearable Gloves},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1766},
URL = {https://www.mdpi.com/1424-8220/21/5/1766},
PubMedID = {33806388},
ISSN = {1424-8220},
ABSTRACT = {The interactions between humans and unmanned aerial vehicles (UAVs), whose applications are increasing in the civilian field rather than for military purposes, are a popular future research area. Human–UAV interactions are a challenging problem because UAVs move in a three-dimensional space. In this paper, we present an intelligent human–UAV interaction approach in real time based on machine learning using wearable gloves. The proposed approach offers scientific contributions such as a multi-mode command structure, machine-learning-based recognition, task scheduling algorithms, real-time usage, robust and effective use, and high accuracy rates. For this purpose, two wearable smart gloves working in real time were designed. The signal data obtained from the gloves were processed with machine-learning-based methods and classified multi-mode commands were included in the human–UAV interaction process via the interface according to the task scheduling algorithm to facilitate sequential and fast operation. The performance of the proposed approach was verified on a data set created using 25 different hand gestures from 20 different people. In a test using the proposed approach on 49,000 datapoints, process time performance of a few milliseconds was achieved with approximately 98 percent accuracy.},
DOI = {10.3390/s21051766}
}



